{
    "paper_title": "MOSPA: Human Motion Generation Driven by Spatial Audio",
    "authors": [
        "Shuyang Xu",
        "Zhiyang Dou",
        "Mingyi Shi",
        "Liang Pan",
        "Leo Ho",
        "Jingbo Wang",
        "Yuan Liu",
        "Cheng Lin",
        "Yuexin Ma",
        "Wenping Wang",
        "Taku Komura"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains a key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop a simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA could generate diverse realistic human motions conditioned on varying spatial audio inputs. We perform a thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our model and dataset will be open-sourced upon acceptance. Please refer to our supplementary video for more details."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 9 4 9 1 1 . 7 0 5 2 : r MOSPA: Human Motion Generation Driven by Spatial Audio Shuyang Xu,1, Zhiyang Dou,,1, Mingyi Shi1, Liang Pan1, Leo Ho1, Jingbo Wang2, Yuan Liu3, Cheng Lin4, Yuexin Ma5, Wenping Wang,6, Taku Komura,1 1The University of Hong Kong 2Shanghai AI Lab 3The Hong Kong University of Science and Technology 4Macau University of Science and Technology 5ShanghaiTech University 6Texas A&M University Figure 1: We introduce novel human motion generation task centered on spatial audio-driven human motion synthesis. Top row: We curate novel Spatial Audio-Driven Human Motion (SAM) dataset, including diverse spatial audio signals and high-quality 3D human motion pairs. Bottom row: We develop generative framework for human MOtion generation driven by SPatial Audio (MOSPA) to produce high-quality, responsive human motion driven by spatial audio. We note that the motion generation results are both realistic and responsive, effectively capturing both the spatial and semantic features of spatial audio inputs."
        },
        {
            "title": "Abstract",
            "content": "Enabling virtual humans to dynamically and realistically respond to diverse auditory stimuli remains key challenge in character animation, demanding the integration of perceptual modeling and motion synthesis. Despite its significance, this task remains largely unexplored. Most previous works have primarily focused on mapping modalities like speech, audio, and music to generate human motion. As of yet, these models typically overlook the impact of spatial features encoded in spatial audio signals on human motion. To bridge this gap and enable high-quality modeling of human movements in response to spatial audio, we introduce the first comprehensive Spatial Audio-Driven Human Motion (SAM) dataset, which contains diverse and high-quality spatial audio and motion data. For benchmarking, we develop simple yet effective diffusion-based generative framework for human MOtion generation driven by SPatial Audio, termed MOSPA, which faithfully captures the relationship between body motion and spatial audio through an effective fusion mechanism. Once trained, MOSPA could generate diverse , denote equal contributions and corresponding authors. Preprint. Under review. realistic human motions conditioned on varying spatial audio inputs. We perform thorough investigation of the proposed dataset and conduct extensive experiments for benchmarking, where our method achieves state-of-the-art performance on this task. Our model and dataset will be open-sourced upon acceptance. Please refer to our supplementary video for more details."
        },
        {
            "title": "Introduction",
            "content": "Humans exhibit varying responses to different auditory inputs within given space. For instance, when exposed to sharp, piercing sounds, individuals are likely to cover their ears and move away in the direction opposite to the sound source. Conversely, when the sound is soft and soothing, they may approach it out of curiosity or to investigate further. Therefore, generating realistic human motion for virtual characters to respond realistically to variety of sounds in their environment is both highly sought-after feature and is crucial for applications such as virtual reality, human-computer interaction, robotics, etc. Unfortunately, while previous studies have extensively explored motion generation from action label [82, 30], text [90, 97, 74], music [75, 88, 43, 1], and speech [2, 94, 3, 99], human motion generation driven by spatial audio remains unexplored to the best of our knowledge. Unlike pure audio signals, e.g., music [68, 45, 75], speech [3, 94], the spatial audio signals not only does it encode semantics, but it also captures spatial characteristics that significantly influence body movements, requiring specialized framework to accurately model motion responses to spatial audio stimuli. To address this overlooked aspect, we propose to model the complex interactions between spatial audio inputs and human motion using generative model. Since there is no such dataset tailored for this task, we first introduce the SAM dataset (Spatial Audio Motion dataset), which captures diverse human responses to various spatial audio conditions. This dataset is meticulously curated to include wide range of spatial audio scenarios, enabling the study of motion conditioned on sound field variations. The SAM dataset has total of more than 9 hours of motion, covering 27 common spatial audio scenarios and more than 70 audio clips. To ensure the diversity of the spatial audio, 480 seconds of motion are captured for each audio clip at different positions in the character space. To ensure diverse motion responses to spatial audio, we introduce 20 distinct motion types (excluding motion genres) and 49 in total when motion genres are included. We visualize samples from SAM in the top row of Fig. 1. See Appendix for detailed statistics. We further conduct benchmarking experiments on the proposed dataset, revealing the limitations of existing methods in this setting. To enable spatial audio-driven human motion generation, we introduce MOSPA, simple yet effective framework tailored for this task. In real-world scenarios, human responses to sound are inherently influenced by spatial perception, intensity variations, directional cues, temporal dynamics, etc. Motivated by this, we generate motion by incorporating features extracted from the input spatial audio signals using [58]. Specifically, to capture intrinsic features across both temporal and spatial dimensions, we mainly utilize Mel-Frequency Cepstral Coefficients (MFCCs)[16] and Tempograms[27] to model the temporal characteristics of the audio. Additionally, we characterize the spatial audio by analyzing the root mean square (RMS) [58] energy, which quantifies signal intensity in audio processing. These features enhance the effective modeling of the spatial and intensity variations of the spatial audio. To capture the distribution of spatial audio features and human motion dynamics effectively, we employ diffusion-based generative model that ensures strong alignment between the two modalitieshuman motion and spatial audio signals. Leveraging diffusion models, MOSPA excels at modeling the complex interplay between spatial audio features and human motion. Besides, residual feature fusion mechanism is employed to model the subtle influences of spatial audio on human movement. Extensive evaluations on the SAM demonstrate that MOSPA achieves state-of-the-art performance on this task, outperforming existing baselines in generating realistic and diverse motion responses to spatial audio. Our contributions are summarized as follows: We introduce novel task of spatial audio-conditioned motion generation and present the first comprehensive dataset SAM with over 9 hours of motion across diverse scenarios. We conduct extensive benchmarking and propose MOSPA, diffusion-based generative framework tailored for modeling and generating diverse human motions from spatial audio. 2 We achieve the SOTA performance on motion generation conditioned on spatial audio. Our dataset, code, and models will be publicly released for further research."
        },
        {
            "title": "2 Related Work",
            "content": "Spatial Audio Many studies have explored spatial audio modeling, perception, and generation [95, 23, 96, 71, 36, 84]. For instance, [95] utilizes the natural synchronization between visual and audio modalities to learn models that jointly parse sounds and images without manual annotations. [23] leverages unlabeled audiovisual data to localize objects, such as moving vehicles, using only stereo sound at inference time. [96] introduces an approach for reasoning about spatial sounds with large language models. Recently, spatial audio generation has been explored from text [71] and video [41]. [36] presents framework for spatial audio generation, capable of rendering 3D soundfields generated by human actions, including speech, footsteps, and hand-body interactions. [84] models 3D spatial audio from body motion and speech, while challenges remain to fully capture these dynamics. Conditional Motion Generation Extensive efforts have been made into motion synthesis conditioning on user control signals [35, 69, 10, 76, 89], text [90, 97, 74, 29, 48, 50, 13], action [82, 30, 8, 38], music [75, 88, 43, 1], speech [2, 65, 39, 94, 3, 99], past trajectories [22, 9, 4, 87, 70], etc. We refer readers to [100] for detailed survey of motion generation. Text-to-Motion. Text-to-motion generation has recently gained popularity as an intuitive and userfriendly approach to synthesizing diverse human body motions. Generative pre-trained transformer frameworks have been utilized for text-to-motion generation [90, 54]. Subsequently, various generation techniques have been explored, including diffusion models [74], latent diffusion models [11], autoregressive diffusion model [66, 10], denoising diffusion GANs [97], consistency models [15], and generative masked modeling [28]. Recent advancements include the integration of motion generation with large language models [40, 91] and investigations into the scaling laws for motion generation [55, 21]. Recently, controllable text-to-motion generation has gained attention, enabling motion synthesis conditioned on both text prompts and control signals, e.g., target control points [81, 76]. Music-to-Motion. Recent advancements in Music-to-Motion generation have been made [20, 68, 47, 75, 26, 85, 67]. DanceFormer [44] adopts two-stage approach, generating key poses for beat synchronization followed by parametric motion curves for smooth, rhythm-aligned movements. Bailando [68] utilizes VQ-VAE to encode motion features via choreographic memory module. [1] introduces diffusion-based probabilistic model for motion generation, using Conformer-based architecture. EDGE [75] also applies diffusion model for dance generation and editing. Furthermore, multimodal approaches incorporating language and music enhance generation quality [26, 85, 14]. Speech-to-Motion. We mainly review studies on audio-driven motion (gesture) generation [94, 12, 3, 2, 99]. Early works are mostly based on GAN models [25, 51, 62, 86], while the recent attempts are mainly based on the generative diffusion model [99]. For instance, [94] proposes generative retrieval framework leveraging large language model to efficiently retrieve semantically appropriate gesture candidates from motion library in response to input speech. [2] introduces co-speech gesture synthesis method by employing segmentation pipeline for temporal alignment and disentangling speech-motion embeddings to capture both semantics and subtle variations. While audio signals have been widely used in musicand speech-to-motion tasks, human motion synthesis driven by spatial audio remains largely unexplored. As result, data-driven methods are highly constrained by limited paired data. The goal of this paper is to develop comprehensive dataset and novel approach for high-quality spatial audio-driven motion synthesis."
        },
        {
            "title": "3 SAM Dataset",
            "content": "We first introduce the Spatial Audio-driven Motion (SAM) dataset designed for human motion synthesis conditioned on spatial audio. We focus on binaural audio, common form of spatial audio that aligns with human and (most) animal perception and can be readily applied to robotic platforms. SAM consists of more than 9 hours of human motions with corresponding binaural audio, and more than 4M frames, covering 27 common spatial audio scenarios and 20 common reaction types in daily life without counting the motion genres. The majority of the audio clips are sourced from the AudioSet [24], while only small portion is extracted from publicly available YouTube videos or through manual recording. More detailed information can be found in Tab. 1 and Appendix A. Visualization results are in Fig. 2. 3 Table 1: Statistics of the SAM dataset. The SAM dataset encompasses 27 common daily spatial audio scenarios, over 20 reaction types excluding the motion genres, and 49 reaction types (see details in Appendix A). The number of subjects covered in SAM is 12, where 5 of them are female and the remaining 7 are male. It is also the first dataset to incorporate spatial audio information, annotated with Sound Source Location (SSL). The total duration of the dataset exceeds 33K seconds. Dataset SSL Dance w/ Melody [72] Music2Dance [101] AIST++ [47] PopDanceSet [57] FineDance [46] SAM (Ours) 3DJointpos/rot Model / / / / / / - - COCO/SMPL COCO/SMPL SMPL-X SMPL-X Joints 21 55 17/24 17/24 55 Subjects Seconds - 2 30 132 27 12 5640 3472 18694 12819 5061K 33600 (a) Look for sound source and approach upon hearing miaow at the left hand side. (b) Step away with ears covered upon hearing crowd yelling at the back. (c) Run away from the sound source upon hearing gunshot at the right hand side. (d) Walk towards the sound source upon hearing insect noise at the left hand side. (e) Start dancing upon hearing music in front of the character. (f) Look for sound source upon hearing the phone ring at the left hand side. Figure 2: Visualization of samples from SAM with expected motions annotated. Red dots indicate the actors trajectory, while the blue sphere represents the sound source. The SAM dataset ensures high diversity by encompassing broad spectrum of audio types and varying sound source locations. 4 (i) Mocap environment. (ii) The speakers. Figure 3: Spatial audio-driven human motion data collection setup. Figure 4: Statistics of action duration in the dataset. Data Capture Settings. We utilize Vicon motion capture system [53] to collect motion data and spatial audio signals. The motion capture is performed in space of 5m 10m 3m, with 28 cameras recording at frame rate of 120 Hz; See Fig. 3. In SAM, each audio clip is paired with 16 randomly sampled relative locations of the sound source. For each location, we capture three motion sequences corresponding to different reaction intensities: dull, neutral, and sensitive, resulting in total of 48 motion sequences, each lasting 10 seconds. Fig. 4 shows the statistics of the approximate duration of the actions. The three motion genres define the varying degrees of responsiveness, decreasing from sensitive to dull. For instance, upon hearing an explosion, dull individual might remain largely unreactive, whereas sensitive one may immediately flee from the sound source. The total number of action types is 49. The percentage of action types covered within the dull, neutral, and sensitive motion genres are 28.57%, 34.69%, and 36.73%, respectively. To capture the binaural sound heard at the position of the actor, we employ two microphones to record the audio at the ear positions of the actors separately; See the inset. The two microphones are connected to Deity PR-2 recorder [17] that has been synchronized with the Vicon mocap system in advance using timecode with frame rate of 30 FPS. With this setting, the stereo sound at the position of the actor can be recorded and has an accurate alignment with the corresponding motion. Data Processing. All motion and audio clips are precisely aligned. The motions are re-targeted and converted from the original BVH format in Vicon to the SMPL-X [60] format. SMPL-X is parametric 3D human body model that encompasses the body, hands, and face, comprising = 10, 475 vertices and = 55 joints. Given shape parameters β and pose parameters θ, the SMPL-X model generates the corresponding body shape and pose through forward dynamics. We extract the locations of the sound sources in each motion clip. The sound source locations are then transformed into the local space of the character aligned with the SMPL-X local coordinate system (also known as the local frame)."
        },
        {
            "title": "4 Method",
            "content": "We introduce MOSPA, diffusion-based probabilistic model that serves as baseline for this novel task of spatial audio-driven human motion generation. First, we extract spatial audio features using feature extractor [58]. During motion generation, the extracted spatial audio feature is combined with the sound source location and the motion genre as conditioning inputs. These inputs are passed to denoiser G, which is trained to reconstruct the original clean motion vector ˆx0 by denoising the given noisy motion vector xt at time step t. Mathematically, we have ˆx0 = G(xt, t; a, s, g)"
        },
        {
            "title": "4.1 Feature Representation",
            "content": "The two vectors that we care the most about are the audio feature vector and the motion vector x. We carefully design the structure of the two vectors in MOSPA. Spatial Audio Feature Extraction. We first extract range of audio features that capture intensity, temporal dynamics, and spatial characteristics. Inspired by [68], our feature set primarily includes Melfrequency cepstral coefficients (MFCC), MFCC delta, constant-Q chromagram, short-time Fourier transform (STFT) of the chromagram, onset strength, tempogram, and beats [16, 64, 27, 58, 6, 19]. 5 On top of these audio features, we additionally add the root mean square (RMS) energy Erms of the audio [58], and the active frames Factive defined as Factive = Erms > 0.01 to capture the distance information of the audio. The dimension of the audio feature vector for each ear is 1136. By concatenating the features from both ears, we obtain combined feature vector of dimension 2272. The detailed construction of the audio vector can be viewed in Appendix B.1. Motion Representation. In this paper, we focus on body motion and leave the modeling of detailed finger movements to future work. Therefore, we exclude all the finger joints and retain only the first = 25 body joints of the SMPL-X model [60]. In addition to the essential translation and joint rotations required for human pose representation, we introduce the residual feature fusion mechanism [29] to incorporate the global joint positions and the velocity of the joints to capture the nuanced difference in audio and further improve the accuracy of the generated samples. Each motion vector is thus composed of the global positions RT (J3), the local rotations RT (J6) and the velocities RT (J3) of the joints (including the root), where = 240 represents the number of frames in each motion sequence. The joint rotations are represented in the 6d format [98] to guarantee the continuity of the change (x0 = (p0, r0, v0), RT (J12)). The dimension of each motion vector is therefore 300."
        },
        {
            "title": "4.2 Framework",
            "content": "Following [74, 10], the diffusion is modeled as Markov chain process which progressively adds noise to clean motion vectors x0 in time steps, i.e. q(xtxt1) = ( αtxt1, (1 αt)I) (1) where αt (0, 1). The model then learns to gradually denoise noisy motion vector xt in time steps, i.e. p(xt1xt). We directly predict the clean sample ˆx0 in each diffusion step ˆx0 = G(xt, t; a, s, g), where is the audio features, is the sound source location and is the motion genre. This strategy, employed by [74, 10, 63, 81], has been proved to be more efficient and accurate than predicting the noise ϵt, suggested by [34]. We employ an encoderonly transformer to reverse the diffusion process and predict the clean samples. The timestep, motions and conditions are projected to the same latent dimension by separate feed-forward networks, and then concatenated together to provide the full tokens z. The tokens are positionally embedded afterward and input into transformer to get the output ˆz. The predicted clean sample is thus extracted from the last tokens of ˆz by inputting it to another feed-forward network, where = 240 is the length of the motion; see Fig. 5. Figure 5: The framework of MOSPA. We perform diffusion-based motion generation given spatial audio inputs. Specifically, Gaussian noise is added to the clean motion sample x0, generating noisy motion vector xt, modeled as q(xtxt1). An encoder transformer then predicts the clean motion from the noisy motion xt, guided by extracted audio features a, sound source location (SSL) s, motion genre g, and timestep t."
        },
        {
            "title": "4.3 Loss Functions",
            "content": "We train MOSPA using the following loss functions. simple mean squared error (MSE) loss is applied to the original clean sample and the predicted clean sample as the main objective: ˆx0 x02 2. To guarantee the smooth variation on the predicted clean sample across frames, we also apply MSE loss to the rate of change of the vectors across frames: Eδ ˆx0 δx02 2. Combining the two simple losses we have Ldata = ˆx0 x02 2. Geometric losses, encompassing position loss and velocity loss, are also incorporated, as we rely solely on joint rotations and translations in motion vectors to represent poses: Lgeo = EF K( ˆx0) K(x0)2 2 + EδF K( ˆx0) δF K(x0)2 2. Furthermore, foot sliding is prevented by introducing the foot contact loss Lf oot that measures the inconsistency in the velocities of the foot joints between the ground truth and the predicted motions. 2 + Eδ ˆx0 δx02 6 2 + Eδ ˆtraj0 δtraj02 We also incorporate trajectory loss and joint rotation loss to underscore their importance in achieving the training objectives and accelerate the convergence of the model, defined as Ltraj = ˆtraj0 traj02 2 respectively, where traj is the trajectory vector of the motion sequence and is the joint rotations represented in the 6d format [98]. Given that trajectory and joint rotations are inherently encoded within the motion vectors, these supplementary losses represent an overlap with the existing loss terms, effectively amplifying the emphasis on trajectory and joint rotation accuracy through increased weighting. Empirically, we observe that this implementation accelerates model convergence and facilitates correct displacement direction generation in motion sequences. In sum, the total loss is given by: 2 and Lrot = Eˆr0 r0 2 + Eδˆr0 δr02 = λdataLdata + λgeoLgeo + λf ootLf oot + λtrajLtraj + λrotLrot (2) All loss weights (λ) are initialized set to 1. At epoch 5,000 of the total 6,000 training epochs, λtraj and λrot are increased to 3, thereby intensifying the emphasis on trajectory and rotation accuracy."
        },
        {
            "title": "4.4 Implementation Details",
            "content": "In MOSPA, the diffusion model is transformer-based diffusion network [10, 74, 97]. The encoder transformer is configured with latent dimension of 512, 8 heads, and 4 layers. We employ AdamW [52] as the optimizer with an initial value of 1 104. The number of denoising steps used is 1000, and the noise schedule is cosine. The training phase concludes after 6, 000 epochs. Exceeding these recommended epoch counts may degrade model quality due to overfitting. The entire training process requires approximately 15 hours on single RTX 4090 GPU with batch size of 128."
        },
        {
            "title": "5 Experiments",
            "content": "Experiment Setup. We use our SAM dataset to evaluate the spatial audio-driven motion generation task. As detailed in Sec. 3, it contains 9 hours of human motion with paired binaural audio and corresponding sound source locations, covering 27 common spatial audio scenarios and 20 common reaction types. The dataset is split into training, validation, and test sub-datasets at common ratio of 8:1:1. Consequently, the training sub-dataset comprises 2,400 motion sequences, while the validation and test sub-datasets each contain approximately 300 motion sequences. To keep fair setting [8], the motions and the audio clips are both downsampled to the frame rate of 30 FPS. The character is rotated to face the negative y-axis and initially translated to the origin in the world space in all motion sequences, and the sound source locations (SSL) are transformed to the local space of the character in every single frame. Baselines and Metrics. Our system is the first work to receive spatial audio as input to generate human motion results. To our best knowledge, as there is no other system achieving this, we made adaptations on other audio2motion methods, such as EDGE [75], POPDG [57], LODGE [45] and Bailando [68] by replacing their original audio input with our spatial audio feature as input. We evaluated four metrics, focusing on motion quality and diversity: 1) R-precision, FID, Diversity These three metrics are calculated using the same setup proposed by [29]. Two bidirectional GRU are trained with hidden size of 1024 for 1,500 epochs with batch size of 64 to extract the audio features and the corresponding motion features, as suggested by [29]. Detailed implementation details of the feature extractor are provided in Appendix B.2. 2) APD [18, 32] is calculated by sj motion sequences, is the number of motion sequences in the set , is the number of frames of each motion sequence, and si ˆxi is state in the motion sequence ˆxi. , where = { ˆxi} is the set of generated AP D(M ) = 2(cid:17) 1 1 (N 1) t=1 si (cid:16)(cid:80)L j=1 j=i (cid:80)N (cid:80)N i="
        },
        {
            "title": "5.1 Comparisons",
            "content": "Qualitative Results. We demonstrate the qualitative comparison in Fig. 6. For the same input spatial audio, our methods show the superiority of producing high-quality and realistic response motion. Other methods often exhibit various limitations due to their unique model characteristics. EDGE [75] and POPDG [57] demonstrate relatively strong performance among the four baselines, sharing diffusion-based foundation with MOSPA, despite differences in their encoding and decoding mechanisms. Their shortcomings in generated samples can primarily be attributed to model size and 7 Figure 6: Qualitative comparison of state-of-the-art methods for the spatial audio-to-motion task. We visualize motion results from five cases. MOSPA produces high-quality movements that closely correspond to the input spatial audio. We provide Expected Motion as description for reference. Table 2: Quantitative evaluation on the SAM, where MOSPA achieves higher alignment with the GT motion while maintaining high diversity, as reflected by the metrics. The error bar is the 95% confidence interval assuming normal distribution, and means the closer to Real Motion the better. Method R-precision Top1 Top2 Top3 FID Diversity APD Real Motion 1.0000.000 1.0000.000 1.0000.000 0. 23.6160.188 59.435 EDGE [75] 0.8860.005 0.9600.003 0.9770.002 13.993 23.0990.196 43.882 POPDG [57] 0.7620.006 0.8860.005 0.9340.003 20.967 22.5360.170 34.996 LODGE [45] 0.4440.006 0.5940.005 0.6790.004 102.289 21.1010.141 11.801 Bailando [68] 0.0770.003 0.1340.003 0.1820.004 168.396 17.3470.247 23.121 MOSPA 0.9370.005 0.9840.002 0.9960.001 7.981 23.5750.188 53.915 their strong focus on music-like audio. The bad performance of LODGE [45] is likely due to its specialization in long-term music-like audio, resulting in deficiencies when handling short-term audio information with abrupt feature changes. Similarly, Bailando [68] faces challenges in processing rapidly changing spatial audio. More critically, due to its separate training process for upper and lower body parts, Bailando occasionally produces distorted or disjointed motions when encountering sudden changes in spatial audio. Please watch our supplementary video for more results. Furthermore, we test MOSPA on out-of-distribution audio-source configurations. As shown in Fig. 8, it maintains motion quality and intent alignment, demonstrating robustness to unseen spatial setups. 8 Table 3: Ablation study on MOSPA on the spatial audio-driven motion generation performance. The error bar is the 95% confidence interval assuming normal distribution, and means the closer to real motions the better. Latent Dim Head Num Diff Steps Genre R-precision Top1 Top Top3 FID Diversity APD Real Motion 1.0000.000 1.0000.000 1.0000.000 0.001 23.6160.188 59.435 1000 0.9370.005 0.9840.002 0.9960.001 7.981 23.5750.188 53.915 1000 0.8910.005 0.9520.002 0.9710.001 9.226 23.0070.198 55.175 1000 0.9230.004 0.9720.002 0.9860.001 9.282 23.2320.170 56.572 100 0.9300.004 0.9800.002 0.9910.001 8.456 23.3510.177 49.824 0.9340.004 0.9890.002 0.9980.001 8.387 23.4740.192 49.507 4 1000 0.8890.005 0.9580.003 0.9770.002 10.930 23.1500.153 46.807 512 256 512 512 512 512 8 8 4 8 8 Quantitative Results. The quantitative results are reported in Tab. 2. MOSPA achieves the best performance as shown by the lowest FID value and the highest R-precision values. Also, our generated motions exhibit the closest diversity and APD [18] values compared with the Real Motion, demonstrating the effectively balanced variation and precision. Bailando [68] has the worst performance among the four baselines in practice, as illustrated by the extremely high FID. The model possibly lacks an ability to perceive commonly heard sounds other than music and also spatial information of the audio. Our method, overall speaking, still demonstrates competitive performance in spatial audio conditioned motion generation, which is proved by the low values in precision-related metrics. User Study. We conducted user study with 25 participants to assess the perceptual quality of motion generation. Participants evaluated five models (MOSPA, EDGE, POPDG, LODGE, Bailando) alongside ground truth (GT), selecting the best motion for: 1) Human Intent Alignment: Does the motion align with real-world intent? 2) Motion Quality: Which has the highest movement quality? 3) GT Similarity:Which best matches the GT motion? We provided GT motion and textual description for reference. As shown in Fig. 7, MOSPA (Ours) outperforms all baselines across all criteria, while LODGE and Bailando received the fewest selections, indicating limitations in generating realistic, semantically meaningful motions. See more details in Appendix C."
        },
        {
            "title": "5.2 Ablation Study",
            "content": "Figure 7: User study results. MOSPA outperforms other methods in intent alignment, motion quality, and similarity to ground truth. The bar chart shows the vote distribution across methods. We conducted ablation studies on the latent dimension, the number of attention heads, the diffusion step number, and the masking of motion genre, with results summarized in Tab. 3. All ablation experiments maintained consistent training epoch counts throughout. Latent Dimension. The default latent dimension of MOSPAs encoder transformer is 512. In our study, reducing it to 256 slightly increases the APD [18] value but also degrades the R-precision and the FID, leading to an overall decline in model performance, as illustrated in row 1 and row 2 in Tab. 3. Number of Attention Heads. We reduced the number of attention heads in MOSPA encoder transformer from 8 to 4, observing degradation in almost all of the metrics except slight improvement in APD [18]. This reduction compromises overall model performance without yielding significant improvements in training efficiency, as seen in row 1 and row 3 in Tab. 3. Number of Diffusion Steps. We evaluated MOSPA with varying diffusion step numbers, reducing it from 1000 to 100 and further to 4, as detailed in rows 1, 4, and 5 of Tab. 3. Fewer steps slightly Figure 8: Test of MOSPA on out-of-distribution spatial audios. Descriptions of motions are provided for reference. Figure 9: Spatial audio-driven physically simulated humanoid robot control based on [33]. Descriptions of expected motion are provided for reference. degrade the performance as shown by the increase in FID and degradation in diversity, thereby lower the upper limit of the power of the model. Genre Masking. Masking motion genres leads to degradation in model performance across all metrics, as demonstrated in row 1 and 6 of Tab. 3. Motion genres are required to provide guidance for the model on the intensity of the expected motions."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce task for enabling virtual humans to respond realistically to spatial auditory stimuli. While prior work maps audio to motion, it often neglects the spatial characteristics of spatial audio. We introduce comprehensive SAM dataset, capturing human movement in response to spatial audio, and propose MOSPA, diffusion-based generative model with an attentionbased fusion mechanism. Once trained, MOSPA synthesizes diverse, high-quality motions that adapt to varying spatial audio input with binaural recording. Extensive evaluations show MOSPA achieves state-of-the-art performance on this task. Limitations and Future Works. 1) Physical Correctness: While MOSPA generates diverse and semantically plausible motions, it lacks physical constraints, which may lead to physically implausible artifacts. Integrating physics-based control methods [18, 56, 73, 37, 92, 93, 59, 79] could improve motion realism and embodiment fidelity (see Fig. 9 for spatial audio-driven humanoid robot control). Body Modeling: This work focuses on body 10 motion and omits finer-grained components such as hand gestures and facial expressions supported by SMPL-X [60]. Extending the model to full-body motion generation [54, 49, 83, 61, 5]including hand motionsremains an important direction for future research. Scene Awareness: The current framework does not incorporate awareness of surrounding environments or physical scene geometry, limiting its ability to produce scene-consistent or contact-aware motions. Future extensions could integrate scene representations or affordance prediction [13, 78, 80, 7, 77] with spatial audio signals to enhance human motion generation."
        },
        {
            "title": "Acknowledgements",
            "content": "We are grateful to Yiduo Hao, Chuan Guo, Chen Wang and Peter Qingxuan Wu for their insightful discussions and constructive feedback, which have been helpful to the development of this research."
        },
        {
            "title": "A Dataset Statistics",
            "content": "We present more statistics on our dataset regarding the audio scenarios and the the motion types implemented in the collection of the dataset. Spatial Audio Scenarios The SAM dataset covers 27 common spatial audio scenarios, as listed in Tab. A4. Each scenario is accompanied by two or three audio clips and for each audio clip 48 10-second motion sequences are collected. aircraft bird church bell drum fire alarm insect music sing thunder"
        },
        {
            "title": "Scenario\nbark\ncall\ncough\nengine\nfirework\ninstrument\nphone ringing\nsiren\nvehicle",
            "content": "bicycle bell cat crowd yell explosion gunshot laughter shout speech wind rain Table A4: The 27 common spatial audio settings covered in SAM, each with two or three 10-second audio clips and around 100 motion sequences. Motion Types The SAM dataset covers 20 common reactions to spatial audio in daily life, as presented in Tab. A5. Each motion type includes at least 10 minutes of motion sequences, ensuring balanced dataset. look at step aside shake change trajectory clap hands shout prey"
        },
        {
            "title": "Motion\nstand still\nrun\ndance\nsquat\nlaugh\ncurl\njump",
            "content": "cover ears look around wave hands nod head stretch cover nose Table A5: The 20 common reactions excluding the motion genres covered in SAM. Motion Genres Three general motion genres are covered in the SAM datasetdull, neutral, and sensitive. The intensity and reaction speed decreased from sensitive to neutral to dull. The motion types listed in Tab. A5 can thus be further divided into motion types with genres, as listed in Tab. A6, A7, A8. change trajectory laugh nod head shout stretch"
        },
        {
            "title": "Dull Motion\nclap hands\nlook around\nprey\nstand still\nwave hands",
            "content": "dance look at run step aside Table A6: Dull motion types. 1 change trajectory cover nose laugh nod head shake stretch"
        },
        {
            "title": "Neutral Motion\nclap hands\ncurl\nlook around\nprey\nstand still\nwave hands",
            "content": "cover ears dance look at run step aside Table A7: Neutral motion types. change trajectory cover nose jump look at run stand still"
        },
        {
            "title": "Sensitive Motion\nclap hands\ncurl\nlaugh\nnod head\nshake\nstep aside",
            "content": "cover ears dance look around prey squat wave hands Table A8: Sensitive motion types."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Audio Features We present more details regarding the audio features extracted. The full list of audio features and their corresponding dimensions are listed in Tab. B9. The final shape of the audio features is thus RT 2272, where T=240 is the number of frames in each motion sequence. Feature Name MFCC MFCC delta constant-Q chromagram STFT chromagram onset strength tempogram one-hot beats RMS energy active frames Dimension 20 20 12 12 1 1068 1 1 1 Table B9: The extracted audio features for single ear comprise 1136-dimensional vector. The total dimension of the audio feature vector is 2272. B.2 Feature Extractor Implementation Details Following the feature extractor architecture from [29], our framework incorporates two bidirectional GRUs (Bi-GRUs) and motion sequence autoencoder, as illustrated in Fig. B10. The Bi-GRUs are applied to extracting condition and motion features respectively, each employ 4 layers with hidden size of 1024. The audio features undergo an initial projection to 1020 dimensions, followed by concatenation with the sound source location and genre information, resulting in 1024-dimensional condition feature vector. The motion autoencoder consists of transformer encoder-decoder structure, where each component contains 4 layers with 4 attention heads and has latent dimension of 512. Notably, the Bi-GRU used to extract features from motion sequences takes the motion autoencoders output as its input. 2 Figure B10: The feature extractor framework consists of motion autoencoder co-trained with two Bi-GRU modules functioning as feature extractors (condition encoder and motion encoder). reconstruction loss is applied between the ground-truth motions and the decoded motions produced by the motion autoencoder. Additionally, contrastive loss operates on the extracted condition features and motion features m. The feature extractor is optimized using two loss functions. Consistent with the implementation in [29], we employ contrastive loss [31] defined as: Dc,m = m2 Lcta = (1 y)Dc,m 2 + (y)max(0, Dc,m)2, where denotes condition features, represents motion features, and = 0 for matched conditionmotion pairs, otherwise = 1. This contrastive loss segregates the feature space by minimizing distances between matched pairs while enforcing minimum separation margin between mismatched pairs. We set = 10 here. Additionally, we apply reconstruction loss: Lrecon = x2 2, to guide the learning of the motion autoencoder, where denotes reconstructed motion sequences from the transformer decoder and represents ground-truth motion sequences. These sequences maintain the identical RT 300 dimensionality format used during MOSPAs training. We employ the Adam optimizer [42] with learning rate of 5 105. The feature extractor undergoes training for 1,500 epochs with batch size of 64, with the motion autoencoder frozen after the initial 1,000 epochs to concentrate optimization efforts on the two Bi-GRU modules in the later 500 epochs. R-precision For each motion sequence and its extracted motion features, condition feature pool is constructed comprising the ground-truth matched condition feature along with 31 randomly selected mismatched condition features from the test dataset serving as distractors. The R-precision metrics are then computed by ranking the Euclidean distances between the extracted condition features and motion features, then determining the probability that the ground-truth condition appears among the top-1, top-2, and top-3 ranked positions. Fréchet Inception Distance (FID) The Fréchet Inception Distance (FID) is computed between motion features derived from ground-truth motion sequences in the test dataset and corresponding generated motion sequences, serving as metric for assessing the quality and fidelity of the synthesized motions. Diversity Following the methodology in [29], diversity is computed to quantify the variance of the generated motions. Two sets of generated motions, each of size Sd = 64, are randomly sampled 3 from the test dataset: {m1, m2, , mSd} and {m calculated as: Diversity ="
        },
        {
            "title": "C User Study Design",
            "content": "Sd(cid:88) i=1 mi i. 1, 2, , Sd }. The diversity metric is then We conducted user study to evaluate the perceptual quality of motion generation. total of 25 participants assessed five models (MOSPA, EDGE, POPDG, LODGE, and Bailando) alongside the ground truth (GT). They were asked to select the best motion based on the following criteria: Human Intent Alignment: Does the motion align with real-world intent? Motion Quality: Which motion exhibits the highest movement quality? GT Similarity: Which motion best matches the GT? To facilitate evaluation, we provided both the GT motion and textual description as references. Fig. C11 presents screenshot of the user study interface. Further Details on the User Study for Spatial Audio-Driven Motion Generation. Before starting the study, users are instructed to: Stay in quiet environment; Wear binaural headphones; Carefully read all instructions. Each case presents six videos with spatial audio: Videos 15 are generated by different methods. The rightmost video is the Ground Truth (GT), shown in green. After watching and listening to each row of videos, participants are asked to answer: (1) Which video best aligns with real-world human Intent in motion generation? (2) Which video exhibits the highest Motion Quality of body movement? (3) Which video best Matches the Ground Truth motion (rightmost)? The following are the 10 testing cases and their corresponding instructions: ... 4 Figure C11: Screenshot of the user study interface."
        },
        {
            "title": "References",
            "content": "[1] Simon Alexanderson, Rajmund Nagy, Jonas Beskow, and Gustav Eje Henter. Listen, denoise, action! audio-driven motion synthesis with diffusion models. ACM Transactions on Graphics (TOG), 42(4):120, 2023. [2] Tenglong Ao, Qingzhe Gao, Yuke Lou, Baoquan Chen, and Libin Liu. Rhythmic gesticulator: Rhythmaware co-speech gesture synthesis with hierarchical neural embeddings. ACM Transactions on Graphics (TOG), 41(6):119, 2022. [3] Tenglong Ao, Zeyi Zhang, and Libin Liu. Gesturediffuclip: Gesture diffusion model with clip latents. ACM Transactions on Graphics (TOG), 42(4):118, 2023. [4] German Barquero, Sergio Escalera, and Cristina Palmero. Belfusion: Latent diffusion for behavior-driven human motion prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 23172327, 2023. [5] Yuxuan Bian, Ailing Zeng, Xuan Ju, Xian Liu, Zhaoyang Zhang, Wei Liu, and Qiang Xu. Motioncraft: In Proceedings of the AAAI Crafting whole-body motion with plug-and-play multimodal controls. Conference on Artificial Intelligence, volume 39, pages 18801888, 2025. [6] Sebastian Böck and Gerhard Widmer. Maximum filter vibrato suppression for onset detection. In Proc. of the 16th Int. Conf. on Digital Audio Effects (DAFx). Maynooth, Ireland (Sept 2013), volume 7, page 4. Citeseer, 2013. [7] Zhi Cen, Huaijin Pi, Sida Peng, Zehong Shen, Minghui Yang, Shuai Zhu, Hujun Bao, and Xiaowei Zhou. Generating human motion in 3d scenes from text descriptions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18551866, 2024. [8] Ling-Hao Chen, Shunlin Lu, Wenxun Dai, Zhiyang Dou, Xuan Ju, Jingbo Wang, Taku Komura, and Lei Zhang. Pay attention and move better: Harnessing attention for interactive motion generation and training-free editing. arXiv preprint arXiv:2410.18977, 2024. [9] Ling-Hao Chen, Jiawei Zhang, Yewen Li, Yiren Pang, Xiaobo Xia, and Tongliang Liu. Humanmac: Masked motion completion for human motion prediction. In Proceedings of the IEEE/CVF international conference on computer vision, pages 95449555, 2023. [10] Rui Chen, Mingyi Shi, Shaoli Huang, Ping Tan, Taku Komura, and Xuelin Chen. Taming diffusion probabilistic models for character control. In ACM SIGGRAPH 2024 Conference Papers, pages 110, 2024. [11] Xin Chen, Biao Jiang, Wen Liu, Zilong Huang, Bin Fu, Tao Chen, and Gang Yu. Executing your commands via motion diffusion in latent space. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1800018010, 2023. [12] Qingrong Cheng, Xu Li, and Xinghui Fu. Siggesture: Generalized co-speech gesture synthesis via semantic injection with large-scale pre-training diffusion models. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [13] Peishan Cong, Ziyi Wang, Zhiyang Dou, Yiming Ren, Wei Yin, Kai Cheng, Yujing Sun, Xiaoxiao Long, Xinge Zhu, and Yuexin Ma. Laserhuman: language-guided scene-aware human motion generation in free environment. arXiv preprint arXiv:2403.13307, 2024. [14] Rishabh Dabral, Muhammad Hamza Mughal, Vladislav Golyanik, and Christian Theobalt. Mofusion: framework for denoising-diffusion-based motion synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97609770, 2023. [15] Wenxun Dai, Ling-Hao Chen, Jingbo Wang, Jinpeng Liu, Bo Dai, and Yansong Tang. Motionlcm: Real-time controllable motion generation via latent consistency model. In European Conference on Computer Vision, pages 390408. Springer, 2024. [16] Steven Davis and Paul Mermelstein. Comparison of parametric representations for monosyllabic word IEEE transactions on acoustics, speech, and signal recognition in continuously spoken sentences. processing, 28(4):357366, 1980. [17] Deity Microphones. Pr-2. https://deitymic.com/products/pr-2/, 2025. Accessed: Feb 5, 2025. [18] Zhiyang Dou, Xuelin Chen, Qingnan Fan, Taku Komura, and Wenping Wang. ase: Learning conditional adversarial skill embeddings for physics-based characters. In SIGGRAPH Asia 2023 Conference Papers, pages 111, 2023. [19] Daniel PW Ellis. Beat tracking by dynamic programming. Journal of New Music Research, 36(1):5160, 2007. [20] Di Fan, Lili Wan, Wanru Xu, and Shenghui Wang. bi-directional attention guided cross-modal network for music based dance generation. Computers and Electrical Engineering, 103:108310, 2022. 6 [21] Ke Fan, Shunlin Lu, Minyue Dai, Runyi Yu, Lixing Xiao, Zhiyang Dou, Junting Dong, Lizhuang Ma, and Jingbo Wang. Go to zero: Towards zero-shot motion generation with million-scale data. arXiv preprint arXiv:2507.07095, 2025. [22] Yuming Feng, Zhiyang Dou, Ling-Hao Chen, Yuan Liu, Tianyu Li, Jingbo Wang, Zeyu Cao, Wenping Wang, Taku Komura, and Lingjie Liu. Motionwavelet: Human motion prediction via wavelet manifold learning. arXiv preprint arXiv:2411.16964, 2024. [23] Chuang Gan, Hang Zhao, Peihao Chen, David Cox, and Antonio Torralba. Self-supervised moving vehicle tracking with stereo sound. In Proceedings of the IEEE/CVF international conference on computer vision, pages 70537062, 2019. [24] Jort Gemmeke, Daniel PW Ellis, Dylan Freedman, Aren Jansen, Wade Lawrence, Channing Moore, Manoj Plakal, and Marvin Ritter. Audio set: An ontology and human-labeled dataset for audio events. In 2017 IEEE international conference on acoustics, speech and signal processing (ICASSP), pages 776780. IEEE, 2017. [25] Shiry Ginosar, Amir Bar, Gefen Kohavi, Caroline Chan, Andrew Owens, and Jitendra Malik. Learning individual styles of conversational gesture. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 34973506, 2019. [26] Kehong Gong, Dongze Lian, Heng Chang, Chuan Guo, Zihang Jiang, Xinxin Zuo, Michael Bi Mi, and Xinchao Wang. Tm2d: Bimodality driven 3d dance generation via music-text integration. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 99429952, 2023. [27] Peter Grosche, Meinard Müller, and Frank Kurth. Cyclic tempograma mid-level tempo representation for musicsignals. In 2010 IEEE International Conference on Acoustics, Speech and Signal Processing, pages 55225525. IEEE, 2010. [28] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19001910, 2024. [29] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 51525161, 2022. [30] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. Action2motion: Conditioned generation of 3d human motions. In Proceedings of the 28th ACM International Conference on Multimedia, pages 20212029, 2020. [31] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimensionality reduction by learning an invariant In 2006 IEEE computer society conference on computer vision and pattern recognition mapping. (CVPR06), volume 2, pages 17351742. IEEE, 2006. [32] Mohamed Hassan, Duygu Ceylan, Ruben Villegas, Jun Saito, Jimei Yang, Yi Zhou, and Michael Black. Stochastic scene-aware motion prediction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1137411384, 2021. [33] Tairan He, Jiawei Gao, Wenli Xiao, Yuanhang Zhang, Zi Wang, Jiashun Wang, Zhengyi Luo, Guanqi He, Nikhil Sobanbab, Chaoyi Pan, et al. Asap: Aligning simulation and real-world physics for learning agile humanoid whole-body skills. arXiv preprint arXiv:2502.01143, 2025. [34] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [35] Daniel Holden, Taku Komura, and Jun Saito. Phase-functioned neural networks for character control. ACM Transactions on Graphics (TOG), 36(4):113, 2017. [36] Chao Huang, Dejan Markovic, Chenliang Xu, and Alexander Richard. Modeling and driving human body soundfields through acoustic primitives. In European Conference on Computer Vision, pages 117. Springer, 2024. [37] Yiming Huang, Zhiyang Dou, and Lingjie Liu. Modskill: Physical character skill modularization. arXiv preprint arXiv:2502.14140, 2025. [38] Yiming Huang, Weilin Wan, Yue Yang, Chris Callison-Burch, Mark Yatskar, and Lingjie Liu. Como: Controllable motion generation through language guided pose code editing. In European Conference on Computer Vision, pages 180196. Springer, 2024. [39] Yinghao Huang, Leo Ho, Dafei Qin, Mingyi Shi, and Taku Komura. Interact: Capture and modelling of realistic, expressive and interactive activities between two persons in daily scenarios. arXiv preprint arXiv:2405.11690, 2024. [40] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36:2006720079, 2023. 7 [41] Jaeyeon Kim, Heeseung Yun, and Gunhee Kim. Visage: Video-to-spatial audio generation. In The Thirteenth International Conference on Learning Representations. [42] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. [43] Nhat Le, Tuong Do, Khoa Do, Hien Nguyen, Erman Tjiputra, Quang Tran, and Anh Nguyen. Controllable group choreography using contrastive diffusion. ACM Transactions on Graphics (TOG), 42(6):114, 2023. [44] Buyu Li, Yongchi Zhao, Shi Zhelun, and Lu Sheng. Danceformer: Music conditioned 3d dance generation with parametric motion transformer. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 12721279, 2022. [45] Ronghui Li, YuXiang Zhang, Yachao Zhang, Hongwen Zhang, Jie Guo, Yan Zhang, Yebin Liu, and Xiu Li. Lodge: coarse to fine diffusion network for long dance generation guided by the characteristic dance primitives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15241534, 2024. [46] Ronghui Li, Junfan Zhao, Yachao Zhang, Mingyang Su, Zeping Ren, Han Zhang, Yansong Tang, and Xiu Li. Finedance: fine-grained choreography dataset for 3d full body dance generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1023410243, 2023. [47] Ruilong Li, Shan Yang, David Ross, and Angjoo Kanazawa. Ai choreographer: Music conditioned 3d dance generation with aist++. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1340113412, 2021. [48] Zhouyingcheng Liao, Mingyuan Zhang, Wenjia Wang, Lei Yang, and Taku Komura. Rmd: simple baseline for more general human motion generation via training-free retrieval-augmented motion diffuse. arXiv preprint arXiv:2412.04343, 2024. [49] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motionx: large-scale 3d expressive whole-body human motion dataset. Advances in Neural Information Processing Systems, 36:2526825280, 2023. [50] Jinpeng Liu, Wenxun Dai, Chunyu Wang, Yiji Cheng, Yansong Tang, and Xin Tong. Plan, posture and go: Towards open-world text-to-motion generation. arXiv preprint arXiv:2312.14828, 2023. [51] Xian Liu, Qianyi Wu, Hang Zhou, Yinghao Xu, Rui Qian, Xinyi Lin, Xiaowei Zhou, Wayne Wu, Bo Dai, and Bolei Zhou. Learning hierarchical cross-modal association for co-speech gesture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10462 10472, 2022. [52] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [53] Vicon Motion Systems Ltd. Vicon motion capture systems, 2025. Accessed: Feb 5, 2025. [54] Shunlin Lu, Ling-Hao Chen, Ailing Zeng, Jing Lin, Ruimao Zhang, Lei Zhang, and Heung-Yeung Shum. Humantomato: Text-aligned whole-body motion generation. arXiv preprint arXiv:2310.12978, 2023. [55] Shunlin Lu, Jingbo Wang, Zeyu Lu, Ling-Hao Chen, Wenxun Dai, Junting Dong, Zhiyang Dou, Bo Dai, and Ruimao Zhang. Scamo: Exploring the scaling law in autoregressive motion generation model. arXiv preprint arXiv:2412.14559, 2024. [56] Zhengyi Luo, Jinkun Cao, Kris Kitani, Weipeng Xu, et al. Perpetual humanoid control for real-time simulated avatars. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1089510904, 2023. [57] Zhenye Luo, Min Ren, Xuecai Hu, Yongzhen Huang, and Li Yao. Popdg: Popular 3d dance generation In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern with popdanceset. Recognition, pages 2698426993, 2024. [58] Brian McFee, Colin Raffel, Dawen Liang, Daniel PW Ellis, Matt McVicar, Eric Battenberg, and Oriol Nieto. librosa: Audio and music signal analysis in python. SciPy, 2015:1824, 2015. [59] Liang Pan, Zeshi Yang, Zhiyang Dou, Wenjia Wang, Buzhen Huang, Bo Dai, Taku Komura, and Jingbo Wang. Tokenhsi: Unified synthesis of physical human-scene interactions through task tokenization. arXiv preprint arXiv:2503.19901, 2025. [60] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1097510985, 2019. [61] Huaijin Pi, Zhi Cen, Zhiyang Dou, and Taku Komura. Coda: Coordinated diffusion noise optimization for whole-body manipulation of articulated objects. arXiv preprint arXiv:2505.21437, 2025. 8 [62] Shenhan Qian, Zhi Tu, Yihao Zhi, Wen Liu, and Shenghua Gao. Speech drives templates: Co-speech gesture synthesis with learned templates. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1107711086, 2021. [63] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. [64] Christian Schörkhuber and Anssi Klapuri. Constant-q transform toolbox for music processing. In 7th sound and music computing conference, Barcelona, Spain, pages 364. SMC, 2010. [65] Mingyi Shi, Dafei Qin, Leo Ho, Zhouyingcheng Liao, Yinghao Huang, Junichi Yamagishi, and Taku Komura. It takes two: Real-time co-speech two-persons interaction generation via reactive auto-regressive diffusion model. arXiv preprint arXiv:2412.02419, 2024. [66] Yi Shi, Jingbo Wang, Xuekun Jiang, Bingkun Lin, Bo Dai, and Xue Bin Peng. Interactive character control with auto-regressive motion diffusion models. ACM Transactions on Graphics (TOG), 43(4):114, 2024. [67] Li Siyao, Tianpei Gu, Zhitao Yang, Zhengyu Lin, Ziwei Liu, Henghui Ding, Lei Yang, and Chen Change Loy. Duolando: Follower gpt with off-policy reinforcement learning for dance accompaniment. arXiv preprint arXiv:2403.18811, 2024. [68] Li Siyao, Weijiang Yu, Tianpei Gu, Chunze Lin, Quan Wang, Chen Qian, Chen Change Loy, and Ziwei Liu. Bailando: 3d dance generation by actor-critic gpt with choreographic memory. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1105011059, 2022. [69] Sebastian Starke, Ian Mason, and Taku Komura. Deepphase: Periodic autoencoders for learning motion phase manifolds. ACM Transactions on Graphics (TOG), 41(4):113, 2022. [70] Jiarui Sun and Girish Chowdhary. Comusion: Towards consistent stochastic human motion prediction via motion diffusion. In European Conference on Computer Vision, pages 1836. Springer, 2024. [71] Peiwen Sun, Sitong Cheng, Xiangtai Li, Zhen Ye, Huadai Liu, Honggang Zhang, Wei Xue, and Yike Guo. Both ears wide open: Towards language-driven spatial audio generation. arXiv preprint arXiv:2410.10676, 2024. [72] Taoran Tang, Jia Jia, and Hanyang Mao. Dance with melody: An lstm-autoencoder approach to musicoriented dance synthesis. In Proceedings of the 26th ACM international conference on Multimedia, pages 15981606, 2018. [73] Chen Tessler, Yunrong Guo, Ofir Nabati, Gal Chechik, and Xue Bin Peng. Maskedmimic: Unified physics-based character control through masked motion inpainting. ACM Transactions on Graphics (TOG), 43(6):121, 2024. [74] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In The Eleventh International Conference on Learning Representations, 2023. [75] Jonathan Tseng, Rodrigo Castellon, and Karen Liu. Edge: Editable dance generation from music. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 448458, 2023. [76] Weilin Wan, Zhiyang Dou, Taku Komura, Wenping Wang, Dinesh Jayaraman, and Lingjie Liu. Tlcontrol: Trajectory and language control for human motion synthesis. arXiv preprint arXiv:2311.17135, 2023. [77] Jingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua Lin, and Bo Dai. Towards diverse and natural scene-aware 3d human motion synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2046020469, 2022. [78] Jingbo Wang, Sijie Yan, Bo Dai, and Dahua Lin. Scene-aware generative network for human motion synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1220612215, 2021. [79] Wenjia Wang, Liang Pan, Zhiyang Dou, Jidong Mei, Zhouyingcheng Liao, Yuke Lou, Yifan Wu, Lei Yang, Jingbo Wang, and Taku Komura. Sims: Simulating stylized human-scene interactions with retrieval-augmented script generation. arXiv preprint arXiv:2411.19921, 2024. [80] Zan Wang, Yixin Chen, Baoxiong Jia, Puhao Li, Jinlu Zhang, Jingze Zhang, Tengyu Liu, Yixin Zhu, Wei Liang, and Siyuan Huang. Move as you say interact as you can: Language-guided human motion generation with scene affordance. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 433444, 2024. [81] Yiming Xie, Varun Jampani, Lei Zhong, Deqing Sun, and Huaizu Jiang. Omnicontrol: Control any joint at any time for human motion generation. arXiv preprint arXiv:2310.08580, 2023. [82] Liang Xu, Ziyang Song, Dongliang Wang, Jing Su, Zhicheng Fang, Chenjing Ding, Weihao Gan, Yichao Yan, Xin Jin, Xiaokang Yang, et al. Actformer: gan-based transformer towards general actionconditioned 3d human motion generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 22282238, 2023. [83] Sirui Xu, Hung Yu Ling, Yu-Xiong Wang, and Liang-Yan Gui. Intermimic: Towards universal whole-body control for physics-based human-object interactions. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1226612277, 2025. [84] Xudong Xu, Dejan Markovic, Jacob Sandakly, Todd Keebler, Steven Krenn, and Alexander Richard. Sounding bodies: modeling 3d spatial sound of humans using body pose and audio. Advances in Neural Information Processing Systems, 36:4474044752, 2023. [85] Han Yang, Kun Su, Yutong Zhang, Jiaben Chen, Kaizhi Qian, Gaowen Liu, and Chuang Gan. Unimumo: Unified text, music and motion generation. arXiv preprint arXiv:2410.04534, 2024. [86] Youngwoo Yoon, Bok Cha, Joo-Haeng Lee, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee. Speech gesture generation from the trimodal context of text, audio, and speaker identity. ACM Transactions on Graphics (TOG), 39(6):116, 2020. [87] Ye Yuan and Kris Kitani. Diverse trajectory forecasting with determinantal point processes. arXiv preprint arXiv:1907.04967, 2019. [88] Canyu Zhang, Youbao Tang, Ning Zhang, Ruei-Sung Lin, Mei Han, Jing Xiao, and Song Wang. Bidirectional autoregessive diffusion model for dance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 687696, 2024. [89] He Zhang, Sebastian Starke, Taku Komura, and Jun Saito. Mode-adaptive neural networks for quadruped motion control. ACM Transactions on Graphics (TOG), 37(4):111, 2018. [90] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan. Generating human motion from textual descriptions with discrete representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1473014740, 2023. [91] Mingyuan Zhang, Daisheng Jin, Chenyang Gu, Fangzhou Hong, Zhongang Cai, Jingfang Huang, Chongzhi Zhang, Xinying Guo, Lei Yang, Ying He, et al. Large motion model for unified multi-modal motion generation. In European Conference on Computer Vision, pages 397421. Springer, 2024. [92] Yufei Zhang, Jeffrey Kephart, Zijun Cui, and Qiang Ji. Physpt: Physics-aware pretrained transformer for estimating human dynamics from monocular videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 23052317, 2024. [93] Yufei Zhang, Jeffrey Kephart, and Qiang Ji. Incorporating physics principles for precise human motion prediction. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 61646174, 2024. [94] Zeyi Zhang, Tenglong Ao, Yuyao Zhang, Qingzhe Gao, Chuan Lin, Baoquan Chen, and Libin Liu. Semantic gesticulator: Semantics-aware co-speech gesture synthesis. ACM Transactions on Graphics (TOG), 43(4):117, 2024. [95] Hang Zhao, Chuang Gan, Andrew Rouditchenko, Carl Vondrick, Josh McDermott, and Antonio Torralba. The sound of pixels. In Proceedings of the European conference on computer vision (ECCV), pages 570586, 2018. [96] Zhisheng Zheng, Puyuan Peng, Ziyang Ma, Xie Chen, Eunsol Choi, and David Harwath. Bat: Learning to reason about spatial sounds with large language models. arXiv preprint arXiv:2402.01591, 2024. [97] Wenyang Zhou, Zhiyang Dou, Zeyu Cao, Zhouyingcheng Liao, Jingbo Wang, Wenjia Wang, Yuan Liu, Taku Komura, Wenping Wang, and Lingjie Liu. Emdm: Efficient motion diffusion model for fast and high-quality motion generation. In European Conference on Computer Vision, pages 1838. Springer, 2025. [98] Yi Zhou, Connelly Barnes, Jingwan Lu, Jimei Yang, and Hao Li. On the continuity of rotation representations in neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 57455753, 2019. [99] Lingting Zhu, Xian Liu, Xuanyu Liu, Rui Qian, Ziwei Liu, and Lequan Yu. Taming diffusion models for audio-driven co-speech gesture generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1054410553, 2023. [100] Wentao Zhu, Xiaoxuan Ma, Dongwoo Ro, Hai Ci, Jinlu Zhang, Jiaxin Shi, Feng Gao, Qi Tian, and Yizhou Wang. Human motion generation: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [101] Wenlin Zhuang, Congyi Wang, Jinxiang Chai, Yangang Wang, Ming Shao, and Siyu Xia. Music2dance: Dancenet for music-driven dance generation. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 18(2):121, 2022."
        }
    ],
    "affiliations": [
        "Macau University of Science and Technology",
        "Shanghai AI Lab",
        "ShanghaiTech University",
        "Texas A&M University",
        "The Hong Kong University of Science and Technology",
        "The University of Hong Kong"
    ]
}