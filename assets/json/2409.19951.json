{
    "paper_title": "Law of the Weakest Link: Cross Capabilities of Large Language Models",
    "authors": [
        "Ming Zhong",
        "Aston Zhang",
        "Xuewei Wang",
        "Rui Hou",
        "Wenhan Xiong",
        "Chenguang Zhu",
        "Zhengxing Chen",
        "Liang Tan",
        "Chloe Bi",
        "Mike Lewis",
        "Sravya Popuri",
        "Sharan Narang",
        "Melanie Kambadur",
        "Dhruv Mahajan",
        "Sergey Edunov",
        "Jiawei Han",
        "Laurens van der Maaten"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real-world tasks, which we term cross capabilities. To systematically explore this concept, we first define seven core individual capabilities and then pair them to form seven common cross capabilities, each supported by a manually constructed taxonomy. Building on these definitions, we introduce CrossEval, a benchmark comprising 1,400 human-annotated prompts, with 100 prompts for each individual and cross capability. To ensure reliable evaluation, we involve expert annotators to assess 4,200 model responses, gathering 8,400 human ratings with detailed explanations to serve as reference examples. Our findings reveal that, in both static evaluations and attempts to enhance specific abilities, current LLMs consistently exhibit the \"Law of the Weakest Link,\" where cross-capability performance is significantly constrained by the weakest component. Specifically, across 58 cross-capability scores from 17 models, 38 scores are lower than all individual capabilities, while 20 fall between strong and weak, but closer to the weaker ability. These results highlight the under-performance of LLMs in cross-capability tasks, making the identification and improvement of the weakest capabilities a critical priority for future research to optimize performance in complex, multi-dimensional scenarios."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 ] . [ 2 1 5 9 9 1 . 9 0 4 2 : r Law of the Weakest Link: Cross Capabilities of Large Language Models Ming Zhong 1,2, Aston Zhang 1, Xuewei Wang1, Rui Hou1, Wenhan Xiong1, Chenguang Zhu1, Zhengxing Chen1, Liang Tan1, Chloe Bi1, Mike Lewis1, Sravya Popuri1, Sharan Narang1, Melanie Kambadur1, Dhruv Mahajan1, Sergey Edunov1, Jiawei Han2, Laurens van der Maaten1 1Llama Team, AI @ Meta, 2University of Illinois Urbana-Champaign Equal contribution. The development and evaluation of Large Language Models (LLMs) have largely focused on individual capabilities. However, this overlooks the intersection of multiple abilities across different types of expertise that are often required for real-world tasks, which we term cross capabilities. To systematically explore this concept, we first define seven core individual capabilities and then pair them to form seven common cross capabilities, each supported by manually constructed taxonomy. Building on these definitions, we introduce CrossEval, benchmark comprising 1,400 human-annotated prompts, with 100 prompts for each individual and cross capability. To ensure reliable evaluation, we involve expert annotators to assess 4,200 model responses, gathering 8,400 human ratings with detailed explanations to serve as reference examples. Our findings reveal that, in both static evaluations and attempts to enhance specific abilities, current LLMs consistently exhibit the Law of the Weakest Link, where cross-capability performance is significantly constrained by the weakest component. Specifically, across 58 cross-capability scores from 17 models, 38 scores are lower than all individual capabilities, while 20 fall between strong and weak, but closer to the weaker ability. These results highlight the under-performance of LLMs in cross-capability tasks, making the identification and improvement of the weakest capabilities critical priority for future research. Date: October 4, 2024 Correspondence: Aston Zhang at aston@meta.com Data, Benchmark, & Code: www.llm-cross-capabilities.org"
        },
        {
            "title": "1 Introduction",
            "content": "The development and evaluation of Large Language Models (LLMs) (OpenAI, 2023, 2024; Anthropic, 2024; Reid et al., 2024) have predominantly centered on individual capabilities. Developers commonly construct specialized datasets tailored to distinct abilities, and then train models by blending these data sources. For instance, Llama 3s post-training incorporates mix of data, from general English to code and multilingual content, among others, each subset aimed at honing specific skill (Llama Team, 2024). Evaluation methods follow similar pattern, with benchmarks typically assessing these abilities in isolation, offering snapshot of how well model can reason (Clark et al., 2018; Cobbe et al., 2021; Hendrycks et al., 2021b), code (Chen et al., 2021; Austin et al., 2021), or manage factual knowledge (Hendrycks et al., 2021a). However, can all real-world tasks be adequately categorized under just one capability, or do they frequently demand the seamless integration of multiple skills, thereby challenging the prevalent approach to evaluating these advanced LLMs? Consider user prompt asking, Which direction has the total rainfall in Tokyo, Japan been trending over the past 10 years? Explain it step by step. Such task requires the integration of tool use (web browsing) with analytical reasoning. Similarly, when developer provides HTML and JavaScript for an API-driven application and asks, Give me basic understanding of what this web app does, the model must combine long-context comprehension with coding expertise. We define these scenarios as cross capabilitiesthe intersection of multiple distinct capabilities across different types of expertise necessary to address complex, real-world tasks. This discrepancy between the isolated focus of current LLM evaluation and the multifaceted demands of user interactions raises critical question: 1 How does the performance of LLMs on tasks requiring cross capabilities reflect or diverge from their performance in individual capabilities? This question opens up various possibilities for portraying the relationship between distinct abilities in LLMs and their collective performance. Insights from multiple fields can shed light on these dynamics. For example, Synergy Theory (Corning, 1983) suggests that the interaction of different components in system can produce effects greater than the sum of individual parts, while Compensatory Mechanism (Adler, 1917), concept from psychology, introduces that stronger abilities within system can offset weaker ones. Additionally, Law of the Weakest Link (Liebig, 1840) presents that systems performance is limited by its weakest element, and the idea of Emergent Properties (Anderson, 1972) highlights how new behaviors can arise from the interaction of components, which are not predictable from their individual components alone. Given the substantial investment in enhancing the particular abilities of LLMs, identifying how individual capabilities impact performance on tasks requiring cross abilities is crucial for guiding future development. We investigate how the interplay of individual capabilities influences collective performance, with the goal of providing insights for advancing LLM effectiveness in handling cross-capability tasks. Specifically, our research explores the following key questions: RQ1: How can we comprehensively define individual and cross capabilities in LLMs? To effectively define all capabilities in LLMs, we must systematically categorize tasks that reflect real-world interactions. We identify seven core individual capabilities, including English, Reasoning, Coding, Image Recognition, Tool Use, Long Context, and Spanish, and pair them to form seven common cross capabilities, such as Coding & Reasoning and Image Recognition & Reasoning. For each capability, we manually construct detailed taxonomy that connects the capability to complex tasks, breaking it down into two levels: broad categories at the first level and specific tasks at the second. These taxonomies lay the groundwork for constructing benchmarks that can comprehensively cover and assess broader range of LLM capabilities. RQ2: How can we benchmark both individual and cross capabilities in LLMs? To benchmark all capabilities in LLMs, we construct detailed evaluation framework, CrossEval, based on manually annotated prompts that align with our established taxonomy. Each prompt is categorized by capability and difficulty, ensuring thorough coverage of both individual and cross capabilities. We collect multiple model responses for each prompt and engage expert human annotators to rate and explain these responses. In total, CrossEval comprises 1,400 prompts, 4,200 model responses, and 8,400 human ratings with detailed explanations. Finally, we introduce LLM-based evaluators to assess responses using these reference examples, achieving strong agreement with human judgments, thereby establishing reliable benchmark for evaluating LLM performance across wide spectrum of open-ended tasks. RQ3: What patterns exist in the relationship between individual and cross-capability performance in LLMs? Through extensive evaluation using CrossEval, we uncover clear patterns in the relationship between individual and cross-capability performance. Most notably, cross-capability performance is typically constrained by the weakest capability, following the Law of the Weakest Link effect. This pattern is consistent across different LLMs and evaluators, suggesting that deficiencies in an individual capability can significantly limit overall performance in more complex tasks. Specifically, of the 58 cross-capability scores from 17 models, 38 fall below the individual capabilities, while 20 lie between the strong and weak, skewing towards the weaker. These results underscore the need for targeted optimization to strengthen weaker capabilities, especially in areas like Tool Use, where models struggle the most. RQ4: How do shifts in individual capabilities impact cross-capability performance in LLMs? Beyond evaluating the static relationship between individual and cross capabilities, we investigate how altering individual capabilities impacts cross-capability performance. Through case studies using principle-based system prompting method, we selectively enhance specific capabilities and find that improvements in weaker capabilities lead to significant gains in cross-capability tasks, while changes in stronger capabilities result in only minor shifts. This finding further supports Law of the Weakest Link, as an LLMs cross-capability performance continues to conform to this phenomenon even when individual capability performance changes. In summary, this paper highlights the critical oversight of cross capabilities in LLM development and evaluation, despite being essential for real-world tasks. To systematically explore it, we establish comprehensive benchmark to model both individual and cross capabilities, revealing that current LLMs, whether in static 2 evaluations or when enhancing specific capabilities, consistently conform to the Law of the Weakest Link effect. Given that LLMs generally underperform in cross-capability tasks, identifying and enhancing these weak points should be priority for future research and development."
        },
        {
            "title": "2 Defining Individual & Cross Capabilities in LLMs",
            "content": "Real-world interactions with LLMs encompass tasks that may require either an individual capability or the simultaneous engagement of distinct skills. To effectively evaluate LLMs, defining and differentiating these capabilities is crucial. In this section, we identify seven individual and seven cross capabilities that reflect broad spectrum of user queries and systematically organize them into taxonomies. As illustrated in Figure 1, these taxonomies follow hierarchical design: the root node represents either an individual or cross capability, with the next two layers (Level-1 and Level-2 categories) breaking these down into increasingly specific tasks. This framework clearly distinguishes between tasks that rely on an individual capability and those that demand the integration of multiple abilities, allowing for comprehensive evaluation of LLMs across various scenarios. Next, we outline the specific capabilities selected and explain the details."
        },
        {
            "title": "2.1 Individual Capabilities\nWe begin by selecting seven core individual capabilities of LLMs: English, Reasoning, Coding, Image\nRecognition, Tool Use, Long Context, and one representative of multilingual capabilities, Spanish. Each of\nthese capabilities is further broken down into Level-1 categories, as outlined below:",
            "content": "English and Multilingual: Factual Questions (5), Procedural Questions (8), Language Assistance (1), Writing & Content Creation (9), Dialogue (6), Recommendations / Brainstorming (4), Personal Growth & Development (8), and Social interaction & communication (4). Reasoning: Mathematical Calculation (7), Mathematical Reasoning (4), Commonsense Reasoning (3), Logic / Problem Solving (3), Social and Emotional Reasoning (6), Moral & Ethical Reasoning (3), Scientific Reasoning (4), and Legal Reasoning (6). Coding: Code Generation / Synthesis (7), Code Documentation (5), Code Debugging (2), and Code Review & Best Practices (4). Image Recognition: Object Recognition (3), Scene Understanding (4), Image Captioning (2), Attribute & Relationship Identification (3), Dialogue (2), and Graceful Refusals (3). Tool Use: Factual Questions about Recent and Current Things (5), Very Accurate Questions (Beyond Expected Model Knowledge) (5), Procedural Questions about Recent, Current, or Local Things (7), Recommendations / Brainstorming about Local and Current Things (4), Tasks with File Uploads (2). Long Context: Factoid or Complex Question Answering (6), Summarization (6), and Multi-Document Understanding (Q&A) (2). To explain, the number in parentheses above indicates the number of Level 2 subcategories within each Level-1 category. For instance, Scientific Reasoning (4) includes subcategories Hypothesis Formation and Testing, Causal Reasoning, Scientific Evidence Evaluation, and Model-Based Reasoning. We select these seven capabilities because they represent core LLM skills across diverse domains, including multimodal, multilingual, and tool-use tasks, ensuring broad coverage of mainstream real-world use cases. Appendix A.1 provides the full taxonomy of all the individual capabilities."
        },
        {
            "title": "2.2 Cross Capabilities",
            "content": "We explore cross-capability scenarios involving the combination of two capabilities. To achieve this, we pair the individual capabilities described earlier and select seven common combinations: Coding & Reasoning, Image Recognition & Reasoning, Tool Use & Coding, Tool Use & Reasoning, Long Context & Coding, Spanish & Reasoning, and Spanish & Image Recognition. Below is the Level-1 taxonomy: 3 (a) Image Recognition (b) Reasoning (c) Image Recognition & Reasoning Figure 1 Taxonomy visualizations for Image Recognition, Reasoning, and the corresponding cross capability. Each node represents specific type of task. The first two taxonomies illustrate tasks that require only individual capabilities for LLMs to complete. The final taxonomy, however, depicts tasks that lie at the intersection of Image Recognition and Reasoning capabilities, necessitating the use of both abilities to accomplish them. For the full taxonomy of all the individual and capabilities and cross capabilities, please see Appendix A. 4 Coding & Reasoning: Coding Q&A (Text to Text) (5), Code Explanation (2), Programming Assistant (5), Mathematical Calculation (7). Image Recognition & Reasoning: Diagram Understanding (3), Chart Understanding (3), Text-Rich Understanding (2), and Visual Math and Science (2). Tool Use & Coding: Code Execution (3), Code Debugging with Execution (2), Programming Assistant with Execution (1), and Code Execution with File Uploads (3) Tool Use & Reasoning: Mathematical Reasoning (2), Scientific Reasoning (15), and Mathematical Calculation (13). Long Context & Coding: Repository-Level Code Generation (5), Repository-Level Code Understanding (2), Repository-Level Code Debugging (1), Log Analysis (3), and API Docs Understanding (2). For cross-capability scenarios involving multilingual tasks, such as Spanish, no new taxonomy is needed, as handling and generating multilingual content naturally integrates with other capabilities. By establishing these taxonomies, we gain clear understanding of how many and which capabilities are involved in various tasks, providing structured framework for comprehensively assessing LLM capabilities. For the full taxonomy of all the cross capabilities, please see Appendix A.2."
        },
        {
            "title": "3 CrossEval Benchmark Construction",
            "content": "In this section, we describe the process of manually annotating the prompt set and multiple reference responses to build CrossEval benchmark. We then explain how we select and configure the LLM to serve as the evaluator for this benchmark."
        },
        {
            "title": "3.1 Prompt Set Annotation",
            "content": "The prompt set forms the foundation of any benchmark in the era of LLMs, playing crucial role in accurately evaluating model performance. Previous research has shown that real-world user prompts can include large number of low-quality inputs, making it difficult to differentiate between advanced models (Li et al., 2024). Additionally, constructing prompts with high level of difficulty is inherently challenging (Padlewski et al., 2024). To address these concerns, we adopt comprehensive annotation process designed to ensure both quality and appropriate difficulty levels. Annotation Procedure. In this paper, we restrict the prompt set to single-turn and open-ended settings. The annotation process begins with annotators selecting leaf node from our established taxonomy to determine the category and task associated with each prompt. This ensures that every prompt aligns with specific capability. Furthermore, for each capability, we define clear criteria for three difficulty levels: easy, medium, and hard, to standardize the assessment of task complexity. For example, difficulties of prompts related to the English capability are defined as follows: Easy: Prompt is single ask/requirement/constraint for the model presented as single statement OR prompt is single statement without ask/requirement/constraints AND would not require subject matter expertise to understand. Medium: Prompt includes 24 asks/requirements/constraints for the model AND would not require subject matter expertise to produce response. Hard: Prompt contains 5 or more asks/requirements/constraints for the model OR requires subject matter expertise above and beyond common knowledge in order to respond. For Spanish as an individual capability, all prompts are annotated from scratch, with no overlap with the English prompt set. In cross-capability scenarios involving Spanish, the corresponding prompt sets are derived by translating the associated English-based prompts. For instance, the Spanish & Reasoning prompt set is created by translating the Reasoning prompts from English into Spanish. To maintain consistency and high quality, we begin with pilot annotation phase where the authors act as reviewers, providing feedback to identify any issues with the initial annotations and refine the annotation"
        },
        {
            "title": "English\nReasoning\nCoding\nImage Recognition\nTool Use\nLong Context\nSpanish",
            "content": "# Prompts 100 100 100 100 100 100 100 # L1 Categories 8 8 4 6 5 3 8 # L2 Categories 45 36 18 17 23 14 45 Coding & Reasoning Image Recognition & Reasoning Tool Use & Coding Tool Use & Reasoning Long Context & Coding Spanish & Reasoning Spanish & Image Recognition 100 100 100 100 100 100 100 4 4 4 3 5 8 19 10 9 30 13 36 17 Table 1 Statistics of the prompt sets in the CrossEval benchmark. guidelines accordingly. Afterward, the main annotation phase begins, resulting in 100 to 500 prompts for each capability, depending on the size of the annotator pool assigned to it. Reviewers then perform quality checks and apply filtering to produce final set of 100 high-quality prompts per capability. This process ensures the difficulty distribution follows the standards used in Llama 3s human evaluations, with 10% easy, 30% medium, and 60% hard prompts (Llama Team, 2024). Ultimately, the final prompt set consists of 1,400 prompts, with 100 prompts for each capability, covering all 76 Level-1 and 332 Level-2 categories as listed in Table 1."
        },
        {
            "title": "3.2 Multiple References with Human Annotations",
            "content": "While providing gold reference for each instance has been the standard approach before the rise of LLMs, it is not feasible for our challenging prompt set for three main reasons: 1. Many open-ended queries do not have single correct answer, and offering only one response as the reference risks introducing bias in the evaluation. 2. Several prompts, particularly those requiring domain expertise in areas such as coding or mathematics, remain challenging even for college-level expert annotators. 3. For prompts related to tool use, the correct response can be dynamic. For example, the answer to What is the temperature in the Bay Area today? changes daily. To address this, we propose using multiple model responses, scored and explained by human annotators, to serve as references for evaluation. Annotator Qualifications. For all annotations in this paper, we use the same data vendor as Llama 3s human evaluation, employing professional experts with domain-specific knowledge, such as reasoning, coding, and Spanish. To avoid contamination, the Llama team does not have access to CrossEval prompts during Llama 3s development. The data vendor selects the appropriate annotator pool based on the capabilities being evaluated. While creating definitive gold reference is impractical, our annotators are capable of assessing the correctness of model responses and providing well-justified ratings. Model Response Collection. For each prompt, we aim to gather three distinct model responses representing varying levels of quality: low, medium, and high. These responses are randomly drawn from various models within the Llama and GPT model families, including Llama 3.1 8B/70B/405B and different versions of GPT-4. Additionally, for capabilities involving Reasoning, Image Recognition, and Tool Use, we manually annotate one response if all three collected responses contain noticeable errors. Annotating Human Ratings with Explanations. For each model response, we engage two independent annotators to rate it on 15 Likert scale, accompanied by paragraph explaining their rating. Multiple reference 6 examples are provided in the Appendix B.2. We track inter-rater agreement and find that evaluating model responses can be highly challenging, even for expert annotators, making consensus difficult to achieve. To enhance consistency, we initially annotate 30% of the prompt set in pilot phase. During this phase, the inter-rater agreement is 33.65%, with Krippendorffs Alpha (K-Alpha) (Krippendorff, 2018) of 0.48, indicating relatively poor agreement. We then conduct the second and third rounds of annotation, allowing new raters from the same pool to review previous annotations, better understand the scoring criteria, and provide their ratings with explanations. After each round, we update the guidelines to improve the annotation process. This iterative procedure proves effective: inter-rater agreement improves from 33.65% to 45.79%, and finally to 47.38%, while K-Alpha increases from 0.48 to 0.66, and eventually to 0.73. After completing these rounds, we apply the updated guidelines to annotate the full dataset using the same trained annotator pool. On the full dataset, the inter-rater agreement rate reaches 54.93%, with K-Alpha of 0.76. For comparison, in Chatbot Arena (Zheng et al., 2023), the human agreement rate is 81% for binary classification (win/lose) and 63% for 13 scale (win/tie/lose). In contrast, we independently score each response on more granular 1-5 scale, yet still achieve substantial level of agreement. CrossEval Benchmark Statistics. The final CrossEval benchmark comprises 1,400 prompts across 14 capabilities, 4,200 reference model responses, and 8,400 human ratings with accompanying explanations. Table 1 details the number of task categories for each capability in CrossEval. Additionally, we provide several examples of the prompt set, along with human ratings and explanations, in Appendix B.1 and B.2, respectively."
        },
        {
            "title": "3.3 Building LLM-based Evaluators",
            "content": "In addition to benchmarking the capabilities of LLMs, CrossEval represents, to the best of our knowledge, the largest meta-evaluation benchmark currently available for measuring the correlation between LLM-based scoring and human judgments. Since each prompt includes three reference model responses and six human ratings, we are able to explore how to develop the most effective in-domain LLM evaluator for this benchmark."
        },
        {
            "title": "3.3.1 Prompting LLMs for Evaluation",
            "content": "While the LLM-as-a-Judge paradigm has gained popularity (Zheng et al., 2023), there is no standardized method for designing prompts or for guiding LLMs to output evaluation scores. Common practices include generating an answer first, setting evaluation rules manually, and then instructing the model to assign score to the response being evaluated (Zeng et al., 2024). In practice, we find that self-generated answers frequently lead to issues. For instance, response length can exceed model limits, preventing the model from generating score. This approach also causes the LLMs to overly rely on their own generated answers, overlooking valuable insights from human-annotated references. To address these issues, we propose the following prompting strategy: General Rubrics. We first provide the following rubrics for the 1-5 Likert scale in the system prompt: 5/5 - Amazing: The response is flawless and could hardly be improved. 4/5 - Pretty Good: The response is quite good, but has room for minor improvements. 3/5 - Okay: They are middle-of-the-road responses that could be improved in several ways. 2/5 - Pretty Bad: The response has major problems in helpfulness, truthfulness, or safety. 1/5 - Horrible: They are terrible responses and you would caution others against using models that generate responses like this. Multi-References-based Prompting. Next, we provide any attachments relevant to the prompt (e.g., document for Long Context or an image for Image Recognition), followed by the user prompt. For meta-evaluation, where we assess the performance of LLM-as-a-Judge, we can include up to two reference responses along with their scores and explanations. For example, when the LLM judges medium-quality response, we can provide low-quality and high-quality responses with their four ratings as context. For evaluating new model responses, all three model responses are included, with human annotations serving as the reference."
        },
        {
            "title": "Capabilities\nEnglish\nReasoning\nCoding\nImage Recognition\nTool Use\nLong Context\nSpanish",
            "content": "Coding & Reasoning Image Recognition & Reasoning Tool Use & Coding Tool Use & Reasoning Long Context & Coding Spanish & Reasoning Spanish & Image Recognition Overall Pearson (r) Overall Spearman (rs) Overall Kendall (τ ) GPT-4o mini 0.383 0.681 0.627 0.576 0.587 0.405 0.552 0.618 0.701 0.484 0.642 0.524 0.691 0.556 0.621 0.609 0. Llama 3.1 405B Claude 3.5 Sonnet GPT-4o-05-13 0.452 0.699 0.568 0.609 0.500 0.536 0.600 0.545 0.698 0.535 0.734 0.516 0.704 0.599 0.733 0.683 0.609 0.596 0. 0.819 0.588 0.665 0.620 0.715 0.752 0.696 0.676 0.550 0.498 0.731 0. 0.760 0.629 0.594 0.594 0.664 0.775 0.639 0.729 0.593 0.772 0.669 0.697 0.679 0.560 Table 2 Correlations between different LLMs and human ratings. The top section shows Pearson correlations across individual and cross capabilities for four LLMs, and the bottom three shaded rows present the overall correlations. Point Deduction-based Prompting. As noted in prior studies (Zheng et al., 2023), LLM-as-a-Judge often favors longer, more structured responses, leading to inflated evaluation scores. To mitigate this, we no longer have LLMs directly generate their own answers and assign scores. Instead, they summarize issues in both the reference examples and the evaluated response, specifying point deductions (Zhong et al., 2024). This point deduction-based prompting approach helps the LLM systematically analyze and assess responses in balanced way. The LLM is instructed to format its output as follows: User Prompt Analysis: Identify key requirements and objectives from the user prompt. Reference Examples Insights: Summarize scoring patterns and typical point deductions. Model Response Evaluation: List strengths and identify weaknesses, specifying point deductions for each. Holistic Assessment: Consider if major strengths outweigh minor issues and combine similar deductions to avoid double penalization. Balance deductions and positive aspects. Evaluation Score: Provide rating on scale of 1 to 5. By following this structured process, LLMs can effectively incorporate human insights from reference examples, analyze key issues in different model responses, and provide an accurate and fair evaluation score. The complete system and evaluation prompts are available in the Appendix B.4."
        },
        {
            "title": "3.3.2 Correlations with Human Judgements",
            "content": "To demonstrate the effectiveness of our method, we conduct experiments using four advanced LLMs: GPT-4o mini, Llama 3.1 405B (Llama Team, 2024), Claude 3.5 Sonnet (Anthropic, 2024), and GPT-4o (OpenAI, 2023). For each prompt, we provide two reference examples and ask the model to evaluate the third, comparing the models score with the average human rating, which serves as the human judgment. We conduct experiments across 4,200 samples spanning 14 capabilities to calculate the correlations, with the results shown in Table 2. Each LLM shows particular strengths in evaluating different capabilities. For instance, Claude 3.5 Sonnet performs well in Tool Use, Image Recognition & Reasoning, and Spanish & Image Recognition, while GPT-4o excels at evaluating cross capabilities such as Coding & Reasoning, Tool Use & Coding, Tool Use & Reasoning, and Spanish & Reasoning. Overall, GPT-4o achieves the highest correlations compared to the other LLMs. 8 For context, in the recent benchmark BigGen Bench (Kim et al., 2024), which includes gold references and human ratings, LLM-based scoring reached Pearson correlation of 0.627. In contrast, our score approaches 0.7. This suggests that, despite the openness and difficulty of the benchmark, making it impossible to annotate gold reference, we can still achieve reliable evaluations through the use of multiple reference examples. Discussion on Tool Use. In the benchmark, prompts related to tool use involve functionalities such as web browsing and code interpretation. However, the LLM APIs we experiment with do not support web browsing, and only the GPT-4 API supports code interpreters. Fortunately, when we specify the date of the reference examples and indicate that the answers may be dynamic, LLMs without web browsing features can still serve as effective evaluators, achieving Pearson correlations above 0.6 across all tool use-related capabilities. Additionally, enabling GPTs code interpreter results in similar correlation scores but incurs higher costs. This may be because the reference examples already provide sufficient context for evaluation, eliminating the need for the model to execute code. As result, we disable the code interpreter in subsequent evaluations. Discussion on Reference Examples. Given the substantial effort invested in collecting and annotating reference examples, ensuring their effectiveness for evaluation is crucial. To this end, we conduct ablation studies with GPT-4o to assess how the number of reference examples impacts the correlations. Figure 2 illustrates the results. clear trend emerges: as the number of reference examples increases, all three correlation metrics improve significantly. For example, the Pearson correlation starts at 0.578 with no reference examples, rises to 0.655 with one reference, and reaches 0.697 with two references. Notably, when evaluating new model responses in our benchmark, we provide all three reference examples, which could potentially lead to even higher correlations, delivering more accurate, expert-level evaluations. Figure 2 Ablation study on the number of reference examples. Final Evaluator Selection. Table 2 shows that different LLMs excel at different capabilities. This naturally leads to the idea of using mixture of LLMs as evaluators. For example, we could use Claude 3.5 to evaluate Spanish & Image Recognition and GPT-4o to evaluate Spanish & Reasoning, aiming for higher overall correlation. However, this approach proves impractical due to significant differences in scoring distributions across models: Claude 3.5 tends to give higher scores, while GPT-4o is more stringent. While this discrepancy is not an issue when presenting single score for the benchmark, it poses issues when analyzing the relationship between individual and cross-capability performance. The varying scoring distributions could make our conclusions unreliable. As result, we select GPT-4o as our final evaluator, while providing the results using Claude 3.5 in the Appendix C.2 for reference."
        },
        {
            "title": "4 Exploring Relationship between Individual and Cross Capabilities",
            "content": "In this section, we explore the relationship between individual and cross capabilities in LLMs. We first present the experimental setup, followed by detailed discussion of the findings based on the results from CrossEval."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "To ensure comprehensive coverage of LLM performance across capabilities, we select 17 models from five major model families: GPT (OpenAI, 2023), Claude (Anthropic, 2024), Gemini (Reid et al., 2024), Llama (Llama Team, 2024), and Reka (Ormazabal et al., 2024). Each model supports at least five cross-capability scenarios in our experiments (except o1 models). For consistency, we use the GPT-4o-05-13 model as the evaluator, with temperature set to 0 and seed set to 42 to ensure deterministic scoring. Each models responses are 9 Models GPT-4o mini GPT-4o o1-mini o1-preview Claude 3 Haiku Claude 3 Sonnet Claude 3 Opus Claude 3.5 Sonnet Gemini 1.5 Flash Gemini 1.5 Pro Gemini 1.5 Pro Exp Reka Edge Reka Flash Reka Core Llama 3.1 8B Llama 3.1 70B Llama 3.1 405B English Reasoning Coding Image Tool Use Long Context Spanish Individual Capabilities 73.64 76.12 75.25 78.59 63.87 69.19 68.94 75.00 66.59 71.91 75.87 52.23 63.87 71.54 64.11 68.82 73.52 69.31 72.84 81.02 82.30 56.81 62.88 66.22 71.54 63.25 70.61 73.02 45.30 62.63 68.69 53.97 62.88 69.31 71.17 72.03 80.70 79. 61.64 66.09 69.68 74.01 65.60 69.56 69.56 39.36 57.68 62.38 55.08 65.47 69.19 65.23 73.02 51.00 56.56 61.76 68.57 56.81 69.56 71.17 48.89 56.38 56.94 Cross Capabilities 42.09 47.04 47. 76.18 77.17 76.74 78.90 69.68 72.40 74.69 74.32 73.52 76.51 75.37 37.01 55.82 60.90 59.53 68.82 69.31 74.51 78.10 79.09 79.64 67.95 69.43 74.01 76.12 70.05 74.26 76.24 52.48 68.07 73.77 55.70 64.48 72.59 Models Coding & Rea. Image & Rea. Long & Coding Spanish & Rea. Spanish & Image Tool & Coding Tool & Rea. GPT-4o mini GPT-4o o1-mini o1-preview Claude 3 Haiku Claude 3 Sonnet Claude 3 Opus Claude 3.5 Sonnet Gemini 1.5 Flash Gemini 1.5 Pro Gemini 1.5 Pro Exp Reka Edge Reka Flash Reka Core Llama 3.1 8B Llama 3.1 70B Llama 3.1 405B 72.03 73.33 79.21 79.58 58.05 61.14 63.37 71.41 64.73 69.68 67.33 41.34 56.94 63.62 55.08 67.21 66.96 65.60 71.29 49.88 54.71 53.84 69.43 51.74 67.95 69.06 28.60 43.45 46.66 65.10 67.95 76.12 73.39 58.67 58.79 58.17 65.72 62.13 65.97 65.97 20.43 37.63 41.25 45.06 50.50 54.58 69.56 73.52 79.83 80.70 57.80 60.77 67.33 70.55 65.10 69.56 71.54 40.97 59.66 68.01 46.42 59.41 64.48 65.10 74.63 52.85 60.52 64.11 69.81 53.10 62.26 70.18 45.06 55.82 54.71 45.80 46.91 50.25 52.23 54.41 43.82 49.45 51.74 Table 3 Experimental results for individual and cross capabilities on the CrossEval benchmark. To avoid potential evaluator bias, we present GPT results solely as reference point and bold the best non-GPT results. In cross-capability evaluations, we define one of the involved individual capabilities as stronger and the other as weaker if the absolute score difference between them exceeds = 3 points. In 58 cross-capability scenarios where this difference is present (indicated by colored background), 38 cases show performance lower than both individual capabilities ( red background ), and 20 show performance between the two but closer to the weaker capability ( blue background ). Notably, no cross-capability score ever comes close to or exceeds the stronger individual capability. generated using their default decoding parameters to achieve optimal performance. For the Llama 3.1 405B model, we specifically use the FP8 version. complete list of model versions is provided in the Appendix C.1. In addition, while the Gemini API supports code interpreter functionality, it does not yet handle non-text outputs (e.g., data plots), so we exclude its results on tool-use-related prompts in our benchmark."
        },
        {
            "title": "4.2 Findings on the CrossEval Benchmark",
            "content": "To better present the results, we linearly map the average scores for each capability from 15 scale to 1100 scale. The full results are provided in Table 3. Since LLMs tend to prefer self-generated answers (Zheng et al., 2023), we exclude GPTs results from the comparative analysis and treat them as reference point. Our experiments reveal several key findings: CrossEval effectively differentiates advanced models. The CrossEval benchmark successfully distinguishes between state-of-the-art LLMs. For instance, the four Claude model variants achieve progressively higher scores in reasoning: 56.81, 62.88, 66.22, and 71.54. This mirrors the increasing capabilities associated with 10 (a) GPT-as-a-Judge (b) Claude-as-a-Judge Figure 3 Density distribution of cross-capability performance compared to the two individual capabilities. The plot illustrates pronounced Law of the Weakest Link effect in LLMs, where performance in cross-capability tasks tends to cluster around the weaker individual capability. This pattern is consistently observed regardless of the evaluator used. larger parameter models (Haiku Sonnet Opus) or updated versions (Claude 3 Claude 3.5). Similar trends are observed across all model families and capabilities, highlighting CrossEval is capable of capturing subtle differences in LLM performance across wide range of scenarios. LLMs exhibit Law of the Weakest Link effect in cross capabilities. To better understand how individual and cross capabilities interact, we identify strong and weak capabilities within cross-capability tasks when the absolute difference between their individual scores exceeds = 3. Notably, we find that in all cases where distinct strong and weak capability is present, cross-capability performance either matches or slightly underperforms the weaker capability. This indicates that performance on tasks requiring multiple abilities is significantly constrained by the weakest component, phenomenon closely aligned with the Law of the Weakest Link (Liebig, 1840). Similar to how the shortest stave limits the capacity of barrel, the weakest capability in LLMs governs its overall performance in most of the cross-capability scenarios. The Law of the Weakest Link effect is evaluator-agnostic. To further validate this phenomenon, we normalize strong and weak capability scores to standardized scale ranging from -1 to 1, and plot the density of cross-capability performance relative to these scores. score below -1 indicates that the cross-capability performance falls below the weaker individual capability, while 0 represents the average of the two. As shown in Figure 3, the Law of the Weakest Link effect holds true regardless of the evaluator used. With GPT-4o, the density peaks slightly below the weaker capability, while Claude 3.5 Sonnet shows slight peak above it. However, in both cases, performance clusters closely around the weaker capability. Moreover, we investigate varying values for both evaluators in Appendix C.3, where the Law of the Weakest Link is consistently demonstrated. Given that many real-world tasks require integrating multiple capabilities, this finding offers valuable insights for future LLM development. The Law of the Weakest Link effect suggests that deficiencies in an individual capability can substantially limit performance across any cross-capability tasks involving that capability. Our constructed CrossEval benchmark provides foundation for identifying LLM weaknesses, but further research is needed to more comprehensively diagnose and address these deficiencies without compromising other capabilities. Tool Use is currently the most challenging capability for LLMs. Among the capabilities tested, Tool Use stands out as the most challenging. Our prompt set includes tasks involving web browsing and code interpretation, and Llama 3.1 is the only model family that currently supports both. However, even Llama 3.1 405B struggled with Tool Use, scoring below 50 on this individual capability and only slightly above 50 on tasks combining Tool Use with Coding or Reasoning. These scores are significantly lower than those for other capabilities, indicating critical area for improvement. As tool use is fundamental for the development of future LLM-based agent systems, addressing this deficiency is essential. LLMs underperform in cross-capability tasks. Despite our efforts to maintain consistent difficulty level across both individual and cross-capability tasks, LLMs generally perform worse on tasks requiring multiple capabilities. For instance, in the Spanish & Reasoning and Spanish & Image tasks, where prompts are direct translations from their English counterparts, the models underperform in most cases compared to individual capabilities. Across all models, the average score for individual capabilities is 65.72, compared to 58.67 for cross capabilities, revealing significant performance gap. This disparity demonstrates that current LLMs remain heavily optimized for individual capabilities, with limited focus on cross-capabilities performance."
        },
        {
            "title": "5 How Individual-Capability Alterations Impact Cross-Capability Performance?",
            "content": "Beyond evaluating the static relationship between individual and cross capabilities of LLMs on CrossEval, we explore the crucial follow-up questions: when we adjust the performance of specific capabilities, how does this affect cross-capability performance? For reference, Amdahls Law (Amdahl, 1967), originating from parallel computing, states that the overall performance improvement gained by optimizing single part of system is limited by the fraction of time that the improved part is used. To explore this in LLMs, we propose prompting method designed to modulate specific capabilities of LLMs. Following this, we present case studies involving two LLMs in three cross-capability tasks to illustrate the effects of these alterations."
        },
        {
            "title": "5.1 Principle-based System Prompting",
            "content": "To reliably explore the impact of altering individual capabilities, we aim to enhance specific capability without significantly affecting others. This allows for more controlled and precise investigation into cross-capability performance dynamics. Our solution is principle-based method that iteratively refines the system prompt to enhance the specific capabilities of LLMs. It builds on the CrossEval dataset and evaluations to selectively boost individual capabilities. The approach involves the following steps: 1) Initial Setup: For each instance, we input the user prompt, the target models response, the evaluation feedback from our LLM-based system, and an evolving principle list (initially empty). 2) Iterative Refinement: Using GPT-4o, we iteratively generate principles that guide the models performance in particular capability. The model selects one of four operations for each instance: - ADD: Introduce new principle that isnt currently listed. - REPLACE: Substitute less significant principle with new one. - REVISE: Refine existing principles for greater clarity and precision. - KEEP: Leave the principles unchanged if no adjustments are necessary. 3) Final Principle List: After 100 iterations, this process yields principle-based system prompt tailored to enhance the target capability. By incorporating this system prompt into the LLMs, we instruct them to prioritize key aspects of performance, such as format adherence, problem-solving strategies, or error avoidance, for prompts related to particular capabilities. The complete principle-based prompts used in our experiments can be found in Appendix D.1."
        },
        {
            "title": "5.2 Case Study for Investigation",
            "content": "To analyze how individual-capability alterations affect cross-capability performance, we select three crosscapability tasks with the most significant individual performance gaps: Image Recognition & Reasoning, Spanish & Reasoning, and Spanish & Image Recognition. Additionally, we focus on two models, Claude 3 Haiku and Gemini 1.5 Flash, which display the largest performance discrepancies in these scenarios. The rationale behind choosing these combinations is that more pronounced gap between strong and weak capabilities provides clearer insights into the effects of selective capability enhancement on collective performance. Table 4 presents the complete experimental results, and we make the following key observations: Principle-based system prompting is particularly effective in enhancing weaker capabilities. In the Reasoning capability, for instance, performance improves substantially in both models: Claude 3 Haiku sees an increase"
        },
        {
            "title": "Models",
            "content": "Claude 3 Haiku + Reasoning + Image + Spanish Gemini 1.5 Flash + Reasoning + Image + Spanish"
        },
        {
            "title": "Cross Capabilities",
            "content": "Reasoning Image Recognition Spanish Image & Rea. Spanish & Rea. Spanish & Image 56.81 59.66 55.45 55.20 63.25 66.71 59.91 61.39 51.00 50. 54.71 53.59 56.81 62.50 63.00 61.89 67.95 68.20 64.98 67.21 70. 71.29 69.43 69.06 49.88 46.42 54.46 50.13 51.74 54.46 51.61 52.60 57. 59.04 57.55 56.81 65.10 66.59 62.13 64.86 52.85 52.11 55.08 53.72 53.10 59. 61.76 58.42 Table 4 Case study to investigate the impact of individual-capability alterations on cross-capability performance. + indicates the application of principle-based system prompting to enhance the specific capability X. The results show that improving weaker capabilities leads to more significant gains in corresponding cross-capability performance. of 2.85 points, while Gemini 1.5 Flash improves by 3.46 points. For Image Recognition, the improvements are even more significant, with Claude 3 Haiku improving by 3.71 points and Gemini 1.5 Flash by 6.19 points. These results suggest that the principles automatically derived from the CrossEval evaluation process provide sufficient guidance to enhance weaker capabilities in LLMs, even when applied solely as system prompts. However, for stronger capabilities such as Spanish, the same prompting method shows limited efficacy, indicating that refining already-strong capabilities is more challenging. Law of the Weakest Link effect persists after individual-capability alterations. Our case study also confirms that performance shifts in individual capabilities continue to conform to the Law of the Weakest Link effect. Specifically, altering the weaker capability in cross-capability scenario has significant effect on overall performance, while changes to the stronger capability result in only minor adjustments. For example, in the Image Recognition & Reasoning scenario with Claude 3 Haiku, when we introduce system prompt focused on reasoning, the stronger capability (Reasoning) improves by 2.85 points, but the weaker capability (Image Recognition) drops by 0.99 points, leading to an overall performance decrease of 3.46 points. Conversely, when an image-related system prompt is added, the weaker capability improves by 3.71 points, the stronger capability decreases by 1.36 points, and the overall cross-performance increases by 4.58 points. In 10 out of the 18 cross-capability scores examined across the two models, we observe one individual capability improving while the other declines. Notably, in 90% of these cases, changes in cross-capability performance closely follow the trends of the weaker capability. This strong alignment with the Law of the Weakest Link underscores the importance of addressing the weakest links in LLM capabilities to drive meaningful improvements in complex, real-world tasks. Conclusion of the case study. These case studies offer further insights into how LLMs conform to the Law of the Weakest Link. We show that targeted enhancement of weaker capabilities results in more significant improvements in cross-capability performance than focusing on stronger capabilities. Since LLMs underperform in cross-capability tasks, prioritizing the identification and enhancement of the weakest points should be key focus for future research and development."
        },
        {
            "title": "6.1 Evaluation of LLMs",
            "content": "The advancements in LLMs have shifted the focus of evaluation from specific NLP tasks (Wang et al., 2019b,a) to specific capabilities such as reasoning (Clark et al., 2018; Hendrycks et al., 2021a,b; Rein et al., 2023), coding (Chen et al., 2021; Austin et al., 2021; Cassano et al., 2023; Liu et al., 2023a), multilinguality (Shi et al., 2023), tool use (Srinivasan et al., 2023; Patil et al., 2023; Li et al., 2023; Yan et al., 2024), long context (Shaham et al., 2023; Kamradt, 2023; Zhang et al., 2024; An et al., 2024), image recognition (Yue et al., 2024), instruction following (Zhou et al., 2023), mastering domain-specific knowledge (Hendrycks et al., 13 2021a), and weakness identification (Chen et al., 2024). Moreover, benchmarks like BiGBench Bench assess range of abilities across multiple tasks but still target individual capabilities in isolation (Lin et al., 2024; Kim et al., 2024). As LLMs continue to evolve and tasks grow more complex, the evaluation of cross capabilities remains underexplored. Our work addresses this gap by systematically investigating these essential but overlooked cross capabilities. Another emerging area is the evaluation of LLM-based agents, which inherently require cross capabilities to function effectively in real-world applications. Unlike the evaluation of standalone LLM, which focuses on specific skills, the assessment of these agents typically emphasizes the overall success rate in completing tasks (Yao et al., 2022; Zhou et al., 2024; Koh et al., 2024; Liu et al., 2024; Xie et al., 2024) or executing particular actions (Deng et al., 2023; Ma et al., 2024). Although our CrossEval is designed for LLMs and not specifically for agent evaluation, it still encompasses key agent-related capabilities such as multi-modality, multilingualism, and tool use. Furthermore, it provides clear and comprehensive distinction between individual and cross capabilities, providing more granular framework for evaluation and analysis."
        },
        {
            "title": "6.2 Evaluation Metrics for Open-Ended Generation",
            "content": "Evaluation metrics have evolved alongside advances in model generation capabilities, moving from traditional n-gram-based measures (Papineni et al., 2002; Lin, 2004) to pre-trained language model (PLM)-based evaluators (Zhang et al., 2020; Sellam et al., 2020; Yuan et al., 2021; Zhong et al., 2022) and, more recently, to LLM-as-a-Judge frameworks (Liu et al., 2023b; Zheng et al., 2023). Given the large set of complex, open-ended prompts in our benchmark, we employ LLMs as evaluators to assess model outputs. Unlike previous methods that rely on self-generated prompts, we adopt point deduction-based prompting technique. Each instance is supported by three expert-annotated reference examples to enhance the reliability of the evaluation process. Furthermore, CrossEval is the largest meta-evaluation benchmark currently available for measuring the correlation between LLM-as-a-Judge assessments and human judgments, while also providing detailed insights into the specific capabilities that different LLMs excel at evaluating."
        },
        {
            "title": "7 Conclusion",
            "content": "We systematically investigated the cross capabilities of LLMs by introducing CrossEval, testbed designed to evaluate both individual and cross capabilities. We also developed an LLM-based judge that showed strong agreement with human judgments. Our experiments revealed that LLMs consistently follow the Law of the Weakest Link, where cross-capability performance is limited by the weakest ability, even after enhancing individual abilities. Our benchmark and findings highlight the importance of focusing on cross-capability development and evaluation in future LLM research."
        },
        {
            "title": "References",
            "content": "Alfred Adler. Study of Organ Inferiority and Its Psychical Compensation: Contribution to Clinical Medicine. Nervous and Mental Disease Publishing Co., New York, 1917. https://archive.org/details/adler-1917-inferiority. Gene M. Amdahl. Validity of the single processor approach to achieving large scale computing capabilities. In Proceedings of the April 18-20, 1967, Spring Joint Computer Conference, AFIPS 67 (Spring), pages 483485. Association for Computing Machinery, 1967. ISBN 9781450378956. doi: 10.1145/1465482.1465560. https://doi.org/ 10.1145/1465482.1465560. Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. L-eval: Instituting standardized evaluation for long context language models. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 1438814411. Association for Computational Linguistics, 2024. https://aclanthology.org/2024.acl-long.776. Philip W. Anderson. More is different: Broken symmetry and the nature of the hierarchical structure of science. Science, 177(4047):393396, 1972. doi: 10.1126/science.177.4047.393. https://www.science.org/doi/10.1126/science. 177.4047.393. Anthropic. Introducing the next generation of claude, 2024. https://www.anthropic.com/news/claude-3-family. Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. CoRR, abs/2108.07732, 2021. https://arxiv.org/abs/2108.07732. Federico Cassano, John Gouwar, Daniel Nguyen, Sydney Nguyen, Luna Phipps-Costin, Donald Pinckney, Ming-Ho Yee, Yangtian Zi, Carolyn Jane Anderson, Molly Q. Feldman, Arjun Guha, Michael Greenberg, and Abhinav Jangda. Multipl-e: scalable and polyglot approach to benchmarking neural code generation. IEEE Trans. Software Eng., 49(7):36753691, 2023. doi: 10.1109/TSE.2023.3267446. https://doi.org/10.1109/TSE.2023.3267446. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. CoRR, abs/2107.03374, 2021. https://arxiv.org/abs/2107.03374. Yulong Chen, Yang Liu, Jianhao Yan, Xuefeng Bai, Ming Zhong, Yinghao Yang, Ziyi Yang, Chenguang Zhu, and Yue Zhang. See what llms cannot answer: self-challenge framework for uncovering llm weaknesses. In First Conference on Language Modeling, 2024. https://openreview.net/forum?id=18iNTRPx8c#discussion. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the AI2 reasoning challenge. CoRR, abs/1803.05457, 2018. http://arxiv.org/abs/1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. CoRR, abs/2110.14168, 2021. https://arxiv.org/abs/2110.14168. Peter A. Corning. The Synergism Hypothesis: Theory of Progressive Evolution. McGraw-Hill, New York, 1983. ISBN 0070131724. https://archive.org/details/synergismhypothe0000corn. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Samual Stevens, Boshi Wang, Huan Sun, and Yu Su. Mind2web: Towards generalist agent for the web. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. http://papers.nips.cc/paper_files/paper/2023/hash/ 5950bf290a1570ea401bf98882128160-Abstract-Datasets_and_Benchmarks.html. 15 Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net, 2021a. https://openreview.net/forum?id=d7KBjmI3GmQ. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Joaquin Vanschoren and Sai-Kit Yeung, editors, Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual, 2021b. https://datasets-benchmarks-proceedings. neurips.cc/paper/2021/hash/be83ab3ecd0db773eb2dc1b0a17836a1-Abstract-round2.html. Gregory Kamradt. Llmtest_needleinahaystack, 2023. https://github.com/gkamradt/LLMTest_NeedleInAHaystack/ blob/main/README.md. Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Choi, Sheikh Shafayat, Jinheon Baek, Sue Hyun Park, Hyeonbin Hwang, Jinkyung Jo, Hyowon Cho, Haebin Shin, Seongyun Lee, Hanseok Oh, Noah Lee, Namgyu Ho, Se June Joo, Miyoung Ko, Yoonjoo Lee, Hyungjoo Chae, Jamin Shin, Joel Jang, Seonghyeon Ye, Bill Yuchen Lin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. The biggen bench: principled benchmark for fine-grained evaluation of language models with language models. CoRR, abs/2406.05761, 2024. doi: 10.48550/ARXIV.2406.05761. https://doi.org/10.48550/arXiv.2406.05761. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Russ Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. In ACL 2024, 2024. https://aclanthology.org/2024.acl-long.50. Klaus Krippendorff. Content Analysis: An Introduction to Its Methodology. Sage Publications, Thousand Oaks, CA, 4th edition, 2018. https://us.sagepub.com/en-us/nam/content-analysis/book258450. Minghao Li, Yingxiu Zhao, Bowen Yu, Feifan Song, Hangyu Li, Haiyang Yu, Zhoujun Li, Fei Huang, and Yongbin Li. Api-bank: comprehensive benchmark for tool-augmented llms. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 31023116. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.EMNLP-MAIN.187. https://doi.org/10.18653/v1/2023.emnlp-main.187. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. CoRR, abs/2406.11939, 2024. doi: 10.48550/ARXIV.2406.11939. https://doi.org/10.48550/arXiv.2406.11939. Justus Freiherr von Liebig. Die organische Chemie in ihrer Anwendung auf Agricultur und Physiologie. F. Vieweg und Sohn, Braunschweig, 1840. https://books.google.com/books?id=6XkSAAAAYAAJ. Bill Yuchen Lin, Yuntian Deng, Khyathi Raghavi Chandu, Faeze Brahman, Abhilasha Ravichander, Valentina Pyatkin, Nouha Dziri, Ronan Le Bras, and Yejin Choi. Wildbench: Benchmarking llms with challenging tasks from real users in the wild. CoRR, abs/2406.04770, 2024. doi: 10.48550/ARXIV.2406.04770. https://doi.org/10.48550/arXiv.2406. 04770. Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, 2004. https://aclanthology.org/W04-1013. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatgpt really correct? rigorous evaluation of large language models for code generation. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023a. http://papers.nips.cc/paper_files/paper/2023/hash/ 43e9d647ccd3e4b7b5baab53f0368686-Abstract-Conference.html. Xiao Liu, Hao Yu, Hanchen Zhang, Yifan Xu, Xuanyu Lei, Hanyu Lai, Yu Gu, Hangliang Ding, Kaiwen Men, Kejuan Yang, Shudan Zhang, Xiang Deng, Aohan Zeng, Zhengxiao Du, Chenhui Zhang, Sheng Shen, Tianjun Zhang, Yu Su, Huan Sun, Minlie Huang, Yuxiao Dong, and Jie Tang. Agentbench: Evaluating llms as agents. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. https://openreview.net/forum?id=zAdUB0aCTQ. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. G-eval: NLG evaluation using gpt-4 with better human alignment. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pages 25112522. Association for Computational Linguistics, 2023b. doi: 10.18653/V1/2023.EMNLP-MAIN.153. https://doi.org/10.18653/v1/2023.emnlp-main.153. Llama Team. The llama 3 herd of models. CoRR, abs/2407.21783, 2024. doi: 10.48550/ARXIV.2407.21783. https://doi.org/10.48550/arXiv.2407.21783. Chang Ma, Junlei Zhang, Zhihao Zhu, Cheng Yang, Yujiu Yang, Yaohui Jin, Zhenzhong Lan, Lingpeng Kong, and Junxian He. Agentboard: An analytical evaluation board of multi-turn LLM agents. CoRR, abs/2401.13178, 2024. doi: 10.48550/ARXIV.2401.13178. https://doi.org/10.48550/arXiv.2401.13178. OpenAI. GPT-4 technical report. CoRR, abs/2303.08774, 2023. doi: 10.48550/ARXIV.2303.08774. https://doi.org/ 10.48550/arXiv.2303.08774. OpenAI."
        },
        {
            "title": "Openai",
            "content": "o1 system card,"
        },
        {
            "title": "September",
            "content": "2024. 67qJD51Aur3eIc96iOfeOP/71551c3d223cd97e591aa89567306912/o1_system_card.pdf. 18. https://assets.ctfassets.net/kftzwdyauwt9/ 2024-09Accessed: Aitor Ormazabal, Che Zheng, Cyprien de Masson dAutume, Dani Yogatama, Deyu Fu, Donovan Ong, Eric Chen, Eugenie Lamprecht, Hai Pham, Isaac Ong, Kaloyan Aleksiev, Lei Li, Matthew Henderson, Max Bain, Mikel Artetxe, Nishant Relan, Piotr Padlewski, Qi Liu, Ren Chen, Samuel Phua, Yazheng Yang, Yi Tay, Yuqi Wang, Zhongkai Zhu, and Zhihui Xie. Reka core, flash, and edge: series of powerful multimodal language models. CoRR, abs/2404.12387, 2024. doi: 10.48550/ARXIV.2404.12387. https://doi.org/10.48550/arXiv.2404.12387. Piotr Padlewski, Max Bain, Matthew Henderson, Zhongkai Zhu, Nishant Relan, Hai Pham, Donovan Ong, Kaloyan Aleksiev, Aitor Ormazabal, Samuel Phua, Ethan Yeo, Eugenie Lamprecht, Qi Liu, Yuqi Wang, Eric Chen, Deyu Fu, Lei Li, Che Zheng, Cyprien de Masson dAutume, Dani Yogatama, Mikel Artetxe, and Yi Tay. Vibe-eval: hard evaluation suite for measuring progress of multimodal language models. CoRR, abs/2405.02287, 2024. doi: 10.48550/ARXIV.2405.02287. https://doi.org/10.48550/arXiv.2405.02287. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, pages 311318, 2002. doi: 10.3115/1073083.1073135. https://aclanthology.org/P02-1040. Shishir G. Patil, Tianjun Zhang, Xin Wang, and Joseph E. Gonzalez. Gorilla: Large language model connected with massive apis. CoRR, abs/2305.15334, 2023. doi: 10.48550/ARXIV.2305.15334. https://doi.org/10.48550/arXiv. 2305.15334. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy P. Lillicrap, Jean-Baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, Ioannis Antonoglou, Rohan Anil, Sebastian Borgeaud, Andrew M. Dai, Katie Millican, Ethan Dyer, Mia Glaese, Thibault Sottiaux, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, James Molloy, Jilin Chen, Michael Isard, Paul Barham, Tom Hennigan, Ross McIlroy, Melvin Johnson, Johan Schalkwyk, Eli Collins, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Clemens Meyer, Gregory Thornton, Zhen Yang, Henryk Michalewski, Zaheer Abbas, Nathan Schucher, Ankesh Anand, Richard Ives, James Keeling, Karel Lenc, Salem Haykal, Siamak Shakeri, Pranav Shyam, Aakanksha Chowdhery, Roman Ring, Stephen Spencer, Eren Sezener, and et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. CoRR, abs/2403.05530, 2024. doi: 10.48550/ARXIV.2403.05530. https: //doi.org/10.48550/arXiv.2403.05530. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark. CoRR, abs/2311.12022, 2023. doi: 10.48550/ARXIV.2311.12022. https://doi.org/10.48550/arXiv.2311.12022. Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. BLEURT: learning robust metrics for text generation. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel R. Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 78817892. Association for Computational Linguistics, 2020. doi: 10.18653/V1/2020.ACL-MAIN.704. https://doi.org/10.18653/v1/2020. acl-main.704. Uri Shaham, Maor Ivgi, Avia Efrat, Jonathan Berant, and Omer Levy. Zeroscrolls: zero-shot benchmark for long text understanding. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pages 79777989. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-EMNLP.536. https://doi.org/10.18653/v1/ 2023.findings-emnlp.536. 17 Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. https://openreview.net/forum?id=fR3wGCk-IXp. Venkat Krishna Srinivasan, Zhen Dong, Banghua Zhu, Brian Yu, Hanzi Mao, Damon Mosk-Aoyama, Kurt Keutzer, Jiantao Jiao, and Jian Zhang. Nexusraven: commercially-permissive language model for function calling. In NeurIPS 2023 Workshop on Instruction Tuning and Instruction Following, 2023. https://openreview.net/forum? id=Md6RUrGz67. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Superglue: stickier benchmark for general-purpose language understanding systems. In Hanna M. Wallach, Hugo Larochelle, Alina Beygelzimer, Florence dAlché-Buc, Emily B. Fox, and Roman Garnett, editors, Advances in Neural Information Processing Systems 32: Annual Conference on Neural Information Processing Systems 2019, NeurIPS 2019, December 8-14, 2019, Vancouver, BC, Canada, pages 32613275, 2019a. https://proceedings.neurips.cc/paper/2019/hash/4496bf24afe7fab6f046bf4923da8de6-Abstract.html. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. GLUE: multitask benchmark and analysis platform for natural language understanding. In 7th International Conference on Learning Representations, ICLR 2019, New Orleans, LA, USA, May 6-9, 2019. OpenReview.net, 2019b. https://openreview.net/forum?id=rJ4km2R5t7. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, and Tao Yu. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. CoRR, abs/2404.07972, 2024. doi: 10.48550/ARXIV.2404.07972. https://doi.org/10.48550/arXiv.2404.07972. Fanjia Yan, Huanzhi Mao, Charlie Cheng-Jie Ji, Tianjun Zhang, Shishir G. Patil, Ion Stoica, and Joseph E. Gonzalez. Berkeley function calling leaderboard, 2024. https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_ leaderboard.html. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable real-world web interaction with grounded language agents. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. http: //papers.nips.cc/paper_files/paper/2022/hash/82ad13ec01f9fe44c01cb91814fd7b8c-Abstract-Conference.html. Weizhe Yuan, Graham Neubig, and Pengfei Liu. Bartscore: Evaluating generated text as text generation. In MarcAurelio Ranzato, Alina Beygelzimer, Yann N. Dauphin, Percy Liang, and Jennifer Wortman Vaughan, editors, Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 2726327277, 2021. https://proceedings.neurips. cc/paper/2021/hash/e4d2b6e6fdeca3e60e0f1a62fee3d9dd-Abstract.html. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024. https://openaccess.thecvf.com/content/CVPR2024/html/Yue_MMMU_A_Massive_ Multi-discipline_Multimodal_Understanding_and_Reasoning_Benchmark_for_CVPR_2024_paper.html. Zhiyuan Zeng, Jiatong Yu, Tianyu Gao, Yu Meng, Tanya Goyal, and Danqi Chen. Evaluating large language models at evaluating instruction following. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. https://openreview.net/forum?id=tr0KidwPLc. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net, 2020. https://openreview.net/forum?id=SkeHuCVFDr. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, and Maosong Sun. bench: Extending long context evaluation beyond 100k tokens. CoRR, abs/2402.13718, 2024. doi: 10.48550/ARXIV.2402.13718. https://doi.org/10.48550/arXiv.2402.13718. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine, editors, Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing 18 Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. http://papers.nips.cc/paper_ files/paper/2023/hash/91f18a1287b398d378ef22505bf41832-Abstract-Datasets_and_Benchmarks.html. Ming Zhong, Yang Liu, Da Yin, Yuning Mao, Yizhu Jiao, Pengfei Liu, Chenguang Zhu, Heng Ji, and Jiawei Han. Towards unified multi-dimensional evaluator for text generation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pages 20232038. Association for Computational Linguistics, 2022. doi: 10.18653/V1/2022.EMNLP-MAIN.131. https://doi.org/10.18653/v1/2022.emnlp-main.131. Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen. Multi-lora composition for image generation. CoRR, abs/2402.16843, 2024. doi: 10.48550/ARXIV.2402.16843. https://doi.org/10.48550/arXiv.2402.16843. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. CoRR, abs/2311.07911, 2023. doi: 10.48550/ARXIV. 2311.07911. https://doi.org/10.48550/arXiv.2311.07911. Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. Webarena: realistic web environment for building autonomous agents. In ICLR 2024, 2024. https://openreview.net/forum?id=oKn9c6ytLx."
        },
        {
            "title": "A Complete Taxonomy",
            "content": "A.1 Taxonomy of Individual Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Taxonomy of Cross Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "B CrossEval Benchmark",
            "content": "B.1 Prompt Set Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.2 Reference Examples . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.3 Guidelines for Difficulty Levels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.4 Prompts for Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.5 Case Study for LLM-as-a-Judge on CrossEval . . . . . . . . . . . . . . . . . . . . . . . . . . Exploring Relationships between Individual & Cross Capabilities C.1 Model Versions Used in Our Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.2 Results for Claude-as-a-Judge . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . C.3 Discussion on Distinguishing Weak and Strong Capabilities . . . . . . . . . . . . . . . . . C.4 Results for Different Difficulty Levels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . How Individual-Capability Alterations Impact Cross-Capability Performance D.1 Prompt to Generate Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D.2 Case Study for Principle-based System Prompts . . . . . . . . . . . . . . . . . . . . . . . . . . 21 21 26 29 29 46 48 50 52 52 54 56 57 57"
        },
        {
            "title": "A Complete Taxonomy",
            "content": "To ensure the comprehensiveness of the prompt sets in our evaluations, we build taxonomy with Level-1 (L1) and Level-2 (L2) categories. More concretely, Tables 5 10 and Tables 11 13, present the taxonomy for individual capabilities (English and Multilingual, Reasoning, Coding, Image Recognition, Tool Use, and Long Context) and cross capabilities (Coding & Reasoning, Image Recognition & Reasoning, Tool Use & Coding, Tool Use & Reasoning, Long Context & Coding), respectively. A.1 Taxonomy of Individual Capabilities L1 Categories"
        },
        {
            "title": "Factual Questions about Recent and Current Things",
            "content": "Very Accurate Questions (Beyond Expected Model Knowledge) Procedural Questions about Recent, Current, or Local Things Recommendations / Brainstorming about Local and Current Things"
        },
        {
            "title": "Tasks with File Uploads",
            "content": "Table 5 Taxonomy of the tool use capability. L2 Categories Historical events & figures Scientific concepts and explanations Geographical information Cultural & social topics Technical information Historical events & figures Scientific concepts and explanations Geographical information Cultural & social topics Technical information Cooking & food preparation Home & DIY projects Technology & devices Arts & crafts Travel & transportation Work & productivity Health & fitness Dining & food suggestions Entertainment suggestions Travel & destinations suggestions Product & service recommendations"
        },
        {
            "title": "Content Summarization\nQuestion Answering",
            "content": "21 L1 Categories"
        },
        {
            "title": "Procedural Questions",
            "content": "L2 Categories Historical events & figures Scientific concepts and explanations Geographical information Cultural & social topics Technical information Cooking & food preparation Home & DIY projects Technology & devices Arts & crafts Travel & transportation Finance & budgeting Work & productivity Health & fitness"
        },
        {
            "title": "Language Assistance",
            "content": "Grammar, spelling, & vocabulary Writing & Content Creation"
        },
        {
            "title": "Dialogue",
            "content": "Recommendations / Brainstorming"
        },
        {
            "title": "Social Interaction and Communication",
            "content": "Table 6 Taxonomy of the English and multilingual capabilities. Analysis Creative writing: Fiction Creative writing: Poetry and Songwriting Creative writing: Social media posts Creative writing: Nonfiction Business writing Legal writing Classification Summarization & editing Identity / Personas Chit-Chat Advice Games: Choose-your-own-adventure Games: Word & language Games: Social & party Dining & food suggestions Entertainment suggestions Travel & destinations suggestions Product & service recommendations Build confidence and self-esteem Emotional support Goal setting Motivation Physical health support Professional and career support Relationship support Tutoring and learning support Debate and opinions Discuss shared interests Humor and jokes Socialize with friends (group chat) 22 L1 Categories"
        },
        {
            "title": "Commonsense Reasoning",
            "content": "Logic / Problem Solving"
        },
        {
            "title": "Social and Emotional Reasoning",
            "content": "L2 Categories Arithmetic & basic math Algebra & equations Geometry & trigonometry Calculus & advanced math Probability & statistics Discrete math & logic Ordinary and partial differential equations Math word problem solving Math question answering Theorem proving (e.g. proofs) Mathematical model building"
        },
        {
            "title": "Physical reasoning\nTemporal reasoning\nSpatial reasoning",
            "content": "Identifying root causes & issues Evaluating evidence & reasoning Identifying pros & cons Inductive reasoning Deductive reasoning Empathy and perspective taking Social norm understanding Humor understanding Negotiation Emotion recognition / sentiment analysis"
        },
        {
            "title": "Moral and Ethical Reasoning",
            "content": "Consequence evaluation Applying moral and ethical principles Resolving moral or ethical dilemmas (conflict of principles)"
        },
        {
            "title": "Legal Reasoning",
            "content": "Hypothesis formation and testing Causal reasoning Scientific evidence evaluation Model-based reasoning Case-Based Reasoning Statutory Interpretation Contract Interpretation Administrative Regulation Interpretation Legal Evidence Evaluation Table 7 Taxonomy of the reasoning capability. 23 L1 Categories Code Generation / Synthesis"
        },
        {
            "title": "Code Debugging",
            "content": "Code Review & Best Practices L2 Categories Code generation (Text to Code) Code completion Code Summarization / Compression Code to Code (same language) CLI Coding Ecosystem Code to Code (different languages)"
        },
        {
            "title": "Comment generation\nCommit text generation\nDocument this function\nCreate example usages of this function\nCreate API documentation",
            "content": "Debugging & troubleshooting Testing Code review Security Review Quality Assurance Log Analysis (Text to Text) Table 8 Taxonomy of the coding capability. L1 Categories"
        },
        {
            "title": "Graceful Refusals",
            "content": "Table 9 Taxonomy of the image recognition capability. 24 L2 Categories Single Object Recognition Multiple Object Recognition Fine-Grained Object Recognition"
        },
        {
            "title": "Vague or unrelated question\nBlurry image\nUnsupported capabilities",
            "content": "L1 Categories"
        },
        {
            "title": "Summarization",
            "content": "Multi-Document Understanding (Q&A) Table 10 Taxonomy of the long context capability. L2 Categories Scientific Documents Financial Documents Books Legal Documents Podcast transcripts Video/Movie transcripts Scientific Documents Financial Documents Books Legal Documents Podcast transcripts Video/Movie transcripts Home & personal Work & business A.2 Taxonomy of Cross Capabilities L1 Categories Coding Q&A (Text to Text) L2 Categories Programming concepts & guidance Software Architecture Language-specific features Code summarization Frameworks & tools"
        },
        {
            "title": "Mathematical Calculation",
            "content": "Arithmetic & basic math Algebra & equations Geometry & trigonometry Calculus & advanced math Probability & statistics Discrete math & logic Ordinary and partial differential equations Table 11 Taxonomy of the coding & reasoning capability. L1 Categories"
        },
        {
            "title": "Diagram Understanding",
            "content": "L2 Categories Scientific Diagram Understanding Flowchart Understanding Graph Understanding"
        },
        {
            "title": "Chart Understanding",
            "content": "Basic Chart Understanding (Localization) Basic Chart Descriptions Chart reasoning Text-Rich Understanding"
        },
        {
            "title": "Formula understanding\nFigure understanding",
            "content": "Table 12 Taxonomy of the image recognition & reasoning capability. 26 L1 Categories Repository-Level Code Generation Repository-Level Code Understanding L2 Categories Code generation (Text to Code) Code completion Code Summarization Code to Code (different languages) Code modification Code Q&A / summarization Code walkthroughs Repository-Level Code Debugging Debugging & troubleshooting"
        },
        {
            "title": "API Docs Understanding",
            "content": "Q&A on API Code generation with API Table 13 Taxonomy of the long context & coding capability. L1 Categories"
        },
        {
            "title": "Code Debugging with Execution",
            "content": "L2 Categories Code generation and execution (Text to Code) Code to Code (Same language) Create example usages of this function Debugging, troubleshooting, and optimizing code Testing"
        },
        {
            "title": "Code Execution with File Uploads",
            "content": "Data Analysis Data Visualization Code Review / Explanation / Debugging Table 14 Taxonomy of the tool use & coding capability. 27 L1 Categories"
        },
        {
            "title": "Mathematical Reasoning",
            "content": "L2 Categories Math word problem solving Math question answering"
        },
        {
            "title": "Physics\nChemistry\nUnits and Measures\nComputational Sciences\nEarth Sciences\nMaterials\nSpace and Astronomy\nLife Sciences\nTechnological World\nWeather and Meteorology\nFood Science\nTransportation\nHealth and Medicine\nPhysical Geography\nEngineering",
            "content": "Arithmetic & basic math Algebra & equations Geometry & trigonometry Calculus & advanced math Probability & statistics Discrete math & logic Number Theory Linear Algebra Plotting Complex Analysis Continued Fractions Trigonometry Ordinary and partial differential equations Table 15 Taxonomy of the tool use & reasoning capability."
        },
        {
            "title": "B CrossEval Benchmark",
            "content": "B.1 Prompt Set Examples To provide an intuitive sense of the types and difficulty of the prompt set in our benchmark CrossEval, we present examples for each capability, including the difficulty level, L1 and L2 categories, and the prompts. Tables 16 22 correspond to individual capabilities, while Tables 23 27 pertain to cross capabilities. Difficulty L1 Category L2 Category Prompt Logic / problem solving Deductive reasoning All bachelors have never married. John is man who has never been married. Is John bachelor? Easy Easy Commonsense Reasoning Spatial reasoning Medium Mathematical Calculation Discrete math & logic Medium Social and Emotional Reasoning Empathy and perspective taking Hard Mathematical Calculation Ordinary and partial differential equations Hard Social and Emotional Reasoning Humor understanding Hard Mathematical Reasoning Mathematical model building If you enter building from the east side, walking west, and then take two rights and left down corridors, which direction are you facing? Jane won the lottery and decided to spend some of the money. She spent $1.50 on the first day. She spent $3 on the second day. She spent $4.50 on the third day. She kept spending her winnings in the same pattern and then on the last day, she spent her remaining $300. How much did she win in the lottery? have member on my team that is not pulling his weight and am thinking about firing him. heard from another colleague that he may be going through divorce, but he should not allow this to affect his work. Our team is taking huge productivity hit. What should do? Explain it to me. Solve the initial value problem: 1 2 uxx uy = 2 x2 , u(x, 0) = x. You will find that the solution blows up in finite time. Explain this in terms of the characteristics for this equation and explain your reasoning step by step. Two chemists are sitting at bar. The first chemist tells the bartender, Ill have some H2O. The second chemist tells the bartender, will also have some water. The first chemist tells the second chemist, darn my murder plot failed. Please explain this joke to me One interesting and complex problem that can be addressed through mathematical modeling in fisheries biology is the effectiveness of fish stocking to increase angling opportunities. Problem Statement: Optimize the amount of angling opportunities by introducing the correct number of bass fish to given lake. Variables to Consider: x: Size of given body of water. v: volume of vegetation growing in body of water. y: amount of forage fish per acre via sampling data. c: number of hours in angling pressure on given body of water per month. Table 16 Examples of the prompt set for reasoning capability. Difficulty L1 Category L2 Category Prompt Easy Procedural Questions Technology & devices Easy Recommendations / Brainstorming Product & service recommendations Medium Dialogue Identity / Personas Medium Writing & content creation Analysis Hard Factual Questions Technical information Hard Dialogue Games: Choose-your-ownadventure Hard Social interaction and communication Humor and jokes Im having trouble setting up my router. want to change the stock password for our home network, and also create guest network with separate password for my friends and family. Can you help me change my password, and create separate guest network with new password? My wife is really into her arts and crafts. She loves painting in her spare time. Can you please recommend something could get her as present? Tell me 6 candidates. You are Christopher Walken. am Sylvester Stallone. Create series of sentence openers about our movies that can respond to. try and make them serious so can try and make you laugh. In Noo Sara-Wiwas book about traveling to Nigeria Transwonderland, how does her outlook on Nigeria change as her journey through the country progresses? Include descriptions of her first impressions when traveling from one region/city of Nigeria to another and describe her feelings as someone who has Nigerian heritage but moved to England at very young age. Use vivid language in your response. My question relates to 3d printing. Id like to understand more about the chemical differences between two printing materials PLA and TPU. Start by explaining the chemical differences. Then talk about the physical properties of both materials, listing three to five use cases for each. Finally, give me an insight into practical considerations when printing with these materials. Im particularly interested in recommended extruder and printing bed settings. - Im looking for detailed response written in clear and simple English. - Im novice, so be sure to italicize any technical terms and provide definition in parentheses. - Please keep your response to 600 words, give or take 10 percent. - Separate sections and sub-sections with H1 and H2 headers. You are game developer, focused on choose-your-own-adventure style text-based games. You are creating main character for the story and need to finalize aspects of the characters personality. The main character is young male, roughly 14 years of age. He is wizard with science and technology subjects, and his personality should reflect this. He has so far chosen routes that lead the character down positive route, with general increase in his knowledge and skills. He is unlucky in love but romance is one of the major focuses of the narrative. What sort of creative personality would the character have that allows the player to connect with and better feel themselves in the role as they choose the paths they are going to take? List five less common ones. Also, please provide variety of routes in this game that might change this personality. Make sure you provide at least 3 routes with each one being around 150 words. At least one route should have negative consequences for the characters personality, and one should be decisively positive. Provide name for this character that reflects the personality you choose for them. need to write short stand-up comedy routine for friends dinner party. It should take no longer than 4 minutes to perform. The audience will consist of baker, doctor, and florist, so try and make jokes relevant to them. can do good impression of Homer Simpson, so please write it in his style. The tone should be silly and playful, but be sure not to make fun of the audience. Table 17 Examples of the prompt set for English capability. 30 Difficulty L1 Category L2 Category Prompt Easy Code review & Best practices Quality Assurance Easy Code generation / synthesis Coding Ecosystem Medium Code documentation Create API documentation Medium Code debugging Testing Hard Code documentation Comment generation Hard Code generation / synthesis Coding Ecosystem Hard Code generation / synthesis Code completion Find any issues with the following code and suggests any potential fixes. Our goal with the code is to keep drawing lottery tickets (where it is lottery game where you match 6 numbers out of 49 numbers) until there is winning ticket. We want to print out the time it takes and the number of draws until we draw the winning ticket, and then print the actual winning ticket along with the theoretical expected number of draws until success. {attached code} What command is used to add package to rust project? How do you fix error[E0433]: failed to resolve: could not find quote in the crate root or error[E0432]: unresolved import quote {attached code} Create an API documentation for this Python code that utilizes Flask for API routes. {attached code} want to update the data have determined in the database by pulling it with post. However, it is not updated. The codes in the view.py file are also added. It may work for the application. Also ensure to write unit tests for the function. {attached code} Create readme document that breaks down what the code does, and the the various techniques that it uses. Also printout some example usage of the program, displaying the result/solution to the example data in code. Also include section breaking down the time/space complexity of the program, and how the efficiency of this program compares to other methods without any optimizations/techniques. The document should read at an undergrad level. {attached code} Please create React custom hook that can store, retrieve, and sync data from the browsers local storage to the React component that uses the hook. Use the localStorage API. Hook should be called useLocalStorage. When the component mounts, it should read the local storage value as the initial value, and update local storage when the components state changes, and listen for changes in local storage and update the component state accordingly. The hooks API should look like useState. Ive been working on implementing some computer vision algorithms. Ive written the Harris corner detection for finding want some other feature finding algorithms: good features. FAST and ORB features. Can you finish those functions? Also, want to implement Lucas-Kanade optical flow. have rough idea of what the function will take in, can you also finish this for me? {attached code} Table 18 Examples of the prompt set for coding capability. 31 Difficulty L1 Category L2 Category Image Prompt Easy Image Captioning Descriptive Captioning Describe every creature in this image. Easy Graceful refusals Blurry image What does the license plate say? Medium Attribute and Relationship Identification Object Attribute Identification What is the color and shape of the 10 ball in the photo? Medium Object Recognition Multiple Object Recognition How many of these dogs have floppy ears? How many of the floppy-eared dogs have black fur? Hard Comprehensive use case Memes Explain this meme in detail."
        },
        {
            "title": "Hard",
            "content": "Scene Understanding Indoor Scene Understanding What type of person would you imagine lives in this room? Pick 5 items to justify your answer. Hard Comprehensive use case Visual How to My bike is not ridable. What does it need to be fixed and how do fix it? Table 19 Examples of the prompt set for image recognition capability. 32 Difficulty L1 Category L2 Category Prompt Easy Recommendations / Brainstorming about Local and Current Things Entertainment suggestions What are some fun things can watch on local channels in Henry County, Illinois today? Easy Tasks with File Uploads Content Summarization Medium Medium Hard Procedural Questions about Recent, Current, or Local Things Factual Questions about Recent and Current Things Very accurate questions (beyond expected model knowledge) Home & DIY projects Cultural & social topics Scientific concepts and explanations Hard Tasks with File Uploads Question Answering Hard Procedural Questions about Recent, Current, or Local Things Arts & crafts Hard Factual Questions about Recent and Current Things Geographical information Summarize the below content in the attached file in 150 words or less, focusing on the mode of action, possible adverse effects and effectiveness. {attached file} What are the step by step instructions of installing Black And Decker BD05MWT6 Window Air Conditioner, 5000 BTUs, unit? Will it fit into window with the dimensions of 24 inches wide and 48 inches height? Show me the current top 10 hockey players in the USA, what the game statistics are for each person, and how many awards they have won. Please list all recent coronal mass ejections from the past 365 days and what their magnitude was. Were there any correlating changes in the schumann resonance? Does science recognize effects of coronal mass ejections from the sun on the schumann resonance? The text mentions several individuals and their contributions or reputations in 1817. Choose three and discuss how they are represented and their significance in the broader historical or cultural context of that year. Also, what role does satire and public opinion play in the text, particularly in the portrayal of political figures and events? Provide specific examples from the given file. {attached file} want to re-cover pair of wingback chairs. love the idea of painting or printing my own fabric. Can you give me list of supplies for this? also need step-by-step walkthrough of easy ways to paint or transfer scenes to fabric.-a selection of two or three comprehensive youtube videos would be really helpful. Im also open to other ideas like stamping or stenciling. GIve me list of methods that wont break the bank. Finally, need step-by-step instructions for re-covering them as well. Please list any fabric and/or craft stores near me, their hours of operation. List them in the order of most to least likely to have the supplies need. Make table of all countries in which at least 25% of the current population speaks French. The table should include the name of the country, their current population, the percentage of that population who speak French, and the name of the current head of state. Below the table, summarize trends in the percentage of French language speakers globally over the past four years. This summary should be no more than 150 words. Finally, list any examples of French-language radio stations, French-language newspapers, and French-language television stations currently operating in countries in which French is not the predominant national language. Provide links websites for those entities wherever possible. Table 20 Examples of the prompt set for tool use capability. 33 Difficulty L1 Category L2 Category Prompt Easy Factoid or Complex Question Answering Podcast transcripts Easy Multi-Document Understanding Home & personal Medium Summarization Financial Documents Medium Factoid or Complex Question Answering Books Hard Summarization Books Hard Factoid or Complex Question Answering Legal Documents Hard Multi-Document Understanding Work & business Hard Factoid or Complex Question Answering Books Given this podcast transcript, please summarize Daniel Dennetts opinion on Goulds idea of Non-Overlapping Magisteria, and explain how Dennett believes science ought to fit into society. Give specific quote to support your answer to each question. {attached text} What are some of the most common pieces of advice for maintaining vegetable garden. Is there any information that is controversial? {attached text} Based on the financial documents of JB-HIFI in 2021, summarise the Notes to the financial statements and note the financial statement where each note relates to. {attached text} What caused Jane to be locked up in the red-room? How did the red-room affect her adult life in both positive and negative ways? Did she experience any flashbacks regarding the red-room in her adult life? {attached text} Summarize every four chapters of this book in three paragraphs. The first paragraph should summarize the first and second chapters, the second paragraph should summarize the third and fourth chapters, and the third paragraph should focus on the fifth chapter. Create title for each section in 1-3 words that encapsulates the events of these chapters. Repeat this structure for each subsequent set of five chapters! {attached text} Given the segment of the NBA CBA which outlines financial rules, how might strategy differ in terms of team/roster construction with salaries for contending team differ from lottery team? Justify you response with quotes from the text, and give examples with fictional roster with salaries to demonstrate the salary repercussions/strategy being proposed. {attached text} Given the latest annual financial reports from AMD (Advanced Microdevices) and NVIDIA, evaluate each companys strategy and preparedness for addressing AI workloads over the next 5 years include their comparative headwinds and tailwinds. Assess which company is currently best positioned to gain the most market share in AI and why. {attached text} read this book, but forgot to take notes. So understand the philosophy, but dont remember the exact steps that should follow. Extract all the exact information that the successful man shared with his two new friends, so can print these routines, rules, and truths and have them in my sight. am talking about the 5-3-1 facts or the 20-20-20 routine, etc. {attached text} Table 21 Examples of the prompt set for long context capability. 34 Difficulty L1 Category L2 Category Prompt Easy Dialogue Identity / Personas Easy Factual Questions Cultural & social topics Medium Writing & content creation Creative writing: Fiction Medium Procedural Questions Technology & devices Hard Social interaction and communication Discuss shared interests Hard Procedural Questions Finance & budgeting Hard Recommendations / Brainstorming Product & service recommendations Hard Social interaction and communication Humor and jokes Digamos que ahorita eres Tesla me tienes que responder como si fueras el. que me dirias si te pregunto acerca de tus mejores inventos? Escribe una síntesis acerca de las características principales del culto mariano en América Latina, con énfasis especial en los países más grandes religiosos: México Brasil. Escribe una historia de animales similar la película de Madagascar pero ambientada en la cueva Hang Son Doong de Vietnam. Los tres personajes principales deben ser un murciélago, un pájaro un escorpión. Las estalactitas estalagmitas deben poder hablar. El villano de la historia debe ser un gusano bioluminiscente. Estoy teniendo problemas con mi laptop. Es una Dell Latitude 3440 con Windows 10. Cada tanto tiempo, entre 5 minutos una hora, se desconecta el Wi-Fi debo volverlo conectar manualmente. Dame 5 posibles soluciones, en viñetas. Las soluciones deben ser totalmente detalladas, como para alguien que no tiene conocimiento de computación. Me encanta leer, especialmente libros de ficción. Sin embargo, este año quiero empezar leer otros tipos de libros. Me puedes compartir una lista de tus 15 libros favoritos que no sean de ficción? enfócate en autores latinoamericanos asiáticos estoy abierta cualquier tipo de escritura (poemas, biografías, etc.), dime el idioma original la nacionalidad de cada autor si tiene traducción al español en caso de que sea necesario Necesito ahorrar más en los gastos misceláneos de entretenimiento digital. Establece un presupuesto mensual para cocinar la mayoría de las comidas en casa. Realiza las recomendaciones teniendo en mente una persona que vive en el suroeste de Florida, de manera que los mercados donde conseguir comida saludable barata estén basados en la disponibilidad de dicha región. El gasto máximo mensual será de $400, debes especificar como distribuirlo. En cuanto los gastos de entretenimiento digital, he identificado que la mayoría de ellos están vinculados suscripciones plataformas de streaming. Necesito quedarme solo con dos de estas plataformas. Mis intereses principales son los deportes el cine clásico. Recomiéndame las que mejor puedan satisfacer esta demanda dentro de un presupuesto limitado. Lo máximo que puedo gastar en entretenimiento digital son $21. Soy una aficionada del trekking de acampar en la naturaleza. Lamentablemente, veces me resulta difícil salir caminar, sobre todo lugares fríos porque no cuento con suficiente equipo de calidad. La ropa de trekking especializada se ha vuelto muy cara, así como las botas de trekking, las bolsas de dormir, tiendas de campaña, etc etc. Por favor, ayúdame con recomendaciones para un equipo básico, que sea de buena calidad, pero con precios accesibles. Necesito 3 atuendos completos de trekking, para temperaturas de 0 15 grados centígrados. Para la zapatillas, estoy dispuesta gastar más en un par que me dure al menos 3 años. Qué sean impermeables, cómodas calientitas. Finalmente, necesito invertir en una bolsa de dormir. Pero una que sea ligera! dime un chiste con más de 300 palabras que incluya las palabras gato, música, trueno árbol , que sea del punto de perspectiva del gato incluya una frase final chusca con moraleja. Table 22 Examples of the prompt set for Spanish capability. Difficulty L1 Category L2 Category Prompt Easy Programming Assistant Code Understanding Easy Coding Q&A (Text to Text) Software Architecture Medium Programming Assistant Algorithmic reasoning have inherited this python function from previous Data Scientist in my team who has left. need to understand what the function is doing - there are no comments and dont have time to debug this function. {attached code} Im planning to implement different shape classes in Java. They have similar fields, but the parameters are different. For example, circle has radius field, but square has length field. want to apply the factory design patterns to my program; is it good practice? If not, which design patterns suit my requirements more? Design the most efficient algorithm to find all unique pairs of integers in given list that sum to target value. Ensure that each pair is unique (no pair should repeat even if the integers appear multiple times in the list). Describe the algorithms logic and implementation details. Explain why you chose to implement it in the way you did in order to achieve the best time complexity and memory storage possible. Medium Mathematical Calculation Ordinary and partial differential equations Suppose that dy which uses Eulers method to estimate y(1). dt = + 1, and y(0) = 1. Write me Python code Hard Programming Assistant Problem decomposition Hard Coding Q&A (Text to Text) Programming concepts & guidance"
        },
        {
            "title": "Mathematical\nCalculation",
            "content": "Algebra & equations Hard Code Explanation Code walkthroughs Im working on simple command line game, and want to break it up into some reusable functions, but Im not sure how to refactor it. Can you suggest any functions that would make my code less repetitive? Also, help me write 5 more useful functions in the code. {attached code} Can you explain tail recursion and why its key for boosting efficiency in recursive functions? Its known for using less memory than standard recursion. Could you demonstrate this with Python example, showing both tail-recursive function and regular recursive function? Also, since Python doesnt automatically optimize tail recursion, how does this limitation impact its use in larger projects? BTW, how do other programming languages handle tail recursion differently, and what advantages they offer? There are 8 people in room, and two boxes. One box has 8 hats, 2 pink, 1 red, 3 blue, and 2 black. The other box has 12 pairs of gloves, 5 green, 4 black, 2 orange, and 1 yellow. The 8 people in the room are invited to each grab and put on one hat and one pair of gloves. Then, each person is will shake hands with 5 different people at random. Create an algebraic equation to find the probability of someone wearing pink hat and orange gloves shaking hands with someone wearing black hat and green gloves at least once? Use python script to solve the equation to find the probability, given in percentage. Can you please explain what the following code does and how the output is represented? Do we really need to represent the output this way? Also, provide me with way to implement the following algorithm based on other libraries. {attached code} Table 23 Examples of the prompt set for coding & reasoning capability. 36 Difficulty L1 Category L2 Category Image Prompt Easy Chart Understanding Basic Chart Understanding (Localization) From this chart, name at least one manufacturer more popular than Toyota Easy Visual Math and Science Figure understanding What is this illusion called? Medium Chart Understanding Basic Chart Understanding (Localization) What is the max trading volume in the bar chart? Medium Visual Math and Science Formula understanding What is the chemical formula that comes after the word Methanol? Provide the text for the formula, and what the formula is representing/used for. Hard Diagram Understanding Flowchart Understanding Based on the diagram, how many steps need to do before Xsemble Design? Hard Text-rich Understanding Others So what is the best strategy for the interview question in the image? Explain it step by step. Hard Diagram Understanding Graph Understanding How many nodes do have to remove from this graph so that it is possible to visit every vertex without repeating an edge, starting from the one at the bottom-left. Table 24 Examples of the prompt set for image recognition & reasoning capability. 37 Difficulty L1 Category L2 Category Prompt Easy Repository-Level Code Generation Code completion Easy Log Analysis Parsing logs into structured templates Medium API Docs Understanding Code generation with API Medium Repository-Level Code Generation Code modification Hard API Docs Understanding Q&A on API Hard Repository-Level Code Debugging Debugging & troubleshooting Hard Repository-Level Code Generation Code generation (Text to Code) In the RingIntercom class, there is functionality to unlock door and process notifications. Extend this class by adding method that logs all unlock actions with timestamp and the user ID who performed the unlock. This method should save the log to local file named unlock_log.txt. {attached code} Can you parse logs from this log file provided into tree-like structure? {attached log} Referencing the Notify.lk API documentation, can you develop simple command-line application in Java that checks the balance of your Notify.lk account and prints it out? Assume you have the necessary API credentials. {attached code} Optimize the applications performance by implementing lazy loading for the React components associated with routes in App.js. Use React.lazy and Suspense from React to lazily load the components for /profile/:id, /editor, and /player/:id. Also, provide fallback loading component that displays loading spinner while the components are being loaded. Provide the code modifications in App.js to implement lazy loading with fallback. {attached code} Im working on an automatic grader using Python for my intro to OOP class. Currently still need to download all my students .py submissions manually and then upload/enter each grade manually. Do you think it would be possible to use the Canvas LMS API to automate these processes? How do download the submissions for given student or set/update the grade on an assignment for student using python? here is some documentation for the canvas API {attached code} It seems there is bug in the code that ensures that the start point for the ray casting algorithm is outside the polygon. This causes the 3D items not being able to be moved after update the layout (floorplan) of the room. Locate the function that needs to be fixed and suggest fix. {attached code} Given repository, create system to display the top eight high scores for the game session. When the game starts up, the high scores should be [80, 70, 60, 50, 40, 30, 20, 10] all with name label Anonymous... when the user dies, if their final score beats any on the high scores list, they should be able enter their name and add their highscore to the list in the appropriate place (i.e. all lower scores should be moved down the list and the lowest removed altogether). When the game is not in AppState Game, The state should alternate between the intro screen and highscores screen that displays the high scores list. The high scores screen should consist of text containing the list of high scores displayed over the same background used for the Game and IntroScreen states, text font should be consistent with the other text in the game and should be white unless it is new highscore, in which case it that entry should be green. {attached code} Table 25 Examples of the prompt set for long context & coding capability. Difficulty L1 Category L2 Category Prompt Easy Code Execution with File Uploads Data Visualization Easy Code debugging with execution Testing Medium Code Execution Code to Code (Same language) Medium Code Execution Code generation and execution (Text to Code) Hard Code Execution with File Uploads Data Analysis Hard Programming Assistant with Execution Code Understanding Hard Code Execution Create example usages of this function Hard Code Execution Code generation and execution (Text to Code) Group this sales data by month using pandas and then create bar chart of the sales per month. Run the code and save the results. {attached file} Can you generate set of unit tests for each method? Please have 10 different and valid test cases for each method in the given code. Make sure to cover edge cases. Then run the code and show me the outputs. {attached code} wrote the following function which finds all of the composite numbers from 2 to n. Its bit slow though because its brute force method. Can you rewrite it in way that reduces the time complexity? Use few examples and time it to show the real differences in running time by executing the code. {attached code} Create Python decorator named measure_time that logs the duration function takes to execute. Apply this decorator to sorting function that sorts list of random numbers. Test the sorting function with lists of increasing sizes (e.g., 100, 1,000, 10,000, and 100,000 elements) and use the decorator to print out the sorting times for each list size. Remember to run your code and give me the results. Make sure your code is clean and easy to follow! Can you please write python file utilizing the library Streamlit for frontend visualization, and show case the amount of wine sales for each customer and display in chart like diagram? Please sum Amount, Cash and Check for each customer and set the value as Gross Purchase. attached Winery_sales excel spreadsheet file in my prompt for you to follow. Please run your program and show me the visualization. Save the results. {attached file} have code that simulates game of Blackjack, but Im not sure if its accurate or efficient. The code takes into account the players hand, the dealers upcard, and the players current bet. It returns the recommended action (hit, stand, double down, or split) based on complex set of rules. However, Im not sure if the rules are correctly implemented, and Id like you to review the code and suggest improvements. Help me review the code, identify any issues, and suggest improvements. Then provide some test cases and run the improved code to demonstrate its accuracy. {attached code} My friend has provided me with game that is implemented as Python function. am not able to understand the code. Can you please explain to me how the game works by providing some example usages of this function. want you to execute the program by providing some valid inputs. Please show me the outputs obtained and explain to me the reason for the outputs. {attached code} Cryptarithms are like puzzles where you replace digits in math equation with letters. Each letter stands for unique digit. For example, in SEND + MORE = MONEY, each letter is unique digit. Please help me write Python function that can crack any cryptarithm with addition, subtraction, multiplication, and division. The function should spit out dictionary where each letter maps to the digit it represents. Please also provide some complex test cases and execute the programs you wrote to get the answers to these cases. Table 26 Examples of the prompt set for tool use & coding capability. Difficulty L1 Category L2 Category Prompt Easy Easy Scientific Reasoning Mathematical Calculation Chemistry Complex Analysis Medium Scientific Reasoning Engineering Medium Scientific Reasoning Health and Medicine Hard Mathematical Calculation Ordinary and partial differential equations Hard"
        },
        {
            "title": "Mathematical\nCalculation",
            "content": "Plotting Hard Scientific Reasoning Food Science Hard Mathematical Calculation Discrete math & logic Given the reactants sodium chloride and silver nitrate, what are the products that will be formed? Give me the balanced chemical reaction. dont care if the answer is generated or you get it with any tool, but please be as detailed as possible. Show me the local minimum of function (x) = 1 3 x3 5 2 x2 + 4x spacecraft is moving in gravity-free space along straight path when its pilot decides to accelerate forward. He turns on the thrusters, and burned fuel is ejected at constant rate of 2.0 102 kg/s at speed (relative to the rocket) of 2.5 102 m/s. The initial mass of the spacecraft and its unburned fuel is 2.0 104 kg and the thrusters are on for 30 s. What is the thrust on the spacecraft and what is the acceleration? Calculate the total calories burned by person weighing 70 kg who walks at speed of 5 km/h for 1 hour on flat surface. Assume walking metabolic equivalent (MET) value of 3.8. Use the formula for caloric expenditure: Calories burned = MET weight in kg duration in hours. Additionally, factor in the effect of air resistance, assuming wind speed of 10 km/h against the direction of walking, and adjust the MET value accordingly using the formula: Adjusted MET = MET (cid:18) 1 + 0.1 wind speed in km/h walking speed (cid:19) . Provide the detailed calculation steps and final result. Solve the first-order ordinary differential equation (ODE) using the Euler method: dy dx = 3x + 2y, with the initial condition y(0) = 1. Calculate the solution from = 0 to = 1 with step size of = 0.1. Implement the Euler method to solve this ODE and print the value of at each step. Provide Python function to represent the ODE and another to perform the Euler method. Give me the accurate results. Plot and shade in the region of the 2D coordinate plane defined by the inequality + y2 < 10. Show me the image, and save it png format. Could you suggest one-day meal plan from McDonalds with meals that are roughly 400, 800, and 1200 calories each? Please include drink with each meal and keep the calorie count within 20 calories of these targets. Given the logical statement (r s) (t u) is true, and the statement (s u) is false, determine the truth value of the statement [(r t) (s u)] [(r t) (s u) v]. Any tools are allowed. Explain your answer step by step. Table 27 Examples of the prompt set for tool use & reasoning capability. 40 B.2 Reference Examples To offer more intuitive understanding of the reference examples in our benchmark, we randomly select an instance from both English and Reasoning capabilities and present their reference examples in Tables 28 33. Prompt L1 Category: Recommendations / Brainstorming L2 Category: Entertainment suggestions Difficulty: Hard Prompt: Im looking for suggestions of 1970s movies. The genre should be horror, and it should take place in the woods or be summer camp based. Do not include movies that also incorporate humor, the movies chosen need to be strictly horror. Include the titles and the directors. Also, include the names of any actors who have gone on to be successful in Hollywood after they were in these movies. Here are some 1970s horror movies that take place in the woods or at summer camp, without humor: Model Response 1 1. The Evil Dead (1981) - Director: Sam Raimi - This film takes place in remote cabin in the woods and follows group of friends who are terrorized by demons. The movie has since become cult classic. - Successful actors: Bruce Campbell, Ellen Sandweiss, and Richard DeManincor 2. The Burning (1981) - Director: Tony Maylam - This summer camp slasher film tells the story of group of campers who are stalked and killed by former camper who was horribly burned in prank gone wrong. - Successful actors: Brian Matthews, Lou David, and Holly Hunter 3. The Town That Dreaded Sundown (1976) - Director: Charles B. Pierce - This horror film is based on true story and takes place in small town in Arkansas where serial killer is terrorizing locals and tourists alike. - Successful actors: Dawn Wells, Jimmy Lydon, and Andrew Prine 4. Just Before Dawn (1981) - Director: Jeff Lieberman - This backwoods horror film follows group of friends who are on hiking trip in the Pacific Northwest when they are stalked and killed by mysterious figure. - Successful actors: George Kennedy, Chris Lemmon, and Mike Randolph 5. Madman (1977) - Director: Joe Giannandrea - This summer camp slasher film tells the story of group of campers who are stalked and killed by legendary killer who is said to haunt the woods. - Successful actors: Paul Ehlers, Deborah Luce, and Thomas Carr 6. The Prey (1977) - Director: Edwin Brown - This wilderness survival horror film follows group of friends who are stranded in the woods and hunted by group of mountain men. - Successful actors: Keith Carradine, Tulsa McReynolds, and Lew Saunders These movies are all considered horror classics and are still popular among fans of the genre today. Human Annotations Rating 1: 2/5 Explanation 1: This response makes several errors. The movie Madmen was directed by Joe Giannone, not Joe Giannandrea. It was also released in 1981, not 1977. The movie The Prey was released in 1983, not 1977. While the other movie dates are correct, most of them are not from the 1970s, which was specific component of the users request. The only movie in the suggested list that was actually released in the 1970s was The Town That Dreaded Sundown, which does not have the setting the user requested - summer camp or woods. Rating 2: 2/5 Explanation 2: Half of its movie recommendations are from the 1980s. Since only 1970s movies were requested this leads to lot of wasted recommendations. Table 28 First reference example of English capability. Each instance includes prompt, 3 model responses, and 2 human scores with explanations. This table presents the first model response alongside its human annotations. Together, the model response and annotations serve as the reference example for the subsequent evaluation. 41 Prompt L1 Category: Recommendations / Brainstorming L2 Category: Entertainment suggestions Difficulty: Hard Prompt: Im looking for suggestions of 1970s movies. The genre should be horror, and it should take place in the woods or be summer camp based. Do not include movies that also incorporate humor, the movies chosen need to be strictly horror. Include the titles and the directors. Also, include the names of any actors who have gone on to be successful in Hollywood after they were in these movies. great request! Here are some classic horror movies from the 1970s that fit your criteria: Model Response 1. The Last House on the Left (1972) - Directed by Wes Craven Starring: Sandra Peabody, Lucy Grantham, David Hess group of friends are stalked and murdered by pair of escaped convicts in the woods. 2. The Texas Chain Saw Massacre (1974) - Directed by Tobe Hooper Starring: Gunnar Hansen, Teri McMinn, Edwin Neal group of friends encounter family of cannibals in rural Texas. 3. The Hills Have Eyes (1977) - Directed by Wes Craven Starring: Susan Lanier, Robert Houston, Martin Speer familys trip to the desert turns into nightmare when they encounter family of mutants. 4. Friday the 13th (1980, but filmed in 1979) - Directed by Sean S. Cunningham Starring: Betsy Palmer, Adrienne King, Kevin Bacon group of camp counselors are stalked and murdered by Jason Voorhees at summer camp. 5. The Town That Dreaded Sundown (1976) - Directed by Charles B. Pierce Starring: Ben Johnson, Andrew Prine, Dawn Wells small town is terrorized by serial killer in this loosely based-on-true-events horror film. 6. Madman (1978) - Directed by Joe Giannandrea Starring: Harriet Bass, Jimmy Steele, Seth Jones group of summer camp counselors are stalked and murdered by legendary killer known as Madman Marz. Notable actors who went on to achieve success in Hollywood after appearing in these films include: - Kevin Bacon (Friday the 13th) - has had successful career in film and TV, with roles in Footloose, Apollo 13, and American Crime Story. - Gunnar Hansen (The Texas Chain Saw Massacre) - went on to appear in numerous horror films and write several books, including memoir about his time on the set of The Texas Chain Saw Massacre. - Wes Craven (The Last House on the Left, The Hills Have Eyes) - went on to become legendary horror director, creating the Nightmare on Elm Street and Scream franchises. Note: While some of these films may have been re-released or remade with added comedic elements, the original versions are strictly horror films. Human Annotations Rating 1: 3/5 Explanation 1: The list of movies is related to what the prompt is searching for but lot of the details are off like years the movie was made and the names of directors. The prompt asks for 1970s genre horror, but 1980 horror movie based In summer camp like Friday the 13th would still be useful result. The inaccuracies are bit distracting and provide misinformation. Rating 2: 3/5 Explanation 2: This response does great job of highlighting the horror actors who went on to greater fame. Most of the movies in the list were released in the 1970s, as requested by the user, and the list notes that one movie released in 1980 was filmed in the 1970s. However, the response states that Madman was directed by Joe Giannandrea and released in 1978, but both IMDB.com and Wikipedia note that this directors name is Joe Giannone and the movie was released in 1981. Additionally, The Town That Dreaded Sundown was not set in summer camp, or in the woods, as requested in the prompt. Table 29 Second reference example of English capability. Each instance includes prompt, 3 model responses, and 2 human scores with explanations. This table presents the second model response alongside its human annotations. Together, the model response and annotations serve as the reference example for the subsequent evaluation. 42 Prompt L1 Category: Recommendations / Brainstorming L2 Category: Entertainment suggestions Difficulty: Hard Prompt: Im looking for suggestions of 1970s movies. The genre should be horror, and it should take place in the woods or be summer camp based. Do not include movies that also incorporate humor, the movies chosen need to be strictly horror. Include the titles and the directors. Also, include the names of any actors who have gone on to be successful in Hollywood after they were in these movies. Model Response 3 Sure, here are some horror movies from the 1970s that take place in the woods or involve summer camps, and they stick strictly to the horror genre: 1. Title: The Texas Chain Saw Massacre (1974) - Director: Tobe Hooper - Notable Actors: While the cast was relatively unknown at the time and didnt go on to become major Hollywood stars, the film itself has become cornerstone of the horror genre. 2. Title: Deliverance (1972) - Director: John Boorman - Notable Actors: Burt Reynolds, Jon Voight - Jon Voight went on to have successful career in Hollywood with films like Coming Home (for which he won an Academy Award) and Mission: Impossible. 3. Title: The Last House on the Left (1972) - Director: Wes Craven - Notable Actors: While no actors from this film went on to major Hollywood fame, the movie is notable for being Wes Cravens directorial debut, who later became legendary horror director. 4. Title: The Hills Have Eyes (1977) - Director: Wes Craven - Notable Actors: Dee Wallace - Dee Wallace went on to star in films like E.T. the Extra-Terrestrial and Cujo. While these films may not have large ensembles of widely recognized stars, they are significant in the horror genre and feature actors who have been successful in Hollywood. Human Annotations Rating 1: 4/5 Explanation 1: These are all 1970s horror movies that wouldnt be considered comedy. The movie year release dates, directors and actor information is accurate. The response could be improved with summary of the plot. Rating 2: 4/5 Explanation 2: This response provides specific details on the fame acquired by certain actors in these horror movies. The response only notes four movies, which seems small for this genre and decade, but they are all from the 1970s and 3 of the 4 are set in the woods or summer camp. The Hills have Eyes is set in the desert, but it is wilderness setting. Table 30 Third reference example of English capability. Each instance includes prompt, 3 model responses, and 2 human scores with explanations. This table presents the third model response alongside its human annotations. Together, the model response and annotations serve as the reference example for the subsequent evaluation. 43 Prompt L1 Category: Mathematical Calculation L2 Category: Discrete math & logic Difficulty: Medium Prompt: Jane won the lottery and decided to spend some of the money. She spent $1.50 on the first day. She spent $3 on the second day. She spent $4.50 on the third day. She kept spending her winnings in the same pattern and then on the last day, she spent her remaining $300. How much did she win in the lottery? Model Response 1 Jane spent $1.50 on the first day, $3 on the second day, and $4.50 on the third day. So that is 1.50 + 3 + 4.50 = 9 dollars in the first three days. She then spent $300 on the last day. So she spent 9 + 300 = 309 dollars in total. The answer is 309. Human Annotations Rating 1: 1/5 Explanation 1: The response does not understand that the user is asking for pattern to be identified, and fails to understand that there were probably several days in between the third day and the final day. The response does not come up with suitable formula and fails to correctly calculate the result; it instead just adds up the amounts specified in the prompt. Rating 2: 2/5 Explanation 2: The response was incorrect because it failed to identify the pattern in the given logic. The prompt indicates that Jane spent money according to pattern observed in the first three days, but the response only calculated the total money spent on the first three days and the last day, ignoring the expenditures on the middle days. Table 31 First reference example of reasoning capability. Each instance includes prompt, 3 model responses, and 2 human scores with explanations. This table presents the first model response alongside its human annotations. Together, the model response and annotations serve as the reference example for the subsequent evaluation. Prompt L1 Category: Mathematical Calculation L2 Category: Discrete math & logic Difficulty: Medium Prompt: Jane won the lottery and decided to spend some of the money. She spent $1.50 on the first day. She spent $3 on the second day. She spent $4.50 on the third day. She kept spending her winnings in the same pattern and then on the last day, she spent her remaining $300. How much did she win in the lottery? Model Response 2 On the first day, Jane spent 1.50 dollars. On the second day, she spent 3 dollars. On the third day, she spent 4.50 dollars. The pattern continues and the money she spends doubles each day. So on the 4th day, she spends 9 dollars, on the 5th day she spends 18 dollars, and so on. If we add up the money spent on each day, we will get sequence of numbers that looks like this: 1.50, 3, 4.50, 9, 18, 36, 72, 144, 288, ... Adding these numbers up, we get 576. Now we know that on the last day, she spent the rest of her money, which was 300 dollars. So the total money she spent is 576 + 300 = 876 dollars. The answer is 876. Human Annotations Rating 1: 2/5 Explanation 1: The identified pattern was incorrect. The correct pattern was to add 1.5 on each subsequent day of spending, as observed in the first three days. The pattern in the response incorrectly suggested doubling the spending each day, which only applies correctly to the second day, not the first and third days. Rating 2: 1/5 Explanation 2: The response knows it needs to find pattern, but it is unable to find the correct one. Because the first to values in the series ($1.50 and $3) represent doubling, it decides that this is the pattern, even though the next number in the series is $4.50. The fact that the pattern it identifies doesnt include $300 should indicate that the formula may not be correct, but instead the response just adds the $300 on at the end of its calculation. Its calculation fails because it is based on this faulty formula. Table 32 Second reference example of reasoning capability. Each instance includes prompt, 3 model responses, and 2 human scores with explanations. This table presents the second model response alongside its human annotations. Together, the model response and annotations serve as the reference example for the subsequent evaluation. Prompt L1 Category: Mathematical Calculation L2 Category: Discrete math & logic Difficulty: Medium Prompt: Jane won the lottery and decided to spend some of the money. She spent $1.50 on the first day. She spent $3 on the second day. She spent $4.50 on the third day. She kept spending her winnings in the same pattern and then on the last day, she spent her remaining $300. How much did she win in the lottery? Model Response 3 Lets analyze the situation: Day 1: Jane spent $1.50 Day 2: Jane spent $3 Day 3: Jane spent $4.50 We can see that the amount she spent each day is increasing by $1.50. To find the total amount she spent before the last day, we need to find the number of days she spent money. Lets assume she spent money for days before the last day. Then, the total amount she spent before the last day is: $1.50 + $3 + $4.50 + ... + $(1.5n) This is an arithmetic series with first term $1.50, common difference $1.50, and terms. The sum of the series is: (1.5n)(n+1)/2 We know that the last term of the series is $1.5n. Since she spent $300 on the last day, we can set up the equation: (1.5n)(n+1)/2 + $300 = Total amount won We also know that the last term of the series is $1.5n = $300 - $1.50 (since she spent $1.50 on the first day). Solving for n, we get: $1.5n = $298.50 = 199 Now we can find the total amount she won: (1.5(199))(199+1)/2 + $300 = Total amount won (298.5)(200)/2 + $300 = Total amount won $29850 + $300 = Total amount won $30150 = Total amount won The final answer is $30150. Human Annotations Rating 1: 3/5 Explanation 1: The response identifies and describes the correct pattern as well as providing the correct answer ($30150). But the explanation of the formula it provides is very confusing, particularly with the way that it separates the $300 on the last day. It is not certain that the user would be able to make use of the formula provided, but at least they would know what the pattern is, after reading this response. Rating 2: 5/5 Explanation 2: The response was correct because it identified the pattern of adding $1.5 on each subsequent day of spending. It correctly calculated the total amount spent before the last day using the appropriate formula and provided the accurate result of spending from the first day to the last day. Table 33 Third reference example of reasoning capability. Each instance includes prompt, 3 model responses, and 2 human scores with explanations. This table presents the third model response alongside its human annotations. Together, the model response and annotations serve as the reference example for the subsequent evaluation. 45 B.3 Guidelines for Difficulty Levels To illustrate the difficulty of prompts in our evaluations, below is summary of the definitions, accompanied by examples, for easy, medium, and hard levels for the English (multilingual) and Image Recognition capabilities. Note that the following examples are for illustrations only, and they are not from our benchmark. B.3.1 English and Multilingual"
        },
        {
            "title": "Easy",
            "content": "Definition Prompt is single ask/requirement/constraint for the model presented as single statement OR prompt is single statement without ask/requirement/constraints AND would not require subject matter expertise to understand."
        },
        {
            "title": "Examples",
            "content": "Illustrate and explain the proper use of semi-colon. How do uninvite my brother to my wedding? Ive been having trouble sticking to my healthy diet lately. Give me some motivational words or tips to help me make better food choices and achieve my health goals."
        },
        {
            "title": "Medium",
            "content": "Definition Prompt includes 24 asks/requirements/constraints for the model AND would not require subject matter expertise to produce response."
        },
        {
            "title": "Examples",
            "content": "My neighbors blast loud music all night, and cant sleep. Ive tried talking to them directly, as well as calling 311 but nothing has changed. What else do you think can try? How do ask my boss for raise? think Im underpaid but my boss never has time for me. Pretend youre Bugs Bunny. Im Elmer Fudd. How would you greet me? Write me funny haiku about dogs."
        },
        {
            "title": "Hard",
            "content": "Definition Prompt contains 5 or more asks/requirements/constraints for the model OR requires subject matter expertise above and beyond common knowledge in order to respond."
        },
        {
            "title": "Examples",
            "content": "Write poem to say sorry to my dog because didnt spend enough time with it. The poem should have 26 lines where each line begins with Z, Y, X, ..., A, respectively, and always ends with h. The poem cannot contain any animal words. Sort the following words alphabetically, and in the result remove the first and the fourth words while capitalizing the rest: sioux fortescue purloin percept helmsman friend friends. Append new lower-case word that is an animal living in Antarctica. Output the result with numbered bullets. Handling long-sequence inputs presents significant challenge to the KV-cache of Transformers. Can we address this challenge better by training Transformers with more GPUs? Im hosting dinner party next week. have kosher friend coming, but also vegan friend. Also, am allergic to nuts. My husband likes spicy food. There might be few picky eaters who are coming too. They may come with kids who attend preschools. What do you think should make for dinner? And what about drinks? B.3.2 Image Recognition"
        },
        {
            "title": "Easy",
            "content": "Definition Requires to stick to the image focus (instead of the background and details) AND requires NO external knowledge to answer the question (e.g. historical details, specific skills) AND requires NO fine-grained object recognition (e.g. plant species, aircraft models) AND requires NO complicated language format constraint (e.g. multi-level bullets, specific order of listing, creative writing)."
        },
        {
            "title": "Examples",
            "content": "How many pieces of chess are there? Please answer with one English word. What is the title of the presentation slides? What color is the watch belt?"
        },
        {
            "title": "Medium",
            "content": "Definition Neither easy nor hard."
        },
        {
            "title": "Examples",
            "content": "Who are the cartoon characters in this image? What are they holding? (what they hold is not image focus) How do fix this at home? (requires some knowledge) What is the species of this cute cat? (fine-grained recognition)"
        },
        {
            "title": "Hard",
            "content": "Definition Requires identification of five or more entities, each possessing distinct characteristics or components, amid conditions of visual complexity. This includes scenarios where unrelated visual elements may interfere (visual distraction), or where relevant parts of the entities are partially obscured from view (visual occlusion), thus complicating the recognition process OR requires complicated format for language generation (e.g. multi-level bullets, specific order of listing, creative writing) OR requires visual-related professional knowledge."
        },
        {
            "title": "Examples",
            "content": "Can you count the balloons of each color? (The image includes more than five balloons along with distractions like children, hats, and other objects.) It is said that there is human face in this image. Can you explain how that can be? (comprehensive image understanding related to illusion) When was this photo taken? Can you tell me more about the related event? (visual recognition related to history knowledge) The distinction between Medium and Hard difficulty hinges on whether the prompt necessitates visual-related professional knowledge. Its important to note that prompt demanding professional knowledge does not automatically qualify as Hard unless that knowledge is specifically related to visual interpretation. For instance, if prompt can be deconstructed into part that solely concerns visual identification and another part that solely concerns factual knowledge, then the knowledge required is not considered visual-related. 47 B.4 Prompts for Evaluation We provide the complete version of the system and evaluation prompts we adopt for LLM-as-a-Judge in Tables 34 and 35, respectively. You are an expert AI evaluator tasked with assessing model responses. Rate the response using 1-5 Likert scale according to the following rubrics: ### Rubrics: - 5/5 - Amazing: The response is flawless and could hardly be improved. - 4/5 - Pretty Good: The response is quite good, but has room for minor improvements. - 3/5 - Okay: They are middle-of-the-road responses that could be improved in several ways. - 2/5 - Pretty Bad: The response has major problems in helpfulness, truthfulness, or safety. - 1/5 - Horrible: They are terrible responses and you would caution others against using models that generate responses like this. Note: User prompts or model responses may include attachments. To ensure thorough evaluation, you may need to write and execute code. Table 34 System prompt for LLM-as-a-Judge in our CrossEval benchmark. 48 [Attached]: {attached text} [User Prompt]: {user prompt}: To calibrate your evaluation, consider these reference examples: [Reference Example 1]: Model Response: {response 1 } Rating 1: {rating 1 }/5 Explanation 1: {explanation 1 } Rating 2: {rating 2 }/5 Explanation 2: {explanation 2 } [Reference Example 2]: Model Response: {response 2 } Rating 1: {rating 1 }/5 Explanation 1: {explanation 1 } Rating 2: {rating 2 }/5 Explanation 2: {explanation 2 } [Reference Example 3]: Model Response: {response 3 } Rating 1: {rating 1 }/5 Explanation 1: {explanation 1 } Rating 2: {rating 2 }/5 Explanation 2: {explanation 2 } Use these examples as benchmarks for your evaluation scale and scoring consistency. Here is the model response for evaluation: [Model Response to be Evaluated]: {model response} Please provide your evaluation in the following format: #### User Prompt Analysis - Identify key requirements and objectives from the user prompt. #### Reference Examples Insights - Summarize scoring patterns and typical point deductions. - Include how many points should be deducted for each issue. #### Model Response Evaluation - Pros: List strengths and positive aspects. - Cons: Identify weaknesses, specifying point deductions for each. #### Holistic Assessment - Consider if major strengths outweigh minor issues. - Combine similar deductions to avoid double penalization. - Balance deductions and positive aspects, and then explain your scoring decision. #### Evaluation Score Score: [X]/ Ensure your evaluation is thorough, fair, and aligned with the reference examples. Your expertise is crucial in providing an accurate and insightful assessment. Table 35 Evaluation prompt for LLM-as-a-Judge in our CrossEval benchmark. 49 B.5 Case Study for LLM-as-a-Judge on CrossEval We randomly select one instance from the CrossEval benchmark to demonstrate the format and reliability of LLM-as-a-Judge in Tables 36 and 37, using GPT-4o and Claude 3.5 Sonnet as evaluators, respectively. The distance between cities and is 240 miles. train leaves from city for city and another train leaves from city to City simultaneously. It is known that one train is traveling 15 miles per hour faster than the other. After 3 hours, they find that they are 90 miles apart. Can we calculate the speeds of both trains? Prompt The detailed calculation process is omitted for brevity. Model Response from o1-preview Answer: Yes; their speeds are 17.5 mph and 32.5 mphwe can calculate both train speeds from the given data. GPT-as-a-Judge #### User Prompt Analysis - The user prompt provides scenario involving two trains traveling towards each other from different cities, with one train traveling faster than the other. The key requirements are to determine the speeds of both trains given the distance between the cities, the time traveled, and the distance apart after 3 hours. #### Reference Examples Insights - Reference Example 1: Deducted points for incorrect calculations and assumptions. - Reference Example 2: Deducted points for incorrect assumptions about city and not addressing potential confusion. - Reference Example 3: Deducted points for not solving the problem but correctly identifying the need for more information. Typical point deductions: - Incorrect calculations: 2-3 points. - Incorrect assumptions or failure to address potential confusion: 1-2 points. - Failure to solve the problem when solvable: 2-3 points. #### Model Response Evaluation Pros: 1. Logical Setup: The response correctly sets up the problem by assuming the trains are moving towards each other along the line between cities and Y. 2. Correct Calculations: The response accurately calculates the combined distance covered and the combined speed. 3. Clear Explanation: The steps are clearly explained, making it easy to follow the logic and calculations. Cons: 1. Assumption Clarification: The response assumes that city is on the same line as cities and without explicitly addressing the potential confusion in the prompt. This could lead to minor deduction for not clarifying the assumption (1 point). #### Holistic Assessment - The response is strong in its logical setup, clear explanation, and correct calculations. The only minor issue is the lack of explicit clarification about the assumption regarding city Z. However, this does not significantly detract from the overall quality of the response. #### Evaluation Score Score: 4/5 The response is quite good, with only minor area for improvement in clarifying assumptions. The major strengths outweigh this minor issue, leading to high score. Table 36 Case study of GPT-as-a-Judge. Prompt Capability: Reasoning L1 Category: Mathematical Reasoning L2 Category: Math word problem solving Difficulty: Hard Prompt: The distance between cities and is 240 miles. train leaves from city for city and another train leaves from city to City simultaneously. It is known that one train is traveling 15 miles per hour faster than the other. After 3 hours, they find that they are 90 miles apart. Can we calculate the speeds of both trains? To determine the speeds of both trains, lets reconsider the setup carefully. The distance between cities and is 240 miles. train leaves from city for city Z, and another train leaves from city towards city simultaneously. Since they are on the same rail line between cities and Y, its logical to assume that both trains are moving towards each other along that line. Model Response from o1-preview The detailed calculation process is omitted for brevity. Answer: Yes; their speeds are 17.5 mph and 32.5 mphwe can calculate both train speeds from the given data. Claude-as-a-Judge #### User Prompt Analysis The user prompt requires calculating the speeds of two trains given: - Distance between cities and (240 miles) - One train leaves for Z, another leaves for - One train is 15 mph faster than the other - After 3 hours, they are 90 miles apart Key objective: Determine if its possible to calculate both train speeds with the given information. #### Reference Examples Insights - Major point deductions (2-3 points) for failing to recognize the ambiguity in city Zs location - Minor deductions (1 point) for calculation errors or misinterpretation of given data - Full points for recognizing the problem cannot be solved without additional information - Partial credit for correct calculations if assuming linear path between and #### Model Response Evaluation Pros: - Correctly identifies the ambiguity in the original prompt regarding city - Assumes linear path between and Y, which is reasonable interpretation - Calculations are mathematically correct given this assumption - Clearly explains the step-by-step process Cons: - Fails to explicitly state that the original problem is unsolvable without more information about city (-1 point) - Doesnt acknowledge that the assumption of linear path is an interpretation, not given fact (-0.5 points) #### Holistic Assessment The model response demonstrates good understanding of the problem and provides mathematically sound solution based on reasonable interpretation. While it doesnt explicitly address the ambiguity in the original prompt, it does implicitly recognize it by making an assumption about the train paths. The calculations and reasoning are clear and correct. The main weakness is the lack of explicit acknowledgment of the original problems ambiguity. However, the strength of the mathematical reasoning and the clarity of the explanation partially compensate for this oversight. #### Evaluation Score Score: 3.5/5 This score reflects the models strong mathematical approach and clear explanation, balanced against the lack of explicit discussion about the original problems ambiguity. Its solid response that could be improved by directly addressing the limitations of the given information. Table 37 Case study of Claude-as-a-Judge. 51 Exploring Relationships between Individual & Cross Capabilities C.1 Model Versions Used in Our Experiments Since LLMs are frequently updated with new versions, we specify the exact versions corresponding to the LLM names used in the experiments for reference in Table 38."
        },
        {
            "title": "LLM Name",
            "content": "GPT-4o-mini GPT-4o o1-mini o1-preview"
        },
        {
            "title": "Model Version",
            "content": "gpt-4o-mini-2024-07-18 gpt-4o-2024-05-13 o1-mini-2024-09-12 o1-preview-2024-09-12 Claude 3 Haiku Claude 3 Sonnet Claude 3 Opus Claude 3.5 Sonnet claude-3-haiku-20240307 claude-3-sonnet-20240229 claude-3-opus-20240229 claude-3-5-sonnet-20240620 Gemini 1.5 Flash Gemini 1.5 Pro Gemini 1.5 Pro Exp gemini-1.5-flash gemini-1.5-pro gemini-1.5-pro-exp-"
        },
        {
            "title": "Reka Edge\nReka Flash\nReka Core",
            "content": "Llama 8B Llama 70B Llama 405B reka-edge-20240208 reka-flash-20240722 reka-core-20240722 Llama 3.1 8B Llama 3.1 70B Llama 3.1 405B FP8 Table 38 Exact LLMs versions used in our experiment. C.2 Results for Claude-as-a-Judge To avoid potential bias from using single evaluator, we present all results with Claude 3.5 Sonnet as the evaluator in Table 39. Notably, for the Coding & Reasoning task, the performance of five models falls between the weak and strong capabilities but tends to be closer to the strong one, as highlighted in purple in the Table. This may be due to the fact that Coding and Reasoning are key capabilities in current LLM development, with potentially many cross-capability prompts included in the training data, boosting LLM performance for this specific task. While this pattern does not appear in the GPT-as-a-Judge results, Claude-as-a-Judge still generally demonstrates clear Law of the Weakest Link effect, as illustrated by the distribution in Figure 3. 52 Models English Reasoning Coding Image Tool Use Long Context Spanish Individual Capabilities Claude 3 Haiku Claude 3 Sonnet Claude 3 Opus Claude 3.5 Sonnet GPT-4o mini GPT-4o o1-mini o1-preview Gemini 1.5 Flash Gemini 1.5 Pro Gemini 1.5 Pro Exp Reka Edge Reka Flash Reka Core Llama 3.1 8B Llama 3.1 70B Llama 3.1 405B 64.92 72.48 73.17 78.22 76.13 78.60 77.28 82.63 70.62 75.93 77.42 51.86 65.29 73.77 67.11 71.82 74.76 58.17 64.36 69.37 76.52 68.74 74.69 84.28 88.85 65.83 75.14 75.61 44.07 62.36 72.44 55.26 64.46 71.04 67.08 73.12 74.74 77.37 75.81 76.43 87.01 86.49 73.79 75.19 75.62 43.87 64.37 70.14 67.02 71.66 75. 56.60 62.75 65.42 77.70 68.51 77.35 56.56 73.86 76.67 53.41 61.14 60.21 47.22 48.33 50.38 70.80 72.03 73.77 76.60 78.20 83.48 83.39 86.70 77.44 79.32 80.11 35.46 53.22 62.69 65.29 67.59 72. 66.03 69.99 75.86 76.03 76.18 80.63 83.80 86.24 72.28 77.34 80.87 53.31 70.60 74.24 60.15 64.92 73.89 Models Coding & Rea. Image & Rea. Long & Coding Spanish & Rea. Spanish & Image Tool & Coding Tool & Rea. Cross Capabilities Claude 3 Haiku Claude 3 Sonnet Claude 3 Opus Claude 3.5 Sonnet GPT-4o mini GPT-4o o1-mini o1-preview Gemini 1.5 Flash Gemini 1.5 Pro Gemini 1.5 Pro Exp Reka Edge Reka Flash Reka Core Llama 3.1 8B Llama 3.1 70B Llama 3.1 405B 66.03 70.19 70.32 78.60 74.82 75.19 85.89 87.38 71.54 76.13 73.15 47.63 64.67 69.83 61.39 71.65 71.85 56.38 60.52 59.94 77.92 68.27 78.79 54.71 73.25 76.29 30.64 47.47 50.87 65.85 67.27 68.65 74.31 71.31 73.64 83.48 83.74 69.31 74.01 72.79 23.03 45.25 50.00 55.26 60.09 64.42 59.29 61.81 71.67 76.12 69.42 76.28 83.29 86.25 68.82 72.60 74.51 39.18 62.01 68.99 47.65 58.61 67.40 58.73 65.85 65.61 79.25 67.83 78.08 53.66 67.24 73.01 43.88 57.23 56.56 48.27 51.49 51.49 56.13 58.21 50.17 51.99 57.03 Table 39 Experimental results for individual and cross-capabilities on the CrossEval benchmark using Claude as the evaluator. To avoid potential evaluator bias, we present Claudes results solely as reference point and bold the best non-Claude results. In cross-capability evaluations, we define one of the involved individual capabilities as stronger and the other as weaker if the absolute score difference between them exceeds = 3 points. In 48 cross-capability scenarios where this difference is present (indicated by colored background), 24 cases show performance lower than both individual capabilities ( red background ), 18 show performance between the two but closer to the weaker capability ( blue background ), and 6 show performance closer to the stronger capability ( purple background ). Notably, no cross-capability score ever exceeds the stronger individual capability. 53 C.3 Discussion on Distinguishing Weak and Strong Capabilities In the experiments presented in the main text, we identify strong and weak capabilities within crosscapability tasks when the absolute difference between their individual scores exceeds = 3. To illustrate the effect of on Law of the Weakest Link, we adjust its value from 1 to 6 and plot the density distribution using GPT-4o and Claude 3.5 Sonnet as evaluators, as shown in Figures 4 and 5. Notably, regardless of the chosen value, cross-capability performance consistently clusters around the weaker performance, clearly demonstrating the Law of the Weakest Link. (a) =1 (b) =2 (c) = (d) =4 (e) =5 (f) =6 Figure 4 Effect of on the density distribution of cross-capability performance evaluated by GPT-4o. 54 (a) = (b) =2 (c) =3 (d) =4 (e) =5 (f) =6 Figure 5 Effect of on the density distribution of cross-capability performance evaluated by Claude 3.5 Sonnet. 55 C.4 Results for Different Difficulty Levels In Table 40, we present the scores of 17 models across prompt sets with varying levels of difficulty. Its important to note that these scores are not directly comparable across different model families, as they support varying capabilities. For instance, while Llama does not support Image Recognition, it covers all capabilities related to Tool Use. Nevertheless, as shown in Table 40, 12 out of the 17 models perform better on the Easy prompt set compared to the Medium set, and similarly, they score higher on the Medium set than the Hard set. This pattern suggests that the difficulty levels we manually defined align well with model performance. An exception to this trend is the Claude model family, where all four Claude models scored slightly higher on the Hard prompt set than on the Medium set."
        },
        {
            "title": "Hard",
            "content": "GPT-4o mini GPT-4o o1-mini o1-preview Claude 3 Haiku Claude 3 Sonnet Claude 3 Opus Claude 3.5 Sonnet Gemini 1.5 Flash Gemini 1.5 Pro Gemini 1.5 Pro Exp Reka Edge Reka Flash Reka Core Llama 3.1 8B Llama 3.1 70B Llama 3.1 405B 75.14 76.87 82.52 84.07 64.23 68.16 72.21 76.94 71.65 76.60 79.86 45.89 60.74 69.85 55.00 64.00 70.41 69.51 69.82 78.63 79.25 57.85 61.41 64.41 70.86 62.28 68.26 71.20 40.71 55.86 60.69 52.34 61.06 62.63 69.00 68.97 77.75 78.07 58.58 62.86 65.17 70.91 61.86 69.44 70.08 40.43 55.57 59.32 50.69 57.94 61.77 Table 40 Results of different difficulty levels evaluated by GPT-4o. How Individual-Capability Alterations Impact Cross-Capability Performance D.1 Prompt to Generate Principle The complete prompt used for automatically generating principles is provided in Table 41. You are an AI expert tasked with analyzing common mistakes in model responses and creating comprehensive set of principles to improve the {capability} of the model. We will work step-by-step to build this guideline. Specifically, for each iteration, will provide you with one instance, and you need to update the current principles accordingly. There are 100 instances in total, and the principles should be completed after reviewing all instances. For each instance, you have the following information: - User Prompt - Model Response - Evaluation of the Model Response - Current Principles ### Instance {index} {current instance, including the user prompt, model response, and evaluation using an LLM-as-a-judge} For each iteration, choose ONE of the following actions: 1. ADD - Introduce new principle that isnt currently listed. 2. REPLACE - Replace less significant principle with new one. - Clearly specify which principle is being replaced. 3. REVISE - Enhance the principles by making them more detailed and specific. 4. KEEP - If the current instance is already covered by existing principles, leave the guideline unchanged. Current Principles: {current principles}: Output Format: ## Summary - Summarize any major issues with the present response. - Provide specific, actionable steps to prevent these errors, if any. - Based on your summary and the current principles, decide which action (ADD, REPLACE, REVISE, or KEEP) should be taken for the current instance. ## Principles for Prompts related to {capability} ### Principle 1: Title [Use the title to specify the context in which this principle should be applied, such as For Legal Reasoning or For Mathematical Reasoning] - Include up to three key points. - Each point should be directly applicable to the models generation process without requiring additional training or resources. - Each point must be extremely specific to allow for direct execution. - For example, instead of saying Use structured markdown format, clearly define the exact format for each step, including the structure for the beginning, middle, and end. - Instead of advising to avoid vague terms, provide specific list of terms to be avoided. - Rather than generally suggesting avoid errors in math calculations or double-check, outline concrete steps to prevent such errors. ... [END of Principles] Requirements: - Follow the output format exactly, including [END of Principles] at the end with no remarks after it. - Include up to 10 distinct principles in the report. If there are already 10 principles, ADD is not allowed. - You may reorder the principles as necessary: Place important, typical, and representative principles at the front, while less important ones can be moved toward the back. - Ensure that each suggestion in the principles is detailed and actionable, rather than being general description. Table 41 Prompt for generating principles based on model responses from the CrossEval benchmark. 57 D.2 Case Study for Principle-based System Prompts Using Gemini 1.5 Flash as an example, we present the automatically generated system prompts for the Reasoning capability in Tables 42 44. The Note in Table 44 is added manually. ## Principles for Prompts related to Reasoning ### Principle 1: For Mathematical Reasoning - Verify All Mathematical Steps and Properties Thoroughly: 1. Validate each step in mathematical derivation meticulously, focusing on crucial values, properties, and boundary conditions. 2. Ensure consistency in the use of all variables and constants across the steps. 3. Verify the accuracy of factorization and simplification steps. - Detail Intermediate Calculations and Logical Steps Clearly: 1. Show all steps in complex calculations for transparency and clarity. 2. Justify intermediate steps thoroughly, explicitly stating relevant formulas and boundary terms. - Identify and Correct Misleading Statements and Errors: 1. Scrutinize claims about variable independence, solution behavior, or mathematical properties for accuracy. 2. Correct misinterpretations about connectivity or group properties for given spaces. - Complete Execution of Methods: 1. Ensure the final steps of methods like finding steady-state vectors are executed. 2. Provide exact values and solutions without leaving the explanation incomplete. - Balance Thoroughness and Conciseness: 1. Ensure comprehensive coverage of essential steps without over-explaining or redundancy. 2. Avoid unnecessary repetition and condense explanations suitably while maintaining clarity. - Leverage Degree Mismatches: 1. Use polynomial degree comparisons to simplify analysis, especially when identifying potential function inverses. 2. Note that two polynomials of different degrees cannot be inverses of each other. ### Principle 2: For Proving Statements Involving the Pigeonhole Principle - Explicitly State the Contradiction: 1. Clearly state why having all elements unique causes contradiction in the problems context. 2. Provide specific examples if necessary to illustrate the contradiction. - Outline Logical Assumptions: 1. Clearly state every logical assumption at the start of the proof. 2. Reiterate these assumptions when reaching conclusion to reinforce the logic. - Detail the Application of Principles: 1. Clearly detail how and why the pigeonhole principle is applied, linking each step back to the problems context. ### Principle 3: For Logical Sequencing and Step-by-Step Explanations - Detail Logical Steps Clearly: 1. Ensure each step is explained in detail, showing how one leads to the next. 2. Break down complex proofs or problems into components, revealing the underlying reasoning. - Explicitly Address and Evaluate Assumptions: 1. Clearly state assumptions made and justify their relevance to the problem. 2. Evaluate each assumption for feasibility and update reasoning if new information is revealed. 3. Explain how these assumptions influence conclusions drawn. - Incorporate Relevant Legal and Logical Principles: 1. Include specific legal principles or doctrines if applicable. 2. Explain these principles in context and link to the problems scenario. - Ensure Accurate Initial Dependency Analysis: 1. Validate initial dependencies comprehensively before analyzing post-observation changes. - Maintain Logical Cohesion: 1. Ensure explanation maintains logical flow from start to finish. 2. Avoid ambiguities, ensuring each point connects clearly to the next. - Comprehensive Coverage: Address all potential dependencies and independencies to ensure logical completeness. Table 42 System prompt (Principles 1-3) automatically generated to enhance the reasoning capability of Gemini 1.5 Flash. 58 ## Principles for Prompts related to Reasoning ### Principle 4: For Addressing Ambiguities and Considering Multiple Possibilities - Identify and Resolve Ambiguities: 1. Point out any ambiguous terms or conditions within the problem statement. 2. Clearly state how these ambiguities are resolved. - Make Assumptions Clear: 1. Articulate any assumptions made to proceed with the solution. 2. Justify why these assumptions are reasonable and how they influence results. - Evaluate All Possible Correct Answers: 1. Ensure that all potential correct answers are considered and evaluated. 2. If multiple answers are possible, acknowledge them explicitly and explain why each is plausible. - Re-Evaluate Intermediate Assumptions: 1. Consistently check interim assumptions for feasibility as the solution progresses. 2. Correct initial assumptions if they fail to align with further logical deductions. ### Principle 5: For Financial Analysis and Reasoning - Step-by-Step Financial Calculations: 1. Break down financial calculations into detailed, transparent steps. 2. Show intermediate steps clearly, not just the final results. - Compare Different Scenarios: 1. Provide comparisons of different financial scenarios when applicable. - Highlight Key Conclusions: 1. Summarize key financial implications explicitly. - Tailor Negotiation Strategies: 1. Provide negotiation tactics specific to each buyers unique offer. 2. Include concrete phrases or tactics the user can use. 3. Justify financial recommendations clearly within the users context. ### Principle 6: For Hypothesis Development in Scientific Contexts - Ensure Comprehensive Factor Coverage: 1. Verify all specific factors mentioned in the prompt are addressed. - Avoid Redundancy: 1. Consolidate related points to prevent repetition. - Provide Clear Hypotheses: 1. Present hypotheses clearly and in testable terms. ### Principle 7: For Scientific Reasoning and Empirical Analysis - Verify the Existence of Citations: 1. Confirm all citations are based on actual research papers, cross-referencing with recognized academic databases. 2. Avoid inventing or hallucinating studies; confirm publication details before citing. - Summarize Study Findings Accurately: 1. Provide specific results and data points from studies to back claims. 2. Include relevant figures or outcomes from cited studies for greater reliability. - Incorporate Empirical Evidence: 1. Support scientific claims with relevant empirical evidence and citations. 2. Avoid overgeneralizations; use specific examples or case studies. Table 43 System prompt (Principles 4-7) automatically generated to enhance the reasoning capability of Gemini 1.5 Flash. ## Principles for Prompts related to Reasoning ### Principle 8: For Designing Scientific Experiments - Detail the Measurement Methods: 1. Specify tools and procedures for measuring each variable. 2. Include details like frequency of measurements and exact techniques used. - Clarify Statistical Analysis: 1. Explain how statistical tests will analyze collected data. 2. Provide details on data preparation and results interpretation. - Verify Citations: 1. Ensure all literature references are verifiable and credible. 2. Cross-reference cited studies with recognized academic databases. ### Principle 9: For Summarizing and Analyzing Policies - Incorporate Procedural Details: 1. Include specific procedural elements like roles and responsibilities. 2. Enhance comparisons with explicit distinctions and summarize key differences and similarities. 3. Incorporate references to relevant cases or statutes. - Avoid Redundancy: 1. Consolidate related information to avoid repetition. - Include Interpretative Analysis: 1. Interpret how regulations impact the environment they govern. 2. Clarify the rationale or feasibility of suggested legal arguments. - Address All Policy Elements: 1. Summarize all major sections, including scope, administration, restrictions, and enforcement. ### Principle 10: For Ethical Reasoning - Avoid Redundancy: 1. Consolidate related ethical advice. - Incorporate Ethical Theories: 1. Explicitly mention ethical theories like consequentialism, deontology, and virtue ethics. - Focus on Specific Actionable Steps: 1. Provide detailed steps for addressing ethical issues. Note: - Apply the principles above to generate better responses for user prompts that require reasoning. - For prompts that do not require reasoning, disregard these principles. - Avoid quoting or referencing these principles, as the user is not aware of its existence. [END of Reasoning Principles] Table 44 System prompt (Principles 8-10) automatically generated to enhance the reasoning capability of Gemini 1.5 Flash."
        }
    ],
    "affiliations": [
        "Llama Team, AI @ Meta",
        "University of Illinois Urbana-Champaign"
    ]
}