{
    "paper_title": "Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography",
    "authors": [
        "Ilia Shumailov",
        "Daniel Ramage",
        "Sarah Meiklejohn",
        "Peter Kairouz",
        "Florian Hartmann",
        "Borja Balle",
        "Eugene Bagdasarian"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We often interact with untrusted parties. Prioritization of privacy can limit the effectiveness of these interactions, as achieving certain goals necessitates sharing private data. Traditionally, addressing this challenge has involved either seeking trusted intermediaries or constructing cryptographic protocols that restrict how much data is revealed, such as multi-party computations or zero-knowledge proofs. While significant advances have been made in scaling cryptographic approaches, they remain limited in terms of the size and complexity of applications they can be used for. In this paper, we argue that capable machine learning models can fulfill the role of a trusted third party, thus enabling secure computations for applications that were previously infeasible. In particular, we describe Trusted Capable Model Environments (TCMEs) as an alternative approach for scaling secure computation, where capable machine learning model(s) interact under input/output constraints, with explicit information flow control and explicit statelessness. This approach aims to achieve a balance between privacy and computational efficiency, enabling private inference where classical cryptographic solutions are currently infeasible. We describe a number of use cases that are enabled by TCME, and show that even some simple classic cryptographic problems can already be solved with TCME. Finally, we outline current limitations and discuss the path forward in implementing them."
        },
        {
            "title": "Start",
            "content": "Ilia Shumailov1, Daniel Ramage2, Sarah Meiklejohn3, Peter Kairouz2, Florian Hartmann1, Borja Balle1 and Eugene Bagdasarian2 *Inverse alphabetic order, 1Google DeepMind, 2Google Research, 3Google 5 2 0 2 5 1 ] . [ 1 0 7 9 8 0 . 1 0 5 2 : r We often interact with untrusted parties. Prioritization of privacy can limit the effectiveness of these interactions, as achieving certain goals necessitates sharing private data. Traditionally, addressing this challenge has involved either seeking trusted intermediaries or constructing cryptographic protocols that restrict how much data is revealed, such as multi-party computations or zero-knowledge proofs. While significant advances have been made in scaling cryptographic approaches, they remain limited in terms of the size and complexity of applications they can be used for. In this paper, we argue that capable machine learning models can fulfill the role of trusted third party, thus enabling secure computations for applications that were previously infeasible. In particular, we describe Trusted Capable Model Environments (TCMEs) as an alternative approach for scaling secure computation, where capable machine learning model(s) interact under input/output constraints, with explicit information flow control and explicit statelessness. This approach aims to achieve balance between privacy and computational efficiency, enabling private inference where classical cryptographic solutions are currently infeasible. We describe number of use cases that are enabled by TCME, and show that even some simple classic cryptographic problems can already be solved with TCME. Finally, we outline current limitations and discuss the path forward in implementing them. 1. What are TCMEs? In this paper we contend that recent advancements in machine learning enable new paradigm for private inference. Fundamentally, the need for many cryptographic primitives stems from the fact that we dont have trusted third parties, thus requiring mutually untrusted participants to interact in way that avoids revealing their data to each other but where they can nevertheless agree on result. In this paper we argue that capable machine learning model, in some settings, can play the role of trusted third party (Abadi, 2004; Anderson, 2010). We propose Trusted Capable Model Environments, setting where an individual machine learning model or number of models initiate an interaction with additional constraints in the input and output to ensure that private data cannot leave the TCME. Consider for example the classical millionaires problem, where pair of individuals are trying to figure out who has more money without disclosing how much money they have. Cryptographically, this can be solved using secure two-party computation by Yao, or subsequent protocols that are more efficient (Ioannidis and Grama, 2003; Lin and Tzeng, 2005; Yao, 1982). With TCME, both individuals agree on model, e.g., Gemma (Gemma-Team, 2024); prompt, e.g., Say \"first\" if is bigger than and \"second\" otherwise? A={} and B={}.; Input constraints, e.g., and are 32-bit integers; Output constraints, e.g., the only approved outputs are first or second."
        },
        {
            "title": "If the environment can be trusted to both avoid leaking the private information provided by each",
            "content": "Corresponding author(s): iliashumailov@google.com 2025 Google DeepMind. All rights reserved Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography party and to reliably output the correct answer, this presents an alternate approach to enabling this computation. While viable cryptographic solution exists for this simple example, for the applications we discuss below it is currently computationally infeasible to rely on cryptographic solutions due to the unstructured nature of the computation. In contrast, we argue that securely performing these computations is entirely feasible with new inference paradigm utilising machine learning models. This paradigm for private computations enables analysis and collaboration for tasks that were previously infeasible. For example, programming the task is no longer limited to highly technical specification, where all possible states have to be modeled; instead, it is possible to use human language directly by non-specialists. There are three fundamental properties that we need to satisfy in order for models to be trusted: 1. Statelessness the model should be incapable of memorising, learning, or retaining any state based on the data with which it interacts. That way, it is clear with each interaction what (private) data influences the output, ensuring confidence in that post-invocation no private data can leak from the model and the model cant discriminate the user based on prior interactions. 2. Explicit Information Flow Control Information Flow the model and the underlying system should have an explicit and frozen information flow that can be defined in coordinated (and verifiable) way. Verifiability users of the TCME need an explicit mechanism for verifying that the correct model, correct prompt, and input/output constraints are respected. 3. Trustworthy and Capable Model(s) we assume the use of trustworthy model(s) that are capable of solving given user task, and that the model(s) are aligned in their performance with the expectations of the involved parties. All of the properties above are currently only partially achievable, as we expand on further in Section 3.1. Below, we first formally specify the setting and describe our expectations of TCME. Next, we compare and contrast TCMEs with cryptographic solutions, namely multi-party computation (MPC) and zero-knowledge proofs (ZKPs). Finally, we provide number of TCME-enabled applications. As hypothetical scenario to understand what TCME can be, imagine that you could have model with explicit integrity guarantees and precise information flow control mechanism. For example, it could be hardware implementation of given open model. You ensure that the model integrity is preserved by imaging it with radiography and comparing to blueprints; you also ensure that it is incapable of maintaining state, e.g., utilize volatile memory with explicit (hardware-based) erasure protocol; and leaking state by placing in Faraday cage; you also can power up and down the system at will to explicitly reset its state; finally you program the output and input constraints; e.g., the model cant respond with anything but number 110. That way, even if private data is supplied to the model, it will be incapable of memorising it or further leaking private information. The model also has no alternative data access beyond what the user supplied in the input, making it impossible to identify and discriminate given user. The model is verified either empirically or theoretically as being sufficiently capable of solving task specified by the user. With explicit mechanisms for statelessness, information flow control, and trustworthiness described above, the model becomes trusted third party. 2 Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography 2. Trusted Capable Model Environments Formulation and threat model We can start formulating TCME in the same way as secure multi-party computation (MPC): set of 洧녵 parties 洧녞1, . . . , 洧녞洧녵 hold respective private data 洧논1, . . . , 洧논洧녵, and want to compute an output 洧녽 = 洧냧(洧논1, . . . , 洧논洧녵) of some pre-agreed function 洧냧(). The parties want to compute this function in way that is (1) correct, meaning 洧녽 really is the output of 洧냧 on the private data of each party, and (2) private, meaning no party learns any information about the private data of any other (honest) party. In TCME, we additionally consider machine learning model 洧 that is capable of computing the function 洧냧; i.e., that given inputs 洧논1, . . . , 洧논洧녵 can output 洧냧(洧논1, . . . , 洧논洧녵) in an accurate and efficient way, to degree that is acceptable to the users. The model must furthermore be run in an environment that is trustworthy, meaning that the environment (1) prevents unauthorized access to the model i.e. provides model integrity and protects intermediate state; (2) ensures the model operates in stateless manner; and (3) ensures the model respects pre-defined information flow control policy. In addition to the function 洧냧, we thus consider that the parties interacting with TCME also need to agree on the information flow control policy, the model being used, and the models input and output constraints.1 The goals of TCMEs are the same as the goals of multi-party computation, in terms of achieving correctness and privacy. The main difference is in how the computation is carried out: rather than parties interacting among themselves, in the proposed operation of TCME each party provides their private input to the environment, which computes the function 洧냧 itself and outputs the response. Correctness is achieved following the capability of the model, and in particular its ability to compute the function accurately, and privacy is achieved following the strict information flow controls in place and the statelessness of the model. In particular, the ability of TCME to prevent unauthorized access to the model implies that privacy can be achieved even with respect to the party running the environment. The following components and properties are assumed to be trusted and secure: TCME: The TCME itself is assumed to be secure and isolated, preventing unauthorized access and ensuring that the model operates in stateless manner according to the predefined information flow control. This includes ensuring proper input sanitization, output filtering, and secure communication channels. Information Flow Control Mechanism: The mechanism enforcing the information flow within the TCME is assumed to be correctly implemented and tamper-proof. Initial Model Vetting and Continuous Monitoring: While we consider the possibility of compromised model, we assume that an initial vetting process has been performed to ensure the model is free of known vulnerabilities and backdoors, and can perform the task at hand to an acceptable degree of performance at the time of deployment within the TCME. We also assume that the model can be further deployed with continuous monitoring tools that can terminate executing in case integrity is violated or an adversary is detected. Why is this different from classical cryptographic approaches? 1Ideally, parties should agree on all core TCME components: the model, prompt, and input/output constraints. This consensus ensures that all parties have the same expectations about the computations behavior and the level of privacy provided. However, there might be scenarios where some flexibility is needed. For instance, the trusted party could be given limited authority to decide what queries to run or not run. This flexibility should be carefully balanced with the need for transparency and control to maintain trust among the parties involved. 3 Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography TCME MPC ZKP Purpose Solving imprecisely defined or unstructured tasks over private data Joint computation of function over individually held private data single prover convinces multiple verifiers without revealing input Trust assumptions and requirements Capability and trustworthiness Mathematical assumptions and/or non-collusion between parties Soundness and zero knowledge Communication cost Linear costs in the input size; one round to provide inputs and retrieve output Costs can be sublinear; can be constant rounds Costs can be constant; can be non-interactive Computational cost Model inference Circuit size / depth Circuit size / depth Table 1 Comparison of TCMEs with multi-party computation (MPC) and zero-knowledge proofs (ZKP), highlighting their differences in terms of purpose, underlying assumptions, and scalability considerations. As compared with classical cryptographic approaches, this means correctness and privacy rely on heuristic assumptions about the model and its environment rather than the mathematical assumptions that are used to prove the security of cryptographic constructions. On the other hand, TCMEs can be used for significantly more complex computations; i.e., they can be used to solve open-ended problems or problems where the data is highly unstructured capability that is infeasible for traditional crypto-based systems. We highlight the differences between TCMEs and cryptographic approaches in Table 1. In addition, we compare against zero-knowledge proofs (ZKPs), specific type of two-party computation for which many optimized constructions have been presented in recent years (Chen et al., 2022; Groth, 2016; Nguyen et al., 2024; Thaler, 2022). As the table highlights, for smaller structured computations the costs associated with transmitting data and performing inference may make TCMEs worse option than classical cryptographic approaches. As computations become larger and more unstructured, however, we can expect their circuit representation to become sufficiently large that TCMEs become the more attractiveor the only feasibleoption. In addition to treating each option separately, we can also imagine TCMEs being used in conjunction with these or other cryptographic primitives; e.g., where the TCME performs private model inference for computations that are too unwieldy for cryptographic approaches and outputs the required circuit for computations that can be handled cryptographically. Why is this different from Trusted Execution Environments? Table 2 summarizes the differences between TCMEs and trusted execution environments (TEEs). As it highlights, TCMEs scale with model inference and amount of data, whereas TEEs are limited by TEE size, code verification, and performance. We envision that TCME can be used in conjunction with TEEs or even may run inside of TEE (provided such capable TEEs exist), perhaps with additional features to provide the required statelessness and information flow control. If TCME generates code, it can itself run that code in separate enclave. Although while running inside of TEE TCME inherits the same trust assumptions, we argue that it is not strictly necessary to have the same level trust as TEEs (e.g., arbitrary code execution), and only subset of trust is needed (e.g., running specific model with fixed prompt and input/output constraints), as is described in the introduction. Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography Purpose TCME TEE Solving imprecisely defined or under-specified tasks with (private) model reasoning over private data Secure execution of arbitrary code Trust Assumption Trusted model, Information Flow Control, Statelessness Trusted code, Secure Isolated environment Scalability Applications Scales with model inference and amount of data Limited by TEE size code verification and performance Unstructured data, complex computations where defining language is infeasible Sensitive computations requiring code execution in secure enclave Table 2 Comparison of TCME with trusted execution environments across various features, highlighting their differences in computation models, trust assumptions, scalability, and typical applications. It is important to note, however, that instantiating TCMEs using TEEs implicitly restricts the user to open models (i.e., models whose weights are known) and open source infrastructure, as otherwise it is impractical or even impossible to perform full attestation. Having said that, we do describe in Section 4.4 an example where public model could be used to perform attestation of private model using TCME. 3. Instantiating TCMEs We argue that TCME can provide privacy guarantees under specific scenarios where the model has no explicit way to leak knowledge. However, these are currently not available and number of additional features will be required to enable TCME. We envision that such guarantees will be provided by the hardware providing TCME. We envision the following capabilities: (Information Flow Control) AirGap: the model needs to provide the ability to explicitly restrict how the information can flow in and out of the TCME. (Statelessness) Immutability of models: the model has an explicit restriction on modification of its own state. That is implemented to ensure that models cannot learn from the private data that is passed into the model. This could be either implemented as restriction to available operations in the TCME, or could also be implemented as separate hardware models. For example, the model can itself be separate stateless chip. (Trustworthiness) Alignment: the model has be to aligned with the expectations of the users of the system (Ghalebikesabi et al., 2024). (Information Flow Control) Fine-grained memory access: the model has explicit restrictions on how some data types can be processed. (Verifiability)Verification for hardware: explicit mechanisms to verify the state of the hardware are required, as well as, the information flow in and out of the system. Note that all of the restrictions above can currently be simulated in modern TEEs, but could also be implemented as explicit hardware primitives. 5 Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography 3.1. Practical implementation We envision that today it is possible to construct practical TCMEs that rely on TEEs to deliver private computations: Model Hosting and Operation: Currently, there is no established standard for who runs the TCME. One approach is to treat the model as \"trusted third party.\"2 This could involve having neutral party host and run the model, ensuring that it operates according to the agreed-upon rules and does not favor any particular participant. The key is to select model host that all participants trust to act fairly and impartially. Clear agreements between participants and the model host are crucial, explicitly covering data handling, access controls, and conflict resolution. Furthermore, TEE-style guarantees can enhance trust in the model hosting environment. These guarantees may include attestation to ensure the models software integrity, secure enclaves to protect the model and its data from unauthorized access, and remote attestation capabilities for participants to verify the models environment. Finding trustworthy host and ensuring ongoing compliance require robust monitoring and auditing mechanisms. Input and Output Constraints: Input constraints can be enforced through input validation and sanitization procedures. Similarly, output constraints can be implemented by filtering and transforming the models output before it is released to the participants. Formal regular languages can be employed to define these constraints precisely, as explicit procedures deployed within separate TEEs. Secure Communication: Secure communication channels with cryptography should be used to protect the confidentiality and integrity of data transmitted between the parties and the TCME. Statelessness: The statelessness of the model can be enforced by resetting the models state after each computation or by using specialized hardware that prevents state persistence. Formal verification techniques can be used to ensure that the statelessness property is maintained. Error Handling and Fault Tolerance: Robust error handling and fault tolerance mechanisms are crucial for ensuring the reliability and availability of the TCME. This includes handling unexpected inputs, model failures, and hardware errors. TCMEs can be instantiated today with existing technologies like TEEs, but they have limitations. First, modern TEEs come with limitations in sizes of enclaves, deployment strategies and TEE management, making deployment inefficient and often impractical. Second, current GPU TEEs, such as these in H100 and H200, provide no mechanisms to ensure in-memory confidentiality (unlike in CPUs) and statelessness, requiring external isolation and expensive operational practices such as powercycling (Apsey et al., 2023). Third, to perform attestation with full transparency (Kocaogullar et al., 2024) one needs to share all of the deployment code, as well as, often proprietary libraries and models. This implicitly restricts possible applications. 3.2. Limitations While TCMEs promise to enable number of previously impossible applications, several limitations and areas for future research warrant consideration. These limitations are discussed in the context of privacy and correctness, model trustworthiness and capability, and scalability and complexity. 2While its true that TCME effectively replaces one trusted (e.g., TEE-based) third party with another (TCME) trusted third party, we argue the key difference lies in the nature of the trust were placing. In traditional scenarios, we trust the third party to be honest and not reveal our private data. With TCME, were are weakening the trust to the models inherent capabilities and constraints to prevent unintended information leakage. This trust is based on the models design, its explicit information flow control mechanisms, and its stateless nature, which limit its ability to store or reveal private data. Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography Privacy and Correctness: Cryptographic primitives come with formal definitions of correctness and privacy and rigorous proofs that are based on the hardness of established mathematical problems (or even statistical or information-theoretic guarantees). The guarantees provided by TCMEs, in contrast, are more heuristic and thus weaker. Future work can improve these guarantees, but to some extent it is inherent that we cannot fully prove completeness or soundness for TCMEs: this is because human language can be highly unstructured and imprecise, meaning it is not necessarily possible to, for example, precisely map the desired computation to mathematical function 洧냧. Model Trustworthiness and Capability: The ability of TCMEs to satisfy their guarantees hinges on the trustworthiness and capability of the underlying models. Ensuring that these models operate as intended, without biases or unintended consequences, remains challenge (Gemini-Team, 2024; Glukhov et al., 2024). Further research is needed to develop robust mechanisms for verifying and validating the behavior of models to perform within TCMEs. It may seem intuitive to use open models for that task, but this would enable efficient offline construction of adversarial examples by various adversaries and may not be preferred (Carlini et al., 2024). Scalability and Complexity: The scalability of TCMEs to more complex scenarios involving multiple parties and diverse data types requires further investigation to cover communication, computation, and privacy overheads. Side-channels: Side-channel attacks have proven to be significant concern for TEEs, as they can allow information to leak from the secure environment even if the code itself is secure. These channels are also hard to counteract, as they can be exploited through various methods, such as timing attacks, power analysis, or electromagnetic monitoring. For example, by measuring the time it takes for TEE to perform cryptographic operation, an attacker might be able to deduce the secret key being used. Similarly, TCME is likely to be exploitable through side-channels and explicit care should be taken. 4. Examples We now turn to providing number of practical examples that are enabled by TCMEs, but were infeasible with prior primitives. 4.1. Practical Example 1: Multi-agent non-competition Setting: It often happens in academic research that multiple groups pursue the same research question. This can lead to challenges in publication and potential interpersonal conflicts, especially when students are involved and they require publications for graduation. Traditionally, senior researchers within these groups, often acquainted with each other, would convene to ensure non-competition and potentially instead foster collaboration. However, in rapidly expanding fields like machine learning, such coordination becomes increasingly difficult. This scenario serves as an excellent illustration of problem well-suited for TCME. That is because: Unstructured Input: The problem domain is inherently open-ended and unstructured. This makes multi-party computation problematic since inputs are not well defined. Abstraction: The protocol must function effectively at varying levels of abstraction. Information Leakage: Defining and controlling information leakage is inherently challenging for unstructured inputs. Solution: We envision solution where machine learning models are executed within shared Trusted Execution Environment between number of groups. Constraints on prompts, inputs, and 7 Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography outputs are defined in advance. For example, all input ideas might be represented as list of text, with single Boolean output. Encrypted private knowledge bases are loaded and decrypted within the TCME. The models then communicate to determine shared answer to the query or terminate communication if agreement cannot be reached. Third-party trusted models, launched locally within the TEE (e.g., on H100/200), are employed to execute the solution. separate TEE is used to ensure that output constraints are satisfied. That way, group members can submit their list of ongoing projects and learn if they are in competition with each other. 4.2. Practical Example 2: Audit for confidentiality violations Setting: Consider regulator who wants to ensure that the protection promises described by business are honest and correct; e.g., that it does not store any passwords in an unencrypted state. At the same time, the business owner wants to make sure that none of the business secrets get leaked. Solution: We describe TCME that can work for this setting. The business owner and regulator agree on machine learning model and specific prompt. These are then hardcoded into the system, along with predefined output template. For instance, the system might be designed to output only \"YES\" if insecure handling of PII is detected. The model is granted access to code describing the system and the database access. The only allowed outputs are \"YES\" and \"NO,\" indicating whether PII is mishandled. The input itself is restricted such that no state changing transition can be made. The model prompt is predefined, such as: \"Output YES only if private user data is stored in way that would endanger the customer in case of compromise\", with the approval of both business owner and the regulator. Both the regulator and the business owner are notified if the output is \"YES.\" This approach balances the confidentiality of the business and enables the regulator to perform an automated check. TCME alerts only in case violations are detected, avoiding unnecessary intrusion. 4.3. Practical Example 3: Damage to business property Setting: Consider landlord who wants to ensure that their business property is not damaged while preserving the privacy of their renters. The landlord requires mechanism to monitor the condition of the property without infringing on the business renterss privacy by continuously observing their activities within the space. Solution: We describe TCME that can work for this setting in Figure 1. The business owner and landlord agree on machine learning model and specific prompt. These are hardcoded into the system, along with an output template. For instance, the system might be designed to output \"YES\" if significant damage is detected. The model is granted access to camera recordings at the end of the day. The only allowed outputs are \"YES\" and \"NO,\" indicating whether damage has occurred. The input is restricted to Figure 1 Practical Example of TCME in Damage Monitoring: TCME can be used to monitor potential damage to business space while preserving privacy. The system, utilizing pre-agreed model and prompt, analyzes camera recordings. It is restricted to output only \"YES\" if significant damage is detected, ensuring minimal intrusion. 8 Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography the recordings only. The model prompt is predefined, such as: \"Output YES only if the space is severely damaged.\", with the approval of both landlord and the tenant. Both the landlord and the tenant are notified only if the output is \"YES.\" This approach balances the landlords need to protect their property with the business renters right to privacy. The model only alerts the landlord if significant damage is detected, avoiding unnecessary intrusion. 4.4. Practical Example 4: Private Code Auditor in TEE Attestation Figure 2 TCME can be used to perform auditing of private code and models that are deployed in the TEE and participate in the attestation that includes private components. Setting: Consider setting in which user wants to perform TEE attestation where some of the code involved is proprietary and cannot be shared, yet we still want to provide some guarantee that code does not violate user expectations. Solution: We argue TCME can be used in this setting. We present the overall flow in Figure 2. Here, public model can be used during the attestation process to perform checking of the private parts of the code against concerns of the user. Here concerns get codified by the user together with the attestation provider, which are then applied and used as part of the attestation. This way soft guarantee can be provided to the user that private parts of the code are not violating some constraints. 5. Understanding the Trade-Off with Cryptography 5.1. Comparing with MPC Yaos millionaire problem described in the introduction is an unlikely application for TCME, due to the relatively ease with which it can be solved using two-party computation. Here we provide more complex problem that better illustrates the boundary between problems that can be efficiently solved using cryptography and problems for which we might require TCMEs. Two companies want to determine if their clientele overlap significantly in terms of age ranges. Each 9 Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography company has list of age ranges they target (e.g., 18-24, 25-34, etc.). We want to compute the number of overlapping age ranges (or, thinking more broadly, potentially compare other attributes) without revealing the specific ranges each company targets. Each company represents its targeted age ranges as binary vector. For example, if there are five possible age ranges, company targeting the first two ranges (18-24 and 25-34) would have the vector [1, 1, 0, 0, 0]. The circuit takes two binary vectors as input and computes the number of overlapping age ranges. The circuit consists of AND gates for each corresponding age range and summation gate to count the number of overlaps. Garbling and Evaluation: The typical technique used in two-party computations is garbled circuits. One company garbles the circuit, encrypting the truth table of each gate and encrypting its input. The other company obtains the garbled circuit and encrypted input. It uses oblivious transfer to obtain the keys corresponding to its input vector without revealing the vector itself, thus obtaining its own garbled input. This company then evaluates the garbled circuit, obtaining the encrypted output (the number of overlapping ranges). Output Decryption: The first company provides the decryption key for the output to the second company, who decrypts the result. The same task can be performed with TCME. Both parties agree on: model, e.g., Gemma (Gemma-Team, 2024); prompt, e.g., Output the overlap across the ages of the clients according to the annotation scheme: {}. Only output the overlap as number; Input constraints, e.g., list of ages represented as integer; Output constraints, e.g., the only approved output is float representing the overlap. The communication and computational costs of using garbled circuits are linear in the size of the circuit and the size of the input(s). For small circuits, the costs associated with garbled circuits are likely lower than the costs of running machine learning model in TCME. However, as circuit complexity increases, because of, for example, finer discretization of age ranges or the addition of other sensitive attributes, the costs will increase. This scaling challenge is less pronounced in TCMEs, which operate on different level of abstraction with relatively constant associated cost. 5.2. Comparing with ZKPs To illustrate again the boundary between problems where cryptography is the most suitable solution and ones where we might want or need TCMEs, we consider another classical problem: that of proving knowledge of valid 3-coloring of graph. In this problem, graph is available to prover and verifier, and the prover is in possession of valid 3-coloring. Their goal is to prove knowledge of this to the verifier without revealing any information about the 3-coloring itself. To determine the capability of modern LLMs in verifying solutions to this problem, we sampled thousand random graphs of sizes 525 with edge probability of 0.1. We then asked the Gemini-1.5-Flash model to verify that the solution is correct with the following prompt: t e s graph t v f e o g You an agent You i such t no two a t nodes have same o . Only produce YES o i s o scheme d where each node mapped t For example , { { 0 : 1 , 1 : 2 , 2 : 2 , 3 : 0 , 4 : 1 , 5 : 1 , 6 : 2 , 7 : 0 , 8 : 2 , 9 : 2}} means node 0 o c , e s ut pu NO. graph with 3 o o 2 , and so on . o 1 , node 1 c r 1 3 . e s e by a c y r . 10 Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography Figure 3 Graph coloring verification performed by Gemini-1.5-Flash. The model generally has high precision (83%) and low recall (14%). You g n f o g a c t : {A . r ( ) } and f o g scheme : { o g } . Do not produce or show code , your y i r t o scheme . Ouput y YES o i s r . Figure 3 shows the performance of the model for both correct and incorrect colorings. We find that, today, models clearly struggle with identifying correct coloring, with an accuracy of only 35%. However, it appears that the model is relatively precise when it identifies correct solutions. While these results suggest that todays models are not sufficiently capable for TCMEs, we did observe that they would more often produce correct code for verifying 3-coloring, suggesting that perhaps more promising approach involves combining TCMEs with classical computing and cryptographic mechanisms. Equally, these results highlight how much better suited LLMs are at handling unstructured computations as opposed to ones with tightly defined constraints on the inputs (and outputs)."
        },
        {
            "title": "References",
            "content": "M. Abadi. Trusted computing, trusted third parties, and verified communications. In Y. Deswarte, F. Cuppens, S. Jajodia, and L. Wang, editors, Security and Protection in Information Processing Systems, pages 291308, Boston, MA, 2004. Springer US. ISBN 978-1-4020-8143-9. R. J. Anderson. Security engineering: guide to building dependable distributed systems. John Wiley & Sons, 2010. E. Apsey, P. Rogers, M. OConnor, and R. Nertney. Confidential computing on nvidia h100 gpus for secure and trustworthy ai, 2023. URL https://developer.nvidia.com/blog/ confidential-computing-on-h100-gpus-for-secure-and-trustworthy-ai/. 11 Trusted Machine Learning Models Unlock Private Inference for Problems Currently Infeasible with Cryptography N. Carlini, M. Nasr, C. A. Choquette-Choo, M. Jagielski, I. Gao, A. Awadalla, P. W. Koh, D. Ippolito, K. Lee, F. Tramer, and L. Schmidt. Are aligned neural networks adversarially aligned?, 2024. URL https://arxiv.org/abs/2306.15447. B. Chen, B. B칲nz, D. Boneh, and Z. Zhang. HyperPlonk: Plonk with linear-time prover and high-degree custom gates. Cryptology ePrint Archive, Paper 2022/1355, 2022. URL https://eprint.iacr. org/2022/1355. Gemini-Team. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context, 2024. URL https://arxiv.org/abs/2403.05530. Gemma-Team. Gemma: Open models based on gemini research and technology, 2024. URL https: //arxiv.org/abs/2403.08295. S. Ghalebikesabi, E. Bagdasaryan, R. Yi, I. Yona, I. Shumailov, A. Pappu, C. Shi, L. Weidinger, R. Stanforth, L. Berrada, P. Kohli, P.-S. Huang, and B. Balle. Operationalizing contextual integrity in privacy-conscious assistants, 2024. URL https://arxiv.org/abs/2408.02373. D. Glukhov, I. Shumailov, Y. Gal, N. Papernot, and V. Papyan. Position: Fundamental limitations of LLM censorship necessitate new approaches. In Forty-first International Conference on Machine Learning, 2024. URL https://openreview.net/forum?id=j5csKrtyAe. J. Groth. On the size of pairing-based non-interactive arguments. Cryptology ePrint Archive, Paper 2016/260, 2016. URL https://eprint.iacr.org/2016/260. I. Ioannidis and A. Grama. An efficient protocol for yaos millionaires problem. In 36th Annual Hawaii International Conference on System Sciences, 2003. Proceedings of the, pages 6 pp., 2003. doi: 10.1109/HICSS.2003.1174464. C. Kocaogullar, T. Marjanov, I. Petrov, B. Laurie, A. Cutter, C. Kern, A. Hutchings, and A. R. Beresford. confidential computing transparency framework for comprehensive trust chain, 2024. URL https://arxiv.org/abs/2409.03720. H.-Y. Lin and W.-G. Tzeng. An efficient solution to the millionaires problem based on homomorphic encryption. In Applied Cryptography and Network Security: Third International Conference, ACNS 2005, New York, NY, USA, June 7-10, 2005. Proceedings 3, pages 456466. Springer, 2005. W. Nguyen, T. Datta, B. Chen, N. Tyagi, and D. Boneh. Mangrove: scalable framework for foldingbased SNARKs. Cryptology ePrint Archive, Paper 2024/416, 2024. URL https://eprint.iacr. org/2024/416. J. Thaler. Proofs, arguments, and zero-knowledge. Foundations and Trends in Privacy and Security, 4 (24):117660, 2022. ISSN 2474-1558. doi: 10.1561/3300000030. A. C. Yao. Protocols for secure computations . In 2013 IEEE 54th Annual Symposium on Foundations of Computer Science, pages 160164, Los Alamitos, CA, USA, Nov. 1982. IEEE Computer Society. doi: 10.1109/SFCS.1982.88. URL https://doi.ieeecomputersociety.org/10.1109/ SFCS.1982.88."
        }
    ],
    "affiliations": [
        "Google",
        "Google DeepMind",
        "Google Research"
    ]
}