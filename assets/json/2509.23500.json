{
    "paper_title": "Beyond Outliers: A Study of Optimizers Under Quantization",
    "authors": [
        "Georgios Vlassis",
        "Saleh Ashkboos",
        "Alexandra Volkova",
        "Torsten Hoefler",
        "Dan Alistarh"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As new optimizers gain traction and model quantization becomes standard for efficient deployment, a key question arises: how does the choice of optimizer affect model performance in the presence of quantization? Despite progress in both areas, systematic evidence on optimizer-quantization interactions remains limited. To fill this gap, we study the impact of optimizer choice on model robustness under quantization, considering both post-training quantization (PTQ), and quantization-aware training (QAT). We first train full-precision models, ranging from 50M to 1.5B parameters, with six optimizers, to explore the hyperparameter landscape, and establish well-tuned baselines. We then apply PTQ to evaluate how model performance degrades when trained with different optimizers. We find that outlier-related metrics, such as the max-to-mean ratio (MMR) and Kurtosis, fail to predict the PTQ performance across different optimizers. We show analytically that this is due to the MMR capturing only isolated layer errors, while ignoring how quantization errors accumulate and propagate through the network. To study the QAT degradation, we train quantized models from scratch and compare them to our original-precision baselines. We find that optimizers performing well in the original pretraining setup may not remain optimal under QAT, and that models trained with Shampoo show the lowest accuracy degradation. Finally, we derive scaling laws for quantization-aware training under different optimizers, showing that Shampoo achieves the highest parameter efficiency of all tested optimizers."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 ] . [ 2 0 0 5 3 2 . 9 0 5 2 : r BEYOND OUTLIERS: STUDY OF OPTIMIZERS UNDER QUANTIZATION Georgios Vlassis ETH Zurich gvlassis@ethz.ch Saleh Ashkboos ETH Zurich saleh.ashkboos@inf.ethz.ch Alexandra Volkova ISTA avolkova@ist.ac.at Torsten Hoefler ETH Zurich htor@ethz.ch Dan Alistarh ISTA & Red Hat AI dan.alistarh@ist.ac.at"
        },
        {
            "title": "ABSTRACT",
            "content": "As new optimizers gain traction and model quantization becomes standard for efficient deployment, key question arises: how does the choice of optimizer affect model performance in the presence of quantization? Despite progress in both areas, systematic evidence on optimizerquantization interactions remains limited. To fill this gap, we study the impact of optimizer choice on model robustness under quantization, considering both post-training quantization (PTQ), and quantization-aware training (QAT). We first train full-precision models, ranging from 50M to 1.5B parameters, with six optimizers, to explore the hyperparameter landscape, and establish well-tuned baselines. We then apply PTQ to evaluate how model performance degrades when trained with different optimizers. We find that outlier-related metrics, such as the max-to-mean ratio (MMR) and Kurtosis, fail to predict the PTQ performance across different optimizers. We show analytically that this is due to the MMR capturing only isolated layer errors, while ignoring how quantization errors accumulate and propagate through the network. To study the QAT degradation, we train quantized models from scratch and compare them to our original-precision baselines. We find that optimizers performing well in the original pretraining setup may not remain optimal under QAT, and that models trained with Shampoo show the lowest accuracy degradation. Finally, we derive scaling laws for quantization-aware training under different optimizers, showing that Shampoo achieves the highest parameter efficiency of all tested optimizers."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large language models (LLMs) based on the Generative Pretrained Transformer (GPT) architecture have billions of parameters, and this number continues to grow, making both training and deployment increasingly challenging. Quantization is common technique to make the training and deployment of large-scale neural networks more efficient. In general, LLM quantization can be categorized into two types: Post-Training Quantization (PTQ), where model is trained in full precision and then quantized, and Quantization-Aware Training (QAT), where quantization is incorporated directly during training. The effectiveness of both methods ultimately depends on how well models can maintain accuracy in the face of quantization-induced degradation. Post-training quantization (PTQ) is one of the most important techniques to address the challenges of LLM deployment. To accelerate the inference, both weights and inputs (also known as activation) are quantized, enabling most computations to be performed in low precision. This technique is known as joint quantization (Ashkboos et al., 2024; 2023; Xiao et al., 2023; Liu et al., 2024). Several studies (Wei et al., 2022; Bondarenko et al., 2023) have shown that joint quantization is challenging due to the presence of outlier features (or OF) in the input matrices. To mitigate OF, various approaches have been proposed, such as reducing kurtosis (Nrusimha et al., 2024; Akhondzadeh et al., Equal contribution. 1 Figure 1: Accuracy correlation with different metrics for the 760M model. Traditional outliersensitive metrics like MMR (Left) and kurtosis (Center) show little to no correlation (measured by œÅ) with model accuracy, whereas our proposed metric (Right) correlates strongly with the models zero-shot performance. MMR and kurtosis are computed row-wise, on the output of the last transformer block. 2025) or the Max-Median Ratio (MMR) (He et al., 2024) prior to PTQ, often through techniques like rotations (Ashkboos et al., 2024) or architectural modifications (He et al., 2024). However, it remains unclear how the optimization process during pretraining affects the presence of such outlier features in the resulting model. Quantization-aware training (QAT) is another important technique for reducing the deployment cost of large language models (LLMs). Unlike post-training quantization, QAT applies quantization during training, allowing the model to adapt to low-precision computations (Panferov et al., 2025a; Ashkboos et al., 2025; Xi et al., 2024). Typically, the forward pass is quantized while the backward pass remains in high precision, with the gradient of the rounding function estimated using the Straight-Through Estimator (Bengio et al., 2013). Most existing works adopt the same optimization process as in full-precision training, and AdamW remains the most commonly used optimizer. Yet, the impact of using different optimizers on the final performance of QAT remains unclear. Over the last decade, Adam (Kingma & Ba, 2015) and AdamW (Loshchilov & Hutter, 2017) have become the default optimizers for LLM training due to their simplicity and stability. At the same time, several alternative optimizers, such as PSGD (Li, 2015), Shampoo (Gupta et al., 2018), Muon (Jordan, 2024), SOAP (Vyas et al., 2025), and Scion (Pethick et al., 2025), have been proposed. Recently, concurrent studies (Wen et al., 2025; Semenov et al., 2025) have benchmarked optimizers in practice by training models with different hyperparameter settings to evaluate their empirical benefits. These studies are restricted to high-precision models, and despite advances in both optimization and quantization techniques, their interaction remains underexplored. In this work, we investigate the gap between optimization and quantization of LLMs for efficient deployment. For post-training quantization (PTQ), we investigate the question: Do two models with the same validation loss but trained with different optimizers perform similarly under PTQ? To answer this, we train the same model to the same validation loss using different optimizers and compare the accuracy drop after quantization. We then evaluate the impact of different optimizers during quantization-aware training (QAT), asking: How sensitive is QAT to the choice of optimizerdoes the optimizer that performs best in full precision maintain its advantage under QAT? Finally, we explore How well do our findings transfer to larger models? and provide scaling law for QAT performance across different optimizers. Our contributions are as follows: 1. We present the first systematic study of the interaction between optimization and quantization, by training models ranging from 50M to 1.5B parameters with six different optimizers, and evaluating full-precision training (FP), post-training quantization (PTQ), and quantization-aware training (QAT) schemes. 2. For FP training, we sweep over different hyperparameters for each optimizermodel combination, and train each using the Chinchilla-optimal data-to-model ratio. We find that Muon outperforms other optimizers across nearly all model sizes. 3. For PTQ, we find that full-precision accuracy does not correlate with quantized recovery, nor with standard metrics of outlier quantification, challenging current understanding. Instead, we provide first theoretical analysis of error propagation during quantization, and 2 propose new metric for predicting the accuracy of quantized models. In particular, we demonstrate that models trained with Shampoo are more robust to quantization. 4. For QAT, we again observe that optimizers performing best in full precision are not necessarily optimal under QAT. Among all optimizers, the model trained with Shampoo yields the lowest accuracy degradation compared to its high-precision counterpart in almost all of the model sizes. We further provide scaling law for different optimizers under 4-bit QAT and show that Shampoo achieves the highest parameter efficiency, supporting the transferability of our results to larger scales."
        },
        {
            "title": "2 BACKGROUND AND EXPERIMENTAL SETUP",
            "content": "In this section, we first present the model architecture used in our experiments and the optimizers applied during training. We then describe the hyperparameter tuning strategy employed in our experiments. Finally, we provide the training and evaluation details for both PTQ and QAT. Model Architecture. We use the OLMo2 architecture (OLMo et al., 2024), which integrates several recent architectural improvements. In particular, OLMo2 combines no biases, rotary positional embeddings (Su et al., 2024), RMSNorm (Zhang & Sennrich, 2019), reordered pre-normalization (Liu et al., 2022; OLMo et al., 2024) and QKNorm (Henry et al., 2020). Our only modifications on top of OLMo2 are the tying of the input and output weights, and the use of the ReLU2 activation function (So et al., 2022). We train models with 50M, 125M, 350M, 500M, 760M and 1.5B parameters, and we list the exact architectural details in Table 1. Optimizers. We evaluate six optimizers. AdamW serves as the standard baseline for training LLMs. To capture curvature-aware methods, we include PSGD and Shampoo. In addition, we also consider Muon and Scion, as methods with motivations rooted in feature learning (Yang et al., 2023). Lastly, we include SOAP, which operates by rotating gradients from and to different eigenspace. We provide complete analysis of the computational complexity and memory overhead of each optimizer for linear layer, with details provided in Appendix A.5. Experimental Setup and Hyper-parameter Protocol. As full-precision baseline, we train models in BFloat16 with the following protocol for hyper-parameter selection: 1. Optimizer Parameters. We initialize each optimizer with hyperparameters recommended in its original paper and perform sequential one-dimensional sweeps to determine the optimal settings. These sweeps are conducted on the smallest 50M model, and the resulting configurations are applied across all experiments. 2. Learning Rate Selection. After fixing the optimizer hyperparameters, we sweep over eight different learning rate values and fully train each modeloptimizer pair to find the optimal learning rate. For the 1.5B model, we select the optimal learning rate for each optimizer based on results from the 760M model, and then test four additional values (two higher and two lower) to identify the best learning rate. We train our 1.5B model using AdamW (as the baseline), Muon (the best-performing optimizer without quantization), and Shampoo (the most effective optimizer for preserving accuracy under quantization). Datasets, Training Steps, and Evaluation Taks. We train all models on the ClimbMix dataset (Diao et al., 2025), high-quality mixture of 400B tokens, following the Chinchilla training regime. For each model size, we define the Common Loss (or CL) as the lowest evaluation loss achieved by all optimizers using their best hyperparameters with at most 20x token-to-parameter ratio. This CL checkpoint is then used for applying the PTQ scheme. For each modeloptimizer pair, we select the best hyperparameters from high-precision experiments and perform quantized training with fixed 20x token-to-parameter ratio. We evaluate our models on three standard zero-shot benchmarks that offer early signal: PIQA (Bisk et al., 2020), HellaSwag (Zellers et al., 2019), and ARC-Easy (Clark et al., 2018). We report the average accuracy across these tasks. All evaluations are performed using the LM Evaluation Harness (Gao et al., 2021) with default parameter settings. We always use 8196 samples from the test set of ClimbMix dataset to calculate the statistics, each with 1024 tokens. 3 Model Exact Num. Parameters Blocks (L) heads KV heads Width (d) Toks Iters. 50M 125M 350M 500M 760M 1.5B 49,748,760 121,813,686 225,335,892 477,024,972 729,974,655 1,489,423,554 4 6 11 17 17 25 6 9 12 12 15 18 2 3 4 4 5 768 1152 1536 1536 1920 2304 1B 2B 6B 10B 14B 30B 2,000 4,000 12,000 20,000 28,000 60,000 CL 3.73 3.32 2.97 2.80 2.75 2.57 Table 1: Training configurations used for our experiments. CL denotes the common loss, which is the lowest validation loss that all optimizers can achieve for given model size and number of tokens (used for the PTQ experiments). Quantization. We apply the round-to-nearest-integer scheme for PTQ experiments. For each tensor, every row is normalized by its largest magnitude value and then scaled by the largest representable number, method known as symmetric AbsMax row-wise quantization. We use 4-bit quantization for both the weights and inputs of all linear layers, while keeping the remaining modules in their original precision. For QAT, we train our models using QuEST (Panferov et al., 2025b), state-of-the-art stable quantization-aware training scheme for very low precisions (e.g., 4-bit). In the forward pass, QuEST applies the Hadamard transform to both the inputs and weights of linear layers, followed by the selection of an optimal clipping ratio to minimize MSE before quantization. During the backward pass, it masks gradients that deviate significantly from the clipping value. We keep the backward pass (and non-linear modules) in the original precision following standard QAT."
        },
        {
            "title": "3 RESULTS AND ANALYSIS",
            "content": "In this section, we present our main results, analyses, and findings from the experiments. We begin with full-precision training, which serves as the baseline for our study and show that the outlierrelated metric MMR correlates with learning rates across different optimizers. Next, we report the results of post-training quantization (PTQ) and provide theoretical analysis of error propagation using our ABC decomposition framework. Finally, we present the results of our quantization-aware training (QAT) experiments, along with the corresponding scaling laws. 3.1 FULL-PRECISION Table 2 shows the zero-shot accuracies of our (unquantized) models across different optimized learningrates. Except for the 50M and 125M models, Muon consistently outperforms other optimizers in zero-shot accuracy, aligning with the concurrent study of Wen et al. (2025). The performance gap widens for larger models (from 0.01% in the 350M to 1.03% in 1.5B model), indicating that Muon is the best optimizer in high precision, when hyperparameters are tuned. We provide additional results in Appendix A.1. Optimizer Model Size 50M 125M 350M 500M 760M 1.5B AdamW 43.75 45.03 Muon 45.08 PSGD 45.41 Scion 44.81 Shampoo 45.73 SOAP 48.64 49.62 49.95 49.83 49.53 50.24 56.58 58.08 55.99 57.53 56.51 58.07 60.39 61.86 60.85 61.68 61.03 61.34 63.90 64.63 61.47 63.82 63.05 62.27 67.93 69.19 N/A N/A 68.16 N/A Table 2: Average zero-shot accuracy ( ) for the fullprecision models, trained with BFloat16. Results for the test loss, which show similar trends, can be found in Table 5. We also investigate how varying the learning rate impacts both the validation loss and the max-tomedian ratio (MMR), which is used to study the outlier patterns, in Figure 2. Our analysis reveals clear trend: increasing the learning rate consistently leads to higher MMR values, independent of the optimizer used. This suggests that larger learning rates tend to amplify the relative difference between the maximum and median value in the input tensor, potentially indicating larger outliers. Furthermore, across all optimizers in our experiments, Muon consistently achieves the lowest MMR. We provide results for other models in Appendix A.1. 4 Figure 2: The effect of changing learning-rate Œ∑ on the validation loss and MMR in different optimizers for 760M model. We average the MMR over the rows in the input tensor of the last linear layer (before head)."
        },
        {
            "title": "3.2 POST-TRAINING QUANTIZATION",
            "content": "In the previous section, we showed that models trained with different optimizers yield substantially different Max-to-Median Ratio (MMR) values. We would thus expect models with higher MMR (larger outlier features) to exhibit more severe degradation than networks with lower MMR. Here, we examine to what extent this intuition holds in practice. To facilitate comparison, we train all models to common loss (CL), defined as the lowest validation loss achievable by all optimizers, given the same number of tokens. Since all the networks were trained to the same CL, the downstream performance before quantization is very similar. Hence, instead of measuring the performance degradation, we can equivalently look at the final downstream performance. According to our results, presented in Table 3, the optimizer that consistently leads to the least degraded networks, at least for model sizes above 125M, is Shampoo. This is counter-intuitive, since Shampoos networks are also characterized by the highest MMR. Following folklore knowledge, this would imply that it would deteriorate the most under quantization. Also noteworthy is the fact that optimizers with low MMR (e.g. Muon), that would be expected to perform near the top, show significant decline in accuracy. Plotting the row-wise MMR of the output of the last transformer block against the average downstream accuracy after PTQ, as in Figure 1-left, reveals that the two metrics are uncorrelated. Changing the metric we use to quantify outliers from MMR to kurtosis, as in Figure 1-center, does not change the pattern. To understand this discrepancy, we introduce new metric, which not only correlates strongly with the PTQ performance, but also allows us to track the quantization error through the network, as well as decompose it into independent and interpretable terms. Theoretical analysis. We start with brief description of the setup for which our analysis works, and present concise summary of the main results. full derivation can be found in Section A.4. } { We consider Feed-Forward Network (FFN) with modules f‚Ñì( ), for ‚Ñì . We denote 1, . . . , the activations of the network with Rn‚Ñì , for which h‚Ñì = f‚Ñì(h‚Ñì h‚Ñì 1), with h0 being the input of the network. Notably, this construction allows for any arbitrary transformation linear layers, convoluf‚Ñì( tions, self-attention etc.), at any level of granularity. ) (e.g. Optimizer Model Size 50M 125M 350M 500M 760M 1.5B AdamW 40.23 41.88 Muon 42.69 PSGD 42.44 Scion 40.84 Shampoo 43.36 SOAP 45.15 45.23 47.39 46.04 45.68 46.35 49.23 47.42 50.09 49.80 53.93 49.08 55.17 50.60 54.01 52.14 55.65 50.91 59.22 50.00 52.11 53.74 59.26 46. 62.51 47.75 N/A N/A 63.88 N/A Table 3: Average zero-shot accuracy ( ) after we applied PTQ (row-wise W4A4) to the networks with the same common-loss (CL). The pre-quantization models used here have the same validation loss (which are shown in Table 1), and thus downstream performance. Thus, the postquantization performance is indicative of the degradation due to quantization. The equivalent table for test loss is in Section A.2. 5 Figure 3: ABC decomposition for the 760M models. The x-axis shows the module index ‚Ñì. Here is the only exception where we use the truncated average (average after we ignore the top 1% of the values) as our summary statistic. Trunc(R‚Ñì) and Avg(R‚Ñì) only differ non-negligibly for single intermediate layer for Shampoo, where spike in Avg(R‚Ñì) would force us to use log-scale. Now assume that we quantize the modules of the network. We can choose to quantize part or all of the modules, with any arbitrary PTQ scheme. If we denote the quantized modules with 1). Thus, there is both change in the input, from h‚Ñì ). 1, as well as change in the function, from f‚Ñì( The change in the input h‚Ñì 1 comes from the propagation of the quantization error through the previous (‚Ñì ) is the newly introduced layerwise perturbation. 1 to hq ‚Ñì 1) modules, while the change in the function f‚Ñì( ‚Ñì , the quantized activations become hq ) to ‚Ñì ( ‚Ñì = ‚Ñì (hq ‚Ñì We aim to precisely quantify these changes. To do so, we study the difference in the activations h‚Ñì := hq h‚Ñì. We prove (Section A.4) that this difference can be written as h‚Ñì = a‚Ñì + b‚Ñì, with: ‚Ñì Change in the input under ‚Ñì ( (cid:122) (cid:125)(cid:124) (f ‚Ñì (hq ‚Ñì (h‚Ñì 1) ‚Ñì ) (cid:123) 1)) + 2 Change in the input under f‚Ñì( (cid:125)(cid:124) (cid:122) (f‚Ñì(hq ‚Ñì f‚Ñì(h‚Ñì 1) ) (cid:123) 1)) , Change in the function under hq (cid:125)(cid:124) (cid:122) (f f‚Ñì(hq ‚Ñì ‚Ñì (hq 1) ‚Ñì ‚Ñì1 (cid:123) 1)) + 2 Change in the function under h‚Ñì1 (cid:123) (cid:125)(cid:124) (cid:122) (f 1)) f‚Ñì(h‚Ñì ‚Ñì (h‚Ñì 1)) . a‚Ñì := b‚Ñì := From the definitions, we see that a‚Ñì is the average of the change in the input under ‚Ñì ( change in the input under f‚Ñì( under hq ‚Ñì in the input, while b‚Ñì encodes the change in the function. ), and the ). On the other side, b‚Ñì is the average of the change in the function 1. Hence, a‚Ñì captures the effect of the change 1, and the change in the function under h‚Ñì In order to reduce h‚Ñì to an interpretable number, we compute the L2-norm. Since h‚Ñì be interpreted relative to the scale of the initial unquantized activations, we also normalize by The norm of the relative change in the inputs is then: should h‚Ñì . h‚Ñì r‚Ñì := h‚Ñì = a‚Ñì + b‚Ñì h‚Ñì . To be able to use the Law of Cosines (which we use to prove the following decomposition), we instead study the square of this quantity R‚Ñì := r2 ‚Ñì . Intuitively, this quantity tracks the deviation of the quantized network from the original one across its layers. Hence, we would expect RL at the output of the network to correlate with loss degradation. 6 Figure 4: Gain decomposition for the 760M models with different optimizers. The x-axis shows the module index ‚Ñì. For any module of the network, the square of the norm of the relative change in the activations R‚Ñì can be decomposed exactly into three terms (see Section A.4): (cid:18) (cid:19) a‚Ñì a‚Ñì, b‚Ñì , C‚Ñì := 2 h‚Ñì h‚Ñì Here A‚Ñì reflects the quantization error accumulated from the previous (‚Ñì 1) modules, B‚Ñì expresses the quantization error introduced in the ‚Ñì-th layer, and C‚Ñì captures the interaction between the two. , B‚Ñì := A‚Ñì := 2 (cid:18) R‚Ñì = A‚Ñì + B‚Ñì + C‚Ñì b‚Ñì h‚Ñì (cid:19)2 To understand how module transforms the quantization error of the previous layer from R‚Ñì A‚Ñì, we can define the gain: 1 to G‚Ñì := A‚Ñì R‚Ñì . 1 In general, without assumptions about the functional form of the module, we cannot further ana1 to calculate it). However, for linear layer under lyze G‚Ñì (even though we can use A‚Ñì and R‚Ñì weight and activation quantization with hq ‚Ñì = (W‚Ñì + ŒµW ‚Ñì are quantization noise introduced by weight and activation quantization respectively, there is very intuitive decomposition of G‚Ñì. Specifically, we can write: ‚Ñì , where ŒµW ‚Ñì and Œµh 1 + Œµh ‚Ñì )hq ‚Ñì G‚Ñì = G1,‚ÑìG2,‚Ñì, G1,‚Ñì := (cid:18) W‚Ñì + 1 2 ŒµW W‚Ñì ‚Ñì (cid:19) , G2,‚Ñì := (cid:19)2 (cid:18) cos œï‚Ñì cos œà‚Ñì where œï‚Ñì is the angle1 between the change in activations h‚Ñì tization, and œà‚Ñì is the angle between the original activations and the unquantized weights. 1 and the weight matrix after quanThe term G1,‚Ñì, which we call the spectral ratio, reflects the change in the spectral norm of the weights due to quantization. We call the term G2,‚Ñì the alignment ratio, since cos œï‚Ñì [0, 1] is the alignment between the change in the activations and the weights after quantization, while [0, 1] is the alignment between the incoming activations and the original weights. cos œà‚Ñì Empirical Validation. To validate our analysis, we monitor R‚Ñì along the residual path of the transformer. We analyze the models at their most granular level, treating layer normalizations, residual connections, linear layers and activation functions separately, with the exception of the multi-head self-attention (MHSA) module, which we consider as single unit2. In our framework, 1Here, we define the angle between matrix and vector as arccos( Ax Ax ). This is non-standard definition, that we nonetheless find useful to quantify how close is to maximizing Ax. Notably, this angle lies between 0 and œÄ 2 , in contrast to angles between vectors, which lie between 0 and œÄ 2We do this simply because MHSA internally has three branches, keys, queries and values, making it nonobvious what is considered the residual path. 7 Rn0 was assumed to be vector. Because transformers process multiple vectors the input h0 (corresponding to tokens), we need to use summary statistic. As long as the statistic is linear, the ABC decomposition still holds with Stat(R‚Ñì) = Stat(A‚Ñì) + Stat(B‚Ñì) + Stat(C‚Ñì). Unless otherwise noted, we use the average as our summary statistic. First, we confirm our intuition that RL should predict the degradation due to quantization in Figure 1(right). This suggests that R‚Ñì will be an informative quantity about how the quantization error propagates through the network. Figure 3 shows the ABC decomposition for the 760M models. Here, we make two observations. First, in nearly all the cases, R‚Ñì is dominated by A‚Ñì, while B‚Ñì and C‚Ñì play minor role. That is, even if MMR is good predictor of the quantization error introduced by quantizing specific layer, the total quantization error, represented by R‚Ñì, is mostly determined by the amplified error from the previous module, represented by A‚Ñì. Second, networks trained with different optimizers exhibit very different error propagation behaviors. Even though all the networks exhibit oscillatory patterns with the R‚Ñì generally increasing with depth, some optimizers (e.g. AdamW, Scion) lead to networks with prominent spikes toward the end, while others (e.g. PSGD, Muon) have flatter profile. Finally, we measure the gain and its related quantities in Figure 4. We separately show the gain of non-linear modules, and the gain of the linear layers, which we can analyze to G1,‚Ñì and G2,‚Ñì. In addition, we show the alignment factors cos œï‚Ñì and cos œà‚Ñì. We notice that, in the non-linear modules, different optimizers produce very similar G‚Ñì profiles, with the exception of Shampoo, which exhibits not only two upward spikes but also sharp downward drop. In contrast, the patterns observed for linear layers are more distinguishable for different optimizers. Notably, Muon has the highest gain for linear layers, possibly explaining its sharp quality degradation despite that it has relatively low MMR. On the other hand, AdamW, along with Shampoo, have the lowest gains. As for the decomposition of the gain for linear layers, we see that the spectral ratio is close to 1 for all optimizers. That is, the spectral norm of the quantized weights is not significantly different than that of the original weights. Hence, G‚Ñì is almost entirely determined by G2,‚Ñì. Looking at cos œï‚Ñì, we see that for Shampoo, and even more so for AdamW, the change in the activations is less aligned with the quantized weights. The other optimizers mostly follow the same trends. Lastly, the factor cos œà‚Ñì is similar across most optimizers with the exception of AdamW, for which the input activations are even less aligned with the weights. 3.3 QUANTIZATION-AWARE TRAINING AND SCALING LAW Finally, we study quantization-aware training (QAT), where we empirically investigate the extent of performance degradation under QAT compared to full-precision training, and how strongly this degradation depends on the choice of optimizer. We then provide scaling law for QAT experiments to examine the transferability of our results to larger-scale models. Empirical Results. We use the state-of-the-art 4-bit QAT QuEST scheme (Panferov et al., 2025a), where weights and activations are quantized to 4-bits on the forward pass using row-wise symmetric quantization. We train each modeloptimizer pair under the same compute budget, following the Chinchilla-optimal ratio (Hoffmann et al., 2022), using the best full-precision (FP) hyperparameters. We report the QAT evaluation losses across model sizes and optimizers in Table 7, as well as averaged zero-shot accuracy results (together with relative differences to their FP-baseline) in Table 4. First, we see that the ranking of the optimizers differs from the full-precision regime in almost all of the model sizes: no single best-performing optimizer could be identified for quantizationaware training. However, Shampoo remains the most effective optimizer in minimizing accuracy degradation during quantization-aware training. Optimizers differ in how much accuracy they lose compared to their full-precision baseline, which is why promising results of an optimizer in FP do not always lead to its strong performance under QAT. Overall, we find that full-precision results do not reliably predict QAT behavior. Scaling Laws. To provide comprehensive comparison of QAT performance, we employ scaling laws. For each optimizer, the scaling law takes the form of Equation (1) from (Hoffmann et al., 2022), where is final test loss (cross-entropy), is model parameter count and is number of 8 Optimizer Model Size 50M 125M 350M 500M 760M 1.5B AdamW 43.37 ( 0.87) 44.07 ( 2.13) Muon 44.33 ( 1.66) PSGD 44.86 ( 1.21) Scion 43.80 ( 2.23) Shampoo 44.41 ( 2.89) SOAP 48.08 ( 1.15) 48.33 ( 2.60) 48.82 ( 2.26) 48.20 ( 3.27) 48.31 ( 2.46) 49.38 ( 1.71) 54.64 ( 3.43) 55.19 ( 4.98) 51.25 ( 8.47) 56.01 ( 2.64) 55.02 ( 2.64) 56.61 ( 2.51) 60.07 ( 0.53) 61.05 ( 0.73) 60.32 ( 0.87) 60.72 ( 1.56) 60.80 ( 0.38) 60.58 ( 1.24) 62.22 ( 2.63) 62.32 ( 3.57) 60.50 ( 1.58) 62.30 ( 2.38) 62.76 ( 0.46) 61.79 ( 0.77) 66.82 ( 1.63) 67.08 ( 2.11) N/A N/A 67.34 ( 1.20) N/A ) for the models trained with 4-bit QAT via QuEST. We Table 4: Average zero-shot accuracy ( report the difference in accuracy relative to the full-precision baseline in the brackets. We bold the best accuracy and the smallest degradation for each model size. The corresponding results for test loss are presented in Table 7. Figure 5: Scaling Laws for each optimizer, for full precision (BF16) and QAT (W4A4). For each optimizer, we report the parameter efficiency œÅ of 4-bit QAT in the subplot title. Shampoo has the highest parameter efficiency, followed by AdamW. training tokens: = (N œÅ)Œ± + DŒ≤ + E. (1) We fit distinct sets of hyperparameters for each optimizer. To account for the QAT results we use the per-optimizer parameter efficiency œÅ, introduced by (Kumar et al., 2024), as an extra hyperparameter in the law. By convention, we set œÅ = 1 for full-precision training, and we denote œÅ4bit the parameter efficiency in QAT W4A4 regime. Intuitively, an optimizer with higher œÅ has higher parameter efficiency under QAT. For example, model of size trained in 4 bits QAT performs equivalently to the model of size œÅ4bit Since our experiments use fixed we report the coefficients of iso-compute law: = 20, the full form of the law would be under-determined, and trained in full-precision. = (N œÅ)Œ± + E. (2) Here, the parameters and Œ± describe the scale and the curvature of the power law, and corresponds to the irreducible error. We estimate parameters of the law with robust non-linear Huber loss 3) on (log N, log D, log L) manifold, and report the values with confidence intervals for (Œ¥ = 10 each optimizer, obtained by leave-one-out cross-validation, in Appendix A.6. Figure 5 shows our fitted curves for different optimizers. The single scalar œÅ4bit serves quantitative measure of robustness to quantization: higher value means the quantized model retains larger effective parameter capacity. Similar to our QAT results, we can see that among the optimizers evaluated in this study, Shampoo yields the highest œÅ4bit, indicating that it is the most resilient to quantization and preserves model performance more effectively. Also, the top optimizers in terms of parameter efficiency (Shampoo, AdamW, SCION) are also leading PTQ recovery  (Table 3)  ."
        },
        {
            "title": "4 CONCLUSION AND FUTURE WORK",
            "content": "In this paper, we present systematic study of the role of optimizer choice in the accuracy drop during quantization. First, we establish full-precision baseline by training single model with multiple optimizers in the original precision, tuned over eight different learning rates and optimized hyperparameters. We then apply post-training quantization, showing that existing outlier-centric metrics like max-to-median ratio (MMR) or Kurtosis, used in previous works, are not predictive of PTQ accuracy, and introducing new metric that correlates well with the accuracy of the quantized model. Finally, we examine the impact of optimizer choice for quantization-aware training (QAT), and demonstrate that the optimizer performing best in higher precision may not remain optimal in the quantized setting. We further support our finding by deriving scaling laws for each optimizer under QAT. There are still opportunities to further extend our study. We plan to investigate additional bitwidths (e.g., 8-bit and 6-bit) for QAT, as well as alternative PTQ schemes, and to study the error propagation of these methods using our ABC decomposition framework from Section 3.2. Studying the error propagation of other 4-bit data-types, like micro-scaling format, is also interesting. Lastly, deriving the gain decomposition analysis for other modules in the network (like self-attention) would be very natural extension of our work."
        },
        {
            "title": "REFERENCES",
            "content": "Mohammad Sadegh Akhondzadeh, Aleksandar Bojchevski, Evangelos Eleftheriou, and Martino Dazzi. Kurtail: Kurtosis-based llm quantization. arXiv preprint arXiv:2503.01483, 2025. Saleh Ashkboos, Ilia Markov, Elias Frantar, Tingxuan Zhong, Xincheng Wang, Jie Ren, Torsten Hoefler, and Dan Alistarh. Quik: Towards end-to-end 4-bit inference on generative large language models. arXiv preprint arXiv:2310.09259, 2023. Saleh Ashkboos, Amirkeivan Mohtashami, Maximilian Croci, Bo Li, Pashmina Cameron, Martin Jaggi, Dan Alistarh, Torsten Hoefler, and James Hensman. Quarot: Outlier-free 4-bit inference in rotated llms. Advances in Neural Information Processing Systems, 37:100213100240, 2024. Saleh Ashkboos, Mahdi Nikdan, Soroush Tabesh, Roberto Castro, Torsten Hoefler, and Dan arXiv preprint Alistarh. Halo: Hadamard-assisted lower-precision optimization for llms. arXiv:2501.02625, 2025. Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In Thirty-Fourth AAAI Conference on Artificial Intelligence, 2020. Yelysei Bondarenko, Markus Nagel, and Tijmen Blankevoort. Quantizable transformers: Removing outliers by helping attention heads do nothing. Advances in Neural Information Processing Systems, 36:7506775096, 2023. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. ArXiv, abs/1803.05457, 2018. URL https://api.semanticscholar.org/ CorpusID:3922816. Shizhe Diao, Yu Yang, Yonggan Fu, Xin Dong, Dan Su, Markus Kliegl, Zijia Chen, Peter Belcak, Yoshi Suhara, Hongxu Yin, et al. Climb: Clustering-based iterative data mixture bootstrapping for language model pre-training. arXiv preprint arXiv:2504.13161, 2025. Leo Gao, Jonathan Tow, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Kyle McDonell, Niklas Muennighoff, et al. framework for few-shot language model evaluation. Version v0. 0.1. Sept, 2021. Vineet Gupta, Tomer Koren, and Yoram Singer. Shampoo: Preconditioned stochastic tensor opIn International Conference on Machine Learning, 2018. URL https://api. timization. semanticscholar.org/CorpusID:3585068. Bobby He, Lorenzo Noci, Daniele Paliotta, Imanol Schlag, and Thomas Hofmann. Understanding and minimising outlier features in transformer training. Advances in Neural Information Processing Systems, 37:8378683846, 2024. Alex Henry, Prudhvi Raj Dachapally, Shubham Pawar, and Yuxuan Chen. Query-key normalization for transformers. arXiv preprint arXiv:2010.04245, 2020. Jordan Hoffmann, Sebastian Borgeaud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, Tom Hennigan, Eric Noland, Katie Millican, George van den Driessche, Bogdan Damoc, Aurelia Guy, Simon Osindero, Karen Simonyan, Erich Elsen, Oriol Vinyals, Jack W. Rae, and Laurent Sifre. Training compute-optimal large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2022. Keller Jordan. Muon: An optimizer for the hidden layers of neural networks. https:// kellerjordan.github.io/posts/muon/, 2024. Diederik P. Kingma and Jimmy Ba. Adam: Method for Stochastic Optimization. In Proceedings of the 3rd International Conference on Learning Representations (ICLR), 2015. URL https: //arxiv.org/abs/1412.6980. Preprint available at arXiv:1412.6980. 11 Tanishq Kumar, Zachary Ankner, Benjamin F. Spector, Blake Bordelon, Niklas Muennighoff, Mansheej Paul, Cengiz Pehlevan, Christopher Re, and Aditi Raghunathan. Scaling laws for precision, 2024. URL https://arxiv.org/abs/2411.04330. Tim Large, Yang Liu, Minyoung Huh, Hyojin Bahng, Phillip Isola, and Jeremy Bernstein. Scalable optimization in the modular norm. Advances in Neural Information Processing Systems, 37: 7350173548, 2024. Xi-Lin Li. Preconditioned stochastic gradient descent. IEEE Transactions on Neural Networks and Learning Systems, 29:14541466, 2015. URL https://api.semanticscholar.org/ CorpusID:5011801. Ze Liu, Han Hu, Yutong Lin, Zhuliang Yao, Zhenda Xie, Yixuan Wei, Jia Ning, Yue Cao, Zheng Zhang, Li Dong, et al. Swin transformer v2: Scaling up capacity and resolution. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1200912019, 2022. Zechun Liu, Changsheng Zhao, Igor Fedorov, Bilge Soran, Dhruv Choudhary, Raghuraman Krishnamoorthi, Vikas Chandra, Yuandong Tian, and Tijmen Blankevoort. Spinquant: Llm quantization with learned rotations. arXiv preprint arXiv:2405.16406, 2024. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2017. URL https://api.semanticscholar.org/ CorpusID:53592270. Aniruddha Nrusimha, Mayank Mishra, Naigang Wang, Dan Alistarh, Rameswar Panda, and Yoon Kim. Mitigating the impact of outlier channels for language model quantization with activation regularization. arXiv preprint arXiv:2404.03605, 2024. Team OLMo, Pete Walsh, Luca Soldaini, Dirk Groeneveld, Kyle Lo, Shane Arora, Akshita Bhagia, Yuling Gu, Shengyi Huang, Matt Jordan, et al. 2 olmo 2 furious. arXiv preprint arXiv:2501.00656, 2024. Andrei Panferov, Jiale Chen, Soroush Tabesh, Roberto Castro, Mahdi Nikdan, and Dan AlarXiv preprint istarh. Quest: Stable training of llms with 1-bit weights and activations. arXiv:2502.05003, 2025a. Andrei Panferov, Jiale Chen, Soroush Tabesh, Mahdi Nikdan, and Dan Alistarh. QuEST: Stable training of LLMs with 1-bit weights and activations. In Forty-second International Conference on Machine Learning, 2025b. URL https://openreview.net/forum?id=I0Ux2nAN6u. Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, and Volkan Cevher. Training deep learning models with norm-constrained LMOs. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/ forum?id=2Oqm2IzTy9. Andrei Semenov, Matteo Pagliardini, and Martin Jaggi. Benchmarking optimizers for large language model pretraining. arXiv preprint arXiv:2509.01440, 2025. David R. So, Wojciech Manke, Hanxiao Liu, Zihang Dai, Noam Shazeer, and Quoc V. Le. Primer: Searching for efficient transformers for language modeling, 2022. URL https://arxiv. org/abs/2109.08668. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Nikhil Vyas, Depen Morwani, Rosie Zhao, Itai Shapira, David Brandfonbrener, Lucas Janson, and Sham M. Kakade. SOAP: Improving and stabilizing shampoo using adam for language modeling. In The Thirteenth International Conference on Learning Representations, 2025. URL https: //openreview.net/forum?id=IDxZhXrpNf. Xiuying Wei, Yunchen Zhang, Xiangguo Zhang, Ruihao Gong, Shanghang Zhang, Qi Zhang, Fengwei Yu, and Xianglong Liu. Outlier suppression: Pushing the limit of low-bit transformer language models. Advances in Neural Information Processing Systems, 35:1740217414, 2022. Kaiyue Wen, David Hall, Tengyu Ma, and Percy Liang. Fantastic pretraining optimizers and where to find them. arXiv preprint arXiv:2509.02046, 2025. Haocheng Xi, Yuxiang Chen, Kang Zhao, Kai Jun Teh, Jianfei Chen, and Jun Zhu. Jetfire: Efficient and accurate transformer pretraining with int8 data flow and per-block quantization. arXiv preprint arXiv:2403.12422, 2024. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: In International Accurate and efficient post-training quantization for large language models. conference on machine learning, pp. 3808738099. PMLR, 2023. Greg Yang, James Simon, and Jeremy Bernstein. spectral condition for feature learning. arXiv preprint arXiv:2310.17813, 2023. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830, 2019. Biao Zhang and Rico Sennrich. Root mean square layer normalization. Advances in Neural Information Processing Systems, 32, 2019."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 ADDITIONAL FULL-PRECISION RESULTS Optimizer Avg. Zero-Shot Acc.( 50M 125M 350M ) Test Loss( 500M ) 760M 1.5B AdamW 43.75 45.03 Muon 45.08 PSGD 45.41 Scion 44.81 Shampoo 45.73 SOAP 3.695 3.632 3.615 3.615 3.648 3.589 48.64 49.62 49.95 49.83 49.53 50. 3.318 3.263 3.283 3.269 3.311 3.241 56.58 58.08 55.99 57.53 56.51 58.07 2.961 2.915 2.978 2.932 2.959 2.916 60.39 61.86 60.85 61.68 61.03 61. 2.834 2.804 2.841 2.805 2.831 2.803 63.9 64.63 61.47 63.82 63.05 62.27 2.744 2.719 2.799 2.726 2.741 2.754 67.93 69.19 2.627 2. N/A N/A 68.16 2.622 N/A Table 5: Results for the best models trained with BFloat16. For each model, we show the average zero-shot accuracy ( ) on the right. As expected, the two track each other closely. While SOAP does better for small sizes, Muon is consistently the best for bigger models. ) on the left, and the test loss ( A.2 ADDITIONAL POST-TRAINING QUANTIZATION RESULTS Optimizer Model Size 50M 125M 350M 500M 760M 1.5B AdamW 5.445 4.305 Muon 4.102 PSGD 4.116 Scion 4.731 Shampoo 4.027 SOAP 4.233 4.145 3.704 3.803 4.786 3.790 4.274 4.941 3.948 3.951 3.246 4.222 3.730 4.819 3.878 4.471 5.219 3.987 3.480 4.928 3.776 3.759 3.217 6.320 3.769 9.725 N/A N/A 3.639 N/A Table 6: Test loss ( ) after we applied PTQ (row-wise W4A4) to the networks with the same common-loss (CL). The results mostly mirror Table 3, with Shampoo generally showing the mildest degradation. The exception is 500M, where Shampoo exhibits noticeable degradation in test loss, despite outperforming other optimizers in downstream tasks. A.3 ADDITIONAL QUANTIZATION-AWARE TRAINING RESULTS Optimizer Model Size 50M 125M 350M 500M 760M 1.5B AdamW 3.757 3.698 Muon 3.696 PSGD 3.703 Scion 3.735 Shampoo 3.682 SOAP 3.375 3.340 3.339 3.335 3.341 3.302 3.009 2.971 3.19 2.980 2.999 2.981 2.905 2.868 2.976 2.850 2.849 2.842 2.787 2.765 2.844 2.763 2.782 2.797 2.655 2.651 N/A N/A 2.640 N/A Table 7: Final test loss ( Shampoo shows the smallest degradation relative to the full-precision baselines. ) for the W4A4 QAT models. Comparing with Table 5, we see that 14 A.4 ABC DECOMPOSITION Consider Feed-Forward Network (FFN) with modules f‚Ñì( RnL for an input the network (x) Rn0 is: ), for ‚Ñì { 1, . . . , } . The output of h0 = h‚Ñì = f‚Ñì(h‚Ñì (x) = hL 1) for ‚Ñì { 1, . . . , } Rn‚Ñì are the activations of the network. where h‚Ñì The modules f‚Ñì( ) can be any arbitrary transformation, including linear layers, convolutions, activation functions, layer normalizations and self-attention. Thus, the class of networks we consider is quite general, and includes some of the most popular architectures, like Convolutional Networks (CNNs) and Transformers. Moreover, this framework is also flexible in terms of the granularity of the module partitioning. Even if module also consists itself of submodules, we can arbitrarily choose to consider it as whole. Therefore, we can study the network in any level of granularity desired. Assume now that we quantize the modules, changing them from f‚Ñì( ). There is again flexibility on what quantize can mean in this context. For example, we can arbitrarily choose to apply weight and activation quantization on linear layers, activation quantization on activation functions, and no quantization on layer normalizations. Other choices are also possible, as we treat quantization as general perturbation in the function space. ) to ‚Ñì ( After quantizing, the activations of the network become: hq 0 = hq ‚Ñì = q(x) = hq ‚Ñì (hq ‚Ñì 1) for ‚Ñì 1, . . . , } { By comparing the activations pre and post-quantization: h‚Ñì = f‚Ñì(h‚Ñì ‚Ñì (hq ‚Ñì = hq ‚Ñì 1) 1) we see that the change in the output h‚Ñì is caused by the change in the input h‚Ñì as well as the change in the function f‚Ñì( from the propagation of the quantization error through the previous (‚Ñì change in the function f‚Ñì( ) is the newly introduced layerwise perturbation. ) (function space). The change in the input h‚Ñì 1 (activation space), 1 comes 1) modules, while the We wish to precisely quantify these changes. In order to do this, we study the difference in the activations: h‚Ñì := hq h‚Ñì = ‚Ñì (hq ‚Ñì 1) f‚Ñì(h‚Ñì 1) ‚Ñì 1), we can get: By adding and subtracting either ‚Ñì (hq ‚Ñì (hq h‚Ñì = (f h‚Ñì = (f ‚Ñì (h‚Ñì 1) 1) ‚Ñì ‚Ñì 1), or f‚Ñì(hq ‚Ñì 1)) + (f ‚Ñì (h‚Ñì ‚Ñì (h‚Ñì 1)) + (f‚Ñì(hq f‚Ñì(hq ‚Ñì ‚Ñì 1)) 1) f‚Ñì(h‚Ñì 1)) f‚Ñì(h‚Ñì 1)) It is instructive to understand what the four terms on the right sides represent. The top left term stands for the change in the input under the function ), the top right term for the change in the ‚Ñì ( function under the input h‚Ñì 1, and the last term for the change in the input under the function f‚Ñì( 1, the bottom left for the change in the function under the input hq ‚Ñì ). 1. On the Thus, the top equation implies that we first perturb f‚Ñì( other hand, the bottom equation implies that we first perturb the input, and then the function. Both equations are valid, but reflect different assumptions about the order of the perturbations. Arbitrarily choosing one can lead to attribution bias, and thus, following the Shapley principle3, we average both 1 to hq ‚Ñì ) to ‚Ñì ( ), and then h‚Ñì 3According to the Shapley principle, the unique way to distribute contributions fairly across two players function) is by averaging the possible orderings. This is the only allocation scheme that input vs. (here: satisfies the three axioms of symmetry, efficiency and linearity. 15 as follows: h‚Ñì = + = + (f ‚Ñì (hq ‚Ñì ‚Ñì (h‚Ñì 1) (f ‚Ñì (hq ‚Ñì 1) f‚Ñì(hq ‚Ñì ‚Ñì (h‚Ñì 1)) + (f 2 1)) + (f‚Ñì(hq ‚Ñì 2 1)) f‚Ñì(h‚Ñì 1)) + 1) f‚Ñì(h‚Ñì 1)) = (f ‚Ñì (hq ‚Ñì ‚Ñì (h‚Ñì 1) 1)) + (f‚Ñì(hq ‚Ñì 1) f‚Ñì(h‚Ñì 1)) + (f ‚Ñì (hq ‚Ñì 1) f‚Ñì(hq ‚Ñì ‚Ñì (h‚Ñì 1)) f‚Ñì(h‚Ñì 1)) = 2 1)) + (f 2 with: = a‚Ñì + b‚Ñì (f ‚Ñì (hq ‚Ñì (f ‚Ñì (hq ‚Ñì 1) 1) a‚Ñì := b‚Ñì := ‚Ñì (h‚Ñì 1)) + (f‚Ñì(hq ‚Ñì 1) f‚Ñì(h‚Ñì 1)) f‚Ñì(hq ‚Ñì 2 1)) + (f 2 ‚Ñì (h‚Ñì 1)) f‚Ñì(h‚Ñì 1)) Hence, the change in the output h‚Ñì := hq first term, a‚Ñì entirely captures the effect of the change in the input from h‚Ñì second term b‚Ñì completely encodes the change in the function from f‚Ñì( h‚Ñì can be exactly decomposed into two terms. The 1, while the 1 to hq ‚Ñì ) to ). ‚Ñì ( ‚Ñì However, h‚Ñì, a‚Ñì, b‚Ñì are all still vectors. In order to reduce them to interpretable numbers, we compute the L2-norm4 of h‚Ñì. Specifically, since should be interpreted relative to the scale of the initial unquantized activations, we normalize with respect to . Thus, we study the L2-norm of the relative change in the activations: a‚Ñì + b‚Ñì h‚Ñì a‚Ñì + b‚Ñì h‚Ñì h‚Ñì r2 ‚Ñì = h‚Ñì = 2 2 By setting R‚Ñì := r2 h‚Ñì r‚Ñì := h‚Ñì ‚Ñì and using the Law of Cosines: a‚Ñì, b‚Ñì a‚Ñì h‚Ñì a‚Ñì, b‚Ñì + 2 h‚Ñì a‚Ñì h‚Ñì = A‚Ñì + B‚Ñì + C‚Ñì R‚Ñì = 2 + 2 (cid:19)2 + = (cid:18) 2 b‚Ñì 2 = 2 + (cid:19)2 = (cid:18) b‚Ñì h‚Ñì where: (cid:19)2 (cid:19)2 (cid:18) (cid:18) B‚Ñì := A‚Ñì := a‚Ñì h‚Ñì b‚Ñì h‚Ñì a‚Ñì, b‚Ñì C‚Ñì := 2 h‚Ñì 2 with A‚Ñì standing for the quantization error accumulated from the previous (‚Ñì 1) modules, B‚Ñì for the new quantization error, introduced in the ‚Ñì-th layer, and C‚Ñì for the interaction between the two. quantity of interest that helps us understand how module amplifies or attenuates the quantization error from the previous layer is the gain: G‚Ñì := A‚Ñì R‚Ñì 1 4We can also use the more natural RMS-norm (as in Large et al. (2024)), which is normalized with respect to dimension, allowing us to compare numbers across networks with different widths. In the end, these two choices are equivalent, because we ultimately investigate ratios of L2-norms, which are equal to ratios of RMS-norms. 16 G‚Ñì determines how the quantization error of the previous layer, expressed by R‚Ñì through the ‚Ñì-th module to appear as A‚Ñì. 1, propagates In general, if we treat the module as black-box, we cannot further analyze the gain G‚Ñì (even though we can calculate it from A‚Ñì and R‚Ñì ) we can exactly recover G‚Ñì in closed-form. The most straightforward example is the case of linear layer under weight and activation quantization. 1). However, for specific functional forms of f‚Ñì( Specifically, for linear layer5 under joint quantization: h‚Ñì = f‚Ñì(h‚Ñì ‚Ñì (hq ‚Ñì = hq ‚Ñì 1) = W‚Ñìh‚Ñì 1) = (W‚Ñì + ŒµW 1 ‚Ñì )hq ‚Ñì 1 + Œµh ‚Ñì where ŒµW tively. ‚Ñì and Œµh ‚Ñì are quantization noise introduced by weight and activation quantization6, respecMoreover: ‚Ñì (h‚Ñì f‚Ñì(hq ‚Ñì By using the last four equations: 1) = (W‚Ñì + ŒµW 1) = W‚Ñìhq 1 ‚Ñì ‚Ñì )h‚Ñì 1 + Œµh ‚Ñì ‚Ñì (hq ‚Ñì f‚Ñì(hq ‚Ñì ‚Ñì (hq ‚Ñì (h‚Ñì 1) 1) 1) 1)) ‚Ñì ‚Ñì )h‚Ñì ‚Ñì (h‚Ñì f‚Ñì(h‚Ñì f‚Ñì(hq ‚Ñì f‚Ñì(h‚Ñì 1) = (W‚Ñì + ŒµW 1) = W‚Ñìh‚Ñì 1) = ŒµW 1) = ŒµW ‚Ñì hq ‚Ñì ‚Ñì h‚Ñì 1 1 + Œµh ‚Ñì 1 + Œµh ‚Ñì In turn, by replacing those into the definitions of a‚Ñì and b‚Ñì we obtain: 1 a‚Ñì = (W‚Ñì + 2 (cid:18) h‚Ñì b‚Ñì = ŒµW ‚Ñì ŒµW ‚Ñì )h‚Ñì 1 + hq ‚Ñì 2 1 (cid:19) + Œµh ‚Ñì Finally, by using the definition of A‚Ñì: (cid:18) A‚Ñì := = (cid:19)2 a‚Ñì h‚Ñì (W‚Ñì + 1 (cid:18) (cid:18) = = W‚Ñì + ‚Ñì )h‚Ñì 1 h‚Ñì ‚Ñì h‚Ñì 1 (cid:19)2 (cid:18) 2 ŒµW W‚Ñìh‚Ñì 2 ŒµW W‚Ñì W‚Ñì + 1 2 ŒµW ‚Ñì W‚Ñì = G1,‚ÑìG2,‚ÑìR‚Ñì = (cid:18) 1 G‚Ñì = G1,‚ÑìG2,‚Ñì (cid:19)2 = 1 cos œï‚Ñì (cid:19)2 = ( 1 cos œà‚Ñì h‚Ñì h‚Ñì 1 1 (cid:19)2 (cid:18) cos œï‚Ñì cos œà‚Ñì (cid:19)2 = denotes the spectral norm) 5We assume no bias for ease of exposition (this also coincides with our architecture). 6Here, without loss of generality, we assume we quantize the activations after the matrix multiplication. ‚Ñì1 + ‚Ñì , which would lead to equivalent analysis, since we can treat the last term as single quantity. Quantizing before the matrix multiplication would mean hq (W‚Ñì + ŒµW ‚Ñì ) = (W‚Ñì +ŒµW ‚Ñì = (W‚Ñì +ŒµW ‚Ñì1 +Œµh ‚Ñì )(hq ‚Ñì )hq ‚Ñì )Œµh 17 where we defined: ‚Ñì )h‚Ñì h‚Ñì 1 1 cos œï‚Ñì := cos œà‚Ñì := G1,‚Ñì := G2,‚Ñì := 2 ŒµW (W‚Ñì + 1 W‚Ñì + 1 2 ŒµW ‚Ñì W‚Ñìh‚Ñì 1 h‚Ñì W‚Ñì 1 (cid:18) W‚Ñì + 1 2 ŒµW ‚Ñì W‚Ñì (cid:19)2 (cid:18) cos œï‚Ñì cos œà‚Ñì (Alignment ratio) (cid:19)2 (Spectral ratio) The angles œï‚Ñì and œà‚Ñì are between matrices and vectors. This is non-standard definition, but quite intuitive nonetheless. The angles lie between 0 (cos œï‚Ñì/œà‚Ñì = 1) and 90 (cos œï‚Ñì/œà‚Ñì = 0), and indicate how close is the direction of the vector to the direction of the top right singular vector of the matrix. An angle of 0 means that the vector points towards direction that is maximally stretched by the matrix. In other words, the vector is colinear with the top right singular vector. In contrast, an angle of 90 indicates that the vector and top right singular vector are orthogonal, and that the vector is being shrunk to 0. 2 ŒµW ‚Ñì ) (the 1 The angle œï‚Ñì is between the change in the activations h‚Ñì 1 and the weight matrix after quantization (W‚Ñì + 1 2 comes from Shapley). The angle œà‚Ñì is between the incoming activations h‚Ñì 1 and the unquantized weight matrix W‚Ñì, and denotes how aligned are the incoming activations with the weights. The spectral ratio G1,‚Ñì is the (squared) ratio of the spectral norms of weights before and after quantization. Finally, the alignment ratio G2,‚Ñì just encodes the (squared) ratio of the alignment between the change in the activations and the weights after quantization, and the alignment between the incoming activations and the original weights. To sum up, for linear layer under joint quantization, we further factored the gain G‚Ñì of quantization error into product of two interpretable terms, the spectral ratio G1,‚Ñì, and the alignment ratio G2,‚Ñì. We can do similar analysis for the terms B‚Ñì and C‚Ñì, which we omit here because we found that R‚Ñì is dominated by A‚Ñì (see Figure 3). similar-style analysis can also be done for other modules, like layer normalizations and self-attention, by using Taylors theorem, but we leave this for future work. A.5 OPTIMIZERS MEMORY AND COMPUTATIONAL COMPLEXITIES Table 8 shows the asymptotic memory (including gradients) and computational complexities for one step of gradient update of hidden layer with weight matrix Rm n. Optimizer Memory Overhead Computational Overhead AdamW Muon PSGD Scion Shampoo SOAP 3mn 2mn mn + m2 + n2 2mn 3mn + m2 + n2 3mn + m2 + n2 O(mn) O(T (2nm2 + m3)), O( m3+n3 O(T (2nm2 + m3)), n + 2(m + n)mn) O((m3 + n3)(1 + 1 O((m3 + n3)(1 + 1 ) + (m + n)mn) ) + 2(m + n)mn) Table 8: Asymptotic memory (including gradients) and computational complexities per one optimizer step for linear layer of size n. Here denotes the number of Newton-Schulz iterations in Muon and Scion algorithms, and is the preconditioner update frequency for PSGD, Shampoo, and SOAP. Recall that multiplication of 2 matrices AB, where plications and mn(k 1) additions, which totals in mn(k + Rm k, 1) n, requires mnk multiRk 2mnk operations."
        },
        {
            "title": "AdamW\nMuon\nPSGD\nScion\nShampoo\nSOAP",
            "content": "Iteration time (s) Memory (GB) 50M (128) 125M (64) 350M (64) 500M (32) 760M (32) 1.5B (16) 0.66 0.70 0.80 0.71 1.30 0.82 63 63 63 63 67 63 1.37 1.41 1.70 1.51 2.18 1.52 41 41 41 41 48 42 3.07 3.11 3.49 3.14 4.34 3.34 66 66 66 66 83 66 4.51 4.59 5.12 4.63 6.43 5.02 48 48 50 48 72 58 6.28 6.35 7.08 6.55 8.38 7.02 57 57 63 57 89 77 66 61 12.89 13.09 N/A N/A 91 16.38 N/A Table 9: Measurements for full-precision training on GH200 96GB GPU. Inside the parentheses next to each model size, we list the maximum batch size that works with all optimizers, also used for the measurements. AdamW and Muon have similar requirements, despite Muon leading to the strongest models (see Table 2). Even though models trained with Shampoo show the mildest degradation (see Table 3), they also take the longest to train, while also requiring the most memory. 1. AdamW Gradients + 1 and 2 moments = 3mn memory. The number of moments and weight update operations is linear with respect to mn, the number of elements in the weight matrix. 2. Muon Gradients (mn) and the first moment (mn) give the total memory overhead of 2mn. Suppose < n, one step of the Newton-Schulz iteration for matrix becomes Rm then = aX + b(XX )X + c(XX )2X, (3) which gives O(mn + 2m2n + 2m3 + 2m2n) = O(2m2n + m3) time complexity. For the total of iterations it then becomes O(T (2m2n + m3)). 3. PSGD Gradients (mn) + 2 preconditioning matrices (m memory. non both sides of the gradim and PR Applying preconditioners PL ent PLgPR uses 2m2n + 2mn2 operations, and the preconditioner update has computational complexity O(m3 + n3). Amortized by update frequency , the total complexity is O(2m2n + 2mn2 + m3+n3 n) give mn + m2 + n2 total and Rm Rn ). 4. Scion Gradients (mn) and the first moment (mn) give the total memory overhead of 2mn. Scion uses the same Spectral LMO as Muon, which results in O(T (2m2n + m3)) complexity overhead. 5. Shampoo Memory overhead consists of gradients, 1st and 2nd moments (3mn), left and right preconditioners and eigenvector matrices (2m2 + 2n2). Updating the preconditioners takes O(m3 + n3), and applying them additional O(m2n + mn2) operations. Preconditioner change with frequency adds extra O( m3+n3 ) complex100, so other terms dominate. ity. In practice 10 6. SOAP The same as for Shampoo, the total memory complexity is 3mn + 2m2 + 2n2. The time complexity is same as for Shampoo with extra O(m2n + mn2) for projecting back from the eigen-space. Recomputing the eigenbasis QL, QR with frequency adds extra O( m3+n3 100, so other terms dominate. ) complexity. In practice 10 19 A.6 SCALING LAWS FITTING PROCEDURE To estimate the parameters of scaling law for each optimizer, we perform nonlinear least-squares regression using scipy.least squares. To improve robustness against outliers, we employ 3. We minimize the residuals between the predicted and the Huber loss with parameter Œ¥ = 10 observed log-loss values, fitting (log Ni, log Di, œÅbi) to log Li, where Ni is the model size, Di the dataset size, and bi is the precision. The loss values are transformed into logarithmic scale prior to fitting for numerical stability. For the total parameter count , we include embedding parameters, which we found improves the fit. We report the values with confidence intervals, obtained by leaveone-out cross-validation, in Table 10. We estimated the confidence intervals of the fitted hyperparameters by computing their standard deviations across folds in leave-one-out cross-validation procedure. We found that excluding the loss value corresponding to the largest model size provides an unstable fit, so we retained the lowest loss point for all the folds. Optimizer Œ± œÅ4bit AdamW Muon PSGD Scion Shampoo SOAP 79 208 77 148 142 706 40 7 6 22 26 132 0.20 0.27 0.18 0.25 0.24 0.35 0.01 0.01 0. 0.01 0.01 0.01 1.40 1.85 1.39 1.75 1.72 2.22 0.05 0. 0.44 0.07 0.09 0.03 0.863 0.852 0.739 0.856 0.879 0.822 0. 0.010 0.049 0. 0.018 0.010 Table 10: Scaling law coefficients for different optimizers."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "ISTA",
        "Red Hat AI"
    ]
}