{
    "paper_title": "Deep Delta Learning",
    "authors": [
        "Yifan Zhang",
        "Yifeng Liu",
        "Mengdi Wang",
        "Quanquan Gu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes a strictly additive inductive bias on feature transformations, thereby limiting the network's capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), a novel architecture that generalizes the standard residual connection by modulating the identity shortcut with a learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes a rank-1 perturbation of the identity matrix, parameterized by a reflection direction vector $\\mathbf{k}(\\mathbf{X})$ and a gating scalar $β(\\mathbf{X})$. We provide a spectral analysis of this operator, demonstrating that the gate $β(\\mathbf{X})$ enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as a synchronous rank-1 injection, where the gate acts as a dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 ] . [ 1 7 1 4 0 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Deep Delta Learning",
            "content": "Yifan Zhang1 Yifeng Liu2 Mengdi Wang1 Quanquan Gu2 1Princeton University 2University of California, Los Angeles yifzhang@princeton.edu January 1, 2026 Abstract The efficacy of deep residual networks is fundamentally predicated on the identity shortcut connection. While this mechanism effectively mitigates the vanishing gradient problem, it imposes strictly additive inductive bias on feature transformations, thereby limiting the networks capacity to model complex state transitions. In this paper, we introduce Deep Delta Learning (DDL), novel architecture that generalizes the standard residual connection by modulating the identity shortcut with learnable, data-dependent geometric transformation. This transformation, termed the Delta Operator, constitutes rank-1 perturbation of the identity matrix, parameterized by reflection direction vector k(X) and gating scalar β(X). We provide spectral analysis of this operator, demonstrating that the gate β(X) enables dynamic interpolation between identity mapping, orthogonal projection, and geometric reflection. Furthermore, we restructure the residual update as synchronous rank-1 injection, where the gate acts as dynamic step size governing both the erasure of old information and the writing of new features. This unification empowers the network to explicitly control the spectrum of its layer-wise transition operator, enabling the modeling of complex, non-monotonic dynamics while preserving the stable training characteristics of gated residual architectures. Project Page: https://github.com/yifanzhang-pro/deep-delta-learning"
        },
        {
            "title": "Introduction",
            "content": "Deep residual networks (He et al., 2016) represent paradigm shift in neural network design, enabling the stable training of models with unprecedented depth. Their core mechanism, the identity shortcut connection, reformulates layers to learn residual function F(X) with respect to their input X. In its canonical form, the residual update is an element-wise addition: (1.1) You can view this as forward Euler step (step size 1) for the ODE = F(X). This viewpoint ties deep networks to dynamical systems (Chen et al., 2018). The strictly additive update also puts strong translation bias on the learned dynamics. The shortcut path keeps fixed Jacobian equal to the identity operator. Xl+1 = Xl + F(Xl) This rigidity limits what state transitions the network can represent. Recent work points to the need for more flexible transitions, including ones that realize negative eigenvalues, when modeling patterns like oscillations or oppositional behavior (Grazzi et al., 2024). 1 Xl+ + kXl k(X) Direction β(X) Gate v(X) Value Xl Identity Xl Deep Delta Update Ordered Operations: 1. Project kXl 2. Compare Proj 3. Gate by β 4. Inject along Figure 1 The Deep Delta Residual Block. The architecture generalizes the standard residual connection. learnable scalar gate β controls rank-1 geometric transformation. To overcome this limitation, we propose principled generalization of the residual connection rooted in geometric linear algebra. We introduce Deep Delta Learning (DDL), featuring novel residual block that applies learnable, rank-1 transformation to the hidden state matrix Rddv . This formulation aligns the network depth with memory-augmented architectures, effectively treating the hidden state as dynamic value matrix. This block utilizes single learned scalar gate β(X) to smoothly interpolate between standard residual connection, an orthogonal projection operator, and full geometric reflection. Our contributions are: 1. We propose the Delta Residual Block, multi-branch architecture that learns to apply generalized Householder operator to the matrix-valued shortcut connection, parameterized by learned direction k(X) and learned gate β(X), which is illustrated in Figure 1. 2. We give spectral analysis of the Delta Operator. We derive its complete eigensystem, and 2 show how β(X) controls the transformation by shaping its spectrum. 3. We unify identity mapping, projection, and reflection in one continuously differentiable module. We also show DDL recovers the Delta Rule update, with the gate β acting like depth-wise step size."
        },
        {
            "title": "2 The Delta Residual Block",
            "content": "We build our method upon the mathematical foundation of the Householder reflection, which we generalize into learnable, state-dependent operator."
        },
        {
            "title": "2.1 Preliminaries: The Householder Transformation",
            "content": "Definition 2.1 (Householder Matrix). For non-zero vector Rd, the Householder matrix Hk is defined as: Hk = 2 kk k2 2 (2.1) Geometrically, Hk reflects any vector across the hyperplane with normal vector k. The Householder matrix is cornerstone of numerical linear algebra and possesses several key properties: it is symmetric (Hk = = I). Its ), orthogonal (H spectrum consists of single eigenvalue of 1 (eigenvector k) and 1 eigenvalues of 1 (the eigenspace k). Hk = I), and involutory (H2 k"
        },
        {
            "title": "2.2 Formulation of the Delta Operator",
            "content": "We generalize the Householder matrix by replacing the constant factor of 2 with learnable, datadependent scalar gate, β(X). This leads to the Delta Residual (Delta-Res) block. Let the hidden state be matrix Rddv , where is the feature dimension and dv denotes the number of value channels. We modify the additive residual to be rank-1 update aligned with the reflection vector k. The block output is computed as: Xl+1 = A(Xl)Xl + β(Xl)k(Xl)v(Xl) (2.2) where Rdv is the residual value vector generated by the branch : Rddv Rdv . Here, the outer product kv constitutes the additive update. Crucially, we apply the gate β(X) to this constructive term as well, linking the erasure and write operations. The term A(X) is the Delta Operator acting spatially on the feature dimension d: A(X) = β(X) k(X)k(X) k(X)k(X) + ϵ (2.3) The architecture learns the reflection direction k(X) Rd, the value vector v(X) Rdv , and the reflection intensity β(X) through separate, lightweight neural network branches. The constant ϵ > 0 ensures numerical stability. For the theoretical analysis, we assume is strictly normalized such that kk = 1 (see Appendix for implementation details). Under this condition (ϵ 0), the operator simplifies to: A(X) = β(X)k(X)k(X) (2.4) 3 Since is matrix, the operator A(X) broadcasts across the value dimension dv, applying the geometric transformation simultaneously to every column of the hidden state. Under the same unit-norm assumption, substituting A(X) = β(X)k(X)k(X) into Eq. (2.2) yields an equivalent additive, rank-1 Delta form: Xl+1 = Xl + β(Xl) k(Xl)(cid:16) v(Xl) k(Xl)Xl (cid:17) , (2.5) which makes explicit that the same scalar β modulates both the erasure term kX and the write term v. The gating function β(X) is parameterized to lie in the range [0, 2] via projection of the state features followed by sigmoid function: β(X) = 2 σ(Linear(G(X))) where G() is pooling, convolution, or flattening operation. This specific range is chosen for its rich geometric interpretations, which we analyze next. (2.6)"
        },
        {
            "title": "3 Analysis",
            "content": "The expressive power of the Delta-Res block comes from the spectral properties of the operator A(X), which are controlled by the learned gate β(X)."
        },
        {
            "title": "3.1 Spectral Decomposition of the Delta Operator",
            "content": "Theorem 3.1 (Spectrum of the Delta Operator). Let = βkk where Rd is unit vector (kk = 1) and β is scalar. The spectrum of A, denoted σ(A), is: , 1 β} σ(A) = {1, 1, . . . , 1 } {z d1 times (3.1) The eigenvector corresponding to the eigenvalue λ = 1 β is k. The eigenspace for the eigenvalue λ = 1 is the orthogonal complement of k, denoted = {u Rd ku = 0}. Proof. Let be any vector in the hyperplane orthogonal to (i.e., such that ku = 0). Applying to yields: Au = (I βkk)u = Iu βk(ku) = βk(0) = = 1 (3.2) Thus, any vector in the (d 1)-dimensional subspace is an eigenvector with eigenvalue λ = 1. Now, consider applying to the vector itself: Ak = (I βkk)k = Ik βk(kk) = βk(1) = (1 β)k (3.3) Thus, is an eigenvector with eigenvalue λ = 1 β. Since we have found linearly independent eigenvectors spanning Rd, we have characterized the full spectrum of A. This theorem provides clear and powerful interpretation of the gate β(X). By learning single scalar, the network can dynamically control the geometry of the residual transformation across all dv columns of the state matrix simultaneously. 4 Lifting to matrix-valued states. The spectral statements above are spatial: they describe the linear map 7 Au on Rd. Since our hidden state is matrix Rddv and the shortcut acts by left-multiplication, each of the dv columns is transformed independently by the same A. Equivalently, under vectorization, the induced linear operator is Idv A. Thus the spectrum of the lifted map consists of the eigenvalues of repeated dv times, and its determinant equals det(A)dv . Orthogonality condition. Because is symmetric, its singular values coincide with the absolute values of its eigenvalues. In particular, is orthogonal if and only if 1 β = 1, i.e., β {0, 2} under the unit-norm assumption. For β (0, 2), performs an anisotropic contraction along (and flips sign along when β > 1). Corollary 3.2 (Spatial Determinant). The determinant of the Delta Operator A(X), acting on the spatial features Rd, is given by: det(A(X)) = i=1 λi = 1d1 (1 β(X)) = 1 β(X) (3.4) Since the shortcut broadcasts across the dv value columns, the induced determinant on the full matrix state space Rddv (equivalently, on vec(X) Rddv ) is det(A(X))dv = (1 β(X))dv . Thus β(X) controls the signed volume change along the spatial direction k(X); in particular, β(X) > 1 introduces negative spatial eigenvalue (a reflection along k), while the global orientation of the lifted state space flips if and only if dv is odd."
        },
        {
            "title": "3.2 Unification of Geometric Operations",
            "content": "Theorem 3.1 reveals that the range [0, 2] for β(X) allows the operator to interpolate between three fundamental linear transformations. Identity Mapping (β(X) 0): As β 0, the eigenvalue 1 β 1. All eigenvalues of A(X) become 1, so A(X) I. Since β also modulates the injection term βkv, the entire update vanishes, meaning Xl+1 Xl. This identity behavior is crucial for preserving signal propagation in very deep networks. Orthogonal Projection (β(X) 1): As β 1, the eigenvalue 1 β 0. The operator A(X) becomes kk, an orthogonal projector (rank 1) onto the hyperplane k. The component of each column of the input state parallel to is explicitly removed (forgotten) before the residual is added. The operator becomes singular, and det(A) 0. In terms of the full block (Eq. (2.5)), this regime can be interpreted as replace-along-k: the shortcut removes the k-component, and the rank-1 write injects new component along specified by v. Full Reflection (β(X) 2): As β 2, the eigenvalue 1 β 1. The operator A(X) becomes 2kk, standard Householder matrix. This performs perfect reflection of each column of across k. This is the only case in this range where the transformation is guaranteed to be orthogonal and spatially volume-preserving, with det(A) 1. The negative spatial determinant signifies change in orientation (a reflection) of the basis. Together with the identity case (β = 0), this is the only setting in [0, 2] for which the shortcut operator is orthogonal. The full block additionally applies the synchronized rank-1 write term, yielding reflection of the incoming state followed by write aligned with k."
        },
        {
            "title": "3.3 Special Case: Gated Residual Learning",
            "content": "A critical property of Deep Delta Learning is its behavior in the limit of the gating scalar. When the gate vanishes (β(X) 0), the Delta Operator converges to the identity matrix (A(X) I), and the constructive term vanishes. Consequently, the update rule in Equation (2.2) simplifies to: Xl+1 = Xl (3.5) This recovers the identity mapping, effectively allowing the layer to be skipped entirely. This behavior is consistent with the zero-initialization strategy often required for training very deep networks. Conversely, when β 1, the layer functions as Gated Rank-1 Matrix ResNet, where β acts as learned step size governing the magnitude of the update. This demonstrates that DDL generalizes residual learning by introducing multiplicative, geometric modulation that is coupled synchronously with the value injection."
        },
        {
            "title": "3.4 Diagonal Feature Matrices Case",
            "content": "To better understand the mixing properties of the Delta Operator, consider the special case where the input state Rdd is square diagonal matrix, = diag(λ1, . . . , λd). This represents state where features are perfectly decoupled across the value dimensions. The application of yields: (AX)ij = (X βkkX)ij = λiδij βλjkikj (3.6) Specifically, the off-diagonal element (i = j) becomes βλjkikj, while the diagonal element (i = j) ). This implies that the output feature is now dependent on the magnitude is scaled to λi(1 βk2 of the input feature j, scaled by the geometric coherence kikj. This result elucidates critical function of the Delta block: it induces controlled feature coupling. Even if the incoming features are independent, non-zero β forces an interaction between the i-th and j-th modes proportional to the projection of the reflection vector k. If β 1 (projection), the shortcut removes the component of each column along k, mapping the state into before the write term reinstates new k-component specified by v. If β 0, the diagonal structure is preserved."
        },
        {
            "title": "3.5 Vector Hidden State Dynamics",
            "content": "While DDL operates on matrix-valued states Rddv , it naturally encapsulates standard vector-based deep learning as specific limit. We identify two distinct regimes: The Scalar Value Limit (dv = 1). When the value dimension is reduced to unity, the hidden state degenerates to standard feature vector Rd. In this limit, the value update becomes scalar R. The Delta update rule Eq. (2.2) simplifies to: xl+1 = xl + βl (vl kl (3.7) xl) } {z γl Here, the geometric transformation collapses into dynamic scalar gating mechanism. The term γl acts as data-dependent coefficient that couples the update magnitude to the discrepancy between the proposed write value vl and the current projection xl. 6 The Independent Feature Limit. Alternatively, one may view the diagonal case in Section 3.4 as representation of vector state embedded in matrix diagonal. As shown in the diagonal analysis, the Delta Operator introduces feature coupling via the term βkikj. To recover the behavior of standard element-wise vector updates (where features do not mix spatially), the reflection vector must be aligned with the canonical basis (i.e., one-hot). In this regime, the Delta Operator acts as an element-wise gating function, strictly preserving the independence of the feature dimensions."
        },
        {
            "title": "4 Connections to Optimization and Delta Architectures",
            "content": "The terminology Deep Delta Learning reflects structural homology with the Delta Rule, fundamental update mechanism recently popularized in efficient sequence modeling, e.g., DeltaNet (Schlag et al., 2021; Yang et al., 2024)."
        },
        {
            "title": "4.1 The Delta Rule for Residual Learning",
            "content": "The standard residual connection, Xl+1 = Xl + F(Xl), imposes strictly additive inductive bias. Information, once generated by F, is simply accumulated. This can lead to residual accumulation, where noisy or interfering features persist across layers because the network lacks an explicit mechanism to selectively filter the hidden state. Deep Delta Learning addresses this by incorporating the Delta Rule structure into the depth dimension. Expanding the Delta Residual update in Equation (2.2) using the rank-1 residual definition: Xl+1 = Xl + βlkl {z} Write l Xl {z } Erase (4.1) This formulation exactly recovers the Delta Rule update utilized in fast associative memories and Xl represents the current projection of the state onto the reflection linear attention. The term vector (the error or old memory). The term (v Since Xl Rddv is matrix, the term Xl yields row vector in R1dv , representing the projection of every value column onto kl. The update rigidly aligns both the erasure (destructive) and injection (constructive) operations along the geometric direction defined by the projector kl, modulated by the step size βl. Xl) acts as the correction signal. k When β(Xl) 1, this subtractive term acts as an orthogonal projection, effectively erasing the component of the incoming state Xl parallel to k(Xl) (forgetting). When β(Xl) 2, the term subtracts twice the projection, resulting in sign inversion (reflection). This provides the network with flexible mechanism to selectively clean or reorient specific feature subspaces layer-by-layer, preventing the accumulation of interference."
        },
        {
            "title": "4.2 Relation to DeltaNets and Householder Products",
            "content": "Our work shares theoretical link with the DeltaNet architecture (Schlag et al., 2021), which replaces the additive accumulation of Linear Transformers with Delta Rule for memory updates. We demonstrate that Deep Delta Learning is the depth-wise isomorphism of the DeltaNet recurrence. In DeltaNet, the hidden state (memory) St evolves over time t. To unify notation with 7 our depth-wise formulation, we present the DeltaNet update using left-multiplication semantics, where the memory state is St Rdkdv : St = (I βtktk )St1 + βtktv (4.2) Here, the operator acts on the key dimension dk, which is analogous to the feature dimension in DDL. Comparing this to our Deep Delta Layer update Equation (2.2) acting over depth l: Xl+1 = (I βlklk )Xl + βlklv (4.3) where vl is the vector output of the value branch. This reveals direct structural correspondence: The memory state St (dimension dk dv) in DeltaNet corresponds to the feature activation Xl (dimension dv) in DDL. Both architectures employ the rank-1 Householder operator to selectively reflect or erase subspace components. DeltaNet applies this over time steps t, whereas DDL applies it over network depth l. Our modified residual update βlklv aligns perfectly with the DeltaNet write operation. By incorporating βl into the constructive term, we interpret βl as layer-wise step size for the depth-wise ODE. This ensures that both the erasure and injection components are modulated synchronously, ensuring the update represents coherent geometric transformation of the state X. Thus, DDL can be interpreted as applying the Delta Rule to layer-wise feature evolution, enabling the network to forget or rewrite features from shallow layers as they propagate deeper."
        },
        {
            "title": "5 Related Work",
            "content": "Our work builds upon several key research themes in deep learning. Gated and Invertible Architectures. Highway Networks (Srivastava et al., 2015) introduced datadependent gating to residual networks, but their gates interpolate between the identity path and the function path, rather than modifying the transformation itself. Invertible Residual Networks (i-ResNets) (Behrmann et al., 2019) constrain the Lipschitz constant of to ensure invertibility, which is useful for applications like normalizing flows. Our Delta shortcut operator is invertible whenever 1 β = 0 (in the ϵ 0 analysis), and becomes an orthogonal involution at β = 2 (a Householder reflection). DDL does not enforce invertibility globally; instead, it allows the network to learn when near-invertible transition is beneficial versus when an intentionally singular (projective) transition is useful for controlled forgetting. Orthogonal and Unitary Networks. significant body of work has focused on constraining network weights to be orthogonal or unitary to improve gradient stability and preserve geometric structure (Arjovsky et al., 2016; Jing et al., 2017). Householder reflections are classic method for parameterizing orthogonal matrices. These methods enforce orthogonality as strict constraint. In contrast, our Delta Residual Network learns to deviate from identity and orthogonality via the gate β(x), providing soft, adaptive constraint that can be relaxed to pure projection or reflection. 8 Neural Ordinary Differential Equations. Neural ODEs (Chen et al., 2018) model the continuous evolution of features. The standard ResNet Eq. (1.1) is discretization of the simple ODE = F(X). Our proposed architecture alters the underlying dynamics to = β(X)k(X)(v(X) k(X)X), introducing state-dependent projection term applied to the matrix state. This allows for much richer family of learnable dynamical systems that can exhibit contractive or oscillatory behavior across multiple value dimensions."
        },
        {
            "title": "6 Conclusion",
            "content": "We have introduced Deep Delta Learning, novel architecture built upon an adaptive, geometric residual connection. Through analysis, we have demonstrated that its core component, the Delta Operator, unifies identity mapping, projection, and reflection into single, continuously differentiable module. This unification is controlled by simple learned scalar gate, which dynamically shapes the spectrum of the layer-to-layer transition operator. By empowering the network to learn transformations with negative eigenvalues in data-dependent fashion, DDL offers significant and principled increase in expressive power while retaining the foundational benefits of the residual learning paradigm."
        },
        {
            "title": "References",
            "content": "Martin Arjovsky, Amar Shah, and Yoshua Bengio. Unitary evolution recurrent neural networks. In International conference on machine learning, pages 11201128. PMLR, 2016. Jens Behrmann, Will Grathwohl, Ricky TQ Chen, David Duvenaud, and Jörn-Henrik Jacobsen. Invertible residual networks. In International conference on machine learning, pages 573582. PMLR, 2019. Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. Riccardo Grazzi, Julien Siems, Arber Zela, Jörg KH Franke, Frank Hutter, and Massimiliano Pontil. Unlocking state-tracking in linear rnns through negative eigenvalues. arXiv preprint arXiv:2411.12537, 2024. Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. Li Jing, Yichen Shen, Tena Dubcek, John Peurifoy, Scott Skirlo, Yann LeCun, Max Tegmark, and Marin Soljačić. Tunable efficient unitary neural networks (eunn) and their application to rnns. In International Conference on Machine Learning, pages 17331741. PMLR, 2017. Imanol Schlag, Kazuki Irie, and Jürgen Schmidhuber. Linear transformers are secretly fast weight programmers. In International conference on machine learning, pages 93559366. PMLR, 2021. 9 Rupesh Kumar Srivastava, Klaus Greff, and Jürgen Schmidhuber. Highway networks. arXiv preprint arXiv:1505.00387, 2015. Songlin Yang, Bailin Wang, Yu Zhang, Yikang Shen, and Yoon Kim. Parallelizing linear transformers with the delta rule over sequence length. arXiv preprint arXiv:2406.06484, 2024."
        },
        {
            "title": "A Implementation and Parameterization Details",
            "content": "The Deep Delta Learning (DDL) framework relies on the efficient estimation of the reflection direction k(X), the scalar gate β(X), and the residual value v(X). While the theoretical results hold regardless of the specific topology used to approximate these functions, we outline two primary architectural instantiations for the generator functions: MLP-based and Attention-based parameterizations. Let the hidden state be Rddv . We denote the generator branch for the reflection vector as function ϕk : Rddv Rd. A.1 Parameterization of the Reflection Direction k(X) The geometric orientation of the Delta Operator is determined by k. We propose two distinct mechanisms for ϕk, allowing for different inductive biases regarding feature interaction. Option 1: MLP Parameterization. For architectures prioritizing global feature mixing with low computational overhead, we parameterize using Multi-Layer Perceptron (MLP) acting on aggregated statistics of the state matrix. ekMLP = MLP(Pool(X)) , kMLP = ekMLP ekMLP2 + ϵk (A.1) Here, Pool() is any aggregation that produces fixed-size vector representation of X, e.g., columnwise averaging (Rddv Rd) or flattening (Rddv Rddv ), followed by an MLP that outputs Rd. We enforce L2 normalization (with small ϵk > 0 for numerical stability) to satisfy the spectral assumptions in Theorem 3.1. Option 2: Attention-based Parameterization. To capture more granular dependencies within the value dimension, we can employ attention mechanism. A.2 Parameterization of the Gate β(X) and Value v(X) The Gating Branch. The scalar gate β requires bounded output in [0, 2]. We maintain lightweight design for this estimator: β(X) = 2 σ (cid:16) β tanh(WinPool(X))(cid:17) (A.2) where σ is the sigmoid function, ensuring smooth interpolation between identity, projection, and reflection. 11 The Value Branch. The residual value vector Rdv represents the content update. This branch, : Rddv Rdv , allows for flexible design choices. In our experiments, we utilize the same architecture chosen for the main backbone (e.g., if DDL is applied in Transformer, mirrors the Feed-Forward Network or Multi-Head Attention block structure) to ensure capacity alignment."
        }
    ],
    "affiliations": [
        "Princeton University",
        "University of California, Los Angeles"
    ]
}