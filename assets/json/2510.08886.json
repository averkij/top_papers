{
    "paper_title": "FinAuditing: A Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs",
    "authors": [
        "Yan Wang",
        "Keyi Wang",
        "Shanshan Yang",
        "Jaisal Patel",
        "Jeff Zhao",
        "Fengran Mo",
        "Xueqing Peng",
        "Lingfei Qian",
        "Jimin Huang",
        "Guojun Xiong",
        "Xiao-Yang Liu",
        "Jian-Yun Nie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The complexity of the Generally Accepted Accounting Principles (GAAP) and the hierarchical structure of eXtensible Business Reporting Language (XBRL) filings make financial auditing increasingly difficult to automate and verify. While large language models (LLMs) have demonstrated strong capabilities in unstructured text understanding, their ability to reason over structured, interdependent, and taxonomy-driven financial documents remains largely unexplored. To fill this gap, we introduce FinAuditing, the first taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs on financial auditing tasks. Built from real US-GAAP-compliant XBRL filings, FinAuditing defines three complementary subtasks, FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, each targeting a distinct aspect of structured auditing reasoning. We further propose a unified evaluation framework integrating retrieval, classification, and reasoning metrics across these subtasks. Extensive zero-shot experiments on 13 state-of-the-art LLMs reveal that current models perform inconsistently across semantic, relational, and mathematical dimensions, with accuracy drops of up to 60-90% when reasoning over hierarchical multi-document structures. Our findings expose the systematic limitations of modern LLMs in taxonomy-grounded financial reasoning and establish FinAuditing as a foundation for developing trustworthy, structure-aware, and regulation-aligned financial intelligence systems. The benchmark dataset is available at Hugging Face."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 1 ] . [ 1 6 8 8 8 0 . 0 1 5 2 : r FINAUDITING: Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs"
        },
        {
            "title": "Fengran Mo\nUniversity of Montreal\nCanada",
            "content": "Xueqing Peng Lingfei Qian Jimin Huang* The Fin AI USA jimin.huang@thefin.ai"
        },
        {
            "title": "Jeff Zhao\nUT Austin\nUSA",
            "content": "Xiao-Yang Liu Columbia University USA Jian-Yun Nie University of Montreal Canada CCS Concepts Applied computing Extensible Markup Language (XML); Information systems Test collections; Information extraction; Question answering. Keywords XBRL auditing, Benchmark, Large language model, Information retrieval, Information extraction, Question answering ACM Reference Format: Yan Wang, Keyi Wang, Shanshan Yang, Jaisal Patel, Jeff Zhao, Fengran Mo, Xueqing Peng, Lingfei Qian, Jimin Huang, Guojun Xiong, Xiao-Yang Liu, and Jian-Yun Nie. 2018. FINAUDITING: Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs. In Proceedings of Make sure to enter the correct conference title from your rights confirmation email (Conference acronym XX). ACM, New York, NY, USA, 14 pages. https: //doi.org/XXXXXXX.XXXXXXX Abstract The complexity of the Generally Accepted Accounting Principles (GAAP) and the hierarchical structure of eXtensible Business Reporting Language (XBRL) filings make financial auditing increasingly difficult to automate and verify. While large language models (LLMs) have demonstrated strong capabilities in unstructured text understanding, their ability to reason over structured, interdependent, and taxonomy-driven financial documents remains largely unexplored. To fill this gap, we introduce FINAUDITING, the first taxonomy-aligned, structure-aware, multi-document benchmark for evaluating LLMs on financial auditing tasks. Built from real USGAAP-compliant XBRL filings, FINAUDITING defines three complementary subtasks, FinSM for semantic consistency, FinRE for relational consistency, and FinMR for numerical consistency, each targeting distinct aspect of structured auditing reasoning. We further propose unified evaluation framework integrating retrieval, classification, and reasoning metrics across these subtasks. Extensive zero-shot experiments on 13 state-of-the-art LLMs reveal that current models perform inconsistently across semantic, relational, and mathematical dimensions, with accuracy drops of up to 6090% when reasoning over hierarchical multi-document structures. Our findings expose the systematic limitations of modern LLMs in taxonomygrounded financial reasoning and establish FINAUDITING as foundation for developing trustworthy, structure-aware, and regulationaligned financial intelligence systems. The benchmark dataset is available at Hugging Face1. *corresponding author 1FinAuditing Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference acronym XX, Woodstock, NY 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX Figure 1: Trends in the proportion of financial restatements among US companies from 2014 to 2024, categorized by reissuance and revision restatements. Data adapted from the Financial Times report, \"Accounting errors force US companies to pull statements in record numbers\" (Dec 9, 2024)."
        },
        {
            "title": "1\nThe Generally Accepted Accounting Principles (GAAP) provide the\nfoundation for corporate financial reporting in the United States, but",
            "content": "Conference acronym XX, June 0305, 2018, Woodstock, NY Yan et al. their growing complexity and continuous updates have made compliance verification increasingly difficult [3, 10]. Financial restatements caused by misapplied GAAP rules have become persistent source of reputational and regulatory risk, with recent reports indicating that severe reissuance restatements now account for the majority of filing corrections as shown in Figure. 1. While the introduction of the eXtensible Business Reporting Language (XBRL) has standardized digital disclosures, its hierarchical and interdependent structure introduces new challenges for automated auditing and anomaly detection. With the rapid advancement of large language models (LLMs), researchers have increasingly explored their applicability across wide range of tasks in the financial domain, including information extraction (IE) [16, 21, 26], text generation (TG) [18, 27], text analysis (TA) [22], question answering (QA) [1, 2, 2931], risk management (RM) [4], and forecasting (FO) [23], showing potential to improve GAAP interpretation and understanding in XBRL filings. However, as summarized in Table 1, existing benchmarks mainly rely on unstructured or semi-structured data drawn from single document or limited contexts, such as financial reports and news, while overlooking inter-dependencies, compositional hierarchies, and relational constraints that characterize real-world XBRL documents. Moreover, most financial benchmarks assess LLMs on isolated understanding and reasoning abilities, focusing on surface-level semantic understanding within plain texts or single documents. They rarely evaluate deeper capabilities such as structured semantic retrieval, hierarchical relation understanding, and multi-step reasoning, which are fundamental for capturing the structural and logical complexity inherent in financial data. Furthermore, while benchmarks such as FINTAGGING [26] initially utilize financial taxonomies, most do not fully leverage their hierarchical and relational structures, leaving taxonomy-driven reasoning largely unexplored. This omission limits the ability of current evaluations to measure how well LLMs can reason over structured, taxonomydriven, and cross-document dependencies, which are crucial for comprehensive understanding of finance. To address these gaps, we propose FINAUDITING, the first benchmark that evaluates LLMs on structured semantic retrieval, hierarchical relation understanding, and multi-step reasoning over taxonomydriven and interconnected XBRL filings, focusing on detecting inconsistencies and errors across structured financial documents. To this end, FINAUDITING encompasses three independent tasks: the financial semantic matching (FinSM) task, which examines whether models can detect semantic inconsistencies between financial disclosures and standardized taxonomy concepts; the financial relationship extraction (FinRE) task, which evaluates the ability to identify structural and hierarchical errors among related financial elements; and the financial mathematical reasoning (FinMR) task, which measures whether models can reason over numerical relationships to verify the logical correctness of reported values. To comprehensively assess these tasks, we design task-specific evaluation metrics that collectively capture retrieval accuracy, structural understanding, and numerical reasoning performance, using measures such as Hit Rate, Macro-F1, and error-rate indicators for multi-perspective evaluation of LLM capabilities. As result, our work introduces more comprehensive and systematic evaluation framework that measures LLMs capabilities on structured, hierarchical, and multiple financial documents. We conduct an extensive evaluation on FINAUDITING under the zero-shot setting, covering 13 state-of-the-art (SOTA) proprietary and open-source LLMs across three structured auditing tasks. Our results show that while leading models such as DeepSeek-V3 in FinSM, GPT-4o in FinRE, and Fin-o1-14B in FinMR achieve the highest performance within their respective tasks, all models exhibit substantial limitations. DeepSeek-V3 demonstrates better retrieval accuracy yet struggles with coverage and semantic alignment across interconnected structured documents. GPT-4o shows strong hierarchical relation understanding across all three relation types, whereas most open-source models have difficulty with the CombinationErr type. Fin-o1-14B, despite its domain specialization, remains weak in schema interpretation and structural consistency during numerical reasoning. Overall, these findings highlight that even frontier models fall short of reliable structured auditing, revealing persistent challenges in semantic retrieval, relational reasoning, and multi-step computation over hierarchical XBRL data. We conclude our main contributions as follows: Benchmark Design with Real-World Relevance: We introduce FINAUDITING, the first benchmark tailored to evaluate LLMs auditing and compliance capabilities over structured, hierarchical, and multi-document financial data. Constructed from real US-GAAPcompliant XBRL filings, it enables an end-to-end assessment of models, detecting semantic, relational, and numerical inconsistencies that mirror real auditing and regulatory scenarios. Task Coverage across Core Auditing Dimensions: We define three complementary subtasks, i.e., FinSM, FinRE, and FinMR, with each designed to capture distinct auditing dimension: semantic retrieval, hierarchical relationship understanding, and multi-step numerical reasoning. Together, these tasks provide structured and interpretable evaluation pipeline covering the primary sources of human and automated errors in corporate filings. Comprehensive Evaluation and Industry Insights: We develop unified evaluation framework with multi-perspective metrics and conduct extensive zero-shot experiments on 13 state-of-the-art LLMs. The results reveal that even the strongest models struggle to maintain consistency across interdependent XBRL documents and taxonomy definitions, highlighting critical gaps toward building reliable, regulation-aligned, and automation-ready financial auditing systems."
        },
        {
            "title": "2 Related work\nExisting financial NLP research has made progress in information\nextraction [16, 21, 26], semantic understanding [18, 22, 27], and\nnumerical reasoning [1, 2, 29‚Äì31], yet most benchmarks focus on\nunstructured or semi-structured data from single documents, such\nas financial reports or news articles, overlooking the structured and\ninterconnected nature of XBRL filings.",
            "content": "Early work on XBRL mainly targeted localized tasks. Loukas et al. [16] developed an XBRL tagging benchmark for numerical entity extraction, and Sharma et al. [21] proposed numeral labeling within XBRL sentences. Recently, Wang et al. [26] explored structured tag normalization via LLMs based on the US-GAAP taxonomy. Steven et al. [12] employed ChatGPT for tag retrieval, while Han et al. [8] introduced XBRL-Agent to assess LLMs analytical and FINAUDITING: Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs Conference acronym XX, June 0305, 2018, Woodstock, NY Table 1: Comparison of existing financial NLP benchmarks and FINAUDITING. The table shows whether each benchmark supports structured retrieval (Struct.Retrieval), hierarchical information extraction (Hiera.IE), multi-step numerical reasoning (MultiStep.Reasoning), taxonomy-driven supervision (Need.Taxonomy), and at the cross-document level (At.Cross-Doc). Benchmark FiNER [16] FNXL [21] Headlines [22] ECTSum [18] CCF [4] BIGDATA22 [23] FinTagging [26] FinQA [1] ConvFinQA [2] TAT-QA [31] MultiHiertt [29] DOCMATH-EVAL [30] FinAuditing (ours) Data Format Source Text Financial reports Text Financial reports Financial news Text Earnings call transcripts Text Credit card transactions Text Tweets, historical prices Text Financial reports Financial reports Financial reports Financial reports Financial reports Financial reports XBRL filings Text & Table Text & Table Text & Table Text & Table Semi-structured and hierarchical Text&Table Semi-structured document segmentation Structured and hierarchical multi-document segmentation Task IE IE TA TG RM FO IE, IR QA QA QA QA QA IE, IR, QA Struct.Retrieval? Hiera.IE? MultiStep.Reasoning? Need.Taxonomy? At.Cross-Doc? numerical abilities on XBRL data. However, these studies evaluate narrow subtasks and fail to capture the hierarchical and relational dependencies that define real-world XBRL structures. Beyond XBRL, benchmarks such as FinQA [1], ConvFinQA [2], TAT-QA [31], MultiHiertt [29], and DocMath-Eval [30] assess reasoning over textual and tabular data, yet they remain text-centric and lack taxonomy-driven supervision. To fill this gap, our proposed FINAUDITING provides the first benchmark to evaluate LLMs on structured, hierarchical, and cross-document reasoning grounded in real XBRL filings."
        },
        {
            "title": "3 FINAUDITING\nDetecting errors across the hierarchical and structured financial doc-\numents requires understanding semantics, hierarchical relations, and\nnumerical logic. As shown in Figure 2, to evaluate these capabilities,\nwe propose FINAUDITING, a benchmark that measures how well\nLLMs identify semantic, structural, and numerical inconsistencies\nacross multi-document XBRL filings.",
            "content": "Figure 2: Overview of the FINAUDITING benchmark framework for evaluating error detection on XBRL filings across three tasks."
        },
        {
            "title": "3.1 Preliminary knowledge\nThe US-GAAP Taxonomy serves as a standardized vocabulary and\nhierarchical framework that defines both the financial concepts (e.g.,\nus-gaap:CashAndDueFromBanks) and the relationships among them.",
            "content": "Table 2: Main Components of XBRL Filings"
        },
        {
            "title": "Calculation Linkbase",
            "content": "Description Actual financial data structured according to predefined taxonomy. Defines the structure, classification, and data types of financial elements. Specifies arithmetic relationships between elements to ensure logical consistency. Presentation Linkbase Organizes financial data into human-readable structure"
        },
        {
            "title": "Label Linkbase",
            "content": "for reporting. Captures semantic relationships between elements, such as component relations. Provides human-readable names and descriptions, supporting multiple languages. It provides consistent semantic structure for financial reporting, ensuring comparability and enabling automated analysis. An XBRL filing is structured digital financial report formatted in XML. It consists of multiple interconnected documents, including an instance document, schema document, and several linkbases, which together define and link the representations and relationships of financial elements (see Table 2 for details). Additionally, Fact corresponds to the reported values associated with US-GAAP tags in the instance documents. Context specifies details about reported fact (e.g. reporting period). Axis represents categorical dimension that allows financial value to be classified from different perspectives. For instance, \"RevenueByGeographyAxis\" organizes revenue based on geographic regions. Member is specific value within an Axis, representing particular category. For example, within \"RevenueByGeographyAxis\", \"USMember\" denotes revenue generated in the United States, while \"EuropeMember\" refers to European revenue."
        },
        {
            "title": "3.2 Task Formulation\nGiven a target XBRL filing with six interconnected, XML-format\ndocuments D = {ùëë1, ùëë2, ùëë3, ùëë4, ùëë5, ùëë6} and a reference structured\ndocument (US-GAAP taxonomy) T , where ùëëùëñ refers to one of the\ndocuments described in Table 2. We aim to enable LLMs to identify\nerrors E, including semantic matching errors, relationship extraction\nerrors, and mathematical reasoning errors, in the target filing by\nunderstanding its internal structure and semantics in conjunction\nwith domain knowledge derived from the reference taxonomy. We\nformulate it as follows:",
            "content": "where is the specific description of an error found in the D. ùëì : (D, ) (1) Conference acronym XX, June 0305, 2018, Woodstock, NY Yan et al. To operationalize error detection across the key dimensions of XBRL filings, we identify three fundamental capabilities required for effective model evaluation: semantic consistency, structural relationship understanding, and numerical reasoning. These capabilities correspond to the primary sources of errors observed in real-world filings and capture the essential challenges in interpreting and validating structured financial data. Based on this decomposition, FINAUDIT defines three tasks: FinSM for identifying mismatches between textual mentions and standardized taxonomy labels, FinRE for detecting errors in hierarchical or compositional relationships among financial items, and FinMR for verifying the correctness of numerical values based on accounting logic and contextual inference. Each sub-task is associated with specific input format tailored to its objective."
        },
        {
            "title": "3.2.1 Financial Semantic Matching (FinSM). The semantic\nmatching task focuses on identifying financial tags in an XBRL\nfiling that do not correctly align with standardized concepts defined\nin the US-GAAP taxonomy. The objective is to assess whether LLMs\ncan accurately detect mismatched US-GAAP tags by filtering out\nsemantically inconsistent or inappropriate concept assignments from\na set of complex, structured documents.",
            "content": "This task follows the information retrieval paradigm: given query ùëÑ describing financial term that represents either currency or concentration of credit risk, an XBRL filing D, and US-GAAP taxonomy . We formulate this task as follows:"
        },
        {
            "title": "FinRE Example",
            "content": "You are an auditor for XBRL filings. Given two concepts and the provided filing (schema, presentation, calculation, definition , label, instance, and US-GAAP taxonomy), determine which one of the following erroneous relationship types best describes the relationship between the two concepts: (1) Reversal Definition: <definition>. (2) Inappropriateness Definition: <definition>. (3) CombinationErr Definition: <definition>. You must reference the US-GAAP taxonomy to retrieve and check the two concepts across the entire filing. Output only one of the following labels exactly, chosen strictly from:[\"Reversal\", \"Inappropriateness\", \"CombinationErr\"]. No explanations. No extra text. Only the label. More specifically, given relationship label space ùêø = {ùëô1, ùëô2, ùëô3} with the definitions for each relationship label, an XBRL filing D, and US-GAAP taxonomy . The goal of this task is to classify the relation type ùëü for two specific elements ùëí1 and ùëí2 based on and . More specifically, we define three relation error types in ùêø: (1) ùëô1 = ùëÖùëíùë£ùëíùëüùë†ùëéùëô, occurs when the hierarchical relationship between sibling and child elements is mistakenly reversed; (2) ùëô2 = ùêºùëõùëéùëùùëùùëüùëúùëùùëüùëñùëéùë°ùëíùëõùëíùë†ùë†, indicates that child element is incorrectly associated with an inappropriate parent element; and (3) ùëô3 = ùê∂ùëúùëöùëèùëñùëõùëéùë°ùëñùëúùëõùê∏ùëüùëü , refers to an invalid combination of axis and member elements that violates the structural constraints defined in the taxonomy. Therefore, we formulate this task as follows. ùëì : (ùëí1, ùëí2, ùêø, D, ) (3) ùëì : (ùëÑ, D, ) (2) where is the specific identified relation type ùëü from the relationship label space ùêø. where is the set of mismatched US-GAAP tags after retrieving."
        },
        {
            "title": "FinSM Example",
            "content": "You are an auditor for XBRL filings. Given the question and the provided filing (schema, presentation, calculation, definition , label, instance, and US-GAAP taxonomy), identify erroneous US-GAAP concepts. You must reference the US-GAAP taxonomy to retrieve and check elements across the entire filing. Output only JSON array of strings, each string is one erroneous concept. Output Example: [\"us-gaap:Revenues\", \"us-gaap:OperatingIncomeLoss\"] No explanations. No extra text. Only the JSON array."
        },
        {
            "title": "3.2.3 Financial Mathematical Reasoning (FinMR). The math-\nematical reasoning task focuses on inferring numerical logic among\nfinancial elements based on structured representations, calculation\nhierarchies, and logical constraints defined in the XBRL filing. The\nobjective is to assess whether reported values are internally consis-\ntent and compliant with domain-specific standards.",
            "content": "Specifically, given two questions ùëû1 and ùëû2, where ùëû1 concerns the extraction of reported value and ùëû2 pertains to the calculation of the corresponding real value, an XBRL filing D, and US-GAAP taxonomy , the task is to extract the reported value ùë£ for given instance in and to compute the numeric value ùúá for that instance, which is then used to verify whether the reported value ùë£ is correct."
        },
        {
            "title": "3.2.2 Financial Relationship Extraction (FinRE). The relation-\nship extraction task aims to identify the structural relationships\namong financial elements in a filing. It assesses an LLM‚Äôs ability to\ninterpret hierarchical and compositional dependencies by aligning\nthe document structure with the external taxonomy. This task reflects\nthe LLMs‚Äô capacity for semantic understanding and structured data\ninterpretation within the context of financial reporting.",
            "content": "ùëì : (ùëû1, ùëû2, D, ) (4) where denotes the generated answer containing the extracted reported value ùë£ and the computed value ùúá, represented in JSON format. FINAUDITING: Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs Conference acronym XX, June 0305, 2018, Woodstock, NY Table 3: Distribution of common error types in XBRL filings. Broad Category Semantic-based Relation-based"
        },
        {
            "title": "Error Type\nFS with no associated calculation\nConcentration risk\nLocation axis with a single member\nSibling and child relationships\nAxis with inappropriate members\nInappropriate cash flow presentation",
            "content": "Calculation-based FS calculation check with no dimensional data"
        },
        {
            "title": "Total",
            "content": "# Cases Proportion (%) DQC US ID DQC_0099 DQC_0109 DQC_0137 DQC_0081 DQC_0001 DQC_0145 DQC_0126 DQC_0015 DQC_0117 - 8.49 4.77 4.58 12.54 6.89 4.82 5.72 8.10 4.44 60.33 386 217 208 570 313 219 260 368 202 2,"
        },
        {
            "title": "FinMR Example",
            "content": "You are an auditor for XBRL filings. Given the question and the provided filing (schema, presentation, calculation, definition , label, instance, and US-GAAP taxonomy), identify the reported value of financial element and calculate the actual value that should be reported based on calculation relationships. Answer strictly in the following JSON format: { \"extracted_value\": \"<numeric value reported in the instance document, or 0 if not found, keep the same format as in the XBRL filing>\", \"calculated_value\": \"<numeric value computed from calculation relationships, or 0 if not computable, use the same number formatting style as extracted_value>\" } No explanations. No extra text. Only the JSON object."
        },
        {
            "title": "4 Data Annotation and Evaluation Setting\n4.1 Raw Data Collection\nTo create this benchmark, we collected a total of 372 XBRL filings,\nwhich were processed through two steps: pre-screening and filtering.\nPre-screening serves as the foundation for filtering, enabling the\nidentification and collection of common errors in XBRL filings.",
            "content": "For the pre-screening step, we accessed the XBRL US official website2 and randomly collected 4,545 error messages from 372 companies spanning the years 2020 to 2024. These messages, linked to the DQC US rules3, were analyzed to assess rule coverage and to identify the most common error types. Our analysis revealed that the 4,545 messages contain more than 100 distinct error types. However, as shown in Table 3, 9 error types occur most frequently, accounting for approximately 60.33% of all cases. These errors can be grouped into three broad categories: semantic-based, relation-based, and calculation-based. For the filtering step, we extracted 1,102 error messages from the initial set of 4,545 as the first version of our benchmark, restricting the scope to data from 2022 to 2024 and selecting only those associated with the 9 common error types listed in Table 3 with corresponding DQC US IDs. Leveraging the sec-url field in these reports, we then accessed the SEC (U.S. Securities and Exchange Commission)4 database to obtain 218 corresponding XBRL filings, which constitute the foundation of our benchmark raw data."
        },
        {
            "title": "4.2 Data Annotation\nBased on the filtered raw data described in Section 4.1, we annotated\na total of 1,102 instances for the FINAUDITING benchmark. As the\nfirst benchmark to focus on structured, multi-document, and",
            "content": "2https://xbrl.us/data-quality/filing-results/ 3https://github.com/DataQualityCommittee/dqc_us_rules/tree/master 4https://www.sec.gov/ Table 4: Statistics of input length (tokens) across tasks. Tokens are calculated using the cl100k_base tokenizer."
        },
        {
            "title": "Task\nFinSM\nFinRE\nFinMR\nTotal",
            "content": "Total Tokens 11,170,024 14,819,627 11,845,255 37,834,906 Max Tokens 62,777 65,030 59,641 65,030 Avg. Tokens 33,848.56 33,680.97 35,678.48 34,338.64 Std. Tokens 11,402.49 10,847.23 9,852.73 10,700.82 Instances 330 440 332 1,102 taxonomy-driven inputs, FINAUDITING provides unique setting for evaluating LLMs. Table 4 shows that each task involves long and complex inputs, with average lengths exceeding 33k tokens per instance when measured using the cl100k_base GPT tokenizer. Among them, FinRE contains the largest total token volume and number of instances, while FinMR exhibits the longest average input length per instance. These statistics highlight the structural and relational complexity of XBRL filings, underscoring the need for careful annotation to ensure reliable evaluation."
        },
        {
            "title": "4.2.2 Annotation with LLM Judgment. Each XBRL filing is\nretrieved using the \"sec-url\" from the error messages, which provide\nexplicit details, including the specific document components (e.g.,\ncalculation linkbase or instance document) where errors occur, the\nincorrect values, the invalid relationships, and the expected correct\nresults. Therefore, we do the following processing for each task\nusing the GPT-4o-mini model. We provide the specific prompts for\neach task in Appendix A.",
            "content": "(1) For the FinSM task, we extract all reported incorrect concepts from the error messages as the ground-truth answers and retrieve the corresponding sub-filing segments based on these concepts to serve as the input XBRL filing. (2) For the FinRE task, we map the three defined relationship types to the corresponding error messages based on the DQC US ID. Specifically, DQC_0081 corresponds to Reversal, DQC_0001 to CombinationErr, and DQC_0145 to Inappropriateness. For each relationship type, we extract the participating elements from the error messages and construct relationship triples in the format: (head element, relation, tail element). The head and tail elements are used as inputs, while the relation serves as the ground-truth label. Similarly, based on the extracted elements, we retrieve the corresponding sub-filing segments to serve as the input XBRL filing. Conference acronym XX, June 0305, 2018, Woodstock, NY Yan et al. (3) For the FinMR task, based on our task formulation, we annotate two key aspects for each question: the reported value in the instance document and the inferred generative value. Both answers are derived from the error message. Since the question in this task is concept-dependent, we extract the relevant concept and period from the error message to construct the input question. Similarly, we retrieve the corresponding sub-filing segments based on the extracted concepts to serve as the input XBRL filing."
        },
        {
            "title": "4.3 Evaluation Metrics\nOur goal is to explore the capabilities of current state-of-the-art\nLLMs on FINAUDITING and gain a deeper understanding of their\nstrengths and limitations. To achieve this, we evaluate LLM perfor-\nmance across various aspects for each task. The details are shown in\nAppendix C.",
            "content": "For the FinSM task, we evaluate retrieval performance using Hit Rate (ùêªùëÖ@ùëò) [9], Recall (ùëÖ@ùëò), and Macro-F1 (ùëÄùêπ 1@ùëò), where ùëò = 1, 5, 10, 20. Hit Rate measures whether the model retrieves at least one relevant item within the top-ùëò results. Recall assesses how completely the model retrieves all relevant elements. Macro-F1 balances precision and recall by averaging query-level F1 scores, providing an overall view of retrieval accuracy and coverage [20]. For the FinRE task, we evaluate model performance using Accuracy (Acc), Macro Precision (P), Macro Recall (R), and Macro F1. Accuracy measures the overall proportion of correctly classified relations. Macro Precision and Macro Recall average performance across the three relation types. Macro F1 provides balanced summary by combining precision and recall for each relation type and averaging the results. For the FinMR task, we evaluated the mathematical reasoning capabilities of various LLMs using the LLM-as-a-judge approach [7, 13] to measure overall Accuracy. Additionally, we designed three error-rate indicators, including the structural error rate (SER), extraction error rate (EER), and calculation error rate (CER), to evaluate the quality of the generated answers by LLMs. Together, these metrics provide comprehensive evaluation of mathematical reasoning performance."
        },
        {
            "title": "5 Results and Analysis\n5.1 FinSM performance\nTable 5 presents the Hit-Rate@k performance of various LLMs on\nthe FinSM task under zero-shot settings. The results show that LLMs\nstill struggle to detect semantically inconsistent US-GAAP tags,\nwith the best model (DeepSeek-V3) achieving only 11.89% average\nhit rate. This highlights the challenge of aligning models with fine-\ngrained financial semantics. Several open-source models, including\nDeepSeek-V3, Qwen3-235B, and gemma-3-27b-it, outperform the\nclosed-source GPT-4o, indicating that open-source progress is clos-\ning the gap in domain-specific retrieval. In contrast, financial domain\nmodels (Fin-o1-14B and Fin-R1) perform poorly, with Fin-o1-14B\nfailing to produce any correct hits. This suggests a mismatch be-\ntween their generative training focus and the retrieval-oriented nature\nof FinSM. Qwen3-32B also yields zero hit rates, reflecting possible\ninstability in mid-scale variants. Overall, model scale contributes to\nperformance gains (e.g., Qwen3-235B vs. Qwen3-32B; gemma-27B",
            "content": "FINAUDITING: Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs Conference acronym XX, June 0305, 2018, Woodstock, NY vs. gemma-12B), but the Llama series shows inconsistent scaling, signifying the importance of architecture and alignment strategies. Table 6: Recall@k (%) performance under the zero-shot settings on the FinSM task. Table 5: Hit-Rate@k (%) performance under the zero-shot settings on the FinSM task. LLMs GPT-4o DeepSeek-V3 Qwen3-235B-A22B-Instruct-2507 Llama-4-Scout-17B-16E-Instruct Qwen2.5-72B-Instruct Llama-3.3-70B-Instruct Qwen3-32B gemma-3-27b-it gemma-3-12b-it Llama-3.1-8B-Instruct Llama-3.2-3B-Instruct Fin-o1-14B Fin-R ùêªùëÖ@1 ùêªùëÖ@5 ùêªùëÖ@10 ùêªùëÖ@20 ùêªùëÖ@ùê¥ùëâ ùê∏ 7.88 11.52 9.09 1.82 7.88 5.15 0.00 10.00 8.79 6.06 3.33 0.00 2.12 9.09 12.42 10.00 3.03 8.48 5.15 0.00 10.30 10.30 6.06 4.55 0.00 2.73 9.09 11.82 10.00 2.42 8.18 5.15 0.00 10.30 9.70 6.06 3.94 0.00 2.12 9.09 11.82 10.00 2.73 8.48 5.15 0.00 10.30 9.70 6.06 4.24 0.00 2.42 8.79 11.89 9.77 2.50 8.26 5.15 0.00 10.23 9.62 6.06 4.02 0.00 2.35 Table 6 reports the Recall@k performance of different LLMs on the FinSM task under zero-shot settings. Overall recall values remain low, with the best model, DeepSeek-V3, achieving only 8.83% on average. This suggests that while some models can identify relevant tags, they fail to comprehensively retrieve all mismatched US-GAAP elements within the top-ùëò predictions. Large open-source models such as DeepSeek-V3, Qwen3-235B, and gemma-3-27b-it outperform GPT-4o, showing growing competitiveness in capturing broader semantic coverage. In contrast, financial domain models (Fin-o1-14B and Fin-R1) perform poorly, with Fin-o1-14B yielding zero recall, indicating mismatch between domain-specific training objectives and retrieval-oriented evaluation. Similarly, Qwen3-32B fails to retrieve relevant tags, reflecting inconsistency in mid-scale variants. As with the Hit Rate results, scaling trends are non-monotonic: larger models often perform better, but the Llama series exhibits irregular improvements. These results highlight the difficulty of FinSM from recall perspective, which demands that models capture all semantically inconsistent tags across interrelated XBRL filings and taxonomy documents. Table 7 presents the Macro-F1@k results for the FinSM task under zero-shot settings. Compared with Hit Rate and Recall, Macro-F1 provides stricter measure by jointly considering precision and recall, reflecting the balance between correctly retrieved mismatched tags and false positives. Overall scores remain very low, with the best model reaching only 9.35% on average, underscoring the difficulty of achieving both accuracy and coverage in structured financial retrieval. DeepSeek-V3 performs best across all cutoffs, followed by Qwen3-235B and gemma-3-27b-it, which also outperform GPT-4o, showing that open-source models are increasingly competitive in balancing precision and recall. In contrast, financial domain models perform poorly, with Fin-o1-14B producing no correct outputs and Fin-R1 reaching only 1.84%, suggesting that domain adaptation without retrieval alignment may increase false positives. Scaling effects appear within some model families, but the Llama series shows inconsistent improvements, indicating that architecture and alignment strategies influence performance more than model size. These findings highlight that FinSM requires models not only to retrieve relevant tags but also to control spurious predictions and maintain precision-recall balance across structured, multi-document inputs. LLMs GPT-4o DeepSeek-V3 Qwen3-235B-A22B-Instruct-2507 Llama-4-Scout-17B-16E-Instruct Qwen2.5-72B-Instruct Llama-3.3-70B-Instruct Qwen3-32B gemma-3-27b-it gemma-3-12b-it Llama-3.1-8B-Instruct Llama-3.2-3B-Instruct Fin-o1-14B Fin-R1 ùëÖ@1 4.93 7.06 6.38 0.92 4.31 3.34 0.00 5.91 5.09 2.88 1.64 0.00 1.28 ùëÖ@5 6.82 8.82 7.77 1.58 4.81 3.78 0.00 7.81 7.06 4.32 3.23 0.00 2.02 ùëÖ@10 6.98 9.33 8.17 1.91 5.18 3.85 0.00 8.20 7.61 4.73 3.68 0.00 2.32 ùëÖ@20 7.01 10.11 8.33 2.21 5.18 3.85 0.00 8.26 8.49 4.82 3.98 0.00 2. ùëÖ@ùê¥ùëâ ùê∏ 6.43 8.83 7.66 1.66 4.87 3.71 0.00 7.54 7.06 4.19 3.13 0.00 2.06 Table 7: Macro-F1@k (%) performance under the zero-shot settings on the FinSM task. LLMs GPT-4o DeepSeek-V3 Qwen3-235B-A22B-Instruct-2507 Llama-4-Scout-17B-16E-Instruct Qwen2.5-72B-Instruct Llama-3.3-70B-Instruct Qwen3-32B gemma-3-27b-it gemma-3-12b-it Llama-3.1-8B-Instruct Llama-3.2-3B-Instruct Fin-o1-14B Fin-R1 ùëÄùêπ 1@1 ùëÄùêπ 1@5 ùëÄùêπ 1@10 ùëÄùêπ 1@20 ùëÄùêπ 1@ùê¥ùëâ ùê∏ 5.43 7.73 6.85 1.08 4.73 3.65 0.00 6.47 5.57 3.29 2.00 0.00 1.46 6.91 9.54 7.83 1.55 5.29 4.07 0.00 8.38 7.62 4.47 2.88 0.00 1. 7.03 9.98 7.96 1.76 5.42 4.14 0.00 8.67 8.04 4.77 2.95 0.00 1.97 7.02 10.17 7.97 1.80 5.42 4.14 0.00 8.71 8.27 4.80 2.94 0.00 2.00 6.60 9.35 7.65 1.55 5.21 4.00 0.00 8.06 7.38 4.33 2.69 0.00 1.84 Overall, the results confirm that FinSM presents highly challenging setting based on structured, multi-document inputs. Across Hit Rate, Recall, and Macro-F1, current LLMs struggle with both coverage and the balance between precision and recall. Large opensource models such as DeepSeek-V3, Qwen3-235B, and gemma3-27b-it outperform GPT-4o, whereas financial domain models fail to adapt, showing that domain specialization alone does not guarantee retrieval effectiveness. FinSM therefore serves as rigorous benchmark for evaluating LLMs capacity for semantic retrieval and taxonomy alignment in realistic financial auditing contexts."
        },
        {
            "title": "5.2 FinRE performance\nTable 8 reports the overall performance of various LLMs on the\nFinRE task under zero-shot settings. The results demonstrate that\nrelationship extraction is highly challenging for current LLMs, as\nit requires precise interpretation of hierarchical and compositional\ndependencies within structured XBRL filings and their alignment\nwith the US-GAAP taxonomy. Among all models, GPT-4o achieves\nthe best performance with 91.82% accuracy and balanced precision,\nrecall, and F1 (around 90%), showing a strong capability to consis-\ntently identify different types of erroneous relationships. DeepSeek-\nV3 follows with 82.73% accuracy and 80.68% macro-F1, which,\nwhile competitive, indicates a noticeable gap compared to GPT-4o\nin balancing precision and recall.",
            "content": "By contrast, the majority of open-source models lag significantly. Qwen2.5-72B and Qwen3-235B achieve moderate performance (Acc 67.50% and 62.73%, respectively), suggesting partial capability in capturing structural errors but with reduced robustness. Mid-scale models such as gemma-3-27b-it (Acc 45.91%) and gemma-3-12b-it Conference acronym XX, June 0305, 2018, Woodstock, NY Yan et al. Table 8: The overall performance (%) under the zero-shot settings on the FinRE task. LLMs GPT-4o DeepSeek-V3 Qwen3-235B-A22B-Instruct-2507 Llama-4-Scout-17B-16E-Instruct Qwen2.5-72B-Instruct Llama-3.3-70B-Instruct Qwen3-32B gemma-3-27b-it gemma-3-12b-it Llama-3.1-8B-Instruct Llama-3.2-3B-Instruct Fin-o1-14B Fin-R1 Acc 91.82 82.73 62.73 27.50 67.50 28.86 0.00 45.91 27.95 30.45 17.73 0.00 32. Macro 90.15 87.81 69.22 17.09 72.14 23.06 0.00 35.97 31.78 15.25 22.31 0.00 34.05 Macro 90.03 80.19 67.18 35.82 71.50 38.26 0.00 53.00 25.81 26.58 12.74 0.00 21.00 Macro F1 90.09 80.68 63.56 22.71 68.27 21.54 0.00 40.85 27.37 19.34 15.70 0.00 25.97 (Acc 27.95%) perform weakly, while the entire Llama series struggles across scales. In particular, Llama-4-Scout-17B-16E-Instruct, despite being an extremely large-scale model, reaches only 27.50% accuracy and 22.71% macro-F1, performing worse than some midscale Qwen and Gemma models. This highlights that scale alone does not translate into structural reasoning ability without proper alignment to taxonomy-driven tasks. Notably, Qwen3-32B and the financial domain-specific Fin-o1-14B completely fail (0.00% across all metrics), while Fin-R1 remains weak with only 32.73% accuracy and 25.97% macro-F1, underscoring that domain specialization without explicit structural training does not generalize to relationship extraction. Figure 3 reveals substantial variation across relation types. GPT4o demonstrates the most consistent performance, exceeding 80% F1 on Reversal and Inappropriateness and achieving 100% on CombinationErr, showing strong robustness across all categories. DeepSeekV3 follows with balanced results, though consistently lower than GPT-4o. In contrast, most open-source models struggle on CombinationErr, with many collapsing to near zero, indicating that axismember validation across linkbases poses the greatest challenge. The Llama family performs poorly across all types, and both Qwen3-32B and Fin-o1-14B completely fail, achieving 0.00 in every category. Interestingly, Fin-R1 reaches moderate 60.51 on CombinationErr but remains weak elsewhere. These results suggest that while top-tier general models can handle structural relationship extraction, most open-source and financial-domain models lack the structural alignment needed, with CombinationErr emerging as the hardest category overall. Figure 3: The F1-score (%) for individual relation type under the zero-shot settings on the FinRE task. In summary, the FinRE results show that while frontier models such as GPT-4o achieve strong and balanced performance across Figure 4: The accuracy (%) under the zero-shot settings on the FinMR task. Figure 5: The error-rate results (%) for the FinMR task, where SER denotes the structural error rate, EER represents the extraction error rate, and CER refers to the calculation error rate. all relation types, most open-source models lag considerably, and domain-specific financial LLMs fail to generalize to this structured classification setting. At the label level, CombinationErr emerges as the most difficult category, where even competitive models face sharp drop, indicating that validating axismember consistency requires reasoning across multiple interrelated documents. Overall, FinRE highlights that accurate financial relationship extraction demands more than scale or domain specialization, instead requiring explicit alignment with taxonomy-driven, cross-document reasoning."
        },
        {
            "title": "5.3 FinMR performance\nFigure 4 reports the zero-shot accuracy on the FinMR task. Overall,\nall models show limited ability to perform structured mathematical\nreasoning, with the best model, Fin-o1-14B, reaching only 13.86%\naccuracy. This highlights the inherent difficulty of the task, which\nrequires models to extract, compute, and verify numerical relations\ndefined across hierarchical calculation structures in XBRL filings.\nAcross all LLMs, as shown in Figure 5, calculation errors (CER)\ndominate the performance gap. Most models exhibit extremely high\nCER values, typically between 70% and 83%, indicating that they\nstruggle to follow multi-step arithmetic relationships or maintain\nconsistency with domain-specific calculation rules. In contrast, ex-\ntraction errors (EER) are comparatively moderate (roughly 10‚Äì22%),\nsuggesting that LLMs can often locate the reported value but fail\nwhen performing reasoning-based computation.",
            "content": "Interestingly, Fin-o1-14B achieves the highest accuracy while maintaining very low CER (9%), yet it suffers from high structural error rate (SER = 71%). This pattern implies that once the model identifies valid structural path, it performs reliable arithmetic reasoning but often fails to interpret the schema and linkbase FINAUDITING: Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs Conference acronym XX, June 0305, 2018, Woodstock, NY structure correctly. Similarly, Qwen3-32B and Fin-R1 collapse due to severe structural errors, showing that without schema alignment, even accurate computation cannot be triggered. Taken together, these results suggest that FinMR poses fundamentally harder challenge than surface-level numerical QA. Success requires not only multi-step calculation ability but also schemaaware reasoning and structural grounding across multiple XBRL documents, capabilities that remain far beyond what current LLMs can reliably achieve."
        },
        {
            "title": "6 Conclusion\nIn this work, we present FINAUDITING, the first benchmark de-\nsigned to evaluate LLMs on structured, hierarchical, and taxonomy-\ndriven financial multi-documents. Built upon real-world XBRL fil-\nings, FINAUDITING defines three complementary tasks, FinSM,\nFinRE, and FinMR, which jointly assess structured semantic consis-\ntency, hierarchical relationship understanding, and multi-step math-\nematical reasoning. Our extensive zero-shot evaluation of SOTA\nLLMs shows that even leading models such as GPT-4o, DeepSeek-\nV3, and Fin-o1-14B still struggle with cross-document reasoning,\nstructured interpretation, and taxonomy alignment. These findings\nhighlight the limitations of current LLMs in financial auditing and\nemphasize the importance of developing models with stronger struc-\ntural grounding and reasoning awareness. We release all datasets and\nevaluation code to support future research on trustworthy financial\nintelligence.",
            "content": "References [1] Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, et al. 2021. Finqa: dataset of numerical reasoning over financial data. arXiv preprint arXiv:2109.00122 (2021). [2] Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022. Convfinqa: Exploring the chain of numerical reasoning in conversational finance question answering. arXiv preprint arXiv:2210.03849 (2022). [3] Roger Debreceny, Stephanie Farewell, Maciej Piechocki, Carsten Felden, and Andr√© Gr√§ning. 2010. Does it add up? Early evidence on the data quality of XBRL filings to the SEC. Journal of Accounting and Public Policy 29, 3 (2010), 296306. [4] Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang Han, Zhengyu Chen, Alejandro Lopez-Lira, and Hao Wang. 2023. Empowering many, biasing few: Generalist credit scoring through large language models. arXiv preprint arXiv:2310.00566 (2023). [5] Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. 2024. framework for few-shot language model evaluation. doi:10.5281/zenodo.12608602 [6] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783 (2024). [7] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. 2024. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594 (2024). [8] Shijie Han, Haoqiang Kang, Bo Jin, Xiao-Yang Liu, and Steve Yang. 2024. Xbrl agent: Leveraging large language models for financial report analysis. In Proceedings of the 5th ACM International Conference on AI in Finance. 856864. [9] Xiangnan He, Lizi Liao, Hanwang Zhang, Liqiang Nie, Xia Hu, and Tat-Seng Chua. 2017. Neural collaborative filtering. In Proceedings of the 26th international conference on world wide web. 173182. [10] Rani Hoitash and Udi Hoitash. 2018. Measuring accounting reporting complexity with XBRL. The Accounting Review 93, 1 (2018), 259287. [11] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [12] Steven Katz, Yu Gu, and Lanxin Jiang. 2024. Information extraction from ESG reports using NLP: ChatGPT comparison. Available at SSRN 4836432 (2024). [13] Haitao Li, Qian Dong, Junjie Chen, Huixue Su, Yujia Zhou, Qingyao Ai, Ziyi Ye, and Yiqun Liu. 2024. Llms-as-judges: comprehensive survey on llm-based evaluation methods. arXiv preprint arXiv:2412.05579 (2024). [14] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437 (2024). [15] Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, Chao Li, Sheng Xu, Dezhi Chen, Yun Chen, Zuo Bai, and Liwen Zhang. 2025. Fin-R1: Large Language Model for Financial Reasoning through Reinforcement Learning. arXiv:2503.16252 [cs.CL] https://arxiv.org/abs/2503.16252 [16] Lefteris Loukas, Manos Fergadiotis, Ilias Chalkidis, Eirini Spyropoulou, Prodromos Malakasiotis, Ion Androutsopoulos, and Georgios Paliouras. 2022. FiNER: Financial numeric entity recognition for XBRL tagging. arXiv preprint arXiv:2203.06482 (2022). [17] AI Meta. 2025. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https://ai. meta. com/blog/llama-4-multimodal-intelligence/, checked on 4, 7 (2025), 2025. [18] Rajdeep Mukherjee, Abhinav Bohra, Akash Banerjee, Soumya Sharma, Manjunath Hegde, Afreen Shaikh, Shivani Shrivastava, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, et al. 2022. Ectsum: new benchmark dataset for bullet point summarization of long earnings call transcripts. arXiv preprint arXiv:2210.12467 (2022). [19] Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Jimin Huang, and Qianqian Xie. 2025. Fino1: On the Transferability of Reasoning Enhanced LLMs to Finance. arXiv preprint arXiv:2502.08127 (2025). [20] Guy Shani and Asela Gunawardana. 2010. Evaluating recommendation systems. In Recommender systems handbook. Springer, 257297. [21] Soumya Sharma, Subhendu Khatuya, Manjunath Hegde, Afreen Shaikh, Koustuv Dasgupta, Pawan Goyal, and Niloy Ganguly. 2023. Financial numeric extreme labelling: dataset and benchmarking. In Findings of the Association for Computational Linguistics: ACL 2023. 35503561. [22] Ankur Sinha and Tanmay Khandait. 2021. Impact of news on the commodity market: Dataset and results. In Future of Information and Communication Conference. Springer, 589601. [23] Yejun Soun, Jaemin Yoo, Minyong Cho, Jihyeong Jeon, and Kang. 2022. Accurate stock movement prediction with self-supervised learning from sparse noisy tweets. In 2022 IEEE International Conference on Big Data (Big Data). IEEE, 16911700. [24] Gemma Team. 2024. Gemma. (2024). doi:10.34740/KAGGLE/M/3301 [25] Qwen Team. 2025. Qwen3 Technical Report. arXiv:2505.09388 [cs.CL] https: //arxiv.org/abs/2505.09388 [26] Yan Wang, Yang Ren, Lingfei Qian, Xueqing Peng, Keyi Wang, Yi Han, Dongji Feng, Xiao-Yang Liu, Jimin Huang, and Qianqian Xie. 2025. FinTagging: An LLM-ready Benchmark for Extracting and Structuring Financial Information. arXiv preprint arXiv:2505.20650 (2025). [27] Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023. Pixiu: large language model, instruction data and evaluation benchmark for finance. arXiv preprint arXiv:2306.05443 (2023). [28] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zhihao Fan. 2024. Qwen2 Technical Report. arXiv preprint arXiv:2407.10671 (2024). [29] Yilun Zhao, Yunxiang Li, Chenying Li, and Rui Zhang. 2022. MultiHiertt: Numerical reasoning over multi hierarchical tabular and textual data. arXiv preprint arXiv:2206.01347 (2022). [30] Yilun Zhao, Yitao Long, Hongjun Liu, Ryo Kamoi, Linyong Nan, Lyuhao Chen, Yixin Liu, Xiangru Tang, Rui Zhang, and Arman Cohan. 2023. DocMath-eval: Evaluating math reasoning capabilities of LLMs in understanding long and specialized documents. arXiv preprint arXiv:2311.09805 (2023). [31] Fengbin Zhu, Wenqiang Lei, Youcheng Huang, Chao Wang, Shuo Zhang, Jiancheng Lv, Fuli Feng, and Tat-Seng Chua. 2021. TAT-QA: question answering benchmark on hybrid of tabular and textual content in finance. arXiv preprint arXiv:2105.07624 (2021). Conference acronym XX, June 0305, 2018, Woodstock, NY Yan et al. LLM Judgment for Data Annotation A.1 Prompt for the FinSM task DQC_0099 Example You are given DQC validation message. Your task is to extract two specific pieces of information: 1. **Statement Name**: Extract the name of the financial statement mentioned in the message. If it appears in phrase like \"0000007 - Statement - Consolidated Statements of Operations\", ignore any prefixes such as \"- Statement -\" or ID numbers. Only return the clean name of the statement (e. g., \"Consolidated Statements of Operations\"). 2. **Erroneous us-gaap Tag(s)**: From all `us-gaap:` prefixed tags mentioned in the message, extract only those that are clearly described as having an error or violation. tag should be included **only if** it satisfies both of the following: - It is explicitly stated to be **not included in any calculation relationship**. - The message states that it **will produce an error**, **violates rule **, or **should have been reported differently**. Do not include `us-gaap:` tags that are merely suggested, referenced as valid abstract parents, or shown as examples for correction. 3. **Reasoning**: For each extracted erroneous tag, provide short explanation summarizing why it was classified as erroneous, based on the message. Output your result as structured JSON in the following format: ```json { \"statement_name\": \"...\", \"error_tags\": [ \"tag\": \"...\", \"reason\": \"...\" { } ] } ``` DQC_0109 Example You are given DQC validation message. Your task is to extract all Dimensions information from the message. 1. Extract all key-value pairs under Dimensions, where each dimension is formatted as us-gaap:XXX=YYY. 2. For each extracted dimension, provide short explanation in the \"reason\" field based on the message context. If no explicit issue is mentioned about specific dimension, you may leave the \"reason\" as an empty string. Output your result as structured JSON in the following format: ```json { \"error_tags\": [ \"dimension\": \"...\", \"reason\": \"...\" { } ] } ``` DQC_0137 Example You are given DQC validation message. Your task is to extract the following information: 1. Target Tag: Extract the primary us-gaap tag being referenced or discussed as erroneous in the message. Only include the tag explicitly stated as problematic or requiring correction. 2. Dimensions: Extract all dimension key-value pairs listed under \"Dimensions\" in the message. Each dimension should be in the format us-gaap:XXX=YYY. 3. For each extracted dimension, provide short explanation in the \"reason\" field based on the message context. If the message does not explicitly mention an issue with the dimension, you may leave the \"reason\" field empty or give general summary. Output your result as structured JSON in the following format: ```json { \"target_tag\": \"...\", \"error_tags\": [ \"dimension\": \"...\", \"reason\": \"...\" { } ] } ``` A.2 Prompt for the FinRE task DQC_0081 Example You are given DQC validation message. Your task is to extract two specific pieces of information: 1. **Statement Name**: Extract the name of the financial statement mentioned in the message. If it appears in phrase like \"00000002 - Statement - BALANCE SHEET\", ignore any prefixes such as ID numbers or \"- Statement -\". Only return the clean name of the statement (e.g., \"BALANCE SHEET\"). 2. **Erroneous us-gaap Tag Relationships**: From all `us-gaap:` tags mentioned in the message, identify pairs of tags where the relationship between them is described as incorrect or inconsistent. tag pair should be extracted **only if** it satisfies both of the following conditions: - The two tags are stated to be in specific structural relationship in the filer's taxonomy (e.g., sibling, parent-child). - The message clearly indicates that this relationship contradicts the definition in the official US-GAAP taxonomy, or requires reviewer attention for potential misclassification. 3. **Reasoning**: For each extracted tag pair, provide short explanation summarizing the inconsistency or incorrect relationship between them as described in the message. Output your result as structured JSON in the following format: ```json { \"statement_name\": \"...\", \"error_tags\": [ \"tag1\": \"...\", \"tag2\": \"...\", \"reason\": \"...\" { } ] } ``` DQC_0001 Example You are an expert in XBRL taxonomies, tasked with parsing DQC validation messages for relationship errors (Rule DQC.US.0001), specifically an axis with an inappropriate member. 1. **Main Concept**: Extract the primary concept that is being dimensionally qualified. This is usually the first concept mentioned in the message. 2. **Dimension Pair**: From the 'Dimensions' field in the message, extract the full axis and member combination string **exactly as it appears**, including all prefixes. 3. **Reasoning**: Provide short explanation that the member is unallowable for the specified axis, as described in the message. Output your result as single, structured JSON object in the following format : ```json { \"main_concept\": \"UnrealizedGainLossOnCashFlowHedgingInstruments\", \"dimension_pair\": \"us-gaap:FairValueByFairValueHierarchyLevelAxis=us-gaap: FairValueMeasurementsRecurringMember\", \"reason\": \"The message indicates the member is unallowable for the specified axis.\" } ``` DQC_0145 Example You are an expert in XBRL taxonomies, tasked with parsing DQC validation messages for relationship errors (Rule DQC.US.0145), specifically ' Inappropriate Cash Flow Presentation'. 1. **Head Concept**: Extract the primary concept that is inappropriately presented. This is usually the first concept mentioned in the message. 2. **Tail Concept**: Extract the presentation concept that the head concept is incorrectly descendant of. 3. **Reasoning**: Provide short explanation that the head concept should not be presented as component of the tail concept. Output your result as single, structured JSON object in the following format : ```json { \"head_concept\": \"element 1\", \"tail_concept\": \"element 2\", \"reason\": \"The message indicates the head concept is inappropriately presented as descendant of the tail concept and should be outside this group.\" } ``` FINAUDITING: Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs Conference acronym XX, June 0305, 2018, Woodstock, NY A.3 prompt for the FinMR task DQC_0015 Example You are given DQC validation message. Your task is to extract the following key information: 1. **Erroneous Tag Information:**: Identify the primary us-gaap tag in the message that is reported with an incorrect value due to sign (positive/negative) error. For this tag, extract: - `tag`: The us-gaap element tag with the error. - `period`: The reporting period for this tag, as shown in the \"Period\" field. - `reported_value`: The actual value reported in the filing, as stated in the message or \"Value\" field. - `correct_value`: The correct value, which should be the absolute value of the reported value (i.e., reported_value without the negative sign), as implied or explicitly required by the message. - `reason`: Briefly explain why the reported value is considered incorrect, based on the message content (for example, \"This element should not have negative value. The amount should be input as positive value and, if necessary, negated label should be provided.\"). Return your result as structured JSON in the following format: ```json { \"error_tags\": \"tag\": \"...\", \"period\": \"...\", \"reported_value\": \"...\", \"correct_value\": \"...\", \"reason\": \"...\" { } } ``` DQC_0117 Example You are an expert financial analyst parsing DQC validation messages for mathematical reasoning errors (Rule DQC.US.0117). Your task is to extract all required components from the message. 1. **Statement Name**: Extract the clean name of the financial statement (e.g ., \"Condensed Consolidated Income Statements\"). 2. **Input Data**: From the properties list at the end of the message, extract the following: * `target_concept`: The full name of the primary `us-gaap:` concept being evaluated. * `period`: The reporting period for the fact (e.g., \"2020-10-01 to 2020-12-31\"). 3. **Output Data**: From the main body of the message, extract the following values: * `extracted_value`: The value of the concept as reported in the filing (e .g., \"255,500,000\"). * `calculated_value`: The correct value based on the dimensional breakdown (e.g., \"142,400,000\"). * `is_correct`: This should always be \"No\" for these error messages. Output your result as single, structured JSON object in the following format : ```json { \"statement_name\": \"...\", \"input\": { \"target_concept\": \"us-gaap: RevenueFromContractWithCustomerIncludingAssessedTax\", \"period\": \"...\" }, \"output\": { \"extracted_value\": \"...\", \"calculated_value\": \"...\", \"is_correct\": \"No\" } } ``` DQC_0126 Example You are given DQC validation message. Your task is to extract the following key information: 1. **Statement Name**: Extract the name of the financial statement mentioned in the message. If it appears in phrase like \"000004 - Statement - CONSOLIDATED STATEMENTS OF OPERATIONS (Unaudited)\", ignore any numeric prefixes and the phrase \"- Statement -\". Only return the clean name of the statement, such as \"CONSOLIDATED STATEMENTS OF OPERATIONS (Unaudited)\". 2. **Erroneous Total Element Tag Information**: Extract detailed information about the total element (`us-gaap:` tag) that is reported incorrectly. Specifically, you should extract: - `tag`: The total element tag, as indicated by the **\"Total Element\"** field in the message. - `period`: The reporting period associated with this tag, as indicated by the **\"Total period\"** field. - `reported_value`: The value that was reported in the filing, taken from the **\"Total Value\"** field. - `correct_value`: The expected value, i.e., the **sum of the child components** as defined in the calculation linkbase. - This is usually explicitly stated in sentence like: *\"The sum of these child components is...\"* - You may cross-validate this by summing the listed child component values in the message if provided. - `reason`: Provide concise explanation of the error - why the reported value is considered incorrect - based directly on the message content. Only include this tag if all of the following are true: - The message identifies total element with incorrect calculation - Both reported and correct values are stated or inferable - The error occurs in specified reporting period Return your result as structured JSON in the following format: ```json { \"statement_name\": \"...\", \"error_tags\": \"tag\": \"...\", \"period\": \"...\", \"reported_value\": \"...\", \"correct_value\": \"...\", \"reason\": \"...\" { } } ``` US-GAAP Taxonomy Chunking To enable structured retrieval and reasoning, we convert the official US-GAAP taxonomy spreadsheets into concept-centric chunks. Each GAAP concept is represented by core chunk capturing its intrinsic attributes and set of relation chunks describing its semantic and structural links to other concepts. The conversion operates directly on the official Excel workbook through systematic normalization and extraction. Sheet Normalization. All sheets are loaded from the taxonomy workbook, column headers are standardized, and relevant sheets (Concept, Presentation, Calculation, Definition, Reference, Enumeration) are identified by keyword matching. Unnamed or irrelevant reference sheets are excluded, and each extracted element is annotated with provenance information such as file name, sheet name, and row number. Core Chunk Extraction. From the Concepts sheet, each concepts identifier, label, type, balance, period type, abstract flag, documentation, and deprecation status are extracted and rendered into canonical text form (concept::core) suitable for retrieval or embedding. Relation Chunk Extraction. For each concept, all relation-bearing sheets are parsed to construct chunks representing presentation hierarchies, calculation formulas, definition arcs, reference citations, and enumerations. The parser handles heterogeneous layouts, missing columns, and varying role conventions. Each chunk is recorded Conference acronym XX, June 0305, 2018, Woodstock, NY Yan et al. with its role, arcrole, and provenance metadata under the identifier concept::relations:prescalcdefrefenum, with integrity checks ensuring that all relation targets correspond to valid core concepts. Outputs. All chunks are written into chunks_core.jsonl and chunks_relations.jsonl, and summary statistics are stored in meta.json. This representation preserves the hierarchical and relational structure of the US-GAAP taxonomy while making each concept independently retrievable. To enable structured retrieval and reasoning, we convert the official US-GAAP taxonomy spreadsheets into concept-centric chunks. Each GAAP concept is represented by two complementary components: core chunk that captures intrinsic attributes and collection of relation chunks that encode its semantic and structural links to other concepts. The conversion process operates directly on the official Excel workbook, performing systematic normalization and extraction as follows. Sheet Normalization and Identification. We first load all sheets within the taxonomy workbook, standardize their column headers by lowercasing and trimming whitespace, and identify relevant sheets by keyword matching (Concept, Presentation, Calculation, Definition, Reference, and Enumeration). Suspicious all-unnamed reference sheets are ignored. The taxonomy version number is automatically inferred from the filename. Each extracted element is annotated with its provenance, including the file name, sheet name, and row number. Core Chunk Extraction. From the Concepts sheet, we extract for each concept its identifier (prefix:name), label, type, balance, period type, abstract flag, documentation, and deprecation status. These attributes are rendered into canonical, human-readable chunk_text and stored with stable identifier in the form concept::core. The core chunk provides self-contained textual summary suitable for direct retrieval or embedding. Relation Chunk Extraction. For every concept, we parse all relationbearing sheets to construct one chunk per relation family: Presentation: hierarchical parentchild paths with associated roles and preferred labels. Calculation: quantitative relationships including parent/child direction, weights, and roles. Definition: arcs between dimensions, domains, and hypercubes, including arcrole, source, and target. Reference: external standard citations consisting of source, section, and note, filtered to the relevant concept. Enumeration: extensible lists aggregated by domain and linkrole, with de-duplicated members. The extraction is designed to handle heterogeneous sheet layouts and missing columns: when explicit from*/to* fields are absent, the parser falls back to parent/child-style columns and merges multiple role or linkrole conventions. Each relation chunk records the roles and arcroles encountered, as well as complete provenance metadata, and uses the identifier concept::relations:{prescalcdefrefenum} to store. Integrity validation ensures that all relation targets reference existing core concepts, and malformed rows are automatically skipped. Outputs. All chunks are written into chunks_core.jsonl and chunks_relations.jsonl, while global statistics are summarized in meta.json. This chunked representation preserves the hierarchical and relational organization of the US-GAAP taxonomy while making each concept addressable as an atomic, retrievable unit, as shown in the following."
        },
        {
            "title": "Example of a Core Chunk",
            "content": "[Concept Core] ID: us-gaap:Revenues Label: Revenues Type: monetaryItemType Balance: credit PeriodType: duration Abstract:"
        },
        {
            "title": "False",
            "content": "Status: active DeprecatedLabel: None DeprecatedDate: None Documentation: Amount of revenue recognized from goods sold or services rendered during the period. Provenance: file=GAAP_Taxonomy_2024.xlsx sheet=Concepts row="
        },
        {
            "title": "Example of a Relation Chunk",
            "content": "[Concept Relations] ID: us-gaap:Revenues Presentation: - Role: Statement - Income Statement Path: Revenues > CostOfRevenue > GrossProfit Calculation: - As Parent: child=us-gaap:CostOfRevenue weight=-1.0 role=Calculation Linkbase - As Parent: child=us-gaap:GrossProfit weight=+1.0 role=Calculation Linkbase Definition (Dimensions/Domain/Hypercube): - domain-member: from=RevenueRecognitionPolicyTextBlock -> to= RevenueCategoryAxis References: - FASB ASC 606 Section 25 Revenue Recognition Provenance: sheets include Presentation-like and Calculation-like sheets Evaluation Metrics C.1 The metrics for FinSM For the FinSM task, we evaluated retrieval performance using Hit Rate (ùêªùëÖ@ùëò), Recall (ùëÖ@ùëò), and Macro-F1 (ùëÄùêπ 1@ùëò), where ùëò = {1, 5, 10, 20}. Hit Rate evaluates whether the LLM retrieves at least one relevant element within the top-ùëò results. It reflects the proportion of queries for which the model can successfully hit relevant item, without considering how many are retrieved. Recall measures the fraction of all relevant elements that are successfully retrieved within the top-ùëò predictions. It captures how completely the model covers the relevant set under cutoff of ùëò. Macro-F1 balances precision and recall at the query level by computing the F1 score for each query using its top-ùëò predictions and then averaging across all queries. It reflects both the accuracy (precision) and completeness (recall) of retrieval, giving equal weight to each query. HR@ùëò = 1 ùëÅ ùëÅ ùëñ=1 1(cid:0)ùê∏ (ùëò ) ùëñ ùê∫ùëñ (cid:1) R@ùëò = 1 ùëÅ ùëÅ ùê∏ (ùëò ) ùëñ ùê∫ùëñ ùê∫ùëñ ùëñ=1 Macro-F1@ùëò = 1 ùëÅ ùëÅ ùëñ=1 2 ùëÉ (ùëò ) ùëñ ùëÉ (ùëò ) ùëñ ùëÖ (ùëò ) ùëñ + ùëÖ (ùëò ) ùëñ (5) (6) (7) where ùëÉ (ùëò ) ùëñ = ùê∫ùëñ ùê∏ (ùëò ) ùëñ ùê∏ (ùëò ) ùëñ 0, , ùê∏ (ùëò ) ùëñ > 0 , ùëÖ (ùëò ) ùëñ = otherwise ùê∏ (ùëò ) ùëñ ùê∫ùëñ ùê∫ùëñ 0, , ùê∫ùëñ > 0 otherwise FINAUDITING: Financial Taxonomy-Structured Multi-Document Benchmark for Evaluating LLMs Conference acronym XX, June 0305, 2018, Woodstock, NY and ùëÅ is the total number of queries, and ùëÅ is the number of queries with non-empty ground truth sets. ùê∏ (ùëò ) denotes the top-ùëò predicted elements for the ùëñ-th query. ùê∫ùëñ denotes the set of ground truth elements for the ùëñ-th query. ùê∏ (ùëò ) ùëñ ùê∫ùëñ is the number of correctly predicted elements among the top-ùëò results. ùëò is the cutoff rank indicating the number of top predictions considered. 1() is the indicator function, returning 1 if the condition holds and 0 otherwise. ùëñ C.2 The metrics for FinRE For the FinRE task, we assessed the performance of various LLMs using Accuracy (Acc), Precision (Macro P), Recall (Macro R), and F1-score (Macro F1). Accuracy measures the overall proportion of correctly classified relations among all predictions. Precision evaluates the proportion of correctly predicted relations within each class, averaged across the three relation categories (Macro P). Recall quantifies the proportion of true relations that were successfully identified for each class, also averaged across classes (Macro R). F1-score provides balanced measure of Precision and Recall by computing their harmonic mean per class and then averaging across the three classes (Macro F1). Acc = 1 ùëÅ ùëÅ ùëñ=1 1( ÀÜùë¶ùëñ = ùë¶ùëñ ) Macro-P = Macro-R = 1 ùê∂ 1 ùê∂ ùëá ùëÉùëê ùëá ùëÉùëê + ùêπ ùëÉùëê ùëá ùëÉùëê ùëá ùëÉùëê + ùêπ ùëÅùëê ùê∂ ùëê= ùê∂ ùëê=1 ùê∂ Macro-F1 = 1 ùê∂ 2 ùëÉùëê ùëÖùëê ùëÉùëê + ùëÖùëê (8) (9) (10) (11) ùëê=1 where ùëÅ is the total number of instances. ùê∂ is the number of relation categories (here ùê∂ = 3). ùë¶ùëñ and ÀÜùë¶ùëñ denote the ground truth and predicted relation labels for the ùëñ-th instance. ùëá ùëÉùëê , ùêπ ùëÉùëê , and ùêπ ùëÅùëê denote the number of true positives, false positives, and false negatives for class ùëê, respectively. ùëÉùëê and ùëÖùëê denote the precision and recall for class ùëê. 1() is the indicator function that equals 1 when the prediction is correct and 0 otherwise. C.3 The metrics for FinMR For the FinMR task, we evaluated the mathematical reasoning capabilities of various LLMs using an LLM-as-a-judge framework to assess the overall Accuracy of generated answers. This metric reflects whether each model produces the correct numerical result under the specified auditing rule. In addition to overall correctness, we introduced three fine-grained error indicators, namely the Structural Error Rate (SER), Extraction Error Rate (EER), and Calculation Error Rate (CER), each corresponding to specific stage of reasoning validation. SER measures the proportion of outputs with invalid structure, where the model fails to produce the required JSON format containing both the extracted and calculated values. EER quantifies errors in identifying or reproducing the correct extracted value, meaning that the predicted numerical entity does not match the true extracted value in mathematical meaning. CER captures computational mistakes, where the predicted calculated value deviates from the ground truth even when the structure and extracted value are both correct. Together, these indicators provide hierarchical view of reasoning quality, reflecting whether an error arises from structural formatting, factual extraction, or numerical computation, thereby offering fine-grained insights into model performance on complex financial mathematical reasoning tasks. The specific prompt for judgment is shown below. LLM-as-a-judge prompt for FinMR Task Instruction: You are an evaluator. Your task is to judge whether the model's output pred_answer is correct compared to the given true_answer. Follow the rules strictly: Step 1 (Structure Check): Verify whether pred_answer has the same structure as true_answer. The required structure is JSON object with exactly two keys: {{\"extracted_value\": <value>, \"calculated_value\": <value>}} Minor formatting differences (e.g., line breaks, indentation, whitespace) are acceptable. If the structure is invalid, output the label: If valid, continue to Step 2 Step 2 (Extracted Value Check): Compare true_answer[\"extracted_value\"] and pred_answer[\"extracted_value\"] by their mathematical meaning, not their string form. For example, \"-1,284\" and \"-1284\" are considered equal. If they are not equal in numeric meaning, output the label: If equal, continue to Step 3 Step 3 (Calculated Value Check): Compare true_answer[\"calculated_value\"] and pred_answer[\"calculated_value \"] strictly in numeric meaning. They must be exactly equal (zero tolerance). If they are not equal, output the label: If equal, then everything is correct Final Decision: If all three checks pass, output the label: Output only one label: S, E, C, or A. Do not explain your reasoning. Given total of ùëÅ evaluated instances, let ùëÅùê¥, ùëÅùëÜ , ùëÅùê∏, and ùëÅùê∂ denote the numbers of instances labeled as Accurate (A), Structural Error (S), Extraction Error (E), and Calculation Error (C), respectively. Let ùëÅvalid represent the number of successfully parsed instances that received valid label. We define the following metrics: Accuracy (ACC) = ùëÅùê¥ ùëÅ valid Structural Error Rate (SER) = Extraction Error Rate (EER) = Calculation Error Rate (CER) = ùëÅùëÜ ùëÅ valid ùëÅùê∏ ùëÅ valid ùëÅùê∂ ùëÅ valid (12) (13) (14) (15) where each instance is judged by the LLM-as-a-judge framework to output exactly one label from {ùê¥, ùëÜ, ùê∏, ùê∂}. ACC, SER, EER, and CER, respectively, quantify the proportions of correct, structural, extraction, and calculation outcomes among validly parsed cases. Evaluation Models Table 9 summarizes all models evaluated in this study, categorized by openness, domain specialization, and architectural foundation. The evaluation covers wide range of both closedand open-source large language models (LLMs) as well as domain-specific and pretrained baselines. Closed-source LLMs: We include GPT-4o [11], accessed via OpenAIs API, as representative of state-of-the-art proprietary models. Although the architecture and model size remain undisclosed, GPT-4o serves as strong upper-bound reference in our benchmark. Conference acronym XX, June 0305, 2018, Woodstock, NY Yan et al. Table 9: Model categories and corresponding repositories."
        },
        {
            "title": "Repository",
            "content": "Closed-source LLMs GPT-4o Open-source LLMs DeepSeek-V3 Llama-4-Scout-17B-16E-Instruct Qwen3-235B-A22B-Instruct-2507 Llama-3.3-70B-Instruct Qwen2.5-72B-Instruct Qwen3-32B gemma-3-27b-it gemma-3-12b-it Llama-3.1-8B-Instruct Llama-3.2-3B-Instruct Financial-specific LLMs Fin-o1-14B Fin-R1 gpt-4o-2024-08-06 685B 109B 235B 70B 72B 32B 27B 12B 8B 3B deepseek-chat meta-llama/Llama-4-Scout-17B-16E-Instruct Qwen/Qwen3-235B-A22B-Instruct-2507 meta-llama/Llama-3.3-70B-Instruct Qwen/Qwen2.5-72B-Instruct Qwen/Qwen3-32B google/gemma-3-27b-it google/gemma-3-12b-it meta-llama/Llama-3.1-8B-Instruct meta-llama/Llama-3.2-3B-Instruct 14B 7B TheFinAI/Fin-o1-14B SUFE-AIFLM-Lab/Fin-R1 Open-source LLMs: This category encompasses diverse set of publicly available, instruction-tuned models, including DeepSeek-V3 [14], Qwen3-235B-A22B-Instruct-2507 [25], Qwen2.5-72B-Instruct [28], and Qwen3-32B [25], offering wide scale range. We also include several Llama models from Meta: Llama-4-Scout-17B-16E-Instruct [17], Llama3.3-70B-Instruct [6], Llama-3.1-8B-Instruct [6], and Llama3.2-3B-Instruct [6], as well as Googles Gemma models [24] (gemma-3-27b-it and gemma-3-12b-it). These models collectively enable cross-architecture and scaling comparisons across modern open-source systems. Financial-specific LLMs: We include Fin-o1-14B [19] and Fin-R1 [15], both trained on domain-specific financial corpora to capture specialized terminology and reasoning patterns in financial reporting. These models allow us to assess the impact of domain adaptation on structured auditing and numerical reasoning tasks. Together, these models form comprehensive evaluation spectrum, spanning closedand open-source systems, general-purpose and domain-specialized models, and encoderversus decoder-based architectures, enabling an in-depth analysis of performance across all proposed benchmark tasks."
        }
    ],
    "affiliations": [
        "Columbia University USA",
        "The Fin AI USA",
        "University of Montreal Canada"
    ]
}