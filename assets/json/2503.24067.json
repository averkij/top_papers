{
    "paper_title": "TransMamba: Flexibly Switching between Transformer and Mamba",
    "authors": [
        "Yixing Li",
        "Ruobing Xie",
        "Zhen Yang",
        "Xingwu Sun",
        "Shuaipeng Li",
        "Weidong Han",
        "Zhanhui Kang",
        "Yu Cheng",
        "Chengzhong Xu",
        "Di Wang",
        "Jie Jiang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing. Recent advancements in Mamba, a state space model (SSM) with linear complexity, offer promising efficiency gains but suffer from unstable contextual learning and multitask generalization. This paper proposes TransMamba, a novel framework that unifies Transformer and Mamba through shared parameter matrices (e.g., QKV and CBx), and thus could dynamically switch between attention and SSM mechanisms at different token lengths and layers. We design the Memory converter to bridge Transformer and Mamba by converting attention outputs into SSM-compatible states, ensuring seamless information flow at TransPoints where the transformation happens. The TransPoint scheduling is also thoroughly explored for further improvements. We conducted extensive experiments demonstrating that TransMamba achieves superior training efficiency and performance compared to baselines, and validated the deeper consistency between Transformer and Mamba paradigms, offering a scalable solution for next-generation sequence modeling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 7 6 0 4 2 . 3 0 5 2 : r Preprint. Under review. TransMamba: Flexibly Switching between Transformer and Mamba Yixing Li2, Ruobing Xie1, Zhen Yang1, Xingwu Sun1,3, Shuaipeng Li1, Weidong Han1, Zhanhui Kang1, Yu Cheng2, Chengzhong Xu3, Di Wang1, Jie Jiang1 1Tencent Hunyuan 2The Chinese University of Hong Kong 3University of Macau li.yixing@outlook.com xrbsnowing@163.com {andreasyang, sammsun, jonnyhan}@tencent.com chengyu@cse.cuhk.edu.hk"
        },
        {
            "title": "Abstract",
            "content": "Transformers are the cornerstone of modern large language models, but their quadratic computational complexity limits efficiency in long-sequence processing. Recent advancements in Mamba, state space model (SSM) with linear complexity, offer promising efficiency gains but suffer from unstable contextual learning and multitask generalization. This paper proposes TransMamba, novel framework that unifies Transformer and Mamba through shared parameter matrices (e.g., QKV and CBx), and thus could dynamically switch between attention and SSM mechanisms at different token lengths and layers. We design the Memory converter to bridge Transformer and Mamba by converting attention outputs into SSMcompatible states, ensuring seamless information flow at TransPoints where the transformation happens. The TransPoint scheduling is also thoroughly explored for further improvements. We conducted extensive experiments demonstrating that TransMamba achieves superior training efficiency and performance compared to baselines, and validated the deeper consistency between Transformer and Mamba paradigms, offering scalable solution for next-generation sequence modeling."
        },
        {
            "title": "Introduction",
            "content": "Transformers (Vaswani et al., 2017; Achiam et al., 2023; Touvron et al., 2023) are the foundation and mainstream model of modern deep learning (Zhao et al., 2023), showing dominating power in language modeling. Recently, Mamba has emerged (Gu & Dao, 2023) and been verified in various fields. Compared with Transformer, Mamba has linear computational complexity, high efficiency in processing long sequences, and lower training and inference costs (Qu et al., 2024). Nevertheless, its contextual learning and multi-task generalization capabilities are unstable (Waleffe et al., 2024). Transformer and Mamba have their own strengths and complement each other. However, Transformer and Mamba have their own flaws that cannot be addressed by naive layer-shared hybrid structures (Yuan et al., 2024; Yang et al., 2024). For example, Transformer has faster training for short contexts while Mamba has better efficiency in longer contexts (see Table 2). Moreover, the naive static hybrid model has structural restrictions such as the order of Mamba and Transformer, mandatory ratios, etc (Lieber et al., 2024; Dao & Gu, 2024). The performance of the Hybrid model will deteriorate if these specific rules are not met, which greatly limits the exploration and breakthrough of the model. Recently, Mamba2 (Dao & Gu, 2024) further enhances the performance of Mamba series, which reveals the surprising consistency of the attention of Transformer and the State Space Corresponding author. Work conducted during internship at Tencent. 1 Preprint. Under review. Figure 1: TransMamba has shared parameters to flexibly switch between Attention and SSM, and TransPoints decide which parts of token sequence use Attention or SSM. Figure 2: TransMamba generally shows better efficiency and performance with different sizes. Model (SSM) of Mamba. Furthermore, (Wang et al., 2024) performed distillation between Mamba and Transformer, distilling the QKV parameters of Attention to obtain CBx of SSM, verifying that the parameters can be interactively transferred as shown in Table 1. These motivate us that we can bravely utilize set of shared parameters of QKV and CBx to build joint Transformer-Mamba framework, which could flexibly decide which structure is suitable for the current training/inference in different layers/token lengths, taking advantage of both structures to balance effectiveness and efficiency while ensuring structural flexibility. Intuitively, to obtain the efficiency advantages of both structures, we can make the model adopt the Transformer mechanism for training on relatively short contexts and the SSM mechanism on long contexts. As shown in Figure 3, such prototype framework has only one set of parameters to flexibly switch between Transformer and Mamba for LM. In the first tokens of the sequence, the parameter matrix is calculated using the attention mechanism. At specific node in the sequence (which we call TransPoints from Transformer mode to Mamba2 mode), the parameter matrix is converted to the SSM mechanism for subsequent sequence generation, so as to achieve better training efficiency with better performance in sequences of different lengths. The implementation of this flexible token-level Transformer-Mamba transformation is non-trivial and has the following challenges: (1) In the TransPoints between, the latter structure (Mamba) should well capture the information of the previous tokens learned by the former structure (Transformer) via an appropriate method that the latter structure could understand. How to losslessly transfer the knowledge learned by the previous Transformer to the latter SSM modeling part is essential. (2) We could flexibly decide when (e.g., at what sequence length) to transfer from Transformer to Mamba at different layers in such framework. Jointly considering effectiveness and efficiency, the selection of reasonable set of TransPoints requires careful explorations under insightful principles. (3) The structures of this framework varies at different sequence lengths (e.g., pure Transformer/Mamba2 or certain Hybrid structures), in which case the model performance should be concerned. To address these problems, we propose novel TransMamba framework that utilizes the same set of shared parameters to flexibly switch between attention and SSM mechanisms in token generation at different sequence lengths and layers, combining the advantages in effectiveness and efficiency of Transformer and Mamba. Specifically, we design sophisticated Memory Converter to convert the intermediate results of the attention part into the state required by the SSM mechanism, ensuring the consistency of the information around the TransPoint with tokens being processed, and no loss will be incurred when converting between attention and SSM. Moreover, we have conducted comprehensive research on the TransPoint schedule, exploring the overall optimal TransPoint setting and insights in different layers and sequence lengths. In this case, our TransMamba framework could be viewed as flexible dynamic combination of hybrid Transformer/Mamba layers varies in different token lengths. Our contributions are summarized as follows: Preprint. Under review. Figure 3: (a) Structure of TransMamba. Attention and SSM have shared parameters WQKV and WCBx. Tokens are either processed via the green path (SSM mode) or the blue path (Attention mode). (b) Memory Converter. (c) The TransPoint Scheduling of TransMamba. SSM Attention = δ(HWC) = δ(HWQ) = δ(HWB) = δ(HWK) = δ(HWx) = δ(HWV) = (L QKT)V = (A CBT)X Model Training FLOPs / Layer O(T2N) O(TN2) Transformer Mamba TransMamba O(P2N + (T P)N2) Table 1: Compare the matrix form of SSM and Attention. The core mechanisms of Attention and SSM show consistency in dual form, which is the mathematical basis that enables us to unify Transformer and Mamba. Table 2: Compare the training FLOPs of Transformer, Mamba and optimal TransMamba. The FLOPs of TransMamba is quadratic function of the TransPoint, and its specific value is related to the speed optimization coefficients of Transformer and Mamba respectively. We propose novel TransMamba framework, which verifies the consistency of Transformer and Mamba in deeper degree, starting from the one shared set of parameters while outputting tokens via two different mechanisms. We design the Memory Converter that conforms to the theoretical solution to ensure the consistency of information in TransMamba during the conversion process, and explore the optimal TransPoint schedule at different layers and token lengths. We conduct extensive experiments to verify the performance and efficiency advantages of TransMamba on both effectiveness and efficiency. In conclusion, TransMamba could be promising structure for LM. Preprint. Under review."
        },
        {
            "title": "2 Method",
            "content": "2.1 Preliminary"
        },
        {
            "title": "2.1.1 Basic Notions and Consistency of Attention and SSM",
            "content": "We use the classic notation from the Transformer and Mamba papers. QKV denotes the key parameters (query, key, value) of Attention, and denotes the additional mask matrix. CBx represents the key parameters in the SSM, is used to control the discrete step size in SSM, and is used to describe the global dependencies of the hidden state, which is similar to the mask matrix in Attention (Gu & Dao, 2023). In order to satisfy the classic symbolic representation and clear expression, we use to denote the input embeddings for attention and SSM. The corresponding calculations are shown in Table 1. Mamba2 (Dao & Gu, 2024) compares the underlying mechanisms of Transformer and Mamba, and introduces the dual form of SSM to illustrate the consistency between the two. In Table 1, we can find that the core mechanisms of Transformer and Mamba (attention and SSM) are completely symmetrical. Wang et al. (2024) aligned the QKV of the transformer weights with CBx of Mamba and performed distillation, achieving improved results on chat and long-text benchmarks. This once again shows that the core weights of Transformer and Mamba are transferable and unified. The above theories and research inspired us to build bolder framework of TransMamba with unified architecture of Transformer and Mamba. 2.1.2 Efficiency of Attention and SSM with different token lengths As shown in Table 2, recent work (Dao & Gu, 2024) theoretically summarizes the FLOPs of Attention and SSM. denotes the sequence length, denotes the state dimension and denotes the TransPoint value. When is greater than N, Transformer has an advantage in efficiency on shorter sequences, while Mamba is efficient at training on long sequences due to its linear complexity of T. This advantage of Mambas training efficiency on longer contexts is present with most of the commonly-used model sizes, which also forms the motivation of our TransMamba that attempts to unleash the maximum potential of the flexible hybrid Transformer-Mamba structure in terms of effectiveness and efficiency. 2.2 Overall Framework of TransMamba Main architecture. As shown in Figure 3 (a), TransMamba is layer-stacked Decoder-only autoregressive model. Each layer of TransMamba contains all the parameters of Mamba, including the parameters required to calculate C, B, x, and . Based on the aforementioned consistency between Transformer and Mamba, we boldly let QKV and CBx share the same parameters (i.e., QC, KB, Vx). In other words, our model has the ability to switch between Transformer and Mamba structures, but with only one set of parameters. In addition, TransMamba contains the crucial Memory Converter used for lossless information conversion when model parameters are switched from QKV to CBx (in Section 2.3), armed with our TransPoint schedule that decides whether we should use Attention mode or SSM mode at certain layer or token length (in Section 2.4). To ensure better training efficiency, we only set single TransPoint (i.e., the token position where the switch from Attention to SSM or vice versa happens) for each layer. The sequence before the TransPoint is calculated using Attention, and the rest is calculated through SSM. Complex structures with multiple TransPoints may have more magical properties and effects, which can be provided for future research. At different token lengths, TransMamba could be flexibly regarded as different structures (e.g., pure Transformer, Mamba, or Hybrid Transformer-Mamba). Formalized calculation process. We denote the hidden state of the input tokens as h. The remaining critical mathematical symbols are the same as given in Section 2.1.1. TransMamba calculates intermediate results through linear project and convolution modules. Since the parts of the input token sequence that are shorter and longer than the TransPoint will be calculated through different mechanisms, for the sake of clarity, we use different symbols to 4 Preprint. Under review. represent the two parts: (a) For the relatively former part of the input before TransPoint: hs = h[: TransPoint], = δ(hsWB), = δ(hsWC), = δ(hsWx). The output ys will be calculated through the attention mechanism before TransPoint: (b) For the relatively latter part of the input after TransPoint: ys = softmax(QKT) V. hl = h[TransPoint :], = eelog WA , = δ(hlWB), = σ(hlW + b), = δ(hlWC), = δ(hlWx). (1) (2) (3) TransMamba utilizes the SSM mechanism to generate outputs yl after TransPoint. The initial state h0 will be obtained through the Memory Converter: h0 = Memory Converter(K, V), hk = Ak1hk1 + Bk, kxk, yk = Ckhk, yl = [y0, , yk]. or in the matrix form: yl = (A CBT)( x). The final output of our TransMamba can be expressed as the combination of ys, yl: = [ys, yl]. (4) (5) (6) Feasibility. The feasibility of our design comes from two key points: (1) Due to the consistency of the attention and SSM mechanisms described in Section 2.1.1, the output of TransMamba can be calculated either through the attention or SSM mechanism flexibly. (2) Due to the power of our memory converter, TransMamba does not lose any information when converting from attention to SSM as the token length increases across the TransPoint. The sequence state required by SSM can be perfectly preserved by the and of attention. 2.3 Lossless Memory Converter The memory converter is aimed to losslessly convert and calculated before TransPoint into the hidden state required for the Mamba mode after TransPoint. First, we expand the mathematical form of SSM in detail: = σ(xkW + b), Bk = δ(xkWB), h0 = B0 0x0, kelog WA , Ak = Ck = δ(xkWC), kxk. hk = Ak1hk1 + Bk Abbreviate to matrix form as: = (A BT)( x) = (A BT)X, (7) (8) where is the lower triangular matrix obtained by arranging the elements of A, and details are shown in Appendix A.2.1. Based on the consistency of the mathematical structure of attention and SSM shown in Section 2.1.1, we can calculate the estimated hidden state from the intermediate results K, of attention as follows: hs = (A KT)V. (9) The initial state of the TransPoint can be obtained as h0 = hs[1]. Therefore, TransMamba can transform losslessly from attention to SSM during sequence generation. It should be noted that our Memory converter does not require additional parameters, but is theoretical solution calculated from existing results. 5 Preprint. Under review. 2.4 Flexible TransPoint Scheduling TransPoint represents the token position of the segmentation of the sequence where the TransformerMamba mode switch happens via the above Memory converter for each layer. The position of TransPoint in the sequence can control the ratio of attention and SSM in this layer of TransMamba. For example, when TransPoint is set to the midpoint of the sequence, this layer is 1:1 combination of Transformer and Mamba in the sequence level; if it is set to the beginning of the sequence, this layer is equal to Mamba. The TransPoint schedule decides the functional (hybrid) structure of TransMamba at different token lengths."
        },
        {
            "title": "2.4.1 Principles of TransPoint Scheduling",
            "content": "The TransPoint Scheduling aims to to maximize the respective advantages of Attention and SSM in short and long context training to optimize the overall efficiency and performance. In summary, TransPoint scheduling should meet the following requirements: TransPoint has great impact on training time. The distribution of TransPoints can be closer to the optimal position in Table 2 for better training efficiency; TransPoints at different layers cannot be too concentrated at one position. Under the premise of the first point, it needs to be distributed over the entire length of the sequence to prevent possible degradation brought by the mutations of simultaneous Transformerto-Mamba transformations for better effectiveness; Due to the asynchronous transformations, our TransMamba could be viewed as different hybrid Transformer-Mamba structures at different token lengths. Therefore, we should take fully advantages of the superior hybrid Transformer-Mamba structures insights to further enhance the effectiveness. 2.4.2 Detailed TransPoint Schedule Designing TransPoint schedule from token length aspect. Due to the flexibility of TransPoint scheduling at different layers and token lengths, the model structure of TransMamba varies at different positions. Suppose the number of layers of the model is and the length of the sequence is T, there are possible TransPoint schedules for our TransMamba with fixed parameters. We denote the value of TransPoint as (indicating that the tokens before position are modeled via Transformer and those after are encoded via Mamba), and we have the FLOPs for TransMamba layer as follows: FLOPSTransMamba = O(P2N + (T P)N2). (10) Theoretically, the training time of TransMamba is quadratic function of the TransPoint. In Section 3.3.1, our experiments confirm that the training efficiency of TransMamba indeed shows quadratic function trend as TransPoint changes, and the optimal efficient point of our TransPoint is nearly 2, 048 for our setting (N = 1, 536 and = 8, 192). TransPoint schedule from layer aspect. We find that simply setting global Transpoint for all layers (e.g., simultaneously at length 4,096) will result in unsatisfactory performance due to the sudden switching. To achieve better results, we need to set more diverse TransPoints at the layer level, which could gradually guide the model structure from pure Transformer to Mamba differently at various layers. To enable smoother transformation, the TransPoints are placed separately but as close as possible to the optimal efficient according to Eq. 10. Specifically, our TransPoints cycle is performed every 8 layers, referring to the work (Dao & Gu, 2024) on hybrid structure. The mean of TransPoints is set slightly smaller than the optimal efficient point to enable more Mamba layers for better performance. TransPoints gradually transition from the beginning to the end of the sequence in logarithmic trend (i.e., 0, 128, 256, 512, 1024, 2048, 4096, 8192), ensuring dispersion and smoothness. Considering the above two aspects, we set our final TransPoint schedule balancing both effectiveness and efficiency. Our TransMamba with flexible TransPoint scheduling could have more potential interesting features to be further explored in the future. More detailed settings, explorations, and results are in the Experiments and Appendix. 6 Preprint. Under review. Model ARC-E ARC-C ACC ACC 60.57 Transformer-400M 56.15 Mamba2-400M Hybrid-400M 62.33 TransMamba-400M 62.50 Transformer-1.5B Mamba2-1.5B Hybrid-1.5B TransMamba-1.5B 60.87 63.64 63.92 64.75 58.72 52.27 55.78 59.33 59.43 56.00 57.97 63.33 CoQA F1-Score ACC ACC OBQA PIQA PhoneBook BoolQ Similarity ACC 5.07 4.68 5.52 6.23 5.93 5.30 6.21 6.97 42.4 40.8 43.6 44.8 48.6 44.0 51.0 50.6 52.75 51.10 53.89 55.76 56.66 58.97 59.25 59. 38.70 13.07 17.60 39.69 41.04 19.08 26.63 40.92 60.72 57.51 61.66 64.15 61.42 59.20 65.48 66.73 Table 3: Main evaluation results. TransMamba generally shows better performance."
        },
        {
            "title": "2.4.3 Diverse Inference Strategy",
            "content": "Intuitively, the inference of TransMamba could adopt the same TransPoint schedule as that in training. However, due the flexibility of TransMamba, we can also choose completely different TransPoints during inference. It provides us whimsical but inspiring idea that we can train TransMamba with the most efficient structure, and choose different structure that best suits the task during inference. In experiments, we will explore its potential."
        },
        {
            "title": "3 Experiments",
            "content": "3.1 Experiment Setup We developed three baseline model families with various sizes (400M, 1.5B): Transformer (Shoeybi et al., 2019), Mamba2 (Dao & Gu, 2024), and Hybrid (Lieber et al., 2024). All models are developed based on the Megatron-LM library. Specifically, all models are unified in model size for fair comparisons. The models are pre-trained utilizing collected in-house dataset which consists of cleaned combination of Chinese and English datasets. We trained all models on 83 billion tokens for all models. For evaluation, we aim to achieve robust conclusions across diverse domains. We conducted comprehensive evaluations involving 8 English tasks, including ARC-E, ARC-C (Clark et al., 2018), CoQA (Reddy et al., 2019), OBQA (Mihaylov et al., 2018), PIQA (Bisk et al., 2020), PhoneBook (Waleffe et al., 2024), BoolQ (Clark et al., 2019), LongBench-v2 (Bai et al., 2024). More details are shown in Appendix A.3.1 and A.3.2. 3.2 Main Results 3.2.1 Evaluations on General Tasks We evaluate the baselines and TransMamba on multiple tasks including question answering and reading comprehension as shown in Table 3 (note that the input contexts of these tasks are longer enough than some of our TransPoints to trigger TransMamba). TransMamba achieves the overall best performance. On the question answering and understanding tasks, TransMamba achieves the best performance or is comparable to the Hybrid model, while it consistently outperforms the original Transformer and Mamba2. The PhoneBook task is given the contact information of multiple people and requires the model to accurately answer the contact of specific person. As introduced in Work (Waleffe et al., 2024), Mamba has significant disadvantage compared to Transformer in this precise search task, and this disadvantage also brings to the Hybrid model. However, due to our smart combination of Transformer-Mamba at the sequence level, TransMamba can give accurate answers at the beginning of the sequence with almost the same accuracy as Transformer. Table 4 shows the performance on the long-text benchmark LongBench-v2, where TransMamba still outperforms all baselines. This further illustrates the role of the lossless Memory Converter in TransMamba, which can effectively preserve the information before TransPoint. 7 Preprint. Under review. Model LongBench-v2 Overall Easy Hard Transformer Mamba Hybrid TransMamba 31.61 30.62 35.79 38. 34.38 29.90 32.81 29.26 38.02 34.41 40.10 37.94 Model Relative Train Time Flops / (Layer, 1010) Transformer Mamba Hybrid TransMamba 1.00 0.77 0.78 0. 10.51 2.01 6.26 1.91 Table 4: Evaluation results of our TransMamba and baselines on the long text benchmark LongBench-v2. The number of parameters of all models is 1.5B. Table 5: Comparison of average training time of baseline and TransMamba. Relative time refers to the ratio of the time to train the same batch-size of the baseline to Transformer. Model Setting Detailed TransPoint Schedule Transformer Layer-shared Layer-specific Broad-range Fine-grained [8192] [2048] [4096] [6144] [3072, 4096, 5120] [2048, 3072, 4096] [512, 1024, 2048] [2048, 4096, 6144] [0, 1024, 2048, 6144, 8192] [0, 128, 256, 512, 1024, 2048, 4096, 8192] V1 V2 V3 V4 V5 V6 V7 V8 Validation Loss 3.098 3.356 3.297 3.308 3.125 3.100 3.135 3.084 3.022 2.898 PPL 2.194 2.401 2.346 2.339 2.287 2.219 2.299 2.185 2.053 1.813 Table 6: Results of different TransPoint schedule. The input token sequence length of the training data is 8192. The validation loss and PPL is calculated at 21 billion tokens. The TransPoint of each layer in the model cyclically alternates through the predefined TransPoints sequence with the pattern repeating. 3.2.2 Efficiency Analysis As described in Table 2 and Section 2.1.2, Transformer and Mamba have efficiency advantages on short and long text, respectively. Our TransMamba is more efficient compared to baselines. Specifically, taking sequence length T=8k and state dimension N=4k as an example, the theoretical FLOPs of Transformer is 2.29 times that of the optimal TransMamba, while that of Mamba is 1.14 times. We conducted experiments on the average training time of the baselines and TransMamba on 3 machines in Table 5. TransMamba has maximum efficiency improvement of 25% compared to Transformer, which will increase to 0.8% if we utilize the optimal efficient TransPoint Schedule. This efficiency improvement is consistent with the relative size of the theoretical FLOPs value. It is worth noting that because attention and SSM have their own engineering acceleration, the actual runtime improvement result does not fully reach the theoretical speedup limit. Optimization of TransMamba acceleration engineering is our future work, and there is still potential for speed improvement. 3.3 In-depth Analyses on Different TransPoint Schedule 3.3.1 Analyses on Layer-Shared TransPoint Schedule TransPoint scheduling has significant impact on the effectiveness and efficiency. We first conducted experiments on Layer-shared TransPoint scheduling for straightforward understanding, where the TransPoint of all layers is set to one unified value. We experimented with the training efficiency of the TransPoint setting from 0 to 8192 in step size of 64. As shown in Figure 4 (a), the relative training time shows quadratic curve trend, and the optimal TransPoint is around 2, 048. V1 V3 in Table 6 shows different Layer8 Preprint. Under review. (a): Layer-Shared TransPoint Schedule (b): Layer-Specific TransPoint Schedule Figure 4: Experiments on TransPoint Schedule and training efficiency. shared schedules at various positions, whose loss and PPL are not satisfactory due to the sudden mutation from Transformer to Mamba for all layers. Hence, we move to explore Layer-specific TransPoint scheduling for better performance. 3.3.2 Analyses on Layer-specific TransPoint Schedule During explorations on Layer-specific scheduling, we observed that three characteristics can bring better results: Layer-specific, Broad-range, and Fine-grained, and conduct three groups of evaluations (V4 V9 in Table 6). For example, the schedule of V4 indicates that its TransPoints cycle every 3 layers, and the TransPoints of layer 1-6 are: [3072, 4096, 5120, 3072, 4096, 5120...]. Note that these Layer-specific schedules also possess the same quadratic curve trend on efficiency as shown in Figure 4 (b). We condensed the following rules for TransPoints scheduling, and the final schedule is based on both effectiveness and efficiency. (a) The TransPoints of each layer should be layer-specific. Setting concentrated TransPoints for all layers has relatively poor validation results. V4 V6 set TransPoints at three positions and achieve better loss and PPL compared to V1 V3 with shared TransPoints. (b) The scheduling of TransPoints should cover broad-range of the sequence. V4 V6 have TransPoints of all layers under the concentrated setting vary within the range of 2k tokens, while the verification loss and PPL are significantly higher compared to V7 and V8. More diverse TransPoint help the model performance gradually improve. (c) Fine-grained transformation of TransPoints improves the performance. Compared to the vanilla broad range setting V7 and V8, V9 (i.e., the final TransMamba setting) have finer-grained and smoother scheduling cycling every 8 layers, achieving the best result. 3.4 Explorations on Inconsistent Training/Inference TransPoint Scheduling Due to the flexibility of the Transformer and Mamba mode transformation based on the unified parameters in TransMamba, we can set completely different TransPoint schedules for training and inference. For this bold exploration, we train our TransMamba with the selected schedule V9, and then inference with different schedules. We surprisingly find that some inconsistent training/inference settings (e.g., inference with Transformer) could not only function normally, but also achieve even better results on certain tasks. We present these charming results in the Appendix A.4.1, which is promising research direction. 3.5 Ablation Study on Other Model Components We conduct experiments on the components of the details of TransMamba framework. We found it beneficial to radiate the key components of Mamba onto the overall structure of TransMamba. The experimental results and details are shown in Appendix A.4.2. 9 Preprint. Under review."
        },
        {
            "title": "4 Related Works",
            "content": "Transformer has always been the focus of language model research (Beltagy et al., 2020; Liu et al., 2021; Tang et al., 2024), but its limitations in processing long sequences (Zhou et al., 2021; Behrouz et al., 2024) and the memory pressure caused by KV cache (Wang et al., 2020; Dao et al., 2022) are also difficult to solve. Mamba has the advantage of linear complexity based on the state space model (Gu & Dao, 2023; Zhang et al., 2024), but struggles in modeling complex contexts (Xiao et al., 2024). Hybrid models (Chen et al., 2024; Lou et al., 2024; Ren et al., 2025) that combine the two are emerging, but most of the work simply cascades them (Hatamizadeh & Kautz, 2024; Lieber et al., 2024). Recent work (Dao & Gu, 2024; Han et al., 2024; Wang et al., 2024) has revealed the consistency of the underlying mathematics between them. However, there is no work that truly attempts to unify Transformer and Mamba in sequence level."
        },
        {
            "title": "5 Conclusion",
            "content": "We proposes TransMamba to unify Transformer and Mamba at the sequence level and proves its superiority in efficiency and performance. Furthermore, we conduct detailed exploration of TransPoints and summarize three criteria of TransPoint Scheduling. In short, our attempt provides insight and inspiration for the next generation of sequence modeling."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Yushi Bai, Shangqing Tu, Jiajie Zhang, Hao Peng, Xiaozhi Wang, Xin Lv, Shulin Cao, Jiazheng Xu, Lei Hou, Yuxiao Dong, et al. Longbench v2: Towards deeper understanding and reasoning on realistic long-context multitasks. arXiv preprint arXiv:2412.15204, 2024. Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. Piqa: Reasoning about physical In Proceedings of the AAAI conference on artificial commonsense in natural language. intelligence, volume 34, pp. 74327439, 2020. Junzhou Chen, Zirui Zhang, Jing Yu, Heqiang Huang, Ronghui Zhang, Xuemiao Xu, Bin Sheng, and Hong Yan. Dsdformer: An innovative transformer-mamba framework for robust high-precision driver distraction identification. arXiv preprint arXiv:2409.05587, 2024. Christopher Clark, Kenton Lee, Ming-Wei Chang, Tom Kwiatkowski, Michael Collins, and Kristina Toutanova. Boolq: Exploring the surprising difficulty of natural yes/no questions. arXiv preprint arXiv:1905.10044, 2019. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. ArXiv, abs/2405.21060, 2024. URL https://api. semanticscholar.org/CorpusID:270199762. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. Advances in neural information processing systems, 35:1634416359, 2022. 10 Preprint. Under review. Daniele Faraglia and Other Contributors. Faker. URL https://github.com/joke2k/faker. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. ArXiv, abs/2312.00752, 2023. URL https://api.semanticscholar.org/CorpusID: 265551773. Dongchen Han, Ziyi Wang, Zhuofan Xia, Yizeng Han, Yifan Pu, Chunjiang Ge, Jun Song, Shiji Song, Bo Zheng, and Gao Huang. Demystify mamba in vision: linear attention perspective. arXiv preprint arXiv:2405.16605, 2024. Ali Hatamizadeh and Jan Kautz. Mambavision: hybrid mamba-transformer vision backbone. arXiv preprint arXiv:2407.08083, 2024. Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformer-mamba language model. arXiv preprint arXiv:2403.19887, 2024. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1001210022, 2021. Meng Lou, Yunxiang Fu, and Yizhou Yu. Sparx: sparse cross-layer connection mechanism for hierarchical vision mamba and transformer networks. arXiv preprint arXiv:2409.09649, 2024. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. Can suit of armor conduct electricity? new dataset for open book question answering. arXiv preprint arXiv:1809.02789, 2018. Haohao Qu, Liangbo Ning, Rui An, Wenqi Fan, Tyler Derr, Hui Liu, Xin Xu, and Qing Li. survey of mamba. arXiv preprint arXiv:2408.01129, 2024. Siva Reddy, Danqi Chen, and Christopher Manning. Coqa: conversational question answering challenge. Transactions of the Association for Computational Linguistics, 7:249266, 2019. Weiming Ren, Wentao Ma, Huan Yang, Cong Wei, Ge Zhang, and Wenhu Chen. Vamba: arXiv preprint Understanding hour-long videos with hybrid mamba-transformers. arXiv:2503.11579, 2025. Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catanzaro. Megatron-lm: Training multi-billion parameter language models using model parallelism. arXiv preprint arXiv:1909.08053, 2019. Yehui Tang, Yunhe Wang, Jianyuan Guo, Zhijun Tu, Kai Han, Hailin Hu, and Dacheng Tao. survey on transformer compression. arXiv preprint arXiv:2402.05964, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Rozi`ere, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Roger Waleffe, Wonmin Byeon, Duncan Riach, Brandon Norick, Vijay Korthikanti, Tri Dao, Albert Gu, Ali Hatamizadeh, Sudhakar Singh, Deepak Narayanan, et al. An empirical study of mamba-based language models. arXiv preprint arXiv:2406.07887, 2024. Junxiong Wang, Daniele Paliotta, Avner May, Alexander Rush, and Tri Dao. The mamba in the llama: Distilling and accelerating hybrid models. arXiv preprint arXiv:2408.15237, 2024. 11 Preprint. Under review. Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Selfattention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Chaodong Xiao, Minghan Li, Zhengqiang Zhang, Deyu Meng, and Lei Zhang. Spatialmamba: Effective visual state space models via structure-aware state fusion. arXiv preprint arXiv:2410.15091, 2024. Kai Yang, Jan Ackermann, Zhenyu He, Guhao Feng, Bohang Zhang, Yunzhen Feng, Qiwei Ye, Di He, and Liwei Wang. Do efficient transformers really save computation? arXiv preprint arXiv:2402.13934, 2024. Danlong Yuan, Jiahao Liu, Bei Li, Huishuai Zhang, Jingang Wang, Xunliang Cai, and Dongyan Zhao. Remamba: Equip mamba with effective long-sequence modeling. arXiv preprint arXiv:2408.15496, 2024. Hanwei Zhang, Ying Zhu, Dan Wang, Lijun Zhang, Tianxiang Chen, Ziyang Wang, and Zi Ye. survey on visual mamba. Applied Sciences, 14(13):5683, 2024. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, et al. survey of large language models. arXiv preprint arXiv:2303.18223, 1(2), 2023. Haoyi Zhou, Shanghang Zhang, Jieqi Peng, Shuai Zhang, Jianxin Li, Hui Xiong, and Wancai Zhang. Informer: Beyond efficient transformer for long sequence time-series forecasting. In Proceedings of the AAAI conference on artificial intelligence, volume 35, pp. 1110611115, 2021. Preprint. Under review."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Detailed Conclusion, Future Work and Limitations This paper proposes TransMamba to unify Transformer and Mamba at the layer level and proves its superiority in efficiency and performance. Specifically, we combine the advantages of Transformer and Mamba in long and short contexts to significantly improve the training speed during training. At the same time, conversion modules such as Memory Converter ensure lossless model conversion and ensure the performance of the model. We conduct detailed exploration of TransPoints and model framework, from naive layershared TransPoint scheduling to sophisticated layer-specific design, and summarize three standards of the TransPoint scheduling. At the same time, due to the flexible model architecture of TransMamba under shared parameters, we boldly tried training and reasoning isomorphism and obtained surprising results. In short, our attempt provides insight and inspiration for the next generation of sequence modeling. This paper also has some limitations for future research, including: 1. Future work could try larger models and explore the form of the scaling law of TransMamba; 2. Since Transformer and Mamba have different degrees of optimization, the actual optimal value of TransPoint has the potential to be further explored. In our current experiments, we concluded that the degree of training optimization of Transformer and Mamba is proportional, and the proportionality coefficient is approximately Transformer: Mamba=2.67:1 (i.e., the training speed of Transformer will be 2.67 times faster than that of Mamba under the same FLOPs). This provides ideas for follow-up work; 3. Transformer and Mamba have their own variants. Follow-up work can try to combine different variants into new TransMamba. This research direction has lot of possible exciting results. A.2 Method Details A.2.1 Memory Converter The matrix utilized in Equation 2.3 is as follows: = A.3 Experiment Details A.3.1 Model Parameters Setting 1 A1 A2A1 1 1 A3A2A1 A3A2 A3 . 1 (11) In this section we introduce the parameter settings of the baseline and TransMamba used in this paper. Empirically, we set the ratio of TransMamba layer to MLP layer to 1:1. (The baseline also has the same setting). Table 7 shows the overall training settings, and Table 8 shows the specific parameters of each model size. A.3.2 Experiment Setup Details The benchmarks used in this paper introduced in Section 3.1 are described as follows: ARC (Clark et al., 2018) dataset, developed by the Allen Institute for Artificial Intelligence (AI2), is collection of 5,197 elementary-level science questions designed to evaluate natural language understanding and reasoning capabilities in AI systems, focusing on straightforward scientific concepts typically encountered in grade school curricula. 13 Preprint. Under review. Settings Global Batch Size Micro Batch Size Sequence Length Train Tokens MLP Ratio Initial lr Min lr lr-Decay-Style weight-decay clip-grad Normalization adam-beta1 adam-beta2 bf16 rope-theta Value 1024 2 8192 81B 0.5 2.5e-4 2.5e-5 cosine 0.1 1.0 RMSNorm 0.9 0.95 True 10000 Table 7: Global parameter settings. Model Setting Num-Layers Hidden-Size FFN-Hidden-Size Num-Attention-Heads Num-Layers Hidden-Size FFN-Hidden-Size Num-Attention-Heads Value 24 1536 4096 16 64 1536 4096 400M 1.5B Table 8: Model parameter setting. CoQA (Reddy et al., 2019) dataset is large-scale collection of 127,000 question-answer pairs from 8,000 dialogues across seven diverse domains (e.g., news, literature, science), designed to evaluate machines ability to answer context-dependent, free-form questions in multi-turn conversations while requiring coreference resolution and pragmatic reasoning. OBQA (Mihaylov et al., 2018) dataset is novel question-answering benchmark designed to evaluate AI systems ability to integrate external commonsense knowledge and perform multi-step reasoning, requiring comprehension beyond direct text retrieval to answer science-based questions aligned with elementary school curricula. PIQA (Bisk et al., 2020) dataset is benchmark designed to evaluate AI systems reasoning capabilities about physical commonsense knowledge through context-dependent questions that require understanding object properties, manipulation strategies, and real-world physics (e.g., How to separate egg yolk using water bottle?), with human accuracy reaching 95% while state-of-the-art models achieve 77% accuracy. PhoneBook is introduced in (Waleffe et al., 2024) and aims to evaluate the exact phone number of specific person given phone book of multiple people. There are two ways to construct specific phone book: conventional construction and reverse construction. We use the open source tool (Faraglia & Other Contributors) to construct completely random test set PhoneBook. BoolQ (Clark et al., 2019) dataset is natural language understanding benchmark comprising 15,942 yes/no questions paired with contextual paragraphs, designed to evaluate models ability to answer binary questions through complex reasoning over real-world 14 Preprint. Under review. web content, where human performance reaches 90% accuracy while models initially struggled to surpass 70%. LongBench-v2 (Bai et al., 2024) dataset is comprehensive multilingual benchmark designed to evaluate large language models (LLMs) deep understanding and reasoning capabilities in ultra-long contexts (8k2M words) through 503 challenging multiple-choice questions spanning six task categories, including single/multi-document QA, long incontext learning, and code repository analysis, with human expert accuracy limited to 53.7% under constrained conditions. A.4 Additional Experiments of Ablation Study A.4.1 Explorations on Inconsistent Training/Inference TransPoint Scheduling In this section, we supplement the experimental results of inconsistent training/inference TransPoint scheduling. Note that this setting is extreme challenging. As shown in Table 9, TransMamba can still maintain certain level in most cases under completely different reasoning structures. Even in some cases, such as the score of OBQA evaluated with the Hybrid structure, it can exceed the original TransMamba and all baselines. This gives us lot of inspiration for future research directions. The structural decoupling of reasoning can bring many research possibilities and unexplored performance. Model Transformer-400M Mamba2-400M Hybrid-400M TransMamba-400M TransMamba-400M-Inf @ Transformer TransMamba-400M-Inf @ Mamba2 TransMamba-400M-Inf @ Hybrid ARC-E OBQA PIQA ACC ACC ACC 60.57 56.15 62.33 62.50 27.27 50.82 52.20 42.4 40.8 43.6 44.8 34.8 38.2 45.0 52.75 51.10 53.89 55. 50.49 50.89 51.30 Table 9: Results of inconsistent training/inference TransPoint scheduling. Although the Inf TransMamba versions perform worse than the original consistent version in bold, the close performance inspires us to conduct future explorations. A.4.2 Model Components In the process of building TransMamba, we conducted detailed experiments on each component of the model. Table 10 shows some of the key results. Our most critical conclusions include: (1) In TransMamba, the attention block is not suitable for mapping with the of SSM; (2) Memory Converter optimization is necessary. After we modified from simple MLP fitting to the theoretical solution Memory Converter, the running speed and training effect of the model were significantly improved. In addition, the SSM block acceleration mentioned in the Mamba paper can also be used for Memory Converter. Experiment Global h residual Attention w/o Memory Converter MLP SSM (Current Version) Training Loss 3.503 3.447 3.39 3.209 3. Table 10: The training losses of ablation versions with different model structures."
        }
    ],
    "affiliations": [
        "Tencent Hunyuan",
        "The Chinese University of Hong Kong",
        "University of Macau"
    ]
}