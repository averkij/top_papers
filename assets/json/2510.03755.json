{
    "paper_title": "Code4MeV2: a Research-oriented Code-completion Platform",
    "authors": [
        "Roham Koohestani",
        "Parham Bateni",
        "Aydin Ebrahimi",
        "Behdad Etezadi",
        "Kiarash Karimi",
        "Maliheh Izadi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The adoption of AI-powered code completion tools in software development has increased substantially, yet the user interaction data produced by these systems remain proprietary within large corporations. This creates a barrier for the academic community, as researchers must often develop dedicated platforms to conduct studies on human--AI interaction, making reproducible research and large-scale data analysis impractical. In this work, we introduce Code4MeV2, a research-oriented, open-source code completion plugin for JetBrains IDEs, as a solution to this limitation. Code4MeV2 is designed using a client--server architecture and features inline code completion and a context-aware chat assistant. Its core contribution is a modular and transparent data collection framework that gives researchers fine-grained control over telemetry and context gathering. Code4MeV2 achieves industry-comparable performance in terms of code completion, with an average latency of 200~ms. We assess our tool through a combination of an expert evaluation and a user study with eight participants. Feedback from both researchers and daily users highlights its informativeness and usefulness. We invite the community to adopt and contribute to this tool. More information about the tool can be found at https://app.code4me.me."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 5 5 7 3 0 . 0 1 5 2 : r Code4Me V2: Research-oriented Code-completion Platform Roham Koohestani* Delft University of Technology Delft, the Netherlands rkoohestani@tudelft.nl Behdad Etezadi* Delft University of Technology Delft, the Netherlands betezadi@tudelft.nl Parham Bateni* Delft University of Technology Delft, the Netherlands pbateni@tudelft.nl Kiarash Karimi* Delft University of Technology Delft, the Netherlands makarimi@tudelft.nl Aydin Ebrahimi* Delft University of Technology Delft, the Netherlands aydinebrahimi@tudelft.nl Maliheh Izadi Delft University of Technology Delft, the Netherlands m.izadi@tudelft.nl Abstract The adoption of AI-powered code completion tools in software development has increased substantially, yet the user interaction data produced by these systems remain proprietary within large corporations. This creates barrier for the academic community, as researchers must often develop dedicated platforms to conduct studies on humanAI interaction, making reproducible research and large-scale data analysis impractical. In this work, we introduce Code4Me V2, research-oriented, open-source code completion plugin for JetBrains IDEs, as solution to this limitation. Code4Me V2 is designed using client-server architecture and features inline code completion and context-aware chat assistant. Its core contribution is modular and transparent data collection framework that gives researchers fine-grained control over telemetry and context gathering. Code4Me V2 achieves industry-comparable performance in terms of code completion, with an average latency of 200ms. We assess our tool through combination of an expert evaluation and user study with eight participants. Feedback from both researchers and daily users highlights its informativeness and usefulness. We invite the community to adopt and contribute to this tool. More information about the tool can be found on https://app.code4me.me. CCS Concepts Human-centered computing Human computer interaction (HCI); Software and its engineering; Keywords AI-assisted Programming, Research Tooling, Open Science, AI4SE ACM Reference Format: Roham Koohestani*, Parham Bateni*, Aydin Ebrahimi*, Behdad Etezadi*, Kiarash Karimi*, and Maliheh Izadi. 2018. Code4Me V2: Research-oriented Code-completion Platform. In . ACM, New York, NY, USA, 4 pages. https: //doi.org/XXXXXXX.XXXXXXX *Authors contributed equally. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. Conference17, Washington, DC, USA 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-1-4503-XXXX-X/2018/06 https://doi.org/XXXXXXX.XXXXXXX"
        },
        {
            "title": "1 Introduction\nWith the increased use of AI in software engineering, developer\nworkflows are undergoing a fundamental transformation. Industry\nsurveys report widespread adoption of AI coding assistants [16],\nwith empirical studies demonstrating significant productivity gains;\nfor instance, GitHub Copilot users have been observed to complete\ntasks up to 55% faster [11]. Similarly, JetBrains reports that users\nsave up to 8 hours per week with its AI Assistant [8], and an Amazon\nstudy found that developers using CodeWhisperer completed tasks\n57% faster than those who did not [1].",
            "content": "While the benefits of these tools are evident [10], the study of interaction modalities and the wide-scale analysis of developer behavioral data remain limited. This is because the most powerful systems are commercial and closed-source, which presents three critical challenges for academic research: lack of transparency into the models decision-making processes, an inability to control experimental conditions (e.g., model versions or data sources), and no access to the rich telemetry data required for fine-grained analysis. Existing academic studies have often required the creation of bespoke, single-purpose tools, consequently restricting the ability to reuse the infrastructure [6] [9]. To address this gap, we propose Code4Me V2, an extensible and research-oriented framework for empirical analysis of AI-assisted software development. As an open-source plugin for JetBrains IDEs, Code4Me V2 provides core functionalities, such as inline code completion and an interactive chat assistant. Its primary purpose, however, is to serve as platform for experimentation. The architecture is explicitly modular, and it separates user-facing components from configurable data collection and model inference backend. We expect this tool to serve as useful infrastructure component for the AI4SE community. By offering shared, transparent, and extensible platform, Code4Me V2 enables researchers to focus on experimental design and data analysis rather than on building and maintaining their own data collection systems. In the subsequent sections, we analyze the domain in section 2, describe the design of Code4Me V2 in section 3, present preliminary analysis in section 4, and discuss limitations and future work in section 5. All relevant links for Code4Me V2 can be found at https://app.code4me.me."
        },
        {
            "title": "2 Domain Analysis\nThe current ecosystem of AI coding assistants is dominated by\npowerful commercial tools, which, despite their utility, present",
            "content": "Conference17, July 2017, Washington, DC, USA Koohestani et al. significant limitations for research. Platforms like GitHub Copilot [5], Cursor [3], and JetBrains AI [7] offer robust, integrated user experiences. However, their proprietary nature makes them effectively \"black boxes.\" Researchers cannot inspect the suggestion logic, control for model updates that could act as confounding variables, or access raw interaction data. Open-source alternatives, such as Cody [15] and Code4Me [6], offer more transparency but were not designed with research extensibility as primary goal. Their architectures often lack modularity, which complicates systematic studies of developerAI interaction. For example, adapting these systems to support different experimental configurations or data collection strategies typically requires substantial modification of the core codebase. To better understand these challenges, we analyzed seven widely used coding assistants and their documentation. 1 Finding that both commercial and open-source options fall short of research needs, we designed Code4Me V2 for the academic community, which is transparent, with an open-source platform from plugin to backend; controllable, giving researchers fine-grained control over data collection through modular system; and extensible, allowing new components such as telemetry modules or context providers to be added with minimal effort."
        },
        {
            "title": "3.2 Backend\nThe backend is a Python application built with modern, scalable\ntechnologies. The API is served by FastAPI [4], chosen for its asyn-\nchronous capabilities, which allow it to handle many concurrent\nrequests efficiently. A critical design choice was to decouple long-\nrunning tasks, such as LLM inference and persistence, from each\nother and from the main API request-response cycle. We achieve\nthis using Celery [14], a distributed task queue, with Redis [13]\nas a message broker. When a completion request arrives, the API\nimmediately places it on the task queue for both the inference\nengine and the database manager. A Celery worker picks up the\ntask, performs the model inference, and publishes the result. This\nasynchronous design ensures the userâ€™s IDE never hangs while\nwaiting for irrelevant processes to the actual completion to finish.\nThe infrastructure provides support for both WebSocket and HTTP-\nbased requests, which in turn gives the front-end developer a lot of\nflexibility in how they want to implement features.",
            "content": "All data, including user profiles, telemetry, and suggestion metadata, is stored in PostgreSQL [12] database, with SQLAlchemy [2] as the ORM to ensure data integrity and adherence to ACID principles. graphical depiction of the processes and responsibilities of the backend components has been presented in part (a) of Figure 1."
        },
        {
            "title": "3.3 Frontend\nThe frontend architecture is central to Code4Me V2â€™s effectiveness\nas both a user-focused application and a research instrument. It\nis centered around a highly modular system. In the design, the\ncore unit is defined by a Module. A module can be responsible for\ncollecting data for the completion, running some update process,\nor running an arbitrary post-acceptance process. Modules follow a\nhierarchical system. To gain a better idea, the Module Manager is\nthe central coordinator that receives data collection requests from\nthe pluginâ€™s main logic. Aggregators are modules that directly re-\nceive dispatched messages from the manager. These are responsible\nfor a specific class of data. Each aggregator invokes one or more\nindividual, self-contained Modules. For example, the Behavioral\nTelemetry Aggregator might call the Typing Speed and Time\nSince Last Completion modules. Keep in mind that Aggregators\nthemselves are modules, so this self-contained module can also be\nan aggregator. This composable structure allows researchers and\ndevelopers to have highly complex interaction systems.",
            "content": "This design makes the system exceptionally extensible. To add new piece of telemetry (e.g., tracking copy-paste events), researcher only needs to implement simple module interface and register it in the configuration. No other part of the system needs to be modified; this drastically lowers the engineering effort required to design new experiments. In line with the guiding principles of IntelliJ Action systems 2, actions initiated by the plugin occur exclusively when user submits completion request, whether inline or through the chat module. We classify module actions into two categories: the data collection loop and the after-insertion loop (these actually also correspond to two separate methods in the Module interface). These represent stages within the request lifecycle where modules can integrate and execute necessary actions. The complete lifecycle of this process is detailed in part of Figure 1. 1The platforms analyzed include: GitHub Copilot, Cursor, Cody, Devin, Replit Ghostwriter, Code4Me [6], and JetBrains AI. 2IntelliJ Action System principles Code4Me V2: Research-oriented Code-completion Platform Conference17, July 2017, Washington, DC, USA Figure 1: Graphical depiction of the high-level architecture of Code4Me V2. (a) represents the backend processes of the server. (b) gives an architectural view of the implemented IntelliJ Plugin for Code4Me V2, and (c) represents the admin dashboard."
        },
        {
            "title": "4 Preliminary Evaluation\nThe platform has been evaluated for performance, stability, and its\nsuitability for research. The systemâ€™s architecture was designed to\nminimize client-side overhead by offloading all model inference to",
            "content": "Figure 2: Plugin screenshot (inside IntelliJ IDEA) Figure 3: Model calibration pane in the analytics dashboard Conference17, July 2017, Washington, DC, USA Koohestani et al. the server. To assess the server-side latency, we create pipeline using recognized Fill-in-the-Middle task dataset 3 and continuously send requests to the server to measure the average delay. In the same manner, for the chat functionality, we use well-known program synthesis dataset 4 to instruct the model to clarify and execute the task, mimicking scenario where user is inquiring from within an IDE. For code completion, we use deepseek-coder-1.3b-base, and for the chat model, we use Ministral-8B-Instruct. End-to-end latency for code completion requests, from the trigger event in the IDE to the rendering of ghost text, is consistently 186.31 ms ( 139.50), with an average of 18.66 tokens, which is well within the threshold for non-disruptive user experience. For chat completions, the latency is 8369.78 ms ( 840.48) with an average of 277.08 tokens. To evaluate the plugins usability and research extensibility, we conducted two-phase user evaluation. First, formative user study was conducted with four expert researchers from the AI for Software Engineering (AI4SE) faculty. Participants were deliberately chosen based on their familiarity with AI4SE and their frequency of using AI coding assistants. They were then asked to complete basic multi-file programming task, which involved creating Shape interface and Circle and Rectangle implementing classes. During this task, they were expected to interact with both the inline completion and chat features. Feedback was gathered through semi-structured interview format and questionnaire. The questionnaire focused on three key areas: General Usability (e.g., relevance and timeliness of suggestions), Setup & Configuration (e.g., clarity of data collection settings), and Extensibility for Research (e.g., modifying the plugin for experiments). The qualitative feedback from this initial study strongly validated the platforms core premise: participants unanimously praised the modularity and extensibility, and expressed confidence in their ability to adapt the tool for their own research workflows. The study also identified areas for improvement, primarily regarding the intuitiveness of the module management UI and inconsistencies in the timing of automatic suggestions. Based on the results of this formative study, we made several iterations on the initial design. We then conducted secondary evaluation with four daily users using the same protocol. Our findings revealed that the individuals did not echo the issues previously identified by experts. Furthermore, fewer participants mentioned timing issues with the suggestions, experiencing fewer challenges with frequency and response time. Nonetheless, one user noted that the suggestions could benefit from slightly increased speed. Lastly, participant requested the inclusion of an Agent feature in the plugin, highlighting (1) rapidly evolving domain and (2) the necessity for quick adaptation from academia, the later of which is now facilitated by Code4Me V2."
        },
        {
            "title": "5 Limitations and Future Work\nOur preliminary analysis also identified several limitations. As\nnoted in our user study, the user interface for experiment config-\nuration, while powerful, requires a learning curve that could be\nsteep for non-expert users. While improved in the second round\nof iterations, there is still room for improvement, both in terms",
            "content": "3Santacoder FIM task. 4We use the MBPP dataset. of how settings are shown and whether settings should be shown at all. While the platform is performant for research purposes, its completion speed and model quality do not yet match that of wellresourced commercial tools. The current context retrieval, while rather effective in single-file and multi-file settings, still requires attention to be on par with commercial counterparts. primary objective is the implementation of project-wide context retrieval to enhance the accuracy of code suggestions. Moreover, continuously improving and modifying the current framework is crucial to continue meeting the communitys needs (see the request for an agent feature in section 4). Therefore, we invite community members to engage with this project to ensure mutual benefit. We will be overseeing the repositories and welcome community contributions."
        },
        {
            "title": "6 Conclusion\nCode4Me V2 is an open-source, modular platform designed to lower\nthe barrier for empirical research in AI-assisted software develop-\nment. Our preliminary evaluations demonstrate that the platform\nis both practical for day-to-day use and adaptable for experimental\nneeds. By providing a transparent, controllable, and extensible tool,\nwe enable the academic community to rigorously investigate the\ncomplex dynamics of human-AI collaboration in programming. We\nbelieve Code4Me V2 can serve as a foundational piece of infrastruc-\nture for the next wave of AI4SE research.",
            "content": "References [1] Inc. Amazon Web Services. 2023. Amazon CodeWhisperer. AWS presenhttps://pages.awscloud.com/rs/112-TZM-766/images/ADC-C3%20tation. %20Amazon%20CodeWhisperer.pdf [2] Mike Bayer. 2025. SQLAlchemy: The Database Toolkit for Python. Retrieved Sep 15, 2025 from https://www.sqlalchemy.org/ [3] Cursor 2025. Cursor - The AI Code Editor. Retrieved Sep 15, 2025 from https: //cursor.com/ [4] FastAPI 2025. FastAPI: Fast and High-performance Web Framework for APIs. Retrieved Sep 15, 2025 from https://fastapi.tiangolo.com/ [5] GitHub 2025. GitHub Copilot. Retrieved Sep 15, 2025 from https://github.com/ features/copilot [6] Maliheh Izadi, Jonathan Katzy, Tim Van Dam, Marc Otten, Razvan Mihai Popescu, and Arie Van Deursen. 2024. Language models for code completion: practical evaluation. In Proceedings of the IEEE/ACM 46th International Conference on Software Engineering. 113. [7] JetBrains 2025. JetBrains AI Intelligent Coding Assistance, AI Solutions, and More. Retrieved Sep 15, 2025 from https://www.jetbrains.com/ai/ [8] Irina Mariasova, Yanina Ledovaya, Olga Lvova, and Mikhail Bogdanov. 2024. Developers save up to 8 hours per week with JetBrains AI Assistant. JetBrains Blog. https://blog.jetbrains.com/ai/2024/04/developers-save-up-to-8-hours-perweek-with-jetbrains-ai-assistant/ [9] Hussein Mozannar, Gagan Bansal, Adam Fourney, and Eric Horvitz. 2024. Reading between the lines: Modeling user behavior and costs in AI-assisted programming. In Proceedings of Conference on Human Factors in Computing Systems. [10] Ruchika Pandey, Prabhat Singh, Raymond Wei, and Shaila Shankar. 2024. Transforming Software Development: Evaluating the Efficiency and Challenges of GitHub Copilot in Real-World Projects. (2024). arXiv:2406.17910 [cs.SE] [11] Sida Peng, Eirini Kalliamvakou, Peter Cihon, and Mert Demirer. 2023. The Impact of AI on Developer Productivity: Evidence from GitHub Copilot. (2023). arXiv:2302.06590 [cs.SE] [12] PostgreSQL Global Development Group 2025. PostgreSQL: The Worlds Most Advanced Open Source Relational Database. Retrieved Sep 15, 2025 from https: //www.postgresql.org/ [13] Redis 2025. Redis - The Real-time Data Platform. Retrieved Sep 15, 2025 from https://redis.io/ [14] Ask Solem. 2025. Introduction to Celery. Retrieved Sep 15, 2025 from https: //docs.celeryq.dev/en/latest/getting-started/introduction.html [15] Sourcegraph 2025. Cody AI coding assistant from Sourcegraph. Retrieved Sep 15, 2025 from https://sourcegraph.com/cody [16] StackOverflow 2025. AI 2025 Stack Overflow Developer Survey. Retrieved Sep 15, 2025 from https://survey.stackoverflow.co/2025/ai#1-ai-tools-in-thedevelopment-process"
        }
    ],
    "affiliations": [
        "Delft University of Technology Delft, the Netherlands"
    ]
}