{
    "paper_title": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving",
    "authors": [
        "Maciej K. Wozniak",
        "Lianhang Liu",
        "Yixi Cai",
        "Patric Jensfelt"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While end-to-end autonomous driving models show promising results, their practical deployment is often hindered by large model sizes, a reliance on expensive LiDAR sensors and computationally intensive BEV feature representations. This limits their scalability, especially for mass-market vehicles equipped only with cameras. To address these challenges, we propose PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving architecture operates using only camera data, without explicit BEV representation and forgoing the need for LiDAR. PRIX leverages a visual feature extractor coupled with a generative planning head to predict safe trajectories from raw pixel inputs directly. A core component of our architecture is the Context-aware Recalibration Transformer (CaRT), a novel module designed to effectively enhance multi-level visual features for more robust planning. We demonstrate through comprehensive experiments that PRIX achieves state-of-the-art performance on the NavSim and nuScenes benchmarks, matching the capabilities of larger, multimodal diffusion planners while being significantly more efficient in terms of inference speed and model size, making it a practical solution for real-world deployment. Our work is open-source and the code will be at https://maxiuw.github.io/prix."
        },
        {
            "title": "Start",
            "content": "PRIX: Learning to Plan from Raw Pixels for End-to-End Autonomous Driving Maciej K. Wozniak1 Lianhang Liu2 Yixi Cai1 Patric Jensfelt1 1KTH Royal Institute of Technology, Sweden 2 Scania CV AB 5 2 0 J 4 2 ] . [ 2 6 9 5 7 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "While end-to-end autonomous driving models show promising results, their practical deployment is often hindered by large model sizes, reliance on expensive LiDAR sensors and computationally intensive BEV feature representations. This limits their scalability, especially for mass-market vehicles equipped only with cameras. To address these challenges, we propose PRIX (Plan from Raw Pixels). Our novel and efficient end-to-end driving architecture operates using only camera data, without explicit BEV representation and forgoing the need for LiDAR. PRIX leverages visual feature extractor coupled with generative planning head to predict safe trajectories from raw pixel inputs directly. core component of our architecture is the Context-aware Recalibration Transformer (CaRT), novel module designed to effectively enhance multi-level visual features for more robust planning. We demonstrate through comprehensive experiments that PRIX achieves state-of-the-art performance on the NavSim and nuScenes benchmarks, matching the capabilities of larger, multimodal diffusion planners while being significantly more efficient in terms of inference speed and model size, making it practical solution for real-world deployment. Our work is open-source and the code will be at https://maxiuw.github.io/prix. 1. Introduction In recent years, end-to-end autonomous driving has emerged as prominent research direction, driven by its all-in-one training pipeline and goal-oriented output (final trajectory) [5]. End-to-end models aim to learn direct mapping from sensor inputs to the vehicles trajectory through large-scale data-driven approaches. Compared with traditional modular pipelines, where perception, prediction, and planning are trained and designed, this paradigm streamlines the overall system and reduces the risk of error propagation between subsystems [33, 37, 45]. However, achieving robust and scalable end-to-end solutions in realFigure 1. Performance vs. inference speed comparing our cameraonly model, PRIX, to leading methods on the NavSim-v1 benchmark. PRIX outperforms or matches the performance of multimodal methods SOTA like DiffusionDrive [34], while being significantly smaller and faster. Notably, it operates at highly competitive framerate, falling only 3 FPS behind the fastest model, Transfuser [10], while substantially outperforming it in PDMS. world, dynamic environments remains major challenge. Whether using cameras, LiDAR, or both, the computationally intensive process of feature extraction remains the primary bottleneck in modern end-to-end architectures. Current state-of-the-art (SOTA) end-to-end autonomous driving methods [28,31,34,53] have focused on fusing multiple sensor modalities, primarily camera and LiDAR, to build comprehensive environmental representation [10,28, 31, 34, 53]. While effective, this reliance on expensive LiDAR sensors and computationally intensive methods limits the scalability of such systems, particularly for mass-market consumer vehicles, which are typically equipped only with cameras, limiting their applicability to vehicles with more expensive sensor suites. Moreover, all these methods depend on the BEV features, which are computationally expensive, especially for the camera branch that has to be cast to BEV by e.g,. LSS-type models [42]. On the other 1 hand, many existing camera-only end-to-end approaches suffer from significant practical limitations. Notably, leading camera-only architectures like UniAD and VAD [24,27] are often oversized, containing over 100 million parameters. This large size makes them computationally expensive, resulting in slower inference speeds and more demanding training requirements. While all components of end-to-end models are integral, we argue that the primary determinant of system performance is the visual feature extractor. Its ability to learn task-relevant representation plays the key role in success of downstream planning task. However, it is also often the visual feature extractor that is driving the computational cost. We posit that it is possible to learn rich visual representations directly from camera inputs for planning without explicitly depending on BEV representation or 3D geometry from LiDAR. Through detailed analysis of training losses, model design, and experiments with various planning heads, we demonstrate the importance of visual features in end-to-end learning. Our focus on visual cameraonly learning is motivated by recent advancements from visual foundation models and world models [2, 38, 49, 51] that have proven that rich, high-fidelity 3D representations of the world can be learned directly from cameras [22, 29, 39, 48, 56]. This camera-only paradigm opens the door for powerful, low-cost autonomous systems suitable for wide range of customer-level vehicles. The autonomous driving domain is particularly well-suited for this approach; vehicles are commonly equipped with 6 to 10 cameras, and each cameras calibration information is known at each frame [1, 3, 4, 12, 15, 46], making learning of spatial visual representation feasible. Inspired by these works, we propose Plan from Raw Pixels (PRIX): novel end-to-end driving architecture that operates using only camera data and forgoes the need for LiDAR or BEV features. Our method uses smart visual feature extractor coupled with generative planning head to directly predict safe trajectories. We demonstrate that our approach successfully predicts future trajectories outperforming other camera-only and most of the multimodal SOTA approaches while being significantly faster and requiring less memory, as shown in Fig. 1. This makes PRIX practical solution for real-world deployment. Our contributions are as follows: We introduce PRIX, novel camera-only, end-to-end planner that is significantly more efficient than multimodal and previous camera-only approaches in terms of inference speed and model size. We propose the Context-aware Recalibration Transformer (CaRT), new module designed to effectively enhance multi-level visual features for more robust planning. We provide comprehensive ablation study that validates our architectural choices and offers insights into optimizing the trade-off between performance, speed, and model size. Our method achieves SOTA performance on the NavSim-v1, NavSim-v2 and nuScenes datasets, outperforming larger, multimodal planners and outperforming other camera-only approaches while being much smaller and faster. 2. Related work Multimodal End-to-End Driving To achieve comprehensive perception of the environment, many recent studies emphasize fusing data from multiple sensors like cameras and LiDAR [52]. Initial works like Transfuser [10] used complex transformer architecture for this fusion. Building this robust world model is the foundational first step; however, the ultimate goal is to translate this perception into safe and effective driving actions. This crucial transition from perception to planning has spurred its own wave of innovation. Early approaches like VADv2 [6] and Hydra-MDP [31] discretized the planning space into sets of trajectories. To overcome the limitations of predefined anchors (pre-set potential trajectories), subsequent research has focused on generating more flexible, continuous paths. This includes diffusion models like DiffE2E [60] and TransDiffuser [28], which create diverse trajectories without anchors. Architectural innovations have also been key; DRAMA leverages the Mamba state-space model for computational efficiency, ARTEMIS [13] uses Mixture of Experts (MoE) for adaptability in complex scenarios, and DualAD [9] disentangles dynamic and static elements for improved scene understanding. An alternative paradigm is Reinforcement Learning (RL), where models like RAD [16] are trained via trial and error in photorealistic simulations built with 3D Gaussian Splatting, helping to overcome the causal confusion issues of imitation learning. Despite these advances, critical perspective from Xu et al. [55] highlights significant performance gap when models are applied to noisy, real-world sensor data, underscoring the importance of robust intermediate perception. While SOTA methods demonstrate powerful capabilities, they are often complex and depend on multimodal sensors. In contrast, our proposed method is designed for simplicity, using only single modality while achieving better or comparable performance. Camera only End-to-End Driving End-to-end autonomous driving has evolved from camera-only systems to language-enhanced models. Early camera-only methods 2 Figure 2. PRIX Overview: Visual features from multi-camera images are extracted by ResNet layers (fi) and together with self-attention and skip connections (CaRT, described in Sec. 3.1). Next, visual features are used for auxiliary perception tasks (see Sec. 3.4) and trajectory planning (see Sec. 3.2). conditional diffusion planner then uses visual features, along with the current ego state and set of noisy anchors, to generate the final output trajectory. like UniAD [24] established unified frameworks for perception, prediction, and planning. To improve efficiency over dense Birds-Eye-View (BEV) representations, subsequent works introduced more structured alternatives, such as the vectorized scenes in VAD [27], sparse representations in Sparsedrive [47], 3D semantic Gaussians [61], and lightweight polar coordinates [14]. Planning processes were also refined through iterative techniques in models like iPAD [19] and PPAD [8], while others focused on robustness with Gaussian processes (RoCA [58]) or precise trajectory selection (DriveSuprim [57], GTRS [32]). Efficiency has also been addressed at the input level with novel tokenization strategies [25]. More recently, Vision Language Models (VLMs) have been integrated to enhance reasoning. LeGo-Drive [41] uses language for high-level goals, while SOLVE [7] and DiffVLA [26] leverage VLMs for action justification and to guide planning. To manage the high computational cost, methods like DiMA [21] distill knowledge from large models into more compact planners. The capabilities of these advanced models are assessed using new evaluation frameworks like LightEMMA [43]. In contrast to many oversized and slower camera-only methods, PRIX is designed to balance high performance with computational speed, as shown in Fig. 1. As shown in Sec. 4, our model outperforms other camera-only models on available benchmarks while being much more efficient. Generative Planning Early end-to-end methods often regressed single trajectory, which can fail in complex scenarios with multiple valid driving decisions. To address this, recent work has shifted towards generating multiple possible trajectories to account for environmental uncertainty. More recently, generative models have become pivotal tool. DiffusionDrive [34] applies diffusion models to trajectory generation, introducing truncated diffusion process to make inference feasible in real-time. In parallel, DiffusionPlanner [62] leverages classifier guidance to inject cost functions or safety constraints into the diffusion process, allowing the generated trajectories to be flexibly steered. To further reduce inference complexity, GoalFlow [53] employs flow matching method, which learns simpler mapping from noise to the trajectory distribution. Lately, TransDiffuser [28] proposed to combine both anchors and endInspired by the speed and performance of these points. methods, generative trajectory heads seems to be go-to approach yielding the best results [30] While generative methods have significantly advanced the field, they are often designed to operate on multi-sensor features. Our work builds upon the insights of generative planning but adapts them to more efficient, camera-only architecture. 3. Method The goal of our end-to-end autonomous driving model, shown in Fig. 2, is to generate the best future trajectory of the ego-vehicle from raw camera data. Camera only feature extraction, detailed in Sec. 3.1, is base for the conditional denoising diffusion planner, described in Sec. 3.2. We detail and justify our design choices in Sec. 3.3 and the main objective and auxiliary tasks are discussed in Sec. 3.4. 3.1. Visual Feature Extraction The foundation of our proposed method is lightweight, camera-only, visual feature extractor designed to derive rich, multi-scale representation of the driving scene, as shown in Fig. 3. This hierarchical approach is critical for autonomous driving, task that demands both high-level semantic understanding (e.g., recognizing an upcoming intersection) and precise low-level spatial detail (e.g., tracking the exact lane curvature). To generate and refine these multi-scale features, we employ ResNet [20] as the hierarchical backbone, which 3 naturally extracts feature maps (xi) at distinct resolutions. However, with raw ResNet features, we face classic dilemma: early layers capture fine spatial details but lack scene-level understanding, while deeper layers possess rich semantic context but are spatially coarse. To address this, we introduce our novel Context-aware Recalibration Transformer (CaRT) module. The feature map xi, where {1, 2, 3, 4}, is first spatially standardized via adaptive average pooling to fixed size (512 in our implementation, see Sec. 3.3 for ablation studies). Next, features are processed by self-attention (SA) part of CaRT module to model long-range dependencies across the spatial domain (see Fig. 3). single, weightshared multi-head self-attention block is applied to each sequence of tokens (explained in Sec. 3.3). For each feature level i, we compute the Query (Qi), Key (Ki), and Value (Vi) matrices using shared linear projection matrices WQ, WK, and WV : Qi = xiWQ, Ki = xiWK, Vi = xiWV . (cid:17) The output of the CaRT module is the attention Ai computed using the scaled dot-product attention (cid:16) QiKT Vi. Ai, which is our reA(Qi, Ki, Vi) = softmax dk calibrated feature map, is then upsampled to the original dimensions of xi, concatenated with the original xi feature map (extracted from ResNet) via skip connection, creating xc , and fed to the next ResNet layer fi+1 as shown in Fig. 3. The iterative recalibration is this process of actively refining the initial feature maps from the ResNet backbone by infusing them with global semantic context learned via SA as an act of adjusting the value and significance of the initial local features based on this newly understood global context. It is not just adding new information; it is fundamentally changing the interpretation of the existing features by infusing them with the global context of the entire scene generated by the CaRT self-attention layers. The final feature map is Global Features, which encapsulates information from all levels. To synthesize the final multi-scale representation, the architecture ends in topdown pathway, analogous to Feature Pyramid Network (FPN). The Semantic Features are passed through series of upsampling and 3x3 convolutional layers to restore higher-resolution feature map, ensuring it benefits from semantic context while retaining precise spatial understanding. The resulting feature map provides comprehensive visual foundation, balancing semantic abstraction and spatial fidelity, for the subsequent generative planning head. 3.2. Diffusion-Based Trajectory Planner For motion planning, we adopt conditional denoising diffusion head from DiffusionDrive [34] that generates trajectories via iterative refinement (we also experiment with different planners in Sec. 4.3, showing that our method can achieve good performance with any planner). Unlike standard regression-based planners, this approach treats trajecFigure 3. Architecture of our visual feature extractor with Context-aware Recalibration Transformer (CaRT) module. An input feature map fi is processed in parallel through skip connection and recalibration path. The recalibration path uses adaptive pooling and self-attention block to capture global context. The resulting features are upsampled and added back to the original map via residual connection, producing refined output that is enhanced with contextual information. tory prediction as denoising process: given an initial set of noisy trajectory proposals (anchors), ego vehicle state, and visual features, the model gradually refines them into feasible plans. The trajectory is represented as sequence of waypoints, τ = {(xt, yt)}Tf t=1, where Tf is the planning horizon and (xt, yt) is the waypoint location at future time in the ego-vehicles coordinate system. The forward process, q, progressively adds Gaussian noise to clean trajectory τ 0 over discrete timesteps. This can be expressed in single step as:q(τ iτ 0) = αiτ 0, (1 αi)n), where is the diffusion timestep, (τ i; and the noise schedule αi = (cid:81)i s=1(1 βs) is predefined. As approaches n, τ converges to an isotropic Gaussian distribution. The reverse process learns to remove the noise to recover the original trajectory. We train neural network, ϵθ, to predict the noise component, ϵ, that was added to the trajectory at timestep i. This process is conditioned on context vector, c, which combines information from the environment and the vehicles state. We define by processing and comfeatures, cvisual, from cameras, vehibining the visual cles current ego-state cego and noisy anchors canch: = combine(cvisual, cego, canch). We start with predefined anchor trajectories with added random noise τ and iteratively apply the model ϵθ to denoise the trajectories at each step, guided by the context vector c, ultimately yielding clean, context-appropriate trajectories τ 0 from which we choose the one with highest confidence rate as the final trajectory (shown in qualitative results in supplementary) Note, while number of steps is commonly large in generative models area [44], larger reduces models latency and as we show in Sec. 3.3, causes the model to fall into the simplest (not the best) solution, as well as dropping the methods speed. suggesting that anchors alone are better approach, which we used in our model. 3.3. Design choices and findings Our initial design consisted of visual feature extractor with separate self-attention modules in CaRT corresponding to each feature level of ResNet backbone and two-step diffusion planner. Throughout this section, we analyze our design in detailed ablation studies (done on Navsim-v1) to arrive at the final configuration of our model. Module Integration Strategy Our experiments show that using CaRT module where the self-attention layers share weights across all feature scales of the backbone outperforms using separate, specialized SA for each xi. As detailed in Tab. 1, this shared-weight design not only achieves higher score but also reduces the parameter count and increases inference speed. This indicates that the core logic of using global context to recalibrate local features is universal principle. Forcing single set of self-attention weights to learn this logic across different levels of feature abstraction results in more robust and generalized representation. Finding I: shared, scale-invariant module for contextual feature refinement is more effective and efficient than using specialized, scale-specific modules, reducing the models parameter count and improving inference speed. Table 1. Ablation on sharing weights in SA layers in CaRT module across different scales. Configuration Params PDMS FPS Separate SA Shared SA 256 Shared SA 512 Shared SA 768 39M 33M 37M 39M 87.3 87.0 87.8 87. 54.4 57.9 57.0 56.0 Anchors with end points Inspired by the concept of GoalFlow [53], in Tab. 2 we experimented with using the final end point as an additional conditioning signal for our diffusion head planner, aiming to help the final trajectory objective. We hypothesized that this would complement the guidance from the anchors. However, our findings indicate that the combination of anchors and end points is counterproductive and appears to confuse the planner, creating conflict between the local, step-by-step guidance from anchors and the global pull of the final destination. As result, this combination led to slight degradation in performance, with the Predictive Driver Model score (PDMS) decreasing Table 2. Ablation on anchors plus end points Model Anchors End-Points PDMS PRIX PRIX PRIX 87.8 83.5 85.9 Overall Impact of CaRT To quantify the contribution of the CaRT module and justify its computational cost, we created baseline version of PRIX without it. The residual connection still exists but processes features that are only downsampled and upsampled, without any transformerbased processing. In Tab. 3 we show that removing the module reduces parameters and increases speed but model performance drastically drops. Therefore, we included the CaRT module in our final model, as it provides significant performance boost while remaining highly efficient. Finding II: The self-attention mechanism plays crucial role in modeling spatial dependencies and recalibrating channel-wise features. Table 3. Ablation on the existence of the CaRT module. Configuration Parameters PDMS FPS PRIX (with CaRT) PRIX (no CaRT) 37M 20M 87.8 76.4 57.0 70.9 Diffusion steps We experimented with various truncated diffusion time steps, specifically 2-50 and evaluated performance using the PDMS shown in Fig. 4. The results showed that performance degrades when the number of diffusion steps increases. Such over-smoothing diminishes the quality of the final predictions, reflected in the notable drop in PDMS at higher step counts; thus, we opt for 2 steps. Finding III: Increasing the number of diffusion steps beyond short, optimal range degrades prediction quality. 3.4. Training Objective Relying solely on trajectory imitation loss, as shown in Tab. 8 and other works [10, 27, 34], is insufficient for an end-to-end model to learn the rich representations needed for robust autonomous driving. To address this, we employ multi-task learning paradigm. By adding auxiliary 5 Figure 4. Diffusion steps vs performance on Navsim-v1. tasks, we introduce powerful inductive bias that compels our camera-only feature extractor to learn more structured and semantically meaningful representation of the world, which ultimately leads to better planning. Our total loss is weighted sum of the primary planning task and auxiliary objectives: = λplanLplan + λdetLdet + λsemLsem, (1) where λ terms are the corresponding loss weights. Detailed architecture of the segmentation and detection heads can be found in the supplementary. Primary Planning Loss (Lplan) Our model learns the ego-vehicles future path by minimizing the L1 distance between the predicted waypoints ˆp1:T and the groundThis loss, defined as Lplan = truth trajectory p1:T . 1 t=1 ˆpt pt1, optimizes the final trajectory. (cid:80)T Auxiliary Task: Object Detection (Ldet) Safe navigation requires awareness of other road users. We add an auxiliary objective to localize traffic participants like vehicles and pedestrians. This ensures the models internal representations are sensitive to dynamic agents that influence planning. The detection loss, Ldet = λclsLcls + λregLreg, combines focal loss for classification and an L1 loss for 3D bounding box regression. Auxiliary Task: Semantic Consistency (Lsem) To ensure the model understands the static driving environment, we introduce semantic consistency loss. This provides dense, pixel-level supervision, compelling the feature extractor to learn the scenes structure, such as drivable areas and lane boundaries. We apply pixel-wise cross-entropy (CE) loss, Lsem = CE(ˆS, S), between the predicted ˆS and groundtruth semantic maps. This contextual understanding enables more feasible and appropriate trajectories. 4. Experiments In this section, we benchmark our method against other SOTA approaches on various datasets. Detailed parameter setup, additional experiments, and more qualitative results can be found in the supplementary. We use scores reported by the authors, unless otherwise indicated. 4.1. Experiment setup Data and metrics: NavSim-v1 [12] is benchmark for evaluating autonomous driving agents using non-reactive simulation where an agent plans trajectory from initial sensor data. This approach avoids costly re-rendering while still enabling detailed, simulation-based analysis of the maneuvers safety and quality. Evaluation is based on the PDMS, which aggregates several metrics. It heavily penalizes safety failures while rewarding driving performance, calculated as: PDMS = (cid:89) scorem m{NC,DAC} (cid:123)(cid:122) (cid:124) penalties (cid:124) (cid:125) (cid:80) (cid:80) w{EP,TTC,C} weightw scorew w{EP,TTC,C} weightw (cid:123)(cid:122) weighted average (cid:125) , (2) where penalties come from collisions (NC) and staying in the drivable area (DAC) with weighted average of scores for progress (EP), time-to-collision (TTC), and comfort (C). NavSim-v2 [4] introduces pseudo-simulation. planned trajectory is executed in simulation with reactive traffic, and performance is measured by an Extended PDM Score (EPDMS). Note, NavSim-v2 is very recent dataset and only few approaches have been tested or adopted to it (most of them still under review). EPDMS = (cid:89) mMpen (cid:124) filterm(agent, human) (cid:80) mMavg (cid:123)(cid:122) penalty terms (cid:124) (cid:125) wm filterm(agent, human) wm (cid:80) mMavg (cid:123)(cid:122) weighted average terms (cid:125) (3) The nuScenes trajectory prediction [3] benchmark challenge is popular and rich resource, where we compare our performance with larger range of camera-only methods. Following previous works [34], we evaluate our performance on open-loop metrics: L2 and collision rate [3]. 4.2. Benchmarks By consistently leading in overall scores and key safety metrics on Navsim-v1 and v2 Tabs. 4 and 5 , PRIX proves to be powerful, effective, and well-balanced solution for autonomous navigation. Additionally, as shown in Fig. 1 PRIX is much faster than other methods. On the Navsim-v1 benchmark, PRIX distinguishes itself as the top-performing model, achieving leading PDMS of 87.8. This result is particularly noteworthy as PRIX, 6 Table 4. Performance comparison of different driving models for Navsim-v1. The up arrow () indicates that higher values are better. Best results are in bold, and second best are underlined. C&L refers to Camera and LiDAR input. Default GoalFlow uses V2-99, but they reported Resnet34 results in the ablations. Input Camera Method VADv2 [6] Hydra-MDP-V [31] & UniAD [24] LTF [10] PARA-Drive [50] Transfuser [10] DRAMA [59] GoalFlow [53] Hydra-MDP++ [30] Camera Camera PRIX (ours) Camera Camera Camera & & & Backbone NC DAC TTC Comf. EP 76.0 Resnet34 77.6 Resnet34 78.8 Resnet34 79.0 Resnet34 79.3 Resnet34 79.2 Resnet34 80.1 Resnet34 79.8 Resnet34 80.4 Resnet34 82.3 Resnet 91.6 92.9 92.9 92.4 93.0 92.8 94.8 94.3 93.1 94.1 100 100 100 100 99.8 100 100 100 100 100 97.2 97.9 97.8 97.4 97.9 97.7 98.0 98.3 97.6 98.1 89.1 91.7 91.9 92.8 92.4 92.8 93.1 93.8 96.0 96.3 PDMS 80.9 83.0 83.4 83.8 84.0 84.0 85.5 85.7 86.6 87.8 (a) Our model can correctly do safe left run on busy intersection. (b) Our trajectory looks safer than GT since it keeps larger safe distance on the left of the other vehicle. Figure 5. Qualitative trajectory predictions from our method. In some cases, like 5b, our predictions are safer than the ground truth. Table 5. Performance comparison of different driving models for Navsim-v2. The up arrow () indicates that higher values are better. Best results are in bold, and second best are underlined. All the methods are camera-only. Method Backbone NC DAC DDC TL EP TTC LK HC EC EPDMS Human Agent Ego Status MLP Transfuser [10] HydraMDP++ [30] PRIX (ours) Resnet34 Resnet34 Resnet34 100 93.1 96.9 97.2 98.0 100 77. 89.9 97.5 95.6 99.8 92.7 97.8 99.4 99.5 100 99.6 99.7 99.6 99.8 87.4 86. 87.1 83.1 87.4 100 91.5 95.4 96.5 97.2 100 89.4 92.7 94.4 97.1 98.1 98. 98.3 98.2 98.3 90.1 85.4 87.2 70.9 87.6 90.3 64.0 76.7 81.4 84.2 camera-only model, not only surpasses other methods using the same input but also outperforms models equipped with richer Camera and LiDAR data, such as DRAMA [59]. Its superiority is further detailed by its first-place rankings in critical safety and performance metrics, underscoring its well-rounded and reliable nature, also highlighted in Fig. 5. This strong performance is consistently replicated on the more recent Navsim-v2 benchmark. Here, PRIX again achieves the best overall EPDM of 84.2, solidifying its position as the leading model. We are especially good on EC, heavily outperforming current SOTA, HydraMDP++ [30]. PRIX also achieves SOTA performance on the nuScenes trajectory prediction challenge, outperforming all existing camera-based baselines, shown in Tab. 6. In terms of average L2 error across 1s to 3s horizons, PRIX achieves the lowest value of 0.57m, surpassing the previously best DiffusionDrive (0.65 m) and SparseDrive (0.61 m). Moreover, PRIX yields the lowest collision rate at 0.07%, with 0.00% collision rate at 1 second, indicating strong shortterm safety. Notably, PRIX also operates at the highest inference speed (11.2 FPS), demonstrating that our model offers superior balance of accuracy, safety, and efficiency. Comparison with DiffusionDrive As shown in Tab. 7 PRIX achieves comparable performance to the current SOTA end-to-end multimodal approach, DiffusionDrive [34] while operating more than 25% faster. This efficiency gain is attributed to our end-to-end models ability to plan trajectories directly from visual input, which eliminates the need for LiDAR data and the costly computational overhead of sensor fusion. This streamlined approach not only reduces hardware cost and complexity but also makes our method more viable and scalable solution. Further7 Table 6. Performance comparison of different driving models for nuScenes. The up arrow () indicates that lower values are better. Best results are in bold, and second best are underlined. Method Input Backbone L2 (m) 3s 2s 1s Collision Rate (%) Avg. 1s 2s 3s Avg. FPS 2.11 EffNet-b4 Camera ST-P3 [23] 0.70 Camera ResNet-101 UniAD [24] 2.13 ResNet-50 Camera OccNet [35] 0.70 ResNet-50 Camera VAD [27] 0.58 ResNet-50 Camera SparseDrive [47] DiffusionDrive*1 [34] Camera 0.62 ResNet-50 0.53 PRIX (ours) ResNet-50 Camera *1 We and other researchers were not able to reproduce results reported on nuScenes. We included the results we obtained. https://github. com/hustvl/DiffusionDrive/issues/57 as well as issues/45. We still outperform the reported results (in the supplementary). 2.90 1.04 2.99 1.05 0.96 1.03 0. 1.27 0.63 1.37 0.41 0.18 0.19 0.18 0.62 0.58 0.59 0.17 0.05 0.06 0.04 2.11 0.73 2.14 0.72 0.61 0.65 0.57 0.71 0.61 0.72 0.22 0.08 0.09 0.07 1.6 1.8 2.6 4.5 9.0 8.2 11.2 1.33 0.45 1.29 0.41 0.29 0.31 0. 0.23 0.62 0.21 0.07 0.01 0.03 0.00 more, when compared to DiffusionDrives camera-only implementation on nuScenes in Tab. 6, our model achieves superior performance, highlighting its advantages in both efficiency and effectiveness. Table 7. Performance comparison with DiffusionDrive on Navsim-v1 [34]. PDMS component comparison in supplementary. Model Sensors PDMS Params FPS DiffusionDrive LiDAR + Camera PRIX (Ours) Camera 88.1 87.8 60M 37M 45.0 57.0 4.3. Ablations We further ablate different components of our model after initial design analysis in Sec. 3.3. All ablations are done on Navsim-v1. also the slowest at 57.0 FPS, simple MLP head is highly competitive. This strong performance from minimal planner proves the richness of the learned visual representation. clear trade-off exists: for applications requiring higher speed, the diffusion head can be swapped for much faster alternatives, like the MLP or the second-best LSTM, with only minor compromise in accuracy. This confirms that foundational heavy lifting is handled by the visual encoder. Table 9. Planners comparison, all models use ResNet34. Model Planner PDMS Params FPS PRIX (baseline) Diffusion PRIX-mlp PRIX-t PRIX-ls MLP Transformer LSTM 87.8 85.1 85.4 86.7 37M 33M 35M 34M 57.0 65.3 62.8 63.4 Loss influence: We demonstrate the progressive benefit of each auxiliary loss. The baseline model, using only the planning loss (Lplan), scores 70.4 on PDMS. Adding tasks responsible for environment understanding as agent detection and classification plus semantic segmentation, successively boosts the score as shown in Tab. 8. That confirms that the planners performance is directly coupled with the quality of the features, which learn semantically rich representation of the scene through these auxiliary tasks. Limitation and future work While PRIX achieves great performance and speed, its camera-only nature makes it vulnerable to adverse weather, occlusions, and sensor failure or decalibration. Future work can enhance robustness through two main avenues. First, self-supervised pre-training on large, unlabeled datasets could help the backbone learn more resilient features [18, 36, 54]. Second, incorporating control-based approaches could better manage uncertainties and improve safety in challenging scenarios [17, 40]. Table 8. Contribution of each loss component. Exp. # Lplan Lbox Lsem Lcls PDMS 1 2 2 3 4 (Full) 70.4 82.3 85.7 86.9 87.8 Different Planners: Results in Tab. 9 affirm our core hypothesis that visual feature extractor is the most critical component. While our top-performing diffusion planner is 5. Conclusions We introduce PRIX, an efficient and fast camera-only driving model that outperforms other vision-based methods and rivals the performance of state-of-the-art multimodal systems. While acknowledging LiDARs importance for robustness, we prove that high performance is achievable with vision alone. PRIX demonstrates that relying directly on rich camera features for planning is viable alternative to the BEV representation and multimodal approaches, establishing new benchmark for what is achievable in efficient, vision-based autonomous driving systems. 8 Acknowledgements This work was partially supported by the Wallenberg AI, Autonomous Systems and Software Program (WASP) funded by the Knut and Alice Wallenberg Foundation. The computations were enabled by the supercomputing resource, Berzelius, provided by the National Supercomputer Centre at Linkoping University and the Knut and Alice Wallenberg Foundation, Sweden."
        },
        {
            "title": "References",
            "content": "[1] Mina Alibeigi, William Ljungbergh, Adam Tonderski, Georg Hess, Adam Lilja, Carl Lindstrom, Daria Motorniuk, Junsheng Fu, Jenny Widahl, and Christoffer Petersson. Zenseact open dataset: large-scale and diverse multimodal dataset In Proceedings of the IEEE/CVF for autonomous driving. International Conference on Computer Vision, pages 20178 20188, 2023. 2 [2] Amir Bar, Gaoyue Zhou, Danny Tran, Trevor Darrell, and Yann LeCun. Navigation world models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1579115801, 2025. 2 [3] Holger Caesar, Varun Bankiti, Alex Lang, Sourabh Vora, Venice Erin Liong, Qiang Xu, Anush Krishnan, Yu Pan, Giancarlo Baldan, and Oscar Beijbom. nuscenes: multiIn Proceedings of modal dataset for autonomous driving. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1162111631, 2020. 2, 6 [4] Wei Cao, Marcel Hallgarten, Tianyu Li, Daniel Dauner, Xunjiang Gu, Caojun Wang, Yakov Miron, Marco Aiello, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, and Kashyap Chitta. PseudoarXiv, 2506.04218, simulation for autonomous driving. 2025. 2, 6 [5] Li Chen, Penghao Wu, Kashyap Chitta, Bernhard Jaeger, Andreas Geiger, and Hongyang Li. End-to-end autonomous driving: Challenges and frontiers. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 1 [6] Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Vadv2: End-to-end vectorized autonomous driving via probabilistic planning. arXiv preprint arXiv:2402.13243, 2024. 2, [7] Xuesong Chen, Linjiang Huang, Tao Ma, Rongyao Fang, Shaoshuai Shi, and Hongsheng Li. Solve: Synergy of language-vision and end-to-end networks for autonomous driving. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1206812077, 2025. 3 [8] Zhili Chen, Maosheng Ye, Shuangjie Xu, Tongyi Cao, and Qifeng Chen. Ppad: Iterative interactions of prediction and In European planning for end-to-end autonomous driving. Conference on Computer Vision, pages 239256. Springer, 2024. 3 [9] Zesong Chen, Ze Yu, Jun Li, Linlin You, and Xiaojun Tan. Dualat: Dual attention transformer for end-to-end autonomous driving. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 1635316359. IEEE, 2024. 2 [10] Kashyap Chitta, Aditya Prakash, Bernhard Jaeger, Zehao Yu, Katrin Renz, and Andreas Geiger. Transfuser: Imitation with transformer-based sensor fusion for autonomous driving. IEEE transactions on pattern analysis and machine intelligence, 45(11):1287812895, 2022. 1, 2, 5, 7 icons. https://www.flaticon.com/free-icons/formula-1. 8 [11] Darius Dan. In Flaticon. Formula 1 [12] Daniel Dauner, Marcel Hallgarten, Tianyu Li, Xinshuo Weng, Zhiyu Huang, Zetong Yang, Hongyang Li, Igor Gilitschenski, Boris Ivanovic, Marco Pavone, Andreas Geiger, and Kashyap Chitta. Navsim: Data-driven nonreactive autonomous vehicle simulation and benchmarking. In Advances in Neural Information Processing Systems (NeurIPS), 2024. 2, 6 [13] Renju Feng, Ning Xi, Duanfeng Chu, Rukang Wang, Zejian Deng, Anzheng Wang, Liping Lu, Jinxiang Wang, and Yanjun Huang. Artemis: Autoregressive end-to-end trajectory planning with mixture of experts for autonomous driving. arXiv preprint arXiv:2504.19580, 2025. 2 [14] Yuchao Feng and Yuxiang Sun. Polarpoint-bev: Bird-eyeview perception in polar points for explainable end-to-end autonomous driving. IEEE Transactions on Intelligent Vehicles, 2024. 3 [15] Felix Fent, Fabian Kuttenreich, Florian Ruch, Farija Rizwin, Stefan Juergens, Lorenz Lechermann, Christian Nissler, Andrea Perl, Ulrich Voll, Min Yan, et al. Man truckscenes: multimodal dataset for autonomous trucking in diverse conditions. Advances in Neural Information Processing Systems, 37:6206262082, 2024. 2 [16] Hao Gao, Shaoyu Chen, Bo Jiang, Bencheng Liao, Yiang Shi, Xiaoyang Guo, Yuechuan Pu, Haoran Yin, Xiangyu Li, Xinbang Zhang, et al. Rad: Training an end-to-end driving policy via large-scale 3dgs-based reinforcement learning. arXiv preprint arXiv:2502.13144, 2025. [17] Barry Gilhuly, Armin Sadeghi, Peyman Yedmellat, Kasra InRezaee, and Stephen Smith. Looking for trouble: formative planning for safe trajectories with occlusions. In 2022 International Conference on Robotics and Automation (ICRA), pages 89858991. IEEE, 2022. 8 [18] Hariprasath Govindarajan, Maciej Wozniak, Marvin Klingner, Camille Maurice, Ravi Kiran, and Senthil Yogamani. Cleverdistiller: Simple and spatially consistent crossmodal distillation. arXiv preprint arXiv:2503.09878, 2025. 8 [19] Ke Guo, Haochen Liu, Xiaojun Wu, Jia Pan, and Chen Lv. ipad: Iterative proposal-centric end-to-end autonomous driving. arXiv preprint arXiv:2505.15111, 2025. 3 [20] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. 2016. 3 [21] Deepti Hegde, Rajeev Yasarla, Hong Cai, Shizhong Han, Apratim Bhattacharyya, Shweta Mahajan, Litian Liu, Risheek Garrepalli, Vishal Patel, and Fatih Porikli. Distilling multi-modal large language models for autonomous driving. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. 9 [22] Georg Hess, Carl Lindstrom, Maryam Fatemi, Christoffer Petersson, and Lennart Svensson. Splatad: Real-time lidar and camera rendering with 3d gaussian splatting for auIn Proceedings of the Computer Vision tonomous driving. and Pattern Recognition Conference, pages 1198211992, 2025. 2 [23] Shengchao Hu, Li Chen, Penghao Wu, Hongyang Li, Junchi Yan, and Dacheng Tao. St-p3: End-to-end vision-based autonomous driving via spatial-temporal feature learning. In European Conference on Computer Vision (ECCV), 2022. 8 [24] Yihan Hu, Jiazhi Yang, Li Chen, Keyu Li, Chonghao Sima, Xizhou Zhu, Siqi Chai, Senyao Du, Tianwei Lin, Wenhai Wang, et al. Planning-oriented autonomous driving. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1785317862, 2023. 2, 3, 7, 8 [25] Boris Ivanovic, Cristiano Saltori, Yurong You, Yan Wang, Wenjie Luo, and Marco Pavone. Efficient multi-camera tokenization with triplanes for end-to-end driving. arXiv preprint arXiv:2506.12251, 2025. 3 [26] Anqing Jiang, Yu Gao, Zhigang Sun, Yiru Wang, Jijun Wang, Jinghao Chai, Qian Cao, Yuweng Heng, Hao Jiang, Zongzheng Zhang, et al. Diffvla: Vision-language guided diffusion planning for autonomous driving. arXiv preprint arXiv:2505.19381, 2025. 3 [27] Bo Jiang, Shaoyu Chen, Qing Xu, Bencheng Liao, Jiajie Chen, Helong Zhou, Qian Zhang, Wenyu Liu, Chang Huang, and Xinggang Wang. Vad: Vectorized scene representation for efficient autonomous driving. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 83408350, 2023. 2, 3, 5, [28] Xuefeng Jiang, Yuan Ma, Pengxiang Li, Leimeng Xu, Xin Wen, Kun Zhan, Zhongpu Xia, Peng Jia, XianPeng Lang, and Sheng Sun. Transdiffuser: End-to-end trajectory generation with decorrelated multi-modal representation for autonomous driving. arXiv preprint arXiv:2505.09315, 2025. 1, 2, 3 [29] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 2 [30] Kailin Li, Zhenxin Li, Shiyi Lan, Jiayi Liu, Yuan Xie, Zuxuan Wu, Zhiding Yu, Jose Alvarez, et al. Hydramdp++: Advancing end-to-end driving via hydra-distillation with expert-guided decision analysis. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (Workshops), 2025. 3, 7 [31] Zhenxin Li, Kailin Li, Shihao Wang, Shiyi Lan, Zhiding Yu, Yishen Ji, Zhiqi Li, Ziyue Zhu, Jan Kautz, Zuxuan Wu, et al. Hydra-mdp: End-to-end multimodal planning with multitarget hydra-distillation. arXiv preprint arXiv:2406.06978, 2024. 1, 2, 7 [32] Zhenxin Li, Wenhao Yao, Zi Wang, Xinglong Sun, Joshua Chen, Nadine Chang, Maying Shen, Zuxuan Wu, Shiyi Lan, and Jose Alvarez. Generalized trajectory scorarXiv preprint ing for end-to-end multimodal planning. arXiv:2506.06664, 2025. 3 [33] Ming Liang, Bin Yang, Wenyuan Zeng, Yun Chen, Rui Hu, Sergio Casas, and Raquel Urtasun. Pnpnet: End-to-end perception and prediction with tracking in the loop. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1155311562, 2020. [34] Bencheng Liao, Shaoyu Chen, Haoran Yin, Bo Jiang, Cheng Wang, Sixu Yan, Xinbang Zhang, Xiangyu Li, Ying Zhang, Qian Zhang, et al. Diffusiondrive: Truncated diffusion model In Proceedings of the for end-to-end autonomous driving. Computer Vision and Pattern Recognition Conference, pages 1203712047, 2025. 1, 3, 4, 5, 6, 7, 8, 2 [35] Haisong Liu, Yang Chen, Haiguang Wang, Zetong Yang, Tianyu Li, Jia Zeng, Li Chen, Hongyang Li, and Limin Wang. Fully sparse 3d occupancy prediction, 2024. 8 [36] Youquan Liu, Lingdong Kong, Jun Cen, Runnan Chen, Wenwei Zhang, Liang Pan, Kai Chen, and Ziwei Liu. Segment any point cloud sequences by distilling vision foundation models. Advances in Neural Information Processing Systems, 36, 2024. 8 [37] Wenjie Luo, Bin Yang, and Raquel Urtasun. Fast and furious: Real time end-to-end 3d detection, tracking and motion forecasting with single convolutional net. In Proceedings of the IEEE conference on Computer Vision and Pattern Recognition, pages 35693577, 2018. 1 [38] Dominic Maggio, Hyungtae Lim, and Luca Carlone. Vggtslam: Dense rgb slam optimized on the sl (4) manifold. arXiv preprint arXiv:2505.12549, 2025. 2 [39] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2 [40] Truls Nyberg, Christian Pek, Laura Dal Col, Christoffer Noren, and Jana Tumova. Risk-aware motion planning for autonomous vehicles with safety specifications. In 2021 ieee intelligent vehicles symposium (iv), pages 10161023. IEEE, 2021. [41] Pranjal Paul, Anant Garg, Tushar Choudhary, Arun Kumar Singh, and Madhava Krishna. Lego-drive: Languageenhanced goal-oriented closed-loop end-to-end autonomous driving. In 2024 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 1002010026. IEEE, 2024. 3 [42] Jonah Philion and Sanja Fidler. Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XIV 16, pages 194210. Springer, 2020. 1 [43] Zhijie Qiao, Haowei Li, Zhong Cao, and Henry Liu. Lightemma: Lightweight end-to-end multimodal model for arXiv preprint arXiv:2505.00284, autonomous driving. 2025. 3 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 4 10 In 2024 IEEE Interend-to-end approaches competitive? national Conference on Robotics and Automation (ICRA), pages 1842818435. IEEE, 2024. 2 [56] Yunzhi Yan, Haotong Lin, Chenxu Zhou, Weijie Wang, Haiyang Sun, Kun Zhan, Xianpeng Lang, Xiaowei Zhou, and Sida Peng. Street gaussians: Modeling dynamic urban scenes with gaussian splatting. In European Conference on Computer Vision, pages 156173. Springer, 2024. [57] Wenhao Yao, Zhenxin Li, Shiyi Lan, Zi Wang, Xinglong Sun, Jose Alvarez, and Zuxuan Wu. Drivesuprim: Towards precise trajectory selection for end-to-end planning. arXiv preprint arXiv:2506.06659, 2025. 3, 1 [58] Rajeev Yasarla, Shizhong Han, Hsin-Pai Cheng, Litian Liu, Shweta Mahajan, Apratim Bhattacharyya, Yunxiao Shi, Risheek Garrepalli, Hong Cai, and Fatih Porikli. Roca: Robust cross-domain end-to-end autonomous driving. arXiv preprint arXiv:2506.10145, 2025. 3 [59] Chengran Yuan, Zhanqi Zhang, Jiawei Sun, Shuo Sun, Zefan Huang, Christina Dao Wen Lee, Dongen Li, Yuhang Han, Anthony Wong, Keng Peng Tee, et al. Drama: An efficient end-to-end motion planner for autonomous driving with mamba. arXiv preprint arXiv:2408.03601, 2024. 7 [60] Rui Zhao, Yuze Fan, Ziguo Chen, Fei Gao, and Zhenhai Gao. Diffe2e: Rethinking end-to-end driving with hybrid action diffusion and supervised policy. arXiv preprint arXiv:2505.19516, 2025. 2 [61] Wenzhao Zheng, Junjie Wu, Yao Zheng, Sicheng Zuo, Zixun Xie, Longchao Yang, Yong Pan, Zhihui Hao, Peng Jia, Xianpeng Lang, et al. Gaussianad: Gaussian-centric end-toend autonomous driving. arXiv preprint arXiv:2412.10371, 2024. 3 [62] Yinan Zheng, Ruiming Liang, Kexin ZHENG, Jinliang Zheng, Liyuan Mao, Jianxiong Li, Weihao Gu, Rui Ai, Shengbo Eben Li, Xianyuan Zhan, and Jingjing Liu. Diffusion-based planning for autonomous driving with flexible guidance. In Proceedings of the International Conference on Learning Representations, 2025. 3 [45] Abbas Sadat, Sergio Casas, Mengye Ren, Xinyu Wu, Pranaab Dhawan, and Raquel Urtasun. Perceive, predict, and plan: Safe motion planning through interpretable semantic representations. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXIII 16, pages 414430. Springer, 2020. [46] Pei Sun, Henrik Kretzschmar, Xerxes Dotiwalla, Aurelien Chouard, Vijaysai Patnaik, Paul Tsui, James Guo, Yin Zhou, Yuning Chai, Benjamin Caine, et al. Scalability in perception for autonomous driving: Waymo open dataset. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 24462454, 2020. 2 [47] Wenchao Sun, Xuewu Lin, Yining Shi, Chuang Zhang, HaoSparsedrive: End-to-end auran Wu, and Sifa Zheng. tonomous driving via sparse scene representation. Proceedings of the IEEE International Conference on Robotics and Automation, 2025. 3, 8, 1 [48] Adam Tonderski, Carl Lindstrom, Georg Hess, William Ljungbergh, Lennart Svensson, and Christoffer Petersson. Neurad: Neural rendering for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1489514904, 2024. 2 [49] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. 2 [50] Xinshuo Weng, Boris Ivanovic, Yan Wang, Yue Wang, and Marco Pavone. Para-drive: Parallelized architecture for realtime autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1544915458, 2024. 7 [51] Maciej Wozniak, Hariprasath Govindarajan, Marvin Klingner, Camille Maurice, Ravi Kiran, and Senthil Yogamani. S3pt: Scene semantics and structure guided clustering to boost self-supervised pre-training for autonomous driving. In 2025 IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), pages 16601670. IEEE, 2025. 2 [52] Maciej Wozniak, Viktor Karefjard, Marko Thiel, and Patric Jensfelt. Toward robust sensor fusion step for 3d object detection on corrupted data. IEEE Robotics and automation letters, 8(11):70187025, 2023. [53] Zebin Xing, Xingyu Zhang, Yang Hu, Bo Jiang, Tong He, Qian Zhang, Xiaoxiao Long, and Wei Yin. Goalflow: Goaldriven flow matching for multimodal trajectories generation In Proceedings of the in end-to-end autonomous driving. Computer Vision and Pattern Recognition Conference, pages 16021611, 2025. 1, 3, 5, 7 [54] Xiang Xu, Lingdong Kong, Hui Shuai, Wenwei Zhang, Liang Pan, Kai Chen, Ziwei Liu, and Qingshan Liu. 4d contrastive superflows are dense 3d representation learners. arXiv preprint arXiv:2407.06190, 2024. 8 [55] Yihong Xu, Loıck Chambon, Eloi Zablocki, Mickael Chen, Alexandre Alahi, Matthieu Cord, and Patrick Perez. Towards motion forecasting with real-world perception inputs: Are"
        },
        {
            "title": "Supplementary Materials",
            "content": "A. Parameters setup Table S1 presents the complete set of hyperparameters used for the PRIX model. We separated backbone configuration, fusion transformer decoder, detection and planning heads, and associated loss weights. The configuration reflects dual-modality ResNet backbone, multi-head attention components, and task-specific head settings for trajectory prediction and segmentation. Table S1. Hyperparameter Configuration for PRIX Model Category Hyperparameter Value Backbone Configuration"
        },
        {
            "title": "Image Architecture\nShared CaRT Dimension\nNumber of CaRT SA Layers\nNumber of Attention Heads",
            "content": "resnet34 512 2 4 Heads Configuration (Detection & Planning) Number of Bounding Boxes Segmentation Feature Channels Segmentation Number of Classes Trajectory 30 64 7 (x,y,yaw) General Dropout Rate Learning rate Loss Weights Trajectory Weight Agent Classification Weight Agent Box Regression Weight Semantic Segmentation Weight 0.1 1e-4 10.0 10.0 1.0 10.0 B. Training setup We train our models on high-performance cluster equipped with eight NVIDIA A100 40GB GPUs. We use NVIDIA 3090 for FPS benchmarks as previous papers [10, 34]. We train everything from scratch, except the ResNets which we initizalize from weights available on HuggingFace1. On Navsim-v1 we trained our model for 100 epochs. On Navsim-v2, we follow recommended training by the Navsim-v2 challenge2 and [57]. For nuScenes we follow Sparsedrive approach [47] and train first on stage 1 (for 100 epochs) and use the weights obtained from stage 1 to fine tune on stage 2 (for 10 epochs). For optimization, we employed the AdamW optimizer with weight decay of 1e-3. The learning rate was man1https://huggingface.co/timm/resnet34.a1_in1k 2https://opendrivelab.com/challenge2025/ 1 aged by MultiStepLR scheduler. We also implemented parameter-wise learning rate configuration, where the learning rate for the image encoder was set to 0.5 that of the rest of the model to facilitate stable fine-tuning of the pretrained backbone. B.1. Task heads simple architecture incorporates"
        },
        {
            "title": "Our model",
            "content": "and lightweight heads for auxiliary tasks. This was deliberate design choice, prioritizing computational efficiency and speed. Initially, we explored more complex, heavier heads, such as deeper feed-forward networks for detection and more elaborate convolutional blocks and large Unet for segmentation. While these heavier heads yielded marginal performance gains of 1-2% of end-to-end planning task, they substantially increased the models parameter count and computational load, leading to significant drop in inference speed. Given that our goal is fast and efficient system, we opted for the simpler, more efficient head designs described below, as they provide the best balance between accuracy and operational performance. Object Detection Head The object detection head is responsible for predicting the state of dynamic agents (cars, pedestrians, etc.) from set of learned object queries. It consists of two parallel feed-forward networks (FFNs) that process each query embedding. The first FFN regresses the 2D bounding box parameters, including the center coordinates, dimensions, and heading angle. To ensure predictions are within plausible range, the networks outputs for the center point and heading are passed through hyperbolic tangent (tanh) activation function before being scaled to appropriate physical units. The second FFN predicts single logit per query, representing the classification score, which indicates the confidence that the query corresponds to valid agent. This dual-pathway design allows the model to simultaneously determine an objects location and its existence from single query feature vector. Segmentation Head The segmentation head is tasked with producing dense semantic map of the scene from It operates on the feature map top-down perspective. from our visual backbone. The head is lightweight convolutional module, starting with 3x3 convolution to refine the spatial features. This is followed by 1x1 convolution which acts as pixel-wise classifier, projecting the feature maps channels to dimensionality equal to the number of semantic classes. Each channel in the resulting output tensor represents the logit map for specific class (e.g., road, lane, vehicle). Finally, bilinear upsampling layer resizes the output to target resolution, facilitating loss computation against the ground truth map. C. Additional experiments C.1. DiffusionDrive Reported on nuscenes We and other researchers were not able to reproduce results reported by DiffusionDrive on nuScenes3. In Tab. S2 we included reported results while in the main paper we shown the results that were obtained by us (and others). We still outperform their reported results. Full comparison on Navsim-v1 As we can see on Tab. S3 we are performing almost as good as DiffusionDrive [34] on average (-0.4 PDMS) and outperforming them on half of the metrics. C.2. Larger Backbone Based on our analysis in Tab. S4, we chose the ResNet34 backbone for its optimal balance of performance and speed. While using larger ResNet50 backbone yields marginal performance gain (87.8 to 88.0 PDMS), it comes at significant speed cost (66.2 to 48.0 FPS). Moreover, the even larger ResNet101 backbone actually degrades performance to 87.5 PDMS while being substantially slower. Therefore, ResNet34 provides the best trade-off, delivering high performance without compromising real-time processing capabilities. D. Intuition behind the speed/performance Initial Architecture The baseline Context-aware Recalibration Transformer (CaRT) architecture consists of transformer module applied across multiple Resnet34 feature scales. The original implementation employed standard multi-head self-attention with separate query, key, and value projections, LayerNorm normalization, and ReLU-based MLP blocks. Each ResNet stage feature map is processed through adaptive pooling to (8 32) spatial dimensions, projected to shared embedding space, processed by the CaRT module, and then projected back to stage-specific dimensions before residual addition. Architectural Optimizations for Speed and Efficiency To enhance throughput and reduce computational overhead, we introduced several key optimizations to the baseline architecture, resulting in significantly faster model. These improvements focus on modernizing the transformer blocks and optimizing data flow. The primary enhancements are: 1. Fused QKV Projection: In the self-attention mechanism, the separate linear layers for query (Q), key (K), and value (V ) were replaced with single, fused linear 3https : / / github . com / hustvl / DiffusionDrive / issues/57 as well as issues/45 layer that computes all three projections in one operation. This reduces three separate matrix multiplications into one larger one, improving GPU utilization and decreasing memory access overhead by minimizing kernel launch latency. 2. Optimized MLP Block: The standard MLP block, which can be inefficient, was replaced by dedicated MLP module. We also substituted the ReLU activation with GELU, smoother activation function that is common in modern high-performance transformers and can lead to better convergence. 3. Efficient Tensor Reshaping: Throughout the model, especially in the attention mechanism and the CaRT modules forward pass, tensor reshaping operations like .reshape() are now preceded by .contiguous(). This ensures the tensor is stored in contiguous block of memory before the view operation, preventing potential performance penalties associated with manipulating non-contiguous tensors. 4. Gradient Checkpointing: We introduced optional gradient checkpointing within the transformer blocks. During training, this technique trades small amount of re-computation in the backward pass for significant reduction in memory usage, allowing for larger batch sizes which can further improve training throughput. 5. In-place and Fused Operations: Smaller optimizations were made throughout the backbone, such as using inplace=True for ReLU activations in the FPN and removing biases from convolution and linear layers where they are followed by normalization layer, which makes them redundant. Together, these structural and operational improvements result in more streamlined and performant backbone that is functionally equivalent to the baseline but executes significantly faster on modern hardware. E. Qualitative results To visually the performance of our model, we present series of qualitative results from diverse driving scenarios in Figures S2-S15. In these figures, the predicted trajectory is shown in red, while the ground truth human-driven path is in green. The results demonstrate that our model consistently generates highly accurate and feasible trajectories that closely align with the ground truth across variety of common maneuvers. For instance, the model accurately handles standard left and right turns (Figure S4, S5), complex lane curvatures (Figure S4), and straight-line driving (S3), showcasing strong understanding of both vehicle dynamics and 2 Table S2. Performance comparison of different driving models for nuScenes. The up arrow () indicates that lower values are better. Best results are in bold, and second best are underlined. Method Input Backbone DiffusionDrive [34] Camera ResNet-50 PRIX (ours) Camera ResNet-50 L2 (m) 3s 2s 0.54 0.53 0.90 0.93 1s 0.27 0. Avg. 0.57 0.57 Collision Rate (%) 1s 2s 3s Avg. FPS 0.03 0.00 0.05 0.04 0.16 0.18 0.08 0. 8.2 11.2 Table S3. Detail performance comparison of different driving models for Navsim-v1. The up arrow () indicates that higher values are better. Best results are in bold, and second best are underlined. C&L refer to Camera and LiDAR input. Method Input DiffusionDrive [34] C&L PRIX (ours) Camera Backbone NC DAC TTC Comf. EP 82.2 Resnet34 82.3 Resnet34 98.2 98. 94.7 94.1 96.2 96.3 100 100 PDMS 88.1 87.8 Table S4. Backbone Comparison on Navsim-v1 Model Backbone PDMS Params FPS PRIX (baseline) ResNet34 ResNet50 PRIX-50 ResNet101 PRIX-101 87.8 88.0 87.5 37M 39M 58M 57.0 47.3 28. road geometry. Even in cluttered, less-structured environments like the multi-lane pickup area in Figure S7, the prediction remains robust and precise. Critically, our model shows the ability to generate plans that are not just accurate but often safer and smoother than the ground truth data as on figure S8 where we keep further on the left than the ground truth, keeping safer distance from the vehicle in the front. 3 Figure S1. Left turn at the intersection (token a589b9ccbe3e5d1c) Figure S2. Visualization of initial noised anchor trajectories and final trajectories (bold red is the one with the highest confidence, bold dark blue is the 2nd highest confidence (token a589b9ccbe3e5d1c). Figure S3. Going straight on the busy road Figure S4. Right turn toekn (bfe607710d0158f9) Figure S5. Left turn (token 8cec7d21f7dc540b) 5 Figure S6. Visualization of initial noised anchor trajectories and final trajectories (bold red is the one with the highest confidence, bold dark blue is the 2nd highest confidence (token 8cec7d21f7dc540b). Figure S7. Left turn on the intersection token cb0c6c918c4d541c. 6 Figure S8. Going straight, our model predicts better trajectory than gt, keeping larger distance to the left from the other car Figure S9. Busy street/traffic jam where our model decides not to drive since there are cars on both sides (token i3a8a4e7b9e0f53ad) Figure S10. Left turn at the busy intersection. 7 Figure S11. Visualization of initial noised anchor trajectories and final trajectories (bold red is the one with the highest confidence, bold dark blue is the 2nd highest confidence. Figure S12. Visualization of initial noised anchor trajectories and final trajectories (bold red is the one with the highest confidence, bold dark blue is the 2nd highest confidence. Going straight. 8 Figure S13. Visualization of initial noised anchor trajectories and final trajectories (bold red is the one with the highest confidence, bold dark blue is the 2nd highest confidence. Right turn. Figure S14. Right turn. 9 Figure S15. Visualization of initial noised anchor trajectories and final trajectories (bold red is the one with the highest confidence, bold dark blue is the 2nd highest confidence. Right turn."
        }
    ],
    "affiliations": [
        "KTH Royal Institute of Technology, Sweden",
        "Scania CV AB"
    ]
}