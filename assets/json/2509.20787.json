{
    "paper_title": "Real-Time Object Detection Meets DINOv3",
    "authors": [
        "Shihua Huang",
        "Yongjie Hou",
        "Longfei Liu",
        "Xuanlong Yu",
        "Xi Shen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Benefiting from the simplicity and effectiveness of Dense O2O and MAL, DEIM has become the mainstream training framework for real-time DETRs, significantly outperforming the YOLO series. In this work, we extend it with DINOv3 features, resulting in DEIMv2. DEIMv2 spans eight model sizes from X to Atto, covering GPU, edge, and mobile deployment. For the X, L, M, and S variants, we adopt DINOv3-pretrained or distilled backbones and introduce a Spatial Tuning Adapter (STA), which efficiently converts DINOv3's single-scale output into multi-scale features and complements strong semantics with fine-grained details to enhance detection. For ultra-lightweight models (Nano, Pico, Femto, and Atto), we employ HGNetv2 with depth and width pruning to meet strict resource budgets. Together with a simplified decoder and an upgraded Dense O2O, this unified design enables DEIMv2 to achieve a superior performance-cost trade-off across diverse scenarios, establishing new state-of-the-art results. Notably, our largest model, DEIMv2-X, achieves 57.8 AP with only 50.3 million parameters, surpassing prior X-scale models that require over 60 million parameters for just 56.5 AP. On the compact side, DEIMv2-S is the first sub-10 million model (9.71 million) to exceed the 50 AP milestone on COCO, reaching 50.9 AP. Even the ultra-lightweight DEIMv2-Pico, with just 1.5 million parameters, delivers 38.5 AP, matching YOLOv10-Nano (2.3 million) with around 50 percent fewer parameters. Our code and pre-trained models are available at https://github.com/Intellindust-AI-Lab/DEIMv2"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 2 ] . [ 2 7 8 7 0 2 . 9 0 5 2 : r Real-Time Object Detection Meets DINOv Shihua Huang1, Yongjie Hou1, 2, Longfei Liu1, Xuanlong Yu1, Xi Shen1 1 Intellindust AI Lab; 2Xiamen University Equal Contribution; Corresponding author. Project Page: https://intellindust-ai-lab.github.io/projects/DEIMv2 Code & Weights: https://github.com/Intellindust-AI-Lab/DEIMv2 (a) COCO performance v.s. Number of Parameters. (b) COCO performance v.s. FLOPs. Figure 1. Compared with state-of-the-art real-time object detectors on COCO [12], all variants of our proposed DEIMv2 (S, M, L, X) achieve superior performance in terms of average precision (AP) while maintaining similar parameters and less computational cost."
        },
        {
            "title": "Abstract",
            "content": "Benefiting from the simplicity and effectiveness of Dense O2O and MAL, DEIM has become the mainstream training framework for real-time DETRs, significantly outperforming the YOLO series. In this work, we extend it with DINOv3 features, resulting in DEIMv2. DEIMv2 spans eight model sizes from to Atto, covering GPU, edge, and mobile deployment. For the X, L, M, and variants, we adopt DINOv3-pretrained / distilled backbones and introduce Spatial Tuning Adapter (STA), which efficiently converts DINOv3s single-scale output into multi-scale features and complements strong semantics with fine-grained details to enhance detection. For ultra-lightweight models (Nano, Pico, Femto, and Atto), we employ HGNetv2 with depth and width pruning to meet strict resource budgets. Together with simplified decoder and an upgraded Dense O2O, this unified design enables DEIMv2 to achieve superior performancecost trade-off across diverse scenarios, establishing new state-of-the-art results. Notably, our largest model, DEIMv2-X, achieves 57.8 AP with only 50.3M parameters, surpassing prior X-scale models that require over 60M parameters for just 56.5 AP. On the compact side, DEIMv2-S is the first sub-10M model (9.71M) to exceed the 50 AP milestone on COCO, reaching 50.9 AP. Even the ultra-lightweight DEIMv2-Pico, with just 1.5M parameters, delivers 38.5 APmatching YOLOv10-Nano (2.3M) with 50% fewer parameters. 1. Introduction Real-time object detection [6, 18, 22, 29] is critical component in many practical applications, including autonomous driving [11], robotics [16], and industrial defect detection [8]. Achieving good balance between detection performance and computational efficiency remains key challenge, especially for lightweight models suitable for edge and mobile devices. Among mainstream real-time object detectors, DETRbased approaches have attracted increasing attention for Table 1. Architectural details and main results of DEIMv2 variants. We report the configurations for different variants and the final Average Precision (AP) on the COCO benchmark. Variant Backbone Model Adapter S Nano Pico Femto Atto ViT-S+ ViT-S ViT-T+ ViT-T HGv2-B0 HGv2-P HGv2-F HGv2-A STA STA STA STA - - - - Hidden Dimension dEnc. dBack. dDec. Layers #Back. #Dec. #Scales #Query #Params GFLOPs Latency AP 384 384 256 192 1024 512 256 256 256 256 256 128 112 96 64 256 256 256 192 128 112 96 64 12 12 12 12 5 4 3 3 6 4 4 3 3 3 3 3 3 3 3 2 2 2 2 300 300 300 300 300 200 150 100 50.26 32.18 18.11 9. 3.57 1.51 0.96 0.49 151.6 96.32 52.20 25.62 6.86 5.15 1.67 0.76 13.75 10.47 8.80 5.78 2.32 2.14 1.91 1.61 57.8 56.0 53.0 50. 43.0 38.5 31.0 23.8 their end-to-end design and the high capacity enabled by Transformers, achieving more favorable trade-off. Within this paradigm, DEIM has emerged as strong training framework that has advanced real-time DETRs and delivered leading models in the field. Meanwhile, DINOv3 [21] has demonstrated strong feature representation capabilities across variety of vision tasks. However, their potential for real-time object detection has not been fully explored. In this work, we introduce DEIMv2, real-time object detector built upon our previous DEIM [7] pipeline and enhanced with DINOv3 [21] features. DEIMv2 employs official DINOv3-pretrained backbones (ViT-Small and ViTSmall+) for its largest variants (L and sizes) to maximize feature richness, while its and variants leverage ViT-Tiny and ViT-Tiny+ backbones distilled from DINOv3, carefully balancing performance and efficiency. To address ultra-lightweight scenarios, we further introduce four specialized variants: Nano, Pico, Femto, and Atto, extending DEIMv2s scalability across wide spectrum of computational budgets. To better leverage the strong feature representations of DINOv3, pretrained on large-scale data, under real-time constraints, we design the Spatial Tuning Adapter (STA). Operating in parallel with DINOv3, STA efficiently converts its single-scale outputs into the multi-scale features required for object detection in parameter-free manner. Simultaneously, it performs fast downsampling of the input image to provide fine-grained, multi-scale detail features with very small receptive fields, complementing DINOv3s strong semantics. We further simplify the decoder by drawing on advances from the Transformer community. Specifically, we replace the conventional FFN and LayerNorm with SwishFFN [20] and RMSNorm [27], both of which have been shown to be efficient without significantly affecting performance. We additionally note that object query locations change minimally during iterative refinement, motivating sharing query position embeddings across all decoder layers. Beyond this, we enhance Dense O2O by introducing object-level CopyBlend augmentation, which increases effective supervision and further improves model performance. Extensive experiments on COCO [12] demonstrate that DEIMv2 achieves state-of-the-art performance across multiple model scales, which can be seen in Figure 1. Despite its simplicity, the family of DEIMv2 exhibits strong performance. For instance, our largest variant, DEIMv2X, achieves 57.6 AP on COCO with only 50.3M parameters, surpassing prior best X-scale detectors DEIM-X that requires over 60M parameters yet attains only 56.5 AP. At the smaller end, DEIMv2-S establishes notable milestone as the first model with fewer than 10M parameters to exceed 50 AP, highlighting the effectiveness of our design at compact scales. Furthermore, our ultra-lightweight DEIMv2Pico, with merely 1.5M parameters, attains 38.5 AP, matching the performance of YOLOv10-Nano (2.3M parameters) while reducing parameter count by 50%, thereby redefining the efficiencyaccuracy frontier at the extreme lightweight regime. Our work highlights how DINOv3 [21] features can be effectively adapted for real-time object detection and provides versatile framework spanning ultra-lightweight to high-performance models. To our knowledge, this is the first work in real-time object detection to simultaneously address such wide range of deployment scenarios. The main contributions of this work are summarized as follows: We present DEIMv2, which offers eight model sizes covering GPU, edge, and mobile deployment. For larger models, we leverage DINOv3 for strong semantic features and introduce the STA to efficiently integrate them into real-time object detection. For ultra-lightweight models, we leverage expert knowledge to effectively prune the depth and width of HGNetv2-B0, meeting strict computational constraints. Beyond backbone, we further simplify the decoder and upgrade Dense O2O, pushing the performance boundaries even further. Finally, we demonstrate on COCO that DEIMv2 outperFigure 2. Backbone design of our ViT-based variants. We integrate DINOv3 with the proposed Spatial Tuning Adapter (STA). forms existing state-of-the-art methods across all resource settings, establishing new SOTA results. 2. Method Overall architecture. Our overall architecture follows the design of RT-DETR [14], comprising backbone, hybrid encoder, and decoder. As shown in Table 1, for the mainstream variants X, L, M, and S, the backbone is based on DINOv3 with our proposed Spatial Tuning Adapter (STA), while the remaining variants use HGNetv2 [1]. Multi-scale features from the backbone are first processed by the encoder to produce initial detection results and select the top-K candidate bounding boxes. The decoder iteratively refines these candidates to generate the final predictions. ViT-based variants. For the larger DEIMv2 variants (S, M, L, X), we carefully design the backbones around the Vision Transformer [3] (ViT) family, balancing model capacity with efficiency. For and X, we leverage two public DINOv3 models [21]: ViT-Small and ViT-Small+, which provide strong semantic representations with 12 layers and 384-dimensional hidden size. For the lighter and variants, we distill compact backbones, ViT-Tiny and ViTTiny+, directly from ViT-Small DINOv3, preserving the 12layer depth while reducing the hidden dimensions to 192 and 256. This design delivers smooth scaling path across X, ensuring that each variant maintains competitive accuracy while adapting to different efficiency requirements. HGNetv2-based variants. HGNetv2 [1], developed by the Baidu PaddlePaddle team, is widely used in realtime DETR frameworks for its efficiencyfor example, D-FINE [17] adopts the full HGNetv2 series as its backIn our ultra-lightweight DEIMv2 models (Nano, bone. Pico, Femto, and Atto), we also build on HGNetv2-B0, but progressively prune its depth and width to meet different parameter budgets. Specifically, the Pico backbone removes the fourth stage of B0, keeping outputs only up to 1/16 scale. Femto further reduces the number of blocks in Picos last stage from two to one. Atto goes step further by shrinking the channels of that last block from 512 to 256. Spatial Tuning Adapter. To better adapt DINOv3 features for real-time object detection, we propose the Spatial Tuning Adapter (STA), as illustrated in Fig. 2. STA is fully convolutional network that integrates an ultralightweight feedforward network for extracting fine-grained multi-scale details, together with Bi-Fusion operator that further strengthens feature representations from DINOv3. DINOv3 is based on ViT backbone, which naturally produces single-scale (1/16) dense features. In object detection, however, objects vary widely in size, and multiscale features are one of the most effective ways to improve performance. To this end, ViTDet [10] introduced the Feature2Pyramid module, which generates multi-scale features from the final ViT output using deconvolution. In contrast, our STA is even simpler: we directly resize the 1/16-scale features from several ViT blocks (e.g., the 5th, 8th, and 11th) into multiple scales via parameter-free bilinear interpolation. These multi-scale features are further enhanced by the Bi-Fusion operator consisting of 1 1 convolution with an ultra-lightweight CNN designed to extract fine-grained details and complement DINOv3s output features. This design achieves an excellent trade-off between efficiency and accuracy, making it well-suited for real-time detection. Efficient Decoder. We enhance the standard deformable attention decoder [31] by incorporating several efficiencyoriented techniques widely adopted in the Transformer Table 2. Detailed training hyperparameters in DEIMv2. Back. denotes the Backbone. We use Local Loss to denote the Fine-Grained Localization (FGL) Loss and the Decoupled Distillation Focal (DDF) Loss in D-FINE [17]. HyperParams Resolution Weigt Decay Base LR Min LR Back. LR Back. MinLR Local Loss Epochs Mosaicprob Mosaicepochs MixUpprob MixUpepochs CopyBprob CopyBepoch 640 1.25e-4 5e-4 2.5e-4 1e-6 5e-7 50 0.5 [4, 29] 0.5 [4, 29] 0.5 [4, 50] 640 1.25e-4 5e-4 2.5e-4 1.25e-5 6.25e-6 60 0.5 [4, 34] 0.5 [4, 34] 0.5 [4, 60] 640 1e-4 5e-4 2.5e-4 2.5e-5 1.25e-5 0.5 [4, 49] 0.5 [4, 49] 0.5 [4, 90] 640 1e-4 5e-4 2.5e-4 2.5e-5 1.25e-5 120 0.5 [4, 64] 0.5 [4, 64] 0.5 [4, 120] Nano 640 1e-4 8e-4 8e-4 4e-4 4e-4 148 0.5 [4, 78] 0.5 [4, 78] 0.4 [4, 78] Pico 640 1e-4 1.6e-3 8e-4 8e-4 4eFemto Atto 416 1e-4 1.6e-3 8e-4 8e-4 4e-4 320 1e-4 2e-3 1e-3 1e-3 5e468 468 468 0.5 [4, 250] 0.0 [ ] 0.0 [ ] 0.5 [4, 250] 0.0 [ ] 0.0 [ ] 0.3 [4, 250] 0.0 [ ] 0.0 [ ] community, achieving favorable performancecost tradeSpecifically, we integrate SwiGLUFFN [20] off. to strengthen nonlinear representation capacity, RMSNorm [27] to stabilize and accelerate training efficiently. Noting that object query locations undergo minimal changes during iterative refinement, we further propose sharing single position embedding across all decoder layers, eliminating redundant computation. Enhanced Dense O2O. In our previous DEIM [7], we proposed Dense O2O, which increases the number of objects per training image to provide stronger supervision, improving convergence and performance. Its effectiveness was initially demonstrated using image-level augmentations such as Mosaic and MixUp [28]. In DEIMv2, we further explore Dense O2O at the object level with Copy-Blend, which adds new objects without their backgrounds. Unlike Copy-Paste [4], which fully overwrites the target region, Copy-Blend blends new objects with the image, better suiting our scenario and consistently improving performance. Training setting and loss. Our training strategy follows DEIM [7], fundamental framework designed for fast convergence and high performance. The overall optimization objective is weighted sum of five components: Matchability-Aware Loss (MAL) [7], Fine-Grained Localization (FGL) Loss [17], Decoupled Distillation Focal (DDF) Loss [17], BBox Loss (L1), and GIoU Loss [19]. The total loss is defined as: Ltotal =λ1Lmal + λ2Lf gl + λ3Lddf + λ4Lbbox + λ5Lgiou (1) with weights set to λ1 = 1.0, λ2 = 0.15, λ3 = 1.5, λ4 = 5, and λ5 = 2 across all experiments. We summarize the training hyperparameters in Table 2, covering input resolution, learning rate, training epochs, and Dense O2O settings. An interesting observation is that applying FGL and DDF losses to ultra-lightweight models degrades performance. We attribute this to their limited capacity and inherently weaker baseline accuracy, which reduces the effectiveness of self-distillation. Consequently, we exclude these two components (i.e., the local loss) from training the Pico, Femto, and Atto variants. 3. Experiments Comparison to state-of-the-art real-time object detectors. Table 3 summarizes the performance of DEIMv2 across the S, M, L, and variants, demonstrating substantial improvements over prior state-of-the-art detectors. For instance, the largest variant, DEIMv2-X, attains 57.8 AP with only 50M parameters and 151 GFLOPs, surpassing the previous best DEIM-X (56.5 AP with 62M parameters and 202 GFLOPs). This demonstrates that DEIMv2 can deliver superior accuracy with both fewer parameters and lower computational cost. At the smaller end, DEIMv2-S sets new milestone as the first model with fewer than 10M parameters to exceed the 50 AP threshold on COCO, achieving 50.9 AP with only 11M parameters and 26 GFLOPs. This marks clear improvement over the prior DEIM-S (49.0 AP with 10M parameters), while requiring nearly the same model size. While CNN-based backbones are generally more hardware-friendly, our ViTbased backbone achieves lightweight design with fewer Table 3. Comparison with real-time object detectors on COCO [12] val2017, sorted by parameter size. Model #Epochs #Params. GFLOPs Latency (ms) APval APval 50 APval 75 APval APval APval YOLOv10-S [23] YOLOv9-S [25] YOLOv12-S-turbo [22] YOLO11-S [6] D-FINE-S [17] DEIM-S [7] YOLOv8-S [5] DEIMv2-S YOLOv10-M [23] D-FINE-M [17] DEIM-M [7] RT-DETRv2-S [14] YOLOv12-M-turbo [22] YOLO11-M [6] YOLOv9-M [25] Gold-YOLO-S [24] DEIMv2-M YOLOv10-L [23] YOLO11-L [6] YOLOv9-C [25] YOLOv8-M [5] YOLOv12-L-turbo [22] YOLOv10-X [23] D-FINE-L [17] DEIM-L [7] RT-DETRv2-M [14] Gold-YOLO-M [24] RT-DETRv2-L [14] YOLOv8-L [5] DEIMv2-L YOLOv9-E [25] YOLO11-X [6] YOLOv12-X-turbo [22] D-FINE-X [17] DEIM-X [7] YOLOv8-X [5] Gold-YOLO-L [24] RT-DETRv2-X [14] DEIMv2-X 500 500 600 500 120 120 500 120 500 120 90 120 600 500 500 300 500 500 500 500 600 500 72 50 120 300 72 500 60 500 500 600 72 50 500 300 72 50 7 7 9 9 10 10 11 10 15 19 19 20 20 20 20 22 18 24 25 25 26 27 30 31 31 31 41 42 43 32 57 57 59 62 62 68 75 76 22 26 19 22 25 25 29 26 59 57 57 60 60 68 76 46 52 120 87 102 79 82 160 91 91 92 88 136 165 96 189 195 185 202 202 257 152 259 151 2.52 8.05 6.28 7.05 3.66 3.66 6.97 5.78 4.70 5.91 5.91 4.61 8.44 9.02 10.18 6.96 8. 7.38 10.41 10.76 9.46 10.45 10.47 8.15 8.15 6.91 9.21 9.29 12.20 10.47 20.52 15.53 15.79 12.90 12.90 15.89 12.31 13.88 13.75 46.3 46.8 47.5 46.6 48.5 49.0 44.9 50.9 51.1 52.3 52.7 48.1 52.6 51.2 51.4 46.4 53.0 53.2 53.4 53.0 50.2 54.0 54.4 54.0 54.7 49.9 51.1 53.4 52.9 56.0 55.6 54.7 55.7 55.8 56.5 53.9 53.3 54.3 57. 63.0 61.8 64.1 63.4 65.6 65.9 61.8 68.3 68.1 69.8 70.0 65.1 69.5 67.9 67.2 63.4 70.2 70.1 70.1 70.2 67.2 70.6 71.3 71.6 72.4 67.5 68.5 71.6 69.8 73.4 72.8 71.6 72.2 73.7 74.0 71.0 70.9 72.8 75.4 50.4 48.6 - 50.3 52.6 53.1 48.6 55.1 55.8 56.4 57.3 57.4 - 55.3 54.6 - 57. 58.1 58.2 57.8 54.6 - 59.3 58.4 59.4 58.6 - 57.4 57.5 60.9 60.6 59.5 - 60.2 61.5 58.7 - 58.8 63.2 26.8 25.7 - 28.7 29.1 30.4 25.7 31.4 33.8 33.2 35.3 36.1 - 33.0 32.0 25.3 34.2 35.8 35.6 36.2 32.0 - 37.0 36.5 36.9 35.8 32.3 36.1 35.3 37.5 40.2 37.7 - 37.3 38.8 35.7 33.8 35.8 39. 51.0 49.9 - 51.3 52.2 52.6 49.9 55.3 56.5 56.5 56.7 57.9 - 56.7 55.7 51.3 57.4 58.5 59.1 58.5 55.7 - 59.8 58.0 59.6 58.6 56.1 57.9 58.3 60.8 61.0 59.7 - 60.5 61.4 59.3 58.9 58.8 62.9 63.8 61.0 - 64.1 65.4 65.7 61.0 70.3 67.0 70.2 69.5 70.8 - 67.5 66.4 63.6 71. 69.4 69.2 69.3 66.4 - 70.9 71.9 71.8 72.1 68.6 70.8 69.8 75.2 71.4 70.2 - 73.4 74.2 70.7 69.9 72.1 75.9 parameters and lower FLOPs, offering better scalability and deployment flexibility. It is worth noting that the latency of the proposed methods has not been optimized. Techniques such as Flash Attention [2], as in Yolov12 [22], could further accelerate inference. Overall, the reduced FLOPs highlight the potential of ViT-based backbones to achieve lowlatency performance with proper optimization. Interestingly, when comparing DINOv3-based DEIMv2 models with their previous DEIM counterparts under comparable parameter and FLOP budgets, the accuracy gains primarily arise from improvements on medium and large objects, while performance on small objects remains largely unchanged. For instance, DEIMv2-S achieves 55.3 APM and 70.3 APL, clearly surpassing DEIM-S (52.6 APM and 65.7 APL), yet the small-object scores are nearly identical (31.4 vs. 30.4 APS). similar trend is observed for larger models: DEIMv2-X improves APM from 61.4 to 62.8 and APL from 74.2 to 75.9, while its small-object AP (39.2) reTable 4. Comparison with real-time object detectors on COCO [12] val2017 for ultra-light models. Model NanoDet-M[15] DEIMv2-Atto YOLOX-Nano[30] PicoDet-S[26] PP-YOLO-Tiny[13] PicoDet-ShufflenetV2 1x[26] DEIMv2-Femto YOLOv10-N[23] YOLOv8-N[5] YOLOv6-3.0-N[9] DEIMv2-Pico YOLOv12-N [22] DEIM-Nano [7] D-FINE-Nano [17] DEIMv2-Nano Input Size Params (M) FLOPs (G) COCO AP 416x416 320x320 416x416 416x416 416x416 416x416 416x416 640x640 640x640 640x640 640x640 640x640 640x640 640x640 640x 1.0 0.5 0.9 1.0 1.1 1.2 1.0 2.3 3.2 4.7 1.5 2.6 3.8 3.8 3.6 0.7 0.8 1.1 1.2 1.0 1.5 1. 6.7 8.7 11.4 5.2 6.5 7.2 7.2 6.9 23.5 23.8 25.8 30.7 22.7 30.0 31.0 38.5 37.4 37.0 38.5 40.6 43.0 42.8 43. mains close to that of DEIM-M (38.8). These results indicate that DEIMv2s main advantage lies in enhancing the representation and detection of medium-to-large objects, whereas small-object detection remains challenge across scales. This observation further confirms that DINOv3 excels at capturing strong global semantics but has limited ability to represent fine-grained details. Exploring ways to better integrate DINOv3 features into real-time detectors thus represents an interesting direction for future work. lightweight DEIMv2-Pico matches YOLOv10-N while using over 50% fewer parameters. Together, these results demonstrate that DEIMv2 is not only efficient but also highly scalable, offering unified framework that advances the accuracyefficiency frontier. This versatility makes DEIMv2 well-suited for deployment in diverse scenarios, ranging from resource-constrained edge devices to highperformance detection systems, paving the way for broader adoption of real-time detection in practical applications. Comparison to competitive ultra-light object detectors. The ultra-light variants of DEIMv2 also exhibit strong performance, as summarized in Table 4. DEIMv2-Atto, with only 0.49M parameters, achieves performance comparable to NanoDet-M despite its substantially smaller size. Similarly, DEIMv2-Pico attains performance on par with YOLOv10-N [23] while requiring less than half the parameters. These results underscore the effectiveness of DEIMv2 in extremely compact regimes and highlight its suitability for deployment on resource-constrained edge devices. 4. Conclusion In this report, we introduced DEIMv2, new generation of real-time object detectors that combines the strong semantic representations of DINOv3 with our lightweight STA. Through careful design and scaling, DEIMv2 achieves state-of-the-art performance across the full spectrum of model sizes. At the high end, DEIMv2-X delivers 57.8 AP with significantly fewer parameters than previous largescale detectors. At the compact end, DEIMv2-S is the first model of its size to surpass 50 AP, and the ultra-"
        },
        {
            "title": "References",
            "content": "[1] Cheng Cui, Ruoyu Guo, Yuning Du, Dongliang He, Fu Li, Zewu Wu, Qiwen Liu, Shilei Wen, Jizhou Huang, Xiaoguang Hu, et al. Beyond self-supervision: simple yet effective network distillation alternative to improve backbones. arXiv, 2021. 3 [2] Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memory-efficient exact attention with io-awareness. In NeurIPS, 2022. 5 [3] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2020. 3 [4] Golnaz Ghiasi, Yin Cui, Aravind Srinivas, Rui Qian, TsungYi Lin, Ekin Cubuk, Quoc Le, and Barret Zoph. Simple copy-paste is strong data augmentation method for instance segmentation. In CVPR, 2021. 4 [5] Jocher Glenn. Yolov8. https://docs.ultralytics. com/models/yolov8/, 2023. 5, 6 [6] Jocher Glenn. Yolo11. https://docs.ultralytics. com/models/yolo11/, 2024. 1, 5 [23] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, and Guiguang Ding. Yolov10: Real-time end-toend object detection. In NeurIPS, 2024. 5, 6 [24] Chengcheng Wang, Wei He, Ying Nie, Jianyuan Guo, Chuanjian Liu, Yunhe Wang, and Kai Han. Gold-yolo: Efficient object detector via gather-and-distribute mechanism. In NeurIPS, 2023. 5 [25] Chien-Yao Wang, I-Hau Yeh, and Hong-Yuan Mark Liao. to learn using proYolov9: Learning what you want grammable gradient information. arXiv, 2024. 5 [26] Guanghua Yu, Qinyao Chang, Wenyu Lv, Chang Xu, Cheng Cui, Wei Ji, Qingqing Dang, Kaipeng Deng, Guanzhong Wang, Yuning Du, et al. Pp-picodet: better real-time object detector on mobile devices. arXiv, 2021. [27] Biao Zhang and Rico Sennrich. Root mean square layer normalization. In NeurIPS, 2019. 2, 4 [28] Hongyi Zhang. mixup: Beyond empirical risk minimization. In ICLR, 2017. 4 [29] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, and Jie Chen. In CVPR, Detrs beat yolos on real-time object detection. 2024. [30] Ge Zheng, Liu Songtao, Wang Feng, Li Zeming, and Sun Jian. Yolox: Exceeding yolo series in 2021. arXiv, 2021. 6 [31] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. In ICLR, 2021. 3 [7] Shihua Huang, Zhichao Lu, Xiaodong Cun, Yongjun Yu, Xiao Zhou, and Xi Shen. Deim: Detr with improved matching for fast convergence. In CVPR, 2025. 2, 4, 5, 6 [8] Muhammad Hussain. Yolo-v1 to yolo-v8, the rise of yolo and its complementary nature toward digital manufacturing and industrial defect detection. 2023. 1 [9] Chuyi Li, Lulu Li, Yifei Geng, Hongliang Jiang, Meng Cheng, Bo Zhang, Zaidan Ke, Xiaoming Xu, and Xiangxiang Chu. Yolov6 v3.0: full-scale reloading. arXiv, 2023. 6 [10] Yanghao Li, Hanzi Mao, Ross Girshick, and Kaiming He. Exploring plain vision transformer backbones for object detection. In ECCV, 2022. 3 [11] Siyuan Liang, Hao Wu, Li Zhen, Qiaozhi Hua, Sahil Garg, Georges Kaddoum, Mohammad Mehedi Hassan, and Keping Yu. Edge yolo: Real-time intelligent object detection system based on edge-cloud cooperation in autonomous vehicles. IEEE Transactions on Intelligent Transportation Systems, 2022. [12] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 1, 2, 5, 6 [13] Xiang Long, Kaipeng Deng, Guanzhong Wang, Yang Zhang, Qingqing Dang, Yuan Gao, Hui Shen, Jianguo Ren, Shumin Han, Errui Ding, and Shilei Wen. Pp-yolo: An effective and efficient implementation of object detector. arXiv, 2020. 6 [14] Wenyu Lv, Yian Zhao, Qinyao Chang, Kui Huang, Guanzhong Wang, and Yi Liu. Rt-detrv2: Improved baseline with bag-of-freebies for real-time detection transformer. arXiv, 2024. 3, 5 [15] Rangi Lyu. Nanodet-plus. https://github.com/ RangiLyu / nanodet / releases / tag / v1 . 0 . 0 - alpha-1, 2021. Version 1.0.0-alpha-1. 6 [16] Debapriya Maji, Soyeb Nagori, Manu Mathew, and Deepak Poddar. Yolo-6d-pose: Enhancing yolo for single-stage monocular multi-object 6d pose estimation. In 3DV, 2024. 1 [17] Yansong Peng, Hebei Li, Peixi Wu, Yueyi Zhang, Xiaoyan Sun, and Feng Wu. D-fine: Redefine regression task in detrs as fine-grained distribution refinement. In ICLR, 2024. 3, 4, 5, [18] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In CVPR, 2016. 1 [19] Hamid Rezatofighi, Nathan Tsoi, JunYoung Gwak, Amir Sadeghian, Ian Reid, and Silvio Savarese. Generalized intersection over union: metric and loss for bounding box regression. In CVPR, 2019. 4 [20] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 2, 4 [21] Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv, 2025. 2, 3 [22] Yunjie Tian, Qixiang Ye, and David Doermann. Yolov12: In NeurIPS, Attention-centric real-time object detectors. 2025. 1, 5,"
        }
    ],
    "affiliations": [
        "Intellindust AI Lab",
        "Xiamen University"
    ]
}