{
    "paper_title": "From Grunts to Grammar: Emergent Language from Cooperative Foraging",
    "authors": [
        "Maytus Piriyajitakonkij",
        "Rujikorn Charakorn",
        "Weicheng Tao",
        "Wei Pan",
        "Mingfei Sun",
        "Cheston Tan",
        "Mengmi Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Early cavemen relied on gestures, vocalizations, and simple signals to coordinate, plan, avoid predators, and share resources. Today, humans collaborate using complex languages to achieve remarkable results. What drives this evolution in communication? How does language emerge, adapt, and become vital for teamwork? Understanding the origins of language remains a challenge. A leading hypothesis in linguistics and anthropology posits that language evolved to meet the ecological and social demands of early human cooperation. Language did not arise in isolation, but through shared survival goals. Inspired by this view, we investigate the emergence of language in multi-agent Foraging Games. These environments are designed to reflect the cognitive and ecological constraints believed to have influenced the evolution of communication. Agents operate in a shared grid world with only partial knowledge about other agents and the environment, and must coordinate to complete games like picking up high-value targets or executing temporally ordered actions. Using end-to-end deep reinforcement learning, agents learn both actions and communication strategies from scratch. We find that agents develop communication protocols with hallmark features of natural language: arbitrariness, interchangeability, displacement, cultural transmission, and compositionality. We quantify each property and analyze how different factors, such as population size and temporal dependencies, shape specific aspects of the emergent language. Our framework serves as a platform for studying how language can evolve from partial observability, temporal reasoning, and cooperative goals in embodied multi-agent settings. We will release all data, code, and models publicly."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 2 7 8 2 1 . 5 0 5 2 : r From Grunts to Grammar: Emergent Language from Cooperative Foraging Maytus Piriyajitakonkij1,2,3 Mingfei Sun3 Wei Pan3 Rujikorn Charakorn5 Cheston Tan1,4 Weicheng Tao2 Mengmi Zhang1,2,4 1Institute for Infocomm Research (I2R), ASTAR, Singapore 2College of Computing and Data Science, Nanyang Technological University, Singapore 3Department of Computer Science, The University of Manchester, United Kingdom 4Centre for Frontier AI Research (CFAR), ASTAR, Singapore 5Sakana AI, Japan Address correspondence to mengmi.zhang@ntu.edu.sg"
        },
        {
            "title": "Abstract",
            "content": "Early cavemen relied on gestures, vocalizations, and simple signals to coordinate, plan, avoid predators, and share resources. Today, humans collaborate using complex languages to achieve remarkable results. What drives this evolution in communication? How does language emerge, adapt, and become vital for teamwork? Understanding the origins of language remains challenge. leading hypothesis in linguistics and anthropology posits that language evolved to meet the ecological and social demands of early human cooperation. Language did not arise in isolation, but through shared survival goals. Inspired by this view, we investigate the emergence of language in multi-agent Foraging Games. These environments are designed to reflect the cognitive and ecological constraints believed to have influenced the evolution of communication. Agents operate in shared grid world with only partial knowledge about other agents and the environment, and must coordinate to complete games like picking up high-value targets or executing temporally ordered actions. Using end-to-end deep reinforcement learning, agents learn both actions and communication strategies from scratch. We find that agents develop communication protocols with hallmark features of natural language: arbitrariness, interchangeability, displacement, cultural transmission, and compositionality. We quantify each property and analyze how different factors, such as population size and temporal dependencies, shape specific aspects of the emergent language. Our framework serves as platform for studying how language can evolve from partial observability, temporal reasoning, and cooperative goals in embodied multi-agent settings. We will release all data, code, and models publicly."
        },
        {
            "title": "Introduction",
            "content": "Before the emergence of structured language, early humans relied on gestures, vocalizations, and simple signals to coordinate their actions in partially observable and socially interdependent world. leading hypothesis is that human language evolved not in isolation, but through cooperative interaction under partial observability [62, 65, 53, 12, 52]. Language is not merely an abstract code but tool shaped by social use and shared goals [70, 67]. Although direct evidence from these early stages is unavailable, multi-agent simulations offer way to study how language emerges from the need to coordinate with others [7, 34, 38, 37]. Prior work on emergent communication has primarily focused on referential games (RG), where speaker conveys task-relevant information to listener over limited communication channel [38, 39, Preprint. Under review. (a) Foraging Games (b) ScoreG (c) TemporalG Figure 1: An Overview of Foraging Games. (a) Two agents operate in 5 5 partially observable grid world. Each agent can observe 3 3 grid centered on itself. The two agents are required to pick up the goal items simultaneously. (b) ScoreG game: Two items are assigned random scores; each agent observes only one score, and both must pick up the higher-scoring item to succeed in an episode. This game is designed to encourage agents to communicate about items scores. (c) TemporalG game: Two items are assigned random spawn times; each agent observes only one item when it spawns. The agents must pick up the items in the same order as their spawn timesi.e., the earlier item must be picked up first. This game is designed to encourage agents to communicate about time. 29, 22]. These settings have advanced our understanding of symbol grounding and compositionality, but often impose simplifying assumptions: agents are limited to unidirectional communication [19], operate in fixed speaker-listener roles, and perform disembodied tasks with passive input processing and no active interaction with the environment. Even recent extensions to population-based learning [30, 9, 45] typically decouple communication from physical behavior. Such setups diverge from the ecologically grounded, socially interdependent, and embodied conditions under which human language likely evolved [65, 15]. To bridge this gap, we introduce Foraging Games (FG), multi-agent framework designed to reflect more realistic ecological and cognitive constraints that may have shaped early language [15, 65]. FG supports bidirectional communication. Agents are trained to jointly learn both physical action and communication, as early humans likely did during cooperative foraging. The environment enforces embodiment: agents must explore, observe, and act within dynamic and partially observable world that they can influence. Moreover, each agent interacts with population of diverse partners, facilitating the study of generalization to new partners, dialect formation, and cultural transmission [30, 59, 45]. Specifically, we formulate FG as partially observable grid world in which agents must complete multi-step games as shown in Figure 1. Success depends not only on taking effective actions, but also on communicating what they see and know. It includes two games: (1) collecting the most valuable item with partner, which encourages communication about items scores and locations, and (2) picking up items in specific order with partner, which requires communication about when items were seen. Each agent has deep neural network modules for perception, memory, and policies to translate internal representations into actions and messages. Agents communicate through discrete messages drawn from finite set of learnable vocabulary exchanged at each time step. They are trained independently using Proximal Policy Optimization (PPO) [61], with no shared modules, parameters, or gradients, reflecting the personalized and decentralized nature of each agent. Subsequently, agents achieve success rate (SR) above 95% across all games. Although two trained agents perform well when paired with each other, each fails when paired with copy of itself at test time, suggesting that an agent understands only its partners language, not the one it produces. We propose two solutions to this problem. The first is to train population with more than two agents. We hypothesize that developing shared language becomes the optimal strategy when agents must communicate with multiple partners. The second is to incorporate self-interaction during training [10, 11], motivated by the observation that humans can speak to themselves. These solutions encourage agents to comprehend their own messages, promoting convergence on shared common language and reflecting the property of interchangeability. Since human language develops in populations, we further explore language properties in groups of agents connected via fully connected and ring-structured social networks. We decode task-relevant information, such as item positions, from the agents messages using logistic regression, verifying that the messages are meaningful rather than random. Above-chance decoding accuracy indicates that language has emerged to communicate item properties. Finally, we show that agents can develop implicit communication [68, 21, 16], i.e., gaining information by observing partners behavior, when they are unable to send messages. Our key contributions are as follows: 1. We introduce Foraging Games, framework for studying emergent communication in physically grounded, socially interdependent environments. FG provides ecological and cognitive constraints that early humans might have faced, including embodiment, behavior coordination, partial observability, bidirectional communication, and temporal reasoning. 2. We introduce hybrid cross-and-self-play training regime that encourages interchangeable language and supports cultural transmission: agents that are closer in the training population develop more similar languages than those farther apart. While agents need not converge on single shared language, reflecting the arbitrariness of language, their messages remain mutually intelligible within their social context. As an ablation, we observe that without this regime, e.g., in purely decentralized cross-play training, two trained agents often develop non-interchangeable languages and even fail to understand their own messages when roles are swapped at test time. 3. We design series of tasks in FG that elicit both temporal and spatial displacement in emergent language. Agents develop time adverbials [44] that refer to when past events occurred, along with messages indicating the location of those events. Rigorous analysis of messages from trained agents across various social networks enables systematic and quantitative evaluation of compositionality and other key attributes characteristic of emergent communication."
        },
        {
            "title": "2 Related Work",
            "content": "Emergent communication (EC) provides insight into how human language may have evolved [38, 37, 34, 9, 6, 5, 3, 56, 36], as well as how to develop more effective representations [49, 8, 74, 14, 72]. Early work in this field relied on evolutionary computation [6, 5] or Bayesian modeling [53, 33] to simulate language emergence and its underlying dynamics. With the advent of deep reinforcement learning (DRL) [46, 47], more powerful and flexible computational framework has emerged for studying EC in realistic and complex settings [18]. Modern EC studies apply DRL in Lewis-style referential games [39], where speaker must convey task-relevant messages to listener through limited-capacity communication channel. The games objective typically involves either target identification or input reconstruction [74, 22, 43, 29]. Since language is inherently social construct developed by communities rather than individuals, recent work has shifted toward examining the properties of language in populations of speakers and listeners [30, 9, 59, 45]. The resulting frameworks often resemble variational autoencoders (VAEs) [32, 23, 66], where language acts as compressed latent code, rather than flexible medium for coordination in the world. Unlike most prior setups, our agents are bidirectional communicators [17, 64, 20, 35, 51] and embodied in the environment [27, 55, 50]. This setup better reflects real-world communication, where agents must both produce and interpret signals to collaboratively complete task effectively. Crucially, communication and sequential decision-making co-evolve: agents learn what to say as they learn how to act [48, 28]. They must actively explore, observe, remember, and adapt to their partners behavior to succeed, resulting in dynamic, context-dependent coordination. The closest work to ours is [48], which also studies emergent communication in physically interactive setting. However, their agents do not coordinate actions to achieve shared goal, and all agents share single deep neural network. Moreover, they rely on differentiable environment that allows gradients to flow directly from the environment to the neural network. In contrast, our setting is fully decentralized and reward-driven, where communication and coordination emerge solely from reward, thus emulating the constraints and limitations faced by early humans. We position our work as an initial step toward studying language emergence within novel embodied framework, one in which language is not an isolated code but survival tool. Within FG, agents must continually integrate perception, memory, and interaction to align their actions, mirroring the ecological and social constraints believed to have shaped human language."
        },
        {
            "title": "3 Foraging Games (FG)\nForaging-style games are used as benchmarks for multi-agent reinforcement learning (MARL)\nalgorithms [1, 54, 71, 26]. However, existing environments often assume full observability and are\nnot designed to support the study of emergent communication in humans. We develop our FG, in",
            "content": "3 (a) Message Exchange (b) Architecture Overview Figure 2: graphical overview of our method. (a) Agents exchange messages at every time step. (b) The neural architecture of single agent. On the input side, the received integer message is mapped to real-valued vector via learnable lookup table and passed through the message encoder. The grid observation and agent position are processed by the grid encoder and position encoder, respectively. The outputs of all three encoders are concatenated and passed to an LSTM module, which maintains temporal memory. On the output side, the message head, action head, and value head produce the next message token, environment action, and estimated state value (See Sec. 4.1). which decentralized agents share common goal and must navigate to and simultaneously pick up the goal items to earn equal rewards. Our FG is designed to encourage agents to communicate about what they observed. Each agent is provided with only partial knowledge of the environment (e.g., observing only one item). Agents must reach consensus based on their perspectives by communicating the message from fixed-size dictionary with learnable embeddings and choosing appropriate actions, where the action space is {move left, move right, move up, move down, and pick up}. FG has two games: ScoreG and TemporalG. The former is used to investigate four properties of emergent language: interchangeability (agents can understand the messages they produce), arbitrariness (symbols acquire meaning through social agreement rather than inherent structure), compositionality (messages are built from reusable parts reflecting task semantics), and cultural transmission (language patterns are passed across agents), under varying population sizes and training regimes. The latter focuses on displacement, aiming to determine whether agents messages encode temporal and spatial information about seen items in the past. is set in 2D grid world with the size of 5 5, containing Game 1: Pickup High Score (ScoreG) 2 items, 2 agents, and wall, as shown in Figure 1. The wall refers to the area outside the 5 5 grid. Each grid cell can be occupied by either an agent or an item, but not both simultaneously. Agents cannot move to the wall, the occupied grid cell, or the same grid cell. Each agent receives an observation o(t) = (x(t), p(t)), where x(t) R332 (3 is the receptive field size and 2 refers to two channels). The first channel of the receptive field x(t) represents an occupancy map: value of zero indicates that grid cell contains neither an item nor wall. The second channel shows the items score. The receptive field of each agent has its center at the agent and observes the agents surroundings, delineated as red box in Figure 1. To simulate incomplete knowledge, each agent observes the score of one item during an episode. The agents absolute position in the environment is given by p(t) R2. The goal item is defined as the item with the highest score. An episode is deemed successful if both agents navigate to and simultaneously pick up the goal item. In this case, they receive shared positive reward of +1; otherwise, they receive shared negative reward of 1. Additionally, similar to [69], successful episodes grant both agents bonus reward rb based on their efficiency: rb = TmaxTtotal , where Tmax = 10 is the maximum number of steps allowed per episode, and Ttotal is the total number of steps taken by the agents. The bonus is to encourage the agents to complete the game as soon as possible. To prevent agents from overfitting to specific scores in the ScoreG task while still evaluating in-distribution generalization, we use held-out set of test scores that do not overlap with those seen during training. Specifically, training scores are randomly sampled from the set {5, 10, 15, . . . , 250}, while test scores are sampled from the disjoint set {2, 4, 6, 8, 12, . . . , 248}. This setup ensures that test scores lie within the same distributional range as the training scores, while remaining unseen during training. To isolate the role of explicit communication, agents are made invisible to one another by default, i.e., grid cell occupied by another agent appears empty. This design prevents any form of implicit communication through visual cues [68, 21, 42, 41]. Tmax 4 Game 2: Pickup Temporal Order (TemporalG) Two agents must cooperatively pick up two items by first navigating to and simultaneously collecting the first spawned item, followed by the second. The pickup must occur in the same order as the items appear. The environment follows the same configuration as ScoreG, except the agents observation x(t) excludes the score channel, and the episode length is limited to Tmax = 20. Initially, agents are spawned on opposite sides of the grid and remain frozen at time step = 1 to = 6. Subsequently, each item appears at distinct time step, drawn uniformly from the fixed duration set {1, 2, 3, . . . , 6}. Each item is guaranteed to appear within at least one agents receptive field, ensuring it is visible when it spawns. Agents begin moving and attempting to pick up the items only after = 6. To promote both spatial and temporal displacement, we restrict the communication range so that agents can only successfully send messages if they occupy adjacent grid cells. Otherwise, zero-value message is sent. Reward is given in the same way as in ScoreG."
        },
        {
            "title": "4 Experimental Setting",
            "content": "4.1 Embodied Social Agents This section provides an overview of how agents interact with their partners and the environment, and how they are trained. formal description is also given in Appendix A. As shown in Figure 2b, each agent in our environment learns both to produce actions and to send discrete messages through policy trained with PPO [61]. We denote the overall policy of an agent as ψ = (π, ϕ), where π is the action policy that selects an environment action, and ϕ is the communication policy that selects discrete message m(t) to send. We formulate communication as sequence of discrete messages. At each time step t, an agent is only allowed to send one discrete message, where each message is represented by single integer m(t) {0, 1, 2, 3}. This integer indexes into lookup table of 4 learnable embeddings (each of dimension 16) in the receiving agent. Each agent maintains its own message lookup table, which is not shared. In other words, upon receiving message m(t), the agent retrieves the corresponding message embedding from its own lookup table. All agents share the same neural network architecture but are initialized with independent random parameters. At each time step, the agent receives three inputs: partial grid observation x(t), its own position p(t), and the message m(t1) from its partner sent in the previous time step (see Figure 2a). Each input is encoded separately, using multi-layer perceptron for the grid, linear layer for the position, and multilayer perceptron for the message. The encoded features are concatenated and passed into long short-term memory network (LSTM) [24], which maintains temporal information in working memory and outputs hidden state. This hidden state is used to produce three outputs: distribution over actions, distribution over messages, and scalar value estimate used for computing the PPO advantage [60]. Both the action and the message are sampled from their respective probability distributions predicted by the agent. The message is used to index into the receiver agents lookup table, retrieving the corresponding embedding, which is then used by the communication policy ϕ of the receiver agent to produce the message output for the current time step t. 4.2 Training Regimes We train each agent independently using standard single-agent PPO, with no shared parameters or centralized critic. Each agent treats its partner as part of the environment and learns solely from its own experience. Communication policies emerge through interaction and reward, without explicit supervision. This decentralized setup is intentional: it allows agents to specialize, diverge, and adapt to others, enabling us to study how communication protocols evolve under varying population sizes, agent heterogeneity, and connectivity patterns among agents. To investigate how communication strategies emerge and generalize in populations, we explore two training regimes that differ in how agents interact with other partners. These regimes allow us to study the effects of population diversity, message alignment, and exposure to self-produced messages on communication development. Cross-Play Training (XP): We define the agent population size as Npop. In each training episode, pair of agents is randomly sampled from the population to play together. Cross-Play-and-Self-Play Training (XP+SP): Humans can speak to themselves and understand their own language. Motivated by this aspect of human learning, in each episode, we randomly select either two distinct agents (Cross-Play, XP) or the same agent twice (Self-Play, SP). Specifically, for each episode, we randomly sample agents and such that either = or = j, where i, {1, . . . , Npop}. 5 4.3 Social Network Structure We investigate two social network structures, as illustrated in Figure 4a and Figure 4b: fully connected (FC) and ring-structured (Ring) networks. In the fully connected setup, each agent interacts with all other agents during training, representing standard approach in multi-agent systems. In contrast, the ring-structured population limits each agent to interacting only with its immediate neighbors, forming circular network. This structure is chosen because it provides an extreme scenario where agents have the minimal number of neighbors, yet still maintain connectivity between all agents through indirect paths. This setup enables us to examine how agents generalize to unseen partners and how communication protocols develop in more constrained, sparsely connected environment. 4.4 Evaluation Metrics on Language Properties To evaluate linguistic properties in emergent communication, we use two standard metrics, Topographic Similarity [4, 37] and Language Similarity [30, 59, 45]. We also introduce new metric, Interchangeability below. Formal definitions of these metrics are provided in Appendix D. Topographic Similarity (topsim) measures the structural alignment between message space and semantic space. It is computed as the Spearman correlation between pairwise distances in the message space and the corresponding distances in the semantic space, which reflects the agents observations and context. higher topsim indicates more compositional and consistent mapping between semantic meanings and messages. We define the semantic space using ground-truth attributes of the environment, specifically the scores and positions of items in the ScoreG game of FG. To implement the topographic similarity metric, we use the EGG toolkit [29]. Language Similarity (LS) measures how similarly two agents communicate in the same situation. It compares the sequences of discrete messages each agent produces and computes the average agreement between them. higher score means the agents tend to use the same messages in similar contexts, indicating stronger convergence in their communication strategies. Interchangeability (IC) is metric we propose to measure whether an agent can understand the language it produces. We define Self-SR as the success rate when an agent plays with copy of itself, and Cross-SR as the success rate when paired with different agent. The overall success rate across both settings is denoted as SR. IC is the ratio of Cross-SR to Self-SR. high IC indicates that agents can better generalize and interpret their own language. 4.5 Implementation Details All training can be performed on single GeForce RTX 4080 (See Appendix Figure S4). We use the ADAM optimizer [31] with an initial learning rate of 0.00025, which linearly decays over time. We report the hyperparameters of the architecture and training algorithm in Appendix G. To examine emergent communication properties, we collect each agents messages over all time steps in every episode. For the ScoreG task, messages are concatenated into single message chain per agent per episode. In the TemporalG task, agents are only allowed to communicate when they occupy adjacent grid cells. Thus, we begin collecting messages only after their first adjacency and concatenate all subsequent messages. To decode environmental attributes, such as item scores, spawn times, and positions, from message chains, we use one-vs-rest logistic regression (LR). For example, to decode four possible item locations, we train four binary classifiers, each distinguishing one location from the rest. During inference, the class with the highest predicted probability is selected. Each LR model is trained on 3,500 message chains and tested on 1,500, using 3-fold cross-validation across 3 random seeds. We report message chain decoding accuracy as the average across all agents. Other metrics, including LS, IC, topsim, and SR, are computed using 1,000 test episodes."
        },
        {
            "title": "5 Results\nWe investigate the emergence of linguistic properties in the FG framework and present our findings\nin this section. Each experimental setting is defined by four factors:\nthe game type, agent\npopulation size, social structure, and training regime. These factors influence the emergence\nof different language properties, making exhaustive analysis infeasible.\nInstead, we report\nresults from a representative subset of settings. For clarity, we adopt the naming convention\nfor each experimental setting: [game]-[population size]-[social structure]-[training\nregime]. For example, ScoreG-P15-Ring-XP+SP refers to a setting where Npop = 15 agents in a\nring-structured network are trained with XP+SP regime on the ScoreG game. This convention is used\nthroughout our analysis.",
            "content": "6 Table 1: Cross-play training can cause non-interchangeable languages. Game performance and language similarity (LS; Sec. 4.4) comparison across different training regimes. Npop is the population size. Cross-SR and Self-SR are the mean ( standard deviation) success rates when agents play with others or with copies of themselves, respectively. Training Npop LS Cross-SR Self-SR XP XP+SP XP 2 2 3 0.215 0.002 0.598 0.010 0.527 0.036 0.987 0.002 0.968 0.005 0.977 0.007 0.065 0.054 0.968 0.002 0.944 0.018 (a) topsim across population sizes (b) IC across population sizes Figure 3: The effect of population training with fully-connected social networks on language properties. (a) Topographic Similarity (topsim), (b) Interchangeability (IC), and (c) Language Similarity (LS) as function of population sizes are shown. See Sec. 4.4 for metrics details. (c) LS across population sizes Increasing population size and self-play training support interchangeable language [ScoreG-FC]. As shown in Table 1, XP agents trained exclusively with each other fail to understand their own messages when paired with copy of themselves during test time as indicated by drop of 6% in Self-SR. Surprisingly, when agents are trained with Npop = 3, they maintain high Self-SR even without explicit self-play, suggesting that population training encourages the emergence of interchangeable language. These agents also develop more consistent language, as indicated by higher LS of 0.53 compared to the XP alone. Furthermore, XP+SP agents achieve high Self-SR due to their direct exposure to SP. Interestingly, they also exhibit greater language similarity across agents, with an LS of 0.59, outperforming XP agents even in population settings. We also provide theoretical analysis to explain why training with more partners and training with SP can lead to shared language in Appendix B. Population size affects compositionality [ScoreG-FC]. As shown in Figure 3a, population size has an effect on topsim. For XP agents, topsim increases from population size 2 and saturates from size 6. In contrast, XP+SP agents show more consistent upward trend in topsim as population size increases, plateauing around size 1215. Interestingly, XP+SP agents trained in population of size 2 also achieve the highest topsim across all population sizes. However, we caution against the interpretation of effective language from such small population because this language could be overfitting to fixed partner and may not generalize well to more diverse partners. Linguistic and cognitive studies suggest that larger communities tend to produce more compositional languages [58, 57]. While population training promotes consistency, we observe that compositionality does not increase monotonically, aligned with previous works [30, 59, 45]. Population size affects language similarity and interchangeability [ScoreG-FC]. Figure 3b shows that IC quickly saturates to nearly 1.0 for XP agents once the population size exceeds 2. This indicates that agents can understand their own messages. For XP+SP agents, IC reaches the maximum level across Npop 2. This indicates that SP encourages language interchangeability. As shown in Figure 3c, for XP agents, LS increases sharply from size 2 to 3, then gradually plateaus, indicating that training with multiple partners encourages the emergence of shared protocol. XP+SP agents, in contrast, show consistently high LS across all population sizes. This suggests that self-play helps promote consistent, shared communication protocols within population. Together, these results suggest that both self-play and population diversity contribute to the emergence of interchangeable and shared communication. Unlike our findings, prior work [30] reported that larger population sizes reduce language similarity, as listeners adapt to multiple speaker-specific languages. However, this discrepancy may stem from their unidirectional communication setup and centralized agent training, resulting in different optimal solutions than ours. (c) LS across circular distance (a) Fully-Connected (b) Ring-Structured Figure 4: Comparison of population training (Npop = 15) with self-play (XP+SP) and without self-play (XP) under ring-structured social networks: (a,b) are social network structures described in Sec. 4.3 (b,c) We plot LS and SR (See Sec. 4.4) as functions of circular distances between two agents in ring-structured network. distance of 1 indicates that two agents were co-trained, while distance greater than 1 indicates that the agents were never paired during training. (d) SR across circular distance Self-play supports interchangeable and generalizable language in ring-structured populations [ScoreG-P15-Ring]. We investigate how language propagates through structured population by training agents in ring topology which is the simplest non-fully connected social network [30, 45]. Specifically, we set the population size to Npop = 15 and train each agent to interact only with its two neighbors. This setup enables us to examine how culture gets transmitted across indirect connections within the population at the test time. In Figure 4, for XP+SP agents, both LS and SR gradually decline as the distance between agents increases. This indicates that the learned language generalizes to indirectly connected agents, though with reduced SR. In contrast, for agents with XP alone, language becomes non-interchangeable across the population. For example, an agent may produce messages more similar to those of non-neighbor than to its direct partner at the test time. This asymmetry implies that agents learn to understand their neighbors but fail to produce messages aligned with the language they understand (Figure S2c). Together, these results suggest that self-play not only improves within-agent consistency but also supports more stable and transmissible communication protocols across structured populations. Spatial and temporal displacement emerge in agent communication [P3-FC-XP]. Humans can refer to past or future eventsfor example, saw an apple yesterday or will collect an apple tomorrow. They can also refer to spatially distant objects, such as an apple in the forest. This ability to refer to things beyond the immediate here and now is known as displacement in natural language. We investigate whether displacement can emerge in artificial foraging agents using the TemporalG game (section 3). XP agents with Npop = 3 achieve strong performance in this setting, with Cross-SR of 0.975 and Self-SR of 0.962 (Table S1). Next, we decode spatial and temporal displacement information from the messages. The items can take on one of 6 possible spawn times, 4 vertical positions (excluding the grids center), and 5 possible horizontal positions. The decoding accuracy for the spawn time and position of items is above chance (Figure 5b), suggesting that messages function as temporal adverbials and spatial references. We also decode items scores and positions from XP agents (Npop = 3) in the ScoreG game (Figure 5a). The items can take on one of 10 possible score ranges, 2 possible vertical positions (top and bottom), and 5 possible horizontal positions. The results align with those observed in the TemporalG game. Moreover, compared to Integer Msg, decoding with chains of message embeddings yields higher accuracy, suggesting that these embeddings encode more meaningful and linearly separable spatial and temporal features. Implicit communication can emerge when explicit message communication is disabled [ScoreG-P3-FC-XP]. We study whether agents can convey information through their sequence of actions in an episode when the explicit communication channel is disabled. We conduct an ablation on two variables: partner visibility and the presence of explicit verbal communication. When partner is invisible, their location appears as an unoccupied cell in the observing agents receptive field. We train XP agents in Inv-NoCom and Vis-NoCom, where agents either have or lack partner visibility, but no explicit message communication is allowed. If an agent observes an extreme score (e.g., 222 or 22), it can confidently infer whether the item is goal or not. To isolate this confounding factor, we evaluate agents only under the condition that both partners observe high scores. Inv-Com agents (XP agents trained in our default FG environments) achieve SR of 85%. Without communication, Inv-NoCom agents perform below chance (40%), likely because they fail to coordinate goal pickup without seeing each other. Interestingly, Vis-NoCom agents achieve an SR of 60%, outperforming 8 (a) ScoreG (b) TemporalG (a) Success Rate (b) Avg Ep Length Figure 6: Decoding item states from messages. V-Pos/H-Pos are item positions, Score is item value, and Time is item spawn time. Integer Msg uses raw message chains composed of integer sequences. Msg Embedding uses chained embeddings mapped from lookup table. Error bars show standard deviations. Implicit communication in the Figure 7: ScoreG games. Inv/Vis denote partner visibility; Com/NoCom indicate presence or absence of verbal communication. Item scores are from [160, 162, . . . , 240]. We report average successful episode length; error bars show standard deviations across seeds and agents. Inv-NoCom. This indicates that both Inv-Com and Vis-NoCom agents can glean information from their partnerseither via explicit messages or by observing partners actionsconsistent with findings in [21, 68]. We also evaluate the average length of successful test episodes. Both Inv-Com and Vis-NoCom agents take around 6 steps, while Inv-NoCom agents take about 5 steps. This indicates that agents with more accurate communication require extra steps to send meaningful signals, increasing their communication bandwidth and hence, longer episode length."
        },
        {
            "title": "6 Discussion",
            "content": "We investigate the language emergence in multi-agent Foraging Games. Through systematic and quantifiable experiments on computational models in simulated environments, we gain valuable insights into key questions in evolutionary linguisticssuch as what drives the evolution of language and how it becomes essential for teamwork. Remarkably, without any direct supervision from human languages, agents with limited capacity trained in cooperative settings develop five hallmark properties of human language [25]. Interchangeability emerges through self-play training or social interactions within larger populations  (Table 1)  . Arbitrariness is demonstrated by two cross-play agents developing non-interchangeable languages that are nonetheless mutually intelligible enough to enable successful cooperation and high task success rate (Table 1 and Figure S3a). Compositionality is indicated by high topsim scores in larger populations, common metric measuring the structural alignment between messages and environmental semantics (Figure 3a). Displacement is demonstrated by high decoding accuracy of items positions, scores, and spawn times, showing that agents can refer to when and where currently invisible events happen (Figure 5a). Cultural transmission is evidenced by the gradual degradation of language similarity and success rate across increasing distance in ring-structured populations, despite no direct training between distant agent pairs. (Figure 4, Figure S1d, and Figure S2d). This suggests that culture is transmitted and shaped through interaction within structured social networks. Furthermore, our framework demonstrates that communication can emerge in both implicit and explicit forms, which standard referential games do not support. Understanding the origins of language is profound and long-standing challenge that requires insights from linguistics, psychology, neuroscience, and artificial intelligence. Our work represents an initial step toward addressing this complex question. Nonetheless, we recognize several limitations in our current approach. While the training method may mirror aspects of human communication learning, it may not scale effectively to complex environments such as visual games or robotic control tasks. Bridging this gap may require loosening some of the constraints currently imposed, for example, solving more complex tasks may require agents to share gradients or parameters to stabilize the training. Additionally, our framework does not incorporate turn-taking dynamics, which is another challenge that the current methods struggle with. Finally, we evaluate compositionality using finite set of symbolic inputs. Future work could explore compositional representations that emerge from compressing high-dimensional sensory data into compact latent variables."
        },
        {
            "title": "References",
            "content": "[1] Stefano Albrecht and Subramanian Ramamoorthy. game-theoretic model and best-response learning method for ad hoc coordination in multiagent systems. In Proceedings of the 2013 international conference on Autonomous agents and multi-agent systems, pages 11551156, 2013. [2] Daniel Bernstein, Robert Givan, Neil Immerman, and Shlomo Zilberstein. The complexity of decentralized control of markov decision processes. Mathematics of operations research, 27(4):819840, 2002. [3] Brendon Boldt and David Mortensen. review of the applications of deep learning-based emergent communication. Transactions on Machine Learning Research, 2024. [4] Henry Brighton and Simon Kirby. Understanding linguistic evolution by visualizing the emergence of topographic mappings. Artificial life, 12(2):229242, 2006. [5] Angelo Cangelosi. Evolution of communication and language using signals, symbols, and words. IEEE Transactions on Evolutionary Computation, 5(2):93101, 2001. [6] Angelo Cangelosi and Domenico Parisi. The emergence of alanguagein an evolving population of neural networks. Connection Science, 10(2):8397, 1998. [7] Angelo Cangelosi and Domenico Parisi. Simulating the evolution of language. Springer Science & Business Media, 2012. [8] Boaz Carmeli, Ron Meir, and Yonatan Belinkov. Ctd: Composition through decomposition in emergent communication. In International Conference on Learning Representations, 2025. [9] Rahma Chaabouni, Florian Strub, Florent Altché, Eugene Tarassov, Corentin Tallec, Elnaz Davoodi, Kory Wallace Mathewson, Olivier Tieleman, Angeliki Lazaridou, and Bilal Piot. Emergent communication at scale. In International conference on learning representations, 2022. [10] Rujikorn Charakorn, Poramate Manoonpong, and Nat Dilokthanakul. Generating diverse cooperative agents by learning incompatible policies. In International Conference on Learning Representations, 2023. [11] Rujikorn Charakorn, Poramate Manoonpong, and Nat Dilokthanakul. Diversity is not all you need: Training robust cooperative agent needs specialist partners. volume 37, pages 5640156423, 2024. [12] Morten Christiansen and Simon Kirby. Language evolution. OUP Oxford, 2003. [13] Christian Schroeder De Witt, Tarun Gupta, Denys Makoviichuk, Viktor Makoviychuk, Philip HS Torr, Mingfei Sun, and Shimon Whiteson. Is independent learning all you need in the starcraft multi-agent challenge? arXiv preprint arXiv:2011.09533, 2020. [14] Kevin Yandoka Denamganai, Tim Bradley, Pierluigi Vito Amadori, Sondess Missaoui, Guy Moss, and James Alfred Walker. Erelela: Exploration in reinforcement learning via emergent language abstractions. 2024. [15] Jean-Louis Dessalles. Why we talk: The evolutionary origins of language. Oxford University Press, 2007. [16] Tabea Dreyer, Amir Haluts, Amos Korman, Nir Gov, Ehud Fonio, and Ofer Feinerman. Comparing cooperative geometric puzzle solving in ants versus humans. Proceedings of the National Academy of Sciences, 122(1):e2414274121, 2025. [17] Katrina Evtimova, Andrew Drozdov, Douwe Kiela, and Kyunghyun Cho. Emergent communication in multi-modal, multi-step referential game. In International Conference on Learning Representations, 2018. 10 [18] Jakob Foerster, Ioannis Alexandros Assael, Nando De Freitas, and Shimon Whiteson. Learning to communicate with deep multi-agent reinforcement learning. Advances in neural information processing systems, 29, 2016. [19] Lukas Galke, Yoav Ram, and Limor Raviv. Emergent communication for understanding human language evolution: Whats missing? In Emergent Communication Workshop at ICLR 2022. [20] Laura Harding Graesser, Kyunghyun Cho, and Douwe Kiela. Emergent linguistic phenomena in multi-agent communication games. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 37003710, 2019. [21] Niko Grupen, Daniel Lee, and Bart Selman. Multi-agent curricula and emergent implicit signaling. In Proceedings of the 21st International Conference on Autonomous Agents and Multiagent Systems, pages 553561, 2022. [22] Eleonora Gualdoni, Mycal Tucker, Roger Levy, and Noga Zaslavsky. Bridging semantics and pragmatics in information-theoretic emergent communication. Advances in Neural Information Processing Systems, 37:2105921078, 2024. [23] Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with constrained variational framework. In International conference on learning representations, 2017. [24] Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. Neural computation, 9(8):17351780, 1997. [25] Charles Hockett and Charles Hockett. The origin of speech. Scientific American, 203(3):8897, 1960. [26] Taher Jafferjee, Juliusz Ziomek, Tianpei Yang, Zipeng Dai, Jianhong Wang, Matthew Taylor, Kun Shao, Jun Wang, and David Mguni. Taming multi-agent reinforcement learning with estimator variance reduction. arXiv preprint arXiv:2209.01054, 2022. [27] Unnat Jain, Luca Weihs, Eric Kolve, Mohammad Rastegari, Svetlana Lazebnik, Ali Farhadi, Alexander Schwing, and Aniruddha Kembhavi. Two body problem: Collaborative visual task completion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66896699, 2019. [28] Ivana Kajic, Eser Aygün, and Doina Precup. Learning to cooperate: Emergent communication in multi-agent navigation. In CogSci, 2020. [29] Eugene Kharitonov, Rahma Chaabouni, Diane Bouchacourt, and Marco Baroni. Egg: toolkit for research on emergence of language in games. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP): System Demonstrations, pages 5560, 2019. [30] Jooyeon Kim and Alice Oh. Emergent communication under varying sizes and connectivities. volume 34, pages 1757917591, 2021. [31] Diederik Kingma and Jimmy Ba. Adam: method for stochastic optimization. In International Conference on Learning Representations, 2015. [32] Diederik Kingma, Max Welling, et al. Auto-encoding variational bayes. In International conference on learning representations, 2014. [33] Simon Kirby, Mike Dowman, and Thomas Griffiths. Innateness and culture in the evolution of language. Proceedings of the National Academy of Sciences, 104(12):52415245, 2007. [34] Simon Kirby, Tom Griffiths, and Kenny Smith. Iterated learning and the evolution of language. Current opinion in neurobiology, 28:108114, 2014. 11 [35] Satwik Kottur, José Moura, Stefan Lee, and Dhruv Batra. Natural language does not emerge naturallyin multi-agent dialog. In Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing, pages 29622967, 2017. [36] Angeliki Lazaridou and Marco Baroni. Emergent multi-agent communication in the deep learning era. arXiv preprint arXiv:2006.02419, 2020. [37] Angeliki Lazaridou, Karl Moritz Hermann, Karl Tuyls, and Stephen Clark. Emergence of linguistic communication from referential games with symbolic and pixel input. In International Conference on Learning Representations, 2018. [38] Angeliki Lazaridou, Alexander Peysakhovich, and Marco Baroni. Multi-agent cooperation and the emergence of (natural) language. In International Conference on Learning Representations, 2017. [39] David Lewis. Convention: philosophical study. John Wiley & Sons, 2008. [40] Chuming Li, Jie Liu, Yinmin Zhang, Yuhong Wei, Yazhe Niu, Yaodong Yang, Yu Liu, and Wanli Ouyang. Ace: Cooperative multi-agent q-learning with bidirectional action-dependency. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pages 85368544, 2023. [41] Dapeng Li, Zhiwei Xu, Bin Zhang, Guangchong Zhou, Zeren Zhang, and Guoliang Fan. From explicit communication to tacit cooperation: novel paradigm for cooperative marl. In Proceedings of the 23rd International Conference on Autonomous Agents and Multiagent Systems, pages 23602362, 2024. [42] Sheng Li, Jayesh Gupta, Peter Morales, Ross Allen, and Mykel Kochenderfer. Deep implicit coordination graphs for multi-agent reinforcement learning. In Proceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems, pages 764772, 2021. [43] Olaf Lipinski, Adam Sobey, Federico Cerutti, and Timothy Norman. Speaking your language: Spatial relationships in interpretable emergent communication. volume 37, pages 140113140137, 2024. [44] Olaf Lipinski, Adam Sobey, Federico Cerutti, and Timothy Norman. Its about time: Temporal references in emergent communication. arXiv preprint arXiv:2310.06555, 2023. [45] Paul Michel, Mathieu Rita, Kory Wallace Mathewson, Olivier Tieleman, and Angeliki Lazaridou. Revisiting populations in multi-agent communication. In International Conference on Learning Representations, 2023. [46] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, and Martin Riedmiller. Playing atari with deep reinforcement learning. arXiv preprint arXiv:1312.5602, 2013. [47] Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei Rusu, Joel Veness, Marc Bellemare, Alex Graves, Martin Riedmiller, Andreas Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature, 518(7540):529533, 2015. [48] Igor Mordatch and Pieter Abbeel. Emergence of grounded compositional language in In Proceedings of the AAAI conference on artificial intelligence, multi-agent populations. volume 32, 2018. [49] Jesse Mu and Noah Goodman. Emergent communication of generalizations. Advances in neural information processing systems, 34:1799418007, 2021. [50] Yao Mu, Shunyu Yao, Mingyu Ding, Ping Luo, and Chuang Gan. Ec2: Emergent communication for embodied control. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67046714, 2023. [51] Mitja Nikolaus. Emergent communication with conversational repair. In International Conference on Learning Representations, 2024. [52] Martin Nowak and Natalia Komarova. Towards an evolutionary theory of language. Trends in cognitive sciences, 5(7):288295, 2001. [53] Martin Nowak, Joshua Plotkin, and Vincent AA Jansen. The evolution of syntactic communication. Nature, 404(6777):495498, 2000. [54] Georgios Papoudakis, Filippos Christianos, and Stefano Albrecht. Agent modelling under partial observability for deep reinforcement learning. volume 34, pages 1921019222, 2021. [55] Shivansh Patel, Saim Wani, Unnat Jain, Alexander Schwing, Svetlana Lazebnik, Manolis Interpretation of emergent communication in heterogeneous Savva, and Angel Chang. collaborative embodied agents. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1595315963, 2021. [56] Jannik Peters, Constantin Waubert de Puiseau, Hasan Tercan, Arya Gopikrishnan, Gustavo Adolpho Lucas de Carvalho, Christian Bitter, and Tobias Meisen. Emergent language: survey and taxonomy. Autonomous Agents and Multi-Agent Systems, 39(1):173, 2025. [57] Limor Raviv, Antje Meyer, and Shiri Lev-Ari. Larger communities create more systematic languages. Proceedings of the Royal Society B, 286(1907):20191262, 2019. [58] Florencia Reali, Nick Chater, and Morten Christiansen. Simpler grammar, larger vocabulary: How population size affects language. Proceedings of the Royal Society B: Biological Sciences, 285(1871):20172586, 2018. [59] Mathieu Rita, Florian Strub, Jean-Bastien Grill, Olivier Pietquin, and Emmanuel Dupoux. On the role of population heterogeneity in emergent communication. In International Conference on Learning Representations, 2022. [60] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. In International Conference on Learning Representations, 2016. [61] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [62] Kim Sterelny. Language, gesture, skill: the co-evolutionary foundations of language. Philosophical Transactions of the Royal Society B: Biological Sciences, 367(1599):21412151, 2012. [63] Mingfei Sun, Sam Devlin, Jacob Beck, Katja Hofmann, and Shimon Whiteson. Trust region bounds for decentralized ppo under non-stationarity. In Proceedings of the 2023 International Conference on Autonomous Agents and Multiagent Systems, pages 513, 2023. [64] Valentin Taillandier, Dieuwke Hupkes, Benoît Sagot, Emmanuel Dupoux, and Paul Michel. Neural agents struggle to take turns in bidirectional emergent communication. In International Conference on Learning Representations, 2023. [65] Michael Tomasello. Origins of human communication. MIT press, 2010. [66] Ryo Ueda and Tadahiro Taniguchi. Lewiss signaling game as beta-vae for natural word lengths and segments. In International Conference on Learning Representations, 2024. [67] Kyle Wagner, James Reggia, Juan Uriagereka, and Gerald Wilkinson. Progress in the simulation of emergent communication and language. Adaptive Behavior, 11(1):3769, 2003. [68] Han Wang, Binbin Chen, Tieying Zhang, and Baoxiang Wang. Learning to communicate through implicit communication channels. In International Conference on Learning Representations, 2025. [69] Erik Wijmans, Abhishek Kadian, Ari Morcos, Stefan Lee, Irfan Essa, Devi Parikh, Manolis Savva, and Dhruv Batra. Dd-ppo: Learning near-perfect pointgoal navigators from 2.5 billion frames. In International Conference on Learning Representations, 2020. [70] Ludwig Wittgenstein. Philosophical investigations. John Wiley & Sons, 2009. 13 [71] Yaodong Yang, Guangyong Chen, Weixun Wang, Xiaotian Hao, Jianye Hao, and Pheng-Ann Heng. Transformer-based working memory for multiagent reinforcement learning with action parsing. volume 35, pages 3487434886, 2022. [72] Shunyu Yao, Mo Yu, Yang Zhang, Karthik Narasimhan, Joshua Tenenbaum, and Chuang Gan. Linking emergent and natural languages via corpus transfer. In International Conference on Learning Representations, 2022. [73] Chao Yu, Akash Velu, Eugene Vinitsky, Jiaxuan Gao, Yu Wang, Alexandre Bayen, and Yi Wu. The surprising effectiveness of ppo in cooperative multi-agent games. volume 35, pages 2461124624, 2022. [74] Rotem Ben Zion, Boaz Carmeli, Orr Paradise, and Yonatan Belinkov. Semantics and spatiality of emergent communication. volume 37, pages 110156110196, 2024."
        },
        {
            "title": "A Learning to Act and Communicate with PPO",
            "content": "Agents in our setting learn to act and communicate simultaneously. As illustrated in Figure 2, each agent receives an observation from the environment and message from its partner sent at the previous time step. Based on this input, the agent produces discrete action and message. During training, the agent also estimates value function to guide learning. In the following, we formalize this process using the multi-agent reinforcement learning (MARL) framework and describe how action and communication are jointly optimized via the MARL algorithm. The method most closely related to ours is RIAL [18]. However, unlike RIAL and related approaches, we do not assume parameter sharing or use centralized critic. While Independent PPO [13] employs parameter sharing among independent agents, technique that can help stabilize learning [63], our method explicitly removes this parameter sharing. These design choices make our setting more reflective of human-like conditions, where individuals operate independently and learn through decentralized interaction. Moreover, by avoiding parameter sharing, our method enables the study of linguistic and behavioral heterogeneity across agents. Problem Formulation Our learning setting is two-player variant of decentralized partially observable Markov decision processes (DecPOMDP) [2]. two-player DecPOMDP with communication is formally defined as the tuple (S, A1, A2, M1, M2, Ω1, Ω2, T, O, r, γ, H), where represents the state space, A1 A2 denotes the joint-action space, M1 M2 denotes the joint-message space, Ω Ω1 Ω2 signifies the joint-observation space (where Ω1 and Ω2 are the individual observation spaces), (ss, a1, a2) provides the transition probability from state to upon executing the joint action (a1, a2), O(o1, o2s) represents the conditional probability of observing the joint observation (o1, o2) given the current state s, r(s, a1, a2) is the shared reward function, γ is the discount factor for rewards, and denotes the horizon length. , m(t1) 2 ). Each agent maintains trajectory τ (t) The first and second agents are controlled by neural policies ψ1 = (π1, ϕ1) and ψ2 = (π2, ϕ2), respectively, where the subscript indicates the agent identity. Here, π denotes the action policy and ϕ denotes the communication policy. Each neural policy ψ is parameterized by θ. At each time step t, agents receive the joint observation o(t) = (o(t) 2 ) and the previous joint message m(t1) = (m(t1) consisting of its own history up to time t. 1 Conditioned on its trajectory τ (t) , agent samples its action and message as follows: a(t) , τ (t) ). The joint trajectory can be written as τ = (o0, a0, m0, r1, o1, ..., oH1, aH1, mH1, rH , oH ) (Ω R)H . The return of trajectory τ is defined as G(τ ) = (cid:80)H t=1 γt1rt, and the expected return of the joint policy (ψ1, ψ2) is J(ψ1, ψ2) = Eτ p(τ θ1,θ2)G(τ ), where p(τ θ1, θ2) denotes the distribution over trajectories induced by the joint policy. and the other agents previous message m(t1) ϕi(m(t) , τ (t) πi(a(t) ) and m(t) m(t1) m(t1) 1 , o(t) i Learning to Act and Communicate with PPO As described above, we treat the communication policy the same as the action policy, i.e., we jointly optimize them with policy gradients. This is because both policies output discrete decisions based on the agents observation history, and both can be optimized with similar objective functions under policy gradient methods. We use PPO [61] because of its effectiveness in optimizing discrete actions, sample efficiency, and good performance in (θ) = ϕ(m(t)τ (t)) multi-agent learning [40, 13, 73]. We first define r(t) ϕold(m(t)τ (t)) as the ratios of action policy and communication policy [61], respectively. The objective function of the neural policy θ can be written as = Ja + Jm + Jent. We use Generalized Advantage Estimation (GAE) [60] to estimate the advantage function. We use single advantage function ˆA(t) estimated from the shared representation output by the LSTM, which is used for optimizing both the action and communication policies. Each agent is trained independently using standard single-agent PPO, which treats its partner as part of the environment, i.e., fully decentralized training. The first term optimizes the action policy π using the estimated advantage function: πold(a(t)τ (t)) and r(t) (θ) = π(a(t)τ (t)) Ja = Et,τ p(τ θ,) (cid:104) min (cid:16) (θ) ˆA(t), clip(r(t) r(t) (θ), 1 ϵ, 1 + ϵ) ˆA(t)(cid:17)(cid:105) The second term optimizes the communication policy ϕ in the same manner: (cid:16) (cid:104) (θ) ˆA(t), clip(r(t) r(t) (θ), 1 ϵ, 1 + ϵ) ˆA(t)(cid:17)(cid:105) Jm = Et,τ p(τ θ,) min . . (1) (2) 15 The final term encourages exploration by maximizing the entropy of both action and message distributions: Jent = Et,τ p(τ θ,) λa (cid:34) π(aτ (t)) log π(aτ (t)) + λm (cid:88) (cid:88) (cid:35) ϕ(mτ (t)) log ϕ(mτ (t)) . (3) Neural Architecture In Figure 2, the agent architecture is divided into input and output components. The encoders are essential for converting heterogeneous inputs such as messages, grid observations, and spatial coordinates into unified feature representation, enabling effective downstream processing and integration by the LSTM. On the input side, the message encoder EM, grid encoder EX , and position encoder EP process the incoming message from the other agent, the grid observation, and the agents current position, respectively. The extracted features from these encoders are concatenated and fed into an LSTM [24], which maintains temporal memory. On the output side, the message head DM, action head DA, and value head DV generate the outgoing message, the selected action, and the estimated state value, respectively. All encoders and heads are implemented as shallow multilayer perceptrons (MLPs). The message encoder EM includes an embedding layer that maps each vocabulary index to real-valued vector before passing it to the MLP."
        },
        {
            "title": "B Theoretical Analysis",
            "content": "We explain why three cross-play (XP) agents and two cross-play-and-self-play (XP+SP) agents tend to develop shared language. We begin with the assumption that converged agents achieve successful coordination when their partners languages are compatible with them. Intuitively, if an agent is paired with partner whose language differs more from its compatible language, their joint performance will decline. Assumption 1 (Coordination depends on language compatibility). Let ψ1 and ψ2 be agents that achieve near-optimal performance, i.e., (1 ϵ)J J(ψ1, ψ2) , where is the optimal return and ϵ 0 is small. Let ψ3 and ψ4 be alternative partners interacting with ψ1. If then the joint returns satisfy: LS(ψ2, ψ3) > LS(ψ2, ψ4), J(ψ1, ψ2) J(ψ1, ψ3) J(ψ1, ψ4). To formalize what it means for two agents to use similar language, we introduce the following definition based on language similarity (LS) and its relationship to joint performance. Definition 1 (ϵ-language threshold w.r.t. fixed partner). Let ψeval and ψref be two agents such that J(ψeval, ψref) (1 ϵ)J . Define the language compatibility threshold δl(ψeval, ψref, ϵ) as: δl(ψeval, ψref, ϵ) := min ψ LS(ψref, ψ) s.t. J(ψeval, ψ) (1 ϵ)J . This is the minimal language similarity (with respect to ψref) that new partner ψ must have in order to maintain near-optimal return when paired with ψeval. Under this setup, we can now show that language similarity within population of XP agents (Npop 3) is lower-bounded. This result ensures that if all agent pairs achieve near-optimal returns, their learned languages cannot be arbitrarily different. Theorem 1 (Language convergence in cross-play (XP) population). Let ψi, ψj, ψk be any three distinct agents from co-trained population with Npop 3. Suppose their pairwise joint returns are bounded: (1 ϵ)J J(ψi, ψj) , for some small ϵ 0. (1 ϵ)J J(ψj, ψk) , (1 ϵ)J J(ψi, ψk) , Then the language similarity between ψi and ψj is lower bounded as: LS(ψi, ψj) δl(ψk, ψi, ϵ), where δl(ψk, ψi, ϵ) is the minimum language similarity to ψi required to achieve at least (1 ϵ)J when paired with ψk. 16 In the same way, we can simply show that two agents trained under XP+SP have lower-bounded language similarity to maintain high task performance. Theorem 2 (Language convergence in cross-and-self-play (XP+SP) agents). Let ψ1 and ψ2 be co-trained agents under cross-play and self-play (XP+SP) regime, and let denote the optimal achievable return in the environment. Suppose both agents achieve near-optimal return when playing with each other and with themselves: (1 ϵ)J J(ψ1, ψ2) , (1 ϵ)J J(ψ1, ψ1) , (1 ϵ)J J(ψ2, ψ2) , for some small ϵ 0. Then their communication protocols must be similar: LS(ψ1, ψ2) max (δl(ψ1, ψ1, ϵ), δl(ψ2, ψ2, ϵ)) , where δl(ψi, ψi, ϵ) is the minimum language similarity to ψi required to maintain return at least (1 ϵ)J when paired with ψi. Proof of Theorem 1. Assume for contradiction that LS(ψi, ψj) < δl(ψk, ψi, ϵ). By definition of δl (Definition 1), this means that ψj is less similar to ψi than any agent known to maintain near-optimal return with ψk when ψi is the reference. We consider that ψk is the fixed agent (evaluator), ψi is known successful partner of ψk, and ψj is new partner. Since LS(ψi, ψj) < δl(ψk, ψi, ϵ), then: J(ψk, ψj) < (1 ϵ)J . This contradicts the near-optimal return condition that J(ψk, ψj) (1 ϵ)J . Therefore, LS(ψi, ψj) δl(ψk, ψi, ϵ). Proof of Theorem 2. Assume for contradiction that: LS(ψ1, ψ2) < max (δl(ψ1, ψ1, ϵ), δl(ψ2, ψ2, ϵ)) . Without loss of generality, assume the maximum is achieved by ψ1, so: LS(ψ1, ψ2) < δl(ψ1, ψ1, ϵ). From Definition 1, this implies that ψ2 is less similar to ψ1 than any agent that can achieve return (1 ϵ)J when paired with ψ1. We use ψ1 as the fixed evaluator, ψ1 and itself are known to perform well: J(ψ1, ψ1) (1 ϵ)J , and ψ2 is new partner being evaluated based on its similarity to ψ1. Since LS(ψ1, ψ2) < δl(ψ1, ψ1, ϵ), then J(ψ1, ψ2) < (1 ϵ)J , which contradicts the near-optimal return condition that J(ψ1, ψ2) (1 ϵ)J . Therefore, LS(ψ1, ψ2) max (δl(ψ1, ψ1, ϵ), δl(ψ2, ψ2, ϵ)) ."
        },
        {
            "title": "C Additional Figures and Tables",
            "content": "Table S1: Performance of XP agents in the TemporalG environment. Npop: population size. LS: language similarity; Cross-SR: success rate with other agents; Self-SR: success rate in self-play. All values are reported as mean standard deviation across held-out agent pairs. Model Npop LS Cross-SR Self-SR XP 3 0.336 0.012 0.975 0.003 0.962 0.006 (a) Fully-Connected: XP (b) Fully-Connected: XP+SP (c) Ring-Structured: XP (d) Ring-Structured: XP+SP Figure S1: Language Similarity between agent pairs under different social structures and training strategies. 18 (a) Fully-Connected: XP (b) Fully-Connected: XP+SP (c) Ring-Structured: XP (d) Ring-Structured: XP+SP Figure S2: Success rate between agent pairs under different social structures and training strategies. (a) 2 XP Agents (b) 2 XP+SP Agents (c) 3 XP Agents Figure S3: In t-SNE space, sequences of messages from two different agents are visually separable in the 2 XP Agents setting. 19 (a) Episodic return (b) Action entropy (c) Message entropy Figure S4: Larger population takes longer time to converge. The figure shows learning dynamics of agents with different population sizes in ScoreG. Return, action entropy, and message entropy are plotted over environment steps, averaged across three seeds. single step is defined as single interaction with the environment. 20 (a) Episodic return (b) Action entropy Figure S5: Learning dynamics of three XP agents in TemporalG. Agents start to converge at around one billion environment steps. (c) Message entropy"
        },
        {
            "title": "D Metrics",
            "content": "Topographic Similarity (topsim) Topographic similarity measures the alignment between the structure of the message space and the semantic space (e.g., environmental states or properties). It is defined as the Spearman rank correlation between all pairwise distances in these two spaces [4, 37]. Let = {m1, m2, . . . , mN } be the set of message sequences, and = {s1, s2, . . . , sN } be the corresponding semantic meanings. Let δM (mi, mj) and δS(si, sj) denote the pairwise distance functions in the message and semantic spaces, respectively. Then: topsim = Spearman (cid:16) {δM (mi, mj)}i<j , {δS(si, sj)}i<j (cid:17) (4) high topographic similarity indicates that similar messages correspond to similar semantic meanings, reflecting structured and grounded communication protocol. We use items positions and scores as semantic meanings. Language Similarity (LS) quantifies the token-level similarity between two agents based on their communication over multiple episodes, starting from the same initial condition. For given episode e, the agents and produce their respective message sequences (e) . Let Dedit() represent the normalized edit distance function, and let Ne denote the total number of evaluation episodes. The Language Similarity (LS) between agents and is defined as: and (e) LS(i,j) = 1 Ne Ne(cid:88) e=1 1 Dedit(M (e) , (e) ) (5) We can then compute the average LS across all pairs of agents in the entire population as follows: LS = (cid:80)Npop 1 Npop(Npop1) (cid:80)Npop i=1 j=i LS(i, j). Interchangeability (IC) refers to property of language wherein speaker can both send and understand the same linguistic signals [25]. In the context of agents, this means that an agent should understand the language it produces. To evaluate interchangeability in agents, we embed the same neural network in two different agent bodies and assess their performance in the game. We compare an agents success when paired with itself to its success when paired with other agents. Formally, consider set of Npop agents. Let SR(i, j) denote the success rate of an agent when playing with an agent j. Therefore, we propose that interchangeability (IC) can be defined as: IC = (Npop 1) (cid:80)Npop i=1 (cid:80)Npop i=1 SR(i, i) (cid:80)Npop j=i SR(i, j) (6) Generalization: Varying Grid Size and Adding Obstacles We analyze the generalization and robustness of 3 XP agents in both the ScoreG and TemporalG environments by evaluating performance across varying grid sizes and obstacle configurations. While not directly focused on communication, this analysis serves to evaluate how well learned behaviors and coordination strategies transfer to novel spatial configurations. Generalization to unseen grid sizes. Tables S2 and S3 report agent performance when evaluated on larger grid sizes than those seen during training. XP agents trained on 5 5 environments experience sharp performance drop as grid size increases in both tasks. In the ScoreG task, the success rate declines from 0.964 on the training grid to 0.333 on 9 9 grid, with longer episode lengths indicating increased exploration difficulty. similar trend is observed in TemporalG, where success drops drastically from 0.971 to 0.179 between 5 5 and 7 7 grids. These results suggest that generalization to larger spatial layouts remains challenge. 22 Figure S6: Visualization of central obstacles in the environment. Obstacles are shown as gray squares randomly placed in the middle of the map. Agents cannot move through gray squares, which increases coordination difficulty. Effect of central obstacles. To evaluate the agents robustness under spatial constraints, we introduce impassable obstacles in the center of the map (visualized in Figure S6). In the ScoreG game (Table S4), performance degrades progressively as the number of obstacles increases. Success rate falls from 0.960 with no obstacles to just 0.053 with four obstacles, while episode length grows, indicating less efficient navigation and coordination. The effect is even more pronounced in TemporalG (Table S5), where communication relies heavily on agents meeting at precise times and locations. With just one obstacle, the success rate drops from 0.971 to 0.275, and with two obstacles, it drops to 0.061. Table S2: Grid Size Generalization performance in the ScoreG environment. Agents were trained on 5 5 grid and evaluated on larger, unseen grid sizes without fine-tuning. SR: average success rate; Episode Length: average number of steps per episode. Grid Size Success Rate (SR) Episode Length 5 5 6 6 7 7 8 8 9 9 0.964 0.022 0.925 0.040 0.643 0.191 0.446 0.228 0.333 0.214 5.03 0.22 9.09 1.40 14.34 3.47 16.34 4.76 17.49 5.44 Table S3: Grid Size Generalization in the TemporalG environment. XP agents are trained on 5 5 grids and evaluated on larger unseen grid sizes without fine-tuning. SR: average success rate; Episode Length: average number of steps per episode. Grid Size Success Rate (SR) Episode Length 5 5 6 6 7 7 0.971 0.010 0.501 0.080 0.179 0.053 15.37 0.10 19.48 0.64 23.93 0.74 23 Table S4: Performance of 3 XP agents in the ScoreG environment with increasing numbers of central obstacles. As obstacle count increases, success rate decreases and episode length increases, indicating reduced coordination efficiency under spatial constraints. Number of Obstacles Success Rate (SR) Episode Length 0 1 2 3 4 0.960 0.026 0.528 0.033 0.280 0.049 0.131 0.047 0.053 0.027 4.98 0.19 5.28 0.24 5.66 0.36 6.16 0.42 6.46 0.56 Table S5: Performance of XP agents in the TemporalG environment with varying numbers of central obstacles. Increased obstacles reduce success rates significantly, suggesting that communication about temporally grounded information becomes less effective when spatial coordination is constrained. Number of Obstacles Success Rate (SR) Episode Length 0 1 2 0.971 0.010 0.275 0.019 0.061 0.008 15.37 0.10 14.75 0.13 14.61 0.37 Ablation: Vocabulary Size In ScoreG, the agents typically complete the game in approximately 5-6 time steps because we introduce time pressure to encourage agents to finish the game as soon as possible. Varying the vocabulary size alters the capacity of information that can be conveyed through the message sequence. Here we train XP agents with Npop = 3 with different vocabulary sizes. The default vocabulary size we used in the main text was 4 (i.e., mt {0, 1, 2, 3}). smaller vocabulary should, in principle, encourage greater compositionality due to smaller capacity. As shown in Table S6, compositionality remains relatively stable across vocabulary sizes of 4, 8, and 16. However, when the vocabulary size increases to 32, the topsim score drops to 0.25, indicating notable decline in compositional structure. Table S6: Ablation Study on Vocabulary Size. Effect of vocabulary size on communication and performance metrics. Values are reported as mean standard deviation. Vocabulary Size topsim IC Self-SR Cross-SR 4 8 16 32 0.311 0.111 0.352 0.032 0.309 0.052 0.251 0.034 0.966 0.017 0.902 0.056 0.954 0.030 0.954 0.031 0.939 0.021 0.880 0.057 0.933 0.030 0.931 0.031 0.972 0.004 0.975 0.003 0.978 0.001 0.977 0."
        },
        {
            "title": "G Training Details",
            "content": "We train our models single GeForce RTX 4080. However, due to the small model size and low environment complexity, training can also be run entirely on CPU as well. For populations smaller than 3, training takes approximately 16 hours, depending on the CPU and the number of available cores. The longest training time occurs with population size of 15 and takes up to 3 days. All experiments are repeated with three different random seeds. We use the ADAM optimizer [31] with an initial learning rate of 0.00025, which linearly decays over time. We report the hyperparameters of the architecture and training algorithm below. Table S7: Neural architecture hyperparameters. Component Hyperparameter"
        },
        {
            "title": "Visual encoder\nVisual input shape\nPosition encoder\nMessage embedding size\nMessage encoder\nLSTM hidden size\nLSTM input size\nAction head\nValue head\nMessage head\nWeight initialization",
            "content": "4-layer MLP: [256, 256, 128, 16] (1, 5, 5) grayscale grid Linear(2, 4) 16 Embedding(Vocab. Size, 16) + Linear(16, 16) 128 16 (visual) + 4 (position) + 16 (message) = 36 Linear(128, num_actions) Linear(128, 1) Linear(128, 16) Orthogonal (std = 2), output layers: std = 0.01 Table S8: PPO hyperparameters. Hyperparameter Total time steps Learning rate Number of environments Number of steps per rollout Number of minibatches Number of update epochs Discount factor γ GAE λ Clip coefficient Clip value loss Normalize advantages Action entropy coefficient Message entropy coefficient Value function coefficient Max gradient norm Learning rate annealing Target KL divergence Batch size Minibatch size Optimizer Value 2 109 2.5 104 128 32 4 4 0.99 0.95 0.1 True True 0.01 0.002 0.5 0.5 True None 4096 1024 Adam (ϵ = 105)"
        }
    ],
    "affiliations": [
        "Centre for Frontier AI Research (CFAR), ASTAR, Singapore",
        "College of Computing and Data Science, Nanyang Technological University, Singapore",
        "Department of Computer Science, The University of Manchester, United Kingdom",
        "Institute for Infocomm Research (I2R), ASTAR, Singapore",
        "Sakana AI, Japan"
    ]
}