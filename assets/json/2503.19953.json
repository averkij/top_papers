{
    "paper_title": "Self-Supervised Learning of Motion Concepts by Optimizing Counterfactuals",
    "authors": [
        "Stefan Stojanov",
        "David Wendt",
        "Seungwoo Kim",
        "Rahul Venkatesh",
        "Kevin Feigelis",
        "Jiajun Wu",
        "Daniel LK Yamins"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Estimating motion in videos is an essential computer vision problem with many downstream applications, including controllable video generation and robotics. Current solutions are primarily trained using synthetic data or require tuning of situation-specific heuristics, which inherently limits these models' capabilities in real-world contexts. Despite recent developments in large-scale self-supervised learning from videos, leveraging such representations for motion estimation remains relatively underexplored. In this work, we develop Opt-CWM, a self-supervised technique for flow and occlusion estimation from a pre-trained next-frame prediction model. Opt-CWM works by learning to optimize counterfactual probes that extract motion information from a base video model, avoiding the need for fixed heuristics while training on unrestricted video inputs. We achieve state-of-the-art performance for motion estimation on real-world videos while requiring no labeled data."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 3 5 9 9 1 . 3 0 5 2 : r Self-Supervised Learning of Motion Concepts by Optimizing Counterfactuals Stefan Stojanov* David Wendt* Seungwoo Kim* Rahul Venkatesh* Kevin Feigelis Jiajun Wu Daniel L.K. Yamins Stanford University"
        },
        {
            "title": "Abstract",
            "content": "Estimating motion in videos is an essential computer vision problem with many downstream applications, including controllable video generation and robotics. Current solutions are primarily trained using synthetic data or require tuning of situation-specific heuristics, which inherently limits these models capabilities in real-world contexts. Despite recent developments in large-scale self-supervised learning from videos, leveraging such representations for motion estimation remains relatively underexplored. In this work, we develop Opt-CWM, self-supervised technique for flow and occlusion estimation from pre-trained nextframe prediction model. Opt-CWM works by learning to optimize counterfactual probes that extract motion information from base video model, avoiding the need for fixed heuristics while training on unrestricted video inputs. We achieve state-of-the-art performance for motion estimation on real-world videos while requiring no labeled data. 1 1. Introduction The ability to extract scene motion properties such as optical flow [15, 38], occlusions, point or object tracks [12, 20], and collisions or other physical events [3, 40] is important in video understanding applications such as automated video filtering [52, 53], action recognition [27, 36] and motion prediction [6, 51]. Recently, scene motion primitives have also been shown to be key ingredient in improving controllability and consistency in video generation [17] and play an important role in downstream vision applications in areas such as robotics [5, 41]. As result, improving the estimation of motion is at the forefront of computer vision. Optical flow and occlusion estimation are two of the core problems in this domain. The most common approach to solving optical flow estimation uses supervised learning from labeled flow data. However, because densely annotating flow in real-world scenes is prohibitively ex- *Equal contribution. 1Project website at: https://neuroailab.github.io/opt_ cwm_page/ pensive, supervised flow estimation methods usually rely on synthetic data [31, 32]. Methods trained on synthetic data have proven to be robust in real-world video [38, 48]. However, relying on this approach has limited flow estimation methods from taking advantage of recent advances in self-supervised visual representation learning from massive video datasets [16, 34, 39] and inherently has to contend with sim-to-real domain gap. In contrast, self-supervised flow methods are typically based on photometric loss learning frame-pair feature correspondences to warp pixels from one RGB frame to corresponding locations in future frame states. However, pure photometric loss is weak constraint, in part because correspondences are often ill-defined (e.g., on internal portions of moving objects with homogeneous textures). Existing state-of-the-art self-supervised motion estimation methods use various nearest neighbor or clustering procedures [7, 22], or complement photometric loss with strong taskspecific regularizations like smoothness [24, 37]. However, because these heuristics are often only correct in narrow scenarios, the performance of such approaches is limited when the correctness conditions for the heuristic fail. In this work, we investigate how to extract selfsupervised flow and occlusion estimates without the use of such heuristics. Our approach is based on Counterfactual World Modeling (CWM) [4, 42], recent method that constructs zero-shot estimates of visual properties from an underlying pre-trained multi-frame model (Figure 1). CWM begins with sparse RGB-conditioned next frame predictor ΨRGB, two-frame masked autoencoder trained with highly asymmetric masking policy [4], in which all the patches of the first frame, but very few patches of the second frame, are given to the predictor. To solve this asymmetric prediction problem, ΨRGB must learn to encode scene dynamics in small number of patch feature tokens that effectively factor temporal dynamics from visual appearance. Motion properties can then be extracted from the base model in zero-shot fashion via simple counterfactual probes. For example, to compute flow from given point in the first frame, perturbation is made to the image at that point, and flow is computed by comparing the delta between ΨRGBs prediction on the perturbed (counterfactual) condition with its prediction in the original unperturbed (factual) condition (see Figure 1B). Intuitively, this corresponds to placing visual marker on the point, predictively flowing it forward, and then analyzing where it gets carried in the predicted next frame. The CWM approach circumvents the key limitation of the heuristicbased methods by replacing situation-specific fixed heuristics (e.g., motion smoothness) with general purpose predictive model, defining the quantity of interest (e.g., flow) as the outcome of probing the models predictions [42]. However, while CWM is an intriguing conceptual proposal, it has substantial drawback: the perturbations that it relies on are hand-designed, and can be out-of-domain in real-world video. Perturbations are often not properly carried along with moving objects, resulting in suboptimal counterfactual motion extractions (Figure 2B). As result, the accuracy of flow extracted from CWM so far has not surpassed state-of-the-art flow estimation solutions. Here we present Opt-CWM, generic solution to this problem. Opt-CWM consists of two core innovations that allow us to leverage the advantages of the CWM idea while making it highly performant in real-world settings. First, we parameterize counterfactual perturbation generator with learnable neural network (Figure 2A) that can predict perturbations specialized for the local appearance of any target points to be tracked. Second, we develop an approach for learning this perturbation generator in principled fashion without relying on any supervision from labeled data or heuristics. The main insight behind our approach is that the perturbation generator can be trained by applying generalization of the asymmetric masking principle used to train the base model ΨRGB itself. In particular, we connect putative flow outputs of the parameterized flow prediction function to randomly initialized sparse flow-conditioned nextframe predictor Ψflow and perform joint optimization (Figure 3) of both Ψflow and the perturbation generator. This forces Ψflow to predict future frame based on present frame and sparse putative flow, creating an information bottleneck that generates useful gradients back on the perturbation prediction functions parameters. We find that Opt-CWM outperforms state-of-the-art selfsupervised motion estimation methods that are purposely built for this task [24, 35, 37] when evaluated on challenging real-world benchmarks [12]. We also find that OptCWM can outperform supervised flow methods in variety of scenarios, including examples with complex motion that are difficult to simulate with synthetic data rendering systems. The success of our approach reveals promising direction for future work to achieve scalable counterfactualbased extraction of variety of visual properties. 2. Related Work Supervised flow estimation. Supervised methods like RAFT [38, 48] approach optical flow as dense regression problem and learn from synthetic optical flow datasets [8, 31]. They also typically use task-specific architectures that are tailor-made for optical flow estimation, with strong inductive biases (e.g., iterative flood-filling) and task-specific regularizations to ensure learning from limited training datasets. While these methods show strong performance in many contexts, their reliance on synthetic supervision and specialized architectures limits their generalizability. It is for this reason that our self-supervised Opt-CWM, which can be trained on unlimited in-the-wild videos, can outperform even supervised methods in certain key contexts. Self-supervised flow with photometric loss. Methods for self-supervised flow learning [25, 28, 37], such as SMURF [37], learn dense visual correspondence by optimizing photometric loss. Because of the weakness of pure photometric loss alone as supervision, these methods rely on complex variety of heuristically chosen regularization losses (e.g., spatial smoothness of flow, among others) to achieve reasonable performance levels. Because these heuristics need to be tuned in dataset-specific manner, these methods have failure models in complex dynamic scenes, especially with variable and large time-frame gaps. In contrast to these methods, Opt-CWM does not rely on such heuristics, as the quality of the flow extraction is directly correlated with the prediction learning objective. Augmenting self-supervised flow with visual pretraining. variety of methods augment photometricbased self-supervised flow using features derived from selfsupervised visual pre-training [2, 7, 24, 49]. For example, the recent state-of-the-art Doduo method [24] uses DinoV2 features as basis on which to compute feature correspondences for downstream photometric loss. This approach allows the extension of these methods to wider video training datasets (such as Kinetics), and thereby improves performance and generalizability. Even when backed by strong image features, photometric loss is comparatively weak constraint, again requiring the use of heuristic regularizers. Opt-CWM, which again does not use scenario-specific heuristics, compares favorably to these methods. Point tracking. Point tracking across multiple frames is related problem to flow and occlusion estimation. The majority of solutions for point tracking are supervised [12, 20] or semi-supervised [13, 26], and as such are further out of scope for this work. However, several recent works propose self-supervised approaches to finding temporal correspondence typically relying on pre-trained representations [7, 22]. These methods then extract point tracks through consistency objectives such as cycle consistency [7, 22, 35] or heuristics like softmax-similarity [43] applied at the frame pair level. The current state-of-the-art in this Figure 1. Extracting flow and occlusion with counterfactual perturbation: (A) CWMs learn to predict the next frame with temporally factored masking policy [4]. (B) The motion of point can be estimated using simple counterfactual probing program FLOW: the model predicts the next frame with and without local perturbation placed on the point, and the difference image between the clean and perturbed predictions reveals the estimated motion. (C) Occlusion is estimated using related probe OCC: when the perturbation difference image is diffuse and low magnitude, that indicates the perturbed point has been occluded. domain, GRMW [35], the main baseline comparison for our proposed Opt-CWM, uses cycle consistency to build tracks based on frame pair-level predictions. Real-world motion benchmarks. The TAP-Vid benchmark [12] provides critical set of metrics for measuring the accuracy of motion-estimation systems in real-world video. This is critical for ensuring that potential advances in motion estimation are tested against the challenges of real-world motion complexities, covering scenarios not encountered in synthetic benchmarks (e.g., non-rigid, highly articulated, deformable and breakable objects, fluids, inelastic collisions, animate objects, and human interactions). While originally intended for the supervised point tracking domain, recent self-supervised tracking works have begun to utilize TAP-Vid as main benchmark for motion estimation [24, 35]. In this work, we also follow this practice. 3. Methods 3.1. Counterfactual World Modeling enc and decoder ΨRGB RGB-Conditioned Next Frame Predictor. We begin by providing some background on Counterfactual World Modeling (CWM) [4, 42]. The first element of CWM is an RGB-conditioned next frame predictor ΨRGB, consisting of an encoder ΨRGB dec , similar to VideoMAE [39], but trained with highly asymmetric masking policy that reveals all patches of the first frame and small fraction of patches of the second frame (see Figure 1A). Specifically, let I1, I2 R3HW be two frame pairs in video, and define Mα as masking function that randomly masks some fraction, α, of patches in an image. Given fully visible first frame I1 and partially visible second frame Mα(I2), ΨRGB is trained to predict I2 by minimizing = MSE( ˆI2, I2), where ˆI2 = ΨRGB(cid:0)I1, Mα(I2)(cid:1). (1) Here we train CWM with α = 0.1 on the Kinetics dataset [27] with frame gap of 150ms. (See the supplement for more details.) As shown in [4], the asymmetric masking training policy forces ΨRGB to separate scene appearancewhich is wholly available in the first frame from scene dynamics, the information of which is now concentrated in the sparse set of visible next frame patches. In other words, ΨRGB is temporally factored. Motion Estimation. Because it induces strong dependence on the appearance and position of the revealed patches from I1 and I2, temporal factoring allows the extraction of visual structure through applying counterfactual probes: small changes to the appearance or the position of visible patches. By measuring the predictors response to these counterfactuals, we can easily extract useful information like object motion, segments, or shape from its representation [4]. As shown in Figure 1B, using the FLOW procedure, colored patch is placed on moving object and its motion can be determined by finding its location in the predicted frame. To track some pixel location p1 = (row1, col1) from one frame to the next, input image I1 is perturbed by adding colored patch δ at pixel location p1 to create the counterfactual input image 1 = I1 + δ. Then, the next frames with and without the counterfactual perturbation are predicted: 2 = ΨRGB(cid:0)I1 + δ, Mα(I2)(cid:1) = ΨRGB(cid:0)I ˆI ˆI2 = ΨRGB(cid:0)I1, Mα(I2)(cid:1). 1, Mα(I2)(cid:1), (2) 2 ˆI2c Subtracting these two predicted frames and taking an L1norm across the color channels produces the difference image = ˆI 1. Finally, the next-frame pixel location ˆp2 can be computed by finding the peak in the difference image: ˆp2 = arg max . Occlusion. The OCC procedure is identical to FLOW up to computation of the difference image (See Figure 1). However, if patch in the first frame gets occluded in the second frame, the response to the perturbation in the difference image will be small in magnitude and diffuse in shape. Applying simple threshold to the maximum value of allows us to determine occlusion. Figure 2. Parameterizing the counterfactual intervention policy as an input-conditioned function. (A) Building on pre-trained RGB-conditioned predictor ΨRGB, Opt-CWM uses an image-conditioned perturbation prediction function δθ containing small MLPθ. As illustrated in B, δθ can learn to predict image-conditioned perturbations that blend naturally with the underlying scene, potentially allowing for the perturbation to be accurately carried over to the next frame prediction. But how should the parameters of δθ be learned to achieve this, without any flow supervision labels? See Figure 3. 3.2. Optimizing Counterfactual Perturbations The problem with hand-designed perturbations. While fixed hand-designed perturbations (e.g., colored squares) can sometimes be effective in probing motion with ΨRGB, they are often suboptimalboth because they are out of domain for the base predictor and, by being imageand position-independent, can be unsuited to the local image context. Anecdotally, this results in visually obvious failure cases such as the perturbation not moving with the object or being suppressed entirely. Using the challenging Tap-Vid benchmark (see Section 4 for more details), we empirically quantified that the original fixed hand-designed perturbations are insufficient for self-supervised motion estimation performance (see CWM results in Table 1). The main requirement for good perturbation is thus that it be sufficiently in-distribution and image-position specific to cause meaningful contextdependent changes for probing the base predictor. But how can perturbations be redesigned for this purpose? Our solution has two basic novel components: parameterizing an image-conditioned counterfactual generator as differentiable function; and formulating general-purpose selfsupervised loss objective for learning the counterfactual generation function parameters. 3.2.1. Parameterized Perturbations We re-formulate the motion extraction procedure from Section 3.1 to make it parameterized differentiable function and introduce the functional form of sum of colored Gaussians as natural perturbation class. (See Figure 2A) Let FLOWθ : (I1, I2, p1) (cid:55) ˆφ be per-pixel motion estimation function with learnable parameters θ that takes an image pair (I1, I2) and pixel location p1 in I1 and outputs the predicted flow ˆφ = ˆp2 p1. Here, ˆp2 is the estimated second frame pixel location. The procedure FLOWθ involves multiple components: the counterfactual perturbation function, δθ(I1, Mα(I2), p1), which now produces variable counterfactual perturbations as function of the frame pair and pixel location (as opposed to fixed perturbation δ, used in the standard CWM); the pretrained, frozen, RGB-conditioned predictor, ΨRGB, as utilized within the FLOWθ program; and softargmax module to predict pixel location using differentiable approximation to the argmax function. Gaussian Perturbations. We choose to parameterize the counterfactual perturbations as Gaussians because this function class presents natural method of forming indomain counterfactual input images. To compute the Gaussian parameters for given counterfactual perturbation, we use the encoder of the RGB-conditioned predictor, ΨRGB enc . This outputs sequence of feature tokens from its last transformer block, which encode global and local scene content for each patch and thus form natural basis from which Gaussian parameters can be computed using shallow MLP. Given pixel location p1, we find its corresponding patch embedding token, tp1 , and use it as an input to an MLP that outputs parameter vector which is in turn used to compute the Gaussian perturbation: tp1 = ΨRGB enc (I1, Mα(I2))p1 δθ(I1, Mα(I2), p1) = Gaussian (MLPθ (tp1 )) (3) Then, FLOWθ computes the difference image, , similar to 2 = ΨRGB(cid:0)I1 + δθ, Mα(I2)(cid:1). the FLOW program, using ˆI Because FLOWθ needs to be differentiable, we use softargmax over in place of an argmax to estimate ˆp2. Softargmax Module. We follow the softargmax formulation proposed in [46] to differentiably approximate the argmax function. Given difference image, = 2 ˆI2c ˆI 1, we first apply temperature-scaled 2D softmax and then take the expectation according to that softmax to find the predicted second frame pixel location ˆp2 = Ep2softmax(/τ )[p2]. The predicted flow is then computed as ˆφ = ˆp2 p1. 3.2.2. Learning Optimized Counterfactuals Now that the perturbation generator has been parameterized, the question arises: how can its parameters be learned? Figure 3. generic principle for learning optimal counterfactuals. A) The parameterized counterfactual flow function FLOWθ extracts motion from frozen RGB-conditioned predictor ΨRGB through counterfactual perturbation (details in Figure 2). Its parameters θ are trained using gradients from flow-conditioned predictor Ψflow that is jointly trained to perform next-frame prediction. The predictor Ψflow can only learn to predict future frames if it is given correct flow vectors. This explicit information bottleneck ensures useful gradients will get passed back to FLOWθ . This setup allows us to get better extractions from pre-trained ΨRGB predictor by training another flow-conditoned predictor Ψflow using the same principle of next-frame prediction. (B) As consequence of tight coupling between the flow-conditioned predictor Ψflow and the learned flow estimation function FLOWθ, both motion estimation and pixel reconstruction simultaneously improve. η What type of self-supervised objective will cause the perturbation generator function to be properly context-specific and result in accurate flow vectors? Our main insight is that this problem can be bootstrapped in robust fashion by generalizing the sparse asymmetric mask learning paradigm to encompass coupled and mixed-mode RGB-Flow prediction problem without using labeled data (see Figure 3). Specifically, we jointly train the parameterized counterfactual motion prediction function, FLOWθ, which estimates set of flow vectors; together with sparse flow-conditioned predictor, Ψflow, which takes single frame along with sparse flow vectors to predict the next frame. We constrain FLOWθ by passing its outputs as inputs to Ψflow and training end-to-end using final RGB reconstruction loss on the predictions of Ψflow. As Ψflow has no access to any RGB patches from the second frame I2, it is only if the putative flows are actually correct that it be possible for Ψflow to use them for minimizing next-frame reconstruction loss. η 1 , p(2) 1 , . . . , p(n) Specifically, given an image pair (I1, I2), we estimate the motion for set of pixels = {p(1) 1 } using FLOWθ, obtaining set of estimated forward flow vec- : (cid:0)I1, ˆF(cid:1) (cid:55) ˆI2 tors ˆF = { ˆφ(1), ˆφ(2), . . . , ˆφ(n)}. Let Ψflow be flow-conditioned next frame predictor with parameters η that takes the first frame RGB input I1 and predicts the next frame ˆI2, conditioned on the flow input ˆF. We jointly optimize θ and η, by minimizing minθ,η LMSE( ˆI2, I2). Figure 3B shows that optimizing end-to-end reconstruction couples tightly to upstream flow accuracy, as required for effective bootstrapping. Additional Enhancements. simple random masking strategy may inadvertently reveal the ground truth RGB at the next frame location we are trying to predict for particular point. In this event, the model will not carry over the counterfactual perturbation to the future frame, leading to an erroneous flow prediction. simple yet effective inference-time solution is multi-mask (MM), in which we apply multiple random masks and average across the resulting delta images to reduce the influence of sub-optimal masks. Following prior work [15, 23], we also perform an iterative multiscale refinement of flow predictions by recursively applying FLOWθ to smaller crops centered on the predicted point location, ˆp2 of the previous iteration. We observe that FLOWθ is able to generate good initial flow predictions, and thus benefits from refinement  (Table 2)  . 4. Experiments 4.1. Evaluation Protocol Our main datasets for evaluation are TAP-Vid DAVIS and TAP-Vid Kinetics [12], the DAVIS [33] and Kinetics [27] datasets with human point track annotations, along with the synthetic Kubric [19] dataset. For TAP-Vid Kinetics and TAP-Vid Kubric, we randomly sample 30 videos (the same size as TAP-Vid DAVIS) for the evaluation. We run experiments on two distinct protocols aimed at measuring performance under various settings. For flow methods without an existing implementation of occlusion prediction, we use cycle consistency: occlusion is the event of inconsistency between forward and backward predictions greater than 6 pixels. Method TAP-Vid CFG RAFT [38] SEA-RAFT [48] Doduo [24] SMURF [37] CWM [4, 42] Opt-CWM (ours) DAVIS Kinetics Kubric AJ AD < δx avg OA OF1 AJ AD < δx avg OA OF1 AJ AD < δx avg OA OF1 69.69 69.89 1.43 1. 83.83 81.98 46.08 79.01 84.82 82. 47.52 75.12 0.86 1.07 87.59 92. 49.49 73.38 85.82 88.90 39.42 77. 1.24 1.00 83.73 87.02 91.00 63. 92.50 68.65 25.61 1.61 72.56 37. 22.59 35.26 1.19 77.62 43.00 11. 56.57 1.74 68.63 87.26 55.01 65. 27.56 69.53 2.40 4.65 1.19 79. 38.55 83.15 82.26 88.90 88.85 42. 5.41 44.17 78.76 34.00 75.98 0. 3.93 1.01 87.16 43.37 84.31 93. 47.69 95.17 5.95 96.34 58.61 69. 30.72 70.70 1.59 4.05 1.26 82. 42.33 82.78 90.84 88.44 90.31 53. 4.27 57.30 TAP-Vid First Main Benchmark RAFT [38] SEA-RAFT [48] 41.77 43.41 25.33 20.18 54.37 58. 66.40 66.34 56.12 56.23 44.02 39. 19.49 24.28 56.76 52.63 75.86 72. 71.25 69.19 69.80 75.64 5.51 4. 80.56 85.12 87.75 90.07 68.48 73. Doduo [24] GMRW [35] SMURF [37] CWM [4, 42] Opt-CWM (ours) 23.34 13.41 48.50 47.91 49.43 14. 16.04 45.84 45.96 53.94 51.85 5. 64.17 82.65 61.97 36.47 30.64 15. 47.53 20.26 27.28 23.53 8.73 54. 44.18 26.30 64.83 76.36 59.15 76. 80.87 42.85 46.91 18.22 60.74 25. 36.99 14.84 44.85 27.65 28.73 30. 13.44 41.63 48.52 25.00 57.74 71. 31.68 67.50 70.42 64.73 70.90 16. 84.12 77.84 63.47 26.54 67.61 3.16 6.71 11.81 4.57 81.74 89.36 35.14 78.78 39.35 80.01 87.07 84.14 87. 58.60 13.70 67.13 Table 1. Quantitative results on TAP-Vid First and CFG protocols. In the First protocol, point is tracked from when it is first visible to the end of the video, which requires estimating motion across large frame gaps. Opt-CWM outperforms both supervised and unsupervised two-frame baselines. In the CFG protocol, point tracking is evaluated at fixed gaps of 5 frames, making it an easier setting that is more favorable to optical flow methods. and indicate supervised and unsupervised, respectively. Doduo is not strictly unsupervised, as explained in Section 4.1. GMRW is trained on the Kubric dataset, (marked with ), making it more favorable evaluation setting for that method because of the minimal domain gap. Best performing supervised models (shaded) are considered separately. Models that can accept variable resolution inputs are run with the resolution closest to native that can be fit into memory, ensuring that each is run optimally. Metrics for both procedures are always computed after rescaling predictions to 256 256 resolution, following [35]. TAP-Vid First. Following the TAP-Vid First protocol proposed in [12], for each point, we take the frame in which it is first visible and track its motion only forward in time. This is challenging setting as it involves tracking points across variable and often large frame gaps. TAP-Vid Constant Frame Gap (CFG). For fair comparison with optical flow models, we also propose an additional protocol with fixed frame gaps that is more advantageous for these baselines (see supplementary for the effect of frame gap on flow baselines). In particular, fixed 5frame gap is used: metrics are computed on all frame pairs that are 5 frames apart (and the point is visible in the first). Metrics. We use the official metrics from the TAP-Vid evaluation protocol [12]: 1) Average Jaccard (AJ), precision metric measuring combination of point tracking and occlusion prediction; 2) Average Distance (AD) between the estimated pixel and ground truth locations; 3) < δx avg, which measures the average percentage of points predicted correctly within variety of pixel distance thresholds; and 4) Occlusion Accuracy (OA), the fraction of points correctly predicted as either occluded or visible. Additionally, to account for the relative lack of occlusion events in the dataset, we also evaluate 5) Occlusion F1 (OF1), which computes the F1 score of the occlusion predictions. 4.2. Baselines Our evaluation protocol requires tracking points in videos through occlusion by finding temporal correspondence: given frame pair, determine where the point went or whether it was occluded. Therefore, the appropriate baselines are supervised and self-supervised optical flow methods, and self-supervised temporal correspondence methods. We run the following baselines: CWM [4, 42] represents motion estimated through counterfactual extractions with fixed perturbation and without additional enhancements. This comparison illustrates the significant performance gains over prior CWM works. We present detailed ablation analysis in Section 4.3. GMRW [35] is recent self-supervised approach to video correspondence. The transformer-based architecture is trained on cycle consistency using contrastive random walks. GMRW is designed for temporal correspondencebased long-range tracking and is the SOTA baseline for comparison on TAP-Vid First. SMURF [37] is an unsupervised method specifically designed for optical flow estimation. This work tailors the Figure 4. Qualitative comparison with baselines on real-world videos. The above examples show the failure modes of previous methods that rely on visual similarity or photometric loss. We observe that the baseline models struggle against subtle but functionally important changes in largely homogeneous scenes depicting objects of similar color and texture ((a) - (e)). Further, the use of photometric loss in self-supervised methods such as SMURF can also be susceptible to differences in light intensity across frame pairs ((f) - (h)). Opt-CWM, however, relies on holistic understanding of scene transformations and object dynamics and is able to find correspondence without arbitrary heuristics. RAFT [38] architecture so it can be trained using optical flow-specific heuristic losses like photometric loss and smoothness regularization. SMURF specializes in estimating motion in consecutive frames, with models trained on KITTI, Sintel, and FlyingChairs. Doduo [24] is method for finding dense correspondence across images trained on unlabeled, in-the-wild videos from Youtube-VOS [50]. It uses photometric losses and leverages the DINO [9] encoder for incorporating sparse correspondence priors. Doduo is not strictly unsupervised, as it uses off-the-shelf Mask2Former segments [11]. SEA-RAFT [48] is supervised flow method that builds upon the original RAFT [38] by adding additional pretraining on TartanAir [47], novel mixture of Laplace loss, and improved initialization of the flow estimation. Evaluation results on TAP-Vid First and CFG are presented in Table 1. Our best-performing model accepts 512 resolution inputs and is evaluated with 10 multi-masks and 4 multiscale iterations (see Section 3.2.2). On TAP-Vid First, Opt-CWM outperforms all baselines on all metrics. In particular, Opt-CWM greatly improves upon AD, demonstrating robustness even in difficult (though more realistic) cases with long frame gaps or high motion. On the CFG protocol, which is favorable to flow methods, Opt-CWM either outperforms or is competitive with the best unsupervised models, especially on DAVIS. Additionally, in Figure 4, we show qualitative comparisons between methods. As expected, the baselines struggle with videos violating the heuristic assumptions for which they were optimized, e.g., photometric similarity. Our experiments on the synthetic Kubric dataset [19], which more similar to synthetic optical flow training datasets and therefore more favorable to methods trained on such data, demonstrate that Opt-CWM has the best performance in this out of domain scenario among models that were not trained on Kubric data. Further OptCWM qualitative examples can be found in the supplement. 4.3. Ablation and Analysis Optimizing Counterfactuals. We compare Opt-CWM with spectrum of types of hard-coded perturbations, representing various forms of unoptimized CWM baseline, and find that learned interventions perform substantially better (see Table 2). This demonstrates not only that the CWM framework is highly effective at unsupervised motion estimation, but also that optimizing counterfactual perturbations is critical for good performance. The highly imagedependent nature of the optimal predicted perturbations is illustrated in Figure 5, which shows variations in Gaussian parameters of the perturbation as function of where the perturbation is to be placed in the image. As shown Figure 5. Perturbation maps emergently reflect scene properties. For two example frame pairs, we show the amplitudes and standard deviations, at each spatial position and for each color channel, of the optimal Gaussian perturbations predicted by MLPθ. These perturbation maps emergently reflect scene properties, with perturbation parameters varying in size and magnitude depending on where they are located in the image, corresponding to the presence of foreground objects and their parts. MM MS Res. AJ AD < δx"
        },
        {
            "title": "Type",
            "content": "learned learned learned learned red square green square learned learned red square green square 10 1 10 3 3 3 3 1 1 1 4 4 0 2 2 2 2 0 0 0 512 512 512 512 512 512 256 256 256 256 47.53 42.85 32.71 40.51 21.37 30.44 37.00 21.85 15.00 19.91 8.73 9.82 11.98 9.72 18.25 12.72 11.62 20.55 23.53 19. avg OA OF1 60.74 80.87 60.20 78.55 41.45 79.28 50.06 80.34 27.21 75.38 19.10 76.89 81.10 57.84 53.10 78.03 18.22 76.63 36.53 78.31 64.83 59.72 49.20 58.57 36.31 47.37 52.82 34.34 26.30 32.73 Table 2. Ablation analysis TAP-Vid DAVIS First protocol. We evaluate multi-mask (MM) and multiscale (MS), in addition to comparing our optimized perturbations (learned) with the fixed ones (red square/green square) [4, 42]. MM and MS columns indicate the number of masking or zooming iterations. We observe clear improvement on all metrics, highlighting the need for bespoke, in-distribution counterfactual perturbations, multi-mask inference and multi-scale refinement. in the supplement, this scene-dependent perturbation is an emergent result of our training procedure that directly contributes to improvements in flow predictions. Resolution and additional refinements. In Table 2, we also observe that models with larger input resolution outperform those with smaller one, and that our multi-mask inference (MM) and multiscale refinement (MS) procedures significantly improve performance. traction scheme, and the flexible ability to train on unrestricted data that this approach enables, rather than the ViT architecture as such."
        },
        {
            "title": "Method",
            "content": "TAP-Vid CFG RAFT [38] SEA-RAFT [48]"
        },
        {
            "title": "DAVIS",
            "content": "AJ AD < δx avg OA OF1 69.69 69.89 1. 1.44 83.83 84.82 81.98 82.00 46. 47.52 SMURF [37] 65.75 2.40 79. 82.26 42.65 Opt-CWM Opt-CWM Distilled 69.53 70.51 1.19 1.30 83.15 82. 88.85 88.05 44.17 55.04 TAP-Vid First Main Benchmark RAFT [38] SEA-RAFT [48] 41.77 43. 25.33 20.18 54.37 58.69 66.40 66.34 56.12 56.23 SMURF [37] 30.64 27.28 44.18 59.15 46.91 Opt-CWM Opt-CWM Distilled 47.53 44.05 8.73 17.49 64.83 57.93 80.87 69.75 60.74 60.72 Table 3. Opt-CWM Distillation Results. To obtain fast test-time inference with small model, we distill Opt-CWM into the smaller and more efficient SEA-RAFT architecture by sparsely pseudolabeling Kinetics with Opt-CWM. This approach outpeforms the self-supervised SMURF and is competitive with the supervised RAFT models, while requiring no labeled training data. 4.4. Distillation into RAFT architecture 5. Conclusion & Future Work RAFT, SEA-RAFT, and SMURF use highly efficient but special-purpose flow architecture, rather than large general purpose ViTs [14]. To isolate the effect of this specific architecture design, we train RAFT-type architecture with pseudo-labels generated by Opt-CWM. Specifically, we take frame pair for each clip pseudo-label the motion for 1% of the pixels, and train SEA-RAFT architecture on this pseudo-labeled dataset. We present our findings in Table 3, and find that this distilled model outperforms the equivalent self-supervised baseline SMURF and is competitive with the supervised techniques. This outcome pinpoints that the core reason for Opt-CWMs improved performance is our contribution of the novel optimized counterfactual exWe have demonstrated the effectiveness of Opt-CWM in understanding motion concepts, achieving state-of-the-art performance on real-world benchmarks. Our paper takes an essential first step in demonstrating the potential of optimized counterfactuals for probing pre-trained video predictors. Opt-CWM paves the way for the next generation of scalable self-supervised point trackers, with potential extensions by training multi-frame predictors, scaling training data, or using different base predictor architectures like autoregressive generative video models. Equally importantly, our twin techniques of parameterizing the input-conditioned counterfactual generator and bootstrapping the learning of the generator parameters with end-to-end sparse prediction loss are generic and not flow-specificand may thus be extensible to optimizing highly performant CWM-style extraction of wide variety of visual quantities, including object segments, depth maps, and 3D shape [4, 42]. 6. Acknowledgements This work was supported by the following awards: To D.L.K.Y.: Simons Foundation grant 543061, National Science Foundation CAREER grant 1844724, National Science Foundation Grant NCS-FR 2123963, Office of Naval Research grant S5122, ONR MURI 00010802, ONR MURI S5847, and ONR MURI 1141386 - 493027. We also thank the Stanford HAI, Stanford Data Sciences and the Marlowe team, and the Google TPU Research Cloud team for computing support."
        },
        {
            "title": "References",
            "content": "[1] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: arXiv preprint Bert pre-training of image transformers. arXiv:2106.08254, 2021. 12 [2] Adrien Bardes, Jean Ponce, and Yann LeCun. Mc-jepa: joint-embedding predictive architecture for self-supervised arXiv preprint learning of motion and content features. arXiv:2307.12698, 2023. 2 [3] Daniel Bear, Elias Wang, Damian Mrowca, Felix Jedidja Binder, Hsiao-Yu Tung, RT Pramod, Cameron Holdaway, Sirui Tao, Kevin Smith, Fan-Yun Sun, et al. Physion: Evaluating physical prediction from vision in humans and machines. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 1), 2023. 1 [4] Daniel Bear, Kevin Feigelis, Honglin Chen, Wanhee Lee, Rahul Venkatesh, Klemen Kotar, Alex Durango, and Daniel LK Yamins. Unifying (machine) vision via counterfactual world modeling. arXiv preprint arXiv:2306.01828, 2023. 1, 3, 6, 8, 9, 12 [5] Homanga Bharadhwaj, Debidatta Dwibedi, Abhinav Gupta, Shubham Tulsiani, Carl Doersch, Ted Xiao, Dhruv Shah, Fei Xia, Dorsa Sadigh, and Sean Kirmani. Gen2act: Human video generation in novel scenarios enables generalizable robot manipulation. arXiv preprint arXiv:2409.16283, 2024. [6] Homanga Bharadhwaj, Roozbeh Mottaghi, Abhinav Gupta, and Shubham Tulsiani. Track2act: Predicting point tracks from internet videos enables generalizable robot manipulaIn European Conference on Computer Vision, pages tion. 306324. Springer, 2024. 1 [7] Zhangxing Bian, Allan Jabri, Alexei Efros, and Andrew Owens. Learning pixel trajectories with multiscale conIn Proceedings of the IEEE/CVF trastive random walks. Conference on Computer Vision and Pattern Recognition, pages 65086519, 2022. 1, 2 [8] D. J. Butler, J. Wulff, G. B. Stanley, and M. J. Black. naturalistic open source movie for optical flow evaluation. In European Conf. on Computer Vision (ECCV), pages 611 625. Springer-Verlag, 2012. 2 [9] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 7 [10] Mark Chen, Alec Radford, Rewon Child, Jeffrey Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In International conference on machine learning, pages 16911703. PMLR, 2020. 12 [11] Bowen Cheng, Ishan Misra, Alexander Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12901299, 2022. [12] Carl Doersch, Ankush Gupta, Larisa Markeeva, Adria Recasens, Lucas Smaira, Yusuf Aytar, Joao Carreira, Andrew Zisserman, and Yi Yang. Tap-vid: benchmark for tracking any point in video. Advances in Neural Information Processing Systems, 35:1361013626, 2022. 1, 2, 3, 5, 6 [13] Carl Doersch, Pauline Luc, Yi Yang, Dilara Gokay, Skanda Koppula, Ankush Gupta, Joseph Heyward, Ignacio Rocco, Ross Goroshin, Joao Carreira, et al. Bootstap: Bootstrapped training for tracking-any-point. In Proceedings of the Asian Conference on Computer Vision, pages 32573274, 2024. 2 [14] Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 8 [15] Alexey Dosovitskiy, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick Van Der Smagt, Daniel Cremers, and Thomas Brox. Flownet: Learning optical flow with convolutional networks. In Proceedings of the IEEE international conference on computer vision, pages 27582766, 2015. 1, 5 [16] Christoph Feichtenhofer, Haoqi Fan, Bo Xiong, Ross Girshick, and Kaiming He. large-scale study on unsupervised In Proceedings of spatiotemporal representation learning. the IEEE/CVF conference on computer vision and pattern recognition, pages 32993309, 2021. 1 [17] Daniel Geng, Charles Herrmann, Junhwa Hur, Forrester Cole, Serena Zhang, Tobias Pfaff, Tatiana Lopez-Guevara, Carl Doersch, Yusuf Aytar, Michael Rubinstein, et al. Motion prompting: Controlling video generation with motion trajectories. arXiv preprint arXiv:2412.02700, 2024. 1 [18] Priya Goyal, Piotr Dollar, Ross Girshick, Pieter Noordhuis, Lukasz Wesolowski, Aapo Kyrola, Andrew Tulloch, large miniYangqing Jia, and Kaiming He. Accurate, arXiv preprint batch sgd: Training imagenet in 1 hour. arXiv:1706.02677, 2017. 12 [19] Klaus Greff, Francois Belletti, Lucas Beyer, Carl Doersch, Yilun Du, Daniel Duckworth, David Fleet, Dan Gnanapragasam, Florian Golemo, Charles Herrmann, et al. Kubric: scalable dataset generator. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 37493761, 2022. 5, 7 [20] Adam Harley, Zhaoyuan Fang, and Katerina Fragkiadaki. Particle video revisited: Tracking through occlusions using point trajectories. In European Conference on Computer Vision, pages 5975. Springer, 2022. 1, [21] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 12 [22] Allan Jabri, Andrew Owens, and Alexei Efros. Space-time correspondence as contrastive random walk. Advances in neural information processing systems, 33:1954519560, 2020. 1, 2 [23] Wei Jiang, Eduard Trulls, Jan Hosang, Andrea Tagliasacchi, and Kwang Moo Yi. Cotr: Correspondence transformer for matching across images. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 6207 6217, 2021. 5 [24] Zhenyu Jiang, Hanwen Jiang, and Yuke Zhu. Doduo: Learning dense visual correspondence from unsupervised In 2024 IEEE International Confersemantic-aware flow. ence on Robotics and Automation (ICRA), pages 12420 12427. IEEE, 2024. 1, 2, 3, 6, 7 [25] Rico Jonschkowski, Austin Stone, Jonathan Barron, Ariel Gordon, Kurt Konolige, and Anelia Angelova. What matters in unsupervised optical flow. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 557572. Springer, 2020. 2 [26] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudoarXiv preprint arXiv:2410.11831, labelling real videos. 2024. [27] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang, Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola, Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman, and Andrew Zisserman. The kinetics human action video dataset, 2017. 1, 3, 5, 12 [28] Liang Liu, Jiangning Zhang, Ruifei He, Yong Liu, Yabiao Wang, Ying Tai, Donghao Luo, Chengjie Wang, Jilin Li, and Feiyue Huang. Learning by analogy: Reliable supervision from transformations for unsupervised optical flow esIn Proceedings of the IEEE/CVF conference on timation. computer vision and pattern recognition, pages 64896498, 2020. 2 [29] Ilya Loshchilov and Frank Hutter. tic gradient descent with warm restarts. arXiv:1608.03983, 2016. 12 Sgdr: StochasarXiv preprint [30] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 12 [31] N. Mayer, E. Ilg, P. Hausser, P. Fischer, D. Cremers, A. Dosovitskiy, and T. Brox. large dataset to train convolutional networks for disparity, optical flow, and scene In IEEE International Conference on flow estimation. Computer Vision and Pattern Recognition (CVPR), 2016. arXiv:1512.02134. 1, 2 [32] Lukas Mehl, Jenny Schmalfuss, Azin Jahedi, Yaroslava Nalivayko, and Andres Bruhn. Spring: high-resolution highdetail dataset and benchmark for scene flow, optical flow In Proceedings of the IEEE/CVF Conference and stereo. on Computer Vision and Pattern Recognition, pages 4981 4991, 2023. 1 [33] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation, 2018. 5 [34] Rui Qian, Tianjian Meng, Boqing Gong, Ming-Hsuan Yang, Huisheng Wang, Serge Belongie, and Yin Cui. Spatiotemporal contrastive video representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 69646974, 2021. [35] Ayush Shrivastava and Andrew Owens. Self-supervised anyIn European point tracking by contrastive random walks. Conference on Computer Vision (ECCV), 2024. 2, 3, 6 [36] Khurram Soomro, Amir Roshan Zamir, and Mubarak Shah. Ucf101: dataset of 101 human actions classes from videos in the wild. arXiv preprint arXiv:1212.0402, 2012. 1 [37] Austin Stone, Daniel Maurer, Alper Ayvaci, Anelia Angelova, and Rico Jonschkowski. Smurf: Self-teaching multiIn Proframe unsupervised raft with full-image warping. ceedings of the IEEE/CVF conference on Computer Vision and Pattern Recognition, pages 38873896, 2021. 1, 2, 6, 8 [38] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. 1, 2, 6, 7, 8 [39] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang. Videomae: Masked autoencoders are data-efficient learners for self-supervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. 1, 3, 12 [40] Hsiao-Yu Tung, Mingyu Ding, Zhenfang Chen, Daniel Bear, Chuang Gan, Josh Tenenbaum, Dan Yamins, Judith Fan, and Kevin Smith. Physion++: Evaluating physical scene understanding that requires online inference of different physical properties. Advances in Neural Information Processing Systems, 36, 2024. 1 [41] Mel Vecerik, Carl Doersch, Yi Yang, Todor Davchev, Yusuf Aytar, Guangyao Zhou, Raia Hadsell, Lourdes Agapito, and Jon Scholz. Robotap: Tracking arbitrary points for few-shot visual imitation. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pages 53975403. IEEE, 2024. 1 [42] Rahul Venkatesh, Honglin Chen, Kevin Feigelis, Daniel Bear, Khaled Jedoui, Klemen Kotar, Felix Binder, Wanhee Lee, Sherry Liu, Kevin Smith, et al. Understanding physical dynamics with counterfactual world modeling. arXiv preprint arXiv:2312.06721, 2023. 1, 2, 3, 6, 8, 9, 12 [43] Carl Vondrick, Abhinav Shrivastava, Alireza Fathi, Sergio Guadarrama, and Kevin Murphy. Tracking emerges by colorizing videos. In Proceedings of the European conference on computer vision (ECCV), pages 391408, 2018. 2 [44] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal segment networks for action recognition in videos. IEEE transactions on pattern analysis and machine intelligence, 41(11):2740 2755, 2018. [45] Limin Wang, Bingkun Huang, Zhiyu Zhao, Zhan Tong, Yinan He, Yi Wang, Yali Wang, and Yu Qiao. Videomae v2: Scaling video masked autoencoders with dual masking. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1454914560, 2023. 12 [46] Qianqian Wang, Xiaowei Zhou, Bharath Hariharan, and Noah Snavely. Learning feature descriptors using camera In Computer VisionECCV 2020: 16th pose supervision. European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part 16, pages 757774. Springer, 2020. 4 [47] Wenshan Wang, Delong Zhu, Xiangwei Wang, Yaoyu Hu, Yuheng Qiu, Chen Wang, Yafei Hu, Ashish Kapoor, and Sebastian Scherer. Tartanair: dataset to push the limits of In 2020 IEEE/RSJ International Conference visual slam. on Intelligent Robots and Systems (IROS), pages 49094916. IEEE, 2020. 7 [48] Yihan Wang, Lahav Lipson, and Jia Deng. Sea-raft: Simple, efficient, accurate raft for optical flow. arXiv preprint arXiv:2405.14793, 2024. 1, 2, 6, 7, 8 [49] Jiarui Xu and Xiaolong Wang. Rethinking self-supervised correspondence learning: video frame-level similarity perIn Proceedings of the IEEE/CVF International spective. Conference on Computer Vision, pages 1007510085, 2021. 2 [50] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. Youtube-vos: large-scale video object segmentation benchmark. arXiv preprint arXiv:1809.03327, 2018. 7 [51] Chengbo Yuan, Chuan Wen, Tong Zhang, and Yang Gao. General flow as foundation affordance for scalable robot In 8th Annual Conference on Robot Learning, learning. 2024. [52] Miao Zhang, Jie Liu, Yifei Wang, Yongri Piao, Shunyu Yao, Wei Ji, Jingjing Li, Huchuan Lu, and Zhongxuan Luo. Dynamic context-sensitive filtering network for video salient In Proceedings of the IEEE/CVF interobject detection. national conference on computer vision, pages 15531563, 2021. 1 [53] Yi Zhou, Guillermo Gallego, Xiuyuan Lu, Siqi Liu, and Shaojie Shen. Event-based motion segmentation with spatiotemporal graph cuts. IEEE transactions on neural networks and learning systems, 34(8):48684880, 2021. 1 A. Implementation Details A.1. Architecture Details A.1.1. ΨRGB The input video is first divided into non-overlapping spatiotemporal patches of size 8 8, with subset of patches masked. Unlike MAE, we train with both revealed input patches and mask tokens provided to the encoder. We train with the ViT-B architecture [21] with each transformer block consisting of multi-head self-attention block and an MLP block, both using LayerNorm (LN). The CWM decoder has the same architecture as the encoder. Each spatiotemporal patch has learnable positional embedding which is added to both the encoder and decoder inputs. CWM does not use relative position or layer scaling [1, 21]. Please refer to [4, 42] for more details on the architecture Default settings We show the default pre-training settings in Table 4. CWM does not use color jittering, drop path, or gradient clip. Following ViTs official code, Xavier uniform is used to initialize all transformer blocks. The learnable masked token is initialized as zero tensor. Following MAE, we use the linear lr scaling rule: lr = base lr batch size / 256 [21]. Table 4. Default pre-training setting of CWM config value optimizer base learning rate weight decay optimizer momentum accumulative batch size learning rate schedule warmup epochs [18] total epochs flip augmentation augmentation AdamW [30] 1.5e-4 0.05 β1, β2 = 0.9, 0.95 [10] 4096 cosine decay [29] 40 800 no MultiScaleCrop [44] A.1.2. Ψflow The architecture of the flow-conditioned predictor, Ψflow, is vision transformer with 16 layers and 132M parameters. Input images are resized to 224x224, and the patch size is 8. Sinusoidal positional encodings are used. For the encoder, the embedding dimension is 768, and 12 attention heads are used. For the decoder, the embedding dimension is 384, and 6 attention heads are used. This model has two parallel streams, the first of which takes RGB input and the second of which takes sparse flow, concatenated with RGB (which is masked to have the same sparsity as the flow), as input. All RGB inputs are from the first frame only; this requires the model to depend solely on flow to modify the RGB and predict the next frame. Mask Type Train % Test % AJ AD < δx 0-90 tube 0-90 tube 0-90 tube 0-90 random 0-75 random 0-90 random 55-55 75-75 90-90 75-75 75-75 0-90 15.61 15.86 18.57 14.64 12.79 11.62 23.94 22.55 15.23 29.09 34.06 37.00 36.90 39.63 32.12 42.57 47.54 52.82 avg OA OF1 52.36 72.19 52.27 58.20 49.20 51.98 57.06 73.51 60.81 76.07 57.80 81.10 Table 5. Ablation analysis of training-time masking policy on TAP-Vid DAVIS First. We train ΨRGB with different non-temporally factored masking policies more similar to VideoMAE [39, 45]. The notation of 55-55 indicates 55% of patches are masked in the first frame and 55% are masked in the second frame. Tube masking selects patches at the same spatial location over time, whereas random independently samples patches in each frame. MAE-style masking during training is strictly worse than the temporally-factored masking policy we use as the standard in this paper (shown for reference in the bottom row). All experiments here use 256x256 resolution, MM-3 and MS-2. The transformer architecture then applies self-attention to each stream and cross-attention between streams. The encoder has 12 layers, split into three groups of 4. In each group, there is one layer with self attention on each stream and cross attention from each stream to the other, followed by three layers with only self-attention on the first stream. The decoder has 4 layers; the first applies self-attention to each stream and cross-attention from each stream to the other; the second applies self-attention to the first stream and cross-attention from the second stream to the first; and the final two only apply self-attention to the first stream. A.2. Training Details A.2.1. ΨRGB We train CWM at 256 resolution for 800 epochs and finetune at 512 resolution for 100 epochs by interpolating the positional embeddings. It takes approximately 4 days to train 800 epochs on TPU v5-128 pod. We pre-train CWM on the Kinetics-400 dataset [27], without requiring any specialized temporal downsampling. Applying the temporal masking strategy, i.e. fully revealing the first frame and partially revealing the second frame, during the pre-training of ΨRGB contributes significantly to downstream flow prediction quality, compared to the baseline Video-MAE-style masking policies  (Table 5)  . The asymmetry forces ΨRGB to separate scene appearance from dynamics, as discussed in Section 3.1 in the main text. A.2.2. Ψflow and FLOWθ We train Ψflow and FLOWθ jointly using an AdamW optimizer with weight decay of 0.05, betas of (0.9, 0.95), and learning rate schedule with max learning rate 1.875 105, 40 warmup epochs (10% of total training epochs), and cosine decay. We used batch size of 32. Masking Ratio AJ AD < δx avg OA OF1 50% 60% 70% 80% 85% 90% (Ref.) 95% 98% 42.78 43.28 43.25 42.68 41.99 40.51 37.68 32.85 10.52 10.12 9.72 9.44 9.57 9.72 10.57 13.15 58.78 59.56 59.95 59.76 59.58 58.57 55.87 50.48 79.18 80.33 81.24 81.64 80.92 80.34 79.63 78. 60.68 60.80 59.68 57.53 54.06 50.06 45.00 41.42 Table 6. Ablation analysis of test-time masking policy on TAPVid DAVIS First. We evaluate 512 resolution ΨRGB across various masking ratios for the second frame using the MM-3 and MS-2 setting. The standard masking ratio for all results in this work is included as 90% (Ref.) in this table. crop of the input frames with original height and width . We center the second frame crop on the location predicted by the previous iteration. The transformer-based architecture of the next frame predictor ΨRGB imposes limit to the input resolution, which may occasionally prevent small objects or minute features of the input frame from being accurately reconstructed in great detail. Multiscale refinement of the initial flow prediction can be greatly beneficial under these circumstances. However, Figure 6 suggests that the improvement is not monotonic; indeed, excessive cropping may lead to the loss of global context that is necessary to accurately reconstruct the scene. Opt-CWM is run on 4 zoom iterations, which we have empirically found to be optimal. A.3.3. Occlusion Estimation The difference image can also be used to predict whether visible point becomes occluded in the next frame. Conceptually, as described in Section 3 in the main text, when point becomes occluded, the counterfactual perturbation placed on the object should not be reconstructed in the second frame. Thus, while we take argmax to compute flow, we can instead use max as signal for occlusion. In particular, we compare max to some threshold tocc to predict occlusion (i.e., we consider the model to have predicted that point becomes occluded if and only if max < tocc). In the multi-masking setting with 10 masking iterations, we have 10 difference images: 1, 2, ..., 10. Instead of thresholding the average, avg, we can get an improved signal by considering max for each = 1, ..., 10. In this setting, we found that checking 1 i=1 max < 0.05 10 provided good signal for predicting occlusion, and this prediction criterion is what was evaluated in the OA and OF1 metrics of Table 1 in the main paper. (cid:80)10 Figure 6. < δavg broken down across thresholds (x-axis). Fraction of points with error less than fixed threshold, as function of number of multiscale (MS) iterations, for pixel thresholds 1, 2, 4, 8, and 16. We find that 4 zoom iterations tends to perform the best, especially for robustness on difficult examples (evidenced by better performance on higher thresholds). A.3. Inference Techniques A.3.1. Multi-Mask In the process of computing flows in FLOWθ, at inference time, we take an argmax over the difference between the predicted next frame with and without the counterfactual perturbation. This difference image, , depends on the choice of the random mask as this mask is used by ΨRGB for the next-frame reconstruction. As discussed in the main text, if random mask reveals patches too close to where the perturbation should be reconstructed, the predictor ΨRGB may not reconstruct the perturbation properly, and the difference image will be noisy and diffuse, preventing the model from accurately predicting the next-frame location. Additionally, the reconstructed pixels will not necessarily be the same across different random samplings of visible patches, which may add random noise to the difference image. Both of these issues are ameliorated by our multi-mask technique, in which we compute difference images for variety of sampled random masks (we found 10 to be good number of masks for multi-masking), average over the difference images, and then take the argmax of this averaged avg to compute next-frame location for determining flow. A.3.2. Multiscale Multiscale refinement of the original flow prediction improves Opt-CWMs performance, as observed in Figure 6. Given an input frame pair and an initial flow prediction, we perform iterative multiscaling through the following procedure. At each zoom iteration, we take 0.75H 0.75W B. Additional Quantitative Results B.1. Masking Ratio at Inference The amount of next-frame information to provide ΨRGB when making counterfactual predictions is an important hyB.4. Perturbation Across Epochs The performance of FLOWθ is greatly dependent on the quality of its learned Gaussian perturbations. In Figure 10, we see that the appearance of the perturbation evolves alongside the training of Opt-CWM. As the perturbation converges into an optimal patch bespoke for the input frame, the quality of the flow prediction improves in tandem. perparameter for downstream performance. Intuitively, decreasing the masking ratio (i.e. revealing more next-frame patches) will improve reconstruction quality. This can improve flow prediction by focusing the source of noise in the delta image on the carried over perturbation (and not on spurious noise induced by poor reconstruction quality). On the other hand, revealing too many patches may uncover the ground truth appearance of the perturbed patch in the next frame, in which case the perturbation will not be reconstructed at all. We investigate this tradeoff in Table 6. B.2. Performance as Function of Frame Gap Figure 7 compares flow estimation performance as function of frame gap for Opt-CWM and three baselines. OptCWM and Doduo maintain performance as frame gap increases more than SEA-RAFT or SMURF. Doduo is trained on larger frame gaps (1-3 seconds) than SEA-RAFT, SMURF, or Opt-CWM (< 500ms), which accounts for its strong performance as frame gap increases. Despite this difference, Opt-CWM is competitive with Doduo on larger frame gaps. TAP-Vid First results are averaged over frame gaps ranging from 1 to the length of the clip (which can extend up to 50-100 frames depending on the video), and Opt-CWM significantly outperforms Doduo on all metrics. Larger frame gaps often entail greater magnitudes of object and camera motion, and therefore Opt-CWMs high performance as evidenced in Figure 7 suggests robustness to challenging scene dynamics. B.3. Precision Analysis Figure 8 attempts to explain the high performance of OptCWM on TAP-Vid First through similar analysis done in Section A.3.2. Our best-performing model (with optimal inference-time configurations) is able to predict the next frame location within 16 pixels of the ground truth for over 85% of the total number of visible points. Unlike baseline models, Opt-CWM is able to predict most points within reasonable boundary. Further, Opt-CWM predictions are precise; it predicts the majority of the query points within 2 pixels of the ground truth. While SEA-RAFT, which is supervised, is also precise for lower thresholds, the magnitude of the error for wrong predictions is evidently higher, as its performance quickly plateaus for higher thresholds. As discussed in Section 4 in the main paper, we further evaluate on custom constant-frame gap protocol (CFG) for fairer comparison with optical flow baselines. As shown here in Figure 9, all models improve significantly under this less challenging setup. In particular, optical flow baselines exhibit strong sub pixel precision. However, we see that in general, compared to self-supervised baselines, Opt-CWM make reasonable predictions of points next frame location more often, at rate comparable to the fully supervised SEA-RAFT. Figure 7. Model comparison as function of frame gap. Higher frame gaps present harder flow estimation problems due to including more motion, as reflected by improved performance across models in lower frame gap settings. Opt-CWM and Doduo perform better as frame gap increases, in contrast to optical flow methods SEA-RAFT and SMURF which decay in performance as motion magnitude increases, especially on the AD metric. Figure 8. TAP-Vid First: comparing baseline models on < δavg broken down across thresholds (x-axis). Fraction of points with error less than fixed threshold, as function of baseline model. Compared to baseline models, Opt-CWM maintains high performance on all thresholds even when making predictions across large frame gaps, as is necessary for TAP-Vid First. Figure 9. TAP-Vid CFG: comparing baseline models on < δavg broken down across thresholds (x-axis). Fraction of points with error less than fixed threshold, as function of baseline model. For fair comparison, we also evaluate on constant frame gap setting that is more favorable to optical flow baselines. While baseline methods show strong performance for very low thresholds (< 2 pixels), we see that in general Opt-CWM outperforms self-supervised methods and is comparable with SEA-RAFT in predicting more points within reasonable boundary. Figure 10. Evolution of perturbations across training epochs: We observe how the predicted perturbations change as the model trains. The perturbation starts as disjoint streak of colors and converges to localized peak. This in turn increasingly concentrates the difference image and leads to better flow prediction. Green is the ground truth flow obtained from the TAP-Vid dataset, and blue is our models prediction."
        }
    ],
    "affiliations": [
        "Stanford University"
    ]
}