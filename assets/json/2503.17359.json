{
    "paper_title": "Position: Interactive Generative Video as Next-Generation Game Engine",
    "authors": [
        "Jiwen Yu",
        "Yiran Qin",
        "Haoxuan Che",
        "Quande Liu",
        "Xintao Wang",
        "Pengfei Wan",
        "Di Zhang",
        "Xihui Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Modern game development faces significant challenges in creativity and cost due to predetermined content in traditional game engines. Recent breakthroughs in video generation models, capable of synthesizing realistic and interactive virtual environments, present an opportunity to revolutionize game creation. In this position paper, we propose Interactive Generative Video (IGV) as the foundation for Generative Game Engines (GGE), enabling unlimited novel content generation in next-generation gaming. GGE leverages IGV's unique strengths in unlimited high-quality content synthesis, physics-aware world modeling, user-controlled interactivity, long-term memory capabilities, and causal reasoning. We present a comprehensive framework detailing GGE's core modules and a hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work charts a new course for game development in the AI era, envisioning a future where AI-powered generative systems fundamentally reshape how games are created and experienced."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 2 ] . [ 1 9 5 3 7 1 . 3 0 5 2 : r Position: Interactive Generative Video as Next-Generation Game Engine Jiwen Yu 1 * Yiran Qin 1 * Haoxuan Che 2 Quande Liu 3 Xintao Wang 3 Pengfei Wan 3 Di Zhang 3 Xihui Liu"
        },
        {
            "title": "Abstract",
            "content": "Modern game development faces significant challenges in creativity and cost due to predetermined content in traditional game engines. Recent breakthroughs in video generation models, capable of synthesizing realistic and interactive virtual environments, present an opportunity to revolutionize game creation. In this position paper, we propose Interactive Generative Video (IGV) as the foundation for Generative Game Engines (GGE), enabling unlimited novel content generation in next-generation gaming. GGE leverages IGVs unique strengths in unlimited high-quality content synthesis, physics-aware world modeling, usercontrolled interactivity, long-term memory capabilities, and causal reasoning. We present comprehensive framework detailing GGEs core modules and hierarchical maturity roadmap (L0-L4) to guide its evolution. Our work charts new course for game development in the AI era, envisioning future where AI-powered generative systems fundamentally reshape how games are created and experienced. 1. Introduction Computer games have witnessed an ever-growing market demand, yet the gaming industry faces three critical challenges. First, current game engines rely heavily on pre-made assets and fixed logic scripts, leading to predetermined content that players will eventually exhaust, even in modern openworld games. Second, existing game engines cannot provide adaptive, personalized gaming content tailored to individual players preferences, habits, and backgrounds. Third, developing high-quality games, especially AAA games, requires substantial human resources and extensive development time. How to rapidly create high-quality games with unlimited personalized content while minimizing costs remains fundamental challenge for the entire gaming industry. *Equal contribution 1The University of Hong Kong 2The Hong Kong University of Science and Technology 3Kuaishou Xintao Wang <xinTechnology. tao.wang@outlook.com>, Xihui Liu <xihuiliu@eee.hku.hk>. Correspondence to: During the past year, video generation models have made remarkable progress (OpenAI, 2024; Lab & etc., 2024; Zheng et al., 2024; Chen et al., 2024b; Yang et al., 2024e), demonstrating unprecedented capabilities in large-scale motion dynamics, semantic understanding and concept composition, 3D consistency with physical laws, and long-term temporal coherence in both object structure and appearance. These advances show great potential for effectively simulating real-world physics (OpenAI, 2024; Yang et al., 2024d; 2023), suggesting that these models could serve as capable world models for generating physically plausible videos. Building upon these advances in video generation, we propose Interactive Generative Video (IGV), new paradigm that extends video generation capabilities with interactive features. IGV centers around video generation while incorporating four key characteristics: user control over the generated content, memory of video context, understanding and simulation of physical rules, and causal reasoning intelligence. By combining these elements, IGV effectively constructs an explorable and interactive virtual world through video generation, functioning similarly to simulator. The virtual worlds created by IGV naturally align with video games as they provide interactive environments where players can explore and engage with dynamically generated content, representing promising direction for next-generation gaming. Recent works (Bruce et al., 2024; DeepMind, 2024; Alonso et al., 2024; Valevski et al., 2024; Che et al., 2024; Feng et al., 2024; Yang et al., 2024b; Yu et al., 2025; Decart, 2024) have demonstrated this potential by training actionconditioned video generation models using action-video pairs collected from classic games like Atari (Alonso et al., 2024), DOOM (Valevski et al., 2024; Yang et al., 2024b), CS:GO (Alonso et al., 2024), Minecraft (Yu et al., 2025; Decart, 2024), and Super Mario Bros (Yang et al., 2024b). These models create interactive gaming experiences by iteratively generating predicted video frames in response to user action inputs. However, as pointed out by some works (Yu et al., 2025; Feng et al., 2024; DeepMind, 2024), merely replicating existing games through IGV offers limited value over traditional game engines. The revolutionary potential of IGV lies in its ability to create infinite entirely new games through its powerful generative capabilities. Imagine future where Position: Interactive Generative Video as Next-Generation Game Engine everyone can become game designer, creating their own games by simply providing design instructions to video generation models, which then generate limitless explorable virtual worlds. This will fundamentally transform both game development and gaming experiences. In conclusion, this position paper argues that Interactive Generative Video (IGV) serves as the core technology for Generative Game Engine (GGE). GGE will significantly reduce technical barriers in game development while boosting development productivity and creativity through infinite AI-driven content generation. In this position paper, we first introduce essential preliminary knowledge about video generation and AI-driven game applications in Sec. 2. Sec. 3 analyzes the core capabilities required for next-generation Generative Game Engines (GGE) and demonstrates why Interactive Generative Video (IGV) is uniquely positioned to fulfill these requirements. Sec. 4 presents our comprehensive framework for GGE, providing detailed definitions, analysis, and future prospects for each module within the framework. To guide future research and development, Sec. 5 proposes hierarchical roadmap that outlines progressive milestones toward fully functional GGE systems. Finally, Sec. 6 and Sec. 7 discuss alternative perspectives and provide concluding remarks. 2. Preliminaries 2.1. Video Generation Models Video Generation Models aim to synthesize temporally consistent video sequences = {x0, x1, . . . , xt}, where xi indicates the i-th frame. The video is generated by an autoregressive model (Razavi et al., 2019), diffusion model (Ho et al., 2022), or masked transformer model (Chang et al., 2022). With the rise of diffusion models (Ho et al., 2020; Song et al., 2021; Song & Ermon, 2019), which have now become the mainstream approach in video generation due to its high-quality generation performance, significant progress has been achieved in this field (Chen et al., 2024b; Blattmann et al., 2023; Zheng et al., 2024; Lab & etc., 2024; Yang et al., 2024e; Peebles & Xie, 2023; OpenAI, 2024). Conditional Video Generation is formulated as p(xc) and it varies depending on the type of control signal which denotes conditions such as text prompts or other control signal. Approaches such as (Guo et al., 2025; Ni et al., 2023; Xing et al., 2023) incorporate images as control signals for the video generator, improving both video quality and temporal relationship modeling. Methods such as Direct-aVideo (Yang et al., 2024c), MotionCtrl (Wang et al., 2024b), and CameraCtrl (He et al., 2024) use camera embedders to adjust camera poses, enabling control over camera movements in generated videos, while (Fu et al., 2025) extends this capability by transforming 2D camera signals into 3D for more advanced control. Autoregressive Video Generation Models. Since game videos require variable-length or even infinite-length generation to enable interactive game experience, autoregressive mechanism is necessary. An intuitive approach is to adopt GPT-like next-token prediction methods (Kondratyuk et al., 2023; Deng et al., 2024; Wang et al., 2024a); however, this approach often falls short in terms of generation quality. Relying on the exceptional performance of diffusion models, Diffusion Forcing (Chen et al., 2024a) implements autoregression by applying different levels of noise to different frames, allowing the denoising of new frames with higher noise levels while conditioning on previous frames with lower noise levels. Methods (Yu et al., 2025; Feng et al., 2024; Valevski et al., 2024; Decart, 2024) leveraging Diffusion Forcing has achieved remarkable results. 2.2. AI-driven Game Applications Game Video Generation. Previous works have utilized GANs (Kim et al., 2020; 2021; Menapace et al., 2021) to generate game videos or used NeRF to reconstruct 3D scenes to simulate the game process (Menapace et al., 2022; 2024), but often fall short in terms of generation quality. Using the powerful generative capabilities of diffusion models, some works (Alonso et al., 2024; Valevski et al., 2024; Yang et al., 2024b; Decart, 2024; Feng et al., 2024) have produced high-quality game videos. However, the content is typically confined to specific preexisting games. Opendomain Methods (DeepMind, 2024; Feng et al., 2024; Yu et al., 2025), by utilizing multi-stage training or large-scale training datasets, create new content for video games. Game Design Assistant. AI-powered design assistants offer numerous advantages throughout the creative process, depending on the tool, the type of AI and the creative workflow. These systems can streamline development, reduce costs, reduce manual effort, improve team collaboration, and even inspire creativity (Liapis, 2015). In the gaming domain, most existing AI-driven design tools primarily assist by auto-completing an ongoing design (Smith et al., 2011) or generating multiple design suggestions for creators to evaluate (Liapis et al., 2014; Migkotzidis & Liapis, 2021; Charity et al., 2022; Torii et al., 2023). Intelligent Game Agent. Reinforcement learning has long been the predominant approach in this domain. Early efforts explored the use of hierarchical RL (Cai et al., 2023; Jiang et al., 2025; Fan et al., 2022; Lin et al., 2021; Oh et al., 2017; Zhou et al., 2024b; BAAI, 2023) in the context of MineRL competitions (Guss et al., 2019). However, due to the absence of guidance from prior knowledge, such approaches often struggle to perform effectively on longhorizon tasks. With the advancement of large language models (Achiam et al., 2023; Touvron et al., 2023), lever2 Position: Interactive Generative Video as Next-Generation Game Engine Figure 1. Demonstration of GameFactory (Yu et al., 2025)s ability to generalize action control capabilities learned from Minecraft data to open-domain scenarios. Examples from its homepage showcase various generalized environments where the learned control mechanisms have been successfully applied. aging their prior knowledge to plan long-horizon tasks has shown promising results. Recent advancements in large language model-related research (Wang et al., 2023a;c; Zhu et al., 2023; Qin et al., 2024b; Yang et al., 2025; Huang et al., 2022) have significantly propelled the progress of agents in long-horizon tasks. 3. Why IGV for Generative Game Engine? Computer games have witnessed an ever-growing market demand, yet developing high-quality games, especially AAA games, requires substantial human resources and extensive development time. Traditional game engines like Unreal and Unity rely heavily on pre-made assets and fixed logic scripts, which not only limits game creation to predefined scenes and plots, but also means players will eventually exhaust all content. Even in open-world games like The Legend of Zelda: Breath of the Wild, while offering extensive freedom, players will ultimately experience all predetermined content. How to rapidly create high-quality and innovative games at scale while minimizing human costs remains critical challenge for the entire gaming industry. We propose Generative Game Engine (GGE) as nextgeneration solution that dynamically generates both assets and logic. This paradigm shift offers several key advantages: (1) lower development costs for game studios through automated content generation; (2) reduced entry barriers for individual developers by eliminating the need for extensive asset creation; and (3) truly open-world experiences with unlimited, dynamically generated content that provides endless unique gameplay experiences. Building upon recent advances in video generation, we propose Interactive Generative Video (IGV) as promising foundation for GGE implementation. IGV extends traditional video generation with three key characteristics that 3 Figure 2. Physics-aware generation capabilities of video models. Top: Examples from Cosmos (NVIDIA, 2025) demonstrating physical understanding in diverse scenarios including robotics, autonomous driving, manufacturing, and home environments. Bottom: Human motion examples generated by Kling. align with GGE requirements: (1) powerful and generalizable generative capabilities for creating unlimited new game content, (2) physics-aware world modeling for realistic interactions, and (3) user-controlled generation for interactive experiences. Beyond these technical alignments, IGV offers another unique advantage: it can leverage the vast amount of existing video data for large-scale model training, enabling robust and diverse content generation. 3.1. Generalizable Generation for Unlimited Games Video generation models demonstrate exceptional capabilities in creating not just high-quality visual content, but more importantly, unlimited novel game content that has never existed before. By learning from vast collections of real-world video data, these models develop deep understanding of visual elements and their relationships, enabling them to generalize beyond specific training game data. Unlike traditional asset creation which is limited to predefined content, these video generation models can continuously generate new, coherent, and diverse game elements by creatively combining and transforming learned patterns. As demonstrated in Figure 1, GameFactory (Yu et al., 2025) is capable of generating action-controllable game videos that can generalize to open-domain scenarios. 3.2. Physics-aware World Modeling Video generation models demonstrate remarkable potential in understanding the inherent rules of the real world, particularly physical knowledge. During training, to ensure accurate video prediction, these models naturally learn implicit physical priors embedded in training videos. These priors encompass various common physical phenomena, including gravity, elasticity, explosions, collisions, as well as complex motion patterns of humans and animals. While traPosition: Interactive Generative Video as Next-Generation Game Engine Figure 3. Proposed framework of Generative Game Engine (GGE). (a) Framework of GGE shows the architecture and interactions between modules. (b) Technical keywords of each module. The game examples shown in gray boxes demonstrate typical applications of each modules capabilities, with detailed analysis provided in the main text. ditional game engines typically rely on predefined physical formulas, extensive manual annotations, or motion capture, IGV leverages its learned physical priors to directly generate physically plausible content. This capability significantly simplifies game engine design and reduces the technical expertise required from developers, thereby enhancing game production efficiency. As shown in Figure 2, the generated video examples demonstrate IGVs physics-aware capabilities, highlighting its potential value for game development. 3.3. Interactive Generation with User Control Current video generation models support various types of control signals that are essential for gaming interactions. These control capabilities are particularly effective in intuitive operations such as camera viewpoint adjustment and character movement control. Such precise and responsive control enables players to naturally interact with the generated content, creating engaging gaming experiences. With rapid development in this field, an increasing number of control signal types are being supported, further expanding interactive possibilities. As shown in Fig. 4, these examples demonstrate IGVs strong interactive control capabilities and validate its potential for game development. 3.4. Video Data Accessibility Enables Scaling Video data offers unique advantages for training generative game engines through its accessibility and unified representation format. Unlike traditional game engines that require various heterogeneous assets (3D models, textures, animations, etc.) with substantial manual effort, videos are widely available across internet platforms and continuously growing through social media and streaming services. Moreover, videos naturally capture diverse real-world phenomena and human experiences, enabling models to learn comprehenFigure 4. Character from GameNGen (Valevski et al., 2024), showing interactive gameplay operations in generated videos. control demonstrations sive world knowledge through large-scale training. This abundant video data facilitates training powerful video generation models at scale, while using video as unified representation simplifies the development process by avoiding the complexity of managing different asset formats, making it an ideal foundation for generative game development. 4. Framework of Generative Game Engine 4.1. Overview We decompose our IGV-centered Generative Game Engine into five functional modules, as illustrated in Figure 3. The framework diagram (a) demonstrates the relationships between these modules, while (b) presents their key technical IGV consists of components and application examples. five core modules: First, the Generation module represents the basic generative capability of the video generation model. Four extension modules are built upon it: the Control module supports different modal control signals and is key to achieving interactivity; the Memory module maintains historical generation content from both dynamic and static aspects, crucial for ensuring temporal consistency; 4 Position: Interactive Generative Video as Next-Generation Game Engine the Dynamics module models the internal rule logic of the games virtual world, especially physical rules; while the Intelligence module enables advanced capabilities including causal reasoning and self-evolution. These five modules, through their video interface, create an independent virtual world with its own emergent properties and behaviors. However, virtual world alone does not constitute complete game experience, as games require external rules that embody game designers intentions, providing players with clear objectives and feedback that create gaming enjoyment. Therefore, we propose an additional GamePlay module based on IGV, which is responsible for implementing these external rule logic within the virtual game world. 4.2. Generation Concept. The Generation Module handles video generation, the fundamental functionality of IGV. While ensuring basic video generation requirements like visual quality and motion coherence, this module encompasses three crucial functionalities to achieve optimal interactive experience: (1) Autoregressive Generation enables continuous video synthesis with frame-level control frequency. This supports endless procedural worlds in No Mans Sky where players can seamlessly explore for hundreds of hours, realtime weather and day-night cycles in Red Dead Redemption 2 that evolve continuously, and instant response to rapid player inputs in rhythm games like Beat Saber where every frame matters. (2) Real-time Processing facilitates low-latency interaction with users. This is essential in competitive games like Counter-Strike, Forza Motorsport, and League of Legends where instant visual feedback is crucial. (3) Multi-modal Generation complements the video output with other modalities like text and audio. This includes dynamic music that responds to gameplay in Journey, positional audio cues for enemy locations in PUBG, ambient sound effects in Minecraft, and real-time dialogue subtitles in Mass Effect. Technical Approaches and Future Directions. (1) Autoregressive Generation: Diffusion-based methods excel at generating high-quality visual content. simple way to achieve autoregressive generation is to use different noise levels across frames. Key methods like diffusion forcing (Chen et al., 2024a) and rolling diffusion (Ruhe et al., 2024) have been widely used in game video generation (Feng et al., 2024; Valevski et al., 2024; Decart, 2024; Yu et al., 2025). This approach enables autoregressive generation while preserving visual quality, without any changes to the model structure. Next token prediction offers another approach to autoregressive video generation (Kondratyuk et al., 2023; Wang et al., 2024a), though its visual quality currently lags behind diffusion methods. However, its potential for integration with large language models, which could enable strong causal reasoning capabilities (Zhou et al., 2024a; Xie et al., 2024), makes it promising direction. Recent attempts to combine diffusion models with next token prediction aim to maintain high quality while modeling frame-to-frame causality (Li et al., 2024; Deng et al., 2024). While these hybrid approaches show promise, they are still in early stages and their potential to surpass established diffusion-based methods remains to be seen. (2) Real-time Generation: From an algorithmic perspective, this can be achieved through various established techniques including distilling to lightweight models (Kim et al., 2025), reducing diffusion sampling steps through ODE distillation (Wang et al., 2023b), using high-compression VAEs or tokenizers (Chen et al., 2024c), and restructuring to causal architectures to avoid redundant computations (Yin et al., 2024). While deployment acceleration strategies are equally important, they are beyond the scope of this discussion. (3) Multi-modal Generation: One approach is to develop unified large multimodal models that support understanding and generation across multiple modalities including text, vision, audio, human motion, depth maps, and even brain waves. Recent works have started exploring this direction (Zhou et al., 2024a; Xie et al., 2024; Kondratyuk et al., 2023; Reed et al., 2022), though significant challenges remain. An alternative strategy is to first develop specialized large models for individual modalities (Deshmukh et al., 2023; Yang et al., 2024a; Jiang et al., 2023; Bai et al., 2024) before integrating them into unified system. Specifically, this requires designing pipeline relationships between different expert models within the unified system. For example, language models generate video generation instructions, the generated videos then serve as input for audio models to produce corresponding audio signals, ultimately leading to unified output. 4.3. Control Concept. As shown in Fig. 5, the control module manages user control of the virtual world through two aspects: (1) navigation control enables players to navigate and explore the virtual world through camera and character movement. For example, in racing games, players use arrow keys or WASD for acceleration, braking, and steering, while in open-world games, players typically use WASD for character movement, mouse for camera rotation, and space bar for jumping or climbing. (2) interaction control allows players to manipulate objects within the virtual environment. 5 Position: Interactive Generative Video as Next-Generation Game Engine Figure 5. The Control module of IGV manages player control through two aspect: Navigation Control and Interaction Control. For instance, in simulation and management games, players use left mouse clicks to select and place buildings, right clicks to rotate structures, and keyboard shortcuts like to access inventory or to demolish objects. Technical Approaches and Future Directions. The technical implementation of control mechanisms has been well-studied. Common approaches include: (1) Cross Attention (Yu et al., 2025; Valevski et al., 2024; Feng et al., 2024), where navigation control signals are transformed into conditional features that serve as keys and values, while the video features serve as queries. (2) Another approach uses external Adaptors (Che et al., 2024; Wang et al., 2024b), which directly fuses control features with video features. While control is easily mastered in fixed scenes, it should generalize to open-domain scenarios. Some works (Yu et al., 2025; Feng et al., 2024; DeepMind, 2024) have leveraged video generation priors for this purpose, but generalizing complex actions with limited control annotations remains challenging and requires further exploration. Learning control, especially for interaction control, goes beyond mechanical execution and requires understanding the underlying rules of how interactions change the environment (as part of physical laws). Following data-driven approach, future work aims to collect large-scale gaming datasets (Yu et al., 2025; Che et al., 2024) and improve the learning of these interaction rules. Physical laws will be further discussed in the Dynamics module in Sec. 4.5. For game control signal design, the key design principle is to align with users gaming intuitions. promising research direction would be developing more natural control signals that better match human habits, such as using gesture recognition or brain-computer interfaces. Figure 6. The Memory module of IGV consists of two components: static memory and dynamic memory. 4.4. Memory Concept. Conventional video generation models rely solely on attention mechanisms, struggling to maintain scene layouts, object appearances, and other visual elements in long-duration or large-motion scenarios. As demonstrated in Fig. 6, the Memory module addresses these challenges through two aspects: (1) static memory encompasses scenelevel and object-level memory, including game maps, buildings, character models, and object appearances. In construction games like Minecraft or SimCity, the module needs to consistently maintain the structure of player-built constructions; inconsistency in building layouts or designs between frames would severely impact player experience. (2) dynamic memory handles short-term motion and behavior patterns, such as character animations, vehicle trajectories, particle effects, and environmental changes like weather transitions. This is crucial in games requiring precise motion consistency, such as fighting games where character movements and attack animations must remain fluid and coherent, or rhythm games where dance movements need to maintain smooth transitions between frames. Technical Approaches and Future Directions. Current methods mainly rely on attention-based memory, utilizing attentions inherent ability to remember historical frames through cross-attention between historical and predicted frames (Valevski et al., 2024; Decart, 2024). However, this approach is unreliable and faces limitations in both precision of memory preservation and limited window size. Another promising solution is using dedicated memory structures, which can be implemented either as implicit high-dimensional features (Kim et al., 2020) or explicit 3D representations (Menapace et al., 2022; 2024; Yu et al., 2024a;b). These structures serve as conditional controls for the generation module, ensuring consistent preservation Position: Interactive Generative Video as Next-Generation Game Engine Figure 7. The Dynamics module of IGV focuses on two aspects: physical laws and physics tuning. of static elements. While similar approaches have been explored in 3D generation (Yu et al., 2024b; Ma et al., 2024), their adaptation as memory mechanisms for game video generation requires further investigation. Dynamic memory, which needs to record animations, actions and trajectories, requires dynamic video datasets, especially those containing significant motions like human movements. Collecting and annotating such high-quality dynamic data remains challenging but crucial for advancing this research direction. 4.5. Dynamics Concept. As demonstrated in Fig. 7, the Dynamics Module focuses on two key aspects: (1) Physical Laws specifically focuses on comprehending and generating videos that comply with fundamental physics, especially rigid body mechanics including gravity, collision, and acceleration. In racing simulators like Forza Motorsport, physics-based puzzle games like Portal, and platformers like Super Mario Odyssey, where precise physical interactions drive core gameplay mechanics. (2) Physics Tuning extends beyond Physical Laws by enabling control over physical parameters rather than simply replicating real-world physics. This includes adjusting gravity, friction coefficients, or directly modifying time, velocity, and mass values. In games like Braid where time manipulation is core to gameplay, Superhot where time moves only when the player moves, and Control where physics manipulation powers create unique gameplay experiences. Technical Approaches and Future Directions. straightforward data-driven approach to implementing physical laws learns probability distributions from largescale video data to generate physically plausible predicFigure 8. The Intelligence module of IGV implements two aspects: causal reasoning and self-evolution. tions (OpenAI, 2024; NVIDIA, 2025). However, this requires extensive high-quality videos demonstrating diverse physical phenomena, which remains significant challenge in current research. Alternatively, physics-based memory control offers more direct approach by simulating physics in the memory space and using it as conditional control for video generation, potentially providing more precise physical behaviors. specific solution is to use video generation models as renderers on top of physics simulators, which ensures perfect physics compliance. However, this approach is limited to physical phenomena that can be mathematically formulated in physics engines. Furthermore, establishing appropriate benchmarks for evaluating the physical accuracy of generated videos represents crucial research direction (Qin et al., 2024a), as it would help identify limitations and guide improvements in physical dynamics modeling. Physics tuning capability, though often overlooked in current research, is crucial for models to truly understand and manipulate physical knowledge. Collecting synthetic video data with annotated physical parameters from simulations could be viable solution. We raise this challenge to encourage future research efforts in this direction. 4.6. Intelligence Concept. As Demonstrated in Fig. 8, the Intelligence module implements two key aspects: (1) Reasoning: This capability enables long-term causal inference based on initial conditions, creating deeply immersive virtual worlds. For example, the system can predict how kingdoms economy and social structure might evolve over centuries based on its initial resources and policies, or simulate wildlife migration patterns when environmental conditions change, 7 Position: Interactive Generative Video as Next-Generation Game Engine Level Name Technical Features Application Examples No AI-Assisted Assets Generation Manual creation and integration of all game assets and logic. Super Mario: fixed levels; Tetris: fixed rules. Category Traditional Manual Game Development AI-Assisted Assets Generation Physics-Compliant Interactive World Generation Causal-Reasoning World Simulation Self-Evolving World Ecosystem Al-assisted creation and integration of game assets and logic. Cyberpunk 2077: AI-generated assets; AI Dungeon: real-time NPC dialogues. Real-time physics-compliant video generation with user interactions, supported by the Dynamics module. E.g., Player sets fire to wooden bridges, AI dynamically renders blazing spans and rerouted enemy paths Next-Gen AI-Driven Generative Game Engine World simulation with causal reasoning across time based on L2, incorporating the Intelligence module. Autonomous world evolution with emergent behaviors based on L2 and L3, requiring advanced Intelligence module. E.g., Killing faction leader in Act 1 triggers city-wide riots in Act 3. E.g. NPCs self-organize governments and trade as population increases. L0 L1 L2 L3 L4 Table 1. Proposed Maturity Levels (L0-L4) of Generative Game Engine. L0-L1 represent traditional manual game development with limited AI assistance, while L2-L4 showcase next-generation game engines featuring video-based world generation. such as animals seeking new water sources after river dries up. Similar mechanics can be found in grand strategy games like Crusader Kings and ecosystem simulations like Planet Zoo. (2) Self-Evolution: This capability goes beyond merely generating continuous video streams with changing virtual worlds; it enables virtual worlds to continuously develop, evolve, and generate new knowledge, rules, and behaviors through emergent properties. In simulation games, civilizations could naturally emerge and form their own cultures, ecosystems could develop new species, and cities could grow and adapt organically. Such technology could eventually realize metaverse similar to The Matrix, where countless agents and players live in self-evolving virtual worlds. Technical Approaches and Future Directions. Similar to large language models with their powerful causal reasoning abilities, implementing reasoning capabilities requires video generation models to possess causal structure. This means new video generation must be conditioned on historical videos through an autoregressive generation approach (as discussed in Generation Module in Sec. 4.2), while also requiring large-scale data training to develop robust causal inference capabilities. An alternative approach is to leverage large language models or multimodal large language models for causal reasoning, working in conjunction with video generation models. This strategy appears promising, particularly in developing unified understanding and generation models that can perform both linguistic causal reasoning and visual video generation (Zhou et al., 2024a; Xie et al., 2024). This represents significant direction for future research. Furthermore, if all previously mentioned capabilities including physics understanding, physical simulation, and causal reasoning are successfully implemented and demonstrate powerful performance in the future, we might witness the emergence of remarkable self-evolution capabilities. This convergence of advanced capabilities could potentially lead to truly autonomous virtual worlds, such as metaverses inhabited by countless intelligent agents, or brain-in-a-vat worlds similar to those depicted in The Matrix. 4.7. Gameplay Concept. The GamePlay Module builds upon Interactive Generated Video (IGV) by implementing external Game Rules, which are designer-imposed rules such as game objectives, rewards, penalties, and constraints that shape the virtual worlds gameplay experience. These include scoring systems in Tetris, health and damage systems in Dark Souls, mission objectives and reward structures in Grand Theft Auto, achievement systems in Minecraft, time limits in Mario, competitive ranking systems in League of Legends, and quest completion rewards in World of Warcraft. Technical Approaches and Future Directions. 8 Position: Interactive Generative Video as Next-Generation Game Engine The implementation of the GamePlay module primarily relies on agent systems empowered by large language models (Achiam et al., 2023; Touvron et al., 2023) or multimodal large models (Liu et al., 2024), leveraging their sophisticated reasoning, planning, and decision-making capabilities. Various gameplay aspects, including level design, difficulty scaling, reward-penalty determination, and NPC character development with dialogue systems, can be implemented through specialized agents. Another key research challenge lies in developing unified multi-agent system framework specifically for game environments. While existing single agents (Li et al., 2023; Wu et al., 2023) show promise, integrating multiple agents to collaboratively control game elements while maintaining coherent gameplay remains an open research problem. Another practical research direction is exploring how agents and agent systems can enable dynamic, adaptive game rules, including reward and penalty mechanisms. As players progress through games, their skill levels, capabilities, and experience continuously evolve, making it essential to adaptively adjust difficulty levels and reward-penalty systems accordingly. 5. Levels of Generative Game Engine We propose five-level maturity model (L0-L4) to evaluate generative game engines and guide their future development. This framework helps assess current technologies and identify key research directions in generative game engines. Below we detail each level, with an overview of the maturity model presented in Table 1. Level 0: No AI-Assisted Assets Generation. At this foundational level, game engines rely entirely on manually crafted content without any AI-generated elements. All game assets and rules must be pre-designed by developers during the development phase. Classic examples include Super Mario, where each level layout is carefully hand-crafted, and Tetris, where the game rules and piece designs are fixed. This traditional approach ensures precise control over game content but requires substantial development resources, and players can only experience limited pre-designed content. Level 1: AI-Assisted Assets Generation. At this level, game development transitions from purely manual processes to AI-assisted creation and integration of game assets and logic, operating both during development and gameplay. During development, AI tools can create diverse game assets to reduce content creation workload. For instance, in Cyberpunk 2077, developers can utilize image generation models like Stable Diffusion (Rombach et al., 2022) to create varied textures for neon billboards, trash piles, and urban details throughout Night City. During gameplay, the game engine can generate short segments in response to player actions, such as unique explosion animations when player destroys bridge in an open-world game, or dynamic NPC dialogues in games like AI Dungeon. While this approach accelerates game development and adds variability to common game events, the overall game framework remains pre-designed and still requires significant human intervention and curation. Level 2: Physics-Compliant Interactive World Generation. This level marks paradigm shift from traditional manual-centric development to new paradigm centered on interactive video generation, representing the Next-Gen AI-Driven Generative Game Engines. The game engine continuously generates physics-compliant video content based on player interactions in real-time. For example, when player sets fire to wooden bridge, the engine dynamically generates not only the realistic blazing effects but also adapts the game world accordingly, such as rerouting enemy paths around the destroyed structure. While many current research works operate at this level (Valevski et al., 2024; Yu et al., 2025; Decart, 2024), significant improvements are still needed in physics understanding, simulation realism, and the generalization of interaction capabilities. Level 3: Causal-Reasoning World Simulation. Building upon Level 2s interactive physics-compliant generation, which focuses primarily on immediate physical responses, this level introduces sophisticated causal reasoning across time in world simulation to address the limitation of shortterm effects. The game engine maintains complex world model that understands both player actions and intrinsic world logic rules, generating video content that reflects longterm cause-and-effect relationships. For example, when player assassinates faction leader in Act 1, the engine simulates the resulting political instability, leading to city-wide riots and power struggles that emerge in Act 3. Through this deep understanding of causality, the game creates complex storylines where players early choices naturally shape the worlds future development. Level 4: Self-Evolving World Ecosystem. Building upon the physics-compliant generation of Level 2 and causal reasoning of Level 3, as these capabilities (physics understanding, simulation, and causal reasoning of internal rules) continuously advance, the model emerges with powerful self-evolution abilities. The game world becomes selfevolving ecosystem where complex systems emerge from initial rules and interactions. For example, as the NPC population grows, they autonomously organize into governance structures and establish trade networks, exhibiting emergent social behaviors beyond their initial programming. At this stage, the generative game engine will be capable of creating virtual worlds similar to those seen in Ready Player One or The Matrix, where future players can not only play but potentially live within these worlds. This advancement 9 Position: Interactive Generative Video as Next-Generation Game Engine will not only revolutionize the gaming industry but also profoundly impact various aspects of human society. 6. Alternative Views Alternative Views #1: Current video generation models lack sufficient physical understanding, making it challenging to implement GGE. Recent benchmarks (Qin et al., 2024a; Kang et al., 2024; Motamed et al., 2025) evaluating video models grasp of physical laws have consistently revealed major limitations. These studies, despite using different evaluation methods, show that current models struggle to accurately predict object trajectories, velocities, and interactions. This gap between visual realism and physical understanding remains significant hurdle for game development. Potential Solution #1: WorldSimBench (Qin et al., 2024a) improves video models physical understanding through the HF-Embodied dataset, while Physics-IQ-benchmark (Motamed et al., 2025) demonstrates that training on large-scale datasets with rich physical interactions can help models learn basic physical principles through observation. Alternative Views #2: IGVs current limitations in scene consistency prevent it from serving as GGEs core. Some approaches (Decart, 2024) show that simple camera movements, like looking up and down, can cause dramatic scene changes (e.g., plains suddenly becoming desert), breaking spatial continuity and gameplay immersion. Potential Solution #2: Scene consistency can be improved through 3D control mechanisms. Recent works (Ma et al., 2024; Yu et al., 2024b; Xie et al., 2025) demonstrate promising results by reconstructing scene point clouds from generated videos and using them to guide subsequent video generation, enhancing spatial coherence. Alternative Views #3: IGVs limited logical reasoning capabilities make it inadequate as GGEs core. Games require complex logical structures, particularly in narrative progression, which current video generation models cannot effectively handle. Existing IGV approaches (Decart, 2024; Yu et al., 2025; Valevski et al., 2024) only work in simplified settings, acting more as rendering engines rather than addressing the deeper logical demands of game environments. Potential Solution #3: An approach is to combine video models with large language models (LLMs). While video models alone struggle with logical reasoning, integrating them with LLMs offers viable solution. Recent unified models (Zhou et al., 2024a) that combine diffusion-based video generation with language understanding show encouraging results in handling both game logic and video content. As this technology matures, IGV could develop the necessary capabilities to function effectively as GGEs core. While achieving GGE through IGV faces challenges, targeted model designs and high-quality data collection can address these issues. We believe that with continued advancements in video generation models, IGV can ultimately serve as the core technology for GGE. 7. Conclusion In this position paper, we have presented Interactive Generative Video (IGV) as promising foundation for nextgeneration game engine. We proposed comprehensive framework with six essential modules and established fivelevel maturity model (L0-L4) to guide future research and development. Through our analysis, we demonstrated that IGVs unique capabilities in content generation, physics simulation, and interactive control make it an ideal candidate for revolutionizing the gaming industry. We believe this work provides clear roadmap for advancing game engine technology while identifying key challenges and opportunities for future exploration."
        },
        {
            "title": "Impact Statement",
            "content": "This paper proposes Interactive Generative Videos (IGV) as core technology for the next-generation game engine, Generative Game Engine (GGE). IGV will transform the way generative AI is utilized, further advancing the development of interactive applications. GGE will revolutionize the gaming industry by enabling rapid production of high-quality, unlimited, and adaptive game content."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Alonso, E., Jelley, A., Micheli, V., Kanervisto, A., Storkey, A., Pearce, T., and Fleuret, F. Diffusion for world modIn Thirty-eighth eling: Visual details matter in atari. Conference on Neural Information Processing Systems, 2024. BAAI, P. Plan4mc: Skill reinforcement learning and planning for open-world minecraft tasks. arXiv preprint arXiv:2303.16563, 2023. Bai, Y., Geng, X., Mangalam, K., Bar, A., Yuille, A. L., Darrell, T., Malik, J., and Efros, A. A. Sequential modeling enables scalable learning for large vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2286122872, 2024. Blattmann, A., Dockhorn, T., Kulal, S., Mendelevitch, D., Kilian, M., Lorenz, D., Levi, Y., English, Z., Voleti, V., Letts, A., et al. Stable video diffusion: Scaling latent Position: Interactive Generative Video as Next-Generation Game Engine video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Bruce, J., Dennis, M. D., Edwards, A., Parker-Holder, J., Shi, Y., Hughes, E., Lai, M., Mavalankar, A., Steigerwald, R., Apps, C., et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. Cai, S., Wang, Z., Ma, X., Liu, A., and Liang, Y. Openworld multi-task control through goal-aware representation learning and adaptive horizon prediction. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1373413744, 2023. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Charity, M., Dave, I., Khalifa, A., and Togelius, J. Baba is yall 2.0: Design and investigation of collaborative mixed-initiative system. IEEE Transactions on Games, 16(1):7589, 2022. Che, H., He, X., Liu, Q., Jin, C., and Chen, H. Gamegenx: Interactive open-world game video generation. arXiv preprint arXiv:2411.00769, 2024. Chen, B., Monso, D. M., Du, Y., Simchowitz, M., Tedrake, R., and Sitzmann, V. Diffusion forcing: Next-token prediction meets full-sequence diffusion. arXiv preprint arXiv:2407.01392, 2024a. Chen, H., Zhang, Y., Cun, X., Xia, M., Wang, X., Weng, C., and Shan, Y. Videocrafter2: Overcoming data limitations for high-quality video diffusion models, 2024b. Chen, J., Cai, H., Chen, J., Xie, E., Yang, S., Tang, H., Li, M., Lu, Y., and Han, S. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024c. Decart, E. Oasis: universe in transformer. https: //oasis-model.github.io/, 2024. DeepMind, G. Genie 2: large-scale foundation world model. https://deepmind.google/discover/blog/genie-2-alarge-scale-foundation-world-model/, 2024. Deng, H., Pan, T., Diao, H., Luo, Z., Cui, Y., Lu, H., Shan, S., Qi, Y., and Wang, X. Autoregressive video generation without vector quantization. arXiv preprint arXiv:2412.14169, 2024. Deshmukh, S., Elizalde, B., Singh, R., and Wang, H. Pengi: An audio language model for audio tasks. In Advances in Neural Information Processing Systems, 2023. Fan, L., Wang, G., Jiang, Y., Mandlekar, A., Yang, Y., Zhu, H., Tang, A., Huang, D.-A., Zhu, Y., and Anandkumar, A. Minedojo: Building open-ended embodied agents with In Thirty-sixth Conference internet-scale knowledge. on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. Feng, R., Zhang, H., Yang, Z., Xiao, J., Shu, Z., Liu, Z., Zheng, A., Huang, Y., Liu, Y., and Zhang, H. The matrix: Infinite-horizon world generation with real-time moving control. arXiv preprint arXiv:2412.03568, 2024. Fu, X., Liu, X., Wang, X., Peng, S., Xia, M., Shi, X., Yuan, Z., Wan, P., Zhang, D., and Lin, D. 3dtrajmaster: Mastering 3d trajectory for multi-entity motion in video generation. In ICLR, 2025. Guo, Y., Yang, C., Rao, A., Agrawala, M., Lin, D., and Dai, B. Sparsectrl: Adding sparse controls to text-to-video diffusion models. In European Conference on Computer Vision, pp. 330348. Springer, 2025. Guss, W. H., Houghton, B., Topin, N., Wang, P., Codel, C., Veloso, M., and Salakhutdinov, R. Minerl: largescale dataset of minecraft demonstrations. arXiv preprint arXiv:1907.13440, 2019. He, H., Xu, Y., Guo, Y., Wetzstein, G., Dai, B., Li, H., and Yang, C. Cameractrl: Enabling camera control for textto-video generation. arXiv preprint arXiv:2404.02101, 2024. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 2020. Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Huang, W., Abbeel, P., Pathak, D., and Mordatch, I. Language models as zero-shot planners: Extracting actionable knowledge for embodied agents. In International conference on machine learning, pp. 91189147. PMLR, 2022. Jiang, B., Chen, X., Liu, W., Yu, J., Yu, G., and Chen, T. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36: 2006720079, 2023. Jiang, H., Yue, J., Luo, H., Ding, Z., and Lu, Z. Reinforcement learning friendly vision-language model for minecraft. In European Conference on Computer Vision, pp. 117. Springer, 2025. 11 Position: Interactive Generative Video as Next-Generation Game Engine Kang, B., Yue, Y., Lu, R., Lin, Z., Zhao, Y., Wang, K., Huang, G., and Feng, J. How far is video generation from world model: physical law perspective. arXiv preprint arXiv:2411.02385, 2024. Ma, B., Gao, H., Deng, H., Luo, Z., Huang, T., Tang, L., and Wang, X. You see it, you got it: Learning 3d creation on pose-free videos at scale. arXiv preprint arXiv:2412.06699, 2024. Kim, B.-K., Song, H.-K., Castells, T., and Choi, S. Bksdm: lightweight, fast, and cheap version of stable diffusion. In European Conference on Computer Vision, pp. 381399. Springer, 2025. Menapace, W., Lathuiliere, S., Tulyakov, S., Siarohin, A., and Ricci, E. Playable video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1006110070, 2021. Kim, S. W., Zhou, Y., Philion, J., Torralba, A., and Fidler, S. Learning to simulate dynamic environments with gamegan. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1231 1240, 2020. Menapace, W., Lathuili`ere, S., Siarohin, A., Theobalt, C., Tulyakov, S., Golyanik, V., and Ricci, E. Playable environments: Video manipulation in space and time. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 35843593, 2022. Kim, S. W., Philion, J., Torralba, A., and Fidler, S. Drivegan: Towards controllable high-quality neural simulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 58205829, 2021. Kondratyuk, D., Yu, L., Gu, X., Lezama, J., Huang, J., Schindler, G., Hornung, R., Birodkar, V., Yan, J., Chiu, M.-C., et al. Videopoet: large language model for zeroshot video generation. arXiv preprint arXiv:2312.14125, 2023. Lab, and P.-Y. etc., T. A. https://github.com/PKU-YuanGroup/ Open-Sora-Plan, 2024. Open-sora-plan. Li, G., Hammoud, H., Itani, H., Khizbullin, D., and Ghanem, B. Camel: Communicative agents for mind exploration of large language model society. Advances in Neural Information Processing Systems, 36:5199152008, 2023. Li, T., Tian, Y., Li, H., Deng, M., and He, K. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. Liapis, A. Searching for sentient design tools for game development. Phd thesis, IT University of Copenhagen, 2015. Liapis, A., Yannakakis, G. N., and Togelius, J. Designer modeling for sentient sketchbook. In 2014 IEEE Conference on Computational Intelligence and Games, pp. 18. IEEE, 2014. Lin, Z., Li, J., Shi, J., Ye, D., Fu, Q., and Yang, W. Juewumc: Playing minecraft with sample-efficient hierarchical reinforcement learning. arXiv preprint arXiv:2112.04907, 2021. Liu, H., Li, C., Wu, Q., and Lee, Y. J. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. Menapace, W., Siarohin, A., Lathuili`ere, S., Achlioptas, P., Golyanik, V., Tulyakov, S., and Ricci, E. Promptable game models: Text-guided game simulation via masked diffusion models. ACM Transactions on Graphics, 43(2): 116, 2024. Migkotzidis, P. and Liapis, A. Susketch: Surrogate models of gameplay as design assistant. IEEE Transactions on Games, 14(2):273283, 2021. Motamed, S., Culp, L., Swersky, K., Jaini, P., and Geirhos, R. Do generative video models learn physical principles from watching videos? arXiv preprint arXiv:2501.09038, 2025. Ni, H., Shi, C., Li, K., Huang, S. X., and Min, M. R. Conditional image-to-video generation with latent flow diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 18444 18455, 2023. NVIDIA. Cosmos world foundation model platform for physical ai. arXiv preprint arXiv:2501.03575, 2025. Oh, J., Singh, S., Lee, H., and Kohli, P. Zero-shot task generalization with multi-task deep reinforcement learning. In International Conference on Machine Learning, pp. 26612670. PMLR, 2017. OpenAI. Creating video from text. https://openai. com/index/sora/, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023. Qin, Y., Shi, Z., Yu, J., Wang, X., Zhou, E., Li, L., Yin, Z., Liu, X., Sheng, L., Shao, J., et al. Worldsimbench: Towards video generation models as world simulators. arXiv preprint arXiv:2410.18072, 2024a. Position: Interactive Generative Video as Next-Generation Game Engine Qin, Y., Zhou, E., Liu, Q., Yin, Z., Sheng, L., Zhang, R., Qiao, Y., and Shao, J. Mp5: multi-modal open-ended embodied system in minecraft via active perception. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1630716316, 2024b. Razavi, A., Van den Oord, A., and Vinyals, O. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. Reed, S., Zolna, K., Parisotto, E., Colmenarejo, S. G., Novikov, A., Barth-Maron, G., Gimenez, M., Sulsky, Y., Kay, J., Springenberg, J. T., et al. generalist agent. arXiv preprint arXiv:2205.06175, 2022. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022. Ruhe, D., Heek, J., Salimans, T., and Hoogeboom, arXiv preprint Rolling diffusion models. E. arXiv:2402.09470, 2024. Smith, G., Whitehead, J., and Mateas, M. Tanagra: Reactive planning and constraint solving for mixed-initiative level design. IEEE Transactions on computational intelligence and AI in games, 3(3):201215, 2011. Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 2019. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. International Conference on Learning Representations, 2021. Wang, X., Zhang, S., Zhang, H., Liu, Y., Zhang, Y., Gao, C., and Sang, N. Videolcm: Video latent consistency model. arXiv preprint arXiv:2312.09109, 2023b. Wang, X., Zhang, X., Luo, Z., Sun, Q., Cui, Y., Wang, J., Zhang, F., Wang, Y., Li, Z., Yu, Q., et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024a. Wang, Z., Cai, S., Chen, G., Liu, A., Ma, X., and Liang, Y. Describe, explain, plan and select: Interactive planning with large language models enables open-world multitask agents. arXiv preprint arXiv:2302.01560, 2023c. Wang, Z., Yuan, Z., Wang, X., Li, Y., Chen, T., Xia, M., Luo, P., and Shan, Y. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, 2024b. Wu, Q., Bansal, G., Zhang, J., Wu, Y., Zhang, S., Zhu, E., Li, B., Jiang, L., Zhang, X., and Wang, C. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023. Xie, J., Mao, W., Bai, Z., Zhang, D. J., Wang, W., Lin, K. Q., Gu, Y., Chen, Z., Yang, Z., and Shou, M. Z. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Xie, Z., Liu, Z., Peng, Z., Wu, W., and Zhou, B. Vid2sim: Realistic and interactive simulation from video for urban navigation. arXiv preprint arXiv:2501.06693, 2025. Xing, J., Xia, M., Zhang, Y., Chen, H., Wang, X., Wong, T.- T., and Shan, Y. Dynamicrafter: Animating open-domain images with video diffusion priors, 2023. Torii, M. G., Murakami, T., and Ochiai, Y. Lottery and sprint: Generate board game with design sprint method on autogpt. In Companion Proceedings of the Annual Symposium on Computer-Human Interaction in Play, pp. 259265, 2023. Yang, J., Dong, Y., Liu, S., Li, B., Wang, Z., Tan, H., Jiang, C., Kang, J., Zhang, Y., Zhou, K., et al. Octopus: Embodied vision-language programmer from environmental feedback. In European Conference on Computer Vision, pp. 2038. Springer, 2025. Touvron, H., Lavril, T., Izacard, G., Martinet, X., Lachaux, M.-A., Lacroix, T., Rozi`ere, B., Goyal, N., Hambro, E., Azhar, F., et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Valevski, D., Leviathan, Y., Arar, M., and Fruchter, S. Diffusion models are real-time game engines. arXiv preprint arXiv:2408.14837, 2024. Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An openended embodied agent with large language models. 2023. Comment: Project website and open-source codebase: https://voyager. minedojo. org/Cited on, pp. 33, 2023a. Yang, L., Kang, B., Huang, Z., Xu, X., Feng, J., and Zhao, H. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1037110381, 2024a. Yang, M., Du, Y., Ghasemipour, K., Tompson, J., Schuurmans, D., and Abbeel, P. Learning interactive real-world simulators. arXiv preprint arXiv:2310.06114, 2023. Yang, M., Li, J., Fang, Z., Chen, S., Yu, Y., Fu, Q., Yang, W., and Ye, D. Playable game generation. arXiv preprint arXiv:2412.00887, 2024b. 13 Position: Interactive Generative Video as Next-Generation Game Engine in the minecraft: Generally capable agents for openworld environments via large language models with arXiv preprint text-based knowledge and memory. arXiv:2305.17144, 2023. Yang, S., Hou, L., Huang, H., Ma, C., Wan, P., Zhang, D., Chen, X., and Liao, J. Direct-a-video: Customized video generation with user-directed camera movement and object motion. In ACM SIGGRAPH 2024 Conference Papers, pp. 112, 2024c. Yang, S., Walker, J. C., Parker-Holder, J., Du, Y., Bruce, J., Barreto, A., Abbeel, P., and Schuurmans, D. Position: Video as the new language for real-world decision making. In Proceedings of the 41st International Conference on Machine Learning, 2024d. Yang, Z., Teng, J., Zheng, W., Ding, M., Huang, S., Xu, J., Yang, Y., Hong, W., Zhang, X., Feng, G., et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024e. Yin, T., Zhang, Q., Zhang, R., Freeman, W. T., Durand, F., Shechtman, E., and Huang, X. From slow bidirectional to fast causal video generators. arXiv preprint arXiv:2412.07772, 2024. Yu, H.-X., Duan, H., Hur, J., Sargent, K., Rubinstein, M., Freeman, W. T., Cole, F., Sun, D., Snavely, N., Wu, J., et al. Wonderjourney: Going from anywhere to everyIn Proceedings of the IEEE/CVF Conference where. on Computer Vision and Pattern Recognition, pp. 6658 6667, 2024a. Yu, J., Qin, Y., Wang, X., Wan, P., Zhang, D., and Liu, X. Gamefactory: Creating new games with generative interactive videos, 2025. Yu, W., Xing, J., Yuan, L., Hu, W., Li, X., Huang, Z., Gao, X., Wong, T.-T., Shan, Y., and Tian, Y. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024b. Zheng, Z., Peng, X., Yang, T., Shen, C., Li, S., Liu, H., Zhou, Y., Li, T., and You, Y. Open-sora: Democratizing efficient video production for all. https://github. com/hpcaitech/Open-Sora, 2024. Zhou, C., Yu, L., Babu, A., Tirumala, K., Yasunaga, M., Shamis, L., Kahn, J., Ma, X., Zettlemoyer, L., and Levy, O. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024a. Zhou, E., Qin, Y., Yin, Z., Huang, Y., Zhang, R., Sheng, L., Qiao, Y., and Shao, J. Minedreamer: Learning to follow instructions via chain-of-imagination for simulated-world control. arXiv preprint arXiv:2403.12037, 2024b. Zhu, X., Chen, Y., Tian, H., Tao, C., Su, W., Yang, C., Huang, G., Li, B., Lu, L., Wang, X., et al. Ghost"
        }
    ],
    "affiliations": [
        "Kuaishou",
        "The Hong Kong University of Science and Technology",
        "The University of Hong Kong"
    ]
}