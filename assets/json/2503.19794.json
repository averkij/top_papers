{
    "paper_title": "PAVE: Patching and Adapting Video Large Language Models",
    "authors": [
        "Zhuoming Liu",
        "Yiquan Li",
        "Khoi Duc Nguyen",
        "Yiwu Zhong",
        "Yin Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Pre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D information) remains challenging. In this paper, we present PAVE, a flexible framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals, such as audio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters, referred to as \"patches,\" which add a small number of parameters and operations to a base model without modifying its architecture or pre-trained weights. In doing so, PAVE can effectively adapt the pre-trained base model to support diverse downstream tasks, including audio-visual question answering, 3D reasoning, multi-view video recognition, and high frame rate video understanding. Across these tasks, PAVE significantly enhances the performance of the base model, surpassing state-of-the-art task-specific models while incurring a minor cost of ~0.1% additional FLOPs and parameters. Further, PAVE supports multi-task learning and generalizes well across different Video LLMs. Our code is available at https://github.com/dragonlzm/PAVE."
        },
        {
            "title": "Start",
            "content": "PAVE: Patching and Adapting Video Large Language Models Zhuoming Liu1, Yiquan Li1, Khoi Duc Nguyen1, Yiwu Zhong2, Yin Li1 1University of Wisconsin-Madison 2The Chinese University of Hong Kong 5 2 0 2 5 2 ] . [ 1 4 9 7 9 1 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Pre-trained video large language models (Video LLMs) exhibit remarkable reasoning capabilities, yet adapting these models to new tasks involving additional modalities or data types (e.g., audio or 3D information) remains challenging. In this paper, we present PAVE, flexible framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals, such as audio, 3D cues, or multi-view videos. PAVE introduces lightweight adapters, referred to as patches, which add small number of parameters and operations to base model without modifying its architecture or pre-trained weights. In doing so, PAVE can effectively adapt the pre-trained base model to support diverse downstream tasks, including audio-visual question answering, 3D reasoning, multi-view video recognition, and high frame rate video understanding. Across these tasks, PAVE significantly enhances the performance of the base model, surpassing state-of-the-art task-specific models while incurring minor cost of 0.1% additional FLOPs and parameters. Further, PAVE supports multitask learning and generalizes well across different Video LLMs. Our code is available at https://github. com/dragonlzm/PAVE. 1. Introduction Large multimodal models have recently demonstrated remarkable success in video understanding [28, 48, 60]. These models, known as video large language models (Video LLMs), are pre-trained on video and text data, exhibiting significant reasoning capabilities across video understanding tasks. They thus promise to serve as video foundation models, characterized by their potential to adapt to many downstream tasks [5]. However, many of these tasks involve side-channel signals in the form of additional modalities or data types beyond the video-text pairs that Video LLMs are typically trained on. For example, audiovisual question answering (QA) [1] necessitates integrating both video and audio to comprehend content, while 3D QA [2] requires reasoning about the 3D scene from video of 3D scan. What if target video task offers side-channel Figure 1. Top: Evaluating Video LLM on audio-visual QA and 3D QA tasks. Bottom: Adapting Video LLMs by adding small patch of additional operations and parameters, without changing its existing architecture or vast pre-trained weights. signals different from the pre-training of Video LLMs? To answer this question, we begin by evaluating the zero-shot performance of recent video LLM (LLaVAOneVision [28]) on audio-visual QA [1] and 3D QA tasks [2]. Our results, as in Figure 1 (top), show that despite lacking access to audio or 3D-specific data, the Video LLM attains promising performance. Yet, with similar number of parameters, the Video LLM lags behind those task-specific models that leverage additional side-channel signals. Noting that these task-specific models are indeed trained using the dedicated training sets, for fair comparison, we further fine-tune the Video LLM on the same training sets yet only with video data (see Figure 1 (top)). Surprisingly, this fine-tuned Video LLM approaches or even surpasses the performance of those specialized models with merely video input. Motivated by this observation, our key research question is, can we leverage the knowledge in pretrained Video LLMs for such tasks? To address this question, we investigate the adaptation of pre-trained Video LLMs to downstream tasks with sidechannel signals, defined as supplementary signals from additional modalities or data types such as audio, 3D cues, multi-view videos, or high frame rate inputs. By consid1 ering different signals, our formulation addresses key challenges in video understanding. For example, considering high frame rate videos as the side-channel explores fundamental questions about video representation [13]. Similarly, incorporating multi-view videos, audio, and 3D cues accounts for cross-view and cross-modality reasoning [1, 17]. We propose to adapt pre-trained base Video LLM through patching adding lightweight adapter (patch) with small number of additional parameters and operations, and without altering the base models architecture or vast pre-trained weights (see Figure 1 (bottom)). Inspired by the success of parameter-efficient adapters (e.g., LoRA [21]) in text and image generation models (e.g., LLMs [9, 46] and diffusion models [50, 53]), this approach allows for flexible customization, and facilitates convenient sharing of the customization by distributing small patches. To this end, we present PAVE, framework designed to Patch and Adapt Video LLMs with side-channel signals. PAVE leverages cross-attention that operates between tokens derived from key video frames (as queries) and tokens from side-channel signals (as keys and values). This operation aligns the visual signal and side-channel signals along the time axis, fuses the signals from both sources, and then updates the input visual tokens to the LLM. In doing so, PAVE integrates side-channel signals with lightweight patches, while enabling effective adaptation to various downstream tasks without altering pre-trained models. To evaluate PAVE, We conduct extensive experiments on four video tasks: (1) audio-visual QA, (2) 3D QA, (3) high frame rate video understanding, and (4) multi-view video recognition. Across all tasks, PAVE effectively adapts base Video LLM [28] and consistently outperforms taskspecific models. For example, compared to latest taskspecific models, PAVE achieves relative boost of 2% on the AVQA [69] dataset for audio-visual QA, 6% on the SQA3D dataset [44] for 3D QA, 1-5% across video benchmarks [14, 33, 81] when integrating high frame rate videos, and 1% on the Ego-Exo4D dataset [17] for multiview recognition. In all cases, PAVE only adds about 0.1% of FLOPs and parameters in addition to the base model. Further, we demonstrate that PAVE can support multi-task learning and generalizes well across different Video LLMs. Our main contributions are summarized as follows. We present PAVE, novel framework to adapt pre-trained Video LLMs for tasks with side-channel signals, greatly extending the capacity of Video LLMs. We design lightweight patches that add small number of parameters and operations to base model, and keep the original architecture and pre-trained weights unchanged, enabling PAVEs adaptation to varying tasks. We demonstrate that PAVE can be applied across video tasks and Video LLMs, surpassing the performance of strong task-specific models. 2. Related Works Video large language models. Recent advances in instruction tuning with visual and text data [3840] have led to surge of interest in developing Video LLMs. Many of these models share common design, where visual features are extracted using pre-trained visual encoder, projected into the text latent space of an LLM, and subsequently processed by the pre-trained LLM to generate responses. Video-ChatGPT [45] introduces instruction tuning into the video domain. Video-LLaVA [36] improves model performance with better text-aligned video features [83], while VideoChat2 [33] resorts to increasing the quality and quantity of the video instruction tuning set. Recent vision-LLM models like LLaVA-NeXT [39], LLaVA-OneVision [28], LLaVA-Video [79], Qwen2-VL [60], and mPlug-Owl3 [71] consider multi-stage training with both video and image, which substantially improves the model performance. Recent works in VideoLLM focus on long video understanding. [12, 25, 56, 64, 65, 76] propose to use Q-former [32] or text-query-based cross-attention to compress vision tokens, while others [47, 63] resort to state-space models [18]. Researchers also extend the instructional tuning into different video sub-domains. For instance, CAT [72] focuses on audio-visual understanding, while Scene-LLM [15] and LLaVA-3D [84] address 3D QA tasks. Built on these developments, our work specifically focuses on adapting pre-trained Video LLMs to downstream tasks with side-channel signals, aiming to significantly extend the capabilities of these models. Adaptation of vision foundation models. Adapting vision foundation models to downstream tasks has received significant attention. Prior works have studied learning lightweight adapters [4, 6, 8, 35, 52, 58], prepending learnable input tokens (e.g. prompts) [24, 82], or in-context learning [3, 62, 68, 78]. Recently, adapterand promptbased methods have been explored for Video LLMs. Adapt2Reward [70] re-purposes video-language model for robotic operation by using it as language-conditioned reward function. Similarly, SeViLA [73] adapts an imagelanguage model for video tasks by introducing Localizer and Answerer modules, derived from BLIP-2 [32], to enable video event localization and question answering. Our work is inspired by the success of adapter-based methods such as LoRA [21]. These methods learn parameter-efficient modules without changing the base models architecture and weights, and has been widely used to customize large, pre-trained diffusion models for image generation [19, 26, 54, 61, 67, 80], extending their capabilities in generating images with different styles or controlling the generated content. Moving beyond diffusion models, our goal is to offer flexible framework that adapts Video LLMs to wide range of tasks with patch-like adapters, allowing these models to effectively account for additional side-channel signals and adapt to downstream tasks. Multimodal video representation learning. Learning unified representation to connect video and other modalities, such as audio, text and point cloud, has received considerable attention. Prior methods [43] build on the idea of modality alignment using contrastive learning [51]. More recent works [7, 16, 41, 77, 83] extend the alignment across multiple modalities. Inspired by these works, our method leverages the shared representation between video and text learned by pre-trained Video LLM, and further embeds side-channel signals into this latent space during adaptation. 3. Patching and Adapting Video LLMs We propose PAVE, framework for adapting pre-trained Video LLMs to downstream tasks with side-channel signals. The key to our solution lies in parameter-efficient, lightweight adapter, referred to as patch. This patch fuses side-channel signals with the original visual tokens and updates them (see Figure 2 (a)), and adds LoRA module [21] to the LLM, enabling efficient adaptation with small number of parameters and operations and without altering the base model. In what follows, we introduce the background on Video LLM, outline our problem formulation, and present our approach. 1, Xv 3.1. Preliminaries: Video LLM Video LLM takes video Xv and text query Xq = {xq} as input, then generates text answer Xa = {xa}. We assume that Xv = {Xv 2, ..., Xv K}, i.e., video is represented by key frames, where may vary across videos. Xv is first encoded by visual encoder hv() into set of visual tokens {zv(k) RM d}, where zv(k) indicates tokens encoded from the k-th key frame within the video (k [1, K]). Similarly, Xq is processed by text encoder ht(), which embeds individual words xq into text tokens {zq Rd} with zq = ht(xq). These tokens are further combined and processed by an LLM () that decodes Xa in an autoregressive manner ([{zv(k)}, {zq}, {za <i}] ; θ) xa , (1) where {za swer xa <i} are text tokens from previously generated an- <i, i.e., za = ht(xa). θ denotes LLM parameters. 3.2. Video Tasks with Side-Channel Signals We consider adapting pre-trained Video LLM to downstream tasks with side-channel signals Xs. Similar to videos, we assume that Xs = {Xs }, i.e., the side-channel signals also follow temporal order and are split into Ks blocks. We additionally assume separate encoder (with possible projector) hs() to process Xs into collection of tokens {zs(ks) RM d}, where zs(ks) 2, ..., Xs Ks 1, Xs is tokens encoded from the ks-th block of the sidechannel. It is worth noting that this formulation encapsulates range of video tasks. We describe some of these tasks considered in our experiments. Audio-visual understanding. This task requires jointly processing video and its accompanying audio data (as side-channel signals) for scene understanding, such as event recognition [66] or QA [1]. 3D scene understanding. This task focuses on reasoning about the 3D scene using video of 3D scan [2]. Camera trajectories, as well as an optional sequence of depth maps, constitute the side-channel signals. Multi-view video understanding. This task involves combining multi-view videos for visual recognition, e.g., using exocentric videos to complement egocentric video for understanding human activities [17]. Enhancing video representations. This task seeks to enhance the visual representation in the Video LLM. This is done by integrating low resolution, high frame rate frames with the original input of high resolution, low frame rate frames, following the key idea of SlowFast networks [13]. 3.3. Our Design of PAVE Our goal is to integrate side-channel signals Xs into the LLM to enhance video reasoning, without modifying the structure of () and encoders (hv() and ht()), nor adding any major set of parameters. To achieve this goal, our key idea is learning function g() to fuse side-channel tokens {zs(ks)} with the original visual tokens {zv(k)}. The fusion results have the same size of {zv(k)}, and will be further used to update {zv(k)}. Formally, our design of PAVE, as shown in Figure 2 (a), is expressed as fusion: {zvs(k)} = g([{zs(ks)}, {zv(k)}]; ϕ) summation: ˆzv(k) = zv(k) + zvs(k), (2) where ϕ denotes learnable parameters of g(). Intuitively, g() injects side-channel information into set of tokens of the same size as the original visual tokens, based on which simple summation can be performed to form residual connection. key feature of this design is that the number of input tokens to () remains unchanged. As the main computational cost lies in (), doing so results in negligible overhead. The fusion function g(). Our fusion function is realized using variant of cross-attention, as illustrated in Figure 2 (b). Specifically, the vision tokens {zv(k)} are transformed into the queries, and the side-channel tokens {zs(ks)} form the keys and values. To align video and side-channel signals in time while maintaining low computation cost, we consider local cross-attention, named temporal-aligned cross-attention, where query zv(k) only attends to keys and values in its temporal neighborhood, i.e., {zs(ks)} with 3 Figure 2. (a) Overview of PAVE. PAVE presents simple, parameter-efficient adapter to integrate videos and side-channel signals. This is done by fusing side-channel tokens zs and video tokens zv, and further adding the results to the original video tokens zv. (b) Details of PAVEs fusion function. The fusion function g() consists of few blocks of temporal-aligned cross-attention layer, MLP, and layer normalization. (c) Temporal-aligned Cross-Attention. Visual tokens zv and side-channel tokens zs are aligned along the temporal axis. video token zv(k) is treated as query, and only attends to keys and values (defined by side-channel tokens) in its temporal neighborhood. ks (k) (see Figure 2 (c)). Before computing the cross attention, rotary positional embeddings [57] are added to the queries and keys. After cross attention, an MLP with layer normalization is applied to further transform the features, similar to standard Transformer block [59]. they produce), as well as their interactions in videos, based on an input of video with its accompanying audio. In this task, we treat audio as the side-channel signals. We now describe our experiment protocol, implementation details, baselines, and results. Integration with LoRA and training loss. We further combine PAVE with LoRA [21] by adding small set of parameters in the form of low rank approximation θ to the LLM (). Putting things together, PAVE minimizes the standard negative log likelihood of its output when adapting to downstream task. This is given by arg min θ,ϕ ED [ log (xa [{ˆzv}, {zq}, {za <i}] ; θ + θ, ϕ)] , where is the data distribution approximated by the training set (Xv, Xq, Xa) D. {ˆzv} is computed using Eq. 2, thus creating dependency on g() and its parameters ϕ. 4. Experiments and Results Our main experiments include (1) audio-visual QA (Sec. 4.1), (2) 3D QA (Sec. 4.2, (3) enhancing video QA by considering high frame rate videos (Sec 4.3), and (4) multiview video recognition (Supplement). In addition, we ablate our design, investigate cross-model generalization, and demonstrate multi-task joint training in Sec. 4.4. Further implementation details for individual experiments can be found in the Supplement. 4.1. Results on Audio-Visual QA Audio-visual QA aims to answer questions related to various visual and auditory concepts (e.g., objects and the sound Experiment protocol. Our experiments consider both open-end QA (AVSD [1]) and closed-end QA (AVQA [69] and Music-AVQA [30]). We follow the protocol in previous works [49, 72] to train and evaluate PAVE using corresponding splits of the datasets. For the open-end AVSD, we use the AVSD@DSTC7 test split and report the CIDEr score as the metric. For the closed-end AVQA and MusicAVQA, we evaluate PAVE on the standard eval / test split, and report the accuracy as the metric. For PAVE, we additionally calculate the model inference FLOPs during prefilling, as well as the total / trainable number of parameters. Implementation details. To encode audio we extract audio features using ImageBind [16]. It samples the audio at 16kHz rate, generating approximately 1 audio token per second. PAVE further integrate these audio tokens into Video LLM (LLaVA-OneVision [28]). We follow the LLaVA-OneVision [28] to evenly sample 32 video frames from an input video. We train PAVE 1 epoch with the AVSD / 2 epochs with AVQA / 2 epochs with Music-AVQA training set, respectively. We then evaluate PAVEs performance on their corresponding test set. Baselines. We consider two types of baseline methods: (1) Multimodal LLMs designed for general video or audio-visual understanding, e.g., LLaVA-OneVision [28] and CAT [72], which have never been trained on the Method Zero-shot LMMs CAT-7B [72] LLaVA-OV-7B [28] Task-specific models MTN [27] COST [49] VALOR [41] VAST [7] PSTP-Net [31] CAT-7B-FT [72] LLaVA-OV-7B-FT PAVE-7B (w/ audio) AVSD [1] AVQA [69] CIDEr Acc. MUSIC-AVQA [30] Audio Acc. Visual Acc. Audio-Visual Acc. Overall Acc. TFLOPs Total / Trainable Params 79.0 70.6 98.5 108.5 - - - - 124.9 152.9 - 85.6 - - - - 90.2 92.0 90.8 93. - 68.8 - - - - - 84.9 75.4 79.7 - 70.6 - - - - - 86.1 89.3 93. - 52.8 - - - - - 83.2 72.3 78.0 48.6 60.4 - - 78.9 80.7 - 84.3 77.4 82. - 98.53 - - - - - - 98.53 98.63 - 8.2B / - - - - - - - 8.2B / 161.5M 8.2B / 170.5M Table 1. Results of audio-visual QA with audio as side-channel signals. We report CIDEr scores on AVSD and the accuracy (Acc.) on AVQA and Music-AVQA. LLaVA-OV-7B-FT refers to directly fine-tuning the LLaVA-OneVision using video. Gray indicates results that require audio-inference only. Our model achieves state-of-the-art performance on AVSD, AVQA and visual split of Music-AVQA by only adding small amount of parameters and FLOPs. target dataset (i.e., zero-short inference); and (2) Taskspecific models, which has been fine-tuned on the target dataset, such as MTN [27] and COST [49] for AVSD, PSTP-Net [31] and CAT-7B-FT [72] for AVQA, and the VALOR [41], VAST [7] and CAT-7B-FT for Music-AVQA. We also include baseline that directly fine-tunes our base model (LLaVA-OneVision) with LoRA on the training set without using the audio, denoted as LLaVA-OV-7B-FT. Results and discussion. Table 1 summarizes the results. PAVE outperforms COST [49] by 44 points in CIDEr scores on AVSD and surpasses CAT-7B-FT by 2% on AVQA, achieving state-of-the-art results. On Music-AVQA, PAVE outperforms latest methods by 7% in the visual split, where the answers can be derived from input video, but lags behind CAT-7B-FT on audio and audio-visual splits, where the answers are from audio alone or must be reasoned by jointly considering audio and video. Compared to LLaVA-OV7B-FT, PAVE consistently improves performance across three datasets, adding only 9M parameters and 0.1 TFLOPs (0.1% of total parameters and FLOPs). While PAVE is not designed for audio-only inference (Music-AVQAs audio split), its performance gap on the audio-visual split is puzzling. Our observation is that part of the questions in this audio-visual split deliberately feature conflicting audio and visual information, e.g., the sound of piano coupled with the video with guitar (see the last example in Figure 3). We find that PAVE, based on Video LLM trained on video-text data, often prioritizes visual cues, leading to degraded performance. 4.2. Results on 3D QA 3D QA focuses on the reasoning about spatial location of individual objects or relative position between objects, based on an input video scan with camera poses and depth information. Previously, the input data is often converted into voxel or mesh-based 3D representation, based on which the reasoning is performed. In this work, we instead treat the 3D cues (camera poses and depth maps) as the sidechannel, and leverage pre-trained Video LLMs for 3D QA. Again, we describe our experiment protocol, implementation details, baselines, and results."
        },
        {
            "title": "Our",
            "content": "experiments Experiment protocol. consider ScanQA [2] and SQA3D [44] datasets, using their corresponding train / test splits. Following previous work [84], we report the CIDEr (C), BLEU-4 (B-4), METEOR (M), ROUGE(R), and top-1 Exact Match (EM@1) metrics on ScanQA and report EM@1 on SQA3D. Similarly, we also report model inference FLOPs and parameters for PAVE. Implementation details. To encode 3D cues, we use the 3D encoder from LLaVA-3D [84]. It encodes camera pose, RGB images, and depth information into multi-view features defined on the RGB image plane. We again build PAVE with LLaVA-OneVision [28]. Specifically, we evenly sample 32 RGB-D frames with their camera poses from the scanning. The 3D encoder creates sequence of 2D feature maps (i.e., multi-view features), leading to around 18K 3D tokens per scan. The visual encoder in LLaVAOneVision separately embeds the 32 key RGB frames into video tokens. PAVE further integrates video tokens and 3D tokens. We train the PAVE 1 epoch with ScanQA / 2 epochs with SQA3D training set, and then evaluating PAVE performance on their test sets, respectively. Baselines. We again compare our method with two types of baselines: (1) Video LLMs for general video understanding, e.g., LLaVA-OneVision[28] and VideoChat2 [33], with zero-shot inference; and (2) Task-specific models fine-tuned on the target dataset, e.g., LEO [22], 3D-LLM [20], SceneLLM [15] and LLaVA-3D [84]. We also add baseline that directly fine-tunes LLaVA-OneVision with LoRA without using 3D cues, denoted as LLaVA-OV-7B-FT. Results. Our results are presented in Table 2. In comparison to our baselines, PAVE achieves highly competitive ScanQA [2] B-4 EM@1 SQA3D [44] EM@1 TFLOPs Total / Trainable Params Method Zero-shot LMMs VideoChat2-7B [34] LLaVA-OV-7B [28] Task-specific models 3D-LLM-7B [20] LEO-7B [22] Scene-LLM-7B [15] LLaVA-3D-7B [84] LLaVA-OV-7B-FT 49.2 91.0 9.6 5.3 9.5 18. 28.2 45.9 19.2 26.7 (44.3) 37.3 8.3 (50.7) 74.5 101.4 80.0 91.7 95.1 12.9 13.2 12.0 14.5 13.5 15.1 20.0 16.6 20.7 19. 37.5 49.2 40.0 50.1 47.4 21.2 24.5 (47.6) 27.2 27.0 (45.0) 27.4 (46.3) 49.79 50.0 (52.4) 54.2 55.6 (57.6) 55.8 (58.1) - 98.53 - - - - 98.53 98. - 8.2B / - - - - - 8.2B / 161.5M 8.2B / 170.5M PAVE-7B (w/ 3D info) 103.4 16. 19.9 49.0 29.1 (48.5) 59.0 (61.4) Table 2. Results of 3D QA with 3D cues as side-channel signals. We report CIDEr (C), BLEU-4 (B-4), METEOR (M), ROUGE(R) for ScanQA, and top-1 Exact Match (EM@1) for both ScanQA and SQA3D datasets. LLaVA-OV-7B-FT refers to directly fine-tuning the LLaVA-OneVision on ScanQA or SQA3D. Gray indicates evaluation results with refined exact-match protocol. PAVE achieves state-ofthe-art performance on both ScanQA and SQA3D with small number of additional parameters and FLOPs. performance across ScanQA and SQA3D datasets. In particular, PAVE attains significant gains in CIDEr and EM@1 scores. For example, on EM@1, PAVE outperforms the best reporting result by 2-3% in absolute values. Importantly, PAVE only adds 9M parameters and 0.15 TFLOPs on top of the base model during inference, accounting for 0.1% of the total parameters and compute cost. Interestingly, finetuning the base model using videos only and without 3D cues (LLaVA-OV-7B-FT) also has competitive results, indicating strong capability of Video LLMs for 3D reasoning. 4.3. Results on Enhanced Video QA Video LLMs [28, 36] often sample fixed number of frames (e.g. 8 or 32) to represent the input video. These sparsely sampled frames may miss detailed temporal information, especially for tasks that require fine-grained motion information. We now consider injecting high frame rate videos as the side-channel signals. We describe our experiment protocol, implementation details, baselines, and results. Experiment protocol. To simply our experiment, we create subset from LLaVA-Video-178K [79] for training PAVE. We first sample all videos longer than 1 minute and then randomly choosing 2 question-answer pairs for each video, leading to 57K videos and 114K question-andanswer pairs. To evaluate PAVE, we consider set of video benchmarks including VideoMME [14], MVBench [33], and MLVU [81]. VideoMME and MVBench are both comprehensive video benchmarks and cover different types of subtasks, while MLVU focuses on long video understanding. All benchmarks use accuracy as the performance metric. Similar to other experiments, we report inference FLOPs and parameters for PAVE. Results on additional benchmarks are included in the Supplement. Implementation details. For side-channel signals, we densely sample the video frame at 2 fps and use the LanguageBind [83] to extract features. Motivated by SlowFast networks [13], we downsample the spatial resolution of LanguageBinds features to 2 2 and treat it as the fast stream, while the original visual features used by the base Video LLM (LLaVA-OneVision) are regarded as the slow stream. We train PAVE for 1 epoch using our subset. Baselines. We use the LLaVA-OneVision 0.5B and 7B models as our baselines, as they achieve top performance on various video benchmarks. This setup allows us to assess PAVEs performance across different model sizes. Results. Table 3 presents our results. In comparison to the base model (LLaVA-OneVision), PAVE achieves consistent improvements across 3 datasets and 2 model sizes. On VideoMME, PAVE achieves major boosts (1.0-4.4%) on the short and median splits, which contain videos below 15 minutes on average, and only minor gains (0.1-0.3%) on longer videos ranging from 30 minutes to 1 hour. On MVBench, PAVE improve the baseline by 1.1% to 1.3% on average. Specifically, PAVE-0.5B outperforms the baseline by 5% on the fine-grained pose (FGP) task, and PAVE-7B improves by 4.5% on the object shuffle (OS) task. On the long video benchmark MLVU, PAVE shows significant improvement (1.3-2.3%). 4.4. Further Analysis Ablation: design of PAVE. In this part, we conduct an ablation study to analyze key design choices of PAVE. We ablate two possible approaches to combine signal-channel tokens zs with the original video tokens zv: (1) Interleaved with the zv as used in [29], noted as FT (interleave), resulting in significantly heightened computational cost due to the increased number of input tokens. (2) Added to zv as considered in PAVE, where the summation operation requires that side-channel tokens zs have the same shape as video tokens zv. When using the summation operation, we further consider two design choices for the cross-attention: (a) Considering learnable tokens as the query, similar to [23], noted as the PAVE (learnable) and (b) Using the original visual tokens zv as the query, noted as the PAVE (visual). For 6 Method LLaVA-OV-0.5B [28] LLaVA-OV-7B [28] PAVE-0.5B (w/ video feature) PAVE-7B (w/ video feature) VideoMME [14] Short Median Long Avg 53.4 70.1 57.8 71.1 41.2 56.6 42.7 59. 37.3 48.9 37.4 49.2 44.0 58.2 46.0 59.9 MVBench [33] FGP OS 49.0 53. 54.0 54.5 33.0 35.5 33.5 39.0 Avg 45.5 56.7 46.6 58. SC 37.5 52.0 40.0 51.5 MLVU [81] FLOPs(TB) Total / Trainable Params 50.3 64. 51.6 67.0 8.01 98.53 8.08 98.63 0.9B / - 8.2B / - 0.9B / 41.4M 8.2B / 170.5M Table 3. Results of using high frame rate videos to enhance video QA, where densely sampled video frames are treated as side-channel signals. For MVBench, we report performance on state change (SC), fine-grained pose (FGP), and object shuffle (OS) subsets. PAVE consistently outperforms baselines on MLVU, VideoMME, and the tasks that need fine-grained motion information in MVBench."
        },
        {
            "title": "Design",
            "content": "EM@1 FLOPs (TB) Trainable Params Method LLaVA-OV-0.5B LLaVA-OV-7B Video-LLaVA-7B [36] Zero-shot FT FT (interleave) PAVE (learnable) PAVE (visual) 0.2 (28.0) 20.5 (36.3) 23.0 (39.9) 22.5 (39.3) 23.1 (40.0) 8.01 8.01 71.41 8.13 8.13 - 35.1M 35.1M 43.9M 41.4M Table 4. Ablation study on the design of PAVE on 3DQA ScanQA benchmark. We report the Top-1 Exact Match score, FLOPs, and the number of trainable parameters. reference, we also include two baselines: (Zero-shot) the zero-shot performance of Video LLM on the ScanQA, and (FT) Video LLM performance on the ScanQA after being fine-tuned on the ScanQA training split with LoRA. We conduct experiments with the LLaVA-OneVision 0.5B model on the ScanQA dataset. We report top-1 Exact Match (EM@1), FLOPs and trainable parameters in Table 4. PAVE (visual) achieves the best performance. Compared with FT (interleave), PAVE effectively reduces the number of tokens sent to the LLM, thereby drastically reducing FLOPs. Further, PAVE (visual) outperforms FT (interleave) in both accuracy and efficiency, indicating that it can more effectively incorporate side-channel information. For query design, we observe that PAVE (visual) yields better performance than PAVE (learnable). We conjecture that explicit alignment between video tokens and side-channel tokens may be critical here. Generalization across Video LLMs. Moving forward, we demonstrate that PAVE can be applied to different models and various model scales. We train PAVE using pretrained LLaVA-Onevision [28] and Video-LLaVA [36] as base models. We choose LLaVA-OneVision as it provides models at both 0.5B and 7B scales, and Video-LLaVA since its pre-training set and model structure differ from those of LLaVA-OneVision, potentially impacting PAVEs adaptation. We conduct experiments on the ScanQA dataset, reporting top-1 Exact Match (EM@1) as the metric. Table 5 presents our results. PAVE can adapt Video LLMs at various scales, accommodate different model architectures, and work with models trained on diverse instruction datasets. Multi-task learning. Moreover, we explore the potential of training multiple patches simultaneously and study the effects of such multi-task learning. We consider the setZero-shot Finetune PAVE 0.2 (28.0) 20.5 (36.3) 23.1 (40.0) 26.7 (44.3) 27.4 (46.3) 29.1 (48.5) 0.0 (25.1) 21.6 (37.3) 25.2 (42.1) Table 5. Generalization of PAVE to Video LLMs with different architectures and sizes. PAVE shows consistent improvements across 3 settings, demonstrating its effectiveness and versatility. Model CIDEr FLOPs (TB) Total / Trainable Params PAVE-7B (audio) PAVE-7B (audio + dense-frames) 152.5 160.0 98.53 98. 8.2B / 170.3M 8.2B / 170.3M Table 6. Multi-task learning with PAVE on the AVSD dataset. Combining both audio and high frame rate patches leads to substantial performance improvement. ting of multiple tasks share overlapping side-channels (e.g., high-frame-rate video). In this case, multiple patches can be learned jointly, leading to potential enhancement across their corresponding side-channels. As first step to demonstrate this possibility, we design an experiment to integrate high frame rate video patch and audio-visual path for high frame rate audio-visual QA. Specifically, we build on the patch learned for injecting high frame rate videos (PAVE-7B model in Table 3), and train an additional patch for audio-visual QA on the AVSD training set. During training, the old patch is kept frozen and only the audio-visual patch is learned. During the inference, we activate both patches, and PAVE takes input key frames (i.e. low frame rate video), high frame rate video, and audio as input. Table 6 shows that our multi-task learning leads to notable improvement (+7.5 in CIDEr scores) on the AVSD test set, demonstrating the possibility and the benefit of training multiple patches together. We provide further discussion in Sec. 5. Results visualization and diagnosis. Finally, we visualize sample results of 3D QA and audio-visual QA in Figure 3. An interesting failure case of PAVE, as we previously mentioned in Sec. 4.1 is shown in sample (6), where the piano is present in the audio but never appears visually in the video, creating conflict between the auditory and visual information. In this case, PAVE produces inaccurate answers. To further diagnose our model, we visualize crossattention scores between video tokens and side-channel tokes in PAVEs fusion function. Figure 4 shows heatmaps 7 Figure 3. Visualization of sample results. We visualize the compare the results from our base model LLaVA-OneVision (under zero-shot inference) and PAVE across 3D QA and audio-visual QA tasks. Both succeful and failure cases are shown. Figure 4. Visualization of cross-attention scores in PAVE when injecting high frame rate videos as the side-channel. Cross-attention scores are calculated between the selected video tokens from the original low frame rate video (red cells on the left) and side-channel tokens from the high frame rate video. Scores are displayed as heatmaps over densely sampled video frames (on the right). of cross-attention scores when injecting high high frame rate videos as the side-channel (corresponding to results of PAVE-7B in Table 3). We calculate the cross-attention scores between the selected video tokens (shown in the red cells) from the original low frame rate videos, and the sidechannel tokens from the high frame rate videos. The visualization shows that the peaks of the heatmap keep track of visually similar regions in densely sampled video frames. This result indicates that the fusion function in PAVE facilitates explicit alignment between video tokens and sidechannel tokens, and thus allows for effectively integrating information from the side-channel. 5. Conclusion and Discussion In this paper, we addressed the problem of adapting pretrained Video LLMs to downstream tasks involving sidechannel signals additional modalities or data types such as audio, 3D cues, high frame rate or multi-view videos. To this end, we presented PAVE, flexible framework that enables adaptation through patching. PAVE built on lightweight adapters (i.e., patches), which adds small number of parameters and operations (0.1%) to base model without modifying its architecture or pre-trained weights. We demonstrated that PAVE can effectively adapt various video LLMs across multiple tasks, often surpassing state-of-the-art task-specific models. Additionally, we showed encourage results of learning across multiple tasks with PAVE. We believe that our work provides solid step towards adapting Video LLMs for diverse tasks, and sheds light on the broader topic of multi-modal reasoning. Distributing and deploying tasks-specific patches. key strength of PAVE is that only small patch is need for adapting large pre-trained Video LLM to new task. For example, for LLaVA-OneVision-7B, this patch is only 20 MB, while the model occupies 16 GB. We anticipate that this compactness makes it practical to distribute and deploy individual patches, similar to LoRA weights used in diffusion models, thereby facilitating broader adoption of Video LLMs in downstream applications. Towards multi-task learning. An exciting future research direction is the joint learning of multiple patches across tasks. While we have demonstrated the feasibility of stagewise multi-task learning, future work should aim to handle multi-task joint training and explore methods for combining individual patches to enable flexible task composition. Acknowledgment: This work was partially supported by National Science Foundation under Grant No. CNS 2333491, by the Army Research Lab under contract number W911NF-2020221, and by contract from General Motors."
        },
        {
            "title": "References",
            "content": "[1] Huda Alamri, Vincent Cartillier, Abhishek Das, Jue Wang, Anoop Cherian, Irfan Essa, Dhruv Batra, Tim Marks, Chiori Hori, Peter Anderson, et al. Audio visual sceneaware dialog. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7558 7567, 2019. 1, 2, 3, 4, 5, 14 [2] Daichi Azuma, Taiki Miyanishi, Shuhei Kurita, and Motoaki Kawanabe. ScanQA: 3D question answering for spatial scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1912919139, 2022. 1, 3, 5, 6, 14, 15 [3] Amir Bar, Yossi Gandelsman, Trevor Darrell, Amir Globerson, and Alexei Efros. Visual prompting via image inpainting. Advances in Neural Information Processing Systems, 35:2500525017, 2022. 2 [4] Deblina Bhattacharjee, Sabine Susstrunk, and Mathieu Salzmann. Vision transformer adapters for generalizable multitask learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1901519026, 2023. 2 [5] Rishi Bommasani, Drew Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. 1 [6] Shoufa Chen, Chongjian Ge, Zhan Tong, Jiangliu Wang, Yibing Song, Jue Wang, and Ping Luo. AdaptFormer: Adapting vision transformers for scalable visual recognition. Advances in Neural Information Processing Systems, 35:1666416678, 2022. [7] Sihan Chen, Handong Li, Qunbo Wang, Zijia Zhao, Mingzhen Sun, Xinxin Zhu, and Jing Liu. Vast: vision-audio-subtitle-text omni-modality foundation model and dataset. Advances in Neural Information Processing Systems, 36:7284272866, 2023. 3, 5 [8] Zhe Chen, Yuchen Duan, Wenhai Wang, Junjun He, Tong Lu, Jifeng Dai, and Yu Qiao. Vision transformer adapter for dense predictions. In The Eleventh International Conference on Learning Representations, 2023. 2 [9] Wei-Lin Chiang, Zhuohan Li, Zi Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph E. Gonzalez, Ion Stoica, and Eric P. Xing. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, 2023. 2 [10] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. ScanNet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. 15 [11] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. 15 [12] Gueter Josmy Faure, Jia-Fong Yeh, Min-Hung Chen, HungTing Su, Winston H. Hsu, and Shang-Hong Lai. HERMES: temporal-coherent long-form understanding with episodes and semantics. arXiv preprint arXiv:2408.17443, 2024. [13] Christoph Feichtenhofer, Haoqi Fan, Jitendra Malik, and In Kaiming He. Slowfast networks for video recognition. Proceedings of the IEEE/CVF international conference on computer vision, pages 62026211, 2019. 2, 3, 6, 15 [14] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-MME: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. 2, 6, 7, 16 [15] Rao Fu, Jingyu Liu, Xilun Chen, Yixin Nie, and Wenhan Xiong. Scene-LLM: Extending language model for arXiv preprint 3d visual understanding and reasoning. arXiv:2403.11401, 2024. 2, 5, 6 [16] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1518015190, 2023. 3, 4, 14 [17] Kristen Grauman, Andrew Westbury, Lorenzo Torresani, Kris Kitani, Jitendra Malik, Triantafyllos Afouras, Kumar Ashutosh, Vijay Baiyya, Siddhant Bansal, Bikram Boote, et al. Ego-Exo4D: Understanding skilled human activity from first-and third-person perspectives. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1938319400, 2024. 2, 3, 13, 15 [18] Albert Gu and Tri Dao. Mamba: Linear-time sequence arXiv preprint modeling with selective state spaces. arXiv:2312.00752, 2023. 2 [19] Yuchao Gu, Xintao Wang, Jay Zhangjie Wu, Yujun Shi, Yunpeng Chen, Zihan Fan, Wuyou Xiao, Rui Zhao, Shuning Chang, Weijia Wu, et al. Mix-of-show: Decentralized lowrank adaptation for multi-concept customization of diffusion models. Advances in Neural Information Processing Systems, 36:1589015902, 2023. 2 [20] Yining Hong, Haoyu Zhen, Peihao Chen, Shuhong Zheng, Yilun Du, Zhenfang Chen, and Chuang Gan. 3d-llm: Injecting the 3d world into large language models. Advances in Neural Information Processing Systems, 36:2048220494, 2023. 5, [21] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In International Conference on Learning Representations, 2022. 2, 3, 4 [22] Jiangyong Huang, Silong Yong, Xiaojian Ma, Xiongkun Linghu, Puhao Li, Yan Wang, Qing Li, Song-Chun Zhu, Baoxiong Jia, and Siyuan Huang. An embodied generalist agent in 3d world. arXiv preprint arXiv:2311.12871, 2023. 5, 6 [23] Andrew Jaegle, Sebastian Borgeaud, Jean-Baptiste Alayrac, Carl Doersch, Catalin Ionescu, David Ding, Skanda Koppula, Daniel Zoran, Andrew Brock, Evan Shelhamer, 9 Olivier Henaff, Matthew Botvinick, Andrew Zisserman, Oriol Vinyals, and Joao Carreira. Perceiver IO: general architecture for structured inputs & outputs. In International Conference on Learning Representations, 2022. 6 [24] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. ViIn European conference on computer sual prompt tuning. vision, pages 709727. Springer, 2022. [25] Bruno Korbar, Yongqin Xian, Alessio Tonioni, Andrew Zisserman, and Federico Tombari. Text-conditioned resampler for long form video understanding. In European Conference on Computer Vision, pages 271288. Springer, 2024. 2 [26] Nupur Kumari, Bingliang Zhang, Richard Zhang, Eli Shechtman, and Jun-Yan Zhu. Multi-concept customization of text-to-image diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 19311941, 2023. 2 [27] Hung Le, Doyen Sahoo, Nancy Chen, and Steven Hoi. Multimodal transformer networks for end-to-end video-grounded dialogue systems. In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics. Association for Computational Linguistics, 2019. 5 [28] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. 1, 2, 4, 5, 6, 7, 13, 14, 15 [29] Feng Li, Renrui Zhang, Hao Zhang, Yuanhan Zhang, Bo Li, Wei Li, Zejun Ma, and Chunyuan Li. Llava-next-interleave: Tackling multi-image, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024. 6 [30] Guangyao Li, Yake Wei, Yapeng Tian, Chenliang Xu, JiRong Wen, and Di Hu. Learning to answer questions in dynamic audio-visual scenarios. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2022. 4, 5, 14 [31] Guangyao Li, Wenxuan Hou, and Di Hu. Progressive spatiotemporal perception for audio-visual question answering. In Proceedings of the 31st ACM International Conference on Multimedia, pages 78087816, 2023. [32] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. BLIP-2: Bootstrapping language-image pre-training with In Infrozen image encoders and large language models. ternational Conference on Machine Learning, pages 19730 19742. PMLR, 2023. 2 [33] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. MVBench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 2, 5, 6, 7, 16 [34] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22195 22206, 2024. 6 [35] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: new baseline for efficient model tuning. Advances in Neural Information Processing Systems, 35:109123, 2022. 2 [36] Bin Lin, Yang Ye, Bin Zhu, Jiaxi Cui, Munan Ning, Peng Jin, and Li Yuan. Video-LLaVA: Learning united visual representation by alignment before projection. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 59715984, Miami, Florida, USA, 2024. Association for Computational Linguistics. 2, 6, 7 [37] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 740755. Springer, 2014. 14 [38] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 2 [39] Haotian Liu, Chunyuan Li, Yuheng Li, Bo Li, Yuanhan Zhang, Sheng Shen, and Yong Jae Lee. LLaVA-NeXT: Improved reasoning, ocr, and world knowledge, 2024. 2 [40] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36, 2024. [41] Jing Liu, Sihan Chen, Xingjian He, Longteng Guo, Xinxin Zhu, Weining Wang, and Jinhui Tang. Valor: Vision-audiolanguage omni-perception pretraining model and dataset. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. 3, 5 [42] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 14 [43] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. CLIP4Clip: An empirical study of clip for end to end video clip retrieval and captioning. Neurocomputing, 508:293304, 2022. 3 [44] Xiaojian Ma, Silong Yong, Zilong Zheng, Qing Li, Yitao Liang, Song-Chun Zhu, and Siyuan Huang. SQA3D: Situated question answering in 3d scenes. In International Conference on Learning Representations, 2023. 2, 5, 6, 14, 15 [45] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-ChatGPT: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. 2 [46] Meta. LLaMA3.2. https://ai.meta.com/blog/ llama - 3 - 2 - connect - 2024 - vision - edge - mobile-devices/, 2024. [47] Thong Thanh Nguyen, Zhiyuan Hu, Xiaobao Wu, CongDuy Nguyen, See-Kiong Ng, and Anh Tuan Luu. Encoding and controlling global semantics for long-form video In Proceedings of the 2024 Conferquestion answering. ence on Empirical Methods in Natural Language Processing, pages 70497066, Miami, Florida, USA, 2024. Association for Computational Linguistics. 2 10 [48] OpenAI. GPT-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 1 [49] Hoang-Anh Pham, Thao Minh Le, Vuong Le, Tu Minh Phuong, and Truyen Tran. Video dialog as conversation about objects living in space-time. In European Conference on Computer Vision, pages 710726. Springer, 2022. 4, 5, 14 [50] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie Gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. 2 [51] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, pages 87488763. PMLR, 2021. 3 [52] Sylvestre-Alvise Rebuffi, Hakan Bilen, and Andrea Vedaldi. Learning multiple visual domains with residual adapters. Advances in neural information processing systems, 30, 2017. 2 [53] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [54] Viraj Shah, Nataniel Ruiz, Forrester Cole, Erika Lu, Svetlana Lazebnik, Yuanzhen Li, and Varun Jampani. ZipLoRA: Any subject in any style by effectively merging LoRAs. In European Conference on Computer Vision, pages 422438. Springer, 2024. 2 [55] Yuzhang Shang, Mu Cai, Bingxin Xu, Yong Jae Lee, and Yan Yan. LLaVA-PruMerge: Adaptive token reduction for efficient large multimodal models. arXiv preprint arXiv:2403.15388, 2024. 16 [56] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, et al. LongVU: Spatiotemporal adaptive compression for long video-language understanding. arXiv preprint arXiv:2410.17434, 2024. 2 [57] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 4 [58] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. VL-adapter: Parameter-efficient transfer learning for vision-and-language In Proceedings of the IEEE/CVF conference on tasks. computer vision and pattern recognition, pages 52275237, 2022. [59] Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. 4 [60] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-VL: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 2 [61] Wen Wang, Canyu Zhao, Hao Chen, Zhekai Chen, Kecheng Zheng, and Chunhua Shen. Autostory: Generating diverse Internastorytelling images with minimal human efforts. tional Journal of Computer Vision, pages 122, 2024. 2 [62] Xinlong Wang, Wen Wang, Yue Cao, Chunhua Shen, and Images speak in images: generalist Tiejun Huang. In Proceedings of painter for in-context visual learning. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 68306839, 2023. 2 [63] Xidong Wang, Dingjie Song, Shunian Chen, Chen Zhang, and Benyou Wang. LongLLaVA: Scaling multi-modal llms to 1000 images efficiently via hybrid architecture. arXiv preprint arXiv:2409.02889, 2024. 2 [64] Yuxuan Wang, Cihang Xie, Yang Liu, and Zilong Zheng. VideoLLaMB: Long-context video understanding with recurrent memory bridges. arXiv preprint arXiv:2409.01071, 2024. [65] Yuetian Weng, Mingfei Han, Haoyu He, Xiaojun Chang, and Bohan Zhuang. Longvlm: Efficient long video understanding via large language models. In European Conference on Computer Vision, pages 453470. Springer, 2024. 2 [66] Fanyi Xiao, Yong Jae Lee, Kristen Grauman, Jitendra Malik, and Christoph Feichtenhofer. Audiovisual slowfast networks arXiv preprint arXiv:2001.08740, for video recognition. 2020. 3 [67] Guangxuan Xiao, Tianwei Yin, William Freeman, Fredo Durand, and Song Han. Fastcomposer: Tuning-free multisubject image generation with localized attention. International Journal of Computer Vision, pages 120, 2024. 2 [68] Chengming Xu, Chen Liu, Yikai Wang, Yuan Yao, and Yanwei Fu. Towards global optimal visual in-context learning prompt selection. In Advances in Neural Information Processing Systems, pages 7494574965, 2024. 2 [69] Pinci Yang, Xin Wang, Xuguang Duan, Hong Chen, Runze Hou, Cong Jin, and Wenwu Zhu. AVQA: dataset for audiovisual question answering on videos. In Proceedings of the 30th ACM international conference on multimedia, pages 34803491, 2022. 2, 4, 5, 14 11 ternational Journal of Computer Vision, 130(9):23372348, 2022. [83] Bin Zhu, Bin Lin, Munan Ning, Yang Yan, Jiaxi Cui, WANG HongFa, Yatian Pang, Wenhao Jiang, Junwu Zhang, Zongwei Li, Cai Wan Zhang, Zhifeng Li, Wei Liu, and Li Yuan. Languagebind: Extending video-language pretraining to nIn The modality by language-based semantic alignment. Twelfth International Conference on Learning Representations, 2024. 2, 3, 6, 15 [84] Chenming Zhu, Tai Wang, Wenwei Zhang, Jiangmiao Pang, and Xihui Liu. LLaVA-3D: simple yet effective pathway to empowering lmms with 3d-awareness. arXiv preprint arXiv:2409.18125, 2024. 2, 5, 6, 15 [70] Yanting Yang, Minghao Chen, Qibo Qiu, Jiahao Wu, Wenxiao Wang, Binbin Lin, Ziyu Guan, and Xiaofei He. Adapt2reward: Adapting video-language models to generIn European alizable robotic rewards via failure prompts. Conference on Computer Vision, pages 163180. Springer, 2024. 2 [71] Jiabo Ye, Haiyang Xu, Haowei Liu, Anwen Hu, Ming Yan, Qi Qian, Ji Zhang, Fei Huang, and Jingren Zhou. mPLUG-Owl3: Towards long image-sequence understanding in multi-modal large language models. arXiv preprint arXiv:2408.04840, 2024. 2 [72] Qilang Ye, Zitong Yu, Rui Shao, Xinyu Xie, Philip Torr, and Xiaochun Cao. Cat: Enhancing multimodal large language model to answer questions in dynamic audio-visual scenarIn European Conference on Computer Vision, pages ios. 146164. Springer, 2024. 2, 4, 5, 14 [73] Shoubin Yu, Jaemin Cho, Prateek Yadav, and Mohit Bansal. Self-chained image-language model for video localization and question answering. Advances in Neural Information Processing Systems, 36, 2024. [74] Zhihang Yuan, Yuzhang Shang, Yang Zhou, Zhen Dong, Zhe Zhou, Chenhao Xue, Bingzhe Wu, Zhikai Li, Qingyi Gu, Yong Jae Lee, et al. LLM inference unveiled: Survey and roofline model insights. arXiv preprint arXiv:2402.16363, 2024. 16 [75] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 13, 14 [76] Peiyuan Zhang, Kaichen Zhang, Bo Li, Guangtao Zeng, Jingkang Yang, Yuanhan Zhang, Ziyue Wang, Haoran Tan, Chunyuan Li, and Ziwei Liu. Long context transfer from language to vision. arXiv preprint arXiv:2406.16852, 2024. 2 [77] Yiyuan Zhang, Kaixiong Gong, Kaipeng Zhang, Hongsheng Li, Yu Qiao, Wanli Ouyang, and Xiangyu Yue. Metatransformer: unified framework for multimodal learning. arXiv preprint arXiv:2307.10802, 2023. 3 [78] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context learning? Advances in Neural Information Processing Systems, 36:1777317794, 2023. 2 [79] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 2, 6, [80] Ming Zhong, Yelong Shen, Shuohang Wang, Yadong Lu, Yizhu Jiao, Siru Ouyang, Donghan Yu, Jiawei Han, and Weizhu Chen. Multi-LoRA composition for image generation. arXiv preprint arXiv:2402.16843, 2024. 2 [81] Junjie Zhou, Yan Shu, Bo Zhao, Boya Wu, Zhengyang Liang, Shitao Xiao, Minghao Qin, Xi Yang, Yongping Xiong, Bo Zhang, Tiejun Huang, and Zheng Liu. MLVU: Benchmarking multi-task long video understanding. arXiv preprint arXiv:2406.04264, 2025. 2, 6, 7, 16 [82] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei InLiu. Learning to prompt for vision-language models. 12 In this supplement, we (1) show additional experiment results on small Video LLM and multiple-view video understanding (Section A); (2) describe additional implementation details (Section B); (3) include additional visualization of the question-answering results (Section C). A. Additional Experiment Results A.1. Results with small Video LLM We now present additional experiment results of PAVE with LLaVA-OneVision 0.5B models for audio-visual QA and 3D QA. Table 7 and Table 8 show the results. PAVE consistently improves the 0.5B and 7B Video LLMs performance by large margin across both settings. This indicates that PAVE effectively leverages additional information when adapting pre-trained Video LLMs into new settings. A.2. Results on Enhanced Video Understanding We present PAVEs result on additional benchmarks in the enhanced video understanding setting. Table 9 shows PAVEs results in the enhanced video understanding setting with additional benchmarks. PAVE demonstrates substantial performance gain on VideoMME (w-subtitles). However, we observe only marginal or no improvement on ActivityNet-QA, EgoSchema, NextQA, and PerceptionTest. We hypothesize that this discrepancy may be due to: (1) domain shiftour training data primarily consists of third-person view videos, which may lead to performance drop in EgoSchema, and (2) the nature of the benchmark questions, which may not require densely temporal information for reasoning. A.3. Results on Multi-view Video Understanding Motivation and task set up. Understanding human activity from video is crucial in many real-world applications, such as augmented reality and robotic learning. Based on the perspective, videos can be broadly classified into ego-centric and exo-centric views. Ego-centric videos capture firstperson interactions, focusing on close-up hand-object interactions, while exo-centric videos provide third-person perspective, recording full-body postures and the surrounding environment. Both perspectives are essential for comprehensive human action understanding. Different from the audio-visual QA and 3D QA, where the side-channel information comes from other modalities, in this context, PAVE regards exo-centric videos as side-channel information and integrates it with ego-centric video to adapt the Video LLMs for multi-view video understanding. Training data. We use the training set from the Ego-Exo4D demonstrator proficiency estimation benchmark [17] as our training data, which consists of 1,904 question-answer pairs. Each pair is associated with one ego-centric video and four exo-centric videos. The task requires the model to classify human action proficiency into one of four categories: Novice, Early Expert, Intermediate Expert, or Late Expert, based on both egoand exo-centric videos. However, only 1,656 question-answer pairs include the corresponding videos, as the videos for the remaining pairs could not be downloaded due to privacy issues. Implementation details. Considering the exoand egocentric videos are synchronized along the temporal axis, we sample 32 frames for each of the exo-centric videos. To keep the encoding procedure consistent between the egoand exo-video, we use the same preprocessing of the LLaVA-OneVision to reshape and crop the video frames. We use SigLIP [75] as the visual encoder and it encodes and downsamples each frame into 196 tokens. We pre-extract the exo-video feature tokens offline to accelerate the training. We build PAVE on top of LLaVA-OneVision [28] and train the model for 2 epochs. Evaluation benchmark. We use the validation set of the Ego-Exo4D [17] demonstrator proficiency estimation benchmark for evaluation and report accuracy as the metric. It contains 466 questions and each of the questions is paired with 1 ego-centric video and 4 exo-centric videos. Baselines. We use the TimeSFormer (Ego+Exo) from EgoExo4D [17] as our baseline. We also include baseline that directly fine-tunes the LLaVA-OneVision with LoRA on the training set without using the exo-centric videos, denoted as LLaVA-OV-7B-FT. This baseline allows us to assess whether PAVE can effectively utilize supplementary information. Results. Table 10 shows the results of PAVE. Compared with the LLaVA-OV-7B-FT, PAVE-7B achieves about 14.4% improvement by adding only 9M parameters and 0.17 TFLOPs during inference. This big improvement indicates that the exo-centric videos provide crucial additional information for human action understanding. Moreover, PAVE achieves state-of-the-art performance on the demonstrator proficiency estimation benchmark, substantiating that PAVE can adapt pre-trained Video LLM to an unseen setting by leveraging supplementary information. B. Implementation and Experiment Details We first describe the general implementation detail of the PAVE in Section B.1. Then, we describe the experiment details for 3 settings considered in the main paper, including audio-visual QA (Section B.2), 3DQA (Section B.3), and enhancing video QA (Section B.4). We also demonstrate how we calculate the Flops for the model in Section B.5. B.1. Implementation Detail of PAVE Inside the temporal-aligned cross-attention layer, we add rotary position embedding to the query and key tokens."
        },
        {
            "title": "Method",
            "content": "Zero-shot LMMs LLaVA-OV-0.5B [28] LLaVA-OV-7B [28] Task-specific models LLaVA-OV-0.5B-FT LLaVA-OV-7B-FT PAVE-0.5B (w/ audio) PAVE-7B (w/ audio) AVSD [1] AVQA [69]"
        },
        {
            "title": "CIDEr",
            "content": "Acc. MUSIC-AVQA [30] Audio Acc. Visual Acc. Audio-Visual Acc. Overall Acc. TFLOPs Total / Trainable Params 65.1 70.6 117.6 124.9 134.5 152. 77.4 85.6 86.4 90.8 90.4 93.8 60.0 68.8 69.6 75.4 77.3 79. 57.1 70.6 76.3 89.3 89.8 93.0 48.5 52.8 62.8 72.3 74.1 78. 52.8 60.4 67.6 77.4 78.8 82.3 8.01 98.53 8.01 98.53 8.08 98. 0.9B / - 8.2B / - 0.9B / 35.2M 8.2B / 161.5M 0.9B / 41.4M 8.2B / 170.5M Table 7. Additional result of PAVE on the audio-visual understanding tasks with audio as additional information."
        },
        {
            "title": "Method",
            "content": "Zero-shot LMMs LLaVA-OV-0.5B [28] LLaVA-OV-7B [28] Task-specific models LLaVA-OV-0.5B-FT LLaVA-OV-7B-FT 17.2 91.0 70.5 95.1 PAVE-0.5B (w/ 3D info) PAVE-7B (w/ 3D info) 84.2 103.4 ScanQA [2] B-4 EM@1 SQA3D[44] EM@1 TFLOPs Total / Trainable Params 1.2 5.3 13.7 18.2 18.4 45.9 0.2 (28.0) 26.7 (44.3) 0.8 (43.0) 8.3 (50.7) 6.5 13. 13.1 16.0 14.3 19.1 17.0 19.9 36.9 47.4 42.1 49.0 20.5 (36.3) 27.4 (46.3) 23.1 (40.0) 29.1 (48.5) 44.1 (45.7) 55.8 (58.1) 51.1 (52.8) 59.0 (61.4) 8.01 98.53 8.01 98.53 8.13 98. 0.9B /- 8.2B / - 0.9B / 35.2M 8.2B / 161.5M 0.9B / 41.4M 8.2B / 170.5M Table 8. Additional result of PAVE on the 3DQA tasks with 3D information as additional information. Specifically, we apply different rotary positional embedding according to the layout of side-channel tokens zs. We mainly consider two types of zs: (a) {zs} includes both spatial and temporal dimensions, such as tokens from video backbones or from 3D backbone; and (b) {zs} only contains temporal dimension, such as audio tokens. For the first case, we will add 3D rotary positional embedding (along the temporal, height, and width dimensions). For the second case, we will only add rotary positional embedding along the temporal axis. After cross-attention, we use two-layer MLP, followed by layer norm. After the PAVE layers, we add another two-layer MLP, followed by layer norm, as the adapter. We initialize the γ in the layer norm to zero. B.2. Audio-Visual QA In this setting, the input of the PAVE has two parts: (1) the visual tokens zv from the Video LLMs visual encoder, and (2) the audio tokens zs from side-channel signal encoder. Visual Encoder. For zv, we follow the default setting used in LLaVA-OneVision [28]. We uniformly sample 32 frames from the video and use the same preprocessing of the LLaVA-OneVision to reshape and crop the video frames. We use SigLIP [75] as the visual encoder and it encodes and downsamples each frame into 196 tokens. Side-Channel Signal Encoder. For zs, we follow the preprocessing step of ImageBind [16], which resamples the audio at 16KHz. We segment the audio into overlapping 2-second clips with 1-second stride and encode each clip using the audio encoder of ImageBind. This process generates 1024-dimensional audio token for every 1 second of the audio signal. Since we do not fine-tune the audio encoder, we extract the audio feature tokens offline in order to accelerate the training. Network Architecture. For the PAVE design, we use 2 cross-attention layers with hidden dimension 512 and have 4 attention heads. For LoRA layers in the LLM, we use LoRA = 64 and LoRA α = 16. Training Details. For training, we use AdamW [42] optimizer with linear warmup using the first 3% of iterations. We use the cosine annealing learning rate during the training. We set the base learning rate as 2e-5 and the batch size as 32. All the experiments are run on 2 A100 80G GPUs. Training data. We choose the open-end QA dataset AVSD [1], and closed-end QA dataset AVQA [69] and Music-AVQA [30] as training dataset. AVSD contains 79k question-answer pairs across 7,985 videos with each paired 10 questions. AVQA has 40k question-answer pairs coupled with 40k Videos. Music-AVQA consists of 32k questionanswer pairs and 9277 videos. Evaluation benchmark. We follow the protocol in previous works [49, 72] to evaluate PAVE. For AVSD, we use the AVSD@DSTC7 test split and report CIDEr score as the metric. This benchmark consists of 1,000 audio-visual questions. We use COCO API [37] to calculate the CIDEr score between the model predictions and the ground truth 14 Method ActivtityNet-QA EgoSchema NextQA PerceptionTest VideoMME (w-subs) FLOPs (TB) Total / Trainable Params LLaVA-OV-0.5B [28] LLaVA-OV-7B [28] PAVE-0.5B (w/ video feature) PAVE-7B (w/ video feature) 50.5 56.6 50.6 57.1 26.8 60.1 27.1 57. 57.2 79.4 56.1 79.6 49.2 57.1 48.8 56.0 43.5 61.5 48.6 62. 8.01 98.53 8.08 98.63 0.9B / - 8.2B / - 0.9B / 41.4M 8.2B / 170.5M Table 9. Result of PAVE on the additional benchmarks in enhanced video understanding setting. PAVE uses densely sampled video frames as additional information. Model Zero-shot LMMs LLaVA-OV-0.5B LLaVA-OV-7B Task-specific models LLaVA-OV-0.5B-FT LLaVA-OV-7B-FT TimeSFormer (Ego+Exo)* [17] PAVE-0.5B PAVE-7B Acc. FLOPs (TB) Total / Trainable Params 23.6 23. 28.2 29.8 43.7 32.4 44.2 8.01 98.53 8.01 98.53 - 8.15 98.70 0.9B / - - 0.9B / 35.2M 8.2B / 161.5M - 0.9B / 41.4M 8.2B / 170.5M Table 10. Performance of PAVE on multi-view video understanding with Ego-Exo4D Demonstrator Proficiency benchmark. LLaVA-OV-7B-FT refers to directly fine-tuning the LLaVAOneVision on the training set. Our model achieves state-of-theart performance by only adding small amount of parameters and FLOPs. * means this baseline may use more training data than PAVE because some of the videos are unavailable to us. answers. For AVQA, we evaluate PAVE on the eval split and report the accuracy as the metric. This benchmark contains 17k questions that require reasoning based on audio and visual information. For Music-AVQA, we evaluate PAVE on the test split and report the accuracy as the metric. This benchmark contains 9185 questions, which can be categorized into visual, audio, and audio-visual questions. B.3. 3DQA In this setting, the input of the PAVE consists of two parts: (1) the visual tokens zv from the Video LLMs visual encoder, and (2) the 3D tokens zs from side-channel signal encoder. Visual Encoder. For zv, we use the same setting as the one in Section B.2. Side-Channel Signal Encoder. For encoding the sidechannels information into zs, we utilize the 3D encoder which contains two parts 1. visual encoder which encodes the RGB frames into visual feature tokens. 2. spatial embedding that adds the encoded 3D information on the visual feature tokens. We uniformly extract 32 RGB-D frames from the scan and use ViT [11] to extract the visual features from the RGB frames. We then add spatial embeddings to visual features following the LLaVA-3D [84] by making use of the depth information and the camera pose. It generates 576 tokens for each frame, with token dimension of 1024. We pre-extract the 3D feature to accelerate the training. Network Architecture and Training Details. For the PAVE design and the training configuration, we use the same hyper-parameters used in Section B.2. Training data. For 3D QA tasks, we consider ScanQA [2] and SQA3D [44]. ScanQA and SQA3D contain 25K and 26K training question-answer pairs, respectively. They share the same scanning data set which contains 562 3D scanning from ScanNet [10]. Evaluation benchmark. We report our model performance on the ScanQA validation set, which contains 4,675 questions covering both object position reasoning and object recognition, and the SQA3D test set with 3519 questions, which consists of 5 different types of questions. Following previous work [84], we report the CIDEr (C), BLEU-4 (B-4), METEOR (M), ROUGE(R), and top-1 Exact Match (EM@1) metrics on ScanQA and report EM@1 on SQA3D. We use the evaluation pipeline set up by LLaVA-3D to evaluate our model on ScanQA and SQA3D. B.4. Enhancing Video QA In this setting, the input of the PAVE has two parts: (1) the visual tokens from the Video LLMs visual encoder zv, extracted at sparsely sample video key frames, and (2) the side-channel visual tokens zs, derived from high frame rate video. Visual Encoder. For zv, we use the same setting as the one in Section B.2. In this case, Side-Channel Signal Encoder. the sidechannel signals zs are high frame rate videos. We sample the video frames at the frame rate of 2fps and use the default pre-processing step of the LanguageBind to reshape and crop the video frames. To leverage LanguageBind [83] to encode the high-frame-rate video frames, we split the video frames along the temporal axis into multiple non-overlap groups with each group containing 8 frames. We later concatenate the encoded features of all groups along the temporal axis. To reduce the overhead of the PAVE, inspired by the Slow-Fast [13], we downsample the spatial resolution of the video feature of each video frame from 16 16 to 2 2. We do not utilize the classification tokens from the output of the LanguageBind. Since we do not fine-tune LanguageBinds video encoder, we pre-extract the video features in order to speed up the training. 15 Enhancing video QA: We assume the length of the video at inference time is 2 minutesclose to the average duration of videos on VideoMME and MVBench. We sample the frames at 2 fps and sent them to the video backbone. We down-sample the tokens of each frame spatially to 2 by 2 grids. It produces 960 video tokens in total. The cross-attention is conducted over 196 query tokens and 30 key tokens. PAVE adds about 0.07 TB and 0.10 TB FLOPs for 0.5B and 7B models, respectively. Multi-view Video Understanding: We uniformly sample 32 frames for each exo-centric video and send them into the SigLIP. It generates 196 tokens for each frame and yields 25,088 tokens for 4 exo-centric videos in total. The cross-attention is conducted over 196 query tokens and 784 key tokens. PAVE adds about 0.14 TB and 0.17 TB FLOPs for 0.5B and 7B models, respectively. C. Additional Visualization We present additional visualization of the PAVEs results for enhanced video QA in Figure 5 with videos from VideoMME [14] and MVBench [33]. Network Architecture and Training Details. For the PAVE design and the training configuration, we use the same hyper-parameters used in Section B.2. Training data. We create subset from LLaVA-Video178K [79] by first sampling all videos longer than 1 minute and then randomly choosing 2 question-answer pairs for each video. This process creates training set that contains 57K videos and 114K question-and-answer pairs. Evaluation benchmark. We use VideoMME [14], MVBench [33], and MLVU [81] as evaluation benchmarks. VideoMME and MVBench are both comprehensive video benchmarks and cover different types of subtasks, while MLVU focuses on long video understanding. VideoMME includes 6 key domains and 30 sub-classes. It contains 900 videos, ranging from less than one minute to nearly one hour. There are 2,700 questions with each accompanied by four options. MVBench includes 20 different sub-tasks, such as object shuffling and fine-grained pose estimation, which require detailed temporal information. In total, it has about 4000 questions and 3900 videos. MLVU contains 2175 questions and 1337 long videos. All benchmarks adopt accuracy as the performance metric. B.5. The Calculation of Inference FLOPs. We now describe how the floating-point operations (FLOPs) are reported in our experiments. Since the visual-encoder and the side-channel information encoder are replaceable modules in PAVE settings (i.e. we can use encoder with different scales at different settings.), we only consider the FLOPs of PAVE and LLM, provided by the LLMViewer [74]. During the FLOPs calculation of LLM, we consider 6272 visual tokens, and following the previous work [55], we add 40 additional tokens for the text. We then calculate and add the FLOPs of PAVE. The FLOPs of PAVE is calculated as follows. The input of the PAVE consists of two parts, the visual tokens zv from the Video LLMs visual backbone, and the side-channel information tokens zs. We consider the case that the visual tokens zv come from 32 video frames and Video LLMs visual backbone generates 196 tokens for each frame. Audio-visual QA: We assume the length of the video at inference time is 2 minutes and the audio encoder will generate 1 token for each second of the audio. It yields 120 audio tokens. The cross-attention is conducted over 196 query tokens and 4 key tokens. PAVE thus introduces about 0.07 TB and 0.10 TB FLOPs for 0.5B and 7B models, respectively. 3D QA: We uniformly sample 32 frames and send them into the 3D backbone. It generates 576 tokens for each frame and yields 18432 tokens in total. The crossattention is conducted over 196 query tokens and 576 key tokens. PAVE introduces about 0.12 TB and 0.15 TB FLOPs for 0.5B and 7B models, respectively. 16 Figure 5. Visualization of the QA results on enhanced video QA task. By making use of the video feature of the densely sampled video frames, PAVE captures more details in the video and thus improves the performance of video understanding."
        }
    ],
    "affiliations": [
        "The Chinese University of Hong Kong",
        "University of Wisconsin-Madison"
    ]
}