{
    "paper_title": "HUME: Measuring the Human-Model Performance Gap in Text Embedding Task",
    "authors": [
        "Adnan El Assadi",
        "Isaac Chung",
        "Roman Solomatin",
        "Niklas Muennighoff",
        "Kenneth Enevoldsen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Comparing human and model performance offers a valuable perspective for understanding the strengths and limitations of embedding models, highlighting where they succeed and where they fail to capture meaning and nuance. However, such comparisons are rarely made, as human performance on embedding tasks is difficult to measure. To fill this gap, we introduce HUME: Human Evaluation Framework for Text Embeddings. While frameworks like MTEB provide broad model evaluation, they lack reliable estimates of human performance, limiting the interpretability of model scores. We measure human performance across 16 MTEB datasets spanning reranking, classification, clustering, and semantic textual similarity across linguistically diverse high- and low-resource languages. Humans achieve an average performance of 77.6% compared to 80.1% for the best embedding model, although variation is substantial: models reach near-ceiling performance on some datasets while struggling on others, suggesting dataset issues and revealing shortcomings in low-resource languages. We provide human performance baselines, insight into task difficulty patterns, and an extensible evaluation framework that enables a more meaningful interpretation of the model and informs the development of both models and benchmarks. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb."
        },
        {
            "title": "Start",
            "content": "HUME: MEASURING THE HUMAN-MODEL PERFORMANCE GAP IN TEXT EMBEDDING TASKS Adnan El Assadi Carleton University Niklas Muennighoff Stanford University Isaac Chung Zendesk Roman Solomatin SberAI Kenneth Enevoldsen Aarhus University"
        },
        {
            "title": "ABSTRACT",
            "content": "Comparing human and model performance offers valuable perspective for understanding the strengths and limitations of embedding models, highlighting where they succeed and where they fail to capture meaning and nuance. However, such comparisons are rarely made, as human performance on embedding tasks is difficult to measure. To fill this gap, we introduce HUME: Human Evaluation Framework for Text Embeddings. While frameworks like MTEB provide broad model evaluation, they lack reliable estimates of human performance, limiting the interpretability of model scores. We measure human performance across 16 MTEB datasets spanning reranking, classification, clustering, and semantic textual similarity across linguistically diverse highand low-resource languages. Humans achieve an average performance of 77.6% compared to 80.1% for the best embedding model, although variation is substantial: models reach near-ceiling performance on some datasets while struggling on others, suggesting dataset issues and revealing shortcomings in low-resource languages. We provide human performance baselines, insight into task difficulty patterns, and an extensible evaluation framework that enables more meaningful interpretation of the model and informs the development of both models and benchmarks. Our code, dataset, and leaderboard are publicly available at https://github.com/embeddings-benchmark/mteb. 5 2 0 2 1 1 ] . [ 1 2 6 0 0 1 . 0 1 5 2 : r Figure 1: Overall ranking of human performance versus 13 embedding models across 16 tasks. Human annotators achieve 4th place with score of 77.6, demonstrating competitive but not dominant performance. The ranking reveals significant variation in model performance across different parameter scales and architectures. Darker shades of blue means larger model."
        },
        {
            "title": "INTRODUCTION",
            "content": "Embedding models are central to modern NLP systems, powering applications such as search, recommendation, semantic analysis, and information retrieval. Many benchmarks test the performance of embedding models, with the most comprehensive offering diverse suite of tasks that test their generality and robustness (Muennighoff et al., 2022; Xiao et al., 2025; Enevoldsen et al., 2025). Despite these advances, the interpretation and quality of these scores are often unclear as it is absent of human performance references. Current metrics define performance in terms of theoretical maxima (e.g., MAP = 1.0) that assume perfect consensus on task outcomes. However, many NLP tasks inherently involve ambiguity and disagreement (Plank, 2022), making models score difficult to meaningfully interpret without reasonable references. For instance, if model achieves 0.85 MAP in reranking, it is unclear whether this should be considered strong, mediocre, or beyond what annotators typically achieve. This disconnect highlights the need for human-centered evaluation that contextualizes benchmark results. To address this, we introduce HUME: Human Evaluation framework for text embedding tasks. HUME evaluates annotator performance across four task categories: reranking, classification, clustering, and semantic textual similarity (STS), using 16 diverse datasets from the Massive Text Embedding Benchmark (MTEB) (Muennighoff et al., 2022), adapted for human annotation feasibility. Through multi-annotator experiments, we analyze task difficulty, quantify variation across humans, and compare results directly against state-of-the-art embedding models. Our contributions are threefold: (1) generalizable framework for human evaluation of embedding tasks, (2) empirical evidence of how humans perform across diverse datasets and task types, and (3) comparative analysis of models and humans that highlights strengths, weaknesses, and ambiguities in both benchmarks and models that yield actionable insights. Together, these contributions establish foundation for human-aligned evaluation of embedding models and guide future benchmark design."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 TEXT EMBEDDING MODELS AND MTEB Text embedding models map natural language into dense vectors that capture semantic information. They have progressed from static embeddings (Mikolov et al., 2013; Pennington et al., 2014) to contextual encoders (Devlin et al., 2019; Liu et al., 2019) and more recently to models optimized for embeddings like Sentence-BERT (Reimers & Gurevych, 2019), E5 (Wang et al., 2022), and GTE (Li et al., 2023), powering applications such as search, classification, and clustering. To evaluate these models across its diverse use-cases, MTEB (Muennighoff et al., 2022; Enevoldsen et al., 2025) has risen as the de facto benchmark framework for embeddings and consolidates evaluation across diverse tasks and datasets. Despite its breadth and community-driven extensions spanning multilingual, multimodal, and domain-specific variants (Xiao et al., 2025; Kasmaee et al., 2025; Tang & Yang, 2025; Xiao et al., 2024; Ciancone et al., 2024; Enevoldsen et al., 2024; Zinvandi et al., 2025; Wehrli et al., 2024; Poswiata et al., 2024; Snegirev et al., 2024), MTEB lacks human performance baselines, making it difficult to contextualize model achievements. 2.2 HUMAN EVALUATION IN NLP Human evaluation is well established in NLP, especially for generative tasks like machine translation (Graham et al., 2013), summarization (Fabbri et al., 2021), and dialogue (Gupta et al., 2019). In contrast, embedding-based tasks have relied almost exclusively on automated metrics, with little attention to human baselines. In information retrieval, initiatives such as TREC (Voorhees & Tice, 2000) collect human relevance judgments, but these serve as gold standards rather than benchmarks of human performance under model metrics (e.g., nDCG, MRR). Similarly, GLUE, SuperGLUE (Wang et al., 2018; 2019), and MERA (Fenogenova et al., 2024) Russian GLUE-like benchmark report human baselines, but mainly for classification and reasoning tasks. For embeddings, works like STS (Cer et al., 2017) report inter-annotator agreement, yet these are not converted into model-comparable performance 2 scores. This leaves gap: human performance on embedding benchmarks such as MTEB remains largely unquantified."
        },
        {
            "title": "3.1 FRAMEWORK DESIGN",
            "content": "Our framework builds on MTEB by establishing reproducible human evaluation protocols that align directly with model evaluation. It consists of task-specific annotation interfaces, principled dataset sampling, standardized results format, and the use of aligned metrics. This design reveals where evaluation practices introduce ambiguity or inconsistency."
        },
        {
            "title": "3.2 TASKS, DATASETS, AND METRICS",
            "content": "Our selection criteria ensures comprehensive coverage across multiple dimensions: (1) linguistic diversity: including both high-resource languages (English, Arabic, Russian) and lower-resource languages (Norwegian Bokm√•l, Danish) 1 to test cross-lingual generalization, (2) domain variety: spanning news, social media, encyclopedic content, scientific literature, and forum discussions to capture real-world application diversity, (3) construction methods: including both curated human annotations and synthetic dataset creation to understand how dataset origin affects humanmodel alignment, (4) task relevance: using tasks from established benchmarks widely adopted in the embedding evaluation community, and (5) task complexity variation: ranging from binary classification to fine-grained similarity judgments. This systematic selection ensures our findings generalize across the diverse landscape of embedding applications while maintaining direct relevance to existing evaluation frameworks. Each task category uses primary evaluation metric to enable consistent humanmodel comparisons. Table 1 summarizes the datasets, their domains, and the primary metrics applied. Detailed task examples are provided in Appendix B. 3.3 ANNOTATION PROCEDURE All annotations are conducted in Argilla (Argilla Project Contributors, 2025) using task-specific interfaces: binary relevance judgments for reranking, categorical labels for classification, free assignment of cluster IDs for clustering, and 05 similarity scores for STS. Sample sizes are balanced by task complexity: reranking uses 2049 queries per dataset, classification 4048 examples, clustering 30 items across clusters, and STS 3050 sentence pairs. All annotators were male, aged 2035, from culturally diverse backgrounds, and experienced NLP practitioners with native or near-native proficiency in the evaluated languages. They followed structured guidelines and completed all annotations independently, without access to ground truth or model predictions. The downsampled task subsets used for comparisons are included in the MTEB package, with detailed task examples provided in Appendix B. English tasks were annotated by two annotators to enable agreement analysis. Multilingual tasks were annotated by single annotator with corresponding language expertise. Inter-annotator agreement was assessed with task-appropriate metrics: Fleiss kappa (Fleiss, 1971) for classification, pairwise ARI (Strehl & Ghosh, 2003) for clustering, pairwise Spearman correlation (Agirre et al., 2012a) for STS, and mean Spearman/Kendalls tau (Manning et al., 2008) for reranking. detailed agreement analysis is provided in Appendix D. This controlled evaluation setup minimizes potential confounds from dataset variation and enables direct performance comparisons on identical evaluation instances. 3.4 MODEL SELECTION AND EVALUATION We evaluate 13 embedding models chosen to cover multiple dimensions: (1) parameter scale (22M7B), (2) architecture (encoderand decoder-based), (3) instruction tuning (instruction-tuned and standard), and (4) multilingual capability (English and multilingual). This selection spans 1With 0 being \"The Left-Behinds\" and 5 being \"The Winners\", we cover eng: 5, ara: 5, rus: 4, dan: 3, nob: 1 according to the 0-5 scale by (Joshi et al., 2021). 3 Datasets Description Metrics Core17, News21, n R Robust04 (Weller et al., 2024) WikiMulti (Enevoldsen et al., 2025) Information retrieval benchmarks (news, documents) MAP, Wikipedia article reranking (eng, dan, nob) Emotion (Saravia et al., 2018) Emotion classification from social media text a fi a Tweet Senti (Barbieri et al., 2022) Toxicity (cjadams et al., 2019) Toxic content detection Multilingual Sentiment (Mollanorozy et al., 2023) Sentiment classification (ara, eng, nob, rus) Sentiment analysis of tweets WikiCities (Foundation) e l Entity clustering from Wikipedia Academic paper topic clustering (derived labels) ArXiv (arXiv.org submitters, 2024) Reddit (Geigle et al., 2021) SIB200 (Adelani et al., 2023) Multilingual sentence clustering (ara, dan, eng, rus) Forum discussion topic clustering STSBenchmark (May, 2021) General semantic similarity benchmark SICK-R (Marelli et al., 2014) Semantic relatedness and entailment STS12 (Agirre et al., 2012b) STS22 (Chen et al., 2022) Shared task semantic similarity Multilingual semantic similarity (ara, eng, rus) MRR@10, nDCG@ Accuracy, F1, Weighted F1 V-Measure, ARI, AMI Spearman, Pearson Table 1: Complete list of 16 datasets and evaluation metrics used for human annotation. For the metrics we use MAP (Manning et al., 2008), MRR (Manning et al., 2008), nDCG (J√§rvelin & Kek√§l√§inen, 2002), Accuracy/F1 (Sokolova & Lapalme, 2009), V-Measure (Rosenberg & Hirschberg, 2007), ARI (Hubert & Arabie, 1985), AMI (Vinh et al., 2010), Spearman/Pearson (Spearman, 2010) (Pearson, 1895), following the MTEB implementations. diverse computational budgets and training paradigms, capturing the current embedding landscape. All evaluated models are provided in Appendix F. All models are evaluated on the downsampled instances annotated by humans, using identical metrics, protocols, and computational settings. Human performance is computed using the metrics in Table 1, mirroring MTEB protocols. For primary analyses, we report MAP for reranking, Accuracy for classification, V-Measure for clustering, and Spearman correlation for STS."
        },
        {
            "title": "4 RESULTS AND ANALYSIS",
            "content": "Figure 1 provides an overview of human performance relative to 13 state-of-the-art embedding models across 16 tasks. Human annotators rank 4th overall with score of 0.776, trailing 3 large models but outperforming 10 others. This positioning reveals that humans neither represent performance ceiling nor lower bound, but rather occupy middle ground that varies significantly by task category and language. Table 2 presents the aggregated human results compared to selected models in various sizes. Below we analyze each category and language group in the following sections and refer to Appendix for the full results on specific task categories. 4.1 PERFORMANCE PATTERNS BY TASK CATEGORY Figure 2 shows human performance relative to model performance ranges across all 26 task-language pairs. Each task shows human performance (point) positioned within the full spectrum from worst to best model performance (range bars). Humans consistently perform in the upper portion of model ranges, typically exceeding median model performance (61.5% of tasks) while rarely matching the best models (15.4% of tasks). Classification tasks show the strongest human performance, with humans outperforming all models in 3 of 7 tasks, while clustering and reranking reveal consistent gaps where humans fall short of top-performing models. Detailed gap analysis can be found in Appendix E. 4 Model Classification ara eng nob rus Clustering Reranking STS Overall ara dan eng rus dan eng nob ara eng rus Number of datasets (1) (4) (1) (1) (1) (1) (4) (1) (1) (4) (1) (1) (4) (1) 57.2 58.8 51.7 55.5 35.2 24.5 55.1 31.4 78.4 93.7 71.2 6.2 83.5 33.1 all-MiniLM-L6-v2 53.5 62.0 47.0 60.5 21.4 22.9 59.7 36.9 79.0 93.3 80.5 13.2 83.0 42.2 all-mpnet-base-v2 74.5 70.0 70.5 70.0 68.5 76.0 82.7 77.7 90.6 96.4 86.1 16.0 85.9 63.0 e5-mistral-7b-instruct 71.0 58.6 54.0 73.5 19.2 43.7 65.1 36.9 74.3 86.9 71.4 36.7 69.9 66.2 embeddinggemma-300m gte-Qwen2-1.5B-instruct 75.2 76.5 70.8 74.5 73.7 67.1 75.9 72.2 84.3 95.3 87.8 28.8 84.0 54.2 jasper_en_vision_language_v1 63.5 87.1 70.5 79.8 64.3 54.7 83.2 43.7 90.1 95.8 90.0 40.9 88.1 69.5 75.8 64.7 73.8 77.2 35.9 40.6 45.6 36.2 92.2 94.4 87.5 31.0 85.2 62.7 multilingual-e5-base 77.0 64.9 75.0 80.0 34.6 31.0 52.5 46.9 95.0 95.3 92.2 33.8 86.3 68.8 multilingual-e5-large 72.2 62.2 69.2 81.2 35.5 38.0 51.7 59.1 88.6 94.2 88.3 28.8 85.2 60.3 multilingual-e5-small 57.2 66.4 52.2 59.0 26.5 34.2 61.9 30.5 90.8 94.5 82.0 12.7 87.6 43.7 mxbai-embed-large-v1 77.2 74.7 59.8 74.8 78.8 58.5 68.4 68.3 90.0 95.5 83.6 38.0 88.5 60.3 Qwen3-Embedding-0.6B 77.5 69.8 68.8 72.5 73.1 71.2 85.1 68.9 89.2 96.3 86.1 15.3 86.4 64.0 SFR-Embedding-Mistral 65.8 84.0 67.0 79.2 36.8 42.6 78.6 46.7 91.7 96.0 88.6 37.2 86.7 62.1 stella_en_1.5B_v5 (26) 61.9 63.4 78.2 64.2 77.5 80.1 68.2 70.4 69.0 66.6 76.8 78.3 76. Human 95.0 70.3 85.0 92.5 76.0 62.7 67.4 68.0 91.4 87.2 89.8 67.5 83.1 58.7 77.6 Table 2: Human performance compared to 13 embedding models across task categories and languages. Bold indicates highest performance (human or model), underline indicates best model performance. Humans achieve top performance in 5 of 14 aggregated task-language pairs, particularly excelling in non-English sentiment analysis and Arabic semantic similarity. Overall results are aggregated over the 26 task-language pairs. Classification: Human performance averages 70.3 across classification tasks, with substantial variation reflecting task-specific challenges. Performance ranges from 45.8 on emotion classification (Œ∫ = 0.39, fair agreement) to 95.0 on Arabic sentiment analysis. Models generally exceed human performance, with the best achieving 87.1 on English tasks. However, humans outperform models on non-English sentiment analysis, particularly in Arabic (95.0 vs. 77.5) and Russian (92.5 vs. 81.2), likely benefiting from native cultural and linguistic understanding that current models fail to capture. Clustering Clustering presents the greatest challenge for humans, averaging 67.4 V-measure with extreme variation. Near-perfect performance on WikiCities (97.6, ARI = 0.91) contrasts sharply with poor performance on ArXiv papers (49.2, ARI = 0.001), which uses derived labels from ArXiv categories. Models consistently outperform humans, with the best reaching 85.1%. The poor inter-annotator agreement on academic clustering indicates fundamental task ambiguity rather than human limitation. Reranking: Humans achieve strong performance in reranking (87.2 average MAP), demonstrating intuitive document relevance understanding. Models exceed human performance (best: 96.4), but humans maintain competitive scores with high inter-annotator agreement (œÅ = 0.64-0.85), suggesting these tasks align well with human judgment. STS: Human performance averages 83.2 Spearman correlation, with notable dataset-specific variation. STS12 achieves 91.2 while STS22-Russian drops to 58.7, likely reflecting dataset quality issues discussed in 4.3. Models achieve comparable performance (best: 88.5), with moderate interannotator agreement (œÅ = 0.58-0.77). These results reveal critical insight that challenges conventional evaluation paradigms: human performance variation often reflects task quality rather than human limitations. Tasks with high human performance and agreement (reranking, toxicity classification) represent well-specified problems with clear ground truth, while tasks with low human agreement (emotion classification, academic clustering) may suffer from ambiguous annotation guidelines or inherently subjective judgments. Models achieve superhuman performance by reproducing consistent label patterns from training data, but this consistency may mask fundamental issues with task specification. Rather than treating low human performance as ceiling to surpass, our findings suggest that it often signals the need for improved task design and clearer annotation frameworks. 5 Figure 2: Comprehensive view of human performance relative to all model performance ranges across 16 tasks by language. 4.2 CROSS-LINGUAL PERFORMANCE ANALYSIS Arabic exhibits the strongest human advantage: 67% win rate against the best models and 100% against the mean, with the largest gap in semantic similarity (67.5% vs. 40.9%, 26.6-point margin). Russian and Norwegian also show consistent human superiority in sentiment analysis, where humans achieve 92.5% and 85.0%, respectively, substantially outperforming the best models. These advantages likely stem from cultural and contextual knowledge that models fail to capture, especially in lower-resource languages. English tasks are more balanced, with models generally matching or exceeding humans, reflecting the languages dominance in training data. Danish shows mixed outcomes, possibly due to stronger multilingual coverage for Germanic languages. 4.3 DATASET QUALITY AND EVALUATION CHALLENGES Our analysis reveals systematic quality issues in several MTEB datasets that fundamentally compromise their reliability as evaluation benchmarks. Human performance variation often correlates with underlying ambiguity rather than genuine human limitations, providing diagnostic tool for identifying problematic evaluation targets. Below we highlight two such examples: Emotion Classification Ambiguity: The emotion classification dataset exemplifies inherent labeling ambiguity, achieving only fair inter-annotator agreement (Œ∫ = 0.39) with 52.1% consensus. Real examples demonstrate the fundamental challenges: feel like such noob when the customers make really dull and stupid jokes that im supposed to find funny could reasonably be labeled as sadness (0), anger (3), or even surprise (5) depending on interpretation. Similarly, am feeling very indecisive and spontaneous contains mixed emotional states that resist single-label categorization. Sarcastic expressions like got paid too much because get so many deliveries at work Im feeling bit shamed present surface emotions that differ from intended meaning. When human experts fundamentally disagree on correct answers for such inherently ambiguous cases, the apparent superhuman model 6 Figure 3: Human win rates across task categories and languages. Top left: By task category shows humans perform moderately in classification but struggle in clustering, reranking, and STS against best models. Top right: English-only vs multilingual tasks reveals humans perform better on multilingual tasks (29% vs 0% against best models). Bottom left: Performance varies dramatically by baseline comparison (15% vs best, 62% vs mean models). Bottom right: Language-specific breakdown shows varying performance across different language codes. performance (87.1% vs. 45.8% human) likely reflects consistent reproduction of arbitrary majority label patterns rather than superior emotional understanding. ArXiv Clustering Breakdown: Academic paper clustering shows complete breakdown of human agreement (ARI=-0.001), indicating fundamental disagreement about how to categorize academic papers. Real examples illustrate the core ambiguity: papers like Self-Supervised Audio-Visual Representation Learning with Relaxed Cross-Modal Synchronicity could legitimately cluster with computer vision, machine learning, or audio processing groups depending on the annotators perspective on primary methodology versus application domain. The architecture of innovation: Tracking face-to-face interactions with ubicomp technologies spans social science, computer science, and architecture domains. Such interdisciplinary papers create fundamental disagreement about correct clustering approaches, with no objectively correct answer. The task uses derived labels from ArXiv categories, but the core issue is that academic papers often span multiple domains, making any single clustering scheme inherently ambiguous. The high model performance (84.6% vs. 49.2% human) suggests that the models are reproducing consistent labeling patterns rather than solving the fundamental categorization challenge. High-Quality Benchmark Identification: Conversely, tasks with high human agreement provide reliable evaluation targets. Reranking tasks achieve strong inter-annotator agreement (œÅ = 0.640.85) with clear performance targets, while toxicity classification shows moderate agreement (Œ∫ = 0.55) with 77.8% annotator consensus. These represent genuine evaluation challenges where model improvements likely reflect meaningful progress rather than pattern matching to flawed labels. These patterns suggest that apparent superhuman model performance often occurs precisely where human agreement is lowest, indicating that models excel not through superior understanding but through consistent reproduction of systematic labeling patterns. This raises concerns about the label 7 quality in embeddings benchmarks, and we encourage future benchmark developers to critically examine the datasets before including them in benchmark, potentially using human annotations framework like HUME. Detailed analysis of specific quality issues is provided in Appendix C."
        },
        {
            "title": "5.1 TASK QUALITY AND EMBEDDING EVALUATION RELIABILITY",
            "content": "Models achieve their highest relative performance precisely where human experts show the least agreement. This striking pattern raises fundamental questions about what constitutes reliable evaluation in embedding benchmarks. Tasks with high human agreement, such as reranking (œÅ = 0.640.85) and toxicity classification (Œ∫ = 0.55), provide reliable evaluation targets, while low-agreement tasks like emotion classification (Œ∫ = 0.39) and ArXiv clustering (ARI = 0.001) suffer from ambiguous guidelines or subjective judgments. While some cases involve clear quality issues, the mechanisms behind this inverse relationship remain unclear. Models may excel through superior semantic understanding or by exploiting systematic annotation patterns. Cultural factors further complicate evaluation. Humans retain substantial advantages in Arabic semantic similarity (67.5% vs. 40.9% best model) and multilingual sentiment analysis, revealing genuine model limitations in cross-cultural understanding. These findings suggest reliable evaluation depends as much on task quality as model capability. Rather than automatically treating high model performance as progress, we recommend prioritizing high-agreement tasks for model and benchmark development, addressing cultural competence gaps, and critically examining whether apparent model superiority on ambiguous tasks reflects genuine capabilities or systematic biases in evaluation datasets. 5.2 IMPLICATIONS FOR MODEL DEVELOPMENT AND EVALUATION PRACTICES Our findings reveal concrete directions for both embedding model development and evaluation methodology that address the fundamental quality issues weve identified. Prioritize High-Agreement Tasks for Development: Development efforts should focus on tasks where humans demonstrate both high performance and agreement, as these provide the most reliable benchmarks for measuring genuine progress. Reranking tasks, with their clear performance targets and strong agreement (œÅ = 0.64 0.85), offer dependable evaluation where the persistent modelhuman gap (96.4% vs. 87.2%) represents meaningful challenges requiring better modeling of relevance relationships. Toxicity classification, despite moderate agreement (Œ∫ = 0.55), provides another reliable target with 77.8% human consensus. In contrast, optimizing for tasks with poor human agreement (emotion classification Œ∫ = 0.39, ArXiv clustering ARI = 0.001) may lead models to excel at reproducing arbitrary labeling patterns rather than developing genuine semantic capabilities. Address Cultural and Linguistic Competence Gaps: The substantial human advantages in nonEnglish tasks reveal critical model limitations that scaling training data alone cannot address. Arabic semantic similarity shows the largest human advantage (67.5% vs. 40.9% best model), while multilingual sentiment analysis demonstrates consistent human superiority in non-English languages (95.0% Arabic, 92.5% Russian). These gaps suggest that current models lack the cultural and contextual knowledge necessary for cross-lingual understanding, requiring architectural innovations or training approaches that go beyond simple data scaling to capture cultural nuances and contextual understanding. Replace Problematic Benchmark Datasets: Our analysis identifies specific datasets that compromise benchmark reliability and should be replaced in future MTEB iterations: emotion classification (Œ∫ = 0.39), ArXiv clustering (ARI = 0.001), and STS22-Russian (systematic parsing artifacts). These tasks provide unreliable evaluation targets that may mislead model development efforts by rewarding pattern matching to flawed gold standards. Replacement datasets should demonstrate reasonable human agreement and clear task specifications, validated through human evaluation studies before inclusion in benchmark suites. 8 Develop Culturally-Aware Evaluation Frameworks: Current benchmarks may underestimate cross-cultural understanding challenges by focusing on tasks where models can leverage Englishcentric training advantages. The pronounced human advantages in Arabic and other non-English tasks demonstrate the need for evaluation frameworks that properly assess cultural and contextual understanding. Future benchmarks should include more diverse cultural contexts and ensure that high model performance reflects genuine cross-cultural competence rather than exploitation of training data biases. Consider Agreement-Weighted Evaluation: Model performance should be interpreted in light of human agreement levels, with evaluation practices adapted accordingly. We propose reporting model performance alongside human agreement metrics to provide proper interpretive context. model achieving 85% accuracy on emotion classification (185% of human performance, Œ∫ = 0.39) represents fundamentally different achievement than 85% on reranking (97% of human performance, œÅ = 0.75). High model performance on low-agreement tasks should be viewed skeptically as potential artifacts of flawed evaluation targets rather than genuine capability improvements."
        },
        {
            "title": "5.3 LIMITATIONS AND FUTURE DIRECTIONS",
            "content": "Our study has several limitations. First, while three annotators participated overall, only two annotations were collected for most tasks, constraining our ability to fully characterize human judgment distributions. Sample sizes (2050 instances per task) balance feasibility and reliability but may not capture the full complexity of human performance variation. Additionally, annotators were average or above-average raters without specialized training; expert annotators could perform better, particularly on technical tasks like academic clustering. However, we believe that the samples provided clearly show that decisions on evaluation datasets can be made from even few samples. Future work should expand annotator pools to better capture human judgment variability and extend evaluations to additional MTEB task categories. Crucially, the field needs rigorous research on designing tasks that balance clear specification with meaningful challenge, avoiding both ambiguity, which depresses human agreement, and oversimplification, which diminishes the evaluations ability to discriminate model capabilities. Finally, embedding evaluation should aim not for model superhuman performance but for frameworks that reliably distinguish genuine semantic understanding from sophisticated pattern matching. Achieving this requires careful attention to task quality, human agreement, and the cultural contexts shaping language understanding."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce HUME, comprehensive human evaluation framework for MTEB, addressing critical gap in understanding empirical performance bounds for embedding models. By measuring human performance across 16 datasets spanning reranking, classification, clustering, and STS, we establish baselines that reframe how model achievements should be interpreted. Our findings show that human performance varies substantially by task categories. Tasks with high agreement provide reliable benchmarks, while low-agreement tasks often indicate design issues rather than human limitations. Apparent model superhuman performance tends to coincide with low human agreement, suggesting models exploit patterns rather than achieving true semantic understanding. HUME provides both methodology and baseline measurements for evaluating embedding models. By releasing our resources and protocols, we aim to support the community in adopting humangrounded evaluation practices and developing models better aligned with human judgment and real-world language understanding."
        },
        {
            "title": "REFERENCES",
            "content": "David Ifeoluwa Adelani, Hannah Liu, Xiaoyu Shen, Nikita Vassilyev, Jesujoba Alabi, Yanke Mao, Haonan Gao, and Annie En-Shiun Lee. Sib-200: simple, inclusive, and big evaluation dataset for topic classification in 200+ languages and dialects. arXiv preprint arXiv:2309.07445, 2023. 9 Eneko Agirre, Daniel Cer, Mona Diab, and Aitor Gonzalez-Agirre. SemEval-2012 task 6: pilot on semantic textual similarity. In Eneko Agirre, Johan Bos, Mona Diab, Suresh Manandhar, Yuval Marton, and Deniz Yuret (eds.), *SEM 2012: The First Joint Conference on Lexical and Computational Semantics Volume 1: Proceedings of the main conference and the shared task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation (SemEval 2012), pp. 385393, Montr√©al, Canada, 7-8 June 2012a. Association for Computational Linguistics. URL https://aclanthology.org/S12-1051/. Eneko Agirre, Mona Diab, Daniel Cer, and Aitor Gonzalez-Agirre. Semeval-2012 task 6: pilot In Proceedings of the First Joint Conference on Lexical and on semantic textual similarity. Computational Semantics - Volume 1: Proceedings of the Main Conference and the Shared Task, and Volume 2: Proceedings of the Sixth International Workshop on Semantic Evaluation, SemEval 12, pp. 385393, USA, 2012b. Association for Computational Linguistics. Argilla Project Contributors. Argilla: collaboration tool for building high-quality ai datasets. https://argilla.io/, 2025. Version as of Sep. 2025; open-source data labeling / curation platform. arXiv.org submitters. arxiv dataset, 2024. URL https://www.kaggle.com/dsv/7548853. Francesco Barbieri, Luis Espinosa Anke, and Jose Camacho-Collados. XLM-T: Multilingual language models in Twitter for sentiment analysis and beyond. In Proceedings of the Thirteenth Language Resources and Evaluation Conference, pp. 258266, Marseille, France, June 2022. European Language Resources Association. URL https://aclanthology.org/2022.lrec-1. 27. Daniel Cer, Mona Diab, Eneko Agirre, I√±igo Lopez-Gazpio, and Lucia Specia. SemEval-2017 task 1: Semantic textual similarity multilingual and crosslingual focused evaluation. In Steven Bethard, Marine Carpuat, Marianna Apidianaki, Saif M. Mohammad, Daniel Cer, and David Jurgens (eds.), Proceedings of the 11th International Workshop on Semantic Evaluation (SemEval-2017), pp. 114, Vancouver, Canada, August 2017. Association for Computational Linguistics. doi: 10.18653/v1/S17-2001. URL https://aclanthology.org/S17-2001/. Xi Chen, Ali Zeynali, Chico Camargo, Fabian Fl√∂ck, Devin Gaffney, Przemyslaw Grabowicz, Scott Hale, David Jurgens, and Mattia Samory. SemEval-2022 task 8: Multilingual news article similarity. In Guy Emerson, Natalie Schluter, Gabriel Stanovsky, Ritesh Kumar, Alexis Palmer, Nathan Schneider, Siddharth Singh, and Shyam Ratan (eds.), Proceedings of the 16th International Workshop on Semantic Evaluation (SemEval-2022), pp. 10941106, Seattle, United States, July 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.semeval-1.155. URL https://aclanthology.org/2022.semeval-1.155. Mathieu Ciancone, Imene Kerboua, Marion Schaeffer, and Wissam Siblini. Mteb-french: Resources for french sentence embedding evaluation and analysis, 2024. URL https://arxiv.org/ abs/2405.20468. cjadams, Daniel Borkan, inversion, Jeffrey Sorensen, Lucas Dixon, Lucy Vasserman, and nithum. Jigsaw unintended bias in toxicity classification, 2019. URL https://kaggle.com/ competitions/jigsaw-unintended-bias-in-toxicity-classification. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In North American Chapter of the Association for Computational Linguistics, 2019. URL https://api.semanticscholar.org/ CorpusID:52967399. Kenneth Enevoldsen, M√°rton Kardos, Niklas Muennighoff, and Kristoffer Nielbo. The scandinavian embedding benchmarks: Comprehensive assessment of multilingual and monolinIn Advances in Neural Information Processing Systems, 2024. URL gual text embedding. https://nips.cc/virtual/2024/poster/97869. Kenneth Enevoldsen, Isaac Chung, Imene Kerboua, M√°rton Kardos, Ashwin Mathur, David Stap, Jay Gala, Wissam Siblini, Dominik Krzeminski, Genta Indra Winata, Saba Sturua, Saiteja Utpala, Mathieu Ciancone, Marion Schaeffer, Gabriel Sequeira, Diganta Misra, Shreeya Dhakal, Jonathan 10 Rystr√∏m, Roman Solomatin, √ñmer √áagatan, Akash Kundu, Martin Bernstorff, Shitao Xiao, Akshita Sukhlecha, Bhavish Pahwa, Rafa≈Ç Poswiata, Kranthi Kiran GV, Shawon Ashraf, Daniel Auras, Bj√∂rn Pl√ºster, Jan Philipp Harries, Lo√Øc Magne, Isabelle Mohr, Mariya Hendriksen, Dawei Zhu, Hippolyte Gisserot-Boukhlef, Tom Aarsen, Jan Kostkan, Konrad Wojtasik, Taemin Lee, Marek ≈†uppa, Crystina Zhang, Roberta Rocca, Mohammed Hamdy, Andrianos Michail, John Yang, Manuel Faysse, Aleksei Vatolin, Nandan Thakur, Manan Dey, Dipam Vasani, Pranjal Chitale, Simone Tedeschi, Nguyen Tai, Artem Snegirev, Michael G√ºnther, Mengzhou Xia, Weijia Shi, Xing Han L√π, Jordan Clive, Gayatri Krishnakumar, Anna Maksimova, Silvan Wehrli, Maria Tikhonova, Henil Panchal, Aleksandr Abramov, Malte Ostendorff, Zheng Liu, Simon Clematide, Lester James Miranda, Alena Fenogenova, Guangyu Song, Ruqiya Bin Safi, Wen-Ding Li, Alessia Borghini, Federico Cassano, Hongjin Su, Jimmy Lin, Howard Yen, Lasse Hansen, Sara Hooker, Chenghao Xiao, Vaibhav Adlakha, Orion Weller, Siva Reddy, and Niklas Muennighoff. MMTEB: Massive multilingual text embedding benchmark. arXiv preprint arXiv:2502.13595, 2025. doi: 10.48550/arXiv.2502.13595. URL https://arxiv.org/abs/2502.13595. Alexander R. Fabbri, Wojciech Kryscinski, Bryan McCann, Caiming Xiong, Richard Socher, and Dragomir Radev. SummEval: Re-evaluating summarization evaluation. Transactions of the Association for Computational Linguistics, 9:391409, 2021. doi: 10.1162/tacl_a_00373. URL https://aclanthology.org/2021.tacl-1.24/. Alena Fenogenova, Artem Chervyakov, Nikita Martynov, Anastasia Kozlova, Maria Tikhonova, Albina Akhmetgareeva, Anton Emelyanov, Denis Shevelev, Pavel Lebedev, Leonid Sinev, Ulyana Isaeva, Katerina Kolomeytseva, Daniil Moskovskiy, Elizaveta Goncharova, Nikita Savushkin, Polina Mikhailova, Denis Dimitrov, Alexander Panchenko, and Sergei Markov. Mera: comprehensive llm evaluation in russian, 2024. URL https://arxiv.org/abs/2401.04531. Joseph L. Fleiss. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76 (5):378382, 1971. doi: 10.1037/h0031619. Wikimedia Foundation. Wikimedia downloads. URL https://dumps.wikimedia.org. Gregor Geigle, Nils Reimers, Andreas R√ºckl√©, and Iryna Gurevych. Tweac: Transformer with extendable qa agent classifiers. arXiv preprint, abs/2104.07081, 2021. URL http://arxiv. org/abs/2104.07081. Yvette Graham, Timothy Baldwin, Alistair Moffat, and Justin Zobel. Continuous measurement scales in human evaluation of machine translation. In LAW@ACL, 2013. URL https://api. semanticscholar.org/CorpusID:1128384. Prakhar Gupta, Shikib Mehri, Tiancheng Zhao, Amy Pavel, Maxine Eskenazi, and Jeffrey P. Bigham. Investigating evaluation of open-domain dialogue systems with human generated multiple references, 2019. URL https://arxiv.org/abs/1907.10568. Lawrence Hubert and Phipps Arabie. Comparing partitions. Journal of Classification, 2(1):193218, Dec 1985. doi: 10.1007/BF01908075. URL https://doi.org/10.1007/BF01908075. Kalervo J√§rvelin and Jaana Kek√§l√§inen. Cumulated gain-based evaluation of ir techniques. ACM Trans. Inf. Syst., 20(4):422446, October 2002. ISSN 1046-8188. doi: 10.1145/582415.582418. URL https://doi.org/10.1145/582415.582418. Pratik Joshi, Sebastin Santy, Amar Budhiraja, Kalika Bali, and Monojit Choudhury. The state and fate of linguistic diversity and inclusion in the nlp world, 2021. URL https://arxiv.org/ abs/2004.09095. Ali Shiraee Kasmaee, Mohammad Khodadad, Mohammad Arshi Saloot, Nicholas Sherck, Stephen Dokas, Hamidreza Mahyar, and Soheila Samiee. Chemteb: Chemical text embedding benchmark, an overview of embedding models performance & efficiency on specific domain, 2025. URL https://arxiv.org/abs/2412.00532. Sean Lee, Aamir Shakir, Darius Koenig, and Julius Lipp. Open source strikes bread - new fluffy embeddings model, 2024. URL https://www.mixedbread.ai/blog/ mxbai-embed-large-v1. Xianming Li and Jing Li. Angle-optimized text embeddings. arXiv preprint arXiv:2309.12871, 2023. Zehan Li, Xin Zhang, Yanzhao Zhang, Dingkun Long, Pengjun Xie, and Meishan Zhang. Towards general text embeddings with multi-stage contrastive learning, 2023. URL https://arxiv. org/abs/2308.03281. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. ArXiv, abs/1907.11692, 2019. URL https://api.semanticscholar.org/CorpusID: 198953378. Christopher D. Manning, Prabhakar Raghavan, and Hinrich Sch√ºtze. Introduction to Information Retrieval. Cambridge University Press, 2008. Marco Marelli, Stefano Menini, Marco Baroni, Luisa Bentivogli, Raffaella Bernardi, and Roberto Zamparelli. SICK cure for the evaluation of compositional distributional semantic models. In Nicoletta Calzolari, Khalid Choukri, Thierry Declerck, Hrafn Loftsson, Bente Maegaard, Joseph Mariani, Asuncion Moreno, Jan Odijk, and Stelios Piperidis (eds.), Proceedings of the Ninth International Conference on Language Resources and Evaluation (LREC14), pp. 216 223, Reykjavik, Iceland, May 2014. European Language Resources Association (ELRA). URL http://www.lrec-conf.org/proceedings/lrec2014/pdf/363_Paper.pdf. Philip May. Machine translated multilingual sts benchmark dataset. 2021. URL https://github. com/PhilipMay/stsb-multi-mt. Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Sfrembedding-mistral:enhance text retrieval with transfer learning. Salesforce AI Research Blog, 2024. URL https://www.salesforce.com/blog/sfr-embedding/. Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. Distributed representations of words and phrases and their compositionality. In Proceedings of the 27th International Conference on Neural Information Processing Systems - Volume 2, NIPS13, pp. 31113119, Red Hook, NY, USA, 2013. Curran Associates Inc. Sepideh Mollanorozy, Marc Tanti, and Malvina Nissim. Cross-lingual transfer learning with {P}ersian. In Lisa Beinborn, Koustava Goswami, Saliha Murado uglu, Alexey Sorokin, Ritesh Kumar, Andreas Shcherbakov, Edoardo M. Ponti, Ryan Cotterell, and Ekaterina Vylomova (eds.), Proceedings of the 5th Workshop on Research in Computational Linguistic Typology and Multilingual NLP, pp. 8995, Dubrovnik, Croatia, May 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.sigtyp-1.9. URL https://aclanthology.org/2023.sigtyp-1.9. Niklas Muennighoff, Nouamane Tazi, Lo√Øc Magne, and Nils Reimers. Mteb: Massive text embedding benchmark. arXiv preprint arXiv:2210.07316, 2022. Karl Pearson. Note on regression and inheritance in the case of two parents. Proceedings of the Royal Society of London, 58:240242, 1895. doi: 10.1098/rspl.1895.0041. Jeffrey Pennington, Richard Socher, and Christopher Manning. GloVe: Global vectors for word representation. In Alessandro Moschitti, Bo Pang, and Walter Daelemans (eds.), Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP), pp. 15321543, Doha, Qatar, October 2014. Association for Computational Linguistics. doi: 10.3115/v1/D14-1162. URL https://aclanthology.org/D14-1162/. Barbara Plank. The problem of human label variation: On ground truth in data, modeling and evaluation. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1067110682, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.731. URL https://aclanthology.org/2022. emnlp-main.731/. Rafa≈Ç Poswiata, S≈Çawomir Dadas, and Micha≈Ç Pere≈Çkiewicz. Pl-mteb: Polish massive text embedding benchmark. arXiv preprint arXiv:2405.10138, 2024. 12 Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Conference on Empirical Methods in Natural Language Processing, 2019. URL https: //api.semanticscholar.org/CorpusID:201646309. Andrew Rosenberg and Julia Hirschberg. V-measure: conditional entropy-based external cluster evaluation measure. In Jason Eisner (ed.), Proceedings of the 2007 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning (EMNLP-CoNLL), pp. 410420, Prague, Czech Republic, June 2007. Association for Computational Linguistics. URL https://aclanthology.org/D07-1043/. Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. CARER: Contextualized affect representations for emotion recognition. In Ellen Riloff, David Chiang, Julia Hockenmaier, and Junichi Tsujii (eds.), Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pp. 36873697, Brussels, Belgium, 2018. Association for Computational Linguistics. doi: 10.18653/v1/D18-1404. URL https://aclanthology. org/D18-1404. Artem Snegirev, Maria Tikhonova, Anna Maksimova, Alena Fenogenova, and Alexander Abramov. The russian-focused embedders exploration: rumteb benchmark and russian embedding model design, 2024. URL https://arxiv.org/abs/2408.12503. Marina Sokolova and Guy Lapalme. systematic analysis of performance measures for classification tasks. Information Processing & Management, 45(4):427437, 2009. ISSN 0306-4573. doi: https://doi.org/10.1016/j.ipm.2009.03.002. URL https://www.sciencedirect.com/ science/article/pii/S0306457309000259. Spearman. The proof and measurement of association between two things. International Journal of Epidemiology, 39(5):11371150, 10 2010. ISSN 0300-5771. doi: 10.1093/ije/dyq191. URL https://doi.org/10.1093/ije/dyq191. Alexander Strehl and Joydeep Ghosh. Cluster ensembles knowledge reuse framework for combining multiple partitions. J. Mach. Learn. Res., 3(null):583617, March 2003. ISSN 1532-4435. doi: 10.1162/153244303321897735. URL https://doi.org/10.1162/ 153244303321897735. Yixuan Tang and Yi Yang. Finmteb: Finance massive text embedding benchmark, 2025. URL https://arxiv.org/abs/2502.10990. Henrique Schechter Vera, Sahil Dua, Biao Zhang, Daniel Salz, Ryan Mullins, Sindhu Raghuram Panyam, Sara Smoot, Iftekhar Naim, Joe Zou, Feiyang Chen, Daniel Cer, Alice Lisak, Min Choi, Lucas Gonzalez, Omar Sanseviero, Glenn Cameron, Ian Ballantyne, Kat Black, Kaifeng Chen, Weiyi Wang, Zhe Li, Gus Martins, Jinhyuk Lee, Mark Sherwood, Juyeong Ji, Renjie Wu, Jingxiao Zheng, Jyotinder Singh, Abheesht Sharma, Divya Sreepat, Aashi Jain, Adham Elarabawy, AJ Co, Andreas Doumanoglou, Babak Samari, Ben Hora, Brian Potetz, Dahun Kim, Enrique Alfonseca, Fedor Moiseev, Feng Han, Frank Palma Gomez, Gustavo Hern√°ndez √Åbrego, Hesen Zhang, Hui Hui, Jay Han, Karan Gill, Ke Chen, Koert Chen, Madhuri Shanbhogue, Michael Boratko, Paul Suganthan, Sai Meher Karthik Duddu, Sandeep Mariserla, Setareh Ariafar, Shanfeng Zhang, Shijie Zhang, Simon Baumgartner, Sonam Goenka, Steve Qiu, Tanmaya Dabral, Trevor Walker, Vikram Rao, Waleed Khawaja, Wenlei Zhou, Xiaoqi Ren, Ye Xia, Yichang Chen, Yi-Ting Chen, Zhe Dong, Zhongli Ding, Francesco Visin, Ga√´l Liu, Jiageng Zhang, Kathleen Kenealy, Michelle Casbon, Ravin Kumar, Thomas Mesnard, Zach Gleicher, Cormac Brick, Olivier Lacombe, Adam Roberts, Yunhsuan Sung, Raphael Hoffmann, Tris Warkentin, Armand Joulin, Tom Duerig, and Mojtaba Seyedhosseini. Embeddinggemma: Powerful and lightweight text representations, 2025. URL https://arxiv.org/abs/2509.20354. Nguyen Vinh, Julien Epps, and James Bailey. Information theoretic measures for clusterings comparison: Variants, properties, normalization and correction for chance. Journal of Machine Learning Research, 11:28372854, 10 2010. Ellen Voorhees and Tice. Building question answering test collection. (34), 2000-07-01 2000. 13 Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: multi-task benchmark and analysis platform for natural language understanding. In Tal Linzen, Grzegorz Chrupa≈Ça, and Afra Alishahi (eds.), Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446/. Alex Wang, Yada Pruksachatkun, Nikita Nangia, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. SuperGLUE: stickier benchmark for general-purpose language understanding systems. Curran Associates Inc., Red Hook, NY, USA, 2019. Liang Wang, Nan Yang, Xiaolong Huang, Binxing Jiao, Linjun Yang, Daxin Jiang, Rangan Majumder, and Furu Wei. Text embeddings by weakly-supervised contrastive pre-training. ArXiv, abs/2212.03533, 2022. URL https://api.semanticscholar.org/CorpusID: 254366618. Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368, 2023. Silvan Wehrli, Bert Arnrich, and Christopher Irrgang. German text embedding clustering benchmark, 2024. URL https://arxiv.org/abs/2401.02709. Orion Weller, Benjamin Chang, Sean MacAvaney, Kyle Lo, Arman Cohan, Benjamin Van Durme, Dawn Lawrie, and Luca Soldaini. Followir: Evaluating and teaching information retrieval models to follow instructions, 2024. Chenghao Xiao, Isaac Chung, Imene Kerboua, Jamie Stirling, Xin Zhang, M√°rton Kardos, Roman Solomatin, Noura Al Moubayed, Kenneth Enevoldsen, and Niklas Muennighoff. Mieb: Massive image embedding benchmark, 2025. URL https://arxiv.org/abs/2504.10471. Shitao Xiao, Zheng Liu, Peitian Zhang, Niklas Muennighoff, Defu Lian, and Jian-Yun Nie. C-pack: Packed resources for general chinese embeddings, 2024. URL https://arxiv.org/abs/ 2309.07597. Dun Zhang, Jiacheng Li, Ziyang Zeng, and Fulong Wang. Jasper and stella: distillation of sota embedding models, 2025a. URL https://arxiv.org/abs/2412.19048. Yanzhao Zhang, Mingxin Li, Dingkun Long, Xin Zhang, Huan Lin, Baosong Yang, Pengjun Xie, An Yang, Dayiheng Liu, Junyang Lin, Fei Huang, and Jingren Zhou. Qwen3 embedding: Advancing text embedding and reranking through foundation models, 2025b. URL https://arxiv.org/abs/2506.05176. Erfan Zinvandi, Morteza Alikhani, Mehran Sarmadi, Zahra Pourbahman, Sepehr Arvin, Reza Kazemi, and Arash Amini. Famteb: Massive text embedding benchmark in persian language. arXiv preprint arXiv:2502.11571, 2025."
        },
        {
            "title": "A DETAILED RESULTS BY TASK CATEGORY",
            "content": "This section provides comprehensive results for all tasks, organized by category. Each table includes human performance alongside all 13 evaluated models, with inter-annotator agreement metrics where available. Table 3 presents full results of the clustering tasks. Table 4 presents full results of the classification tasks. Table 5 presents full results of the reranking tasks. Table 6 presents full results of the STS tasks."
        },
        {
            "title": "B TASK EXAMPLES",
            "content": "This section provides screenshots of the actual Argilla annotation interfaces used in our study, illustrating the annotation challenges and interface design that human annotators encountered. 14 Figure 4: Emotion Classification annotation interface showing the 6-category emotion labeling task. This task achieved fair inter-annotator agreement (Œ∫ = 0.39) due to ambiguous emotional states and mixed emotions in social media text. Human performance: 45.8%, Best model: 87.1%. Figure 5: Tweet Sentiment Classification annotation interface demonstrating sentiment polarity annotation. This task achieved moderate inter-annotator agreement (Œ∫ = 0.48) with reasonable consensus on positive/negative sentiment. Human performance: 84.4%, Best model: 90.9%. Figure 6: ArXiv Clustering annotation interface showing academic papers that caused complete annotator disagreement (ARI = 0.001) due to interdisciplinary research overlap. Papers could be categorized by methodology, application domain, or research community, leading to fundamental disagreement. Human performance: 49.2%, Best model: 84.6%. Figure 7: Reddit Clustering annotation interface demonstrating thematic grouping of discussion topics. This task achieved fair agreement (ARI = 0.34) due to overlapping themes across different discussion topics. Human performance: 68.8%, Best model: 100%. Figure 8: SIB200 Clustering annotation interface showing multilingual sentence clustering task. This task achieved moderate inter-annotator agreement (ARI = 0.42) with variation across languages depending on cultural context and sentence complexity. 15 Figure 9: Robust04 Reranking annotation interface showing document relevance assessment for information retrieval queries. This task achieved strong inter-annotator agreement (œÅ = 0.72). Human performance: 88.5%, Best model: 98.8%. Figure 10: Wikipedia Multilingual Reranking annotation interface demonstrating cross-lingual relevance judgment. This task achieved moderate agreement (œÅ = 0.64) due to cross-lingual complexity. Figure 11: STS12 annotation interface showing semantic similarity assessment using 0-5 scale. This well-curated dataset achieved strong inter-annotator agreement (œÅ = 0.77). Human performance: 91.2%, Best model: 92.0%. Figure 12: SICK-R annotation interface showing semantic relatedness and entailment task. This task achieved moderate agreement (œÅ = 0.68) due to task complexity. Human performance: 82.6%, Best model: 94.1%."
        },
        {
            "title": "Arxiv Reddit",
            "content": "SIB200 WikiCities all-MiniLM-L6-v2 all-mpnet-base-v2 e5-mistral-7b-instruct embeddinggemma-300m gte-Qwen2-1.5B-instruct jasper_en_vision_language_v1 multilingual-e5-base multilingual-e5-large multilingual-e5-small mxbai-embed-large-v1 Qwen3-Embedding-0.6B SFR-Embedding-Mistral stella_en_1.5B_v"
        },
        {
            "title": "Human",
            "content": "61.8 56.0 77.2 73.0 70.8 83.9 31.9 51.1 30.5 48.3 69.1 75.5 84.6 49.2 56.5 63.0 74.9 72.8 88.4 95.1 60.0 55.4 76.9 72.1 76.3 81.4 100.0 68.8 33.2 33.2 73.8 36.0 72.3 59.0 34.4 37.8 38.7 33.0 65.5 74.8 44.2 65. Table 3: Full clustering results. 73.8 86.7 100.0 70.3 73.6 95.1 66.2 69.2 72.6 90.8 74.8 100.0 86.4 97."
        },
        {
            "title": "Emotion MultilSenti ToxicConvo TweetSenti",
            "content": "all-MiniLM-L6-v2 all-mpnet-base-v2 e5-mistral-7b-instruct embeddinggemma-300m gte-Qwen2-1.5B-instruct jasper_en_vision_language_v1 multilingual-e5-base multilingual-e5-large multilingual-e5-small mxbai-embed-large-v1 Qwen3-Embedding-0.6B SFR-Embedding-Mistral stella_en_1.5B_v"
        },
        {
            "title": "Human",
            "content": "42.1 44.0 47.3 40.4 56.2 75.4 36.2 38.5 33.8 42.1 51.9 46.7 71.9 45.8 57.9 61.5 75.9 64.6 78.1 77.3 78.6 81.1 75.8 64.4 74.1 77.1 76.1 87.5 66.4 60.4 68.0 66.4 78.7 86.7 66.0 63.3 65.1 68.7 74.4 67.3 82.9 73. 59.6 58.4 75.8 67.8 79.3 90.9 69.1 65.3 69.6 65.8 87.8 75.6 89.1 84.4 Table 4: Full Classification results."
        },
        {
            "title": "C TASK QUALITY ANALYSIS",
            "content": "C.1 DATASET QUALITY ISSUES Our analysis revealed quality issues across multiple datasets that significantly impact human-model performance comparisons. These issues fall into several categories that help explain performance patterns observed in our study. C.1.1 STS22-RUSSIAN The Russian subset of STS22 dataset shows patterns that may help explain the comparatively low human agreement we observed."
        },
        {
            "title": "Model",
            "content": "Core17 News21 Robust04 Wikipedia all-MiniLM-L6-v2 all-mpnet-base-v2 e5-mistral-7b-instruct embeddinggemma-300m gte-Qwen2-1.5B-instruct jasper_en_vision_language_v1 multilingual-e5-base multilingual-e5-large multilingual-e5-small mxbai-embed-large-v1 Qwen3-Embedding-0.6B SFR-Embedding-Mistral stella_en_1.5B_v5 95.6 98.6 98.8 84.2 97.5 98.2 96.2 95.7 95.6 97.2 97.0 97.9 98.6 85.2 98.8 98.6 99.5 91.4 99.2 100.0 98.6 97.8 98.1 98.0 100.0 99.7 100.0 92. 96.3 97.8 98.8 89.8 98.5 98.7 96.9 97.2 97.5 98.6 98.5 98.8 98.3 88.5 77.8 79.2 88.4 76.0 86.1 88.8 88.5 92.6 87.6 85.6 86.8 87.9 89.2 87."
        },
        {
            "title": "Model",
            "content": "Table 5: Full Reranking results. SICK-R STS12 STS"
        },
        {
            "title": "STSB",
            "content": "all-MiniLM-L6-v2 all-mpnet-base-v2 e5-mistral-7b-instruct embeddinggemma-300m gte-Qwen2-1.5B-instruct jasper_en_vision_language_v1 multilingual-e5-base multilingual-e5-large multilingual-e5-small mxbai-embed-large-v1 Qwen3-Embedding-0.6B SFR-Embedding-Mistral stella_en_1.5B_v"
        },
        {
            "title": "Human",
            "content": "91.5 89.8 93.2 72.6 93.4 93.8 91.5 89.4 88.6 93.4 93.3 94.1 92.3 82.6 85.7 83.7 89.1 77.9 86.9 92.0 86.5 89.9 87.6 91.1 91.8 89.2 89.1 91.2 48.4 54.3 58.5 57.9 60.3 67.2 63.3 65.9 63.3 53.9 63.6 59.3 64.3 68. 81.8 78.4 85.9 58.3 80.0 88.7 82.9 83.6 81.9 88.9 90.9 86.4 87.1 80.4 Table 6: Full STS results. Context Expansion Issues: Sentence pairs labeled as 4 (identical meaning) where one sentence contains basic information and the paired sentence includes additional backstory or context Translated example pattern: Company reports earnings vs. Company reports earnings of $X million, exceeding expectations due to strong performance in sector Human annotators correctly identify these as semantically different (similarity 2-3), while gold labels mark them as identical 18 This explains the low human performance on STS22-Russian (58.5%) compared to models (69.5%) Parsing and Processing Errors: Incomplete sentence parsing affecting semantic interpretation Parsing artifacts from web pages (e.g., page numbers, lists of automatically generated related news, ads) C.1.2 MULTILINGUAL SENTIMENT CLASSIFICATION-RUSSIAN The Russian subset of MultilingualSentimentClassification consists of news articles from different news sites. The task is to classify each text as positive or negative. However, the dataset presents several challenges: Neutral and Ambiguous Content: Many samples are based on press releases from companies or government departments, which are often neutral in tone and difficult to categorize as positive or negative. Translated example: The total amount of pension savings accumulated in JSC Unified Accumulative Pension Fund (UAPF) as of September 1, 2016, amounted to about 6.41 trillion tenge, the press center of the pension fund said, KazTAG reports. ... Such sentences are more informative than sentiment-bearing. Parsing and Processing Errors: Similar to the issues described in C.1.1, the dataset contains parsing artifacts from web pages. C.1.3 EMOTION CLASSIFICATION The emotion classification dataset suffers from inherent label ambiguity that explains the low human agreement (Œ∫ = 0.39): Mixed Emotional States: Texts expressing multiple emotions simultaneously: am feeling very indecisive and spontaneous (labeled as fear but could be surprise) was feeling pretty anxious all day but my first day at work was very good day and that helped lot (contains both fear and joy) am feeling crampy and cranky (physical discomfort mixed with anger) Sarcastic and Ironic Expressions: got paid too much because get so many deliveries at work im feeling bit shamed so will curb the spending for bit (sarcasm about being overpaid) feel like such noob when the customers make really dull and stupid jokes that im supposed to find funny (surface sadness but underlying anger/frustration) feel very cheated since am supporting the family and doing all the other stuff while he spends hours day gaming (labeled as joy but clearly expressing anger) Contextual and Subjective Interpretation: feel shame in strange way (ambiguous emotional context, labeled as surprise) feel all funny sometimes (vague emotional description that could be multiple categories) feel underappreciated and under valued (could be sadness, anger, or fear depending on interpretation) C.1.4 ARXIV CLUSTERING CHALLENGES Academic paper clustering presents fundamental categorization difficulties that explain the complete breakdown of human agreement (ARI = 0.001). This task uses derived labels from ArXiv paper categories: Interdisciplinary Research Papers: Self-Supervised Audio-Visual Representation Learning with Relaxed Cross-Modal Synchronicity (could cluster with computer vision, audio processing, or self-supervised learning) The architecture of innovation: Tracking face-to-face interactions with ubicomp technologies (spans social science, computer science, and architecture) PIINET: 360-degree Panoramic Image Inpainting Network Using Cube Map (computer vision, graphics, or deep learning focus) Methodological vs. Application Domain Confusion: Convergent Actor-Critic Algorithms Under Off-Policy Training and Function Approximation (reinforcement learning methodology vs. control theory application) Learning-Based Adaptive IRS Control with Limited Feedback Codebooks (machine learning method vs. wireless communications application) Structure-preserving numerical methods for stochastic Poisson systems (numerical methods vs. mathematical physics) Emerging and Cross-Domain Research: The modularity of action and perception revisited using control theory and active inference (cognitive science, control theory, or neuroscience) Food-chain competition influences genes size (evolutionary biology, computational biology, or mathematical modeling) Wavelet Analysis of Dengue Incidence and its Correlation with Weather and Vegetation Variables in Costa Rica (epidemiology, signal processing, or environmental science) C.2 IMPACT ON EVALUATION These quality issues have several critical implications for embedding evaluation: 1. Artificial Model Advantages: Models may achieve superhuman performance by consistently reproducing systematic labeling patterns rather than demonstrating superior semantic understanding. This is particularly evident in tasks with low human agreement where models can exploit consistent but incorrect labeling patterns. 2. Misleading Benchmarks: Tasks with fundamental quality issues provide unreliable targets for model development. High model performance on such tasks may not indicate genuine capability improvements but rather successful pattern matching to flawed gold standards. 3. Cultural and Linguistic Bias: Quality issues disproportionately affect non-English tasks, potentially masking genuine model limitations in cross-cultural understanding while artificially inflating performance on problematic English datasets. 4. Evaluation Validity: The validity of using these datasets as benchmarks is questionable when human experts cannot agree on correct answers, suggesting fundamental issues with task specification rather than human limitations. INTER-ANNOTATOR AGREEMENT ANALYSIS D.1 AGREEMENT METRICS BY TASK CATEGORY This section provides detailed inter-annotator agreement analysis using task-appropriate metrics. Agreement levels follow standard guidelines: Œ∫ > 0.8 (excellent), 0.6 < Œ∫ 0.8 (substantial), 20 0.4 < Œ∫ 0.6 (moderate), 0.2 < Œ∫ 0.4 (fair), Œ∫ 0.2 (poor). For correlations: œÅ > 0.7 (strong), 0.4 < œÅ 0.7 (moderate), œÅ 0.4 (weak). D.1.1 CLASSIFICATION TASKS Emotion Classification: Œ∫ = 0.39 (fair agreement) 2 annotators, 48 items, 96 total annotations Mean percentage agreement: 52.1% Performance: Human 45.8%, Best model 87.1% Toxicity Classification: Œ∫ = 0.55 (moderate agreement) 2 annotators, 45 items, 90 total annotations Mean percentage agreement: 77.8% Performance: Human 73.3%, Best model 86.7% Tweet Sentiment Classification: Œ∫ = 0.41 (moderate agreement) 2 annotators, 45 items, 90 total annotations Mean percentage agreement: 62.2% Performance: Human 84.4%, Best model 90.9% Multilingual Sentiment Classification: Agreement only for English English: Œ∫ = 0.24 (fair agreement), 2 annotators, 40 items, 62.5% agreement Arabic, Norwegian, Russian: Single annotator (no agreement metrics) Performance: Human advantages in non-English variants D.1.2 CLUSTERING TASKS ArXiv Clustering: ARI = 0.001 (no agreement) 2 annotators, 30 items, 60 total annotations Complete breakdown of consensus on academic paper categories Performance: Human 49.2%, Best model 84.6% Reddit Clustering: ARI = 0.42 (moderate agreement) 2 annotators, 30 items, 60 total annotations Moderate consensus on discussion topic groupings Performance: Human 68.8%, Best model 100% WikiCities Clustering: ARI = 0.91 (excellent agreement) 2 annotators, 30 items, 60 total annotations High consensus on geographical entity groupings Performance: Human 97.6%, Best model 100% SIB200 Clustering: Agreement only for English English: ARI = 0.15 (weak agreement), 2 annotators, 30 items Arabic, Danish, Russian: Single annotator (no agreement metrics) Performance varies significantly across languages D.1.3 RERANKING TASKS News21: œÅ = 0.85 (strong agreement) 2 annotators, 31 items, 62 total annotations Mean Kendall tau: 0.85, Binary kappa: 0.83 Performance: Human 92.7%, Best model 100% Core17: œÅ = 0.80 (strong agreement) 2 annotators, 20 items, 40 total annotations Mean Kendall tau: 0.80, Binary kappa: 0.78 21 Performance: Human 85.2%, Best model 98.8% Robust04: œÅ = 0.75 (strong agreement) 2 annotators, 49 items, 98 total annotations Mean Kendall tau: 0.75, Binary kappa: 0.72 Performance: Human 88.5%, Best model 98.8% Wikipedia Multilingual Reranking: Agreement only for English English: œÅ = 0.64 (moderate agreement), 2 annotators, 30 items Mean Kendall tau: 0.64, Binary kappa: 0.60 Danish, Norwegian: Single annotator (no agreement metrics) Performance varies across languages D.1.4 STS TASKS STS12: œÅ = 0.77 (strong agreement) 2 annotators, 50 items, 100 total annotations Performance: Human 91.2%, Best model 92.0% STSBenchmark: œÅ = 0.58 (moderate agreement) 2 annotators, 50 items, 100 total annotations Performance: Human 80.4%, Best model 90.9% SICK-R: œÅ = 0.63 (moderate agreement) 2 annotators, 40 items, 80 total annotations Performance: Human 82.6%, Best model 94.1% STS22: Agreement only for English English: œÅ = 0.75 (strong agreement), 2 annotators, 30 items Arabic, Russian: Single annotator (no agreement metrics) Performance varies significantly by language D.2 AGREEMENT PATTERNS AND TASK RELIABILITY D.2.1 HIGH-AGREEMENT TASKS (RELIABLE BENCHMARKS) Tasks with high human agreement (Œ∫ > 0.6 or œÅ > 0.7) consistently demonstrate: Clear, objective task specifications with minimal ambiguity Adequate context for making informed judgments Cultural and linguistic familiarity for annotators Well-defined evaluation criteria with concrete examples Minimal dataset quality issues or processing artifacts Consistent performance patterns across annotators Examples: WikiCities clustering, News21/Core17/Robust04 reranking, STS12, STSBenchmark D.2.2 LOW-AGREEMENT TASKS (PROBLEMATIC BENCHMARKS) Tasks with low agreement (Œ∫ < 0.4 or œÅ < 0.6) often exhibit: Ambiguous annotation guidelines or subjective judgment requirements Cross-cultural interpretation challenges Insufficient context for accurate assessment Systematic dataset quality issues or processing artifacts Inherently subjective or multi-faceted concepts Inconsistent or contradictory gold standard labels Examples: Emotion classification, ArXiv clustering, STS22-Russian"
        },
        {
            "title": "E ADDITIONAL HUMAN VS MODEL ANALYSIS",
            "content": "Figure 13: Human performance gaps versus best-performing models across 26 task-language pairs. Humans outperform the best models on only 4 tasks (15.4%), with largest advantages in Arabic semantic similarity and sentiment analysis. The analysis reveals systematic model advantages in technical domains (clustering, reranking) versus human advantages in culturally-informed tasks. This section contains additional analysis on human vs model. Figure 14 shows the human performance gaps versus median-performing models over all tasks by language. Figure 15 shows the task difficulty categorization based on human performance levels. Figure 16 shows the model consistency analysis showing performance ranges across tasks. Figure 2 shows comprehensive view of human performance relative to all model performance ranges across 16 tasks by language."
        },
        {
            "title": "F MODELS EVALUATED",
            "content": "Table 7 shows information about each evaluated model. 23 Figure 14: Human performance gaps versus median-performing models across 26 tasks by language. Humans achieve 61.5% win rate against median models, demonstrating competitive performance when compared to typical rather than best-performing models. This analysis reveals that human performance is much more competitive when compared against representative model performance rather than cherry-picked best results. 24 Figure 15: Task difficulty categorization based on human performance levels. The majority of tasks (69%) fall into the easy category (human performance 0.7), shown in green. Only two tasks fall below 0.5 (shown in red), both with notably low inter-annotator agreement, suggesting fundamental task ambiguity rather than limitations of human ability. Figure 16: Model consistency analysis showing performance ranges across tasks. Higher value indicates greater variability across models (lower consistency). Tasks with small ranges (high consistency) often align with high human agreement, whereas tasks with large ranges (low consistency) typically correspond to tasks where humans also struggle. This pattern suggests that both human and model performance reflect underlying task quality and clarity of task specification. 26 Model Parameters (Millions) Alibaba-NLP/gte-Qwen2-1.5B-instruct Li et al. (2023) google/embeddinggemma-300m Vera et al. (2025) intfloat/e5-mistral-7b-instruct Wang et al. (2023; 2022) intfloat/multilingual-e5-large Wang et al. (2022) intfloat/multilingual-e5-base Wang et al. (2022) intfloat/multilingual-e5-small Wang et al. (2022) mixedbread-ai/mxbai-embed-large-v1 Lee et al. (2024); Li & Li (2023) NovaSearch/jasper_en_vision_language_v1 Zhang et al. (2025a) Qwen/Qwen3-Embedding-0.6B Zhang et al. (2025b) Salesforce/SFR-Embedding-Mistral (Meng et al., 2024) sentence-transformers/all-MiniLM-L6-v2 Reimers & Gurevych (2019) sentence-transformers/all-mpnet-base-v2 Reimers & Gurevych (2019) 1780 300 7111 560 278 118 335 1999 596 7110 22.7 Table 7: List of all evaluated models. Model sizes are in millions of parameters"
        },
        {
            "title": "G LLM USAGE STATEMENT",
            "content": "Large language models were used to assist with formatting, citation integration, and writing polish during the preparation of this manuscript. Specifically, we used LLMs for: Formatting assistance for LaTeX tables and mathematical notation Integration and standardization of citation formats Minor writing improvements for clarity and flow Code documentation and data processing script organization All substantive content, including research design, data analysis, interpretation of results, and scientific conclusions, was developed entirely by the authors. The core contributions, methodology, and findings presented in this work are the original intellectual contribution of the research team. LLM assistance was limited to technical formatting and presentation improvements that did not influence the scientific content or conclusions of the study."
        }
    ],
    "affiliations": [
        "Aarhus University",
        "Carleton University",
        "SberAI",
        "Stanford University",
        "Zendesk"
    ]
}