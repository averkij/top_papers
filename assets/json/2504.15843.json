{
    "paper_title": "Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using a Guiding Reference Model",
    "authors": [
        "Junshu Pan",
        "Wei Shen",
        "Shulin Huang",
        "Qiji Zhou",
        "Yue Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of a data weight adjuster. However, the common practice of initializing the policy and reference models identically in DPO can lead to inefficient data utilization and impose a performance ceiling. Meanwhile, the lack of a reference model in Simple Preference Optimization (SimPO) reduces training robustness and necessitates stricter conditions to prevent catastrophic forgetting. In this work, we propose Pre-DPO, a simple yet effective DPO-based training paradigm that enhances preference optimization performance by leveraging a guiding reference model. This reference model provides foresight into the optimal policy state achievable through the training preference data, serving as a guiding mechanism that adaptively assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Extensive experiments on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently improves the performance of both DPO and SimPO, without relying on external models or additional data."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 3 4 8 5 1 . 4 0 5 2 : r Preprint. Under review. Pre-DPO: Improving Data Utilization in Direct Preference Optimization Using Guiding Reference Model Junshu Pan1,2,3 Wei Shen4 Shulin Huang1,2 Qiji Zhou2 Yue Zhang2 1Zhejiang University 3Shanghai Innovation Institute 2School of Engineering, Westlake University 4Independent Researcher {panjunshu,zhangyue}@westlake.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "Direct Preference Optimization (DPO) simplifies reinforcement learning from human feedback (RLHF) for large language models (LLMs) by directly optimizing human preferences without an explicit reward model. We find that during DPO training, the reference model plays the role of data weight adjuster. However, the common practice of initializing the policy and reference models identically in DPO can lead to inefficient data utilization and impose performance ceiling. Meanwhile, the lack of reference model in Simple Preference Optimization (SimPO) reduces training robustness and necessitates stricter conditions to prevent catastrophic forgetting. In this work, we propose Pre-DPO, simple yet effective DPO-based training paradigm that enhances preference optimization performance by leveraging guiding reference model. This reference model provides foresight into the optimal policy state achievable through the training preference data, serving as guiding mechanism that adaptively assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Extensive experiments on AlpacaEval 2.0 and Arena-Hard v0.1 benchmarks demonstrate that Pre-DPO consistently improves the performance of both DPO and SimPO, without relying on external models or additional data."
        },
        {
            "title": "Introduction",
            "content": "Reinforcement learning from human feedback (RLHF) has become crucial paradigm in aligning large language models (LLMs) with human preferences and values (Christiano et al., 2017; Ouyang et al., 2022; Achiam et al., 2023). This approach involves training an explicit reward model to provide reward signals and performing reinforcement learning under KL constraint by reference model instantiated identically to the initial policy. Direct Preference Optimization (DPO) (Rafailov et al., 2023), as representative of referencebased preference optimization methods (Rafailov et al., 2023; Ethayarajh et al., 2024; Azar et al., 2024; Gorbatovski et al., 2024), transforms the objective of RLHF into direct preference training of the language model, enabling offline preference learning under the constraint of the reference model. Recently, it has been shown that reference model is not necessary for achieving effective preference optimization. Simple Preference Optimization (SimPO) (Meng et al., 2024), as representative of reference-free preference optimization methods (Xu et al., 2023; Xu et al., 2024; Hong et al., 2024; Meng et al., 2024), eliminates the need for reference model and yields better performance and efficiency, though at the cost of an increased risk of catastrophic forgetting (Meng et al., 2024). Meanwhile, other studies have empirically demonstrated that DPO can benefit either from relaxing the constraints imposed by the Corresponding author. 1Our code and data can be found at https://github.com/DtYXs/Pre-DPO. 1 Preprint. Under review. Figure 1: Pre-DPO further enhances the performance of DPO and SimPO by leveraging guiding reference model, consistently across different models and scales. reference model (Gorbatovski et al., 2024) or from stronger external reference models (Liu et al., 2024). However, decreasing reliance on reference model imposes stricter practical requirements to ensure effective learning (Meng et al., 2024), and clear methodologies for obtaining an appropriate reference model are still lacking. Despite the empirical efforts, the role of the reference model in DPO and its impact on training dynamics remain insufficiently explored. We find that during DPO training, the reference model plays the role of data weight adjuster (see Section 4.1). It adaptively tends to assign higher weights to data aligned with itself while reducing weights for conflicting data. However, due to the common practice in DPO of initializing the policy and reference models identically (Rafailov et al., 2023), as training progresses, the reference model increasingly constrains the policy model by penalizing deviations, potentially introducing performance ceiling. Moreover, identical initialization of policy and reference models results in the nearly uniform weighting of training examples during the early training stages, which is in contrast to prior studies showing that assigning non-uniform weights to training data can lead to improved learning and performance (Lin et al., 2017; Ren et al., 2018; Shu et al., 2019). In light of the limitations of conventional reference models, we hypothesize that an effective reference model for DPO should (1) differ from the initial policy model and (2) provide insights into potential directions for policy improvement based on the preference data. We define this type of reference model as guiding reference model, which can better support learning by transforming the role of the reference model from static constraint into guide with foresight (see Section 4.2). Building on this insight, we propose Pre-DPO, simple yet effective training paradigm that enhances data utilization and improves the performance of existing preference optimization methods without relying on external models or additional data. Pre-DPO first optimizes the initial policy using standard preference optimization method (e.g., DPO or SimPO). The resulting optimized policy is then employed as the guiding reference model. Finally, the initial policy is re-optimized using DPO with this guiding reference model, yielding better-optimized policy through more effective data reweighting. The guiding reference model in Pre-DPO essentially serves as an adaptive guiding mechanism that dynamically assigns higher weights to samples more suitable for the model and lower weights to those less suitable. Specifically, these suitable cases typically correspond to examples that are easier to learn, allowing the model to leverage data that aligns well with its learning trajectory efficiently. 2 Preprint. Under review. Our extensive experiments results conducted on the Llama3.2 (Grattafiori et al., 2024) and Qwen2.5 (Yang et al., 2024) model series demonstrate that Pre-DPO can consistently improve the performance of both DPO and SimPO on the AlpacaEval 2 (Li et al., 2023; Dubois et al., 2024) and Arena-Hard v0.1 (Li et al., 2024) benchmarks, achieving average improvements of 2.5 points in the AlpacaEval 2 length-controlled win rate (LC) and 2.6 points in the Arena-Hard v0.1 win rate (WR). By introducing the guiding reference model, Pre-DPO can further improve the performance of existing well-tuned preference optimization methods (as illustrated in Figure 1), effectively overcoming the performance ceiling caused by inefficient data utilization under traditional reference model settings. Notably, Pre-DPO does not rely on external models or additional data, making it highly flexible and easy to deploy."
        },
        {
            "title": "2 Related work",
            "content": "RLHF is currently an effective technique for aligning LLMs with human values and preferences (Ouyang et al., 2022; Achiam et al., 2023; Grattafiori et al., 2024; Yang et al., 2024; Shen et al., 2025). Typically, pretrained LLMs first undergo SFT to acquire the ability to follow human instructions. Subsequently, preference data is annotated by human or AI annotators to train reward model. Finally, policy optimization is performed using reinforcement learning methods, such as Proximal Policy Optimization (PPO) (Schulman et al., 2017), Group Relative Policy Optimization (GRPO) (Shao et al., 2024), REINFORCE++ (Hu, 2025), etc. Although RLHF has demonstrated superior performance, the entire pipeline is complex and challenging. In contrast, offline preference optimization eliminates the need for an explicit reward model and avoids the complex online reinforcement learning optimization process. well-optimized model obtained via preference optimization can also serve as strong initial policy for subsequent reinforcement learning optimization (Yang et al., 2024). Depending on whether or not reference model is required, preference optimization can be classified into two categories: reference-based preference optimization methods (Rafailov et al., 2023; Ethayarajh et al., 2024; Azar et al., 2024; Gorbatovski et al., 2024) and reference-free preference optimization methods (Xu et al., 2023; Xu et al., 2024; Hong et al., 2024; Meng et al., 2024). DPO (Rafailov et al., 2023), as representative of reference-based preference optimization methods, directly trains on preference data and implicitly optimizes the same objective as existing reinforcement learning algorithms. SimPO(Meng et al., 2024), as representative of reference-free preference optimization methods, removes the need for reference model and can achieve better results than DPO at the cost of lower training robustness (Meng et al., 2024). In this paper, we experimentally demonstrate that DPO can also benefit from reference-free preference optimization methods through guiding reference models. For the reference models in DPO, prior work (Liu et al., 2024) empirically suggested that DPO can benefit from stronger and more suitable reference model in certain cases. However, it mainly focuses on stronger external reference models and does not provide theoretical explanation for why they can be beneficial. Gorbatovski et al. (2024) proposed dynamic update strategy to reset the reference model based on the current policy, which could weaken the regularization effect of the reference model and tends to assign more identical weights to the data samples with more frequent updates. In this work, we introduce the concept of guiding reference model, analyze its impact on maximizing data utilization in DPO through data reweighting, and propose an effective methodology for leveraging guiding reference model in DPO."
        },
        {
            "title": "3 Preliminaries",
            "content": "Reinforcement learning from human feedback (RLHF). Given text prompt x, the RLHF training stage aims to increase the probability that an LLM generates response that is better aligned with human values. Specifically, the objective is to maximize the expected reward r(x, y) while controlling the deviation between the policy probability distribution πθ(y x) and the reference probability distribution πref(y x). The optimization objective is 3 Preprint. Under review. formulated as follows: max πθ (x,y)Dπθ [r(x, y)] βD KL [πθ(y x) πref(y x)] , (1) where Dπθ denotes the distribution of the policy model. The KL divergence term constrains the deviation of the policy πθ from the reference model, and β controls the strength of this constraint. Direct Preference Optimization (DPO). DPO (Rafailov et al., 2023) is widely used reference-based preference optimization method that eliminates the need for explicit reward signals and has become component in the post-training pipeline of many popular opensource LLMs (Bi et al., 2024; Jiang et al., 2024; Yang et al., 2024; Xu et al., 2025). It reformulates Eq. 1 into direct optimization process on preference dataset = {(xi, y+ i=1, where is the prompt, y+ is the preferred response, and is the less-preferred response. The objective of DPO is as follows: )}D , LDPO(πθ; πref) = (x,y+,y)D (cid:20) (cid:16) log σ β log πθ(y+ x) πref(y+ x) β log πθ(y x) πref(y x) (cid:17)(cid:21) , (2) where σ() is the sigmoid function, and β is hyperparameter that controls the strength of the reference models constraint. large β imposes strong constraint, which can limit the models performance improvement, while small β may result in insufficient constraints, leading to model degradation (Liu et al., 2024). In practical LLM training, πθ and πref are typically initialized as the supervised fine-tuning (SFT) model. πref remains fixed during training. Simple Preference Optimization (SimPO). SimPO (Meng et al., 2024) is reference-free preference optimization method that removes the reference model in DPO while introducing length normalization and target reward margin. Its loss function is defined as: LSimPO(πθ) = (cid:20) (x,y+,y)D log σ (cid:18) β y+ log πθ(y+ x) log πθ(y x) γ (cid:19)(cid:21) , β (3) where y+ and denote the lengths, β is hyperparameter constant, and γ is the target reward margin. SimPO has the potential to surpass DPO in performance but suffers from reduced robustness due to the lack of reference constraints (Meng et al., 2024)."
        },
        {
            "title": "4 Method",
            "content": "In this section, we first present the motivation behind Pre-DPO, focusing on the limitations of the reference setting in DPO and the specific challenges Pre-DPO addresses. Then, we provide detailed explanation of Pre-DPO, outlining how it adaptively reweights the training examples and describing its overall process (as shown in Figure 2)."
        },
        {
            "title": "4.1 The reference model in DPO is a data weight adjuster",
            "content": "From the loss function of DPO (Eq. 2), we can derive the gradient with respect to the parameters θ: θLDPO(πθ; πref) = βE (x,y+,y)D (cid:20) λ θ log πθ(y+ x) πθ(y x) (cid:21) , where λ is defined as: (cid:18) β log λ = σ πref(y+x) πref(yx) β log πθ(y+x) πθ(yx) (cid:19) . (4) (5) Preprint. Under review. Figure 2: An overview of Pre-DPO. DPO constrains training using the initial policy model as the reference, and SimPO is reference-free. Pre-DPO first obtains an optimized policy model via DPO or SimPO, resets it as guiding reference model, and then re-optimizes the initial policy using DPO. This process improves data utilization and leads to better optimized policy model. From the perspective of example reweighting, DPO learns from preference pairs with weights λ, where the reference model πref controls the training process by adjusting λ. When the policy πθ and the reference πref are initialized from an identical SFT model, λ starts around the constant 0.5 in the early training period due to σ(0) = 0.5. However, constant λ can lead to degeneracy (Rafailov et al., 2023), and more importantly, previous research in the field of example reweighting has shown that assigning appropriate and varying weights to training samples can improve model performances and data efficiency (Lin et al., 2017; Ren et al., 2018; Shu et al., 2019). On the other hand, as training progresses, the reference continuously constrains the policys deviation by adjusting the value of λ. Specifically, when πref(y+x) πref(yx) is large, it encourages larger value of λ, promoting learning from the corresponding preference pair. small ratio typically results in reduced value of λ, which can reduce the models learning from that sample. Therefore, suboptimal configured reference model can lead to suboptimal weighting of training samples."
        },
        {
            "title": "4.2 Pre-DPO: improving data utilization in DPO using a guiding reference model",
            "content": "One straightforward solution is to employ reference model that differs from the initial policy and provides foresight into promising directions for policy improvement based on the preference data D, enabling more effective data reweighting and guidance during training. Notably, model already undergone preference optimization contains information about the entire training process. More importantly, it reflects the outcomes that the initial policy can achieve through the available preference data. Specifically, when the reference model in 5 Preprint. Under review. DPO is set to guiding reference model πguide, the weight λ becomes: (cid:32) λ = σ β log πguide(y+x) πguide(yx) β log πθ(y+x) πθ(yx) (cid:33) . (6) The foresight of the guiding reference model is embodied in its ability to dynamically modulate λ, assigning higher weights to samples that the policy model is capable of learning well and down-weighting those that are difficult to learn or even conflict with the models learning trajectory. This behavior naturally satisfies the conditions suggested in the concurrent work (Gao et al., 2025), where avoiding overly difficult examples benefits alignment. Compared to the reference model used in standard DPO, which applies fixed constraint with no foresight, the guiding reference model offers adaptive and data-dependent reweighting, leading to more efficient and targeted policy improvement. Hence, we propose simple yet effective paradigm for obtaining suitable reference without needing external models. Specifically, we reset the optimized policy model as the reference for the next training iteration. Since the reference model retains all the information from prior policy training, its role shifts from constraint to that of guide, which we refer to as the guiding reference model. Employing this guiding reference model in DPO adaptively assigns higher weights to training data that aligns with it while reducing the weights of conflicting samples. Let πθ denote the policy model to be optimized, πref represent the reference model (which can also be set as None), and M(πθ; πref) indicate the preference optimization method. The procedure of Pre-DPO (illustrated in Figure 2) is described in detail below: Step1: Instantiate the initial reference model. πref to πSFT. Otherwise, for reference-free methods, πref is set to None: (cid:26)πSFT None for reference-based M, for reference-free M. πref = If is reference-based, set the reference (7) Step2: The first round of preference optimization. Perform preference optimization on πSFT with the preference dataset D: πM = M(πSFT; πref). (8) Step3: Set the guiding reference model. After the first round of optimization, reset πref to the optimized model πM obtained from the previous round. This optimized model now serves as the guiding reference model πguide: πguide = πM. (9) Step4: Preference optimization with the guiding reference model. Apply DPO to πSFT using the guiding reference πguide on the same dataset to obtain the better optimized model πPre-DPO: πPre-DPO = MDPO(πSFT; πguide). (10)"
        },
        {
            "title": "5 Experiments",
            "content": "We empirically evaluate the effectiveness of Pre-DPO in enhancing existing preference optimization methods through guiding reference model. To ensure comprehensive and fair assessment, we conduct experiments on the Llama3.2 (Grattafiori et al., 2024) and Qwen2.5 (Yang et al., 2024) model series, including both Base and Instruct versions. We evaluate performance on two widely-used preference optimization benchmarks: AlpacaEval 2 (Li et al., 2023; Dubois et al., 2024) and Arena-Hard v1.0 (Li et al., 2024). Given the sensitivity of preference optimization to hyperparameters (Meng et al., 2024), we follow SimPOs experimental methodology and conduct an extensive hyperparameter search to ensure reliable results. All experiments are conducted based on the LlamaFactory (Zheng et al., 2024) repository, and all models and datasets used are publicly available. 6 Preprint. Under review. 5.1 Experimental setup Models. In our experiments, we primarily consider two widely recognized series of opensource models, Llama3.2-3B and Qwen2.5-7B, including both Base and Instruct versions. The diversity in model types and scales enables more comprehensive evaluation of the methods effectiveness. Datasets. For Base models, we obtain their corresponding SFT versions by training on the UltraChat-200k (Ding et al., 2023) dataset2. Subsequently, the binarized version of the UltraFeedback (Cui et al., 2024) dataset3 is utilized for preference optimization. For Instruct models, we use them directly as SFT models. In the preference optimization stage, we construct two more on-policy preference datasets for Llama3.2-3B-Instruct and Qwen2.5-7B-Instruct, respectively. Specifically, we use prompts from the UltraFeedback dataset to sample responses from the model, generating 6 responses per prompt with temperature of 0.8 and top-p of 0.95. Subsequently, following previous work(Meng et al., 2024), we utilize the ArmoRM-Llama3-8B-v0.1 (Wang et al., 2024) reward model to score each response and select the highest-scoring and lowest-scoring responses to form preference pairs (x, y+, y). Implementation details. For the SFT stage in Base models, we train for 3 epochs using batch size of 32, maximum sequence length of 4096, learning rate of 2 106, and cosine learning rate schedule with 6% warmup ratio. We obtain the guiding reference models for Pre-DPO based on two popular preference optimization methods: DPO (Rafailov et al., 2023), representing reference-based methods, and SimPO (Meng et al., 2024), representing reference-free methods. All preference optimization experiments use batch size of 128, maximum sequence length of 4096, and cosine learning rate schedule with 6% warmup ratio, training for 1 epoch. Given the importance of hyperparameter tuning in offline preference optimization (Meng et al., 2024), we perform extensive hyperparameter searches for all preference optimization experiments to ensure fairness. Specifically, for the key hyperparameters, including the learning rate, β (for DPO and SimPO), and γ (for SimPO), two-stage tuning strategy is employed. We first fix the learning rate and search for the optimal β or γ. Then, with the best β or γ fixed, we search for the optimal learning rate. More details of hyperparameter tuning and the best hyperparameter setting in our experiments can be found in Appendix A.1. Evaluation benchmarks. We evaluate methods primarily on two open-source instructionfollowing benchmarks: AlpacaEval 2 (Li et al., 2023; Dubois et al., 2024) and Arena-Hard v0.1 (Li et al., 2024), which are widely adopted in the community for evaluating the instruction-following capabilities of LLMs (Meng et al., 2024). We report the raw win rate (WR) and the length-controlled win rate (LC) on AlpacaEval 2, and the win rate (WR) on Arena-Hard v0.1, with GPT-4-Turbo serving as the judge model. More evaluation details can be found in Appendix A.2."
        },
        {
            "title": "5.2 Main results",
            "content": "Pre-DPO further improves DPO and SimPO by leveraging guiding reference models. In Table 1, we report the performance on the AlpacaEval 2 and Arena-Hard v0.1 across the Llama3.2-3B-Base, Llama3.2-3B-Instruct, Qwen2.5-7B-Base, Qwen2.5-7B-Instruct settings. Compared with baselines, Pre-DPO achieves better performance on the AlpacaEval 2 LC and WR benchmarks, yielding average improvements of 2.5 and 2.8 points, respectively. On the Arena-Hard v0.1 benchmark, Pre-DPO also consistently demonstrates improvements across most settings. For instance, on Qwen2.5-7B-Instruct, Pre-DPO achieves an improved WR of 68.8 compared to 62.9 of the DPO baseline. These results indicate that Pre-DPO is effective in 2https://huggingface.co/datasets/HuggingFaceH4/ultrachat 200k 3https://huggingface.co/datasets/HuggingFaceH4/ultrafeedback binarized 7 Preprint. Under review. Llama3.2-3B-Base Llama3.2-3B-Instruct Method Ref. AlpacaEval 2 Arena-Hard AlpacaEval 2 Arena-Hard LC (%) WR (%) Len. WR (%) LC (%) WR (%) Len. WR (%) SFT - DPO SimPO - SFT 6.1 10.5 13.1 12.5 Pre-DPO DPO Pre-DPO SimPO 18.1 4.0 12.0 13. 13.9 18.4 1012 2042 1950 2061 2020 2.1 10.6 11. 11.9 14.0 19.0 36.3 33.8 39.3 35.0 18.9 36.9 29. 40.9 32.3 1956 2026 1797 2095 1846 18.5 30.6 28. 34.7 30.0 Qwen2.5-7B-Base Qwen2.5-7B-Instruct Method Ref. AlpacaEval 2 Arena-Hard AlpacaEval 2 Arena-Hard LC (%) WR (%) Len. WR (%) LC (%) WR (%) Len. WR (%) SFT - DPO SimPO - SFT 18.6 24.8 34. Pre-DPO DPO 27.4 Pre-DPO SimPO 37.2 6.9 22.2 31.9 24.5 32.6 892 1778 1790 1758 9.4 33.1 38.1 32.6 41.6 31.2 52.2 51. 53.3 54.6 31.0 56.8 52.4 59.4 55.5 2020 2270 2322 2121 55.9 62.9 62.4 68.8 64.5 Table 1: Performance of Pre-DPO under four different model settings on AlpacaEval 2 and Arena-Hard v0.1. LC and WR denote the length-controlled and raw win rate, respectively. Ref. denotes the reference model and Len. denotes the average response length. The SFT models for the Base settings are trained on the UltraChat-200k dataset, while the Instruct models are used as the SFT models directly for the Instruct settings. The guiding reference models are obtained from DPO and SimPO. further improving both reference-based and reference-free methods by leveraging guiding reference models. Pre-DPO improves performance without significantly increasing the response length. Although Pre-DPO continuously improves performance, we observe that it does not significantly increase the response length compared to the baselines. Notably, in the Qwen2.5-7BBase setting with SimPO as the guiding reference model, Pre-DPO even achieves the best performance with the shortest response length. Additional observations. Interestingly, we observe that SimPO shows significant advantage over DPO in the Base settings, while DPO slightly outperforms SimPO in the Instruct settings. possible reason is that the preference datasets constructed via on-policy sampling exhibit smaller length differences between positive and negative responses (see Appendix B.1 for details), thus diminishing the advantage of SimPOs length normalization."
        },
        {
            "title": "5.3 Ablations",
            "content": "The guiding reference model plays critical role in the improvement of Pre-DPO. Although Pre-DPO consistently improves the performance of DPO and SimPO, it introduces additional computational cost due to the need to obtain guiding reference model. To investigate whether the performance gain is simply due to increased training, we compare it with baseline that trains DPO for 2 epochs with the original reference setting. As shown in Table 2, under the same computational budget, Pre-DPO with guiding reference model consistently achieves the best LC and WR on AlpacaEval 2, benefiting from the better data utilization enabled by the guiding reference model and avoiding the excessive constraints imposed by traditional reference model setups. 8 Preprint. Under review. Policy Model Method Ref. Epochs Llama3.2-3B-Base-SFT DPO SFT SFT DPO Pre-DPO DPO1 DPO SFT Llama3.2-3B-Instruct DPO Pre-DPO DPO1 SFT 1 2 1 2 1 AlpacaEval 2 LC (%) WR (%) Len. 10. 11.0 12.5 36.3 35.2 39.3 12.0 12.0 13.9 36. 37.1 40.9 2042 1976 2061 2026 2113 2095 Table 2: Ablation studies of the guiding reference model. DPO trained for 2 epochs has the same computational cost as Pre-DPO. DPO1 denote the guiding reference model trained with DPO for 1 epoch."
        },
        {
            "title": "Method",
            "content": "Ref."
        },
        {
            "title": "SFT",
            "content": "DPO Pre-DPO DPOA Pre-DPO DPOB DPO Pre-DPO DPOB Pre-DPO DPOA"
        },
        {
            "title": "B\nB\nB",
            "content": "AlpacaEval 2 LC (%) WR (%) Len. 11.6 13.9 12.9 13.9 14.1 13.8 10.5 13.2 12. 12.9 13.4 12.7 1702 1819 1791 1765 1792 1708 Table 3: Ablation studies of Pre-DPO under different guiding reference models and preference datasets. The SFT model is obtained by training Qwen2.5-3B-Base on the UltraChat200k dataset. The preference training sets and are created by randomly splitting the UltraFeedback dataset into two equal parts. DPOA and DPOB denote the guiding reference models trained on training sets and B, respectively. Guiding reference models perform better on previously encountered preference data. To investigate the optimal relationship between the guiding reference model and the preference training data, we divide the preference dataset into two equal parts, and B. We then perform DPO on each part separately to obtain two distinct guiding reference models. For each subset of preference data, we apply Pre-DPO using both guiding reference models for comparison. The results, as shown in Table 3, indicate that guiding reference model trained on the same subset of preference data yields better performance on AlpacaEval 2. This suggests that guiding reference models perform better when guiding policies on data they have previously encountered, as they possess more accurate information about potential policy optimization directions and enable more effective data weighting."
        },
        {
            "title": "5.4 The λ distribution after training of Pre-DPO and vanilla DPO",
            "content": "Based on Eq. 5, Pre-DPO demonstrates distinct advantage through its adaptive data reweighting mechanism, which dynamically assigns higher weights to preference data that align with the learned outcomes of the guiding reference model while diminishing the influence (assign lower weights) of data that may introduce contradictory learning signals. Figure 3 illustrates the differences in λ distributions on the training preference dataset between Pre-DPO and DPO on Llama3.2-3B settings. For clarity, we report the statistics of the static λ values computed on the training dataset after training completion. Pre-DPO produces higher mean and median λ values compared to DPO, indicating that the guiding reference model effectively guides the policy model toward better data utilization. More comparisons between Pre-DPO and the baselines are provided in Appendix B.2. 9 Preprint. Under review. (a) Llama3.2-3B-Base (b) Llama3.2-3B-Instruct Figure 3: Comparative differences in λ (when β is set to 0.01) distributions between PreDPO and DPO across Llama3.2-3B settings. Each subplot presents violin plot illustrating the distribution of λ values on the training preference dataset. The black horizontal line indicates the mean, while the dark gray line represents the median. The width of the violin at each point represents the data density, where thicker sections correspond to regions with higher data concentration."
        },
        {
            "title": "6 Conclusion",
            "content": "We proposed Pre-DPO, simple yet effective DPO-based preference optimization paradigm that enhances data utilization and improves performance by leveraging guiding reference model. Unlike traditional DPO, which uses reference identical to the initial policy, PreDPO reuses previously optimized guiding reference model to re-optimize the initial policy model. This shifts the reference models role from constraint to guidance, enabling better data reweighting. Extensive experiments demonstrate that Pre-DPO consistently outperforms both DPO and SimPO across various models and scales, without relying on external models or additional data. We hope this work can inspire more exploration and discussion on the role and improvement of reference models in LLM alignment."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from human preferences. In International Conference on Artificial Intelligence and Statistics, pp. 44474455. PMLR, 2024. Xiao Bi, Deli Chen, Guanting Chen, Shanhuang Chen, Damai Dai, Chengqi Deng, Honghui Ding, Kai Dong, Qiushi Du, Zhe Fu, et al. Deepseek llm: Scaling open-source language models with longtermism. arXiv preprint arXiv:2401.02954, 2024. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning from human preferences. Advances in Neural Information Processing Systems, 30, 2017. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et al. Ultrafeedback: Boosting language models with scaled ai feedback. In International Conference on Machine Learning, pp. 97229744. PMLR, 2024. Ning Ding, Yulin Chen, Bokai Xu, Yujia Qin, Shengding Hu, Zhiyuan Liu, Maosong Sun, and Bowen Zhou. Enhancing chat language models by scaling high-quality instructional conversations. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 30293051, 2023. 10 Preprint. Under review. Yann Dubois, Balazs Galambosi, Percy Liang, and Tatsunori Hashimoto. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Model In International Conference on Machine alignment as prospect theoretic optimization. Learning, pp. 1263412651. PMLR, 2024. Chengqian Gao, Haonan Li, Liu Liu, Zeke Xie, Peilin Zhao, and Zhiqiang Xu. Principled data selection for alignment: The hidden risks of difficult examples. arXiv preprint arXiv:2502.09650, 2025. Alexey Gorbatovski, Boris Shaposhnikov, Alexey Malakhov, Nikita Surnachev, Yaroslav Aksenov, Ian Maksimov, Nikita Balagansky, and Daniil Gavrilov. Learn your reference model for real good alignment. arXiv preprint arXiv:2404.09656, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Jiwoo Hong, Noah Lee, and James Thorne. Orpo: Monolithic preference optimization without reference model. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 1117011189, 2024. Jian Hu. Reinforce++: simple and efficient approach for aligning large language models. arXiv preprint arXiv:2501.03262, 2025. Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Tianhao Wu, Banghua Zhu, Joseph Gonzalez, and Ion Stoica. From crowdsourced data to high-quality benchmarks: Arenahard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca eval, 2023. Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pp. 29802988, 2017. Yixin Liu, Pengfei Liu, and Arman Cohan. Understanding reference policies in direct preference optimization. arXiv preprint arXiv:2407.13709, 2024. Yu Meng, Mengzhou Xia, and Danqi Chen. Simpo: Simple preference optimization with reference-free reward. Advances in Neural Information Processing Systems, 37:124198124235, 2024. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36:5372853741, 2023. Mengye Ren, Wenyuan Zeng, Bin Yang, and Raquel Urtasun. Learning to reweight examples for robust deep learning. In International conference on machine learning, pp. 43344343. PMLR, 2018. 11 Preprint. Under review. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Wei Shen, Guanlin Liu, Zheng Wu, Ruofei Zhu, Qingping Yang, Chao Xin, Yu Yue, and Lin Yan. Exploring data scaling trends and effects in reinforcement learning from human feedback. arXiv preprint arXiv:2503.22230, 2025. Jun Shu, Qi Xie, Lixuan Yi, Qian Zhao, Sanping Zhou, Zongben Xu, and Deyu Meng. Meta-weight-net: Learning an explicit mapping for sample weighting. Advances in Neural Information Processing Systems, 32, 2019. Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. In Findings of the Association for Computational Linguistics: EMNLP 2024, pp. 1058210592, 2024. Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine translation. In International Conference on Machine Learning, pp. 5520455224. PMLR, 2024. Jin Xu, Zhifang Guo, Jinzheng He, Hangrui Hu, Ting He, Shuai Bai, Keqin Chen, Jialin Wang, Yang Fan, Kai Dang, Bin Zhang, Xiong Wang, Yunfei Chu, and Junyang Lin. Qwen2.5-omni technical report. arXiv preprint arXiv:2503.20215, 2025. Jing Xu, Andrew Lee, Sainbayar Sukhbaatar, and Jason Weston. Some things are more cringe than others: Iterative preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 2023. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. Yaowei Zheng, Richong Zhang, Junhao Zhang, YeYanhan YeYanhan, and Zheyan Luo. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pp. 400410, 2024. 12 Preprint. Under review. Policy Model Method Ref. β SFT SFT DPO SFT DPO SFT SimPO DPO Pre-DPO Pre-DPO SimPO DPO Llama3.2-3B-Base-SFT 0.005 Llama3.2-3B-Base-SFT SimPO 2.5 0.05 Llama3.2-3B-Base-SFT Pre-DPO Llama3.2-3B-Base-SFT Pre-DPO SimPO 0.05 0.05 Llama3.2-3B-Instruct 2.5 Llama3.2-3B-Instruct 0.05 Llama3.2-3B-Instruct Llama3.2-3B-Instruct 0.1 0.005 Qwen2.5-7B-Base-SFT 2.5 Qwen2.5-7B-Base-SFT 0.2 Qwen2.5-7B-Base-SFT 0.2 Qwen2.5-7B-Base-SFT 0.01 Qwen2.5-7B-Instruct 2.5 Qwen2.5-7B-Instruct 0.05 Qwen2.5-7B-Instruct 0.1 Qwen2.5-7B-Instruct SFT DPO SFT SimPO DPO Pre-DPO Pre-DPO SimPO SFT DPO SFT SimPO DPO Pre-DPO Pre-DPO SimPO γ - 1.2 - - - 1.0 - - - 1.4 - - - 1.2 - - Learning rate 1 106 1 106 1 106 6 107 6 107 1 106 1 106 1 106 8 107 8 107 8 107 1 106 5 107 1 106 1 106 6 10 Table 4: Optimal hyperparameters for different experiments."
        },
        {
            "title": "A More experimental details",
            "content": "A.1 Training hyperparameters Hyperparameter tuning. Considering that hyperparameter tuning is crucial for preference optimization, we adopt two-stage hyperparameter search method to ensure both fairness and efficiency. Specifically, in the first stage, we fix the learning rate at 6 107 and individually search for the optimal β in [0.005, 0.01, 0.05, 0.1, 0.2, 0.5, 1.0] for DPO, and search for β in [2.0, 2.5] and γ in [0.3, 0.5, 1.0, 1.2, 1.4, 1.6] for SimPO. In the second stage, we select the two best hyperparameter settings found in the first stage and individually search for the optimal learning rate in [3 107, 5 107, 8 107, 1 106]. The search ranges for the hyperparameters are chosen with reference to prior work (Meng et al., 2024). The optimal hyperparameter values. Table 4 shows the optimal hyperparameter values in our main experiments. It can be observed that Pre-DPO generally requires larger β compared to DPO. A.2 Evaluation details Following previous work (Meng et al., 2024), we adopt sampling-based decoding strategy for AlpacaEval 2 with temperature of 0.7, top-p of 0.9, and greedy decoding strategy for Arena-Hard v0.1. We evaluate the AlpacaEval 2 and Arena-Hard v0.1 results using their official repositories, respectively."
        },
        {
            "title": "B More analysis",
            "content": "B.1 Response length differences We define the normalized length difference between positive and negative responses in preference datasets as follows: Normalized Length Difference = len(y+) len(y) max (len(y+), len(y)) , (11) where len(y) denotes the number of tokens in response y. 13 Preprint. Under review. (a) Qwen2.5-7B-Base (b) Qwen2.5-7B-Instruct (c) Llama3.2-3B-Base (d) Llama3.2-3B-Instruct (e) Qwen2.5-7B-Base (f) Qwen2.5-7B-Instruct Figure 4: More violin plots showing the distribution of λ values on the training preference dataset for Pre-DPO and DPO / SimPO across different settings (with β = 0.01). We compute this metric on three datasets: the original UltraFeedback preference data used for Base models, and the resampled UltraFeedback-Llama3.2 and UltraFeedback-Qwen2.5 preference datasets used for Instruct models. The original UltraFeedback dataset exhibits an average normalized length difference of 0.465, while the resampled on-policy datasets, UltraFeedback-Llama3.2 and UltraFeedback-Qwen2.5, have smaller values of 0.296 and 0.224, respectively. This indicates that the resampled datasets contain less variation in response length, potentially imposing more stringent optimization conditions for SimPO. B.2 The λ distribution after training of Pre-DPO and vanilla DPO / SimPO More differences in λ distributions on the training preference dataset between Pre-DPO and DPO / SimPO are shown in Figure 4. Similar to the case where DPO is used as the guiding reference model, Pre-DPO also produces higher mean and median λ values when using SimPO as the guiding reference."
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "School of Engineering, Westlake University",
        "Shanghai Innovation Institute",
        "Zhejiang University"
    ]
}