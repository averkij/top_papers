{
    "paper_title": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond",
    "authors": [
        "Harsha Nori",
        "Naoto Usuyama",
        "Nicholas King",
        "Scott Mayer McKinney",
        "Xavier Fernandes",
        "Sheng Zhang",
        "Eric Horvitz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Run-time steering strategies like Medprompt are valuable for guiding large language models (LLMs) to top performance on challenging tasks. Medprompt demonstrates that a general LLM can be focused to deliver state-of-the-art performance on specialized domains like medicine by using a prompt to elicit a run-time strategy involving chain of thought reasoning and ensembling. OpenAI's o1-preview model represents a new paradigm, where a model is designed to do run-time reasoning before generating final responses. We seek to understand the behavior of o1-preview on a diverse set of medical challenge problem benchmarks. Following on the Medprompt study with GPT-4, we systematically evaluate the o1-preview model across various medical benchmarks. Notably, even without prompting techniques, o1-preview largely outperforms the GPT-4 series with Medprompt. We further systematically study the efficacy of classic prompt engineering strategies, as represented by Medprompt, within the new paradigm of reasoning models. We found that few-shot prompting hinders o1's performance, suggesting that in-context learning may no longer be an effective steering approach for reasoning-native models. While ensembling remains viable, it is resource-intensive and requires careful cost-performance optimization. Our cost and accuracy analysis across run-time strategies reveals a Pareto frontier, with GPT-4o representing a more affordable option and o1-preview achieving state-of-the-art performance at higher cost. Although o1-preview offers top performance, GPT-4o with steering strategies like Medprompt retains value in specific contexts. Moreover, we note that the o1-preview model has reached near-saturation on many existing medical benchmarks, underscoring the need for new, challenging benchmarks. We close with reflections on general directions for inference-time computation with LLMs."
        },
        {
            "title": "Start",
            "content": "From Medprompt to o1: Exploration of Run-Time Strategies for Medical Challenge Problems and Beyond Harsha Nori*, Naoto Usuyama* Nicholas King, Scott Mayer McKinney, Xavier Fernandes, Sheng Zhang, Eric Horvitz 4 2 0 2 6 ] . [ 1 0 9 5 3 0 . 1 1 4 2 : r Microsoft OpenAI Abstract Run-time steering strategies like Medprompt are valuable for guiding large language models (LLMs) to top performance on challenging tasks. Medprompt demonstrates that general LLM can be focused to deliver state-of-the-art performance on specialized domains like medicine by using prompt to elicit run-time strategy involving chain of thought reasoning and ensembling. OpenAIs o1-preview model represents new paradigm, where model is designed to do run-time reasoning before generating final responses. We seek to understand the behavior of o1-preview on diverse set of medical challenge problem benchmarks. Following on the Medprompt study with GPT-4, we systematically evaluate the o1-preview model across various medical benchmarks. Notably, even without prompting techniques, o1-preview largely outperforms the GPT-4 series with Medprompt. We further systematically study the efficacy of classic prompt engineering strategies, as represented by Medprompt, within the new paradigm of reasoning models. We found that fewshot prompting hinders o1s performance, suggesting that in-context learning may no longer be an effective steering approach for reasoning-native models. While ensembling remains viable, it is resource-intensive and requires careful cost-performance optimization. Our cost and accuracy analysis across run-time strategies reveals Pareto frontier, with GPT-4o representing more affordable option and o1-preview achieving state-of-the-art performance at higher cost. Although o1-preview offers top performance, GPT-4o with steering strategies like Medprompt retains value in specific contexts. Moreover, we note that the o1-preview model has reached near-saturation on many existing medical benchmarks, underscoring the need for new, challenging benchmarks. We close with reflections on general directions for inference-time computation with LLMs. *These authors contributed equally. Correspondence: hanori@microsoft.com, horvitz@microsoft.com"
        },
        {
            "title": "Introduction",
            "content": "Prompt engineering as research area and craft has evolved in step with the fast-paced rise of applications of large language models (LLMs). Prompts shape and focus the capabilities of LLMs trained to follow instructions. In our previous work, we introduced Medprompt, highlighting the effectiveness of inference-time capabilities through use of structured, multi-step prompt pipeline. Medprompt, developed through exploratory work on prompting strategies to enhance model performance on medical challenge benchmarks, significantly boosts performance by leveraging dynamic chain-of-thought reasoning, curated few-shot examples, and choice-shuffle ensembling [NLZ+23b]. We found that these methods focus and amplify the reasoning abilities of LLMs, with particularly valuable application in high-stakes domains like medical diagnostics and decision-making. Figure 1: Pareto frontier showing accuracy versus total API cost (log scale) on the MedQA benchmark (1273 questions total). We compare o1-preview (Sep 2024), GPT-4o (Aug 2024), and GPT-4 Turbo (Nov 2023) with various run-time steering strategies. Medprompt shows that inference-time strategies can bridge the gap between general-purpose LLMs and domain-specific models that rely on fixed set of expert-curated prompts and fine-tuning on specialized datasets. In particular, Medprompt demonstrates how error rates on complex medical benchmarks such as MedQA can be reduced by nearly 50%, without adapting model weights to the medical domain. Recent advancements in model training methodologies, exemplified by OpenAIs o1-preview model, represent novel approach to harnessing the inherent capabilities of LLMs. In distinction to previous models, o1-preview incorporates chain-of-thought (CoT) reasoning as part of its training process, yielding reasoning-native models that inherently perform sophisticated step-by-step problem-solving 2 during inference. The integration of such intrinsic inference capabilities potentially reduces the need for extensive prompt engineering techniques aimed at extracting maximum performance. We re-examine the role and value of sophisticated prompt-engineering strategies, as represented by Medprompt, given the advent of new paradigm of models that perform run-time CoT reasoning, as represented by the o1 series. Our findings (Figure 1 and 2) indicate that the o1-preview model outperforms GPT-4 augmented with Medprompt on the benchmarks studied and suggest diminishing necessity for elaborate prompt-engineering techniques that were highly advantageous for earlier generations of LLMs. These results underscore an evolving landscape in which advances in model training have begun to internalize the principles behind prompt engineering aimed at run-time optimization. (a) (b) Figure 2: (a) Comparative analyses of performance of multiple models on MedQA. (b) Comparisons on wide range of medical challenge benchmarks."
        },
        {
            "title": "2.1 Leveraging LLMs for medical challenge problems",
            "content": "We focus on benchmarks in medical challenge problems as representative specialist area for studying the use of generalist models in realms that rely on highly trained specialists. Generalist LLMs have demonstrated remarkable capabilities for challenging problems in the health and life sciences, often surpassing models constructed with significant domain-specific training and fine-tuning [NKM+23, SAT+22]. In the early era of foundation models, domain-specific pretraining was crucial due to their limited size and computational power. Models such as PubMedBERT [GTC+21], BioGPT [LSX+22], and BioMedLM [BVY+24] were pretrained on specialized datasets like the PubMed corpus and the UMLS knowledge graph using self-supervised learning. Despite their relatively small sizes, these models delivered strong performance on biomedical NLP tasks. However, recent advancements have shown that newer, larger general-domain foundation models can achieve superior performance in medical challenges, even without the need for domain-specific pretraining. Several studies have evaluated the capabilities 3 of generalist foundation models on medical tasks. Notably, ChatGPT-3.5 was able to pass the United States Medical Licensing Exam (USMLE) without specialized training [KCM+23]. Similarly, GPT-4, using simple five-shot prompting, surpassed the USMLE passing score by over 20 points [NKM+23]. The remarkable capabilities of these models have also been observed in medical exams conducted in non-English languages [KKS+23]. To further improve the performance of generalist frontier models in medical challenge problems, the Medprompt strategy [NLZ+23b] was proposed. Medprompt integrates multiple advanced prompting techniques, including nearest-neighbor dynamic few-shot prompting and choice-shuffle ensembling. Notably, GPT-4s performance improved when it generated its own chain-of-thought reasoning for few-shot examples. Using Medprompt, GPT-4 achieves MedQA score of 90.2% and shows strong performance across various other medical benchmarks [NLZ+23b]. While computationally expensive, fine-tuning LLMs for medical applications has also proven effective. Models like Med-PaLM [SAT+22] and Med-PaLM 2 [STG+23], which use the 540B-parameter FlanPaLM with instruction prompt tuning, have achieved strong results in medical QA tasks. Med-PaLM was developed with clinician-curated dataset, while Med-PaLM 2 used comprehensive instructionbased fine-tuning. Similarly, Med-Gemini, fine-tuned on multiple medical datasets and enhanced with web search during training and inference, achieved 91.1% score on MedQA [STW+24]. The recent introduction of OpenAIs o1 preview model [Ope24d], released in September 2024, marks another significant advancement. According to [Ope24d], the o1 preview model is trained using reinforcement learning to think before generating final responses. For challenging problems, o1 can dynamically increase computational resources during inference to achieve better results. While best practices for using the o1 model are still evolving, initial findings indicate that conventional approaches, such as explicit CoT prompting, may sometimes reduce its performance [Ope24a]."
        },
        {
            "title": "2.2 Medprompt: Steering generalist models for specialized domains",
            "content": "While frontier models like GPT-4 exhibit impressive general-purpose performance out-of-the-box, some specialized domains such as medicine require more adaptation for real-world utility. Medprompt offers principled approach toward steering powerful general purpose models toward specialized domains at run-time by combining several advanced prompting techniques to enhance performance in medical contexts. The framework consists of three main components: (1) dynamic, instance-specific in-context learning, (2) chain-of-thought reasoning, and (3) ensembling. As shown in Figure 3, each of these elements significantly improves performance [NLZ+23b]. 4 Figure 3: Visual illustration of Medprompt components and additive contributions to performance on MedQA. The prompting strategy combines kNN-based few-shot example selection, GPT-4generated chain-of-thought prompting, and answer-choice shuffled ensembling. Relative contributions of each component are shown at the bottom. Figure from [NLZ+23b] 2.2.1 Dynamic Few-Shot Prompting: Making the most of context Language models exhibit flexible behavior depending on the content provided within their input context window, making prompting techniques essential for steering LLMs effectively. When solving task, providing clear task definition is important; otherwise, the model may become confused. Clear and precise instructions help narrow the text generation space, ensuring more focused and relevant responses. An important technique for leveraging context is in-context learning [BMR+20], where the model learns patterns and generalizes from few examples provided at run-time. For instance, in medical question-answering (QA) tasks, few relevant QA pairs can be presented to induce the desired answer style in response to trailing question. This approach allows the model to adjust its token generation effectively based on examplesessentially adjusting model behavior at run-time by better specifying the context in which the model will operate. At the limit, dramatically increasing input tokens with thousands of few-shot examples can yield better performance at the cost of additional computational resources [ASZ+24]. Another approach to building more useful context is to tap external resources, such as databases, search engines, and tools. For example, rather than relying on fixed or random sets of prompts, the user can retrieve relevant information from external resources, technique often referred to as RetrievalAugmented Generation (RAG). Medprompt performed RAG by searching databases for the most relevant examples to address each specific problem. Specifically, we employ text-embedding models in our studies with Medprompt: queries and examples are embedded into shared semantic space, enabling the retrieval of closely aligned matches from the database. The strategic use of context remains important, even for next-generation models like the o1 family. Clear, concise instructions, combined with dynamic access to tools, search capabilities, and databases, enable these models to build rich contextual understanding and improve responses. 5 2.2.2 Chain of Thought: Using more tokens for reasoning Chain-of-thought reasoning, which encourages the model to think step-by-step rather than generating direct answers, is also central component of Medprompt [WWS+22b]. For complex medical queries, breaking the problem into smaller, incremental steps improves accuracy. By either prompting (e.g., Lets think step by step) or using few-shot examples with explanations, we can guide LLMs to perform CoT reasoning. This technique can be seen as run-time scaling strategy, as it requires more tokens, i.e., marshaling more compute to produce each answer. One approach with Medprompt is to use few-shot examples with explanations to induce chainof-thought reasoning. To prepare these few-shot examples, Medprompt employed GPT-4 to generate explanations for each question from the candidate pool. If GPT-4s explanations did not align with the ground truth answers, those candidates were removed from the pool. Interestingly, the GPT-4-generated explanations were more detailed than those prepared by human experts and proved to be more effective as few-shot examples. The OpenAI o1 model is trained using reinforcement learning to reason internally before generating its final response [Ope24b]. Previously, explicit prompting was necessary to induce this behavior. However, with o1, this strategy is internalized and can outperform external guidance. In fact, dedicated CoT prompting for o1 is unnecessary and officially not recommended by the model developers [Ope24c]. 2.2.3 Ensembling: Beyond single LLM run Another key direction is orchestrating multiple models, or multiple calls of the same model, to improve accuracy and reliability. One such example is ensembling. Ensembling over portfolios of inferential chains has been employed in AI research for decades for projects focused on inference (such as studies in theorem proving) and machine learning. In machine learning, ensembling methods can reduce variance with little increase in bias, which also proves highly beneficial for language models [CNMCK04, WWS+22a]. Medprompt applies ensembling by generating answers from multiple independent runs of the same question and using majority voting to determine the final output. This ensembling process aligns with the idea of different agents exploring diverse reasoning paths in parallel, followed by consensus step to reach conclusion. To further enhance diversity in reasoning paths, Medprompt randomizes the order of multiple-choice options across runs. Language models, like humans, can exhibit slight biases toward the order of presented options [Blu84, ZCS+23, KLK+20]. Shuffling helps mitigate such positional biases. As result, the aggregated output from these multiple runs is more robust, leading to improved accuracy and consistency. Orchestrating multiple LLMs remains an important growth area for LLMs including o1. Emerging techniques such as multi-agent collaboration, discussions, consensus-building, and orchestration hold great potential to further enhance the capabilities of LLMs."
        },
        {
            "title": "3 Experimental Setup",
            "content": "We evaluate the performance of o1-preview on set of medical benchmarks and compare its accuracy with state-of-the-art models, including capabilities revealed via Medprompt. Our goal is to assess both the medical knowledge and reasoning ability of the models, particularly in scenarios involving patient cases. Our evaluation focuses on multiple-choice question (MCQ) formats, and we report accuracy as the primary evaluation metric for all datasets. We conduct experiments on medical benchmarks: MedQA [JPO+21], MedMCQA (Dev set) [PUS22], MMLU (Medical subset) [HBB+20], NCLEX (Nurse 6 licensing exam) [NLZ+23b], and the newly introduced JMLE-2024 benchmark (Section 4.2). We also evaluate against two sets of official preparatory materials offered by the National Board of Medical Examiners (NBME) to help candidates prepare for the US Medical Licensing Exam (USMLE). For fair comparison with prior work, all experiments follow the benchmark setups described in our prior publication on Medprompt [NLZ+23b]."
        },
        {
            "title": "4.1 Main Results",
            "content": "Tables 1 and 2 present the performance of the o1-preview model and baseline models across the benchmarks. The results show that o1-preview achieves impressive performance in many tasks, surpassing the baseline GPT-4 models. Notably, the o1-preview numbers involve simple 0-shot prompting, in contrast to more sophisticated strategies like Medprompt. While o1-previews performance is impressive, it does not always outperform GPT-4 models. For example, MMLU Clinical Knowledge yield better results with Medprompt-enhanced GPT-4. OpenAIs report corroborates this observation, noting that the effectiveness of o1-preview depends on the specific task setup [Ope24b]. These findings suggest that prompting techniques are less necessary for the o1 model, but they remain valuable tools for traditional GPT-4 models in achieving high performance on specialized tasks. Table 1: Performance of different models on suite of medical benchmarks. Dataset GPT-4* GPT-4-Turbo** GPT-4o GPT-4 Turbo** o1-preview (0 shot) (Medprompt) (0 shot) (0 shot) (5 shot) MedQA (US 4-option) 78.9% JMLEMedMCQA Dev MMLU Clinical Knowledge Medical Genetics Anatomy Professional Medicine College Biology College Medicine - 69.5% 86.0% 91.0% 80.0% 93.0% 95.1% 76.9% 81.4% 87.3% 72.4% 86.4% 92.0% 80.0% 93.8% 95.1% 76.9% 84.4% 92.7% 76.4% 89.1% 96.0% 88.2% 97.4% 95.1% 85.6% 90.2% 92.7% 79.1% 95.8% 98.0% 89.6% 95.2% 97.9% 89.0% 96.0% 98.2% 83.9% 93.6% 99.0% 93.3% 97.0% 98.6% 90.2% * Results originally reported in [NKM+23] on the initial GPT-4 model release. ** Experimental results originally reported in [NLZ+23b], except JMLE-2024. 7 Table 2: Comparative analysis of performance of different models on USMLE Sample Exam and USMLE Self Assessment. Dataset USMLE Sample Exam Step 1 Step 2 Step 3 USMLE Self Assessment Step 1 Step 2 Step 3 GPT-4* GPT-4o (0 shot) (0 shot) o1-preview (0 shot) 80.7% 81.7% 89.8% 89.8% 89.1% 93.1% 87.5% 83.5% 91.8% 84.8% 81.3% 80.6% 91.6% 92.5% 96.4% 92.4% 93.4% 80.7% * Results reported originally [NKM+23] on the initial GPT-4 model release."
        },
        {
            "title": "4.2 Performance on Newly Curated Multilingual Benchmark: JMLE-2024",
            "content": "As with any study of public benchmarks on LLMs that consume massive amounts of training data, the question of test-set contamination and its influence on measured capabilities remains. For this study, we prepared new benchmark, JMLE-2024, derived from the national medical licensing exam held in Japan in February 2024 1. This exam was administered after the knowledge cutoff date for the o1 model. Figure 4: JMLE-2024: National medical licensing exam held in Japan in February 2024 1https://www.mhlw.go.jp/seisakunitsuite/bunya/kenkou_iryou/iryou/topics/tp240424-01.html 8 To construct the dataset, we filtered out questions containing images or those with multiple correct answers, retaining only five-choice questions with single correct answer. After applying these criteria, the final dataset consisted of 275 questions. The questions spanned broad range of medical topics, from fundamental clinical knowledge to complex case-based scenarios. The passing score for test takers on this exam is typically around 80%. Although the Japanese medical licensing exam includes forbidden options, where selecting four or more such options results in automatic failure, we did not incorporate this criterion in our analysis. We further divided the dataset into short and long question sets. Long questions typically include patient profiles and pose more intricate clinical problems. The results are presented in Figure 4. Consistent with other medical benchmarks, o1-preview demonstrates remarkable performance. Interestingly, o1-preview performed better on the long question set, further increasing the gap compared to the other baselines. This finding suggests that o1-preview can leverage its reasoning capabilities more effectively when answering more challenging problems. Notably, we did not translate the questions to English; instead, we directly used Japanese. Further performance gains were achieved using run-time steering techniques, such as Medprompt, along with ensemble approaches, which boosted the results for both GPT-4o and the o1-preview model. While further evaluation is required, these results suggest: (1) o1-previews strong performance is not merely based on memorization of well-established benchmarks, as the exam was made public after the models knowledge cutoff date, and (2) o1-preview demonstrates remarkable proficiency in answering non-English medical questions. 4.3 Impact of Prompting Techniques on o1-Preview Performance We now explore how different prompting strategies influence the performance of the o1-preview model. Our goal is to determine whether more advanced prompting methods provide additional benefits compared to simpler approaches with this new class of model. Since the o1-preview model generates non-deterministic outputs by design (e.g., with fixed sampling temperature of 1.0) [Ope24c], we conducted three independent runs for each evaluation to minimize the impact of stochastic variation. We report both the mean performance and standard deviation across these runs to ensure robustness of the results. Figure 5: Comparison of prompting techniques on MedQA with the o1-preview model. Error bars indicate one standard deviation from three independent samples. Figure 5 presents the results on the MedQA dataset, including error bars to illustrate performance variability. Notably, five-shot prompting resulted in significant decrease in performance, suggesting that providing multiple similar examples in the prompt may have confused the model. This observation aligns with the OpenAI official guide, which notes that excessive retrieved context may impair performance [Ope24c]. In Figure 6, we individually test the efficacy of Medprompts core components across the benchmark datasets. As seen with MedQA, tailored prompting has small positive effect on performance. Fewshot prompting was inconsistent and, on average, hurt performance. Ensembling outputs across multiple runs, however, provided consistent and meaningful boost across every benchmark task. We employed the same majority vote ensemble technique as [NLZ+23b], shuffling answer choices across each ensemble run to further increase diversity across the reasoning chains. As shown in the Medprompt and SelfConsistency prompting strategies [NLZ+23b, WWS+22a], marginalizing over several diverse reasoning chains continues to be an effective variance reduction technique. 10 Figure 6: Tests of different prompting strategies across benchmark datasets. Ensembling outputs consistently yielded strong boosts in performance. Writing custom-tailored prompt that describes the task in greater detail (Figures 12,13) had marginal positive effect. Use of few-shot prompting was generally neutral or negative. Detailed results from all experiments are presented in Section 8.1."
        },
        {
            "title": "4.4 Role and Performance of Reasoning Tokens",
            "content": "input tokens, repreThe token usage of the o1-preview model is decomposed into three components: senting the tokens used in the initial prompt; reasoning tokens, which are generated during intermediate reasoning steps; and output tokens, corresponding to the final response. We experimented with an alternative prompt format, instructing the model to provide only the final answer without accompanying explanations or rationales. This setup takes advantage of o1-previews reasoning tokens. Even when instructed to output only the final answer, the o1-preview model can freely reason using its internal reasoning tokens. The results are presented in Figure 7. OpenAI has suggested that leveraging higher numbers of reasoning tokens typically leads to higher performance [Ope24b]. We corroborate this guidance with our experiments; we find that we can elicit significantly more reasoning tokens from the model purely via prompting, and that performance improves meaningfully when the model is explicitly told to spend longer amounts of time reasoning. 11 Figure 7: Effect of two prompting strategies which elicit variable length reasoning chains across benchmark datasets. Model accuracy tends to trend upwards when more reasoning tokens are used. Reasoning token count is returned from the OpenAI API. Exact prompt specifications are displayed in Figures 10 and 11."
        },
        {
            "title": "4.5 Accuracy and Cost Frontiers",
            "content": "We now explore tradeoffs between accuracy and cost using the MedQA benchmark. Evaluating the performance of LLMs involves balancing accuracy against cost. Techniques for run-time steering, such as Medprompt, can enhance output quality. However, they increase token consumption and, thus, raise inference costs. For instance, while the o1-preview model achieves excellent performance, it comes with higher operational expenses. Figure 1 summarizes the total cost of processing 1,273 MedQA questions. Table 3 outlines the API pricing used in our analysis. Notably, o1-preview delivers superior accuracy at lower cost compared to Medprompt with GPT-4 Turbo (November 2023). We note that GPT-4 Turbo (November 2023) powered the original Medprompt implementation [NLZ+23b] and was identified as state-of-the-art solution at that time. Interestingly, o1-preview with minimal or few-shot prompting employs more tokens but performs worse than with tailored prompts, reinforcing the importance of prompt optimization. Additional ensembling further boosts the accuracy of o1-preview but this strategy also increases costs. The latest model in the GPT-4 series, GPT-4o (August 2024), introduces new balance between cost and accuracy. Compared to GPT-4 Turbo, GPT-4o consistently offers both improved accuracy and lower costs across all prompting strategies. As illustrated in Figure 1, GPT-4o significantly shifts the 12 cost-accuracy Pareto frontier, outperforming GPT-4 Turbo on both metrics. While o1-preview excels in terms of absolute accuracy, GPT-4o offers well-balanced solution with strong performance at lower cost. For example, increasing from 88% 92% 96% accuracy on the MedQA dataset each requires an order of magnitude increase in price ($5 $50 $500) with the GPT-4o Few-shot, GPT-4o Medprompt, and o1-preview (5x ensemble) strategies respectively. Depending on the specific use case and overall system setup, it is crucial to carefully consider cost-benefit tradeoffs and to optimize resources according to goals and preferences. Model Cost ($) per 1M Tokens Input Tokens Reasoning Tokens Output Tokens o1-preview (Sep 2024) GPT-4o (Aug 2024) GPT-4 Turbo (Nov 2023) 15 2.5 10 60 - - 60 10 30 Table 3: API costs per 1M tokens used in the cost analysis. Cost is based on latest prices for generating tokens as of Oct 2024 for all models under consideration. Reasoning tokens are new concept introduced in the o1 line of models. They are priced identically to output tokens."
        },
        {
            "title": "5 Directions with LLM Run-Time Strategies",
            "content": "Scaling laws [KMH+20] for large language models (LLMs) have proven to be valuable for predicting how model capabilities grow with increasing data, compute, and model size. However, inference-time scalingwhich focuses on the value of investing in additional computation at model run-timeis promising new area of study. We see opportunities for innovating on how best to guide such run-time allocations for advancing efficiency, accuracy, and reasoning abilities. We review in this section promising directions in this emerging area of research and development, partly framed by our experimental results and prior research in AI on reasoning, ensembling, and control of the nature and extent of inference."
        },
        {
            "title": "5.1 Metareasoning principles and machinery",
            "content": "Prior research in AI on metareasoningand in cognitive science on metacognitioncan provide valuable insights into controlling and optimizing real-time deliberation over multiple generative threads in large language models (LLMs). High-level metareasoning methods can facilitate runtime decision making by dynamically allocating computing resources across different generative processes and their combinations. Specifically, these methods can incorporate considerations such as formulating and testing chains of thought, assessing confidence levels, estimating success likelihoods, and implementing control strategies to guide lower-level token-generation processes. Metareasoning methods drawn from the AI literature can be adapted for control and optimization to guiding run-time streams of token generation or can alternatively inspire more deeply woven, implicit representations that are learned from training data about problem solving. Metareasoning principles and mechanisms include methods to guide reasoning, execute portfolio strategies, and dynamically balance computational accuracy and cost. More broadly, context-sensitive control over cognitive resource 13 allocation has been proposed as unifying perspective on intelligence, bridging neuroscience, psychology, and AI research [GHT15]. Relevant prior work includes optimizing the value of inference under varying constraints and uncertainties in allocated computing resources [Hor87, Hor88] and use of estimates of solution likelihood or expected value of computation to determine when to continue, switch, or halt reasoning threads [HCH89, Hor01, HRG+01]. Other research has investigated models for ideal context-sensitive partition of time or other problem-solving resources to solution planning versus execution [HB21]. Such opportunities for performing ideal metalevel partition can be valuable for building ideal run-time inference systems that maximize the efficiency or likelihood of success of base-level generation processes in LLMs."
        },
        {
            "title": "5.2 Optimizing Input for LLMs",
            "content": "5.2.1 In-Context Learning The in-context learning paradigm, introduced by [BMR+20], showed that extending model prompts with several demonstration examplescommonly referred to as few-shot promptingsignificantly improves task accuracy across wide range of challenges. Advancements in hardware and algorithms are enabling long-context modeling, but fully harnessing the potential of extended contexts remains key challenge. [ASZ+24] explored the paradigm at scale, showing that language models continue to benefit from thousands of demonstration examples. OpenAIs guidelines suggest that few-shot prompting or incorporating additional retrieved context may not always improve performance and, in some cases, might degrade it [Ope24c]. Determining how to efficiently provide relevant examples and additional context to optimize performanceespecially in models such as o1-previewremains promising area of research. 5.2.2 Integrating External Resources at Runtime The progression of LLMs, such as the o1-preview model, has markedly reduced the dependence on intricate prompt engineering by baking reasoning behavior into the models standard mode of operation. However, an essential avenue for further enhancing these models lies in their ability to actively acquire information at run-time from external sources such as the web and knowledge bases (KBs) [LPP+20]. The integration of active information acquisition has potential in several aspects. By leveraging external databases, models can effectively bypass the limitations of their fixed training data, accessing virtually unlimited pool of information. Additionally, models can also remain upto-date without the need for frequent retraining, as they can incorporate the latest information from external sources, which is particularly beneficial in rapidly evolving fields like medicine or technology. There is opportunity for run-time inference to compute estimations of the expected value of information for different types of information. Allowing models to leverage tools, typically in the form of software libraries and APIs, is another form of effective inference time compute scaling [SDYD+23, BCE+23, PZWG23]."
        },
        {
            "title": "5.3 Guiding LLM Inference and Sampling",
            "content": "New tools are emerging to enable finer-grained control and scaling of language model inference. Innovations in sampling methods, like entropy based sampling techniques [xa23], show promise in better leveraging the inherent calibrated uncertainty model assigns to each token as it generates text [Ope23, NKM+23]. Software tooling like Guidance [LNRT24] and Outlines [WL23] allow for model developers to dynamically manipulate per-token probabilities, leading to higher quality outputs when 14 language model inference is tightly coupled with external systems [HGP+22]. While these tools currently operate without tight coupling to model, incorporating this style of token steering mechanisms directly into model training may unlock further capabilities."
        },
        {
            "title": "5.4 Reasoning",
            "content": "Chain-of-Thought prompting, introduced by [KGR+22, WWS+22b], promotes step-by-step, auto-regressive token generation. The method has been demonstrated to significantly improve performance on tasks that appear to benefit from more complex steps of reasoning. The CoT technique encourages LLMs to break down their thought processes, improving the accuracy of intermediate steps, which in turn boosts final outcomes. Remarkably, [PMB24] found that this improvement arises even when models produce meaningless intermediate tokens, suggesting that simply expending computational resources can enhance the reasoning performance of transformer-based models. Recent research demonstrates that advanced prompting compositions further refine reasoning abilities. Methods such as ReAct [YZY+22], skeleton-based prompting [NLZ+23a], and tree-based reasoning [YYZ+24] allow for more structured problem-solving, improving outcomes in multi-step tasks. Training LLMs to improve real-time reasoning capabilities remains highly active research area. The Self-Taught Reasoner (STaR) framework, introduced by [ZWMG22, ZHS+24], exemplifies this trend. STaR iteratively refines the models thought process by generating multiple reasoning paths (rationales), filtering them for correctness, and fine-tuning the model using successful outcomes. This self-improvement cycle allows models to enhance their reasoning abilities progressively. Another notable framework, Lets Verify Step-by-Step [PMB24], introduces process supervision to improve mathematical reasoning. By focusing on step-by-step validation rather than just output correctness, it improves the reliability of reward models, fostering better outcomes on tasks with high logical complexity. growing trend emphasizes scaling test-time computation over merely increasing model size or pre-training compute. Research by [SLXK24, WSL+24, WLY+24] highlights that leveraging iterative reasoning strategies at inference time can significantly enhance performance without the need for larger models. OpenAIs latest model, o1, exemplifies this trend by employing reinforcement learning (RL) to improve reasoning capabilities. Trained to think before responding, o1 delivers exceptional performance on reasoning-intensive tasks."
        },
        {
            "title": "5.5 Leveraging Multiple Runs and Models",
            "content": "5.5.1 Ensembling Ensembling is powerful technique in machine learning that enhances model performance by combining multiple models or their outputs [CNMCK04]. It has also demonstrated promising results for large language models (LLMs) [WWS+22a, NLZ+23b]. While simple majority voting is popular approach for aggregating outputs, more sophisticated methods are emerging. One such method is Ensemble Refinement, introduced with Med-PaLM 2 [STG+23]. This technique employs multi-stage aggregation by generating multiple reasoning paths through stochastic sampling. Rather than relying solely on majority voting, the model iteratively re-conditions on intermediate outputs, refining its reasoning at each stage to achieve higher precision. Another approach, LLM-Blender [JRL23], incorporates two key components: ranker and fuser. The ranker performs pairwise evaluations of outputs from multiple models, identifying the most promis15 ing candidates. The fuser then merges these candidates into coherent response, leveraging the strengths of each for an optimal outcome. However, major challenge with ensembling is the computational cost. One direction addressing this issue is dynamic self-consistency [WWCL24, WFL+24], which optimizes the sampling process by stopping early when sufficient consistency is detected among reasoning paths. Future research is needed for developing adaptive ensembling strategies and further optimizing sampling methods to maximize performance while maintaining computational efficiency. 5.5.2 Model Federation and Multi-Agent Architectures Recent research has investigated the use of agent frameworks and multi-agent orchestration, wherein models are equipped with access to various tools and collaborate to achieve state-of-the-art performance on complex, real-world tasks. These frameworks enable models to dynamically select and integrate the tools required to solve given problem, thereby distributing reasoning and computation across specialized agents. [WBZ+23] presents an example of this orchestration, wherein models collaborate to leverage their respective strengths to accomplish intricate tasks. These multi-agent frameworks have yielded breakthrough performance on real-world tasks, beyond what could be elicited from single model call alone [JYW+23]."
        },
        {
            "title": "6 Limitations",
            "content": "We evaluated o1-preview on set of medical benchmarks. We found that o1-preview significantly outperforms GPT-4 guided by Medprompt, set of advanced dynamic prompting strategies that had previously achieved state-of-the-art performance on the medical benchmarks at focus. We note that this study is preliminary and limited in scope, and more extensive evaluations are necessary to fully understand o1-previews capabilities. While the initial results are impressive, several challenges and opportunities for growth remain that warrant further discussions. Benchmark Saturation. One significant issue is the rapid saturation of existing benchmarks, limIn the MedGemini paper [STW+24], Google iting their utility in evaluating state-of-the-art models. researchers analyzed the MedQA dataset and found that small but notable portion of the questions had labeling errors, missing information, or ambiguities. The effective performance of o1-preview on MedQA might have reached this datasets noise ceiling. We anticipate similar issues in other benchmarks. We need to develop new benchmarks that are more challenging and relevant to real-world medical challenges. o1-preview API Limitations. Another area for consideration involves the limitations of the o1preview API [Ope24c]. While o1 exhibits strong reasoning abilities by thinking before outputting responses, its internal planning and chain-of-thought processes are hidden, so we cannot report on the nature of these tokens. In addition, the o1 API currently exposes fewer sampling parameters (such as temperature, Top P) or custom system prompts, so we cannot yet explore these variables. Best Practices with o1. Optimizing interactions with o1 remains an ongoing challenge. Our experimentation with various prompting techniquessome of which had previously yielded significant improvements with GPT-4 revealed mixed results. Certain techniques provided no noticeable gains or, in some cases, even degraded performance when applied to o1-preview. Given that o1-preview is 16 still relatively new model, this opens up valuable opportunity for further exploration. Identifying optimal interaction strategies represents an exciting area for research, as refining these best practices will be instrumental in helping users unlock the models full potential. Ethical Use. Finally, the rapid development of models like o1-preview raises important questions It is crucial to not only develop advanced about safety, ethical considerations, and responsible use. capabilities but also ensure they are deployed in manner that is safe, transparent, and beneficial for end users. Addressing these aspects will be essential as we move forward with refining and applying the o1 family of models in the medical domain."
        },
        {
            "title": "7 Conclusions",
            "content": "We expect the landscape of run-time strategies for LLMs to evolve rapidly. We systematically evaluated OpenAIs o1-preview model on set of medical benchmarks. Remarkably, even without advanced prompting techniques, o1-preview outperformed GPT-4 with Medprompt. Medprompt was initially designed to guide generalist models like GPT-4 in specialized domains using methods such as selfgenerated CoT and few-shot prompting. However, our results indicate that the o1-preview model reduces reliance on some of these techniques. In some cases, few-shot prompting even hindered o1-previews performance, suggesting that in-context learning is not an effective way to steer these models. While ensembling remains viable approach, it is resource-intensive and requires careful optimization to balance costs and performance gains. Our cost and accuracy analysis of various run-time strategies revealed new Pareto frontier, illustrating the trade-offs between cost-efficiency and accuracy. Although GPT-4o with steering strategies like Medprompt retains value in specific contexts depending on task requirements, optimizing run-time strategies for advanced models like o1-preview will require nuanced balance between control, performance, and resource allocation. Moreover, our findings highlight that the o1-preview model has achieved near-saturation on many existing medical benchmarks. This study primarily utilized multiple-choice questions to assess knowledge and reasoning, which, despite their utility, have inherent limitations. Consequently, there is an urgent need for new, more challenging benchmarks to accurately evaluate these advanced models. Further evaluation of the o1-preview model in real-world applications and across diverse aspects of medical understanding will be essential."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Rich Caruana, Hoifung Poon, and Paul Vozila for insightful discussions and feedback."
        },
        {
            "title": "References",
            "content": "[ASZ+24] [BCE+23] Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John Co-Reyes, Eric Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018, 2024. Sebastien Bubeck, Varun Chandrasekaran, Ronen Eldan, Johannes Gehrke, Eric Horvitz, Ece Kamar, Peter Lee, Yin Tat Lee, Yuanzhi Li, Scott Lundberg, et al. Sparks of artificial general intelligence: Early experiments with gpt-4. arXiv preprint arXiv:2303.12712, 2023. [Blu84] Niels Blunch. Position bias in multiple-choice questions. Journal of Marketing Research, 21(2):216220, 1984. [BMR+20] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. [BVY+24] Elliot Bolton, Abhinav Venigalla, Michihiro Yasunaga, David Hall, Betty Xiong, Tony Lee, Roxana Daneshjou, Jonathan Frankle, Percy Liang, Michael Carbin, et al. Biomedlm: 2.7 parameter language model trained on biomedical text. arXiv preprint arXiv:2403.18421, 2024. [CNMCK04] Rich Caruana, Alexandru Niculescu-Mizil, Geoff Crew, and Alex Ksikes. Ensemble selection from libraries of models. In Proceedings of the twenty-first international conference on Machine learning, page 18, 2004. [GHT15] Samuel Gershman, Eric Horvitz, and Joshua Tenenbaum. Computational rationality: converging paradigm for intelligence in brains, minds, and machines. Science, 349(6245), 2015. [GTC+21] Yu Gu, Robert Tinn, Hao Cheng, Michael Lucas, Naoto Usuyama, Xiaodong Liu, Tristan Naumann, Jianfeng Gao, and Hoifung Poon. Domain-specific language model pretraining for biomedical natural language processing. ACM Trans. Comput. Healthcare, 3(1), oct 2021. [HB21] Eric Horvitz and John Breese. Ideal partition of resources for metareasoning. arXiv preprint arXiv:2110.09624, 2021. [HBB+20] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [HCH89] Eric J. Horvitz, Gregory F. Cooper, and David E. Heckerman. Reflection and action under scarce resources: Theoretical principles and empirical study. In Proceedings of the 11th International Joint Conference on Artificial Intelligence - Volume 2, IJCAI89, page 11211127, San Francisco, CA, USA, 1989. Morgan Kaufmann Publishers Inc. 18 [HGP+22] Thomas Hartvigsen, Saadia Gabriel, Hamid Palangi, Maarten Sap, Dipankar Ray, and Ece Kamar. Toxigen: large-scale machine-generated dataset for adversarial and implicit hate speech detection. arXiv preprint arXiv:2203.09509, 2022. [Hor87] [Hor88] [Hor01] [HRG+01] [JPO+21] [JRL23] Eric J. Horvitz. Reasoning about beliefs and actions under computational resource constraints. In Proceedings of the Third Conference on Uncertainty in Artificial Intelligence, UAI87, page 429447. AUAI Press, 1987. Eric Horvitz. Reasoning under varying and uncertain resource constraints. volume 88, pages 111116. Citeseer, 1988. In AAAI, Eric Horvitz. Principles and applications of continual computation. Artificial Intelligence, 126(1-2):159196, 2001. Eric Horvitz, Yongshao Ruan, Carla Gomes, Henry Kautz, Bart Selman, and Max Chickering. Bayesian approach to tackling hard computational problems. In Proceedings of the Seventeenth Conference on Uncertainty in Artificial Intelligence, pages 235244, July 2001. Di Jin, Eileen Pan, Nassim Oufattole, Wei-Hung Weng, Hanyi Fang, and Peter Szolovits. What disease does this patient have? large-scale open domain question answering dataset from medical exams. Applied Sciences, 11(14):6421, 2021. Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llm-blender: Ensembling large language models with pairwise ranking and generative fusion. arXiv preprint arXiv:2306.02561, 2023. [JYW+23] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [KCM+23] Tiffany Kung, Morgan Cheatham, Arielle Medenilla, Czarina Sillos, Lorie De Leon, Camille Elepano, Maria Madriaga, Rimel Aggabao, Giezel Diaz-Candido, James Maningo, et al. Performance of chatgpt on usmle: potential for ai-assisted medical education using large language models. PLoS digital health, 2(2):e0000198, 2023. [KGR+22] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. Advances in neural information processing systems, 35:2219922213, 2022. [KKS+23] Jungo Kasai, Yuhei Kasai, Keisuke Sakaguchi, Yutaro Yamada, and Dragomir Radev. Evaluating gpt-4 and chatgpt on japanese medical licensing examinations. arXiv preprint arXiv:2303.18027, 2023. [KLK+20] Miyoung Ko, Jinhyuk Lee, Hyunjae Kim, Gangwoo Kim, and Jaewoo Kang. Look at the first sentence: Position bias in question answering. arXiv preprint arXiv:2004.14602, 2020. [KMH+20] Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. 19 [LNRT24] Scott Lundberg, Harsha Nori, Marco Tulio Ribeiro, and Guidance AI Team. Guidance: guidance language for controlling large language models, 2024. GitHub repository. [LPP+20] [LSX+22] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Kuttler, Mike Lewis, Wen-tau Yih, Tim Rocktaschel, et al. Retrievalaugmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474, 2020. Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and TieYan Liu. BioGPT: generative pre-trained transformer for biomedical text generation and mining. Briefings in Bioinformatics, 23(6), 09 2022. bbac409. [NKM+23] Harsha Nori, Nicholas King, Scott Mayer McKinney, Dean Carignan, and Eric Horvitz. Capabilities of GPT-4 on medical challenge problems. arXiv preprint arXiv:2303.13375, 2023. [NLZ+23a] Xuefei Ning, Zinan Lin, Zixuan Zhou, Zifu Wang, Huazhong Yang, and Yu Wang. Skeletonof-thought: Large language models can do parallel decoding. Proceedings ENLSP-III, 2023. [NLZ+23b] Harsha Nori, Yin Tat Lee, Sheng Zhang, Dean Carignan, Richard Edgar, Nicolo Fusi, Nicholas King, Jonathan Larson, Yuanzhi Li, Weishung Liu, Renqian Luo, Scott Mayer McKinney, Robert Osazuwa Ness, Hoifung Poon, Tao Qin, Naoto Usuyama, Chris White, and Eric Horvitz. Can generalist foundation models outcompete special-purpose tuning? Case study in medicine. arXiv preprint arXiv:2311.16452, 2023. [Ope23] OpenAI. Gpt-4 technical report, 2023. [Ope24a] [Ope24b] [Ope24c] [Ope24d] [PMB24] [PUS22] Advice OpenAI. examples. https://platform.openai.com/docs/guides/reasoning/advice-on-prompting? reasoning-prompt-examples=coding-planning, 2024. Accessed: 2024-10-20. on prompting and planning reasoning, coding, - OpenAI. Learning to reason with large language models. https://openai.com/index/ learning-to-reason-with-llms/, 2024. Accessed: 2024-10-20. OpenAI. Openai api guide - reasoning. https://platform.openai.com/docs/guides/ reasoning, September 2024. Accessed: 2024-10-20. OpenAI. Openai o1 system card. https://cdn.openai.com/o1-system-card-20240917. pdf, September 2024. Accessed: 2024-10-20. Jacob Pfau, William Merrill, and Samuel Bowman. Lets think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758, 2024. Ankit Pal, Logesh Kumar Umapathi, and Malaikannan Sankarasubbu. Medmcqa: largescale multi-subject multi-choice dataset for medical domain question answering. In Conference on Health, Inference, and Learning, pages 248260. PMLR, 2022. [PZWG23] Shishir Patil, Tianjun Zhang, Xin Wang, and Joseph Gonzalez. Gorilla: Large language model connected with massive apis. arXiv preprint arXiv:2305.15334, 2023. 20 [SAT+22] Karan Singhal, Shekoofeh Azizi, Tao Tu, Sara Mahdavi, Jason Wei, Hyung Won Chung, Nathan Scales, Ajay Tanwani, Heather Cole-Lewis, Stephen Pfohl, et al. Large language models encode clinical knowledge. arXiv preprint arXiv:2212.13138, 2022. [SDYD+23] Timo Schick, Jane Dwivedi-Yu, Roberto Dess`, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36, 2023. [SLXK24] [STG+23] Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Karan Singhal, Tao Tu, Juraj Gottweis, Rory Sayres, Ellery Wulczyn, Le Hou, Kevin Clark, Stephen Pfohl, Heather Cole-Lewis, Darlene Neal, et al. Towards expert-level medical question answering with large language models. arXiv preprint arXiv:2305.09617, 2023. [STW+24] Khaled Saab, Tao Tu, Wei-Hung Weng, Ryutaro Tanno, David Stutz, Ellery Wulczyn, Fan Zhang, Tim Strother, Chunjong Park, Elahe Vedadi, et al. Capabilities of gemini models in medicine. arXiv preprint arXiv:2404.18416, 2024. [WBZ+23] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Shaokun Zhang, Erkang Zhu, Beibin Li, Li Jiang, Xiaoyun Zhang, and Chi Wang. Autogen: Enabling next-gen llm applications via multi-agent conversation framework. arXiv preprint arXiv:2308.08155, 2023. [WFL+24] Xinglin Wang, Shaoxiong Feng, Yiwei Li, Peiwen Yuan, Yueqi Zhang, Boyuan Pan, Heda Wang, Yao Hu, and Kan Li. Make every penny count: Difficulty-adaptive self-consistency for cost-efficient reasoning. arXiv preprint arXiv:2408.13457, 2024. [WL23] Brandon Willard and Remi Louf. Efficient guided generation for llms. arXiv preprint arXiv:2307.09702, 2023. [WLY+24] Tianhao Wu, Janice Lan, Weizhe Yuan, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Thinking llms: General instruction following with thought generation. arXiv preprint arXiv:2410.10630, 2024. [WSL+24] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. An empirical analysis of compute-optimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024. [WWCL24] Guangya Wan, Yuqi Wu, Jie Chen, and Sheng Li. Dynamic self-consistency: Leveraging reasoning paths for efficient llm sampling. arXiv preprint arXiv:2408.17017, 2024. [WWS+22a] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. [WWS+22b] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. 21 [xa23] xjdr alt. Entropix: fast entropy-based data pruning tool, 2023. Accessed: 2024-10-21. [YYZ+24] [YZY+22] [ZCS+23] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Tom Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. Shunyu Yao, Jeffrey Zhao, Dian Yu, Nan Du, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. React: Synergizing reasoning and acting in language models. arXiv preprint arXiv:2210.03629, 2022. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mtbench and chatbot arena. Advances in Neural Information Processing Systems, 36:46595 46623, 2023. [ZHS+24] Eric Zelikman, Georges Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. Quiet-star: Language models can teach themselves to think before speaking. arXiv preprint arXiv:2403.09629, 2024. [ZWMG22] Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah Goodman. Star: Bootstrapping reasoning with reasoning. Advances in Neural Information Processing Systems, 35:1547615488, 2022."
        },
        {
            "title": "8.1 Full Experimental Results",
            "content": "Figure 8: Heatmap showing absolute accuracy and relative performance over baseline zeroshot prompt (in parenthesis) across all benchmark datasets. As shown in Figure 6, ensembling strategies consistently helped performance, while few-shot produced mixed results."
        },
        {
            "title": "8.2 Prompt Templates",
            "content": "Few-Shot The following are multiple choice questions (with answers) about medical knowledge. {examples} **Question**: {question} {answer choices} **Answer**: ( Figure 9: Template used for zero-shot and five-shot prompts. Quick Response Please answer the following question as quickly as possible. We have narrowed down the possibilities to four different answers. am in an emergency, and speed is of utmost importance. It is more important to answer quickly than it is to analyze too carefully. Return just the answer as quickly as possible. ------ # QUESTION {question} # ANSWER CHOICES {answer choices} ------ Please remember to answer quickly and succinctly. Time is of the essence! Figure 10: Prompt that elicits the model to respond with less reasoning and completion tokens. Extended Reasoning Please answer the following multiple choice question. Take your time and think as carefully and methodically about the problem as you need to. am not in rush for the best answer; would like you to spend as much time as you need studying the problem. When youre done, return only the answer. ------ # QUESTION {question} # ANSWER CHOICES {answer choices} ------ Remember, think carefully and deliberately about the problem. Take as much time as you need. will be very sad if you answer quickly and get it wrong. Figure 11: Prompt that elicits the model to respond with more reasoning and completion tokens. 24 Tailored - Medical Questions You are tasked with solving complex medical questions that assess both the knowledge and clinical reasoning required for medical licensing exam. These questions cover critical topics such as anatomy, physiology, pathology, pharmacology, and patient management. Read the following question carefully and select the most accurate answer from the provided options. **Question**: {question} **Options**: {answer choices} **Instructions**: - Think deeply and thoroughly, then choose the best possible answer from the given options (only one choice). - Your final response must contain only the letter corresponding to the correct answer (e.g., A). Do not include explanations or additional text in your output. **Answer**: Figure 12: Prompt for medical questions requiring knowledge and clinical reasoning. Tailored - MMLU You are tasked with answering academic questions from various subjects, including medicine, clinical knowledge, biology, anatomy, and more. Carefully read the following question and select the most accurate answer from the provided options. **Question**: {question} **Options**: {answer choices} **Instructions**: - Think deeply and thoroughly, then choose the best possible answer from the given options (only one choice). - Your final response must contain only the letter corresponding to the correct answer (e.g., A). Do not include explanations or additional text in your output. **Answer**: Figure 13: Prompt for MMLU dataset questions requiring accurate academic responses across multiple subjects."
        }
    ],
    "affiliations": []
}