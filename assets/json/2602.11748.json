{
    "paper_title": "Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning",
    "authors": [
        "Futing Wang",
        "Jianhao Yan",
        "Yun Luo",
        "Ganqu Cui",
        "Zhi Wang",
        "Xiaoye Qu",
        "Yue Zhang",
        "Yu Cheng",
        "Tao Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Achieving effective test-time scaling requires models to engage in In-Context Exploration -- the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within a single continuous context. Grounded in State Coverage theory, our analysis identifies a critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, a phenomenon we term the ``Shallow Exploration Trap''. To bridge this gap, we propose Length-Incentivized Exploration(\\method). This simple yet effective recipe explicitly encourages models to explore more via a length-based reward coupled with a redundancy penalty, thereby maximizing state coverage in two-step manner. Comprehensive experiments across different models (Qwen3, Llama) demonstrate that \\method effectively incentivize in-context exploration. As a result, our method achieves an average improvement of 4.4\\% on in-domain tasks and a 2.7\\% gain on out-of-domain benchmarks."
        },
        {
            "title": "Start",
            "content": "Think Longer to Explore Deeper: Learn to Explore In-Context via Length-Incentivized Reinforcement Learning Futing Wang * 1 2 3 Jianhao Yan * 1 2 3 Yun Luo 3 Ganqu Cui 3 Zhi Wang 4 3 Xiaoye Qu 3 Yue Zhang 2 5 Yu Cheng 6 Tao Lin 2 6 2 0 2 2 1 ] . [ 1 8 4 7 1 1 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Achieving effective test-time scaling requires models to engage in In-Context Exploration the intrinsic ability to generate, verify, and refine multiple reasoning hypotheses within single continuous context. Grounded in State Coverage theory, our analysis identifies critical bottleneck to enabling this capability: while broader state coverage requires longer reasoning trajectories, the probability of sampling such sequences decays exponentially during autoregressive generation, phenomenon we term the Shallow Exploration Trap. To bridge this gap, we propose LengthIncentivized Exploration(LIE). This simple yet effective recipe explicitly encourages models to explore more via length-based reward coupled with redundancy penalty, thereby maximizing state coverage in two-step manner. Comprehensive experiments across different models (Qwen3, Llama) demonstrate that LIE effectively incentivize in-context exploration. As result, our method achieves an average improvement of 4.4% on in-domain tasks and 2.7% gain on out-ofdomain benchmarks. Our code is publicly available in https://github.com/LINs-lab /LIE. 1. Introduction Scaling test-time computation, often conceptualized as enabling models to think harder before answering, has emerged as powerful paradigm for breaking the performance ceiling of Large Language Models (LLMs) (Snell et al., 2024; Wu et al., 2024; Liu et al., 2025a). Broadly, 1Zhejiang University 2Westlake University 3Shanghai AI Laboratory 4Nanjing University 5Institute of Advanced Technology, Westlake Institute for Advanced Study 6The Chinese University of Hong Kong. Correspondence to: Yun Luo <luoyun1@pjlab.org.cn>, Yu Cheng <chengyu@cse.cuhk.edu.hk>, Tao Lin <lintao@westlake.edu.cn>. Preprint. February 13, 2026. Figure 1. The difference between In-Context Exploration and Training Exploration. Our framework distinguishes between the exploration of the training process and in-context inference. In the training phase, reinforcement learning incentivizes the model to explore and learn from diverse state distributions. In contrast, during test-time inference, in-context exploration empowers the model to actively traverse and navigate states. Test-Time Scaling (TTS) strategies fall into two primary regimes: Parallel Scaling (Lightman et al., 2023; Snell et al., 2024; Liu et al., 2025a; Brown et al., 2024; Wang et al., 2022), which aggregates outputs from multiple independent samples, and Sequential Scaling (Guo et al., 2025; Jaech et al., 2024; Kumar et al., 2024), which prioritizes extended reasoning chains or iterative refinement. While Parallel Scaling structures the search space externally, Sequential Scaling via Long CoT represents an intrinsic dimension of TTS, as it directly harnesses the models intrinsic reasoning capabilities (Snell et al., 2024). One iconic capability among this intrinsic reasoning is incontext exploration (Setlur et al., 2025), which is achieved through the generation, verification, and refinement of multiple hypotheses within continuous context (Gandhi et al., 2024; Setlur et al., 2025). Consequently, fostering such in-context exploration serves as the critical catalyst for unlocking the full potential of sequential scaling. As illustrated in Figure 1, in-context exploration enables the model to sequentially traverse diverse reasoning states to find the correct answer. However, how to effectively train LLMs to Learn to Explore In-Context via Length-Incentivized Reinforcement Learning acquire in-context exploration remains underexplored. 2. Related Work To study this problem, we first analyze the bottleneck of in-context exploration through the lens of Count-Based Exploration theory in RL training (Auer, 2002), which establishes state coverage as fundamental proxy for exploration quality. We extend the theory into test-time in-context exploration and adopt state coverage, the in-context state diversity, as theoretical proxy for the exploration quality. Figure 1 demonstrates the difference between in-context exploration and the classic rl training exploration. Our theoretical analysis of state coverage exposes critical bottleneck (Remark 4.3): achieving broader state coverage requires longer reasoning sequences, yet such sequences face exponentially diminishing sampling probabilities (Shallow Exploration Trap) (Lemma 4.2). Empirically, we analyze the training dynamics of GSPO and GRPO baselines. Our observations validate our theoretical results and also reveal that these methods implicitly incentivize response length to some degree, while decreasing states density (Section 4.3). This observation prompts our primary research question: How to effectively incentivize in-context exploration in RL for test-time scaling? To address this, we propose Length-Incentivized Exploration (LIE), an RL recipe that maximizes in-context state coverage, in two-step manner. The length reward elevates the upper bound of states, and the redundancy penalty matches the number of states given length (as introduced in Section 5). Experiments demonstrate that LIE significantly improves performance compared to GRPO and GSPO baselines across diverse models (Qwen3 (Yang et al., 2025a) and LLaMA (Meta, 2024)). Specifically, our method achieves consistent performance boosts, yielding 4.4% average gain on in-domain reasoning tasks and 2.7% improvement on out-of-domain benchmarks, underscoring its superior generalization capabilities. Our key contributions are summarized as follows: Identifying the In-Context Exploration Bottleneck.. We identify Shallow Exploration Trap as the bottleneck of in-context exploration, substantiating its existence through both theoretical derivations (Proposition 4.1 & Lemma 4.2) and empirical validations (Figure 3). Length-Incentivized Exploration (LIE) Recipe. We propose simple yet effective training recipe that explicitly incentivizes the model to overcome the bottleneck, enabling the model to explore during both training and inference. Empirical Validation and Test-Time Scaling. Comprehensive experiments demonstrate that LIE significantly outperforms standard baselines, achieving effective test-time scaling and eliciting more cognitive behaviors. Scaling Test-Time Compute via Long CoT. Recent works have shown remarkable potential in scaling test-time computation (Snell et al., 2024) by training models to generate the long chains of thoughts (CoT) that enable strategic behaviors, including verification, self-correction (Kumar et al., 2024), etc. Scaling test-time computation significantly enhances the performance of Large Language Models (Guo et al., 2025; Team et al., 2025; Luo et al., 2025; Jaech et al., 2024) on various domains. Reinforcement Learning (RL), in particular, facilitates natural scaling of test-time computation through intrinsic adaptive exploration and strategy application (Guo et al., 2025; Team et al., 2025; Gandhi et al., 2025; Setlur et al., 2025; Yeo et al., 2025). These processes are mechanistically driven by negative gradients (Setlur et al., 2025; Zhu et al., 2025) or high entropy (Cui et al., 2025; Wang et al., 2025a; Cheng et al., 2025). S1 (Muennighoff et al., 2025) forces the model to think longer through explicitly forcing the generation of additional tokens to extrapolate thinking. However, we focus directly on increasing the computation usage during reinforcement training to activate in-context exploration. Length-Aware Reasoning. Recent studies have investigated length-aware reasoning, specifically aiming to mitigate overthinking and improve both efficiency and controllability. Several works (Jiang et al., 2025; Huang et al., 2025; Zhang et al., 2025b; Kang et al., 2025; Zhang et al., 2025c; Yu et al., 2025c; Yang et al., 2025b; Liu et al., 2025b) propose adaptively switching between long and short Chainof-Thought (CoT) based on problem difficulty through training. Others (Zhang et al., 2025a; Ma et al., 2025) focus on controlling token usage during inference, while some studies address reasoning within specific budget (Wen et al., 2025; Aggarwal & Welleck, 2025; Xu et al., 2025). In contrast to these efficiency-centric approaches that primarily aim to optimize token usage or curb overthinking, we posit that extended reasoning is prerequisite for reaching deep reasoning states. We investigate explicitly scaling up reasoning trajectories not as burden, but as mechanism to break the Shallow Exploration Trap and activate intrinsic exploratory behaviors. 3. Background In this section, we formulate LLM reasoning as Markov Decision Process (MDP) and review the theoretical foundations of Count-Based Exploration in traditional reinforcement learning. 3.1. MDP Formulation of LLM Reasoning LLM Reasoning as an MDP. We model the autoregressive generation process of Large Language Model (LLM) 2 Learn to Explore In-Context via Length-Incentivized Reinforcement Learning as deterministic MDP tuple (S, A, π, ), where State Space (S): The state space consists of all possible sequences of tokens from the vocabulary V. Crucially, specific state st at time step is the concatenation of the input query (prompt) and the thought chain generated so far y<t. Formally, st = [x, y1, . . . , yt1]. Action Space (A): The action space is discrete and equivalent to the models vocabulary V, where an action at corresponds to selecting the next token to append to the current sequence. Transition Dynamics (T ): The transition is deterministic. Given the current state st and action at, the next state is uniquely determined by appending the token to the history: st+1 = st at. The process terminates when special end-of-sequence (EOS) token is generated. Policy (πθ): The LLM functions as parameterized policy πθ(atst), which maps the current context st to probability distribution over the vocabulary V. Due to the combinatorial nature of language, the size of grows exponentially with sequence length (S VL), making the state space extremely vast and sparse. Reinforcement Learning for LLM Reasoning. To optimize the policy πθ to maximize the expected return J(πθ) = E[(cid:80) rt], the optimization is typically performed via Policy Gradient (Sutton et al., 1999): θJ(πθ) = Eτ (cid:34) (cid:88) (cid:35) θ log πθ(at st) Aπ(st, at) , t=0 where Aπ represents the advantage of taking action at. GRPO and GSPO are instantiations of this framework. Group Relative Policy Optimization (GRPO). GRPO (Shao et al., 2024) performs optimization at the token level without value function. It utilizes the standard per-token probability ratio: ρi,t(θ) = πθ(yi,tx, yi,<t) πθold(yi,tx, yi,<t) , (1) where yi,t is the t-th token of the i-th sequence. The advantage is computed via group normalization: ˆAi = Ri mean({Rj}G std({Rj}G j=1) j=1) . (2) The objective averages the PPO-clip loss (Schulman et al., 2017) over all tokens in the generated sequences: GRPO(θ) = Ex,{yi}πθold (cid:20) 1 (cid:88) i=1 1 yi yi (cid:88) (cid:16) min ρi,t(θ) ˆAi, clip(ρi,t(θ), 1 ϵ, 1 + ϵ) ˆAi t=1 (cid:17)(cid:21) . (3) 3 Group (GSPO). Sequence Policy Optimization GSPO (Zheng et al., 2025) elevates optimization to the sequence level using length-normalized importance ratios: ρi(θ) = yi (cid:89) t=1 πθ(yi,tx, yi,<t) πθold (yi,tx, yi,<t) 1/yi , (4) where the 1/yi exponent reduces variance from varying sequence lengths. Using the same group-based advantage ˆAi as GRPO, the objective is computed once per sequence. 3.2. Theoretical Foundation: Incentivizing State Coverage via Count-Based Exploration Count-based exploration is fundamental strategy to address the exploration-exploitation tradeoff. Central to this approach is the state visitation count (s), which records the cumulative frequency of visiting state s. These counts serve as proxy for state coverage the diversity of states visited during exploration. Guided by the principle of Optimism in the Face of Uncertainty (Auer et al., 2002), count-based methods augment the extrinsic reward with an exploration bonus b(s), typically inversely proportional to the counts (e.g., 1/(cid:112)N (s)). Theoretical Guarantees for Exploration. The optimality of count-based exploration is formally established in the Multi-Armed Bandit (MAB) setting. More detailed descriptions are shown in Appendix A.1. The Upper-Confidence Bound (UCB) algorithm selects action at by balancing reward estimates with visitation counts: at = arg max aA ˆRt(a) + (cid:115) 2 log nt(a) , (5) where ˆRt(a) and nt(a) is the reward estimate and visitation count for action at time t. The bonus term (cid:112)2 log t/nt(a) decreases as the action is sampled more frequently. Crucially, this bonus minimizes cumulative regret, yielding theoretical guarantee for efficiently identifying the optimal action, formalized as: Theorem 3.1 (Optimality of Count-based Exploration (Auer, 2002)). In an MAB setting, let L(T ) = E[(cid:80)T t=1 (R R(at))] denote the total regret over steps, where R(a) = ERa [R] is the expected reward for any action a. Ra is the reward distribution for action and is the reward for the optimal action. The UCB algorithm (Eq. 15) achieves this optimal bound: lim L(T ) 8 log (cid:88) , aa>0 (6) where is the reward gap between action and the optimal action. Learn to Explore In-Context via Length-Incentivized Reinforcement Learning Remark 3.2. The count-based exploration principle provides fundamental insight for LLM reasoning: effective exploration requires maximizing state coverage. It is crucial to note that in standard RL training, the visitation count (s) is aggregated across the whole state space over many training episodes. For in-context exploration, however, we must adapt this principle to quantify state diversity within single reasoning trajectory. 4. In-Context Exploration While the standard MDP formulation addresses trainingtime optimization, In-Context Exploration addresses how the model scales at test time. As highlighted in Remark 3.2 and Figure 1, the focus shifts from global state coverage to maximizing state coverage within single trajectory. 4.1. In-Context State Space Defining in-context states. Given single reasoning trajectory τ = (y1, y2, . . . , yL), where is the trajectory length, we define the in-context state space as the sequence of autoregressive states visited during generation: SIC(τ ) = {s1, s2, . . . , sL} , (7) direct application of count-based states to in-context states faces fundamental challenge: since every state st contains unique history prefix, no raw state is ever visited twice within trajectory, rendering state coverage measurement trivial and meaningless. State abstraction. To derive meaningful visitation counts, we introduce state abstraction function ϕ that maps raw states to abstract states capturing the logical state of reasoning rather than the literal history. Following evidence that semantics is predominantly encoded in immediate local patterns (Levy & Goldberg, 2014), we employ last-n-grams as our abstraction: ϕ(st) = (ytn+1, . . . , yt) . (8) By aggregating states based on these local patterns, we obtain pseudo-visitation counts that meaningfully estimate state coverage within single context. Quantifying in-context exploration. Furthermore, we can now quantify in-context exploration quality. Following the count-based exploration principle from Section 3.2, we define the In-Context Distinct State Count Ccontext(τ ) as the cardinality of unique context states visited within trajectory: Ccontext(τ ) = (cid:12) (cid:12){ϕ(st) = 1, . . . , L}(cid:12) (cid:12) . (9) This metric captures the volume of the state space covered during reasoning. Figure 2. The Length Bottleneck of In-Context Exploration. (a) Capacity: Trajectory length dictates the maximum possible state coverage (Proposition 4.1). (b) Shallow Exploration Trap: The probability of reaching deep states decays exponentially (Lemma 4.2), preventing the model from utilizing this capacity. Section 3.2 posits that the optimal exploration strategy involves assigning bonus reward proportional to the inverse of state visitation counts, i.e, 1/(cid:112)N (s), which corresponds to maximizing Ccontext. However, as described in Goodharts law (Goodhart, 1984) and discussed in Appendix C.1, directly maximizing Ccontext leads to losing efficacy and reward hacking in RL training. 4.2. The Length Bottleneck of In-Context Exploration Given the unreliability of direct optimization, we instead analyze more fundamental question: what structural factor bottlenecks the growth of Ccontext? Length as the capacity for exploration. From the definition in Eq. 8 and Eq. 9, we first derive relationship between trajectory length and exploration potential: Proposition 4.1 (Length as the Capacity for Exploration). The cumulative exploration utility of trajectory is strictly upper-bounded by its length L: Ccontext(τ ) . (10) Consequently, extending the sequence length is necessary condition to raise the ceiling of exploration potential. Proposition 4.1 establishes that sequence length defines the theoretical capacity for state discovery (as evidenced in Figure 2a). To maximize state coverage, the model must generate sufficiently long trajectories. The Shallow Exploration Trap. However, language models face intrinsic difficulties in sampling long sequences, as stated below. Lemma 4.2 (Exponential Decay of Long Sequences). Let SL denote the set of states (i.e., responses in LLM reasoning) with sentence length L, and p(SL) denote the sampling probability for that state set. Then, there exists constant ϵ (0, 1), such that the sampling probability of SL is upper-bounded by an exponentially decaying function of its length L, the proof can be found in Appendix A.2: p(SL) < (1 ϵ)L1 . (11) Learn to Explore In-Context via Length-Incentivized Reinforcement Learning Figure 3. The training dynamics of Ccontext and RContext in GRPO and GSPO on Qwen3-4B-Base. Two limitations are observed: (1) Shallow Exploration Trap: GRPO faces bottlenecks in extending trajectory length and performance, while GSPO shows slow length expansion. (2) Degrading Information Density: Both methods display degradation in ratio over time. Lemma 4.2 characterizes the Shallow Exploration Trap: the model is exponentially biased against the long trajectories required to reach reasoning states (Figure 2b). These limitations indicate that enhancing in-context exploration requires not only longer reasoning trajectories but also efficient and non-redundant exploration. Remark 4.3 (The Exploration-Length Conflict). Together, Proposition 4.1 and Lemma 4.2 reveal fundamental conflict for in-context exploration: long trajectories are necessary for broad state coverage, yet they are exponentially unlikely to be sampled. 4.3. Empirical Validation in RL Training To further validate the relationship between response length, state coverage, and model performance (Proposition 4.1), we conduct pilot study using GRPO and GSPO. Length correlates with coverage and performance. As illustrated in Figure 3, we observe remarkably high Pearson correlation (ρ(L, Ccontext) > 0.96) between Response Length and Ccontext. This confirms our Proposition 4.1: extending the physical length of the trajectory is the primary mechanism by which the model expands its exploration volume. Crucially, as the model accesses these longer sequences, reasoning accuracy improves. The limitations of standard RL Training. However, our analysis reveals bottlenecks to incentivized in-context exploration in current RL training: 1. Shallow exploration trap (Lemma 4.2): In GRPO, performance plateaus along with the response length stabilization. While GSPO shows continuous length growth, it is prohibitively slow. Both algorithms eventually succumb to the Shallow Exploration Trap, failing to push far enough to reach complex states. 2. Degrading state density: As shown in Figure 3, length increases, the Distinct Ccontext Ratio (Rcontext) tends to decrease. This indicates that standard RL incentives increase trajectory length but fail to improve state density. The model fills the extra capacity with repetitive tokens rather than novel reasoning steps, degrading the efficiency of exploration. 5. Length-Incentivized Exploration In this section, we introduce targeted reward shaping recipe Length-Incentivized Exploration(LIE) to enforce in-context exploration in reinforcement learning. 5.1. The Length-Incentivized Reward We first introduce Length-Incentivized Reward (Rlen) designed to explicitly encourage the model to extend its reasoning trajectories beyond its initial policys tendencies when it fails to reach the correct answer Racc=0. We define sample-wise target length Ltarget,i = Lref,i+L, where Lref,i denotes the response length of the initial policy of i-th sample and represents the desired least increment for exploration. The length reward is defined as: Rlen = 0 Ltarget, Racc = 0 η(Ltarget L) < Ltarget, Racc = 0 0 Racc = 1 . (12) the model is reThis formulation creates curriculum: warded for extending its reasoning process beyond its usual depth whenever it cannot solve problem immediately. 5.2. Redundancy Reward: Enforcing Effective State Coverage While Rlen increases the exploration capacity L, it does not guarantee that this capacity is converted into effective state coverage Ccontext (as revealed in Section 4.3). Recall Proposition 4.1: Ccontext(τ ) L. The equality holds only when redundancy is minimized. Redundancy Reward. We adopt hard-threshold approximation of redundant in-context states that is in line with the widely used repetition penalty (Yu et al., 2025b). Let Nτ (ϕ(st)) be the visitation count of state abstraction ϕ(st) 5 Learn to Explore In-Context via Length-Incentivized Reinforcement Learning in the current trajectory. The reward is defined as: Rred = β (cid:89) [Nτ (ϕ(st)) > Θ] , (13) where Θ is the threshold and β is the penalty magnitude. Theoretical Consistency. This formulation aligns with the count-based exploration bonus bt 1/ Nt (Section 3.2). Maximizing Rlen + Rred is equivalent to maximizing state coverage Ccontext, in two-step manner. The length reward elevates the upper bound of states, and the redundancy penalty matches the number of states given length. Final recipe. To prioritize solution accuracy while encouraging test-time scaling, we shape the reward to favor correct responses and longer reasoning trajectories, while penalizing redundant exploration: = Racc + Rlen + β Rred , (14) 6. Experiments In this section, we empirically validate the effectiveness of LIE. We conduct comprehensive experiments to address the following core research questions: RQ1: performance & test-time scaling. Does our recipe mitigate the \"Shallow Exploration Trap\" to achieve more effective test-time scaling? RQ2: impact of model capability. How does the effectiveness of our recipe on the different policy models initial state? RQ3: exploration dynamics. How does the lengthincentivized recipe impact global state diversity and exploration during training? 6.1. Experimental Setup Evaluation. We assess performance across eight reasoning benchmarks, categorized into in-domain mathematical tasks and out-of-distribution (OOD) general reasoning. The in-domain suite comprises AIME 2024/2025, AMC (Li et al., 2024), MATH-500 (Hendrycks et al., 2021), and OlympiadBench (He et al., 2024). The OOD evaluation includes ARC-c (Clark et al., 2018), GPQA-Diamond (denoted as GPQA* (Rein et al., 2024)), and MMLU-Pro (Wang et al., 2024). Regarding metrics, for benchmarks with limited sample sizes (AIME and AMC), we report the average accuracy over 32 independent runs (Avg@32). For all other benchmarks, we report Pass@1. All evaluations are conducted with sampling temperature of 0.6, top-p=1.0, and maximum response length of 32k tokens, which surpasses the training budget. RLVR setups. Training is performed with prompt batch size of 128, generating 8 rollouts per prompt. We update the policy using mini-batch size of 32 and learning rate of 1e-6. During the training phase, the sampling temperature is set to 1.0, with maximum response length of 8,192 tokens. Experiments are implemented using the verl framework1 on nodes equipped with 4H100 GPUs, employing MathVerify2 for outcome-based reward calculation. We default set = 10, η = 0.3/9000, β = 0.6 and Θ = 10 in reward. We discussed these hyperparameters in Appendix C.5. Models and baselines. We designate Qwen3-4B-Base as our primary testbed. To verify the universality of our recipe across different training stages and model families, we also extend our evaluation to Qwen3-4B (non-thinking) and Llama-OctoThinker-3B-Base (Wang et al., 2025b). The training datasets are selected to align with the respective models: DAPO-Math-17k (Yu et al., 2025a) for Qwen34B-Base, Polaris (An et al., 2025) for Qwen3-4B-Instruct, and DeepMath-5k (He et al., 2025; Tan et al., 2025) for Llama-OctoThinker. We primarily investigate GSPO as the core algorithm. Additionally, for Qwen3-4B-Base, we benchmark against standard GRPO and GRPO variant with high clip ratio (0.28) to validate our recipes effectiveness across different algorithms. 6.2. Results LIE achieves superior performance across algorithms on Qwen3-4B-Base. As shown in Table 1, our recipe consistently outperforms baselines across all benchmarks. On Qwen3-4B-Base, applying our method to GSPO boosts average in-domain accuracy from 49.4% to 53.8% and OOD accuracy from 62.7% to 67.1%. Notably, we achieve 6.2% gain on the challenging AIME25, confirming that breaking the Shallow Exploration Trap effectively unlocks complex problem-solving capabilities. Moreover, our recipe achieves consistent gains across different model sizes (as shown in Appendix C.3). Effects of recipe on in-context state coverage. To understand the source of these gains, we analyze the training dynamics in Figure 4. Compared to the GSPO baseline, our method explicitly drives longer trajectories, resulting in rapid expansion of Ccontext. This confirms that our recipe incentive effectively increases the Capacity for Exploration (Proposition 4.1). While the state density Rcontext initially drops, the model eventually learns to utilize this extended budget, converting the increased state coverage into reasoning paths that correlate with the accuracy spike. Test-time scaling via Long CoT. Figure 5 illustrates the performance trajectory as we increase the inference compute budget. While standard RL models (Blue line) saturate or degrade when forced beyond their learned policy length, our recipe (Red line) exhibits superior scaling curve, main1https://github.com/volcengine/verl 2https://github.com/huggingface/Math-Verify 6 Learn to Explore In-Context via Length-Incentivized Reinforcement Learning Table 1. In-Domain and Out-of-Domain evaluation performance based on Qwen3-4B-Base. Bold and underline indicate the best and second-best results. Gains of our methods compared to corresponding baselines are marked in blue. Model Qwen3-4B-Base GRPO GRPO w/Clip-higher GSPO GRPO + LIE GRPO w/Clip-higher + LIE GSPO + LIE MATH Olympiad AMC AIME AIME25 In-Domain Performance 66.0 80.4 86.4 85.2 85.0 88.8 88.4 33.2 47.1 54.1 51.7 49.9 54.1 57. 36.6 55.2 61.8 62.7 60.5 65.5 66.2 8.5 16.8 25.2 26.7 22.9 30.4 30. 6.9 18.7 22.2 20.5 16.4 24.5 26.7 Avg. 30.2 43.6 49.9 49. 46.9+3.3 52.6+2.7 53.8+4.4 Out-of-Domain Performance ARC-c GPQA MMLU-Pro 66.9 84.6 89.6 88.4 90.3 91.5 91. 26.3 44.4 46.0 48.5 46.5 43.9 47.5 30.9 60.1 62.8 61.5 60.4 63.0 63. Avg. 41.4 63.0 66.1 66.1 65.7+2.7 66.1+0.0 67.6+1.5 Table 2. In-Domain Evaluation performance based on Qwen34B and Llama-OctoThinker-3B. Bold indicates the best result. Gains of our methods compared to corresponding baselines are marked in blue. MATH Olympiad AMC AIME AIME Avg. Qwen3-4B GSPO GSPO + LIE OctoThinker GSPO GSPO + LIE 82.8 94 94.4 23.4 55.8 60.8 51.9 68.1 9.0 23.1 28.1 60.4 82 85.3 10.4 28.2 30.3 24.2 54.2 57.7 1.1 3.8 4.5 19.4 42.5 46. 0.6 2.3 4.4 47.7 68.2 70.2+2.0 8.9 22.6 25.6+3.0 Table 3. Continual Scaling via Curriculum Training based on Qwen3-4B-Base. Stage 1 represents the initial training, while Stage 2 employs further length extension. The results demonstrate consistent improvements across benchmarks. MATH Olympiad AMC AIME AIME25 Avg. Qwen3-4B-Base GSPO Stage1 Stage2 66.0 85.2 88.4 89.4 33.2 51.7 57.2 59.0 36.6 62.7 66.2 71.7 8.5 26.7 30.5 33.4 6.9 20.5 26.7 27. 30.2 49.4 53.8+4.4 56.3+2.5 gains (2%3%) across models. The training dynamics in Figures 11 and 6 reveal that our recipe universally drives the expansion of state coverage, converting the increased trajectory length into improved in-context exploration. 6.3. Analysis and Discussion Global diversity. Beyond in-context state diversity, we track Global Distinct Count (Cglobal) and policy entropy to assess exploration across the state space during training time. As shown in Figure 7, standard GSPO exhibits saturation and rapid entropy drop, indicating premature convergence (mode collapse). In contrast, our recipe maintains continuous growth in and significantly higher entropy. This sustained global exploration prevents mode collapse, enabling the policy to discover rare, high-reward states during training that standard methods miss. Continual scaling via curriculum training. To validate scaling laws beyond the initial setup, we conducted second training stage (Stage 2) with relaxed 12k token limit. As shown in Table 3, the model exhibits continuous improvement. This confirms that our recipe effectively converts Figure 4. Ccontext, Rcontext, response length, and performance on the valid dataset comparing GSPO baseline and our recipe. Figure 5. Test-time extrapolation performance. While standard baselines saturate or degrade when forced beyond their learned policy length, the Length-Incentivized Exploration recipe exhibits superior scaling curve. taining an upward trend. This confirms that LIE effectively utilizes the extended token budget to explore broader hypothesis spaces by improved in-context exploration. LIE is effective across different models. We extended our recipe to Qwen3-4B (post-trained) and LlamaOctoThinker (Wang et al., 2025b). Table 2 reports consistent 7 Learn to Explore In-Context via Length-Incentivized Reinforcement Learning Figure 6. Tracking Ccontext, Rcontext, response length L, and valid performance during the training of Qwen3-4B. Compared to the GSPO baseline (Blue), the LIE (Purple) successfully breaks the Shallow Exploration Trap and achieves performance gains. Figure 7. Global Exploration Dynamics: Diversity and Entropy. Our recipe maintains significantly higher entropy and continuous growth in global state coverage compared with the baseline. Figure 8. Frequency analysis of cognitive behaviors. The proposed recipe (+ LIE) significantly boosts all behaviors, with notable enhancement in Backtracking. additional compute into reasoning accuracy, establishing scaling trend with the allocated reasoning horizon. Changes in Reasoning Behaviors. Adopting the framework from (Gandhi et al., 2025), we employ Claude-3.7Thinking to quantify four representative cognitive behaviors: Backtracking, Verification, Subgoal Setting, and Enumeration. As illustrated in Figure 8, the integration of LIE consistently stimulates more frequent reasoning activities across all categories. Crucially, we observe substantial increase in Backtracking (from 103 to 121), which is sophisticated cognitive capability. This improvement underscores that LIE effectively enhances the models capacity for in-context exploration. We also discuss the changes in reasoning structure in Appendix D. 8 Figure 9. Impact of the Length-Incentivized Reward (Llen). Forcing the model to \"think longer\" yields performance gains. However, the accompanying drop in the distinct ratio Rcontext reveals tendency towards repetition. Only Rlen incentives exploration but introduce repetition. As shown in Figure 9, explicitly incentivizing longer trajectories rapidly expands Ccontext and improves accuracy, confirming that length is capacity for additional in-context exploration (Proposition 4.1). However, this expansion is accompanied by decline in Rcontext revealing that the model often satisfies the length reward through redundant rather than informative generation. This observation necessitates redundancy penalty to maintain exploration quality, synergy further detailed in the ablation study of our reward components (Equation 14) in Appendix C.6. 7. Conclusion In this work, we identified the conflict between the length required for broader state coverage and the Shallow Exploration Trap as critical bottleneck in in-context exploration through theoretical and empirical analysis. To overcome this limitation, we proposed Length-Incentivized Exploration, simple yet effective recipe that maximizing the in-context state coverage. Our experiments across diverse models and benchmarks confirm that LIE incentivize the in-context exploration during test time. Overall, our work advances Learn to Explore In-Context via Length-Incentivized Reinforcement Learning test-time scaling by explicitly activating in-context exploration, demonstrating how additional computation can be systematically converted into more effective reasoning."
        },
        {
            "title": "Impact Statements",
            "content": "This paper presents work whose goal is to advance the field of machine learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Agarwal, S., Ahmad, L., Ai, J., Altman, S., Applebaum, A., Arbus, E., Arora, R. K., Bai, Y., Baker, B., Bao, H., et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Aggarwal, P. and Welleck, S. L1: Controlling how long reasoning model thinks with reinforcement learning. arXiv preprint arXiv:2503.04697, 2025. An, C., Xie, Z., Li, X., Li, L., Zhang, J., Gong, S., Zhong, M., Xu, J., Qiu, X., Wang, M., and Kong, L. Polaris: post-training recipe for scaling reinforcement learning on advanced reasoning models, 2025. URL https: //hkunlp.github.io/blog/2025/Polaris. Auer, P. Using confidence bounds for exploitationexploration trade-offs. Journal of machine learning research, 3(Nov):397422, 2002. Auer, P., Cesa-Bianchi, N., and Fischer, P. Finite-time analysis of the multiarmed bandit problem. Machine learning, 47(2):235256, 2002. Bellemare, M., Srinivasan, S., Ostrovski, G., Schaul, T., Saxton, D., and Munos, R. Unifying count-based exploration and intrinsic motivation. In Advances in Neural Information Processing Systems, volume 29, 2016. Brown, B., Juravsky, J., Ehrlich, R., Clark, R., Le, Q. V., Ré, C., and Mirhoseini, A. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Cheng, D., Huang, S., Zhu, X., Dai, B., Zhao, W. X., Zhang, Z., and Wei, F. Reasoning with exploration: An entropy perspective. arXiv preprint arXiv:2506.14758, 2025. anism of reinforcement learning for reasoning language models. arXiv preprint arXiv:2505.22617, 2025. Feng, Y., Kempe, J., Zhang, C., Jain, P., and Hartshorn, A. What characterizes effective reasoning? revisiting length, review, and structure of cot. arXiv preprint arXiv:2509.19284, 2025. Gandhi, K., Lee, D., Grand, G., Liu, M., Cheng, W., Sharma, A., and Goodman, N. D. Stream of search (sos): Learning to search in language. arXiv preprint arXiv:2404.03683, 2024. Gandhi, K., Chakravarthy, A., Singh, A., Lile, N., and Goodman, N. D. Cognitive behaviors that enable selfimproving reasoners, or, four habits of highly effective stars. arXiv preprint arXiv:2503.01307, 2025. Goodhart, C. A. Problems of monetary management: the uk experience. In Monetary theory and practice: The UK experience, pp. 91121. Springer, 1984. Guo, D., Yang, D., Zhang, H., Song, J., Zhang, R., Xu, R., Zhu, Q., Ma, S., Wang, P., Bi, X., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., et al. Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 38283850, 2024. He, Z., Liang, T., Xu, J., Liu, Q., Chen, X., Wang, Y., Song, L., Yu, D., Liang, Z., Wang, W., et al. Deepmath-103k: large-scale, challenging, decontaminated, and verifiable mathematical dataset for advancing reasoning. arXiv preprint arXiv:2504.11456, 2025. Hendrycks, D., Burns, C., Kadavath, S., Arora, A., Basart, S., Tang, E., Song, D., and Steinhardt, J. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. Huang, S., Wang, H., Zhong, W., Su, Z., Feng, J., Cao, B., and Fung, Y. R. Adactrl: Towards adaptive and controllable reasoning via difficulty-aware budgeting. arXiv preprint arXiv:2505.18822, 2025. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Jaech, A., Kalai, A., Lerer, A., Richardson, A., El-Kishky, A., Low, A., Helyar, A., Madry, A., Beutel, A., Carney, A., et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Cui, G., Zhang, Y., Chen, J., Yuan, L., Wang, Z., Zuo, Y., Li, H., Fan, Y., Chen, H., Chen, W., et al. The entropy mechJiang, L., Wu, X., Huang, S., Dong, Q., Chi, Z., Dong, L., Zhang, X., Lv, T., Cui, L., and Wei, F. Think only 9 Learn to Explore In-Context via Length-Incentivized Reinforcement Learning when you need with large hybrid-reasoning models. arXiv preprint arXiv:2505.14631, 2025. Kang, Y., Sun, X., Chen, L., and Zou, W. C3ot: Generating shorter chain-of-thought without compromising effectiveness. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pp. 2431224320, 2025. Kolter, J. Z. and Ng, A. Y. Near-Bayesian exploration in polynomial time. In Proceedings of International Conference on Machine Learning, pp. 513520, 2009. Kumar, A., Zhuang, V., Agarwal, R., Su, Y., Co-Reyes, J. D., Singh, A., Baumli, K., Iqbal, S., Bishop, C., Roelofs, R., et al. Training language models to self-correct via reinforcement learning. arXiv preprint arXiv:2409.12917, 2024. Levy, O. and Goldberg, Y. Neural word embedding as implicit matrix factorization. Advances in neural information processing systems, 27, 2014. Li, J., Beeching, E., Tunstall, L., Lipkin, B., Soletskyi, R., Huang, S., Rasul, K., Yu, L., Jiang, A. Q., Shen, Z., et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13(9):9, 2024. Lightman, H., Kosaraju, V., Burda, Y., Edwards, H., Baker, B., Lee, T., Leike, J., Schulman, J., Sutskever, I., and In The Twelfth Cobbe, K. Lets verify step by step. International Conference on Learning Representations, 2023. Liu, R., Gao, J., Zhao, J., Zhang, K., Li, X., Qi, B., Ouyang, W., and Zhou, B. Can 1b llm surpass 405b llm? rethinking compute-optimal test-time scaling. arXiv preprint arXiv:2502.06703, 2025a. Liu, W., Zhou, R., Deng, Y., Huang, Y., Liu, J., Deng, Y., Zhang, Y., and He, J. Learn to reason efficiently with adaptive length-based reward shaping, 2025b. URL https://arxiv.org/abs/2505.15612. Luo, M., Tan, S., Wong, J., Shi, X., Tang, W. Y., Roongta, M., Cai, C., Luo, J., Li, L. E., Popa, R. A., and Stoica, I. Deepscaler: Surpassing 01-preview with 1.5b model by scaling rl. https://pretty-radio-b75.notio n.site/DeepScaleR-Surpassing-01-Previ ew-with-a-1-5B-Model-by-Scaling-RL-1 9681902c1468005bed8, 2025. Ma, X., Wan, G., Yu, R., Fang, G., and Wang, X. Cot-valve: Length-compressible chain-of-thought tuning. arXiv preprint arXiv:2502.09601, 2025. Meta, A. Llama 3.2: Revolutionizing edge ai and vision with open, customizable models. Meta AI Blog. Retrieved December, 20:2024, 2024. Muennighoff, N., Yang, Z., Shi, W., Li, X. L., Fei-Fei, L., Hajishirzi, H., Zettlemoyer, L., Liang, P., Candès, E., and Hashimoto, T. B. s1: Simple test-time scaling. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pp. 20286 20332, 2025. Rein, D., Hou, B. L., Stickland, A. C., Petty, J., Pang, R. Y., Dirani, J., Michael, J., and Bowman, S. R. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Setlur, A., Yang, M. Y., Snell, C., Greer, J., Wu, I., Smith, V., Simchowitz, M., and Kumar, A. e3: Learning to explore enables extrapolation of test-time compute for llms. arXiv preprint arXiv:2506.09026, 2025. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Bi, X., Zhang, H., Zhang, M., Li, Y., Wu, Y., et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Snell, C., Lee, J., Xu, K., and Kumar, A. Scaling llm testtime compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Strehl, A. L. and Littman, M. L. An analysis of modelbased interval estimation for markov decision processes. Journal of Computer and System Sciences, 74(8):1309 1331, 2008. Sutton, R. S., McAllester, D., Singh, S., and Mansour, Y. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems, 12, 1999. Tan, Y., Wang, M., He, S., Liao, H., Zhao, C., Lu, Q., Liang, T., Zhao, J., and Liu, K. Bottom-up policy optimization: Your language model policy secretly contains internal policies. arXiv preprint arXiv:2512.19673, 2025. Tang, H., Houthooft, R., Foote, D., Stooke, A., Xi Chen, O., Duan, Y., Schulman, J., DeTurck, F., and Abbeel, P. # exploration: study of count-based exploration for deep reinforcement learning. In Advances in Neural Information Processing Systems, volume 30, 2017. Team, K., Du, A., Gao, B., Xing, B., Jiang, C., Chen, C., Li, C., Xiao, C., Du, C., Liao, C., et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. Learn to Explore In-Context via Length-Incentivized Reinforcement Learning Wang, S., Yu, L., Gao, C., Zheng, C., Liu, S., Lu, R., Dang, K., Chen, X., Yang, J., Zhang, Z., et al. Beyond the 80/20 rule: High-entropy minority tokens drive effective reinforcement learning for llm reasoning. arXiv preprint arXiv:2506.01939, 2025a. Yu, Z., Xu, T., Jin, D., Sankararaman, K. A., He, Y., Zhou, W., Zeng, Z., Helenowski, E., Zhu, C., Wang, S., et al. Think smarter not harder: Adaptive reasoning with inference aware optimization. arXiv preprint arXiv:2501.17974, 2025c. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Zhang, J., Dong, R., Wang, H., Ning, X., Geng, H., Li, P., He, X., Bai, Y., Malik, J., Gupta, S., et al. Alphaone: Reasoning models thinking slow and fast at test time. arXiv preprint arXiv:2505.24863, 2025a. Zhang, J., Lin, N., Hou, L., Feng, L., and Li, J. Adaptthink: Reasoning models can learn when to think. arXiv preprint arXiv:2505.13417, 2025b. Zhang, S., Wu, J., Chen, J., Zhang, C., Lou, X., Zhou, W., Zhou, S., Wang, C., and Wang, J. Othink-r1: Intrinsic fast/slow thinking mode switching for over-reasoning mitigation. arXiv preprint arXiv:2506.02397, 2025c. Zheng, C., Liu, S., Li, M., Chen, X.-H., Yu, B., Gao, C., Dang, K., Liu, Y., Men, R., Yang, A., et al. Group sequence policy optimization. arXiv preprint arXiv:2507.18071, 2025. Zhu, X., Xia, M., Wei, Z., Chen, W.-L., Chen, D., and Meng, Y. The surprising effectiveness of negative reinforcement in llm reasoning. arXiv preprint arXiv:2506.01347, 2025. Wang, Y., Ma, X., Zhang, G., Ni, Y., Chandra, A., Guo, S., Ren, W., Arulraj, A., He, X., Jiang, Z., et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. Advances in Neural Information Processing Systems, 37:9526695290, 2024. Wang, Z., Zhou, F., Li, X., and Liu, P. Octothinker: Mid-training incentivizes reinforcement learning scaling. arXiv preprint arXiv:2506.20512, 2025b. Wen, H., Wu, X., Sun, Y., Zhang, F., Chen, L., Wang, J., Liu, Y., Liu, Y., Zhang, Y.-Q., and Li, Y. Budgetthinker: Empowering budget-aware llm reasoning with control tokens. arXiv preprint arXiv:2508.17196, 2025. Wu, Y., Sun, Z., Li, S., Welleck, S., and Yang, Y. Inference scaling laws: An empirical analysis of computeoptimal inference for problem-solving with language models. arXiv preprint arXiv:2408.00724, 2024. Xu, Y., Dong, H., Wang, L., Sahoo, D., Li, J., and Xiong, C. Scalable chain of thoughts via elastic reasoning. arXiv preprint arXiv:2505.05315, 2025. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025a. Yang, W., Ma, S., Lin, Y., and Wei, F. Towards thinkingoptimal scaling of test-time compute for llm reasoning. arXiv preprint arXiv:2502.18080, 2025b. Yeo, E., Tong, Y., Niu, M., Neubig, G., and Yue, X. Demystifying long chain-of-thought reasoning in llms. arXiv preprint arXiv:2502.03373, 2025. Yu, Q., Zhang, Z., Zhu, R., Yuan, Y., Zuo, X., Yue, Y., Dai, W., Fan, T., Liu, G., Liu, L., et al. Dapo: An open-source llm reinforcement learning system at scale. arXiv preprint arXiv:2503.14476, 2025a. Yu, T., Ji, B., Wang, S., Yao, S., Wang, Z., Cui, G., Yuan, L., Ding, N., Yao, Y., Liu, Z., et al. Rlpr: Extrapolating rlvr to general domains without verifiers. arXiv preprint arXiv:2506.18254, 2025b. 11 Learn to Explore In-Context via Length-Incentivized Reinforcement Learning"
        },
        {
            "title": "Contents",
            "content": "A. Theoretical Foundation A.1. Count-Based Exploration Theoretical Foundation The exploration-exploitation tradeoff is long-standing challenge in RL literature. This becomes more pronounced in LLM reasoning tasks due to the vast state-action space with reward sparsity, where existing methods often suffer from deficient exploration and poor sample efficiency. classic, theoretically justified exploration principle is optimism in the face of uncertainty, which augments the reward estimates of less-visited states/actions with an exploration bonus proportional to their uncertainty. In the minimal multi-armed bandit (MAB) setting, the well-known upper-confidence bound (UCB) algorithm (Auer, 2002) chooses the action at according to: at = arg max aA ˆRt(a) + (cid:115) 2 log nt(a) , (15) where ˆRt(a) and nt(a) is the reward estimate and visitation count for action at time t. Theorem A.1 establishes the theoretical optimality of this count-based exploration strategy, demonstrating that it yields cumulative regret that grows only logarithmically over time. Theorem A.1. (Auer, 2002) In an MAB setting, let L(T ) = E[(cid:80)T t=1 (R R(at))] denote the total regret over steps, where R(a) = ERa [R] is the expected reward for any action and is the reward for the optimal action. Let = R(a) denote the reward gap between action and the optimum. For any bandit algorithm, the asymptotic total regret is at least logarithmic in the number of steps: lim L(T ) log (cid:88) aa>0 KL(RaRa ) . The UCB algorithm in Eq. 15 achieves logarithmic asymptotic total regret: lim L(T ) 8 log (cid:88) a. aa>0 (16) (17) Subsequent studies (Bellemare et al., 2016; Tang et al., 2017) extend this principle to MDPs by counting state-action pairs n(s, a) and adding bonus reward to encourage exploring less-visited pairs as R(s, a) = R(s, a) + β (cid:112)n(s, a) , (18) which is shown to be formally near-optimal within the probably approximately correct (PAC-MDP) framework (Strehl & Littman, 2008; Kolter & Ng, 2009). A.2. Exponential Decay of Long Sequences Broadening the coverage of the state space naturally drives the model to discover rare yet correct reasoning chains, preventing premature convergence to local optima in sparse-reward landscapes and incentivizing reasoning capacities. However, naively counting n(s, a) in vast language space becomes computationally prohibitive and statistically inefficient. Instead, we employ the response length as coarse-grained abstraction of the state space. Lemma A.2 shows that the expected count of response decays exponentially w.r.t. its length, resulting in long-tailed distribution. Lemma A.2 (Exponential Decay of Long Sequences). Let SL denote the set of states (i.e., responses in LLM reasoning) with sentence length L, and p(SL) denote the sampling probability for that state set. Then, there exists constant ϵ (0, 1), such that the sampling probability of SL is upper-bounded by an exponentially decaying function of its length L: p(SL) < (1 ϵ)L1. (19) Proof. Given generative LLM model, let pL([EOS]) denote the sampling probability of the end token at step L. With the softmax function as the output probability in language models, we have pL([EOS]) (0, 1), L. There exists lower bound 12 Learn to Explore In-Context via Length-Incentivized Reinforcement Learning of sampling probability ϵ (0, 1), such that pL([EOS]) ϵ, L. In an autoregressive manner, the sampling probability of response with length is: p(SL) = L1 (cid:89) i= (1 pi([EOS])) pL([EOS]) (1 ϵ)L1 pL([EOS]) < (1 ϵ)L1. (20) This completes the proof. B. Experimental Details B.1. Template Prompt We adopt the following template for all experiments involving Qwen models, building upon the Qwen3 template for Qwen3-4B-Base and the Qwen-Nothinking template for Qwen3. Qwen3 Template <im_start>user {problem} Lets think step by step and output the final answer within boxed{}. <im_end> <im_start>assistant Qwen3-NoThinking Template <im_start>user {problem} Lets think step by step and output the final answer within boxed{}. <im_end> <im_start>assistant <think> </think> For training the Llama-OctoThinker models, we adopt the original prompt in (Wang et al., 2025b) to ensure performance. OctoThinker Template conversation between User and Assistant. The user asks question, and the Assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer. User: You must put your answer inside boxed{} and Your final answer will be extracted automatically by the boxed{} tag. {problem} Assistant: B.2. Implementation of RL In this section, we describe the RL training setup in detail. We implement GRPO and other baseline algorithms using the Verl framework. Across all algorithms and model variants, we adopt unified set of hyperparameters, as reported in Table 4, and do not employ entropy regularization or KL-based losses. C. Additional Results C.1. Pitfalls of Direct State Coverage Maximization As discussed in Section 4, natural inclination for incentivizing in-context exploration is to directly use the In-Context Distinct State Count Ccontext(τ ) as reward signal. However, our empirical analysis in Figure 10 reveals critical failure 13 Learn to Explore In-Context via Length-Incentivized Reinforcement Learning Table 4. RL Hyperparameters. Hyperparameter Value Optimizer Policy learning rate Training batch size Samples per prompt Mini-batch size Policy updates per rollout Max prompt length Max response length Rollout temperature AdamW 1e6 128 prompts 8 32 prompts 16 1024 tokens 8192 1.0 modes: reward hacking. When Ccontext is directly maximized (as shown in the red curves), the model initially exhibits sharp increase in state coverage. However, this is quickly followed by \"collapse\" where the model learns to generate semantically hollow but structurally diverse tokens (e.g., random character permutations or irrelevant LaTeX symbols) to artificially inflate the state count. This results in significant drop in reasoning accuracy. In contrast, our LIE recipe (purple curves) uses length as proxy for capacity and redundancy as constraint, which leads to more stable and meaningful expansion of the state space, ultimately translating into higher and more stable task performance. Figure 10. Training dynamics of maximizing Cdistinct as reward. C.2. Training dynamics of OctoThinker As discussed in Section 6.2, our Length-Incentivized (LIE) recipe demonstrates strong generalization across various model architectures. Figure 11 illustrates the training dynamics of OctoThinker-3B-Base, comparing standard GSPO with our LIE-enhanced approach. These observations reinforce that \"thinking longer\" via length incentivization is reliable mechanism for performance extrapolation, regardless of the underlying policy model. Figure 11. The training dynamics of OctoThinker-3B-Base. 14 Learn to Explore In-Context via Length-Incentivized Reinforcement Learning C.3. Scalability Across Model Scales To verify the scalability of LIE, we evaluate the recipe across different model sizes: Qwen3-1.7B-Base, 4B-Base, and 8B-Base. As summarized in Table 5, LIE consistently delivers performance gains regardless of the base models capacity. Specifically, on the in-domain reasoning average, LIE improves the 1.7B model by 7.6%, the 4B model by 4.4%, and the 8B model by 2.5% compared to the GSPO baseline. Notably, on out-of-distribution (OOD) tasks, the 8B model with LIE achieves 73.4% average accuracy, 2.9% improvement over the baseline. These results demonstrate that the \"Shallow Exploration Trap\" is universal bottleneck for LLM reasoning, and LIE provides robust mechanism to unlock deeper reasoning capabilities across the scaling spectrum. Table 5. LIE on different model sizes. Model Qwen3-1.7B-Base GSPO + LIE Qwen3-4B-Base GSPO + LIE Qwen3-8B-Base GSPO + LIE MATH Olympiad AMC AIME AIME25 Avg. ARC-c GPQA MMLU-Pro Avg. In-Domain Performance Out-of-Domain Performance 51.2 70.6 77.0 66.0 85.2 88.4 67.8 89.8 91.4 20.7 32.3 39.6 33.2 51.7 57. 35.3 58.8 60.4 25.8 38.4 45.2 36.6 62.7 66.2 38.9 72.9 73.4 3.4 8.1 17.5 8.5 26.7 30. 10.3 34.4 37.2 1.7 3.8 11.5 6.9 20.5 26.7 8.5 25.8 31.6 20.6 30.6 38.2+7.6 30.2 49.4 53.8+4. 32.2 56.3 58.8+2.5 54.1 79.4 79.1 66.9 88.4 91.4 58.5 93.2 94.5 20.2 28.2 33.8 26.3 48.5 47. 32.3 50.0 55.1 27.5 41.0 45.6 30.9 61.5 63.8 51.2 68.3 70.7 33.9 49.5 52.8+3.3 41.4 66.1 67.6+1. 47.3 70.5 73.4+2.9 C.4. Synergizing with SFT: Injection vs. Activation We further investigate the interplay between Supervised Fine-Tuning (SFT) and our LIE. We posit that these two stages are orthogonal and complementary: SFT serves as the injection stage, embedding specific reasoning patterns and primitives into the model, while LIE serves as the activation mechanism, compelling the model to compose these patterns into longer, more complex reasoning chains. Setup. We constructed curated SFT dataset by randomly sampling 4k problems from the training set and generating oracle Chain-of-Thought (CoT) data using GPT-OSS-120B (Agarwal et al., 2025). We fine-tuned the Qwen3-4B-Base model on this data for 3 epochs with 4,096 token context limit. Results. The training dynamics in Figure 12 strongly support our hypothesis: 1. SFT provides the capability foundation (Injection). Both SFT-initialized runs (Red and Purple) start with significantly higher state coverage (Ccontext) and accuracy compared to the Base model (Blue), confirming that SFT successfully injects necessary reasoning primitives. 2. LIE drives further extrapolation (Activation). While the standard SFT+GSPO baseline (Purple) shows improvement, it tends to plateau in trajectory length (L 5k) and coverage, remaining tethered to the SFT data distribution. In contrast, applying LIE on top of SFT (Red) effectively \"activates\" the model to explore beyond the supervised horizon. The model utilizes the injected patterns to sustain continued expansion in reasoning length (L 7k+) and distinct state coverage. 3. Orthogonality. The highest performance is achieved by SFT GSPO + ILE (Red). This demonstrates that our recipe functions as an independent scaling lever that synergizes with the strong priors from SFT to unlock the maximum reasoning potential. 15 Learn to Explore In-Context via Length-Incentivized Reinforcement Learning Figure 12. Training Dynamics on Qwen3-4B-Base after SFT. C.5. Hyperparameter Sensitivity Analysis We conduct an ablation study to verify that our methods improvements are robust to hyperparameter choices and to understand the impact of exploration constraints. Figure 13. Ccontext and Rcontext dynamics across 6, 10, 15-grams. Sensitivity to State Abstraction (n-gram). We first analyze the impact of the context window size used for defining state abstraction (Equation 8). As shown in Figure 13, the relative performance trends between standard GSPO and our LIE-enhanced method remain consistent across {6, 10, 15}, confirming that the improvements are not an artifact of specific metric configuration. However, qualitative inspection reveals the trade-offs at extreme values: Small (n = 6): The metric becomes overly sensitive to trivial syntactic patterns. We observed that = 6 frequently flags structural tokens (e.g., repeated LaTeX formatting like ) as redundancy, introducing noise into the reward signal. Large (n = 15): The state space becomes overly sparse. With longer windows, the Unique State Count remains naturally high even for semantically repetitive chains, making it harder to detect the \"Shallow Exploration Trap.\" We adopt = 10 as the optimal balance, capturing semantic logical steps while filtering out low-level syntactic noise. 16 Learn to Explore In-Context via Length-Incentivized Reinforcement Learning Figure 14. Training dynamics of different Impact of Lengthening (L). In the training stage, we further ablate the effect of the exploration increment limit L, which governs the pace at which we force the model to expand its reasoning horizon. As illustrated in Figure 14, we observe that: Moderate Extension (L = 100, 500): These settings successfully activate the \"capacity for exploration,\" yielding stable growth in both trajectory length and accuracy. Aggressive Extension (L = 8k): Removing the progressive constraint (i.e., immediately demanding max length) destabilizes training. The model, unable to legitimately reason at such lengths instantly, collapses into degenerate repetition loops to satisfy the length reward, leading to degradation in reasoning quality. Impact of Repetition Threshold (Θ). We examine the sensitivity of the redundancy penalty threshold Θ. As shown in Table 6, our method achieves optimal performance at Θ = 10 (Avg 52.7%), surpassing both stricter (Θ = 6) and looser (Θ = 15) settings. Notably, the strict threshold (Θ = 6) leads to performance drop on the challenging AIME25 benchmark (21.5%) compared to the baseline (22.2%). We speculate the reasons are twofold: Over-penalization (Θ = 6): Mathematical reasoning requires degree of natural repetition (e.g., restating variables or structural connectors like \"therefore\"). An overly strict threshold likely triggers false positives, penalizing valid reasoning steps and disrupting the logical flow required for complex problems. Ineffective Constraint (Θ = 15): loose threshold fails to sufficiently curb the \"Shallow Exploration Trap.\" It allows the model to \"game\" the length-incentivized reward (Rlen) by generating repetitive low-entropy sequences that satisfy the length requirement without achieving genuine state expansion, thereby reducing exploration efficiency. Table 6. The Ablation study of Θ. MATH Olympiad AIME AMC AIME25 Avg. GRPO w/Clip-Higher +LIE (Θ = 6) +LIE (Θ = 10) +LIE (Θ = 15) 86.4 86.0 88.8 86.8 54.1 55.4 54.1 56. 25.2 29.2 30.4 28.5 61.8 65.4 65.5 63.9 22.2 21.5 24.5 22. 49.9 51.5 52.7 51.5 Impact of Repetition Magnitude β The redundancy penalty weight β (Equation 13) serves as hyperparameter for balancing exploration capacity and efficiency. As illustrated in the training dynamics in Figure 15, we observe that the final reasoning accuracy is robust to the choice of β. Both β = 0.3 and β = 0.6 achieve nearly identical convergence in accuracy, indicating that the penalty magnitude does not affect the models fundamental problem-solving capability. The primary effect of higher β is the promotion of conciseness. As shown in the length (L) and entropy plots, β = 0.6 leads to shorter reasoning trajectories and more rapid decrease in policy entropy compared to β = 0.3. This suggests that while smaller β allows for more loose exploration, larger β encourages the model to find more efficient, high-density reasoning paths to the correct answer. 17 Learn to Explore In-Context via Length-Incentivized Reinforcement Learning C.6. Ablation Study of the Reward Shaping Recipe Figure 15. Training dynamics of different β. To understand the contribution of each component in our final recipe (Equation 14), we perform an exhaustive ablation study. As illustrated in Figure 16, the interaction between the length reward (Rlen) and the redundancy penalty (Rred) is key to unlocking effective in-context exploration. Length reward (Rlen) as the capacity driver: (Proposition 4.1) The brown curves (GSPO + Rlen) demonstrate that explicitly incentivizing length successfully breaks the Shallow Exploration Trap, leading to surge in the total distinct state count Ccontext. However, this comes at significant cost: the distinct ratio Rcontext plummets, and length becomes extremely large. This indicates that without constraints, the model tends to satisfy the length requirement through thought padding generating repetitive or low-value tokens. Redundancy reward (Rred) as the quality filter: Conversely, when only the redundancy penalty is added (red curves), the model maintains high Rcontext, but the average trajectory length remains low. This suggests that while the penalty prevents repetition, it does not provide the necessary drive to navigate deeper into the state space for complex reasoning. Synergistic effect of the LIE recipe: Our complete recipe (GSPO + LIE, purple curves) achieves superior balance. By combining Rlen and Rred, we create synergistic effect: Rlen provides the computational budget to explore, while Rred ensures that this budget is spent on diverse and meaningful reasoning states. As result, the model exhibits steady growth in both Ccontext and length, while maintaining healthy Rcontext. This structured exploration ultimately leads to the highest and most stable accuracy gains among all variants. In summary, the LIE recipe is not merely sum of its parts; it is mechanism that converts raw sequence length into effective reasoning depth. Figure 16. Training dynamics of different reward components. D. Case Study To analyze the internal reasoning dynamics, we follow the principle of (Feng et al., 2025) and employ Claude-3.7-Thinking to extract the underlying reasoning graph structure from the models generated responses. This allows us to visualize and 18 Learn to Explore In-Context via Length-Incentivized Reinforcement Learning quantify the \"thought process\" beyond simple token sequences or behaviors. Qualitative Analysis. We select representative problem from the AIME benchmark where the baseline fails while our method succeeds. As visualized in Figure 17a, the GSPO baseline (Left) follows linear and shallow reasoning path. It attempts direct derivation but fails to cross-check its intermediate steps, quickly converging to an incorrect answer (324). In contrast, the model trained with our LIE recipe (Figure 17b) exhibits significantly richer reasoning topology. Crucially, the length incentive activates in-context exploration behaviors: the model spawns alternative hypotheses (Alternative Approach: Using Polar Form), performs explicit self-verification (Step 5: Verification), and successfully identifies and corrects calculation error (Calculation Error). This capacity to branch out and backtrack allows the model to recover from initial pitfalls and ultimately derive the correct solution (540). Quantitative Structural Metrics To verify if this observation holds statistically, we compute the average Depth (maximum path length in the reasoning graph) and Width (average branching factor) across the 40 samples from AIME. As shown in Table 7, our method consistently expands the reasoning structure: Increased Depth (13.8 14.75): The model constructs longer logical chains, enabling deeper decomposition of complex problems. Increased Width (2.15 2.30): More importantly, the increased width indicates that the model is not merely padding the response with empty tokens. Instead, it engages in broader exploration by considering multiple parallel hypotheses or verification paths. These structural metrics confirm that explicitly incentivizing response length effectively translates into broader search horizon, enabling the model to navigate the state space more thoroughly. Table 7. Exploration width and depth metrics."
        },
        {
            "title": "Depth Width",
            "content": "GSPO GSPO + LIE 13.8 14.75 2.15 2.3 19 Learn to Explore In-Context via Length-Incentivized Reinforcement Learning (a) GSPO (b) GSPO + LIE Figure 17. case from AIME on GSPO and our recipe models."
        }
    ],
    "affiliations": [
        "Institute of Advanced Technology, Westlake Institute for Advanced Study",
        "Nanjing University",
        "Shanghai AI Laboratory",
        "The Chinese University of Hong Kong",
        "Westlake University",
        "Zhejiang University"
    ]
}