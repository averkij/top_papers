{
    "paper_title": "Schoenfeld's Anatomy of Mathematical Reasoning by Language Models",
    "authors": [
        "Ming Li",
        "Chenrui Fan",
        "Yize Cheng",
        "Soheil Feizi",
        "Tianyi Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfeld's Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), a scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as a critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 5 9 9 9 1 . 2 1 5 2 : r Schoenfelds Anatomy of Mathematical Reasoning by Language Models Ming Li*, Chenrui Fan*, Yize Cheng*, Soheil Feizi, Tianyi Zhou University of Maryland, College Park {minglii,cfan42,yzcheng,sfeizi}@umd.edu, tianyi.david.zhou@gmail.com (cid:135) Project: https://github.com/MingLiiii/ThinkARM"
        },
        {
            "title": "Abstract",
            "content": "Large language models increasingly expose reasoning traces, yet their underlying cognitive structure and steps remain difficult to identify and analyze beyond surface-level statistics. We adopt Schoenfelds Episode Theory as an inductive, intermediate-scale lens and introduce ThinkARM (Anatomy of Reasoning in Models), scalable framework that explicitly abstracts reasoning traces into functional reasoning steps such as Analysis, Explore, Implement, Verify, etc. When applied to mathematical problem solving by diverse models, this abstraction reveals reproducible thinking dynamics and structural differences between reasoning and non-reasoning models, which are not apparent from token-level views. We further present two diagnostic case studies showing that exploration functions as critical branching step associated with correctness, and that efficiency-oriented methods selectively suppress evaluative feedback steps rather than uniformly shortening responses. Together, our results demonstrate that episode-level representations make reasoning steps explicit, enabling systematic analysis of how reasoning is structured, stabilized, and altered in modern language models."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable performance on complex reasoning tasks (OpenAI, 2024c; Marjanovic et al., 2025; Comanici et al., 2025; Qwen Team, 2025a), particularly when generating explicit chains of thought (CoT) (Wei et al., 2023). Consequently, evaluation of reasoning models has predominantly focused on outcome-oriented metrics such as accuracy, solution length, or aggregate token counts (Lightman et al., 2023; Jiang et al., 2025a). While these measures are effective for comparing final performance, they provide limited insight into how these models *Co-First Authors. 1 organize their reasoning traces and how different reasoning behaviors emerge across models. It is still unclear which parts of generated chainof-thought correspond to problem understanding, exploration, execution, or verification, making it difficult to interpret model behavior beyond surface statistics. This opacity is particularly salient when studying overthinking (Chen et al., 2025b; Fan et al., 2025; Kumar et al., 2025), where longer or more elaborate reasoning does not necessarily translate into improved correctness (Feng et al., 2025b), yet the underlying thinking dynamics and structure are hard to characterize systematically. Various reasoning behaviors that are often discussed conceptually, e.g., abstract planning versus concrete execution, open-ended exploration versus evaluative checking, lack rigorous formulation and quantitative comparison across models. To better interpret such reasoning traces, natural question is whether they contain meaningful structure at an intermediate level of abstraction beyond individual tokens. Motivated by prior work (Li et al., 2025c) that introduced Schoenfelds Episode Theory (Schoenfeld, 1985) as framework for characterizing problem-solving behaviors, we adopt episode-level representations as an inductive lens for analyzing LLM reasoning traces. Episode Theory conceptualizes problem solving in terms of functional episodes, thereby providing an interpretable intermediate-scale representation that bridges low-level token statistics and high-level reasoning intents. condensed example is shown in Figure 1. While earlier work (Li et al., 2025c) first leveraged this theory to reasoning models, their scope is limited to one model and one dataset. Thus, it remains unclear whether such an episode-level abstraction can reveal systematic, reproducible structure in LLM reasoning at scale. In this work, we build upon this foundation and extend episode-level annotation to broader, comparative setting, usFigure 1: condensed example of reasoning trace that is annotated in our framework. Each sentence in response is tagged with one of the eight episode categories. ing it to examine what principal patterns emerge when diverse reasoning traces are viewed through theory-grounded abstraction. Our study is organized around three research questions. RQ1: Do reasoning traces exhibit consistent linguistic and dynamic structure when viewed through episodelevel representations? RQ2: How do reasoning dynamics differ across models and reasoning styles, as indicated by episode sequencing and transition patterns? RQ3: Can we utilize this framework to analyze the correlation of thinking patterns to final correctness, and the structural differences between overthinking and efficient models?"
        },
        {
            "title": "To address",
            "content": "these questions, we apply ThinkARM (Anatomy of Reasoning in Models), an episode-level annotation framework grounded in Schoenfelds Episode Theory (Harskamp and Suhre, 2007), to large-scale analysis of mathematical reasoning traces from diverse set of LLMs. Concretely, we curate corpus of 410, 991 sentences generated by 15 models solving 100 problems from subset of Omni-MATH (Gao et al., 2024). To support reliable automated analysis, we construct human-verified gold set of 7, 067 sentences and evaluate multiple state-of-the-art LLMs as automatic annotators, and finally select GPT-5 (OpenAI, 2025b) for full-scale episode labeling due to its strongest agreement with human annotations. This pipeline enables consistent, sentence-level episode annotation at scale and sets up controlled setting for comparing reasoning dynamics across models and methods. Contributions. We extend cognitive scienceinspired episode annotation framework to an automatic, scalable, sentence-level representation that supports large-scale analysis of reasoning traces and conduct systematic study of reasoning dynamics across diverse set of LLMs. Moreover, we demonstrate the practical utility of episode-level 2 representations through downstream case studies on correctness and efficiency, illustrating how reasoning dynamics can be analyzed beyond outcomebased metrics. Key Findings. 1. When reasoning traces are analyzed at the episode level, functional progression from abstract reasoning to concrete execution, and finally to evaluative control, consistently emerges. Episodes associated with analysis and exploration use more abstract, conceptual language and decrease steadily as reasoning progresses, while execution-oriented episodes dominate the middle of the trace through sustained concrete operations. In contrast, verificationrelated episodes are characterized by evaluative and meta-level language and increase toward the end of the reasoning process. 2. Comparing and reasoning non-reasoning models, the difference is not merely how many tokens they generate, but how reasoning is structured. Non-reasoning models allocate most of their response trace to execution, with episode transitions largely following feed-forward pattern toward implementation. In contrast, reasoning models distribute effort across analysis, exploration, execution, and iterative verification, and exhibit frequent Explore-Monitor/Verify loops. 3. Through our correctness-oriented case study, we find that exploration reflects uncertainty and serves as critical branching point: correct solutions more often route exploration into monitoring or re-analysis, whereas incorrect solutions tend to continue execution or terminate prematurely after exploration. 4. Through our efficiency-oriented case study, we find that different efficient reasoning methods selectively suppress evaluation-oriented episodes and feedback loops, leading to varying degrees of divergence from the reasoning patterns of the base model. Episode-level analysis thus reveals which episodes can be removed to gain efficiency, beyond token-level pruning. Together, these findings make explicit range of intermediate-scale reasoning behaviors that are often discussed intuitively but rarely characterized structurally."
        },
        {
            "title": "2 The ThinkARM Framework",
            "content": "In this section, we formalize our analytical methodology. We first ground our approach in cognitive science theory in mathematical problem solving and then detail the implementation of ThinkARM, scalable, automated pipeline to quantify the reasoning dynamics of LLMs with high precision."
        },
        {
            "title": "2.1 Schoenfeld’s Episode Theory",
            "content": "To scientifically dissect the reasoning traces of LLMs, we ground our framework in Schoenfelds Episode Theory (Schoenfeld, 1985), seminal framework in cognitive science originally developed to decode the black box of human mathematical problem-solving. Schoenfelds theory is descriptive, derived from the rigorous analysis of hundreds of hours of videotaped think-aloud protocols. By observing students and mathematicians tackling mathematical problems, they identified that the performance is not distinguished by superior domain knowledge alone, but by the dynamic regulation of that knowledge. The theory frames problem-solving as temporally ordered sequence of episodes that reveal the solvers evolving goal structure and metacognitive decisions1. Schoenfelds framework ultimately consists of 7 episodes: the original 6 episodes, including Read, Analyze, Plan, Implement, Explore, and Verify, plus later-added Monitor episode that specifically captures the solvers metacognitive behaviors. Episode theory has since become foundational analytic lens in mathematics-education research and in studies of human reasoning, offering fine-grained vocabulary for tracing cognitive control and strategy shifts (Harskamp and Suhre, 2007). Li et al. (2025c) first applied this theory to annotate reasoning traces of LLMs with thinking behaviors. However, their work is limited to single-task scenarios 1More discussion on mathematical problem solving in cognitive science can be found in Appendix Reasoning Non-Reasoning Overall Model Acc Kappa Acc Kappa Acc Kappa GPT-4.1 GPT-5 Gemini-2.5-Flash Gemini-2.5-Pro 85.75 86.02 82.45 80.53 82.39 82.54 78.21 75.60 89.34 89.34 87.16 84.43 85.36 85.35 82.35 78.62 86.10 86.33 82.90 80.89 82.74 82.85 78.67 75. Table 1: Performance of candidate annotators on the human-annotated gold set. GPT-5 shows the highest agreement with human annotations overall and is used for further large-scale annotation. and single-model case studies and does not provide systematic analysis of the reasoning dynamics of diverse LLMs. 2.2 Refined Taxonomy While Schoenfelds original taxonomy captures key functional stages of human problem solving, it does not include an explicit state for structural convergence. In the context of LLM reasoning, where models are trained to produce final answer in prescribed format, this distinction becomes practically important. We therefore extend the taxonomy by introducing an Answer episode, resulting in Eight episodes, which allows us to explicitly identify when the model commits to producing final solution and to analyze convergence behavior separately from verification or monitoring, as shown in Figure 1. Prior work (Li et al., 2025c) explored episodebased annotation using hierarchical schemes that combine paragraph-level and sentence-level labels. In this study, we adopt sentence-level annotation only, as it provides uniform granularity that facilitates large-scale aggregation, transition analysis, and comparison across models. This design choice reflects trade-off toward scalability and analytical simplicity, rather than claim about the relative merits of different annotation granularities."
        },
        {
            "title": "2.3 Data Collection and Gold Annotation",
            "content": "To construct diverse and representative dataset for our analysis, we sample problems from OmniMATH using its domain annotations. Specifically, we stratify the full benchmark by domain and select problems from each group proportionally, aiming to preserve domain coverage while maintaining diversity in problem types and difficulty. This procedure yields subset of 100 problems spanning broad range of mathematical topics. We then collect reasoning traces utilizing 15 widely used LLMs2 to solve each problem, result2The complete list of models is in Appendix 3 Figure 2: Word clouds visualizing the most frequent semantic tokens for each cognitive episode. The distinct lexical distributions highlight the semantically separable cognitive patterns captured by ThinkARM. ing in 1, 500 responses and total of 410, 991 sentences. The collected traces include both thinking and answering tokens for native reasoning models and their distilled variants, and only answering tokens for standard instruction-following baselines and proprietary reasoning models without accessible thinking tokens. This diverse model coverage enables comparative analysis across model families and reasoning paradigms. To support the reliable evaluation of automated episode annotation, we construct human-verified gold set. From the 100 sampled problems, we select 9 representative problems following the same domain-based grouping strategy and manually annotate3 all resulting reasoning traces using the refined episode taxonomy and guidebook. This process produces 7, 067 annotated sentences, which we use as the gold standard for testing the effectiveness of the automatic annotators and selecting the annotation model used in our large-scale analysis."
        },
        {
            "title": "2.4 The ThinkARM Framework",
            "content": "In ThinkARM, we utilize LLMs for automatic annotation. During the automatic annotation process, we provide the detailed annotation guidebook, which consists of the definition and examples of each episode, along with the previous contexts and annotated categories. Furthermore, when annotating each sentence, we not only ask models to generate the annotated category, but also generate the justifications for each annotation to ensure the reliability of the annotation4. For the annotation model selection, we evaluate the performance of several state-of-the-art models, including GPT-4.1 (OpenAI, 2025a), GPT-5 (OpenAI, 2025b), Gemini-2.5-Flash (Comanici et al., 2025), and Gemini-2.5-Pro, on the gold standard 3The detailed annotation guidebook is in Appendix 4The detailed ThinkARM Framework is in Appendix and the annotation prompt is in Appendix annotated traces. The performance of the annotation models is shown in Table 1, containing the accuracy and kappa scores on the traces from reasoning and non-reasoning models. Based on the performance, we select GPT-5 for the full-scale annotation and utilize its results for further analysis."
        },
        {
            "title": "3 Episode Patterns of Reasoning Models",
            "content": "With ThinkARM, we now move beyond surfacelevel performance metrics to empirically analyze the cognitive dynamics of current LLMs."
        },
        {
            "title": "3.1 Episode Divergence",
            "content": "Our analysis asks whether different functional modes of reasoning, often discussed intuitively, manifest as separable patterns in language. Figure 2 visualizes the most frequent tokens associated with each episode and shows that episodes occupy distinct lexical regions, suggesting that ThinkARM captures meaningful behavioral differences rather than superficial variation. Analyze vs. Implement: clear separation emerges between abstract reasoning and concrete execution. Language associated with Analyze emphasizes conceptual and structural elements of the problem (e.g., coprime, boundary), reflecting the construction and manipulation of an abstract In contrast, Implement problem representation. is dominated by procedural and symbol-level language (e.g., variable names and concrete values), indicating step-by-step execution of chosen approach. Thus, ThinkARM captures genuine shift from conceptual thinking to solid implementation in reasoning behavior. Verify vs. Monitor: Two qualitatively different forms of reflection also emerge. Verify is characterized by decisive evaluative language (e.g., wrong, helpful), indicating explicit checking of logical correctness or validity. Monitor, by contrast, is as4 Figure 3: Thinking dynamics of cognitive episodes reveal three-phase heartbeat pattern of reasoning models: (1) Initialization, dominated by Read, Analyze, and Plan; (2) Execution, where Implement peaks; and (3) Convergence, characterized by surge in Verify and Monitor before the final Answer. sociated with meta-level expressions of uncertainty or progress tracking (e.g., confusing, messy), reflecting awareness of the state of the reasoning process rather than evaluation of specific step. These two behaviors that are separated lexically support treating verification and monitoring as distinct reasoning modes, which are important in later temporal and structural analyses. Plan vs. Explore: Forward-looking reasoning also exhibits two separable modes. Plan is dominated by directive verbs and goal-oriented language (e.g., calculate, formalize), signaling commitment to concrete strategy. In contrast, Explore is marked by tentative and hypothesis-driven expressions (e.g., suspect, maybe), reflecting openended search over possible solution paths. This separation highlights exploration as distinct behavioral mode characterized by uncertainty, rather than merely an early stage of execution."
        },
        {
            "title": "3.2 Temporal Dynamics",
            "content": "By normalizing each reasoning trace to 0-100% progress scale5, we analyze how episode frequencies evolve over the generation (Figure 3). consistent coarse-grained temporal organization emerges, captured by ThinkARM, across reasoning models. We refer to this pattern as the cognitive heartbeat of machine reasoning. Early-stage scaffolding: Episodes associated with initial scaffolding (Read, Analyze, Plan, Ex5Details in Appendix plore) exhibit distinct decay patterns. Read drops sharply after the beginning, while Analyze and Plan decay more gradually, indicating that structural reasoning persists beyond the first few steps rather than being confined to fixed planning prefix. Explore is typically front-loaded as well, consistent with early hypothesis search that narrows as execution proceeds. Mid-stage execution: Implement exhibits characteristic bell shape trajectory, peaking in the middle of the trace. This pattern suggests that concrete execution occupies the longest continuous region of the reasoning process, providing the backbone onto which other behaviors (e.g., exploration and verification) attach. The model shifts from high-level strategizing to mechanical execution (symbolic manipulation and calculation) as the core engine of the response. Late-stage convergence: Verify increases steadily over progress, and Monitor follows Ushaped profile with elevated mass near the beginning and end. Together, these trends indicate that evaluative and process-regulatory behaviors become more prominent as generation approaches completion, rather than appearing only as final end check. Finally, Answer remains near-zero for most of the trace and rises sharply near the end, reflecting that answer commitment is concentrated in the terminal stage. 5 Model Percentage (%) Token Count (#) Read Ana Plan Imp Exp Ver Ans Mon Read Ana Plan Imp Exp Ver Ans Mon DeepSeek-R1 R1-Distill-Qwen-1.5B R1-Distill-Qwen-7B R1-Distill-Qwen-32B QwQ-32B Phi-4-Reasoning Qwen-3-32B (R) GPT-4o Gemini-2.0-Flash Qwen-2.5-32B Phi-4 Qwen-3-32B Gemini-2.5-Flash GPT-o1-mini GPT-o3-mini 3.47 4.18 3.80 3.12 3.03 4.16 2.88 8.49 5.63 5.85 4.87 6.87 2.64 9.42 6.17 30.75 26.93 25.82 25.23 24.53 27.83 25.58 22.66 19.24 22.78 15.66 13.03 28.04 22.97 31. 5.67 4.20 4.44 4.75 6.28 6.42 7.79 12.96 4.33 13.58 6.66 6.82 6.47 8.64 8.04 36.35 34.39 36.47 36.49 35.71 37.51 36.86 40.77 61.17 45.62 64.12 65.76 52.28 42.01 35. 9.21 13.23 12.24 10.94 13.98 11.56 11.56 2.58 0.97 0.88 0.21 0.97 0.42 0.71 0.97 10.16 11.43 11.97 10.67 11.05 8.91 10.54 4.84 4.45 3.39 4.27 3.42 6.75 4.26 4. 2.86 3.80 3.40 6.85 3.32 2.75 3.04 6.94 4.14 7.25 3.89 2.82 3.05 11.32 9.90 1.54 1.84 1.86 1.95 2.09 0.87 1.76 0.77 0.08 0.65 0.31 0.30 0.35 0.67 3. 321 414 336 300 378 415 372 59 63 41 64 182 60 53 63 2844 2666 2280 2422 3068 2771 3308 156 215 160 206 345 642 129 524 416 392 456 785 639 1007 89 48 95 88 181 148 49 83 3363 3404 3220 3502 4466 3734 4767 281 682 320 844 1742 1197 237 852 1309 1081 1050 1748 1151 1495 18 11 6 3 26 10 4 10 940 1131 1057 1025 1382 887 1363 33 50 24 56 91 155 24 265 376 300 658 416 273 393 48 46 51 51 75 70 64 102 142 182 165 187 261 86 227 5 1 5 4 8 8 4 Avg Tok 9250 9897 8831 9598 12504 9957 12931 690 1115 700 1316 2649 2290 563 1027 Table 2: Episode-level allocation across model families. Standard instruction-following models are strongly Implement-heavy, whereas reasoning models exhibit more balanced allocation with substantial mass on Analyze, Explore, and Verify. Distilled models largely preserve the teachers allocation profile across scales. 3.3 Episode Coverage Table 2 reports the episode-level allocation of (i) generated tokens. We group models into: open-source reasoning models where full reasoning traces are available, (ii) standard instructionfollowing models evaluated on their direct responses, and (iii) proprietary reasoning models where only the final responses are observable. From Doing to Thinking: clear allocation gap emerges between reasoning and non-reasoning models. Standard instruction-following models allocate the majority of tokens to Implement, with minimal mass assigned to Explore, Verify, or Monitor. In contrast, reasoning models exhibit more balanced profile, allocating substantially more budget to Analyze and Explore, and maintaining nontrivial allocation to Verify. This suggests that the distinguishing factor is not merely response length, but how the generation budget is distributed across reasoning behaviors. The Nature of Non-reasoning Responses: For proprietary reasoning models where only the final formulated answers are accessible, the episode allocation from the observable outputs is much closer to the non-reasoning group than to opentrace reasoning models. In other words, the externally visible responses exhibit limited allocation to explorationand verification-associated episodes. Distillation Preserves Structure: Within the distilled series, we observe that episode allocation profiles remain similar across model sizes. R1Distill-Qwen-1.5B exhibits an episode distribution close to its teacher DeepSeek-R1 despite large differences in parameter count. This indicates that distillation can transfer not only answers but also vs. Reasoning (Answer) vs. Non-Reasoning Models Idx Score Pattern Idx Score Pattern 1 2 3 4 5 6 7 8 9 10 0.2862 Exp-Mon 0.2815 Ver-Exp 0.2771 Mon-Exp 0.2754 Exp-Plan 0.2605 Exp-Ver 0.2579 Exp-Imp 0.2568 Exp-Ana-Exp 0.2567 Exp-Plan-Imp 0.2560 Imp-Ver-Exp 0.2519 Ana-Exp 1 2 3 4 5 6 7 8 9 10 0.2510 Exp-Mon 0.2405 Mon-Exp 0.2073 Exp-Ana-Imp 0.2033 Ana-Exp-Ana 0.2028 Exp-Ana-Exp 0.1974 Exp-Ana 0.1917 Ver-Exp 0.1838 Exp-Ver 0.1830 Exp-Mon-Exp 0.1806 Ana-Exp-Mon Table 3: Top discriminative episode -grams ranked by Mutual Information (MI). Left: patterns that distinguish reasoning models reasoning trace from the final answer segment. Right: patterns that distinguish reasoning traces from non-reasoning model responses. Higher MI indicates stronger association between the pattern and the source group. episode-level reasoning structure, as reflected in allocation patterns."
        },
        {
            "title": "3.4 Transition Patterns",
            "content": "Beyond marginal episode allocations, episode transitions characterize how reasoning behaviors interact with each other. We convert each trace into symbolic episode sequence (e.g., Read Plan Implement) and use an information-theoretic criterion to identify distinctive local structures6. Concretely, we compute the Mutual Information (MI) between the presence of episode -grams and the source group, and extract the most discriminative patterns. Formally, the MI between an episode -gram pattern and the source group variable is defined as: I(P ; G) = (cid:88) (cid:88) p(p, g) log p{0,1} gG p(p, g) p(p) p(g) , (1) 6Details in Appendix Positive Contributors Negative Contributors # Feature β # Feature 1 2 3 4 5 6 7 8 9 10 Ratio (Mon) Trans (Exp Mon) +0.41 Trans (Exp Ana) +0.31 Trans (Mon Ana) +0.28 Trans (Read Ver) +0.27 +0.22 Ratio (Think) Trans (Ans Ver) +0.22 +0.21 Ratio (Ver) Trans (Read Mon) +0.18 Trans (Read Read) +0.17 +0.16 Ratio (Exp) Trans (Exp Ver) Trans (Exp Ans) Count (Total) Trans (Imp Read) Trans (Imp Imp) Trans (Ana Mon) Trans (Ver Read) Trans (Ver Plan) 1 2 3 4 5 6 7 8 9 10 Trans (Mon Read) β -0.54 -0.45 -0.41 -0.36 -0.32 -0.28 -0.26 -0.25 -0.25 -0.22 Table 4: Top predictive episode-level features for correctness in diagnostic Lasso logistic regression case study. Features are ranked by their coefficient magnitude (β), where the sign and magnitude of β indicate the direction and relative strength of association with correctness under the fitted model. where indicates the presence of given episode -gram in trace, and denotes the source group. After obtaining the top-k discriminative patterns, we use conditional probability to determine the attribution of the feature to different classes. Reasoning vs. Answer Tokens: Comparing the reasoning portion of reasoning model to the final formulated answer tokens, we find that the most discriminative patterns are short feedback loops involving Explore, Monitor, and Verify  (Table 3)  . In particular, Exp-Mon and Mon-Exp rank among the top patterns, indicating frequent alternation between exploration and process monitoring during the reasoning trace. Patterns such as Ver-Exp further suggest that verification is often followed by renewed exploration rather than immediate convergence. In contrast, the final answer tokens are characterized by more feed-forward transitions with limited recurrence of these evaluative loops. Reasoning vs. Non-Reasoning Models: When comparing reasoning traces to responses from standard instruction-following models, we observe an overlapping set of discriminative patterns: non-reasoning responses are dominated by feedforward transitions, where exploration-monitoringverification loops are much less prevalent. This indicates that the gap between reasoning and nonreasoning models is reflected not only in how much time is allocated to different behaviors, but also in the presence of recurrent transitions that interleave exploration with evaluation."
        },
        {
            "title": "4.1 How Episodes Correlate to Correctness",
            "content": "To demonstrate the utility of our framework, we conduct correctness-oriented case study examining how episode-level patterns are associated with solution correctness. Using stratified sample of 500 reasoning traces from the 5 representative open-source reasoning models, we formulate binary classification setting that predicts answer correctness from quantitative cognitive features extracted from episode annotations7. Methodology We map each reasoning trace to feature vector consisting of three components: (i) global statistics (total token count, thinking-token count, and thinking-token ratio), (ii) episode intensities (token ratios for each of the eight episodes), and (iii) transition features (the flattened 8 8 episode transition matrix capturing frequencies of state shifts). Then, we fit Lasso-regularized logistic regression model to predict the correctness of reasoning trace: = (cid:88) (cid:2)yi log ˆyi + (1 yi) log(1 ˆyi)(cid:3) + λw1, (2) where ˆy = σ(wx), σ() denotes the sigmoid function and the ℓ1 penalty encourages sparsity in the coefficient vector. Table 4 reports the top features ranked by coefficient magnitude (β). Here, each coefficient β corresponds to weight in for specific episode-level feature. positive (negative) β indicates that higher values of the feature are associated with increased (decreased) likelihood of correct answer under this model, and the magnitude of β reflects its relative importance among the selected features. Results The most predictive positive features highlight how exploratory behavior is resolved during successful reasoning. In particular, Explore Monitor and Explore Analyze rank among the strongest positive coefficients, suggesting that correct solutions tend to route exploratory uncertainty into meta-level monitoring and renewed conceptual analysis. Similarly, Monitor Analyze indicates that monitoring is often followed by additional analysis rather than immediate execution, consistent with an uncertainty-to-reasoning redirection pattern. Verification-related transitions also appear as informative signals at different points in the trace. Read Verify and Answer Verify suggest that traces associated with correctness more frequently include explicit checking both near the beginning and near the end. On the negative side, higher Explore ratio is strong risk indicator, suggesting that sustained 7Details in Appendix Model Percentage (%) Token Count (#) Avg Read Ana Plan Imp Exp Ver Ans Mon Read Ana Plan Imp Exp Ver Ans Mon Tok R1-Distill-Qwen-1.5B 4.18 26.93 4.20 34. 13.23 11.43 3.80 1.84 L1 (1.5B) (Aggarwal and Welleck, 2025a) ThinkPrune (1.5B) (Hou et al., 2025) Qwen-1.5B (Arora and Zanette, 2025) 9.33 6.48 5. 18.34 20.75 28.32 6.01 5.16 4.69 34.03 39.39 36.90 15.14 10.74 8.76 6.99 8.37 9.94 8.52 7.31 4. 1.64 1.80 1.64 414 227 263 272 2666 447 841 1505 147 209 249 3404 1309 1131 829 1597 1960 369 436 170 339 528 376 208 297 246 182 9897 40 73 2437 4054 5312 Table 5: Cognitive behavior divergence of different efficient reasoning methods. L1 and ThinkPrune drastically reduce the budget for Verify and Analyze, while the last one maintains distribution closer to the baseline. vs. L1 vs. ThinkPrune vs. Alpha Idx Score Pattern Idx Score Pattern Idx Score Pattern 0.1506 N-V-N 1 0.3762 N-V-N 1 0.2491 V-N-V 2 0.1465 V-N-I 2 0.1240 V-N 0.2476 M-N-V 3 3 0.1200 I-N-V 4 0.2291 V-N 4 0.1120 E-V-N 5 0.2261 M-V 5 0.1073 V-N-V 6 0.2233 N-M 6 0.1070 V-N-E 7 0.2144 I-V-M 7 0.1018 I-V-N 8 0.2075 V-M 8 0.0924 N-I-V 9 0.2017 I-N-M 9 10 0.0881 V-M-P 10 0.1949 V-N-I 0.1039 M-E 1 0.0928 E-N-I 2 0.0919 M-E-I 3 0.0911 I-N-M 4 0.0897 N-M-I 5 0.0836 E-V-I 6 0.0789 V-A-M 7 0.0746 I-M-N 8 0.0701 E-I 9 10 0.0692 I-N-V Table 6: Cognitive patterns that are lost in efficiency models compared to the baseline. L1 shows high distinctive scores, indicating significant loss of complex verification loops (V-N-V). Conversely, Alpha shows much lower divergence scores, suggesting it preserves the baselines topological structure more effectively. exploration without subsequent stabilization is associated with incorrect outcomes. Two additional failure-associated patterns emerge. First, Explore Verify reflects verification applied before exploration has been consolidated into coherent hypothesis. Second, Implement Read indicates breakdowns during execution that coincide with reverting to problem re-reading, signal of disrupted reasoning flow. This study illustrates how episode-level representations can surface interpretable transitionlevel signatures associated with correctness, complementing outcome-based evaluation with structured diagnostic view of reasoning behavior. the baseline, L1 and ThinkPrune substantially reduce the budget assigned to evaluative and conceptual episodes (e.g., Verify and Analyze) and shift the profile toward more Implement-heavy allocation. In contrast, Arora and Zanette (2025) maintains an allocation profile closer to the baseline, retaining substantial Verify and Analyze budget. These patterns suggest that efficiency methods are not uniformly compressive: they can induce qualitatively different redistributions of reasoning behaviors. To further localize which transition-level structures are most affected, Table 6 reports the most distinctive episode -grams suppressed by each efficiency strategy relative to the baseline by computing the Mutual Information. L1 exhibits high divergence scores (peaking at 0.37), particularly for loop-like patterns such as N-V-N (AnalyzeVerifyAnalyze) and V-N-V, indicating that recurrent verification loops are strongly attenuated. ThinkPrune shows similar but generally weaker suppression of such loop structures. By contrast, Arora and Zanette (2025) yields much lower divergence scores (peaking around 0.10), suggesting that it preserves more of the baseline transition topology while still reducing overall cost. Overall, this case study illustrates that efficiency is not behaviorally neutral: strategies that achieve similar surface-level reductions in length can correspond to markedly different changes in episode-level allocation and transition structure."
        },
        {
            "title": "5 Conclusion",
            "content": "Efficient reasoning methods for LLMs are often evaluated primarily through surface metrics such as response length, leaving it unclear which reasoning behaviors are being altered. In this case study, we use ThinkARM to characterize how different efficiency paradigms reshape episode-level dynamics. Specifically, we compare baseline model (R1-Distill-Qwen-1.5B) against three representative strategies: (1) L1 (Aggarwal and Welleck, 2025a), (2) ThinkPrune (Hou et al., 2025), and (3) model proposed by Arora and Zanette (2025). Table 5 summarizes how episode-level token allocation changes for these methods. Compared to In this work, we introduce ThinkARM as an inductive, intermediate-scale framework that abstracts reasoning traces into functional reasoning steps grounded in cognitive theory. Through large-scale empirical analysis, we show that this abstraction makes previously opaque reasoning structures explicit, revealing consistent temporal organization and interpretable diagnostic patterns related to correctness and efficiency. Our framework provides principled lens for analyzing, comparing, and diagnosing reasoning behavior in modern language models."
        },
        {
            "title": "Limitations",
            "content": "Large-scale episode annotation relies on an automatic annotator, which may introduce labeling noise despite strong agreement with human annotations on verified gold set. Moreover, our experiments focus primarily on mathematical problem solving; extending episode-level analysis to other domains and reasoning settings remains an important direction for future work."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, and 8 others. 2024. Phi-4 technical report. https://arxiv.org/ abs/2412.08905. ArXiv preprint 2412.08905, Dec 2024. Pranjal Aggarwal and Sean Welleck. 2025a. L1: Controlling how long reasoning model thinks with reinforcement learning. Preprint, arXiv:2503.04697. Pranjal Aggarwal and Sean Welleck. 2025b. L1: Controlling how long reasoning model thinks arXiv preprint with reinforcement learning. arXiv:2503.04697. Janice Ahn, Rishu Verma, Renze Lou, Di Liu, Rui Zhang, and Wenpeng Yin. 2024. Large language models for mathematical reasoning: Progresses and challenges. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: Student Research Workshop, pages 225237, St. Julians, Malta. Association for Computational Linguistics. Daman Arora and Andrea Zanette. 2025. Training language models to reason efficiently. arXiv preprint arXiv:2502.04463. Maciej Besta, Julia Barth, Eric Schreiber, Ales Kubicek, Afonso Catarino, Robert Gerstenberger, Piotr Nyczyk, Patrick Iff, Yueling Li, Sam Houliston, Tomasz Sternal, Marcin Copik, Grzegorz Kwasniewski, Jürgen Müller, Łukasz Flis, Hannes Eberhard, Hubert Niewiadomski, and Torsten Hoefler. 2025. Reasoning language models: blueprint. Preprint, arXiv:2501.11223. Paul C. Bogdan, Uzay Macar, Neel Nanda, and Arthur Conmy. 2025. Thought anchors: Which llm reasoning steps matter? Preprint, arXiv:2506.19143. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wanxiang Che. 2025a. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. Preprint, arXiv:2503.09567. Xingyu Chen, Jiahao Xu, Tian Liang, Zhiwei He, Jianhui Pang, Dian Yu, Linfeng Song, Qiuzhi Liu, Mengfei Zhou, Zhuosheng Zhang, Rui Wang, Zhaopeng Tu, Haitao Mi, and Dong Yu. 2025b. Do not think that much for 2+3=? on the overthinking of o1-like llms. Preprint, arXiv:2412.21187. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, Luke Marris, Sam Petulla, Colin Gaffney, and et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. Preprint, arXiv:2507.06261. Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, Jiarui Yuan, Huayu Chen, Kaiyan Zhang, Xingtai Lv, Shuo Wang, Yuan Yao, Xu Han, Hao Peng, Yu Cheng, and 4 others. 2025. Process reinforcement through implicit rewards. Preprint, arXiv:2502.01456. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, and etc. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Preprint, arXiv:2501.12948. Chenrui Fan, Ming Li, Lichao Sun, and Tianyi Zhou. 2025. Missing premise exacerbates overthinking: Are reasoning models losing critical thinking skill? arXiv preprint arXiv:2504.06514. Gongfan Fang, Xinyin Ma, and Xinchao Wang. 2025. Thinkless: Llm learns when to think. Preprint, arXiv:2505.13379. Sicheng Feng, Gongfan Fang, Xinyin Ma, and Xinchao Wang. 2025a. Efficient reasoning models: survey. Preprint, arXiv:2504.10903. Yunzhen Feng, Julia Kempe, Cheng Zhang, Parag Jain, and Anthony Hartshorn. 2025b. What characterizes effective reasoning? revisiting length, review, and structure of cot. Preprint, arXiv:2509.19284. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, and 1 others. 2024. Omnimath: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985. Carole Greenes. 1995. Mathematics learning and knowing: cognitive process. Journal of Education, 177(1):85106. E. Harskamp and C. Suhre. 2007. Schoenfelds problem solving theory in student controlled learning environment. Computers & Education, 49(3):822839. 9 Bairu Hou, Yang Zhang, Jiabao Ji, Yujian Liu, Kaizhi Qian, Jacob Andreas, and Shiyu Chang. 2025. Thinkprune: Pruning long chain-of-thought Preprint, of llms via reinforcement arXiv:2504.01296. learning. Ming Li, Zhengyuan Yang, Xiyao Wang, Dianqi Li, Kevin Lin, Tianyi Zhou, and Lijuan Wang. 2025b. What makes reasoning models different? follow the reasoning leader for efficient decoding. arXiv preprint arXiv:2506.06998. Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, and Hongsheng Li. 2025a. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. Preprint, arXiv:2502.09621. Gangwei Jiang, Yahui Liu, Zhaoyi Li, Qi Wang, Fuzheng Zhang, Linqi Song, Ying Wei, and Defu Lian. 2025b. What makes good reasoning chain? uncovering structural patterns in long chain-ofthought reasoning. Preprint, arXiv:2505.22148. Priyanka Kargupta, Shuyue Stella Li, Haocheng Wang, Jinu Lee, Shan Chen, Orevaoghene Ahia, Dean Light, Thomas L. Griffiths, Max Kleiman-Weiner, Jiawei Han, Asli Celikyilmaz, and Yulia Tsvetkov. 2025. Cognitive foundations for reasoning and their manifestation in llms. Preprint, arXiv:2511.16660. David R. Krathwohl. 2002. revision of blooms taxonomy: An overview. Theory into Practice, 41(4):212 218. Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, and Eugene Bagdasarian. 2025. Overthink: Slowdown attacks on reasoning llms. Preprint, arXiv:2502.02542. Ana Kuzle. 2013. Patterns of metacognitive behavior during mathematics problem-solving in dynamic geometry environment. International Electronic Journal of Mathematics Education, 8(1):2040. Tamera Lanham, Anna Chen, Ansh Radhakrishnan, Benoit Steiner, Carson Denison, Danny Hernandez, Dustin Li, Esin Durmus, Evan Hubinger, Jackson Kernion, Kamile Lukošiute, Karina Nguyen, Newton Cheng, Nicholas Joseph, Nicholas Schiefer, Oliver Rausch, Robin Larson, Sam McCandlish, Sandipan Kundu, and 11 others. 2023. Measuring faithfulness in chain-of-thought reasoning. Preprint, arXiv:2307.13702. Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, and Tianyi Zhou. 2024a. Selective reflectiontuning: Student-selected data recycling for LLM instruction-tuning. In Findings of the Association for Computational Linguistics ACL 2024, pages 16189 16211, Bangkok, Thailand and virtual meeting. Association for Computational Linguistics. Ming Li, Yanhong Li, and Tianyi Zhou. 2025a. What happened in LLMs layers when trained for fast vs. slow thinking: gradient perspective. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3201732154, Vienna, Austria. Association for Computational Linguistics. Ming Li, Nan Zhang, Chenrui Fan, Hong Jiao, Yanbin Fu, Sydney Peters, Qingshu Xu, Robert Lissitz, and Tianyi Zhou. 2025c. Understanding the thinking process of reasoning models: perspective from schoenfelds episode theory. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1827818299, Suzhou, China. Association for Computational Linguistics. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2024b. From quantity to quality: Boosting LLM performance with self-guided data selection for instruction tuning. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 75957628, Mexico City, Mexico. Association for Computational Linguistics. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Preprint, 2023. arXiv:2305.20050. Lets verify step by step. Wei Liu, Ruochen Zhou, Yiyun Deng, Yuzhen Huang, Junteng Liu, Yuntian Deng, Yizhe Zhang, and Junxian He. 2025. Learn to reason efficiently with adaptive length-based reward shaping. Preprint, arXiv:2505.15612. Sara Vera Marjanovic, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han Lù, and 1 others. 2025. Deepseek-r1 thoughtology: Lets think about llm reasoning. arXiv preprint arXiv:2504.07128. John Mason, Leone Burton, and Kaye Stacey. 2010. Thinking Mathematically, second edition. Pearson Education, Harlow, England. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. Preprint, arXiv:2501.19393. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, and et al. 2024. Gpt-4o system card. Preprint, arXiv:2410.21276. OpenAI. 2024a. Learning to reason with llms. OpenAI. 2024b. OpenAI o1-mini System Card. 10 Wei Xiong, Hanning Zhang, Chenlu Ye, Lichang Chen, Nan Jiang, and Tong Zhang. 2025. Self-rewarding correction for mathematical reasoning. Preprint, arXiv:2502.19613. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. Preprint, arXiv:2502.03387. Joseph B. W. Yeo and Ban Har Yeap. 2010. Characterising the cognitive processes in mathematical investigation. International Journal for Mathematics Teaching and Learning, pages 110. OpenAI. 2024c. OpenAI o1 System Card. OpenAI. 2025a. Introducing GPT-4.1 in the API. OpenAI. 2025b. OpenAI GPT-5 System Card. OpenAI. 2025c. OpenAI o3-mini System Card. George Pólya. 1945. How to Solve It. Princeton University Press, Princeton. Qwen Team. 2025a. Qwen3: Think deeper, act faster. Qwen Team. 2025b. Qwq-32b: Embracing the power of reinforcement learning. Alan H. Schoenfeld. 1985. Mathematical Problem Solving. Academic Press. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. 2024. Scaling llm test-time compute optimally can be more effective than scaling model parameters. Preprint, arXiv:2408.03314. Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Shaochen Zhong, Na Zou, Hanjie Chen, and Xia Hu. 2025. Stop overthinking: survey on efficient reasoning for large language models. Preprint, arXiv:2503.16419. Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, and et al. 2024. Gemini: family of highly capable multimodal models. Preprint, arXiv:2312.11805. Qwen Team. 2024. Qwen2.5: party of foundation models. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-thought prompting elicits reasoning in large language models. Preprint, arXiv:2201.11903. Xuyang Wu, Jinming Nian, Ting-Ruen Wei, Zhiqiang Tao, Hsin-Tai Wu, and Yi Fang. 2025. Does reasoning introduce bias? study of social bias evaluation and mitigation in llm reasoning. Preprint, arXiv:2502.15361. Violet Xiang, Chase Blagden, Rafael Rafailov, Nathan Lile, Sang Truong, Chelsea Finn, and Nick Haber. 2025. Just enough thinking: Efficient reasoning with adaptive length penalties reinforcement learning. Preprint, arXiv:2506.05256. 11 13 13 13 13 13 15 15 15 15 16 16 16 18 19 19"
        },
        {
            "title": "Table of Contents for Appendix",
            "content": "A Related Work A.1 Cognitive Theories of Math Problem Solving . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Large Reasoning Models . . A.3 Efficient Reasoning Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 Reasoning Analysis Methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Implementation Details B.1 Full Model List . . B.2 Temporal Dynamics Details . . B.3 Transition Patterns Details . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Episode-level Diagnostics of Correctness Details . C.1 Feature Engineering. . C.2 Model Specification. C.3 Full Feature Coefficients. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Detailed ThinkARM Framework"
        },
        {
            "title": "E Refined Annotation Guidebook",
            "content": "E.1 Label Definitions and Guidelines . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Important Considerations for Annotators . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "F Annotation Prompt",
            "content": ""
        },
        {
            "title": "A Related Work",
            "content": "A.1 Cognitive Theories of Math Problem Solving Analyzing human problem-solving has evolved from broad cognitive taxonomies to domain-specific frameworks. Early models like Blooms Taxonomy (Krathwohl, 2002) categorized cognitive processes hierarchically but failed to capture the iterative nature of mathematical problem-solving. Similarly, instructional frameworks such as Pólya (1945) four-phase model and subsequent refinements by Mason et al. (2010) and Yeo and Yeap (2010) provided valuable pedagogical scaffolding but lacked the finegrained operationalization required for rigorous empirical annotation. Even more detailed models like Greenes (1995), while emphasizing metacognition, remained too sequential to effectively code the nonlinear dynamics often observed in real-world reasoning tasks. Schoenfeld (1985) episode theory offers robust, empirically validated scheme for coding behaviors into distinct episodes such as Reading, Analysis, Exploration, and Verification. Unlike prescriptive models, Schoenfelds framework explicitly captures the strategic decisions and metacognitive controlor lack thereofthat determine problem-solving success (Kuzle, 2013). This granular focus on cognitive transitions makes it particularly suitable for analyzing AI-generated reasoning, which often struggles with self-regulation. Consequently, Schoenfelds model provides the necessary precision to systematically annotate and evaluate the complex, often non-linear reasoning traces investigated in this study. Li et al. (2025c) first applied this framework to analyze the reasoning process of large language models. However, they havent conducted systematic fine-grained analysis of the episode-level patterns of annotated reasoning traces or compared the episode-level patterns between different models. A.2 Large Reasoning Models The surge in LLM development has catalyzed substantial efforts to bolster their reasoning skills (Ahn et al., 2024; Besta et al., 2025; Chen et al., 2025a). Most research has centered on enhancing these abilities via post-training methods. For instance, reinforcement learning has been utilized to steer models towards superior reasoning strategies (Shao et al., 2024; Xiong et al., 2025; Cui et al., 2025). Furthermore, instruction tuning using rigorously selected, high-quality datasets has proven effective in boosting performance (Li et al., 2024b,a; Ye et al., 2025; Muennighoff et al., 2025; Li et al., 2025a). Moreover, Snell et al. (2024); OpenAI (2024a) have shown that scaling up test time compute can also improve the reasoning capabilities of models. Yet, while these models demonstrate remarkable gains in mathematical reasoning on benchmarks, there remains notable gap in systematically understanding and quantifying how these enhancements alter model behavior. A.3 Efficient Reasoning Methods Overthinking (Chen et al., 2025b; Fan et al., 2025) has been an identified issue of reasoning models. They tend to produce excessive intermediate steps or consider unnecessary details, which can lead to inefficient reasoning. To address this issue, several methods (Sui et al., 2025; Feng et al., 2025a) have been proposed to encourage the reasoning models to reason more efficiently. Specifically, L1 (Aggarwal and Welleck, 2025a) proposes simple reinforcement learning method that optimizes for accuracy and adherence to user-specified length constraints. ThinkPrune (Hou et al., 2025) offers simple solution that continuously trains the long-thinking LLMs via reinforcement learning with an added token limit. Arora and Zanette (2025) train reasoning models to dynamically allocate inference-time compute based on task complexity. More works (Fang et al., 2025; Liu et al., 2025; Xiang et al., 2025; Li et al., 2025b) have been proposed to encourage the efficient reasoning of models. However, currently, very limited work explicitly analyzes how these methods are different in behavior or episode-level patterns. Our work is the first to systematically analyze the episode-level patterns of these methods in comparison to the baseline reasoning models. A.4 Reasoning Analysis Methods Chain-of-thoughts (CoT) (Wei et al., 2023) can significantly elicit the reasoning capabilities of models. Various works have studied the different properties of CoT, including bias (Wu et al., 2025), faithful13 ness (Lanham et al., 2023), and redundancy (Chen et al., 2025b). Specifically, for o1-like reasoning models that have particularly long reasoning traces and showcase more system-II level reasoning abilities, efforts have been made to uncover the structural patterns of CoT (Jiang et al., 2025b). Bogdan et al. (2025) introduced black-box method that measures each sentences counterfactual importance to understand the sentence-level importance in long CoT chains. Feng et al. (2025b) introduced graph view of CoT to extract structure and identified an effective statistic that correlated to the correctness of the reasoning trace. Li et al. (2025c) and Kargupta et al. (2025) incorporated theories from cognitive psychology to label the episodes of CoT in analogy of human problem-solving processes. Our work, built upon (Li et al., 2025c), is the first to systematically analyze the statistical patterns of reasoning traces grounded in cognitive theories."
        },
        {
            "title": "B Implementation Details",
            "content": "B.1 Full Model List We conduct our analysis on variety of reasoning and non-reasoning models. The reasoning models include DeepSeek-R1 (DeepSeek-AI et al., 2025), DeepSeek-R1-Distill-Qwen, QwQ-32B (Qwen Team, 2025b), Phi4 (Abdin et al., 2024), Qwen3-32B (Qwen Team, 2025a), Gemini-2.5-Flash (Comanici et al., 2025), GPT-o1-mini (OpenAI, 2024b) and GPT-o3-mini (OpenAI, 2025c). The non-reasoning models include GPT-4o (OpenAI et al., 2024), Gemini-2.0-Flash (Team et al., 2024), Qwen2.5-32B (Team, 2024) and the non-reasoning mode of Phi4 and Qwen3-32B. We also study some efficient reasoning models including L1 (Aggarwal and Welleck, 2025b), ThinkPrune (Hou et al., 2025), and models released by Arora and Zanette (2025). B.2 Temporal Dynamics Details To investigate the temporal dynamics of reasoning phases across model responses, we analyze how the distribution of cognitive phases evolves over the course of generation. For each annotated response, we divide the sequence into equal-sized temporal bins based on token position, where = 25 in our analysis. Within each bin, we compute the frequency of tokens belonging to each reasoning phase category. Specifically, for response with total length tokens, we define bin size as = L/B. Each token at position is assigned to bin = t/, ensuring uniform temporal coverage across responses of varying lengths. For each category and bin b, we count the number of tokens belonging to that category, yielding raw frequency distribution fc,b. To enable comparison across responses with different phase compositions, we normalize the frequency distribution within each response. For category in response r, we compute the normalized frequency in bin as: (r) c,b = (r) c,b b=1 (r) c,b (cid:80)B (3) where the denominator represents the total number of tokens of category in response r. This normalization ensures that the distribution sums to 1 for each category within each response, allowing us to examine the relative temporal positioning of phases independent of their absolute frequency. B.3 Transition Patterns Details After annotation, each response can be represented with sequence of cognitive phases. To study which phase combinations are specific to certain model or kinds of models (e.g. Deepseek vs others, reasoning vs non-reasoning), we conduct the phase-based N-gram analysis that treats each phase as gram and investigate what combinations of grams are most significant patterns of type of models. Specifically, we use letter to represent each phase (e.g. for READ), and each response becomes string. We use the mutual information to identify the most discriminative patterns between two groups: MI(g) = (cid:88) i,j p(xi, yj) log2 p(xi, yj) p(xi) p(yj) (4) where xi {present, absent} indicates whether n-gram appears in sequence, yj {Group 1, Group 2} denotes the model group membership, and p(xi, yj), p(xi), and p(yj) are the joint and marginal probabilities computed from the 2 2 contingency table of n-gram occurrences across the two groups. 15 Episode-level Diagnostics of Correctness Details In this section, we provide the detailed implementation of the correctness diagnostic analysis presented in Section 4.1. We formulate the problem as binary classification task, where we predict the correctness of reasoning trace based on its episode-level cognitive features. C.1 Feature Engineering. For each reasoning trace, we extract comprehensive set of features capturing global statistics, episode intensity, and transition dynamics. Let = {s1, s2, . . . , sN } be the sequence of sentences in trace, where each sentence si is associated with an episode tag ei and token count ti. The set of episode categories consists of the 8 refined episodes: Read, Analyze, Plan, Implement, Explore, Verify, Monitor, Answer. We compute the following feature groups: Global Statistics: Total Tokens: The total number of tokens in the response, (cid:80) ti, estimated using the GPT-4 tokenizer (cl100k_base). Think Ratio: The proportion of tokens belonging to the reasoning process (excluding the final Answer content) relative to the total tokens. Episode Intensity (Token Ratios): For each episode category E, we compute the proportion of the total budget allocated to it: Ratioc = (cid:80) i:ei=c ti (cid:80) tj This yields 8 features representing the relative dominance of each cognitive behavior. Transition Features: We construct transition matrix capturing the frequency of shifts between reasoning states. We compute the raw count of transitions from episode src to episode tgt: Transsrctgt = 1 (cid:88) i=1 I(ei = src ei+1 = tgt) This results in 8 8 = 64 transition features, capturing the structural flow of reasoning (e.g., Explore Verify). C.2 Model Specification. We employ Lasso-regularized Logistic Regression model to identify the most predictive features while enforcing sparsity for interpretability. The model predicts the probability of correctness p(y = 1x): p(y = 1x) = σ(wx + b) where is the standardized feature vector, are the learned coefficients, and σ() is the sigmoid function. To handle the high-dimensional feature space (particularly the transition matrix) and select only the most robust signals, we use L1 regularization (Lasso). The objective function is: min w,b"
        },
        {
            "title": "1\nM",
            "content": "M (cid:88) j=1 [yj log ˆyj + (1 yj) log(1 ˆyj)] + λw1 where is the number of samples and λ controls the regularization strength. 16 Implementation Details. We use the reasoning traces from 5 representative open-source reasoning models (DeepSeek-R1, DeepSeek-R1-Distill-Qwen-32B, Phi-4, Qwen3-32B, and QwQ-32B). All features are standardized using Z-score normalization before training. This ensures that the magnitude of coefficients directly reflects the relative importance of features. We use the liblinear solver which is well-suited for smaller datasets with L1 regularization. We set the regularization parameter = 0.5 (where = 1/λ) to promote feature selection, and use maximum of 2,000 iterations to ensure convergence. We analyze the learned coefficients w. Features with positive coefficients increase the probability of correct answer, while those with negative coefficients are associated with incorrect outcomes. Features with zero coefficients are pruned by the Lasso regularization, indicating they are less relevant for predicting correctness in this linear approximation. C.3 Full Feature Coefficients. Table 7 shows the full feature coefficients for the Lasso logistic regression model in the diagnostic Lasso logistic regression case study"
        },
        {
            "title": "Negative Contributors",
            "content": "#"
        },
        {
            "title": "Feature",
            "content": "β #"
        },
        {
            "title": "Feature",
            "content": "1 2 3 4 5 6 7 8 9 10 Ratio (Mon) 11 Trans (Read Imp) 12 Trans (Exp Read) 13 Trans (Plan Ver) 14 Trans (Plan Plan) 15 Ratio (Plan) 16 Trans (Imp Mon) 17 Trans (Ana Ver) 18 Trans (Ver Imp) 19 Trans (Mon Ver) 20 Trans (Ana Plan) 21 Trans (Mon Mon) 22 Trans (Plan Read) 23 Trans (Plan Exp) Trans (Exp Mon) +0.41 Trans (Exp Ana) +0.31 Trans (Mon Ana) +0.28 Trans (Read Ver) +0.27 +0.22 Ratio (Think) Trans (Ans Ver) +0.22 +0.21 Ratio (Ver) Trans (Read Mon) +0.18 Trans (Read Read) +0.17 +0.16 +0.15 +0.14 +0.12 +0.12 +0.10 +0.07 +0.07 +0.05 +0.05 +0.04 +0.04 +0.03 +0.01 Ratio (Exp) Trans (Exp Ver) Trans (Exp Ans) Count (Total) Trans (Imp Read) Trans (Imp Imp) Trans (Ana Mon) Trans (Ver Read) Trans (Ver Plan) 1 2 3 4 5 6 7 8 9 10 Trans (Mon Read) 11 Ratio (Ans) 12 Trans (Plan Mon) 13 Trans (Read Ans) 14 Trans (Imp Ans) 15 Trans (Ana Ana) 16 Ratio (Read) 17 Trans (Ans Ana) 18 Trans (Read Exp) 19 Trans (Ver Mon) 20 Trans (Exp Plan) 21 Trans (Plan Imp) 22 Trans (Ans Imp) 23 Trans (Ans Exp) 24 Trans (Imp Ana) 25 Trans (Read Plan) 26 Trans (Ans Mon) 27 Trans (Ver Ana) β -0.54 -0.45 -0.41 -0.36 -0.33 -0.28 -0.26 -0.25 -0.25 -0.22 -0.21 -0.19 -0.18 -0.15 -0.14 -0.13 -0.11 -0.11 -0.08 -0.08 -0.06 -0.05 -0.05 -0.03 -0.03 -0.02 -0. Table 7: The full predictive episode-level features for correctness in the diagnostic Lasso logistic regression case study. Features are ranked by their coefficient (β) magnitude."
        },
        {
            "title": "D Detailed ThinkARM Framework",
            "content": "The detailed ThinkARM Framework is shown in Figure 4. For each batch of sentences in the model response, the annotation model tags the episode category for each sentence based on the guidebook, question, previous context and outputs the rationale and annotation in JSON format as instructed by the format prompt. The labels after batch processing are then concatenated to form the labels for the whole response. The guidebook and prompt template in the figure are in Appendix and Appendix F. Figure 4: The ThinkARM Framework. For each question-response pair, the model response is first segmented into sentences. They are then tagged by the annotation models in batches, along with information about the guidebook, question, context, and format. The guidebook is in Appendix E, the prompt template is in Appendix F."
        },
        {
            "title": "E Refined Annotation Guidebook",
            "content": "In this project, we aim to analyze the reasoning process of current large language models (LLMs) with advanced reasoning capabilities, i.e., Large Reasoning Models (LRMs), based on modified version of Alan Schoenfelds (1985) Episode-Timeline framework for problem-solving. The original Schoenfeld theory was built on hundreds of hours of recorded tapes of students tackling non-routine math problems while being asked to think aloud. Widely regarded as gold-standard framework in mathematics education research, this theory offers rigorously validated, fine-grained lens for dissecting both expert and novice problem-solving strategies. After thorough investigation, we find that the thinking process of LRMs can be well-aligned with the episodes in the theory, as they also follow similar problem-solving processes. Thus, in this project, we aim to annotate the model (solver) responses with these episode categories. To better apply the theory to the analysis of model responses, we utilize sentence-level annotation, which is used to capture the fine-grained behavior of each sentence, including eight categories: Read, Analyze, Plan, Implement, Explore, Verify, Monitor, and Answer. The original Schoenfeld theory only has six categories: Read, Analyze, Plan, Implement, Explore, and Verify. These categories describe the thinking behaviors of how humans solve problem. Later, an additional Monitor category was included in the system to capture behaviors that do not contain specific content but are still important, such as Let me think. Moreover, when trying to apply the theory to analyzing LLM behaviors, we introduce another category, Answer, to represent the sentence that delivers the answer. Thus, in total, there are eight categories. For each sentence, the annotation depends on both the current sentence itself and its context. E.1 Label Definitions and Guidelines 1. Read Definition: This is usually the initial phase, which focuses on extracting or restating the given information, conditions, and the goal of the problem as presented. It involves understanding the question without any inference of strategy or reasoning. Guidelines: Sentences in this category should directly present the content of the original problem statement. Look for phrases that recall or repeat elements of the question. This label is mostly presented for the models initial processing of the problem. Potential Keywords/Indicators: The question asks..., The problem requires..., We are given..., The goal is to..., The choices are..., direct quotes from the problem. Distinguishing Features: This stage is purely about understanding the input, not about processing it or deciding how to solve it. It should not contain content like trying to understand or analyze the question. Avoid labeling sentences as Read if they include any form of analysis or evaluation of the problem. The Read stage usually appears at the beginning of the reasoning. However, it can also appear in the middle of the reasoning, in order to ensure that the question was understood correctly. Example: The question asks us to find the value of in the equation 2x + 5 = 10. 2. Analyze Definition: This stage involves constructing or recalling relevant theories, introducing necessary symbols, and deducing relationships based on the problem statement and existing knowledge. The core activity is explanation or logical inference that sets the stage for the solution but does not involve concrete calculations yet. Guidelines: 19 Sentences should explain the underlying mathematical concepts or principles relevant to the problem. This category includes analysis of either the problem itself or intermediate results. This label applies to logical deductions and inferences made with certainty. Potential Keywords/Indicators: According to..., We can define..., This implies that..., Therefore..., Based on this..., We can infer that..., Lets note that..., Let me observe that..., Lets recall that... Distinguishing Features: The Analyze episode involves certain inferences and explanations, unlike Explore, which shows uncertainty. The actual execution of calculations is mainly in the Implement stage. Analyze does not involve any concrete calculation, which is unlike Implement. The behavior of setting some basic notations should be in the Implement stage, e.g., Let me set = 1, then.... Important Note: Be careful not to include sentences that involve substituting values or performing calculations, as those belong to the Implement stage. Example: According to the Pythagorean theorem, in right-angled triangle, the square of the hypotenuse is equal to the sum of the squares of the other two sides. or If can get the equation in slope-intercept form (y = mx + b), then can plug in = 4 and solve for x, which should be d. 3. Plan Definition: This stage involves announcing the next step or outlining the entire solution strategy. It represents commitment to particular course of action before the actual execution begins. Guidelines: Sentences should clearly state the intended next step or the overall plan. Look for explicit declarations of intent, often using the first person or imperative voice. This stage signifies that decision has been made on how to proceed, and the next step should be related to math problem solving, rather than generally saying lets think about it. Potential Keywords/Indicators: Next, we will..., The next step is to..., We need to..., Lets proceed by..., will now..., The plan is to..., We should first..., To..., do..., The xxx we need/want is..., Lets..., Then/Now calculate/consider.... Distinguishing Features: The Plan phase clearly indicates the intended action, unlike Analyze, which explains concepts, or Explore, which suggests possibilities. It precedes the actual carrying out of the plan in the Implement stage. Note that sentences like Lets denote... are Analyze, because this is introducing new variable, rather than making plan. Sentences like lets verify..., lets test... or lets double-check are Verify. Example: Next, we will differentiate both sides of the equation with respect to x. 20 4. Implement Definition: This stage is the operational phase where the planned strategy is executed. It involves setting up basic notations, performing specific calculations, constructing diagrams, enumerating possibilities, or coding solutions using numerical values, symbols, or geometric objects. Guidelines: Sentences should describe the actual steps taken to solve the problem. Look for mathematical operations, substitutions, and the generation of intermediate results. This stage is about doing the math. Potential Keywords/Indicators: Substituting = 2, we get..., Therefore, (1) = 1, Expanding the expression..., The matrix becomes..., Let me set = 1, then..., Lets denote = 10..., actual mathematical equations and calculations. Distinguishing Features: Implement involves concrete actions and calculations, unlike Analyze, which focuses on theoretical explanations, or Plan, which outlines future actions. If conclusion follows the implementation of math, that conclusion is tagged as Implement, such as therefore, the sum of all possible values is 5. Example: Substituting = 3 into the equation, we get 2(3) + 5 = 6 + 5 = 11. 5. Explore Definition: This stage is characterized by generating potential ideas, making guesses, drawing analogies, or attempting trial calculations that might be abandoned later. The model is exploring different avenues without committing to specific solution path. This stage often involves uncertainty. Guidelines: Sentences should suggest alternative approaches or possibilities. Look for tentative language and expressions of uncertainty. This stage involves brainstorming and initial investigations without clear commitment to particular method. Potential Keywords/Indicators: Maybe we can try..., Perhaps we could use..., What if we consider..., Another possibility is..., Could this be related to..., Maybe should..., Maybe there is another way..., Maybe we can try..., Maybe there is better way..., Maybe consider..., Perhaps... is..., Lets try..., Alternatively, maybe use..., Wait, but maybe..., But in soccer, its possible to lose game but still have more total goals?; question marks indicating uncertainty about step. Distinguishing Features: Explore is marked by uncertainty and lack of commitment, unlike Plan, which announces definite course of action. It involves considering various options before settling on specific plan. If sentence contains analyzing the problem, implementing the calculation, or verifying the result or thought, even if it follows sentences like Maybe we can try..., the sentences are not considered Explore at the sentence level, and therefore should not be labeled as Explore. Rather, these sentences are considered Analyze, Implement, or Verify within the Explore episode at the paragraph level. Only sentences like Maybe we can try... will be labeled as Explore at the sentence level. Example: Maybe we can try substituting different values for to see if we can find pattern. 21 6. Verify Definition: This stage involves judging the correctness, effectiveness, or simplicity of the obtained result or the method used. It might include checking the answer, using an alternative method for calculation, or estimating bounds. Guidelines: Sentences should express an evaluation or confirmation of the solution or the process. Look for keywords related to checking, confirming, or validating. This stage ensures the solution and result are accurate and make sense. Potential Keywords/Indicators: Let me double-check..., This is consistent with..., Plugging it back in..., Therefore, the answer is correct., Lets confirm..., Let me check again..., We can confirm this by..., This result seems reasonable because..., The answer is...?, Is the answer...?, Is there any mistake?, Did make mistake?, This is the same/correlated as previous..., But this seems to contradict..., ...lead/arrive to the same answer, Wait, we dont know... yet, Lets try another way to verify..., XXX is possible/impossible. When the following sentences are meant as conclusions, ...is indeed..., ...should be... Distinguishing Features: Verify focuses on evaluating the solution, unlike Implement, which focuses on generating it. It often involves comparing the result with initial conditions or using alternative methods. Example: Let me double-check my calculations: 2 3 + 5 = 11, which matches the previous result. 7. Monitor Definition: This additional category captures sentences that are typically short interjections or expressions indicating the models self-monitoring, hesitation, or reflection at the juncture between different episodes. These often do not contain substantial problem-solving content and are brief pauses in the thought process. Guidelines: Sentences should be short phrases indicating shift in thought or brief pause. Look for expressions of uncertainty, reflection, or transition. This label is for meta-comments that dont fit neatly into the other problem-solving stages. Potential Keywords/Indicators: Hmm..., Wait..., Let me think., Okay..., Lets see., Hold on., But wait, hold on. Distinguishing Features: Monitor sentences lack the substantive content of the other categories and primarily serve as indicators of the models internal processing flow. They are often very short and act as bridges between more content-heavy stages. In most cases, it should not contain evaluation for previous steps or contain any specific question or solution content. If it contains specific content, e.g., Wait, the problem says..., it should be categorized into others like Read. Example: Wait. 22 8. Answer Definition: This stage is used for sentences that explicitly state an answer or conclusion to the problem. These sentences deliver the result, either as final answer at the end of the response or as an intermediate answer that may be subject to later verification or revision. Note: it should be the answer to the given problem, rather than an intermediate answer for calculation step. Guidelines: Sentences should directly present solution, value, or conclusion in response to the given problem statement. Look for clear, declarative statements that summarize the outcome of the reasoning or calculation. This category applies whether the answer is final or provisional. Potential Keywords/Indicators: The answer is..., Hence, the result is..., So, the final answer is.... Distinguishing Features: Answer sentences are characterized by their directness in providing result to the given problem, unlike Verify, which focuses on checking correctness, or Implement, which details the process of obtaining the result. These sentences often appear at the end of solution but can also occur mid-response as provisional answers. Example: Therefore, the answer is 24. E."
        },
        {
            "title": "Important Considerations for Annotators",
            "content": "Sentence-Level Focus: Annotate each sentence individually based on its primary function within the problem-solving process. Context is Key: While keywords can be helpful, always consider the context of the sentence within the overall response. sentence might contain keyword but function differently based on the surrounding text. Refer to Examples: The examples provided in this guidebook and any additional examples you encounter should serve as valuable references."
        },
        {
            "title": "F Annotation Prompt",
            "content": "We use the prompt template in Figure 5 to annotate the reasoning traces in our study. The detailed content of the guidebook can be found in Appendix E. 24 Prompt Template for Annotation In this project, we aim to analyze the reasoning process of current large language models (LLMs) with advanced reasoning capabilities, i.e., Large Reasoning Models, LRMs, based on modified version of Alan Schoenfelds (1985) \"Episode-Timeline\" framework for problem-solving. Given the model response you need to annotate the sentence-level behavior of the model response with the eight categories: Read, Analyze, Explore, Plan, Implement, Verify, Monitor, and Answer. The [Guidebook] - [End of the Guidebook] section provides the detailed introduction and definition of each category. The [Math Problem] - [End of the Math Problem] section provides math problem. The [Overall Response] - [End of the Overall Response] section provides the overall response of the model to the math problem. The [Previous Context] - [End of the Previous Context] section provides all the previous context of the response that has been annotated and their corresponding labels. The [Input] - [End of the Input] section provides the sentences that need to be annotated. The [Format] - [End of the Format] section provides the format of the output. [Guidebook] (Guidebook Content) [End of Guidebook] [Math Problem] (The Question) [End of the Math Problem] [Previous Context] The previous sentences are:(Previous batch sentences) [End of Previous Context] [Input] The following sentences that you need to classify:([1] xxx [2] xxx ... [batch_num] xxx) [End of Input] [Format] You should format the output in JSON format regarding the index, short rationale and the finegrained class of the indexed sentence. The format is as follows:\" {sentences: [ {index The index of the sentence, reason: The short reason of the classification, category: The fine-grained class of the sentence}, {index: The index of the sentence, reason: The short reason of the classification, category: The fine-grained class of the sentence}, ... ]}\" [End of Format] Now, annotate the sentences in the [Input] - [End of the Input] section. Refer to the guidebook to make the decision. Strictly follow the index number of the sentence in the [Input] - [End of the Input] section for labeling. You should output the label for batch_num sentences. Figure 5: The prompt template we used to annotate the reasoning episode."
        }
    ],
    "affiliations": [
        "University of Maryland, College Park"
    ]
}