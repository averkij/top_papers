{
    "paper_title": "Robust Watermarking Using Generative Priors Against Image Editing: From Benchmarking to Advances",
    "authors": [
        "Shilin Lu",
        "Zihan Zhou",
        "Jiayou Lu",
        "Yuanzhi Zhu",
        "Adams Wai-Kin Kong"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. In this work, we introduce W-Bench, the first comprehensive benchmark designed to evaluate the robustness of watermarking methods against a wide range of image editing techniques, including image regeneration, global editing, local editing, and image-to-video generation. Through extensive evaluations of eleven representative watermarking methods against prevalent editing techniques, we demonstrate that most methods fail to detect watermarks after such edits. To address this limitation, we propose VINE, a watermarking method that significantly enhances robustness against various image editing techniques while maintaining high image quality. Our approach involves two key innovations: (1) we analyze the frequency characteristics of image editing and identify that blurring distortions exhibit similar frequency properties, which allows us to use them as surrogate attacks during training to bolster watermark robustness; (2) we leverage a large-scale pretrained diffusion model SDXL-Turbo, adapting it for the watermarking task to achieve more imperceptible and robust watermark embedding. Experimental results show that our method achieves outstanding watermarking performance under various image editing techniques, outperforming existing methods in both image quality and robustness. Code is available at https://github.com/Shilin-LU/VINE."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 2 ] . [ 1 5 7 7 8 1 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "ROBUST WATERMARKING USING GENERATIVE PRIORS AGAINST IMAGE EDITING: FROM BENCHMARKING TO ADVANCES Shilin Lu1, Zihan Zhou1, Jiayou Lu1, Yuanzhi Zhu2, Adams Wai-Kin Kong1 1Nanyang Technological University, 2ETH Zurich {shilin002, zihan010, jiayou001}@e.ntu.edu.sg zyzeroer@gmail.com, adamskong@ntu.edu.sg"
        },
        {
            "title": "ABSTRACT",
            "content": "Current image watermarking methods are vulnerable to advanced image editing techniques enabled by large-scale text-to-image models. These models can distort embedded watermarks during editing, posing significant challenges to copyright protection. In this work, we introduce W-Bench, the first comprehensive benchmark designed to evaluate the robustness of watermarking methods against wide range of image editing techniques, including image regeneration, global editing, local editing, and image-to-video generation. Through extensive evaluations of eleven representative watermarking methods against prevalent editing techniques, we demonstrate that most methods fail to detect watermarks after such edits. To address this limitation, we propose VINE, watermarking method that significantly enhances robustness against various image editing techniques while maintaining high image quality. Our approach involves two key innovations: (1) we analyze the frequency characteristics of image editing and identify that blurring distortions exhibit similar frequency properties, which allows us to use them as surrogate attacks during training to bolster watermark robustness; (2) we leverage large-scale pretrained diffusion model SDXL-Turbo, adapting it for the watermarking task to achieve more imperceptible and robust watermark embedding. Experimental results show that our method achieves outstanding watermarking performance under various image editing techniques, outperforming existing methods in both image quality and robustness. Code is available at https://github.com/Shilin-LU/VINE"
        },
        {
            "title": "INTRODUCTION",
            "content": "The primary function of an image watermark is to assert copyright or verify authenticity. key aspect of watermark design is ensuring its robustness against various image manipulations. Prior deep learning-based watermarking methods (Bui et al., 2023; Tancik et al., 2020; Zhu, 2018) have proven effective at withstanding classical transformations (e.g., compression, noising, scaling, and cropping). However, recent advances in large scale text-to-image (T2I) models (Chang et al., 2023; Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022) have significantly enhanced image editing capabilities, offering wide array of user-friendly manipulation tools (Brooks et al., 2023; Zhang et al., 2024b). These T2I-based editing methods produce highly realistic modifications, rendering the watermark nearly undetectable in the edited versions. This poses challenges for copyright and intellectual property protection, as malicious users can easily alter an artists or photographers work, even with embedded watermarks, to create new content without proper attribution. In this work, we present W-Bench, the first holistic benchmark that incorporates four types of image editing techniques to assess the robustness of watermarking methods, as shown in Figure 1(a). Eleven representative watermarking methods are evaluated on W-Bench. The benchmark encompasses image regeneration, global editing, local editing, and image-to-video generation (I2V). (1) Image regeneration involves perturbing an image into noisy version and then reconstructing it, which can be categorized as either stochastic (Meng et al., 2021; Zhao et al., 2023b) or deterministic (also known as image inversion) (Mokady et al., 2022; Song et al., 2020a). (2) For global"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: (a) Flowchart of the W-Bench evaluation process. (b) Watermarking performance. Each method is illustrated with diamond and four bars. The area of the diamond represents the methods encoding capacity. The y-coordinate of the diamonds center indicates normalized image quality, calculated by averaging the normalized PSNR, SSIM, LPIPS, and FID between watermarked and input images. The x-coordinate represents robustness, measured by the True Positive Rate at 0.1% False Positive Rate (TPR@0.1%FPR) averaged across four types of image editing methods, encompassing total of seven distinct models and algorithms. The four bars are oriented to signify different editing tasks: image regeneration (left), global editing (top), local editing (right), and image-to-video generation (bottom). The length of each bar reflects the methods normalized TPR@0.1%FPR after each type of image editingthe longer the bar, the better the performance. editing, we use models such as Instruct-Pix2Pix (Brooks et al., 2023) and MagicBrush (Zhang et al., 2024b), which take the image and text prompt as inputs to edit images. (3) For local editing, we employ models like ControlNet-Inpainting (Zhang et al., 2023) and UltraEdit (Zhao et al., 2024c), which allow an additional mask input to specify the region to be modified. (4) Additionally, we evaluate watermarking models in the context of image-to-video generation using Stable Video Diffusion (SVD) (Blattmann et al., 2023) to determine whether the watermark remains detectable in the resultant video frames. Although this is not conventional image editing approach, we consider it special case that allows us to identify if the generated videos use copyrighted images. Experimental results (Figure 1(b)) reveal that most previous watermarking models struggle to extract watermarks after images are edited by these methods. StegaStamp (Tancik et al., 2020) and MBRS (Jia et al., 2021) manage to retain watermarks in certain cases, but at the expense of image quality. To this end, we propose VINE, an invisible watermarking model designed to be robust against image editing. Our improvements focus on two key components: the noise layers and the watermark encoder. For the noise layers, straightforward way to train watermarking model robust to image editing would be to incorporate editing processes into the training pipeline. However, this is nearly infeasible for large scale T2I model-based image editing because it requires backpropagating through the entire sampling process, which can lead to memory issues (Salman et al., 2023). Instead, we seek surrogate attacks by analyzing image editing from frequency perspective. The key insight is that image editing tends to remove patterns embedded in high-frequency bands, while those in low-frequency bands are less affected. This property is also observed in blurring distortions (e.g., pixelation and defocus blur). The experiments show that incorporating various blurring distortions into the noise layers can enhance the robustness of the watermarking against image editing. However, this robustness comes at the cost of watermarked image quality, which is limited by the capability of the watermark encoder. To address this, we leverage large-scale pretrained generative model, such as SDXL-Turbo (Sauer et al., 2023), as powerful generative prior, adapting it specifically for the watermarking task. In this context, the watermark encoder functions as conditional generative model, taking original images and watermarks as inputs and generating watermarked images with distinct distribution that can be reliably recognized by the corresponding decoder. By utilizing this strong generative prior, the watermark is embedded more effectively, resulting in both improved perceptual image quality and enhanced robustness. Our contributions are summarized as follows: 1. We present W-Bench, the first comprehensive benchmark designed to evaluate eleven representative watermarking models across various image editing methods: image regeneration,"
        },
        {
            "title": "Preprint",
            "content": "global editing, local editing, and image-to-video generation. This evaluation covers seven widely used editing models and algorithms and demonstrates that current watermarking models are vulnerable to them. 2. We reveal that image editing predominantly removes watermarking patterns in highfrequency bands, while those in low-frequency bands remain less affected. This phenomenon is also observed in certain types of blurring distortion. Such distortions can be used as surrogate attacks to overcome the challenges from using T2I models during training and to enhance the robustness of the watermark. 3. We approach the watermark encoder as conditional generative model and introduce two techniques to adapt SDXL-Turbo, pretrained one-step text-to-image model, for the watermarking task. This powerful generative prior improves both the perceptual quality of watermarked images and their robustness to various image editing. Experimental results demonstrate that our model, VINE, is robust against multiple image editing methods while maintaining high image quality, outperforming existing watermarking models."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Watermarking benchmark. To the best of our knowledge, WAVES (An et al., 2024) is currently the only comprehensive benchmark for evaluating the robustness of deep learning-based watermarking methods against image manipulations driven by large-scale generative models. However, WAVES considers only image regeneration (Zhao et al., 2023b) among prevalent image editing techniques and does not include other T2I-based editing models. In contrast, W-Bench encompasses not only image regeneration but also global editing (Brooks et al., 2023), local editing (Zhang et al., 2023), and image-to-video generation (Blattmann et al., 2023), thereby broadening the scope of our assessment of image editing methods. Furthermore, WAVES evaluates only three watermarking methodsStegaStamp (Tancik et al., 2020), Stable Signature (Fernandez et al., 2023), and TreeRing (Wen et al., 2023). Notably, Stable Signature and Tree-Ring are limited to generated images and cannot be applied to real ones. In contrast, W-Bench is designed to evaluate watermarking models that work with any type of image, thereby enhancing their effectiveness for copyright protection. Robust watermarking. Image watermarking has long been studied for purposes such as tracking and protecting intellectual property (Al-Haj, 2007; Cox et al., 2007; Navas et al., 2008). Recently, deep learning-based methods (Bui et al., 2023; Chen & Li, 2024; Fang et al., 2022; 2023; Jia et al., 2021; Kishore et al., 2021; Luo et al., 2020; 2024; Ma et al., 2022; Tancik et al., 2020; Wu et al., 2023; Zhu, 2018; Zhang et al., 2019; 2021) have demonstrated competitive robustness against wide range of transformations. However, these methods remain vulnerable to image editing powered by large-scale generative models. Three recent studiesEditGuard (Zhang et al., 2024d), RobustWide (Hu et al., 2024), and JigMark (Pan et al., 2024)have begun to develop watermarking models that are robust to such image editing. However, although EditGuard, which replaces the unedited regions of an edited image with corresponding areas of the unedited watermarked version, demonstrates good performance, its application in real-world scenarios could be ineffective if the detector has no access to the unedited watermarked image. In our benchmark, we assume that the detector has access only to the edited image without any additional information. JigMark employs contrastive learning to train watermark encoder and classifier that determines if an image is watermarked, but it does not support decoding free-form messages. Robust-Wide incorporates Instruct-Pix2Pix into the noise layer using gradient truncation, but its generalization to other editing models could be less effective. Moreover, neither JigMark nor Robust-Wide has been open-sourced."
        },
        {
            "title": "3 METHOD",
            "content": "Given an original image xo and watermark w, our goal is to imperceptibly embed the watermark into the image by an encoder E() to obtain watermarked image xw = E(xo, w). The watermark should be accurately extracted by corresponding decoder D() from xw, i.e., = D(xw), even when xw undergoes image editing ϵ(). In Section 3.1, we examine the frequency properties of various image editing methods and identify surrogate attacks that enhance the robustness of watermarking against them. In Section 3.2, we further improve both the robustness and quality of the watermarked image by adapting one-step"
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Process for analyzing the impact of image editing on an images frequency spectrum. In this example, the editing model Instruct-Pix2Pix, denoted as ϵ(), is employed. The function F() represents the Fourier transform, and we visualize its magnitude on logarithmic scale. text-to-image model as the watermark encoder. Additionally, we introduce several techniques to facilitate this adaptation. Section 3.3 details the training loss functions and strategies employed, as well as resolution scaling method used in our experiments. 3.1 FREQUENCY NATURE OF IMAGE EDITING To develop robust watermarking model against image editing, straightforward way is to integrate image editing models in the noise layers between the encoder and decoder during training. However, many prevalent image editing methods are based on diffusion models, which typically involve multiple sampling steps to produce edited images. This can lead to memory issues when backpropagating through the denoising process. Alternative methods, such as gradient truncation (Hu et al., 2024; Yuan et al., 2024), achieve subpar results, and the straight-through estimator (Bengio et al., 2013) fails to converge when training from scratch. Thus, we seek surrogate attacks during training. We start by examining how image editing methods influence the spectrum of an image. Specifically, we conduct three sets of experiments in which symmetric patterns are inserted into the low-, mid-, and high-frequency bands. Figure 2 illustrates the analysis process for pattern inserted into the lowfrequency band. In this procedure, ring-shaped pattern, w, with constant value is embedded in the low-frequency region of the Fourier spectra of the RGB channels of the original image, xo, i.e., F(xo) + w. The inverse Fourier transform is then applied to obtain the watermarked one, xw. The image editing model, denoted as ϵ(), is applied to both the original image xo and the watermarked one xw, producing edited versions ϵ(xo) and ϵ(xw), respectively. Finally, we compute the difference between their Fourier spectra, F(ϵ(xw)) F(ϵ(xo)), to assess how the inserted pattern is affected by the editing process. The details of the used image editing methods are provided in Section 4.1. Figure 3 illustrates that image editing methods typically remove patterns in the mid and highfrequency bands, while low-frequency patterns remain relatively unaffected. This suggests that T2Ibased image editing methods often fail to reproduce intricate mid and high-frequency details. We infer that this occurs because T2I models are trained to prioritize capturing the overall semantic content and structure of images (i.e., primarily the low-frequency components) for aligning with the text prompts. As result, high-frequency patterns are smoothed out during the generation process. To develop robust watermarking model against image editing, it should learn to embed information into the low-frequency bands. To identify effective surrogate attacks, we explore various image distortions, denoted as (), that resemble image editing to some extent. Both image editing and distortion methods preserve the overall image layout and most of the content, although image distortions typically result in lower perceptual quality. Notably, as shown in Figure 3, certain blurring distortions (e.g., pixelation and defocus blur) exhibit similar trend to image editing, where patterns in the low-frequency bands are less affected than those in the midand high-frequency bands. In contrast, widely used distortions, such as JPEG compression and saturation, do not display this behavior. Since these blurring distortions are computationally efficient, we incorporate them with varying severity levels into our noise layers during training. This encourages the model to embed in-"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Impact of various image editing techniques and distortions on the frequency spectra of images. Results are averaged over 1,000 images. Image editing methods tend to remove frequency patterns in the midand high-frequency bands, while low-frequency patterns remain largely unaffected. This trend is also observed with blurring distortions such as pixelation and defocus blur. In contrast, commonly used distortions like JPEG compression and saturation do not exhibit similar behavior in the frequency domain. The analysis of SVD is not included, as it removes all patterns, rendering them invisible to the human eye. discussion on SVD can be found in Section 4.3. formation within the low-frequency bands (see the frequency patterns of each watermarking method in Appendix B). Consequently, the robustness against image editing is enhanced, as demonstrated by the ablation study in Table 2. The complete set of distortions applied in the noise layer includes common distortions such as saturation, contrast, brightness adjustments, JPEG compression, Gaussian noise, shot noise, impulse noise, and speckle noise to counteract image degradation caused by transmission. Additionally, we incorporate various blurring distortionssuch as pixelation, defocus blur, zoom blur, Gaussian blur, and motion blurto resist image editing. 3.2 GENERATIVE PRIOR FOR WATERMARK ENCODING Although incorporating image distortions into the noise layers can enhance robustness against image editing, this improvement comes at the expense of watermarked image quality, which is constrained by the capabilities of the watermark encoder. The watermark encoder can be viewed as conditional generative model, where the conditions include both watermark and detailed image, rather than simpler representations like depth maps, Canny edges, or scribbles. We hypothesize that powerful generative prior can facilitate embedding information more invisibly while enhancing robustness. Thus, we aim to adapt large-scale T2I model as watermark encoder. There are two types of large scale T2I models: multi-step and one-step. Multi-step T2I models complicate the backpropagation of watermark extraction loss and suffer from slow inference speeds. Thus, we use one-step pretrained text-to-image model, SDXL-Turbo (Sauer et al., 2023). To convert SDXL-Turbo into watermark encoder, an effective strategy to incorporate both the input image and the watermark into the model is essential. common strategy for integrating conditions into diffusion models is to introduce additional adapter branches (Mou et al., 2024; Zhang et al., 2023). However, in the one-step generative model, the noise mapthe input to the UNetdirectly determines the final layout of the generated images (Sauer et al., 2023). This contrasts with multistep diffusion models, where the image layout is gradually established during the early sampling stages. Adding an extra conditional branch to the one-step model causes the UNet to receive two sets of residual features, each representing distinct structures. This makes the training process more challenging and results in subpar performance, as demonstrated by the ablation study in Table 2. Instead, as illustrated in Figure 4, we employ condition adaptor to fuse the information from the input image and the watermark (the architecture of the condition adaptor is shown in Figure 9). This fused data is then fed into the VAE encoder to obtain latent features, which are subsequently input into the UNet and VAE decoder to generate the final watermarked image. We also attempted to input the watermark through the text prompt and finetune the text encoder simultaneously, but this approach failed to converge. Thus, during training, the text prompt is set to null prompt. Despite the general effectiveness of SDXL-Turbos VAE, its architecture is not ideally suited for watermarking tasks. The VAE is designed to balance reconstructive and compressive capabilities,"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: The overall framework of our method, VINE. We utilize the pretrained one-step text-toimage model SDXL-Turbo as the watermark encoder. condition adaptor is incorporated to fuse the watermark with the image before passing the information to the VAE encoder. Zero-convolution layers (Zhang et al., 2023) and skip connections are added for better perceptual similarity. For decoding the watermark, we employ ConvNeXt-B (Liu et al., 2022b) as the decoder, with an additional fully connected layer to output 100-bit watermark. Throughout the entire training process, the SDXLTurbo text prompt is set to null prompt. Figure 9 shows the condition adaptor architecture. thus the reconstructive fidelity is traded off for smoother latent spaces and better compressibility. In the context of watermarking, however, the reconstruction capability is crucial to make sure that the watermarked image is perceptually identical to the input image. Thus, we enhance the VAE by introducing skip connections between the encoder and decoder (Figure 4). Concretely, we extract four intermediate activations after each downsampling block in the encoder, pass them through zeroconvolution layers (Zhang et al., 2023), and feed them into the corresponding upsampling block in the decoder. This modification significantly improves the perceptual similarity between the watermarked image and the input image, as indicated in Table 2. To decode the watermark, we utilize ConvNeXt-B (Liu et al., 2022b) as the decoder, with an additional fully connected layer to output 100-bit watermark. 3.3 OBJECTIVE FUNCTION AND TRAINING STRATEGY Objective function. We follow the standard training scheme, which balances the quality of the watermarked image with the effectiveness of watermark extraction under various image manipulations. The total loss function is as follows: LALL = LIMG (xo, xw) + αLBCE (w, w) , where α is trade-off hyperparameter, LBCE is the standard binary cross-entropy loss calculated between the extracted watermark and the ground truth. The image quality loss LIMG is defined as: (1) LIMG = βMSELMSE (γ(xo), γ(xw)) + βLPIPSLLPIPS (xo, xw) + βGANLGAN (xo, xw) , (2) where βMSE, βLPIPS, and βGAN are the weights of the respective loss terms. Here, γ() is differentiable, non-parametric mapping that transforms the input image from RGB color space into the more perceptually uniform YUV color space, LLPIPS (xo, xw) is the perceptual loss, and LGAN (xo, xw) is standard adversarial loss from GAN discriminator Ddisc: LGAN = Exo [log Ddisc(xo)] + Exo,w [log (1 Ddisc(E(xo, w)))] . Training strategy. In the first training stage, we prioritize the watermark extraction loss by setting α to 10 and βMSE, βLPIPS, βGAN each to 0.01. To preserve the generative prior, the UNet and VAE decoder of the SDXL-Turbo, along with the added zero-convolution layers, are frozen. Once the bit accuracy exceeds 0.85, we transition to the second stage, unfreezing all parameters for further training. At this point, the loss weighting factors are adjusted to α = 1.5, βMSE = 2.0, βLPIPS = 1.5, and βGAN = 0.5. The model after the first two stages serves as our base model, dubbed VINE-B. We then fine-tune VINE-B in the third stage by incorporating Instruct-Pix2Pix (Brooks et al., 2023), (3)"
        },
        {
            "title": "Preprint",
            "content": "representative instruction-driven image editing model, into our noise layer. Gradients are backpropagated through straight-through estimator (Bengio et al., 2013). Note that it cannot converge if it is applied directly during the early training stages. The fine-tuned model is referred to as VINE-R. Additional implementation details are provided in Appendix E. Resolution scaling. Different watermarking models are typically trained using fixed input resolution, limiting them to accepting only fixed-resolution inputs during testing. However, in practical applications, supporting watermarking at the original resolution is crucial to preserve the input image quality. Bui et al. (2023) propose method (detailed in Appendix C.1) to adapt any watermarking model to handle arbitrary resolutions without compromising the quality of the watermarked images and their inherent robustness, as demonstrated in Appendices C.2 and C.3. In our experiments, we apply this resolution scaling method to all methods, enabling them to operate at uniform resolution of 512 512, which is compatible with image editing models."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "In W-Bench, we assess the robustness of eleven representative watermarking models against variety of image editing methods, including image regeneration, global editing, local editing, and imageto-video generation. Section 4.1 and Section 4.2 outline the employed image editing methods and the benchmark setup, respectively. Section 4.3 analyzes the benchmarking results. In Section 4.4, we conduct ablation studies to understand the impact of the key components. 4.1 IMAGE EDITING METHODS Image regeneration. Image regeneration involves perturbing an image into noisy version and then reconstructing it. The perturbing process can be either stochastic or deterministic. In the stochastic method (Meng et al., 2021; Nie et al., 2022; Zhao et al., 2023b), random Gaussian noise is introduced to the image, with the noise level typically controlled by timestep ts using common noise schedulers such as VP (Ho et al., 2020), VE (Song & Ermon, 2019; Song et al., 2020b), FM (Lipman et al., 2022; Liu et al., 2022a), and EDM (Karras et al., 2022). The diffusion model then denoises the noisy image starting from timestep ts to produce clean image. In contrast, the deterministic method (Mokady et al., 2022; Song et al., 2020a; Wallace et al., 2022), also known as image inversion, utilizes diffusion model to deterministicly invert clean image into noisy version through multiple sampling steps nd. The image is then reconstructed by applying the same sampling methods over the same number of sampling steps nd. We employ the widely used VP scheduler for stochastic regeneration, testing noise timesteps ts from 60 to 240 in increments of 20. For deterministic regeneration, we utilize the fast sampler DPM-solver (Lu et al., 2022a;b) and evaluate sampling steps nd of 15, 25, 35, and 45. See Appendix F.1 for the effects of image regeneration on images. Global and local editing. Although global editing typically involves stylization, we also consider editing methods guided solely by text prompts. In these cases, unintended background changes frequently occur, regardless of the requested editadding, replacing, or removing objects; altering actions; changing colors; modifying text or patterns; or adjusting object quantities. Even though the edited background often appears perceptually similar to the original, these unintended alterations can compromise the embedded watermark. In contrast, local editing refers to editing models that use region masks as input, ensuring that the area outside the mask remains unchanged in the edited image. We employ Instruct-Pix2Pix (Brooks et al., 2023), MagicBrush (Zhang et al., 2024b), and UltraEdit (Zhao et al., 2024c) for global editing, while ControlNet-Inpainting (Zhang et al., 2023) and UltraEdit are used for local editing. Notably, UltraEdit can accept region mask or operate without it, allowing us to utilize this model for both global and local editing. We use each models default sampler and perform 50 sampling steps to generate edited images. The difficulty of global editing is controlled by the classifier-free guidance scale of text prompts (Ho & Salimans, 2022), which ranges from 5 to 9, while the image guidance is fixed at 1.5. For local editing, difficulty is determined by the percentage of the edited region with respect to the entire image (i.e., the size of the region mask), with intervals set at 1020%, 2030%, 3040%, 4050%, and 5060%. In all cases of local editing, the image and text guidance values are consistently set to 1.5 and 7.5, respectively. Image-to-video generation. In the experiments, we utilize SVD (Blattmann et al., 2023) to generate video from single image. We assess whether the watermark remains detectable in the resulting"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Comparison of watermarking performance in terms of watermarked image quality and detection accuracy across various image editing methods. Quality metrics are averaged over 10,000 images, and the TPR@0.1%FPR for each specific editing method is averaged over 5,000 images. The best value in each column is highlighted in bold, and the second best value is underlined. Abbreviations: Cap = Encoding Capacity; Sto = Stochastic Regeneration; Det = Deterministic Regeneration; Pix2Pix = Instruct-Pix2Pix; Ultra = UltraEdit; Magic = MagicBrush; CtrlN = ControlNet-Inpainting; SVD = Stable Video Diffusion. Method Cap PSNR SSIM LPIPS FID TPR@0.1%FPR (%) (averaged over all difficulty levels) Regeneration Global Editing Local Editing I2V Sto Det Pix2Pix Ultra Magic Ultra CtrlN SVD MBRS (Jia et al., 2021) CIN (Ma et al., 2022) PIMoG (Fang et al., 2022) RivaGAN (Zhang et al., 2019) SepMark (Wu et al., 2023) DWTDCT (Al-Haj, 2007) DWTDCTSVD (Navas et al., 2008) SSL (Fernandez et al., 2022) StegaStamp (Tancik et al., 2020) TrustMark (Bui et al., 2023) EditGuard (Zhang et al., 2024d) VINE-Base VINE-Robust 30 30 30 32 30 30 30 30 100 100 64 100 27.37 43.19 37.72 40.43 35.48 40.46 40.40 41.77 29.65 41.27 37.58 40.51 37.34 0.8940 0.9847 0.9863 0.9702 0.9814 0.9705 0.9799 0.9796 0.9107 0.9910 0.9406 0.9954 0.9934 0.1877 0.0270 0.0289 0.0488 0.0150 0.0136 0.0265 0.0350 0.0645 0.0026 0.0171 0.0029 0. 6.85 1.13 3.43 1.86 1.72 0.24 0.86 3.54 7.61 0.86 0.51 0.08 0.15 99.53 44.85 82.85 10.12 61.21 0.09 3.12 1.76 91.09 9.22 0.09 91.03 99.66 99.35 51.65 71.18 12.50 73.85 0.00 1.43 9.70 92.13 34.20 6.00 99.25 99. 83.50 51.40 72.78 6.22 87.74 0.04 3.82 25.06 93.72 77.72 0.06 96.30 97.46 7.50 17.00 40.14 4.14 51.84 0.06 4.02 10.58 51.24 43.48 1.16 80.90 86.86 88.54 68.38 81.88 33.96 82.58 0.04 30.84 50.10 91.18 85.90 0.24 89.29 94. 99.60 51.28 74.30 34.28 92.94 0.32 24.56 25.28 98.84 76.62 0.18 99.60 99.96 89.16 66.04 64.22 56.92 97.14 0.56 50.04 31.46 99.06 59.78 2.66 89.68 93.04 13.55 2.93 14.33 3.15 8.81 0.01 0.76 3.65 30.85 39.60 0.18 25.44 36. video frames. Since the initial frames closely resemble the input image, we begin our analysis with frame 5 and continue through frame 19, selecting every second frame. 4.2 EXPERIMENTAL SETUP Datasets. We train VINE using the OpenImage dataset (Kuznetsova et al., 2020) at resolution of 256 256. The training details are provided in Appendix E. For evaluation, we randomly sample 10,000 instances from the UltraEdit dataset (Zhao et al., 2024c), each containing source image, an editing prompt, and region mask. The images in UltraEdit dataset are photographs sourced from datasets such as COCO (Lin et al., 2014), Flickr (Young et al., 2014), and ShareGPT4V (Chen et al., 2023). Of these 10,000 samples, 1,000 are allocated for stochastic regeneration, another 1,000 for deterministic regeneration, and 1,000 for global editing. 5,000 samples are designated for local editing. These consist of five sets, each containing 1,000 images, which are edited using mask sizes covering 1020%, 2030%, 3040%, 4050%, and 5060% of the total image area. Additionally, we include 1,000 samples for image-to-video generation and 1,000 for testing conventional distortion, thereby completing the total evaluation set. Baselines. We compare VINE with eleven watermark baselines, all utilizing their officially released checkpoints. These baselines include MBRS (Jia et al., 2021), CIN (Ma et al., 2022), PIMoG (Fang et al., 2022), RivaGAN (Zhang et al., 2019), SepMark (Wu et al., 2023), TrustMark (Bui et al., 2023), DWTDCT (Al-Haj, 2007), DWTDCTSVD (Navas et al., 2008), SSL (Fernandez et al., 2022), StegaStamp (Tancik et al., 2020), and EditGuard (Zhang et al., 2024d). Although the baselines were trained at different fixed resolutions (as detailed in Appendix C), we apply resolution scaling (described in Section 3.3) to standardize all of them to uniform resolution of 512 512. This standardization does not compromise their robustness, as demonstrated in Appendix C.3. Metrics. We evaluate the imperceptibility of watermarking models using standard metrics, including PSNR, SSIM, LPIPS (Zhang et al., 2018), and FID (Parmar et al., 2022). For watermark extraction, it is essential to strictly control the false positive rate (FPR), as incorrectly labeling non-watermarked images as watermarked can be detrimentala concern often overlooked in previous studies. Neither high bit accuracy nor AUROC alone guarantees high true positive rate (TPR) at low FPR levels. Thus, we primarily focus on TPR@0.1%FPR and TPR@1%FPR as our main metrics. Accordingly, both the watermarked and original images are fed into watermark decoders for evaluation. Additionally, we also provide bit accuracy and AUROC for reference. Note that all reported baseline bit accuracies do not include error correction methods, such as BCH (Bose & Ray-Chaudhuri, 1960), which can be applied to all watermarking models. 4.3 BENCHMARKING RESULTS AND ANALYSIS Table 1 summarizes the overall evaluation results. As discussed in Section 4.2, we report TPR@0.1%FPR as the primary metric, with additional metrics provided in Figure 5. Each reported"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: The performance of watermarking methods under (a) Stochastic regeneration, (b) Global editing, and (c) Local editing. Additional results are available in Figure 16. TPR@0.1%FPR value is averaged across total of m1,000 images, where represents the number of difficulty levels for the specific image editing task. The quality metricsPSNR, SSIM, LPIPS, and FIDare calculated for each pair of watermarked and input images and then averaged across all 10,000 image pairs. MBRS and StegaStamp perform well in image regeneration and local editing tasks; however, they have lower image quality. Qualitative comparison is provided in Appendix H. Additionally, MBRS has limited encoding capacity of only 30 bits. Although SepMark, PIMoG, and TrustMark strike better balance between image quality and detection accuracy, their detection accuracy remains unsatisfactory. In contrast, our methods, VINE-B and VINE-R, offer the best trade-off. VINE-B provides superior image quality with slightly lower detection accuracy under image editing, while VINE-R delivers greater robustness by sacrificing some image quality. Figure 5 illustrates the watermarking performance across various difficulty levels for different image editing methods. The evaluation results against representative editing models are displayed, while additional results are available in Appendix I. For image regeneration, both VINE-R and MBRS maintain high TPR@0.1%FPR across various difficulty levels. VINE-B, StegaStamp, and PIMoG perform well when subjected to minor perturbations. It is important to note that stochastic regeneration with large noise perturbation steps can significantly alter the image, as shown in Appendix F.1. Although SSL achieves higher bit accuracy and AUROC than TrustMark, it exhibits lower TPR@0.1%FPR. Further investigation revealed that SSL has high FPR, often producing bit accuracy exceeding 0.7 for unwatermarked images. Therefore, bit accuracy and AUROC alone are insufficient for evaluating watermarking performance. For global editing and local editing, VINE-B, VINE-R, and StegaStamp achieve high TPR@0.1%FPR across various classifier-free guidance scales. Notably, we also use UltraEdit for global editing, which provides better alignment between the edited image and the editing instruc-"
        },
        {
            "title": "Preprint",
            "content": "Table 2: Ablation study examining the impact of key components on image regeneration and global editing. Each configuration builds upon the previous one, with changes highlighted in red. Config Config Config Config Blurring Distortions Ø Watermark Encoder Backbone Condition Skip Pretrained Simple UNet N.A. N.A. N.A. Config Config Config (VINE-B) Config (VINE-R) Config SDXL-Turbo Ø ControlNet Ø Cond. Adaptor Cond. Adaptor Cond. Adaptor Cond. Adaptor Ø Finetune Ø Ø Ø Ø Ø PSNR SSIM LPIPS FID TPR@0.1%FPR (%) Sto Det Pix2Pix Ultra 38.21 35.85 31.24 32.68 36.76 40.51 37.34 35. 0.9828 0.9766 0.9501 0.9640 0.9856 0.9954 0.9934 0.9812 0.0148 0.0257 0.0458 0.0298 0.0102 0.0029 0.0063 0.0137 1.69 2.12 4.67 2.87 0.53 0.08 0.15 1. 54.61 86.85 98.59 90.82 90.86 91.03 99.66 99.67 66.86 92.28 99.29 94.89 94.78 99.25 99.98 99.92 64.24 80.98 96.01 91.86 92.88 96.30 97.46 96. 32.62 62.14 84.60 70.69 70.68 80.90 86.86 84.66 tions compared to instruct-Pix2Pix and MagicBrush (a quantitative analysis of editing models is presented in Appendix F.2). However, methods that perform well for local editing with UltraEdit do not perform satisfactorily when applied to global editing using the same model, as shown in Figure 16(b). This suggests that global editing more significantly degrades watermarks. Image-to-video generation is not form of traditional image editing, but we are interested in whether watermarks can persist in the generated frames. As illustrated in Figure 16(e), the overall detection rate is not high. Upon analyzing I2V generation in the frequency domain, we discovered that this process significantly reduces the intensity of nearly all patterns across all frequency bands, rendering them unobservable to the human eye. We infer this is because the generated video frames alter the image layout as objects or the camera move. In this case, the intensity of the watermarking patterns should be substantially increased, at least to levels exceeding those shown in Figure 6. 4.4 ABLATION STUDY In this section, we showcase the effectiveness of our designs through an extensive ablation study summarized in Table 2. We start with Config A, baseline utilizing simple UNet as the watermark encoder and incorporating only the common distortions outlined in Section 3.1. Building upon this, Config introduces blurring distortions to the noise layer, which significantly enhances robustness against image editing but compromises image quality. Config further refines Config by using straight-through estimator to fine-tune with Instruct-Pix2Pix. This enhances robustness, albeit at the cost of some image quality. Config replaces the UNet backbone with the pretrained SDXLTurbo and integrates image and watermark conditions via ControlNet (Zhang et al., 2023), boosting robustness while degrading image quality due to conflicts from the additional branch. Config substitutes ControlNet with our condition adaptor, restoring image quality to the level comparable with Config while maintaining the robustness of Config D. Config (VINE-B) enhances Config by introducing skip connections and zero-convolution layers, further improving both image quality and robustness. Config (VINE-R) fine-tunes Config using straight-through estimator with Instruct-Pix2Pix, which increases robustness but reduces image quality. Notably, compared to Config C, Config leverages larger model and powerful generative prior, resulting in significant improvements in image quality and modest gains in robustness. Finally, Config is trained with randomly initialized weights instead of pretrained ones while retaining all other settings from Config G, leading to lower image quality (particularly on the FID metric) but no change in robustness."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this work, we introduce W-Bench, the first comprehensive benchmark that incorporates four types of image editing powered by large-scale generative models to evaluate the robustness of watermarking models. Eleven representative watermarking methods are selected and tested on W-Bench. We demonstrate how image editing commonly affects the Fourier spectrum of the image and identify an effective and efficient surrogate to simulate these effects during training. Our model, VINE, achieves outstanding watermarking performance against various image editing techniques, outperforming prior methods in both image quality and robustness. These results suggest that one-step pre-trained models can serve as strong and versatile backbones for watermarking, and that powerful generative prior enhances information embedding in more imperceptible and robust manner. Limitations. While our method delivers exceptional performance against common image editing tasks powered by generative models, its effectiveness in I2V generation remains limited. Moreover, our model is larger than the baseline models, leading to increased memory requirements and slightly slower inference speeds, as detailed in Table 6."
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This research is supported by the National Research Foundation, Singapore under its Strategic Capability Research Centres Funding Initiative. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not reflect the views of National Research Foundation, Singapore."
        },
        {
            "title": "REFERENCES",
            "content": "Ali Al-Haj. Combined dwt-dct digital image watermarking. Journal of computer science, 3(9): 740746, 2007. Bang An, Mucong Ding, Tahseen Rabbani, Aakriti Agrawal, Yuancheng Xu, Chenghao Deng, Sicheng Zhu, Abdirisak Mohamed, Yuxin Wen, Tom Goldstein, and Furong Huang. WAVES: In Proceedings of the 41st International Benchmarking the robustness of image watermarks. Conference on Machine Learning, pp. 14561492, 2024. Omri Avrahami, Ohad Fried, and Dani Lischinski. Blended latent diffusion. ACM transactions on graphics (TOG), 42(4):111, 2023. Yoshua Bengio, Nicholas Leonard, and Aaron Courville. Estimating or propagating gradients through stochastic neurons for conditional computation. arXiv preprint arXiv:1308.3432, 2013. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Raj Chandra Bose and Dwijendra Ray-Chaudhuri. On class of error correcting binary group codes. Information and control, 3(1):6879, 1960. Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1839218402, 2023. Tu Bui, Shruti Agarwal, and John Collomosse. Trustmark: Universal watermarking for arbitrary resolution images. arXiv preprint arXiv:2311.18297, 2023. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Lin Chen, Jisong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. Weitong Chen and Yuheng Li. Rowsformer: robust watermarking framework with swin transformer for enhanced geometric attack resilience. arXiv preprint arXiv:2409.14829, 2024. Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zeroshot object-level image customization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 65936602, 2024. Hai Ci, Pei Yang, Yiren Song, and Mike Zheng Shou. Ringid: Rethinking tree-ring watermarking for enhanced multi-key identification. arXiv preprint arXiv:2404.14055, 2024. Ingemar Cox, Matthew Miller, Jeffrey Bloom, Jessica Fridrich, and Ton Kalker. Digital watermarking and steganography. Morgan kaufmann, 2007. Yingqian Cui, Jie Ren, Han Xu, Pengfei He, Hui Liu, Lichao Sun, Yue Xing, and Jiliang Tang. Diffusionshield: watermark for copyright protection against generative diffusion models. arXiv preprint arXiv:2306.04642, 2023."
        },
        {
            "title": "Preprint",
            "content": "Ming Ding, Wendi Zheng, Wenyi Hong, and Jie Tang. Cogview2: Faster and better text-to-image generation via hierarchical transformers. Advances in Neural Information Processing Systems, 35:1689016902, 2022. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Han Fang, Zhaoyang Jia, Zehua Ma, Ee-Chien Chang, and Weiming Zhang. Pimog: An effective screen-shooting noise-layer simulation for deep-learning-based watermarking network. In Proceedings of the 30th ACM international conference on multimedia, pp. 22672275, 2022. Han Fang, Yupeng Qiu, Kejiang Chen, Jiyi Zhang, Weiming Zhang, and Ee-Chien Chang. Flowbased robust watermarking with invertible noise layer for black-box distortions. In Proceedings of the AAAI conference on artificial intelligence, volume 37, pp. 50545061, 2023. Pierre Fernandez, Alexandre Sablayrolles, Teddy Furon, Herve Jegou, and Matthijs Douze. Watermarking images in self-supervised latent spaces. In IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). IEEE, 2022. Pierre Fernandez, Guillaume Couairon, Herve Jegou, Matthijs Douze, and Teddy Furon. The stable signature: Rooting watermarks in latent diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2246622477, 2023. Tsu-Jui Fu, Wenze Hu, Xianzhi Du, William Yang Wang, Yinfei Yang, and Zhe Gan. GuidarXiv preprint ing instruction-based image editing via multimodal large language models. arXiv:2309.17102, 2023. Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. Erasing concepts from diffusion models. arXiv preprint arXiv:2303.07345, 2023. Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt image editing with cross attention control. arXiv preprint arXiv:2208.01626, 2022. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. Runyi Hu, Jie Zhang, Tianwei Zhang, and Jiwei Li. Robust-wide: Robust watermarking against instruction-driven image editing. arXiv preprint arXiv:2402.12688, 2024. Zhaoyang Jia, Han Fang, and Weiming Zhang. Mbrs: Enhancing robustness of dnn-based watermarking by mini-batch of real and simulated jpeg compression. In Proceedings of the 29th ACM international conference on multimedia, pp. 4149, 2021. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in neural information processing systems, 35:2656526577, 2022. Gwanghyun Kim, Taesung Kwon, and Jong Chul Ye. Diffusionclip: Text-guided diffusion models for robust image manipulation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 24262435, June 2022. Varsha Kishore, Xiangyu Chen, Yan Wang, Boyi Li, and Kilian Weinberger. Fixed neural network steganography: Train the images, not the network. In International Conference on Learning Representations, 2021. Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig, and Vittorio Ferrari. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. IJCV, 2020."
        },
        {
            "title": "Preprint",
            "content": "Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In Computer Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740755. Springer, 2014. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022a. Yugeng Liu, Zheng Li, Michael Backes, Yun Shen, and Yang Zhang. Watermarking diffusion model. arXiv preprint arXiv:2305.12502, 2023. Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1197611986, 2022b. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. arXiv preprint arXiv:2206.00927, 2022a. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b. Shilin Lu, Yanzhu Liu, and Adams Wai-Kin Kong. Tf-icon: Diffusion-based training-free crossdomain image composition. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 22942305, 2023. Shilin Lu, Zilan Wang, Leyang Li, Yanzhu Liu, and Adams Wai-Kin Kong. Mace: Mass concept erasure in diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 64306440, 2024. Ting Luo, Jun Wu, Zhouyan He, Haiyong Xu, Gangyi Jiang, and Chin-Chen Chang. Wformer: transformer-based soft fusion model for robust image watermarking. IEEE Transactions on Emerging Topics in Computational Intelligence, 2024. Xiyang Luo, Ruohan Zhan, Huiwen Chang, Feng Yang, and Peyman Milanfar. Distortion agnostic deep watermarking. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1354813557, 2020. Rui Ma, Mengxi Guo, Yi Hou, Fan Yang, Yuan Li, Huizhu Jia, and Xiaodong Xie. Towards blind watermarking: Combining invertible and non-invertible mechanisms. In Proceedings of the 30th ACM International Conference on Multimedia, pp. 15321542, 2022. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. Zheling Meng, Bo Peng, and Jing Dong. Latent watermark: Inject and detect watermarks in latent diffusion space. arXiv preprint arXiv:2404.00230, 2024. Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. arXiv preprint arXiv:2211.09794, 2022. Chong Mou, Xintao Wang, Jiechong Song, Ying Shan, and Jian Zhang. Dragondiffusion: Enabling drag-style manipulation on diffusion models. arXiv preprint arXiv:2307.02421, 2023. Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 4296 4304, 2024."
        },
        {
            "title": "Preprint",
            "content": "KA Navas, Mathews Cheriyan Ajay, Lekshmi, Tampy Archana, and Sasikumar. Dwt-dct-svd based watermarking. In 2008 3rd international conference on communication systems software and middleware and workshops (COMSWARE08), pp. 271274. IEEE, 2008. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. Weili Nie, Brandon Guo, Yujia Huang, Chaowei Xiao, Arash Vahdat, and Anima Anandkumar. Diffusion models for adversarial purification. arXiv preprint arXiv:2205.07460, 2022. Minzhou Pan, Yi Zeng, Xue Lin, Ning Yu, Cho-Jui Hsieh, Peter Henderson, and Ruoxi Jia. Jigmark: black-box approach for enhancing image watermarks against diffusion model edits. arXiv preprint arXiv:2406.03720, 2024. Xingang Pan, Ayush Tewari, Thomas Leimkuhler, Lingjie Liu, Abhimitra Meka, and Christian Theobalt. Drag your gan: Interactive point-based manipulation on the generative image manifold. In ACM SIGGRAPH 2023 Conference Proceedings, 2023. Gaurav Parmar, Richard Zhang, and Jun-Yan Zhu. On aliased resizing and surprising subtleties in gan evaluation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1141011420, 2022. Gaurav Parmar, Taesung Park, Srinivasa Narasimhan, and Jun-Yan Zhu. One-step image translation with text-to-image models. arXiv preprint arXiv:2403.12036, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Ahmad Rezaei, Mohammad Akbari, Saeed Ranjbar Alvar, Arezou Fatemi, and Yong Zhang. Lawa: Using latent space for in-generation image watermarking. arXiv preprint arXiv:2408.05868, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 22500 22510, 2023. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems, 35:3647936494, 2022. Hadi Salman, Alaa Khaddaj, Guillaume Leclerc, Andrew Ilyas, and Aleksander Madry. Raising the cost of malicious ai-powered image editing. arXiv preprint arXiv:2302.06588, 2023. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. Yujun Shi, Chuhui Xue, Jun Hao Liew, Jiachun Pan, Hanshu Yan, Wenqing Zhang, Vincent YF Tan, and Song Bai. Dragdiffusion: Harnessing diffusion models for interactive point-based image editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 88398849, 2024. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a."
        },
        {
            "title": "Preprint",
            "content": "Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Matthew Tancik, Ben Mildenhall, and Ren Ng. Stegastamp: Invisible hyperlinks in physical photographs. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 21172126, 2020. Narek Tumanyan, Michal Geyer, Shai Bagon, and Tali Dekel. Plug-and-play diffusion features for text-driven image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 19211930, June 2023. Bram Wallace, Akash Gokul, and Nikhil Naik. Edict: Exact diffusion inversion via coupled transformations. arXiv preprint arXiv:2211.12446, 2022. Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi Pont-Tuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David J. Fleet, Radu Soricut, Jason Baldridge, Mohammad Norouzi, Peter Anderson, and William Chan. Imagen editor and editbench: Advancing and evaluating textguided image inpainting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1835918369, June 2023. Yanghao Wang and Long Chen. Improving diffusion-based data augmentation with inversion spherical interpolation. arXiv preprint arXiv:2408.16266, 2024. Yuxin Wen, John Kirchenbauer, Jonas Geiping, and Tom Goldstein. Tree-ring watermarks: Fingerprints for diffusion images that are invisible and robust. arXiv preprint arXiv:2305.20030, 2023. Xiaoshuai Wu, Xin Liao, and Bo Ou. Sepmark: Deep separable watermarking for unified source In Proceedings of the 31st ACM International Conference on tracing and deepfake detection. Multimedia, pp. 11901201, 2023. Cheng Xiong, Chuan Qin, Guorui Feng, and Xinpeng Zhang. Flexible and secure watermarking for latent diffusion model. In Proceedings of the 31st ACM International Conference on Multimedia, pp. 16681676, 2023. Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and In ProFang Wen. Paint by example: Exemplar-based image editing with diffusion models. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18381 18391, 2023. Zijin Yang, Kai Zeng, Kejiang Chen, Han Fang, Weiming Zhang, and Nenghai Yu. Gaussian shading: Provable performance-lossless image watermarking for diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1216212171, 2024. Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the Association for Computational Linguistics, 2:6778, 2014. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2022. Hangjie Yuan, Shiwei Zhang, Xiang Wang, Yujie Wei, Tao Feng, Yining Pan, Yingya Zhang, Ziwei Liu, Samuel Albanie, and Dong Ni. Instructvideo: instructing video diffusion models with human feedback. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 64636474, 2024. Ziqi Zeng, Chen Zhao, Weiling Cai, and Chenyu Dong. Semantic-guided adversarial diffusion model for self-supervised shadow removal. arXiv preprint arXiv:2407.01104, 2024."
        },
        {
            "title": "Preprint",
            "content": "Chaoning Zhang, Adil Karjauv, Philipp Benz, and In So Kweon. Towards robust deep hiding under non-differentiable distortions for practical blind watermarking. In Proceedings of the 29th ACM international conference on multimedia, pp. 51585166, 2021. Guokai Zhang, Lanjun Wang, Yuting Su, and An-An Liu. training-free plug-and-play watermark framework for stable diffusion. arXiv preprint arXiv:2404.05607, 2024a. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. Advances in Neural Information Processing Systems, 36, 2024b. Kevin Alex Zhang, Lei Xu, Alfredo Cuesta-Infante, and Kalyan Veeramachaneni. Robust invisible video watermarking with attention. arXiv preprint arXiv:1909.01285, 2019. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 38363847, 2023. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern visual editing. Recognition, pp. 90269036, 2024c. Xuanyu Zhang, Runyi Li, Jiwen Yu, Youmin Xu, Weiqi Li, and Jian Zhang. Editguard: Versatile In Proceedings of the image watermarking for tamper localization and copyright protection. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1196411974, 2024d. Chen Zhao, Weiling Cai, Chenyu Dong, and Chengwei Hu. Wavelet-based fourier information interaction with frequency diffusion adjustment for underwater image restoration. arXiv preprint arXiv:2311.16845, 2023a. Chen Zhao, Weiling Cai, Chengwei Hu, and Zheng Yuan. Cycle contrastive adversarial learning with structural consistency for unsupervised high-quality image deraining transformer. Neural Networks, pp. 106428, 2024a. Chen Zhao, Chenyu Dong, and Weiling Cai. Learning physical-aware diffusion model based on transformer for underwater image enhancement. arXiv preprint arXiv:2403.01497, 2024b. Haozhe Zhao, Xiaojian Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. arXiv preprint arXiv:2407.05282, 2024c. Xuandong Zhao, Kexun Zhang, Zihao Su, Saastha Vasan, Ilya Grishchenko, Christopher Kruegel, Giovanni Vigna, Yu-Xiang Wang, and Lei Li. Invisible image watermarks are provably removable using generative ai. arXiv preprint arXiv:2306.01953, 2023b. Yunqing Zhao, Tianyu Pang, Chao Du, Xiao Yang, Ngai-Man Cheung, and Min Lin. recipe for watermarking diffusion models. arXiv preprint arXiv:2303.10137, 2023c. Dewei Zhou, Zongxin Yang, and Yi Yang. Pyramid diffusion models for low-light image enhancement. arXiv preprint arXiv:2305.10028, 2023. Dewei Zhou, You Li, Fan Ma, Zongxin Yang, and Yi Yang. Migc: Multi-instance generation controller for text-to-image synthesis. arXiv preprint arXiv:2402.05408, 2024a. Dewei Zhou, You Li, Fan Ma, Zongxin Yang, and Yi Yang. Migc++: Advanced multi-instance generation controller for image synthesis. arXiv preprint arXiv:2407.02329, 2024b. Dewei Zhou, Ji Xie, Zongxin Yang, and Yi Yang. 3dis: Depth-driven decoupled instance synthesis for text-to-image generation. arXiv preprint arXiv:2410.12669, 2024c."
        },
        {
            "title": "Preprint",
            "content": "J Zhu. Hidden: hiding data with deep networks. arXiv preprint arXiv:1807.09937, 2018. Luyang Zhu, Dawei Yang, Tyler Zhu, Fitsum Reda, William Chan, Chitwan Saharia, Mohammad Norouzi, and Ira Kemelmacher-Shlizerman. Tryondiffusion: tale of two unets. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 4606 4615, June 2023. Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting, 2023. 17 19 19 19 20 20 20 21 24 24 24 24 25 25 25 26"
        },
        {
            "title": "Table of Contents",
            "content": "A Related Work A.1 In-generation image watermarking . . A.2 Generative model-based image editing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Watermarking Frequency Patterns Resolution Scaling . C.1 Method . . . C.2 Impact on watermarked image quality . . C.3 Impact on robustness against distortions . . . . . . . . . . . . . . Condition Adapter Architecture Implementation Details . . . E.1 Pretraining . E.2 Fine-tuning . . . . . . . . . . . . . . . . . . . Analysis of Image Editing Image regeneration . . F.1 F.2 Global editing and local editing . . Image-to-video generation . F.3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Inference Speed and GPU Memory Evaluation Qualitative Comparison Additional Benchmarking Results"
        },
        {
            "title": "A RELATED WORK",
            "content": "A.1 IN-GENERATION IMAGE WATERMARKING Another line of research involves watermarking generated images by altering the generation process or fine-tuning the generative models. These methods (Cui et al., 2023; Ci et al., 2024; Fernandez et al., 2023; Liu et al., 2023; Meng et al., 2024; Rezaei et al., 2024; Wen et al., 2023; Xiong et al., 2023; Yang et al., 2024; Zhao et al., 2023c; Zhang et al., 2024a), also known as in-generation image watermarking, aim to facilitate the detection of AI-generated content by inherently embedding watermark during image generation. These methods can also help protect the copyright of the generated content. However, since in-generation watermarking can only be applied to generated images, not real ones, and this study focuses on protecting copyright and intellectual property in real-world scenarios, these techniques are not included in our benchmark. Nonetheless, it is important to note that our proposed methods could also enhance the robustness of in-generation watermarks against image editing. A.2 GENERATIVE MODEL-BASED IMAGE EDITING Recent and significant advancements in text-to-image generative models (Chang et al., 2023; Ding et al., 2022; Nichol et al., 2021; Ramesh et al., 2022; Rombach et al., 2022; Saharia et al., 2022; Yu et al., 2022; Peebles & Xie, 2023; Esser et al., 2024) have enhanced numerous applications (Avrahami et al., 2023; Ruiz et al., 2023; Hertz et al., 2022; Kim et al., 2022; Tumanyan et al., 2023; Zhang et al., 2023; Mou et al., 2023; Shi et al., 2024; Zhu et al., 2023; Zhou et al., 2023; 2024a; Lu et al., 2023; 2024; Zhao et al., 2023a; 2024a;b; Wang & Chen, 2024; Gandikota et al., 2023; Parmar et al., 2024; Zhou et al., 2024b;c; Zeng et al., 2024). In this study, we focus on real image editing, which allows users to freely modify actual photographs, producing highly realistic results. Typically, the inputs for image editing include an image and various conditions that help users accurately describe their desired changes. These conditions can encompass text prompts using natural language to specify the edits (Brooks et al., 2023; Zhang et al., 2024c; Fu et al., 2023; Zhang et al., 2024b), region masks to designate areas for modification (Zhao et al., 2024c; Zhuang et al., 2023; Wang et al., 2023), additional images to provide desired styles or objects (Chen et al., 2024; Lu et al., 2023; Yang et al., 2023), and drag points (Pan et al., 2023; Shi et al., 2024; Mou et al., 2023) that enable users to interactively move specific points in the image to target positions. We broadly categorize image editing into two types: local editing and global editing. Local editing involves modifying only specific region of an image while keeping the other areas unchanged. In contrast, global editing modifies the entire image, though certain changes may occur unintentionally due to the nature of generative models, rather than users intent. These editing methods can compromise embedded watermarks, potentially undermining copyright protection. Note that we do not include drag-based editing methods (Pan et al., 2023; Shi et al., 2024; Mou et al., 2023) in our benchmark. This exclusion is due to the lengthy optimization times required to edit single image using these methods and the limited availability of datasets that provide valid drag points. Extending the benchmark to include drag-based methods presents potential direction for future research."
        },
        {
            "title": "B WATERMARKING FREQUENCY PATTERNS",
            "content": "In this analysis, we examine each watermarking method within the frequency domain to understand how they alter the frequency spectrum of input images. Specifically, we calculate the absolute difference between the Fourier spectrum F() of the watermarked image xw and the original image xo as F(xw) F(xo), and then average this metric over 1,000 pairs of watermarked and original images. Figure 6 presents the results on logarithmic scale. Interestingly, aside from our method, the top four watermarking methods that demonstrate robustness against image editing in certain scenarios (as illustrated in Figure 1)namely StegaStamp, MBRS, SepMark, and PIMoGall exhibit prominent patterns in the low-frequency bands, accompanied by cross-shaped high-intensity pattern. Among these, StegaStamp shows the strongest pattern intensity. This cross pattern is also part of the low-frequency spectrum, as the Y-axis represents zero frequency in the X-direction, and the X-axis represents zero frequency in the Y-direction. Although CIN and TrustMark also focus their influence on the low-frequency bands,"
        },
        {
            "title": "Preprint",
            "content": "Figure 6: Frequency pattern visualizations for each watermarking method. The DWTDCT method is excluded because it closely resembles DWTDCTSVD and their pattern intensity is too weak to be discerned on the uniform scale. Please zoom in for closer look. their robustness remains relatively low. We infer that this may be related to the absence of the crossshaped pattern and the lower intensity of their patterns. These patterns support our conclusion in Section 3.1: the more watermarks impact is concentrated in the low-frequency bands of the spectrum, the more robust it is against image editing. Although the model trained using our method, VINE, exhibits less pronounced influence along the and axes compared to StegaStamp, it displays highly dense patterns near these axes, which we infer contribute to its enhanced robustness. Additionally, compared to VINE-B, VINE-R shows higher brightness in the central region, further increasing its robustness."
        },
        {
            "title": "C RESOLUTION SCALING",
            "content": "C.1 METHOD Bui et al. (2023) propose method to adapt any watermarking model to handle arbitrary resolutions, as presented in Algorithm 1. This approach preserves the quality of the watermarked images and maintains or even improves their robustness against the image transformations that the models can inherently handle at their native resolution. In our experiments, we apply this resolution scaling method to all methods, enabling them to operate at uniform resolution of 512512, which is compatible with image editing models. C.2 IMPACT ON WATERMARKED IMAGE QUALITY Among the evaluated methods, MBRS (Jia et al., 2021), CIN (Ma et al., 2022), PIMoG (Fang et al., 2022), SepMark (Wu et al., 2023), StegaStamp (Tancik et al., 2020), TrustMark (Bui et al., 2023), VINE-B, and VINE-R are trained at resolutions lower than 512512, necessitating resolution scaling during the encoding process. Table 3 presents the watermarked image quality at their original"
        },
        {
            "title": "Preprint",
            "content": "Algorithm 1 Resolution scaling 1: Input: Input image xo, binary watermark 2: Output: Watermarked image xw 3: Model: Watermark Encoder E() trained on the resolution of interpolate(xo, (u, v)) 4: h, Size(xo) 5: xo xo/127.5 1 // normalize to range [-1, 1] 6: 7: E(x o) 8: interpolate(r, (h, w)) 9: xw clamp(xo + r, 1, 1) 10: xw xw 127.5 + 127.5 // residual image Table 3: Comparison of watermarking performance, evaluating both image quality of the watermarked images and detection accuracy under normal conditions (no distortion or editing applied) at the original training resolution. The best value in each column is highlighted in bold, and the second best value is underlined. Method Resolution Capacity PSNR SSIM LPIPS FID TPR@0.1%FPR (%) MBRS (Jia et al., 2021) CIN (Ma et al., 2022) PIMoG (Fang et al., 2022) SepMark (Wu et al., 2023) StegaStamp (Tancik et al., 2020) TrustMark (Bui et al., 2023) VINE-Base VINE-Robust 128 128 128 128 128 128 128 128 400 400 256 256 256 256 256 256 30 30 30 30 100 100 100 100 25.14 41.70 37.54 35.50 29.33 40. 40.22 37.07 0.8348 0.9812 0.9814 0.9648 0.8992 0.9819 0.9961 0.9942 0.0821 0.0011 0.0140 0.0116 0.1018 0.0015 0.0022 0.0048 13.51 2.20 2.97 2.95 8.29 1. 0.10 0.19 100.0 100.0 100.0 100.0 100.0 100.0 100.0 100.0 training resolutions, enabling comparison with their performance after resolution scaling. It is observed that, in terms of image quality, most methods show slight improvement in PSNR, SSIM, and FID after resolution scaling, while exhibiting slight increase in LPIPS. C.3 IMPACT ON ROBUSTNESS AGAINST DISTORTIONS Figure 7 demonstrates the robustness of the evaluated watermarking methods against classical transmission distortions at resolution of 512512 pixels. In contrast, Figure 8 examines watermarking methodsincluding MBRS, CIN, PIMoG, SepMark, StegaStamp, TrustMark, VINE-B, and VINERthat were originally trained at resolutions lower than 512512 pixels by evaluating them at their respective training resolutions. This comparison helps determine whether scaling the resolution affects their inherent robustness. The results indicate that inherent robustness is either unaffected or even enhanced. This occurs because, for the same level of distortion, larger images experience comparatively less impact. This phenomenon is particularly evident with Gaussian blurring. At resolution of 512512 pixels, MBRS, PIMoG, SepMark, CIN, and TrustMark can withstand Gaussian blurring with kernel size of 5 (see Figure 7(a)). However, at their original resolutions of 128128 or 256256 pixels, kernel size of 5 has more significant impact compared to the 512512 resolution, resulting in poorer performance (see Figure 8(a)). It is important to note that the kernel size range on the x-axis in Figure 8 is halved compared to Figure 7 because the image resolution is reduced by half or more, resulting in larger kernel sizes that cause excessive blurring. Although comparing different methods at their original training resolutions is somewhat unfair due to their varying training resolutions, the results demonstrate that resolution scaling does not compromise the robustness of these watermarking methods and can sometimes even enhance it. For other types of attacks, the difficulty levels remain consistent across different resolutions. Additionally, scaling the resolution significantly improves robustness against JPEG compression while maintaining robustness against brightness adjustments, contrast modifications, and Gaussian noise. Another noteworthy observation is that, in Figure 7(a) (i.e., at the 512512 resolution), the methods that exhibit strong robustness against Gaussian blurring also demonstrate robustness against image editing. This finding corroborates our conclusions in Section 3.1, as the characteristics of image editing in the frequency domain are similar to those of blurring distortions."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Performance of watermarking methods at resolution of 512512 pixels under (a) Gaussian blurring, (b) brightness adjustments, (c) contrast modifications, (d) Gaussian noise, and (e) JPEG compression."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Assessment of watermarking methods at their respective training resolutions under the following conditions: (a) Gaussian blurring, (b) brightness adjustments, (c) contrast modifications, (d) Gaussian noise, and (e) JPEG compression. Training resolutions: MBRS, CIN, PIMoG, and SepMark were trained at 128128 pixels; TrustMark, VINE-B, and VINE-R at 256256 pixels; and StegaStamp at 400400 pixels."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Architecture of the condition adaptor in Figure 4. Each fully connected and convolutional layer is followed by an activation layer."
        },
        {
            "title": "D CONDITION ADAPTER ARCHITECTURE",
            "content": "Figure 9 showcases the condition adaptor architecture within our watermark encoder (Figure 4). This architecture consists of multiple fully connected and convolutional layers, each followed by ReLU activation layer."
        },
        {
            "title": "E IMPLEMENTATION DETAILS",
            "content": "E.1 PRETRAINING The complete training objective is detailed in Section 3.3. To train VINE-B, we initially prioritize the watermark extraction loss by setting α to 10 and each of βMSE, βLPIPS, and βGAN to 0.01. To preserve the generative prior, the UNet and VAE decoder of the SDXL-Turbo, along with the additional zero-convolution layers, are kept frozen. Once the bit accuracy exceeds 0.85, we proceed to the second stage by unfreezing all parameters for further training. At this stage, the loss weighting factors are adjusted to α = 1.5, βMSE = 2.0, βLPIPS = 1.5, and βGAN = 0.5. In both stages, we employ the Adam optimizer with learning rate of 1 104 and batch size of 112, training on the entire OpenImage dataset (Kuznetsova et al., 2020) at resolution of 256256 pixels. We apply transformations that randomly select either random cropping or resizing, followed by center cropping. VINE-B is trained on 8NVIDIA A100-80GB GPUs for 111k iterations. E.2 FINE-TUNING VINE-R is obtained by further fine-tuning VINE-B on the Instruct-Pix2Pix dataset (Brooks et al., 2023). During this fine-tuning stage, we reduce the learning rate to 5e-6 and adjust the batch size to 80, while keeping the weighting factors for different loss terms the same as in the second stage. Watermarked images are edited using Instruct-Pix2Pix with text prompts and 25 denoising steps. Although the quality of images edited with 25 steps is not as high as those edited with 50 steps, it remains satisfactory and effectively doubles the fine-tuning speed. Gradients are then backpropagated using straight-through estimator (Bengio et al., 2013). VINE-R is fine-tuned using 8NVIDIA A100-80GB over 80k iterations."
        },
        {
            "title": "F ANALYSIS OF IMAGE EDITING",
            "content": "F."
        },
        {
            "title": "IMAGE REGENERATION",
            "content": "In this study, we assess the reconstruction quality of image regeneration across various difficulty levels. As illustrated in Figure 10, increasing the difficulty levelby either raising the noise timestep in stochastic regeneration or decreasing the sampling step in deterministic regenerationdegrades the reconstruction quality. Figure 11 presents several examples. The regenerated images are perceptually similar to the original images. However, as the difficulty level increases, stochastic regeneration tends to introduce more hallucinated details into the original image, whereas deterministic regeneration tends to smooth out the original images. Both approaches can degrade the embedded watermark, thereby posing challenges to copyright protection. Our method effectively embeds and detects watermarks even under extremely challenging difficulty levels. Figure 10: The reconstruction quality of (a) stochastic regeneration and (b) deterministic regeneration. The PSNR is calculated by comparing the regenerated image to the original image. Figure 11: The reconstruction quality of stochastic regeneration and deterministic regeneration. Please zoom in for closer look. F.2 GLOBAL EDITING AND LOCAL EDITING Here, we assess the editing quality of various global editing models using three metrics: CLIPdir, CLIPout, and CLIPimg. CLIPdir evaluates whether the edits correspond to the modifications specified by the editing prompt, which represents the difference between the captions of the source and target images. This metric is calculated by measuring the cosine similarity between the difference in CLIP image embeddings of the source and edited images and the difference in CLIP text embeddings of the source and target captions. CLIPout measures the alignment of the edited image with the"
        },
        {
            "title": "Preprint",
            "content": "Table 4: Comparison of editing quality for different global editing methods and the effect of different watermarks on image editing outcomes. All models use an image guidance scale of 1.5 and text guidance scale of 7. Method Unwatermarked Image MBRS (Jia et al., 2021) CIN (Ma et al., 2022) PIMoG (Fang et al., 2022) RivaGAN (Zhang et al., 2019) SepMark (Wu et al., 2023) DWTDCT (Al-Haj, 2007) DWTDCTSVD (Navas et al., 2008) SSL (Fernandez et al., 2022) StegaStamp (Tancik et al., 2020) TrustMark (Bui et al., 2023) EditGuard (Zhang et al., 2024d) VINE-Base VINE-Robust Instruct-Pix2Pix UltraEdit MagicBrush CLIPdir CLIPimg CLIPout CLIPdir CLIPimg CLIPout CLIPdir CLIPimg CLIPout 0.7268 0.2693 0.7913 0.2930 0.7283 0. 0.2732 0.3230 0.3025 0.2494 0.2625 0.2518 0.2647 0.2659 0.2644 0.2581 0.2583 0.2436 0.2634 0.2722 0.2743 0.2624 0.7385 0.7232 0.7021 0.7317 0.7292 0.7317 0.7220 0.7218 0.6826 0.7181 0.7045 0.7260 0.7248 0.2733 0.2729 0.2746 0.2721 0.2743 0.2734 0.2751 0.2752 0.2697 0.2729 0.2722 0.2743 0. 0.2919 0.3152 0.3010 0.3168 0.3145 0.3189 0.3115 0.3093 0.2904 0.3172 0.3155 0.3186 0.3176 0.6654 0.7111 0.6940 0.7133 0.7181 0.7250 0.7118 0.7065 0.6886 0.7146 0.7170 0.7189 0.7183 0.2891 0.3010 0.3024 0.3003 0.3002 0.3009 0.3004 0.3019 0.3007 0.2994 0.3021 0.2996 0.3001 0.2857 0.2949 0.2815 0.3020 0.2975 0.2959 0.2869 0.2896 0.2663 0.2943 0.2882 0.2977 0.2981 0.7816 0.7841 0.7662 0.7948 0.7891 0.7942 0.7793 0.7780 0.7512 0.7853 0.7708 0.7889 0.7953 0.2929 0.2928 0.2962 0.2930 0.2936 0.2934 0.2939 0.2944 0.2944 0.2936 0.2940 0.2931 0. Table 5: Comparison of editing quality for different local editing methods and the effect of different watermarks on image editing outcomes. All models use an image guidance scale of 1.5 and text guidance scale of 7. Method Unwatermarked Image MBRS (Jia et al., 2021) CIN (Ma et al., 2022) PIMoG (Fang et al., 2022) RivaGAN (Zhang et al., 2019) SepMark (Wu et al., 2023) DWTDCT (Al-Haj, 2007) DWTDCTSVD (Navas et al., 2008) SSL (Fernandez et al., 2022) StegaStamp (Tancik et al., 2020) TrustMark (Bui et al., 2023) EditGuard (Zhang et al., 2024d) VINE-Base VINE-Robust ControlNet-Inpainting UltraEdit CLIPdir CLIPimg CLIPout CLIPdir CLIPimg CLIPout 0.1983 0.7076 0.2917 0.2589 0.2778 0. 0.1846 0.1966 0.1828 0.1975 0.1932 0.1982 0.1922 0.1911 0.1752 0.1959 0.1921 0.1953 0.1951 0.7058 0.7042 0.6909 0.7117 0.7126 0.7197 0.6995 0.6995 0.6684 0.7001 0.6944 0.7023 0.7030 0.2588 0.2613 0.2600 0.2612 0.2582 0.2602 0.2608 0.2604 0.2606 0.2594 0.2606 0.2591 0.2591 0.2657 0.2745 0.2578 0.2748 0.2716 0.2776 0.2705 0.2677 0.2439 0.2728 0.2696 0.2726 0.2710 0.7175 0.7389 0.7371 0.7469 0.7588 0.7558 0.7469 0.7380 0.7246 0.7451 0.7392 0.7494 0.7475 0.2913 0.2922 0.2920 0.2937 0.2921 0.2924 0.2940 0.2940 0.2919 0.2919 0.2923 0.2906 0. target caption by calculating the cosine similarity between the CLIP text embeddings of the target caption and the CLIP image embeddings of the edited images. Meanwhile, CLIPimg assesses content preservation by evaluating the cosine similarity between the CLIP image embeddings of the source and edited images. Table 4 presents comparison of editing results from different editing models applied to unwatermarked images in the first row. Each editing model employs an image guidance scale of 1.5 and text guidance scale of 7. Within these settings, UltraEdit most effectively aligns the edited image with the editing prompt, while MagicBrush excels at preserving the image layout. Since text-driven image editing is more sensitive than image regeneration, we investigated whether watermarking the input image affects editing quality. The results, presented in Table 4, show that editing quality remains largely intact with only minor fluctuations. If the watermark degrades the image quality, it can slightly impact editing performance in terms of both text alignment and content preservation. Figure 12 shows several examples. Similarly, the evaluation results for local editing models are presented in Table 5 and Figure 13. UltraEdit also demonstrates superior editing quality for local edits, excelling in both text-image alignment and image content preservation. F.3 IMAGE-TO-VIDEO GENERATION Figure 14 presents the results of image-to-video generation applied to watermarked images using various watermarking methods. The presence of watermarks does not perceptually affect the imageto-video generation."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Different watermarks have minimal impact on the image global editing outcomes, resulting in only slight changes."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Different watermarks have minimal impact on the image local editing outcomes, resulting in only slight changes."
        },
        {
            "title": "Preprint",
            "content": "Figure 14: Different watermarks have little effect on image-to-video generation, leading to only minor changes."
        },
        {
            "title": "G INFERENCE SPEED AND GPU MEMORY EVALUATION",
            "content": "We evaluate the inference speed and GPU memory usage of watermarking methods on an NVIDIA Quadro RTX6000 GPU. The results are averaged over 1,000 images. SSL employs instance-based iterative optimization, resulting in longer processing times. EditGuard utilizes an inverse neural network to encode secret images and bits simultaneously, thereby also requiring additional running time. Although our model demands more GPU memory and longer inference times, these requirements remain within acceptable ranges. Table 6: Comparison of watermarking methods based on running time per single image and GPU memory usage. The results are averaged over 1,000 images. Since the implementations we employed for DWTDCT, DWTDCTSVD, and RivaGAN support CPUs exclusively, they have been omitted from the comparison. Method Running Time per Image (s) GPU Memory Usage (MB) MBRS (Jia et al., 2021) CIN (Ma et al., 2022) PIMoG (Fang et al., 2022) RivaGAN (Zhang et al., 2019) SepMark (Wu et al., 2023) DWTDCT (Al-Haj, 2007) DWTDCTSVD (Navas et al., 2008) SSL (Fernandez et al., 2022) StegaStamp (Tancik et al., 2020) TrustMark (Bui et al., 2023) EditGuard (Zhang et al., 2024d) VINE 0.0053 0.0741 0.0212 - 0.0109 - - 2.1938 0.0672 0.0705 0.2423 0.0795 938 2944 878 - 928 - - 1072 1984 648"
        },
        {
            "title": "H QUALITATIVE COMPARISON",
            "content": "Figure 15 showcases qualitative examples of 512512 images encoded using the watermarking methods evaluated in this study. The residuals are calculated by normalizing the absolute difference xw xo. For more detailed examination, please zoom in on the images. It can be observed that the images watermarked by MBRS and PIMoG exhibit slight color distortions, whereas the StegaStamp watermarked images display black artifacts. Other methods produce watermarked images that, to the human eye, appear identical to the original images."
        },
        {
            "title": "I ADDITIONAL BENCHMARKING RESULTS",
            "content": "Additional benchmarking results are presented in Figure 16, including deterministic regeneration with DPM-Solver, global editing with UltraEdit, global editing with Instruct-Pix2Pix, local editing with ControlNet-Inpainting, and image-to-video generation with Stable Video Diffusion."
        },
        {
            "title": "Preprint",
            "content": "Figure 15: Qualitative comparison of the evaluated watermarking methods. Please zoom in for closer look."
        },
        {
            "title": "Preprint",
            "content": "Figure 16: The performance of watermarking methods under (a) Deterministic regeneration with DPM-Solver, (b) Global editing with UltraEdit, (c) Global editing with Instruct-Pix2Pix, (d) Local editing with ControlNet-Inpainting, and (e) Image-to-video generation with Stable Video Diffusion."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Nanyang Technological University"
    ]
}