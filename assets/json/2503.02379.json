{
    "paper_title": "Teaching Metric Distance to Autoregressive Multimodal Foundational Models",
    "authors": [
        "Jiwan Chung",
        "Saejin Kim",
        "Yongrae Jo",
        "Jaewoo Park",
        "Dongjun Min",
        "Youngjae Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As large language models expand beyond natural language to domains such as mathematics, multimodal understanding, and embodied agents, tokens increasingly reflect metric relationships rather than purely linguistic meaning. We introduce DIST2Loss, a distance-aware framework designed to train autoregressive discrete models by leveraging predefined distance relationships among output tokens. At its core, DIST2Loss transforms continuous exponential family distributions derived from inherent distance metrics into discrete, categorical optimization targets compatible with the models' architectures. This approach enables the models to learn and preserve meaningful distance relationships during token generation while maintaining compatibility with existing architectures. Empirical evaluations show consistent performance gains in diverse multimodal applications, including visual grounding, robotic manipulation, generative reward modeling, and image generation using vector-quantized features. These improvements are pronounced in cases of limited training data, highlighting DIST2Loss's effectiveness in resource-constrained settings."
        },
        {
            "title": "Start",
            "content": "Jiwan Chung Saejin Kim Yongrae Jo Jaewoo Park Dongjun Min Youngjae Yu Yonsei University LG AI Research jiwan.chung.research@gmail.com 5 2 0 2 M 4 ] . [ 1 9 7 3 2 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "As large language models expand beyond natural language to domains such as mathematics, multimodal understanding, and embodied agents, tokens increasingly reflect metric relationships rather than purely linguistic meaning. We introduce DIST2Loss, distance-aware framework designed to train autoregressive discrete models by leveraging predefined distance relationships among output tokens. At its core, DIST2Loss transforms continuous exponential family distributions derived from inherent distance metrics into discrete, categorical optimization targets compatible with the models architectures. This approach enables the models to learn and preserve meaningful distance relationships during token generation while maintaining compatibility with existing architectures. Empirical evaluations show consistent performance gains in diverse multimodal applications, including visual grounding, robotic manipulation, generative reward modeling, and image generation using vector-quantized features. These improvements are pronounced in cases of limited training data, highlighting DIST2Losss effectiveness in resource-constrained settings. 1. Introduction Large language models (LLMs) [41] have recently emerged as promising backbones for general-purpose foundational models across variety of domains [4]. These models rely on two core architectural principles in probabilistic modeling. First, they represent sample text as sequence of tokens and train the model autoregressively, predicting each token conditioned on the previous ones. Second, each token is treated as discrete categorical variable, optimized to match one-hot target distribution during training. key challenge in foundational model development lies in the ability to effectively model numerically structured elements. This includes not only traditional numbers, but also entities within general metric spacessuch as integers in arithmetic tasks [63], spatial coordinates and rotation angles in object detection and manipulation [34, 58], and high-dimensional quantized embeddings in image or Figure 1. DIST2Loss is training framework for foundational models that leverages distance relationships among output tokens, enabling improvements across diverse tasks such as alignment reward modeling, visual grounding, robotic manipulation, and autoregressive image generation. video generation [14, 15, 62]. Typically, the intrinsic distance relationships among these elements are disregarded, as each token is encoded as discrete one-hot target within categorical distribution framework. In this work, we introduce DIScreTized DISTance Loss (DIST2Loss), framework that integrates predefined distance relationships between tokens into the training process of an autoregressive discrete foundational model. DIST2Loss requires no additional external training data and incurs minimal computational overhead, allowing plugand-play style adaptation across diverse setups. By directly encoding distance relationships, DIST2Loss accelerates convergence in tasks where distance modeling is semantically meaningful, particularly in multimodal applications such as object detection (Sec. 3.2), object manipulation (Sec. 3.3), and image generation (Sec. 3.5). Additionally, we demonstrate its efficacy in accelerating convergence in scalar function optimization (Secs. 3.1 and 3.4). DIST2Loss builds on prior works on discretizing continuous distributions [7, 9]. Like prior distribution discretization works, DIST2Loss begins with well-defined continuous metric and its corresponding distribution within the exponential family and converts it into discrete categorical distribution. Unlike prior works, however, DIST2Loss introduces the use of the induced discrete distributions as targets for optimizing foundational models. During training, Figure 2. Left: DIST2Loss demonstrates broad applicability for tasks where foundational models need to generate tokens with embedded distance semantics. Right: Illustration of the DIST2Loss framework applied to visual grounding task. Here, for an input subsequence with spatial metrics (e.g., coordinates (x1, y1, x2, y2) in visual grounding), the process is as follows: (a) calculate the distance d(x, x) between the target token and each token based on the defined metric; (b) transform this distance into continuous probabilistic distribution p(x, x) within the exponential family; (c) discretize the continuous distribution into categorical distribution pd(x, x) compatible with the discrete foundational model; (d) optimize the foundational model by minimizing the KL divergence between the models likelihood pθ and the distance-aware target distribution pd at each timestep. DIST2Loss first collects token positions corresponding to the target tokens within the metric space. It then constructs the target distribution by calculating the distance between the target token and other token candidates, thereby inducing the target likelihood for each candidate token. Finally, it optimizes the Kullback-Leibler (KL) divergence [32] between the target and model likelihoods. We also propose two extensions of DIST2Loss. In cases where elements in the metric space correspond to multiple tokens within the language models vocabulary, modeling of token-level distance becomes challenging under conventional teacher-forcing [56]. To address this, we apply weighting to per-token loss based on each tokens place value and introduce multi-token negative samples as contrastive targets. Furthermore, we extend distance modeling to cases where each token corresponds to high-dimensional vector representations, proposing method to construct metric space within the representation space defined by pretrained feature extractor model. DIST2Loss has computational requirements nearly identical to the conventional cross-entropy loss on one-hot targets, with the exception of contrastive target augmentation, which can be omitted with minimal impact for small digit distances. Additionally, no computational overhead is incurred at inference time, as DIST2Loss is applied solely during the training process. This simplicity enables plugand-play integration of the framework across diverse tasks."
        },
        {
            "title": "Our",
            "content": "experiments show broad generalizability of DIST2Loss across domains, enhancing downstream performance even in data-scarce settings. Specifically, it achieves tighter bounding box coordinates in visual grounding tasks (Sec. 3.2), increases success rates in robotic arm manipulation by accelerating learning of robotic actions (Sec. 3.3), improves generative reward modeling in LLM alignment (Sec. 3.4), and enhances learning of vector-quantized image representations in autoregressive models (Sec. 3.5). We hope the generalizability and simplicity of DIST2Loss will inspire researchers to explore objectives better aligned with the diverse challenges of general foundational models beyond one-hot next token prediction. 2. Method Our goal is to design an objective function that achieves two main objectives: first, to exploit the given metric distance to build targets for optimization, and second, to ensure compatibility with the categorical distribution formulation widely adopted by foundational models built on LLMs. We hypothesize that loss function leveraging this nontrivial metric prior will improve data efficiency in scenarios where the metric distance holds significance. This section is structured as follows: we begin with simple case, 1) single-token loss on scalar Euclidean metrics. We then extend this to two scenarios: 2) loss functions applied where the distance metric spans multiple tokens, and 3) cases where the distance metric involves highdimensional vectors. 2.1. Preliminaries Let denote the vocabulary of the foundaNotations. tional model, and consider subset Vd with cardinality Vd = . Define metric space (X , d), where each element represents sequence = (x1, . . . , xL) with xi Vd. The metric : assigns distance d(x, y) between any pair of sequences (x, y) . This distance d(x, y) is determined by the underlying data structure, such as the Euclidean distance for integers or an embedding distance for multi-dimensional vectors. Consider the discrete input sequence = (s1, . . . , sn), representing sequence of tokens in an autoregressive discrete foundational model. single forward pass through the model generates logits over the entire vocabulary for each token in the sequence: lt = fθ(s<t), 1, . . . , (1) where lt represents the logit vector at time step and fθ denotes the model parameterized by θ. These logits lt are then transformed into probability distributions over the vocabulary subset Vd by applying the softmax function: pθ(vs<t) = softmax(lt), (2) Cross-Entropy Loss. In training discrete autoregressive model, the standard approach involves teacher-forcing, where the target and model predictions are compared independently at each token. Cross-entropy loss [46], LCE, is commonly used to compare two categorical distributions: LCE = (cid:88) (cid:88) t=1 vV ptarget(vst) log pθ(vs<t) (3) where ptarget(vst) denotes the target distribution at time step t. In most cases, ptarget(vst) is one-hot distribution that corresponds to the ground truth token st. 2.2. Discretized Distance Loss Consider subsequence = [xi : xj] with in the input sequence s, represented as = [. . . , si1, xi : xj, sj+1, . . .]. Following the notations defined above, is also an element of the metric space . While this formulation easily generalizes to cases where multiple elements from are present within s, we limit our explanation to single x-subsequence here for clarity. To incorporate the metric distance into the models objective, we define target distribution pd(vx, t) that reflects the similarity of the tokens according to chosen distance metric in the token space Vd. This target aligns probability mass with the similarity structure, encouraging model outputs that respect the defined metric distance. We propose formulating the target distribution pd using discretized version of the exponential family distribution: pd(vx, t) = (cid:80) vVd (cid:16) exp (cid:17) d(v,x,t) τ (cid:16) d(v,x,t) τ exp (cid:17) , (4) where the temperature hyperparameter τ controls the smoothness of the target distribution, with lower values of τ assigning higher probability to tokens closer to the target in the metric space. Note that in single token case, where each element in the metric space consists of single token from subset of the vocabulary, we have d(v, x, t) = d(v, xt) and thus in turn ptarget(vx, t) = ptarget(vxt). In the specific case where the root mean squared error (RMSE) is used as the distance metric, this formulation is equivalent to discretized Gaussian distribution, often referred to as discrete Gaussian in prior work [7]. Our framework generalizes this approach, offering flexible loss function applicable across range of distance metrics and training setups for foundational models. The discretized distance loss is defined by comparing ptarget(vx, t) with the models predicted distribution pθ(vs<t) via KL divergence: Ldist = (cid:88) (cid:88) t= vVd pd(vx, t) log pd(vx, t) pθ(vs<t) . (5) The final objective combines the cross-entropy loss LCE with this distance-based regularization: = LCE + αLdist (6) where α adjusts the weighting between accuracy and metric coherence. For simplicity, we fix α = 0.1 throughout the experiments without hyperparameter tuning. Example. Consider single-token case, denoted as xsingle Xsingle with xsingle = (xi). To simplify, we restrict the metric space to scalar Euclidean metrics. Suppose the target token xsingle is 5, with the Euclidean distance metric defined as d(v, x) = (v xi)2. We construct target distribution pd(vx) that assigns higher probabilities to tokens closer to 5 according to this distance. For example, token 4 receives higher probability than token 2, reflecting its proximity to the target within the metric space. This setup is used directly in our experiments in Sec. 3.4. 2.3. Multi-Token Distance Next, consider multi-token case where each element in the metric space consists of sequence of tokens from the vocabulary, denoted xmulti Xmulti with xmulti = (x1, . . . , xL). For instance, this could represent multidigit integer split into individual tokens. Applying multitoken objectives directly in autoregressive models trained with teacher-forcing is extremely inefficient, as it requires training-time sequence generation. To circumvent this limitation, we propose two practical alternative approaches. Contrastive Target Augmentation. Instead of evaluating all possible multi-token sequences, we propose sampling contrastive multi-token candidate Xmulti for training. Such candidate is selected from nearby neighbors of the target in the metric space, without reference to the training model fθ. For example, in the case of integer sequences, 39 might be chosen as close neighbor to the target 40, with each digit tokenized separately. For each token in the sequence, we extend the target distribution by incorporating the negative sample x. The contribution of each token in to the overall distance is defined based on its position-wise difference from the target x. For instance, when the target is 40, the negative sample 39 is assigned token-wise distance where the tens digit 3 has distance 0 from the targets 4, while the units digit 9 has distance of 1 from the targets 0. We then concatenate the logits of and the selected logit at each token position, forming an extended likelihood distribution. The distance loss Ldist is applied to this extended distribution. Place Value Weighting. For tasks involving multi-digit integers or sequences where token positions have different significance, we introduce place value weighting. In this approach, tokens are weighted according to their positional importance, so that differences in higher place values have greater impact on the loss. For example, in multi-digit integer setting, we directly multiply the distance loss by the place value weight for each token, assigning more weight to tokens in higher positions. Let xmulti = (x1, . . . , xL) represent the target sequence, with xi denoting the digit in the i-th place (e.g., thousands, hundreds, tens, units). The place-weighted loss is formulated as: Lplace = (cid:88) i=1 wi Ldist(xi), (7) where wi is the place weight for position i, set to 4 for thousands, 3 for hundreds, 2 for tens, and 1 for units. 2.4. High Dimensional Distance Our DIST2Loss is flexible and can be applied to any distance metric defined over the vocabulary Vd, including the distance between high-dimensional continuous vectors. Here, we outline practical case where the distance is defined over high-dimensional vector embeddings, which are commonly used in representation learning [8, 40] and information retrieval [27] literature. Consider vector representation v(x) for each token Vd, where v(x) RD is high-dimensional embedding. Suppose that we have two singleton sequences = (x1) and = (y1), each represented by their embedding v(x1) and v(y1). To compute the distance between these sequences, we use distance metric over their embeddings, such as cosine similarity or Euclidean distance. For instance, when using cosine similarity as the distance metric, the distance between v(x) and v(y) is given by: (v(x), v(y)) = 1 v(x) v(y) v(x)v(y) , (8) which captures the angular separation between token embeddings. The choice of distance metric often depends on the training objective of the embedding function v. For instance, with vector-quantized representations, the distance metric is typically chosen to match the quantization function used during the training of the embedder, as discussed in experiments in Sec. 3.5. 3. Experiments We propose general approach for leveraging metric space information to train discrete foundational models. Our method can be applied whenever model needs to generate numeric or discretized representations with regression targets. To validate its generality, we apply our approach across range of tasks: (1) synthetic function regression as toy task, (2) generative reward modeling for human feedback in LLMs, (3) object detection within multimodal LLMs, (4) object manipulation in embodied AI, and (5) image generation on vector-quantized representations, showcasing its capacity for high-dimensional distance modeling. Baselines. Across our experiments, we evaluate two ablated baselines alongside the full distance-aware loss (dist): the sft baseline, which applies only the standard crossentropy loss LCE without any distance-specific objective, and the vocab baseline, which replaces the distance loss Ldist with cross-entropy loss constrained to subset of the vocabulary Vd. The vocab objective is intended to assess the impact of the distance-aware target distribution on model performance, and is defined as: Lvocab = LCE(V) + α LCE(Vd), (9) where LCE(V) denotes the cross-entropy loss over the entire vocabulary and LCE(Vd) is the cross-entropy loss over the numeric-constrained subset Vd. 3.1. Toy: Learning to Regress This experiment represents learning-to-learn task where the model is trained to learn the process of linear regression itself, rather than simply fitting specific input-output mappings. Each training sample consists of three distinct (x, y) data points, along with target input for which the model must predict the corresponding value based on the linear relationship among the points, as shown in Fig. 3. We investigate the data efficiency of DIST2Loss in learning structured tasks by providing an extremely limited number of training samples. Specifically, we train the model with one to ten examples, where each sample corresponds to distinct linear regression problem with varying intercepts and slopes. We fine-tune an LLM1 [2] with different loss functions to assess its ability to generalize to unseen regression problems. Importantly, the model is not explicitly informed that the task is linear regression; it must infer the underlying structure from the provided samples. 1meta-llama/Llama-3.2-1B-Instruct rectly finetune Phi3V2 [1], model initially lacking grounding capability, on the RefCOCO [28, 38, 61] dataset without additional training on object detection tasks. Following prior research [58], we aggregate the training sets of RefCOCO, RefCOCO+, and RefCOCOg to build the finetuning dataset. Our choice of visual grounding over plain object detection aligns with the practical objective of extending an LLMs language understanding to new domains. For comparison, DIST2Loss is evaluated against robust baselines pretrained on large-scale visual grounding datasets, including UNINEXT [59], Ferret [60], Ferretv2 [65], and Florence-2 [58], alongside ablated baselines, with each baseline finetuned on the same dataset used in our approach. Accuracy, measured as IOU 0.5, is reported as the standard metric for this task. Results. incorporating Table 1 demonstrates that DIST2Loss consistently enhances performance over the sft baseline. In contrast, the vocab baseline results varied, underscoring the importance of metric-informed target distribution for improved outcomes. With DIST2Loss, Phi3V attains visual grounding performance on par with state-ofthe-art models trained on large-scale pretraining datasets optimized for object detection and grounding tasks. Refer to Appendix for the qualitative samples. 3.3. Embodied: Robotic Manipulation Robotic manipulation is another domain where foundational models frequently encounter numerical data. Here, the model must generate robotic joint actionstypically represented by position coordinates and rotation anglesbased on contextual inputs and task instructions. Setup. VIMABench [23] is benchmark for robotic manipulation, encompassing diverse array of robot arm manipulation tasks organized into 17 distinct categories. It assesses generalization abilities across four levels (L1L4), with this study focusing on levels L1 and L2. Baseline models include recent multimodal LLM-based approaches, notably RT-2 [5] and LLaRA [34]. This work follows the experimental framework of LLaRA, which fine-tunes the multimodal LLM, LLaVA-1.53 [35], using instruction-tuning data. Additionally, the scalability protocol from the same study is implemented, where data splits are defined according to dataset size. Consistent with LLaRAs setup, only the loss function is modified, with LLaRA-sft serving as direct baseline. Furthermore, auxiliary tasks introduced in the study are incorporated to expand the training dataset. Results. Table 2 shows consistent increase in robotic manipulation accuracy with DIST2Loss. Notably, its advantages are pronounced in data-scarce conditions, where 2microsoft/Phi-3.5-vision-instruct (4.2b) 3liuhaotian/llava-v1.5-7b Figure 3. Top: Overview of the task setup in the meta linear regression experiment, where the model learns to perform linear regression based on provided data points. Bottom: Experimental results showing MAE and RMSE across varying numbers of training samples. The y-axis is inverted for visualization. Setup. We construct regression problems by sampling slopes from the range [0.1, 1.0] and intercepts from [0.0, 0.5]. For each random seed, we generate ten training problems and 1,000 test problems, using subsets of the training set to evaluate performance across different data scales. To ensure robustness, results are averaged across five random seeds, capturing variability in the sampled training data. Predictions are evaluated at = 0.5, with performance reported in terms of Mean Absolute Error (MAE) and Root Mean Square Error (RMSE) for the predicted values. All values use three decimal places for precision. More details are available in Appendix A.2. Results. The bottom panel of Fig. 3 demonstrates that DIST2Loss consistently outperforms the baselines (sft and vocab) in terms of MAE, except when only single training sample is provided. This exception reflects the challenge of generalizing the linear regression property from single example. Additionally, the vocab baseline shows high variability in regression accuracy across different training data scales due to its tendency to sharpen the target distribution on numerical outputs, leading to inconsistent performance. 3.2. Multimodal: Visual Grounding We begin by evaluating DIST2Loss on the multimodal task of visual grounding, which involves generating the coordinates of the bounding box for specified object based on the corresponding referring expression provided as input. Setup. To assess the data efficiency of DIST2Loss, we di-"
        },
        {
            "title": "Models",
            "content": "#PT #FT UNINEXT [59] Ferret [60] Ferretv2 [65] Florence-2-B [58] Florence-2-L [58] Phi3V [1]-sft Phi3V-vocab Phi3V-dist val 92.6 89.5 92.8 92.6 93.4 94.3 600K 127K 1.1M 127K 1.1M 127K 126M 127K 126M 127K 127K 127K 94.5 (0.2) 127K 94.8 (0.5) 0 0 RefCOCO test-A 94.3 92.4 94.7 94.8 95.3 93.5 93.2 (0.3) 94.5 (1.0) test-B 91.5 84.4 88.7 91.5 92.0 86.0 86.0 () 87.3 (1.3) val 85.2 82.8 87.4 86.8 88.3 85.9 85.9 () 87.1 (1.2) RefCOCO+ test-A 89.6 88.1 92.8 91.7 92.9 91.6 90.6 (1.0) 92.2 (0.6) test-B 79.8 75.2 79.3 82.2 83.6 78.7 78.2 (0.5) 81.4 (2.7)"
        },
        {
            "title": "RefCOCOg",
            "content": "val 88.7 85.8 89.4 89.8 91.2 92.2 92.4 (0.2) 92.8 (0.6) test 89.4 86.3 89.3 82.2 91.7 87.4 87.6 (0.2) 88.0 (0.6) Table 1. Visual grounding experiment results on the RefCOCO [28, 38, 61] dataset, reported as accuracy (%). To evaluate the data efficiency of DIST2Loss, we directly fine-tune (FT) model lacking pretrained object detection capabilities (Phi3V [1]) for the downstream task. Baseline models, in contrast, are pretrained (PT) on large-scale object detection datasets before fine-tuning. #Data RT-2 [5] LLaRA [34]-sft LLaRA-vocab LLaRA-dist 1K 1.9 49.6 50.8 53.9 10K 100K 73.1 21.9 88.5 82.3 87.0 81.0 89.5 83.4 L2 10K 100K 70.4 17.7 84.6 78.1 83.5 77.2 86.1 82.8 1K 3.8 46.2 44.6 51.5 reported in accuracy (%)."
        },
        {
            "title": "Object manipulation experiment",
            "content": "results on Table 2. Results are VIMABench [23], presented for two test protocols (L1 and L2) and various training data scales, following the specifications in LLaRA [34]. Values for RT-2 and LLaRA-sft are taken from the original paper. For details on baseline scores, refer to Appendix A.3. training data is limited to approximately 1K samples, further underscoring the effectiveness of the distance metric as meaningful prior in robotic manipulation learning. Although the performance difference between dist and sft loss narrows with the inclusion of more data, DIST2Loss maintains an edge in generalization. This advantage is further highlighted in the more challenging L2 test protocol, where enhanced coordinate calibration by DIST2Loss significantly improves generalizability to complex tasks. 3.4. Textual: Generative Reward Modeling Next, we apply DIST2Loss to generative reward modeling within the RLHF (Reinforcement Learning from Human Feedback) framework. In RLHF, language models are trained to align with human feedbackprovided as scores or rankingsallowing them to adjust behavior based on human preference rather than solely imitating patterns in training data. Generative reward modeling [66, 67] leverages the standard next-token prediction objective in language models to generate rewards within natural language templates, bypassing the need for architecture changes typically required by sequential classification objectives. Setup. We train language models to directly predict human feedback scores for given instruction-response pairs, based on prior methods [55]. Our models are optimized to estimate the sum of the multi-facet scores, and the expected score over the score range for evaluation, as detailed in Appendix A.2. To assess the effectiveness of the proposed approach, we also train baseline model using standard sequential classification objective-binary. Evaluation is conducted on two benchmarks: RewardBench [33] and MT-Bench [67]. Additionally, we incorporate existing baselines from the official leaderboard, including UltraRM [11], Tulu-v2.5-RM [21], GPT-3.5 [6], Claude-3-Haiku [3], and Prometheus-2 [30]. Open-source model sizes are chosen for fair comparison against our backbone LLM4 [2]. Results. Table 3 summarizes our reward modeling results. DIST2Loss shows substantial improvement over the standard cross-entropy loss (dist vs. sft), highlighting its effectiveness in generative reward modeling. Moreover, generative reward modeling variants outperform the sequential classification baseline (binary), suggesting that generative reward modeling is competitive approach, especially in data-scarce settings, as it fully leverages the pretrained language modeling strengths of the LLM backbone better. The performance gain of dist over binary is consistent, with notable improvements observed in the Chat Hard and Safety domains. Lastly, our model with DIST2Loss approaches strong baseline performance using significantly less feedback data, highlighting its efficiency. 3.5. High-Dimension: Image Generation Effects of Token Distance on Image Semantics. Prior to training the image generation model, we analyze the semantic effects of distance between quantized image tokens. This involves encoding each image, replacing specific tokens, and reconstructing the images for assessment. In this process, four central tokens are randomly substituted with tokens from: 1) the top ten closest tokens (excluding the original), 2) random token, and 3) the bottom ten tokens by similarity. Tokens closely aligned with the original typically retain the semantic integrity of the image, whereas 4meta-llama/Llama-3.1-8B-Instruct"
        },
        {
            "title": "Type",
            "content": "#Data UltraRM-13B [11] Tulu-v2.5-RM-13B [21] Tulu-v2.5-RM-13B [21] GPT-3.5 [6] Claude-3-haiku [3] Prometheus-2-7B [30] Llama [2]-binary Llama-sft Llama-dist Seq. Classifier Seq. Classifier Seq. Classifier Generative Generative Generative Seq. Classifier Generative Generative 64K [11] 64K [11] 2M [21] - - 300K [29] 21K [55] 21K [55] 21K [55] Chat 96.4 39.4 93.6 92.2 73.7 85.5 83.8 89.1 95.0 (4.9) Chat Hard 55.5 42.3 68.2 44.5 92.7 49.1 34.7 49.3 69.1 (19.8) RewardBench Safety 59.9 55.5 77.3 65.5 52.0 77.1 39.9 79.2 86.5 (7.3) Reasoning 62.4 47.4 88.5 59.1 79.5 76.5 73.5 83.9 90.4 (6.5) Average 68.5 46.1 81.9 65.3 70.6 72.0 58.0 75.3 85.3 (10.0) MT-Bench 91.4 56.2 91.4 83.3 82.9 75.8 62.8 87.3 88.1 (0.8) Table 3. Results of reward modeling experiments on RewardBench [33] and MT-Bench [67], reported in classification accuracy (%). Improvements of DIST2Loss (dist) over the sft baseline are indicated with ."
        },
        {
            "title": "Models",
            "content": "GigaGAN [26] LDM-4 [44] VQGAN [15] VQGAN [15] LlamaGen [48]-sft LlamaGen-dist LlamaGen [48]-sft LlamaGen-dist Epoch #Params 569M 400M 227M 1.4B 111M 111M 343M 343M"
        },
        {
            "title": "Full",
            "content": "FID - - - - 10.03 9.41 4.24 4.18 IS - - - - 116.37 127.44 206.74 209.41 FID 3.45 3.60 18.65 15.78 6.44 6.27 3.08 3.04 IS 225.5 247.7 80.4 74.3 157.17 164.32 256.07 258.19 Table 4. Class-conditional image generation results on ImageNet [13], with guidance scale of 2.0, following the LlamaGen [48] setup. Values for all models, except LlamaGen-dist, are sourced from the original paper. LlamaGen models were trained for 300 epochs in Full configurations. For details on baseline scores, refer to Appendix A.3. Results. The results in Tab. 4 demonstrate that LlamaGen trained with DIST2Loss consistently outperforms the standard sft baseline across various model sizes. This performance advantage is observed at both early (50 epochs) and later (300 epochs) stages of training. 3.6. Ablation Study We further investigate the contribution of each design choice in DIST2Loss using the meta linear regression experiment detailed in Sec. 3.1. Setup. Three additional baselines are incorporated by independently ablating each component. First, we examine the impact of ablating Place value weighting or Contrastive loss for multi-token distances, as described in Sec. 2.3. We also assess the effect of substituting the Distance-aware target with label smoothing baseline [49] of 0.1. Each decimal place is tokenized separately with three precision points (e.g., 0.123), framing the task as three-token distance modeling problem. Results. As shown in Tab. 5, each component contributes positively to DIST2Losss performance. Notably, the label smoothing baseline, lacking distance-aware target, falls Figure 4. Illustration of token distance impact on image semantics. Each row shows an image encoded and decoded by the VQ model, with four central tokens modified by: the original token, random close token (top-10), random token, and random distant token (bottom-10). Tokens close to the original mostly maintain the images semantics, while random and distant tokens lead to visual distortions or introduce entirely new elements. random replacements cause visual distortions, and more distant tokens introduce new, unrelated concepts, as shown in the reconstructed images in Fig. 4. These findings highlight the strong influence of token distances on image semantics. Setup. Following the LlamaGen [48] training pipeline, we first extract discrete features from Vector Quantization (VQ) model and then train an autoregressive transformer on the quantized index tokens. We employ the 16x16 compression pretrained VQ model as detailed in the same paper. The choice of distance metric remains an open question in vector quantization literature; consistent with LlamaGen, we utilize MSE as the distance metric. To compute token distances, we apply MSE within the embedding space by leveraging the token embedding weights from the VQ model."
        },
        {
            "title": "Ablation",
            "content": "Llama-dist - Place value weighting - Contrastive loss - Distance-aware target Llama-sft MAE RMSE mean 0.092 0.098 0.099 0.099 0.113 std 0.017 0.016 0.015 0.016 0.016 mean 0.124 0.137 0.139 0.142 0. std 0.026 0.032 0.020 0.035 0.025 Table 5. Ablation study results for the meta linear regression experiment, with scores aggregated over ten random seeds. behind the dist model by wide margin. This outcome reinforces our hypothesis that modeling distance relationships is central to DIST2Losss performance gains. Building on these discretization methods, we propose novel training objective that embeds distance semantics into discrete autoregressive generation. Distance Modeling in Loss Functions. Metric-based objectives have shown effectiveness across applications, such as enhancing explainability in image classification [10] and boosting accuracy in few-shot learning [16]. Likewise, Earth Mover Distance Optimization (EMO) better aligns distributions in language modeling compared to traditional cross-entropy [43]. Unlike previous approaches, our method leverages inherent metric relationships within data for metric information and targets general foundational models rather than specific objectives, such as image classification. 4. Related Work 5. Conclusion Distance Modeling in Foundational Models. Extensions of LLMs are increasingly adapted for tasks that require precise spatial, temporal, and relational distance modeling. Vision-centric tasks like object detection and segmentation, which rely on generating spatial coordinates, are now addressed by multimodal LLMs [12, 58, 60, 65]. In LLM alignment, generative reward models mimic human feedback to guide instruction tuning [66, 67]. LLMs have also been applied in arithmetics [63] and timeseries forecasting [20, 24], where encoding relational and temporal proximities reduces predictive errors. Additionally, LLMs In have shown potential as function regressors [47, 51]. robotics, tasks such as manipulation and navigation represent action outputs explicitly through coordinates and joint rotations [5, 23, 34] or implicitly via discrete embeddings [39, 45]. LLMs are further adapted to fields like geospatial analysis [37], RNA structure prediction [64], and clinical outcome forecasting [68], where modeling distance relations is essential for understanding spatial and relational data. We introduce simple and general training objective for distance modeling in LLM-like architectures, broadly applicable across these diverse domains. Discretizing Continuous Distribution. The discretization of continuous distributions is well-studied area in statistics [9]. Discrete analogues of continuous distributions, such as the Laplace [19] and Gaussian [7], are commonly employed in differential privacy for efficient sampling, often in conjunction with federated learning [25]. For non-analytic continuous distributions, discrete approximations using vector quantization [52] and the GumbelSoftmax trick [22] are common, enabling categorical representations suitable for multimodal generation tasks such as image, video, and audio synthesis [14, 15, 62]. Recently, these quantization approaches have been adopted by general-purpose multimodal generative LLMs [17, 50, 54]. We presented DIST2Loss, framework that incorporates predefined distance relationships between tokens into the training of autoregressive discrete foundational models. By embedding distance-aware targets within the training objective, DIST2Loss effectively improves model convergence and data efficiency across tasks requiring structured output relationships. Our experiments demonstrate DIST2Losss versatility, achieving notable performance gains in reward modeling, visual grounding, robotic manipulation, and vector-quantized image representation tasks. This approach enables seamless adaptation to range of domains without added inference costs, highlighting the potential for broader applications in general-purpose foundational models. Future work includes: Broadening DIST2Losss use of informative target distributions to encompass contexts beyond metric spaces, such as relational and hierarchical structures. Extending DIST2Losss approach beyond the loss function to improve interpretation of numerical relationships within input data as well as outputs. Expanding DIST2Losss applicability to wider range of tasks, including time series forecasting, to further assess its flexibility and potential for generalization. 6. Limitations DIST2Loss, as modified loss function, presents the following limitations: 1) DIST2Loss improves generation only when output tokens exhibit metric relationships, without enhancing numeric understanding in context tokens. 2) With infinite data and computational resources, DIST2Loss cannot outperform the base cross-entropy, akin to the wellknown bias-variance trade-off [18], where methods using informative priors inherently introduce estimation bias. Nonetheless, such cases of data sufficiency are uncommon in practice, as the diverse problem domains foundational models address, as shown in our experiments, rarely meet this condition."
        },
        {
            "title": "References",
            "content": "[1] Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. 5, 6, 1 [2] AI@Meta. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 4, 6, 7, 1, 2 [3] Anthropic. Claude 3 haiku, 2024. Model version: claude-3haiku-20240307. Accessed: 2024-11-01. 6, 7 [4] Rishi Bommasani, Drew Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. arXiv preprint arXiv:2108.07258, 2021. [5] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 5, 6, 8 [6] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 6, 7 [7] Clement Canonne, Gautam Kamath, and Thomas Steinke. The discrete gaussian for differential privacy. Advances in Neural Information Processing Systems, 33:1567615688, 2020. 1, 3, 8 [8] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 4 [9] Subrata Chakraborty. Generating discrete analogues of continuous probability distributions-a survey of methods and constructions. Journal of Statistical Distributions and Applications, 2:130, 2015. 1, 8 [10] Hongjun Choi, Anirudh Som, and Pavan Turaga. Amcloss: Angular margin contrastive loss for improved exIn Proceedings of plainability in image classification. the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 838839, 2020. [11] Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Bingxiang He, Wei Zhu, Yuan Ni, Guotong Xie, Ruobing Xie, Yankai Lin, et al. Ultrafeedback: Boosting language models with scaled ai feedback. In Forty-first International Conference on Machine Learning, 2024. 6, 7 [12] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. 8 [13] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 7 [14] Prafulla Dhariwal, Heewoo Jun, Christine Payne, Jong Wook Kim, Alec Radford, and Ilya Sutskever. Jukebox: generative model for music. arXiv preprint arXiv:2005.00341, 2020. 1, 8 [15] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming In Protransformers for high-resolution image synthesis. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. 1, 7, [16] Farong Gao, Lijie Cai, Zhangyi Yang, Shiji Song, and Cheng Wu. Multi-distance metric network for few-shot learning. International Journal of Machine Learning and Cybernetics, 13(9):24952506, 2022. 8 [17] Yuying Ge, Sijie Zhao, Ziyun Zeng, Yixiao Ge, Chen Li, Xintao Wang, and Ying Shan. Making llama see and draw with seed tokenizer. In The Twelfth International Conference on Learning Representations, 2024. 8 [18] Stuart Geman, Elie Bienenstock, and Rene Doursat. Neural networks and the bias/variance dilemma. Neural computation, 4(1):158, 1992. 8 [19] Arpita Ghosh, Tim Roughgarden, and Mukund Sundararajan. Universally utility-maximizing privacy mechanisms. In Proceedings of the forty-first annual ACM symposium on Theory of computing, pages 351360, 2009. 8 [20] Nate Gruver, Marc Finzi, Shikai Qiu, and Andrew Wilson. Large language models are zero-shot time series forecasters. Advances in Neural Information Processing Systems, 36, 2024. 8 [21] Hamish Ivison, Yizhong Wang, Jiacheng Liu, Zeqiu Wu, Valentina Pyatkin, Nathan Lambert, Noah Smith, Yejin Choi, and Hannaneh Hajishirzi. Unpacking dpo and ppo: Disentangling best practices for learning from preference feedback. arXiv preprint arXiv:2406.09279, 2024. 6, 7 [22] Eric Jang, Shixiang Gu, and Ben Poole. Categorical reparameterization with gumbel-softmax. In International Conference on Learning Representations, 2022. [23] Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li Fei-Fei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: Robot manipulation In International Conference on with multimodal prompts. Machine Learning, pages 1497515022. PMLR, 2023. 5, 6, 8, 1, 3 [24] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, YuanFang Li, Shirui Pan, et al. Time-llm: Time series forecasting by reprogramming large language models. arXiv preprint arXiv:2310.01728, 2023. 8 [25] Peter Kairouz, Ziyu Liu, and Thomas Steinke. The distributed discrete gaussian mechanism for federated learning with secure aggregation. In International Conference on Machine Learning, pages 52015212. PMLR, 2021. 8 [26] Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1012410134, 2023. 7 [27] Vladimir Karpukhin, Barlas Oguz, Sewon Min, Patrick Lewis, Ledell Wu, Sergey Edunov, Danqi Chen, and Wentau Yih. Dense passage retrieval for open-domain question answering. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 67696781, 2020. 4 [28] Sahar Kazemzadeh, Vicente Ordonez, Mark Matten, and Tamara Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. 5, 6, [29] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, et al. Prometheus: Inducing fineIn The grained evaluation capability in language models. Twelfth International Conference on Learning Representations, 2024. 7 [30] Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535, 2024. 6, 7 [31] Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization. In ICLR (Poster), 2015. 1 [32] Solomon Kullback and Richard Leibler. On information and sufficiency. The annals of mathematical statistics, 22(1): 7986, 1951. 2 [33] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, et al. Rewardbench: Evaluating reward models for language modeling. arXiv preprint arXiv:2403.13787, 2024. 6, 7, [34] Xiang Li, Cristina Mata, Jongwoo Park, Kumara Kahatapitiya, Yoo Sung Jang, Jinghuan Shang, Kanchana Ranasinghe, Ryan Burgert, Mu Cai, Yong Jae Lee, et al. Llara: Supercharging robot learning data for vision-language policy. arXiv preprint arXiv:2406.20095, 2024. 1, 5, 6, 8 [35] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. 5, 1 [36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2019. 1 [37] Rohin Manvi, Samar Khanna, Gengchen Mai, Marshall Burke, David Lobell, and Stefano Ermon. Geollm: Extracting geospatial knowledge from large language models. In The Twelfth International Conference on Learning Representations, 2024. 8 [38] Junhua Mao, Jonathan Huang, Alexander Toshev, Oana Camburu, Alan Yuille, and Kevin Murphy. Generation and comprehension of unambiguous object descriptions. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 1120, 2016. 5, 6, 1 [39] Luke Metz, Julian Ibarz, Navdeep Jaitly, and James Davidson. Discrete sequential prediction of continuous actions for deep rl. arXiv preprint arXiv:1705.05035, 2017. 8 [40] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. [41] Alec Radford et al."
        },
        {
            "title": "Improving language understanding by",
            "content": "generative pre-training. OpenAI, 2018. 1 [42] Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. {Zero-offload}: Democratizing {billionIn 2021 USENIX Annual Technical scale} model training. Conference (USENIX ATC 21), pages 551564, 2021. 1 [43] Siyu Ren, Zhiyong Wu, and Kenny Zhu. Emo: Earth mover distance optimization for auto-regressive language In The Twelfth International Conference on modeling. Learning Representations, 2024. 8 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 7 [45] Nur Muhammad Shafiullah, Zichen Cui, Ariuntuya Arty Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning modes with one stone. In Advances in neural information processing systems, pages 2295522968, 2022. 8 [46] Claude Elwood Shannon. mathematical theory of communication. The Bell system technical journal, 27(3):379423, 1948. 3 [47] Xingyou Song, Oscar Li, Chansoo Lee, Bangding Yang, Daiyi Peng, Sagi Perel, and Yutian Chen. Omnipred: Language models as universal regressors. arXiv preprint arXiv:2402.14547, 2024. [48] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. 7, 1 [49] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the inception architecture for computer vision. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 28182826, 2016. 7 [50] Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. 8 [51] Robert Vacareanu, Vlad Andrei Negru, Vasile Suciu, and Mihai Surdeanu. From words to numbers: Your large language Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusion-tokenizer is key to visual In The Twelfth International Conference on generation. Learning Representations, 2024. 1, 8 [63] Zheng Yuan, Hongyi Yuan, Chuanqi Tan, Wei Wang, and Songfang Huang. How well do large language models perform in arithmetic tasks? arXiv preprint arXiv:2304.02015, 2023. 1, [64] LI Zablocki, LA Bugnon, Gerard, Di Persia, Stegmayer, and DH Milone. Comprehensive benchmarking of large language models for rna secondary structure prediction. arXiv preprint arXiv:2410.16212, 2024. 8 [65] Haotian Zhang, Haoxuan You, Philipp Dufter, Bowen Zhang, Chen Chen, Hong-You Chen, Tsu-Jui Fu, William Yang Wang, Shih-Fu Chang, Zhe Gan, et al. Ferretv2: An improved baseline for referring and grounding with large language models. arXiv preprint arXiv:2404.07973, 2024. 5, 6, 8 [66] Lunjun Zhang, Arian Hosseini, Hritik Bansal, Mehran Kazemi, Aviral Kumar, and Rishabh Agarwal. Generative verifiers: Reward modeling as next-token prediction. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. 6, 8 [67] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In Advances in Neural Information Processing Systems, pages 4659546623, 2023. 6, 7, 8 [68] Wenhao Zheng, Dongsheng Peng, Hongxia Xu, Yun Li, Hongtu Zhu, Tianfan Fu, and Huaxiu Yao. Multimodal clinical trial outcome prediction with large language models. arXiv preprint arXiv:2402.06512, 2024. 8 model is secretly capable regressor when given in-context examples. In First Conference on Language Modeling, 2024. 8 [52] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. In Advances in neural information processing systems, 2017. [53] Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouedec. Trl: Transformer reinforcement learning, 2020. 1 [54] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. arXiv preprint arXiv:2409.18869, 2024. 8 [55] Zhilin Wang, Yi Dong, Olivier Delalleau, Jiaqi Zeng, Gerald Shen, Daniel Egert, Jimmy Zhang, Makesh Narsimhan Sreedhar, and Oleksii Kuchaiev. Helpsteer2: Open-source dataset for training top-performing reward models. arXiv preprint arXiv:2406.08673, 2024. 6, 7, 1 [56] Ronald Williams and David Zipser. learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270280, 1989. 2 [57] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 38 45, Online, 2020. Association for Computational Linguistics. 1 [58] Bin Xiao, Haiping Wu, Weijian Xu, Xiyang Dai, Houdong Hu, Yumao Lu, Michael Zeng, Ce Liu, and Lu Yuan. Florence-2: Advancing unified representation for variety of vision tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 4818 4829, 2024. 1, 5, 6, 8 [59] Bin Yan, Yi Jiang, Jiannan Wu, Dong Wang, Ping Luo, Zehuan Yuan, and Huchuan Lu. Universal instance percepIn Proceedings of tion as object discovery and retrieval. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1532515336, 2023. 5, [60] Haoxuan You, Haotian Zhang, Zhe Gan, Xianzhi Du, Bowen Zhang, Zirui Wang, Liangliang Cao, Shih-Fu Chang, and Yinfei Yang. Ferret: Refer and ground anything anywhere at any granularity. In The Twelfth International Conference on Learning Representations, 2024. 5, 6, 8 [61] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling context in referring expresIn Computer VisionECCV 2016: 14th European sions. Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 6985. Springer, 2016. 5, 6, 1 [62] Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng,"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Implementation Details A.1. Global Setups We use the HuggingFace Trainer [57] and TRL trainer [53] with DeepSpeed ZeRO-3 [42] and the AdamW optimizer [36]. The base foundational models are detailed in Tab. 6, with computational requirements specified in Tab. 7. A.2. Task-Specific Setups Toy: Learning to Regress. The learning rate is set to 2e5 with linear decay schedule and no warmup. Training epochs are configured to ensure each model is exposed to approximately 250 samples to prevent underfitting. For example, with training dataset size of 2, the epoch count is set to 125. Each experiment is repeated five times with random seeds [1 : 5] for statistical stability. Textual: Generative Reward Modeling. For finetuning, the helpsteer2 dataset [55] was reformatted into an instruction-following structure, where scores for each of the five categories were designated as model outputs. The model was trained for two epochs with learning rate of 1 105 using the paged Adam optimizer [31]. The prompt used during training is illustrated in Fig. 5. During inference, logit-based score prediction function was implemented to evaluate two samples by generating score probabilities on 0-20 points scale. The model calculated weighted averages from the softmax probabilities, assigning final reward based on higher scores for preferred outputs. Multimodal: Visual Grounding. For fine-tuning, we concatenate the training sets of RefCOCO, RefCOCO+, and RefCOCOg [28, 38, 61]. All images are resized to 1024 1024 to constrain the range of generated digits, with coordinate values rounded to the nearest integer. During inference, outputs that cannot be parsed as bounding box coordinates are considered incorrect. Training is conducted with learning rate of 2e5, 100 steps of linear warmup, and total of three epochs. Embodied: Robotic Manipulation. We convert the VIMA dataset [23] into an instruction-tuning-compatible format using the provided script from the LLaRA [34] repository . The pretrained LLaVA-1.5 [35] model is then fine-tuned on the object manipulation task. Following [34], we incorporate auxiliary objective augmentations from the same repository into the training set. We the oracle object detection labels for evaluation. Training is conducted with learning rate of 2e5, using 0.3 ratio of linear warmup Experiment Type Toy (3.1) Textual (3.4) Multimodal (3.2) Embodied (3.3) High-Dimension (3.5) Size 1B 8B 3.8B 7B Backbone meta-llama/Llama-3.2-1B-Instruct [2] meta-llama/Llama-3.1-8B-Instruct [2] microsoft/Phi-3-mini-4k-instruct [1] liuhaotian/llava-v1.5-7b [35] 343M Scratch [48] Table 6. Base foundational models used for finetuning in each experiment type. Experiment Type Toy (3.1) Textual (3.4) Multimodal (3.2) Embodied (3.3) High-Dimension (3.5) GPU Model VRAM (GB) RTX 3090 A6000 A6000 L40S L40S 24 48 48 48 48 # GPUs 1 4 4 8 8 Table 7. Computational requirements for each experiment are reported per single run; multiple runs may be needed depending on configuration or random seeds. and cosine decay over two epochs. High-Dimension: Image Generation. We employ the pretrained image vector quantization model from the LlamaGen [48] repository . All images are resized to 384 384 using random center cropping. During evaluation, images are generated at 384 384 and then resized to 256 256 for model-based metric computations. Classifier-free guidance with scale of 2.0 is applied during inference. Experimental protocols strictly adhere to the repositorys guidelines. A.3. Baseline Scores Embodied: Robotic Manipulation. For LLaRAsft, we adopt results from Tables 15, 17, and 19 of the original paper [34], using D-inBC + Aux with all six auxiliary tasks (epoch: 2, iteration: 14) for data sizes of 0.8k, 8k, and 80k. Notably, at the 80k scale, using all auxiliary tasks does not outperform using only subset, as reported in Table 1 of the same paper. However, we adopt the former for consistency and generalizability across different scales. High-Dimension: Image Generation. We use the classconditional ImageNet 256256 results with CFG 2.0 from Table 9 of the LlamaGen paper [48] as baselines. B. Extended Quantitative Results Toy: Learning to Regress. We provide scores corresponding to Fig. 3 in the main paper in Tab. 8. Textual Task: Generative Reward Modeling. De-"
        },
        {
            "title": "Prompt for Generative Reward Evaluation",
            "content": "Please act as an impartial judge and evaluate the quality of the response provided by AI assistant to the user question displayed below. Your evaluation should consider five factors helpfulness, correctness, coherence, complexity, verbosity. Heres brief explanation of each factor: - Helpfulness: Overall helpfulness of the response to the prompt. - Correctness: Inclusion of all pertinent facts without errors. - Coherence: Consistency and clarity of expression. - Complexity: Intellectual depth required to write response (i.e. whether the response can be written by anyone with basic language competency or requires deep domain expertise). - Verbosity: Amount of detail included in the response, relative to what is asked for in the prompt. Do not allow the length of the responses to influence your evaluation. Be as objective as possible. Please first provide an overall score over model response. You must provide overall score as number between 0 and 20. Then provide set of 5 score over model response. Only provide the score as number between 0 and 4. [User Question] {user input} [Start of Model Response] {model response to evaluate} [End of Model Response] Figure 5. Instruction-tuning prompt template for generative reward modeling. Models mean std mean std mean std mean std mean std mean std mean std mean std mean std mean std MAE RMSE MAE RMSE MAE RMSE MAE RMSE MAE RMSE Training Problems: 1 Training Problems: 2 Training Problems: Training Problems: 4 Training Problems: 5 Llama-3.2 1B [2]-sft Llama-3.2 1B-vocab Llama-3.2 1B-dist 0.2 0.2 0.21 0.039 0.039 0.032 0.243 0.243 0. 0.05 0.05 0.037 0.182 0.157 0.17 0.014 0.022 0.006 0.221 0.202 0.212 0.02 0.023 0.008 0.17 0.167 0. 0.039 0.031 0.032 0.216 0.215 0.198 0.056 0.05 0.035 0.152 0.127 0.114 0.018 0.007 0.017 0.2 0.166 0. 0.035 0.016 0.022 0.159 0.133 0.122 0.03 0.006 0.023 0.211 0.172 0.165 0.042 0.009 0.032 Training Problems: Training Problems: 7 Training Problems: 8 Training Problems: 9 Training Problems: 10 Llama-3.2 1B-sft Llama-3.2 1B-vocab Llama-3.2 1B-dist 0.144 0.122 0. 0.024 0.016 0.018 0.185 0.162 0.153 0.032 0.017 0.021 0.129 0.141 0.104 0.011 0.034 0.013 0.173 0.189 0. 0.02 0.046 0.031 0.119 0.122 0.104 0.017 0.029 0.035 0.154 0.164 0.15 0.019 0.035 0.081 0.115 0.122 0. 0.012 0.014 0.053 0.154 0.163 0.163 0.022 0.023 0.093 0.113 0.111 0.092 0.016 0.008 0.017 0.154 0.151 0. 0.025 0.014 0.026 Table 8. Meta linear regression experiment results on one to ten training problems and 1,000 test problems, with scores averaged over five random seeds. Models Model Type Average Llama-3.1-8B [2]-binary Llama-3.1-8B-sft Llama-3.1-8B-dist Seq. Classifier Generative Generative 58 75.3 85.3 Easy 94.5 89.0 97.0 AlpacaEval Hard 94.7 97.9 98.9 Length 76.3 77.9 88.4 LLMBar Adver. GPTInst Adver. GPTOut Adver. Manual Adver. Neighbor Natural Llama-3.1-8B-binary Llama-3.1-8B-sft Llama-3.1-8B-dist 13.6 32.6 57.6 36.2 63.8 72.3 23.9 32.6 67.4 24.6 29.1 63.4 61.5 82.0 84. Do-Not-Answer 16.9 44.9 78.7 MATH PRM 92.8 84.1 84.1 CPP 54.9 84.1 89.6 GO 55.8 80.5 90.2 HumanEvalPack Javascript 52.4 83.5 87.8 Python 48.5 84.1 90. Java 56.1 89.0 89.6 Rust 56.7 81.1 85.4 MT-Bench Refusal XSTest - - - - - Easy Hard Medium Dangerous Offensive 64.3 96.4 100.0 62.5 90.0 92.5 62.1 78.3 75.7 0.3 99.0 100 0.4 93.0 96.0 Should Refuse 22.7 92.9 94. Should Respond 92.0 76.0 88.0 Table 9. Fine-grained statistics on model performance on RewardBench [33]. tailed results for each data source in RewardBench [33] are reported in Tab. 9. paring the base cross-entropy loss with our proposed DIST2Loss. C. Additional Qualitative Samples Textual: Generative Reward Modeling. Figure 7 shows inference results of Llama-based generative reward model trained with DIST2Loss. Multimodal: Visual Grounding. Figure 8 presents qualitative results from visual grounding experiments, comInstruction-Response Pair Sample for Object Manipulation [Prompt] <image0> <task>Sweep any <p>polka dot small block</p> at <b>(0.500, 0.617), 0.203, 0.578</b> into <p>blue three-sided rectangle</p> at <b>(0.500, 0.625), 0.297, 0.398</b> without exceeding <p>plastic line</p> at <b>(0.500, 0.617), 0.328, 0.047</b>.</task> Can you explain what needs to be done to perform the task in this scene? Every action you take must include two locations in the format of <b>(x, y)</b> and one clockwise rotation angle in the format of <r>[r]</r>. The first location is the image coordinate where you start to sweep the object using spatula, and the second location is where you stop sweeping. The image coordinate ranges from 0 to 1. The rotation angle indicates how many degrees you rotate the spatula clockwise, and it ranges from -359 to 359. [Answer] Step 1: Sweep the <p>polka dot small block</p> at <b>(0.629, 0.805)</b>, rotate <r>[0]</r> degrees, and stop at <b>(0.500, 0.609)</b>. Figure 6. Example of an instruction-response pair from the VIMA dataset [23], illustrating an object manipulation task. Metric-related output tokens are bolded for emphasis. Query: Who created the Superman cartoon character? Chosen Answer: Superman, the iconic comic book superhero, was created by writer Jerry Siegel and artist Joe Shuster. Superman first appeared in Action Comics #1, which was published by Detective Comics, Inc. (later DC Comics) in June 1938. The characters immense popularity established him as one of the most enduring and recognizable figures in the superhero genre. DIST2Loss helpfulness score expectation: 2.81 rejected Answer: The Superman cartoon character was created by Jerry Siegel and Joe Shuster in 1938. DIST2Loss helpfulness score expectatione: 0.96 Query: Hi, Im in the mood for Bloody Mary. Can you give me recipe for making one? Chosen Answer: Certainly! The Bloody Mary is classic cocktail known for its bold flavor and is commonly enjoyed as brunch drink. Heres basic recipe for making single serving: ### Ingredients: - 1 1/2 ounces (45 ml) vodka - 3 ounces (90 ml) tomato juice - 1/2 ounce (15 ml) lemon juice, freshly squeezed - 1 dash of Worcestershire sauce - Tabasco sauce (to taste) - Ground black pepper (to taste) - Celery salt (to taste) - Horseradish (optional, to taste) - 1 celery stalk (for garnish) - 1 lemon wedge or lime wedge (for garnish) - Ice cubes ### Instructions: 1. **Prepare the Glass:** - Rub the lemon or lime wedge along the lip of tall glass (often highball glass). - Dip the wetted rim into celery salt if you like salted rim. 2. **Mix the Drink:** - Fill the glass with ice cubes. - Add vodka, tomato juice, and lemon juice to the glass. - Add Worcestershire sauce, few dashes of Tabasco, pinch of celery salt, ground black pepper, and horseradish (if using). - Stir well to combine all the ingredients. 3. **Garnish:** - Garnish with celery stalk and lemon or lime wedge on the rim. 4. **Adjust and Serve:** - Taste and adjust the seasoning, adding more Tabasco, lemon juice, or other ingredients as necessary. - Serve immediately. ### Variations and Tips: - **Garnish Variations:** You can also garnish with olives, pickles, cocktail onions, or even strip of crispy bacon. - **Spice Level:** Adjust the spiciness to your liking with more or less Tabasco or by adding hot sauce of your choice. - **Clamato:** Some people like to use Clamato (a mixture of clam and tomato juices) instead of plain tomato juice for different twist known as Bloody Caesar. - **Make it Virgin:** To make non-alcoholic version, simply omit the vodka for Virgin Mary. Feel free to customize your Bloody Mary to your taste preferences. Enjoy your cocktail! DIST2Loss Overall score expectation: 1.33 rejected Answer: Sure, heres quick and easy recipe for making delicious Bloody Mary: Ingredients: - 2 ounces vodka - 3-4 dashes Worcestershire sauce - 1-2 teaspoons hot sauce - 1 lime wedge for garnish - 2 ounces tomato juice - 1 ounce horseradish - 1/2 ounce beet juice - 1/2 ounce olive juice Instructions: Fill an Old-Fashioned glass with ice cubes. In shaker, combine the vodka, Worcestershire sauce, hot sauce, lime wedge, and tomato juice. Shake well to combine. Strain the mixture into the prepared glass and top off with beet juice and olive juice. Garnish with lime wedge and serve. DIST2Loss Overall score expectation: 1.20 Figure 7. Qualitative examples from the generative reward modeling experiment. Figure 8. Qualitative examples from the visual grounding experiment. Top: our proposed DIST2Loss loss demonstrates higher visual grounding accuracy compared to the standard cross-entropy loss. Bottom: manual examination of inference results reveals that substantial portion of the RefCOCO dataset [28, 38, 61] contains labels that are ambiguous, even for human annotators, which may lead to underestimation of model performance."
        }
    ],
    "affiliations": [
        "LG AI Research",
        "Yonsei University"
    ]
}