{
    "paper_title": "PUSA V1.0: Surpassing Wan-I2V with $500 Training Cost by Vectorized Timestep Adaptation",
    "authors": [
        "Yaofang Liu",
        "Yumeng Ren",
        "Aitor Artola",
        "Yuxuan Hu",
        "Xiaodong Cun",
        "Xiaotong Zhao",
        "Alan Zhao",
        "Raymond H. Chan",
        "Suiyun Zhang",
        "Rui Liu",
        "Dandan Tu",
        "Jean-Michel Morel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The rapid advancement of video diffusion models has been hindered by fundamental limitations in temporal modeling, particularly the rigid synchronization of frame evolution imposed by conventional scalar timestep variables. While task-specific adaptations and autoregressive models have sought to address these challenges, they remain constrained by computational inefficiency, catastrophic forgetting, or narrow applicability. In this work, we present Pusa, a groundbreaking paradigm that leverages vectorized timestep adaptation (VTA) to enable fine-grained temporal control within a unified video diffusion framework. Besides, VTA is a non-destructive adaptation, which means it fully preserves the capabilities of the base model. By finetuning the SOTA Wan2.1-T2V-14B model with VTA, we achieve unprecedented efficiency -- surpassing the performance of Wan-I2V-14B with $\\leq$ 1/200 of the training cost (\\$500 vs. $\\geq$ \\$100,000) and $\\leq$ 1/2500 of the dataset size (4K vs. $\\geq$ 10M samples). Pusa not only sets a new standard for image-to-video (I2V) generation, achieving a VBench-I2V total score of 87.32\\% (vs. 86.86\\% of Wan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as start-end frames and video extension -- all without task-specific training. Meanwhile, Pusa can still perform text-to-video generation. Mechanistic analyses reveal that our approach preserves the foundation model's generative priors while surgically injecting temporal dynamics, avoiding the combinatorial explosion inherent to vectorized timesteps. This work establishes a scalable, efficient, and versatile paradigm for next-generation video synthesis, democratizing high-fidelity video generation for research and industry alike. Code is open-sourced at https://github.com/Yaofang-Liu/Pusa-VidGen"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 6 1 1 6 1 . 7 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "PUSA V1.0: SURPASSING WAN-I2V WITH $500 TRAINING COST BY VECTORIZED TIMESTEP ADAPTATION Yaofang Liu1,7 Yumeng Ren1,7 Aitor Artola1,7 Yuxuan Hu2,3 Xiaodong Cun4 Xiaotong Zhao5 Alan Zhao5 Raymond H. Chan6,7 Rui Liu3 Dandan Tu3 Jean-Michel Morel1 Suiyun Zhang3 1City University of Hong Kong 4Great Bay University 7Hong Kong Centre for Cerebro-Cardiovascular Health Engineering Corresponding authors 2The Chinese University of Hong Kong 5AI Technology Center, Tencent PCG 6Lingnan University 3Huawei Research Project Page: https://yaofang-liu.github.io/Pusa_Web/ Figure 1: Overview of Pusas performance and efficiency. Specifically, Pusa outperforms Wan-I2V on Vbench-I2V with only 1/2500 dataset, 1/200 training budget, and 1/5 inference steps. Besides, Wan-I2V can only do image-to-video generation, while the same Pusa model has many other capabilities including: start-end frames, video extension, text-to-video, and so on."
        },
        {
            "title": "ABSTRACT",
            "content": "The rapid advancement of video diffusion models has been hindered by fundamental limitations in temporal modeling, particularly the rigid synchronization of frame evolution imposed by conventional scalar timestep variables. While task-specific adaptations and autoregressive models have sought to address these challenges, they remain constrained by computational inefficiency, catastrophic Work partially done during an internship at Huawei Research."
        },
        {
            "title": "Preprint",
            "content": "In this work, we present Pusa1, groundforgetting, or narrow applicability. breaking paradigm that leverages vectorized timestep adaptation (VTA) to enable fine-grained temporal control within unified video diffusion framework. Besides, VTA is non-destructive adaptation, which means it fully preserves the capabilities of the base model. By finetuning the SOTA Wan2.1-T2V-14B model with VTA, we achieve unprecedented efficiencysurpassing the performance of Wan-I2V-14B with 1/200 of the training cost ($500 vs. $100,000) and 1/2500 of the dataset size (4K vs. 10M samples). Pusa not only sets new standard for image-to-video (I2V) generation, achieving VBench-I2V total score of 87.32% (vs. 86.86% of Wan-I2V-14B), but also unlocks many zero-shot multi-task capabilities such as start-end frames and video extension all without task-specific training. Meanwhile, Pusa can still perform text-to-video generation. Mechanistic analyses reveal that our approach preserves the foundation models generative priors while surgically injecting temporal dynamics, avoiding the combinatorial explosion inherent to vectorized timesteps. This work establishes scalable, efficient, and versatile paradigm for next-generation video synthesis, democratizing high-fidelity video generation for research and industry alike. Code will be open-sourced at https: //github.com/Yaofang-Liu/Pusa-VidGen"
        },
        {
            "title": "INTRODUCTION",
            "content": "The advent of diffusion models (Song et al., 2020; Ho et al., 2020) has heralded paradigm shift in generative modeling, particularly in the domain of image synthesis. These models, which leverage an iterative noise reduction process, have demonstrated remarkable efficacy in producing highfidelity samples. Naturally, extending this framework to video generation (Ho et al., 2022; He et al., 2022; Chen et al., 2023; Wang et al., 2023; Ma et al., 2024; OpenAI, 2024; Xing et al., 2023b; Liu et al., 2024a) has been focal point, yet it exposes fundamental limitations in modeling complex temporal dynamics. Conventional video diffusion models (VDMs) typically employ scalar timestep variables, enforcing uniform temporal evolution across all frames. This approach, while effective for text-to-video (T2V) clips generation, struggles with nuanced temporal dependencies task like image-to-video (I2V) generation, as highlighted in the FVDM work (Liu et al., 2024b) which solved this problem by introducing vectorized timestep approach to enable independent frame evolution. Concurrently, methods like Diffusion Forcing (Chen et al., 2024) and AR-Diffusion (Sun et al., 2025) also explored autoregressive paradigms to avoid this rigid synchronization modeling form of conventional VDMs. Nonetheless, their applications w.r.t. video generation remained constrained by the autoregressive designs with token/frame-level noise. Large-scale models such as MAGI-1 (Teng et al., 2025) and SkyReels V2 Chen et al. (2025) advanced scalability but still faced challenges in balancing computational efficiency and multi-task capability. In this work, we bridge the gap between theoretical innovation and industrial deployment by extending the FVDM framework to industrial scale through fine-tuning on the SOTA open-source T2V model Wan2.1-T2V-14B (Wan-T2V) Wan et al. (2025) with vectorized timestep adaptation (VTA). Our key insight is to leverage the vectorized timestep variable (VTV) designs of FVDM within robust large-model ecosystem, enabling efficient adaptation to diverse video generation tasks, especially I2V, while drastically reducing computational and data requirements. As demonstrated in Table 1, our approach (Pusa) achieves landmark improvement: with thousands of times smaller datasets (4K samples vs. 10M ) and hundreds of times less computation (training cost reduced to 0.5Kvs. 100K), we surpass prior art Wan2.1-I2V-14B (Wan-I2V) Wan et al. (2025) on the VBench-I2V benchmark Huang et al. (2024). Notably, Pusa not only achieves higher total score (87.32% vs. 86.86%) with superior I2V quality (94.84% vs. 92.90%), but also extends capabilities beyond T2V and I2V generation to support start-end frame generation, video extension, 1Pusa (/pu: sA:/) normally refs to Thousand-Hand Guanyin in Chinese, reflecting the iconography of many hands to symbolize her boundless compassion and ability. We use this name to indicate that our model uses many timestep variables to achieve numerous video generation capabilities, and we will fully open source it to let the community benefit from this tech."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Paradigm comparison between (c) Pusa and (b) Wan-I2V, both support image-to-video (I2V) generation, and are finetuned from text-to-video model (a) Wan-T2V. Specifically, Wan-I2V modifies the model with an additional mask mechanism and adds clip embedding of the condition image to enable I2V capability. However, this is destructive adaptation of the original model that changes the models input and internal calculation process, which indicates it cannot fully utilize the pretrained priors of the base model. In contrast, our proposed model, Pusa, only inflates the models timestep variable from scalar to vector, which is non-destructive adaptation. With this method, Pusa can fully utilize the pretrained priors and use much less resources to learn temporal dynamics. Regarding the I2V task, Pusa achieves unprecedented efficiency, surpassing Wan-I2V with 1/2500 training data, revolutionizing the video diffusion paradigm. and other complex temporal tasksall within single unified model. This marks critical departure from previous models like Wan-I2V, which are limited to I2V and require prohibitive resources. The core of our innovation lies in the synergistic combination of FVDMs temporal modeling prowess with the SOTA generative capacity of Wan-T2V, optimized through lightweight finetuning strategy. By preserving the vectorized timestep formulation, each frame evolves along its independent temporal trajectory during diffusion, enabling the model to capture intricate inter-frame dependencies without global synchronization. This architecture not only enhances temporal coherence but also enables zero-shot generalization to new tasks, as validated by our results in I2V generation without task-specific training. Our contributions can be summarized as: Industrial-Scale Efficiency: We demonstrate the first large-model adaptation of FVDM, achieving unprecedented data efficiency ( 1/2500 dataset size) and computational efficiency ( 1/200 training cost) compared to Wan-I2V, revolutionizing the video diffusion paradigm. Multi-Task Generalization: our proposed model supports not only T2V and I2V, but also start-end frames, video extension, and more without additional training. Quality-Throughput Tradeoff: Despite significantly reduced resources, Pusa achieves superior total score (87.32% vs. 86.86%) on Vbench-I2V, proving the FVDM paradigm works well for large foundational models and greatly exceeds previous methods. This work represents pivotal step toward democratizing advanced video generation: by unlocking the full potential of the FVDM paradigm within practical, scalable framework, we enable highfidelity, multi-task video synthesis accessible to researchers and industries alike. Through rigorous benchmarking and novel fine-tuning strategies, we establish that temporal modeling innovation, when paired with strategic large-model adaptation, can overcome the long-standing tradeoff between performance, efficiency, and versatility in video diffusion."
        },
        {
            "title": "2 METHODOLOGY",
            "content": "This section details the mathematical framework for our proposed Pusa model. We begin by reviewing the fundamentals of flow matching for generative modeling. Subsequently, we introduce the concept of vectorized timestep variable from FVDM. Finally, we integrate these concepts to formulate the Pusas objective and describe the video generation process."
        },
        {
            "title": "2.1 PRELIMINARIES: FLOW MATCHING FOR GENERATIVE MODELING",
            "content": "Generative modeling aims to learn model capable of synthesizing samples from target data distribution q0(z) over RD. Continuous Normalizing Flows (CNFs) (Chen, 2018) achieve this by transforming samples z1 from simple base distribution q1(z) (e.g., standard Gaussian (0, I)) to samples z0 that approximate the target distribution q0(z). This transformation is defined by an invertible mapping, often conceptualized as an ordinary differential equation (ODE) trajectory. Specifically, probability path {zt}t[0,1] is defined, connecting z0 q0 at = 0 to z1 q1 at = 1. The dynamics along this path are described by an ODE: dzt dt = vt(zt, t), [0, 1] (1) where vt : RD [0, 1] RD is time-dependent vector field. Flow Matching (FM) (Lipman et al., 2022; Liu et al., 2022; Tong et al., 2023) is simulationfree technique to directly learn this vector field vt(zt, t) by training neural network vθ(zt, t) to approximate it. This is achieved by regressing vθ(zt, t) against target vector field ut(ztz0, z1). This target field is defined along specified probability paths pt(ztz0, z1) that connect samples z0 q0 to corresponding samples z1 q1. common choice for these paths is linear interpolation between data sample z0 and prior sample z1: For such paths, the conditional target vector field is the time derivative of zt: zt = (1 t)z0 + tz1, [0, 1] ut(z0, z1) = dzt dt = z1 z0 (2) (3) Note that for linear interpolation paths, ut is independent of and zt, depending only on the endpoints z0 and z1. The (conditional) flow matching objective function to train the neural network vθ is then: LFM(θ) = EtU [0,1],z0q0,z1q1 vθ((1 t)z0 + tz1, t) (z1 z0)2 (4) (cid:104) (cid:105) where U[0, 1] is the uniform distribution over [0, 1] and 2 2 denotes the squared Euclidean norm. Once vθ is trained, new samples approximating q0 can be generated by first sampling z1 q1 and then solving the ODE dzt dt = vθ(zt, t) from = 1 down to = 0. The resulting z0 is generated sample. 2.2 FRAME-AWARE FLOW MATCHING We now extend the flow matching framework to video generation with the vectorized timestep proposed by FVDM Liu et al. (2024b), allowing for nuanced temporal modeling in video generation, which differs from conventional video diffusion or flow matching models. video clip is represented as sequence of frames. Each frame xi Rd is d-dimensional column vector. The entire video clip can be represented as an matrix X, where the i-th row is xi. This can be written as = [x1, x2, . . . , xN ], thus RN d. In contrast to the single scalar time variable used in standard flow matching (Eq. 4), we introduce vectorized timestep variable τ [0, 1]N , defined as: τ = [τ 1, τ 2, . . . , τ ] (5) Here, each component τ [0, 1] represents the individual progression parameter of the i-th frame along its respective probability path from the data distribution to prior distribution. This vectorization allows each frame to evolve at potentially different rate or stage within the generative process. Let X0 = [x1 Rd. Similarly, let X1 = [x1 (e.g., each frame xi 0 1 ] be video sampled from simple prior distribution qprior(X) 0 ] be video sampled from the true data distribution qdata(X), where xi 1 is independently drawn from (0, σ2Id)). 0, . . . , xN 1, . . . , xN"
        },
        {
            "title": "Preprint",
            "content": "For each frame i, we define conditional probability path p(xi 1) indexed by its individual timestep τ i. Adopting the linear interpolation strategy from Eq. 2 for each frame (which are ddimensional vectors): τ ixi 0, xi (6) The state of the entire video, corresponding to specific vectorized timestep τ , is then given by the matrix Xτ , whose i-th row is xi τ : Xτ = [x1 τ = (1 τ i)xi xi 0 + τ ixi τ 2, . . . , xN τ 1, x2 τ ] (7) We aim to learn single neural network vθ(X, τ ) that models the joint dynamics of all frames conditioned on their respective timesteps. This network takes the current video state RN (which is Xτ during training) and the vectorized timestep τ [0, 1]N as input. It outputs velocity field for the entire video, an matrix denoted as vθ(X, τ ) = [v1, . . . , vN ], where each vi Rd is the velocity vector for the i-th frame. Thus, vθ : RN [0, 1]N RN d. The target vector field for the entire video Xτ , conditioned on the initial video X0 and target prior X1, is an matrix U(X0, X1). Its i-th row is the transpose of the derivative of the i-th frames path (Eq. 6) with respect to its individual timestep τ i. Using the derivative from Eq. 3 for each frame: dxi τ dτ = xi 1 xi 0 (8) Thus, the target video-level vector field is: U(X0, X1) = [(x1 0), . . . , (xN (9) Notably, for the linear interpolation path, this target vector field X1 X0 is independent of both the current video state Xτ and the vectorized timestep τ itself, simplifying the regression target. The video state Xτ at timestep τ is constructed via frame-wise linear interpolation: 0 )] = X1 1 xN 1 x1 Xτ = (1 τ ) X0 + τ X1 (10) where denotes element-wise multiplication between the timestep vector τ = [τ 1, τ 2, ..., τ ] and each frame. Key Properties: 1. Path Consistency: Each frame evolves linearly: xi τ = (1 τ i)xi 0 + τ ixi 1 2. Vector Field Simplicity: dXτ dτ = X1 X0 (constant for all τ ) 3. Decoupling: Frame dynamics depend only on their own τ i, enabling asynchronous evolution. The parameters θ of the neural network vθ are optimized by minimizing the Frame-Aware Flow Matching (FAFM) objective function: LFAFM(θ) = EX0qdata,X1qprior,τ pPTSS(τ ) where: (cid:104) vθ(Xτ , τ ) (X1 X0)2 (cid:105) (11) Xτ is the video state constructed according to Eq. 7 using the sampled X0, X1, and τ . 2 If vθ(Xτ , τ ) = [v1, . . . , vN ], then the denotes the squared Frobenius norm. squared Frobenius norm is equivalent to (cid:80)N i=1vi (xi 1 xi 0)2 2. τ pPTSS(τ ) indicates that the vectorized timestep τ is sampled according to Probabilistic Timestep Sampling Strategy (PTSS). This strategy is designed to expose the model to both synchronized and desynchronized frame evolutions during training. The PTSS is defined as follows: With probability pasync [0, 1]: Each component τ of τ is sampled independently from U[0, 1]: τ i.i.d. U[0, 1] for = 1, . . . , . This represents asynchronous evolution. With probability 1 pasync: single timestep τsync is sampled from U[0, 1]. All components of τ are set to this value: τ = τsync for = 1, . . . , . This represents synchronous evolution, where all frames share the same progression parameter."
        },
        {
            "title": "2.3 PUSA VIA VECTORIZED TIMESTEP ADAPTATION",
            "content": "Our model, named Pusa, adapts large-scale, pre-trained T2V diffusion transformer to the vectorized timestep introduced by FVDM Liu et al. (2024b). The adaptation, we term Vectorized Timestep Adaptation (VTA), along with lightweight fine-tuning process, imbues the model with fine-grained temporal control, enabling advanced capabilities such as zero-shot Image-to-Video (I2V) generation. 2.3."
        },
        {
            "title": "IMPLEMENTATION OF VECTORIZED TIMESTEP ADAPTATION",
            "content": "The foundational principle of our implementation is to re-engineer the core architecture to process vectorized timestep τ instead of scalar timestep t. The architectural adaptation is primarily focused on the models temporal conditioning mechanism. We introduce two key modifications: Vectorized Timestep Embedding: The timestep embedding module is modified to process the input vector τ , generating sequence of frame-specific embeddings Eτ RN1D, where N1 is the number of frames in the video latent sequence, each vector in the sequence corresponds to latent frames individual. Per-Frame Modulation: These frame-specific embeddings are subsequently projected to produce per-frame modulation parameters (i.e., scale, shift, and gate) within each block of the DiT architecture. The operation of DiT Peebles & Xie (2023) block on the latent representation zi of the i-th frame is thus conditioned on its individual timestep τ i, which can be conceptually expressed as: out = DiTBlock(zi zi in, context, modulate(τ i)) This design allows the model to process batch of frames that exist at different points along their respective generative paths simultaneously, forming the basis for fine-grained temporal control. Most importantly, this modification is non-destructive, which means it fully preserves the T2V capability of the base model. The adapted model generates same samples by setting all frame timesteps to same values as the base model. 2.3.2 TRAINING PROCEDURE The optimization follows the FAFM objective defined in Eq. 11. key advantage of our approach is its simplicity: by leveraging the robust generative prior of the base model, we circumvent the need for sampling synchronous timesteps. Instead, we train the model directly with fully randomized vectorized timestep (pasync = 1 ), where each component τ is sampled independently from [0, 1]. This stochastic training regimen compels the model to learn fine-grained temporal control from maximally diverse distribution of temporal states. 2.3.3 INFERENCE FOR IMAGE-TO-VIDEO GENERATION Pusa performs zero-shot I2V generation by strategically manipulating the vectorized timestep τ during sampling. To condition generation on starting image I0, for simplicity and fair comparison with baselines, we clamp its timestep component to zero throughout inference (i.e., τ 1 = 0 for all steps s). Note that we can also add some noise (e.g., set τ 1 = 0.2 or any level of noise) to the first frame, which may synthesize more coherent videos with slight change to the first frame. During sampling, which follows the Euler method for ODE integration, this ensures the change in the first frames latent is always zero, effectively fixing it as clean condition. This flexible control scheme naturally extends to other complex temporal tasks, such as start-end frames and video extension. The detailed I2V sampling procedure is outlined in Algorithm 1."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "Our experiments are designed to rigorously validate the three core contributions of this work: (1) the unprecedented efficiency of adapting large foundation models via VTA; (2) the superior performance of our model, Pusa, on the primary task of I2V generation; and (3) the emergent zero-shot multi-task capabilities of Pusa."
        },
        {
            "title": "Preprint",
            "content": "Initialize video with clean first frame and noisy subsequent frames. 1, z2 1, . . . , zN1 1 ] (0, I). Algorithm 1 Pusa: Sampling for Image-to-Video Generation Require: Trained model vθ, VAE Encoder and Decoder D, Scheduler. Require: Initial image I0, text prompt c, number of frames N1, inference steps S. 1: Encode prompt: cemb EncodePrompt(c). 2: Encode image to initial latent: ˆz1 1 E(I0). 3: Sample noise for remaining frames: [z1 1, z2 4: 5: Construct initial latent video: Z1 [ˆz1 6: Retrieve scheduler noise levels {σs}S 7: for 1, . . . , 1 do 8: 9: 10: 11: 12: 13: 14: 15: 16: end for 17: Decode final latent video: Xout D(ZS). 18: return Output video Xout. Set current path parameter τs σ1([0, σcurrent, . . . , σcurrent]). Set current noise levels σs [0, σcurrent, . . . , σcurrent]. Set next noise levels σs+1 [0, σnext, . . . , σnext]. Predict vector field: ˆUs vθ(Zs, τs, cemb). 1, . . . , zN1 1 ]. s=1, where σ1 > > σS 0. Zs+1 Zs + ˆUs (σs+1 σs). Let σcurrent σs and σnext σs+1. Construct vectorized timestep to freeze the first frame. Update latents; first frame remains unchanged. Table 1: Comprehensive Vbench-I2V Model Performance Leaderboard. This table presents comparison of leading Image-to-Video (I2V) models, evaluated on suite of automated metrics. Models are ranked by the overall Total Score. Our model demonstrates state-of-the-art performance, achieving top-tier rank among open-source models and notably surpassing its architectural baseline, Wan-I2V, especially in total score and dynamic motion generation. All scores are reported in percentages (%). Higher is better for all metrics. Best score in each column is in bold. Abbreviations: SC: Subject Consistency, BC: Background Consistency, MS: Motion Smoothness, DD: Dynamic Degree, AQ: Aesthetic Quality, IQ: Imaging Quality, I2V-S: I2V Subject Consistency, I2V-B: I2V Background Consistency, CM: Camera Motion. Model Overall Scores Quality Metrics I2V Metrics Total I2V Quality SC BC MS DD AQ IQ I2V-S I2V-B CM Closed / Proprietary Models Gen-4-I2V (API) STIV (Apple) 88.27 86. 95.65 93.48 80.89 79.98 93.23 98.40 96.79 98.39 98.99 99.61 55.20 15. 61.77 66.00 70.41 70.81 97.84 98.96 97.46 97.35 68.26 11.17 Open Source Models 89.28 Magi-1 88.36 Step-Video-TI2V DynamiCrafter-512 86.99 CogVideoX-5b-I2V 86.70 86.48 Animate-Anything 85.52 SEINE-512x512 85.28 I2VGen-XL 84.07 ConsistI2V 82.57 VideoCrafter 71.58 CogVideoX1.5-5B SVD-XT-1.1 SVD-XT-1.0 Wan-I2V Ours 86.86 87.32 96.12 95.50 93.53 94.79 94.25 92.67 92.11 91.91 86.31 92.25 92.90 94.84 3.1 SETUP 82.44 81.22 80.46 78.61 78.71 78.37 78.44 76.22 78.84 50.90 79.40 80.11 80.82 79.80 93.96 96.02 93.81 94.34 98.90 95.28 94.18 95.27 97.86 91.80 95.42 95.52 94.86 92.27 96.74 97.06 96.64 96.42 98.19 97.12 97.09 98.28 98.79 94.66 96.77 96.61 97.07 96. 98.68 99.24 96.84 98.40 98.61 97.12 98.34 97.38 98.00 40.98 98.12 98.09 97.90 98.49 68.21 48.78 69.67 33.17 2.68 27.07 26.10 18.62 22.60 62.29 43.17 52.36 51.38 52.60 64.74 62.29 60.88 61.87 67.12 64.55 64.82 59.00 60.78 70.21 60.23 60.15 64.75 63. 69.71 70.44 68.60 70.01 72.09 71.39 69.14 66.92 71.68 97.07 70.23 69.80 70.44 68.27 98.39 97.86 97.21 97.19 98.76 97.15 96.48 95.82 91.17 96.46 97.51 97.52 96.95 97.64 99.00 98.63 97.40 96.74 98.58 96.94 96.83 95.95 91.31 95.50 97.62 97.63 96.44 99. 50.85 49.23 31.98 67.68 13.08 20.97 18.48 33.92 33.60 39.71 34.76 29.46 Towards efficient adaptation with fewer resources, we perform lightweight fine-tuning on the SOTA open-source Wan-T2V model using the LoRA (Low-Rank Adaptation) technique Hu et al. (2022), which enables parameter-efficient training. The training infrastructure consists of 8 GPUs, each"
        },
        {
            "title": "Preprint",
            "content": "has 80GB memory and high memory bandwidth, with DeepSpeed Zero2 Rajbhandari et al. (2020) for memory optimization, achieving total batch size of 8. This is much more resource-friendly compared to full fine-tuning, which requires at least 4 8 80G GPUs with DeepSpeed Zero3 and is prohibitively expensive for most researchers. Note that our method also works for full finetuning as already implemented in Pusa V0.5 Liu & Liu (2025), which is adapted from Mochi Team (2024) with ultra-low cost, $100. Overall, this configuration reduces the training cost of our final model to $500, which is at least 200 more efficient than the Wan-I2V baseline (100K). The LoRA implementation is based on DiffSynth-Studio2, leveraging its optimized diffusion model training pipeline. Regarding the finetuning dataset, we utilize the samples from Vbench2.0 Zheng et al. (2025), which contains 3,860 high-quality caption-video pairs generated by Wan-T2V. This dataset spans diverse visual domains and temporal structures (e.g., natural scenes, human activities, camera motion), ensuring robust model generalization and aligned with Wans original distribution. Notably, the dataset size is less than 4K samples, representing 2, 500 reduction compared to the 10M samples required by Wan-I2V, demonstrating our data efficiency. Table 2: Comprehensive studies on key hyperparameters. This table presents detailed analysis of our models performance by varying training iterations, LoRA configurations, and the number of inference steps. All scores are reported in percentages (%), with higher values indicating better performance. For each ablation group, the best score per metric is highlighted in bold. Group Setting Overall Quality Metrics I2V Metrics Total Quality I2V SC BC MS DD AQ IQ I2V-S I2V-B CM (a) LoRA Configurations 256 512 α = 1.0 α = 1.4 α = 1.7 α = 2.0 α = 1.0 α = 1.4 α = 1.7 α = 2. (b) Inference Steps 2 steps 5 steps 10 steps 20 steps (c) Training Iterations 150 750 900 1200 α = 1.0 α = 1.4 α = 1.7 α = 2.0 α = 1.0 α = 1.4 α = 1.7 α = 2.0 α = 1.0 α = 1.4 α = 1.7 α = 2. α = 1.0 α = 1.4 α = 1.7 α = 2.0 α = 1.0 α = 1.4 α = 1.7 α = 2.0 79.75 83.12 85.86 84.22 80.17 86.46 87.11 85.98 79.92 86.03 87.69 87.84 78.23 79.33 82.79 84. 79.27 85.72 87.37 85.96 80.17 86.46 87.11 85.98 81.78 87.69 86.93 85.91 82.08 87.32 86.86 86.01 73.25 76.07 77.96 76.30 74.35 79.79 80.42 79. 67.97 77.59 80.55 81.17 73.36 73.01 74.99 76.25 74.20 79.41 80.95 79.54 74.35 79.79 80.42 79.10 74.63 80.55 79.19 77.96 75.01 80.30 79.70 78. 86.25 90.17 93.76 92.14 85.99 93.13 93.80 93.87 91.87 94.48 94.83 94.51 83.11 85.65 90.60 92.37 84.35 92.04 93.80 92.39 85.99 93.13 93.80 93. 88.93 94.83 94.67 93.86 89.14 94.34 94.02 93.69 84.35 85.65 97.18 96.71 83.16 88.64 92.93 93.88 77.18 88.53 91.39 89.54 80.29 81.61 84.37 87. 82.20 88.07 91.67 92.88 83.16 88.64 92.93 93.88 84.23 91.39 94.78 95.15 84.81 90.72 93.66 94.21 88.20 96.67 96.68 94.63 86.99 93.54 95.58 93. 92.90 95.65 96.02 95.89 86.09 86.20 89.92 91.86 86.81 93.34 95.51 94.12 86.99 93.54 95.58 93.93 88.48 96.02 95.80 94.26 88.30 95.44 95.46 94. 98.91 98.99 99.06 99.05 98.05 97.68 97.72 97.96 98.18 98.25 98.10 98.10 98.63 98.74 98.84 98.86 98.32 98.12 97.61 97.46 98.05 97.68 97.72 97. 98.44 98.10 98.22 98.38 98.15 97.58 98.02 98.40 28.92 30.06 10.40 6.40 62.40 78.80 62.80 51.20 16.80 50.00 66.40 79.88 50.80 48.00 48.40 53. 59.20 73.54 72.54 60.69 62.40 78.80 62.80 51.20 60.73 66.40 41.20 32.40 63.20 73.20 54.80 41.20 60.90 60.13 63.04 60.34 58.44 61.12 61.78 60. 51.69 60.21 62.42 61.02 60.19 59.32 60.05 59.03 59.48 61.88 62.25 61.04 58.44 61.12 61.78 60.57 58.56 62.42 61.84 60.57 58.17 61.51 60.77 59. 64.73 67.15 70.70 69.63 62.49 67.47 70.34 70.38 55.44 65.96 68.57 68.99 63.62 61.92 63.25 62.36 62.62 66.69 69.91 70.22 62.49 67.47 70.34 70. 60.10 68.57 70.14 70.17 61.90 68.01 69.70 69.96 91.69 90.33 98.60 97.96 91.23 95.72 97.30 96.92 94.20 97.03 97.78 97.07 88.67 90.79 93.62 95. 90.32 94.94 96.87 96.51 91.23 95.72 97.30 96.92 93.14 97.78 98.31 98.22 93.02 97.19 97.65 97.90 91.18 98.78 98.30 95.77 90.70 97.60 97.93 97. 97.55 99.25 99.33 99.10 88.47 91.11 95.99 97.04 89.00 96.98 98.23 97.52 90.70 97.60 97.93 97.69 93.73 99.33 99.26 98.95 94.04 98.83 98.77 98. 28.00 23.48 8.40 16.00 34.40 38.40 29.60 17.60 30.40 29.20 26.40 31.10 34.00 26.91 31.60 30.20 33.60 32.94 30.34 14.51 34.40 38.40 29.60 17. 32.80 26.40 18.00 6.15 34.40 29.89 18.80 9.16 2https://github.com/modelscope/DiffSynth-Studio"
        },
        {
            "title": "3.2 BASELINE COMPARISON",
            "content": "As shown in Table 1, Pusa, with only 10 inference steps, achieves SOTA performance among opensource models and surpasses its direct baseline, Wan-I2V, which was trained with vastly greater resources. Our model obtains total score of 87.32, outperforming Wan-I2Vs 86.86. This is achieved despite using less than 1/2500th of the training data (4K vs. 10M samples) and 1/200th of the computational budget. Notably, Pusa demonstrates superior performance in key I2V metrics, such as I2V Background Consistency (99.24 vs. 96.44) and I2V Subject Consistency (97.64 vs. 96.95), indicating more faithful adherence to the input image condition. Furthermore, our model exhibits higher Dynamic Degree (52.60 vs. 51.38), producing more motion-rich videos while maintaining high Motion Smoothness (98.49 vs. 97.90)."
        },
        {
            "title": "3.3 HYPERPARAMETER STUDY",
            "content": "To validate our hyperparameter choices and understand their impact on performance, we conducted series of studies, summarized in Table 2. Lora Configurations Lora rank is critical ingredient that influences the finetuing performance. As we know, Lora learns much less with small ranks compared to full finetuing Biderman et al. (2024), thus, Lora rank should be large enough to have the capacity to learn the new capabilities since tasks like I2V is very general. We investigated the influence of LoRA rank, proxy for the adaptations capacity. As shown in Table 2(a), higher rank of 512 consistently outperforms rank of 256 across most metrics, particularly in overall quality. This suggests that larger adaptation capacity is beneficial for capturing the nuances of temporal dynamics required for I2V tasks. We also find that the LoRA alpha scaling at inference time is critical; an alpha of 1.7 yields the best results for the 750-iteration checkpoint, balancing the influence of the LoRA weights against the pretrained model. Inference Steps. As detailed in Table 2(b), we analyzed the trade-off between computational cost at inference and generation quality using our best checkpoint with rank 512 and alpha 1.4 with 900 iterations. Performance scales predictably with the number of steps, with significant gains observed up to 10 steps. While 20 steps provide marginal improvement, the results at 10 steps are nearly identical (87.69 vs. 87.84). Consequently, we adopt 10 inference steps as our default to ensure an optimal balance between quality and generation speed. Training Progression. We evaluated checkpoints of Lora rank 512 at various stages of training, from 150 to 1200 iterations, using 10 inference steps. Table 2(c) shows clear trend of improving performance up to 900 iterations, where we achieve our peak score of 87.69 with an alpha of 1.4. Beyond this point, performance begins to plateau or slightly degrade, indicating that the model has converged. This rapid convergence underscores the data efficiency of our approach. Our final model for comparison in Table 1 uses the 900-iteration checkpoint of rank 512. 3.4 ANALYSIS OF THE ADAPTATION MECHANISM We now delve into the underlying mechanisms that enable Pusas remarkable efficiency and performance. Our analysis reveals that the Pusa framework facilitates highly targeted adaptation that leverages, rather than overwrites, the pretrained knowledge of the foundation model. I2V Qualitative In Fig. 3, Wan-T2V with VTA uses the same configurations as our method to do I2V generation, which is by directly setting the first frame/input image noise free. Since the model is purely for the T2V task, the generated frames are just aligned with the text prompt but have no relation to the first frame. Meanwhile, after finetuing of Wan-T2V model, our Pusa model can generate content aligned with both text and the input image seamleassly, and the result is better than Wan-I2V, which has visible distortions and does not preserve the character well from the condition image. Attention Mechanism. visualization of the self-attention maps using queries and keys within the final Transformer (block 39) block across different inference steps (10 steps in total, we visu-"
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Image-to-video results, where our method generates smooth and realistic animation of the first frame while Wan2.1-T2V generates completely different frames from the first frame, though aligned with text prompt, and Wan2.1-I2V generates frames with noticeable distortions. All generated with the same input image (Frame 0) and text prompt: man in black suit and sombrero, shouting loudly. alize steps 0, 3, 6, and 9)  (Fig. 4)  provides critical insights. The base Wan-T2V model exhibits diagonal attention pattern, indicating that each frame primarily attends to itself, with little temporal cross-attention. In contrast, both Wan-I2V and Pusa show strong attention from all frames to the first frame (the first column of the map), which is essential for maintaining consistency with the input image. However, key difference emerges: the attention patterns in Wan-I2V are globally altered compared to the base model, and the attention to the first frame only strengthened in the early inference steps (e.g., step 0), which indicates lots of ineffective learning redundancy. In Pusa, the attention to the first frame is significantly modified across all steps. The rest of the attention map resembles that of the original Wan-T2V more closely than Wan-I2V. This demonstrates that our method successfully decouples the learning of temporal dynamics and basic generation priors, surgically injects the conditioning mechanism while preserving the base models powerful, pretrained capabilities. Wan-I2V, on the other hand, appears to have learned fundamentally new attention distribution, revealing the disadvantages of extensive retraining with its I2V method. Parameter Divergence. This observation is further substantiated by an analysis of parameter changes  (Fig. 5)  . The parameter drift in Wan-I2V is substantial and concentrated in modules critical for content generation, such as the text encoder and cross-attention blocks. This implies significant alteration of the models core generative priors. Pusa, conversely, exhibits minimal parameter changes, with modifications almost exclusively in the self-attention blocks responsible for temporal dynamics. The magnitude of parameter change in Wan-I2V is over an order of magnitude larger than in Pusa. This confirms that our approach constitutes minimal, targeted adaptation, preserving the integrity of the foundation model and explaining its efficiency. Why Vectorized Timestep Adaptation Succeeds. fundamental challenge of VTV, as first identified in the FVDM paper Liu et al. (2024b), is the combinatorial explosion of the temporal composition space. With each frame possessing an independent timestep, the number of possible configurations grows exponentially (e.g., to 1048 for 16 frames), making convergence from scratch exceedingly difficult. FVDM introduced the PTSS to solve this, decoupling the learning of temporal dynamics from foundational generation capability by alternating between synchronized and asynchronized timesteps during training. In our work, this challenge is elegantly circumvented by leveraging powerful, pretrained foundation model. Since the Wan-T2V model has already mastered video content generation, our adaptation does not need to learn this capability from scratch. Instead, our finetuning method can focus exclusively on mastering the temporal control offered by the VTV architecture with totally random timesteps for all frames. The base models robust generative prior means only brief period of finetuning with independent timesteps is required to instill this new, fine-grained control."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Visualization of attention maps corresponding to the generation of videos in Fig. 3. Each value in the attention map represents frame-to-frame correlation/attention, larger value means higher correlation. Zoom in for better view. Evidence of the base models inherent robustness to timestep desynchronization is presented in Fig. 3. When we attempt zero-shot I2V generation by setting the first frames timestep to zero, the base Wan-T2V model still produces coherent video that aligns with the text prompt, even though it fails to adhere to the image condition. This demonstrates that the generative processes for frames are originally independent, not significantly influenced by other frames. The mechanistic explanation lies in the models attention structure  (Fig. 4)  : the strongly diagonal self-attention patterns in Wan-T2V reveal that each frames synthesis relies primarily on its own latent features. Our finetuning surgically modifies this behavior, introducing targeted attention to conditioning frames while preserving the models stable generative core. This non-destructive adaptation is the key to sidestepping the VTV compositionality problem and achieving state-of-the-art results with unprecedented efficiency. 3.5 MULTI-TASK CAPABILITIES key advantage of our approach is its ability to generalize to variety of video generation tasks, including and beyond T2V, without any task-specific training. This is direct result of the flexible vectorized timestep settings, which allow for arbitrary conditioning on any subset of frames. Text-to-Video Generation. Unlike specialized I2V models, Pusa still retains the text-to-video capabilities of its foundation model. As shown in Fig. 13, the qualitative output remains high. This demonstrates that our fine-tuning process does not cause catastrophic forgetting of the primary T2V task, making Pusa truly unified video generation model. Complex Temporal Tasks. The true power of the FVDM framework is revealed in its zero-shot performance on complex temporal synthesis tasks. More results of seamless I2V generation is given in Fig. 3. Apart from that, Pusa can perform start-end frames by conditioning on the first frame and the last frame (encoded to single latent frame like the first frame) in Fig. 7, or conditioning on the first frame and the last 4 frames (encoded to single latent frame as default) in Fig. 9. Since"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Analysis on finetuned models parameter shifts. The cloumns, from left to right, represent average relative change of parameters by model component/modules, top 20 parameters with largest relative change, and average change by transformer blocks (in total 40 blocks from 0 to 39). Overall, our model has less variations to Wan2.1-T2V compared to Wan2.1-I2V, which indicates our model preserves the original distribution better. Note that for our model, we use our final configurations in Table 1, i.e., Lora rank 512 with lora 1.4 at 900 steps. Zoom in for better view. there is 4 compression rate for the last frames brought by VAE, only conditioning on the last frame yields worse results, as it will be viewed as 4 still frames. Thanks to the unique properties of our framework, we can add some noise (e.g., set τ 1 = 0.3 and τ = 0.7 t) to the encoded latents to generate motion and content for the condition frames and thus synthesize more coherent videos as shown in Fig. 8. Furthermore, Figures 10 11 and 12 showcase its capability for video completion/transition, video extension, seamlessly completing or continuing given video sequence. These capabilities are all emergent properties of our vectorized timesteo adaptation strategy and require no task-specific training, highlighting the versatility and power of our approach."
        },
        {
            "title": "4 RELATED WORK",
            "content": "The field of video generation has rapidly evolved, driven by the success of diffusion models in image synthesis. Our work builds upon and extends several key research threads, positioning itself as highly efficient and versatile solution for the next paradigm of video diffusion models. 4.1 CONVENTIONAL VIDEO DIFFUSION MODELS The initial extension of image diffusion models to video generation established foundational paradigm. Seminal works like VDM Ho et al. (2022) and subsequent large-scale models such as Latent Video Diffusion Models (LVDM) He et al. (2022), VideoCrafter1 Chen et al. (2023), and others Wang et al. (2023); Ma et al. (2024); Wan et al. (2025); Kong et al. (2024) all adopted the same diffusion framework. core characteristic of these conventional models is their reliance on scalar timestep variable. This single variable governs the noise level and evolution trajectory uniformly across all frames of video clip during the diffusion process. While this synchronized-frame approach proved effective for generating short, self-contained clips, particularly for T2V tasks, it imposes rigid temporal structure. The uniform noise schedule inherently limits the models ability to handle tasks requiring asynchronous frame evolution, such as I2V generation, where the first frame or condition image is given, or complex editing tasks like video interpolation. Recognizing the limitations of conventional VDMs in temporal modeling, the research community has developed numerous extensions targeting specific video generation tasks Xing et al. (2023b). These approaches predominantly focus on adapting existing scalar timestep based models through fine-tuning strategies or zero-shot techniques to handle domain-specific challenges such"
        },
        {
            "title": "Preprint",
            "content": "as image-to-video generation Xing et al. (2023a); Guo et al. (2023); Zhang et al. (2023); Li et al. (2024); Ni et al. (2024), video interpolation Wang et al. (2024b;a); Wan et al. (2025), and long video synthesis Duan et al. (2024); Henschel et al. (2024); Kim et al. (2024); Lu et al. (2024); Dalal et al. (2025); Zhao et al. (2025) These methods typically involve extensive fine-tuning of large, pre-trained T2V model on task-specific data or employing zero-shot domain transfer techniques. I2V generation has emerged as particularly active area. For example, the Wan-I2V model presented in the Wan paper required fine-tuning Wan T2V model on its T2V pretraining dataset to achieve its SOTA I2V capabilities and can only do I2V after this process Wan et al. (2025). Overall, these extensions reveal fundamental challenges in balancing flexibility, generalization, and the retention of original model capabilities. Fine-tuning approaches often suffer from catastrophic forgetting, where adaptation to specific tasks severely degrades performance on its original capabilityPan et al. (2024); Ramasesh et al. (2021). Zero-shot methods, exemplified by TI2V-Zero introduces zero-shot method for conditioning T2V diffusion models on images Ni et al. (2024). However, its generalization and generation quality are limited by potential visual artifacts and reduced robustness, as its simple repeat-and-slide strategy struggles with diverse input and can produce blurry or flickering videos. The reliance on task-specific architectures and training procedures highlights the need for more unified and general approach that can handle diverse video generation scenarios without requiring extensive finetuning. Our work departs from these limitations by adopting the VTV introduced by FVDM Liu et al. (2024b), enabling fine-grained control over the generative process. 4.2 AUTOREGRESSIVE VIDEO DIFFUSION MODELS Recently, people have explored autoregressive paradigms for video diffusion models, where frames are generated sequentially rather than simultaneously (Chen et al., 2024; Sun et al., 2025; Teng et al., 2025; Chen et al., 2025; Huang et al., 2025). This direction includes methods like Diffusion Forcing, which trains causal next-token model to predict one or multiple future tokens without fully diffusing past ones, enabling variable-length generation capabilities. CauseVid Yin et al. (2024) represents another significant advancement in this direction, proposing fast autoregressive video diffusion models that can generate frames on-the-fly with streaming capabilities. Large-scale autoregressive models such as MAGI-1 Teng et al. (2025) and SkyReels-V2 Chen et al. (2025) have demonstrated the potential for scalable video generation through chunk-by-chunk processing, where each segment is denoised holistically before proceeding to the next. Self-Forcing Huang et al. (2025) addresses the critical issue of exposure bias in autoregressive video diffusion by introducing training paradigm where models condition on their own previously generated outputs rather than ground-truth frames. This approach enables real-time streaming video generation while maintaining temporal coherence through innovative key-value caching mechanisms. Despite these advances, autoregressive video diffusion models face inherent limitations that constrain their applicability. The sequential nature of generation restricts these models to unidirectional tasks, making them inadequate for many scenarios, such as start-end frames, video transitions, and keyframe interpolation. Moreover, error accumulation and drift issues represent persistent challenges in autoregressive approaches, where small prediction errors compound over time, leading to quality degradation in longer sequences Huang et al. (2025). Recent theoretical analyses have also identified both error accumulation and memory bottlenecks as fundamental phenomena in autoregressive video diffusion models, revealing Pareto frontier between these competing constraintsWang et al. (2025). 4.3 FRAME-AWARE VIDEO DIFFUSION MODEL Frame-aware video diffusion model (FVDM) Liu et al. (2024b) is parallel line of research to reconstruct the paradigm for video diffusion models, by enabling independent temporal evolution for each frame. This paradigm shift addresses fundamental shortcomings in conventional VDMs by enhancing the models capacity to capture fine-grained temporal dependencies without the constraints of synchronized evolution. The vectorized timestep approach enables unprecedented flexibility across multiple video generation tasks, including T2V, I2V, start-end frames, video extension, and so on, all within single unified"
        },
        {
            "title": "Preprint",
            "content": "framework. Unlike conventional approaches that require extensive fine-tuning and destructive architectural modifications, the FVDM-based paradigm demonstrates strong zero-shot capabilities across diverse scenarios, while only using minor finetuning to adapt the model to support VTV. Project Pusa-Mochi (V0.5) Liu & Liu (2025); Team (2024) have initially demonstrated the practical viability of this approach. Further in this work, Pusa-Wan (V1.0) achieves remarkable efficiency gains with training costs reduced to mere 500 dollars from above 100K of Wan-I2V, while outperforming it on Vbench-I2V. The FVDM framework represents fundamental departure from previous temporal modeling approaches, offering solution that can perform both directional generation tasks (like autoregressive models) and bidirectional temporal reasoning tasks that autoregressive approaches cannot handle. This unified capability, combined with the demonstrated computational efficiency and strong empirical results, positions this approach as promising direction for next-generation video diffusion models."
        },
        {
            "title": "5 CONCLUSION",
            "content": "This work introduces transformative approach to video diffusion models through the FVFM framework, culminating in Pusa V1.0. By decoupling temporal dynamics from content generation, our method achieves SOTA level I2V performance with unprecedented efficiency, requiring only $500 and 4K samples to surpass Wan-I2V. The key innovation lies in our non-destructive adaptation strategy: preserve the foundation models robust priors while enabling frame-specific evolution via VTV. This unlocks many zero-shot generalization to diverse tasksa property unmatched by conventional or autoregressive VDMs. Our mechanistic studies (e.g., attention maps, parameter drift) demonstrate that Pusas success stems from its minimal, targeted modifications to the base models temporal attention mechanisms, avoiding the catastrophic forgetting observed in task-specific fine-tuning. The implications are profound: Pusa redefines the efficiency-quality tradeoff in video generation, enabling high-fidelity, multi-task generation at fraction of traditional costs. Future directions include extending VTV to long-form video generation, references-to-video, and video editing. By bridging theoretical innovation with practical deployment, this work paves the way for new era of scalable, general-purpose video diffusion models, with far-reaching applications in creative industries, education, and beyond."
        },
        {
            "title": "REFERENCES",
            "content": "Dan Biderman, Jacob Portes, Jose Javier Gonzalez Ortiz, Mansheej Paul, Philip Greengard, Connor Jennings, Daniel King, Sam Havens, Vitaliy Chiley, Jonathan Frankle, et al. Lora learns less and forgets less. arXiv preprint arXiv:2405.09673, 2024. Boyuan Chen, Diego Martı Monso, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Junchen Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengcheng Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for highquality video generation. arXiv preprint arXiv:2310.19512, 2023. Ricky T. Q. Chen. torchdiffeq. torchdiffeq, 2018. URL https://github.com/rtqichen/ Karan Dalal, Daniel Koceja, Jiarui Xu, Yue Zhao, Shihao Han, Ka Chun Cheung, Jan Kautz, Yejin Choi, Yu Sun, and Xiaolong Wang. One-minute video generation with test-time training. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1770217711, 2025. Zhongjie Duan, Wenmeng Zhou, Cen Chen, Yaliang Li, and Weining Qian. Exvideo: Extending video diffusion models via parameter-efficient post-tuning. arXiv preprint arXiv:2406.14130, 2024."
        },
        {
            "title": "Preprint",
            "content": "Figure 6: More image-to-video results. The first frames of each row are the given condition images extracted from Veo2 & Sora demos. Each generated video has 81 frames in total."
        },
        {
            "title": "Preprint",
            "content": "Figure 7: Zero-shot results w.r.t. start & end frames to video. The first and last frames are given condition frames extracted from Veo2 & Sora demos. Each generated video has 81 frames in total."
        },
        {
            "title": "Preprint",
            "content": "Figure 8: Zero-shot results w.r.t. start & end frames with noise. The first and last frames are given conditions and added 30% and 70% noise during sampling to make the generated video more coherent. Condition frames are extracted from Veo2 & Sora demos. Each generated video has 81 frames in total."
        },
        {
            "title": "Preprint",
            "content": "Figure 9: Zero-shot results w.r.t. start & end frames to video. The first and last 4 frames (encoded to one latent frame) are given condition frames extracted from Veo2 & Sora demos. Each generated video has 81 frames/21 latent frame in total."
        },
        {
            "title": "Preprint",
            "content": "Figure 10: Zero-shot results w.r.t. video completion/transition. The first 9 frames and the last 12 frames extracted from Veo2 demos are given as conditions and encoded to the first 3 latent frames and the last 3 latent frames, respectively. Each generated video has 81 frames/21 latent frames in total."
        },
        {
            "title": "Preprint",
            "content": "Figure 11: Zero-shot results w.r.t. video extension. The first 13 frames extracted from Veo2 demos are given as conditions and encoded to the first 4 latent frames. Each generated video has 81 frames/21 latent frames in total."
        },
        {
            "title": "Preprint",
            "content": "Figure 12: Zero-shot results w.r.t. video extension. The first 41 frames extracted from Veo2 demos are given as conditions and encoded to the first 11 latent frames. Each generated video has 81 frames/21 latent frames in total."
        },
        {
            "title": "Preprint",
            "content": "Figure 13: Text-to-video results. Prompts all from Vbench2.0."
        },
        {
            "title": "Preprint",
            "content": "Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Chongyang Ma, Weiming Hu, Zhengjun Zha, Haibin Huang, Pengfei Wan, et al. I2v-adapter: general image-to-video adapter for video diffusion models. arXiv preprint arXiv:2312.16693, 2023. Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. 2022. Roberto Henschel, Levon Khachatryan, Daniil Hayrapetyan, Hayk Poghosyan, Vahram Tadevosyan, Zhangyang Wang, Shant Navasardyan, and Humphrey Shi. Streamingt2v: Consistent, dynamic, and extendable long video generation from text. arXiv preprint arXiv:2403.14773, 2024. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:8633 8646, 2022. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. Ziqi Huang, Fan Zhang, Xiaojie Xu, Yinan He, Jiashuo Yu, Ziyue Dong, Qianli Ma, Nattapol Chanpaisit, Chenyang Si, Yuming Jiang, et al. Vbench++: Comprehensive and versatile benchmark suite for video generative models. arXiv preprint arXiv:2411.13503, 2024. Jihwan Kim, Junoh Kang, Jinyoung Choi, and Bohyung Han. Fifo-diffusion: Generating infinite videos from text without training. arXiv preprint arXiv:2405.11473, 2024. Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. Weijie Li, Litong Gong, Yiran Zhu, Fanda Fan, Biao Wang, Tiezheng Ge, and Bo Zheng. arXiv preprint Tuning-free noise rectification for high fidelity image-to-video generation. arXiv:2403.02827, 2024. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Yaofang Liu and Rui Liu. Pusa: Thousands timesteps video diffusion model, 2025. URL https: //github.com/Yaofang-Liu/Pusa-VidGen. Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2213922149, 2024a. Yaofang Liu, Yumeng Ren, Xiaodong Cun, Aitor Artola, Yang Liu, Tieyong Zeng, Raymond Chan, and Jean-michel Morel. Redefining temporal modeling in video diffusion: The vectorized timestep approach. arXiv preprint arXiv:2410.03160, 2024b. Yu Lu, Yuanzhi Liang, Linchao Zhu, and Yi Yang. Freelong: Training-free long video generation with spectralblend temporal attention. arXiv preprint arXiv:2407.19918, 2024. Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, arXiv preprint Latte: Latent diffusion transformer for video generation. and Yu Qiao. arXiv:2401.03048, 2024."
        },
        {
            "title": "Preprint",
            "content": "Haomiao Ni, Bernhard Egger, Suhas Lohit, Anoop Cherian, Ye Wang, Toshiaki Koike-Akino, Sharon Huang, and Tim Marks. Ti2v-zero: Zero-shot image conditioning for text-to-video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 90159025, 2024. OpenAI. Sora: Creating video from text. https://openai.com/sora, 2024. Jiadong Pan, Hongcheng Gao, Zongyu Wu, Taihang Hu, Li Su, Qingming Huang, and Liang Li. Leveraging catastrophic forgetting to develop safe diffusion models against malicious finetuning. Advances in Neural Information Processing Systems, 37:115208115232, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: Memory optimizations toward training trillion parameter models. In SC20: International Conference for High Performance Computing, Networking, Storage and Analysis, pp. 116. IEEE, 2020. Vinay Venkatesh Ramasesh, Aitor Lewkowycz, and Ethan Dyer. Effect of scale on catastrophic forgetting in neural networks. In International conference on learning representations, 2021. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Mingzhen Sun, Weining Wang, Gen Li, Jiawei Liu, Jiahui Sun, Wanquan Feng, Shanshan Lao, SiYu Zhou, Qian He, and Jing Liu. Ar-diffusion: Asynchronous video generation with auto-regressive diffusion. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 7364 7373, 2025. Genmo Team. Mochi 1. https://github.com/genmoai/models, 2024. Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. Alexander Tong, Kilian Fatras, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid RectorBrooks, Guy Wolf, and Yoshua Bengio. Improving and generalizing flow-based generative models with minibatch optimal transport. arXiv preprint arXiv:2302.00482, 2023. Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. Cong Wang, Jiaxi Gu, Panwen Hu, Haoyu Zhao, Yuanfan Guo, Jianhua Han, Hang Xu, and Xiaodan Liang. Easycontrol: Transfer controlnet to video diffusion for controllable generation and interpolation. arXiv preprint arXiv:2408.13005, 2024a. Jing Wang, Fengzhuo Zhang, Xiaoli Li, Vincent YF Tan, Tianyu Pang, Chao Du, Aixin Sun, and Zhuoran Yang. Error analyses of auto-regressive video diffusion models: unified framework. arXiv preprint arXiv:2503.10704, 2025. Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. Xiaojuan Wang, Boyang Zhou, Brian Curless, Ira Kemelmacher-Shlizerman, Aleksander Holynski, and Steven Seitz. Generative inbetweening: Adapting image-to-video models for keyframe interpolation. arXiv preprint arXiv:2408.15239, 2024b. Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023a."
        },
        {
            "title": "Preprint",
            "content": "Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. arXiv preprint arXiv:2310.10647, 2023b. Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, arXiv preprint and Xun Huang. From slow bidirectional to fast causal video generators. arXiv:2412.07772, 2024. Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, I2vgen-xl: High-quality image-to-video synthesis via cascaded Deli Zhao, and Jingren Zhou. diffusion models. arXiv preprint arXiv:2311.04145, 2023. Min Zhao, Guande He, Yixiao Chen, Hongzhou Zhu, Chongxuan Li, and Jun Zhu. Riflex: free lunch for length extrapolation in video diffusion transformers. arXiv preprint arXiv:2502.15894, 2025. Dian Zheng, Ziqi Huang, Hongbo Liu, Kai Zou, Yinan He, Fan Zhang, Yuanhan Zhang, Jingwen He, Wei-Shi Zheng, Yu Qiao, et al. Vbench-2.0: Advancing video generation benchmark suite for intrinsic faithfulness. arXiv preprint arXiv:2503.21755, 2025."
        }
    ],
    "affiliations": [
        "AI Technology Center, Tencent PCG",
        "City University of Hong Kong",
        "Great Bay University",
        "Hong Kong Centre for Cerebro-Cardiovascular Health Engineering",
        "Huawei Research",
        "Lingnan University",
        "The Chinese University of Hong Kong"
    ]
}