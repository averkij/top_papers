{
    "paper_title": "Distill Visual Chart Reasoning Ability from LLMs to MLLMs",
    "authors": [
        "Wei He",
        "Zhiheng Xi",
        "Wanxu Zhao",
        "Xiaoran Fan",
        "Yiwen Ding",
        "Zifei Shan",
        "Tao Gui",
        "Qi Zhang",
        "Xuanjing Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Solving complex chart Q&A tasks requires advanced visual reasoning abilities in multimodal large language models (MLLMs). Recent studies highlight that these abilities consist of two main parts: recognizing key information from visual inputs and conducting reasoning over it. Thus, a promising approach to enhance MLLMs is to construct relevant training data focusing on the two aspects. However, collecting and annotating complex charts and questions is costly and time-consuming, and ensuring the quality of annotated answers remains a challenge. In this paper, we propose Code-as-Intermediary Translation (CIT), a cost-effective, efficient and easily scalable data synthesis method for distilling visual reasoning abilities from LLMs to MLLMs. The code serves as an intermediary that translates visual chart representations into textual representations, enabling LLMs to understand cross-modal information. Specifically, we employ text-based synthesizing techniques to construct chart-plotting code and produce ReachQA, a dataset containing 3k reasoning-intensive charts and 20k Q&A pairs to enhance both recognition and reasoning abilities. Experiments show that when fine-tuned with our data, models not only perform well on chart-related benchmarks, but also demonstrate improved multimodal reasoning abilities on general mathematical benchmarks like MathVista. The code and dataset are publicly available at https://github.com/hewei2001/ReachQA."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 2 ] . [ 1 8 9 7 8 1 . 0 1 4 2 : r Preprint. Work in Progress."
        },
        {
            "title": "DISTILL VISUAL CHART REASONING ABILITY\nFROM LLMS TO MLLMS",
            "content": "Wei He1, Zhiheng Xi1, Wanxu Zhao1, Xiaoran Fan1, Yiwen Ding1, Zifei Shan2, Tao Gui1, Qi Zhang1, Xuanjing Huang1 1Fudan University, 2Weixin Group, Tencent whe23@m.fudan.edu.cn, {tgui,qz}@fudan.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Solving complex chart Q&A tasks requires advanced visual reasoning abilities in multimodal large language models (MLLMs). Recent studies highlight that these abilities consist of two main parts: recognizing key information from visual inputs and conducting reasoning over it. Thus, promising approach to enhance MLLMs is to construct relevant training data focusing on the two aspects. However, collecting and annotating complex charts and questions is costly and timeconsuming, and ensuring the quality of annotated answers remains challenge. In this paper, we propose Code-as-Intermediary Translation (CIT), cost-effective, efficient and easily scalable data synthesis method for distilling visual reasoning abilities from LLMs to MLLMs. The code serves as an intermediary that translates visual chart representations into textual representations, enabling LLMs to understand cross-modal information. Specifically, we employ text-based synthesizing techniques to construct chart-plotting code and produce REACHQA, dataset containing 3k reasoning-intensive charts and 20k Q&A pairs to enhance both recognition and reasoning abilities. Experiments show that when fine-tuned with our data, models not only perform well on chart-related benchmarks, but also demonstrate improved multimodal reasoning abilities on general mathematical benchmarks like MathVista. The code and dataset are publicly available at https://github.com/hewei2001/ReachQA."
        },
        {
            "title": "INTRODUCTION",
            "content": "Multimodal large language models (MLLMs) have made significant achievements, particularly in visual recognition tasks (OpenAI, 2024a; Anthropic, 2024). While they can handle simple visual inputs well, there has been growing emphasis on complex chart understanding, driven by the widespread use of charts in real-world contexts (Masry et al., 2022; Huang et al., 2024). However, addressing reasoning-intensive questions involving charts remains challenging for these models. Existing benchmarks underscore the need for more advanced and generalized visual reasoning abilities, which are still underdeveloped in current MLLMs (Wang et al., 2024c; Lu et al., 2024). Our analysis of the error distribution in ChartQA (Figure 1) also highlights two main types of model failure: 62% of errors stem from misrecognition, while 36% arise from reasoning mistakes after correct recognition. This shows that even advanced MLLMs struggle with basic recognition and often make superficial reasoning errors. In contrast, humans excel at these tasks by purposefully identifying query-relevant information from images and engaging in step-by-step reasoning (Wang et al., 2024c;a). In light of these findings, enabling models to solve problems in human-like manner, becomes essential for advancing visual reasoning performance. One promising strategy is to distill the rationales of reasoning from experts, such as human or stronger models (Han et al., 2023; Meng et al., 2024; Masry et al., 2024a;b) However, creating highquality training data for chart-related tasks is costly and time-consuming. The challenges arise from both the complexity of constructing the chart data and the instruction data. Collecting charts from specific online sources typically involves manual crawling, filtering, and extensive human annotation to create relevant questions(Masry et al., 2022; Wang et al., 2024c). Although some approaches use Equal contribution. Work done during Wei Hes internship at Tencent. Corresponding authors. 1 Preprint. Work in Progress. Figure 1: Error distribution of incorrect answers by MiniCPM-V2.5-Llama3 (Yao et al., 2024) on ChartQA test set (Masry et al., 2022), as judged by GPT-4o. We present an example chart from ChartQA along with two error cases: one for recognition and one for reasoning. The Other Errors include question misunderstood errors, knowledge and hallucination errors, or refusal to answer. LLMs to automate Q&A synthesis, they rely on the data table of charts (Han et al., 2023; Masry et al., 2024a), overlooking critical visual features like color, layout, and structure. Even with the use of MLLMs for Q&A generation (Masry et al., 2024b), our preliminary analysis ( 2.2) shows their performance remains suboptimal, struggling to produce accurate and challenging chart-related Q&A pairs. In contrast, LLMs tend to perform better when processing charts in textual format, demonstrating lower costs and higher accuracy, as also observed by Fu et al. (2024). Given the limitations of MLLMs in this task, we propose leveraging the strengths of LLMs through an intermediary representation: code. Inspired by the concept of intermediary translation (Zarechnak, 1986; Leon, 2007), which refers to using bridge language to improve translation quality across diverse languages in literary studies, we introduce Code-as-Intermediary Translation (CIT). The code acts as an intermediary, translating chart images into textual representations while preserving visual features faithfully. This process enables LLMs to interpret cross-modal information and generate more accurate and visually complex Q&A pairs. Furthermore, it facilitates the adoption of text-based instruction augmentation strategies, such as Self-Instruct (Wang et al., 2023) and EvolInstruct (Xu et al., 2024), to enhance the diversity and complexity of the generated charts. Starting with 33 seed codes collected from the Matplotlib gallery, we synthesize more chart-plotting codes covering diverse types and topics, and then complicate them to create richer ones. Finally, using the synthetic codes as bridge, we generate charts (via Python) and instructions (via LLMs) in bi-directional process, ensuring the alignment between the two modalities. With the CIT approach, we construct REACHQA, multimodal instruction dataset containing 3, 249 reasoning-intensive charts and 19, 963 Q&A pairs, all at remarkably low cost of just $300. The dataset comprises 8k questions focused on visual recognition and 12k on reasoning, designed to address the dual challenges of extracting visual information and performing advanced reasoning. Additionally, we create 500 charts and 2, 000 manually verified Q&A pairs to independently assess models recognition and reasoning abilities. When fine-tuning on REACHQA, all models demonstrate substantial performance gains across seven benchmarks, with LLaVA-Next-Llama3-8B (Li et al., 2024) improving by over 30% on average. More importantly, we observe that these gains generalized beyond chart-specific tasks to broader multimodal reasoning tasks, such as MathVista and MATH-Visionan outcome previously unattainable with existing chart-focused datasets. Furthermore, we conduct experiments to investigate deeper insights, including (1) the real impact of expert rationales on reasoning abilities, (2) the effects of recognitionand reasoning-oriented training data ratios, and (3) the benefits of mixing general-purpose multimodal instruction data. 2 Preprint. Work in Progress. Table 1: Comparison of existing chart-related datasets across three properties. Only the chart question-answering (CQA) task is considered, despite some datasets having multiple tasks. Abbreviations: Vis.=visual, Comp.=complexity, Temp.=template, Refer.=Reference, Reas.=reasoning, Rat.=rationale, Annot.=annotation and Scal.=scalable. Cells marked with indicate mixed attributes (e.g., partially template-based; scalable Q&A but non-scalable chart data.). * indicates that while the chart-plotting codes are public, the Q&A generation still relies on data tables. Chart Properties Q&A Properties Dataset Properties Datasets # Chart Type # Chart Topic PlotQA (Methani et al., 2020) ChartQA (Masry et al., 2022) OpenCQA (Kantharaj et al., 2022) MathVista (Lu et al., 2024) CharXiv (Wang et al., 2024c) ChartBench (Xu et al., 2023) ChartX (Xia et al., 2024) MMC (Liu et al., 2024a) ChartLlama (Han et al., 2023) ChartAst (Meng et al., 2024) ChartInstruct (Masry et al., 2024a) ChartGemma (Masry et al., 2024b) REACHQA (ours) 3 3 5 - - 9 / 42 18 6 10 9 - - 10 / 32 - 15 10 - - - 22 5 - - - - Textual Format Table Table Caption - - Table Code* Caption Table Table Table - Code Vis. Comp. Temp. Free Vis. Refer. Rat. Annot. Train Set Test Set Scal."
        },
        {
            "title": "2 BACKGROUND",
            "content": "2.1 DEFICIENCIES IN EXISTING CHART-RELATED DATASETS Existing chart-related datasets fall into two categories: benchmarks for evaluation and training data for improving model performance. The charts are either collected from online data sources or generated by models, sometimes requiring manual annotation or automated question generation. Most of them focus on basic visual recognition tasks. Though some recent works target more advanced reasoning, they often lack scalability. Table 1 summarizes these datasets, with further details below. Chart Properties. The visual diversity is shaped by the variety of chart types and topics (Wang et al., 2024c). Early datasets like ChartQA and OpenCQA (Kantharaj et al., 2022), sourced from limited websites, featured uniform styles with minimal diversity. To address this, recent works like ChartAst (Meng et al., 2024) synthesize charts with randomized attributes (e.g., color, fonts) using LLMs. However, beyond the superficial variations in chart appearance, many of them overlook the visual complexity (Zeng et al., 2024). As models evolve, simple style changes no longer pose challenges. Datasets like CharXiv and MMC (Liu et al., 2024a), which include complex scientific charts from arXiv papers, naturally exhibit greater complexity in recognition. Additionally, the textual format of charts is critical, as it can be used to expand the datasets via language models. Q&A Properties. Some benchmarks like PlotQA and ChartBench (Xu et al., 2023) use predefined templates to generate Q&A pairs, resulting in monotonous and simplistic questions. Other datasets, such as ChartQA and CharXiv, required manual annotation, which improved quality but increased costs and hindered scalability. With the advent of LLMs, works like ChartLlama (Han et al., 2023) and ChartInstruct (Masry et al., 2024a) use them to generate diverse questions from data tables while also providing rationale annotations for training. However, these methods fail to capture fine-grained visual elements like color, layout, and structure because they rely on only the data table. To address this, ChartGemma (Masry et al., 2024b) uses MLLMs to generate Q&A pairs directly from charts. Dataset Properties. While manually annotated datasets like MathVista (Lu et al., 2024) and CharXiv provide high-quality data, their development is resource-intensive, typically resulting in datasets of only few thousand samples. In the era of LLMs, such methods are impractical for scaling to the size needed for training large models. Recent efforts, such as ChartAst, ChartInstruct, and ChartGemma, have explored Q&A generation for dataset expansion, but they remain limited by the difficulty of collecting large set of charts. more scalable approach is to leverage the generative capabilities of LLMs to synthesize charts like ChartBench and ChartX (Xia et al., 2024). Preprint. Work in Progress."
        },
        {
            "title": "2.2 CAN LLMS UNDERSTAND CHARTS WITHOUT VISUAL INPUT?",
            "content": "To explore whether there is more effective textual format for representing visual information than data tables, we propose using code. By precisely encoding chart structures and details, the code may serve as an ideal bridge between modalities. We design an experiment to test this hypothesis. We collect 25 complex charts, along with their corresponding data tables and code, from recent research papers by graduate students in the college. These charts often feature multiple or overlay plots and dense data groups, with the code averaging over 100 lines. Such content is unlikely to be included in the training data of current models. For each sample, GPT-4o receives three types of inputtable, code, and chart imagesto generate challenging Q&A pair. In total, 75 pairs are created, randomly shuffled, and then presented to three annotators for blind evaluation. Table 2: Rating results and costs for different input types in our study. The annotators are asked to rate each pair on accuracy, reasoning complexity, and visual reference, using scale of 1 (low) to 3 (high). The results in Table 2 indicate that both text-based inputs outperform visual chart input in the first two aspects, with code scoring 2.60 in accuracy (vs. 1.91) and 2.56 in reasoning complexity (vs. 1.53). As expected, table input has the lowest visual reference score (1.19), while chart input scores highest in this (2.36), confirming the ability of MLLMs to directly interpret visual information. Surprisingly, the code achieves relatively high visual reference score (2.15) even without image input. It suggests that code has the potential to translate visual charts into textual representations. 0.047 0.092 0.107 Table Code Chart Reas. Comp. 2.72 2.60 1.91 2.51 2.56 1.53 1.19 2.15 2.36 Input Acc. Vis. Refer. Cost ($)"
        },
        {
            "title": "3 REACHQA: SYNTHESIZING CHART Q&A WITH CIT",
            "content": "REACHQA is multimodal Chart Question-Answering (CQA) dataset fully generated by LLMs, consisting of synthetic charts paired with corresponding Q&A. It is constructed with Code-asIntermediary Translation (CIT), data synthesis method for distilling visual reasoning abilities from LLMs to MLLMs, as illustrated in Figure 2. In the following sections, we describe how we synthesize the codes ( 3.1), generate charts and instructions ( 3.2), and ensure data quality ( 3.3). 3.1 INTERMEDIARY CODE SYNTHESIS This section describes how the intermediary code is synthesized from the seed code. Seed Code Collection. We start by collecting small set of 33 seed code samples, which we refer to as Cseed. These samples are sourced directly from the official Matplotlib gallery1 to ensure quality and minimize manual effort. Collectively, the code samples, each averaging around 40 lines in length, cover diverse range of chart types, including common types like bar, line, and scatter charts, as well as more specialized charts such as bubble, contour, and donut charts. All samples are verified for executability to guarantee the reliability of the subsequent code synthesis process. Self-Instruct for Diverse Code Generation. To expand the diversity and coverage of the synthetic chart set, we apply the Self-Instruct method (Wang et al., 2023), which has been used to generate new instructions by presenting LLMs with existing ones. Plenty of works have demonstrated its effectiveness (Liu et al., 2024b). In our approach, instead of instructions, we provide code samples as context, guiding the model to generate novel chart-plotting code. At each step, 3 randomly selected code snippets from the code pool serve as few-shot examples (Brown et al., 2020). To diversify chart generation, chart type is randomly chosen from 10 major and 32 minor categories for the model to synthesize. For chart content, we provide two topic options, allowing the model to freely combine or expand on these themes based on its knowledge, leading to varied topics and data. chain-of-thought (CoT) process (Wei et al., 2022) is used for code generation, starting with the charts background and data, followed by the final executable code. This step-by-step approach 1https://matplotlib.org/stable/gallery/index.html 4 Preprint. Work in Progress. Figure 2: Overview of the Code-as-Intermediary Translation (CIT) method for synthesizing multimodal instruction data. The process begins with 33 seed codes and generates plot codes across various chart types, topics, and complexity levels through the Self-Instruct and Evol-Instruct stages. The chart set and instruction set are constructed bi-directionally, and the final filtered data yields REACHQA, dataset for distilling visual chart reasoning abilities from LLMs to MLLMs. ensures logical coherence and code functionality. The generated codes are referred to as Ceasy for use in subsequent phases of the construction. The chart types and topics are detailed in Appendix A. Evol-Instruct for Complex Code Generation. To enhance the complexity of the synthetic chart set, we adopt the Evol-Instruct method (Xu et al., 2024). Evol-Instruct leverages LLMs to evolve simple chart-plotting code into more intricate versions by presenting existing code alongside an evolution strategy as context. This approach addresses key limitation in prior work that emphasizes the quantity of charts while often neglecting the difficulty of chart interpretation. At each step, we take code samples from Ceasy and evolve them with randomly selected strategy from four predefined directions: (1) expanding the data size or number of data groups; (2) adding or modifying visual elements to enhance presentation; (3) overlaying different type of chart on the original plot; (4) introducing an additional subplot beside the original plot. These strategies ensure that the resulting charts demand more nuanced visual interpretation and in-depth reasoning. As in previous phases, we follow CoT process, where the model first analyzes the given code, outlines the evolution details, and then generates the evolved code. The evolved codes, referred to as Chard, are also added to the code pool for subsequent use. 3.2 BI-DIRECTIONAL TRANSLATION This section outlines the bi-directional construction process of charts and instruction sets. Chart Generation through Code Execution and Self-Repair. We generate charts by executing the Python plotting code. However, during the generation and evolution process, program errors are inevitable. To ensure correctness, we will validate the code before adding it to the pool. When errors occur, the code is not immediately discarded; instead, we apply Self-Repair method (Chen et al., 2024a), feeding the code and execution results into the LLMs for correction. This process repeats until the code is fixed or reaches an iteration limit, after which the code is discarded if it remains faulty. On average, this approach fixes about 15% of the code generated by GPT-4o, with 5% remaining unrepairable and filtered out. Preprint. Work in Progress. Instruction Generation through Guided Prompting. After verifying the executability of codes, we use them to create instruction sets in the form of question-answer pairs. Building on prior work of in-context Q&A generation (Chen et al., 2023; He et al., 2024), we guide the model through two-step process: first synthesizing batch of questions and then generating corresponding answers separately. To ensure high-quality answers, we also employ step-by-step approach where the model first generates detailed calculations and analyses, which are subsequently refined into concise, educational answers optimized for learning (Gunasekar et al., 2023). The model will generate two types of instructions: recognition-oriented, focusing on visual information retrieval, and reasoning-oriented, requiring both comprehensive recognition and multi-step reasoning. With minimal constraints on question content, the model is encouraged to explore creative and diverse instructions. Each chart can be used to generate multiple questions, and we filter redundant ones using ROUGE-L overlap, following the Self-Instruct method (Wang et al., 2023)."
        },
        {
            "title": "3.3 QUALITY ASSURANCE",
            "content": "This section aims to improve the quality of generated data. Multimodal Validation for Enhanced Data Quality. Although our dataset is synthesized entirely with LLMs, we recognize the need to integrate visual information for higher data quality (Masry et al., 2024b; Zeng et al., 2024). Therefore, we introduce multimodal validation step, utilizing MLLMs to verify both generated charts and their corresponding instructions. Due to the variations in model architectures, visual encoders, and training recipes, different models may focus on varying aspects of the images. Taking this into account, we adopt an majority voting strategy using multiple open-source models such as MiniCPM-V2.5-Llama3 (Yao et al., 2024), InternVL2-8B (Chen et al., 2024b) and Qwen2-VL8B (Wang et al., 2024b). This ensures reliable visual validation while remaining cost-effective by ensembling smaller, locally hosted models. Table 3: REACHQA dataset statistics. Question and answer lengths are calculated based on the GPT-4o tokenizer. Statistics Train Set Test Set Total charts - # Chart types - # Overlay plots - # Multiple plots - Average size (px) Unique questions - # Reco. per chart - # Reas. per chart 3,249 10 / 32 1,030 593 500 10 / 32 220 251 24801571 27981601 19, 963 2.53 3.62 2,000 2 Avg. Reco. Q. length Avg. Reco. A. length Avg. Reas. Q. length Avg. Reas. A. length 22.1 38.3 38.2 68.4 21.0 7.0 35.4 24.9 For chart validation, we define set of criteria for good chart, and each model assigns score between 1 and 5. The average score is calculated, and charts falling below threshold are filtered out. For instructions, both the Q&A pairs and the associated charts are fed into the models, which verify the answers step-by-step and give yes/no judgment. Samples receiving multiple negative votes are discarded. Testing Set Construction and Annotation Refinement. For the REACHQA testing set, we follow similar process as in previous data generation, but with stricter filtering criteria to ensure higher quality. In addition, annotators are recruited for manual review and refinement. All had at least bachelors degree and are familiar with common chart types. They also had prior experience in data annotation. For the charts, annotators first review the images to identify any potential visual errors. For the Q&A pairs, they ensure the questions are relevant to the chart and answerable, then correct any hallucinations or logical inconsistencies in the answers. Afterwards, annotators conduct two rounds of review to confirm the data meets the multimodal recognition or reasoning standards in our settings. Only samples where at least two annotators agree are accepted into our dataset. The inter-annotator agreement, reflected by kappa coefficient of 0.82, demonstrates strong consistency (Landis, 1977). Table 3 summarizes the final dataset statistics. The total cost of data construction, excluding open-source model usage and annotation labor for the testing set, was about $300. detailed expense breakdown is provided in Appendix B. All the prompt templates we use are shown in Appendix D. 6 Preprint. Work in Progress."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUPS",
            "content": "Benchmarks. We evaluate the models on three categories of tasks. (1) Traditional chart-related benchmarks focused on recognition tasks, including ChartQA (Masry et al., 2022), ChartBench (Xu et al., 2023), and ChartX (Xia et al., 2024). For ChartQA, we use its 2.5k test set. For multi-task datasets like ChartBench and ChartX, we only use QA tasks. Specifically, from ChartBench, we select 2k binary QA samples (Yes/No answers) and 2.1k numerical QA samples. From ChartX, we use its 6k QA samples. (2) Novel chart-related benchmarks that assess both recognition and reasoning abilities, including CharXiv (Wang et al., 2024c) and our REACHQA test set. For CharXiv, we use the validation set, which contains 4k descriptive questions and 1k reasoning questions. Our REACHQA test set includes 1k recognition-oriented and 1k reasoning-oriented questions. (3) General multimodal reasoning benchmarks, including MathVista (Lu et al., 2024) and MATH-Vision (Wang et al., 2024a). For MathVista, we use the testmini set with 540 math-targeted questions and 460 general VQA questions. For MATH-Vision, we use its 3,040 math competition problems. Models and baselines. We evaluate range of multimodal large language models (MLLMs) across three categories. (1) Powerful proprietary models, including GPT-4o (OpenAI, 2024a), GPT4o mini (OpenAI, 2024b), and Claude 3.5 Sonnet (Anthropic, 2024). (2) Chart-augmented opensource models, such as ChartInstruct-7B (Masry et al., 2024a), ChartAssistant-13B (Meng et al., 2024), and ChartGemma-3B (Masry et al., 2024b), which are specifically enhanced for chart-related tasks. (3) Latest general open-source models, including LLaVA-Next-Llama3-8B (Li et al., 2024), MiniCPM-V2.5-Llama3-8B (Yao et al., 2024), and InternVL2-8B (Chen et al., 2024b). Besides, we provide text-only baseline, denoted as Random (GPT-4o), where we prompt GPT-4o to reasonably guess the answer without seeing the image following Wang et al. (2024c). Experiment Details. For each general open-source model, we conduct supervised fine-tuning (SFT) using our REACHQA training set. We fine-tuned three versions of each model: (1) using 8k recognition-oriented Q&A samples (denoted as Reco.), (2) using 12k reasoning-oriented samples (denoted as Reas.), and (3) combined version with 20k samples (denoted as All). We apply Low-rank Adapters (LoRA, Hu et al., 2022) to all linear layers of the language model and projector, with LoRA rank of 16, LoRA alpha of 8 and learning rate of 2e-5. To fully leverage their capabilities, we prompt all models with zero-shot CoT prompt, Lets think step by step (Kojima et al., 2022), following OpenAI (2024a) and Anthropic (2024). Thus, to extract answers from the model responses and assess their correctness, we employ the LLM-as-a-judge method (Zheng et al., 2023) to calculate relaxed accuracy. The judge model used is GPT-4o, and the prompt template for evaluation can be found in Appendix D.4. 4.2 EXPERIMENTAL RESULTS Table 4 presents the quantitative results for all models and baselines across each task, allowing for clear comparison of their recognition and reasoning abilities. We can find that: Synthetic datasets can also effectively measure abilities. Our REACHQA test set effectively evaluates models reasoning and recognition skills, showing trends similar to human-annotated datasets like CharXiv. For instance, GPT-4o exhibits reasoning score of 39.70 and recognition score of 66.80 on REACHQA, closely mirroring its performance on CharXiv (i.e., 47.10 and 84.45, respectively). This consistency suggests that LLM-generated datasets, with minimal human intervention, can rival human-labeled data. Moreover, REACHQA presents significant challenge to models visual abilities, as random guessing results in very low scores. In contrast, traditional benchmarks like ChartQA and ChartBench may allow models to leverage pre-existing knowledge, inflating results without truly testing visual capabilities (Yue et al., 2024; Wang et al., 2024c). Proprietary models demonstrate more balanced performance. Proprietary multimodal models, including the cost-effective GPT-4o mini, achieve competitive results on both traditional chartrelated benchmarks and reasoning-intensive tasks like REACHQA and CharXiv. In contrast, opensource models, whether chart-augmented or general-purpose, excel in recognition tasks with simpler 7 Preprint. Work in Progress. Table 4: Evaluation results on seven benchmarks. Details for these benchmarks and models are presented in 4.1. The best performance for each category and task is in bold. The percentage of performance improvements compared to the vanilla model is denoted by (). Models Avg. () ChartQA ChartBench ChartX REACHQA CharXiv MathVista MATH-V QA Binary NQA QA Reas. Reco. Reas. Desc. Math General QA Human Random (GPT-4o) GPT-4o mini GPT-4o Claude 3.5 Sonnet ChartInstruct-7B ChartAssistant-13B ChartGemma-3B - 20.82 49.34 59.85 64.50 25.93 28.25 33.08 Baselines - 30. - - 40.21 22.73 - 19.85 65.10 84.60 80.50 92.10 8.20 13.30 10.80 19.85 Proprietary Multimodal Large Language Models 77.52 85.70 90.80 70.26 34.93 81.03 52.88 76.72 48.29 35.45 46.60 58.24 27.20 53.50 34.10 74.92 39.70 66.80 47.10 84.45 51.70 74.30 60.20 84.30 60.30 17. 56.70 63.80 67.70 Chart-augmented Multimodal Large Language Models 66.64 79.90 80.16 61.40 26.95 26. 58.15 24.62 78.90 34.10 23.20 35.15 6.00 10.50 8.80 21.40 15.37 10.70 19.60 11.70 16.93 17.78 9.20 27.80 12.50 21.30 19.07 Open-Source Multimodal Large Language Models LLaVA-Next-Llama3-8B + REACHQA (Reco.) 24.46 32.88 (+34.4%) + REACHQA (Reas.) + REACHQA (All) 32.39 (+32.4%) 32.98 (+34.8%) MiniCPM-V2.5-Llama3 + REACHQA (Reco.) 33.39 38.62 (+15.7%) + REACHQA (Reas.) + REACHQA (All) 38.52 (+15.4%) 38.67 (+15.8%) InternVL2-8B 40.03 + REACHQA (Reco.) + REACHQA (Reas.) + REACHQA (All) 48.21 (+20.4%) 47.87 (+19.6%) 48.35 (+20.8%) 45.80 66.96 64.48 64.56 66.92 71.12 71.72 71.44 73.80 82.92 82. 82.44 42.90 15.86 56.95 29.52 56.80 25.14 57.00 29.33 48.90 22.29 56.65 33.29 56.65 29.62 55.80 30.43 52.05 32.86 66.35 46.14 64.05 46.52 65.90 47.29 15.45 27.25 25.90 27.08 23.72 29.53 28.23 29. 35.10 46.62 44.88 45.38 6.50 17.90 17.20 31.45 22.41 8.80 29.00 22.20 32.58 27.40 8.40 26.30 22.70 35.67 28.89 11.10 29.60 22.50 32.33 27.59 10.30 25.30 22.00 46.20 37.22 10.60 34.10 25.60 48.75 41.48 11.00 33.00 27.50 48.70 43.52 11.00 35.10 28.30 47.62 42.22 16.20 33.70 26.30 46.10 46.11 19.90 49.50 32.20 54.38 47.96 20.10 49.40 32.80 52.40 49.44 21.30 49.80 32.70 54.83 48.89 31.52 39. 38.04 44.13 49.78 50.65 50.43 53.04 60.43 60.22 60.00 61.74 67. 66.52 66.30 75.66 25.36 28.85 30.39 32.76 10.07 8.55 7. 9.44 11.25 11.38 11.25 11.45 13.22 13.52 13.75 16.38 16.78 17. 17.01 charts but struggle in complex ones. This disparity highlights their imbalanced capabilities, and also suggests potential overfitting to traditional benchmarks. Although proprietary models may not always lead in specific tasks, their stable performance across recognition and reasoning tasks indicates better generalization, making them more suitable for real-world applications. Specialized training data significantly improves model performance. Models trained on 8k REACHQA recognition data outperform in recognition tasks, while those trained on 12k reasoning data could do better in reasoning tasks. When both data types are combined (i.e., 20k in total), models see the greatest improvement, with performance increasing by at least 15% across all models we test. Notably, the LLaVA-Next-Llama3-8B model achieves 34.8% boost in average performance. This suggests that models visual capability comprises two complementary aspects, and training on both data types together produces optimal results. Moreover, despite the absence of math-target data in the training set, the models generalize well to the MathVista and MATH-Vision benchmarks, highlighting the transferability of multimodal reasoning abilities distilled from expert rationales."
        },
        {
            "title": "5 DISCUSSION",
            "content": "5.1 DISTILLING EXPERT RATIONALES IMPROVES REASONING To explore the origins of general reasoning abilities, we conduct an experiment comparing several open-source training datasets, including ChartBench (Xu et al., 2023), ChartAst (Meng et al., 2024), and ChartGemma (Masry et al., 2024b). To ensure fairness, we uniformly sample 20,000 instructions from Q&A tasks in each dataset and train LLaVA-Next-Llama3-8B under the same settings. As shown in Table 5, the model trained on ChartBench performs the worst. This may be due to its instruction responses, which contain only final answers without the reasoning process, limiting the model to learning the reasoning patterns. Although ChartAst includes rationale annotations, their effectiveness is constrained by the simplicity of its template-based questions and lack of reasoning 8 Preprint. Work in Progress. Figure 3: Performance comparison of different Reco.- & Reas.-oriented training data ratios with 8k total data. The dashed line represents the models performance trained with full 20k data. Figure 4: Performance comparison of models on 6 general tasks and 4 specialized tasks. depth. In contrast, the models trained on ChartGemma and REACHQA achieve better performance. This improvement can be attributed to the distillation of high-quality expert rationales (i.e., from Gemini Flash 1.5 and GPT-4o), which significantly enhance the models visual reasoning abilities. Table 5: Performance comparison of models trained on different datasets. The REACHQA and CharXiv scores refer to Reas. splits here. Models Avg. REACHQA CharXiv MathVista Math-V 16.39 Base Model 17.06 + ChartBench + ChartAst 17.67 + ChartGemma 19.11 + REACHQA 20.74 6.50 7.30 7.10 10.00 11. 17.20 17.00 20.40 19.40 22.50 32.40 33.60 32.10 36.40 38.10 9.44 10.33 11.08 10.62 11.25 While we only use SFT to train the model, incorporating advanced methods like RL (Xi et al., 2024) or DPO (Mitra et al., 2024) could further improve the results. Additionally, the visual richness of charts, as detailed in Appendix C, may enhance the models generalization abilities, too. 5.2 INTERACTION BETWEEN RECOGNITION AND REASONING ABILITIES As mentioned before, the recognition and reasoning abilities are likely interdependent. Wang et al. (2024c) also suggest that recognition skills serve as prerequisites for effective reasoning. To explore this further, we conduct an experiment by fixing the total training data at 8k and varying the ratio of reasoning to recognition data from 8 : 0 to 0 : 8. We train the LLaVA-Next-Llama3-8B model and evaluate them on both recognition-focused benchmarks (i.e., ChartQA, ChartBench, ChartX) and reasoning-focused ones (i.e., REACHQA-Reas., CharXiv-Reas., MathVista). As shown in Figure 3, increasing the proportion of recognition or reasoning data results in improvements in corresponding task performance. Notably, models trained with higher ratio of recognition data outperform those trained on 20k total data in recognition tasks. In contrast, as the ratio of reasoning data increases, performance gains plateau and even decline when the ratio reaches 100%. This suggests that simply increasing the proportion or scale of reasoning-oriented data may lead to diminishing returns. We hypothesize that this is because reasoning abilities are partially dependent on recognition skills. When model fails to accurately interpret an image, its reasoning process is likely compromised (Wang et al., 2024c). Due to resource limitations, we are unable to scale the total data size further in this study. However, we expect that with larger datasets, the interaction between recognition and reasoning data would become even more significant. 5.3 BALANCING GENERAL VISUAL UNDERSTANDING AND SPECIALIZED ABILITIES We are also interested in how models trained on specialized data perform in non-reasoning generalpurpose multimodal tasks. To investigate this, we conduct an experiment using 6 popular general multimodal benchmarksMME-Reasoning, MME-Perception (Fu et al., 2023), SeedBench (Li et al., 2023a), CCBench (Liu et al., 2023), POPE (Li et al., 2023b), and HallusionBench (Guan et al., 2024)along with 4 specialized benchmarks: REACHQA, CharXiv, MathVista, and MATH-Vision. We test three versions of LLaVA-Next-Llama3-8B models: the vanilla model, the one trained on 20k 9 Preprint. Work in Progress. Figure 5: An example of attention visualization from the ChartQA dataset. The top row shows the results from the vanilla LLaVA-Next-Llama3-8B model, while the bottom row displays the results from our fine-tuned model. For each output, we present the attention distribution (highlighted zones) at three key steps, calculated by averaging the attention values of all tokens in each step. REACHQA samples, and another trained on 20k REACHQA samples combined with 20k generalpurpose multimodal data randomly selected from the 779k LLaVA-NeXT-Data2. This dataset is chosen because the LLaVA-NeXT family of models was officially fine-tuned on it (Li et al., 2024), allowing us to approximate its original data distribution. The results are shown in Figure 4. We can observe that the vanilla model (green area) struggles with reasoning-intensive tasks, while the model trained on 20k REACHQA data (orange area) improves in reasoning but loses general visual performance. Surprisingly, by incorporating just 20k of general instruction data (blue area), the model not only recovers its general multimodal performance but also retains the enhanced reasoning ability. This results in well-balanced model with significant boost in reasoning improvements and minimal drops in general domains. 5.4 INTERPRETABILITY STUDY FROM THE PERSPECTIVE OF ATTENTION To explore the mechanism behind the improved performance of our fine-tuned model, we conduct an analysis of the attention patterns during the next token prediction (Liang et al., 2022; Faysse et al., 2024). Figure 5 presents comparative case study between the vanilla model and the fine-tuned model. Here, we apply full-parameter fine-tuning instead of LoRA to induce more pronounced changes in the attention layers (Hu et al., 2022). The results show that the vanilla model produces lengthy outputs with redundant analysis and dispersed attention across the image, reaching wrong conclusion at the end. In contrast, the fine-tuned model identifies the key information at each step, with attention that accurately focuses on relevant visual elements (i.e., labels, axes and values). This suggests that the model not only imitates expert rationales but also learns the underlying attention patterns crucial for effective visual reasoning. The model automatically establishes synergistic relationship between recognition and reasoning capabilities, understanding what to recognize during the reasoning process and utilizing these recognition results to guide subsequent reasoning steps. 2https://huggingface.co/datasets/lmms-lab/LLaVA-NeXT-Data 10 Preprint. Work in Progress."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we delve into the core challenges faced by MLLMs in complex chart Q&A tasks, highlighting their deficiencies in both recognition and reasoning. Building on our analysis of existing datasets and the untapped potential of LLMs, we propose Code-as-Intermediary Translation (CIT) as novel method to distill LLMs abilities and improve MLLMs. With code as bridge between visual and textual modalities, CIT enables language models to interpret complex charts more precisely, facilitating the generation of higher-quality Q&A pairs. Our synthetic dataset, REACHQA, demonstrates significant performance improvements across multiple models and benchmarks, with gains extending beyond chart-specific to broader multimodal reasoning tasks. We believe that CIT offers promising direction for scalable and cost-effective multimodal instruction data synthesis."
        },
        {
            "title": "LIMITATIONS",
            "content": "We summarize the limitations of our method as follows: (1) While CIT effectively uses code to link text and abstract images like charts and diagrams, applying this approach to natural images remains challenging. Current text-to-image models still lack precise control over fine details (Betker et al., 2023; Zhang et al., 2023), which can lead to misaligned synthetic data. Once more controllable techniques are developed, the synthesis of multimodal data could become more flexible and applicable. (2) Although multimodal validation steps were introduced to reduce errors, the synthesized charts and Q&A pairs might still contain occasional inaccuracies. Therefore, to ensure data quality for larger-scale applications, stronger models and stricter thresholds are essential."
        },
        {
            "title": "REFERENCES",
            "content": "Anthropic. Introducing claude 3.5 sonnet, June 2024. URL https://www.anthropic.com/ news/claude-3-5-sonnet. James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science. https://cdn. openai. com/papers/dall-e-3. pdf, 2(3):8, 2023. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners. In Hugo Larochelle, MarcAurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual, 2020. Wei-Lin Chen, Cheng-Kuang Wu, Yun-Nung Chen, and Hsin-Hsi Chen. Self-icl: Zero-shot incontext learning with self-generated demonstrations. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 1565115662. Association for Computational Linguistics, 2023. Xinyun Chen, Maxwell Lin, Nathanael Scharli, and Denny Zhou. Teaching large language models to self-debug. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024a. Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, et al. Internvl: Scaling up vision foundation models and aligning In Proceedings of the IEEE/CVF Conference on Computer for generic visual-linguistic tasks. Vision and Pattern Recognition, pp. 2418524198, 2024b. Manuel Faysse, Hugues Sibille, Tony Wu, Bilel Omrani, Gautier Viaud, Celine Hudelot, and Pierre Colombo. Colpali: Efficient document retrieval with vision language models. CoRR, abs/2407.01449, 2024. 11 Preprint. Work in Progress. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Zhenyu Qiu, Wei Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, and Rongrong Ji. MME: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Deqing Fu, Ghazal Khalighinejad, Ollie Liu, Bhuwan Dhingra, Dani Yogatama, Robin Jia, and Willie Neiswanger. Isobench: Benchmarking multimodal foundation models on isomorphic representations. arXiv preprint arXiv:2404.01266, 2024. Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, Dinesh Manocha, and Tianyi Zhou. Hallusionbench: An advanced diagnostic suite for entangled language hallucination and visual illusion in large visionlanguage models. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pp. 1437514385, 2024. doi: 10.1109/CVPR52733.2024.01363. Suriya Gunasekar, Yi Zhang, Jyoti Aneja, Caio Cesar Teodoro Mendes, Allie Del Giorno, Sivakanth Gopi, Mojan Javaheripi, Piero Kauffmann, Gustavo de Rosa, Olli Saarikivi, et al. Textbooks are all you need. arXiv preprint arXiv:2306.11644, 2023. Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. Chartllama: multimodal llm for chart understanding and generation. arXiv preprint arXiv:2311.16483, 2023. Wei He, Shichun Liu, Jun Zhao, Yiwen Ding, Yi Lu, Zhiheng Xi, Tao Gui, Qi Zhang, and Xuanjing Huang. Self-demos: Eliciting out-of-demonstration generalizability in large language models. In Kevin Duh, Helena Gomez-Adorno, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pp. 3829 3845. Association for Computational Linguistics, 2024. Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, In The Tenth Interand Weizhu Chen. Lora: Low-rank adaptation of large language models. national Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022, 2022. Kung-Hsiang Huang, Hou Pong Chan, Yi Fung, Haoyi Qiu, Mingyang Zhou, Shafiq Joty, Shih-Fu Chang, and Heng Ji. From pixels to insights: survey on automatic chart understanding in the era of large foundation models. arXiv preprint arXiv:2403.12027, 2024. Shankar Kantharaj, Xuan Long Do, Rixie Tiffany Ko Leong, Jia Qing Tan, Enamul Hoque, and Shafiq R. Joty. Opencqa: Open-ended question answering with charts. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang (eds.), Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, EMNLP 2022, Abu Dhabi, United Arab Emirates, December 7-11, 2022, pp. 1181711837. Association for Computational Linguistics, 2022. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. JR Landis. The measurement of observer agreement for categorical data. Biometrics, 1977. Jacqueline Leon. From universal languages to intermediary languages in machine translation. In History of Linguistics 2002: Selected Papers from the Ninth International Conference on the History of the Language Sciences, 2730 August 2002, Sao Paulo-Campinas, volume 110, pp. 123. John Benjamins Publishing Amsterdam, 2007. Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Llava-next: Stronger llms supercharge multimodal caURL https://llava-vl.github.io/blog/ Ziwei Liu, and Chunyuan Li. pabilities in the wild, May 2024. 2024-05-10-llava-next-stronger-llms/. 12 Preprint. Work in Progress. Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. Seed-bench: Benchmarking multimodal llms with generative comprehension. arXiv preprint arXiv:2307.16125, 2023a. Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. Evaluating object hallucination in large vision-language models. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP 2023, Singapore, December 6-10, 2023, pp. 292305. Association for Computational Linguistics, 2023b. Youwei Liang, Chongjian Ge, Zhan Tong, Yibing Song, Jue Wang, and Pengtao Xie. Evit: Expediting vision transformers via token reorganizations. In The Tenth International Conference on Learning Representations, ICLR 2022, Virtual Event, April 25-29, 2022. OpenReview.net, 2022. Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. MMC: advancing multimodal chart understanding with large-scale instruction tuning. In Kevin Duh, Helena Gomez-Adorno, and Steven Bethard (eds.), Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), NAACL 2024, Mexico City, Mexico, June 16-21, 2024, pp. 12871310. Association for Computational Linguistics, 2024a. Ruibo Liu, Jerry Wei, Fangyu Liu, Chenglei Si, Yanzhe Zhang, Jinmeng Rao, Steven Zheng, Daiyi Peng, Diyi Yang, Denny Zhou, et al. Best practices and lessons learned on synthetic data for language models. arXiv preprint arXiv:2404.07503, 2024b. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, KaiWei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024. Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq R. Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio (eds.), Findings of the Association for Computational Linguistics: ACL 2022, pp. 22632279, 2022. Ahmed Masry, Mehrad Shahmohammadi, Md. Rizwan Parvez, Enamul Hoque, and Shafiq Joty. Chartinstruct: Instruction tuning for chart comprehension and reasoning. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics, ACL 2024, Bangkok, Thailand and virtual meeting, August 11-16, 2024, pp. 1038710409. Association for Computational Linguistics, 2024a. Ahmed Masry, Megh Thakkar, Aayush Bajaj, Aaryaman Kartha, Enamul Hoque, and Shafiq Joty. Chartgemma: Visual instruction-tuning for chart reasoning in the wild. arXiv preprint arXiv:2407.04172, 2024b. Fanqing Meng, Wenqi Shao, Quanfeng Lu, Peng Gao, Kaipeng Zhang, Yu Qiao, and Ping Luo. Chartassisstant: universal chart multimodal language model via chart-to-table pre-training and multitask instruction tuning. arXiv preprint arXiv:2401.02384, 2024. Nitesh Methani, Pritha Ganguly, Mitesh M. Khapra, and Pratyush Kumar. Plotqa: Reasoning over scientific plots. In IEEE Winter Conference on Applications of Computer Vision, WACV 2020, Snowmass Village, CO, USA, March 1-5, 2020, pp. 15161525. IEEE, 2020. Arindam Mitra, Hamed Khanpour, Corby Rosset, and Ahmed Awadallah. Orca-math: Unlocking the potential of slms in grade school math. arXiv preprint arXiv:2402.14830, 2024. OpenAI. Gpt-4o, August 2024a. URL https://openai.com/index/hello-gpt-4o/. OpenAI. Gpt-4o mini, August 2024b. URL https://openai.com/index/ gpt-4o-mini-advancing-cost-efficient-intelligence/. 13 Preprint. Work in Progress. Ke Wang, Junting Pan, Weikang Shi, Zimu Lu, Mingjie Zhan, and Hongsheng Li. Measuring multimodal mathematical reasoning with math-vision dataset. arXiv preprint arXiv:2402.14804, 2024a. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 1348413508. Association for Computational Linguistics, 2023. Zirui Wang, Mengzhou Xia, Luxi He, Howard Chen, Yitao Liu, Richard Zhu, Kaiqu Liang, Xindi Wu, Haotian Liu, Sadhika Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. arXiv preprint arXiv:2406.18521, 2024c. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, 2022. Zhiheng Xi, Wenxiang Chen, Boyang Hong, Senjie Jin, Rui Zheng, Wei He, Yiwen Ding, Shichun Liu, Xin Guo, Junzhe Wang, Honglin Guo, Wei Shen, Xiaoran Fan, Yuhao Zhou, Shihan Dou, Xiao Wang, Xinbo Zhang, Peng Sun, Tao Gui, Qi Zhang, and Xuanjing Huang. Training large language models for reasoning through reverse curriculum reinforcement learning. In Forty-first International Conference on Machine Learning, ICML 2024, Vienna, Austria, July 21-27, 2024, 2024. Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Min Dou, Botian Shi, Junchi Yan, et al. Chartx & chartvlm: versatile benchmark and foundation model for complicated chart reasoning. arXiv preprint arXiv:2402.12185, 2024. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, Qingwei Lin, and Daxin Jiang. Wizardlm: Empowering large pre-trained language models to follow complex instructions. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024, 2024. Zhengzhuo Xu, Sinan Du, Yiyan Qi, Chengjin Xu, Chun Yuan, and Jian Guo. Chartbench: benchmark for complex visual reasoning in charts. arXiv preprint arXiv:2312.15915, 2023. Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, et al. Minicpm-v: gpt-4v level mllm on your phone. arXiv preprint arXiv:2408.01800, 2024. Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Ming Yin, Botao Yu, Ge Zhang, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024. Michael Zarechnak. The intermediary language for multilanguage translation. Computers and translation, 1(2):8391, 1986. Xingchen Zeng, Haichuan Lin, Yilin Ye, and Wei Zeng. Advancing multimodal large language models in chart question answering with visualization-referenced instruction tuning. IEEE Transactions on Visualization and Computer Graphics, 2024. Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In IEEE/CVF International Conference on Computer Vision, ICCV 2023, Paris, France, October 1-6, 2023, pp. 38133824. IEEE, 2023. 14 Preprint. Work in Progress. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023."
        },
        {
            "title": "A CHART TYPES AND TOPICS",
            "content": "We predefined several chart types and topics for Self-Instruct prompting. Table 6 presents the 9 major categories we established, along with their corresponding subcategories. Additionally, Table 7 lists the 38 topics we specified. It is important to note that these topics do not reflect the actual topic distributions in the generated charts, as we encourage the model to combine and expand upon them. Table 6: Major categories and minor categories of charts in REACHQA. Major Category Minor Category Line Charts Pie Charts Bar Charts 3D Bar Charts Node Charts Radar Charts Area Charts Box Charts Scatter Charts Specific Charts line chart, line chart with data annotation, line chart with error bar pie chart, donut pie chart, sector pie chart, ring chart bar chart, bar chart with data annotation, stacked bar chart, percentage bar chart, horizontal bar chart 3D bar chart, stacked 3D bar chart, percentage 3D bar chart directed node chart, undirected node chart radar chart, radar chart with area filling area chart, stacked area chart vertical box chart, horizontal box chart scatter chart, scatter chart with smooth fitting, 3D scatter chart (bubble chart) heat map, rose chart, funnel chart, waterfall chart, histogram, tree map Table 7: Predefined chart topics in Self-Instruct prompting. Art and Design Music and Performance Business and Finance Travel and Exploration Books and Publishing Literature and Writing History and Culture Architecture and Building Fashion and Style Marketing and Advertising Law and Legal Affairs Film and Cinema Mathematics and Statistics Agriculture and Food Production Transportation and Logistics Real Estate and Housing Market Government and Public Policy Education and Academics Environment and Sustainability Language and Communication Social Sciences and Humanities Manufacturing and Production Futurism and Innovation Astronomy and Space Social Media and the Web Society and Community Physics and Chemistry Energy and Utilities Biology and Life Sciences Retail and E-commerce Religion and Spirituality Food and Beverage Industry Artificial Intelligence and Robotics Healthcare and Health Sports and Entertainment Science and Engineering Human Resources and Employee Management Computer Science and Information Technology"
        },
        {
            "title": "B COST OF REACHQA TRAINING DATA CONSTRUCTION",
            "content": "Table 8 provides detailed expense breakdown. We executed Self-Instruct and Evol-Instruct 3,000 times each to synthesize chart-plotting code, theoretically generating 6,000 charts. However, after accounting for non-executable code and images filtered out by MLLM rating, we ultimately produced 3,249 charts for Q&A synthesis. 15 Preprint. Work in Progress. Table 8: The average number of input and output tokens is calculated for each step in the REACHQA construction process. In the equation, each term represents the average number of tokens per step (used only in multi-step framework), while each multiplier corresponds to the number of times that step is executed. The pricing for GPT-4o-2024-08-06 is $2.50 per 1M input tokens and $10.00 per 1M output tokens. As result, the total cost amounts to approximately $303.95. Step Avg. #tokens of Input Avg. #tokens of Output Times Cost ($) Self-Instruct Evol-Instruct Self-Repair Reas-QA-Gen. Reco-QA-Gen. 1, 500 + 2, 000 = 3, 500 700 + 1, 300 = 2, 000 500 1, 000 + 1, 500 4 = 7, 000 800 + 1, 200 4 = 5, 600 500 + 500 = 1, 000 300 + 700 = 1, 000 500 500 + 300 4 = 1, 700 300 + 200 4 = 1, 100 3,000 56.25 3,000 45.00 1,500 9.38 3,249 112.09 3,249 81."
        },
        {
            "title": "C VISUALIZATION OF CHARTS IN DIFFERENT DATASET",
            "content": "We randomly sample several charts from the training set of ChartQA (Masry et al., 2022), ChartBench (Xu et al., 2023), ChartAst (Meng et al., 2024), ChartGemma (Masry et al., 2024b), and REACHQA. The visualization of the results is presented in Figure 6."
        },
        {
            "title": "D PROMPT TEMPLATES",
            "content": "Here, we present the prompt templates used in this paper. D.1 INTERMEDIARY CODE SYNTHESIS The prompts used for code generation via the Self-Instruct method are presented in Figure 7, and Figure 8 shows the prompts for the Evol-Instruct method. As illustrated in Figure 9, we utilize four predefined directions to evolve the simple chart-plotting code. D.2 BI-DIRECTIONAL TRANSLATION The prompt used for the Self-Repair method is presented in Figure 10. Additionally, the prompt templates for generating reasoning-oriented questions and answers are listed in Figure 11 and Figure 12. The prompt details for generating recognition-oriented questions and answers are listed in Figure 13 and Figure 14. D.3 QUALITY ASSURANCE The prompt details for rating charts and Q&A are illustrated in Figure 15 and 16. D.4 EVALUATION In the evaluation process, we utilize the LLM-as-a-judge method. The detailed prompt template is illustrated in Figure 17. 16 Preprint. Work in Progress. (a) ChartQA contains 3 types of charts collected from 4 websites. (b) ChartBench contains 9 types of synthetic charts but no visual complexity. (c) ChartAssistant contains 9 types of synthetic charts but no visual complexity. (d) ChartGemma contains charts collected from boarder websites. (e) REACHQA contains 10 types of charts and more complex variations. Figure 6: Visualizations of different chart-related training datasets. As shown, REACHQA and ChartGemma exhibit higher chart richness compared to several other datasets. But the charts in ChartGemma require manual collection from multiple sources (Masry et al., 2024b). 17 Preprint. Work in Progress. User: As MatplotLib expert, you are asked to write new Python plotting script. This script will be used to generate type-specific chart with artificial data. Here are the requirements: 1. There are several script examples from which you can draw inspiration, but try not to repeat patterns already shown in the examples to maximize diversity. 2. Use the Matplotlib library in Python for plotting. You can use auxiliary libraries such as Numpy, but make sure the code works! 3. The type of chart you need to plot is {type}. Therefore, everything you create must be adapted to fit this type of chart. 4. The topic of the chart can be anything you like, for example, {topic1}, {topic2}, etc. 5. Based on the given chart type and the topic you choose, you need to construct suitable backstory, which should be reflected in the title, labels, legend, etc. 6. Based on the backstory, you need to construct contextual data inputs in the form Information contained in the data can be adapted as of Python lists or Numpy arrays. appropriate to fit the type of chart. 7. You must not use random() to construct the data, as it needs to be explicitly created regardless of your chart type and topic. 8. Be as imaginative and creative as possible in drawing the chart, both in terms of data and plotting details. Here are three examples to consider: {demo1} {demo2} {demo3} Now, lets take this task step by step. First, we have to plan out the title and backstory of the chart and create data based on the above. Assistant: {model response} User: Please complete the entire script by plotting chart based on the data generated. Here are some highlighted requirements and notes. Requirements: 1. If you find that the generated data is not appropriate while plotting the chart, modify it further as needed. 2. The information on the chart should be complete enough to be understandable, but avoid including the full backstory or too much text in the figure. 3. Avoid occlusion of visual elements. If necessary, automatically adjust the image layout before plt.show() using tight layout(). 4. If the text in the chart is too long, find way to make it all visible instead of overlapping. If the title is too long, you can break it into multiple lines. 5. Once again, be as imaginative and creative as possible in creating the details of the chart. 6. Above all, double-check to ensure the code works. Reduce unnecessary comments and focus on functionality. Now, generate your final plotting script in single python code block. Figure 7: Prompt template for code generation via Self-Instruct method. 18 Preprint. Work in Progress. User: As MatplotLib expert, you are asked to optimize Python plotting script to make the plotted chart more complex. The script will be used to generate charts for mathematical test, so you should make it little more challenging. This is the code you need to optimize: {code} Heres what Id like you to do to optimize the chart: {direction} Now, lets take this task step by step. First, please read the given code carefully and analyze the chart it draws. Then, think about your optimization ideas with the given directions. In this step, you dont need to give the final code, only show the design ideas. Assistant: {model response} User: Please implement bined with the original code. the final optimized script based on the above design ideas comRemember: 1. Avoid visual elements that obscure each other, e.g., legends, labels. Automatically adjust the image layout before plt.show() using tight layout(). if necessary. 2. If the text in the chart is too long, find way to make all the text show up instead of overlapping. If the title is too long, you can break it into multiple lines. 3. Be as imaginative and creative as possible in creating details of the chart, but dont make the chart redundant just to cope. 4. If you are adding new plot, take care that the chart is complete with all the elements, such as labels, axes, legends, and colors, unless it is intended to be shared with the original chart. 5. If you are adding new plot, carefully construct meaningful data and consider whether to give the new sub-plot sub-title. 6. You must not use random() to construct the data, as it needs to be explicitly constructed regardless of your chart type and topic. 7. Above all, double-check to make sure the code works. Reduce unnecessary comments and focus on functionality. Now, generate your optimized plotting script in single python code block. Figure 8: Prompt template for code generation via Evol-Instruct method. 19 Preprint. Work in Progress. Evolution Direction: Increase the size of the input data or the number of data groups as appropriate so that it requires higher level of mathematical understanding. Note if there is sum requirement. Try changing or adding some visual elements to make visual effect better. The elements you add must make sense and not be redundant. Incorporate an overlay plot of different type on the original chart. Use related but not identical data for the added plot. Extend an additional subplot of different type beside the original chart (2 in total). Use related but not identical data for the added plot. Figure 9: Predefined evolution directions for Evol-Instruct method. User: As Python and Matplotlib expert, you have been asked to fix the following code. The error code is: {code} The code reports the following error message when run: {error} Please analyze the error first, and then provide the revised code within single Python code block. There should only be one Python code block in your response, containing the complete revised code. Figure 10: Prompt template for Self-Repair. 20 Preprint. Work in Progress. User: You are both an expert Matplotlib plotter and professional maths teacher. Now, you are asked to generate mathematical reasoning question about given chart. This chart and question will be used as question on this years college admissions examination. As question writer, you need to ensure that the question is challenging yet fair, testing the students ability to analyze data, interpret trends, and apply mathematical concepts. First, please read the following plotting script in Python, try to visualize the figure in your mind and understand the meaning of the chart. After youve analyzed this chart, well start generating the associated question. Here are some tips for you: 1. The plotting script (including the code itself, data mapping and labels) is absolutely correct, and you can trust it completely. 2. The question needs to be based on the chart type, chart topic, and the given data. It can relate to the chart as whole or to localized details, so you need to look closely. 3. The question should be challenging, requiring visual observation skills and mathematical reasoning skills. So, you need to have deep understanding of the chart. 4. If there is no data annotation in the figure, try not to generate questions that require too much numerical recognition to reduce inconsistent answers due to visual errors. 5. If some numerical recognition is needed, choose distinguishable colors, lines, heights, and other features that make it easy to estimate without data annotation. 6. You dont need to describe what the chart shows in the question text, including values, labels, etc. This can be left to the student to recognize. Here is the plotting script: {code} Now, please generate 4 questions at time, each of which needs to look at different aspect of the chart. Your output needs to follow this JSON format, and no other text included: {question list: [the question you generate]} Figure 11: Prompt template for generating reasoning-oriented questions. Preprint. Work in Progress. User: You are both Matplotlib graphing expert and professional math teacher. Now, you have been asked to generate an answer to given chart and question. This chart and question will be used as question on this years college admissions examination. As the answer writer, you need to ensure that the answer is correct, detailed, and educational. First, please read the following plotting script in Python, try to visualize the figure in your mind and understand the meaning of the chart. After youve analyzed this chart, well start generating the answer. Here is the plotting script: {code} Here are some tips for you to generate the answer: 1. First and foremost, the answer needs to be based on the chart information. 2. In the answer, you will also need to solve the question step-by-step, including reasoning steps and recognition steps (but keep concise). 3. You need to explicitly involve final answer; the type of answer can be certain number, noun, or Yes/No, etc. 4. The answer should contain multiple reasoning or calculation steps and be presented in an understandable and educational paragraph. 5. NEVER include any information relating to the Python script in the answer text, as students will ONLY have access to the plotted figure. Here is the question: {question} Your output needs to follow this JSON format, and no other text should be included: {analysis: your analysis about the scirpt and question, answer: your step-by-step answer} Figure 12: Prompt template for generating reasoning-oriented answers. 22 Preprint. Work in Progress. User: You are both an expert Matplotlib plotter and professional maths teacher. Now, you are asked to generate recognition-oriented question about given chart. This chart and question will be used as question on this years elementary math examination to test students ability to read charts. First, please read the following plotting script in Python, try to visualize the figure in your mind and understand the meaning of the chart. After youve analyzed this chart, well start generating the associated question. Here are some tips for you: 1. The plotting script (including the code itself, data mapping, and labels) is absolutely correct and you can trust it completely. 2. Descriptive questions are questions that can be answered based on basic chart information, such as titles, labels, tick marks, colors, etc. 3. The generated Q&A needs to be based on the chart type and data. It should be answerable through visual observation. 4. If there is no data annotation in the figure, try not to generate questions that require too many numerical recognitions to reduce inconsistent answers due to visual errors. 5. If some numerical recognition is needed, choose distinguishable colors, lines, heights, and other features that make it easy to estimate without data annotation. 6. You dont need to describe the content of the figure in the question text. This can be left for students to think about. 7. This question needs to explicitly involve final answer; the type of answer can be certain number, noun, or Yes/No, etc. 8. NEVER include any information relating to the Python script in the question or answer, as students will ONLY have access to the plotted figure. Here are some examples of recognition-oriented questions: - How many colors are used in the chart? How many city categories are in the chart? - Whats the leftmost value of the bar in China? And what is the value of the bar next to it? - For the subplot at row 2 and column 1, what is the minimum value of the solid line? - Which name does the second-largest sector represent? What is its value? - Does the blue triangle in the chart represent higher value than the red circle? Here is the plotting script: {code} Now, please generate 4 questions at time, each of which needs to look at different aspect of the chart. Your output needs to follow this JSON format, and no other text included: {question list: [the question you generate]} Figure 13: Prompt template for generating recognition-oriented questions. 23 Preprint. Work in Progress. User: You are both Matplotlib graphing expert and professional math teacher. Now, you have been asked to generate an answer to given chart and question. This chart and question will be used as question on this years elementary math examination to test students ability to read charts. As the answer writer, you need to ensure that the answer is correct, detailed, and educational. First, please read the following plotting script in Python, try to visualize the figure in your mind and understand the meaning of the chart. After youve analyzed this chart, well start generating the answer. Here is the plotting script: {code} Here are some tips for you to generate the answer: 1. First and foremost, the answer needs to be based on the chart information. 2. In the answer, you will also need to solve the question step-by-step, including reasoning steps and recognition steps (but keep concise). 3. You need to explicitly involve final answer; the type of answer can be certain number, noun, or Yes/No, etc. 4. The answer should contain multiple reasoning or calculation steps and be presented in an understandable and educational paragraph. 5. NEVER include any information relating to the Python script in the answer text, as students will ONLY have access to the plotted figure. Here is the question: {question} Your output needs to follow this JSON format, and no other text should be included: {analysis: your analysis about the scirpt and question, answer: your step-by-step answer} Figure 14: Prompt template for generating recognition-oriented answers. Preprint. Work in Progress. User: <image> You are strict MatplotLib plotter and have been asked to evaluate the given chart. Rate the chart from 1 to 5 based on these criteria: 1 point: This chart is the poorest in quality and fails to accurately represent any relevant data. It is characterized by complete breakdown in visual representation; elements are cluttered, text heavily overlaps, legend is missing, or large areas are left blank, making the chart unreadable. The design shows no understanding of effective data visualization practices. 2 points: The chart displays incorrect or irrelevant visual elements, with significant inaccuracies that misrepresent the data. The layout suffers from clutter, substantial overlapping of text and other visual elements, such as the legend or labels, and poorly designed axes that result in uneven distribution, severely impeding accurate interpretation. 3 points: This chart represents some correct data points but makes basic errors in visual representation. It may use misleading scales, inappropriate chart types, omit key data. Visual clutter and overlapping elements, such as text obscuring parts of the chart or sub-diagrams overlapping each other, detract from the charts clarity and readability. 4 points: The chart accurately represents most of the major data points and important details of the dataset. Minor visual errors exist, such as slight occlusions of text or sub-optimal positioning of elements like legends or labels, but these do not significantly affect the overall accuracy or readability. The chart demonstrates good understanding of effective visualization techniques but could still be improved in terms of visual layout and the balance of details. 5 points: This is an exemplary chart that perfectly encapsulates all critical data points and relationships with outstanding visual clarity and no occlusions. It demonstrates thorough understanding of data visualization techniques, making excellent use of space and visual elements. The chart is informative, clear, engaging, and free from any visual errors. Score the chart on this scale, providing short analysis and single value. Your response should be in the format: Analysis: (your analysis) Rating: (int) Figure 15: Prompt template for rating the chart quality. 25 Preprint. Work in Progress. User: <image> You are visual question answering (VQA) data annotator. Your task is to review the following chart and question, and determine if the answer is correct based on the information in the chart. You should carefully analyze the chart, taking into account all relevant data points, labels, and trends. Then, conduct an in-depth analysis to determine if there are any unreasonable or incorrect aspects in the figure, question, or answer. Specifically, consider the following points: 1. Are the provided question and answer relevant to the chart? Can the answer be found in the chart? 2. Do the colors in the charts and questions correspond correctly? Are there instances where the colors are incorrectly referred to? 3. Do the data in the charts and questions correspond correctly? Are there any errors in the data or misalignment of information? 4. Is the provided answer correct? Are there any logical errors or unreasonable points? 5. Apart from the points listed above, is there anything else in this question and answer that doesnt make sense? Here is the question and answer about the given chart: Question: {question} Answer: {answer} You are asked to provide short analysis and decide whether to keep the example. Your response should be in the format: Analysis: (your analysis) Decision: (yes/no) Figure 16: Prompt template for rating Q&A quality. User: Compare the ground truth with the prediction from AI model and determine if the prediction is correct. The question is about an image, which we have not given here. You need to determine whether the models prediction is consistent with the ground truth. No points will be awarded for wrong answers, over answers or under answers. The reasoning process in the prediction does not need to be considered too much, you only need to determine if the final answer is consistent. There are times when the answer may have different form of expression and some variation is acceptable. ## Question: {question} ## Ground Truth: {answer} ## Prediction: {prediction} Now, lets analyze it and then provide your judgment. Your response must follow the format below: Analysis: (analyze the correctness briefly) Correctness: (Yes or No) Figure 17: Prompt template for evaluating the model prediction with LLMs."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Weixin Group, Tencent"
    ]
}