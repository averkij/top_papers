{
    "paper_title": "UnicEdit-10M: A Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits",
    "authors": [
        "Keming Ye",
        "Zhipeng Huang",
        "Canmiao Fu",
        "Qingyang Liu",
        "Jiani Cai",
        "Zheqi Lv",
        "Chen Li",
        "Jing Lyu",
        "Zhou Zhao",
        "Shengyu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing behaviors. Existing data construction methods face a scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce a lightweight data pipeline that replaces multi-toolchains with an end-to-end model and a unified post-verification stage. For scalable quality control, we train a 7B dual-task expert model, \\textbf{Qwen-Verify}, for efficient failure detection and instruction recaptioning. This pipeline yields \\textbf{UnicEdit-10M}, a 10M-scale dataset spanning diverse basic and complex editing tasks. We also propose \\textbf{UnicBench}, a general benchmark that extends beyond basic edits to explicitly assess spatial and knowledge-driven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including \\textit{Non-edit Consistency} and \\textit{Reasoning Accuracy}. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 0 9 7 2 0 . 2 1 5 2 : r UnicEdit-10M: Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits Keming Ye1* Zhipeng Huang2 Canmiao Fu2 Qingyang Liu3* Jiani Cai4 Zheqi Lv1 Chen Li2 Jing LYU2 Zhou Zhao1 Shengyu Zhang1 1Zhejiang University 3Shanghai Jiao Tong University 2WeChat Vision, Tencent Inc. 4Xinjiang University (cid:128) Project Page GitHub Code"
        },
        {
            "title": "Benchmark",
            "content": "Figure 1. UnicEdit-10M covers 22 edit tasks spanning basic and complex edits, with unified post-verification stage that filters failures and refines instructions to yield high-quality triplets. We also introduce UnicBench with fine-grained metrics for comprehensive evaluation."
        },
        {
            "title": "Abstract",
            "content": "With the rapid advances of powerful multimodal models such as GPT-4o, Nano Banana, and Seedream 4.0 in Image Editing, the performance gap between closed-source and open-source models is widening, primarily due to the scarcity of large-scale, high-quality training data and comprehensive benchmarks capable of diagnosing model weak- *Work done during an internship at WeChat Vision, Tencent Inc. Corresponding author. nesses across diverse editing behaviors. Existing data construction methods face scale-quality trade-off: human annotations are high-quality but not scalable, while automated pipelines suffer from error propagation and noise. To address this, we introduce lightweight data pipeline that replaces multi-toolchains with an end-to-end model and unified post-verification stage. For scalable quality control, we train 7B dual-task expert model, Qwen-Verify, for efficient failure detection and instruction recaptioning. This pipeline yields UnicEdit-10M, 10M-scale dataset Figure 2. Representative examples of all sub-tasks from UnicEdit-10M. spanning diverse basic and complex editing tasks. We also propose UnicBench, general benchmark that extends beyond basic edits to explicitly assess spatial and knowledgedriven reasoning. To enable fine-grained diagnosis, we introduce novel metrics, including Non-edit Consistency and Reasoning Accuracy. Our analysis of mainstream models on UnicBench reveals their limitations and provides clear directions for future research. 1. Introduction Instruction-based image editing [15, 33] has progressed rapidly, enabling complex visual edits from naturallanguage instructions. Closed-source models such as GPT4o [17], Nano Banana [7], and Seedream 4.0 [31] exemplify this progress, showcasing capabilities in understanding nuanced commands and producing semantically consistent edits. Yet the gap between closedand open-source models continues to widen, primarily due to the lack of two critical components: (1) large-scale, high-quality public datasets for training, and (2) comprehensive benchmarks capable of diagnosing model weaknesses across diverse editing tasks. However, existing open-source datasets and benchmarks face significant limitations. Current datasets present scalequality trade-off: human-annotated collections [48] offer high fidelity but are small, while large-scale automated pipelines introduce systematic noise like mismatched instructions and failed edits [12, 46]. On the benchmark side, prevailing evaluations emphasize basic edits [2, 25, 26] and offer sparse coverage of complex reasoning tasks, limiting their ability to diagnose higher-order model weaknesses. We attribute these limitations to three technical causes: (1) Error propagation from complex toolchains. Automated pipelines that rely on complex toolchains [23, 46] are prone to error propagation, where minor inaccuracies in early stages amplify into significant artifacts (2) Insufficient or narrowly scoped postdownstream. verification. While some automated methods incorporate post-verification, these steps are often one-dimensional and prohibitively expensive. For instance, some approaches [20] perform simple failure detection without correcting semantic misalignments, while others [38] focus solely on instruction refinement using the GPT-4o [17] API at high cost, overlooking image quality. (3) Evaluation blind spots for complex edits. Existing benchmarks often emphasize objectand attribute-level changes, lacking systematic tests for complex reasoning and spatial understanding [23, 46]. Moreover, current VLM-based metrics may ignore unintended changes in non-edited regions and tend to be sensitive to stylistic variations when judging naturalness, preventing comprehensive assessment of higherorder editing skills. To overcome these limitations, we introduce scalable pipeline that curates 10M Universal Dataset for comprehensive image editing (UnicEdit-10M), spanning basic attribute/object edits, spatial viewpoint transformaTable 1. Comparison of different Image Editing datasets."
        },
        {
            "title": "Dataset",
            "content": "#Size #Subtasks"
        },
        {
            "title": "Complex Edit",
            "content": "MagicBrush [48] InstructPix2Pix [4] HQ-Edit [16] SEED-Data-Edit [12] UltraEdit [50] ImgEdit [46] NHR-Edit [20] GPT-Image-Edit-1.5M [38] UnicEdit-10M 10K 313K 197K 3.7M 4M 1.2M 358K 1.5M 10M 5 4 6 6 9 13 - - tions, and reasoning-driven tasks. We further propose general benchmark that systematically evaluates both basic and complex editing capabilities. Our pipeline consists of three stages: First, we leverage open-source VLMs to generate diverse editing instructions. Second, to circumvent error propagation, we employ an end-to-end editing model for instruction-based image editing, instead of multitool invocation. Third, all synthesized original, instruction, edited triplets pass through unified post-verification stage, which not only filters out faulty cases but also refines the corresponding instructions to enhance semantic alignment with the visual edits. To make verification practical at scale, we train compact 7B dual-task expert model Qwen-Verify that jointly (1) performs fine-grained failure detection and (2) generates instruction recaptions, providing efficient, post-hoc validation and low-cost correction without relying on expensive, large API calls. To fix evaluation blind spots, we construct UnicBench, benchmark that extends beyond conventional basic edits to include spatial viewpoint changes, multi-object coordination, and knowledge-driven reasoning, collectively referred to as complex edits. For precise evaluation, we augment the VIEScore [19] with Non-edit Consistency metric that detects unexpected changes in unmodified regions, and propose Reasoning Accuracy, which evaluates whether the semantic or causal outcomes implied by an instruction are faithfully realized in the edited image. In summary, our contributions are threefold: We propose scalable, quality-aware data curation pipeline that yields UnicEdit-10M, 10M dataset, extending beyond basic edits to cover complex spatial, viewpoint transformation and reasoning edits, achieving the SOTA aesthetic quality across other datasets. We introduce Qwen-Verify, 7B dual-task expert model that performs failure detection and instruction recaptioning. It achieves this with high efficiency, outperforming Qwen2.5-VL-72B at fraction of the computational and economic cost. We construct UnicBench, comprehensive benchmark with novel metrics to assess complex reasoning and spatial capabilities beyond simple edits. Using UnicBench, we analyze mainstream models, providing detailed diagnosis of current limitations and clear roadmap for future research. 2. Related Works Instruction-based Image Editing. Instruction-based image editing has progressed rapidly with diffusion models [11, 15, 33, 44]. Early methods [21, 36] often employed modular pipelines for editing, with InstructPix2Pix [4] pioneering the use of synthetic data for training. Recent works like OmniGen2 [42] and BAGEL [8] focused on unified generative and understanding abilities, while Step1XEdit [23] enhanced its editing capabilities by training on high-quality, self-curated dataset. Current open-source SOTA models, such as FLUX.1-Kontext [3] and QwenImage-Edit [41], deliver high-fidelity edits through novel architectures and robust multimodal foundations. Concurrently, closed-source models like GPT-4o [17], Nano Banana [7], and Seedream 4.0 [31] define the performance ceiling with exceptional zero-shot reasoning. Image Edit Datasets. High-quality datasets are crucial for training instruction-based editing models, with summary in Tab. 1. Human-annotated datasets like MagicBrush [48] offer high fidelity but limited scale. Consequently, automated pipelines are prevalent, falling into two main categories. Multi-tool pipelines, used by UltraEdit [50], ImgEdit [46], and Step1X-Edit [23], combine various vision modules [6, 29] but are prone to error propagation; SEED-Data-Edit [12] mitigates this by adding human annotations. End-to-end approaches avoid error accumulation. For instance, InstructPix2Pix [4] uses Stable Diffusion [30] with P2P [13], and HQ-Edit [16] uses LLM [5] and DALLE 3 to generate input and output images in diptych form. However, these methods can suffer from distribution shift due to their reliance on synthetic images, are costly, and often lack explicit quality verification. Recent works like NHR [20] and GPT-Image-Edit-1.5M [38] inFigure 3. Data curation pipeline with three stages: (1) data preparation, (2) image editing, (3) post verification performing failed edits filtration and recaption. corporate post-filtering, but their verification is often onedimensional and costly. In contrast, our pipeline uses opensource models with unified post-verification process to ensure data quality in scalable and cost-effective manner. Image Edit Benchmarks. growing set of benchmarks targets different facets of instruction-based image editing [14, 18]. Traditional benchmarks such as EditBench [37] focus on specific tasks like inpainting, while EmuEdit [32] and AnyEdit [47] rely on metrics like L1 and CLIP [27] score, which may misalign with human perception of edit faithfulness. Recent works like ImgEditBench [46] and GEdit-Bench [23] span diverse tasks and leverage VLMs for assessment, yet are constrained by few complex cases or metrics that are insensitive to unintended changes in non-edited regions. KRIS-Bench [43] specifically probes complex reasoning but omits basic edits, limiting its role as general benchmark. Our benchmark addresses these gaps with comprehensive coverage of both basic and complex edits and introduces fine-grained metrics for unintended-change control and reasoning fidelity, enabling more diagnostic evaluation. 3. UnicEdit-10M Dataset In this section, we propose scalable data generation pipeline to curate UnicEdit-10M, large-scale, high-quality image editing dataset. 3.1. Dataset Preparation Editing Taxonomy. As shown in Fig. 1, to ensure comprehensive task coverage, we established fine-grained taxonomy that classifies edits into four high-level categories: Object Editing, Attribute Editing, Scene Editing, and Reasoning Editing, totaling 22 sub-tasks. We define tasks requiring spatial awareness and factual knowledge as Complex Edits, with all others classified as Basic Edits. This two-level classification ensures comprehensive task coverage. Further details are in the supplementary material (Sec. A). Data Source. Our pipeline begins with large-scale internal library of diverse real-world and synthetic images, all pre-filtered for high aesthetic scores to ensure quality. Model Selection. For instruction generation, we selected Qwen2.5-VL-72B [1] over its 7B counterpart due to its superior visual comprehension and ability to generate accurate and diverse instructions. For the image editing module, we employed FLUX.1-Kontext [3] and Qwen-Image-Edit [41], two leading open-source models renowned for their proficiency across broad spectrum of editing tasks. 3.2. Dataset Curation instruction generaOur pipeline is three-stage process: tion, image editing, and crucial post-verification step, as shown in Fig. 3. Instruction Generation. We automate instruction generation by prompting Qwen2.5-VL-72B [1] with source image and predefined editing taxonomy. The model generates 3-7 distinct, content-aware instructions per image, each corresponding to unique sub-task, ensuring balanced task distribution without manual annotation. Image Editing. Each original image, instruction pair is then processed by editing models to synthesize an edited image, forming an initial triplet. To handle varying input resolutions, all source images are preprocessed via centercropping and resizing, with quality check to discard images requiring more than 20% cropping to prevent significant content loss. However, this automated process is not infallible. Failures arise from ambiguous instructions (e.g., remove the bag from the person in multi-person scene) or inherent limitations in the editing models semantic grounding. This can result in unchanged images or unexpected edit, necessitating rigorous post-verification stage. Post-Verification and Refinement. The raw generated triplets contain noisy data that can impair model training. We identify three primary failure modes: Figure 4. Post-verification examples of the expert model. Base denotes Qwen2.5-VL-7B; SFT denotes Base model after Stage-1 SFT; Ours denotes the dual-task expert model Qwen-Verify. Edit Failure: The edited image is identical or nearly identical to the source. Instruction-Image Misalignment: The model performs unintended edits or fails to follow the instruction accurately. Others: Less frequent but critical failures, such as degraded quality or anatomical errors, detailed in the supplementary material (Sec. E). To overcome these limitations, we introduce an expert verification framework that integrates failure filtration and instruction recaptioning into unified process driven by Chain-of-Thought [40] reasoning. We employ Qwen2.5VL-72B [1] as our expert verifier, guided by multi-step prompt. The process unfolds as follows: First, the model captions the original and edited images to expose finegrained changes. Second, it decides whether valid edit occurred from their visual differences. Then, for valid edits, the model rewrites the instruction to precisely match the visual modification. Finally, it outputs structured JSON object containing: (1) boolean flag, is changed, indicating valid edit, and (2) the refined instruction. However, this approach is computationally expensive, and the recaptioned instructions are prone to hallucination. To address these issues, we developed specialized expert model for this task, which we detail in Sec. 4. This expert model is ultimately deployed in our pipeline for large-scale data curation. 3.3. Dataset Statistics Our pipeline produces high-quality dataset comprising approximately 10M triplets, organized into 22 fine-grained editing categories. The dataset is structurally divided into four major editing types: Scene Editing (3.063M samples), Attribute Editing (3.529M), Object Editing (3.242M), and Reasoning Editing (1.746M). In terms of visual quality, the dataset predominantly consists of high-resolution images, with 10241024 resolution accounting for 50% of the total collection. More details can be found in the the supplementary material (Sec. B.3). 4. Qwen-Verify While leveraging large model like Qwen2.5-VL-72B [1] for post-verification is effective, it is computationally expensive and still prone to hallucination. To address these issues, we introduce Qwen-Verify, 7B expert model trained to perform our dual-task verification with high efficiency. 4.1. Data Our training data targets dominant failure modes and is partitioned into three categories. All samples are meticulously screened and corrected by human annotators to ensure high fidelity. Normal: High-quality triplets with salient, instructionaligned edits. No Edit: Triplets with no discernible change between the original and edited images. Hallucination: Cases where the target is correct but the action or attribute is misstated. The training data is assembled in stages: First, we prepare 200k samples for SFT, consisting of Normal and No Edit data. Normal data instructions are reviewed and refined by human experts. No Edit aggregates (1) triplets rejected during post-verification and (2) no-op triplets that slipped through automated screening but were later flagged by annotators. Next, We use all three categories to construct 20k DPO samples. Normal and No Edit follow the same construction as SFT. For the Hallucination set, we select postverified triplets with recaptioned instructions and then corTable 2. Overall dataset quality comparison between UnicEdit-10M and other datasets. VIEScore reports Semantic Consistency (SC) and Perceptual Quality (PQ) with their Overall score. Aesthetics columns give the aesthetic scores for the Source (original) and Target (edited) images. Best results are in bold, and second best are underlined. Datasets SEED-Data-Edit [12] ImgEdit [46] X2Edit [24] NHR-Edit [20] GPT-Image-Edit-1.5M [38] Nano-consistent-150k [45] Ours SC 5.7884 6.3233 7.3527 8.3180 8.6780 7.9180 8.4500 VIEScore Aesthetics PQ 6.3430 7.8819 7.2776 7.9420 7.1560 8.0000 8.1950 Overall Source Target 5.0043 6.2462 6.8693 7.7796 7.7451 7. 8.0768 5.72 6.49 7.52 7.35 6.23 6.81 8.00 5.74 7.03 7.54 7.42 7.59 7.40 7.76 rect them to create high-quality preference pairs. This correction process involves using GPT-4o [17] to generate candidate corrections, which are then rigorously reviewed and finalized by human experts before DPO training. 4.2. D2PO We developed Qwen-Verify by training the Qwen2.5-VL7B [1] in two-stage training strategy: supervised finetuning (SFT) for foundational capabilities, followed by preference alignment stage [28] to refine the models understanding of nuanced instruction quality. We first perform SFT on the base model using mix of Normal and No Edit data. This stage equips the model with the core abilities to distinguish failed edits and generate accurate instructions for successful ones. While SFT provides strong starting point, it is agnostic to the subtle yet critical semantic differences between adequate and excellent instructions. To address this, in the second stage, we introduce Differential Direct Preference Optimization (D2PO), novel framework that adapts preference optimization for the challenge of interpreting visual changes. Unlike conventional methods that treat visual inputs as static context, D2PO re-frames the learning problem by conditioning the policy on dynamically computed visual differential context. We define visual encoder, V, that processes the image pair to extract latent representation of the edit: cv = V(Io, Ie) (1) This vector cv encapsulates the transformation from Io to Ie and conditions optimization. We learn from curated preference set = {(Io, Ie, pw, pl)}i, where (pw, pl) are preferred (corrected) and rejected (hallucination) instructions. Assuming latent reward r(p, cv), the Bradley-Terry model gives (pw plcv) = σ(r(pw, cv) r(pl, cv)). D2PO avoids explicit reward modeling by re-parameterizing rewards via policy probabilities. We define the Policy Advantage Function: Aπθ,πref(p, cv) = β log πθ(pcv) πref(pcv) (2) where πθ is the trainable policy, πref is frozen SFT copy, and β controls deviation. The objective maximizes the advantage margin: LD2PO = E(cv,pw,pl)D [log σ (Aπθ,πref (pw, cv) Aπθ,πref(pl, cv))] (3) Optimizing this loss aligns the model with human judgments of precise and faithful edit descriptions. 5. UnicBench To enable more comprehensive assessment of model capabilities, we introduce UnicBench, unified benchmark spanning both basic and complex edits. It is accompanied by fine-grained metrics designed to evaluate model performance across these different dimensions. 5.1. Benchmark Construction UnicBench is built upon diverse set of high-quality realworld and synthetic images. We generate instructions using hybrid VLM-human workflow: Qwen2.5-VL [1] first produces set of candidate instructions based on image content, and then we employ human experts to review the resulting image, instruction pairs. This step removes ambiguous or semantically inconsistent prompts, with subsequent rewriting applied to align instructions with specific editing task categories. The benchmark follows the same taxonomy as the training data, with 50 test cases per category. As detailed in the supplementary material (Sec. H), comparative analysis confirms that UnicBench offers more comprehensive task coverage than existing benchmarks, enabling more precise and multidimensional assessment of model capabilities. 5.2. Evaluation Metric While the VIEScore [19] provides foundational assessment, it suffers from key limitations. Its Semantic Consistency (SC) metric is insensitive to unintended alterations in non-edited regions, and its Perceptual Quality (PQ) metric prioritizes naturalness, potentially underestimating stylized outputs. Crucially, neither metric effectively handles reasoning-based nor geometrically complex edits. To address these gaps, we propose new evaluation framework composed of four specialized metrics: Instruction Following (IF): Measures how well the edited image satisfies the instruction via VLM-based crossmodal alignment score. Non-edit Consistency (NC): Assesses preservation of non-target regions, penalizing unintended changes outside the specified edit area. Visual Quality (VQ): Instruction-conditioned assessment of naturalness, coherence, and adherence to the intended visual style. Reasoning Accuracy (RA): Targets knowledge-intensive edits. VLM first derives an intended-outcome specification from the instruction and original image. To ground this inference, each sample provides list of reasoningpoints (targets, operations, expected visual changes), which guides the verifiers attention. The edited image is then checked against this specification. Each metric is scaled from 0 to 10, and score of zero in any category indicates failed edit. The overall edit score is computed as the geometric mean of all applicable metrics for given task. This is formalized as: Score = (cid:33)1/M (cid:32) (cid:89) mM (4) where is the set of relevant metrics for the task category: = {IF, C, Q} for Basic Edits, and = {IF, C, Q, RA} for Complex Edits. This formulation ensures that severe failures in any dimension are reflected in the final score, providing more reliable and interpretable measure of editing performance. 6. Experiments 6.1. Evaluation Models & Settings For Dataset Quality, we employ the VIEScore [19] to compute both the Semantic Consistency (SC) and Perceptual Quality (PQ) for mainstream datasets, and employ Aesthetic score for image aesthetics of source and edited images. For Expert Model, we use Qwen2.5-VL-7B and Qwen2.5-VL72B [1] as baselines. For Benchmark Evaluation, we test some representative models on UnicBench. These include the closed-source Nano Banana [7], SeedEdit 3.0 [35], Seedream 4.0 [31], and GPT-4o [17], alongside open-source models such as Instruct-Pix2Pix [4], MagicBrush [48], UniWorld-V1 [22], OmniGen2 [42], BAGEL [8], NextStep1 [34], Step1X-Edit-v1.1 [23], FLUX.1-Kontext [3], and Qwen-Image-Edit [41]. Models that support Chinese are evaluated with both English and Chinese prompts. All assessments are conducted by gpt-4.1-2025-04-14 for consistent scoring. Table 3. Data volume statistics at each stage of our pipeline. Ratio indicates the percentage change from the previous stage, and Gen. abbreviates Generation. FLUX and Qwen refer to FLUX.1Kontext [3] and Qwen-Image-Edit [41], respectively. Processing Stage Method Ratio(%) Data Volume Initial Images Instruction Gen. Editing Gen. Failed Edit Filter Recaption - Internal Images Qwen2.5-VL-72B +447.26 FLUX/Qwen Qwen-Verify Qwen-Verify -30.03 -25.97 - Final Data - - 5001199 22368563 15651530 11586583 11586583 6.2. UnicEdit-10M Dataset Evaluation Quantitative. The data volume at each pipeline stage is detailed in Tab. 3. For each source image, we generate multiple candidate prompts and select 35 to produce edited images. Notably, post-verification with Qwen-Verify filters 26% failed edits, ensuring the final set contains valid image pairs. For quantitative comparison (Tab. 2), we adopt the X2Edit [24] protocol, employing GPT-4o [17] to assesse 1K randomly sampled triplets, averaged over three runs. UnicEdit-10M attains the highest PQ and overall score, and surpasses all competitors by large margin in Aesthetic score, validating our post-verification process. While both our dataset and GPT-Image-Edit-1.5M [38] achieve high SC due to the instruction recaptioning step in both pipelines, targeted analysis of face consistency reveals key difference: UnicEdit achieves robust consistency score of 0.89, significantly outperforming GPT-Image-Edit-1.5Ms 0.3025. This highlights our methods superior ability to maintain critical subject details, crucial factor for training reliable editing models. Qualitative. Qualitative samples of UnicEdit-10M are shown in Fig. 2. We also conduct targeted case studies to visually demonstrate our pipelines effectiveness, as shown in the supplementary material (Secs. B.1 and F). We process samples from ImgEdit and SEED-Data-Edit, which initially suffer from low edit quality and instruction misalignment. Our pipeline significantly enhances these triplets, correcting errors and improving semantic alignment. Furthermore, comparison of facial consistency reveals that UnicEdit10M is substantially better at preserving key facial features than GPT-Image-Edit-1.5M, validating our methods ability to produce high-fidelity training data. 6.3. Expert Model Performance Qwen-Verify vs. Qwen2.5-VL-72B. To evaluate QwenVerify, we curated test set spanning the three defined tasks and measured performance via Alignment Accuracy (Acc.), calculated by GPT-4.1. More details are in the supplementary material (Sec. C). As shown in Tab. 5, our model significantly outperforms all baselines, including the strong Table 4. Overall performance of different model on UnicBench. The performance of open-source and closed-source models is separately marked with the best performance in bold, and the second best underlined. Model Overall-EN Overall-CN IF NC VQ RA Overall IF NC VQ RA Overall Instruct-Pix2Pix [4] MagicBrush [48] OmniGen2 [42] UniWorld-v1 [22] FLUX.1-Kontext [3] NextStep-1 [34] BAGEL [8] Step1X-Edit-v1.1 [23] Qwen-Image-Edit [41] Nano Banana [7] SeedEdit 3.0 [35] Seedream 4.0 [31] GPT-Image-1 [17] 2.8526 2.3403 6.2455 5.3055 6.7755 7.2311 7.2491 6.9945 8.2055 7.9753 8.2717 8.3764 9.1551 4.0983 3.3849 7.4973 7.3091 8.4718 6.1802 8.1982 8.2045 8. 8.9808 8.4251 8.7200 7.8449 Open-source Models 2.9221 2.3407 6.1246 5.6013 6.8045 6.4076 6.9794 6.9202 7.7273 1.9560 1.7240 5.1240 4.0160 5.5040 6.2920 5.2600 5.0400 6.4480 Closed-source Models 7.8792 7.8671 8.0428 8.3546 6.8680 6.9393 7.5960 8.3392 3.9672 3.4559 6.4891 6.4827 7.3600 6.5014 7.1391 7.3382 8. 8.1954 7.8392 8.0736 8.6830 - - - - - - 7.3018 7.0282 8.3718 8.1550 8.3721 8.3418 9.2759 - - - - - - 8.2845 8.4118 7.8000 9.0438 8.4502 8.6600 7.8906 - - - - - - 7.3118 7.5600 8. 8.3291 7.9795 8.1364 8.6980 - - - - - - 5.2840 5.0560 6.6560 6.8960 6.8395 7.1240 8.2247 - - - - - - 7.1056 7.0620 7.7790 8.0358 7.9753 8.0474 8.4506 Table 5. Performance of post-verification expert model. Model Normal Acc. No Edit Acc. Hallucination Acc. Qwen2.5-VL-7B Qwen2.5-VL-72B Qwen2.5-VL-7B+SFT Qwen-Verify 4.39 5.25 5.62 6.32 4.84 9.60 9.40 9.80 3.95 6.12 5.47 6. Qwen2.5-VL-72B, across all test categories. Qualitative cases  (Fig. 4)  further show that Qwen-Verify captures finegrained visual changes and produces instructions precisely aligned with the edits. This strength stems from its dualtask design, which jointly optimizes failure detection and instruction refinement, enabling it to grasp subtle semantic nuances. Qwen-Verify vs. SSIM. We also compare Qwen-Verify against the traditional SSIM method for filtering No edit pairs, as discussed in the supplementary material (Sec. B.2). Qwen-Verify achieves significantly higher accuracy. Case studies reveal that SSIM is insensitive to semantically meaningful but visually subtle changes, while being overly sensitive to the minor, imperceptible pixel shifts inherent in the generation process. This confirms that traditional vision-based methods are ill-suited for our semanticallydriven post-verification task. 6.4. UnicBench Analysis Model Performance on UnicBench. We present comprehensive evaluation of mainstream models on UnicBench in Tab. 4. The results confirm that closed-source models exhibit more powerful and comprehensive capabilities, consistently outperforming open-source models. GPT-Image1 [17] achieves the highest overall score on both EN and CN tasks, demonstrating SOTA general editing ability. Seedream 4.0 [31] follows as the second-best model, even surpassing GPT-Image-1 [17] on the NC metric. Among open-source models, Qwen-Image-Edit [41] leads in performance, beginning to close the gap. key insight from the metric breakdown is that while most models effectively follow basic instructions, nearly all exhibit significant performance drop on the RA metric. This reveals common limitation in executing edits that require complex logical inference or world knowledge, suggesting clear direction for future research. This underscores the importance of our work, which provides high-quality dataset to train more advanced models, while offers scalable method for generating specific task data. Metric Comparison with VIEScore. We compare our evaluation metrics against VIEScore, with detailed cases in the supplementary material (Sec. D). Our analysis reveals that while VIEScore can accurately assess whether an instruction was executed, it is largely insensitive to unintended changes in non-edited regions. For instance, in cases involving the unwanted removal of person or accidental modification of text, VIEScore still assigned high SC score. In contrast, our metrics decouple this assessment into Instruction Following (IF) and Non-edit Consistency (NC). In the same failure cases, our NC dimension correctly identified the unintended changes and penalized the score accordingly. This comparison demonstrates that our metrics provide more fine-grained and diagnostic evaluation, enabling precise localization of specific model deficiencies. 7. Conclusion We introduce scalable pipeline featuring the Qwen-Verify expert model for data curation, yielding UnicEdit-10M, 10M dataset covering 22 editing tasks. We also presented UnicBench, comprehensive benchmark with novel metrics for fine-grained diagnosis. Our analysis on UnicBench pinpoints complex reasoning and spatial understanding as the critical bottleneck for current models. Our scalable pipeline provides direct path forward, enabling the targeted, automated generation of large-scale training data for these specific tasks. By releasing the dataset, benchmark, and reusable pipeline, we aim to provide the community with essential resources to accelerate progress and close the capability gap with closed-source models."
        },
        {
            "title": "References",
            "content": "[1] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 4, 5, 6, 7, 8 [2] Samyadeep Basu, Mehrdad Saberi, Shweta Bhardwaj, Atoosa Malemir Chegini, Daniela Massiceti, Maziar Sanjabi, Shell Xu Hu, and Soheil Feizi. Editval: Benchmarking diffusion based text-guided image editing methods. arXiv preprint arXiv:2310.02426, 2023. 2 [3] Stephen Batifol, Andreas Blattmann, Frederic Boesel, Saksham Consul, Cyril Diagne, Tim Dockhorn, Jack English, Zion English, Patrick Esser, Sumith Kulal, et al. Flux. 1 kontext: Flow matching for in-context image generation and editing in latent space. arXiv e-prints, pages arXiv2506, 2025. 3, 4, 7, 8, 6 [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1839218402, 2023. 3, 7, 8, 6 [5] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. 3 [6] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, Xinggang Wang, and Ying Shan. Yolo-world: Real-time In Proceedings of the open-vocabulary object detection. IEEE/CVF conference on computer vision and pattern recognition, pages 1690116911, 2024. 3 [7] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 2, 3, 7, 8, pretraining. arXiv preprint arXiv:2505.14683, 2025. 3, 7, 8, 6 [9] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep In Proceedings of the IEEE/CVF conface recognition. ference on computer vision and pattern recognition, pages 46904699, 2019. 6 [10] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single-shot multiIn Proceedings of level face localisation in the wild. the IEEE/CVF conference on computer vision and pattern recognition, pages 52035212, 2020. 6 [11] Kunyu Feng, Yue Ma, Bingyuan Wang, Chenyang Qi, Haozhe Chen, Qifeng Chen, and Zeyu Wang. Dit4edit: DifIn Proceedings of fusion transformer for image editing. the AAAI Conference on Artificial Intelligence, pages 2969 2977, 2025. 3 [12] Yuying Ge, Sijie Zhao, Chen Li, Yixiao Ge, and Ying Shan. Seed-data-edit technical report: hybrid dataset for instructional image editing. arXiv preprint arXiv:2405.04007, 2024. 2, 3, 6, 1 [13] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. [14] Yuzhou Huang, Liangbin Xie, Xintao Wang, Ziyang Yuan, Xiaodong Cun, Yixiao Ge, Jiantao Zhou, Chao Dong, Rui Huang, Ruimao Zhang, et al. Smartedit: Exploring complex instruction-based image editing with multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8362 8371, 2024. 4 [15] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Liangliang Cao, and Shifeng Chen. Diffusion model-based image editing: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2025. 2, 3 [16] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. 3, 4 [17] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2, 3, 6, 7, 8, 9 [18] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 60076017, 2023. 4 [19] Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. Viescore: Towards explainable metrics for arXiv preprint conditional arXiv:2312.14867, 2023. 3, 6, 7 image synthesis evaluation. [8] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal [20] Maksim Kuprashevich, Grigorii Alekseenko, Irina Tolstykh, Georgii Fedorov, Bulat Suleimanov, Vladimir Dokholyan, and Aleksandr Gordeev. Nohumansrequired: Autonomous high-quality image editing triplet mining. arXiv preprint arXiv:2507.14119, 2025. 2, 3, 6 [21] Yaowei Li, Yuxuan Bian, Xuan Ju, Zhaoyang Zhang, Junhao Zhuang, Ying Shan, Yuexian Zou, and Qiang Xu. Brushedit: arXiv preprint All-in-one image inpainting and editing. arXiv:2412.10316, 2024. 3 [22] Bin Lin, Zongjian Li, Xinhua Cheng, Yuwei Niu, Yang Ye, Xianyi He, Shenghai Yuan, Wangbo Yu, Shaodong Wang, Yunyang Ge, et al. Uniworld: High-resolution semantic encoders for unified visual understanding and generation. arXiv preprint arXiv:2506.03147, 2025. 7, 8, 6 [23] Shiyu Liu, Yucheng Han, Peng Xing, Fukun Yin, Rui Wang, Wei Cheng, Jiaqi Liao, Yingming Wang, Honghao Fu, Chunrui Han, et al. Step1x-edit: practical framework for general image editing. arXiv preprint arXiv:2504.17761, 2025. 2, 3, 4, 7, 8, 1, 5, [24] Jian Ma, Xujie Zhu, Zihao Pan, Qirong Peng, Xu Guo, Chen Chen, and Haonan Lu. X2edit: Revisiting arbitraryinstruction image editing through self-constructed data arXiv preprint and task-aware representation learning. arXiv:2508.07607, 2025. 6, 7 [25] Yiwei Ma, Jiayi Ji, Ke Ye, Weihuang Lin, Zhibin Wang, Yonghan Zheng, Qiang Zhou, Xiaoshuai Sun, and Rongrong Ji. I2ebench: comprehensive benchmark for instructionbased image editing. Advances in Neural Information Processing Systems, 37:4149441516, 2024. 2 [26] Yulin Pan, Xiangteng He, Chaojie Mao, Zhen Han, Zeyinzi Jiang, Jingfeng Zhang, and Yu Liu. Ice-bench: unified and comprehensive benchmark for image creating and editing. arXiv preprint arXiv:2503.14482, 2025. 2, 4 [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 4 [28] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. 6 [29] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. [30] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [31] Team Seedream, Yunpeng Chen, Yu Gao, Lixue Gong, Meng Guo, Qiushan Guo, Zhiyao Guo, Xiaoxia Hou, Weilin Huang, Yixuan Huang, et al. Seedream 4.0: Toward nextarXiv preprint generation multimodal image generation. arXiv:2509.20427, 2025. 2, 3, 7, 8, 6 [32] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 8871 8879, 2024. 4 [33] Xincheng Shuai, Henghui Ding, Xingjun Ma, Rongcheng survey of Tu, Yu-Gang Jiang, and Dacheng Tao. multimodal-guided image editing with text-to-image diffusion models. arXiv preprint arXiv:2406.14555, 2024. 2, 3 [34] NextStep Team, Chunrui Han, Guopeng Li, Jingwei Wu, Quan Sun, Yan Cai, Yuang Peng, Zheng Ge, Deyu Zhou, Haomiao Tang, Hongyu Zhou, Kenkun Liu, Ailin Huang, Bin Wang, Changxin Miao, Deshan Sun, En Yu, Fukun Yin, Gang Yu, Hao Nie, Haoran Lv, Hanpeng Hu, Jia Wang, Jian Zhou, Jianjian Sun, Kaijun Tan, Kang An, Kangheng Lin, Liang Zhao, Mei Chen, Peng Xing, Rui Wang, Shiyu Liu, Shutao Xia, Tianhao You, Wei Ji, Xianfang Zeng, Xin Han, Xuelin Zhang, Yana Wei, Yanming Xu, Yimin Jiang, Yingming Wang, Yu Zhou, Yucheng Han, Ziyang Meng, Binxing Jiao, Daxin Jiang, Xiangyu Zhang, and Yibo Zhu. Nextstep1: Toward autoregressive image generation with continuous tokens at scale. arXiv preprint arXiv:2508.10711, 2025. 7, 8, [35] Peng Wang, Yichun Shi, Xiaochen Lian, Zhonghua Zhai, Xin Xia, Xuefeng Xiao, Weilin Huang, and Jianchao Yang. Seededit 3.0: Fast and high-quality generative image editing. arXiv preprint arXiv:2506.05083, 2025. 7, 8, 6 [36] Qian Wang, Biao Zhang, Michael Birsak, and Peter Wonka. Improving automatic masks for diffusionInstructedit: based image editing with user instructions. arXiv preprint arXiv:2305.18047, 2023. 3 [37] Su Wang, Chitwan Saharia, Ceslee Montgomery, Jordi PontTuset, Shai Noy, Stefano Pellegrini, Yasumasa Onoe, Sarah Laszlo, David Fleet, Radu Soricut, et al. Imagen editor and editbench: Advancing and evaluating text-guided image inpainting. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 18359 18369, 2023. 4 [38] Yuhan Wang, Siwei Yang, Bingchen Zhao, Letian Zhang, Qing Liu, Yuyin Zhou, and Cihang Xie. Gpt-image-edit1.5 m: million-scale, gpt-generated image dataset. arXiv preprint arXiv:2507.21033, 2025. 2, 3, 6, 7, 5 [39] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. 2, 3 [40] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. [41] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng-ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, et al. Qwen-image technical report. arXiv preprint arXiv:2508.02324, 2025. 3, 4, 7, 8, 6 [42] Chenyuan Wu, Pengfei Zheng, Ruiran Yan, Shitao Xiao, Xin Luo, Yueze Wang, Wanli Li, Xiyan Jiang, Yexin Liu, Junjie Zhou, et al. Omnigen2: Exploration to advanced multimodal generation. arXiv preprint arXiv:2506.18871, 2025. 3, 7, 8, 6 [43] Yongliang Wu, Zonghui Li, Xinting Hu, Xinyu Ye, Xianfang Zeng, Gang Yu, Wenbo Zhu, Bernt Schiele, MingHsuan Yang, and Xu Yang. Kris-bench: Benchmarking next-level intelligent image editing models. arXiv preprint arXiv:2505.16707, 2025. 4, 9 [44] Yiyan Xu, Jinghao Zhang, Alireza Salemi, Xinting Hu, Wenjie Wang, Fuli Feng, Hamed Zamani, Xiangnan He, and TatSeng Chua. Personalized generation in large model era: survey. arXiv preprint arXiv:2503.02614, 2025. 3 [45] Junyan Ye, Dongzhi Jiang, Zihao Wang, Leqi Zhu, Zhenghao Hu, Zilong Huang, Jun He, Zhiyuan Yan, Jinghua Yu, Hongsheng Li, et al. Echo-4o: Harnessing the power of gpt4o synthetic images for improved image generation. arXiv preprint arXiv:2508.09987, 2025. [46] Yang Ye, Xianyi He, Zongjian Li, Bin Lin, Shenghai Yuan, Zhiyuan Yan, Bohan Hou, and Li Yuan. Imgedit: unified image editing dataset and benchmark. arXiv preprint arXiv:2505.20275, 2025. 2, 3, 4, 6, 1, 9 [47] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2612526135, 2025. 4 [48] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36:3142831449, 2023. 2, 3, 7, 8, 4, 6 [49] Shu Zhang, Xinyi Yang, Yihao Feng, Can Qin, Chia-Chih Chen, Ning Yu, Zeyuan Chen, Huan Wang, Silvio Savarese, Stefano Ermon, et al. Hive: Harnessing human feedback for instructional visual editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90269036, 2024. 4 [50] Haozhe Zhao, Xiaojian Shawn Ma, Liang Chen, Shuzheng Si, Rujie Wu, Kaikai An, Peiyu Yu, Minjia Zhang, Qing Li, and Baobao Chang. Ultraedit: Instruction-based fine-grained image editing at scale. Advances in Neural Information Processing Systems, 37:30583093, 2024. 3, 1 UnicEdit-10M: Dataset and Benchmark Breaking the Scale-Quality Barrier via Unified Verification for Reasoning-Enriched Edits"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 5. Qualitative comparison of data curation pipelines. (a) shows example triplets from ImgEdit [46]. (b) shows example from SEEDData-Edit [12]. In each subfigure, the left columns display original triplets, while the right columns show the same images reprocessed through Our Pipeline. Red boxes highlight regions of blurring, artifacts, and color inconsistencies in the originals. Our pipeline consistently yields higher quality and more precisely aligned instructions, demonstrating the effectiveness of our unified verification process. In this supplementary material, we further elaborate on three core components of our framework: the data construction pipeline, the expert verification model, and the UnicBench benchmark. For the Data Construction Pipeline, we present the dataset taxonomy in Sec. A, analyze our pipeline and post-verification procedures in Sec. B, discuss noisy cases in Sec. E, and study facial consistency in Sec. F. For the Expert Model Evaluation, we describe the experimental setup and metrics used to assess our verification model in Sec. C. For the Benchmark Analysis, we validate the evaluation metrics in Sec. D, report comprehensive benchmark results in Sec. G, compare UnicBench with existing benchmarks in Sec. H, and provide qualitative examples and evaluation prompts in Secs. and J. A. Dataset Taxonomy To establish our taxonomy, we surveyed prominent datasets, including ImgEdit [46], Step1X-Edit [23], and SEED-DataEdit [12], to consolidate and reclassify their editing tasks. We further introduced novel reasoning category to enhance complex reasoning capabilities and address gaps in existing benchmarks. This process yielded comprehensive classification of 22 unique editing tasks, spanning from basic object manipulation to complex semantic reasoning, as detailed in Tab. 6. B. Data Pipeline Analysis B.1. Comparison with Multi-toolchain Pipelines Automated data curation pipelines [12, 23, 46, 50] that rely on sequence of vision tools are susceptible to error propagation, where inaccuracies from upstream modules are amplified in downstream processes, ultimately degrading dataset quality. To illustrate this, Fig. 5 presents qualitative analysis of samples from ImgEdit [46] and SEEDData-Edit [12], both of which employ such multi-toolchain architectures. The examples exhibit common artifacts stemming from this approach, including significant blurring and inconsistent color blending at edit boundaries. Furthermore, Fig. 5(a) highlights critical issue of misalignment between the instruction and the resulting edit. We processed these challenging samples through our proposed pipeline to demonstrate its corrective capabilities. The results show marked improvement in image quality, with artifacts eliminated and semantic coherence reTable 6. Definitions of editing taxonomy of UnicEdit-10M."
        },
        {
            "title": "Subject Addition\nSubject Removal\nSubject Replacement\nObject Extraction\nCounting Change\nText Modification\nPortrait Editing",
            "content": "Color Alteration Material Modification Motion Change Texture Editing Shape-Size Alteration Adding new object or person to the image. Removing specified object or person from the image. Replacing an object with another. Isolating and extracting target object from its background. Modifying the number of objects in the image. Editing textual content within the image. Enhancing or modifying facial features. Changing the color of an object or region. Changing the material properties of an object. Adjusting the dynamic pose or action of an object. Modifying the surface texture details of an object. Changing the shape or size of an object."
        },
        {
            "title": "Background Change\nStyle Transfer\nTone Transformation\nViewpoint Transformation\nLens Zooming",
            "content": "Replacing or modifying the image background. Converting the overall image to target artistic style. Adjusting the images color tone and atmosphere. Changing the cameras viewpoint or position. Simulating an optical lens zoom effect."
        },
        {
            "title": "Reasoning Editing",
            "content": "Moving objects based on spatial logic. Spatial Reasoning Multi-Object Coordination Coordinately modifying the attributes or positions of multiple objects. Compound Operations Relation Change Implicit Change Edits Executing multiple, combined editing operations. Modifying the interactive relationship between objects. Inferring the actual editing task based on context and real-world knowledge. stored. Concurrently, our expert model automatically recaptioned the instructions to align precisely with the actual visual transformations, thereby correcting the instructionimage mismatch and validating the efficacy of our unified verification and refinement process. B.2. Comparison of No Edit Filtration Methods critical step in our post-verification pipeline is the accurate filtration of No Edit samples, where the output image is visually identical or nearly identical to the original. Traditional pixel-based metrics like the Structural Similarity Index (SSIM) [39] are ill-suited for this task, as they lack semantic understanding and are sensitive to imperceptible artifacts introduced by generative models. Fig. 6 illustrates this deficiency. SSIM [39] proves insensitive to semantically meaningful but visually subtle changes. For instance, the addition of small bottle (topleft example) yields high SSIM [39] of 0.9474, incorrectly suggesting no change occurred. Conversely, SSIM [39] is overly sensitive to minor, imperceptible pixel shifts inherent to the editing models process. In the top-right and bottomleft examples, no visible edit was made, yet their SSIM [39] scores are lower than the sample with clear object addition, leading to false positives. An even more extreme case (bottom-right example) shows minor color artifact causing the SSIM [39] to decline to 0.3241. In contrast, our expert model demonstrates robust, semantically-aware judgment. It correctly identifies the subtle yet valid edit in the first example while accurately classifying the other visually unchanged pairs as No Edit, ignoring trivial pixel-level noise. This capability is crucial for reliable, large-scale data curation. The bar chart in Fig. 6 provides quantitative validation, confirming that our expert model achieves superior performance in identifying No Edit instances compared to baseline methods. B.3. Comparison with Other Datasets We provide comprehensive comparison with existing datasets in Tab. 1 of the main text. Unlike other datasets, UnicEdit-10M employs unified post-verification stage that not only filters low-quality samples but also refines instructions to ensure precise alignment with the actual edits. Furthermore, UnicEdit-10M provides extensive data for complex edits, offering broader functional diversity and greater scale than currently available resources. C. Expert Model Evaluation To evaluate Qwen-Verify, we engaged two human experts to curate total of 390 samples, comprising 300 Normal cases, 50 No Edit cases, and 40 Hallucination cases. For all samples, the experts manually revised any erroneous descripFigure 6. Examples of No Edit filtration. The left side shows four examples with detection results from SSIM [39] and Qwen-Verify. The top-left example has clear edit (red dashed box) but receives high SSIM [39] score, while the other three visually unchanged pairs receive lower scores. The right side provides quantitative comparison, confirming that Qwen-Verify performs best at identifying failed edits. tions to establish reliable ground-truth instructions. To systematically evaluate the performance of Qwen-Verify, we designed an automated evaluation protocol using GPT-4.1. This protocol parses atomic editing tasks from each instruction and computes score to objectively quantify the alignment between the ground-truth instruction and the instruction generated by our model. Here, the first term calculates the recall of correctly identified atomic tasks. The second term imposes penalty, weighted by w, for any extraneous tasks generated by the model. In our experiments, we set = 0.5. This metric provides quantitative measure of the precision and fidelity of the generated answers. After validation, the expert model is deployed in our pipeline for large-scale data curation. Atomic Task Decomposition. We formalize an instruction as set of atomic editing tasks, TP . Each atomic task ti is tuple representing core operation: parse TP = {ti}N i=1, where ti = (oi, ai) (5) In this formulation, oi denotes the target object or region, and ai represents the corresponding edit action. Alignment Accuracy. Given the set of atomic tasks from the ground-truth instruction, TGT , and the set from the generated instruction, TGEN , we compute an Alignment Accuracy (Acc.). The score measures the coverage of groundtruth tasks while penalizing for hallucinated or redundant tasks. It is defined as: Acc(TGEN , TGT ) = TGT TGEN TGT (cid:123)(cid:122) Coverage (Recall) (cid:125) (cid:124) TGEN TGT TGT (cid:125) (cid:123)(cid:122) (cid:124) Redundancy Penalty (6) D. Validation of Benchmark Metrics Our evaluation protocol offers fine-grained assessment by decomposing edit quality into four key dimensions: Instruction Following (IF), Non-edit Consistency (NC), Visual Quality (VQ), and Reasoning Accuracy (RA). This approach offers more diagnostic alternative to other metrics like the VIEScore [19], which can obscure specific failure modes. Comparison with VIEScore. As illustrated in Fig. 8, our protocol demonstrates superior sensitivity to common editing failures. In example (a), the model replaces the man instead of adding woman to his left. While VIEScore [19] assigns an SC score of 9, failing to penalize this critical error in non-edit preservation, our NC metric correctly identifies the unintended removal and assigns lower score of 7. Similarly, in example (b), changing CLASSIC MOJITO to BABY MILKSHAKE also results in collateral damage to the text below it. VIEScore [19] again overlooks Figure 7. Examples of our evaluation protocol on two complex edit cases. For each example, the edited image pair and instruction are shown on the left. The right side displays the scores for four evaluation dimensions, along with the detailed reasons from the VLM evaluator, which specifies points of credit and deduction. Table 7. Comparison of different image editing benchmarks. Benchmarks #Size #Subtasks Human Filtering Basic Edit Complex Edit Public Available EditBench [37] EmuEdit [32] HIVE [49] HQ-Edit [16] MagicBrush [48] AnyEdit-Test [47] ICE-Bench [26] ImgEdit-Bench [46] GEdit-Bench [23] KRIS-Bench [43] UnicBench 240 3055 1000 1640 1053 1250 6538 737 606 1267 1100 1 7 1 7 7 25 31 9 11 22 22 this whereas our NC metric penalizes the collateral change. These cases show that our fine-grained metrics more reliably assess preservation of non target regions. Alignment with Human Judgment. To validate that our VLM-based evaluation aligns with human judgment, we analyze its scoring on complex cases in Fig. 7. In the top example, the instruction involves two tasks: changing the leftmost cow to sheep and the background to forest. While the model executes these tasks, it incorrectly alters two other cows into sheep. Our protocol accurately diagthe IF score is lowered to 5.0 noses this partial failure: because the instruction was not precisely followed (only the leftmost cow was specified), and the NC score is also 5.0, penalizing the unintended modifications to the other animals. The VQ score is 7.0, reflecting good overall image quality but noting the unnatural facial features of the edited animals. Finally, guided by the reasoning-points list, the RA metric correctly identifies the erroneous object-type change, resulting in score of 5.0. The bottom example tests the models reasoning capability by requesting to make the coffee pot boil. The model correctly adds steam and bubbles but undesirably alters the pots shape and adds an exaggerated flame. Both IF and Figure 9. Qualitative comparison of facial consistency. (a) shows examples from GPT-Image-Edit-1.5M [38]. (b) shows examples from UnicEdit-10M. made; (2) Instruction-Image Misalignment, where the visual change deviates from the instruction; and (3) Others, which encompasses range of quality issues. Our expert model, Qwen-Verify, is specifically trained to address the first two, more prevalent issues by filtering null edits and recaptioning misaligned ones, thereby ensuring semantic and instructional fidelity. For the third category, which includes issues like low aesthetic quality, anatomical deformities (e.g., incorrect limbs on human subjects), and other structural distortions, we employ pre-trained aesthetic scoring models and specialized human-body detectors to automatically flag and remove such low-quality samples. This multi-stage refinement strategy ensures that the final dataset is not only semantically aligned but also meets high standard of visual quality. F. Analysis of Facial Consistency As reported in the main text, our dataset and GPT-ImageEdit-1.5M [38] achieve comparable Semantic Consistency scores. This prompted deeper investigation into the qualitative differences between the two datasets. sampling analysis of GPT-Image-Edit-1.5M [38] revealed frequent occurrence of facial identity inconsistency, critical issue Table 8. Comparison of facial consistency between GPT-ImageEdit-1.5M [38] and UnicEdit-10M."
        },
        {
            "title": "Facial Consistency",
            "content": "GPT-Image-Edit-1.5M [38] Ours 0.3025 0.8911 Figure 8. Comparison between GEdit-Benchs [23] metrics and UnicBenchs metrics. (a) compares scoring for case with an unexpected removal. (b) compares scoring for case with an unexpected text change. NC scores are set to 7.0, acknowledging the partial success while penalizing the unintended shape distortion. The VQ score is reduced due to the unrealistic flame. The RA score is 5.0, recognizing that the concept of boiling was successfully rendered but penalizing the failure to preserve the objects core identity (its shape). These case studies demonstrate that our multi-dimensional metric, as judged by VLM, provides comprehensive and human-aligned assessment, effectively pinpointing the specific strengths and weaknesses of an editing model in single evaluation. E. Analysis of Noisy Data As outlined in Sec. 3.2, our initial data generation process produces noisy triplets that fall into three primary categories: (1) Edit Failure, where no discernible edit is Figure 10. Overall score of each model on the sub-tasks in UnicBench, for EN (left) and CN (right) instructions. All results are evaluated by GPT-4.1. Table 9. Detailed performance across different editing tasks (EN). The performance of open-source and closed-source models is separately marked with the best performance in bold, and the second best underlined. All results are evaluated by GPT-4.1. Model Attribute Editing Object Editing Scene Editing Reasoning Editing IF NC VQ Overall IF NC VQ Overall IF NC VQ Overall IF NC VQ RA Overall Instruct-Pix2Pix [4] MagicBrush [48] OmniGen2 [42] UniWorld-v1 [22] FLUX.1-Kontext [3] NextStep-1 [34] BAGEL [8] Step1X-Edit-v1.1 [23] Qwen-Image-Edit [41] Nano Banana [7] SeedEdit 3.0 [35] Seedream 4.0 [31] GPT-Image-1 [17] 3.1320 2.3655 7.1600 5.8080 6.8760 7.8600 7.5440 7.7560 8.3960 7.7000 8.5265 8.6160 9.1532 4.3880 3.6466 7.9080 8.1480 8.9160 6.3160 8.8120 8.7280 8.2800 9.3040 8.9959 9.1040 7. 4.3840 3.6185 6.8640 7.0200 7.4880 6.9320 7.3400 7.8440 8.0720 8.2160 8.0082 8.1880 8.6000 3.2273 2.4559 6.9011 6.3074 7.0269 6.7591 7.3184 7.6342 7.9378 7.9117 8.2087 8.3225 8.2475 2.0143 2.5229 5.5514 4.8457 6.4286 6.3000 6.6343 6.7886 7.8400 7.6319 7.7609 7.8257 8. 3.3152 3.7000 7.1400 6.6886 8.1486 5.5229 7.9771 8.1400 7.8029 9.0348 8.3382 8.5286 8.0090 Open-Source Models 3.3754 3.6514 6.3514 6.1486 7.2800 6.0029 7.0800 7.4314 8.0657 2.1075 2.6074 5.6477 5.1441 6.5028 5.6326 6.6784 6.9090 7.5128 4.2880 2.3120 6.6880 6.1360 7.7040 8.4056 8.9200 7.4880 9.2960 Closed-source Models 8.2174 7.7230 7.9200 8. 7.7034 7.6277 7.6482 8.2422 8.7400 9.1215 8.9960 9.6680 5.7240 2.7320 7.9680 7.5480 8.4960 7.1165 8.2400 7.9280 8.3440 8.8760 8.1417 8.8560 8.1618 4.8600 3.1960 6.4560 6.8840 7.6200 7.0241 7.5960 7.2280 8.5800 8.4800 8.2672 8.3280 8. 4.5773 2.1907 6.6075 6.4598 7.6453 7.3901 8.1083 7.2437 8.6277 8.5360 8.4011 8.5813 8.8009 2.3080 2.0880 5.8600 4.6160 6.2320 6.7360 6.1440 6.0280 7.4360 7.9600 7.8785 8.2880 9.0705 3.2760 3.3360 7.1160 7.1000 8.4560 6.0400 7.8520 8.0480 7.7680 8.6880 8.2632 8.4680 7. 3.4840 3.2800 6.3400 6.0120 7.0840 6.2480 6.5640 6.8120 7.5840 7.8600 7.4049 7.9200 8.4978 1.9560 1.7240 5.1240 4.0160 5.5040 6.2920 5.2600 5.0400 6.4480 6.8680 6.9393 7.5960 8.3392 2.0990 2.0027 5.5329 4.6767 6.1634 6.1626 5.9326 5.8982 6.9168 7.4324 7.3269 7.7770 8. for practical applications. To quantify this observation, we conducted comparison of facial consistency. For portrait image pairs from both datasets, we employed RetinaFace [10] for face detection and ArcFace [9] to extract feature vectors, subsequently calculating the cosine similarity between the faces in the source and edited images. The results, presented in Tab. 8, quantitatively confirm that our dataset maintains significantly higher face consistency score. Qualitative examples in Fig. 9 further illusIn the examples from GPT-Imagetrate this distinction. Edit-1.5M [38] shown in Fig. 9 (a), the left image pair shows noticeable changes in the womans facial features and face shape, and the right image pair shows change in the womans perceived ethnicity. In contrast, our dataset, as shown in Fig. 9 (b), exhibits much stronger facial identity preservation. This evidence indicates that our data generation pipeline better preserves subject identity throughout the editing process, enhancing the datasets overall quality and utility. G. Analysis of Benchmark Results G.1. Overall Analysis While Tab. 4 of the main text presents the aggregate scores for mainstream models, we visualize their performance breakdown by sub-task in the radar chart in Fig. 10. The chart clearly reveals performance dichotomy: most models perform well on foundational tasks like Background Change and Subject Replacement, but all models exhibit significant performance drop on Viewpoint Transformation, Text Modification, and Spatial Reasoning Edits. This indicates that current architectures still lack robust spaFigure 11. Performance of four evaluation dimensions for each sub-task. The top row shows results for EN tasks, and the bottom row shows results for CN tasks. All results are evaluated by GPT-4.1. Table 10. Detailed performance across different editing tasks (CN). The performance of open-source and closed-source models is separately marked with the best performance in bold, and the second best underlined. All results are evaluated by GPT-4.1. Model Attribute Editing Object Editing Scene Editing Reasoning Editing IF NC VQ Overall IF NC VQ Overall IF NC VQ Overall IF NC VQ RA Overall BAGEL [8] Step1X-Edit-v1.1 [23] Qwen-Image-Edit [41] 7.5560 8.0080 8.6200 8.8120 8.8640 7.9400 7.5080 7.9360 8.2240 7.4498 7.8829 8.0001 6.7114 6.9229 7.8971 8.1829 8.4171 7. Nano Banana [7] SeedEdit 3.0 [35] Seedream 4.0 [31] GPT-Image-1 [17] 7.8040 8.7854 8.6840 9.2876 9.4760 9.0040 9.0080 7.8541 8.4640 8.1457 8.2760 8.6481 8.1122 8.4379 8.3881 8.3420 8.0836 7.8006 7.8114 9. 9.0317 8.3900 8.3771 8.0150 Open-source Models 7.3029 7.8429 8.2286 6.9385 7.1484 7.5682 9.0360 7.2600 9.4200 Closed-source Models 8.3026 7.8240 7.9657 8.7688 8.0299 7.6794 7.6935 8. 8.7800 9.2418 9.0520 9.6292 8.2000 8.2800 8.1120 7.6880 7.4000 8.5920 8.1469 7.2448 8.5920 6.1400 5.9640 7.7400 7.9840 8.0840 7. 6.7520 6.9480 7.7960 5.2840 5.0560 6.6560 5.9540 5.9374 7.0400 8.7960 8.2213 8.9280 8.2625 8.4480 8.4590 8.3600 8.8375 8.5037 8.5126 8.6731 8. 7.9800 7.8807 8.0320 9.0925 8.8760 8.2016 8.4400 7.3524 8.1120 7.5473 8.0120 8.4978 6.8960 6.8395 7.1240 8.2247 7.4994 7.3807 7.5765 8.1594 tial awareness and the ability to precisely edit in-image text. Furthermore, the visualization highlights critical gap: open-source models lag behind their closed-source counterparts, particularly on complex reasoning edits. This suggests that the primary bottleneck for open-source models is not basic instruction following, but the advanced inference and world knowledge required for complex manipulation. G.2. Analysis across Different Tasks and Metrics detailed analysis of model performance across different editing tasks is provided in Tab. 9 and Tab. 10. For basic tasks, the performance gap between top open-source models like Qwen-Image-Edit [41] and closed-source models is narrow. On some tasks, open-source models even achieve comparable results. For example, Qwen-Image-Edit [41] surpasses Nano Banana [7] in Attribute Editing and matches GPT-Image-1 [17] in Scene Editing. This suggests that for attribute and scene-level transformations, the capabilities of open-source and closed-source models are converging. However, substantial performance gap remains in reasoning-intensive tasks, where closed-source models maintain clear advantage. In this category, Qwen-ImageEdit [41], despite being the SOTA open-source model, lags significantly behind all closed-source models on the IF and RA metrics. This divergence points to the heightened demand for advanced semantic understanding and logical reasoning in complex edits, capabilities that are more densely embedded in closed-source foundation models. Table 11. Detailed performance across different editing tasks (EN). The performance of open-source and closed-source models is separately marked with the best performance in bold, and the second best underlined. All results are evaluated by Qwen2.5-VL-72B [1]. Model Attribute Editing Object Editing Scene Editing Reasoning Editing IF NC VQ Overall IF NC VQ Overall IF NC VQ Overall IF NC VQ RA Overall Instruct-Pix2Pix [4] MagicBrush [48] OmniGen2 [42] UniWorld-v1 [22] FLUX.1-Kontext [3] NextStep-1 [34] BAGEL [8] Step1X-Edit-v1.1 [23] Qwen-Image-Edit [41] Nano Banana [7] Seededit 3.0 [35] Seedream 4.0 [31] GPT-Image-1 [17] 4.3520 4.2080 7.3760 5.5800 6.6400 8.4240 7.7600 7.8000 8.3880 7.6200 8.5184 8.3160 8. 3.8800 2.9480 6.8440 7.8920 8.3120 6.5560 7.2600 8.1120 8.1200 8.5200 8.2408 8.2720 8.0128 6.0560 5.2080 7.8560 7.1400 7.6840 8.1440 7.9960 8.2360 8.5840 8.1000 8.3837 8.4840 8.6681 2.4986 2.6521 6.3789 5.0760 6.1066 7.0760 6.7454 7.2982 7.8205 7.2225 7.8729 7.7476 8. 3.5029 4.2229 6.3543 5.1343 7.0629 7.6714 7.6143 7.3571 8.6771 8.0493 8.4111 8.1600 9.0418 3.3114 3.0600 6.6714 6.4657 8.2400 5.7800 7.4343 7.9000 8.0086 8.7304 8.0029 8.4657 8.3373 Open-source Models 5.6400 5.8600 7.4829 7.1057 8.1400 7.8886 8.1143 8.2457 8.7343 1.9398 2.7003 5.5498 4.6795 6.5634 6.1585 6.8647 6.8988 7. 5.2880 4.1840 7.3880 6.5000 7.7520 8.5120 8.5000 7.6840 8.7920 Closed-source Models 8.6406 8.5598 8.5914 8.8448 7.5940 7.7193 7.6575 8.5047 7.9520 8.5870 8.3880 8.7178 3.6360 2.4040 5.7560 7.2640 6.7280 5.6040 5.9800 6.8600 6.9960 7.5000 7.0567 7.2440 6. 5.9120 5.4400 7.5720 7.0400 7.9320 8.1760 8.2360 7.7720 8.7000 8.1080 8.4818 8.3560 8.6432 3.1577 1.9384 5.3390 5.4614 6.4008 6.5466 6.7331 6.4434 7.4982 7.3382 7.5827 7.3499 7.3529 3.0320 3.1160 7.2400 5.3520 7.3600 8.1436 7.4840 7.5560 8.0400 8.3520 8.4696 8.4760 8. 2.8480 2.8200 7.3080 7.5760 8.4040 6.1264 7.3960 8.0760 8.1400 8.3040 8.0040 8.3440 7.9295 5.3400 5.3720 8.0080 7.1760 8.2800 8.0800 8.1080 8.1240 8.5800 8.6040 8.5547 8.5680 8.7753 2.4800 2.4880 6.2160 4.0920 6.2600 7.5960 6.5240 5.8760 7.0560 7.3440 7.4737 7.8240 8. 1.3405 1.6255 5.7325 3.8848 5.9478 6.6022 6.0373 5.6262 6.8429 7.0548 7.1893 7.5647 8.0882 Table 12. Detailed performance across different editing tasks (CN). The performance of open-source and closed-source models is separately marked with the best performance in bold, and the second best underlined. All results are evaluated by Qwen2.5-VL-72B [1]. Model Attribute Editing Object Editing Scene Editing Reasoning Editing IF NC VQ Overall IF NC VQ Overall IF NC VQ Overall IF NC VQ RA Overall BAGEL [8] Step1X-Edit-v1.1 [23] Qwen-Image-Edit [41] 7.5240 7.7800 8.3640 7.6440 8.0240 7.9480 8.0200 8.1720 8.5760 6.9105 7.2283 7. 7.7286 7.6829 8.6086 7.5629 8.0286 7.9029 Nano Banana [7] Seededit 3.0 [35] Seedream 4.0 [31] GPT-Image-1 [17] 7.5680 8.4696 8.2720 8.5322 8.6440 8.2551 8.3600 8.0172 8.2480 8.5749 8.3320 8. 7.3271 8.0304 7.8557 8.0269 8.0980 8.2581 8.0314 8.9940 8.4784 8.0821 8.2657 8.2312 Open-source Models 8.1486 8.2429 8.5971 6.9739 7.0884 7.9068 8.3560 7.2760 8. Closed-source Models 8.5447 8.5161 8.4829 8.8739 7.6920 7.7057 7.4085 8.4280 8.2080 8.5779 8.3080 8.7833 5.9000 6.8880 6.5000 8.2360 7.6560 8.6560 6.5937 5.9153 7. 7.3360 7.5320 8.1280 7.7040 8.2320 8.0760 8.1280 8.1560 8.6160 6.1880 5.7560 7.1640 5.8825 5.6335 6.9095 7.5200 7.0205 6.9400 6. 8.4080 8.6598 8.3440 8.6208 7.3560 7.4739 7.0822 7.1456 8.1920 8.3210 8.4680 8.7753 8.4920 8.1687 8.2320 7.9163 8.5440 8.5638 8.6400 8.7137 7.2880 7.7202 7.6400 8. 6.9491 7.4344 7.3179 8.1429 The radar chart in Fig. 11 visualizes the scores for the four evaluation dimensions and reveals clear performance disparities. On Instruction Following (IF), GPTImage-1 [17] shows strong semantic understanding and adherence to instructions and it significantly outperforms other models. Notably, nearly all models falter on Portrait Editing, Viewpoint Transformation, and Text Modification, which indicates persistent limitations in portrait editing, spatial understanding, and in-image text manipulation. In contrast, Visual Quality (VQ) scores are more uniform across models, with Qwen-Image-Edit [41] performing on par with closed-source models, which suggests that most mainstream models can produce high-quality images. For Non-edit Consistency (NC), GPT-Image-1 [17] lags behind other closed-source models and is surpassed by open-source models such as Qwen-Image-Edit [41] and BAGEL [8], which implies difficulty in preserving non-target regions during fine-grained edits despite strong instruction understanding. The gaps are most pronounced on Reasoning Accuracy (RA). In EN tasks, closed-source models show clear advantages and open-source models struggle, especially on Implicit Change Edits and Spatial Reasoning Edits. In CN tasks, the gap narrows, and Qwen-Image-Edit [41] matches or exceeds some closed-source models on Implicit Change Edits and Multi-object Coordination, which suggests that the reasoning abilities of some closed-source models are less robust for Chinese instructions. By integrating basic and complex tasks, UnicBench pinpoints critical limitations in consistency and reasoning. Our findings suggest that future research should focus on aligning semantic understanding with edit execution and on enhancing reasoning capability so that models can leverage world knowledge to satisfy complex user intents. G.3. Evaluation on Qwen2.5-VL-72B To validate the robustness of our evaluation protocol and mitigate potential biases from single proprietary evaluator, we employed Qwen2.5-VL-72B [1] as an open-source scoring proxy. The results, detailed in Tabs. 11 and 12, reveal trends that are largely consistent with the GPT-4.1 evaluation, reinforcing the reliability of our findings. Notably, Qwen-Image-Edit [41] maintains its position as the leading open-source model and even surpasses Nano Banana [7] on EN instructions for all categories except Reasoning Editing. This confirms that top-tier open-source models are becoming highly competitive in foundational editing tasks. However, closed-source models retain decisive advantage in complex edits, particularly for EN instructions. While this gap narrows for CN instructions, closed-source models still maintain leading position. This underscores that advanced reasoning remains critical area for the future development of open-source models. The consistent top performance of GPT-Image-1 [17] across both evaluators solidifies its status as the current state-of-the-art image editing model. H. Comparison with Other Benchmarks We compare UnicBench with existing benchmarks in Tab. 7. As shown, most benchmarks focus primarily on basic edits, offering incomplete coverage of model capabilities. Specialized benchmarks like KRIS-Bench [43] target only reasoning tasks and thus cannot serve as generalpurpose evaluation tools. While ImgEdit-Bench [46] includes complex edits, its coverage is minimal, with only 47 samples across limited categories, making it insufficient for comprehensive measurement. UnicBench addresses these shortcomings by providing balanced coverage of both basic and complex edits across 22 sub-tasks, with all samples manually reviewed for quality. This makes UnicBench more comprehensive and reliable tool for evaluating the full spectrum of image editing capabilities. I. Benchmark Cases In this section, we present additional qualitative results. We showcase examples from mainstream models across our four main editing categories: Attribute Editing  (Fig. 12)  , Object Editing  (Fig. 13)  , Scene Editing  (Fig. 14)  , and Reasoning Editing  (Fig. 15)  . In each figure, the first column displays the original image, and the subsequent columns show the outputs from the evaluated models. J. Evaluation Prompts Prompts 1 to 4 detail the instructions provided to the VLM evaluator for assessing Instruction Following, Nonedit Consistency, Visual Quality, and Reasoning Accuracy, respectively. For each dimension, the evaluation is based on set of inputs including the original image, the edited image, and the edit instruction. To facilitate more accurate assessment of complex tasks, the evaluation of Reasoning Accuracy is uniquely supplemented with list of Reasoning Points that guide the VLM in its evaluation process. Figure 12. Qualitative results for Attribute Editing tasks on UnicBench (EN). Figure 13. Qualitative results for Object Editing tasks on UnicBench (EN). Figure 14. Qualitative results for Scene Editing tasks on UnicBench (EN). Figure 15. Qualitative results for Reasoning Editing tasks on UnicBench (EN). Prompt 1: Evaluation Prompt for Instruction Following **Precision Image Editing - Instruction Following Evaluation Protocol** # SYSTEM ROLE You are an expert visual evaluator specialized in image transformation analysis. Your task is to rigorously assess how well the edited image adheres to the original instruction by identifying all visual changes with precision, with focus on accuracy in human-related edits. # INPUT DATA - Original Image: Reference image before editing. - Edited Image: Result image after editing. - Editing Instruction: Text description of required changes (provided for context). # OUTPUT FORMAT You MUST output JSON object with exactly two keys: \"score\" (a number between 0 and 10) and \"reason\" (a concise string). Do not include any other text or formatting. Example: { \"score\": 8, \"reason\": \"concise factual summary\" } # EXECUTION STEPS (perform internally) ## 1. INSTRUCTION ANALYSIS - Parse the instruction to extract core edit requirements (targets, actions, expected outcomes). - Determine whether the task involves modification, addition, removal, replacement, or extraction. ## 2. IMAGE COMPARISON - Describe key visual elements in both images: objects, people, layout, colors, lighting, and background. - Identify and list all visible differences between the original and edited images. - Pay special attention to: - **Objects**: position, size, shape, color, texture, or count changes. - **People**: facial expressions, limb completeness, count, and posture. - **Background**: any added, removed, or replaced components. - **Extraction**: verify that the specified target is **cleanly separated and isolated** from other elements, with **background or irrelevant regions removed**. - **Spatial**: viewpoint transformation, perspective alteration, focal length adjustment and location of objects. ## 3. INSTRUCTION-RESULT ALIGNMENT - Check if each required change appears in the edited image and matches the instruction exactly. - Identify: - **Missing Edits**: instructed changes not reflected in the image. - **Extra Edits**: unintended or unrelated modifications. - **Incorrect Edits**: wrong objects or attributes edited. - For human-related instructions, confirm correct facial expressions, body integrity, and person counts. - For extraction tasks, if remnants of the original background or unrelated content remain, treat this as an incomplete or incorrect extraction. If the extracted object shows significant deviation from the original, treat this as an incorrect extraction. - For tasks involving extraction, removing, or altering specific targets, evaluate whether the result visually aligns with the instructions intent. - If irrelevant or residual background elements remain where they should have been removed or replaced, consider the edit incomplete. - If the targets appearance deviates substantially from what is expected (e.g., distorted, missing key parts, or visually inconsistent with the instruction), consider the edit incorrect. - Only treat targets complete removal as completely incorrect edit when the instruction explicitly requires its preservation or extraction. ## 4. SCORING CRITERIA - Start from base_score = 10. - Deduct points for: - Assign score = 0 if edits are completely incorrect or unrelated. - Missing required edits: -3 per key omission. - Extra or unrelated edits: -3 per occurrence. - Incorrectly applied edits (wrong area/object): -2 each. - Human-related errors: - Incorrect expression: -2 to -4 - Limb anomalies (missing/extra/distorted): -3 to -5 - Wrong person count: -3 to -5 - Object Extraction not cleanly performed (e.g., background retained or partial extraction): -3 to -5 per occurrence. - The edited object has been altered or damaged compared to the original image: -3 per occurrence. - Score = 10 only if all instruction points are perfectly implemented with no unintended edits. - Round score to the nearest integer within [0,10]. ## 5. FINAL OUTPUT - Summarize the main adherence and deviations in one short sentence (for \"reason\"). - Output JSON only, no additional text. # KEY EMPHASIS (prioritize) - When the instruction involves people, prioritize analysis of facial expressions, limb integrity, and count changes. For other subjects, focus on attributes specified in the instruction. - Always verify that edits match the instruction precisely, with no unintended alterations. - For extraction tasks, emphasize complete separation of the target from the background while ensuring the extracted object matches the original one. - Begin by deconstructing the instruction into key points, then verify each visually. # PROHIBITIONS - Do not assign score of 10 unless adherence is perfect across all points. - Do not output any text beyond the JSON object. - Avoid assumptions; base analysis solely on visual evidence. - Do not ignore severe errors like limb anomalies or incorrect counts. # INPUT **Editing instruction**: <instruction> Prompt 2: Evaluation Prompt for Non-edit Consistency **Precision Image Editing - Non-Edited Region Consistency Protocol** # SYSTEM ROLE You are an expert Visual Language Model (VLM) evaluator specialized in detecting unintended or harmful changes outside the explicitly requested edit areas. # TASK Given an Original Image, an Edited Image, and an Editing Instruction, determine whether non-instruction regions were altered (removed, added, color/texture changed, count change, text change or corrupted) and produce concise scored verdict. # INPUT DATA - Original Image: Reference image before editing. - Edited Image: Result image after editing. - Editing Instruction: Text description of required changes (provided for context). # OUTPUT FORMAT You MUST output JSON object with exactly two keys: \"score\" (a number between 0 and 10) and \"reason\" (a concise string). Do not include any other text or formatting. Example: { \"score\": 8, \"reason\": \"concise factual summary\" } # EXECUTION STEPS (perform internally) ## 1. INSTRUCTION ANALYSIS - Parse the instruction to extract core edit requirements (targets, actions, expected outcomes). - Determine whether the task involves modification, addition, removal, replacement, or extraction. ## 2. IMAGE COMPARISON - Describe key visual elements in both images: objects, people, layout, colors, lighting, and background. - Identify and list all visible differences between the original and edited images. - Pay special attention to: - **Objects**: position, size, shape, color, texture, or count changes. - **People**: facial expressions, limb completeness, count, and posture. - **Background**: any added, removed, or replaced components. - **Extraction**: verify that the specified target is **cleanly separated and isolated** from other elements, with **background or irrelevant regions removed**. - **Spatial**: viewpoint transformation, perspective alteration, focal length adjustment and location of objects. ## 3. NON-EDITED REGION CONSISTENCY CHECK - Compare observed changes with the instructions intended scope. - Identify **two categories** of non-edit consistency issues: - **Within the edited target**: unintended modifications beyond what was instructed (e.g., color change required, but shape, action or structure also altered). - **Outside the edited target**: any visual difference in other image regions not mentioned in the instruction. - Check for: - **Count Consistency**: unexpected additions or removals of people, animals, or objects. - **Structural Integrity**: missing or extra limbs, distorted shapes, identity loss, or large occlusions. - **Background Continuity**: unintended texture, color, or lighting changes; visible seams, tiling, blurring, or retouch artifacts. - **Extraction Tasks**: for extraction tasks, ensure the target object remains intact and non-target regions (e.g., background) are fully removed unless new background is specified. - **Shadow/Contact Consistency**: missing or incorrect shadows that break realism or contact. - **Local Detail Preservation**: fine textures or small visual details lost, blurred, or warped without instruction. - **Artifacts**: duplication, floating fragments, warped faces, or visible compositing errors. - **Text Consistency**: unintended text additions, removals, or alterations. - Treat any visual change outside the explicitly edited regions as penalty, regardless of its magnitude. ## 4. SCORING - Start base_score = 10. - Non-instruction region penalties (apply per distinct violation observed): - Unexpected removal/addition or unexpected count change: -3 each. - Significant visual alteration (color, shape, or structure) in non-instruction regions: -3 each. - Minor unintended modification (small lighting, shading, or texture inconsistency): -2 each. - Severe compositing or structural errors (e.g., duplicated objects, identity loss, limb errors): -4 to -5. - Final calculation: - Start from base_score = 10 and subtract penalties. - Compute strictly - ensure arithmetic is correct, then apply rounding (half up) and clamp to the [0,10] range. ## 5. FINAL OUTPUT - Summarize the key findings concisely. - Output **only** the JSON object. # KEY EMPHASIS - Evaluate both: 1. The **edited object itself**, ensuring no unintended alterations beyond the instruction. 2. The **rest of the image**, ensuring complete consistency with the original. - Any unexpected change outside the instructed edit area - even minor - must reduce the score. - Prioritize structural integrity, identity preservation, and environmental consistency. - Human anomalies and background distortions are considered high-severity violations. - Do not assume intent beyond the given instruction. # INPUT **Editing instruction**: <instruction> Prompt 3: Evaluation Prompt for Visual Quality **Precision Image Editing - Visual Quality Evaluation Protocol** # SYSTEM ROLE You are an expert Visual Language Model (VLM) evaluator specializing in assessing the *visual quality* and *naturalness* of image edits. # TASK Evaluate whether the edited regions appear visually natural and seamlessly integrated with the surrounding non-edited areas. Check carefully for any distortions, artifacts, unnatural blending, or unrealistic inconsistencies. For *realistic* edits, judge physical plausibility and seamless integration. For *non-realistic or stylized* edits (e.g., cartoonization, painting), assess the completeness, stylistic coherence, and consistency of the applied style without broken, missing, or incomplete regions. # INPUT DATA - Original Image: The reference image before editing. - Edited Image: The result image after editing. - Editing Instruction: The text description specifying the required edits (provided for contextual reference). # OUTPUT FORMAT You MUST output JSON object with exactly two keys: \"score\" (a number between 0 and 10) and \"reason\" (a concise factual explanation). Do not include any other text or formatting. Example: { \"score\": 8, \"reason\": \"Smooth integration but minor edge inconsistencies around the object.\" } # EXECUTION STEPS (perform internally) ## 0. PRECHECK - Determine whether the edit expects **realistic** or **stylized** output. - If global adjustments (tone, color) are explicitly allowed, treat them as in-scope. - If style or scope is ambiguous, mark as **ambiguous** and apply penalty later. ## 1. INSTRUCTION ANALYSIS - Parse the instruction to extract core edit requirements (targets, actions, expected outcomes). - Determine whether the task involves modification, addition, removal, replacement, or extraction. ## 2. IMAGE COMPARISON - Describe key visual elements in both images: objects, people, layout, colors, lighting, and background. - Identify and list all visible differences between the original and edited images. - Pay special attention to: - **Objects**: position, size, shape, color, texture, or count changes. - **People**: facial expressions, limb completeness, count, and posture. - **Background**: any added, removed, or replaced components. - **Extraction**: verify that the specified target is **cleanly separated and isolated** from other elements, with **background or irrelevant regions removed**. - **Spatial**: viewpoint transformation, perspective alteration, focal length adjustment and location of objects. ## 3. VISUAL QUALITY CHECKS - Evaluate the overall visual quality of the **Edited Image** from both technical and aesthetic perspectives: - **Edge & Boundary Integration**: Seamless blending without visible seams, halos, or artificial cutouts. - **Color & Texture Continuity**: Natural transitions between edited and original regions; penalize abrupt changes. - **Lighting & Shadow Consistency**: Physically plausible lighting direction, shadow intensity, and reflections. - **Geometric & Perspective Coherence**: Proper object sizing, positioning, and perspective alignment. - **Resolution & Sharpness**: Consistent sharpness and noise levels across all regions. - **Artifact Detection**: Identify distortions, warping, ghosting, or compositing defects. - **Global Consistency**: Avoid unintended global color or tone shifts in unrelated areas. - **Aesthetic Quality**: Assess visual appeal, composition balance, and adherence to aesthetic standards. ## 4. HUMAN CHECKS (for edits involving people) - For edits involving people, prioritize human-centric consistency: * **Face naturalness**: No warping, asymmetry, or texture inconsistency. * **Hair integrity**: Natural flow without abrupt cutouts or pasted strands. * **Limb and joint continuity**: No missing, duplicated, or misaligned limbs. * **Clothing boundaries**: Preserve realistic shading and contact with the environment. ## 5. SCORING - Initialize base_score = 10. - Apply penalties as follows: * Severe distortions, heavy artifacts, or major inconsistencies: -4 each. * Moderate blending, color, or lighting mismatches: -3 each. * Minor imperfections (blur, small boundary issues): -2 each. * Incomplete stylization: -2 to -4 depending on area affected. * Unrequested global tone/color changes: -2 to -3. * Critical human-related defects (face warp, missing limb): -4 each. * Ambiguity penalty (if applicable): -2. - Final calculation: - Start from base_score = 10 and subtract penalties. - Compute strictly - ensure arithmetic is correct, then apply rounding (half up) and clamp to the [0,10] range. # KEY EMPHASIS - Evaluate *naturalness*, *seamless integration*, and *aesthetic harmony*. - Prioritize human-related areas - even subtle defects there have large impact. - For realistic edits: check physics-based plausibility (light, shadow, perspective). - For stylized edits: check consistency and visual completeness. - Do not assume intent beyond the instruction; base judgment purely on visual evidence. - Do not assign 10 unless absolutely no defects or unnatural transitions exist. # INPUT **Editing instruction**: <instruction> Prompt 4: Evaluation Prompt for Reasoning Accuracy **Precision Image Editing - Reasoning Accuracy Evaluation Protocol** # SYSTEM ROLE You are an expert visual evaluator specialized in complex-instruction image editing tasks. # TASK Judge whether the Edited Image logically and visually satisfies the Editing Instruction, ensuring that all reasoning-dependent sub-tasks were correctly inferred and executed. Use the provided Reasoning Points as reference to validate expected edits at fine granularity. # INPUT DATA - Original Image: The reference image before any editing. - Edited Image: The resulting image after editing. - Editing Instruction: The textual description specifying the required changes. - Reasoning Points: list of key sub-tasks or inferred reasoning points derived from the complex editing instruction; provided as reference guidance to help evaluation. # OUTPUT FORMAT You MUST output JSON object with exactly two keys: \"score\" (a number between 0 and 10) and \"reason\" (a concise string). Do not include any other text or formatting. Example: { \"score\": 8, \"reason\": \"concise factual summary\" } # EXECUTION STEPS (perform internally) ## 1. INSTRUCTION ANALYSIS - Decompose the Editing Instruction into explicit actions and implicit reasoning requirements. - Identify what kinds of edits are needed (addition, removal, modification, relocation, extraction, attribute change, relational update, etc.). - Determine reasoning-dependent components, such as spatial relationships, contextual cues, object interactions, or background logic. ## 2. IMAGE COMPARISON - Conduct detailed visual analysis of both images, focusing on: - **Object attributes**: shape, count, color, texture, size, and spatial position. - **Structural elements**: layout, perspective, lighting, and shadows. - **Human subjects**: facial expressions, posture, limb integrity, and count accuracy. - **Background consistency**: unchanged regions and contextual elements. - Document all observed differences at attribute level for precise evaluation. ## 3. REASONING POINTS INTERPRETATION - Combine the Editing Instruction and the Original Image to deduce the intended editing targets and logical objectives. - Use the provided Reasoning Points as reference checklist to clarify what edits and outcomes are expected for each reasoning step. ## 4. REASONING ACCURACY EVALUATION - Cross-check the implemented edits against the expected reasoning outcomes: - Are all required reasoning-based edits present and correctly applied? - Are spatial or contextual relationships accurately reflected in the result? - Do the edits follow real-world logic (e.g., lighting, physical feasibility, causality)? - For relational edits, verify whether dependent elements have been properly updated (e.g., object moved: original location should change). - Assess fine-grained accuracy: small positional errors, minor attribute inaccuracies, partial implementations. ## 5. SCORING - Start base_score = 10. - Deduct points according to the following guidance: - Assign **score = 0** only if no edit was made at all or the Edited Image is completely incorrect relative to the instruction. - **Missing edits**: Subtract 3 points per missing key point. - **Extra edits** (edits not requested): Subtract 3 points per extra edit. - **Reasoning errors**: - Reasoning is completely wrong and contradicts facts: subtract 3 points. - Each missing key reasoning point (from the internal checklist/Reasoning Points): subtract 2 points. - Reasoning is correct but ignores required effects on other objects: subtract 2 points. - Reasoning is correct and target object/position changed, but the original objects original location or state was not updated (i.e., duplicate added instead of moved): subtract 2 points. - Reasoning is correct but the implemented edit is inaccurate (small errors in position/attribute): subtract 2 points. - Full score (10) only if: all instructions and reasoning points are perfectly implemented, no extra edits exist, and all human-related changes are anatomically correct. - Compute strictly: start from base_score, subtract penalties, round half-up, and clamp to [0,10]. ## 6. FINAL OUTPUT - Provide single concise factual explanation summarizing correctness and key reasoning issues. - Output **only** the required JSON object with \"score\" and \"reason\" keys-no intermediate steps, reasoning traces, or extra text. # PROHIBITIONS - Do NOT assign score of 10 unless all edits and reasoning are fully correct, with no extra or missing edits. - Do NOT output step-by-step reasoning or verbose text. - Do NOT assume information not visually supported by evidence. - Do NOT ignore human or object count errors-penalize according to scoring rules. # INPUT **Editing instruction**: <instruction> **Reasoning points**: <reasoning_points>"
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University",
        "WeChat Vision, Tencent Inc.",
        "Xinjiang University",
        "Zhejiang University"
    ]
}