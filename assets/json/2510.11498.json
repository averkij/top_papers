{
    "paper_title": "ReLook: Vision-Grounded RL with a Multimodal LLM Critic for Agentic Web Coding",
    "authors": [
        "Yuhang Li",
        "Chenchen Zhang",
        "Ruilin Lv",
        "Ao Liu",
        "Ken Deng",
        "Yuanxing Zhang",
        "Jiaheng Liu",
        "Wiggin Zhou",
        "Bo Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Large Language Models (LLMs) excel at algorithmic code generation, they struggle with front-end development, where correctness is judged on rendered pixels and interaction. We present ReLook, an agentic, vision-grounded reinforcement learning framework that empowers an agent to close a robust generate--diagnose--refine loop by invoking a multimodal LLM (MLLM) as a tool. During training, the agent uses the MLLM-in-the-loop both as a visual critic--scoring code with screenshots--and as a source of actionable, vision-grounded feedback; a strict zero-reward rule for invalid renders anchors renderability and prevents reward hacking. To prevent behavioral collapse, we introduce Forced Optimization, a strict acceptance rule that admits only improving revisions, yielding monotonically better trajectories. At inference, we decouple the critic and run a lightweight, critic-free self-edit cycle, keeping latency comparable to base decoding while retaining most of the gains. Across three widely used benchmarks, ReLook consistently outperforms strong baselines in vision-grounded front-end code generation, highlighting the benefits of agentic perception, visual rewards, and training-inference decoupling."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . [ 1 8 9 4 1 1 . 0 1 5 2 : r ReLook: Vision-Grounded RL with Multimodal LLM Critic for Agentic Web Coding 2025-10-14 Yuhang Li1,, Chenchen Zhang1,,, Ruilin Lv2, Ao Liu1, Ken Deng2, Yuanxing Zhang3, Jiaheng Liu4, Wiggin Zhou1,, Bo Zhou1, 1LLM Department, Tencent 3Peking University 2Independent Researcher 4Nanjing University"
        },
        {
            "title": "Abstract",
            "content": "While Large Language Models (LLMs) excel at algorithmic code generation, they struggle with front-end development, where correctness is judged on rendered pixels and interaction. We present ReLook, an agentic, vision-grounded reinforcement learning framework that empowers an agent to close robust generatediagnoserefine loop by invoking multimodal LLM (MLLM) as tool. During training, the agent uses the MLLM-in-the-loop both as visual criticscoring code with screenshotsand as source of actionable, vision-grounded feedback; strict zero-reward rule for invalid renders anchors renderability and prevents reward hacking. To prevent behavioral collapse, we introduce Forced Optimization, strict acceptance rule that admits only improving revisions, yielding monotonically better trajectories. At inference, we decouple the critic and run lightweight, critic-free self-edit cycle, keeping latency comparable to base decoding while retaining most of the gains. Across three widely used benchmarks, ReLook consistently outperforms strong baselines in vision-grounded front-end code generation, highlighting the benefits of agentic perception, visual rewards, and traininginference decoupling."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) excel on closed-form benchmarksprogramming contests (Li et al., 2022), SQL synthesis (Liu et al., 2024), and mathematical reasoning (Yang et al., 2024; Deng et al., 2025)yet still underperform on front-end code generation, where visual fidelity and interaction are first-class. Unlike binary unit tests in algorithmic tasks, front-end quality lies on continuum: single misaligned pixel can signify failure. This perceptual barrier explains current shortcomings: text-only models are blind to pixel-level consequences, yielding (i) layout drift, (ii) interaction breakage, and (iii) aesthetic inconsistency. To address this, model must (1) see rendered HTML/CSS/JS/SVG, (2) diagnose misalignments and broken interactions, and (3) iteratively refine in situ. Existing methods miss this loop: one-shot vision-to-code systems (pix2code (Wust et al., 2024), Design2Code (Si et al., 2024), UICoder (Wu et al., 2024)) generate but do not refine; self-refinement frameworks (CodeRL (Le et al., 2022), Self-Refine (Madaan et al., 2023), Reflexion (Shinn et al., 2023), CRITIC (Gou et al., 2023; Peng et al., 2025; Zhang et al., 2025b)) iterate but cannot see, relying on pixel-blind unit tests or linters. To bridge this gap, we introduce ReLook, vision-grounded agentic reinforcement learning framework that completes the generatediagnoserefine loop. The agent actively invokes an MLLM as tool to see rendered outputs and obtain rich textual suggestions during inference, enabling true iterative refinement. Training uses comprehensive reward system: powerful MLLM (e.g., Qwen2.5-VL (Wang et al., 2024)) supplies the perceptual signal text-only methods lack, and rendering-integrity rule assigns zero reward when required screenshots are invalid to deter reward hacking. However, we identify critical challenge: behavioral collapse, where despite high-quality feedback, subsequent revision can be worse. We adopt Forced Optimization strategy that accepts only strictly improving steps, ensuring high-quality, monotonically improving trajectories. For low-latency inference, the external critic can be discarded; the model performs lightweight self-edit cyclerender, self-edit, and converge quickly to human-aligned result. Evaluator validity and choice. Our offline evaluation strictly follows the ARTIFACTSBENCH protocol(Zhang et al., 2025a). ARTIFACTSBENCH establishes evaluator validity through: (i) controlled human studies demonstrating over 90% agreement between MLLM judges (Gemini-2.5-Pro, Qwen2.5-VL-72B) and human experts, and (ii) strong ranking correlation with WebDev Arena(LMSYS Org, 2024), large-scale crowdsourced platform. Since our test sets are strict subsets of ARTIFACTSBENCHs evaluated tasks, we directly inherit this established human-alignment evidence. To further mitigate on-policy judge overfitting, we decouple the training-time critic (Qwen2.5-VL-72BInstruct) from the offline evaluator (Gemini-2.5-Pro). We do not conduct additional human studies; validity rests on ARTIFACTSBENCHs rigorous validation. See Appendix for detailed protocol adherence and cross-judge analysis. The first two authors contributed equally to this work. Corresponding authors. (cid:66) {adamwzhang,wigginzhou,chaysezhou}@tencent.com 1 Our contributions are as follows: Robust Reward System. We employ an MLLM as the reward model to provide the rich, pixel-level training signal that text-only methods cannot capture. This is critically supplemented by zero-reward rule for answers without screenshots, which is designed to prevent reward hacking by forcing the agent to produce renderable code. Agent reinforcement learning Framework. We empower the agent to perform generatediagnoserefine loop by actively invoking an MLLM as diagnostic tool. The agent can see its rendered output and receive rich, actionable feedback for iterative improvement. To ensure this powerful loop is productive and stable, we introduce Forced Optimization strategy that addresses the challenge of behavioral collapse by guaranteeing the construction of high-quality, monotonically improving rollout trajectories. Broad Applicability. We perform extensive experiments on three widely-used benchmark datasets, and demonstrate that ReLook significantly outperforms the baselines. Moreover, we show the compatibility of ReLook by integrating it with different LLMs."
        },
        {
            "title": "2 Method",
            "content": "Figure 1: Overview of ReLook. Left: training closes generatediagnoserefine cycle: policy LLM generates code, pages rendered to temporal screenshots, and vision-aware critic (MLLM) provides scores and feedback. Rewards combine visual scoring and format constraints; the policy is optimized with GRPO. Right: at inference the model runs lightweight Re-Look cycle external critic may be omitted for latency or used for higher accuracy. 2.1 Problem Formulation The task of front-end code generation is to produce code snippet c, consisting of SVG, HTML, CSS, and JavaScript, that correctly implements users intent specified in natural language question q. This code is typically preceded by textual chain-of-thought, t, which outlines the generation plan. As established in the introduction, the correctness of is not determined by its syntax alone, but by its rendered appearance and behavior. Given the absence of traditional unit tests for front-end code, we propose using an MLLM as the reward model to assess the perceptual quality of the output. Simultaneously, to enable the model to see its rendered results and make improvements, we design an agentic reinforcement learning framework that empowers the agent to invoke the MLLM as tool for obtaining vision-grounded suggestions for improvement. By doing so, we aim to enhance the front-end code generation capabilities of current LLMs. 2.2 Overall Framework The overall framework of the proposed ReLook is shown in Figure 1. At its core, our goal is to empower the agent to see its rendered output and iteratively improve upon it during inference. We achieve this by designing an agentic reinforcement learning framework that establishes generatediagnoserefine loop, where the agent learns to invoke an MLLM as diagnostic tool. To adjudicate the quality of each code revision, we institute comprehensive reward system centered on powerful MLLM. Crucially, to prevent the phenomenon of behavioral collapse, where optimizations paradoxically lead to inferior results, we introduce Forced Optimization strategy. This strategy refines rollouts to construct higher-quality trajectories, thereby instilling behavioral logic of monotonic improvement in the agent. Ultimately, this robust training allows the agent to internalize its reflective capabilities, enabling the MLLM critic to be discarded during inference to dramatically accelerate the process. 2.3 Iterative Reflection Mechanism For each query q, the policy emits and c. Upon <get feedback>, we execute in sandbox, capture screenshots, and query the MLLM for feedback (wrapped in <mllm feedback>). We feed {q, t, c, m} into the next round and stop when feedback is not requested or round cap is reached. The final output is represented as: = [t1 c1 m1 tR cR] where tr, cr, mr denote the r-th rounds text, code and feedback blocks, and is the total number of reflection rounds. The prompt template is provided in the appendix. (1) 2.4 Reinforcement Learning Framework To optimize this framework, we employ Group Relative Policy Optimization (GRPO) as our training algorithm, which is based on token-level policy gradient loss and is related to PPO (Schulman et al., 2017) while differing from preference-based objectives such as DPO (Rafailov et al., 2023). The objective is defined as follows: JGRPO(θ) = (cid:104) P(Q), {oi}G i=1 πθcombined (Oq) (cid:105) (cid:40) = 1 i=1 oi i=1 (cid:18) πθ(oi,tqi,t) (oi,tqi,t) Only tokens in t, contribute non-zero advantages; critic tokens are masked ( ˆAi,t=0 on m). (cid:20) πθ(oi,tqi,t) (oi,tqi,t) πθold (cid:21) (cid:19) , 1 ε, 1 + ε β DKL πθπre oi t=1 ˆAi,t, πθold ˆAi,t min clip (cid:105) (cid:104) (2) (cid:41) Advantage estimation and credit assignment. We sample trajectories per query and compute returns from RReLook(oi) (Eq. 5). Using the group mean as baseline, the advantage is Ai = RReLook(oi) b, broadcast to policy tokens (text t, code c) while masking critic tokens ( ˆAi,t=0 on m). Advantages are standardized and clipped to [2, 2]. We regularize toward πre with KL weight β. The combined policy πθcombined is defined as: [πθold for tr, cr] [πMLLM for mr], where πMLLM is frozen MLLM critic (Qwen2.5-VL-72B-Instruct). Trajectories mix policy tokens (tn, cn) and critic tokens (mn). During training, only πθ is updated. The hyperparameters ε and β control clipping range and KL regularization strength. Token masking and lightweight feedback distillation. We optimize GRPO over policy tokens (t, c) and mask critic tokens by zeroing advantages. To enable critic-free inference, we optionally distill m-tokens with lightweight loss Lm distill toward frozen MLLM outputs (see Appendix for details). The total objective is = t,c GRPO + γ Lm distill (default γ=0.1, sweep γ [0.05, 0.3]). This lets RL improve visual quality while distillation transfers feedback style for test-time self-reflection. 2.5 Reward Design Conventional RL signals for code, such as unit tests or linters, operate purely in the text domain and are blind to visual defects. To address this, our reward is derived from powerful Multimodal LLM (Qwen2.5-VL-72B-Instruct) that scores rendered pages based on the user prompt, the generated code, and series of temporal screenshots. Capturing screenshots at multiple time points allows the critic to assess dynamic behavior. Within each reflection round we capture three time points {S1, S2, S3} (e.g., post-load, +1s, +2s) and jointly evaluate them in single scoring call to obtain one round-level score. To properly credit incremental progress across rounds, we then average the round-level scores within trajectory. critical component of this design is safeguard against reward hacking, where malformed but syntactically plausible code might be over-rewarded. We enforce strict renderability constraint: if any required screenshot is invalid (e.g., due to render failure or timeout), the reward is zero. While this eliminates degenerate reward channels, it can lead to sparse rewards early in training. To mitigate this, we engineer all prompts to include visual-output constraint that explicitly instructs the agent to write executable HTML/CSS/JavaScript/SVG code suitable for browser rendering (see Appendix for full prompt template). This simple but effective technique increases the initial Valid Render Rate (from 40% at the start of training to 80% upon convergence) and better aligns the generation task with our vision-grounded reward. This reward function is formally defined as: RMLLM(o) = (cid:26)VisualScore(o) 0 if screenshot valid otherwise (3) 3 To discourage repetition, we apply linear length penalty from Lstart to Lend (12k and 14k tokens): Rlen(o) = 1 Lendlen(o) LendLstart if len(o) < Lstart if Lstart len(o) Lend if len(o) > Lend RReLook(o) = RMLLM(o) Rlen(o) (4) (5) The final training reward is: 2.6 Forced Optimization While the MLLM critic provides rich, detailed feedback, we observe critical instability we term behavioral collapse: despite high-quality suggestions, subsequent revision may score lower than the previous one, degrading trajectories. We therefore adopt Forced Optimization mechanism. We initially explored negative-reward penalty for regressions (rejecting worse-than-previous outcomes by penalizing returns), but found it incentivized the agent to reduce reflection frequency to avoid penalties, i.e., reward hacking. Hence, we discard the penalty design and enforce strict acceptance rule: refinement step (new tr+1, cr+1) is accepted only if its reward strictly exceeds the best-so-far in the trajectory (no ε margin or re-scoring). Non-improving steps are rejected and new attempt is sampled, with maximum of 10 resampling attempts per reflection round. If the limit is reached without improvement, we terminate further reflection for that trajectory and use the best-so-far result. This guarantees monotonically improving accepted trajectories and stabilizes learning without suppressing useful reflections. 2.7 Efficient Inference During training we retain the MLLM feedback loop for self-correction. At inference, we drop the external MLLM and run lightweight, critic-free self-edit (at most three rounds), with screenshots and MLLM calls disabled. This preserves most gains while substantially reducing latency, following train-slow, run-fast paradigm. See Appendix for pseudocode."
        },
        {
            "title": "3 Experiment",
            "content": "3.1 Experimental Settings Training Data Curation. We curate 3,000-task corpus of front-end-only tasks, normalize descriptions, and remove near-duplicates via lexical/DOM/code similarity. Prompts are audited to remove hints and sanitized; the final data are split into train/val stratified by UI archetypes. TrainTest De-duplication Protocol. To prevent leakage to ArtifactsBench, FullStack-Bench-Html and WebBench, we run instance-level, multi-view de-duplication before training: Lexical: TFIDF over char 3-grams; cosine > 0.85. DOM: tag-bigram Jaccard > 0.90; fallback tree-edit distance for edge cases. Code: token-set Jaccard > 0.90 after stripping comments/whitespace and minifying. If any criterion triggers, the instance is removed. Borderline cases (e.g., lexical in [0.80, 0.85] plus structural overlap) are manually reviewed. The same procedure purges intra-train near-duplicates. Dataset. We evaluate the performance of our method on three widely used datasets: ArtifactsBench (Zhang et al., 2025a), FullStack-Bench-Html (Cheng et al., 2024) and Web-Bench (Xu et al., 2025). ArtifactsBench contains 1,825 tasks focused on generating dynamic and interactive visual outputs, such as SVG visualizations and mini-games. Its evaluation protocol uses Multimodal LLM to score the visual fidelity and interactive integrity of the rendered code. ArtifactsBench utilizes Gemini-2.5-Pro as an expert judge to evaluate model outputs across its 1,825 tasks, demonstrating over 90% agreement with human evaluators. To manage evaluation costs, we conduct our experiments on six of its sub-datasets. We use the shorthand A-* for ArtifactsBench subsets: A-Lite (300 randomly sampled cases), A-Easy (305 simple frontend cases), A-Game (all 413 game-related cases), A-SVG (all 123 SVG-focused cases), A-Web (all 447 web-specific cases), and A-Si (all 75 simulation-oriented cases). FullStack-Bench-Html provides collection of front-end programming tasks where functional correctness is programmatically validated by passing suite of predefined unit tests. Web-Bench simulates realistic web development workflows through 50 complex projects of 20 sequential tasks each. Following its official protocol, performance is measured by passing end-to-end test cases that validate the final projects functionality, and we report the pass@2 rate. 4 Sandboxed Rendering Environment. We execute model-produced code within headless Chromiumbased renderer that is both deterministic and secure. This sandboxed environment operates at the OS level, with filesystem and network access disabled. Safety is further enhanced by blocking dangerous APIs (e.g., window.open, alert/confirm/prompt, eval/Function, clipboard access, non-local fetch/XMLHttpRequest/WebSocket), and enforcing per-sample wall-clock timeout. Requests to nonwhitelisted origins are intercepted and failed closed. To ensure determinism, all external resources like fonts and images are replaced with local fixtures; timers and animations use deterministic seeds; and we enforce strict Content Security Policy that disallows inline scripts and remote scripts. Within this controlled environment, our visual capture mechanism is dynamic: we take full-page screenshots that automatically adjust to the contents full dimensions, guaranteeing no elements are missed. To capture dynamic behavior, we record these screenshots at T=3 distinct time points (e.g., post-load, +1s, +2s), which are then passed to the MLLM critic to compute the reward signal. Evaluator validity and cross-judge robustness. We adopt the evaluation protocol from ARTIFACTSBENCH(Zhang et al., 2025a), which establishes validity through two key mechanisms: (i) controlled human study showing over 90% agreement between MLLM evaluators (Gemini-2.5-Pro and Qwen2.5-VL-72B) and human experts across diverse front-end tasks, and (ii) strong ranking correlation (Spearman ρ > 0.85) with WebDev Arena(LMSYS Org, 2024), large-scale crowdsourced evaluation platform. Critically, our test sets (A-Lite, A-Easy, A-Game, A-SVG, A-Web, A-Si) are strict subsets of ARTIFACTSBENCHs human-validated tasks, allowing us to directly inherit the established human-alignment evidence. Consistent with ARTIFACTSBENCH, we decouple the training-time critic (Qwen2.5-VL-72B-Instruct, open-source) from the offline evaluator (Gemini-2.5-Pro, proprietary). Cross-judge consistency and further details are summarized in Appendix B."
        },
        {
            "title": "3.2 Evaluation Protocol",
            "content": "We follow pixel-grounded evaluation protocol aligned with ARTIFACTSBENCH. For vision-based front-end tasks, models produce code that is executed in our sandboxed browser to capture temporal screenshots at three time points (post-load, +1s, +2s). An independent evaluator (Gemini-2.5-Pro) assigns VisualScore on the [0, 100] scale considering: (i) adherence to the textual specification, (ii) layout alignment and spatial fidelity, (iii) typography and color coherence, and (iv) interactive integrity when actions are specified. If no valid screenshot is produced, the score is set to zero. We report means over three runs (three random seeds) with identical decoding parameters. For FullStack-Bench-Html, we follow the benchmarks official unit-test protocol and report the pass rate under the same inference setup as other methods. For Web-Bench, we follow its official end-to-end evaluation and report pass@2 across projects. Unless otherwise stated, all reported metricsincluding Web-Bench pass@2are averaged over three runs. Decoding hyperparameters (temperature and top-p) are fixed across systems unless otherwise stated; local fixtures are cached to avoid network variance. Baselines and variants. Our experiments are conducted on two strong instruction-tuned base models: Qwen2.57B-Instruct and Llama-3.1-8B-Instruct. On each of these backbones, we compare three distinct approaches: (i) Base Model (the frozen instruction-tuned model, serving as direct baseline), (ii) Web-RL (vision-grounded RL using the MLLM reward but without the agentic reflection mechanism), and (iii) ReLook (our full framework with agentic MLLM-in-the-loop reflection). All methods use identical inference parameters, prompts, and rendering infrastructure for fair comparison. We also include results for GPT-4o (via OpenAI API) and Qwen2.5-32B-Instruct (local deployment) as reference points representing stronger base models; these are evaluated under the same protocol. We do not compare with prior specialized visual code generation methods (e.g., Design2Code(Si et al., 2024), UICoder(Wu et al., 2024)) because: (i) they were evaluated on different datasets without established crossbenchmark protocols, and (ii) official implementations are not publicly available for controlled comparison on our benchmarks. Our Web-RL baseline serves as strong vision-aware RL reference that isolates the contribution of agentic reflection. Manual validation (GSB). To complement automated evaluation, we conduct double-blind human study on 100 randomly sampled tasks comparing Qwen2.5-7B-ReLook against Qwen2.5-7B-Instruct. Five independent annotators directly run both systems code in our sandboxed renderer and select one of three labels: (ReLook better), (same), (ReLook worse). Majority voting aggregates per-task labels. Results are summarized as G:S:B = 50 : 30 : 20, indicating clear preference for ReLook under human judgment. Implementation Details We implement ReLook with grouped rollouts under GRPO. The model is trained for maximum of 40 steps with training batch size of 256. The learning rate is set to 1e-6, and we employ linear warmup (first 5%) and cosine decay schedule. For the GRPO loss function, we set the group size for rollouts per query to 8 and the clipping parameter ε to 0.2. We apply KL penalty toward reference policy with non-zero weight β and sweep β [0.01, 0.05] (step 0.01). We also sweep the advantage clipping bound in {1, 2, 3} for robustness. The group size and the number of sampled trajectories per query are chosen to fit accelerator memory A-Lite A-Easy A-Game A-SVG A-Web A-Si Model 25.30 25.73 Qwen2.5-32B-Instruct 31.44 33.25 GPT-4o 19.18 21.04 Llama-3.1-8B-Instruct 20.44 Llama-3.1-8B-Instruct-Web-RL 21.67 22.97 Llama-3.1-8B-Instruct-ReLook-w/o-MLLM 22.32 24.52 23.08 Llama-3.1-8B-Instruct-ReLook 18.61 21.59 Qwen2.5-7B-Instruct 18.82 24.89 Qwen2.5-7B-Instruct-Web-RL 22.15 25.44 Qwen2.5-7B-Instruct-ReLook-w/o-MLLM 26.36 27.88 Qwen2.5-7B-Instruct-ReLook 33.52 34.23 27.11 29.80 30.52 31.86 30.70 32.64 33.29 34.12 26.36 33.74 20.33 21.64 22.41 22.74 17.70 18.87 18.92 20.92 27.34 34.31 21.91 23.15 24.03 25.42 25.69 26.73 27.11 28.31 24.36 33.04 19.25 20.61 21.18 22.04 20.62 22.14 23.05 26.72 FullStack-Bench-Html Web-Bench 70.00 71.25 57.50 61.75 63.75 63.75 65.00 63.25 67.50 67.50 10.90 23.80 2.50 2.75 2.90 2.90 3.00 3.48 4.20 4.20 Table 1: Main results on ArtifactsBench subsets (A-Lite/Easy/Game/SVG/Web/Si), FullStack-Bench-Html, and Web-Bench (pass@2). VisualScores follow ARTIFACTSBENCH [0, 100] scale. ReLook uses up to 3 reflection rounds; ReLook-w/o-MLLM relies on internalized self-reflection without external critic. For unit-test benchmarks (FullStack, Web-Bench), both variants use identical critic-free inference, yielding same scores. Bold: best per backbone. Means over 3 seeds (temp=1.0, top-p=0.7). while maintaining adequate exploration; gradient accumulation is used to emulate larger effective batch sizes. Mixed precision (bfloat16/fp16) and activation checkpointing reduce memory footprint. For critic feedback tokens, GRPO is masked out; we optionally add lightweight distillation loss with weight γ on the feedback tokens to imitate the frozen MLLMs feedback style. Unless otherwise noted, we use γ=0.1 and sweep γ [0.05, 0.3] in sensitivity checks. For the length penalty, we set the bounds to Lstart = 12k and Lend = 14k. We use 64 GPUs (32 policy, 32 MLLM), 80 minutes per step, and curated corpus. Decoding is identical across systems (temp 1.0, top-p 0.7); results average three seeds. Prompts and sandbox are shared. We select checkpoints by mean VisualScore. We focus on 7B/8B backbones; larger-scale RL is future work. 3.3 Main Results Figure 2: Radar plot showing ReLooks consistent improvements across all ArtifactsBench subsets for both Qwen2.5-7B and Llama-3.1-8B backbones (averaged over 3 seeds). Figure 3: Performance on ArtifactsBench-Lite showing consistent ordering: ReLook > Web-RL > Base Model. Results averaged over 3 seeds. Table 1 and Figure 2 present the main results. ReLook achieves substantial improvements over both base models and Web-RL (vision-grounded RL without agentic reflection). Notably, ReLook-w/o-MLLMwhich relies solely on internalized reflection without external critic callsstill outperforms Web-RL, demonstrating successful internalization of the refinement mechanism with minimal inference overhead. Beyond absolute scores, we consistently observe the strictly monotone ordering predicted by our design: ReLook > Web-RL > Base Model. The gap between Web-RL and ReLook underscores the importance of vision-aware training signal coupled with the generatediagnoserefine loop. Qualitative examples are shown in Appendix Figure 6. Figure 3 summarizes performance on the ArtifactsBench-Lite subset. It exhibits the strictly monotone ordering ReLook > Web-RL > Base Model and highlights consistent gains for both Qwen2.5-7B and Llama-3.1-8B backbones, echoing the trends in Table 1. This compact subset mirrors the broader benchmark and provides an intuitive visualization of the average improvements delivered by ReLook. Figure 4 evidences behavioral collapse in the base model after 23 rounds despite feedback, while ReLook improves monotonically. We cap reflections at three rounds for efficiency and keep this at inference. Figure 5 shows that reflection frequency increases and stabilizes during training, aligning with Forced Optimizations incentive structure. The average converging to 2 rounds suggests most tasks reach capacity after two refinements. 6 Figure 4: Behavioral collapse mitigation. Base model (Qwen2.5-7B-Instruct) degrades after initial attempts despite MLLM feedback, while ReLook exhibits monotonic improvement across eight forced reflection rounds on ArtifactsBench-Lite. Scores from training-time judge (Qwen2.5-VL-72B). Figure 5: Intermediate Results of RL Training. The figure shows the average reward score on the validation set and the number of optimization steps during inference for our training of Relook using Qwen2.5-Instruct-7B as the base model. 3.4 Ablation Study We ablate three components: (i) vision-based MLLM reward, (ii) format constraint invalidating non-renderable outputs, and (iii) Forced Optimization. Table 2 shows each component is critical. Vision reward provides essential perceptual signal (+3.3 points). Format constraint prevents reward hacking (+1.0). Forced Optimization has the largest impact (+2.0), directly mitigating behavioral collapse. All ablations use identical seeds and the external evaluator to avoid bias."
        },
        {
            "title": "Format Constraint",
            "content": "Forced Optimization ArtifactsBench-Lite 21.59 24.89 25.84 27.88 Table 2: Ablation Study on ArtifactsBench-Lite. Results are averaged over 3 random seeds. 3.5 Inference speed improvement after removing MLLM We use VLLM to deploy the Qwen2.5-7B-Instruct model on four H20 GPUs and the Qwen2.5-VL-72B-Instruct model on eight H20 GPUs. We set the number of parallel threads to 1 and limit the maximum number of reflection steps to 3. We conduct inference on 100 queries and measure the average inference time per query. ReLook takes an average of 123.04 seconds per query, whereas ReLook-w/o-MLLM takes only 18.03 seconds. The results indicate that removing the screenshot and MLLM invocation mechanism substantially improves inference efficiency. 3.6 Error Analysis and Task-Level Performance We analyze ReLooks improvements across different task types to understand where vision-grounded RL is most effective. Visual-centric tasks (e.g., A-SVG, A-Game, A-Web) show the largest gains: ReLook improves by 3.2-6.1 points over base models on these subsets, as the MLLM critic directly addresses layout precision, color fidelity, and animation dynamicsfailures invisible to text-only methods. On A-Easy (simpler static pages), gains are modest (2.4-3.4 points), as base models already achieve reasonable outputs. The most challenging scenario remains complex, multi-file project tasks like Web-Bench, where ReLook shows improvement (40% relative gain for Qwen2.5-7B: 3.00 4.20 pass@2) but absolute performance stays low, indicating that long-horizon reasoning and cross-file dependencies require further architectural advances beyond single-artifact refinement. 3.7 Qualitative Analysis Appendix Figure 6 compares baseline and ReLook outputs. Non-vision baselines exhibit: (i) layout drift (misaligned components), (ii) interaction breakage (missing event listeners), and (iii) aesthetic inconsistency (clashing colors). ReLook mitigates these through render-aware refinements guided by visual critique."
        },
        {
            "title": "4 Related Work",
            "content": "4.1 Visual Code Generation Early vision-to-code systems translate static screenshots to HTML/CSS (Wust et al., 2024) in one shot. Recent methods add structureDesign2Code (Si et al., 2024), Web2Code (Yun et al., 2024), UICoder (Wu et al., 2024), DesignCoder (Chen et al., 2025)but mainly optimize static similarity, struggling with dynamics and iterative refinement. ReLook trains with renderer in the loop, uses vision-grounded rewards from temporal screenshots, and internalizes refinement via RL, aligning with cross-modal supervision (Feng et al., 2022). 4.2 Feedback-Driven Code Reinforcement Learning RL for program synthesis leverages unit-test rewards and large candidate sets (Le et al., 2022; Li et al., 2022); reflective/tool-driven critiques improve correction (Madaan et al., 2023; Shinn et al., 2023; Gou et al., 2023; Peng et al., 2025). For front-end, unit tests are pixel-blind; even with structured visual instructions (Yun et al., 2024), pixel signals are missing. LLM critics help surface model errors (McAleese et al., 2024). We couple the policy with visual reward from temporal screenshots and stabilize training via Forced Optimization and zero-reward for invalid renders. Acceptance criteria, best-of-N, and verifier-assisted search. broad line of work improves generation via external selection/search: Codex/AlphaCode sample-and-test (Chen et al., 2021; Li et al., 2022); self-consistency/tree deliberation aggregate candidates (Wang et al., 2022; Yao et al., 2023); agentic self-refinement uses iterative critique (Madaan et al., 2023; Shinn et al., 2023; Gou et al., 2023; Huang et al., 2025). Our Forced Optimization differs in both criterion and rule: vision-grounded score from temporal screenshots (not unit tests), and acceptance strictly requiring monotonic improvement within one trajectory. Unlike best-of-N or offline re-ranking, our rule is online, in-trajectory, preventing regressions and reward hacking, yielding stable, visually aligned improvements for front-end code. 4.3 Multimodal UI Perception and Evaluation Recent MLLMs ground web elements and layouts (OpenAI, 2023; Wang et al., 2024); web-agent/GUI benchmarks show the value of vision-conditioned reasoning (Zhou et al., 2023; Koh et al., 2024; Li et al., 2025). For evaluation, MLLM-as-Judge is common (Ge et al., 2023; Zheng et al., 2023); front-end benchmarks emphasize visual and interactive quality (Xu et al., 2025; Zhang et al., 2025a). Accordingly, we place visual scoring in training to internalize layout/interaction principles, and drop the critic at inference for latency. ReLook unifies these threads via MLLM-based visual rewards within RL, offering practical path to visually aware, self-improving front-end generation."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduced ReLook, vision-grounded RL framework that closes generatediagnoserefine loop for front-end code. By coupling multimodal LLM critic with two safeguardszero-reward for invalid renders and Forced OptimizationReLook achieves consistent gains over strong baselines (ReLook > Web-RL > Base) and enables critic-free inference for substantial speedups. We expect this blueprintplacing perception-aligned evaluator inside the learning loopto generalize beyond web UIs to other perceptual programming domains. See Appendix for limitations and future directions."
        },
        {
            "title": "References",
            "content": "Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Yunnong Chen, Shixian Ding, YingYing Zhang, Wenkai Chen, Jinzhou Du, Lingyun Sun, and Liuqing Chen. Designcoder: Hierarchy-aware and self-correcting ui code generation with large language models. arXiv preprint arXiv:2506.13663, 2025. Bytedance-Seed-Foundation-Code-Team Yao Cheng, Jianfeng Chen, Jie Chen, Li Chen, Liyu Chen, Wentao Chen, Zhengyu Chen, Shijie Geng, Aoyan Li, Bowen Li, Bowen Li, Linyi Li, Boyi Liu, Jerry Liu, Kaibo Liu, Qi Liu, Shukai Liu, Si-Han Liu, Tianyi Liu, Tingkai Liu, Yongfei Liu, Rui Long, Jing Mai, Guanghan Ning, Zhongyan Peng, Kai Shen, Jiahao Su, Jing Su, Tao Sun, Yifan Sun, Yu Tao, Guoyin Wang, Siwei Wang, Xuwu Wang, Yite Wang, Zihan Wang, Jinxiang Xia, Liang Xiang, Xianzhong Xiao, Yongsheng Xiao, Chenguang Xi, Shulin Xin, Jingjing Xu, Shi-Bo Xu, Hongxia Yang, Jack Yang, Yingxiang Yang, Jian-Ming Yuan, Jun Zhang, Yufeng Zhang, Yuyu Zhang, Shen Zheng, He Zhu, and Ming Zhu. Fullstack bench: Evaluating llms as full stack coders. ArXiv, abs/2412.00535, 2024. URL https://api.semanticscholar.org/CorpusID:274437428. Ken Deng, Zizheng Zhan, Wen Xiang, Wen ya Zhu, Tianhao Peng, Xinping Lei, Weihao Li, Jingxuan Xu, Kun Wu, Yifan Yao, Haoyang Huang, Huaixi Tang, Kepeng Lei, Zhiyi Lai, Songwei Yu, Zongxian Feng, Zuchen Gao, Weihao Xie, Chenchen Zhang, Yanan Wu, Yuanxing Zhang, Lecheng Huang, Yuqun Zhang, Jie Liu, Zhaoxiang Zhang, Haotian Zhang, Bin Chen, and Jiaheng Liu. Hipo: Hybrid policy optimization for dynamic reasoning in llms. 2025. URL https://api.semanticscholar.org/CorpusID:281676175. Weixin Feng, Xingyuan Bu, Chenchen Zhang, and Xubin Li. Beyond bounding box: Multimodal knowledge learning for object detection. arXiv preprint arXiv:2205.04072, 2022. Wentao Ge, Shunian Chen, Guiming Hardy Chen, Junying Chen, Zhihong Chen, Nuo Chen, Wenya Xie, Shuo Yan, Chenghao Zhu, Ziyue Lin, et al. Mllm-bench: evaluating multimodal llms with per-sample criteria. arXiv preprint arXiv:2311.13951, 2023. Zhibin Gou, Zhihong Shao, Yeyun Gong, Yelong Shen, Yujiu Yang, Nan Duan, and Weizhu Chen. Critic: Large language models can self-correct with tool-interactive critiquing. arXiv preprint arXiv:2305.11738, 2023. Hui Huang, Yancheng He, Hongli Zhou, Rui Zhang, Wei Liu, Weixun Wang, Wenbo Su, Bo Zheng, and Jiaheng Liu. Think-j: Learning to think for generative llm-as-a-judge. arXiv preprint arXiv:2505.14268, 2025. Jing Yu Koh, Robert Lo, Lawrence Jang, Vikram Duvvur, Ming Chong Lim, Po-Yu Huang, Graham Neubig, Shuyan Zhou, Ruslan Salakhutdinov, and Daniel Fried. Visualwebarena: Evaluating multimodal agents on realistic visual web tasks. arXiv preprint arXiv:2401.13649, 2024. Hung Le, Yue Wang, Akhilesh Deepak Gotmare, Silvio Savarese, and Steven Chu Hong Hoi. Coderl: Mastering code generation through pretrained models and deep reinforcement learning. Advances in Neural Information Processing Systems, 35:2131421328, 2022. Kaixin Li, Ziyang Meng, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, and Tat-Seng Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378 (6624):10921097, 2022. Xinyu Liu, Shuyu Shen, Boyan Li, Peixian Ma, Runzhi Jiang, Yuxin Zhang, Ju Fan, Guoliang Li, Nan Tang, and Yuyu Luo. survey of nl2sql with large language models: Where are we, and where are we going? arXiv preprint arXiv:2408.05109, 2024. LMSYS Org. Chatbot Arena Leaderboard. https://web.lmarena.ai/leaderboard, 2024. Accessed: 2024-05-23. Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024. 9 OpenAI. Gpt-4v(ision) system card. https://cdn.openai.com/papers/GPTV_System_Card.pdf, 2023. Zhongyuan Peng, Yifan Yao, Kaijing Ma, Shuyue Guo, Yizhe Li, Yichi Zhang, Chenchen Zhang, Yifan Zhang, Zhouliang Yu, Luming Li, et al. Criticlean: Critic-guided reinforcement learning for mathematical formalization. arXiv preprint arXiv:2507.06181, 2025. Raphael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Chelsea Finn, and Christopher D. Manning. Direct preference optimization: Your language model is secretly reward model. In Advances in Neural Information Processing Systems, 2023. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. In Proceedings of the 34th International Conference on Machine Learning Workshop, 2017. Noah Shinn, Federico Cassano, Ashwin Gopinath, Karthik Narasimhan, and Shunyu Yao. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Chenglei Si, Yanzhe Zhang, Ryan Li, Zhengyuan Yang, Ruibo Liu, and Diyi Yang. Design2code: Benchmarking multimodal code generation for automated front-end engineering. arXiv preprint arXiv:2403.03163, 2024. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc V. Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Jason Wu, Eldon Schoop, Alan Leung, Titus Barik, Jeffrey Bigham, and Jeffrey Nichols. Uicoder: Finetuning large language models to generate user interface code through automated feedback. arXiv preprint arXiv:2406.07739, 2024. Antonia Wust, Wolfgang Stammer, Quentin Delfosse, Devendra Singh Dhami, and Kristian Kersting. Pix2code: Learning to compose neural visual concepts as programs. arXiv preprint arXiv:2402.08280, 2024. Kai Xu, YiWei Mao, XinYi Guan, and ZiLong Feng. Web-bench: llm code benchmark based on web standards and frameworks. arXiv preprint arXiv:2505.07473, 2025. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Karthik Narasimhan, and Yuan Cao. Tree of thoughts: Deliberate problem solving with large language models. arXiv preprint arXiv:2305.10601, 2023. Sukmin Yun, Haokun Lin, Rusiru Thushara, Mohammad Qazim Bhat, Yongxin Wang, Zutao Jiang, Mingkai Deng, Jinhong Wang, Tianhua Tao, Junbo Li, et al. Web2code: large-scale webpage-to-code dataset and evaluation framework for multimodal llms. arXiv preprint arXiv:2406.20098, 2024. Chenchen Zhang, Yuhang Li, Can Xu, Jiaheng Liu, Ao Liu, Shihui Hu, Dengpeng Wu, Guanhua Huang, Kejiao Li, Qi Yi, et al. Artifactsbench: Bridging the visual-interactive gap in llm code generation evaluation. arXiv preprint arXiv:2507.04952, 2025a. Chenchen Zhang, Jinxiang Xia, Jiaheng Liu, Wei Zhang, Yejie Wang, Jian Yang, Ge Zhang, Tianyu Liu, Zhongyuan Peng, Yingshui Tan, Yuanxing Zhang, Zhexu Wang, Weixun Wang, Yancheng He, Ken Deng, Wangchunshu Zhou, Wenhao Huang, and Zhaoxiang Zhang. Codecriticbench: holistic code critique benchmark for large language models, 2025b. URL https://arxiv.org/abs/2502.16614. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. Shuyan Zhou, Frank Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, et al. Webarena: realistic web environment for building autonomous agents. arXiv preprint arXiv:2307.13854, 2023."
        },
        {
            "title": "A Limitations and Future Work",
            "content": "Despite strong empirical gains, ReLook has several limitations. First, the reliance on large MLLM critic during training increases compute and monetary cost; although we show critic-free inference, reducing trainingtime overhead via distillation or lighter judges remains future work. Second, our sandboxed renderer improves determinism but may under-represent real-world variability across devices, locales, and resource conditions, leaving robustness gaps. Third, rewards are mediated by an external evaluator and length penalty; metric drift and sensitivity to prompt templates could bias optimization. Fourth, the Forced Optimization constraint stabilizes training but might reduce exploration in challenging cases. Fifth, as shown in our error analysis (Section 3), complex multi-file project tasks (e.g., Web-Bench) remain challenging, indicating that long-horizon reasoning beyond single-artifact refinement requires further architectural innovation. Finally, our experiments focus on 7B/8B-scale models and front-end code; scaling to larger models and broader software stacks requires additional investigation. We follow and inherit ARTIFACTSBENCHs cross-judge and human-alignment evidence rather than re-running additional human studies in this work. To facilitate reproducibility and future research, we plan to release our code upon publication. External validity and cross-judge analysis We align our evaluator setup with ARTIFACTSBENCH(Zhang et al., 2025a), which establishes validity through rigorous empirical validation: Human-MLLM Agreement. ARTIFACTSBENCH conducted controlled human study with expert web developers evaluating stratified sample of 200 tasks. Results showed over 90% agreement (Cohens κ > 0.85) between human judgments and MLLM evaluators (Gemini-2.5-Pro and Qwen2.5-VL-72B) on visual fidelity, layout correctness, and interactive integrity. The study used double-blind protocol with three independent human raters per task. Crowdsourced Validation. Beyond controlled studies, ARTIFACTSBENCH validated MLLM judges against WebDev Arena(LMSYS Org, 2024), large-scale platform with over 10,000 pairwise comparisons from web developers. The MLLM rankings showed strong correlation (Spearman ρ = 0.87 for Gemini-2.5-Pro, ρ = 0.83 for Qwen2.5-VL-72B) with crowd preferences. Our Protocol Adherence. Since our test sets (A-Lite, A-Easy, A-Game, A-SVG, A-Web, A-Si) are strict subsets of ARTIFACTSBENCHs 1,825 human-validated tasks, we directly inherit this established validity. We use the official evaluation scripts, judge prompts, and scoring rubric without modification. To mitigate on-policy overfitting, we decouple training-time critic (Qwen2.5-VL-72B-Instruct) from offline evaluator (Gemini-2.5-Pro). We do not re-run human studies; validity is anchored in ARTIFACTSBENCHs reported evidence. Score Interpretation. The absolute VisualScore range (20-30 for 7B/8B models) reflects benchmark difficulty: ARTIFACTSBENCH tasks span complex interactions, dynamic animations, and pixel-perfect layouts. Reference models (GPT-4o: 33, Qwen2.5-32B: 26) establish that even frontier systems find these tasks challenging. The [0, 100] scale provides fine-grained discrimination; we report relative improvements over baselines."
        },
        {
            "title": "D Evaluator rubric and scoring range",
            "content": "We use fixed evaluation rubric for VISUALSCORE. During training, the critics visual score used for RL is normalized to [0, 1] for stability. For offline reporting and tables, we follow ARTIFACTSBENCH and report evaluator outputs on [0, 100] scale; all numbers in the main results tables and figures are on this [0, 100] scale unless otherwise specified. The rubric jointly considers: (i) specification adherence; (ii) layout alignment and spatial fidelity; (iii) typography and color coherence; and (iv) interactive integrity for tasks specifying actions. For dynamic behavior, each reflection round is evaluated on three temporal screenshots jointly in single scoring call to obtain one round-level score; trajectory-level reward averages round-level scores. We do not perform multi-judge averaging or smoothing."
        },
        {
            "title": "E The loss function for Forced Optimization",
            "content": "Faced with the behavior collapse problem, we first attempted negative-reward penalty comparing post-feedback scores to pre-feedback scores. If an optimized answer was worse than its predecessor, negative reward was applied. We observed that this encouraged the model to reduce reflection frequency to avoid penalties (reward hacking), 11 Algorithm 1 ReLook Training Framework Require: Task specification q, base policy πθ, vision critic MLLM (Qwen2.5-VL-72B), max steps S, max reflections R, max resampling K=10. Ensure: Optimized policy π 1. Initialize: 0, history [q]. 2. While < S: θ with internalized visual cognition. (a) Initialize: 0, ϵ, sprev 1. (b) While (r < and room for improvement): i. 0, accepted False. ii. While (k < and not accepted): (Forced Optimization: resample up to times) A. Generate: tr, cr πθ(next token history). B. Extract code cr from <answer> block. C. Attempt rendering cr capture screenshots {S1, S2, S3}. D. Generate feedback mr MLLM(q, cr, {Si}). E. Compute visual score sr VisualScore(q, cr, {Si}). F. If sr > sprev (accept only strictly improving steps): append tr, cr to history and o; append mr to history and within <mllm feedback>; set sprev sr, accepted True. G. + 1. iii. If not accepted: break. (Terminate further reflections, use best-so-far) iv. + 1. (c) Calculate reward components for the trajectory o: RMLLM(o) = (cid:26)VisualScore(o) 0 if screenshot valid otherwise Rlen(o) = 1 Lend len(o) Lend Lstart 0 if len(o) < Lstart if Lstart len(o) Lend if len(o) > Lend RReLook(o) = RMLLM(o) Rlen(o) (d) Perform policy update using GRPO to optimize parameters θ based on RReLook. (e) If convergence criterion is met (no significant improvement), exit loop. (f) + 1. degrading ReLook into regular RL and harming performance. Therefore, we removed this mechanism and adopted the strict acceptance rule described in Method: only strictly improving steps are accepted into trajectories."
        },
        {
            "title": "F Lightweight distillation loss on feedback tokens",
            "content": "Let denote the index set of critic feedback tokens within trajectory, and let the frozen MLLM critic define teacher distribution qt() at each position M. The student policy distribution is pθ( xt). The lightweight distillation loss used to transfer the feedback style for test-time self-reflection is Lm distill := 1 tM KL(cid:0)qt() pθ( xt)(cid:1) equivalently, as teacher-forced cross-entropy Lm distill := 1 tM qt(v) log pθ(v xt) (6) (7) Only policy-controlled tokens (t, c) contribute non-zero advantages in GRPO; feedback tokens are masked in the RL objective (advantages set to zero). The total training objective is = t,c GRPO + γ Lm distill, with default γ=0.1, γ [0.05, 0.3]. (8) Reproducibility notes: GRPO and implementation details Reference policy and KL. We regularize toward fixed reference policy πre (the frozen instruction-tuned backbone before RL). KL is computed token-wise over policy-controlled tokens with weight β (see ranges in Experiment); we do not anneal β within step. Trajectory composition and masking. Trajectories mix policy tokens (t, c) and critic tokens (m). During optimization, advantages on are masked to zero; optional lightweight distillation on uses small KL/CE with weight γ to the frozen MLLM outputs. Advantage baseline and clipping. We use group-relative baseline (mean return over trajectories per query); advantages are standardized within-batch and clipped to [2, 2]. Sampling limits and rejection handling. For Forced Optimization, we cap resampling attempts at 10 per reflection round to avoid infinite loops; non-improving proposals are rejected without re-scoring. When the 10attempt limit is reached without improvement, we terminate further reflection rounds for that trajectory and use the best-so-far result. This limit balances exploration (allowing sufficient attempts to find improving revisions) with computational efficiency. Length penalty parameters. We set Lstart = 12,000 and Lend = 14,000 tokens based on empirical analysis of typical front-end code lengths in our training corpus, ensuring reasonable generation length while discouraging degenerate repetition. Valid Render Rate dynamics. During training, the Valid Render Rate increases from approximately 40% at initialization to approximately 80% upon convergence, demonstrating that the model learns to produce syntactically valid and renderable code through the combination of the visual-output constraint in prompts and the zero-reward penalty for invalid renders. Decoding and seeds. Unless otherwise noted, decoding uses identical hyperparameters across systems (temperature=1.0, top-p=0.7) with three random seeds; fixtures are cached to avoid network variance. These notes complement Section 2 and Section 3 to facilitate faithful reimplementation."
        },
        {
            "title": "H Qualitative Analysis",
            "content": "Figure 6 presents qualitative comparison between the baseline model and ReLook across several representative front-end generation tasks. Prompt column lists natural-language instructions of varying complexity, including layout composition, template library rendering, login/registration forms, and chessboard. 13 Base Model column shows the outputs of an instruction-tuned baseline. While it can produce code that renders without error, the resulting webpages often suffer from issues such as layout drift, incomplete functionality, and lack of visual coherence (e.g., missing interactivity in the login page, overly simplistic rendering of the chessboard). ReLook column demonstrates the effect of our vision-grounded reinforcement learning framework. By incorporating visual feedback into training, ReLook produces outputs that are not only executable but also visually faithful and functionally aligned with the prompts. For instance, the template library page is correctly populated with clickable cards, the login/registration form has clean layout and interactive elements, and the chessboard renders with precise alignment. Overall, the comparison highlights how ReLook systematically reduces layout drift, strengthens interaction correctness, and achieves higher visual fidelity compared to the baseline. Figure 6: Visual Comparison of Frontend Websites Generated by Baseline and ReLook."
        },
        {
            "title": "I Template prompt for ReLook rollout",
            "content": "Solve the following problem step by step. You now have the ability to selectively write executable HTML, CSS, JavaScript, or SVG code to receive feedback from the multimodal large model on the code. The code you provided will be executed, and the feedback (wrapped in <mllm feedback> output str <mllm feedback>) can be returned to aid your reasoning and help you arrive at the final answer. Unless you believe the current answer is flawless, please output <get feedback> after providing the complete answer to receive feedback from the multimodal large model and improve the code based on the feedback. *user question:* {$query} Figure 7: Template prompt for ReLook rollout."
        }
    ],
    "affiliations": [
        "Independent Researcher",
        "LLM Department, Tencent",
        "Nanjing University",
        "Peking University"
    ]
}