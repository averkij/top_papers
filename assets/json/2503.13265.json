{
    "paper_title": "FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View Synthesis",
    "authors": [
        "Luxi Chen",
        "Zihan Zhou",
        "Min Zhao",
        "Yikai Wang",
        "Ge Zhang",
        "Wenhao Huang",
        "Hao Sun",
        "Ji-Rong Wen",
        "Chongxuan Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generating flexible-view 3D scenes, including 360{\\deg} rotation and zooming, from single images is challenging due to a lack of 3D data. To this end, we introduce FlexWorld, a novel framework consisting of two key components: (1) a strong video-to-video (V2V) diffusion model to generate high-quality novel view images from incomplete input rendered from a coarse scene, and (2) a progressive expansion process to construct a complete 3D scene. In particular, leveraging an advanced pre-trained video model and accurate depth-estimated training pairs, our V2V model can generate novel views under large camera pose variations. Building upon it, FlexWorld progressively generates new 3D content and integrates it into the global scene through geometry-aware scene fusion. Extensive experiments demonstrate the effectiveness of FlexWorld in generating high-quality novel view videos and flexible-view 3D scenes from single images, achieving superior visual quality under multiple popular metrics and datasets compared to existing state-of-the-art methods. Qualitatively, we highlight that FlexWorld can generate high-fidelity scenes with flexible views like 360{\\deg} rotations and zooming. Project page: https://ml-gsai.github.io/FlexWorld."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 5 6 2 3 1 . 3 0 5 2 : r FlexWorld: Progressively Expanding 3D Scenes for Flexiable-View Synthesis Luxi Chen1,2,, Zihan Zhou1,2,, Min Zhao3, Yikai Wang4, Ge Zhang5, Wenhao Huang5, Hao Sun1, Ji-Rong Wen1, Chongxuan Li1,2, 1Gaoling School of AI, Renmin University of China, 2Beijing Key Laboratory of Big Data Management and Analysis Methods, 3Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch MLCenter, Tsinghua University, 4School of Artificial Intelligence, Beijing Normal University, 5ByteDance Seed Equal Contribution, Corresponding author"
        },
        {
            "title": "Abstract",
            "content": "Generating flexible-view 3D scenes, including 360 rotation and zooming, from single images is challenging due to lack of 3D data. To this end, we introduce FlexWorld, novel framework consisting of two key components: (1) strong video-to-video (V2V) diffusion model to generate high-quality novel view images from incomplete input rendered from coarse scene, and (2) progressive expansion process to construct complete 3D scene. In particular, leveraging an advanced pre-trained video model and accurate depth-estimated training pairs, our V2V model can generate novel views under large camera pose variations. Building upon it, FlexWorld progressively generates new 3D content and integrates it into the global scene through geometry-aware scene fusion. Extensive experiments demonstrate the effectiveness of FlexWorld in generating highquality novel view videos and flexible-view 3D scenes from single images, achieving superior visual quality under multiple popular metrics and datasets compared to existing state-of-the-art methods. Qualitatively, we highlight that FlexWorld can generate high-fidelity scenes with flexible views like 360 rotations and zooming. Project page: https://ml-gsai.github.io/FlexWorld."
        },
        {
            "title": "Introduction",
            "content": "Creating 3D scene with flexible views from single image holds transformative potential for applications where direct 3D data acquisition is costly or impractical, such as archaeological preservation and autonomous navigation. However, this task remains fundamentally ill-posed: single 2D observation provides insufficient information to disambiguate the complete 3D structure. In particular, when extrapolating to extreme viewpoints (e.g., 180 rotations), previously occluded or entirely absent content may emerge, introducing significant uncertainty. Generative models, particularly diffusion models [16, 42, 43], offer principled and effective solution to this problem. While existing methods often rely on pre-trained generative models as priors for novel view synthesis, they face notable limitations. Image-based diffusion methods [9, 50, 64, 65] tend to accumulate geometric errors, whereas video-based diffusion approaches [13, 55] struggle with dynamic content and poor camera supervision. Recent attempts [33, 66] to incorporate point cloud priors for improved consistency have shown 1 (a) Input Videos generated by our V2V model given camera trajectories (b) Input Flexible-view 3D scene generated by FlexWorld Figure 1 FlexWorld generates high-quality videos with camera control and flexible-view 3D scenes progressively. (a) FlexWorld introduces V2V diffusion producing high-quality videos from incomplete scene renderings given diverse camera trajectories with large variation. (b) FlexWorld progressively generates flexible-views (e.g., 360 rotations and zooming) 3DGS scenes via the V2V diffusion. promise but remain limited in scalability, often failing under large viewpoint changes. To this end, we propose FlexWorld for flexible-view 3D scene generation from single images. In contrast to existing methods [29, 44, 67], FlexWorld progressively expands persistent 3D representation by synthesizing and integrating novel 3D content. FlexWorld consists of two key components: (1) strong video-to-video (V2V) diffusion model to generate complete view images from incomplete ones rendered from coarse scenes, and (2) geometry-aware 3D scene expansion process, which extracts new 3D content and integrates it to the global structure. In particular, we fine-tuned an advanced video foundation model [62] on accurate depth-estimated training pairs so that it can generate high-quality content under large camera variations. Built upon the V2V model, the scene expansion process employs camera trajectory planning, scene integration, and refinement process to progressively construct detailed 3D scene from single image. Our extensive experiments show the effectiveness of FlexWorld in both high-quality video and flexible-view 3D scene synthesis. In particular, our V2V model achieves superior visual quality compared to the current state-of-the-art baselines [13, 33, 44, 55, 66] while maintaining excellent camera controllability across multiple benchmarks [23, 72] (see Tab. 1). similar conclusion holds for the 3D scene generation benchmarks (see Figure 2 Overview of FlexWorld. FlexWorld trains strong V2V diffusion capable of generating high-quality videos from incomplete views rendered from coarse 3D scenes. It progressively expands the 3D scene by adding new 3D content estimated from the refined videos via dense stereo model. Ultimately, from single image, it yields detailed 3D scene capable of rendering flexible viewpoints. Tab. 2). In addition, FlexWorld enables the synthesis of 3D scenes with flexible view in high-fidelity (see Fig. 1), consistent with our quantitative results. In summary, our key contributions are: We introduce FlexWorld, progressive framework for flexible-view scene generation that utilizes consistent video sequences to expand and construct scenes. We present video-to-video diffusion model fine-tuned on carefully designed training data, enabling the generation of novel views under large camera variations while ensuring consistency with the existing scene. FlexWorld exhibits superior performance in video and scene generation compared with baseline models on various benchmark datasets [23, 72]."
        },
        {
            "title": "2 Related work",
            "content": "2.1 3D scene generation With the emergence of 3D representations that enable differentiable rendering [21, 35], 3D object generation from single texts/images has advanced rapidly [7, 18, 27, 37, 46, 47, 53, 57, 60], closely followed by advancements in 3D scene generation. Several works [9, 17, 41, 50, 64, 65, 68] employ image diffusion models [36, 39] for novel view synthesis and monocular depth estimation [3, 5, 20, 38] to derive 3D structures for corresponding views. Another line of the work [6, 8, 10, 28, 45, 61, 69] involves training network to obtain 3D representation from single or sparse images directly. Recent studies have integrated camera control into image [11, 33, 40, 56] or video models [29, 31, 44, 66, 67] to facilitate the generation of novel views, subsequently performing 3D reconstruction [26, 51] to obtain representations of 3D scenes. However, these models often struggle to generate novel views under significant view change, which limits the flexibility in generating scene viewpoints. By fine-tuning V2V model on carefully designed training data, FlexWorld can generate novel views under 3 ] 6 6 [ f w ] 3 3 [ 3 s View 45.0 View 52.5 View 165.0 View 172.5 Figure 3 We improve our video diffusion model to enable generating 3D consistent videos under large camera variation. We present novel views generated from each model when the camera is rotated 180 degrees to the left. The red bounding box indicates 3D inconsistency or poor visual quality in the generated content. Our model generates higher quality and more consistent static 3D scenes. significant variations in camera trajectories, thereby facilitating the creation of flexible-view 3D scenes."
        },
        {
            "title": "2.2 Camera-controlled video diffusion models",
            "content": "Recently, Camera-controlled video diffusion models have received widespread attention. Several works [1, 2, 13, 55, 59] explore the generation of videos under camera conditions. However, these models are not designed for static scene generation, as the dynamics in the generated videos hinder reconstruction. DimensionX [44] achieves basic camera control via several LoRAs [19] but lacks flexibility in complex movements. Wonderland [29] and StarGen [67] can generate videos from single views and camera trajectories; however, they are unable to produce new videos to complement existing 3D structures, restricting the range of generated scenes. See3D [33] and ViewCrafter [66] can accept missing information from specific scene perspectives and perform completion, but they struggle to accommodate significant perspective changes (see Fig. 3). In contrast, we propose training V2V model on more advanced video foundation model, leveraging existing scene information to enable large camera variation and offering powerful tool for flexible view scene generation."
        },
        {
            "title": "3 Method",
            "content": "In this section, we will first introduce the preliminary for FlexWorld. Subsequently, we will present our flexible-view 3D scene generation framework in Sec. 3.2. Finally, we will discuss our improved V2V model that supports our framework in Sec. 3.3."
        },
        {
            "title": "3.1 Preliminaries\nVideo diffusion model. A diffusion model [16, 42, 43] consists of a forward and a denoising process. In the\nforward process, the diffusion model gradually adds Gaussian noise to a clean image x0 from time 0 to T .\nThe noisy image xt at a certain time t ∈ [0, T ] can be expressed as xt = αtx0 + σtϵ, where αt and σt are\npredefined hyperparameters. In the denoising process, a noise predictor ϵθ(xt, t) with parameters θ is trained\nto predict noise in xt for generation. Given the corresponding condition y for x, the training objective of a\ndiffusion model is:",
            "content": "EtU (0,1),ϵN (0,I) (cid:2)ϵθ(xt, t; y) ϵ2 2 (cid:3) . min θ (1) 4 Recent video diffusion models [4, 12, 24, 32, 62, 71] typically employ 3D-VAE [22] encoder to compress the source video into latent space where the diffusion model is trained. The generated latent video is subsequently decoded to the pixel space using the corresponding decoder D. Dense stereo model. The dense stereo models [26, 48, 51], e.g., DUSt3R [51] and MASt3R [26], provide an advanced tool for obtaining corresponding point maps, depth maps, and camera parameters from single or sparse views, facilitating the reconstruction of 3D point clouds. This approach offers means to derive rough 3D structure and camera estimation from single image. 3D Gaussian splatting. As currently one of the most popular 3D representations, 3D Gaussian splatting (3DGS) [21] models the 3D scene by multiple 3D Gaussians parameterized by colors, centers, opacities, scales, and rotation quaternions. The effectiveness and efficiency of 3DGS in 3D reconstruction and generation have been widely demonstrated [68, 21, 46, 47]. In addition to the L1 loss and SSIM loss LSSIM [54] presented in the original paper [21], optimizing 3D scenes loss function typically incorporates the LPIPS loss LLPIPS [44, 70] to improve optimization. The weights λ1, λSSIM, and λLPIPS are adjustable hyperparameters. Formally, the specific loss function is expressed as: = λ1L1 + λSSIMLSSIM + λLPIPSLLPIPS. (2)"
        },
        {
            "title": "3.2 Progressive flexible-view 3D scene expansion",
            "content": "To overcome the limitation of insufficient multi-views in single videos for 3D scene generation discussed in Sec. 1, we propose progressive scene expansion method, named FlexWorld. FlexWorld consists of two key parts: 1) novel view video synthesis guided by pre-defined camera trajectories that explore unseen regions and 2) geometry-aware 3D scene expansion that updates persistent global structure while maintaining geometric coherence. We employ video-to-video (V2V) model that can generate corresponding high-quality videos based on incomplete videos rendered from rough scene for novel view synthesis, which will be discussed in detail in Sec. 3.3. In this section, we focus on geometry-aware 3D scene expansion, which consists of three key subparts: (1) Camera trajectory planning, which determines the regions to be expanded; (2) Scene integration, which integrates the newly generated 3D content into the global scene; and (3) Refinement process, which enhances the visual quality of the scene. Each component is discussed in detail below. Camera trajectory planning. Camera trajectory determines the regions to be expanded, but areas without any 3D information will lose camera control for V2V model. We prioritize camera movement toward specific regions to ensure the input videos always contain 3D information. Specifically, starting with an initial rough scene from single image, we first generate novel views by zooming out to expand the scene. Next, we alternately rotate the camera 180 left and right to add more scene details, ultimately achieving flexible 360 view. See Sec. 4.4 for ablation on the necessity of the initial trajectory of zooming out. Scene intergration. To extract 3D information from the generated video, we first develop method to obtain new 3D content, which is then integrated into the global scene. Subsequently, we treat all video frames as ground truth and optimize the scene to enhance reconstruction. Formally, we select keyframes from the generated video to facilitate the extraction of 3D content, i.e., point cloud. We utilize DUSt3R [51] to generate initial depth maps ˆD0, ..., ˆDm for each of the keyframes I1, ..., Im and reference view I0 simultaneously. For each view, we render the corresponding incomplete depth maps D0, ..., Dm from the existing scene, along with their masks M0, ..., Mm. The reference view is usually well optimized, and its rendered depth D0 is completely known and can be used to measure the depth scale. For each 1 m, the new adding point cloud Pi from view can be obtained by: Di = Depth-align (cid:32) Median(D0)) Median( ˆD0)) (cid:33) ˆDi, Di, Mi , Pi = { Di(u, v)E1 1 (u, v, 1)T Mi(u, v) = 1}, (3) (4) 5 r u ] 6 2 [ 3 M u Frame 10 Frame 20 Frame 30 Frame 49 Figure 4 Our dataset construction method yields more accurate training pairs. We present frames of incomplete videos rendered from initial point clouds generated by dense stereo model MASt3R [26] (i.e., ViewCrafter [66]s dataset construction method) and our 3DGS reconstruction. Our approach produces incomplete videos with better alignment to ground truth, resulting in higher-quality training pairs. where Ei denotes extrinsic for keyframe i, denotes intrinsic, and (u, v) stands for the pixel coordinates of the frame, ranging from 0 to frame size. Median() represents extracting the median value from given depth map. By aligning the depth scale of the reference view, we mitigate the instability inherent in the depth estimation model. Depth-align() denotes any further depth alignment operation, and we employ guided filtering [14] here to achieve smoother overall depth transitions. In the end, we convert these point clouds {P1, ..., Pm} to 3DGS and add to the scene and optimize using views from the entire video with the loss function in Eq. (2). The corresponding hyperparameters can be seen in Sec. 4. Refine process. To further enhance the visual quality of the generated scene, we adopt SDEdit [34] by rendering multi-view images from fixed viewpoints, adding random noise, and applying multi-step denoising process using the FLUX.1-dev [25] image diffusion model: ˆI = fθ(αtI + σtϵ, t), (5) where is the timestamp of the forward diffusion process, fθ represents the denoising model, and ˆI denotes the refined images. We use ˆI to refine the corresponding views of the 3DGS scene with the same loss function as Eq. (2). See Fig. 7 for ablation. From the discussion above, we note that while the proposed framework is feasible, generating flexible-view scenes requires the V2V model to produce new consistent content under large camera variation. Otherwise, achieving flexible-view scenes may necessitate multiple iterations, potentially introducing cumulative errors that affect the consistency of the 3D scene."
        },
        {
            "title": "3.3 Improved diffusion for novel view synthesis",
            "content": "Existing V2V approaches fail to handle significant viewpoint changes (180) [33, 66], as the results show in Fig. 3, primarily due to using weaker base models [4, 58] trained on suboptimal data, as shown in Fig. 4. We improve our V2V diffusion model by conducting video conditioning on an advanced base model and carefully designed training data. Video conditioning. To develop V2V diffusion model suitable for FlexWorld, we select more advanced base model, CogVideoX-5B-I2V [62], as our foundation and replace the original image conditioning with video conditioning. Specifically, the 3D-VAE encoder compresses the conditional videos, which are then concatenated channel-wise with noise latents. Given camera trajectory c, our model aims to learn the 6 distribution p(xy), where represents the incomplete video rendered from the rough scene under c, and denotes the high-quality video of the scene from the corresponding view. The training objective aligns with the original diffusion model, as formulated in Eq. (1). Training data construction. As illustrated in Fig 4, inaccurate training pairs created by the dense stereo model [26, 51, 66] often exhibit significant deviations from the ground truth and contain fragmented textures. As result, the trained V2V model also demonstrates similar artifacts (see Appendix A). To address this issue, we carefully construct training data pairs by employing 3DGS reconstruction to obtain more precise depth estimation. We implement synthetic training data pipeline as follows: Reconstruct the 3DGS scene using all available images; Start at random frame, extract its depth from the 3DGS, and perform back-projection to obtain point cloud; Render incomplete video sequences (49 consecutive frames) under complex camera trajectories from datasets; Pair with ground truth to form training pairs (x, y). This approach ensures more precise depth estimation in the generated data, resulting in more accurate initial point cloud. This enhancement can improve the quality of training pairs, as illustrated in Fig. 4. To support the generation of static scenes and large camera variations, we select the high-quality DL3DV10K [30] scene dataset, which contains various camera movements. We exclude the RealEstate10K dataset [72] from our training dataset, as its videos frequently contain moving objects and simple camera motions, which fail to meet our needs. After training, our video model generalizes to generate high-quality novel views for rough scenes from incomplete inputs under arbitrary trajectories, particularly under large camera variation (see Fig. 3). This positions our model as the optimal video diffusion model in FlexWorld, significantly enhancing the generation of flexible-view 3D scenes."
        },
        {
            "title": "4 Experiment",
            "content": "We present the implementation details for FlexWorld, comparison of novel view synthesis and 3D scene generation, and ablation study sequentially. 4."
        },
        {
            "title": "Implementation details",
            "content": "We build our video-to-video model based on the image-conditioned video diffusion model CogVideoX-5BI2V [62]. The model is trained at resolution of 480 720, with learning rate of 5e-5 and batch size of 32, for total of 5000 steps on 16 NVIDIA A800 80G GPUs. We retain the default settings for other hyperparameters in the original I2V fine-tuning process. In the training dataset, we utilize data from the DL3DV-10K dataset [30], discarding any data with failed COLMAP camera annotations. The coefficient for the 3DGS loss function, specifically λ1, λSSIM, and λLPIPS, are set to 0.8, 0.2, and 0.3, respectively. More details can be found in Appendix B."
        },
        {
            "title": "4.2 Comparison on novel view synthesis",
            "content": "We evaluate the capability of our video-to-video model for novel view synthesis by comparing the visual generation quality and camera accuracy of 5 open-source baseline models, including MotionCtrl [55], CameraCtrl [13], DimensionX [44], See3D [33], ViewCrafter [66]. Evaluation datasets. To ensure fairness, we selected the RealEstate10K (RE10K) test dataset [72] and Tanksand-Temples (Tanks) [23] datasets, which are separate from our training dataset, for evaluation. Following previous work [29, 66], we randomly selected 300 video clips with sample stride ranging from 1 to 3 in the RealEstate10K. In the Tanks-and-Temples dataset, we randomly sampled 100 video clips with stride of 4 across 14 test scenes. Notably, this dataset does not contain pre-labeled cameras; therefore, we utilized the 7 n u n G ] 5 5 [ t i ] 3 1 [ C m ] 6 6 [ f w ] 3 3 [ 3 S u Figure 5 Qualitative comparison on novel view synthesis. We assessed the generative capabilities of various models using the same camera trajectory, focusing on the midpoint. The green bounding box in the ground truth highlights regions requiring consistency with the input, while the remaining areas demand coherent content generation. The red bounding box marks low-quality outputs in baseline models. Our model demonstrates superior visual generation quality, even under effectively controlled camera conditions. MASt3R [26] model to annotate the cameras. Each selected video clip involves camera length of 49. For models generating fewer than 49 frames, we uniformly excluded cameras from the original trajectory to match the required length. Evaluation metrics. We followed previous works [29, 66] to evaluate the generated videos using various metrics comprehensively. The metrics include FID [15] and FVD [49] for assessing visual quality, as well as PSNR, SSIM [54], and LPIPS [70] to evaluate the similarity between the generated frames and the ground truth, with the average of the calculated metrics for each frame taken. Additionally, we estimated the corresponding camera poses for each generated frame and the ground truth using MASt3R [26]. The camera accuracy was calculated using the formula from prior research [13, 29, 66]. Qualitative comparison. From the qualitative comparison shown in Fig. 5, all models exhibit certain level of control over camera movements, and methods like ViewCrafter, See3D, and FlexWorld demonstrated relatively precise control; however, the visual quality of the generated outputs varied. The results from MotionCtrl often exhibited artifacts, while the content produced by CameraCtrl appeared somewhat blurred. See3D Table 1 Quantitative comparison on novel view synthesis. Our method achieves superior visual quality while maintaining commendable camera control compared to the baselines."
        },
        {
            "title": "Metric",
            "content": "FID FVD PSNR SSIM LPIPS Rerr Terr RealEstate10K 20.41 MotionCtrl 22.73 CameraCtrl DimensionX 33.77 24.24 See3D 16.99 ViewCrafter 226.62 381.38 548.19 259.62 143. FlexWorld 13.88 100.41 Tanks and Temples 54.24 MotionCtrl CameraCtrl 60.21 DimensionX 54.13 53.29 See3D 41.18 ViewCrafter 651.47 1338.53 1051.15 564.19 549.10 FlexWorld 37.31 376.49 13.19 16.03 11.77 14.44 15.74 16.62 11.39 11.08 11.26 12.95 12.52 13. 0.516 0.604 0.491 0.546 0.595 0.612 0.361 0.363 0.358 0.404 0.386 0.405 0.515 0.416 0.659 0.477 0.372 0. 0.606 0.688 0.678 0.584 0.526 0.525 0.141 0.040 0.864 0.026 0.032 0.026 0.336 0.202 0. 0.035 0.111 0.048 0.216 0.117 0.615 0.355 0.380 0.297 0.589 0.535 0.695 0.108 0.200 0.100 Table 2 Quantitative comparison on 3D scene generation. Scenes generated from single images by our method achieve nearly superior metric results across various datasets. Results on RealEstate10K"
        },
        {
            "title": "Metric",
            "content": "LucidDreamer DimensionX See3D ViewCrafter PSNR 13.03 11.55 14.60 15.06 SSIM LPIPS PSNR 0.590 0.498 0.718 0.438 0.483 0.544 0.446 0.562 11.67 11.02 12.82 12.35 Results on Tanks and Temples LPIPS SSIM 0.661 0.342 0.700 0.308 0.584 0.581 FlexWorld 16.18 0.604 0.369 12.99 0.544 0.396 0.356 0. struggled to generate distinct new objects from novel viewpoints, and ViewCrafter produced dark content. In contrast, our method maintained effective camera control and surpassed all baseline models in the visual quality of the generated content. Quantitative comparisons. Our quantitative results are presented in Tab. 1. FlexWorld outperforms all baselines across datasets, achieving the best FID and FVD scores, indicating that generated content distribution closely aligns with the ground truth. It also attains optimal PSNR, SSIM, and LPIPS scores, demonstrating superior visual quality. Additionally, our model excels in camera control, with lower Rerr and Terr values."
        },
        {
            "title": "4.3 Comparison on scene generation",
            "content": "We mainly evaluate our method for 3D scene generation by comparing the visual quality of the rendering results with 4 open-source baseline methods: LucidDreamer [9], DimensionX [44], See3D [33], and ViewCrafter [66]. Using the same sampling strategy as in Sec. 4.2, we randomly selected 100 and 50 images from the RE10K [72] and Tanks [23] datasets for evaluation. Except for LucidDreamer, which generates scenes using its original implementation, scenes for other methods are reconstructed from the videos generated to 3DGS, with reconstruction hyperparameters set in [44]. We choose PSNR, SSIM, and LPIPS for the evaluation metrics to compare the renderings from the generated 3D scenes by each baseline against the ground truth frames. As illustrated in the qualitative comparison in Fig. 6, the scenes generated by FlexWorld exhibit higher consistency with the content of the input images compared to other baselines. Furthermore, FlexWorld generates content with higher visual quality in new regions beyond the input. We also conducted quantitative comparison, as presented in Tab. 2, which shows that FlexWorld outperforms nearly all baselines in terms of metrics, with only slight decrease compared to See3D on the SSIM in the Tanks [23] dataset. All results 9 n u n G ] 9 [ m D u ] 6 6 [ f w ] 3 3 [ 3 s Figure 6 Qualitative comparison on 3D scene generation. We present images rendered from scenes generated by various single image-to-3D methods. The green and red bounding boxes have the same meaning as in Fig. 5. Our approach achieves superior visual results. indicate that FlexWorld generates scenes with higher 3D consistency and visual quality."
        },
        {
            "title": "4.4 Ablation study",
            "content": "We conduct an ablation study to demonstrate the necessity of each component in FlexWorld, as illustrated in Fig. 7. Ablation on video diffusion. As shown in Fig. 7a, replacing our V2V model in FlexWorld with ViewCrafter resulted in blurred scene content. This is due to inconsistencies in ViewCrafters output under large camera variations, as discussed in Sec. 3.2. Ablation on camera trajectory. zoom-out movement is crucial for enlarging the scene to enhance camera control. Without it, the generated video will mismatch with the input trajectory, leading to inconsistencies and blurriness in the generated scene, as shown in Fig. 7b. Ablation on refinement process. The video models generation quality restricts the detail in the generated scene. refinement process, as discussed in Sec. 3.2, further enhances the generated details while preserving the existing geometric structure of the scene, as shown in Fig. 7c. 10 (a) w/o V2V (b) w/o zoom-out (c) w/o refine (d) Full Figure 7 Ablation study. To generate 360 view 3D scene, FlexWorld necessitates our video-to-video model and an initial zoom-out trajectory. Additionally, refinement process can further enhance the visual quality of the generated 3D scene."
        },
        {
            "title": "5 Conclusion",
            "content": "We present FlexWorld, framework for generating flexible-view 3D scenes from single image. It combines fine-tuned video-to-video diffusion model for high-quality novel view synthesis and progressive flexible-view 3D scene generation process. Leveraging an advanced pre-trained video foundation model and accurate training data, FlexWorld handles large camera pose variations, enabling consistent and detailed 3D scene generation that supports 360 rotations and zooming. Extensive experiments show FlexWorlds superior viewpoint flexibility and visual quality performance compared to existing methods. We believe that FlexWorld is promising and holds significant potential for virtual reality content creation and 3D tourism."
        },
        {
            "title": "References",
            "content": "[1] Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. arXiv preprint arXiv:2411.18673, 2024. [2] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, Hsin-Ying Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, et al. Vd3d: Taming large video diffusion transformers for 3d camera control. arXiv preprint arXiv:2407.12781, 2024. [3] Shariq Farooq Bhat, Reiner Birkl, Diana Wofk, Peter Wonka, and Matthias Müller. Zoedepth: Zero-shot transfer by combining relative and metric depth. arXiv preprint arXiv:2302.12288, 2023. [4] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [5] Aleksei Bochkovskii, Amaël Delaunoy, Hugo Germain, Marcel Santos, Yichao Zhou, Stephan Richter, and Vladlen Koltun. Depth pro: Sharp monocular metric depth in less than second. arXiv preprint arXiv:2410.02073, 2024. [6] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. Pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1945719467, 2023. URL https://api.semanticscholar.org/CorpusID: 266362208. [7] Luxi Chen, Zhengyi Wang, Zihan Zhou, Tingting Gao, Hang Su, Jun Zhu, and Chongxuan Li. Microdreamer: Efficient 3d generation in sim20 seconds by score-based iterative reconstruction. arXiv preprint arXiv:2404.19525, 2024. [8] Yuedong Chen, Haofei Xu, Chuanxia Zheng, Bohan Zhuang, Marc Pollefeys, Andreas Geiger, Tat-Jen Cham, and Jianfei Cai. Mvsplat: Efficient 3d gaussian splatting from sparse multi-view images. 2024. 11 [9] Jaeyoung Chung, Suyoung Lee, Hyeongjin Nam, Jaerin Lee, and Kyoung Mu Lee. Luciddreamer: Domain-free generation of 3d gaussian splatting scenes, 2023. URL https://arxiv.org/abs/2311.13384. [10] Zhiwen Fan, Wenyan Cong, Kairun Wen, Kevin Wang, Jian Zhang, Xinghao Ding, Danfei Xu, Boris Ivanovic, Marco Pavone, Georgios Pavlakos, et al. Instantsplat: Unbounded sparse-view pose-free gaussian splatting in 40 seconds. arXiv preprint arXiv:2403.20309, 2(3):4, 2024. [11] Ruiqi Gao, Aleksander Holynski, Philipp Henzler, Arthur Brussee, Ricardo Martin-Brualla, Pratul Srinivasan, Jonathan T. Barron, and Ben Poole. Cat3d: Create anything in 3d with multi-view diffusion models, 2024. URL https://arxiv.org/abs/2405.10314. [12] Yuwei Guo, Ceyuan Yang, Anyi Rao, Yaohui Wang, Yu Qiao, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-to-image diffusion models without specific tuning. Proc. ICLR, 2024. [13] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. [14] Kaiming He, Jian Sun, and Xiaoou Tang. Guided image filtering. IEEE transactions on pattern analysis and machine intelligence, 35(6):13971409, 2012. [15] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [16] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. [17] Lukas Höllein, Ang Cao, Andrew Owens, Justin Johnson, and Matthias Nießner. Text2room: Extracting textured 3d meshes from 2d text-to-image models. arXiv preprint arXiv:2303.11989, 2023. [18] Yicong Hong, Kai Zhang, Jiuxiang Gu, Sai Bi, Yang Zhou, Difan Liu, Feng Liu, Kalyan Sunkavalli, Trung Bui, and Hao Tan. Lrm: Large reconstruction model for single image to 3d. arXiv preprint arXiv:2311.04400, 2023. [19] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021. [20] Bingxin Ke, Anton Obukhov, Shengyu Huang, Nando Metzger, Rodrigo Caye Daudt, and Konrad Schindler. Repurposing diffusion-based image generators for monocular depth estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94929502, 2024. [21] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), 2023. [22] Diederik Kingma and Max Welling. Auto-encoding variational bayes. stat, 1050:1, 2014. [23] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: Benchmarking large-scale scene reconstruction. ACM Transactions on Graphics, 36(4), 2017. [24] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [25] Black Forest Labs. Flux: Decentralized computation framework, 2023. URL https://github.com/ black-forest-labs/flux. Accessed: 2024-11-14. [26] Vincent Leroy, Yohann Cabon, and Jérôme Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. [27] Jiahao Li, Hao Tan, Kai Zhang, Zexiang Xu, Fujun Luan, Yinghao Xu, Yicong Hong, Kalyan Sunkavalli, Greg Shakhnarovich, and Sai Bi. Instant3d: Fast text-to-3d with sparse-view generation and large reconstruction model. arXiv preprint arXiv:2311.06214, 2023. [28] Xinyang Li, Zhangyu Lai, Linning Xu, Yansong Qu, Liujuan Cao, Shengchuan Zhang, Bo Dai, and Rongrong Ji. Director3d: Real-world camera trajectory and 3d scene generation from text. Advances in Neural Information Processing Systems, 37:7512575151, 2025. 12 [29] Hanwen Liang, Junli Cao, Vidit Goel, Guocheng Qian, Sergei Korolev, Demetri Terzopoulos, Konstantinos Plataniotis, Sergey Tulyakov, and Jian Ren. Wonderland: Navigating 3d scenes from single image. arXiv preprint arXiv:2412.12091, 2024. [30] Lu Ling, Yichen Sheng, Zhi Tu, Wentian Zhao, Cheng Xin, Kun Wan, Lantao Yu, Qianyu Guo, Zixun Yu, Yawen Lu, et al. Dl3dv-10k: large-scale scene dataset for deep learning-based 3d vision. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2216022169, 2024. [31] Fangfu Liu, Wenqiang Sun, Hanyang Wang, Yikai Wang, Haowen Sun, Junliang Ye, Jun Zhang, and Yueqi Duan. Reconx: Reconstruct any scene from sparse views with video diffusion model, 2024. URL https: //arxiv.org/abs/2408.16767. [32] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. [33] Baorui Ma, Huachen Gao, Haoge Deng, Zhengxiong Luo, Tiejun Huang, Lulu Tang, and Xinlong Wang. You see it, you got it: Learning 3d creation on pose-free videos at scale. arXiv preprint arXiv:2412.06699, 2024. [34] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. [35] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. [36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [37] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [38] René Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE transactions on pattern analysis and machine intelligence, 44(3):16231637, 2020. [39] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. [40] Kyle Sargent, Zizhang Li, Tanmay Shah, Charles Herrmann, Hong-Xing Yu, Yunzhi Zhang, Eric Ryan Chan, Dmitry Lagun, Li Fei-Fei, Deqing Sun, and Jiajun Wu. Zeronvs: Zero-shot 360-degree view synthesis from single image, 2024. URL https://arxiv.org/abs/2310.17994. [41] Jaidev Shriram, Alex Trevithick, Lingjie Liu, and Ravi Ramamoorthi. Realmdreamer: Text-driven 3d scene generation with inpainting and depth diffusion, 2024. URL https://arxiv.org/abs/2404.07199. [42] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning, pages 22562265. PMLR, 2015. [43] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations, 2021. [44] Wenqiang Sun, Shuo Chen, Fangfu Liu, Zilong Chen, Yueqi Duan, Jun Zhang, and Yikai Wang. Dimensionx: Create any 3d and 4d scenes from single image with controllable video diffusion. arXiv preprint arXiv:2411.04928, 2024. [45] Stanislaw Szymanowicz, Eldar Insafutdinov, Chuanxia Zheng, Dylan Campbell, Joao Henriques, Christian Rupprecht, and Andrea Vedaldi. Flash3d: Feed-forward generalisable 3d scene reconstruction from single image. arXiv preprint arXiv:2406.04343, 2024. [46] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. 13 [47] Jiaxiang Tang, Zhaoxi Chen, Xiaokang Chen, Tengfei Wang, Gang Zeng, and Ziwei Liu. Lgm: Large multi-view gaussian model for high-resolution 3d content creation. arXiv preprint arXiv:2402.05054, 2024. [48] Zhenggang Tang, Yuchen Fan, Dilin Wang, Hongyu Xu, Rakesh Ranjan, Alexander Schwing, and Zhicheng Yan. Mv-dust3r+: Single-stage scene reconstruction from sparse views in 2 seconds. arXiv preprint arXiv:2412.06974, 2024. [49] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphaël Marinier, Marcin Michalski, and Sylvain Gelly. Fvd: new metric for video generation. 2019. [50] Haiping Wang, Yuan Liu, Ziwei Liu, Wenping Wang, Zhen Dong, and Bisheng Yang. Vistadream: Sampling multiview consistent images for single-view scene reconstruction. arXiv preprint arXiv:2410.16892, 2024. [51] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2069720709, 2024. [52] Xintao Wang, Liangbin Xie, Chao Dong, and Ying Shan. Real-esrgan: Training real-world blind super-resolution with pure synthetic data. In International Conference on Computer Vision Workshops (ICCVW). [53] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36, 2024. [54] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [55] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM SIGGRAPH 2024 Conference Papers, pages 111, 2024. [56] Rundi Wu, Ben Mildenhall, Philipp Henzler, Keunhong Park, Ruiqi Gao, Daniel Watson, Pratul Srinivasan, Dor Verbin, Jonathan Barron, Ben Poole, et al. Reconfusion: 3d reconstruction with diffusion priors. arXiv preprint arXiv:2312.02981, 2023. [57] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. arXiv preprint arXiv:2412.01506, 2024. [58] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Wangbo Yu, Hanyuan Liu, Gongye Liu, Xintao Wang, Ying Shan, and Tien-Tsin Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pages 399417. Springer, 2024. [59] Dejia Xu, Weili Nie, Chao Liu, Sifei Liu, Jan Kautz, Zhangyang Wang, and Arash Vahdat. Camco: Cameracontrollable 3d-consistent image-to-video generation. arXiv preprint arXiv:2406.02509, 2024. [60] Jiale Xu, Weihao Cheng, Yiming Gao, Xintao Wang, Shenghua Gao, and Ying Shan. Instantmesh: Efficient 3d mesh generation from single image with sparse-view large reconstruction models. arXiv preprint arXiv:2404.07191, 2024. [61] Yuanbo Yang, Jiahao Shao, Xinyang Li, Yujun Shen, Andreas Geiger, and Yiyi Liao. Prometheus: 3d-aware latent diffusion models for feed-forward text-to-3d scene generation. arXiv preprint arXiv:2412.21117, 2024. [62] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [63] Vickie Ye, Ruilong Li, Justin Kerr, Matias Turkulainen, Brent Yi, Zhuoyang Pan, Otto Seiskari, Jianbo Ye, Jeffrey Hu, Matthew Tancik, and Angjoo Kanazawa. gsplat: An open-source library for Gaussian splatting. arXiv preprint arXiv:2409.06765, 2024. URL https://arxiv.org/abs/2409.06765. [64] Hong-Xing Yu, Haoyi Duan, Charles Herrmann, William Freeman, and Jiajun Wu. Wonderworld: Interactive 3d scene generation from single image. arXiv preprint arXiv:2406.09394, 2024. 14 [65] Hong-Xing Yu, Haoyi Duan, Junhwa Hur, Kyle Sargent, Michael Rubinstein, William Freeman, Forrester Cole, Deqing Sun, Noah Snavely, Jiajun Wu, et al. Wonderjourney: Going from anywhere to everywhere. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66586667, 2024. [66] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis, 2024. URL https://arxiv.org/abs/2409.02048. [67] Shangjin Zhai, Zhichao Ye, Jialin Liu, Weijian Xie, Jiaqi Hu, Zhen Peng, Hua Xue, Danpeng Chen, Xiaomeng Wang, Lei Yang, et al. Stargen: spatiotemporal autoregression framework with video diffusion model for scalable and controllable scene generation. arXiv preprint arXiv:2501.05763, 2025. [68] Jingbo Zhang, Xiaoyu Li, Ziyu Wan, Can Wang, and Jing Liao. Text2nerf: Text-driven 3d scene generation with neural radiance fields. IEEE Transactions on Visualization and Computer Graphics, 30(12):77497762, 2024. [69] Kai Zhang, Sai Bi, Hao Tan, Yuanbo Xiangli, Nanxuan Zhao, Kalyan Sunkavalli, and Zexiang Xu. Gs-lrm: Large reconstruction model for 3d gaussian splatting. European Conference on Computer Vision, 2024. [70] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. [71] Min Zhao, Guande He, Yixiao Chen, Hongzhou Zhu, Chongxuan Li, and Jun Zhu. Riflex: free lunch for length extrapolation in video diffusion transformers. arXiv preprint arXiv:2502.15894, 2025. [72] Tinghui Zhou, Richard Tucker, John Flynn, Graham Fyffe, and Noah Snavely. Stereo magnification: Learning view synthesis using multiplane images. arXiv preprint arXiv:1805.09817, 2018."
        },
        {
            "title": "A Impact of training data in video diffusion",
            "content": "] 6 6 [ f w s Figure 8 Artifacts generated by ViewCrafter [66]. Compared to FlexWorld, ViewCrafter produces more artifacts that resemble those found in the incomplete videos within the training dataset constructed by its method. We argue that employing dense stereo model for data construction can introduce peculiar artifacts in the generated videos. As demonstrated in Fig. 8, videos produced by ViewCrafter may exhibit artifacts closely resembling those in the incomplete data from the dataset, as shown in Fig. 4. In contrast, our method generates videos free from such artifacts. This demonstrates that our dataset construction approach is more conducive to model training and better aligned with FlexWorld framework, ultimately enhancing the quality of the generated outputs."
        },
        {
            "title": "B More implementation details",
            "content": "B.1 Training of V2V model. FlexWorlds V2V model is based on CogVideoX-5B-I2V [62] and fine-tuned using the SAT framework. The original I2V model accepts single image input encoded by 3D-VAE into latents with temporal dimension of 1, followed by zero tensor concatenation to match the dimension compressed frame. In contrast, our V2V model directly accepts video input, encoding it into latents with the compressed temporal dimension using 3D-VAE, without the need for zero tensor concatenation. B.2 Flexible-view 3D scene generation FlexWorld begins with constructing an initial 3D point cloud from single input image using DUSt3R [51]. Since dense stereo requires paired images, we duplicate the single input image to serve as both the source and reference views. As for scene representation, we employ 3DGS [21] as the core representation, utilizing gsplat [63] for implementation. When facing point cloud data (e.g., the initial point cloud), it will be immediately converted into 3DGS, serving as the initial scene representation. Unlike the original 3DGS, which uses spherical harmonics, our implementation directly represents color using RGB values. We avoid downsampling during the initialization of 3DGS from the point cloud, so the number of Gaussian counts equals the number of point clouds. Gaussian properties are initialized directly from the point clouds position and color, with scale and opacity set to isotropic values of 3e-4 and 0.8, respectively, and rotation initialized using the identity matrix. As for camera trajectory planning, we interpolate camera motion between the first and last frames to generate 49 camera poses, aligning with the input requirements of our V2V model. The generated video primarily utilizes linear interpolation and cubic spline interpolation in spatial coordinates, while spherical interpolation is applied to rotation matrices to ensure smooth camera transitions. To prevent collisions during camera movement, we calculate the minimum depth from the input images estimated depth to plan the movement range. 16 When integrating sampled 3DGS into an existing scene, we utilize DUSt3R [51] to extract consistent depth from keyframes. we select = 6 keyframes and employ the fully connected pairing strategy to achieve more accurate depth estimation. Keyframes are selected deterministically using uniform sampling strategy, with the reference view typically chosen as the input images corresponding view due to its superior visual quality and role as the starting point for scene expansion. After depth alignment, we utilize alpha maps as masks rendered from the scene to avoid the inclusion of redundant content. We apply 25 iterations of dilation to the alpha map to mitigate fragmentation in the added points. During 3DGS training, we enhance visual quality by upscaling input video frames using the image superresolution model Real-ESRGAN [52]. We use the original Gaussian papers strategies for splitting, duplicating, and pruning Gaussians, but we disable the reset opacity strategy. Compared to the original Gaussian, we use higher learning rates: 1e-5 for position, 5e-3 for color, 5e-2 for opacity, 5e-4 for scale, and 1e-4 for rotation. During the refinement process, the timestamp for the forward diffusion process is set at 0.6T , where represents the total duration of the diffusion process. We focus on refining images when rotating cameras rather than translating ones. Specifically, we refine 5 frames from panoramic scene using image-to-image refinement, followed by 1000 iterations of Gaussian optimization across all images to refine the overall Gaussian representation."
        },
        {
            "title": "C More results",
            "content": "We present more results for our video generation in Fig. 10 and the 360 3D scene generation in Fig. 9."
        },
        {
            "title": "Input",
            "content": "Flexible-view 3D scene generated by FlexWorld Figure 9 More results of generated 360 scene from FlexWorld."
        },
        {
            "title": "Input",
            "content": "Videos generated by our V2V model given camera trajectories Figure 10 More results of generated videos from FlexWorld."
        }
    ],
    "affiliations": [
        "Beijing Key Laboratory of Big Data Management and Analysis Methods",
        "ByteDance",
        "Dept. of Comp. Sci. & Tech., BNRist Center, THU-Bosch MLCenter, Tsinghua University",
        "Gaoling School of AI, Renmin University of China",
        "School of Artificial Intelligence, Beijing Normal University"
    ]
}