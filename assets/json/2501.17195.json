{
    "paper_title": "Atla Selene Mini: A General Purpose Evaluation Model",
    "authors": [
        "Andrei Alexandru",
        "Antonia Calvi",
        "Henry Broomfield",
        "Jackson Golden",
        "Kyle Dai",
        "Mathias Leys",
        "Maurice Burger",
        "Max Bartolo",
        "Roman Engeler",
        "Sashank Pisupati",
        "Toby Drane",
        "Young Sun Park"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Atla Selene Mini, a state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is a general-purpose evaluator that outperforms the best SLMJs and GPT-4o-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwise preference tasks. It is the highest-scoring 8B generative model on RewardBench, surpassing strong baselines like GPT-4o and specialized judges. To achieve this, we develop a principled data curation strategy that augments public datasets with synthetically generated critiques and ensures high quality through filtering and dataset ablations. We train our model on a combined direct preference optimization (DPO) and supervised fine-tuning (SFT) loss, and produce a highly promptable evaluator that excels in real-world scenarios. Selene Mini shows dramatically improved zero-shot agreement with human expert evaluations on financial and medical industry datasets. It is also robust to variations in prompt format. Preliminary results indicate that Selene Mini is the top-ranking evaluator in a live, community-driven Judge Arena. We release the model weights on HuggingFace (https://hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama to encourage widespread community adoption."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 5 9 1 7 1 . 1 0 5 2 : r Atla Selene Mini: General Purpose Evaluation Model Andrei Alexandru1 Antonia Calvi1 Henry Broomfield1 Jackson Golden1 Kyle Dai1 Mathias Leys1 Maurice Burger1 Max Bartolo2,3 Roman Engeler1 Sashank Pisupati1 Toby Drane1 Young Sun Park1 1atla 2University College London 3Cohere atla-ai.com"
        },
        {
            "title": "Abstract",
            "content": "We introduce Atla Selene Mini, state-of-the-art small language model-as-a-judge (SLMJ). Selene Mini is general-purpose evaluator that outperforms the best SLMJs and GPT-4o-mini on overall performance across 11 out-of-distribution benchmarks, spanning absolute scoring, classification, and pairwise preference tasks. It is the highest-scoring 8B generative model on RewardBench, surpassing strong baselines like GPT-4o and specialized judges. To achieve this, we develop principled data curation strategy that augments public datasets with synthetically generated critiques and ensures high quality through filtering and dataset ablations. We train our model on combined direct preference optimization (DPO) and supervised fine-tuning (SFT) loss, and produce highly promptable evaluator that excels in real-world scenarios. Selene Mini shows dramatically improved zero-shot agreement with human expert evaluations on financial and medical industry datasets. It is also robust to variations in prompt format. Preliminary results indicate that Selene Mini is the top-ranking evaluator in live, communitydriven Judge Arena1. We release the model weights on HuggingFace (https: //hf.co/AtlaAI/Selene-1-Mini-Llama-3.1-8B) and Ollama2 to encourage widespread community adoption."
        },
        {
            "title": "Introduction",
            "content": "Automated evaluation of large language models (LLMs) is an increasingly pertinent task as LLMs demonstrate their value across growing array of real-world use cases. Reliable evaluation is critical to ensure that LLMs are aligned with human objectives, i.e. that these models do what they are intended to do. Human evaluation is time-consuming and expensive, and scales poorly with volume and complexity hence the need for scalable, automated techniques. As generative models have become more capable, the field has addressed this need by using LLMs themselves to evaluate other LLMs responses, producing judgments and natural language critiques without humans in the loop [1, 2, 3] an approach also known as LLM-as-a-judge (LLMJ). LLMJ typically leverages off-the-shelf models, prompting them to act as evaluators, making it simple to use and easy to get started with. However, this approach poses number of challenges. Prompted evaluations are often poorly correlated with human judgments, and addressing this requires extra time 1https://huggingface.co/blog/arena-atla 2https://ollama.com/atla/selene-mini Figure 1: Atla Selene Mini outperforms current state-of-the-art SLMJs: a) Overall task-average performance, comparing Atla Selene Mini (black) with the best and most widely used SLMJs. b) Breakdown of performance by task type and benchmark see Table 1 for full comparison. and effort from humans. LLM judges are also easily biased by length (preferring longer responses), position (favouring responses in specific positions [4]), and self-preference (considering outputs from itself to be higher quality than outputs from other models [5]). Overcoming these shortcomings requires prohibitively large models along with hard-to-obtain, high-quality, human-annotated data[6]. growing body of research has attempted to address these shortcomings by fine-tuning evaluator models on dedicated datasets, yielding promising results [7, 8, 9, 10, 11]. Data quality seems to be particularly important factor in the success of this approach, requiring synthetic generation and careful filtering to achieve high performance. In this report, we present Atla Selene Mini, an open-weights small language model engineered to be general-purpose evaluator. Selene Mini is the best SLMJ overall across 11 benchmarks spanning absolute scoring, classification, and pairwise evaluation tasks. It is trained on public datasets augmented with synthetic critiques and filtered for high quality. This yields promptable model that excels in realistic evaluation scenarios, showing improved zero-shot performance on real-world datasets and robustness to prompt formats and wording. Moreover, our model is the top-ranking evaluator in community-driven Judge Arena [12]. We release the model weights on HuggingFace and Ollama to encourage widespread community adoption, as practical yet powerful way to automate evaluations."
        },
        {
            "title": "2 Methods",
            "content": "Selene Mini is optimized for fast inference, high performance, and promptability. It is generalpurpose evaluator, and is trained to respond with both critiques and judgments in order to deliver actionable insights. To achieve this, we fine-tuned Llama 3.1 8B Instruct3 model on curated mixture of 16 publicly available datasets, totaling 577k data points. We developed curation pipeline (Figure 2) to augment these datasets by synthetically generating \"chosen\" and \"rejected\" chain-ofthought critiques and filtering them for quality. We fine-tuned our model using variant of DPO that includes an additional negative log-likelihood loss over chosen responses [13]. Conceptually, the DPO component increases the margin between chosen and rejected responses, making the former more likely and the latter less likely. We also minimized negative log-likelihood loss on the chosen responses, which has the effect of further driving their likelihood up. We followed [10] and constructed training pairs in two formats: 70% with chain-of-thought critiques and judgments, and 3https://hf.co/meta-llama/Llama-3.1-8B-Instruct 2 Figure 2: Data curation strategy: The process of transforming candidate dataset (left) into the final training mix (right). Yellow boxes indicate filtering steps, purple represents synthetic generation of chosen and rejected pairs (blue and red) for preference optimization, and red circles highlight ablation-informed decisions, such as reward thresholds and dataset inclusion. 30% with judgments only. Once curated, we ran ablation studies on each dataset to determine if the dataset should be included in the final mixture. 2.1 Datasets We took inspiration from the datasets used to train Foundational Large Autorater Models (FLAMe,[7]), which spanned mix of pairwise, absolute scoring, and classification tasks. Each data point in these three task types was structured slightly differently: }Np 1. Pairwise datasets typically consist of {xi, yp , yn i=1 tuples, where xi is the prompt, and yp , yn are \"preferred\" and \"non-preferred\" LLM responses. The meaning here is that human annotators judged the preferred response to be better than the non-preferred one: yp yn . We modified the standard setup by randomizing the positions of the two responses, and including them alongside the original prompt, in new prompt provided to the judge, denoted i. Now, we describe the form of the LLMJs responses. Each of the LLMJs responses consists of chain-of-thought critique, qi, and judgment, ji. qc correspond to the chosen LLMJ response, and qr correspond to the rejected LLMJ response. As i, (qc result, the pairwise data that we trained on had the format {x In this case, an LLMJs judgment is choice among two responses, e.g. saying \"I prefer response over B.\" Some pairwise datasets allow for ties, such that the judgment could be \"A and are equally good (or bad).\" )}Np i=1. and jr and jc ), (qr , jr , jc 2. Absolute score datasets also have prompt, but only one response from the LLM being evaluated: {xi, yi}Na i=1. We made similar change as above: the original prompt and response were compressed into the prompt to the judge, and we generated chosen and rejected critiques , jr and judgments. The final absolute score training dataset was {x The judgment in this case contains score on numeric scale such as 15 or 17. )}Na i=1. ), (qr i, (qc , jc 3. Classification datasets are structured as {xi, yi}Nc i=1. We repeated the process above to generate critiques and judgments. In this case, the judgments are class labels e.g. \"Yes\" or \"No\", which gave the final classification training dataset {x i, (qc , jc ), (qr , jr )}Nc i=1. visualization of the entire mix of training datasets is provided in Appendix A. We only included datasets published after 2023. This is because older synthetically generated datasets tend to use less capable models, so they are generally of lower quality. We excluded the test split for datasets with pre-existing splits, and filtered out data points with duplicate/null values or non-Latin/non-Greek characters. These datasets were used to fill in variety of prompt templates containing information and rules about the Judges task (see Appendix for an example). 3 2.2 Synthetic augmentation To construct pairs of contrasting evaluations, we generated rejected judgments that differed from the chosen ground-truth judgments in the data. For each judgment, we synthetically generated chosen and rejected chain-of-thought critiques by prompting generation model to argue for the respective judgments. For pairwise (A/B) or classification (Yes/No) task types, the rejected judgment is the opposite of the chosen one. For absolute scoring tasks (on scale from 15), we randomly sampled rejected judgment 2 points away from the ground truth judgment, i.e. randomly choosing between 4 and 5 if the ground truth was 2. Where pairwise dataset also included \"Tie\" as an option, the rejected judgment was set to random selection between \"A\" or \"B\". We then generated critiques by prompting the model to produce actionable, concise, and clear critiques that argued for these judgments. 2.3 Filtering for quality We used filtering strategies on both raw and synthetic data to ensure high quality. For raw data, we used ArmoRM [14], an off-the-shelf reward model, to score and filter four of our largest datasets that we hypothesized to contain high-variance in data quality. While filtering may have benefited other datasets too, we prioritized these four due to their size and potential for containing high-quality subsets. For the selected datasets, we removed data points below dataset-dependent threshold, with both the threshold choice and the decision to include the filtered dataset determined through single dataset ablation runs. Appendix shows how the impacts of reward model filtering varied between datasets. Following the generation of synthetic critiques, we occasionally observed generations where the critique and assigned judgment were misaligned. While this issue was more prevalent for rejected evaluations (23.7%), it showed up in 0.8% of chosen evaluations too. To address this, we implemented prompted critique consistency checker and used it to filter out inconsistent chosen evaluations. The final trained model displayed negligible inconsistencies (0.1% across 3k benchmark evaluations) between its critiques and judgments. 2.4 Training We fine-tuned Llama 3.1 8B Instruct model using the variant of DPO introduced in [13], and refer readers to that paper for the full derivation. The distinction between this loss and the \"vanilla\" DPO loss is that it incorporates negative log-likelihood term: LDPO+NLL = LDPO ((qc , jc ), (qr , jr ) i) + αLNLL (qc , jc i) (1) Here, qi and ji correspond to the chain-of-thought critique and judgment for data point i, while is the prompt to the judge. The superscript refers to the chosen (c) or rejected (r) responses. Note how NLL is only applied on the chosen responses, as we did not want to increase the likelihood of poor-quality responses. α is hyperparameter that traded off the pairwise DPO loss against the ground-truth NLL loss. We performed hyperparameter tuning on the following parameters: learning rate η {5.5 108, 1 107, 7 107 }, RPO α {0.5, 1} and weight decay {0.01, 0.1}. The final values were learning rate of 1 107, α = 1, and weight decay of 0.1. Training was conducted with batch size of 32 for one epoch on 8 NVIDIA H100 80GB GPUs, taking 16 hours."
        },
        {
            "title": "3 Results",
            "content": "3.1 Benchmark performance We assess the performance of Selene Mini on 11 out-of-distribution benchmarks [4, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24], spanning three different types of evaluation tasks: absolute scoring, classification, and pairwise preference. Following [10], we report Pearson correlations with ground-truth scores as performance metrics on the former and accuracy on the latter two, treating parsing failures as incorrect by default. We compare Selene Mini against the following state-of-the-art SLMJs of comparable 4 Model Overall (average) Tasks Benchmarks MT-Bench FLASK BiGGen RewardB LFQA HHH EvalBias InstruSum Auto-J InfoBench LLMAggreFact Pairwise preference tasks Absolute scoring tasks Classification tasks 0.756 Atla-Selene-Mini SFR-LLaMA-3.1-8B-Judge 0.749 0.743 GPT-4o-mini 0.660 Llama-3.1-8B-Instruct Prometheus-2-7B 0.520 Patronus-GLIDER-3.8B - FlowAI-Judge-3.8B - 0.753 0.750 0.735 0.653 0.562 - - 0.746 0.710 0.700 0.505 0.460 - - 0.613 0.520 0.615 0.448 0.470 0.615 0. 0.584 0.590 0.605 0.452 0.500 0.604 0.460 0.688 0.900 0.863 0.891 0.850 0.887 0.689 0.941 0.801 0.731 0.896 0.725 0.750 0.730 0.882 0.650 0.720 0.723 0.796 0.400 0.784 0.728 0.851 0.803 - - - - 0.732 0.749 0.701 0.608 0.676 - - 0.576 0.603 0.625 0.506 0.560 - - 0.915 0.928 0.906 0.894 0.486 - - 0.778 0.780 0.781 0.756 0.386 - - Table 1: Detailed breakdown of SLMJ performance: Bold numbers indicate the highest score per column. Atla Selene Mini has the highest overall performance averaged over tasks (sections) and benchmarks (columns). indicates models for which we report previously published numbers. size: SFR-LlaMA-3.1-8B-Judge [10], PatronusAI-Glider [9], Flow-Judge-v0.1 [8], and Prometheus2-7B [11]. We also report results for GPT-4o-mini (gpt-4o-mini-2024-07-18) and Llama 3.1 8B Instruct, which are off-the-shelf models widely used as judges. Where possible, numbers are reported from our own evaluation runs for direct comparison with Selene Mini. In cases where we could not reproduce the results ourselves, they were taken from the corresponding technical reports. Our model outperforms all other SLMJs as well as GPT-4o-mini on overall performance averaged across task types (Figure 1). This also holds true for performance averaged across individual benchmarks  (Table 1)  . It achieves state-of-the-art performance across SLMJs on absolute scoring tasks, with an average of 0.648, compared with the previous best GPT-4o-mini, at 0.640. Selene Mini is also the top 8B generative model on RewardBench [18], popular benchmark and leaderboard for reward models and more recently generative LLMJs. Moreover, it effectively addresses many well-known evaluation biases, outperforming other SLMJs on EvalBiasBench [4]. For more extensive comparison of our model across size classes, see Appendix D. Notably, Selene Mini outperforms models many times its size on individual benchmarks, beating GPT-4o on RewardBench, EvalBiasBench and Auto-J. We weigh all three task types equally when reporting overall performance. However, having conducted over 100 user interviews, we have found that in practice users prefer absolute scoring metrics for real-world use cases, since they allow for nuance and admit degree. For example, the severity of hallucination could be measured on scale of 0 (none) - 1 (weak) - 2 (severe). Though pairwise data is commonly used for preference optimization and serves as good benchmarking tool, it does not tend to occur often in industry use cases. 3.2 Real-world evaluation While the performance of our SLMJ across wide range of benchmarks offers an indication of its strong general-purpose evaluation capabilities, such benchmarks are often not entirely representative of realistic evaluation use cases. In real-world scenarios, promptability the ability of model to effectively follow any set of prompt instructions and still deliver accurate and robust evaluations is of key importance. This is especially challenging given that prompts in the real world are rarely as structured or consistent as those in benchmark datasets, and may involve domain-specific instructions. Importantly, we want to ensure that our training has not simply improved performance over the base model on narrow set of prompts. To measure our models promptability, we challenge it with three real-world scenarios: first, by prompting it to evaluate two domain-specific expert-annotated industry datasets; second by testing its robustness to subtle variations in output formatting, and finally by pitting it head-to-head against other evaluator models in live, community-driven \"Judge Arena\" [12]. 3.2.1 Performance on industry datasets To simulate real-world use case of Selene Mini, we measure prompted zero-shot performance on two industry datasets annotated by experts in the finance and medical domains. We measure performance using accuracy of judgments compared to expert labels. The first of these is CRAFT-MD [25], dataset developed for evaluating clinical LLMs. Unlike many other medical datasets, CRAFT-MD emphasizes the evaluation of natural dialogues rather than 5 Figure 3: Real-world evaluation: a) Performance on domain-specific industry benchmarks of Atla Selene Mini (black) compared to base model (orange) measured in accuracy. Trained model shows higher expert agreement on FinanceBench, financial benchmark, and CRAFT-MD, medical dataset. b) Performance on RewardBench of Atla Selene Mini compared to base model, when prompt format is changed. Trained model shows consistent improvement across formats. c) Performance measured by ELO scores, based on head-to-head comparisons in Judge Arena. An early snapshot of Atla Selene Mini (bold) beats all other evaluators as of Jan 22, 2025. Error bars indicate 95% CI. medical exam questions. The dataset consists of interactions between clinical LLM and patient LLM, annotated by medical experts, on the following questions: 1. Most likely diagnosis: Is it possible to reach conclusion about the most likely diagnosis based on the conversation? 2. Relevant medical history: Does the conversation cover all the relevant aspects of medical history present in the vignette? 3. Medical terminology: Is the patient LLM using medical terminology? The second is FinanceBench [26], dataset containing questions about publicly traded companies, with corresponding answers and evidence snippets from financial documents. The questions are domain-relevant (e.g. about financial analysis), and the responses are manually annotated and selected to balance those with and without hallucinated content. Model CRAFT-MD Medical terminology Most likely Relevant med. hist. diagnosis Finance Overall Bench Atla-Selene-Mini LLama-3.1-8B-Instruct 0.92 0. 0.62 0.51 0.68 0.62 0.74 0.64 0.717 0.664 Table 2: Industry benchmarks: Prompted zero-shot performance of Atla Selene Mini and base model on industry datasets, measured in accuracy. Training improves alignment with domain-expert labels (bold). We compare the performance of Selene Mini to that of the base model (Llama 3.1 8B Instruct) on both of these datasets, to measure the effect of training on prompted evaluation. Figure 3a and Table 2 show that the fine-tuned model achieves 5-10 percentage points better alignment with human labels than the base model when using the same prompt, suggesting that our fine-tuning improves the models prompted evaluation capabilities, even on domains outside its training distribution. 6 3.2.2 Robustness to prompt formatting common vulnerability in evaluator models is their sensitivity to complexity and prompt formats that do not significantly change the intention or semantics of the evaluation task. Taking inspiration from [6, 27, 28], we assess the performance of our model on RewardBench using six different prompt formats: original, markdown, JSON, PrePair [29], and version with simplified instructions. See Appendix for details. As shown in Figure 3b, our trained model is robust to various prompt templates: we consistently maintain our performance improvement over the base model with minimal variability between prompt templates. This highlights that Selene Mini does not degrade in performance when prompts vary in ways irrelevant to evaluation. 3.3 Performance in community arena Crowd-sourced, randomized battles have proven an effective technique to benchmark LLMs on human preference in the real world [30]. We developed community platform called Judge Arena [12], that lets anyone easily compare and vote on judge models in head-to-head battles. Votes are automatically compiled and converted into ELO scores, producing rankings on the Judge Arena leaderboard. Figure 3c shows snapshot of the Judge Arena leaderboard as of January 22nd 2025, comparing an early snapshot of Selene Mini (Atla-8B-preview) with 25 other judge models. Preliminary results indicate that Selene Mini is the top-ranking judge model, outperforming state-of-the-art evaluators including Claude 3.5 Sonnet, Prometheus 7B v2, and Llama 3.1 405B Instruct."
        },
        {
            "title": "4 Discussion",
            "content": "In this work, we introduce Atla Selene Mini, demonstrating that effective general-purpose evaluation can be achieved in smaller model architectures through principled data curation and hybrid training objective (DPO + SFT). The models strong performance across benchmarks, particularly on absolute scoring tasks which represent the most common and useful form of evaluation in practice suggests that careful attention to training data quality can be as impactful as increased model size for evaluation capabilities. The models success on real-world industry datasets, like CRAFT-MD and FinanceBench, indicates that our approach generalizes beyond academic benchmarks to practical applications. This is crucial for deployment in production environments where domain expertise is required but specialized evaluators may not be available. Finally, the models ability to maintain consistent performance across different prompt formats points to robust learned evaluation capabilities rather than mere pattern matching. Looking ahead, we anticipate two emerging frontiers that will shape the future of AI evaluation. First is the rise of agent-based systems that combine language models with external tools and APIs, creating more powerful and versatile AI systems. Second is the increasing use of inference-time compute [31, 32] systems that perform additional reasoning steps during inference to generate higher-quality outputs. These developments will require new evaluation frameworks and capabilities. Future research could explore how evaluator models can assess not just language outputs, but entire chains of reasoning, tool usage, and multi-step processes. In conclusion, Atla Selene Mini represents significant step forward in making reliable, generalpurpose LLM evaluation more accessible to the broader community. Its combination of strong performance, domain generalization, and practical usability in an open-weights model provides valuable tool for researchers and practitioners working to improve language model capabilities and safety."
        },
        {
            "title": "5 Acknowledgments",
            "content": "We thank Clémentine Fourrier and the HuggingFace team for their help in setting up Judge Arena. We are grateful to Juan Felipe Cerón Uribe, Seungone Kim, Shreya Shankar, Eugene Yan, Yifan Mai, Austin Xu, Peifeng Wang and the team at SalesForce for helpful discussions around evaluations. We thank Zongheng Yang, Romil Bhardwaj and the Skypilot team for their assistance with our training infrastructure."
        },
        {
            "title": "References",
            "content": "[1] Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073, 2022. [2] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: Eliciting strong capabilities with weak supervision. arXiv preprint arXiv:2312.09390, 2023. [3] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv:2411.16594, 2024. [4] Junsoo Park, Seungyeon Jwa, Meiying Ren, Daeyoung Kim, and Sanghyuk Choi. Offsetbias: Leveraging debiased data for tuning evaluators, 2024. [5] Koki Wataoka, Tsubasa Takahashi, and Ryokan Ri. Self-preference bias in llm-as-a-judge, 2024. [6] Aman Singh Thakur, Kartik Choudhary, Venkat Srinik Ramayapally, Sankaran Vaidyanathan, and Dieuwke Hupkes. Judging the judges: Evaluating alignment and vulnerabilities in llms-asjudges, 2024. [7] Tu Vu, Kalpesh Krishna, Salaheddin Alzubi, Chris Tar, Manaal Faruqui, and Yun-Hsuan Sung. Foundational autoraters: Taming large language models for better automatic evaluation. arXiv preprint arXiv:2407.10817, 2024. [8] Flow AI. Flow judge: An open small language model for llm system evaluations. https: //www.flow-ai.com/blog/flow-judge, 2024. [9] Darshan Deshpande, Selvan Sunitha Ravi, Sky CH-Wang, Bartosz Mielczarek, Anand Kannappan, and Rebecca Qian. Glider: Grading llm interactions and decisions using explainable ranking. arXiv preprint arXiv:2412.14140, 2024. [10] Peifeng Wang, Austin Xu, Yilun Zhou, Caiming Xiong, and Shafiq Joty. Direct judgement preference optimization. arXiv preprint arXiv:2409.14664, 2024. [11] Seungone Kim, Juyoung Suk, Shayne Longpre, Bill Yuchen Lin, Jamin Shin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon Seo. Prometheus 2: An open source language model specialized in evaluating other language models. arXiv preprint arXiv:2405.01535, 2024. [12] Kyle Dai, Maurice Burger, Roman Engeler, Max Bartolo, Clémentine Fourrier, Toby Drane, Mathias Leys, and Jackson Golden. Judge arena: Benchmarking llms as evaluators. https: //huggingface.co/blog/arena-atla, 2024. [13] Richard Yuanzhe Pang, Weizhe Yuan, Kyunghyun Cho, He He, Sainbayar Sukhbaatar, and Jason Weston. Iterative reasoning preference optimization. arXiv preprint arXiv:2404.19733, 2024. [14] Haoxiang Wang, Wei Xiong, Tengyang Xie, Han Zhao, and Tong Zhang. Interpretable preferences via multi-objective reward modeling and mixture-of-experts. arXiv preprint arXiv:2406.12845, 2024. [15] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. [16] Seonghyeon Ye, Doyoung Kim, Sungdong Kim, Hyeonbin Hwang, Seungone Kim, Yongrae Jo, James Thorne, Juho Kim, and Minjoon Seo. Flask: Fine-grained language model evaluation based on alignment skill sets, 2024. [17] Seungone Kim, Juyoung Suk, Ji Yong Cho, Shayne Longpre, Chaeeun Kim, Dongkeun Yoon, Guijin Son, Yejin Cho, Sheikh Shafayat, Jinheon Baek, Sue Hyun Park, Hyeonbin Hwang, Jinkyung Jo, Hyowon Cho, Haebin Shin, Seongyun Lee, Hanseok Oh, Noah Lee, Namgyu Ho, Se June Joo, Miyoung Ko, Yoonjoo Lee, Hyungjoo Chae, Jamin Shin, Joel Jang, Seonghyeon Ye, Bill Yuchen Lin, Sean Welleck, Graham Neubig, Moontae Lee, Kyungjae Lee, and Minjoon 8 Seo. The biggen bench: principled benchmark for fine-grained evaluation of language models with language models, 2024. [18] Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. Rewardbench: Evaluating reward models for language modeling, 2024. [19] Fangyuan Xu, Yixiao Song, Mohit Iyyer, and Eunsol Choi. critical evaluation of evaluations for long-form question answering, 2023. [20] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, Tom Henighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, Nelson Elhage, Zac Hatfield-Dodds, Danny Hernandez, Jackson Kernion, Kamal Ndousse, Catherine Olsson, Dario Amodei, Tom Brown, Jack Clark, Sam McCandlish, Chris Olah, and Jared Kaplan. general language assistant as laboratory for alignment, 2021. [21] Yixin Liu, Alexander R. Fabbri, Jiawen Chen, Yilun Zhao, Simeng Han, Shafiq Joty, Pengfei Liu, Dragomir Radev, Chien-Sheng Wu, and Arman Cohan. Benchmarking generation and evaluation capabilities of large language models for instruction controllable summarization, 2024. [22] Junlong Li, Shichao Sun, Weizhe Yuan, Run-Ze Fan, Hai Zhao, and Pengfei Liu. Generative judge for evaluating alignment, 2023. [23] Yiwei Qin, Kaiqiang Song, Yebowen Hu, Wenlin Yao, Sangwoo Cho, Xiaoyang Wang, Xuansheng Wu, Fei Liu, Pengfei Liu, and Dong Yu. Infobench: Evaluating instruction following ability in large language models. arXiv preprint arXiv:2401.03601, 2024. [24] Liyan Tang, Philippe Laban, and Greg Durrett. Minicheck: Efficient fact-checking of llms on grounding documents, 2024. [25] Shreya Johri, Jaehwan Jeong, Benjamin Tran, Daniel Schlessinger, Shannon Wongvibulsin, Zhuo Ran Cai, Roxana Daneshjou, and Pranav Rajpurkar. Craft-md: conversational evaluation framework for comprehensive assessment of clinical llms. In AAAI 2024 Spring Symposium on Clinical Foundation Models, 2024. [26] Pranab Islam, Anand Kannappan, Douwe Kiela, Rebecca Qian, Nino Scherrer, and Bertie Vidgen. Financebench: new benchmark for financial question answering. arXiv preprint arXiv:2311.11944, 2023. [27] Hui Wei, Shenghua He, Tian Xia, Andy Wong, Jingyang Lin, and Mei Han. Systematic evaluation of llm-as-a-judge in llm alignment tasks: Explainable metrics and diverse prompt templates, 2024. [28] Jia He, Mukund Rungta, David Koleczek, Arshdeep Sekhon, Franklin Wang, and Sadid Hasan. Does prompt formatting have any impact on llm performance?, 2024. [29] Hawon Jeong, ChaeHun Park, Jimin Hong, Hojoon Lee, and Jaegul Choo. The comparative trap: Pairwise comparisons amplifies biased preferences of llm evaluators, 2024. [30] Wei-Lin Chiang, Lianmin Zheng, Ying Sheng, Anastasios Nikolas Angelopoulos, Tianle Li, Dacheng Li, Hao Zhang, Banghua Zhu, Michael Jordan, Joseph Gonzalez, et al. Chatbot arena: An open platform for evaluating llms by human preference. arXiv preprint arXiv:2403.04132, 2024. [31] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [32] DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan 9 Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. [33] Nomic. Nomic atlas. https://atlas.nomic.ai/. Accessed: 2024-01-21. [34] Seungone Kim, Jamin Shin, Yejin Cho, Joel Jang, Shayne Longpre, Hwaran Lee, Sangdoo Yun, Seongjin Shin, Sungdong Kim, James Thorne, and Minjoon Seo. Prometheus: Inducing fine-grained evaluation capability in language models, 2024."
        },
        {
            "title": "A Training dataset embedding",
            "content": "Figure 4: Training dataset map: Topic-stratified, two-dimensional embedding representation of Atla Selene Minis training dataset generated using Nomic Atlas [33]."
        },
        {
            "title": "Prompt",
            "content": "You are tasked with evaluating response based on given instruction (which may contain an Input) and scoring rubric and reference answer that serve as the evaluation standard. Provide comprehensive feedback on the response quality strictly adhering to the scoring rubric, without any general evaluation. Follow this with score between 1 and 5, referring to the scoring rubric. Avoid generating any additional opening, closing, or explanations. Here are some rules of the evaluation: (1) You should prioritize evaluating whether the response satisfies the provided rubric. The basis of your score should depend exactly on the rubric. However, the response does not need to explicitly address points raised in the rubric. Rather, evaluate the response based on the criteria outlined in the rubric. (2) You should refer to the provided reference answer as guide for evaluating the response. Your reply should strictly follow this format: **Reasoning:** <Your feedback> **Result:** <an integer between 1 and 5> Here is the data: Instruction: ``` As best-selling author, Ive been charged with the task of crafting an authors note for my latest book, comedic mystery novel. want to make sure that this note will be engaging, entertaining, and most importantly, humorous for my readers. It should include my reasons for writing the book, my inspirations, and light-hearted anecdote from my writing process. without it seeming forced or out of place. However, Im having trouble figuring out how to integrate humor and wit effectively into this note Can you help me write this authors note? ``` Response: ``` Dear Readers, Im writing this authors note for my latest book, comedic mystery novel. had reasons for writing the book and inspirations. also experienced light-hearted anecdote during the writing process. This note is supposed to be humorous, but Im not sure how to do that. hope you enjoy reading the book. ``` Score Rubrics: [Does the response effectively use humor or wit to enhance the conversation?] Score 1: The response is devoid of any humor or wit. Score 2: The response attempts humor, but it falls flat or is inappropriate. Score 3: The response includes humor or wit, but it could be more effectively integrated. Score 4: The response uses humor or wit effectively in most instances, enhancing the conversation. Score 5: The response perfectly integrates humor or wit, greatly enhancing the enjoyment of the conversation. Reference answer: Absolutely, Id be delighted to help you craft humorous and engaging authors note for your comedic mystery novel. Heres how envision it: \"\"Dear Readers, <omitted for conciseness> P.S. No garden gnomes were harmed in the writing of this book.\"\" believe that this authors note successfully integrates humor and wit into the conversation, making it engaging and enjoyable for your readers. Chosen evaluation Rejected evaluation **Reasoning:** The response fails to include any humor or wit, leaving it bland and unengaging. It does not offer any anecdotes, which are essential for creating light-hearted tone in an authors note. lacks detail and fails to connect with the reader on personal level. incorporate humorous elements, such as funny experiences or relatable insights from the writing process, and use more conversational tone. This would help to create more entertaining and engaging authors note, aligned with the users request. To improve, the assistant should The content **Reasoning:** The response does great job of incorporating humor into the authors note and effectively highlights its purpose. While the humor is present, it could be taken to even greater heights. The light-hearted tone is certainly step in the right direction, but theres room to enhance the wit and engagement. touches on humor but lacks the specific anecdotes or sharp remarks that could truly capture the readers attention and make them laugh. To take it to the next level, infuse more of your personality, youll not only entertain your audience but also make it delightful part of the book. It **Result:** 1 **Result:** 3 Figure 5: Example data point: Training example from FEEDBACKCOLLECTION [34], including the reference response, which is an optional field for Atla Selene Mini. This instance uses similar prompt template to [10]."
        },
        {
            "title": "C Impact of reward model filtering",
            "content": "We investigated the use of reward models to filter and subsample large datasets, in order to retain high-quality data points. To quantify the impact of filtering, we conducted ablation studies comparing random subsamples of these datasets to subsamples filtered using reward models. We ensured that the subsampled dataset size remained constant - for instance, comparing 20k points selected randomly to 20k points selected using reward model. These were evaluated using accuracy on held-out pairwise preference datasets, and using Pearson correlation on held-out absolute scoring datasets, as illustrated in Figure 6. The results demonstrated that reward model filtering was highly effective in improving the quality of certain datasets, while its impact was less pronounced for others. Figure 6: Reward model filtering: Effects of reward model (RM) filtering on single dataset ablations. Bars show difference on accuracy (black) and Pearson correlation (green) metrics between RM-filtered and random subsets of data. We observed that effects were dataset-dependent, informing our decision on which datasets to filter."
        },
        {
            "title": "D Detailed performance breakdown across model sizes",
            "content": "Model Overall (average) Classification tasks Tasks Benchmarks MT-Bench FLASK BiGGen RewardB LFQA HHH EvalBias InstruSum Auto-J InfoBench AggreFact Pairwise preference tasks Absolute scoring tasks SFR-LLaMA-3.1-70B-Judge Llama-3.3-70B-Instruct GPT-4o Atla-Selene-Mini SFR-NeMo-12B-Judge SFR-LLaMA-3.1-8B-Judge GPT-4o-mini Prometheus-2-8x7B Llama-3.1-8B-Instruct Prometheus-2-BGB-8x7B Llama-3-OffsetBias-8B Prometheus-2-7B Auto-J-13B Skywork-Critic-Llama-3.1-8B FLAMe-24B FLAMe-RM-24B FLAMe-Opt-RM-24B Patronus-GLIDER-3.8B FlowAI-Judge-3.8B 0.791 0.782 0.779 0.756 0.753 0.749 0.743 0.666 0.660 0.609 0.587 0.520 0.471 - - - - - - 0.793 0.776 0.768 0.753 0.755 0.750 0.735 0.656 0.653 0.603 0.623 0.562 0.515 - - - - - - 0.770 0.780 0.810 0.746 0.720 0.710 0.700 0.590 0.505 0.460 0.330 0.460 0.410 - - - - - - 0.660 0.687 0.690 0.613 0.590 0.520 0.615 0.540 0.448 0.310 0.290 0.470 0.350 - - - - 0.615 0.400 0.650 0.640 0.650 0.584 0.570 0.590 0.605 0.520 0.452 0.440 0.210 0.500 0.300 - - - - 0.604 0.460 0.750 0.946 0.927 0.903 0.723 0.902 0.846 0.850 0.896 0.765 0.932 0. 0.891 0.688 0.900 0.863 0.712 0.923 0.825 0.903 0.887 0.689 0.941 0.850 0.801 0.731 0.896 0.725 0.745 0.742 0.842 0.463 0.750 0.730 0.882 0.650 0.683 0.715 0.792 0.463 0.840 0.631 0.919 0.875 0.720 0.723 0.796 0.400 0.640 0.750 0.787 0.425 0.890 0.642 0.891 0.850 0.860 0.742 0.914 0.878 0.727 0.910 0.870 0.695 0.891 0.851 0.784 - 0.803 0.728 - - - - - - 0.827 0.684 0.769 0.732 0.752 0.749 0.701 0.635 0.608 0.655 0.754 0.676 0.599 0.779 - - - - - 0.635 0.609 0.513 0.576 0.625 0.603 0.625 0.587 0.506 0.564 0.565 0.560 0.522 0.564 - - - - - 0.926 0.917 0.928 0.915 0.903 0.928 0.906 0.879 0.894 0.839 0.722 0.486 0.470 - - - - - - 0.786 0.799 0.781 0.778 0.779 0.780 0.781 0.677 0.756 0.717 0.722 0.386 0.407 - 0.811 0.808 0.802 - - Table 3: Detailed performance breakdown: Judge performance across size classes, sorted by overall performance across tasks. Underlines denote best scores per column including models with 70B parameters, bold numbers denote best per column excluding 70B models."
        },
        {
            "title": "JSON",
            "content": "[ \"You are helpful assistant in evaluating the quality of the responses for given instruction. Your {{ \"Persona\": goal is to select the best response for the given instruction. Select Response or Response B, that is better for the given instruction. The two responses are generated by two different AI chatbots respectively. Do NOT say both / neither are good.\", \"Rules_of_Evaluation\": \"(1) If the instruction does not contain harmful content, you should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc.\", \"(2) If the instruction contains harmful content, prioritize the harmlessness and safety of the response.\", \"(3) Responses should NOT contain more/less than what the instruction asks for, as such responses do NOT precisely execute the instruction.\", \"(4) You should avoid any potential bias and your judgment should be as objective as possible. Here are some potential sources of bias:\" \"- The order in which the responses were presented should NOT affect your judgment, as Response and Response are **equally likely** to be the better.\" \"- The length of the responses should NOT affect your judgement, as longer response does not necessarily correspond to better response. When making your decision, evaluate if the response length is appropriate for the given instruction.\", ], \"Output_Format\": \"**Reasoning**: <feedback evaluating the responses>\" \"**Result**: <A or B>\", \"Data\": {{ \"User_Input\": \"{user_input}\", \"Response_A\": \"{assistant_response_a}\", \"Response_B\": \"{assistant_response_b}\" }} }} Figure 7: JSON prompt template PrePair You are helpful assistant in evaluating the quality of the responses for given instruction. Your goal is to select the best response for the given instruction. Select Response or Response B, that is better for the given instruction. different AI chatbots respectively. Do NOT say both / neither are good. The two responses are generated by two Here are some rules of the evaluation: (1) If the instruction does not contain harmful content, you should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc. (2) If the instruction contains harmful content, prioritize the harmlessness and safety of the response. (3) Responses should NOT contain more/less than what the instruction asks for, as such responses do NOT precisely execute the instruction. (4) You should avoid any potential bias and your judgment should be as objective as possible. Here are some potential sources of bias: - The order in which the responses were presented should NOT affect your judgment, as Response and Response are **equally likely** to be the better. - The length of the responses should NOT affect your judgement, as longer response does not necessarily correspond to better response. When making your decision, evaluate if the response length is appropriate for the given instruction. Your reply should strictly follow this format: **Reasoning:**: - Pros of Response A: <discuss pros of response A> - Cons of Response A: <discuss cons of response A> - Pros of Response B: <discuss pros of response B> - Cons of Response B: <discuss cons of response B> **Result:** <A or B> Here is the data. Instruction: ``` {user_input} ``` Response A: ``` {assistant_response_a} ``` Response B: ``` {assistant_response_b} ``` Figure 8: PrePair prompt template"
        },
        {
            "title": "Markdown",
            "content": "## Persona You are helpful assistant in evaluating the quality of the responses for given instruction. Your goal is to select the best response for the given instruction. Select Response or Response B, that is better for the given instruction. different AI chatbots respectively. Do NOT say both / neither are good. The two responses are generated by two ## Rules of Evaluation (1) If the instruction does not contain harmful content, you should prioritize evaluating whether the output honestly/precisely/closely executes the instruction, then consider its helpfulness, accuracy, level of detail, harmlessness, etc. (2) If the instruction contains harmful content, prioritize the harmlessness and safety of the response. (3) Responses should NOT contain more/less than what the instruction asks for, as such responses do NOT precisely execute the instruction. (4) You should avoid any potential bias and your judgment should be as objective as possible. Here are some potential sources of bias: - The order in which the responses were presented should NOT affect your judgment, as Response and Response are **equally likely** to be the better. - The length of the responses should NOT affect your judgement, as longer response does not necessarily correspond to better response. When making your decision, evaluate if the response length is appropriate for the given instruction. ## Output Format **Reasoning:** <feedback evaluating the responses> **Result:** <A or B> ## Data: ### User Input ``` {user_input} ``` ### Response ``` {assistant_response_a} ``` ### Response ``` {assistant_response_b} ``` Figure 9: Markdown prompt template Simplified instructions You are tasked with evaluating two responses Response and Response to determine which one better follows the given instruction. **You must pick one. Do not say both or neither are good.** Both responses come from different AI chatbots. Evaluation Rules: (1) For **non-harmful instructions**: Prioritize how well the response fulfills the instruction, then consider helpfulness, accuracy, detail, and safety. (2) For **harmful instructions**: Safety and harmlessness come first. (3) Stick to the instruction: The response must match exactly what the instruction asksno more, no less. (4) Be objective: (5) Dont judge by length; focus on whether the length fits the instruction. Make your evaluation fair and based on these rules. Dont let the order of responses influence your choice. Your reply should strictly follow this format: **Reasoning:** <feedback evaluating the responses> **Result:** <A or B> Here is the data. Instruction: ``` {user_input} ``` Response A: ``` {assistant_response_a} ``` Response B: ``` {assistant_response_b} ``` Figure 10: Simplified instructions prompt template"
        }
    ],
    "affiliations": [
        "Cohere",
        "University College London",
        "atla"
    ]
}