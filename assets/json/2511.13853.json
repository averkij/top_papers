{
    "paper_title": "Can World Simulators Reason? Gen-ViRe: A Generative Visual Reasoning Benchmark",
    "authors": [
        "Xinxin Liu",
        "Zhaopan Xu",
        "Ming Li",
        "Kai Wang",
        "Yong Jae Lee",
        "Yuzhang Shang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physics-governed dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoning -- materializing thought as frame-by-frame visual sequences, with each frame representing a physically-grounded reasoning step. Despite compelling demonstrations, a challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce Gen-ViRe (Generative Visual Reasoning Benchmark), a framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions -- from perceptual logic to abstract planning -- and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLM-assisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators."
        },
        {
            "title": "Start",
            "content": "Can World Simulators Reason? Gen-ViRe: Generative Visual Reasoning Benchmark Xinxin Liu1* , Zhaopan Xu2* , Ming Li1, Kai Wang2, Yong Jae Lee3, Yuzhang Shang1 1University of Central Florida, 2National University of Singapore, 3UW-Madison"
        },
        {
            "title": "Code",
            "content": "5 2 0 2 1 ] . [ 2 3 5 8 3 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "While Chain-of-Thought (CoT) prompting enables sophisticated symbolic reasoning in LLMs, it remains confined to discrete text and cannot simulate the continuous, physicsgoverned dynamics of the real world. Recent video generation models have emerged as potential world simulators through Chain-of-Frames (CoF) reasoningmaterializing thought as frame-by-frame visual sequences, with each frame representing physically-grounded reasoning step. Despite compelling demonstrations, challenge persists: existing benchmarks, focusing on fidelity or alignment, do not assess CoF reasoning and thus cannot measure core cognitive abilities in multi-step planning, algorithmic logic, or abstract pattern extrapolation. This evaluation void prevents systematic understanding of model capabilities and principled guidance for improvement. We introduce GenViRe (Generative Visual Reasoning Benchmark), framework grounded in cognitive science and real-world AI applications, which decomposes CoF reasoning into six cognitive dimensions from perceptual logic to abstract planning and 24 subtasks. Through multi-source data curation, minimal prompting protocols, and hybrid VLMassisted evaluation with detailed criteria, Gen-ViRe delivers the first quantitative assessment of video models as reasoners. Our experiments on SOTA systems reveal substantial discrepancies between impressive visual quality and actual reasoning depth, establishing baselines and diagnostic tools to advance genuine world simulators. 1. Introduction Recent breakthroughs in Chain-of-Thought (CoT) prompting have demonstrated that large-scale language models (LLMs) can exhibit emergent reasoning capabilities across diverse symbolic domains, from mathematical problemsolving to code generation [17, 23, 43]. These advances *Equal contribution. Corresponding author Figure 1. comparison of reasoning approaches for mazesolving task. Humans visualize the path via mental simulation. Multimodal Large Language Model (MLLM) uses symbolic reasoning (CoT) to describe the path, e.g., via coordinates. In contrast, Video Generation Model (VGM) uses generative visual reasoning (CoF) to physically simulate the process, generating frames of the square moving from start to finish. represent significant milestone in AIs ability to perform complex reasoning. However, CoT operates exclusively in the symbolic realmreasoning over language tokens about abstract concepts. While this enables powerful logical inference, it fundamentally cannot capture the continuous, visual, and spatial nature of the real world. This limitation is evident in tasks ranging from abstract spatial navigation to complex physical interaction. For instance, in the maze-solving task (shown in Figure 1), when an MLLM using CoT is asked to find the path, it describes the solution symbolicallyperhaps by outputting list of coordinates [e.g., \"(3, 1), (2, 1)...\"]. It cannot, however, simulate the actual, continuous, dynamic process of the \"red square\" moving through the path. Simi1 larly, consider robotic manipulation task: planning how to open nailed wooden box. CoT can describe the steps (use crowbar, apply leverage), but it cannot verify whether the plan is physically feasiblewhether the robots gripper can grasp the crowbar, whether the force angle is sufficient, or whether nearby objects will interfere. This reveals critto build AI systems that truly understand ical limitation: and simulate spatial dynamics and physical interaction, we need models that can reason through continuous visual simulation, not just symbolic description. This gapthe need for dynamic, generative reasoningis also precisely where conventional computer vision paradigms fall short. Conventional vision models function as specialized, passive perceiversexcelling at singular discriminative tasks like object detection or segmentation, yet lacking the unified generative reasoning capabilities needed to simulate dynamic processes. Recognizing this gap, recent pioneering works have proposed revolutionary paradigm shift: video models as zero-shot learners and world simulators [2, 3, 15, 46]. As world simulators, these models are transitioning from merely rendering pixels to building an implicit spatiotemporal and physical engine by training on massive video data. This emergent simulator then enables them to act as zero-shot learners; they are no longer confined to the specific compositions seen during training but can, much like humans, leverage fundamental understanding of the world to zero-shot imagine, reason about, and create entirely new, logical scenarios. At the core of this new paradigm lies the Chain-ofFrames (CoF) mechanism [46], where reasoning is actualized through frame-by-frame video generation. Unlike CoTs discrete symbolic transitions (text text), CoF materializes reasoning as goal-directed, continuous visual state evolution (frame frame). When model solves visual Sudoku puzzle, navigates maze, or plans multi-step tool use through CoF, each generated frame represents an incremental reasoning step that is both physically grounded and temporally coherent. The generative process itself becomes an act of thinking, not just describing the symbolic coordinates for the maze but generating the dynamic trajectory of the red square moving. It transforms models from passive descriptors into executable world simulators. The potential of this paradigm is evidenced by rapid progress in video generation technology. Large-scale modelsfrom commercial systems like Sora [2] and Kling [28] to open models like CogVideo [19] and HunyuanVideo [27]have demonstrated impressive emergent capabilities in understanding physical interactions, temporal causality, and spatial relationships. Qualitative demonstrations from pioneering works are compelling: models generating coherent multi-step processes, respecting object permanence, and exhibiting intuitive physics. Yet despite this progress, fundamental question remains unanswered: Figure 2. Our Gen-ViRe evaluates six core cognitive dimensions: (1) Perceptual, (2) Analogical, (3) Abstract, (4) Planning, (5) Spatial & Temporal, and (6) Algorithmic & Logical, with each dimension comprising four different sub-categories. how well do these models actually reason? Qualitative showcases, while impressive, provide no systematic measure of reasoning depth. More critically, existing video generation benchmarks, which focus primarily on fidelity, alignment, or object-level consistency, do not assess CoF reasoning and therefore cannot measure models core cognitive abilities in multi-step planning and algorithmic logic, or abstract pattern extrapolation [6, 10, 11, 18, 20, 29, 30, 32, 33, 39, 40, 42, 47, 50]. Without rigorous quantitative evaluation, we cannot distinguish genuine reasoning from sophisticated pattern matching, nor can we diagnose where models failperception, physics understanding, or planningto guide systematic improvement. To address this critical evaluation gap, we propose Gen-ViRe (Generative VisualReasoning Benchmark), the first comprehensive framework purpose-built to systematically assess CoF reasoning by video generation models. Gen-ViRe is grounded in dual foundations: cognitive science principles defining core pillars of human reasoning [4, 14, 25, 26, 3538] , and application-driven requirements from emerging domains like embodied AI and autonomous systems [12, 49]. We decompose Generative VisualReasoning into six fundamental dimensions Perceptual, Analogical, Algorithmic & Logical, Spatio-Temporal & Dynamic, Procedural & Planning, and Abstract Reasoningspanning the complete spectrum from foundational perception to high-order planning [1, 31, 48]. Our data curation employs multi-source strategy as shown in Figure 4: curating from web resources and academic papers, integrating existing datasets (e.g., ARC-AGI for abstract reason2 ing, KiVA for analogical tasks), and generating novel tasks using T2I models where suitable data is absent. We implement rigorous prompt design through minimal prompting principlesdeliberately providing only high-level goals to assess autonomous reasoning rather than instructionfollowingvalidated through iterative peer review. Our evaluation methodology features hybrid VLM-assisted approach: Image VLMs judge final outputs for static reasoning tasks, while Video VLMs assess the complete generation process for dynamic tasks, with each judge provisioned with detailed, sub-category-specific criteria refined through multi-round human validation. Through extensive experiments evaluating 7 state-of-the-art video generation models across 72 distinct reasoning prompts (with each model generating 5 instances per prompt, totaling 360 videos per model and over 2,500 overall), Gen-ViRe delivers the first systematic quantification of CoF reasoning capabilities, revealing critical gaps between qualitative potential and quantitative performance, and establishing scientific baselines to guide the development of genuine world simulators. Our main contributions are as follows: We propose Gen-ViRe, the first comprehensive benchmark specifically designed to systematically evaluate Chain-of-Frames reasoning across six fundamental dimensions grounded in cognitive science and practical AI applications, providing rigorous scientific assessment from foundational perception to high-order planning. We establish complete evaluation methodology combining minimal prompting design, multi-source data curation (web/academic resources, existing datasets, generative creation), and hybrid VLM-assisted assessment with detailed criteria, enabling quantitative measurement of generative reasoning and failure mode diagnosis. Through extensive experiments on state-of-the-art video generation models, we provide the first systematic analysis of current CoF reasoning capabilities, revealing significant gaps between qualitative demonstrations and quantitative performance, and establishing baselines for future research toward genuine world simulators. 2. Related Work 2.1. Discrete Chain-of-Thought Reasoning. Chain-of-Thought (CoT) prompting has revolutionized how large language models approach complex reasoning tasks by explicitly generating intermediate reasoning steps [43]. GPT-o1 [23] demonstrated that LLMs can leverage CoT for test-time scalingtrading computation for reasoning depthwhile DeepSeek-R1 [17] advanced this through Reinforcement Learning with Verifiable Rewards (RLVR), democratizing sophisticated reasoning via open-source release. Recent work has extended CoT to multimodal domains: GPT-4o [21] pioneered vision-language joint reasoning, and Bagel [9] leverages vision-text interleaved CoT to enhance visual generation. However, CoT remains fundamentally symbolic and discreteoperating in the space of language tokens rather than continuous visual states. While effective for formal logic, CoT is inherently discriminative and passive: it cannot dynamically simulate the physical evolution of the visual world. CoT chain can describe \"a ball rolls down slope,\" but cannot generate the actual frame-by-frame trajectory governed by gravity and collision. This limitation necessitates new paradigm grounded in continuous, generative visual simulation. 2.2. Continuous Chain-of-Frames Reasoning. The emerging Chain-of-Frames (CoF) paradigm represents revolutionary shift in how AI systems perform reasoning [46]. Unlike CoT, which operates on discrete symbolic transitions (text text), CoF materializes reasoning as continuous visual state evolution (frame frame). In this paradigm, reasoning is not merely describedit is executed and visualized through frame-by-frame video generation. When model generates sequence showing how to solve visual puzzle, navigate maze, or manipulate objects to achieve goal, each generated frame represents an incremental reasoning step that is physically grounded and temporally coherent. Recent pioneering works have begun to explore this new frontier. The Genie series [3] demonstrates how interactive world generation can enable embodied reasoning, allowing agents to learn causal relationships through simulated physical interactions. Veo-3 pushes the boundaries of long-horizon reasoning, generating extended video sequences that maintain spatial-temporal consistency across complex dynamic scenes [15]. These works provide compelling qualitative evidence that video models have the potential as world simulators, capable of understanding physics, causality, and spatial relationships [46]. 2.3. Video Generation: Potential World Simulators. Recent years have witnessed remarkable progress in video generation, predominantly driven by diffusion-based architectures. Models such as Sora [2], Kling [28], and SeeDance [13] have achieved breakthroughs in visual quality, generation length, and resolution through large-scale diffusion transformers. While these commercial models remain closed-source, the research community has also contributed open-sourced models like CogVideo [19] and HunyuanVideo [27] and WanVideo [41]. These models demonstrate impressive implicit learning of physical principles purely from observing massive video datasets. Crucially, these models are evolving beyond mere visual generatorsthey are becoming potential world simulators. For instance, when prompted to generate video of basketball bouncing downstairs, Veo-3 [15] can produce sequences that respect gravity, conserve momentum, and exhibit realistic deformation upon impact [46]. 3 3. Gen-ViRe Benchmark 3.1. Definition: Generative Visual Reasoning In this paper, we aim to evaluate an emergent capability beyond traditional discriminative VQA, which we term Generative Visual Reasoning (GVR). We conceptually define GVR as an agents ability to solve complex visual problem by simulating spatio-temporal dynamic or executing multi-step plan through sequential, generative process. Unlike traditional benchmarks that aim to evaluate single, deterministic final answer (e.g., class label or bounding box), the goal of GVR task is to evaluate the generation process (i.e., frame sequence ) itself for its logical coherence, physical plausibility, and goal-orientation. This Generative Visual Reasoning (GVR) capability is primarily actualized through the Chain-of-Frames\" (CoF) mechanism. We can formalize this generative process as an autoregressive sequence: = {f1, f2, . . . , fN }, fi = g(Ii, q, F<i), (1) where is the initial context or task query (e.g., Solve this maze), is the complete reasoning sequence, and F<i = {f1, . . . , fi1} is the generation history up to the previous step. fi is the i-th step in the sequence. As this work focuses on video generation models, fi represents the next generated video frame. The entire reasoning sequence thus constitutes complete and coherent video, which itself is the manifestation of the models thinking\" process. 3.2. Taxonomy of Generative Visual Reasoning Our proposed Gen-ViRe taxonomy is rooted in two complementary perspectives: (i) the theoretical foundations of cognitive science and (ii) the practical demands of key emerging GVR application domains. More specifically: (i) Cognitive Science Foundations: robust GVR benchmark should be able to evaluate the core pillars of human cognitive ability [4, 11, 19, 20, 2730]. This provides the theoretical basis for our categories of Perceptual Reason- (ii) ing, Analogical Reasoning, and Abstract Reasoning. Application-Driven Requirements: Concurrently, as AI expands from the symbolic to the physical realm, emerging applications (e.g., Embodied AI, Autonomous Driving) demand dynamic, multi-step reasoning [9, 36]. These capabilities are essential for the next frontier of AI but cannot be evaluated by existing benchmarks. These practical requirements provide the basis for our categories of Planning Reasoning, Spatial/Temporal Reasoning, and Algorithmic & Logical Reasoning. Grounded on the above two pillars, we decompose GVR into six critical and complementary dimensions that form full spectrum of capabilities, from foundational perception to high-order planning: (1) Perceptual Reasoning; (2) Analogical Reasoning; (3) Algorithmic & Logical Reasoning; (4) Spatial&Temporal & Dynamic Reasoning; (5) Procedural & Planning Reasoning; (6) Abstract Reasoning. This framework is not only theoretically robust but also practically essential. It provides the first comprehensive evaluation framework for this new paradigm, allowing us to scientifically quantify and diagnose the nascent reasoning abilities driven by the Chain-of-Frames paradigm. Perceptual Reasoning. This category probes models the ability to move beyond pasfoundational cognition: sive perception and actively reason about the logical relationships between visual attributes. We test four key logic types via worksheet-style puzzles: association (color), morphology (shape), quantification (quantity-to-numeral), and Gestalt (part-to-whole). Models must execute precise, procedural, spatio-temporal action (e.g., draw connecting line\") to demonstrate their conclusion, rather than just outputting static answer. Spatial & Temporal Reasoning. Tasks in this category assess models ability to reason about motion, causality, and change within continuous scenarios. These tasks require the model to generate temporally coherent and physically plausible chain-of-frames (CoF). We probe faculties such as the ability to predict and model motion and to plan and execute navigation under constraints (e.g., Autonomous Driving, VLA Manipulation, Maze Traversal, Spatial Obstacle Navigation). It measures the models ability to build an internal world model. Planning Reasoning. This category targets models higher-order cognitive ability to perform multi-step, goaldirected planning. It requires models to decompose complex goal into discrete, logical, and correctly-ordered sequence of sub-actions. We probe four domains: (1) Causal Tool Reasoning (e.g., selecting the correct tool); (2) Sequential Task Decomposition (e.g., changing lightbulb); (3) Hierarchical Digital Planning (e.g., GUI navigation); and (4) Physically-Constrained Assembly (e.g., brick-bybrick construction with physical stability). Analogical Reasoning. This dimension focuses on the capability of relational abstraction. We adopt the classic visual analogy task (A :: ?). The model must perform two-stage inferential process: (1) Discover the latent transformation rule by comparing the source pair (A B); and (2) Apply this inferred rule to the new target object C. Algorithmic & Logical Reasoning. Here, we evaluate models ability to follow and execute formal rules and constraints. It requires the model to apply symbolic reasoning to the visual domain to solve intellectual puzzles, including Visual Sudoku, Graph Traversal, Geometry, and Crosswords. In these tasks, the model must demonstrate its understanding of abstract rules and successfully apply them within the visual context to arrive at correct solution. Abstract Reasoning. This category measures models highest-order cognitive ability: identifying and extrapolatFigure 3. Qualitative examples of Gen-ViRe tasks. It illustrates sample inputs and their expected Chain-of-Frames (CoF) visual reasoning outputs across the six cognitive dimensions, highlighting the benchmarks breadth from foundational perception to high-order planning. ing abstract patterns and rules. This is closely related to human fluid intelligence.\" Tasks require the model to look beyond superficial features to discover the hidden generative principles in the data. Tasks we test include Symmetry, 2D/3D Rule Extrapolation, and Ravens Progressive Matrices. Success in these tasks indicates the model is not just pattern mimic but rule discoverer. 4. Data Curation 4.1. Data Collection As described in Section 3.2, our taxonomy covers six key dimensions. To construct diverse and information-rich benchmark capable of comprehensively evaluating GVR abilities, we designed multi-source data collection strategy tailored to the unique requirements of each category. Our data sources are primarily divided into three categories: Web and Academic Sources, Integration of Existing Datasets, and Generative Data Creation. Web and Academic Sources. The foundation of our data collection comes from public web resources and academic publications. We use targeted keywords to collect candidate images from search engines like Google. Concurrently, we extract high-quality figures, illustrations, and qualitative examples from relevant academic papers [16] [45]. Notably, for the Perceptual Reasoning category, we also sourced numerous childrens intelligence tests from the web (e.g., color, shape, and quantity matching puzzles). All collected materials underwent rigorous manual screening and secondary editing (e.g., cropping, annotation, content modification)such as the modifications made to the aforementioned Spatial Obstacle imageto ensure they perfectly align with the logical and visual constraints of our benchmark tasks. Integration of Existing Datasets. To evaluate model capabilities in specific domains, we extracted or adapted tasks from several public datasets. This includes GUI navigation datasets [22] for Planning Reasoning, Geometry datasets [44] for Algorithmic & Logical Reasoning, and tasks borrowed from KiVA [24] for Analogical Reasoning. Critically, to test high-order abstract reasoning, we also incorporated challenging tasks from the ARC-AGI benchmark [7], which is widely considered gold standard for evaluating fluid intelligence. Generative Data Creation. For many tasks in our benchmarkespecially in the Planning Reasoning category (e.g., Tool Selection and Use)no large-scale, logicallyconsistent datasets readily exist. Inspired by the qualitative examples in the pioneering work on Chain-of-Frames [46], we defined the generative rules and underlying logic for these advanced reasoning tasks and leveraged advanced Text-to-Image models to create entirely new visual puzzles. This approach allows us to systematically control task difficulty, compositional complexity, and generalization requirements, which is unachievable by passively collecting existing data. 4.2. Prompt Design and Validation Prompt Design and Validation. Our prompt design and validation process ensures high task fidelity through minimal prompting principle and rigorous, iterative peerreview workflow. It begins with core design adherence to minimal prompting, aiming to assess the models autonomous reasoning capabilities rather than its proficiency 5 Figure 4. The evaluation framework of Gen-ViRe. (a) Data Curation: Shows the benchmark development process, including defining the taxonomy, collecting data from multiple channels (web, existing datasets, AI generation), and designing & validating prompts through Peer Review. (b) Formulation of Evaluation Criteria: Demonstrates the process of formulating detailed, multi-dimensional evaluation criteria (as shown by C1-C5 in the figure) for each prompt of every subtask. (c) VLM-based Autorating Framework: Illustrates how the VLM (Autorater) conducts item-by-item analysis and automatic scoring of the generated videos based on the specific criteria defined in (b). in adhering to complex instructional formats. For instance, in our Embodied Spatial Obstacle task, the model is provided with an initial state image (e.g., first-person robot view of kitchen, blocked by large table and high stool) and high-level objective (e.g., This is robots first-person perspective. The task is to go to the coffee machine in the kitchen and get the paper towels.). We deliberately abstain from specifying how to handle the obstacles. successful output requires the model to autonomously reason about the implicit physical and spatial constraints. The prompt does not mention the obstacles, but the model must infer that (1) it must navigate around the large table, not pass through it, (2) it must perform the mandatory action of physically moving the stool aside, as bypassing or stepping over it is defined as failure, and (3) the interaction must be performed by robotic manipulator, not human hand, to adhere to the robots perspective constraint. Iterative Peer Review Process. To ensure the clarity and robustness of all task prompts, we implemented strict, iterative peer-review process. task draft formulated by one annotator is submitted to at least one other independent annotator for review. This review scrutinizes the task for clarity, potential ambiguities, and whether the ground truth is an indisputable, sole answer. Any flagged issues are returned for team discussion and revision. primary focus of this process is the resolution of ambiguous references, common source of model error. Our annotation team is trained to identify and rectify such vague language (e.g., ambiguous pronouns like it or that), replacing them with precise descriptions to ensure high prompt fidelity. 5. Evaluation Methodology Formulation of Evaluation Criteria. The core of our evaluation pipeline is the development of detailed criteria for each task subcategory. This formulation process is unique, hybrid approach combining VLM assistance with multiround human refinement. First, our team drafts preliminary evaluation standards for each task. We then provide these preliminary standards, along with the corresponding input image, text prompt, and task objective, to Gemini 2.5 Pro. The models role is to refine these standards into more detailed, rigorous, and operational evaluation rubric based on the full context of the specific task. Finally, the detailed criteria generated with VLM assistance undergo final multiperson review and refinement by our team to ensure absolute accuracy and consistency. VLM-Assisted Evaluation Methods. Our evaluation methodology employs powerful Vision Language Models (VLMs) as automated judges. We utilize Gemini 2.5 Pro [8] as our unified VLM judge, leveraging its respective modality capabilities based on the tasks requirements. For tasks where the evaluation is contingent upon the final visual output (such as analogical or geometric reasoning), we employ Gemini 2.5 Pro as an Image VLM judge. For more complex dynamic tasks (such as planning and spatial-temporal reasoning) that require assessing the entire generated process, we utilize Gemini 2.5 Pro as Video VLM judge. Crucially, for every task, the designated VLM judge (Gemini 2.5 Pro) is provisioned with detailed, sub-categoryspecific set of criteria. The judge then decomposes and assesses the models output against each criterion, providing an independent score for its decision. This criteria6 centric approach ensures consistent and rigorous evaluation across the entire benchmark. 6. Experiments 6.1. Experimental Setup Evaluated Models. We evaluated comprehensive suite of state-of-the-art (SOTA) video models, including Klingv1, Seedance-1.0-Pro, Seedance-1.0-Lite, Veo-3.1, Sora-2, Wan-2.5, and Hailuo-2.3. Videos were generated in either 16:9 or 9:16 aspect ratio, determined by the native orientation of the input task image. For generation duration, we adopted the default 5-second for the Hailuo-2.3, 8-second for Veo-3.1 and Sora-2, and the default 10-second for Klingv1, Seedance-1.0-Pro, Seedance-1.0-Lite, and Wan-2.5. Evaluation Metrics. The VLM-assisted evaluation process described in Section 5 yields an output score for each task. To aggregate these individual scores, we follow the aggregation strategy of MEGA-BENCH [5]. The score for each individual task is first normalized to consistent [0, 1] range, where 1.0 signifies perfect adherence to all standards. Subsequently, to report comprehensive performance, we compute the macro-mean score of all normalized scores. 6.2. Main Results 6.2.1. Qualitative Analysis and Case Studies Case 1: Analogical Reasoning (Rotation and Color) Figure 7. Showcase of Sora-2s failures in Spatio-Temporal & Dynamic Reasoning. The top row shows input images; the bottom shows Sora-2s mid-frames. These frames reveal fundamental failures in simulating basic physical laws: (Left) violating object permanence by showing dog phasing through closed glass door ; and (Middle) failing to simulate continuous process, instead spawning paper towels into the scene ; (Right) depicting telekinesis, where an object is retrieved without contact. Takeaway. Abstract logical reasoning (e.g., following algorithmic rules) and physical reality simulation (e.g., adhering to physical plausibility) are two distinct capabilities. The models excellence in the former does not equate to mastery in the latter. Case 3: Algorithmic & Logical Reasoning (Geometry) Figure 8. Showcase of Sora-2s in geometry task. Both Sora2 and Veo-3.1 failed to identify the existing point in the static image. This perceptual error caused them to ignore the instruction connect point to point and instead hallucinate new point D, connecting to this incorrect location. Takeaway. Models expose critical perceptual flaw in complex, symbol-reliant tasks (like geometry): they fail to parse in-context abstract symbols (like \"D\") as addressable logical components, instead misinterpreting them as incidental visual noise. Case 4: Algorithmic & Logical Reasoning (Sudoku) Figure 6. Showcase of Analogical Reasoning by Sora-2 and Veo-3.1. On the complex Rotation task (top row), both Sora-2 and Veo-3.1 failed to recognize and apply the abstract rotational rule. In sharp contrast, both models easily solved the simpler Color analogy (bottom row), which only required matching an attribute. I2V Prompt. Create smooth animation to generate the missing object in the lower right region and solve the visual analogy. The original three objects must remain still. Static shot, no zoom no pan no dolly. Takeaway. Current models analogical reasoning performance correlates directly with task abstraction complexity: they can easily solve simple attribute matching (e.g., color) but expose core deficits when handling abstract, rule-based transformations (e.g., rotation). Case 2: Spatio-Temporal Reasoning (Spatial Obstacle) Figure 9. Showcase of Sora-2s Sudoku task. In the Sudoku task, Sora-2 exhibits an emergent, human-like thinking process. The model uses question mark (?) as placeholder for the unknown value in the third row. This suggests it can hold an internal state of the problem (this cell is unsolved\"). Following the placeholder, the model generates frames that simulate the moving\" of numbers (2) into their correct, logically-deduced positions. Takeaway. Sora-2s Sudoku solving process (using ?\" placeholders and moving\" numbers) indicates it is acquiring genuine algorithmic capability, as it is simulating the problem-solving process of following Sudoku rules\" rather than just pattern-matching the final answer. Table 1. Performance comparison across different reasoning categories for various video generation models. We highlight the top-three performing models in each column with varying shades of purple, where darker shade indicates higher rank. Methods #Videos Avg. Abstract Algorithmic & Logical Analogy Perceptual Planning Kling-v1 [28] Seedance-1.0-Lite [13] Seedance-1.0-Pro [13] Wan-2.5 [41] Veo-3.1 [15] Hailuo-2.3 [34] Sora-2 [2] 360 360 360 360 360 360 360 0.198 0.279 0.301 0.490 0.486 0.493 0. 0.071 0.087 0.154 0.412 0.440 0.494 0.604 0.057 0.256 0.164 0.411 0.451 0.355 0.472 0.117 0.083 0.083 0.500 0.367 0.383 0.483 0.140 0.146 0.171 0.378 0.386 0.425 0.496 0.443 0.572 0.609 0.702 0.722 0.778 0.768 SpatioTemporal 0.359 0.532 0.621 0.536 0.550 0.524 0.537 Figure 5. Left: The main chart compares the overall performance of the 7 state-of-the-art models across the six core cognitive dimensions (Abstract, Algorithmic, Analogy, Perceptual, Planning, and Spatial Reasoning). Right: The six sub-charts provide detailed performance breakdown for the individual subtasks within each dimension. The legend (bottom) links each colored line to its respective model. 6.2.2. Quantitative Results 7. Conclusion Our evaluation of 7 state-of-the-art models across six reasoning dimensions reveals clear performance hierarchy, as shown in Figure 5 and Table 1. Sora-2 achieves the highest overall score (0.560), establishing the top tier with particularly strong performance in the most cognitively demanding domains: Abstract Reasoning (0.604), Algorithmic & Logical (0.472), and Perceptual (0.496). The second tier comprises three highly competitive modelsHailuo-2.3 (0.493), Wan-2.5 (0.490), and Veo-3.1 (0.486)each exhibiting distinct specialized strengths. Hailuo-2.3 achieves the highest score in Planning (0.778), showcasing exceptional sequential decision-making capabilities, while Wan-2.5 leads in Analogy (0.500), excelling at analogical reasoning. Veo-3.1 has balanced performance, ranking second in both Algorithmic & Logical (0.451) and Planning (0.722). In contrast, Kling-v1 (0.198) and Seedance1.0-Lite (0.279) form the lower tier with scores substantially below the leading models, indicating considerable room for improvement in reasoning capabilities. Video generation models are transitioning from visual synthesizers to potential world simulators capable of physically-grounded reasoning. However, without rigorous evaluation, we cannot distinguish genuine understanding from sophisticated pattern matching. Gen-ViRe addresses this by providing systematic assessment across six cognitive dimensions, establishing the foundation for quantitative science in generative visual reasoning. Our experiments reveal critical gap: while current models achieve impressive visual fidelity, they exhibit limitations in sustained logical coherence, physics compliance, and multi-step planning. By diagnosing these specific deficitswhether in perceptual grounding, spatial reasoning, or goal-directed planningGen-ViRe provides actionable insights to guide targeted improvements. As AI systems increasingly simulate and interact with physical reality, benchmarks like GenViRe are essential for measuring genuine progress toward intelligent world models that reason about the world, rather than merely rendering plausible pixels."
        },
        {
            "title": "References",
            "content": "[1] M. Bober-Irizar, S. Recanatesi, J. Collins, et al. Neural networks for abstraction and reasoning. Scientific Reports, 14: 73582, 2024. 2 [2] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. Technical report, OpenAI, 2024. 2, 3, 8 [3] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In ICML, 2024. 2, 3 [4] Ruth Byrne. The Rational Imagination: How People Create Alternatives to Reality. MIT Press, 2007. 2 [5] Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, et al. Mega-bench: Scaling multimodal evaluation to over 500 real-world tasks. arXiv preprint arXiv:2410.10563, 2024. [6] Xi Chen, Kai Wang, and Juncheng Li. Videodreambench: benchmark for personalized text-to-video generation. arXiv preprint arXiv:2311.08937, 2023. 2 [7] François Chollet. On the measure of intelligence. arXiv preprint arXiv:1911.01547, 2019. 5 [8] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 6 [9] Chaorui Deng, Deyao Zhu, Kunchang Li, Chenhui Gou, Feng Li, Zeyu Wang, Shu Zhong, Weihao Yu, Xiaonan Nie, Ziang Song, et al. Emerging properties in unified multimodal pretraining. arXiv preprint arXiv:2505.14683, 2025. 3 [10] Carl Doersch and et al. Tap-vid: benchmark for tracking any point in any video. In Advances in Neural Information Processing Systems, 2022. 2 [11] Fanda Fan, Chunjie Luo, Wanling Gao, and Jianfeng Zhan. AIGCBench: Comprehensive evaluation of image-to-video BenchCouncil Transactions content generated by AI. on Benchmarks, Standards and Evaluations, 3(4):100152, 2024. [12] Y. Feng, R. Huang, X. Li, et al. Embodied ai: From arXiv preprint large language models to world models. arXiv:2509.20021, 2025. 2 [13] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. 3, 8 [14] Dedre Gentner. Structure-mapping: theoretical framework for analogy. Cognitive Science, 7(2):155170, 1983. 2 [15] Google DeepMind. Veo: general purpose-built ai model for video understanding and generation. Technical report, Google, 2024. 2, 3, 8 [16] Ji Gu, Boyuan Chen, and Yuke Zhu. Controlvla: Few-shot object-centric adaptation for pre-trained vision-languageaction models, 2024. 5 [17] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 1, [18] Hui Han, Siyuan Li, Jiaqi Chen, Yiwen Yuan, Yuling Wu, Chak Tou Leong, Hanwen Du, Junchen Fu, Youhua Li, Jie Zhang, Chi Zhang, Li Li-jia, and Yongxin Ni. Video-bench: Human-aligned video generation benchmark. arXiv preprint arXiv:2504.04907, 2025. 2 [19] Wenyi Hong, Ming Ding, Wendi Zheng, Xinghan Liu, and Jie Tang. Cogvideo: Large-scale pretraining for text-to-video generation via transformers. arXiv preprint arXiv:2205.15868, 2022. 2, 3 [20] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [21] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 3 [22] ikaixin. Screenspot-pro: benchmark for gui navigation. https : / / github . com / ikaixin / ScreenSpot - Pro. Accessed: 2024-11-13. 5 [23] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 1, [24] Zhiyuan Jiang, Chen-Lin Zhang, Zixuan Liu, Yizhou Wang, Yixuan Weng, Zhaoyu Li, and Haotian Liu. Kiva: benchmark for evaluating visual-analogical reasoning in large vision-language models, 2024. 5 [25] Philip Johnson-Laird. Mental Models: Towards Cognitive Science of Language, Inference, and Consciousness. Harvard University Press, 1983. 2 [26] Daniel Kahneman. Thinking, Fast and Slow. Farrar, Straus and Giroux, 2011. 2 [27] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 2, 3 [28] Kuaishou Technology. Kling ai, 2024. 2, 3, 8 [29] Rakesh Kumar, Ashutosh Singh, and Zhenzhong Li. Tempo: Benchmarking temporal consistency in video generation. arXiv preprint arXiv:2312.07897, 2023. [30] Wenhao Li, Changan Chen, Haofei Lin, and Hexiang Hu. Mvbench: comprehensive video understanding benchmark. In NeurIPS Datasets and Benchmarks, 2023. 2 9 [47] Shentong Wu, Yuxiang Chen, Rui Huang, and Dan Xu. Vq2: unified benchmark for text-to-video quality assessment. arXiv preprint arXiv:2404.05678, 2024. 2 [48] L. Xie, Y. Yan, Y. Tang, et al. survey of neurosymbolic visual reasoning with scene graphs and common sense knowledge. Neurosymbolic AI Journal, 2023. 2 [49] H. Zhang, K. Liu, L. Zhao, et al. Embodiedvsr: Dynamic scene graph-guided chain-of-thought reasoning for visual spatial tasks. arXiv preprint arXiv:2503.11089, 2025. 2 [50] Li Zhang, Wenqin Chen, and Hao Su. Mme-video: comprehensive benchmark for video-based multimodal llm evaluation. arXiv preprint arXiv:2405.01234, 2024. 2 [31] X. Liang, Y. Wang, Z. Chen, et al. Ai reasoning in the deep learning era: From symbolic ai to neuralsymbolic ai. Mathematics, 13(11):1707, 2025. [32] Yuanxin Liu, Lei Li, Shuhuai Ren, Rundong Gao, Shicheng Li, Sishuo Chen, Xu Sun, and Lu Hou. FETV: benchmark for fine-grained evaluation of open-domain text-tovideo generation. In Advances in Neural Information Processing Systems (NeurIPS), Datasets and Benchmarks Track, 2023. 2 [33] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evalIn Proceedings of uating large video generation models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2213922149, 2024. 2 [34] MiniMax. Hailuo ai video generation, 2024. 8 [35] Allen Newell and Herbert Simon. Human Problem Solving. Prentice-Hall, 1972. 2 [36] John Raven. The ravens progressive matrices: review and critical assessment. Journal of Educational Measurement, 38(1):138, 2000. [37] Elizabeth Spelke and Katherine Kinzler. Core knowledge. Developmental Science, 10(1):8996, 2007. [38] Elizabeth Spelke, Sang Ah Lee, and Veronique Izard. Origins of knowledge. Proceedings of the National Academy of Sciences, 109(Supplement 2):20182033, 2012. 2 [39] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: comprehensive benchmark for compositional text-to-video generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 2 [40] Zhengzhong Tu, Yuming Wang, Hojat Yeganeh, and Alan Bovik. Videval: no-reference video quality assessment IEEE Transactions on Image Processing, 30: benchmark. 44494464, 2021. 2 [41] Alibaba Wan Team. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. 3, 8 [42] Xiaoyang Wang, Chengzhi Wu, Jiawei Li, and Hang Xu. Longvideobench: Benchmarking long-horizon text-to-video generation. arXiv preprint arXiv:2403.12345, 2024. 2 [43] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. NeurIPS, 2022. 1, [44] Yue Weng, Zejun Li, Shujin Huang, Rui Wang, Chen-Lin Zhang, Haotian Liu, and Xindi T. Wang. Geolaux: benchmark for evaluating mllms geometry performance on longstep problems requiring auxiliary lines, 2024. 5 [45] Thaddaus Wiedemer, Paul Vicol, Yuxuan Li, Shixiang Shane Gu, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Test-time scaling with world models for spatial reasoning, 2024. 5 [46] Thaddäus Wiedemer, Yuxuan Li, Paul Vicol, Shixiang Shane Gu, Nick Matarese, Kevin Swersky, Been Kim, Priyank Jaini, and Robert Geirhos. Video models are zero-shot learners and reasoners. arXiv preprint arXiv:2509.20328, 2025. 2, 3,"
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "UW-Madison",
        "University of Central Florida"
    ]
}