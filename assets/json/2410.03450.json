{
    "paper_title": "MLLM as Retriever: Interactively Learning Multimodal Retrieval for Embodied Agents",
    "authors": [
        "Junpeng Yue",
        "Xinru Xu",
        "Börje F. Karlsson",
        "Zongqing Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "MLLM agents demonstrate potential for complex embodied tasks by retrieving multimodal task-relevant trajectory data. However, current retrieval methods primarily focus on surface-level similarities of textual or visual cues in trajectories, neglecting their effectiveness for the specific task at hand. To address this issue, we propose a novel method, MLLM as ReTriever (MART), which enhances the performance of embodied agents by utilizing interaction data to fine-tune an MLLM retriever based on preference learning, such that the retriever fully considers the effectiveness of trajectories and prioritize them for unseen tasks. We also introduce Trajectory Abstraction, a mechanism that leverages MLLMs' summarization capabilities to represent trajectories with fewer tokens while preserving key information, enabling agents to better comprehend milestones in the trajectory. Experimental results across various environments demonstrate our method significantly improves task success rates in unseen scenes compared to baseline methods. This work presents a new paradigm for multimodal retrieval in embodied agents, by fine-tuning a general-purpose MLLM as the retriever to assess trajectory effectiveness. All benchmark task sets and simulator code modifications for action and observation spaces will be released."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 0 5 4 3 0 . 0 1 4 2 : r Preprint. MLLM AS RETRIEVER: INTERACTIVELY LEARNING MULTIMODAL RETRIEVAL FOR EMBODIED AGENTS Junpeng Yue1, Xinru Xu2, Börje F. Karlsson3, and Zongqing Lu1 1School of Computer Science, Peking University 2Institute of Software, Chinese Academy of Sciences 3Beijing Academy of Artificial Intelligence"
        },
        {
            "title": "ABSTRACT",
            "content": "MLLM agents demonstrate potential for complex embodied tasks by retrieving multimodal task-relevant trajectory data. However, current retrieval methods primarily focus on surface-level similarities of textual or visual cues in trajectories, neglecting their effectiveness for the specific task at hand. To address this issue, we propose novel method, MLLM As ReTriever (MART), which enhances the performance of embodied agents by utilizing interaction data to fine-tune an MLLM retriever based on preference learning, such that the retriever fully considers the effectiveness of trajectories and prioritize them for unseen tasks. We also introduce Trajectory Abstraction, mechanism that leverages MLLMs summarization capabilities to represent trajectories with fewer tokens while preserving key information, enabling agents to better comprehend milestones in the trajectory. Experimental results across various environments demonstrate our method significantly improves task success rates in unseen scenes compared to baseline methods. This work presents new paradigm for multimodal retrieval in embodied agents, by fine-tuning general-purpose MLLM as the retriever to assess trajectory effectiveness. All benchmark task sets and simulator code modifications for action and observation spaces will be released. Figure 1: Similarity-Based Retriever vs. MART. Traditional multimodal retrieval methods (1) depend on calculating weighted sums of image and text embedding similarities, while our approach (2) introduces interactive learning to assess the relevance between the current and expert trajectories."
        },
        {
            "title": "INTRODUCTION",
            "content": "Embodied agents interacting with complex environments require understanding both the current context and task-specific domain knowledge to perform effectively (Wang et al., 2023c; Lifshitz et al., 2023). Recently, Multimodal Large Language Models (MLLMs), which are capable of processing both textual and visual data, have shown promise in various embodied tasks e.g., table manipulation (Handa et al., 2023; Chen et al., 2022a), robot navigation (Zhang et al., 2024b; Shah et al., 2022), and 3D games (Wang et al., 2024a; Tan et al., 2024; Jiang et al., 2024). However, such models typically lack effective grounding in the embodied environments in which agents operate, greatly limiting their performance in embodied tasks (Long et al., 2024; Wang et al., 2024c). Junpeng Yue and Xinrun Xu work as interns at BAAI. Correspondence to Zongqing Lu <zongqing.lu@pku.edu.cn>. 1 Preprint. To mitigate this limitation, providing additional task-relevant grounding information is essential to better leverage the general capabilities of MLLMs. Trajectory data, consisting of sequences of actions and observations, can be easily available and provide valuable insights into task execution (Zheng et al., 2024; Zhao et al., 2024), therefore serving as good information source for grounding. By using trajectory data in prompting an MLLM, the embodied agent can readily leverage previous experiences to better guide agents through similar tasks in new situations or environments (Zhang et al., 2024a; Lee et al., 2024). However, retrieving the most effective trajectories those that can significantly enhance task performance remains challenge, particularly when multiple trajectories appear similar in both textual and visual modalities (Jeurissen et al., 2024). Existing retrieval methods mainly focus on surface-level textual or visual similarities of trajectories, often neglecting key aspects critical for task effectiveness, e.g., trajectory with similar task instruction but in different scene, or one in the same scene but with different layout. In such cases, these trajectories fail to provide useful information for the current task and can mislead the agent. As shown in Figure 2, relying solely on similarity is not effective in retrieving useful trajectories, as similarity does not directly correlate with success rate. To better support agents in embodied tasks, trajectory retriever model needs to consider the effectiveness of trajectories for given task. (a) AI2-THOR (b) LEGENT Figure 2: Scatter plots illustrating the relationship between success rate and embedding similarity (left) or effectiveness score (right) in two environments. The red line indicates linear fit to the data. To achieve better retriever, we propose new paradigm that integrates interactive learning with the retriever. Firstly, we consider expert trajectories of training scenarios as prompt for an MLLM agent, and let the agent interact with the environment to collect different success rates for different such reference trajectories. This interactive feedback data is then organized into preference pairs, which are used to fine-tune an MLLM LLaVA (Liu et al., 2023) in our case with Bradley-Terry model (Bradley & Terry, 1952), such that the fine-tuned retriever model is capable of prioritizing more effective trajectories for unseen tasks. Combining this functionality with the inherent general capabilities of MLLM allows embodied agents to operate more effectively in unseen environments by leveraging their most useful past experiences. We also introduce new Trajectory Abstraction mechanism, which uses MLLMs summarization capabilities to represent trajectories in reduced number of tokens, while preserving key information and enabling agents to better understand such information in the trajectory (e.g., key relevant overarching actions). This mechanism is especially important in long-horizon tasks, both reducing the required context window length and removing distracting information from trajectory samples. Combining the aforementioned components, we present our approach MART (MLLM As ReTriever) which adapts embodied agents in unseen scenarios by fine-tuning MLLM through preference data. To assess the benefits of our method, we conduct empirical experiments across diverse environments. The experimental results show that MART achieves significantly higher task success rates compared to baselines, demonstrating its effectiveness. With this approach, we present new paradigm for multimodal retrieval in embodied agents, fine-tuning general-purpose MLLM as retriever capable of considering trajectory effectiveness. Our contributions can be summarized as follows: To the best of our knowledge, MART is the first approach that integrates interactive learning with retriever and uses interactive feedback to fine-tune an MLLM retriever in evaluating trajectory effectiveness, combining its inherent general capabilities with the ability to assess the task-guiding effectiveness of trajectories. 2 Preprint. We introduce Trajectory Abstraction, new mechanism that utilizes MLLM capabilities to significantly condense trajectories. This method reduces the token number while retaining essential information, allowing agents to effectively use this condensed knowledge in novel situations and provide guidance for long-horizon tasks. The effectiveness of MART is empirically validated through comprehensive experiments in various environments, demonstrating significant performance improvements on unseen tasks. MART consistently surpasses baselines by over 10% across different environments."
        },
        {
            "title": "2.1 EMBODIED AGENTS BASED ON LARGE MODELS",
            "content": "Recently, there have been several attempts to utilize the general-purpose capabilities of large models for complex embodied tasks. These efforts can be broadly categorized into two types: VLA models and LLM/MLLM-based agents. 1) VLA models, including PaLM-E (Driess et al., 2023), RT-2 (Brohan et al., 2023), Gato (Reed et al., 2022), VIMA (Jiang et al., 2022), and MOO (Stone et al., 2023), rely on trajectory data to train Transformer-based VLM for action planning, without explicitly constructing memory. However, their generalization capabilities are limited due to the inherent issue of catastrophic forgetting in neural networks. 2) LLM/MLLM-based agents, like Voyager (Wang et al., 2024a) and DEPS (Wang et al., 2023b) for Minecraft, Cradle (Tan et al., 2024) for RDR2, LLM-Planner (Song et al., 2023) for ALFRed, and Code-as-Policies (Liang et al., 2023) for real-world embodied control, do not involve directly training new models. Instead, they leverage the general-purpose capabilities of LLM/MLLM primarily through prompt engineering. Most of these agents build and maintain comprehensive memory systems to assist in task completion. However, memory retrieval mostly focuses on surface-level similarity, overlooking actual effectiveness in completing complex tasks."
        },
        {
            "title": "2.2 MEMORY RETRIEVAL IN AGENTS",
            "content": "Agents can continuously learn and improve by recalling task-related experiences (Zhang et al., 2024c; Xi et al., 2023). During interactions with the environment, two main types of information are stored in memory. 1) Semantic information: Early LLM agents faced limitations due to input token constraints, leading to reliance on short-term memory and greedy strategies (Chen et al., 2024; Zhang et al., 2023; Abdelnabi & et al, 2023; Wang et al., 2023a). However, summarizing memory over short periods risked information loss (Light et al., 2023; Kaiya & et al, 2023). While storing comprehensive memory (semantic, episodic, procedural) can provide great value, its effective utilization for decision-making remains challenging (Sumers et al., 2024). More recent approaches try to prioritize relevant memories based on embedding similarity or LLM-based relevance identification (e.g., (Park & et al., 2023), (Hong & et al., 2023), (Lin et al., 2023a), (Wang et al., 2024b), and (Xu et al., 2023)). 2) Image information: Recent advances in MLLM agents have greatly enhanced their grounding abilities by allowing image memory retrieval during actions. As illustrated in Figure 1, Similarity-Based Retrievers for agents, e.g., (Zhou et al., 2024; Wang et al., 2023c; Li et al., 2024), leverage image embedding. while Groot (Cai et al., 2023) encodes visual and temporal information from video frames for guiding actions. But neither considers direct task effectiveness."
        },
        {
            "title": "2.3 MULTI-MODAL INFORMATION RETRIEVAL",
            "content": "Recent advances in multimodal retrieval have developed various methods for encoding, fusing, and measuring similarities across different modalities. ViLT (Kim et al., 2021) directly embeds image patches with text using Transformer, while ALIGN (Li et al., 2021) and MURAL (Jain et al., 2021) use dual-encoder architectures with EfficientNet and BERT to align modalities through contrastive learning. IMAGEBIND (Girdhar et al., 2023) extends this by creating joint embeddings for six modalities, using ViT and Transformer models. Furthermore, (Changpinyo et al., 2021) integrates users mouse trace interactions for refined image retrieval, while ReViz (Luo et al., 2023) employs advanced encoding mechanisms for visual question answering. Building on these encoding strategies, retrieval augmentation further enhances multimodal generation (e.g., RAG (Lewis et al., 2020)). FLMR (Lin et al., 2023b) addresses RA-VQA limitations by combining multidimensional embeddings from ColBERTv2 and ViT-based models for accurate knowledge retrieval. 3 Preprint. RA-CM3 (Yasunaga et al., 2023) enhances image captioning and text-to-image generation by using pre-trained CLIP model to augment inputs for CM3 Transformer. Similarly, UniRAG (Sharifymoghaddam et al., 2024) integrates retrievals using UniIRs (Wei et al., 2023) CLIP Score Fusion and BLIP Feature Fusion, improving performance in MLLMs like LLaVA. Lastly, MuRAG (Chen et al., 2022b) introduces retrieval-augmented transformer for KB-VQA, employing T5 and ViT for multimodal encoding and retrieval from large-scale memory bank. In contrast, our method prioritizes the effectiveness of retrieved information by employing interactive learning, ensuring that the information contributes directly to task completion."
        },
        {
            "title": "INTERACTIVELY LEARNING MULTIMODAL RETRIEVAL",
            "content": "Figure 3: Overview of MART. Our approach interactively learns multimodal retriever to score expert trajectories and retrieve most effective trajectory to guide an agent in novel situations. By considering trajectories with higher success rates as positive samples and those with lower success rates as negative trajectories, we obtain the preference pairs, which are used to fine-tune an MLLM retriever to score trajectory effectiveness for specific task."
        },
        {
            "title": "3.1 PROBLEM FORMULATION",
            "content": "In this study, we investigate interactions of retrieval-augmented MLLM agent with an environment to complete embodied tasks drawn from specific distribution. Figure 3 provides an overview of our MLLM As ReTriever approach MART. The agent is assigned task instruction ℓc sampled from the task instruction distribution p(ℓ), and operates over finite horizon H. At each timestep 1, 2, ..., H, the agent selects an action at from the action space based on the current observat from the observation space and reference trajectory τ retrieved from the expert trajectory tion oc memory M. The memory contains set of multimodal expert trajectories = {τ n}, where each trajectory τ sampled from the same task instruction distribution p(ℓ). The trajectory τ = {oe }, and i1 action sequences ae also contains observation sequences oe , ..., ae , ae iH i2 includes task instructions ℓe = {ae i1 , ..., oe iH 2 , ..., τ 1 , τ , oe i2 }. The agent follows frozen policy π(aℓc, τ e, oc), implemented as Multimodal Large Language Model (MLLM), and the reference trajectory τ plays significant role in grounding the agent within the embodied environment, supporting task accomplishment. This reference trajectory is retrieved through an MLLM retriever qθ. We fine-tune our MLLM retriever on training task distribution ptrain(ℓ) and evaluate its performance on test task distribution ptest(ℓ), which has no overlap with training tasks. This retriever aims to identify and retrieve trajectory τ from the expert memory pool that is most effective for the current task ℓc, i.e., which can help ground the MLLM agent with specific embodied task and enable its effective completion. It is worth noting that we have different memories Mtrain and Mtest, which corresponds to training tasks and test tasks, respectively. Preprint."
        },
        {
            "title": "3.2 MEMORY",
            "content": "To enable trajectory retrieval for task execution and fine-tuning our MLLM retriever, we first construct memory databases containing expert trajectories from previous successful executions for tasks , oe }, represents task ℓe = {ℓe both from ptrain(ℓ) and ptest(ℓ). For each trajectory, τ completed , and corresponding actions ae in Hi steps and comprises the sequence of observations oe . In multi-modal environments, such as AI2-THOR (Kolve et al., 2017) and LEGENT (Cheng et al., 2024), each timestep observation oi includes an egocentric image. Moreover, in the AI2-THOR environment, we assign numerical IDs (e.g. Cup 1) to all objects in the current visible field of view to identify target objects for interaction. These IDs appear in the environment feedback output in natural language, which is also part of the observation. , ae Expert trajectories are collected via planner-based method (Hoffmann & Nebel, 2001). Storing these trajectories allows the agent to later leverage past experience when facing new task instances. Trajectory data is collected independently for the training and test sets. For each task, we initialized task instance and used planner-based method to collect expert trajectory data. It is worth mentioning that the initialization position and orientation in each task is randomly chosen. Since each task is directly corresponds to one unique task in the task set, the size of our memory used for experiments is relatively small. For example, in the experiments performed in the LEGENT environment, the memory pool consisted of 40 trajectories for training, and distinct 32 trajectories during testing. More details are available in the experimental settings (Section 4.1)."
        },
        {
            "title": "3.3 MULTIMODAL RETRIEVER",
            "content": "The core of MART is the innovative use of interactive learning to train the trajectory retriever. For an embodied task, different trajectories stored in memory can be provided as references to the MLLM agent, leading to varying effects on the completion of the current task, depending on the degree of grounding with the environment they provide. Even if trajectory has text instruction similar to the current task or an image sequence similar to the initial egocentric observation of the task, it does not guarantee that this trajectory can provide effective grounding. This is due to plain similarity alone not being able to reflect the effectiveness of the trajectory for the embodied task. For instance, failed trajectory for related task could have high textual and visual similarity to the target task. In order to retrieve the trajectory that can provide the most benefit (i.e., effective grounding) for the current task from the trajectory memory, we propose an interactive learning method for the MLLM retriever. Specifically, for each task in training set, we sample trajectories from the training memory Mtrain, and feed them as prompt for MLLM agent to execute the embodied task respectively. After that, based on the induced success rates of task execution, we can get the effectiveness of each trajectory for the embodied task. We can then obtain partial order list based on success rate comparisons, producing (cid:0)K (cid:1) pairs through pairwise comparison, where the trajectory with higher success rate is treated as the positive item, and the one with lower success rate as the negative item. In this way, these preference pairs from interactive feedback are arranged as positive-negative pair dataset D, which we use to fine-tune the MLLM according to the Bradley-Terry (Bradley & Terry, 1952) Reward Modeling loss to enhance its critiquing ability, as in Equation 1: 2 L(θ) = E(ℓc 1,oc 1,A(τ w),A(τ ))D [log (σ (qθ(ℓc, oc 1, A(τ w)) qθ(ℓc, oc 1, A(τ ))))] . (1) In particular, we use LLaVA-7B (Liu et al., 2023) as base MLLM and add Bradley-Terry score head based on hidden states of base model output. The score head is one-layer MLP that takes as input the last token in the hidden state and outputs scalar score. The input to the MLLM retriever includes the trajectory abstraction result of expert trajectory τ ), current observation oc 1, current task instruction ℓc, and prompt for it to judge the effectiveness of this trajectory for the current task and state. Upon inference, the MLLM outputs the score of the trajectory, which indicates its effectiveness for the current task. We select the trajectory with the highest score as the reference for the embodied agent to complete the current task. , A(τ 5 Preprint."
        },
        {
            "title": "3.4 TRAJECTORY ABSTRACTION",
            "content": "A complete multimodal trajectory often has dozens of timesteps, which correspond to dozens of observations, actions, and feedback, including redundant or irrelevant information. Furthermore, inputting all trajectory tokens, especially image tokens, to the MLLM retriever and agent will likely lead to confusing the models/agent or even exceeding their context windows. We thus use another MLLM (in our experiments, GPT-4o) to automatically create an abstract trajectory in zero-shot manner. The initial input to the MLLM is the trajectory τ (consisting of task instruction ℓe, observation sequence oe, and action sequence ae) as well as the current task instruction ℓc, and we let the MLLM find whether each observation contained in the trajectory τ is helpful for the current task ℓc. If it is considered to be useful, we will keep the observation into the resulting trajectory abstraction A(τ e). To be more specific, we let the MLLM comprehend the tasks accomplished in the given trajectory τ e, and then identify important observations in the trajectory as milestones and preserve them into the resulting trajectory abstraction. These milestones are steps that are essential for accomplishing the trajectory task of ℓe, such as steps where important decisions are made, goals are achieved, or notable changes in the environment or state occur. Also if the target object of current task instruction ℓc appears in the trajectory feedback, then the step where it appears is also considered to be significant milestone as it is strongly related to current task ℓc. The milestone output format consists of: 1) description of the milestone; 2) the corresponding image ({image x}); 3) the corresponding feedback ({feedback x}); and 4) the overarching actions taken between this milestone and the next one. The summarized trajectory manages to remove redundant information without affecting relevance for the current task ℓc, so that the agent can better receive grounding information contained in the trajectory. Quantitatively, for the tasks in the test set, the input trajectories of trajectory abstraction have an average of 11.51 steps, while the output milestones have an average count of 3.13."
        },
        {
            "title": "4.1 EXPERIMENTAL SETUP",
            "content": "(a) AI2-THOR (b) LEGENT Figure 4: Environment comparison."
        },
        {
            "title": "4.1.1 ENVIRONMENTS",
            "content": "Table 1: Environment complexity comparison. AI2THOR contains more than four times the number of interactive objects per scene, and more complex object hierarchy, compared to LEGENT. E.g., AI2-THOR supports relationships such as inside\" (e.g., inside microwave (Figure 7a), or inside an open container, like sink (Figure 7c)). AI2-THOR LEGENT Avg. Objects/Scene Object Hierarchy Layout Complexity Task decomposition Observation"
        },
        {
            "title": "11.13\nOn\nLow\nNo\nImage",
            "content": "To validate the effectiveness of our method in various environments, we perform evaluations on multiple scenarios in two environments, AI2-THOR (Kolve et al., 2017) and LEGENT (Cheng et al., 2024). Unlike LLM agents, which use only text as input and simplify the action space by employing teleportation actions (e.g., move directly to target\"), we believe that MLLM agents should undertake more challenging tasks as they have access to visual input and are no longer limited to blind mode of operation. Therefore, both target environments are multimodal, whose observations are egocentric images, and both make use of fine-grained control actions. 6 Preprint. AI2-THOR simulates embodied household tasks supporting natural language instructions and egocentric visual observations, where agents must navigate and interact with various household items within realistic 3D environments, including kitchens, living rooms, and other indoor spaces. it allows agents to perform fine-grained navigation actions including move ahead, turn left/right degrees, look up/down, and interactive atomic action including pick up object A, put object on/in object B, open/close object A, toggle on/off object A. LEGENT is designed to imitate human activities and tasks in home environments, including crossroom navigation. The action space is similar to AI2-THORs, including fine-grained movement actions. It also includes speak action, which sends message to the user. Table 1 and Figure 4 show comparison between the two environments. Besides having many more objects in its scenes, AI2-THOR supports more complex object hierarchy relationships, such as inside\" (e.g., inside cabinet\", which require open/close interaction to complete task, or cup inside sink\", i.e., an open container), not supported in LEGENT 1."
        },
        {
            "title": "4.1.2 TASK SETTINGS",
            "content": "Notably, in all tasks, the initial position and orientation of the agent is chosen randomly. Each task is tested 5 times to reduce the impact of random errors. AI2-THOR. To better assess the fine-grained control ability of MLLM agents to complete realworld embodied tasks, we integrate characteristics of two AI2-THOR-based benchmarks ALFWorld (Shridhar et al., 2021) and ALFRed (Shridhar et al., 2020) and built an environment setting that is more suitable for the MLLM agent. Since tasks are long-horizon, we follow the method in ALFRed and apply task decomposition to divide them into sub-tasks before execution. Each sub-task is then provided to the agent, and it determines sub-task success based on environmental feedback by itself (details in Appendix B.2 and B.3 ). Once sub-task is successfully completed, the agent proceeds to the next sub-task. Task types include pick_and_place, pick_clean_then_place, pick_cool_then_place, and pick_heat_then_place. Completing these tasks requires dozens of steps of navigation, as well as interaction with objects. There are 45 tasks comprising total of 260 sub-tasks in training set, and 28 tasks including 158 sub-tasks in testing set. LEGENT. Tasks for the MLLM agent are categorized into two types: Come Here and Where Is. Each task is further divided into One-room and Two-room types, based on whether it requires traversing between rooms. In LEGENT, task decomposition is not performed as task instructions are simpler and do not contain combinations of sub-tasks; i.e., the granularity of tasks is similar to that of sub-tasks in AI2-THOR. To train the retriever, we use 40 tasks (10 tasks for each task type) and we use 32 tasks, also covering all task types, as test set."
        },
        {
            "title": "4.1.3 MEMORY CONSTRUCTION",
            "content": "Memory initialization follows the procedure described in Section 3.2; with randomized starting positions. AI2-THOR. Once trajectories are collected, we decompose them into sub-task trajectories (following the task decomposition procedure in ALFRed) and treat each sub-task level trajectory as expert trajectory into memory. Similar redundant trajectories are then filtered out, accounting for about one-third of total, resulting in collection of 170 memory trajectories for the training memory and 118 trajectories for the testing memory. LEGENT. As decomposition is not necessary due to the simpler tasks in this environment, the training memory is initialized with 40 trajectories, and the test memory with distinct 32 trajectories; one trajectory per task."
        },
        {
            "title": "4.1.4 TASK EVALUATION",
            "content": "To evaluate the effectiveness of the retrieval-augmented embodied agents, we assess their performance using two metrics: Success Rate (SR) and Average Steps (AS). 1Although there are cabinets in LEGENT, they do not need to be opened/closed to complete tasks. 7 Preprint. Success Rate denotes the percentage of tasks attempts successfully completed by the agent. In AI2-THOR it indicates the percentage of completed full tasks, and we additionally use SR-Sub to represent the percentage of completed sub-tasks. Average Steps represents the average number of steps the agent takes to complete task. In AI2THOR it indicates the number of steps to complete full task, and AS-Sub represents the step average to complete sub-task."
        },
        {
            "title": "4.1.5 BASELINES",
            "content": "It is worth noting that MART is the first work to retrieve multimodal trajectories as references for embodied MLLM agents and let the agent directly output fine-grained control actions. We compare MART against three baseline methods to explore the performance of our approach: Plain-Agent (PA) is an embodied MLLM agent without making use of reference trajectories, i.e., without any memory. In our experiments, we use GPT-4o (2024-05-13 version). LLaVA-Plain (LP) is pre-trained LLaVA with no modified head and no finetuning. We use the probability of special token generation to represent the score. Its input is the same as MART, and it is prompted to output only Yes/No tokens, and the final score is calculated based on the probability of token generation (more details and limitations in Appendix A.3). Similarity+LLaVA (SL) is reasonable retrieval+ranking approach. Such approach is common in the text retrieval field e.g., Sun et al. (2023b;a); Dong et al. (2024) and it can take into account both similarity and effectiveness. We use similarity to choose the top-K candidate trajectories, and then choose the most likely effective one using plain LLaVA model (i.e., same as LP). RAP (Kagaya et al., 2024) performs retrieval based on plain similarity per modality and is the most similar setting in literature. It mainly targets text modality experiments in the ALFWorld (Shridhar et al., 2021) and WebShop (Yao et al., 2022) (treated as text-only) environments, and simple tasks in the multimodal environments Franka-Kitchen (Gupta et al., 2019) and Meta-World (Yu et al., 2019)."
        },
        {
            "title": "4.2 AI2-THOR",
            "content": "We firstly demonstrate the effectiveness of MART over test tasks in the AI2-THOR environment. The experimental results demonstrate the effectiveness of our approach compared to the baselines. As shown in Table 2, MART surpasses all baselines over 10% in Success Rate, and reaches best performance across all metrics. Table 2: Performance comparison of different methods in AI2-THOR. PA LP SL"
        },
        {
            "title": "RAP MART",
            "content": "SR SR-Sub AS AS-Sub 0.18 0.63 159.66 44.65 0.26 0.69 144.18 39.88 0.24 0.68 147.48 40.79 0.22 0.67 147.03 40.67 0.40 0.75 123.19 34."
        },
        {
            "title": "4.3 LEGENT",
            "content": "We then conduct experiments in the LEGENT environment. This environment includes tasks involving crossing between rooms, thereby enriching the experimental space and demonstrating the effectiveness of our method. The experimental results, shown in Table 3, demonstrate MART greatly surpasses all baselines in all four task types."
        },
        {
            "title": "4.4 ABLATIONS",
            "content": "In this section, we use set of ablation studies to examine the contribution of key components in MART. More specifically, we aim to answer the following questions. Q1. Does Trajectory Abstraction indeed improve embodied agent performance? 8 Preprint. Table 3: Performance comparison of different methods in LEGENT. PA LP"
        },
        {
            "title": "SL RAP MART",
            "content": "SR AS 0.70 23.62 0.69 25.01 0.75 20.92 0.75 20.62 0.87 13. We compare the full MART approach and the ablation of removing its Trajectory Abstraction module (w/o Abstraction). As shown in Table 4, MART consistently reaches the best results across settings. Even if in AI2-THOR sub-tasks the average success rate is comparable, the improvement margin leads to 9 percentage points improvement in full task success rate. Q2. How does the MART approach compare against typical retrieve and rank approach, even if it uses fine-tuned ranking model for trajectory usefulness? Table 4 shows decomposing the unified MART approach into separate retrieval and ranking steps (i.e., similarity-based retrieval and MARTs MLLM as ranker Sim.+FTM) decreases success rates across settings. It is also interesting to note that Sim.+FTM outperforms both SL and RAP (Tables 2 and 3), further illustrating MARTs trajectory utility scoring. Table 4: Ablation studies of MART in the AI2-THOR and LEGENT environments."
        },
        {
            "title": "Environment Metric",
            "content": "w/o Abstraction Sim.+FTM MART AI2-THOR"
        },
        {
            "title": "LEGENT",
            "content": "SR SR-Sub AS AS-Sub SR AS 0.31 0.73 130.26 36.03 0.77 18.48 0.34 0.74 125.20 34.63 0.77 18. 0.40 0.75 123.19 34.07 0.87 13.81 Figure 5: Comparison between similarity-based retriever and MART."
        },
        {
            "title": "4.5 CASE STUDY",
            "content": "We present two case studies for more in-depth discussion of MARTs capabilities handling challenges of the MLLM agent setting. The first case (Figure 5) demonstrates the effective handling of 9 Preprint. very long-horizon trajectory. Given 73-step task trajectory navigate to DishSponge, put it in the Plate, and place the Plate on the Cabinet\" Trajectory Abstraction identifies 5 key milestones. MART achieves an 80% success rate with an average of 28 steps, while both the similarity-based method and the agent without memory reach only 40% success rate, averaging 69.6 and 74.6 steps, respectively. The second case (Figure 6) shows how MART extracts implicit rules for long sequence tasks. For the task put the Potato into the microwave, heat it, and pick it up, Trajectory Abstraction analyzes the agents actions (including exploration, attempts, and success) and generates an abstract set of inferred rules, pruning non-contributory and redundant actions, reducing 13 transitions to just 6. MART achieves an 80% success rate with an average of 30.2 steps, while other methods have 0% success rate. Figure 6: Showcase of the significance of the Trajectory Abstraction mechanism."
        },
        {
            "title": "5 CONCLUSION AND LIMITATIONS",
            "content": "We propose MART, new paradigm for trajectory retrieval incorporating interactive learning, to enhance embodied agents performance by providing them with task-relevant trajectory data. Our approach utilizes interaction-based feedback to identify the most effective trajectories, and constructs preference pairs based on the comparisons between trajectories. An MLLM retriever is finetuned through these preference pairs, effectively prioritizing the trajectories that improve task performance. We also introduce Trajectory Abstraction in MART, novel mechanism that leverages MLLMs summarization capabilities, to abstract trajectories, i.e., reduce the required number of tokens to represent them, while preserving key information and enabling agents to better understand relevant information. Experimental results in different environments demonstrate that our method significantly enhances task success rates in unseen tasks, compared to multiple baselines. This work helps bridge the gap between general-purpose MLLMs and the specific requirements of embodied tasks, offering new paradigm for multimodal trajectory retrieval for embodied agents. As the context window is limited for the MLLM we used, it cannot accurately process large number of images, so we only retrieve one trajectory at time to input to the agent in the manner of one-shot learning. If multiple trajectories can be used as input at time, as in few-shot learning, an agent could splice the skills corresponding to different sub-trajectories, which has the potential to further improve performance in more complex and longer-horizon tasks. Such setting, and also methods for retrieving trajectory sub-segments instead of full trajectories, are left as future work. 10 Preprint."
        },
        {
            "title": "REFERENCES",
            "content": "Sahar Abdelnabi and Amr Gomaa et al. Cooperation, Competition, and Maliciousness: LLMStakeholders Interactive Negotiation. arXiv:2309.17234, 2023. Constructions Aeronautiques, Adele Howe, Craig Knoblock, ISI Drew McDermott, Ashwin Ram, Manuela Veloso, Daniel Weld, David Wilkins Sri, Anthony Barrett, Dave Christianson, et al. Pddl the planning domain definition language. Technical Report, Tech. Rep., 1998. Akari Asai, Zeqiu Wu, Yizhong Wang, Avirup Sil, and Hannaneh Hajishirzi. Self-rag: Learning to retrieve, generate, and critique through self-reflection. In The Twelfth International Conference on Learning Representations (ICLR), 2024. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Xi Chen, Krzysztof Choromanski, Tianli Ding, Danny Driess, Avinava Dubey, Chelsea Finn, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. Shaofei Cai, Bowei Zhang, and et al. Groot: Learning to follow instructions by watching gameplay videos. arXiv:2310.08235, 2023. Soravit Changpinyo, Jordi Pont-Tuset, Vittorio Ferrari, and Radu Soricut. Telling the what while pointing to the where: Multimodal queries for image retrieval. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pp. 1213612146, 2021. Jiaqi Chen, Yuxian Jiang, Jiachen Lu, and Li Zhang. S-agents: self-organizing agents in open-ended environment. arXiv:2402.04578, 2024. Tao Chen, Jie Xu, and Pulkit Agrawal. system for general in-hand object re-orientation. In Conference on Robot Learning, pp. 297307. PMLR, 2022a. Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William W. Cohen. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. Conference on Empirical Methods in Natural Language Processing, EMNLP, 2022b. Zhili Cheng, Zhitong Wang, Jinyi Hu, Shengding Hu, An Liu, Yuge Tu, Pengkai Li, Lei Shi, Zhiyuan Liu, and Maosong Sun. LEGENT: Open platform for embodied agents. In Yixin Cao, Yang Feng, and Deyi Xiong (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), 2024. Hanze Dong, Wei Xiong, Bo Pang, Haoxiang Wang, Han Zhao, Yingbo Zhou, Nan Jiang, Doyen Sahoo, Caiming Xiong, and Tong Zhang. Rlhf workflow: From reward modeling to online rlhf. arXiv preprint arXiv:2405.07863, 2024. Danny Driess, Fei Xia, Mehdi SM Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, et al. Palm-e: An embodied multimodal language model. arXiv preprint arXiv:2303.03378, 2023. Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (CVPR), pp. 1518015190, 2023. Abhishek Gupta, Vikash Kumar, Corey Lynch, Sergey Levine, and Karol Hausman. Relay policy learning: Solving long-horizon tasks via imitation and reinforcement learning. In Leslie Pack Kaelbling, Danica Kragic, and Komei Sugiura (eds.), 3rd Annual Conference on Robot Learning, CoRL 2019, volume 100 of Proceedings of Machine Learning Research, pp. 10251037. PMLR, 2019. Ankur Handa, Arthur Allshire, Viktor Makoviychuk, Aleksei Petrenko, Ritvik Singh, Jingzhou Liu, Denys Makoviichuk, Karl Van Wyk, Alexander Zhurkevich, Balakumar Sundaralingam, et al. In 2023 IEEE Dextreme: Transfer of agile in-hand manipulation from simulation to reality. International Conference on Robotics and Automation (ICRA), pp. 59775984. IEEE, 2023. 11 Preprint. Jörg Hoffmann and Bernhard Nebel. The ff planning system: Fast plan generation through heuristic search. Journal of Artificial Intelligence Research, 14:253302, 2001. Sirui Hong and Mingchen Zhuge et al. MetaGPT: Meta programming for multi-agent collaborative framework. arXiv:2308.00352, 2023. Aashi Jain, Mandy Guo, Krishna Srinivasan, Ting Chen, Sneha Kudugunta, Chao Jia, Yinfei Yang, and Jason Baldridge. MURAL: Multimodal, multitask representations across languages. In Findings of the Association for Computational Linguistics: EMNLP, Punta Cana, Dominican Republic, 2021. Dominik Jeurissen, Diego Perez-Liebana, Jeremy Gow, Duygu Cakmak, and James Kwan. Playing nethack with llms: Potential & limitations as zero-shot agents. arXiv:2403.00690, 2024. Haobin Jiang, Junpeng Yue, Hao Luo, Ziluo Ding, and Zongqing Lu. Reinforcement learning friendly vision-language model for minecraft. European Conference on Computer Vision (ECCV), 2024. Yunfan Jiang, Agrim Gupta, Zichen Zhang, Guanzhi Wang, Yongqiang Dou, Yanjun Chen, Li FeiFei, Anima Anandkumar, Yuke Zhu, and Linxi Fan. Vima: General robot manipulation with multimodal prompts. arXiv preprint arXiv:2210.03094, 2022. Tomoyuki Kagaya, Thong Jing Yuan, Yuxuan Lou, Jayashree Karlekar, Sugiri Pranata, Akira Kinose, Koki Oguri, Felix Wick, and Yang You. Rap: Retrieval-augmented planning with contextual memory for multimodal llm agents. arXiv preprint arXiv:2402.03610, 2024. Zhao Kaiya and Michelangelo Naim et al. Lyfe agents: Generative agents for low-cost real-time social interactions. arXiv:2310.02172, 2023. Wonjae Kim, Bokyung Son, and Ildoo Kim. Vilt: Vision-and-language transformer without convolution or region supervision. In International conference on machine learning, pp. 55835594. PMLR, 2021. Eric Kolve, Roozbeh Mottaghi, Winson Han, Eli VanderBilt, Luca Weihs, Alvaro Herrasti, Matt Deitke, Kiana Ehsani, Daniel Gordon, Yuke Zhu, et al. Ai2-thor: An interactive 3d environment for visual ai. arXiv preprint arXiv:1712.05474, 2017. Sunjae Lee, Junyoung Choi, Jungjae Lee, Munim Hasan Wasi, Hojun Choi, Steven Y. Ko, Sangeun Oh, and Insik Shin. Explore, select, derive, and recall: Augmenting llm with human-like memory for mobile task automation, 2024. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33: 94599474, 2020. Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. Align before fuse: Vision and language representation learning with momentum distillation. Advances in Neural Information Processing Systems, 34:96949705, 2021. Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, and Liqiang Nie. Optimus1: Hybrid multimodal memory empowered agents excel in long-horizon tasks. arXiv preprint arXiv:2408.03615, 2024. Jacky Liang, Wenlong Huang, Fei Xia, Peng Xu, Karol Hausman, Brian Ichter, Pete Florence, and Andy Zeng. Code as policies: Language model programs for embodied control. In 2023 IEEE International Conference on Robotics and Automation (ICRA), pp. 94939500. IEEE, 2023. Shalev Lifshitz, Keiran Paster, Harris Chan, Jimmy Ba, and Sheila A. McIlraith. STEVE-1: generative model for text-to-behavior in minecraft. In Thirty-seventh Conference on Neural Information Processing Systems (NeurIPS), 2023. Jonathan Light, Min Cai, Sheng Shen, and Ziniu Hu. Avalonbench: Evaluating llms playing the game of avalon. In NeurIPS 2023 Foundation Models for Decision Making Workshop, 2023. 12 Preprint. Jiaju Lin, Haoran Zhao, Aochi Zhang, Yiting Wu, Huqiuyue Ping, and Qin Chen. Agentsims: An open-source sandbox for large language model evaluation. arXiv:2308.04026, 2023a. Weizhe Lin, Jinghong Chen, Jingbiao Mei, Alexandru Coca, and Bill Byrne. Fine-grained lateinteraction multi-modal retrieval for retrieval augmented visual question answering. In Thirtyseventh Conference on Neural Information Processing Systems (NeurIPS), 2023b. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in Neural Information Processing Systems, 36, 2023. Yuxing Long, Xiaoqi Li, Wenzhe Cai, and Hao Dong. Discuss before moving: Visual language navigation via multi-expert discussions. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 1738017387, 2024. Man Luo, Zhiyuan Fang, Tejas Gokhale, Yezhou Yang, and Chitta Baral. End-to-end knowledge retrieval with multi-modal queries. Association for Computational Linguistics (ACL), 2023. Joon Sung Park and Joseph OBrien et al. Generative agents: Interactive simulacra of human behavior. In The 36th UIST, 2023. Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio Gomez Colmenarejo, Alexander Novikov, Gabriel Barth-Maron, Mai Gimenez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, et al. generalist agent. arXiv preprint arXiv:2205.06175, 2022. Dhruv Shah, Blazej Osinski, Brian Ichter, and Sergey Levine. Lm-nav: Robotic navigation with large pre-trained models of language, vision, and action. In Karen Liu, Dana Kulic, and Jeffrey Ichnowski (eds.), Conference on Robot Learning, CoRL 2022, volume 205 of Proceedings of Machine Learning Research, pp. 492504. PMLR, 2022. Sahel Sharifymoghaddam, Shivani Upadhyay, Wenhu Chen, and Jimmy Lin. Unirag: Universal retrieval augmentation for multi-modal large language models. arXiv preprint arXiv:2405.10311, 2024. Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020. Mohit Shridhar, Xingdi Yuan, Marc-Alexandre Côté, Yonatan Bisk, Adam Trischler, and Matthew Hausknecht. ALFWorld: Aligning Text and Embodied Environments for Interactive Learning. In Proceedings of the International Conference on Learning Representations (ICLR), 2021. Chan Hee Song, Jiaman Wu, Clayton Washington, Brian M. Sadler, Wei-Lun Chao, and Yu Su. Llm-planner: Few-shot grounded planning for embodied agents with large language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), October 2023. Austin Stone, Ted Xiao, Yao Lu, Keerthana Gopalakrishnan, Kuang-Huei Lee, Quan Vuong, Paul Wohlhart, Brianna Zitkovich, Fei Xia, Chelsea Finn, et al. Open-world object manipulation using pre-trained vision-language models. arXiv preprint arXiv:2303.00905, 2023. Theodore R. Sumers, Shunyu Yao, Karthik Narasimhan, and Thomas L. Griffiths. Cognitive architectures for language agents. Transactions on Machine Learning Research (TMLR), 2024. Weiwei Sun, Zheng Chen, Xinyu Ma, Lingyong Yan, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Instruction distillation makes large language models efficient Dawei Yin, and Zhaochun Ren. zero-shot rankers. arXiv preprint arXiv:2311.01555, 2023a. Weiwei Sun, Lingyong Yan, Xinyu Ma, Shuaiqiang Wang, Pengjie Ren, Zhumin Chen, Dawei Yin, and Zhaochun Ren. Is chatgpt good at search? investigating large language models as re-ranking agents. arXiv preprint arXiv:2304.09542, 2023b. 13 Preprint. Weihao Tan, Wentao Zhang, Xinrun Xu, Haochong Xia, Ziluo Ding, Boyu Li, Bohan Zhou, Junpeng Yue, Jiechuan Jiang, Yewen Li, Ruyi An, Molei Qin, Chuqiao Zong, Longtao Zheng, Yujie Wu, Xiaoqiang Chai, Yifei Bi, Tianbao Xie, Pengjie Gu, Xiyun Li, Ceyao Zhang, Long Tian, Chaojie Wang, Xinrun Wang, Börje F. Karlsson, Bo An, Shuicheng Yan, and Zongqing Lu. Cradle: Empowering Foundation Agents Towards General Computer Control. arXiv preprint arXiv:2403.03186, 2024. Guanzhi Wang, Yuqi Xie, Yunfan Jiang, Ajay Mandlekar, Chaowei Xiao, Yuke Zhu, Linxi Fan, and Anima Anandkumar. Voyager: An open-ended embodied agent with large language models. Transactions on Machine Learning Research, 2024a. Noah Wang, Z.y. Peng, Haoran Que, Jiaheng Liu, Wangchunshu Zhou, Yuhan Wu, Hongcheng Guo, Ruitong Gan, Zehao Ni, Jian Yang, Man Zhang, Zhaoxiang Zhang, Wanli Ouyang, Ke Xu, Wenhao Huang, Jie Fu, and Junran Peng. RoleLLM: Benchmarking, eliciting, and enhancing role-playing abilities of large language models. In Findings of the Association for Computational Linguistics (ACL), 2024b. Shu Wang, Muzhi Han, Ziyuan Jiao, Zeyu Zhang, Ying Nian Wu, Song-Chun Zhu, and Hangxin Liu. Llm3:large language model-based task and motion planning with motion failure reasoning, 2024c. Zhilin Wang, Yu-Ying Chiu, and Yu Cheung Chiu. Humanoid agents: Platform for simulating human-like generative agents. In Yansong Feng and Els Lefever (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, EMNLP, pp. 167176. Association for Computational Linguistics, 2023a. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. Describe, explain, plan and select: Interactive planning with large language models enables open-world multi-task agents. In ICML, 2023b. Zihao Wang, Shaofei Cai, Anji Liu, Xiaojian Ma, and Yitao Liang. JARVIS-1: Open-world multitask agents with memory-augmented multimodal language models. In Second Agent Learning in Open-Endedness Workshop, 2023c. Cong Wei, Yang Chen, Haonan Chen, Hexiang Hu, Ge Zhang, Jie Fu, Alan Ritter, and Wenhu Chen. Uniir: Training and benchmarking universal multimodal information retrievers. arXiv preprint arXiv:2311.17136, 2023. Zhiheng Xi, Wenxiang Chen, Xin Guo, Wei He, Yiwen Ding, Boyang Hong, Ming Zhang, Junzhe Wang, Senjie Jin, Enyu Zhou, et al. The rise and potential of large language model based agents: survey. arXiv preprint arXiv:2309.07864, 2023. Yuzhuang Xu, Shuo Wang, Peng Li, Fuwen Luo, Xiaolong Wang, Weidong Liu, and Yang Liu. Exploring large language models for communication games: An empirical study on werewolf. arXiv preprint arXiv:2309.04658, 2023. Shunyu Yao, Howard Chen, John Yang, and Karthik Narasimhan. Webshop: Towards scalable realworld web interaction with grounded language agents. In Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (eds.), Advances in Neural Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, 2022. Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi, Rich James, Jure Leskovec, Percy Liang, Mike Lewis, Luke Zettlemoyer, and Wen tau Yih. Retrieval-augmented multimodal language modeling. International Conference on Machine Learning (ICML), 2023. Tianhe Yu, Deirdre Quillen, Zhanpeng He, Ryan Julian, Karol Hausman, Chelsea Finn, and Sergey Levine. Meta-world: benchmark and evaluation for multi-task and meta reinforcement learning. In Conference on Robot Learning (CoRL), 2019. Ceyao Zhang, Kaijie Yang, Siyi Hu, Zihao Wang, Guanghe Li, Yihang Sun, Cheng Zhang, Zhaowei Zhang, Anji Liu, Song-Chun Zhu, Xiaojun Chang, Junge Zhang, Feng Yin, Yitao Liang, and Yaodong Yang. Proagent: Building proactive cooperative agents with large language models. arXiv:2308.11339, 2023. 14 Preprint. Jiayi Zhang, Chuang Zhao, Yihan Zhao, Zhaoyang Yu, Ming He, and Jianping Fan. Mobileexperts: dynamic tool-enabled agent team in mobile devices, 2024a. Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, and He Wang. Navid: Video-based vlm plans the next step for vision-andlanguage navigation, 2024b. Zeyu Zhang, Xiaohe Bo, Chen Ma, Rui Li, Xu Chen, Quanyu Dai, Jieming Zhu, Zhenhua Dong, and Ji-Rong Wen. survey on the memory mechanism of large language model based agents. arXiv preprint arXiv:2404.13501, 2024c. Andrew Zhao, Daniel Huang, Quentin Xu, Matthieu Lin, Yong-Jin Liu, and Gao Huang. Expel: Llm agents are experiential learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1963219642, 2024. Longtao Zheng, Rundong Wang, Xinrun Wang, and Bo An. Synapse: Trajectory-as-exemplar prompting with memory for computer control. In The Twelfth International Conference on Learning Representations (ICLR), 2024. Enshen Zhou, Yiran Qin, Zhenfei Yin, Yuzhou Huang, Ruimao Zhang, Lu Sheng, Yu Qiao, and Jing Shao. Minedreamer: Learning to follow instructions via chain-of-imagination for simulatedworld control. arXiv preprint arXiv:2403.12037, 2024. 15 Preprint."
        },
        {
            "title": "A EXPERIMENTAL SETUP DETAILS",
            "content": "In this appendix we provide more low-level details on the implementation of MART experiments. A.1 MULTIPLE IMAGE INPUT IN LLAVA The LLaVA architecture itself is compatible with using multiple images as input, but the released model weights do not have the ability to handle multiple images. In other words, when you input multiple images, it will only focus on the contents of the first image. Therefore, we fine-tune LLaVA through multi-image captioning data to enable it to understand multiple images. We utilized LLaVAs single-image perception capabilities to enhance its multi-image perception. Specifically, we sampled images collected from the environments and employed the pre-trained LLaVA to describe the content of each image. We organized the responses into multi-image captioning data, which we then used to fine-tune LLaVA. After our verification, the fine-tuned LLaVA demonstrated the ability to perceive multiple input images. After fine-tuning with multiple image datasets, LLaVA serves as the basez model for our subsequent retriever training. We will release all the code. A.2 PARAMETER SETTINGS The specific parameter settings in the experiment are shown in Table 5. Table 5: Hyperparameters of LLaVA fine-tuned by LoRA"
        },
        {
            "title": "Value",
            "content": "llava-v1.6-mistral-7b LLaVA_version 32 train_batch_size 8 eval_batch_size 8 gradient_accumulation_steps learning_rate_AI2THOR 2e-5 mm_projector_lr_AI2THOR 2e-5 3e-6 learning_rate_LEGENT 3e-6 mm_projector_lr_LEGENT 16 lora_r 32 lora_alpha 0.05 warmup_ratio 32768 model_max_length cosine lr_scheduler_type clip-vit-large-patch14-336 vision_tower A.3 DETAILS OF LLAVA-PLAIN Building on existing work (Asai et al., 2024) (Sun et al., 2023b) that employs LLM for text retrieval, we use the generation probability of special token to represent the score for LLaVA-Plain. In detail, the effectiveness score si is measured by the probability of LLaVA-Plain to generate the special token Yes and No, as in Equation 2, where (Yes/No) denoted the probability of LLaVA-Plain to generate Yes or No. E. si = (Yes) (Yes) + (No) (2) In detail, the prompts we use for LLaVA-Plain and MART are presented in prompt 1 of Appendix E. 16 Preprint. AI2-THOR ENVIRONMENT SPECIFICS B.1 SETTING DIFFERENCES Two popular benchmarks, ALFRED (Shridhar et al., 2020), and ALFWorld (Shridhar et al., 2021) derived from ALFRED are both built on AI2-THOR. However, none of them are directly suitable for benchmarking MLLM agents performing real-world tasks. ALFRED is multimodal benchmark in AI2-THOR that uses fine-grained navigation actions. However, it requires pixel-level masks to specify objects for interaction actions. MLLM lacks the capability to generate such pixel-level masks, making ALFRED incompatible with MLLM agents without adaptation at either side. ALFWorld adapts ALFRED for LLM agents (i.e., text-only) by simplifying it. Firstly, it provides text feedback as observation, detailing objects in the agents field of view along with their corresponding IDs. Secondly, it simplifies the action space by replacing all navigation actions with the teleportation action go to and composite high-level actions like heat, clean, and cool, each involving multiple atomic interactions. For example, the cool object a\" action is equivalent to: open the refrigerator, put object inside the refrigerator, close the refrigerator, open the refrigerator, and pick up the object A. These modifications significantly reduce task difficulty, while allowing LLM agents to perform in ALFWorld. To better evaluate the fine-grained control abilities of MLLM agents in real-world tasks and longerhorizon more realistic tasks, we reject ALFWorlds setting approach, which uses teleportation and composite high-level actions, opting instead for fine-grained navigation actions and finer-grained actions. However, unlike in ALFRED, since MLLMs cannot generate pixel-level masks by default, we allow interaction actions to reference objects using numerical ID (e.g., cup 1) provided by the environments feedback, instead of pixel-level mask. All selected task sets and modified simulator code for adapting the MLLM agent will be released upon paper publication. B.2 TASK DECOMPOSITION We adopt the ALFRed (Shridhar et al., 2020) method to decompose the entire task into multiple sub-tasks. In ALFRed, tasks are decomposed as follows: First, they encode agent and object states, along with high-level environment dynamics, into Planning Domain Definition Language (PDDL) rules (Aeronautiques et al., 1998). Next, they define task-specific PDDL goal conditions, such as heated potato resting on tabletop. The planner assumes fully observable environment with perfect knowledge of world dynamics. Consequently, each task is decomposed into several sub-tasks, with instructions provided for each subtask by human labelers. We follow the ALFRed method but adjust the decomposition results. PDDL-based decomposition can result in inconsistent sub-task difficulty. For example, sub-task like pick up object or put object on object B, which often follows navigation sub-task. If the previous navigation sub-task is executed successfully, the current sub-task can be completed in one step according to the PDDL decomposition. We adjusted the task decomposition by merging sub-tasks that can be completed in one step with adjacent sub-tasks to balance their difficulty. We will release the benchmark including our tasks and modified sub-tasks. B.3 SUCCESS DETECTION In AI2-THOR, we provide the agent with the environments metadata and feedback in the Success Detection module. After each step is executed, the agent then determines whether the current subtask is completed using few-shot approach. For specific prompts, please refer to Appendix E. B.4 HIERARCHY EXAMPLES Figure 7 illustrates multiple cases of the object hierarchy relationship Inside, only available in the AI2-THOR environment. Preprint. Moreover, Figure 8 illustrates the different types of rooms in our LEGENT experiments, showing connectivity, but simpler On hierarchies than AI2-THOR. (a) Microwave. (b) Cabinet. (c) Sink. (d) Coffee machine. Figure 7: Image examples of object hierarchy in AI2-THOR. (a) Where-2room. (b) Where-1room. (c) Come-2room. (d) Come-1room. Figure 8: Image examples of different types of tasks in LEGENT."
        },
        {
            "title": "C FULL STEP COUNT RESULTS",
            "content": "Due to space limitations, we present the full tables with Success Rate (SR) and Average Steps (AS) results here. Table 6 shows the performance comparison of different types in LEGENT, with full step count results. While, Table 7 shows the performance comparisons of the ablation studies in both AI2THOR and LEGENT environments, with full step count results. Table 6: Performance comparison of different types in LEGENT, with full step count results."
        },
        {
            "title": "Metric Type",
            "content": "Where-2room Where-1room Come-2room Come-1room Average PA 0.65 0.78 0.63 0.75 0.70 Where-2room 25.23 Where-1room 10.40 Come-2room 32.05 Come-1room 26.80 Average 23.62 LP 0.60 0.80 0.55 0.82 0. 28.98 16.49 34.95 19.62 25."
        },
        {
            "title": "SL RAP MART",
            "content": "0.60 0.80 0.63 0.96 0.75 28.30 15.36 28.80 11.22 20.92 0.73 0.89 0.46 0.93 0.75 26.23 9.82 35.33 11.09 20.62 0.88 0.91 0.73 0.98 0.87 14.03 5.07 26.90 9.24 13. SR AS"
        },
        {
            "title": "D ALGORITHM",
            "content": "MARTs Agent Execution Pseudocode is shown in Algorithm 1. 18 Preprint. Table 7: Ablation studies of MART in the AI2-THOR and LEGENT environments, with full step count results."
        },
        {
            "title": "Environment",
            "content": "AI2-THOR"
        },
        {
            "title": "Metric",
            "content": "SR SR-Sub AS AS-Sub"
        },
        {
            "title": "LEGENT",
            "content": "SR AS Where-2room Where-1room Come-2room Come-1room Average Where-2room Where-1room Come-2room Come-1room Average w/o Abstraction Sim.+FTM MART 0.31 0.73 130.26 36. 0.78 0.74 0.63 0.95 0.77 17.36 12.36 32.75 11.47 18.48 0.34 0.74 125.20 34.63 0.40 0.75 123.19 34.07 0.72 0.89 0.55 0.91 0.77 15.96 11.51 34.13 13.73 18.83 0.88 0.91 0.73 0.98 0.87 14.03 5.07 26.90 9.24 13.81 algorithm 1 MART Agent Execution Pseudocode Input: Expert Trajectory Memory M, Retriever qθ, Policy π, Task ℓc, Horizon H, Initial Observa1, M) tion oc 1, Preference Pairs t1) t1, , rt) end if if == 1 then rt SelfReflection(at1, oc Select action at π(ℓc, τ e, oc Output: the Success status of task execution 1: Fine-tune retriever qθ with Preference PairsD 2: Retrieve reference trajectory τ qθ(ℓc, oc 3: TrajectoryAbstraction to simplify τ 4: for = 1 to do if t! = 1 then 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: end for end if (oc , if SuccessDetection(ℓc, return True else if then return False Select action at π(ℓc, τ e, oc ) ) ActionExecution(at) t+1) then end if // Task successfully completed // Task failed after reaching horizon 19 Preprint."
        },
        {
            "title": "E AGENT PROMPTS",
            "content": "Prompt 1: Retriver Prompt to LLaVA. You are highly intelligent vision language assistant agent situated in virtual environment. You have been given task instruction that you need to complete. Additionally, you are provided with reference trajectory, which includes previous task instructions, actions, and egocentric observations from the same virtual environment. This reference trajectory represents successful completion of task and is intended to guide you in performing the current task. The current task instruction is: [current task]. The task instruction of reference trajectory is [memory task]. Among these input images, the 1st image is your current observation, while the other images are milestones of observations from the reference trajectory. The abstraction of the reference trajectory is: <Description of Milestone 0>, <Image 0>, <Feedback 0>, <Action 0>; <Description of Milestone 1>, <Image 1>, <Feedback 1>, <Action 1>; ... Your should thoroughly understand the current task and the reference trajectory. Then, analyze whether the reference trajectory can assist in executing the current task by answering Yes or No. You should only respond in the format described below, and you should not output comments or other information: Answer: Yes or No. 20 Preprint. Prompt 2: Trajectory Abstraction Prompt. You are highly intelligent vision-language assistant agent placed within virtual environment. You are provided with trajectory consisting of: - Trajectory Task Instruction: {Task Instruction} - sequence of first-person perspective observations (<Image x>) - sequence of environment feedbacks (<Feedback x>) - sequence of actions (<Action x>) - Another Task Instruction: {Current Task Instruction} Your Tasks: 1. Fully comprehend the tasks accomplished in the trajectory. 2. Identify the significant milestones in the trajectory that are essential for accomplishing the task. These are points where important decisions are made, goals are achieved, or notable changes in the environment or state occur. Do not treat every image as significant milestone. If the target object of Another Task Instruction appears in the trajectory feedback, then the point where it appears is also significant milestone because it is very important to Another Task Instruction. 3. For each significant milestone, provide: - description of the milestone. - The corresponding image (<Image x>). - The corresponding feedback (<Feedback x>). - The sequence of actions taken between this milestone and the next one. <Image 0>, <Feedback 0>, <Action 0>; <Image 1>, <Feedback 1>, <Action 1>; ... Response Format (do not include any comments or additional information): 1. {Description of significant milestone 1}: <Image a>. <Feedback a>. Actions: {Actions taken between this significant milestone and the next significant milestone} (such as <Action a>, <Action b>). 2. {Description of significant milestone 2}: <Image c>. <Feedback c>. Actions: {Actions taken between this significant milestone and next significant milestone} ... Notes: - Ensure that the number of <Image x> and <Action x> matches the provided trajectory. - Only include the specified information in your response. - significant milestone is point in the trajectory where key part of the task is accomplished. If the target object of Another Task Instruction appears in the trajectory feedback, then the point where it appears is also significant milestone because it is very important to complete Another Task . 21 Preprint. Prompt 3: Self-Reflection Prompt. You are vision language assistant agent with high intelligence. You are placed inside virtual environment, equipped to handle wide range of tasks in the virtual environment. Your advanced capabilities enable you to process and interpret egocentric observation screenshots and environment feedback. Your task is to examine these inputs, interpret the environmental feedback, and determine whether the executed action takes effect. Current task: {task_instruction} Previous actions: {previous_action_1}, {previous_action_2}, ... Previous environment feedback: {previous_feedback} Reasoning for the previous actions: {previous_reasoning_1}, {previous_reasoning_2}, ... Previous observations: {previous_image_1}, {previous_image_2}, ... Reasoning: You need to answer the following questions step by step to get some reasoning based on the previous actions and sequential images of the execution of the previous actions. 1. What is the last executed action? 2. Was the last executed action successful? Give reasons. You should refer to the following rules: - If the action involves movement of position, change of view, or interaction with an object, it is considered unsuccessful when the image you currently observe remains unchanged as the previous frame. 3. If the last action is not executed successfully, what is the most probable cause? You should give only one cause and refer to the following rules: - If it is an interaction action, the most probable cause was that the object id of the interaction was wrong. - If it is movement action, the most probable cause was that you were blocked by seen or unseen obstacles. 4. If the last action is executed successfully, Does the previous action sequence promote the progress of the current task? 5. If the answer to Reasoning Question 4 is No, how should you adjust to promote the progress of the current task? You should only respond in the format described below, and you should not output comments or other information: Reasoning: 1. ... 2. ... 3. ... 4. ... 5. ... ... 22 Preprint. Prompt 4: Action Planning Prompt. You are robot working in household environment. You can move and interact with the objects you see. The actions you can perform include: 1. MoveAhead: Move one step forward. 2. RotateLeft_$degree: Turn to the left by the specified number of degrees, ranging from 0 to 180 degrees. 3. RotateRight_$degree: Turn to the right by the specified number of degrees, ranging from 0 to 180 degrees. 4. LookUp: Look up 30 degrees. 5. LookDown: Look down 30 degrees. 6. Take $objectID: You can take any object in your line of sight. The $objectID can be obtained from the environment feedback. 7. Put $objectID on/in $targetID: You can put the object in your hand onto/into the target receptacle. The $targetID can be obtained from the environment feedback, and $objectID refers to the object in your hand. 8. Open $objectID: You can open any openable object in your line of sight. The $objectID can be obtained from the environment feedback. 9. Close $objectID: you can close any open object in your line of sight . The $objectID can be obtained from the environment feedback. 10. ToggleOn $objectID: You can toggle on the switch of the object, such as faucet or microwave. The $objectID can be obtained from the environment feedback. 11. ToggleOff $objectID: You can toggle off the switch of the object, such as faucet or microwave. The $objectID can be obtained from the environment feedback. 12. Slice $objectID: You can slice any object in your line of sight. The $objectID can be obtained from the environment feedback. Examples of actions: RotateLeft_30; MoveAhead; Take mug 1; Open fridge 1; ToggleOn microwave 1; Close fridge 1;... You need to follow the task instructions to complete the task. Here is some helpful information. Current task: {task} Current environment feedback: {obs} Previous environment feedback: {previous_obs} Previous action and reasoning: {previous_action} Current observation <image> Previous observation <image> Reference trajectory abstraction: The reference trajectory is successful trajectory, which is used to guide you to complete the current task. The task of reference trajectory is {current task}. <Description of Milestone 0>, <Image 0>, <Feedback 0>, <Action 0>; <Description of Milestone 1>, <Image 1>, <Feedback 1>, <Action 1>; ... Based on the above information, you should first analyze the current situation, and provide the reasoning for what you should do for the 23 Preprint. next step to complete the task. Then, you should output the exact action you want to execute in the simulator. Reasoning: You should think step by step and provide detailed reasoning to determine the next action executed on the current state of the task. You need to answer the following questions step by step. 1. Does reference trajectory abstraction exist? If the answer is no, ignore the questions from number 2 to number 4. 2. What process does reference trajectory abstraction describe? 3. Consider what is your current task. Based on the Observation of the previous step and Current Observation, which waypoint has the current task reached? 4. Based on the answer of the question number 3, you should consider how the current waypoint and the parts after that in the reference trajectory abstraction can help you with your current task. The help provided by the reference trajectory abstraction can be knowing the location of the target object or knowing the execution flow of combined action. 5. Based on the completion progress of the current task and the answer to question number 4, what should you do for the next step? 6. Why do you take this action next step? Action: The best action to execute next to progress in completing the task. You should pay more attention to the following action rules: 1. Given the current situation and task, you should only choose the most suitable action from the valid action set. You cannot use actions that are not in the valid action set to control the application, especially Await Next Task. 2. If the Action of the previous step fails, you should not continue trying but should consider adjusting your position to get closer to the target object. 3. You MUST NOT match or imitate the reference trajectory. You should think about how to complete the current task based on the answer to Reasoning Question 4. You should only respond in the format described below, and you should not output comments or other information: Reasoning: 1. ... 2. ... 3. ... 4. ... 5. ... 6. ... Action: ... 24 Preprint. Prompt 5: Success Detection Prompt. You are highly intelligent vision-language assistant agent. You are situated in virtual environment, equipped to handle diverse array of tasks. Your advanced capabilities allow you to process and interpret egocentric observation screenshots, environmental feedback and environmental metadata. Your task is to examine these inputs, understand the environmental metadata, and assess the success of the current task. Current task: <task> Environmental Metadata <environment metadata> Environmental Feedback <environment feedback> Current Inventory <inventory> You need to refer to the following rules: 1. If the current task contains multiple tasks, it is considered successful only when each task succeeds. 2. If the task is navigation task, you need to check the environmental metadata. The navigation task succeeds when the target object is in view and the distance is less than 1m. 3. If the task is pickup task, then according to the environmental feedback, the pickup task succeeds when the target object is in your inventory. 4. If the task is put down task (put object on/in object b), the put down task succeeds when the environmental feedback from the environment includes You put object on/in the object successfully . 5. If the task is clean task, the clean task succeeds when the cleaned_objects in environmental metadata include the target object and the same target object is also in inventory. 6. If the task is cool task, the cool task succeeds when the cooled_objects in environmental metadata includes the target object and the same target object is also in inventory. 7. If the task is heat task, the heat task succeeds when the heated_objects in environmental metadata includes the target object and the same target object is also in inventory. You should only respond in the format described below, and you should not output comments or other information: Answer: True or False."
        }
    ],
    "affiliations": [
        "Beijing Academy of Artificial Intelligence",
        "Institute of Software, Chinese Academy of Sciences",
        "School of Computer Science, Peking University"
    ]
}