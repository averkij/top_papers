{
    "paper_title": "Expert Race: A Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts",
    "authors": [
        "Yike Yuan",
        "Ziyu Wang",
        "Zihao Huang",
        "Defa Zhu",
        "Xun Zhou",
        "Jingyi Yu",
        "Qiyang Min"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have emerged as mainstream framework in visual generation. Building upon this success, the integration of Mixture of Experts (MoE) methods has shown promise in enhancing model scalability and performance. In this paper, we introduce Race-DiT, a novel MoE model for diffusion transformers with a flexible routing strategy, Expert Race. By allowing tokens and experts to compete together and select the top candidates, the model learns to dynamically assign experts to critical tokens. Additionally, we propose per-layer regularization to address challenges in shallow layer learning, and router similarity loss to prevent mode collapse, ensuring better expert utilization. Extensive experiments on ImageNet validate the effectiveness of our approach, showcasing significant performance gains while promising scaling properties."
        },
        {
            "title": "Start",
            "content": "Expert Race: Flexible Routing Strategy for Scaling Diffusion Transformer with Mixture of Experts Yike Yuan1,, Ziyu Wang1,2,, Zihao Huang1, Defa Zhu1, Xun Zhou1, Jingyi Yu2,, Qiyang Min1, 1ByteDance Seed, 2ShanghaiTech University Equal Contribution, Corresponding Authors"
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have emerged as mainstream framework in visual generation. Building upon this success, the integration of Mixture of Experts (MoE) methods has shown promise in enhancing model scalability and performance. In this paper, we introduce Race-DiT, novel MoE model for diffusion transformers with flexible routing strategy, Expert Race. By allowing tokens and experts to compete together and select the top candidates, the model learns to dynamically assign experts to critical tokens. Additionally, we propose per-layer regularization to address challenges in shallow layer learning, and router similarity loss to prevent mode collapse, ensuring better expert utilization. Extensive experiments on ImageNet validate the effectiveness of our approach, showcasing significant performance gains while promising scaling properties. Date: March 21, 2025 Correspondence: Qiyang Min at minqiyang@bytedance.com, Jingyi Yu at yujingyi@shanghaitech.edu.cn 5 2 0 2 0 2 ] . [ 1 7 5 0 6 1 . 3 0 5 2 : r"
        },
        {
            "title": "Contents",
            "content": "1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4 Taming Diffusion Models with Expert Race . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.1 General Routing Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .\n4.2 Expert Race . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "5 Load Balancing via Router Similarity Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.1 Mode Collapse in Balancing Loss. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5.2 Router Similarity Loss. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 Per-Layer Regularization for Efficient Training . . . . . . . . . . . . . . . . . . . . . . . . . . Implementation Details 7 Experiments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.2 Routing Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.3 Gating Function . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.4 Load Balance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.5 Core Components . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.6 Scaling Law . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.7 Extended Routing Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 7.8 More Results on ImageNet 256 256 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 8 Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Implemention of the Per-Layer Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . Analysis of Router Similarity Loss . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Combination Usage . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Additional Comparisons with DiT-MoE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 5 5 5 5 5 6 6 6 7 8 8 9 9 10 10 11 11 12 12 13 14 15 15 19 21 21 Additional Image Generation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "Introduction",
            "content": "Figure 1 (Left) Our proposed Expert Race routing employs Top-k selection over full token-expert affinity logits, achieving the highest flexibility compared to prior methods like Token Choice and Expert Choice. (Right) Training curve comparisons between DiT-XL [25] and ours. Our model, with equal number of activated parameters, achieves 7.2 speedup in iterations when reaching the same training loss. Recent years have seen diffusion models earning considerable recognition within the realm of visual generation. They have exhibited outstanding performance in multiple facets such as image generation [7, 23, 28, 32, 33], video generation [3, 13], and 3D generation [2, 41]. Thus, diffusion models have solidified their position as pivotal milestone in the field of visual generation studies. Mimicking the triumph of transformer-based large language models (LLMs), diffusion models have effectively transitioned from U-Net to DiT and its variants. This transition yielded not only comparable scaling properties but also an equally successful pursuit of larger models. In the quest for larger models, the Mixture of Experts (MoE) approach, proven effective in scaling large language models (LLMs) [18], exhibits promising potential when incorporated into diffusion models. Essentially, MoE utilizes routing module to assign tokens among experts (typically, set of Feed-Forward Networks (FFN)) based on respective scores. This router module, pivotal to MoEs functionality, employs common strategies such as token-choice and expert-choice. Meanwhile, we observe that the visual signals processed by diffusion models exhibit two distinct characteristics compared to those in LLMs. First, visual information tends to have high spatial redundancy. For instance, significant disparity in information density exists between the background and foreground regions, with the latter typically containing more critical details. Second, denoising task complexity exhibits temporal variation across different timesteps. Predicting noise at the beginning of the denoising process is substantially simpler than predicting noise towards the end, as later stages require finer detail reconstruction. These unique characteristics necessitate specialized routing strategies for visual diffusion models. Consider these characteristics under the MoE, the presence of routing module can adaptively allocate computational resources. By assigning more experts to challenging tokens and fewer to simpler ones, we can enhance model utilization efficiency. Previous strategies like expert-choice anticipated this, but their routing design limit the assignment flexibility to image spatial regions without considering temporal denoising timestep complexity. In this paper, we introduce Race-DiT, novel family of MoE models equipped with enhanced routing strategies, Expert Race. We find that simply increasing strategy flexibilities greatly boost the models performance. Specifically, we conduct race among tokens from different samples, timesteps, and experts, and select the top-k tokens from all. This method effectively filters redundant tokens and optimizes computational resource deployment by the MoE. 3 Figure 2 The Race-DiT Architecture. We replace the Multi-Layer Perceptron (MLP) with the MoE block, which consists of Router and multiple Experts. In Race-DiT, the token assignment is done once for all. Each token can be assigned to any number of experts, and each expert can process any number of tokens (including zero). Expert Race introduces high degree of flexibility in token allocation within the MoE framework. However, there are several challenges when extending DiT to larger parameter scales using MoE. First, we observe that routing in the shallow layers of MoE struggles to learn the assignment, especially with high-noise inputs. We believe this is due to the weakening of the shallow components in the identity branch of the DiT framework. To address this, we propose an auxiliary loss function with layer-wise regularization to aid in learning. Second, considering the substantial expansion of the candidate space, to prevent the collapse of the allocation strategy, we extend the commonly used balance loss from single experts to combinations of experts. This extension is complemented by our router similarity loss, which ensures effective expert utilization by regulating pairwise expert selection patterns. To validate the proposed method, we conducted experiments on ImageNet [5], performing detailed ablations on the proposed modules and investigating the scaling behaviors of multiple factors. Results show that our approach achieves significant improvements across multiple metrics compared to baseline methods. In summary, our main contributions include Expert Race, novel MoE routing strategy for diffusion transformers that supports high routing allocation flexibility in both spatial image regions and temporal denoising steps. Router similarity loss, new objective that optimizes expert collaboration through router logits similarity, effectively maintaining workload equilibrium and diversifying expert combinations without compromising generation fidelity. Per-layer Regularization that ensures effective learning in the shallow layers of MoE models. Detailed MoE scaling analysis in terms of hidden split and expert expansion provides insights for extending this MoE model to diverse diffusion tasks."
        },
        {
            "title": "2.1 Mixture of Experts",
            "content": "Mixture of Experts (MoE) improves computational efficiency by activating only subset of parameters at time and forcing the other neurons to be zero. Typically, MoE is used to significantly scale up models beyond their current size leveraging the natural sparsity of activations. This technique has been widely applied in LLMs first [8, 20] and then extended to the vision domain [29]. The most commonly used routing strategy in MoE is token-choice, in which each token selects subset of experts according to router scores. For its variants, THOR [46] employs random strategy, BASELayer [21] addresses the linear assignment problem, HASHLayer [30] uses hashing function, and MoNE [16] uses greedy top-k. All of these methods allocate fixed number of experts to each token. DYNMoE [12] and ReMoE [38] activates different number of experts for each token by replacing TopK with threshold and using additional regularization terms to control the total budget. Also, some auxiliary regularization terms are applied to constrain the model to activate experts uniformly [4, 36, 45]. Expert choice [44] has been proposed to avoid load imbalance without additional regularizations and enhance routing dynamics, but due to conflicts with mainstream causal attention, it is less commonly applied in large language models (LLMs)."
        },
        {
            "title": "2.2 Multiple Experts in Diffusion",
            "content": "Diffusion follows multi-task learning framework that share the same model across different timesteps. Consequently, many studies have explored whether performance can be enhanced by disentangling tasks according to timesteps inside the model. Ernie [10] and e-diff [1] manually separate the denoising process into multiple stages and train different models to handle each stage. MEME [19] uses heterogeneous models and DTR [24] heuristically partitions along the channel dimension. DyDiT [42] introduce nested MLPs and channel masks to fit varing complexities across time and spatial dimensions. DiT-MoE [9], EC-DiT [35], and Raphael [39] have applied MoE architectures, learning to assign experts to tokens in an end-to-end manner. Compared with previous works, the methods proposed in this paper builds on the MoE but further enhancing its flexibility on dynamically allocate experts on all dimensions to unleash its potential."
        },
        {
            "title": "3 Preliminaries",
            "content": "Before introducing our MoE design, we briefly review some preliminaries of diffusion models and mixture of experts."
        },
        {
            "title": "3.1 Diffusion Models",
            "content": "Diffusion models [15] are class of generative models that sample from noise distribution and learn gradual denoising process to generate clean data. It can be seen as an interpolation process between the data sample x0 and the noise ϵ. typical Gaussian diffusion models formulates the forward diffusion process as xt = αtx0 + 1 αtϵ (1) where ϵ (0, I) is the Gaussian noise and αt is monotonically decreasing function from 1 to 0. Diffusion models use the neural networks to estimate the reverse denoising process pθ(xt1xt) = (µθ(xt), Σθ(xt)). They are trained by minimizing the following objectives: Ex0,t,ϵ (cid:2)y Fθ(xt; c, t)2(cid:3) , min θ (2) where is the timestep which uniformly distributed between 0 to , is the condition information. e.g. class labels, image or text-prompt. The training target can be Gaussian noise ϵ, the original data sample x0 or velocity = 1 αϵ αx0. Early diffusion models used U-net [6, 31] as their backbone. Recently, Transformer-based diffusion models [25] with adaptive layer normalization (AdaLN) [26] have become mainstream, showing significant advantages in scaling up."
        },
        {
            "title": "3.2 Mixture of Experts\nMixture-of-Experts (MoE) is a neural network layer comprising a router R and a set {Ei} of NE experts, each\nspecializing in a subset of the input space and implemented as FFN. The router maps the input X ∈ RB×L×D\ninto token-expert affinity scores S ∈ RB×L×E, trailed by a gating function G:",
            "content": "S = G(R(x)). (3) The input will be assigned to subset of experts with top-k highest scores for computation and its output is the weighted sum of these experts output. unified expression is as follows: (cid:40) = if TopK (S, K) S, 0, Otherwise (cid:88) MoE(X) = Gi(X) Ei(X) (4) (5) iNE where RBLE is the final gating tensor and TopK(, K) is an operation that build set with largest value in tensor. To maintain constant number of activated parameters while increasing the top-k expert selection, the MoE model often splits the inner hidden dimension of each expert based on the top-k value, named fine-grained expert segmentation [4]. In the subsequent discussions, an \"x-in-y\" MoE means there are candidate experts, with the top-x experts activated, and the hidden dimension of experts intermediate layer will be divided by x."
        },
        {
            "title": "3.3 The Rationality of Using MoE in Diffusion Models",
            "content": "Diffusion models possess several distinctive characteristics. Multi-task in nature, tasks at different timesteps predicting the target are not identical. Prior works like e-diff [1] validate this dissimilarity. Redundancy of image tokens. The information density varies across different regions, leading to unequal difficulties in generation. Given these traits, MoE presents suitable architecture for diffusion models. Its routing module can flexibly allocate and combine tokens and experts based on the predicted difficulties. We consider the allocation process as distribution of computational resources. More challenging timesteps and complex image patches should be allocated to more experts. Achieving this requires routing strategy with sufficient flexibility to distribute resources with broader degrees of freedom. Our method is designed following this principle."
        },
        {
            "title": "4.1 General Routing Formulation\nFor computational tractability, we decompose the original score tensor S into two operational dimensions\nthrough permutation and reshaping, obtaining matrix S′ ∈ RDA×DB , where",
            "content": "DB: Size of the expert candidate pool; DA: Number of parallel selection operations. This dimensional reorganization enables independent top-k selection within each row while preserving cross-row independence. Following the sparse gating paradigm in [20, 44], we control the MoE layer sparsity through parameter k, which specifies the expected number of activated experts per token. To satisfy system capacity constraints, 6 the effective selection size per candidate pool is defined as: = NE DB. (6) The routing objective, aligning with the optimization framework in [21], formalizes as the maximization of aggregated gating scores: max DA(cid:88) (cid:88) i=1 jTi i,j, (7) where Ti denotes the set of indices corresponding to the top-K values in the i-th row of S. Suboptimal in Conventional Strategies As shown in figure 3, the unified framework generalizes existing routing methods through top-K selection in S. However, standard row-wise approaches like Token-Choice and Expert-Choice exhibit inherent sub-optimality. These selection methods struggle to achieve optimal allocation in practice, as the required uniform distribution of top DA elements across rows, which is necessary for attaining the theoretical optimum in equation (7), rarely holds with real-world data distributions. In practical scenarios like image diffusion model training, generation complexity varies across two key dimensions: denoising timesteps (B) and spatial image regions (L). To address this computational heterogeneity, the routing module must dynamically allocate more experts to tokens with greater generation demands. However, the token-choice strategy, since DA is constituted by dimensions B&L, both dimensions will receive an identical amount of activation experts. Expert-Choice mitigates this issue but remains constrained by its L-dimensional top-K selection, limiting optimal allocation potential. Method DA Token-choice Expert-choice Expert-Race DB k L/E Figure 3 Top-K Selection Flexibility and Specifications. B: batch size; L: sequence length; E: the number of experts. (a) Token Choice selects top-K experts along the expert dimension for each token. (b) Expert Choice selects top-K tokens along the sequence dimension for each expert. (c) Expert Race selects top-K across the entire set."
        },
        {
            "title": "4.2 Expert Race\nTo address these limitations, we propose Expert-Race, which performs global top-K selection across all gating\nscores in a single routing pass. The \"Race\" mechanism provides an optimal solution to equation (7) by setting\nDA = 1, ensuring the selected K elements are globally maximal. This design maximizes router flexibility to\nlearn adaptive allocation patterns, enabling arbitrary expert-to-token assignments and dynamic allocation\nbased on computational demands. However, applying Expert-Race naively presents two challenges.",
            "content": "Gating Function Conflict. While softmax over the expert dimension is standard for score normalization in existing routing strategies, it disrupts cross-token score ordering in Race. Additionally, applying softmax across the full sequence incurs high computational costs and risks numerical underflow as sequence length grows. We therefore explore alternative activation functions, finding through table 1 that the identity G(x) = yields improved results. Training-Inference Mismatch. Batch-wise candidate aggregation creates fundamental mismatch between training and inference. During training, samples influence each others routing selection and timesteps are randomly sampled per batch, whereas inference operates on independent samples with consistent timesteps. Since timesteps directly control noise mixing levels, this inconsistency degrades generation quality and can 7 Algorithm 1 Pytorch-style Pseudocode of Expert-Race # m: momentum # tau: ema updated threshold # x: input of shape (B, L, D) # experts: list of FFN # router: linear layer # Compute router logits for each token logits = router(x) score = logits.flatten() gates = nn.Identity()(logits) # activation expect_k = * * # (B, L, E) # Get kthvalue and update threshold if training: kth_val = torch.kthvalue(score, k=expect_k) mask = score >= kth_val tau = * tau + (1. - m) * kth_val else: mask = score >= tau # Process tokens by each expert expert_outputs = torch.stack([expert(x) for expert in experts], dim=-1) # Aggregate the output by mask gates = gates * mask output = torch.sum(gates.unsqueeze(-1) * expert_outputs, dim=-1) lead to model failure. At the same time, the mutual influence between samples during routing selection causes unstable inference. To mitigate these effects, we propose learnable threshold τ that estimates the K-th largest value through exponential moving average (EMA) updates during training. τ mτ + (1 m) 1 DA DA(cid:88) i= i,K, (8) where applied during inference, ensuring sample independence and consistent performance. i,K represents the K-th largest element in the i-th row of S. This adaptive threshold is directly Pseudocode. We provide core pseudocode in PyTorch style in algorithm 1, illustrating how the expert selects the k-th largest logits and updates the threshold. Our algorithm is easy to implement, requiring only minor modifications to the existing MoE framework."
        },
        {
            "title": "5 Load Balancing via Router Similarity Loss",
            "content": "In MoE systems, balanced token allocation across experts remains critical engineering challenge. For our proposed Race strategy, the increased policy flexibility imposes greater demands on routing balancing."
        },
        {
            "title": "5.1 Mode Collapse in Balancing Loss.",
            "content": "The conventional balancing loss [8, 34], originally designed for token-choice, promotes load balance by enforcing uniform token distribution across experts, thereby preventing dominance by small subset of experts. However, by only constraining the marginal distribution of scores per expert, this approach fails to prevent collapse between experts with similar selection rules. As shown in figure 4, if multiple experts follow the same rules for selecting tokens, they are downgraded to one wider expert. Although such configurations satisfy balance loss constraints, they undermine the specialization benefits of fine-grained expert design [4], ultimately degrading overall performance. 8 Figure 4 Toy examples of token assignment. Both of the two cases show perfect load balance that each expert process two tokens. But in the case above, experts 1 and 2 are assigned the same token, as are experts 3 and 4, where the 2-in-4 MoE collapse into 1-in-2. The example below shows more diverse assignment, making full use of the expert specialization."
        },
        {
            "title": "5.2 Router Similarity Loss.",
            "content": "To tackle this issue, we propose maximizing expert specialization by promoting pairwise diversity among experts. Specifically, inspired by [40], we compute cross-correlation matrices and minimize their off-diagonal elements to encourage expert differentiation. Given the router logits R(BL)E, we apply softmax along the expert dimension to obtain normalized probabilities , and compute two correlation matrices where is the indicator matrix that Mi,j = 1 if expert selects the i-th token and 0 otherwise. Then, we define the router similarity loss: = M, = Lsim = 1 (cid:88) (i, j) i,j i,j[1,E] where (i, j) is the weighting function defined as: (i, j) = i,j i=j i,j i=j i,j i,j (cid:80) (cid:80) E, if = (E2 E), if = (9) (10) (11) In more detail, the off-diagonal elements denote the similarity between each pair of experts based on token selection patterns in the current batch. From probabilistic perspective, captures the joint probability of i,j token being routed to both expert and j. This formulation regularizes consistent co-selection patterns across experts while promoting diverse expert combinations. Regarding the diagonal elements, represents i,i geometric mean version of the balance loss, effectively encouraging individual expert utilization (see appendix for detailed analysis)."
        },
        {
            "title": "6 Per-Layer Regularization for Efficient Training",
            "content": "DiT employs adaptive layer normalization (adaLN) when introducing conditions. In the pre-normalization (pre-norm) architecture, we observe that adaLN progressively amplifies the outputs of deeper layers. This causes the output magnitudes of shallow layers to be relatively diminished, as illustrated in figure 5. This imbalance results in the learning speed of shallow layers lagging behind that of deeper layers, which is detrimental to the MoE training process. This imbalance has both advantages and disadvantages. On one hand, the outputs from deeper layers are more accurate, and their larger magnitudes make them less susceptible to the substantial noise present in shallow layers, facilitating more precise regression results. On the other hand, due to the presence of the normalization module in the final layer, the component of shallow 9 Figure 5 The norm of each blocks output before added to the shortcuts. The output norm increases rapidly in deep layers, resulting in the weakening of shallow-layer components. This issue is alleviated with our proposed per-layer regularization. layers in the residuals is diminished, posing risk of gradient vanishing and resulting in the learning speed of shallow layers lagging behind that of deeper layers, which is detrimental to the MoE training process. To mitigate this issue, we introduce pre-layer regularization that enhances gradients in supervised manner without altering the core network structure. Specifically, given the hidden output hl from the l-th layer, we add projection layer : RLd RLd to predict the final target RLd, where and represent the number of patches before and after the patchify operation []. The pre-layer loss is defined as: LPLR = Ex0,t,ϵ,l (cid:34) 1 (cid:88) i=1 (cid:13) (cid:13) 2(cid:35) (cid:13)y[n] H(hl)[n](cid:13) (cid:13) (cid:13) (12) where is the total number of patches, and is the patch index. In our implementation, the projection layer is integrated into the MLP router (see appendix for details). By supervising the projection layers predictions against final targets, we enhance shallow layer contributions during training, improving overall MoE performance."
        },
        {
            "title": "7 Experiments",
            "content": "7."
        },
        {
            "title": "Implementation Details",
            "content": "Following training configurations in [25], we conduct experiments on ImageNet [5], employ AdamW optimizer, set batch size to 256, and use constant learning rate 1e-4 without weight decay for models of any size. During initialization, we use zero-initialization on all adaLN layers, and xavier initialization of uniform distribution on all linear layers. When training MoE, we substitute all FFNs with MoE block and make sure they activate same amount of parameters. Specially, we set smaller initialization range for each expert, enlarging the inner dimension by factor of to make the intialization range the same with its dense counterpart. We employ our per-layer regularization with weight 1e-2 and router similarity loss with 1e-4. We maintain an exponential moving average (EMA) of model weights over training and report all results using the EMA model. The metrics used include FID [14], CMMD [17], and CLIP Score [27]. We present series of MoE size configurations, denoted as k-in-E where represents the total number of experts and indicates the number of average activated experts. Additionally, we set the inner hidden dimension of each expert to be 1/k of its dense counterpart to ensure that the number of activated parameters remains the same. In all ablation studies, we train DiT-B and its 2-in-8 MoE variant for 500K iterations unless specified otherwise. 10 Figure 6 Average token allocation at different time steps. Expert-Race assigns more experts to the more complex denoising time steps, which occur at lower timestep indices that handle finer-grain image details. Table 1 Ablation study on routing strategy and gating function. Routing Gating FID CMMD CLIP Token Choice Expert Choice Expert Race Token Choice Expert Choice Expert Race Token Choice Expert Choice Expert Race softmax sigmoid identity 17.28 16.71 16.47 15.25 15.73 13.85 15.98 15.70 13. .7304 .7267 .7104 .6956 .6821 .6471 .6938 .6963 .6317 21.87 21.95 21.97 22.09 22.06 22.23 22.01 22.04 22."
        },
        {
            "title": "7.2 Routing Strategy",
            "content": "Expert racing enables extensive exploration within the logit space during training, enabling complexity-aware expert allocation across diffusion timesteps. As shown in figure 6, along the timestep dimension, our method initially uses fewer experts at the beginning of denoising (higher timestep indices) and dynamically assigns more experts to tokens at timesteps requiring higher image detail (lower timestep indices). Along the spatial dimension, the assignment of computation follows \"diffusion process\" from the center of image to significant object and then to the entire image. This indicates that the model first focuses on object construction and subsequently refines the details. In contrast, both token-choice and expert-choice strategies maintain fixed average expert allocations per timestep, lacking the temporal dynamic allocation capability. From the result proposed in table 1, expert race outperforms other routing strategies by significant margin. Within the framework proposed in section 4.1, we conducted ablation studies on routing strategies for combinations of different dimensions. See section 7.7 for more results."
        },
        {
            "title": "7.3 Gating Function",
            "content": "table 1 shows that identity gating outperforms both softmax and sigmoid variants. In this experiment, we isolate other components (learnable threshold and regularizations) to verify the impact of the gating function on performance. We found that identity gating outperforms softmax and sigmoid under expert-race, and it enhances both token-choice and expert-choice compared to softmax. Under expert-race, both sigmoid and identity significantly outperform softmax. We attribute this to the fact that softmax normalizes scores of each token along the expert dimension, disrupting the partial ordering of scores across different tokens. In contrast, sigmoid and identity preserve this partial ordering, ensuring that important token-expert combinations are selected, thereby improving performance."
        },
        {
            "title": "7.4 Load Balance",
            "content": "table 2 compares our proposed router similarity loss with conventional load balancing loss [4]. This setting is similar to the gating function ablation above, but here the MoE is 4-in-32 to further observe the impact of load balancing. The MaxVio [37] metric measures how much the most violated expert exceeds its capacity limit. Combination Usage Ratio, abbreviated as Comb, estimates the proportion of activations for each pairwise combination of experts (higher is better). See appendix for details. The weights for both the load balance loss and the router similarity loss are set to 1e-4, as this configuration yielded the highest generation quality in our ablation experiments. Results show our method improves expert combination ratio, image generation quality, and load-balancing performance. figure 7 further demonstrates the evaluation of MoE configurations across multiple scales (4-in-16,32,64,128), highlighting our approachs capability to diversify expert activation pattern compared to existing methods. Table 2 Load balance for 4-in-32 MoE. Setting FID MaxVio Comb No Constraint Balance Loss Router Similarity 11.38 11.67 10.77 6.383 2.052 0.850 18.98 72.36 83.10 Figure 7 Combination usage comparison between different number of experts."
        },
        {
            "title": "7.5 Core Components",
            "content": "table 3 demonstrates the improvements brought by each component. Starting from the baseline of expert racing with softmax gating, we incrementally added identity gating, learnable thresholds, per-layer regularization, and router similarity loss. Notably, FID and CMMD consistently decrease and CLIP score increases with the addition of each technique, indicating enhancements in image quality and alignment with conditional distributions. Table 3 Ablation study of core components. Setting FID CMMD CLIP Expert Race (softmax) + Identity Gating + Learnable Threshold + Per-Layer Reg. + Router Similarity 16.47 13.66 11.56 8.95 8. .7104 .6317 .5863 .4847 .4587 21.97 22.25 22.56 22.94 23."
        },
        {
            "title": "7.6 Scaling Law",
            "content": "We first scale the base model sizes in the full pipeline, as detailed in table 4. We have four settings: Base (B), Medium (M), Large (L), and Extra-large (XL). Under the same configuration we compared our 4-in-32 MoE models with their dense counterparts, noting that their activation parameter counts are nearly identical. The experimental results and figure 8 demonstrate that our model significantly outperforms the corresponding Dense models given same activation parameter count. Furthermore, our MoE-4in32 model surpasses the XL-Dense model with less than half the total number of parameters, further showcasing the efficiency of our model design. Table 4 Model specifications and evaluation results of the comparison between MoE and Dense models. Model Config. Total Param. Activate # Layers Hidden # Heads FID CMMD CLIP B/2-Dense M/2-Dense L/2-Dense XL/2-Dense B/2-MoE-4in32 M/2-MoE-4in32 L/2-MoE-4in32 XL/2-MoE-4in 0.127B 0.265B 0.453B 0.669B 0.531B 1.106B 1.889B 2.788B 0.127B 0.265B 0.453B 0.669B 0.135B 0.281B 0.479B 0.707B 12 16 24 28 12 16 24 28 768 960 1024 1152 768 960 1024 1152 12 16 16 16 12 16 16 16 18.03 11.18 7.88 6.31 7.35 5.16 4.04 3. .7532 .5775 .4842 .4338 .4460 .3507 .2775 .1784 21.83 22.56 23.00 23.27 23.15 23.50 24.12 24.68 Figure 8 comparison between Dense and our MoE models. Our MoE models consistently outperform their Dense counterparts across the FID and training curves. Notably, the MoE model with activation size shows better performance compared to the Dense model scaled to size XL. We further expand the models size while maintaining the same number of activation parameters under the model configurations (B/2-MoE). The model expansion is achieved by varying the hidden split ratios of experts and increasing the number of candidate experts. As shown in figure 9 and table 5, increasing both the number of candidate experts and splitting the hidden dimensions of MoE leads to improvement in performance, highlighting the potential of our MoE architecture for scaling up. Table 5 Evaluation results of different MoE configurations with the same number of activation parameters. Config. Hidden Split Total Param. FID CMMD CLIP 1-in-4 1-in-8 1-in-16 2-in-8 2-in-16 2-in-32 4-in-16 4-in-32 4-in-64 8-in-32 8-in-64 8-in-128 1 2 4 9.70 9.05 8.65 8.03 7.78 7.57 7.78 7.35 6.91 7.56 6.87 6.28 .5200 .4976 .5019 .4587 .4607 .4483 .4628 .4460 .4244 .4516 .4263 .4015 22.82 22.91 22.92 23.09 23.06 23.12 23.09 23.15 23.21 23.11 23.24 23.35 0.297B 0.524B 0.978B 0.297B 0.524B 0.977B 0.297B 0.524B 0.977B 0.297B 0.524B 0.977B 13 Figure 9 Scaling results of DiT-B in different MoE configurations. Our method demonstrates linear performance improvement when varying expert split ratios and increasing the number of candidate experts."
        },
        {
            "title": "7.7 Extended Routing Strategy",
            "content": "We extend the token-choice and expert-choice routing strategies by introducing varying degrees of routing selection flexibility, aiming to investigate how training-stage router freedom impacts final model performance. All compared methods are trained for 500K iterations under identical configurations: full pipeline with DiT-B/2-MoE-2-in-8 architecture. To ensure experimental consistency, all approaches employ learnable thresholds for inference queries. Specifically, we develop three new strategies: BL-Choice, BE-Choice, and LE-Choice, obtained through pairwise combinations of selection dimensions - Batch (B), Sequence Length (L), and Expert count (E), as illustrated in figure 10. Experimental results in table 6 demonstrate that strategies with higher training selection freedom consistently outperform conventional fixed-dimension top-k selection approaches (such as token-choice/expert-choice). Notably, our Expert Race achieves the best performance across all evaluated routing strategies. Figure 10 Top-K selection flexibility in more extended routing strategies. Table 6 Design Choices and Evaluation Results of Different Routing Strategies Token Choice Expert Choice BL Choice BE Choice LE Choice Expert Race DA DB FID CMMD CLIP E 9.50 .5202 22.81 L/E 10.13 .5639 22.73 B k/E 9.08 .5145 22. B 8.28 .4636 23.05 L 8.89 .4871 22.99 1 8.03 .4587 23."
        },
        {
            "title": "7.8 More Results on ImageNet 256 × 256",
            "content": "Figure 11 Random generated 256 256 samples from RaceDiT-XL/2-4in32. Classifier-free guidance scale is 4. We present additional results on the ImageNet 256 256. Our model is capable of generating high-quality images, as evidenced by the random samples shown in figure 11 and the single-class generation results presented in appendix E. We further provide comparison with leading approaches shown in table 7, where Samples is reported as training iterations batch size. In our experiments, we train vanilla DiT (marked with *) and MoE model with Expert Race following the training protocol from the original DiT paper [25], except that we use larger batch size 1024 to improve memory utilization and train for only 1.75M steps. The learning rate remains unchanged at 1e4. We also provide the training curves in figure 1 (Right). Our MoE model, of similar amount of activated parameters, achieves better performance and faster convergence. Model Config. Table 7 Comparison with other methods on ImageNet 256x256. Samples Total Activated FID IS Precision Recall ADM-G [6] LDM-8-G [31] MDT [11] MDT [11] DiT-XL/2 [25] SiT-XL [22] MaskDiT [43] DiT-MoE-XL/2 [9] DiT-XL/2* RaceDiT-XL/2-4in32 0.608B 0.506B 0.675B 0.675B 0.669B 0.669B 0.737B 4.105B 0.669B 2.788B 0.608B 0.506B 0.675B 0.675B 0.669B 0.669B 0.737B 1.530B 0.669B 0.707B 2.0M 256 4.8M 64 2.5M 256 6.5M 256 7.0M 256 7.0M 256 2.0M 1024 7.0M 1024 1.7M 1024 1.7M 1024 4.59 7.76 2.15 1.79 2.27 2.06 2.50 1. 3.02 2.06 186.70 209.52 249.27 283.01 278.24 277.50 256.27 315.73 261.49 318.64 0.82 0.84 0.82 0.81 0.83 0.83 0.83 0.83 0.81 0.83 0.52 0.35 0.58 0.61 0.57 0.59 0.56 0. 0.51 0."
        },
        {
            "title": "8 Conclusion\nThis paper proposes Expert Race, a novel Mixture-of-Experts (MoE) routing strategy that enables stable and\nefficient scaling of diffusion transformers. Compared to previous methods with fixed degrees of freedom in\nexpert-token assignments, our strategy achieves higher routing flexibility by enabling top-k selection across\nthe full routing space spanning batch, sequence, and expert dimensions. This expanded selection capability\nprovides greater optimization freedom, significantly improving performance when scaling diffusion transformers.\nTo address challenges from increased flexibility, we propose an EMA-based threshold adaptation mechanism\nthat mitigates timestep-induced distribution shifts between training (randomized per-sample timesteps) and\ninference (uniform timesteps), ensuring generation consistency. Additionally, per-layer regularization improves\ntraining stability, while router similarity loss promotes diverse expert combinations and better load balancing,\nas shown on 256×256 ImageNet generation tasks. As a general routing strategy, future work will extend\nExpert Race to broader diffusion-based visual tasks.",
            "content": ""
        },
        {
            "title": "References",
            "content": "[1] Yogesh Balaji, Seungjun Nah, Xun Huang, Arash Vahdat, Jiaming Song, Qinsheng Zhang, Karsten Kreis, Miika Aittala, Timo Aila, Samuli Laine, et al. ediff-i: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. [2] Raphael Bensadoun, Tom Monnier, Yanir Kleiman, Filippos Kokkinos, Yawar Siddiqui, Mahendra Kariya, Omri Harosh, Roman Shapovalov, Benjamin Graham, Emilien Garreau, et al. Meta 3d gen. arXiv preprint arXiv:2407.02599, 2024. [3] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, Clarence Ng, Ricky Wang, and Aditya Ramesh. Video generation models as world simulators. 2024. [4] Damai Dai, Chengqi Deng, Chenggang Zhao, RX Xu, Huazuo Gao, Deli Chen, Jiashi Li, Wangding Zeng, Xingkai Yu, Wu, et al. Deepseekmoe: Towards ultimate expert specialization in mixture-of-experts language models. arXiv preprint arXiv:2401.06066, 2024. [5] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR). Ieee, 2009. [6] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems (NeurIPS), 34:87808794, 2021. [7] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning (ICML), 2024. [8] William Fedus, Barret Zoph, and Noam Shazeer. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research (JMLR), 23(120):139, 2022. [9] Zhengcong Fei, Mingyuan Fan, Changqian Yu, Debang Li, and Junshi Huang. Scaling diffusion transformers to billion parameters. arXiv preprint arXiv:2407.11633, 2024. [10] Zhida Feng, Zhenyu Zhang, Xintong Yu, Yewei Fang, Lanxin Li, Xuyi Chen, Yuxiang Lu, Jiaxiang Liu, Weichong Yin, Shikun Feng, et al. Ernie-vilg 2.0: Improving text-to-image diffusion model with knowledge-enhanced mixture-of-denoising-experts. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1013510145, 2023. [11] Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 2316423173, 2023. [12] Yongxin Guo, Zhenglin Cheng, Xiaoying Tang, Zhaopeng Tu, and Tao Lin. Dynamic mixture of experts: An auto-tuning approach for efficient transformer models. arXiv preprint arXiv:2405.14297, 2024. [13] Agrim Gupta, Lijun Yu, Kihyuk Sohn, Xiuye Gu, Meera Hahn, Fei-Fei Li, Irfan Essa, Lu Jiang, and José Lezama. Photorealistic video generation with diffusion models. In European Conference on Computer Vision (ECCV), pages 393411. Springer, 2024. [14] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in Neural Information Processing Systems (NeurIPS), 30, 2017. [15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems (NeurIPS), 33:68406851, 2020. [16] Gagan Jain, Nidhi Hegde, Aditya Kusupati, Arsha Nagrani, Shyamal Buch, Prateek Jain, Anurag Arnab, and Sujoy Paul. Mixture of nested experts: Adaptive processing of visual tokens. arXiv preprint arXiv:2407.19985, 2024. [17] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards better evaluation metric for image generation. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 93079315, 2024. [18] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [19] Yunsung Lee, JinYoung Kim, Hyojun Go, Myeongho Jeong, Shinhyeok Oh, and Seungtaek Choi. Multi-architecture multi-expert diffusion models. In AAAI Conference on Artificial Intelligence (AAAI), volume 38, pages 13427 13436, 2024. [20] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. {GS}hard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations (ICLR), 2021. [21] Mike Lewis, Shruti Bhosale, Tim Dettmers, Naman Goyal, and Luke Zettlemoyer. Base layers: Simplifying training of large, sparse models. In International Conference on Machine Learning (ICML), pages 62656274, 2021. [22] Nanye Ma, Mark Goldstein, Michael Albergo, Nicholas Boffi, Eric Vanden-Eijnden, and Saining Xie. Sit: In European Exploring flow and diffusion-based generative models with scalable interpolant transformers. Conference on Computer Vision (ECCV), pages 2340, 2024. [23] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. [24] Byeongjun Park, Sangmin Woo, Hyojun Go, Jin-Young Kim, and Changick Kim. Denoising task routing for diffusion models. In International Conference on Learning Representations (ICLR), 2024. [25] William Peebles and Saining Xie. Scalable diffusion models with transformers. In IEEE International Conference on Computer Vision (ICCV), pages 41954205, 2023. [26] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film: Visual reasoning with general conditioning layer. In AAAI Conference on Artificial Intelligence (AAAI), volume 32, 2018. [27] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning (ICML), pages 87488763. PMLR, 2021. [28] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 2022. [29] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, André Susano Pinto, Daniel Keysers, and Neil Houlsby. Scaling vision with sparse mixture of experts. Advances in Neural Information Processing Systems (NeurIPS), 34:85838595, 2021. [30] Stephen Roller, Sainbayar Sukhbaatar, Jason Weston, et al. Hash layers for large sparse models. Advances in Neural Information Processing Systems (NeurIPS), 34:1755517566, 2021. [31] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. [33] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in Neural Information Processing Systems (NeurIPS), pages 3647936494, 2022. [34] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. In International Conference on Learning Representations (ICLR), 2017. [35] Haotian Sun, Tao Lei, Bowen Zhang, Yanghao Li, Haoshuo Huang, Ruoming Pang, Bo Dai, and Nan Du. Ec-dit: Scaling diffusion transformers with adaptive expert-choice routing. arXiv preprint arXiv:2410.02098, 2024. [36] Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, and Damai Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. arXiv preprint arXiv:2408.15664, 2024. [37] Lean Wang, Huazuo Gao, Chenggang Zhao, Xu Sun, and Damai Dai. Auxiliary-loss-free load balancing strategy for mixture-of-experts. arXiv preprint arXiv:2408.15664, 2024. [38] Ziteng Wang, Jianfei Chen, and Jun Zhu. Remoe: Fully differentiable mixture-of-experts with relu routing. arXiv preprint arXiv:2412.14711, 2024. [39] Zeyue Xue, Guanglu Song, Qiushan Guo, Boxiao Liu, Zhuofan Zong, Yu Liu, and Ping Luo. Raphael: Textto-image generation via large mixture of diffusion paths. Advances in Neural Information Processing Systems (NeurIPS), 36, 2024. [40] Jure Zbontar, Li Jing, Ishan Misra, Yann LeCun, and Stéphane Deny. Barlow twins: Self-supervised learning via redundancy reduction. In International Conference on Machine Learning (ICML), pages 1231012320. PMLR, 2021. [41] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. ACM Transactions on Graphics (TOG), 43(4):120, 2024. [42] Wangbo Zhao, Yizeng Han, Jiasheng Tang, Kai Wang, Yibing Song, Gao Huang, Fan Wang, and Yang You. Dynamic diffusion transformer. arXiv preprint arXiv:2410.03456, 2024. [43] Hongkai Zheng, Weili Nie, Arash Vahdat, and Anima Anandkumar. Fast training of diffusion models with masked transformers. arXiv preprint arXiv:2306.09305, 2023. [44] Yanqi Zhou, Tao Lei, Hanxiao Liu, Nan Du, Yanping Huang, Vincent Zhao, Andrew Dai, Quoc Le, James Laudon, et al. Mixture-of-experts with expert choice routing. Advances in Neural Information Processing Systems (NeurIPS), 35:71037114, 2022. [45] Barret Zoph, Irwan Bello, Sameer Kumar, Nan Du, Yanping Huang, Jeff Dean, Noam Shazeer, and William Fedus. St-moe: Designing stable and transferable sparse expert models. arXiv preprint arXiv:2202.08906, 2022. [46] Simiao Zuo, Xiaodong Liu, Jian Jiao, Young Jin Kim, Hany Hassan, Ruofei Zhang, Jianfeng Gao, and Tuo Zhao. Taming sparsely activated transformer with stochastic experts. In International Conference on Learning Representations (ICLR), 2022."
        },
        {
            "title": "Appendix",
            "content": "A Implemention of the Per-Layer Regularization Figure 12 An illustration of the calculation for Per-Layer Regularization. figure 12 illustrates the Per-Layer Regularization mechanism. The input hl at layer passes through two-layer MLP router. The first layer applies linear transformation and GELU activation while preserving the original hidden dimension. The second layer branches into two heads: (1) gating head producing routing logits, and (2) target head predicting the output target through linear projection. This dual-head design enables concurrent routing and target prediction. The L2 loss is computed per-layer between the target heads prediction H(hl) and ground truth y. By aligning intermediate layer outputs with the final target, this regularization effectively enhances the contribution of shallow MoE layers to the final results in pre-norm architectures, significantly accelerating their optimization."
        },
        {
            "title": "B Analysis of Router Similarity Loss",
            "content": "In this section, we will demonstrate that the router similarity loss is an extended form of the balance loss, expanding from constraining individual experts to combinations of experts. Load balance loss [4] are designed to encourage tokens to be evenly distributed across experts. For an MoE configuration with experts and top-k value of K, the load balance loss for batch with tokens is as follows: Lblc = = (cid:88) i= (cid:88) i=1 fi Pi (cid:32) KT (cid:88) t=1 1(i, t) (cid:33) (cid:32) (cid:33) s(i, t) 1 T (cid:88) t=1 (13) where 1(i, t) is the indicator function that equals 1 when token is assigned to expert and s(i, t) is the router logits of token and expert i. Since softmax activation is applied over the expert dimension, s(i, t) can be regarded as the probability that expert is selected given token as condition, marked as p(it). Considering that each token is uniformly sampled from the dataset, 1 (cid:88) t=1 s(i, t) = (cid:88) t=1 19 p(it) p(t) = Pi denotes to the marginal probability of choosing expert i. And fi ="
        },
        {
            "title": "E\nKT",
            "content": "T (cid:88) t=1 1(i, t) = 1 (cid:80)T t= 1(i, t) (cid:80)E (cid:80)T i=1 t=1 1(i, t) is the ratio of the actual number of tokens assigned to expert to the expected number of tokens assigned to each expert. As for router similarity loss, each element of matrix in equation (9) can be formulated as below:"
        },
        {
            "title": "1\nT",
            "content": "P i,j ="
        },
        {
            "title": "1\nT",
            "content": "T (cid:88) t=1 s(i, t) s(j, t) (cid:88) = p(it) p(jt) p(t) t=1 = p(i, j) (14) The matrix actually represents the probability of the pair of experts and being selected. Similarly, each element in of equation (9) is the ratio of the actual number of tokens assigned to the expert pair (i, j) to the expected number of tokens assigned to each pairwise combination and thus (i, j) of equation (10) has similar form to fi in balance loss. Noted that the above equations model the probability of sampling with replacement for experts. However, in practical implementation, sampling is performed without replacement, and the case that one expert is selected more than once does not exist. Therefore, in equation (11), we normalize the diagonal and off-diagonal parts of the matrix separately. The diagonal part degenerates into an estimation for individual experts as follows: 1 E E Lsim_diag = = = = = = (cid:88) i=1 (i, i) i,i (cid:88) i=1 (cid:88) i=1 (cid:88) i=1 (cid:88) (cid:88) s(i, t)2 t=1 1(i, t)2 (cid:80)E i,i j,j (cid:80)T (cid:80)E t=1 (cid:80)T 1(j, t)2 j=1 (cid:80)T t=1 1(i, t) t=1 (cid:80)T t=1 1(i, t) (cid:80)E j=1 (cid:80)T i=1 KT 1(j, t) (cid:88) s(i, t)2 (cid:88) t=1 s(i, t) (cid:88) t=1 s(i, t)2 i=1 (cid:32) KT (cid:88) i=1 (cid:88) t=1 t=1 (cid:33) (cid:32) 1(i, t) 1 (cid:33) s(i, t)2 (cid:88) t=1 (15) Therefore, the diagonal part of router similarity loss represents geometric mean version of the balance loss as described above. The (i, i) in the router similarity loss is identical to fi in the balance loss. Similarly, for the off-diagonal weights (i, j), we estimate the expected number of times that each expert pair is selected by dividing the sum of all elements, (cid:80) i=j (i, j), by the number of terms, E2 E. The complete router similarity loss can be viewed as an augmentation of the traditional balance loss, particularly by incorporating the pairwise interaction between experts, to help the model make more effective use of different combinations of experts. It is worth noting that we normalize both the weights and the probabilities, making the calculation of the loss function independent of the number of experts, top-k, and token sequence length. Considering random assignment case, assume that the scores of the entire score matrix are the same constant. In this case, Pi,j = E2 , substituting this into the equation (10) yields Lsim = 1."
        },
        {
            "title": "C Combination Usage",
            "content": "Figure 13 Compute process of Combination Usage. To estimate the actual number of pairwise expert combinations activated and involved in computation, we use the metric called Combination Usage whose process is shown in the figure 13. From the perspective of pairwise combinations, for experts, there are total of (cid:0)E (cid:1) pairwise combinations. First, we count the number of times each expert pair is selected across all tokens, resulting in histogram. Then, we sort the counts in descending order and normalize them to obtain sorted normalized histogram. Finally, we compute the cumulative sum and count the number of bins whose cumulative sum is less than 95%. Finally, we compute the cumulative sum and calculate the proportion of bins whose cumulative sum is less than 95% relative to the total number of bins, i.e., NA/C 2 . This proportion is referred to as the Combination Usage. In other words, those expert pairs with their amount less than 5% of the total count are considered inactive. We set k=4 and conducted experiments with different numbers of experts, as shown in the figure 7. The experiment results indicate that our method effectively enhances the utilization of larger number of combinations. 2 Additional Comparisons with DiT-MoE Table 8 Comparison to DiT-MoE Model Config. Total Param. Activated Param. Iters. FID CMMD CLIP DiT-MoE-B/2-8E2A [9] RaceDiT-B/2-2in8 0.795B 0.297B 0.286B 0.135B 500K 500K 9.06 8.02 .5049 .4587 22.87 23.09 Figure 14 Training curve comparisons between DiT-MoE [9] and our model. We provide an additional comparison with DiT-MoE [9], our most-related work using its official opensource code. We conduct experiments under the DiT-MoE-B/2-8E2A setting and compared it with our RaceDiT-B/2-2in8 model using the same training configuration. Both models were trained for 500K iterations. There are several differences between the models: DiT-MoE uses GLU, while our method employs standard MLP. Additionally, DiT-MoE includes two extra shared experts, which our model does not. As result, our model has fewer total and activated parameters. Despite these differences, as demonstrated in table 8 and figure 14, our method achieves better training loss and evaluation metrics, even with smaller number of activated parameters."
        },
        {
            "title": "E Additional Image Generation Results",
            "content": "Figure 15 Samples from RaceDiT-XL/2-4in32 (256256). Classifier-free guidance: 4.0 Label: Macaw (88) Figure 16 Samples from RaceDiT-XL/2-4in32 (256256). Classifier-free guidance: 4.0 Label: loggerhead turtle (33) Figure 17 Samples from RaceDiT-XL/2-4in32 (256256). Classifier-free guidance: 4.0 Label: lion (291) Figure 18 Samples from RaceDiT-XL/2-4in32 (256256). Classifier-free guidance: 4.0 Label: lesser panda (387) 22 Figure 19 Samples from RaceDiT-XL/2-4in32 (256256). Classifier-free guidance: 4.0 Label: convertible (511) Figure 20 Samples from RaceDiT-XL/2-4in32(256256). Classifier-free guidance: 4.0 Label: balloon (417) Figure 21 Samples from RaceDiT-XL/2-4in32 (256256). Classifier-free guidance: 4.0 Label: lakeside (975) Figure 22 Samples from RaceDiT-XL/2-4in32 (256256). Classifier-free guidance: 4.0 Label: volcano (980)"
        }
    ],
    "affiliations": [
        "ByteDance Seed",
        "ShanghaiTech University"
    ]
}