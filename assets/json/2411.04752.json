{
    "paper_title": "RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval",
    "authors": [
        "Aniket Deroy",
        "Subhankar Maity"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Code-mixing, the integration of lexical and grammatical elements from multiple languages within a single sentence, is a widespread linguistic phenomenon, particularly prevalent in multilingual societies. In India, social media users frequently engage in code-mixed conversations using the Roman script, especially among migrant communities who form online groups to share relevant local information. This paper focuses on the challenges of extracting relevant information from code-mixed conversations, specifically within Roman transliterated Bengali mixed with English. This study presents a novel approach to address these challenges by developing a mechanism to automatically identify the most relevant answers from code-mixed conversations. We have experimented with a dataset comprising of queries and documents from Facebook, and Query Relevance files (QRels) to aid in this task. Our results demonstrate the effectiveness of our approach in extracting pertinent information from complex, code-mixed digital conversations, contributing to the broader field of natural language processing in multilingual and informal text environments. We use GPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant documents to frame a mathematical model which helps to detect relevant documents corresponding to a query."
        },
        {
            "title": "Start",
            "content": "RetrieveGPT: Merging Prompts and Mathematical Models for Enhanced Code-Mixed Information Retrieval Aniket Deroy1,*,, Subhankar Maity1 1IIT Kharagpur, Kharagpur, India Abstract Code-mixing, the integration of lexical and grammatical elements from multiple languages within single sentence, is widespread linguistic phenomenon, particularly prevalent in multilingual societies. In India, social media users frequently engage in code-mixed conversations using the Roman script, especially among migrant communities who form online groups to share relevant local information. This paper focuses on the challenges of extracting relevant information from code-mixed conversations, specifically within Roman transliterated Bengali mixed with English. This study presents novel approach to address these challenges by developing mechanism to automatically identify the most relevant answers from code-mixed conversations. We have experimented with dataset comprising of queries and documents from Facebook, and Query Relevance files (QRels) to aid in this task. Our results demonstrate the effectiveness of our approach in extracting pertinent information from complex, code-mixed digital conversations, contributing to the broader field of natural language processing in multilingual and informal text environments. We use GPT-3.5 Turbo via prompting alongwith using the sequential nature of relevant documents to frame mathematical model which helps to detect relevant documents corresponding to query. Keywords GPT, Relevance, Code Mixing, Probability, Prompt Engineering 1. Introduction Code-mixing, where elements from multiple languages are blended within single sentence, is natural and widespread phenomenon in multilingual societies [1, 2, 3]. It is particularly prevalent in India, country with rich linguistic diversity where speakers often switch between languages depending on context, audience, and medium of communication [4, 5]. With the rapid rise of online social networking, this practice has become increasingly common in digital conversations, where users frequently combine their native languages with others, often using foreign scripts [6, 7]. One notable trend in India is the use of the Roman script to communicate in native languages on social media platforms [8, 9]. This practice is especially common among migrant communities who form online groups to share information and experiences relevant to their unique circumstances [10, 11]. For instance, Bengali speakers from West Bengal who have migrated to urban centers like Delhi or Bangalore often establish groups such as \"Bengali in Delhi\" on platforms like Facebook and WhatsApp. These groups serve as vital hubs for exchanging advice on wide range of local issues, from housing and employment to navigating new social environments. The COVID-19 pandemic highlighted the importance of these online communities as critical sources of information [12, 13]. During this period, these groups became essential for sharing experiences, seeking support, and keeping up with the frequently changing government guidelines. However, the informal and often colloquial nature of the language used in these code-mixed conversations, typically transliterated into Roman script, presents significant challenges for information retrieval. The lack of standardization, combined with the blending of languages, makes it difficult to identify and extract relevant answers, especially for those who might seek similar information at later time [14, 15]. Forum for Information Retrieval Evaluation, December 12-15, 2024, India *Corresponding author. $ roydanik18@kgpian.iitkgp.ac.in (A. Deroy); subhankar.ai@kgpian.iitkgp.ac.in (S. Maity) (cid:26) 0000-0001-7190-5040 (A. Deroy); 0009-0001-1358-9534 (S. Maity) 2022 Copyright for this paper by its authors. Use permitted under Creative Commons License Attribution 4.0 International (CC BY 4.0). 4 2 0 N 7 ] . [ 1 2 5 7 4 0 . 1 1 4 2 : r This paper addresses the challenge of extracting relevant information from code-mixed digital conversations, with specific focus on Roman transliterated Bengali mixed with English. While code-mixing is well-recognized phenomenon in natural language processing (NLP), the unique characteristics of transliterated textsuch as variations in spelling, grammar, and syntaxcomplicate the task of effective information retrieval [16, 17]. To tackle this issue, we have developed mechanism that identifies the most relevant answers from these complex, multilingual discussions. We begin experimenting with dataset of code-mixed conversations collected from Facebook, which has been carefully annotated to reflect query relevance (QRels). This dataset forms the basis of our study and is crucial for evaluating the effectiveness of our approach. We leverage GPT-3.5 Turbo [18, 19] by employing carefully designed prompts that guide the model to evaluate the relevance of documents with respect to given query. This involves not only the semantic understanding capabilities of GPT-3.5 Turbo but also the strategic use of the sequential nature of documents. Often, documents are part of series or conversation where the relevance to query can be influenced by preceding or succeeding documents. By acknowledging this sequence, we can better capture contextual relationships that might be missed if documents were considered in isolation. To formalize this process, we integrate GPT-3.5 Turbos outputs into mathematical model. This model takes into account the sequential dependencies among documents, treating the task of relevance detection as problem of finding the optimal path or chain of relevance across the sequence. 2. Related Work Code-mixing and transliteration have gained increasing attention in the field of natural language processing (NLP), especially as global communication becomes more digital and multilingual [20, 21, 22, 23]. This section reviews key studies related to code-mixing, information retrieval from code-mixed text, and the challenges of processing Roman transliterated languages, particularly in the context of Indian languages. Code-mixing, where speakers blend elements from multiple languages within single utterance, is common linguistic phenomenon in multilingual societies [22, 24, 25]. Early studies on code-mixing focused primarily on sociolinguistic aspects, examining how and why speakers switch languages within conversations [20, 21, 22, 26, 27]. However, with the advent of digital communication, researchers have increasingly turned their attention to computational methods for processing and understanding code-mixed text [28, 29, 30]. Several studies have explored various NLP tasks, such as part-of-speech tagging, language identification, and sentiment analysis, in code-mixed settings [31, 32]. [33, 34] provided one of the earliest comprehensive analyses of code-mixed text, highlighting the unique challenges it poses for traditional NLP pipelines, such as non-standard spelling, syntax variations, and the blending of multiple languages within single text. More recent work by [35, 36, 37] introduced code-mixed dataset, spanning multiple Indian languages, which has become benchmark for evaluating NLP models in this domain. Information retrieval (IR) in code-mixed settings is relatively underexplored compared to other NLP tasks [38, 39, 40]. However, the need for effective IR systems that can handle multilingual and code-mixed queries has become increasingly important, particularly in the context of digital information exchange on social media platforms. [41, 40] investigated the problem of query-focused summarization in code-mixed social media data, emphasizing the complexity of extracting relevant information from noisy, informal text. Work by [42] addressed code-mixed question answering, where the goal is to identify correct responses from mixed-language corpus. Their approach involved using translation models to standardize the text before applying traditional IR techniques, demonstrating that even simple translation-based methods can significantly improve performance. However, these methods often fail to capture the nuances of code-mixed language, such as cultural context and colloquial expressions. Roman script transliteration of Indian languages, commonly referred to as \"Romanagari\" for languages like Hindi, is widespread practice in digital communication. Transliteration introduces additional challenges for NLP, as it often involves non-standard spellings and inconsistent usage. For instance, multiple transliterations may exist for the same word, depending on the speakers regional accent, literacy in the original script, or personal preference. Notable efforts in this area include the work by [43, 44], which explored transliteration normalization for Hindi-English code-mixed text. They developed algorithms to map Romanized text back to its original script, enabling more accurate processing by traditional NLP models. However, normalization remains challenging task due to the inherent variability in transliterated text. In the context of Bengali, the Roman script transliteration is less standardized than for Hindi, leading to even greater variability in spelling and grammar. [45, 46] addressed this issue by creating Roman Bengali dataset and proposed methods for transliteration normalization and language identification. Their work highlights the difficulties of processing Roman Bengali and the need for specialized approaches tailored to the characteristics of the language. While these studies provide valuable insights into code-mixing, transliteration, and information retrieval, there is noticeable gap in addressing the specific challenges of extracting relevant information from code-mixed conversations in Roman transliterated Bengali. Our work builds on the foundations laid by previous research but focuses on the unique intersection of these challenges in real-world context. By developing mechanism to identify relevant answers in code-mixed discussions, we aim to contribute to the growing body of research on multilingual NLP and enhance the accessibility of information in linguistically diverse online communities. Large Language Models (LLMs) [46, 47, 48, 49, 50, 51, 52] like GPT-3 have shown promise in various NLP tasks, including LI. Previous works have demonstrated the capability of GPT-3 in performing zero-shot and few-shot learning, making it potentially powerful tool for LI in resource-constrained settings. However, the application of LLMs [53, 54, 55, 56, 57, 58, 59] to code-mixed and morphologically rich languages remains underexplored. Recent studies, have started to explore the use of transformers and pre-trained models for multilingual LI, but the effectiveness of these models in Bengali languages requires further investigation. This section places our work within the context of existing research, highlighting the contributions of prior studies while identifying gaps that our research aims to fill. 3. Dataset This shared task consists of single dataset for code mixed information retrieval. The corpus consists of 107900 documents in the training set and 20 queries in the training set. There are 30 queries in the testing set. The dataset is in roman transliterated bengali mixed with english language. 4. Task Definition The task is to automatically determine the relevance of query to document within code-mixed data, specifically focusing on English and Roman transliterated Bengali. Given query and document, the goal is to classify whether the query is relevant or not relevant to the document. Based on the relevance we have to rank the documents. This involves handling the complexities of code-mixing, where elements from both languages are used within the same text, and dealing with the informal and non-standardized nature of the language. The system must accurately capture the semantic relationship between the query and the document despite these linguistic challenges. 5. Methodology 5.1. Why Prompting? Prompting [60] for Information Retrieval is burgeoning approach that leverages large language models (LLMs) to enhance the retrieval of relevant information from complex, unstructured data, such as code-mixed text or informal online conversations [60]. Below are several reasons why prompting is becoming an effective strategy in information retrieval (IR): - Handling Ambiguity and Contextual Nuances: Traditional IR systems often struggle with understanding the nuanced language, ambiguity, and context found in unstructured or informal text, such as code-mixed conversations. Prompting LLMs allows these models to interpret context more effectively by guiding them to generate or rank responses that are contextually appropriate, even when dealing with code-mixing or informal language structures [61]. By crafting specific prompts, users can elicit more relevant and accurate results that account for the complexities of the input text. - Enhanced Language Understanding: Large language models like GPT-3.5 are pre-trained on vast datasets that include variety of languages and dialects [62]. This extensive training enables them to understand and generate text across different languages and contexts [62]. By using prompting, these models can be directed to focus on the most relevant aspects of query or document, improving the retrieval process even in multilingual and code-mixed scenarios. For example, when retrieving information from Roman transliterated Bengali mixed with English, an LLM can be prompted to recognize and process the code-mixed language more effectively than traditional IR systems. - Adaptability to Informal and Unstructured Text: Prompting allows LLMs to adapt to the informal and often unstructured nature of social media text [63], which is common in online communities. This flexibility is particularly beneficial when dealing with code-mixed or transliterated text, where the lack of standardization poses challenge to conventional IR techniques. Prompted language models can generate or filter responses that align more closely with the informal tone and style of the original text, thereby improving the relevance of the retrieved information. - Reduction of Noise and Irrelevance: One of the major challenges in IR is filtering out irrelevant or noisy data, especially in informal online conversations where off-topic or redundant information is common. By using targeted prompts, LLMs can be instructed to prioritize certain types of information, such as direct answers to specific questions, while de-emphasizing or ignoring irrelevant content [64]. This leads to more efficient and effective retrieval process, particularly in environments where users are seeking specific answers within sea of mixed and informal language. - Scalability and Customization: Prompting for information retrieval offers scalability and customization that traditional IR systems might lack. By designing prompts tailored to specific contexts or types of queries, LLMs can be dynamically adjusted to meet the needs of different retrieval tasks [64, 65]. This customization is particularly useful in handling domain-specific language or code-mixed scenarios, where standard IR systems might require extensive re-training or re-configuration. - Real-Time Processing and Interaction: In real-time communication platforms, the ability to quickly retrieve relevant information based on ongoing conversations is crucial. Prompting enables LLMs to process and respond to queries in real-time, enhancing the interactivity and responsiveness of the IR system [64]. This is especially beneficial in scenarios where users are engaged in active discussions and require immediate, contextually relevant information. 5.2. Merging Prompt and Mathematical Model-Based Approaches We used the GPT-3.5 Turbo model via prompting through the OpenAI API1 to solve the document retrieval task. We used the following prompt: \"Given the query <query> and the document <document>, find how relevant is the query to the document based on semantic similarity. Provide relevance score between 0 and 1. Only state the score.\" After the prompt is provided to the LLM, the following steps happen internal to the LLM while generating the output. The following outlines the steps that occur internally within the LLM, 1https://platform.openai.com/docs/models/gpt-3-5-turbo summarizing the prompting approach using GPT-3.5 Turbo: Step 1: Tokenization Prompt: 𝑋 = [𝑥1, 𝑥2, . . . , 𝑥𝑛] The input text (prompt) is first tokenized into smaller units called tokens. These tokens are often subwords or characters, depending on the models design. Tokenized Input: 𝑇 = [𝑡1, 𝑡2, . . . , 𝑡𝑚] Step 2: Embedding Each token is converted into high-dimensional vector (embedding) using an embedding matrix 𝐸. Embedding Matrix: 𝐸 R𝑉 𝑑, where 𝑉 is the size of the vocabulary and 𝑑 is the embedding dimension. Embedded Tokens: 𝑇emb = [𝐸(𝑡1), 𝐸(𝑡2), . . . , 𝐸(𝑡𝑚)] Step 3: Positional Encoding Since the model processes sequences, it adds positional information to the embeddings to capture the order of tokens. Positional Encoding: 𝑃 (𝑡𝑖) Input to the Model: 𝑍 = 𝑇emb + 𝑃 Step 4: Attention Mechanism (Transformer Architecture) Attention Score Calculation: The model computes attention scores to determine the importance of each token relative to others in the sequence. Attention Formula: Attention(𝑄, 𝐾, 𝑉 ) = softmax ( 𝑄𝐾𝑇 𝑑𝑘 ) 𝑉 (1) where 𝑄 (query), 𝐾 (key), and 𝑉 (value) are linear transformations of the input 𝑍. This attention mechanism is applied multiple times through multi-head attention, allowing the model to focus on different parts of the sequence simultaneously. Step 5: Feedforward Neural Networks The output of the attention mechanism is passed through feedforward neural networks, which apply non-linear transformations. Feedforward Layer: FFN(𝑥) = max(0, 𝑥𝑊1 + 𝑏1)𝑊2 + 𝑏 (2) where 𝑊1, 𝑊2 are weight matrices and 𝑏1, 𝑏2 are biases. Step 6: Stacking Layers Multiple layers of attention and feedforward networks are stacked, each with its own set of parameters. This forms the \"deep\" in deep learning. Layer Output: 𝐻 (𝑙) = LayerNorm(𝑍(𝑙) + Attention(𝑄(𝑙), 𝐾(𝑙), 𝑉 (𝑙))) 𝑍(𝑙+1) = LayerNorm(𝐻 (𝑙) + FFN(𝐻 (𝑙))) (3) (4) Step 7: Output Generation The final output of the stacked layers is sequence of vectors. These vectors are projected back into the token space using softmax layer to predict the next token or word in the sequence. Softmax Function: 𝑃 (𝑦𝑖𝑋) = exp(𝑍𝑖) 𝑗=1 exp(𝑍𝑗) 𝑉 (5) where 𝑍𝑖 is the logit corresponding to token 𝑖 in the vocabulary. The model generates the next token in the sequence based on the probability distribution, and the process repeats until the end of the output sequence is reached. Step 8: Decoding The predicted tokens are then decoded back into text, forming the final output. Output Text: 𝑌 = [𝑦1, 𝑦2, . . . , 𝑦𝑘] After obtaining the relevance score, we used the following mathematical formulation to account for the sequential presence of relevant documents. This can be written as follows: 𝑃 (𝐷𝑛+1 𝐷𝑛) = Score(𝐷𝑛+1) Score(𝐷𝑛+1) 0.2 + Score(𝐷𝑛+1) Score(𝐷𝑛+1) if Score(𝐷𝑛+1) < 0.3, 𝐷𝑛 = 𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑡 if 𝑛 = 1 if Score(𝐷𝑛+1) >= 0.3, 𝐷𝑛 = 𝑟𝑒𝑙𝑒𝑣𝑎𝑛𝑡 𝑜𝑡ℎ𝑒𝑟𝑤𝑖𝑠𝑒 This equation now reflects that if the score of the current document 𝐷𝑛 is less than 0.3 and the previous document is relevant, the probability of the current document being relevant is simply equal to the relevance score of current document. If the previous document is relevant and if the score of the current document 𝐷𝑛 is greater than equal to 0.3 then the probability that the current document is relevant is 0.2 + Score for the current document. For the first document, the probability is equal to the relevance score of current document. In all other situations, the probability is equal to the relevance score of current document. If the probability score of particular document is greater than 0.5, we consider the document to be relevant to the query. Like this we found out all documents which are relevant to query. For the five results reported, we ran the GPT model at different temperature values namely 0.5, 0,6, 0.7, 0.8, and 0.9. The diagram for GPT-3.5 Turbo is shown in Figure 1. The figure representing the methodology is shown in Figure 2. 6. Results Table 1 presents the evaluation metrics for different submissions for the team named \"TextTitans\". The metrics used to assess the performance are MAP Score, ndcg Score, p@5 Score, and p@10 Score. Heres what these results imply. MAP is common metric in information retrieval that measures the precision of results across multiple queries. higher MAP score indicates that relevant documents are consistently ranked higher across all queries. In the table, the MAP scores for the first four submissions are identical (0.701773), while the fifth submission slightly improves to 0.703734. This indicates that the fifth submission is marginally better in terms of ranking relevant results across multiple queries. The ndcg score measures the quality of the ranking based on the position of relevant documents. higher ndcg score suggests that relevant documents are placed higher in the ranking. The scores are also very similar across submissions, with the first four submissions having an ndcg score of 0.797937, and the fifth submission showing slight improvement to 0.799196. This suggests minor improvement in Figure 1: An overview of the GPT-3.5 Turbo architecture. Figure 2: Overview diagram of the methodology followed for GPT-3.5 Turbo. MAP Score ndcg Score p@5 Score p@10 Score Team Name Submission File Rank 0.701773 0.701773 0.701773 0.701773 0.703734 0. 0.797937 0.797937 0.797937 0.799196 0.793333 0. 0.793333 0.793333 0.793333 0.766667 0.766667 0. 0.766667 0.766667 TextTitans submit_cmir TextTitans submit_cmir_ TextTitans submit_cmir_2 TextTitans submit_cmir_3 TextTitans submit_cmir_ 5 4 3 2 1 Table 1 Comparison of MAP, NDCG, P@5, and P@10 Scores for the TextTitans Team. ranking relevant documents for the fifth submission. p@5 measures how many of the top 5 ranked documents are relevant. score of 1 would mean that all 5 of the top-ranked documents are relevant. All submissions have the same p@5 score of 0.793333, indicating that the top 5 results are equally accurate across all submissions. Precision@10 measures how many of the top 10 ranked documents are relevant. Like p@5, higher score is better. Similar to p@5, all submissions have the same p@10 score of 0.766667, showing no variation in the top 10 results across the different submissions. The metrics are very consistent across all submissions, with only minor improvements in MAP and NDCG scores for the fifth submission. The fifth submission shows slight improvement in ranking and retrieval performance, but the changes are minimal. The p@5 and p@10 scores indicate that the precision of the top 5 and top 10 results is identical across all submissions, suggesting that the models are performing similarly in identifying the most relevant documents. Overall, while there is slight improvement in the last submission, the models generally perform similarly across all metrics. 7. Conclusion In conclusion, this study addresses the critical challenges of extracting relevant information from code-mixed conversations, specifically within Roman transliterated Bengali mixed with English. This linguistic phenomenon is prevalent among migrant communities in India, who often rely on social media platforms to share and seek vital information, especially during crises like the COVID-19 pandemic. The informal and non-standardized nature of these conversations presents unique difficulties for information retrieval. To tackle these challenges, we developed novel approach that leverages the GPT-3.5 Turbo model in conjunction with sequential engineering approach, achieving notable success in retrieving pertinent answers from complex, code-mixed digital conversations. The effectiveness of our method is demonstrated through the results on the test set documents and queries, which provides valuable resource for future research in natural language processing within multilingual and informal text environments. This work contributes to enhancing information accessibility for marginalized communities, underscoring the potential of advanced AI models in bridging communication gaps in diverse linguistic landscapes. We observe that the GPT-3.5 model along with mathematical formulation approach performs well for the task of Code mixed information retrieval, though there is scope for improvement."
        },
        {
            "title": "References",
            "content": "[1] E. Sippola, Multilingualism and the structure of code-mixing, in: The Routledge handbook of Pidgin and Creole languages, Routledge, 2020, pp. 474489. [2] E. O. Aboh, Lessons from neuro-(a)-typical brains: universal multilingualism, code-mixing, recombination, and executive functions, Frontiers in psychology 11 (2020) 488. [3] A. Deroy, K. Ghosh, S. Ghosh, How ready are pre-trained abstractive models and llms for legal case judgement summarization?, arXiv preprint arXiv:2306.01248 (2023). [4] A. De Swaan, Words of the world: The global language system, John Wiley & Sons, 2013. [5] A. Deroy, K. Ghosh, S. Ghosh, Ensemble methods for improving extractive summarization of legal case judgements, Artificial Intelligence and Law 32 (2024) 231289. [6] C. Lee, Multilingual resources and practices in digital communication, in: The Routledge handbook of language and digital communication, Routledge, 2015, pp. 118132. [7] K. S. Rao, et al., novel approach to unsupervised pattern discovery in speech using convolutional neural network, Computer Speech & Language 71 (2022) 101259. [8] S. Shekhar, H. Garg, R. Agrawal, S. Shivani, B. Sharma, Hatred and trolling detection transliteration framework using hierarchical lstm in code-mixed social media text, Complex & Intelligent Systems 9 (2023) 28132826. [9] A. Deroy, P. Bhattacharya, K. Ghosh, S. Ghosh, An analytical study of algorithmic and expert summaries of legal cases, in: Legal Knowledge and Information Systems, IOS Press, 2021, pp. 9099. [10] L. Komito, Social media and migration: Virtual community 2.0, Journal of the American society for information science and technology 62 (2011) 10751086. [11] S. Maity, A. Deroy, S. Sarkar, novel multi-stage prompting approach for language agnostic mcq generation using gpt, in: European Conference on Information Retrieval, Springer, 2024, pp. 268277. [12] M. M. Meurer, M. Waldkirch, P. K. Schou, E. L. Bucher, K. Burmeister-Lamp, Digital affordances: How entrepreneurs access support in online communities during the covid-19 pandemic, Small Business Economics (2022) 127. [13] S. Maity, A. Deroy, S. Sarkar, Harnessing the power of prompt-based techniques for generating school-level questions using large language models, in: Proceedings of the 15th Annual Meeting of the Forum for Information Retrieval Evaluation, 2023, pp. 3039. [14] D. D. Lewis, K. S. Jones, Natural language processing for information retrieval, Communications of the ACM 39 (1996) 92101. [15] S. Maity, A. Deroy, S. Sarkar, How ready are generative pre-trained large language models for explaining bengali grammatical errors?, in: B. PaaÃŸen, C. D. Epp (Eds.), Proceedings of the 17th International Conference on Educational Data Mining, International Educational Data Mining Society, Atlanta, Georgia, USA, 2024, pp. 664671. doi:10.5281/zenodo.12729912. [16] M. Janse, N. Vassalou, D. Papazachariou, Variation in the vowel system of mišótika cappadocian: Findings from two refugee villages in greec, in: 13th International Conference on Greek Linguistics, University of Westminster, 2017. [17] S. Maity, A. Deroy, S. Sarkar, Exploring the capabilities of prompted large language models in educational and assessment applications, in: B. PaaÃŸen, C. D. Epp (Eds.), Proceedings of the 17th International Conference on Educational Data Mining, International Educational Data Mining Society, Atlanta, Georgia, USA, 2024, pp. 961968. doi:10.5281/zenodo.12730013. [18] T. B. Brown, Language models are few-shot learners, arXiv preprint ArXiv:2005.14165 (2020). [19] A. Deroy, S. Maity, Questioning biases in case judgment summaries: Legal datasets or large language models?, arXiv preprint arXiv:2312.00554 (2023). [20] T. Jauhiainen, H. Jauhiainen, K. Linden, survey on automatic language identification in written texts, in: Journal of Artificial Intelligence Research, volume 65, 2019, pp. 675782. [21] Y. Muthusamy, R. A. Cole, B. T. Oshika, Automatic language identification: review/tutorial, in: IEEE Signal Processing Magazine, volume 11, 1994, pp. 3341. [22] G. I. Ahmad, J. Singla, Sentiment analysis of code-mixed social media text (sa-cmsmt) in indianin: 2021 International Conference on Computing Sciences (ICCS), IEEE, 2021, pp. languages, 2533. [23] S. K. Nigam, A. Deroy, N. Shallum, A. K. Mishra, A. Roy, S. K. Mishra, A. Bhattacharya, S. Ghosh, K. Ghosh, Nonet at semeval-2023 task 6: Methodologies for legal evaluation, arXiv preprint arXiv:2310.11049 (2023). [24] A. Deroy, K. Ghosh, S. Ghosh, Applicability of large language models and generative models for legal case judgement summarization, Artificial Intelligence and Law (2024) 144. [25] S. K. Nigam, A. Deroy, Fact-based court judgment prediction, in: Proceedings of the 15th Annual Meeting of the Forum for Information Retrieval Evaluation, 2023, pp. 7882. [26] A. Deroy, S. Maity, S. Ghosh, Prompted zero-shot multi-label classification of factual incorrectness in machine-generated summaries., in: FIRE (Working Notes), 2023, pp. 734746. [27] S. Maity, A. Deroy, S. Sarkar, How effective is gpt-4 turbo in generating school-level questions from textbooks based on blooms revised taxonomy?, 2024. URL: https://arxiv.org/abs/2406.15211. arXiv:2406.15211. [28] A. F. Hidayatullah, A. Qazi, D. T. C. Lai, R. A. Apong, systematic review on language identification of code-mixed text: techniques, data availability, challenges, and framework development, IEEE access 10 (2022) 122812122831. [29] A. Deroy, S. Maity, Multi-label classification of covid-tweets using large language models, arXiv preprint arXiv:2312.10748 (2023). [30] A. Deroy, N. K. Bailung, K. Ghosh, S. Ghosh, A. Chakraborty, Artificial intelligence (ai) in legal data mining, arXiv preprint arXiv:2405.14707 (2024). [31] G. I. Ahmad, J. Singla, A. Anis, A. A. Reshi, A. A. Salameh, Machine learning techniques for sentiment analysis of code-mixed and switched indian social media text corpus: comprehensive review, International Journal of Advanced Computer Science and Applications 13 (2022). [32] A. Deroy, S. Maity, Ai-powered answer assessment: comprehensive overview, Authorea Preprints (2024). [33] A. F. Hidayatullah, A. Qazi, D. T. C. Lai, R. A. Apong, systematic review on language identification of code-mixed text: techniques, data availability, challenges, and framework development, IEEE access 10 (2022) 122812122831. [34] A. Deroy, S. Maity, short case study on understanding the capabilities of gpt for temporal reasoning tasks, Authorea Preprints (2024). [35] A. Pratapa, G. Bhat, M. Choudhury, S. Sitaram, S. Dandapat, K. Bali, Language modeling for code-mixing: The role of linguistic theory based synthetic data, in: Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2018, pp. 15431553. [36] A. Deroy, S. Maity, Question generation: Past, present & future, Authorea Preprints (2024). [37] A. Deroy, S. Maity, Exploring the mathematical reasoning capabilities of gemini, Authorea Preprints (2024). [38] U. Barman, Automatic processing of code-mixed social media content, Ph.D. thesis, Dublin City University, 2019. [39] S. Ghosh, K. Ghosh, D. Ganguly, A. Bhattacharya, P. P. Chakrabarti, S. Guha, A. Pal, K. Rudra, P. Majumder, D. Roy, et al., Report on the 2nd symposium on artificial intelligence and law (sail) 2022, in: ACM SIGIR Forum, volume 56, ACM New York, NY, USA, 2023, pp. 17. [40] S. Maity, A. Deroy, Natural language correction with an emphasis on bangla (2023). [41] D. Gupta, A. Ekbal, P. Bhattacharyya, semi-supervised approach to generate the code-mixed text using pre-trained encoder and transfer learning, in: T. Cohn, Y. He, Y. Liu (Eds.), Findings of the Association for Computational Linguistics: EMNLP 2020, Association for Computational Linguistics, Online, 2020, pp. 22672280. URL: https://aclanthology.org/2020.findings-emnlp.206. doi:10.18653/v1/2020.findings-emnlp.206. [42] K. R. Chandu, A. W. Black, Style variation as vantage point for code-switching, arXiv preprint arXiv:2005.00458 (2020). [43] K. Bali, J. Sharma, M. Choudhury, Y. Vyas, am borrowing ya mixing?\" an analysis of english-hindi code mixing in facebook, in: Proceedings of the first workshop on computational approaches to code switching, 2014, pp. 116126. [44] A. Deroy, Exploiting Machine Learning Techniques for Unsupervised Clustering of Speech Utterances, Ph.D. thesis, Indian Institute of Technology Kharagpur, 2019. [45] B. Sarkar, N. Sinhababu, M. Roy, P. K. D. Pramanik, P. Choudhury, Mining multilingual and International Journal of multiscript twitter data: unleashing the language and script barrier, Business Intelligence and Data Mining 16 (2020) 107127. [46] A. Deroy, S. Maity, S. Sarkar, Mirror: novel approach for the automated evaluation of open-ended question generation, arXiv preprint arXiv:2410.12893 (2024). [47] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Language models are unsupervised multitask learners, in: OpenAI Blog, volume 1, 2019. [48] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, P. J. Liu, Exploring the limits of transfer learning with unified text-to-text transformer, Journal of Machine Learning Research 21 (2020) 167. [49] J. Devlin, M.-W. Chang, K. Lee, K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, in: Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 2019, pp. 41714186. [50] S. K. Nigam, A. Deroy, S. Maity, A. Bhattacharya, Rethinking legal judgement prediction in realistic scenario in the era of large language models, arXiv preprint arXiv:2410.10542 (2024). [51] Y. Liu, M. Ott, N. Goyal, J. Du, M. Joshi, D. Chen, O. Levy, M. Lewis, L. Zettlemoyer, V. Stoyanov, Roberta: robustly optimized bert pretraining approach, in: arXiv preprint arXiv:1907.11692, 2019. [52] S. Maity, A. Deroy, S. Sarkar, Exploring the capabilities of prompted large language models in educational and assessment applications (2024). [53] S. Maity, A. Deroy, Generative ai and its impact on personalized intelligent tutoring systems, arXiv preprint arXiv:2410.10650 (2024). [54] W. X. Zhao, K. Zhou, J. Li, X. Tang, J. J. Wang, J. Liu, T. Wang, Y. Bao, J.-R. Wen, survey of large language models, in: arXiv preprint arXiv:2303.18223, 2023. [55] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, I. Polosukhin, Attention is all you need, Advances in neural information processing systems 30 (2017) 59986008. [56] A. Deroy, S. Maity, Code generation and algorithmic problem solving using llama 3.1 405b, arXiv preprint arXiv:2409.19027 (2024). [57] A. Radford, J. Wu, R. Child, D. Luan, D. Amodei, I. Sutskever, Fine-tuning gpt-2 for human-like text generation, in: arXiv preprint arXiv:1907.11692, 2019. [58] R. Zellers, A. Holtzman, H. Rashkin, Y. Bisk, A. Farhadi, F. Roesner, Y. Choi, Defending against neural fake news, in: Advances in Neural Information Processing Systems, volume 32, 2019, pp. 90549065. [59] S. Maity, A. Deroy, The future of learning in the age of generative ai: Automated question generation and assessment with large language models, arXiv preprint arXiv:2410.09576 (2024). [60] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, G. Neubig, Pre-train, prompt, and predict: systematic survey of prompting methods in natural language processing, ACM Computing Surveys 55 (2023) 135. [61] P. Singh, M. Patidar, L. Vig, Translating across cultures: Llms for intralingual cultural adaptation, arXiv preprint arXiv:2406.14504 (2024). [62] G. Yenduri, M. Ramalingam, G. C. Selvi, Y. Supriya, G. Srivastava, P. K. R. Maddikunta, G. D. Raj, R. H. Jhaveri, B. Prabadevi, W. Wang, et al., Gpt (generative pre-trained transformer)a comprehensive review on enabling technologies, potential applications, emerging challenges, and future directions, IEEE Access (2024). [63] M. Johnsen, Large Language Models (LLMs), Maria Johnsen, 2024. [64] J. Kaddour, J. Harris, M. Mozes, H. Bradley, R. Raileanu, R. McHardy, Challenges and applications of large language models, arXiv preprint arXiv:2307.10169 (2023). [65] S. Maity, A. Deroy, Human-centric explainable ai in education, 2024. URL: https://arxiv.org/abs/ 2410.19822. arXiv:2410.19822."
        }
    ],
    "affiliations": [
        "IIT Kharagpur, Kharagpur, India"
    ]
}