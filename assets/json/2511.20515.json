{
    "paper_title": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs",
    "authors": [
        "Kuniaki Saito",
        "Risa Shinoda",
        "Shohei Tanaka",
        "Tosho Hirasawa",
        "Fumio Okura",
        "Yoshitaka Ushiku"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Assessing image-text alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, a benchmark that provides a new indicator of image-text alignment by evaluating detailed image-caption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking a wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/."
        },
        {
            "title": "Start",
            "content": "AlignBench: Benchmarking Fine-Grained Image-Text Alignment with Synthetic Image-Caption Pairs Kuniaki Saito1*, Risa Shinoda2*, Shohei Tanaka1, Tosho Hirasawa1, Fumio Okura2, Yoshitaka Ushiku1 1OMRON SINIC Corporation 2The University of Osaka 5 2 0 2 ] . [ 3 5 1 5 0 2 . 1 1 5 2 : r Figure 1. We introduce novel benchmark, AlignBench, which evaluates the VLMs ability for text-image alignment. We employ state-ofthe-art Image-to-Text and Text-to-Image models to create synthetic image-caption pairs with or without subtle hallucinations. Misaligned words are highlighted in red. Using this dataset, we benchmark diverse VLMs to assess their ability to understand the alignment of imagesentence pairs. We find that subtle hallucinations generated by multimodal models can be hard to detect, even by state-of-the-art VLMs."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Assessing imagetext alignment models such as CLIP is crucial for bridging visual and linguistic representations. Yet existing benchmarks rely on rule-based perturbations or short captions, limiting their ability to measure fine-grained alignment. We introduce AlignBench, benchmark that provides new indicator of imagetext alignment by evaluating detailed imagecaption pairs generated by diverse image-to-text and text-to-image models. Each sentence is annotated for correctness, enabling direct assessment of VLMs as alignment evaluators. Benchmarking wide range of decoder-based VLMs reveals three key findings: (i) CLIP-based models, even those tailored for compositional reasoning, remain nearly blind; (ii) detectors systematically over-score early sentences; and (iii) they show strong self-preference, favoring their own outputs and harming detection performance. Our project page will be available at https://dahlian00.github.io/AlignBench/. *Equal contribution. Kuniaki serves as the project lead, while Risa is responsible for dataset construction.kuniaki.saito@sinicx.com, shinoda.risa@ist.osaka-u.ac.jp. Image-text alignment models, such as CLIP [42], play an essential role in bridging visual and linguistic representations. These models have the potential to enhance the performance on diverse downstream tasks, including classification [64], captioning [4], dataset curation [28], and even in 3D scene understanding [9]. While recent studies suggest that CLIP has limited ability to measure fine-grained alignment between images and texts [22, 40, 48, 61], large vision-language models (VLMs) are expected as promising alternatives since they exhibit strong multimodal reasoning and generation capabilities [7, 29, 32, 33, 53]. As the importance of VLMs increases, evaluating the ability of image-text alignment models becomes crucial. However, existing benchmarks [22, 61] for evaluating image-text alignment often rely on rule-based caption perturbations, such as replacing words with semantically related or unrelated terms or simple captions [48], which are not suitable for evaluating fine-grained alignment. As foundation for the future development of imagetext alignment models, we propose new benchmark, AlignBench, designed to evaluate fine-grained imagetext alignment. Unlike existing benchmarks relying on 1 Figure 2. AlignBench spans diverse Image2Text (i.e., Captioner) and Text2Image models, diverse image domains, and provides highquality annotations enriched with hallucination-type labels for deep analysis. The rightmost figure presents the example of annotations. We first conduct sentence-level correctness annotation and further annotate the segment of hallucination and its type label. rule-based perturbations, we provide annotations for image-captions pairs generated by VLM Captioners1 and Text2Image models, indicating whether the text correctly describes the image content, and assess VLM Evaluator models to measure their image-text alignment performance, as illustrated in Fig. 1. Using VLM-generated captions as evaluation targets offers two major advantages. First, VLM-generated captions can serve as hard-negatives for evaluators. While modern captioners can capture coarse visual content effectively, they often struggle to express fine-grained details in language, producing hallucinationstextual content inconsistent with the image. Evaluating against such mismatched image-text pairs allows us to directly assess an evaluators ability to judge challenging cases where achieving alignment is inherently difficult for VLMs. Second, evaluating VLM-generated captions enables direct assessment of models ability to filter synthetic image-caption pairs and clean noisy multimodal datasets. This task can be regarded as hallucination detection in image captions, yet we highlight an important aspect of this task, i.e., revealing the fundamental image-text alignment ability. Existing datasets for hallucination detection [10, 20, 58] are not suited for benchmarking the models ability because of the lack of model coverage and the small number of evaluation samples. Instead, as shown in Fig. 2, we aim to cover diverse range of models with sufficient samples, which is useful for comprehensive analysis. Our experiments focus on benchmarking diverse VLM evaluators on sentence-level alignment evaluation. Empirically, we observe that models performing well on AlignBench also achieve strong results on prior hallucination and compositionality benchmarks, whereas the reverse does not necessarily hold. This asymmetry indicates that AlignBench provides new and more comprehensive indicator of imagetext alignment ability. Our extensive analysis further yields several key insights. First, CLIPbased models, even the one tailored for compositional alignment, remain nearly blind. Second, evaluators tend to recognize sentences at the beginning of response as correct, regardless of their correctness. Third, they exhibit self-preference, i.e. consider their own output captions as correct, which degrades performance as detectors. This observation is consistent with prior findings [39]. 2. Related Work Benchmarks for VLMs. Many benchmarks evaluate the reasoning ability of VLMs [18, 19, 26, 49, 59, 60] or expert knowledge [34]. We focus on assessing their fundamental capability to understand the fine-grained image-text alignment. Tong et al. [50] evaluate fine-grained visual comprehension using CLIP-blind image pairs and related questions; while we propose new task for fine-grained benchmark, i.e., AlignBench provides the task of detecting the actual errors in captions generated by advanced VLMs. Image-caption alignment datasets. Winoground [48], SUGARCREPE [22], and ARO [61] assess CLIPs finegrained imagetext alignment ability. All focus on compositional structure in short sentences, and SUGARCREPE and ARO rely on rule-based perturbations. In contrast, AlignBench uses VLM-generated captions that enable evaluation on more complex and natural texts. This allows us to directly assess models on cases where VLMs themselves tend to fail in understanding, which is shown to be more challenging as shown in Sec. 4.2. SeeTRUE [58] uses AIgenerated imagecaption pairs, but its captions are short and simple, making it unsuitable for fine-grained alignment evaluation. GenAI-Bench [31] benchmarks scoring metrics by ranking images generated from the same prompt, relying on concise captions and relative ranking rather than binary alignment labels. Some datasets are introduced for hallucination detection in image captions [10, 20, 51, 52]2, but they are limited as VLM benchmarks, often lacking diverse captioners or sufficient samples per model. Our benchmark, AlignBench, addresses these gaps by (i) covering more responses, (ii) balancing data across diverse models, and (iii) incorporating text-to-image outputs. We provide more detailed comparison in Table of the appendix. 1We refer to VLM used for caption generation as Captioner, and to one used for assessing image-text alignment as an Evaluator or Detector. 2ZINA [51] is concurrent with ours, and details were unavailable at the time of submission; we compare as best we can. Figure 3. Examples of hallucinated sentences in AlignBench. The hallucinated portions are often subtle, requiring fine-grained imagetext alignment ability to detect them. Hallucination detection and mitigation in image captioning. Hallucination detection in image captioning has been widely studied [43]. CHAIR [43] was the first metric to evaluate image-caption alignment at the object level using an object detector. However, its effectiveness is limited due to the detectors coverage and accuracy. Besides, many works attempt to mitigate the hallucinations in image captions [15, 16, 25, 45, 56, 63, 65, 66], especially, mitigating hallucinations in long captions is important as they are prone to contain more hallucinations [21, 65], which we confirm in Sec. 3.3. Refining captioning model based on image-caption alignment score computed by VLMs is promising approach [13], and our work also contributes to this line of work. Despite these methodological developments and the use of VLMs as detector, VLMs fundamental image-text alignment ability remains unclear. In our experiments, we focus on benchmarking diverse VLMs and clarifying their ability to align images and text. 3. Datasets We aim to collect datasets that cover diverse image-caption pairs equipped with high-quality annotations of semantic alignment. This section first explains how we collect imagecaption pairs and provide annotations to them, followed by an analysis of the dataset. We focus on obtaining labels for sentence-level semantic alignment for two reasons: (i) sentence-level labels give cue to easily find more finedetailed locations of unaligned descriptions, and (ii) spanlevel annotation suffers more from the subjectivity of annotation than sentence-level. For deeper analysis, we additionally provide span-level hallucination presence labels and categorize the types of hallucinations. Due to the limited space, we leave most details in the appendix. 3.1. Collecting Image-Caption Pairs We aim to assess the ability to judge whether given textimage pair is semantically aligned. Thus, coverage of diverse image domains and caption patterns is essential to building benchmark. We thus obtain image-caption pairs using six image2text and two text2image models, where each caption consists of multiple consecutive sentences. Image-to-text models (captioner). We employ CC12M [6] and the validation split of COCO 2017 [30] as image inputs. To ensure the diversity of the test images, we cluster images into 50 clusters and pick 40 images from each cluster, resulting in 2000 images in total. We manually categorize each cluster to enable interpretable analysis. We employ GPT-4o, ShareGPT (S-GPT) [8], LLaVA-NeXT [27], Llama-4 [35], Qwen 2 [53], and CogVLM [54], covering diverse architectures, scales, and openness. This diversity enables the collection of captions with differing levels of detail and language style. For each captioner-image pair, we apply chat template (e.g., Describe the image in detail.), yielding 12K outputs in total. Text-to-image models. To ensure the diversity in text prompt, we first pick 170 common object categories and prompt GPT-4o-mini [37] to include at least one of the categories and generate 3-4 sentences per prompt, resulting in 1000 prompts. To convert the prompts into images, we utilize Stable Diffusion 3.5 (SD) [44] as an open-source model and image generation model accessible through GPT-4omini (GPT-Gen), yielding 2K outputs in total. 3.2. Annotation In the main paper, we focus on how to obtain labels for sentence-level semantic alignment and leave the finedetailed annotation process in the appendix. We ask annotators to determine if the image presents all the details described in the text correctly. The use of SOTA models as captioners makes the annotation non-trivial because hallucinations produced by such models are often subtle and not immediately apparent at first glance, as shown in Fig. 3. Annotation labels. We are inspired by the labeling scheme of [20], where annotators assign one of three categories: correct, incorrect, or unknown. An example of an annotation screen is illustrated on the right of Fig. 2. sentence is labeled correct if it accurately describes the image, and incorrect if it contains part that does not correctly describe the image. When correctness cannot be determinedfor example, if the object is too small to recognize or if the description involves non-visible attributes such as smell or windit is labeled unknown. The unknown category is in3 Table 1. Stats of AlignBench in sentence-level correctness annotations. AlignBench contains large number of annotated sentences, enough for benchmarking models. We exclude sentences with unknown label. Image-to-Caption (Captioners) Text-to-Image CogVLM GPT4o ShareGPT (S-GPT) Llama-4 LLaVA-NeXT Qwen-2 Stable Diffusion (SD) gpt-image-1 (GPT-Gen)"
        },
        {
            "title": "Sentences",
            "content": "Sentences / Image Positive / Total (%) Word / Sentence 7372 3.7 91. 15.9 12790 17610 14800 15170 6.4 91.8 14.7 8.8 73.4 18. 7.4 85.2 19.4 6.9 80.9 17. 7.9 88.8 17.3 2417 2.4 34. 22.9 3524 3.5 79.7 22.5 6.3 82.7 17.7 Figure 4. Left: Ratio of incorrect sentences by position; all captioners make fewer errors at the first position. Different colors indicate different positions. Right: Number of unaligned sentences per category; most mistakes occur in attributes and text. Table 2. Comparison with existing image-text alignment datasets. AlignBench enables evaluation on longer sentences and broader vocabulary, using outputs of diverse VLMs."
        },
        {
            "title": "Dataset",
            "content": "# Sentences Words/Sent. Vocab Sent. Generation Foil [41] HAT [41] ARO [61] Winoground [48] SugarCrepe [22] SeeTrue [58] GenAI-Bench [31] MHalDetect [20] AlignBench 5k 0.4k 2k 1.6k 2k 6.9k 1.6k 14k 89k 11.8 13.6 7.1 9.0 11.1 11.5 12.6 18.0 17.7 4.1k 1.2k 0.6k 0.9k 2.2k 1.5k 4.3k 4.4k 42k Rule-base Rule-base Rule-base Human Language Model T2I model T2I model Captioner Captioner + T2I troduced to exclude unreliable cases from evaluation. Annotation process. To ensure high-quality annotations, we adopt two-stage process for sentence-level annotation: (i) crowd-sourced workers annotate each sentence, and (ii) we review the merged outcomes to guarantee quality. In the first stage, five independent workers annotate each sentence, reducing the risk of missing hallucinations. Moreover, their performance is continuously monitored through regular checks. The results are then merged based on majority voting, as detailed in the appendix, and subsequently reviewed. During the review, to minimize the inclusion of ambiguous cases in the evaluation, sentences that are difficult to judge are labeled as unknown. This process creates the dataset with sentence-level correctness annotations. We additionally provide annotator agreement scores as an indicator of task difficulty. We further annotate this dataset to provide segment-level hallucination presence labels and hallucination category labels, where hallucinations are categorized into eight types: Attribute, Object, Number, Location, Illusion, Direction, Text, Relation, and Other. These categories should reveal the weakness of the current VLMs in understanding the image content. 3.3. Analysis of the Dataset Examples of annotated sentences. Figure 3 presents unaligned image-sentence pairs. Captioners errors often involve visual details or object relationships rather than clear mistakes, making them hard to detect. Therefore, detectors need fine-grained understanding of image-text alignment. Basic stats. Table 1 summarizes the statistics of about 90K annotated correct or incorrect sentences. AlignBench provides correct and incorrect sentences enough for evaluation (excluding unknown cases). GPT-4o achieves the highest accuracy (91.8%) while Qwen (88.8%) and CogVLM (91.5%) perform comparably to GPT-4o among open-source VLMs. This suggests that large number of outputs are needed to obtain sufficient incorrect samples for advanced captioners. Comparison to existing image-text alignment datasets. Table 2 compares with existing image-text alignment datasets. AlignBench substantially surpasses prior datasets in scale, with over 89K sentencesone to two orders of It magnitude larger than existing alignment benchmarks. contains longer, image-caption pairs generated by captioner and T2I models, with far richer vocabulary, enabling evaluation on diverse and long-tail concepts. Positions of the hallucinations. The left of Fig. 4 computes the ratio of incorrect sentences in each position of the sentence. Across models, incorrect sentences appear most frequently in the second to fourth positions. This observation is consistent with previous work [21, 65]. The first sentence is less likely to contain hallucinations, likely because it often provides an overall image summary and language tokens attend to image features well. 4 Table 3. AUROC results across VLMs. Cells with the best performance within open-source and closed-source groups are highlighted in blue background, while the best model within each model family is marked in bold."
        },
        {
            "title": "Params",
            "content": "Image-to-Caption Models Text-to-Image Models S-GPT Llava Qwen-2 GPT4o CogVLM Llama-4 (109B) SD GPT-Gen Open-Source Models TripletCLIP SigLIP BLIP-2 Phi-4 QwenDeepseek-VL2 LLaVA-NeXT Pixtral-12B Gemma-3 [40] [62] [29] [1] [53] [57] [27] [2] [47] InternVL2 [12] InternVL2.5 Qwen-2.5 Llama- [11] [3] [35] Closed Models Gemini-2.0 Flash [46] GPT4o-mini GPT4o GPT4.1-mini GPT5-mini GPT5 [37] 0.3B 0.2B 1.2B 5.6B 7B 27B 72B 12B 12B 27B 2B 8B 26B 40B 78B 7B 32B 109B 400B N/A N/A N/A N/A N/A N/A 50.7 51.7 53.4 54.2 60. 58.0 59.4 64.3 67.0 67.6 55. 66.6 63.9 69.3 74.1 68.6 73. 80.7 81.1 76.8 69.9 75.8 77. 81.5 85.4 51.9 52.0 55.9 53. 55.3 56.6 56.6 60.8 63.3 64. 56.0 65.7 60.6 63.0 70.3 65. 71.6 78.6 53.9 53.3 52.4 52. 55.9 56.9 58.9 60.6 63.1 67. 58.4 69.1 61.2 66.5 73.7 66. 70.6 77.6 53.9 52.8 52.5 51. 46.0 54.4 56.7 57.1 57.6 61. 55.7 63.9 57.1 59.6 63.9 55. 66.1 67.5 51.0 53.1 52.1 51. 51.9 54.2 57.5 57.3 59.4 66. 60.3 67.6 58.9 61.9 68.1 64. 69.0 77.2 80.9 79.0 71.9 81. 73.5 65.7 72.6 75.8 82.2 86. 74.9 68.2 72.8 74.4 80.2 85. 65.5 60.0 58.2 65.8 69.7 72. 70.4 62.3 69.7 69.2 81.1 85. 50.9 54.9 52.2 50.8 53.2 53. 54.9 55.7 54.0 60.6 56.1 60. 55.6 61.5 63.1 61.0 66.0 59. 64.7 64.8 60.3 63.8 66.0 73. 78.4 48.2 50.7 48.8 52.6 54. 56.1 59.0 64.7 64.1 63.7 52. 64.3 53.1 69.5 72.0 65.4 68. 81.1 83.0 73.4 56.5 63.2 68. 83.8 84.9 Avg. 50.7 52.9 51. 52.1 52.9 55.1 57.0 59.6 59. 62.8 55.3 63.8 56.8 63.4 67. 62.9 68.4 73.4 44.9 55.2 42. 50.9 46.0 50.5 53.3 56.7 52. 50.5 48.5 52.5 43.8 56.5 55. 55.7 61.0 64.7 67.8 76.2 57. 47.7 52.4 56.1 65.7 72.0 69. 61.1 66.1 69.2 77.2 81.2 Hallucination categories. We show the stats of hallucination types on the right of Fig. 4. Errors in attributes are the leading category for many models, indicating that adjectival descriptions (e.g., color, texture) are prone to hallucination as indicated by Fig. 1 and 3. Also, many models tend to cause errors in text, probably because small texts are hard to read even with advanced models. Difference between Image-to-Text and Text-to-Image models. We observe that T2I models, such as GPT-Gen, frequently make errors related to Direction, e.g., the orientation of an eye or an object. This is likely because our text prompts often contain directional descriptions, which T2I models struggle to accurately represent. 4. Experiments We aim to benchmark and analyze diverse VLMs on AlignBench to uncover key factors for building performant image-text alignment model. After describing the experimental setup, we first present an overview of In the empirical results, followed by detailed analysis. summary, our analysis reveals five key findings: (i) AlignBench spans diverse difficulty levels, enabling systematic and interpretable evaluation; (ii) AlignBench serves as new indicator of imagetext alignmentmodels strong on AlignBench also perform well on prior benchmarks, while the reverse does not necessarily hold; (iii) CLIP-like models struggle to align pairs generated by modern captioners and T2I models; (iv) VLMs over-score early sentences regardless of correctness; and (v) evaluators show strong self-preference, consistently favoring their own outputs. i.e., Setups. We aim to benchmark diverse VLMs in sentencelevel image-text alignment, the image presents all the details described in the single sentence correctly. Specifically, each sentence and image is independently fed into VLMs. We choose this evaluation protocol since the prior work on hallucination detection [36] and image-text alignment [22, 48, 58] also employs sentence-level evaluation. Following [5], we prompt the decoder-based VLMs to output the score of the alignment between the image and an input sentence, ranging from 0 to 100, as shown in the appendix. We also include BLIP-2 [29], TripletCLIP [40], and SigLIP [62] as fundamental image-text alignment models. Given the alignment score, we compute the AUROC within each captioner, which enables threshold-free evaluation, 5 Figure 5. Examples of incorrect sentences with detectors correctness scores. Higher scores indicate greater confidence in correctness. Detectors are prone to being overconfident in these examples. We highlight detectors errors in red within the text and mark the grounded incorrect regions in the image with orange boxes. and the random prediction results in score of 50. 4.1. Overview of Results Table 3 presents the results evaluated on diverse VLMs. Samples of evaluators outputs are available in Fig. 5. AlignBench covers diverse levels of hallucination detection. We see variations in the performance across the tested VLMs and caption models. Thus, AlignBench is suitable to quantify VLMs as an image-text alignment model. CLIP-based models tailored for compositional alignment remain nearly blind. TripletCLIP [40], despite being trained for compositional understanding, achieves an AUROC of only around 50, indicating that it fails to distinguish correct from incorrect sentences. The same trend holds for both SigLIP and BLIP-2. Best model. On average, GPT-5 shows the best performance of all models, while Llama-4, the best open-source model, performs on par with GPT-5-mini. Llama-4 outperforms many private models with large margin. Its activated parameters during inference are only 17B. When considering the balance of inference time and accuracy, Llama4 is the best in open-source models. Examples of evaluators outputs. Figure 5 illustrates that even advanced evaluators can misinterpret the visual content, despite the captioners errors not being particularly subtle, as in the bottom-left example. Advanced captioners produces hard-to-detect errors. Hallucinations from GPT-4o, GPT-Gen, and Llama-4 are difficult to detect, even for proprietary models, as shown by their low scores. Since SOTA models like GPT-4o and Llama-4 accurately understand many scenes, their errors might be subtle and harder to identify. GPT-Gens hallucinations often involve eye direction or fine visual details, which are also challenging. Increasing the model size improves performance. Within the same model family, larger language models tend to yield better performance, probably because the task requires interpreting diverse captions. Robustness to text-to-image models differs by VLMs. Models such as Llama-4 and GPT-5-mini show the highest performance in SD across captioners, indicating that SD is the easiest split for these models. By contrast, for Gemma-3 (27B) and Qwen-2.5 (32B), the performance on SD is lower than S-GPT and Qwen-2. The difference should be due to the domains of images and the difference in language patterns. Inclusion of diverse data helps to find such trends. 4.2. Detailed Analysis Hallucinations generated by better captioners are harder to detect. The left of Fig. 6 plots captioner performance on MMMU (x-axis) against AUROC measured by GPT-5-mini (y-axis). Captioners with higher MMMU scores tend to yield lower AUROC, indicating that stronger captioners generate hallucinations that are harder to detect. The performance on AlignBench is highly correlated with that on MMMU. The right of Fig. 6 plots the performance on MMMU (x-axis) and AlignBench (y-axis), and indicates that models effective on MMMU perform well on AlignBench and vice versa. Evaluators favor the sentence near the beginning of the caption. Figure 7 computes the detectors output scores averaged within each sentence position. For both correct and incorrect image-text pairs, the detectors give higher score to the sentences located near the beginning of the caption. The first sentence often provides the overview of the image, and VLMs seem to prefer such sentences irrespective of their correctness, possibly because such sentences are abundant in training datasets. AlignBench serves as new indicator of an image-text alignment capability. Table 4 compares AlignBench with existing hallucination-detection and VL-compositionality datasets. Although Gemma-3 slightly outperforms Llama4 on Winoground, FOIL, and HAT, it underperforms by large margin on many of the AlignBench tasks. This suggests that strong performance on short and simple sentences does not translate to the longer and more diverse 6 Figure 6. The size of plots indicates the parameter size. Left: MMMU performance measured on Captioners (X-axis) vs. AUROC measured by GPT-5-mini (Y-axis) for each Captioner. Advanced Captioners tend to produce errors that are difficult to detect. Right: MMMU (X-axis) vs. AUROC (Y-axis) for each detector. Detectors with better MMMU performance tend to perform better on AlignBench. Figure 7. Detectors show positional bias in scoring. We average the detectors correctness scores (Y-axis) by sentence position (X-axis) and visualize the results using GPT-4o (Left) and Llama-4 (Right) as detectors. Both detectors assign higher scores to sentences appearing near the beginning of the output. The detector is not provided with any positional information during inference. Table 4. Comparison to existing datasets for hallucination detection and compositionality understanding (AUROC). Results of prior benchmarks that exceed AlignBench are highlighted. Excelling on AlignBench indicates strong results on prior benchmarks. Detector Params Qwen-2.5 Qwen-2.5 Gemma-3 7B 32B 27B Llama-4 109B GPT-5-mini - AlignBench Hallucination Detection VL-Compositionality ShareGPT Llava GPT4o Llama-4 CogVLM MHalDetect Foil HAT ARO SugarCrepe Winoground SeeTrue 68.6 73.6 67.6 80. 81.5 65.3 71.6 64.4 78.7 82. 55.7 66.1 61.1 67.8 69.7 61. 66.0 60.6 59.9 73.0 64.6 69. 66.4 77.2 81.1 78.7 83.0 81. 82.8 85.4 85.5 89.6 91.8 90. 94.6 68.0 77.6 76.6 76.3 84. 78.3 78.3 79.2 84.8 92.4 84. 84.9 87.6 89.4 92.4 63.3 73. 77.6 77.1 90.8 74.6 81.5 81. 83.1 87.2 sentences in AlignBench. Conversely, Llama-4, which performs well on AlignBench, consistently shows strong results on these prior benchmarks, indicating that AlignBench better captures the broader alignment capabilities required across datasets. While detectors perform well on FOILindicating these hallucinations are easy for current SOTA VLMsthey perform poorly on Winoground and HAT, showing that even top models struggle with complex object compositions. Detectors struggle to detect their own hallucinations. Table 3 shows that Llama-4 (109B) and GPT-4o perform poorly on their own outputs (highlighted by underline). This finding aligns with prior work reporting LLM evaluators favor their own outputs [39]. We annotate captions generated by Qwen-2.5 (32B) and Gemma-3 (27B) to enable more extensive selfand cross-evaluations. Figure 8 (left) confirms much lower AUROC on self-generated captions (diagonal elements). Figure 8 (right) shows that GPT-4o scores its own incorrect sentences higher than those of Llama-4, and the gap between incorrect and correct scores is small in its own output, which is causing the performance degradation. Detectors are poor at detecting Direction and Number hallucinations. Figure 9 assesses detectors robustness across hallucination categories using their output scores (lower is better since only hallucinated sentences are accounted). Direction errors occur when object orientation is misdescribed; identifying the errors requires finegrained visual understanding, and detectors consistently perform poorly. Number errors arise from incorrect object countsan issue long recognized in early VLMs like CLIP [38] and still evident in advanced models. 7 Table 5. Hallucinated segment localization results. Each cell shows AP / mIoU (%). Localizing hallucinated segments remains difficult even for performant models."
        },
        {
            "title": "Params",
            "content": "Qwen-2.5 GPT-4o mini Llama-4 Llama-4 32B - 109B 400B Image-to-Caption Models Text-to-Image Models Avg. S-GPT"
        },
        {
            "title": "Llava",
            "content": "Qwen-2 GPT4o CogVLM Llama-4 SD GPT-Gen 17.3 / 13. 22.4 / 15.1 14.0 / 11.7 20.8 / 15.1 22.5 / 16.0 12.6 / 10.7 15.9 / 11. 12.1 / 9.4 17.2 / 12.9 25.2 / 21.6 28.9 / 22.4 20.6 / 18.3 29.5 / 23. 28.0 / 21.5 18.7 / 16.4 14.7 / 12.5 14.6 / 11.9 22.5 / 18.5 24.9 / 22. 27.0 / 20.8 24.8 / 22.7 34.3 / 26.4 29.3 / 23.2 19.6 / 17.3 15.1 / 10. 11.0 / 9.0 23.3 / 19.1 28.2 / 24.8 29.1 / 22.1 26.2 / 23.3 34.0 / 26. 29.8 / 21.7 19.4 / 18.0 15.4 / 11.9 12.0 / 9.3 24.2 / 19.6 Table 6. Ensembling detectors outputs improves performance in almost all cases. The increase or decrease from the better model used for ensembling is highlighted next to each score. Detector 1 Detector 2 Image-to-Caption Models Text-to-Image Models S-GPT Llava Qwen-2 GPT4o CogVLM Llama-4 SD GPT-Gen Qwen-2.5 (7B) Gemma-3 (27B) 71.9 (+3.3) 68.4 (+4.0) 71.3 (+3.4) 64.6 (+3.5) 69.3 (+2.9) 63.3 (+2.3) 67.6 (+2.2) 54.8 (0.9) Llama-4 (109B) LLama-4 (400B) 84.6 (+3.5) 83.4 (+2.4) 82.9 (+3.9) 74.0 (+2.1) 83.5 (+2.2) 65.8 (+1.1) 84.6 (+1.6) 69.1 (+1.2) Llama-4 (109B) GPT5-mini 86.0 (+4.5) 84.8 (+2.7) 83.6 (+3.4) 73.9 (+4.2) 84.4 (+3.2) 72.3 (-0.7) 85.2 (+1.4) 68.6 (+2.9) span, explicitly noting that one exists. Performance is measured by alignment with human annotations (see Appendix for prompts and metrics). As shown in Table 5, Llama-4 (400B), the best model, localizes only 24.2% of hallucinated segments on average, underscoring substantial room for improvement. Notably, GPT-4o mini outperforms Qwen-2.5 (32B), in contrast to Table 3, indicating that strong sentence-level detectors are not always effective for segment-level localization. Ensembling improves performance. We study whether ensembling improves detection. We average alignment scores from two comparably strong models  (Table 3)  and observe consistent gains  (Table 6)  . This suggests that models apply distinct criteria for image-caption alignment, and combining them enhances performance. drop occurs for ensembling Llama-4 and GPT-5-mini on Llama-4 captions, likely due to the large performance gap between the two models. Contents in the appendix. Refer to the appendix for more empirical results. We give their overview below. 1. We provide comparison with the prior approach in hallucination detection in image captioning, indicating that VLM-based detectors can surpass the prior one with large margin. 2. The chain-of-thought and self-ensembling are effective in improving the performance of detectors. 3. More visualizations of annotation and detectors outputs. 4. Detailed stats of AlignBench. Figure 8. Detectors struggle to detect their own hallucination. Left: Selfand cross-evaluation results. AUROC scores for each Captioner (columns), normalized by the average AUROC of each Detector (rows). Diagonal entries show self-evaluation. Right: We pick GPT-4o as detector, with their output correctness scores averaged by sentence position. Blue and red lines show scores for correct and incorrect GPT-4os outputs; green shows scores for incorrect Llama-4 outputs. Figure 9. Detectors score averaged within each hallucination type. Detectors show weakness in Direction and Number. Segment-level localization has more room for improvement. AlignBench includes hallucination segments for each hallucinated sentence, enabling segment-level evaluation. We present VLMs with hallucinated sentenceimage pair and prompt them to localize the hallucinated 5. Conclusion We introduced AlignBench, benchmark for evaluating hallucination detection in image captioning across diverse models, domains, and fine-grained annotations. Our analysis shows that CLIP-like models are nearly blind to hallu8 cinations, VLMs over-score early sentences, and detectors exhibit strong self-preference. Importantly, performance on AlignBench reliably predicts performance on existing benchmarks, while the reverse does not necessarily hold, positioning AlignBench as new and more comprehensive indicator of imagetext alignment. We expect it to support the development of better alignment model. Acknowledgement. This work is supported by JST Moonshot R&D Program Grant Number JPMJMS2236. This work is supported by TSUBAME Encouragement Program for Young/Female Users of Center for Information Infrastructure at Institute of Science Tokyo and by Joint Usage/Research Center for Interdisciplinary Large-scale Information Infrastructures in Japan."
        },
        {
            "title": "References",
            "content": "[1] Abdelrahman Abouelenin, Atabak Ashfaq, Adam Atkinson, Hany Awadalla, Nguyen Bach, Jianmin Bao, Alon Benhaim, Martin Cai, Vishrav Chaudhary, Congcong Chen, et al. Phi-4-mini technical report: Compact yet powerful multimodal language models via mixture-of-loras. arXiv preprint arXiv:2503.01743, 2025. 5 [2] Pravesh Agrawal, Szymon Antoniak, Emma Bou Hanna, Baptiste Bout, Devendra Chaplot, Jessica Chudnovsky, Diogo Costa, Baudouin De Monicault, Saurabh Garg, arXiv preprint Theophile Gervet, et al. arXiv:2410.07073, 2024. 5 Pixtral 12b. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. 5 [4] Manuele Barraco, Marcella Cornia, Silvia Cascianelli, Lorenzo Baraldi, and Rita Cucchiara. The unreasonable effectiveness of clip features for image captioning: an experimental analysis. In CVPR, 2022. 1 [5] David Chan, Suzanne Petryk, Joseph Gonzalez, Trevor Darrell, and John Canny. Clair: Evaluating image captions with large language models. In EMNLP, 2023. [6] Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text preIn CVPR, training to recognize long-tail visual concepts. 2021. 3 [7] Jun Chen, Deyao Zhu, Xiaoqian Shen, Xiang Li, Zechun Liu, Pengchuan Zhang, Raghuraman Krishnamoorthi, Vikas Chandra, Yunyang Xiong, and Mohamed Elhoseiny. large language model as unified interface Minigpt-v2: arXiv preprint for vision-language multi-task learning. arXiv:2310.09478, 2023. 1 [8] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. In ECCV, pages 370387. Springer, 2024. 3 [9] Runnan Chen, Youquan Liu, Lingdong Kong, Xinge Zhu, Yuexin Ma, Yikang Li, Yuenan Hou, Yu Qiao, and Wenping Wang. Clip2scene: Towards label-efficient 3d scene understanding by clip. In CVPR, pages 70207030, 2023. 1 [10] Xiang Chen, Chenxi Wang, Yida Xue, Ningyu Zhang, Xiaoyan Yang, Qiang Li, Yue Shen, Lei Liang, Jinjie Gu, and Huajun Chen. Unified hallucination detection for multiIn ACL, 2024. 2, 16, 19, modal large language models. 20 [11] Zhe Chen, Weiyun Wang, Yue Cao, Yangzhou Liu, Zhangwei Gao, Erfei Cui, Jinguo Zhu, Shenglong Ye, Hao Tian, Zhaoyang Liu, et al. Expanding performance boundaries of open-source multimodal models with model, data, and testtime scaling. arXiv preprint arXiv:2412.05271, 2024. 5 [12] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. 5 [13] Ailin Deng, Zhirui Chen, and Bryan Hooi. Seeing is believing: Mitigating hallucination in large visionlanguage models via clip-guided decoding. arXiv preprint arXiv:2402.15300, 2024. 3 [14] Antonio Farinhas, Jose GC de Souza, and Andre FT Martins. An empirical study of translation hypothesis ensembling with large language models. In EMNLP, 2023. 19 [15] Sebastian Farquhar, Jannik Kossen, Lorenz Kuhn, and Yarin Gal. Detecting hallucinations in large language models using semantic entropy. Nature, 630(8017):625630, 2024. 3 [16] Alessandro Favero, Luca Zancato, Matthew Trager, Siddharth Choudhary, Pramuditha Perera, Alessandro Achille, Ashwin Swaminathan, and Stefano Soatto. Multi-modal hallucination control by visual information grounding. In CVPR, 2024. 3 [17] Joseph Fleiss. Measuring nominal scale agreement among many raters. Psychological Bulletin, 76:378382, 1971. 13 [18] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. 2 [19] Tianrui Guan, Fuxiao Liu, Xiyang Wu, Ruiqi Xian, Zongxia Li, Xiaoyu Liu, Xijun Wang, Lichang Chen, Furong Huang, Yaser Yacoob, et al. Hallusionbench: an advanced diagnostic suite for entangled language hallucination and visual illusion In CVPR, pages 14375 in large vision-language models. 14385, 2024. 2 [20] Anisha Gunjal, Jihan Yin, and Erhan Bas. Detecting and preventing hallucinations in large vision language models. In AAAI, 2024. 2, 3, 4, 16 [21] Yusuke Hirota, Boyi Li, Ryo Hachiuma, Yueh-Hua Wu, Boris Ivanovic, Yuta Nakashima, Marco Pavone, Yejin Choi, Yu-Chiang Frank Wang, and Chao-Han Huck Yang. Lotus: leaderboard for detailed image captioning from quality to societal bias and user preferences. In ACL, 2025. 3, 4 [22] Cheng-Yu Hsieh, Jieyu Zhang, Zixian Ma, Aniruddha Kembhavi, and Ranjay Krishna. Sugarcrepe: Fixing hackable benchmarks for vision-language compositionality. NeurIPS, 2023. 1, 2, 4, 5, [23] Hugging Face. Hugging face. 12 9 [24] Dongfu Jiang, Xiang Ren, and Bill Yuchen Lin. Llmblender: Ensembling large language models with pairwise ranking and generative fusion. In ACL, 2023. 19 [25] Sicong Leng, Hang Zhang, Guanzheng Chen, Xin Li, Shijian Lu, Chunyan Miao, and Lidong Bing. Mitigating object hallucinations in large vision-language models through visual contrastive decoding. In CVPR, 2024. 3 [26] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In CVPR, pages 1329913308, 2024. 2 [27] Bo Li, Kaichen Zhang, Hao Zhang, Dong Guo, Renrui Zhang, Feng Li, Yuanhan Zhang, Ziwei Liu, and Chunyuan Li. Llava-next: Stronger llms supercharge multimodal capabilities in the wild, 2024. 3, [28] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for unified In ICML. vision-language understanding and generation. PMLR, 2022. 1 [29] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In ICML. PMLR, 2023. 1, 5 [30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740755, 2014. 3 [31] Zhiqiu Lin, Deepak Pathak, Baiqi Li, Jiayao Li, Xide Xia, Graham Neubig, Pengchuan Zhang, and Deva Ramanan. Evaluating text-to-visual generation with image-to-text generation. In ECCV, 2024. 2, 4, 16 [32] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. NeurIPS, 36:3489234916, 2023. 1 [33] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, 2024. [34] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. 2 [35] Meta.AI. The llama 4 herd: The beginning of new era of natively multimodal ai innovation, 2025. 3, 5 [36] Abhika Mishra, Akari Asai, Vidhisha Balachandran, Yizhong Wang, Graham Neubig, Yulia Tsvetkov, and Hannaneh Hajishirzi. Fine-grained hallucination detection and editing for language models. In COLM, 2024. 5, 12 [37] OpenAI. ChatGPT. https://chat.openai.com/ chat, 2023. 3, [38] Roni Paiss, Ariel Ephrat, Omer Tov, Shiran Zada, Inbar Mosseri, Michal Irani, and Tali Dekel. Teaching clip to count to ten. In ICCV, pages 31703180, 2023. 7 [40] Maitreya Patel, Naga Sai Abhiram Kusumba, Sheng Cheng, Changhoon Kim, Tejas Gokhale, Chitta Baral, et al. Tripletclip: Improving compositional reasoning of clip via synthetic vision-language negatives. NeurIPS, 2024. 1, 5, 6 [41] Suzanne Petryk, David Chan, Anish Kachinthaya, Haodi Zou, John Canny, Joseph Gonzalez, and Trevor Darrell. Aloha: new measure for hallucination in captioning models. In ACL, 2024. 4, 16 [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 1 [43] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. Object hallucination in image captioning. In EMNLP, 2018. 3 [44] Stability AI. Stable https://stability.ai/news/introducing-stable-diffusion-3-5, 2024. 3 diffusion 3.5 large. [45] Wei Suo, Lijun Zhang, Mengyang Sun, Lin Yuanbo Wu, Peng Wang, and Yanning Zhang. Octopus: Alleviating hallucination via dynamic contrastive decoding. In CVPR, 2025. 3 [46] Gemini team. Gemini 2.0 flash, 2024. 5 [47] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, arXiv preprint et al. arXiv:2503.19786, 2025. Gemma 3 technical report. [48] Tristan Thrush, Ryan Jiang, Max Bartolo, Amanpreet Singh, Adina Williams, Douwe Kiela, and Candace Ross. Winoground: Probing vision and language models for visiolinguistic compositionality. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 52385248, 2022. 1, 2, 4, 5, 16 [49] Peter Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, IYER, Sai Charitha Akula, Adithya Jairam Vedagiri Shusheng Yang, Jihan Yang, Manoj Middepogu, Ziteng Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. NeurIPS, 37:8731087356, 2024. 2 [50] Shengbang Tong, Zhuang Liu, Yuexiang Zhai, Yi Ma, Yann LeCun, and Saining Xie. Eyes wide shut? exploring the In CVPR, pages visual shortcomings of multimodal llms. 95689578, 2024. 2 [51] Yuiga Wada, Kazuki Matsuda, Komei Sugiura, and Graham Neubig. Zina: Multimodal fine-grained hallucination detection and editing. arXiv preprint arXiv:2506.13130, 2025. 2, [52] Junyang Wang, Yiyang Zhou, Guohai Xu, Pengcheng Shi, Chenlin Zhao, Haiyang Xu, Qinghao Ye, Ming Yan, Ji Zhang, Jihua Zhu, et al. Evaluation and analysis of hallucination in large vision-language models. arXiv preprint arXiv:2308.15126, 2023. 2, 16 [39] Arjun Panickssery, Samuel Bowman, and Shi Feng. Llm their own generations. evaluators recognize and favor NeurIPS, 2024. 2, 7 [53] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models 10 perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 1, 3, [54] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Song XiXuan, et al. Cogvlm: Visual expert for pretrained language models. NeurIPS, 2024. 3 [55] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. In NeurIPS, 2022. 19 [56] Sangmin Woo, Donguk Kim, Jaehyuk Jang, Yubin Choi, and Changick Kim. Dont miss the forest for the trees: Attentional vision calibration for large vision language models. In ACL Findings, 2025. 3 [57] Zhiyu Wu, Xiaokang Chen, Zizheng Pan, Xingchao Liu, Wen Liu, Damai Dai, Huazuo Gao, Yiyang Ma, Chengyue Wu, Bingxuan Wang, Zhenda Xie, Yu Wu, Kai Hu, Jiawei Wang, Yaofeng Sun, Yukun Li, Yishi Piao, Kang Guan, Aixin Liu, Xin Xie, Yuxiang You, Kai Dong, Xingkai Yu, Haowei Zhang, Liang Zhao, Yisong Wang, and Chong Ruan. Deepseek-vl2: Mixture-of-experts vision-language models arXiv preprint for advanced multimodal understanding. arXiv:2412.10302, 2024. 5 [58] Michal Yarom, Yonatan Bitton, Soravit Changpinyo, Roee Aharoni, Jonathan Herzig, Oran Lang, Eran Ofek, and Idan Szpektor. What you see is what you read? improving textimage alignment evaluation. NeurIPS, 2023. 2, 4, 5, 16 [59] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In CVPR, pages 95569567, 2024. 2 [60] Xiang Yue, Tianyu Zheng, Yuansheng Ni, Yubo Wang, Kai Zhang, Shengbang Tong, Yuxuan Sun, Botao Yu, Ge Zhang, Huan Sun, et al. Mmmu-pro: more robust multi-discipline multimodal understanding benchmark. In ACL, 2025. 2 [61] Mert Yuksekgonul, Federico Bianchi, Pratyusha Kalluri, Dan Jurafsky, and James Zou. When and why visionlanguage models behave like bags-of-words, and what to do about it? In ICLR, 2023. 1, 2, 4, [62] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In ICCV, 2023. 5 [63] Ruiyang Zhang, Hu Zhang, and Zhedong Zheng. Vluncertainty: Detecting hallucination in large visionlanguage model via uncertainty estimation. arXiv preprint arXiv:2411.11919, 2024. 3 [64] Kaiyang Zhou, Jingkang Yang, Chen Change Loy, and Ziwei Liu. Learning to prompt for vision-language models. IJCV, 130(9):23372348, 2022. 1 [65] Yiyang Zhou, Chenhang Cui, Jaehong Yoon, Linjun Zhang, Zhun Deng, Chelsea Finn, Mohit Bansal, and Huaxiu Yao. Analyzing and mitigating object hallucination in large vision-language models. In ICLR, 2024. 3, 4 [66] Xianwei Zhuang, Zhihong Zhu, Yuxin Xie, Liming Liang, and Yuexian Zou. Vasparse: Towards efficient visual hallucination mitigation via visual-aware token sparsification. In CVPR, 2025. 3 A. Limitation"
        },
        {
            "title": "Supplementary Material for AlignBench",
            "content": "Methodology. HalDec needs to be light-weight model, considering its application to curate datasets. However, our results indicate that VLMs with more parameters show superior performance. Also, our evaluation relies on sentence-by-sentence score output, which regards each sentence as independent. However, this protocol ignores the context of consecutive sentences. We observe that many sentences can be regarded as independent, yet considering multiple sentences together might improve the performance of hallucination detection. Annotations. Judging the hallucinations in image captions involves subjective criteria of annotators. Captions may look hallucinated to some annotators, while they do not to others. Having unified consensus on this criterion is difficult. For sentence-level annotation, we introduce category unknown, which allows us to exclude such ambiguous samples during evaluation. This issue can be more significant in segment localization and categorizing hallucination types. Then, we focus on sentence-level detection to benchmark VLMs following Mishra et al. [36]. B. The Use of Large Language Models (LLMs) In preparing this manuscript, we made limited use of large language models (LLMs) such as ChatGPT. Specifically, LLMs were employed only to assist with polishing the writing for grammar, clarity, and readability. No part of the research design, analysis, interpretation, or results was generated or influenced by LLMs. All scientific content, data, and conclusions are the sole work of the authors. C. Attribution to an icon We employ the chatbot icons created by Freepik - Flaticon3 for Fig. 1. D. Dataset D.1. Image-Caption Collection We describe the list of models used for collection in Table A. All models except for closed ones are downloaded from Hugging Face [23]. Table A. Details of VLMs picked as Captioners and Text2Image models. We cover diverse models considering their size, provider, and release date. Model Provider Open/Closed Scale Release GPT-4o ShareGPT (Share Captioner) LLaVA-NeXT (llava-next-72b-hf) Llama-4-Scout (17B-16E) Qwen2.5-VL (7B-Instruct) CogVLM ( cogvlm2-llama3-chat-19B ) Stable-diffusion-3.5-medium (SD) GPT-Gen (GPT4o-mini) OpenAI Shanghai AI Laboratory Microsoft Meta Alibaba Tsinghua Univ. Stability AI OpenAI Closed Open Open Open Open Open Open Closed - 7B 72B 109B 7B 19B 2.5B - 2024/05 2023/11 2024/01 2025/04 2024/12 2024/06 2024/10 2024/ Captioner models. We collect data from two sources and employ two text-to-image models. The first source is CC12M, which is designed for vision-and-language pre-training and provides broad domain coverage. The second source is the COCO 2017 dataset, where we use the validation split. For both datasets, we cluster images into 50 domains based on ResNet features and then sample 40 images from each cluster, resulting in total of 2,000 images per dataset. For the Captioner models, we randomly select one of the following instructions: 3https://www.flaticon.com/free-icons/chatbot"
        },
        {
            "title": "Instruction given to captioner models",
            "content": "1. Describe this image in detail. 2. Describe this image in detail. Instead of describing the imaginary content, only describe the content one can determine confidently from the image. 3. Provide detailed description of the image, but only include elements that are clearly visible and verifiable. 4. Describe this image in detail. Minimize aesthetic descriptions as much as possible. 5. Provide detailed, factual description without using emotional language. Text-to-image models. We employ two text-to-image models. The first is stabilityai/stable-diffusion-3.5-medium, diffusion-based generative model that we run locally via the Diffusers library on GPU hardware. The second is OpenAIs gptimage-1, which is accessed through the Responses API with gpt-4o-mini acting as the controller for image generation. For both models, we use identical prompts. To encourage category diversity, we predefine 170 object categories and randomly select one to be included in each prompt. The selected category is then inserted into an instruction given to gpt-4o-mini, which produces 34 sentence description following the specification below. Instruction given to GPT-4o-mini for producing text-to-image prompts want to create prompts to generate image using text to image model. The prompts need to satisfy the following criteria. 1. The prompts include 3-4 sentences. 2. They need to describe scene including target. 3. They need to describe the state of the objects, what they are doing. 4. They need to describe the location of the object in image, (e.g., left, right, bottom, top, etc) 5. They also need to describe where the objects are looking at (e.g., left, right, bottom, top, or towards some) if the object is some organism. Can you suggest prompt? Please return in the form of dictionary, with key of prompt. Output: D.2. Voting and Quality Control We first recruited five annotators and conducted pilot on one hundred images. The authors reviewed all annotations, and annotators who failed to meet our quality standards were not assigned further items. This process allowed us to identify trusted annotators. Each trusted annotator was then assigned between one thousand and two thousand images. The authors checked the quality for every batch of about two hundred images. If the annotations did not meet our standards, annotators were required to re-annotate before proceeding. After the main annotation, we applied multi-round voting. Annotator-specific weights were assigned, with trusted annotators given higher weights. The aggregated votes were used to determine the final labels. For the incorrect (hallucination) category, we adopted stricter rule: if one trusted annotator or two annotators labeled an item as incorrect, the authors manually reviewed it, since hallucinations are more difficult to detect reliably than correctness. Finally, the authors adjudicated all ambiguous cases. This combination of pilot screening, ongoing audits, weighted voting, and final review ensured high-quality hallucination detection annotations. D.3. Inter-Annotator Agreement We evaluated inter-annotator consistency on AlignBench using Fleiss kappa [17] with three labels (correct/incorrect/unknown). The overall agreement reached  = 0.28 with an observed agreement of Pbar = 0.77, which is reasonable given the strong class imbalance in the dataset. When analyzing per-model subsets, the highest agreement was observed for cogvlm with  = 0.40, indicating that its outputs tend to yield more consistent judgments. In contrast, the lowest agreement appeared for gpt4o, which achieved  = 0.22 despite high observed agreement (Pbar = 0.86), suggesting that its infrequent errors are more ambiguous and therefore harder to annotate. Importantly, the authors manually adjudicated all samples that received conflicting labels from annotators. 13 Figure A. Example of an interface used for the hallucination detection. D.4. Annotator Recruitment For the hallucination detection task, we recruited crowd annotators and offered compensation based on the phase and level of effort. On average, annotators received around $100 for completing 2,000 images during the detection phase. Since the hallucination type annotation required more careful reading and reasoning, the compensation was higher, averaging around $150 for each model output. The exact amount varied slightly depending on the annotators country of residence. We did not restrict annotators by location, but we required strong English reading skills, which were verified during the pilot stage. We recruited annotators on Upwork4, Freelancer5, and CrowdWorks6. Figure shows the annotation interface for the hallucination detection phase, while Figure shows the interface used for the hallucination type annotation phase. D.5. Hallucination Type and Location Annotation Table shows the eight hallucination type categories used in the HalCap dataset. These categories cover both fine-grained objectand attribute-level mistakes as well as broader contextual errors. Figure shows annotation examples for each error 4https://www.upwork.com/ 5https://www.freelancer.com/ 6https://crowdworks.jp/ 14 Figure B. Example of an interface used for the hallucination type annotation. Table B. Types of hallucinations categorized for analysis."
        },
        {
            "title": "Description",
            "content": "Misidentifies an object or uses an incorrect noun (e.g., calling dog cat). Incorrect description of an objects property such as color, size, or action (e.g., red car described as blue). Incorrectly states the number of objects or people (e.g., three people when only two are present). Misreads or misrepresents textual information in the image (e.g., misreading store sign). Incorrect description of relationships between objects (e.g., man riding horse when he is standing next to it). Object Attribute Number Text Relation Location Misrepresents the position of an object in the image (e.g., cup on the table when it is on the floor). Direction Illusion Incorrectly describes the direction/orientation of an object (e.g., person facing left when they face right). Describes objects, scenes, or actions that do not exist at all (e.g., mentioning flying bird when no bird is present). type. Hallucinations are highlighted in red. D.6. Additional Analysis Detailed comparison against existing datasets. Table describes the detailed comparison against prior hallucination detection datasets applicable for HalDec. Our dataset includes more responses and includes text-to-image models as the evaluation target. In particular, it offers larger textual coverage, covering 1.6M words, 94k sentences, and vocabulary of 17k unique word types, than prior datasets. Image domain. Figure illustrates the ratio of incorrect sentences on each image category in the CC12M. All Captioners tend to produce more errors in Text and Illustration domains, while they are relatively robust in real images. This can be because of the bias in the training data of the Captioners. Error analysis w.r.t position of the sentence. In Fig. E, we present the ratio of incorrect sentences across sentence positions for each model. Among image captioning models, incorrect sentences tend to appear most frequently in the second to fourth positions. Interestingly, the very first sentence is less likely to contain hallucinations. This may be because the first sentence often serves as an overall image caption. In contrast, the second and subsequent sentences typically provide more detailed descriptions, which are more prone to errors. For positions beyond the sixth sentence, the error rate decreases again. These 15 Figure C. Example annotations of error type. Hallucinations are highlighted in red. Table C. Compared to existing hallucination detector benchmarks for image captions based on their evaluation split, AlignBench offers the largest number of responses, providing annotations for at least 1,000 responses per model across eight models. This scale enables detailed, model-wise performance analysis and facilitates deeper understanding of detector characteristics. For datasets that are not publicly available or lack information, the corresponding statistics are reported as NA."
        },
        {
            "title": "Dataset",
            "content": "Foil [41] HAT [41] ARO [61] Winoground [48] SugarCrepe [22] SeeTrue [58] GenAI-Bench [31] MHalDetect [20] HaELM [52] MHaluBench [10] ZINA [51] AlignBench (Ours)"
        },
        {
            "title": "Segment\nAnnotation",
            "content": "# halluc. types # models # sentences words/Sent. # vocab. # unique image 3 3 6 4 6 9 0 1 0 0 0 1 10 1 3 8 12 8 5k 0.4k 52k 1.6k 2k 6.9k 1.6k 14k 1.5k 0.7k NA 89k 11.8 13.6 7.6 9.0 11.1 11.5 12.6 18.0 13.7 14.6 NA 17.7 4.1k 1.2k 1.5k 0.9k 2.2k 1.5k 4.3k 4.4k 1.6k 1.3k NA 42k 2.5k 0.4k 6.6k 0.8k 1.5k 6.9k 9.6k 0.8k NA 0.2k NA 4k Sent. Generation Rule-base Captioner Rule-base Human Language Model T2I model T2I model Captioner Captioner Captioner + T2I Captioner Captioner + T2I later sentences often serve as overall conclusions or closing remarks rather than detailed descriptions, which may make them similar to the first sentence and thus less prone to errors. Analysis w.r.t hallucination types. Figure describes the type of hallucinations we provide. Our dataset covers various kinds of hallucinations. 16 Figure D. Ratio of incorrect sentences for each image domain. All models tend to produce more errors in domains such as illustration and Text. Figure E. Ratio of incorrect sentences within each sentence position per model. Different colors indicate different positions. All models produce fewer errors at the 1st position. Figure F. Number of hallucinations for each category. Most models make many mistakes in attributes and text. E. Details of Experimental Setups E.1. Details of Evaluation Source of models. We employ models available in HuggingFace and base our code on the HuggingFace Transformers package. Computation. At most eight A100 80GB GPUs are used for inference of single model. Prompt. We employ the prompt below to compute the alignment score for decoder-based VLM. 17 Prompt to compute image-sentence alignment You are given an image and caption describing the given image. Your task is to judge if the caption describes the image correctly. If you think the sentence does not describe the image correctly, return low the score. If you think there is no mistake in the caption, return high score. Judge the correctness from 0-100 points. Return the output in the form of dictionary, e.g., score: 50. Please first output the correctness points before explaining the reason for the score. Caption: Similarly, we use the prompt below to obtain the results of the chain of thought. Chain-of-thought prompt You are given an image and caption describing the given image. Your task is to judge if the caption describes the image correctly. If you think the sentence does not describe the image correctly, return low the score. If you think there is no mistake in the caption, return high score. Judge the correctness from 0-100 points. Return the output in the form of dictionary, e.g., score: 50. Please first explain the reason of scoring in ** two or three ** sentences and output the correctness points as shown above. Caption: Parsing. After obtaining the text output, we write parser to convert the output into an integer. Models sometimes did not properly follow the prompt, and we could not parse such output. For such sample, we assign 50 as its alignment score. In Table 3, we present models with their failure rate less than 5%. Also, the failure rate of well-performing model is very low. Annotation details in self-preference analysis. In Sec. 4.2, we additionally provide sentence-level hallucination existence labels for Qwen-2.5 (32B) and Gemma-3 (27B). To reduce the cost of annotation, we follow an annotation procedure different from the other 8 models, yet in quality-ensured manner. Specifically, we randomly pick 500 images and generate captions using two models. Then, one quality-ensured annotator gives an annotation to 500 captions. This produces enough samples for analysis. We will include this split when publishing the dataset. Prompt in hallucination localization. We employ the prompt below to obtain the results of hallucination localization."
        },
        {
            "title": "Prompt for hallucination localization",
            "content": "You are given an image and caption describing the given image. Your task is to localize the segment of the caption, which describes the image incorrectly. Please output the segment by marking the incorrect parts by **[]**, e.g., **[red]** bird singing in tree. Return the output in the form of dictionary. Example format. json { \"output\": \"A **[red]** bird singing in tree.\" } Caption: Evaluation metric in hallucination localization. We evaluate the alignment between the word spans predicted by models and the ground-truth (GT) spans using an Intersection-over-Union (IoU) based criterion. Concretely, we compute the IoU between the predicted word range and the GT word range. In Table 5, prediction is considered correct if its IoU with GT span is greater than or equal to 0.3. Based on this criterion, we measure precision as the proportion of predicted spans that are judged correct. F. Additional Experiments Does incorporating the preceding sentences lead to improved performance? In our main paper, we focus on the evaluation of alignment between single sentence and an image. This setting ignores the context from the preceding sentence in caption since the model does not see the preceding sentences during inference. We study the effectiveness of adding the 18 Table D. Results of using preceding sentences as context."
        },
        {
            "title": "Context",
            "content": "S-GPT"
        },
        {
            "title": "Llava CogVLM",
            "content": "Llama-4 (109B) GPT5-mini 80.7 80. 81.5 82.6 78.6 79.1 82.2 81. 77.2 73.5 81.1 79.5 preceding sentences. Specifically, we feed all preceding sentences as well as the target sentence for evaluation as prompt, e.g., cat is running in park. The cat is next to kid. Think about the correctness of **The cat is next to kid. **. Table compares the effect of adding contextual sentences. Surprisingly, we find that providing this additional context does not clearly improve imagetext alignment. For Llama-4, adding context often degrades instruction following, leading to more frequent parsing errors. For GPT5-mini, no parsing errors are observed, but the performance still drops slightly. These results suggest that incorporating preceding sentences as context can interfere with judging the alignment between the target sentence and the image. Since not all language models excel at handling long sequences, evaluation on single sentence can be fair in evaluating the image-text alignment ability. Table E. Results of using Chain-of-Thought (COT). Detector COT Llama-4 (109B) GPT4.1-mini Image-to-Caption Models Text-to-Image Models S-GPT 80.7 Llava 78. Qwen-2 GPT4o CogVLM Llama-4 77.6 67.5 77. 59.9 SD 81.1 GPT-Gen 64.7 80.6 (0.1) 80.8 (+2.2) 80.0 (+2.4) 71.1 (+3.6) 80.1 (+2.9) 62.4 (+2.5) 80.8 (0.3) 65.1 (+0.4) 77.8 75.8 74.4 65.8 69. 66.0 68.7 56.1 79.0 (+1.2) 76.2 (+0.4) 75.0 (+0.6) 63.4 (2.4) 71.6 (+2.4) 63.8 (2.2) 73.2 (+4.5) 56.2 (+0.1) Chain-of-Thought improves the performance? Table evaluates the impact of chain-of-thought reasoning [55], where detectors are prompted to generate reasoning path before producing score (see above for prompt details). For Llama-4, COT generally improves performance, whereas for some Captioners, the gains are marginal or even slightly negative. Results for GPT4.1-mini are mixed, wherein improvements highly depend on the evaluation target. Table F. Ensembling detectors output improves performance in almost all cases. We highlight the increase or decrease from the better model used for ensembling next to each score. Detector Num. of Ensemble Image-to-Caption Models Text-to-Image Models Llama-4-scout Llama-4-scout Llama-4-scout 1 5 10 S-GPT 80.6 Llava 80.8 Qwen-2 GPT4o CogVLM Llama80.0 71.1 80.1 62.4 SD 80. GPT-Gen 65.1 83.2 (+2.6) 83.0 (+2.2) 81.8 (+1.9) 74.4 (+3.4) 82.2 (+2.1) 65.1 (+2.7) 83.0 (+2.2) 65.9 (+0.8) 83.7 (+3.1) 83.4 (+2.6) 82.2 (+2.3) 75.0 (+3.9) 82.8 (+2.7) 65.7 (+3.3) 83.4 (+2.6) 66.4 (+1.3) Self-ensemble improves performance. We further study the potential of ensembling. Unlike the analysis above, we ensemble outputs from single model to refine detectors score [14, 24]. To get different scores from single model, we obtain different reasoning paths by stochastic sampling in the chain-of-thought. To ensure the diversity of COT, we set the temperature as 1.5 and topp as 0.9. Table presents the results in Llama-4, where the performance consistently improves in all Captioners. Also, using more ensemble paths tends to improve the performance, while the increase seems to saturate. Model ensembling can be an interesting direction to improve the performance in this task. VLM detectors can surpass prior approaches. Table presents the comparison to UniHD [10], which prompts LLM to utilize an open-vocabulary detector and OCR engine. The results indicate that advanced VLMs can surpass the approach without using such external tools. More detailed discussion is available in the appendix. Mean intersection over union in hallucination localization. Table shows the results of mean IoU in hallucinated segment localization. Specifically, we compute the intersection over union between the predicted and ground-truth segments and compute the average for all samples. Overall, the performance is consistent with what is reported in Table 5. Additional results in self-preference evaluation. Fig. illustrates self-preference score analysis for Gemma-27B, Llama-4, and Qwen2.5. Their self-preference tendency is significant, especially for Gemma-27B and Qwen2.5. 19 Table G. Comparison to existing HalDec approaches."
        },
        {
            "title": "Detector",
            "content": "GPT4o SD UniHD [10] Qwen-2.5 32B Gemma-3 27B Llama-4 109B GPT-4.1-mini 62.6 66.1 61.0 67.7 65. 71.0 68.9 63.7 81.1 68.7 Table H. Mean IoU for hallucination localization task. Localizing the segment of the hallucinated caption remains difficult even for performant models."
        },
        {
            "title": "Params",
            "content": "Image-to-Caption Models Text-to-Image Models S-GPT Llava Qwen-2 GPT4o CogVLM Llama-4 Qwen-2.5 GPT-4o mini Llama-4 Llama-4 32B - 109B 400B 13.8 21.6 22.6 24.8 15.1 22. 20.8 22.1 11.7 18.3 22.7 23. 15.1 23.3 26.4 26.0 16.0 21. 23.2 21.7 10.7 16.4 17.3 18. SD 11.4 12.5 10.7 11.9 GPT-Gen 9.4 11.9 9.0 9.3 Avg. 12. 18.5 19.1 19.6 Figure G. Detectors output score for their own output captions. Distributions of evaluators outputs. Fig. illustrates the distribution of evaluators scores for GPT-4o captions. Scores tend to concentrate on the points near 0 and 100 for Llama-4. Additional examples of VLMs outputs. Figure illustrates examples of input images, sentences, and corresponding correctness scores inferred by VLMs. VLMs tend to make errors in the location of the objects, the relationship between them, and small visual details. G. Additional Examples of Annotations We provide additional figures illustrating annotation results and representative hallucination cases: ShareGPT (Fig. J), LLaVA (Fig. K), Qwen-2 (Fig. M), GPT-4o (Fig. M), CogVLM (Fig. N), LLaMA-4 (Fig. O), Stable Diffusion (Fig. P), and GPT-Gen (Fig. Q). 20 Figure H. Distributions of evaluators output scores. We visualize the evaluators scores for GPT-4o captions. 21 Figure I. Examples of input image and sentences with detectors correctness scores. Higher scores indicate greater confidence in correctness. We highlight detectors errors in red within the text. Figure J. Example annotations of Share-GPT. 23 Figure K. Example annotations of LLaVA. 24 Figure L. Example annotations of Qwen-2. 25 Figure M. Example annotations of GPT4o. 26 Figure N. Example annotations of CogVLM. 27 Figure O. Example annotations of Llama-4. 28 Figure P. Example annotations of Stable Diffusion. 29 Figure Q. Example annotations of GPT-Gen."
        }
    ],
    "affiliations": [
        "OMRON SINIC Corporation",
        "The University of Osaka"
    ]
}