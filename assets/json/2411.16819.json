{
    "paper_title": "Pathways on the Image Manifold: Image Editing via Video Generation",
    "authors": [
        "Noam Rotstein",
        "Gal Yona",
        "Daniel Silver",
        "Roy Velich",
        "David Bensaïd",
        "Ron Kimmel"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in image editing, driven by image diffusion models, have shown remarkable progress. However, significant challenges remain, as these models often struggle to follow complex edit instructions accurately and frequently compromise fidelity by altering key elements of the original image. Simultaneously, video generation has made remarkable strides, with models that effectively function as consistent and continuous world simulators. In this paper, we propose merging these two fields by utilizing image-to-video models for image editing. We reformulate image editing as a temporal process, using pretrained video models to create smooth transitions from the original image to the desired edit. This approach traverses the image manifold continuously, ensuring consistent edits while preserving the original image's key aspects. Our approach achieves state-of-the-art results on text-based image editing, demonstrating significant improvements in both edit accuracy and image preservation."
        },
        {
            "title": "Start",
            "content": "Pathways on the Image Manifold: Image Editing via Video Generation"
        },
        {
            "title": "Noam Rotstein\nRoy Velich",
            "content": "Gal Yona David Bensaıd"
        },
        {
            "title": "Daniel Silver\nRon Kimmel",
            "content": "Technion - Israel Institute of Technology 4 2 0 2 5 2 ] . [ 1 9 1 8 6 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in image editing, driven by image diffusion models, have shown remarkable progress. However, significant challenges remain, as these models often struggle to follow complex edit instructions accurately and frequently compromise fidelity by altering key elements of the original image. Simultaneously, video generation has made remarkable strides, with models that effectively function as consistent and continuous world simulators. In this paper, we propose merging these two fields by utilizing image-tovideo models for image editing. We reformulate image editing as temporal process, using pretrained video models to create smooth transitions from the original image to the desired edit. This approach traverses the image manifold continuously, ensuring consistent edits while preserving the original images key aspects. Our approach achieves stateof-the-art results on text-based image editing, demonstrating significant improvements in both edit accuracy and image preservation. 1. Introduction Image editing has witnessed remarkable advancements through deep learning and text-guided diffusion networks. These developments have set new benchmark for image manipulations, enhancing both control and quality. However, current approaches continue to face significant limitations in real-world scenarios. These methods often struggle with two key challenges: achieving precise edits that accurately reflect the intended modifications, and preserving the essential characteristics of the original image content. State-of-the-art techniques predominantly rely on textguided diffusion models, which iteratively denoise random latent representations to generate edited images. Such methods condition the generation process on both the source imageusing techniques such as latent inversion [30] or model fine-tuning [21]and the target edit description. However, these approaches require the model to generate single output image that preserves source image fidelity while implementing complex edits, often compromisFigure 1. Visualization of Frame2Frames editing process. Temporal progression of our video-based approach. Starting from the source image (leftmost), frames illustrate the natural evolution toward the target edit (rightmost). Our method produces temporally coherent intermediate states while preserving fidelity to both the source content and the editing intent. ing edit accuracy and content preservation. In this paper, we propose paradigm shift in image editing by reformulating it as video generation task. Instead of attempting this single state transition, our approach harnesses temporal coherence: the source image serves as the initial frame of video that progressively and naturally transforms toward the target edit. This temporal evolution allows the editing process to unfold through physically 1 plausible intermediate states, providing continuous path between source and target images, as illustrated in Figure 1. This temporal approach leverages the sophisticated world understanding embedded in recent video generation models, which have achieved breakthrough results in temporal coherence and visual quality through training on large-scale internet data [2, 46]. From geometrical perspective, conventional editing approaches project initial noise onto the natural image manifold, targeting single point where the image aligns with both the source and the edit request. In contrast, our approach generates continuous path along the manifold between the original and edited image, producing smooth realistic transition across different image states, as thoroughly discussed in Section 4. We implement the proposed approach through structured pipeline called Frame2Frame (F2F). First, we transform the edit instruction into Temporal Editing Caption scenario describing how the edit should naturally evolve over timeusing pretrained Vision-Language Model (VLM). Next, state-of-the-art image-to-video model generates temporally coherent sequence guided by the temporal caption. Finally, we identify the frame that best realizes the desired edit with the assistance of VLM. Extensive experiments demonstrate improvements over existing imageto-image approaches. We evaluate on TedBench [21] and PosEdit, newly curated dataset derived from UTD-MHAD [6], which focuses on human pose transformations. PosEdit pairs source images with ground-truth targets of the same subject in different poses, enabling rigorous evaluation of both edit accuracy and source fidelity. Beyond commonly defined editing tasks, our framework shows promising results in more classical computer vision problems such as de-blurring, de-noising, and relighting by recasting them as temporal progressions, suggesting broader applications for video-based image transformations. Our main contributions include: 1. Reformulating image editing as generative video task leveraging temporal coherence to create edit paths on the natural image manifold, enabling high-fidelity manipulations while preserving source characteristics. 2. Frame2Frame: an end-to-end framework that realizes the reformulation through three key components: (1) temporal editing captions, (2) generated video-based editing, and (3) automated frame selection. 3. Comprehensive evaluation showing state-of-the-art performance on TEdBench and PosEdit, new dataset for evaluating human pose edits. 2. Related Efforts 2.1. Image Editing Text-based image editing has advanced significantly with the success of generative diffusion models [18]. These models, in their text-to-image version, generate images through denoising diffusion process conditioned on input text, relying on ground truth pairs of text and images for training [35, 37]. In contrast, image editing often lacks predefined ground truth data for source and target images, presenting unique challenges. This limitation has led researchers to explore diverse range of editing methodologies [19]. For example, SDEdit [28] introduces noise into an image and then denoises it based on an editing target prompt. Imagic [21] fine-tunes text-to-image model on single image, subsequently interpolating between input and target text embeddings to produce edits. Other methods first invert the input image into diffusion models latent space [20, 30] and then generate the edited image from that latent representation using various techniques for structure preservation and manipulation [17, 32]. InstructPix2Pix [5] synthesizes an editing dataset using the approach in [17], filters it, and employs this dataset to train diffusion model in supervised fashion. Paint-by-Inpaint [45] further explores this supervised approach, generating real-image dataset for object insertion. 2.2. Generative Video Models Recent years have seen remarkable advancements in video generation, progressing from systems limited to specific domains [15] and brief clips to models capable of generating diverse, high-fidelity content. This progress has been notably driven by paradigm shifts and the significant scaling of both datasets [7] and architectures. Approaches have evolved from recurrent networks [16, 40] and generative adversarial networks (GANs) [9, 41, 42] to latent diffusion models (LDMs) [2, 3, 11], which leverage large U-Net or transformer-based architectures alongside vast internetsourced datasets. Notable recent efforts include Stable Video Diffusion [2], which utilizes large curated datasets to train latent diffusion model, and OpenAIs Sora [26], which demonstrates impressive capabilities by reportedly leveraging large architecture and extensive public and private datasets. These models have been described as world simulators due to their emergent understanding of physical dynamics and temporal coherence. Our work builds upon CogVideoX [46], transformer-based latent diffusion model that employs 3D Variational Autoencoder to compress videos across both spatial and temporal dimensions, enhancing coherence. To improve text-video alignment, CogVideoX also integrates an expert transformer with adaptive LayerNorm, allowing deep fusion between visual and textual modalities. Close to our work, several recent efforts have utilized the world simulation capabilities of video diffusion models for various computer vision tasks. Make-A-Video3D [39] temporally extends static NeRFs using Score Distillation Sampling from video models. ViVid-1-to-3 [23] generates 2 Figure 2. Editing Manifold Pathway. Given an input image and target caption happy person making heart shape with their hands, our method generates continuous path on the natural image manifold. Each generated frame (indicated by black arrows) represents plausible intermediate state between the source and target, maintaining temporal consistency throughout the transformation. As result, in contrast to the competing approach, F2F achieves the desired edit while preserving the AI text on the persons shirt. images along camera trajectory around objects to enable novel view synthesis. PhysDreamer [49] models rigid object properties through 3D Gaussians and material fields, trained via distillation from pretrained video generators. 2.3. Image Editing and Video The intersection of image editing and video has received limited attention. Existing methods focus on sampling pairs of random frames from videos to build image pair datasets that capture the same subject under varying conditions. For example, AnyDoor [8] uses the paired frames as an augmentation method, segmenting foreground objects in each frame and assigning one masked object as the target edited appearance of the subject. MagicFixup [1] employs these frames to build dataset focused on refining user-made subject coarse 2D edits. Recently, drag-based editing approaches [27, 38], used frames extracted from video and computed optical flow to collect dataset aimed at editing by spatially dragging points within the image. In contrast to these approaches, which rely on frame sampling for image data collection, we are the first to directly perform image editing using generative video models. 3. Frame2Frame We present Frame2Frame, framework that reformulates image editing as temporal transformation process. Our approach leverages video generation models to create natural transitions between source and target images, achieving consistent and realistic edits. The proposed method has three main steps, as illustrated in Figure 3. 3.1. Temporal Editing Captions Text-based image editing methods typically operate on two inputs: source image Is and target caption c, where specifies the desired modifications to Is. Our approach differs fundamentally by modeling editing as temporal process. This requires novel type of promptthe Temporal Editing Caption, denoted by cthat describes the sequential transformation from source to target image. We construct by combining information from Is and to create description of how the desired edit unfolds over time. To automate this process, we leverage recent advances in Vision-Language Models (VLMs) [10, 14, 36, 43, 44]. The VLM, specifically ChatGPT-4o [31], is instructed to produce concise video scenario that highlights how elements within the image change or move over time. The generated caption captures the essential transformations while maintaining static camera perspective unless movement is necessary. To improve generation quality, we utilize in-context learning (ICL) [12], providing the VLM with nine exemplar prompt-caption pairs. The complete prompt template, ICL examples, and an ablation study comparing this approach with directly using the target captions are included in the Appendix. 3 3.2. Video Generation We employ CogVideoX [46], pretrained generative video latent diffusion model utilizing transformer-based architecture. Specifically, we use its image-to-video variant, which has been fine-tuned to generate videos starting from an input image Is. During generation, Is is encoded and concatenated with noise in latent space, where the model applies denoising process guided by the temporal caption c. As elaborated in Section 4, this conditioning allows generated videos to start from Is and evolve naturally along the image manifold, maintaining temporal coherence and consistency. Additionally, the models transformer architecture enables effective fusion of visual and textual information, allowing precise control over the editing process through our temporal captions. Formally, given the video generator G, we define the generation process as: G(Is, c) = = {f1, . . . , fT } where denotes the generated video with frames, and ft represents the frame at timestep t. 3.3. Frame Selection We observed that the optimal number of frames required for an edit can varysmall changes may be completed in fewer frames, while more extensive transformations often necessitate additional ones. Additionally, later frames tend to deviate further from the source image. Thus, even though serves as an editing path originating from Is, there is no guarantee that fT is the optimal edited image in . For instance, in Figure 3, the edited image of the cat might be expected to capture the midpoint of its jump, but the last frame shows the jump already completed. Therefore, we aim to identify the optimal edited frame, denoted ft , which corresponds to the earliest timestep that achieves the desired edit. The transition from the initial frame f1 (or Is) to the final edited frame ft motivates our methods name, Frame2Frame. To automate the selection of and avoid manual frameby-frame review, we employ an automated approach. After generating the sequence , we sample every fourth frame, imprinting each with unique identifier and assembling them into an image collage alongside Is. Inspired by [22], which introduces novel approach to video comprehension by transforming videos into image grids, we use VLM, specifically GPT-4o, to assist in selecting by providing it with the collage and the editing prompt c. The VLM is tasked with identifying the frame that best fulfills the editing intent, evaluating each frames alignment with and fidelity to Is. The model is instructed to select the optimal frame with the lowest index that completes the edit. An ablation study, detailed in the appendix, evaluates the effectiveness of our automated approach against the baseline of selecting the final frame of the video. Figure 3. Frame2Frame Overview. Given source image and editing prompt, our pipeline proceeds in three steps. First, Vision-Language Model generates temporal caption describing the transformation. Next, this caption guides video generator to create natural progression of the edit. Finally, our frame selection strategy identifies the optimal frame that best realizes the desired edit, producing the final image of the cat mid-leap. It is important to note that while we refer to an optimal t, the definition of the required edit can be subjective and dependent on user preferences. For example, in Figure 3, different users may select frames of the cat jumping based on variations in its altitude. Thus, frame selection could serve as flexible and customizable advantage in certain scenarios. 4. Editing Manifold Pathway To illustrate the advantages of our method over conventional image-to-image approaches, we aim to visualize the editing process within the natural image manifold, where realistic images reside. Given the high dimensionality of this space, we project it into lower-dimensional representation suitable for interpretation. To achieve this, we first generate three sets of images, each containing 200 samples, using the text-to-image generator FLUX.1-dev [24]. The sets are based on the following prompts: 1. AI: Full-body portrait of happy person wearing shirt with the word AI on it. 2. AI + Heart Hands: Full-body portrait of happy person wearing shirt with the word AI on it and making heart shape with their hands. 3. Heart Hands: Full-body portrait of happy person making heart shape with their hands. We manually filter the outcomes to ensure alignment with the descriptions, with examples provided in the supplementary material. Then, we use CLIP ViT-B/32 model [34] to extract 512-dimensional feature vector for each image. Using these features, along with 25 feature vectors extracted from random noise images, we perform Principal Component Analysis (PCA) to reduce the dimensionality into two-dimensional subspace. The resulting subspace, now suitable for visualization, is depicted in Figure 2. As illustrated in the figure, the natural images form smooth manifold, distinctly separated from the distribution 4 Source Target LPIPS CLIP-I CLIP"
        },
        {
            "title": "Model",
            "content": "LPIPS CLIP-I LPIPS CLIP-ICLIP Source Target"
        },
        {
            "title": "Model",
            "content": "SDEdit Pix2Pix-ZERO LEDITS++ Imagic F2F 0.30 0.29 0.23 0.52 0.22 0.85 0.84 0.87 0.86 0. 0.60 0.62 0.63 0.63 0.63 Table 1. TEdBench Results. Quantitative comparison on TEdBench benchmark. Source metrics (LPIPS and CLIP-I) measure content preservation, while Target metric (CLIP) measures edit accuracy. Our Frame2Frame (F2F) method achieves better or comparable performance across all metrics. of noise images. Within this manifold, there is clear images of people with AI shirts semantic progression: (green cluster) are close to images of people with AI shirts making heart shape (purple cluster), which are adjacent to images of people only making heart shape (red cluster). Thus, transitioning smoothly along the manifold allows person with an AI shirt to perform heart shape with their hands while preserving the shirts text. Consider an original image from the AI group that we wish to edit using the target prompt: happy person making heart shape with their hands. Current editing methods generate single image, which may result in an abrupt transition to the red cluster, effectively removing the AI on the shirt. This behavior is illustrated in the figure, where the edit performed by LEDITS++ is positioned near the red cluster, and the AI text on the shirt disappears. In contrast, our method leverages video generation to perform the edit smoothly, moving along the manifold with incremental changes until it reaches the required edit in the purple cluster, as depicted by the black arrows in Figure 2. The temporal editing caption guiding this process is: happy person very slowly raising their hands to form heart shape. This gradual progression enables us to reach the purple cluster, resulting in an edited image where the person maintains the AI on their shirt while making heart shapea faithful preservation of the original images key attributes. This experiment illustrates that the proposed paradigm enables smooth traversal across the image manifold, enabling consistent edits while preserving the essential characteristics of the original image. 5. Experiments We evaluate Frame2Frame against state-of-the-art image editing methods, including LEdits++ [4], SDEdit [29], Pix2Pix-Zero [33] and Imagic [21]. Our evaluation spans two benchmarks: the established TEdBench [21] for genimage editing, and our newly introduced PosEdit eral 5 SDEdit 0.39 Pix2Pix-ZERO 0.39 0.26 LEDITS++ F2F 0.14 0.61 0.57 0. 0.82 0.39 0.40 0.28 0.15 0.64 0.60 0.69 0.57 0.56 0.64 0. 0.64 Table 2. PosEdit Results. Quantitive evaluation on PosEdit. Source metrics assess similarity to the original image, while Target metrics now include both LPIPS and CLIP-I comparisons to the ground-truth target image, along with the CLIP score for edit accuracy. benchmark, specifically designed to evaluate human pose editing tasks. Finally, we conduct human evaluation to better understand our methods performance based on real user preferences. 5.1. Evaluation Protocol We conduct our experiments following consistent evaluation protocol across all methods and benchmarks. Following common practice [4, 21], for each method and source image, we manually select the best result from fifteen random seeds based on visual quality and edit accuracy, ensuring the same seed set is used across all methods. For all methods, we use the default hyperparameters and settings as provided in their official implementations or official Hugging Face repositories 1. Image Preprocessing and Generation Details. We use CogVideoX [46] as our video generation backbone, which operates at fixed resolution of 720480 pixels. Both TEdBench and PosEdit benchmarks consist of images with 1:1 aspect ratio. To accommodate CogVideoXs resolution requirements while preserving image content, we first resize all source images to 480 480 pixels, then horizontally pad them with black pixels to reach the required 720 480 resolution (adding 120 pixels on each side). After generating the video sequence, we crop the central 480 480 region of the selected frame to remove the padding and resize it to the final evaluation resolution of 512512 pixels, matching the standard resolution used in prior work. For video generation, we adopt the default hyperparameters proposed by CogVideoX: guidance scale of 6, 49 generated frames per sequence, and 50 denoising inference steps. This configuration generates videos approximately 6 seconds in duration at frame rate of 8 frames per second. Method-Specific Requirements. Pix2Pix-Zero [33] requires additional source image descriptions along with the 1LEdits++, Pix2Pix-Zero, SDEdit Figure 4. Qualitative Results on TEdBench. Comparison with other methods across various editing tasks. Our approach consistently produces edits that better align with the target prompt while preserving the source images content and structure. For instance, in the teddy bear example, our method uniquely achieves complex structural modifications while maintaining high visual quality. standard inputs. We generate these automatically using BLIP-2 [25], state-of-the-art image captioning model, ensuring consistent source descriptions across all experiments. For our method, we transform the original editing prompts from both benchmarks into temporal editing captions, as described in Section 3.1. ident in the comparisons presented in Figure 4. The figure demonstrates our methods superior performance across diverse editing scenarios, producing results that both faithfully execute the intended edits while maintaining strong alignment with the source image. Additional examples showcasing these capabilities are provided in the appendix. 5.2. TEdBench Evaluation Results 5.3. PosEdit Benchmark We quantitatively evaluate our method and baselines on the TEdBench benchmark using three complementary metrics. For each method, we compute the metrics between the source image and its corresponding best edited version. LPIPS [48] measures perceptual similarity to assess how much the edit preserves the source images content, with lower values indicating better preservation. CLIP-I evaluates the similarity between source and edited images in CLIPs [34] feature space, where higher values indicate better preservation of semantic content. Finally, CLIP score measures alignment between the edited image and the target prompt, with higher values indicating better adherence to the editing instruction. As shown in Table 1, our method achieves strong performance across all metrics, demonstrating effective balance between preserving source content and achieving the desired edit. The qualitative advantages of our method are visually evWe introduce PosEdit, benchmark for human pose editing derived from the UTD-MHAD dataset [6]. UTD-MHAD includes RGB videos of 8 subjects (4 females and 4 males) performing predefined actions in controlled indoor environment. We carefully curated 58 editing tasks encompassing 8 distinct action categories, ranging from simple poses like raised hand to complex athletic poses such as basketball shooting and lunging. The proposed benchmark specifically focuses on human pose manipulation. Unlike TEdBench, each editing task in PosEdit includes ground truth frame extracted from the same subject in the target pose. The availability of reference images allows for more comprehensive evaluation of both editing accuracy and identity preservation. In the dataset, each editing task consists of two images: source image showing the subject in neutral standing pose with arms relaxed at their sides, and ground-truth edited target"
        },
        {
            "title": "Method",
            "content": "Overall Per-Image Overall Per-Image Edit Accuracy Edit Quality F2F LEDITS++ 54.1% 53.0% 47.0% 45.9% 65.6% 67.0% 33.0% 34.4% Table 3. Human Survey Results. Human evaluation on TEdBench shows that F2F surpasses LEDITS++ in edit accuracy while demonstrating significant advantage in preserving the original image content. only reported on TEdBench. 5.4. Human Evaluation Survey To further evaluate our method, we conducted human survey. As indicated in Table 1, LEDITS++ emerges as the most competitive method compared to ours, making it natural choice for comparison. Participants were presented with 20 random TEdBench samples, including the source image, the target edit prompt, and the corresponding edited outputs from both methods. Inspired by the methodology proposed by [47], participants evaluated each comparison based on two criteria: (1) edit accuracy relative to the prompt, and (2) edit qualitydefined as the preservation of visual fidelity to the source image, seamless integration of edited elements, and the overall natural appearance of modifications. We collected responses from 59 randomly selected online evaluators. To ensure an unbiased assessment, evaluators were not informed of the studys objectives. The results were quantified using two metrics: (i) overall global preference, expressed as percentage, and (ii) aggregated per-image preference, where tie resulted in each method receiving 0.5 points. Our results, summarized in Table 3, demonstrate that F2F outperforms LEdits++ on both metrics. Specifically: For edit quality, F2F achieved global preference score of 53%, slightly surpassing the 47% obtained by LEdits++. In terms of edit accuracy, F2F achieved 65.6% global preference compared to LEdits++s 34.4%. The per-image preference results mirrored this trend, indicating robustness across individual examples and no significant influence of outliers. These findings reinforce our claim that smooth temporal editingusing video as mediumpreserves essential scene characteristics while successfully performing the edit. Our results suggest that the gap between F2F and LEdits++ in source preservation is larger than reflected by the LPIPS scores in Table 1. The full survey details are provided in the supplementary materials. 5.5. Additional Vision Tasks We demonstrate our frameworks applicability beyond traditional editing by applying it to fundamental image manipulation tasks: denoising, deblurring, outpainting, and reFigure 5. Qualitative Results on PosEdit. Comparison between our Frame2Frame method and LEDITS++ on human motion editing tasks. For each example, we show the source image, edited results from both methods, and the ground-truth target image. Our method better preserves subject identity while achieving more natural pose transitions. image capturing the subject performing in specific pose. The benchmark also provides prompt for each task that describes the target pose the subject should achieve (e.g., person in basketball shooting posture.). Benchmark Evaluation. The evaluation on PosEdit follows the same protocol established for TEdBench: generating multiple variants using nine different seeds and manually selecting the best result for each method. However, PosEdits ground-truth target images enable additional evaluation metrics. Beyond measuring preservation of source content (via LPIPS and CLIP-I between source and edited images) and edit accuracy (via CLIP score with the prompt), we compute LPIPS and CLIP-I metrics between the edited image and its corresponding ground-truth target. This enhanced evaluation directly assesses both the accuracy of the transformation and preservation of identity features. Table 2 demonstrates that our method consistently outperforms competitive methods across all metrics, with particularly strong performance in similarity measures with the target ground-truth image. This advantage highlights our methods superior ability to preserve key features while achieving the desired edit, as illustrated by the qualitative comparisons in Figure 5. The figure shows that our method generates more natural pose transitions while maintaining crucial identity attributes such as facial features, body proportions, and clothing details. Two important implementation notes: First, for Pix2PixZero, which requires source image descriptions, we use the same static prompt across all tasks: person standing naturally with his arms relaxed at his sides. Second, we exclude Imagic [21] from this comparison as their official implementation is not publicly available and their results are 7 Figure 6. Additional Vision Tasks. Qualitative results of our image-to-video-to-image editing approach on selected traditional tasks. lighting. For these experiments, we utilize Runway Gen-3 [13] as our video generation backbone, as it showed superior performance on these specific tasks compared to other generative video models. Our results, shown in Figure 6, demonstrate strong performance across all tasks. We associate this success to the natural alignment between these operations and common video scenarios: deblurring maps to camera focusing, outpainting to camera motion, and relighting to time-lapse lighting changes. This alignment enables our method to leverage the video models learned temporal dynamics, achieving high-quality results without taskspecific training. Full prompts are provided in the supplementary material. 6. Limitations While our approach addresses key shortcomings in current image editing methods, it also introduces unique challenges. For example, natural camera motion sometimes appears in video sequences, which, when replicated in generated content, can lead to unintended perspective shifts. As with all generative models, video models are trained on specific data domains, making it challenging to produce results that deviate significantly from the models training data, which predominantly includes real-world transformations. Despite these challenges, F2F demonstrates success in several cases. For example, in the middle row of Figure 1, the vase is magically filled with green water, showcasing the models ability to perform imaginative edits. Additionally, our method is computationally intensive, as transforming an image into video sequence is resource-heavy and often slower than other image editing methods. However, video generation efficiency is advancing rapidly, as demonstrated by models like Runway Turbo2 and LTX-Video3, which can execute the process in seconds, potentially making this approach significantly less resource-intensive in the future. 7. Conclusions We introduced Frame2Frame, novel approach that reformulates image editing through video generation. By leveraging video models inherent understanding of temporal transformations, our method achieves state-of-the-art editing results while maintaining high fidelity to source images. We demonstrated our frameworks effectiveness on standard benchmarks, introduced PosEdit for human pose editing, and showed promising results on more classical vision tasks. As video generation technology advances, we expect our approach to enable increasingly sophisticated image manipulations while maintaining natural and physically plausible results. Future Research. Our work opens several promising directions. First, fine-tuning video generators specifically for image editing presents an exciting opportunity, including straightforward solutions like enforcing static camera scenarios or using datasets curated for editing tasks, alongside more complex approaches yet to emerge. Secondly, key direction could involve reducing the overhead of full video generation while preserving the benefits of gradual, temporally coherent transformations, potentially enhancing both the efficiency and speed of editing. 2Runway Turbo API 3LTX-Video"
        },
        {
            "title": "References",
            "content": "[1] Hadi Alzayer, Zhihao Xia, Xuaner Zhang, Eli Shechtman, Jia-Bin Huang, and Michael Gharbi. Magic fixup: Streamlining photo editing by watching dynamic videos. arXiv preprint arXiv:2403.13044, 2024. 3 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2 [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with laIn Proceedings of the IEEE/CVF tent diffusion models. Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 2 [4] Manuel Brack, Felix Friedrich, Katharina Kornmeier, Linoy Tsaban, Patrick Schramowski, Kristian Kersting, and Apolinario Passos. Ledits++: Limitless image editing using text-to-image models. 2023. 5 [5] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 2 [6] Chen Chen, Roozbeh Jafari, and Nasser Kehtarnavaz. Utdmhad: multimodal dataset for human action recognition utilizing depth camera and wearable inertial sensor. In 2015 IEEE International conference on image processing (ICIP), pages 168172. IEEE, 2015. 2, [7] Tsai-Shien Chen, Aliaksandr Siarohin, Willi Menapace, Ekaterina Deyneka, Hsiang-wei Chao, Byung Eun Jeon, Yuwei Fang, Hsin-Ying Lee, Jian Ren, Ming-Hsuan Yang, et al. Panda-70m: Captioning 70m videos with multiple In Proceedings of the IEEE/CVF cross-modality teachers. Conference on Computer Vision and Pattern Recognition, pages 1332013331, 2024. 2 [8] Xi Chen, Lianghua Huang, Yu Liu, Yujun Shen, Deli Zhao, and Hengshuang Zhao. Anydoor: Zero-shot object-level In Proceedings of the IEEE/CVF image customization. Conference on Computer Vision and Pattern Recognition, pages 65936602, 2024. 3 [9] Aidan Clark, Jeff Donahue, and Karen Simonyan. AdversarIn ICCV, 2019. ial video generation on complex datasets. 2 [10] Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards generalpurpose vision-language models with instruction tuning, 2023. 3 [11] Duolikun Danier, Fan Zhang, and David Bull. Ldmvfi: Video frame interpolation with latent diffusion modIn Proceedings of the AAAI Conference on Artificial els. Intelligence, 2024. 2 [12] Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, et al. survey on in-context learning. arXiv preprint arXiv:2301.00234, 2022. 3 [13] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models, 2023. 8 [14] Roy Ganz, Yair Kittenplon, Aviad Aberdam, Elad Ben Avraham, Oren Nuriel, Shai Mazor, and Ron Litman. Question aware vision transformer for multimodal reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1386113871, 2024. [15] Yue Gao, Yuan Zhou, Jinglu Wang, Xiao Li, Xiang Ming, and Yan Lu. High-fidelity and freely controllable talking In Proceedings of the IEEE/CVF head video generation. Conference on Computer Vision and Pattern Recognition, pages 56095619, 2023. 2 [16] David Ha and Jurgen Schmidhuber. World models. arXiv preprint arXiv:1803.10122, 2018. 2 [17] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. 2 [18] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 2 [19] Yi Huang, Jiancheng Huang, Yifan Liu, Mingfu Yan, Jiaxi Lv, Jianzhuang Liu, Wei Xiong, He Zhang, Shifeng Chen, and Liangliang Cao. Diffusion model-based image editing: survey. arXiv preprint arXiv:2402.17525, 2024. 2 [20] Inbar Huberman-Spiegelglas, Vladimir Kulikov, and Tomer Michaeli. An edit friendly ddpm noise space: Inversion and manipulations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1246912478, 2024. [21] Bahjat Kawar, Shiran Zada, Oran Lang, Omer Tov, Huiwen Chang, Tali Dekel, Inbar Mosseri, and Michal Irani. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60076017, 2023. 1, 2, 5, 7 [22] Wonkyun Kim, Changin Choi, Wonseok Lee, and Wonjong Rhee. An image grid can be worth video: Zeroshot video question answering using vlm. arXiv preprint arXiv:2403.18406, 2024. 4, 12 [23] Jeong-gi Kwak, Erqun Dong, Yuhe Jin, Hanseok Ko, Shweta Mahajan, and Kwang Moo Yi. Vivid-1-to-3: Novel view synthesis with video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 67756785, 2024. 2 [24] Black Forest Labs. Flux.1-dev. https://github.com/ black-forest-labs/FLUX.1-dev, 2024. Accessed: 2024-11-14. 4 [25] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with In frozen image encoders and large language models. International conference on machine learning, pages 19730 19742. PMLR, 2023. 6 [38] Yujun Shi, Jun Hao Liew, Hanshu Yan, Vincent YF Tan, and Jiashi Feng. Instadrag: Lightning fast and accurate dragbased image editing emerging from videos. arXiv preprint arXiv:2405.13722, 2024. 3 [39] Uriel Singer, Shelly Sheynin, Adam Polyak, Oron Ashual, Iurii Makarov, Filippos Kokkinos, Naman Goyal, Andrea Vedaldi, Devi Parikh, Justin Johnson, et al. Text-to-4d dynamic scene generation. arXiv preprint arXiv:2301.11280, 2023. 2 [40] Nitish Srivastava, Elman Mansimov, and Ruslan Salakhudinov. Unsupervised learning of video representations using lstms. In International conference on machine learning. PMLR, 2015. 2 [41] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In CVPR, 2018. 2 [42] Carl Vondrick, Hamed Pirsiavash, and Antonio Torralba. Generating videos with scene dynamics. In NeurIPS, 2016. 2 [43] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. [44] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, Jiazheng Xu, Bin Xu, Juanzi Li, Yuxiao Dong, Ming Ding, and Jie Tang. Cogvlm: Visual expert for pretrained language models, 2024. 3 [45] Navve Wasserman, Noam Rotstein, Roy Ganz, and Ron Kimmel. Paint by inpaint: Learning to add image objects by removing them first. arXiv preprint arXiv:2404.18212, 2024. 2 [46] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 4, 5 [47] Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instructionguided image editing. Advances in Neural Information Processing Systems, 36, 2024. 7, 14 [48] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 6 [49] Tianyuan Zhang, Hong-Xing Yu, Rundi Wu, Brandon Feng, Changxi Zheng, Noah Snavely, Jiajun Wu, and William Freeman. Physdreamer: Physics-based interaction with 3d objects via video generation. In European Conference on Computer Vision. Springer, 2025. [26] Yixin Liu, Kai Zhang, Yuan Li, Zhiling Yan, Chujie Gao, Ruoxi Chen, Zhengqing Yuan, Yue Huang, Hanchi Sun, Jianfeng Gao, et al. Sora: review on background, technology, limitations, and opportunities of large vision models. arXiv preprint arXiv:2402.17177, 2024. 2 [27] Grace Luo, Trevor Darrell, Oliver Wang, Dan Goldman, and Aleksander Holynski. Readout guidance: Learning control from diffusion features. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 82178227, 2024. 3 [28] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. 2 [29] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. SDEdit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. 5 [30] Ron Mokady, Amir Hertz, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Null-text inversion for editing real images using guided diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 60386047, 2023. 1, 2 [31] OpenAI. Chatgpt-4o (october 2024 version). https:// chat.openai.com/chat, 2024. 3 [32] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-toIn ACM SIGGRAPH 2023 Conference image translation. Proceedings, pages 111, 2023. 2 [33] Gaurav Parmar, Krishna Kumar Singh, Richard Zhang, Yijun Li, Jingwan Lu, and Jun-Yan Zhu. Zero-shot image-toimage translation. In ACM SIGGRAPH 2023 Conference Proceedings, New York, NY, USA, 2023. Association for Computing Machinery. 5 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 4, 6 [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [36] Noam Rotstein, David Bensaıd, Shaked Brody, Roy Ganz, and Ron Kimmel. Fusecap: Leveraging large language models for enriched fused image captions. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 56895700, 2024. [37] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022."
        },
        {
            "title": "Appendix",
            "content": "A. Additional Editing Examples We provide additional qualitative examples of the editing results on the TEdBench benchmark, generated based on the experimental setup detailed in Section 5.2. These examples further supplement those presented in Figure 1 and Figure 4. Each example is accompanied by the temporal editing caption used to perform the edit, which was generated using the method described in Section 3.1. Figure S7. TEdBench Editing Examples. Figure S8. TEdBench Editing Examples. 11 B. Temporal Editing Captions B.2. Ablation B.1. VLM Instruction As outlined in Section 3.1, we propose framework for automatically generating the temporal editing caption by leveraging the original target editing prompt in conjunction with the source image. The instruction given to the VLM, along with the source image is: Write one-sentence description of short video that begins with the provided image and smoothly transitions into scene of CAPTION, highlighting how elements in the image undergo changes or movement over time. Keep the description simple, concise and short, focusing only on essential changes and actions without altering unnecessary details. Avoid mentioning elements that do not contribute to the main change needed, and focus the description on the main transitions. Do not add objects that are not in the original image or described in the final scene. The camera should remain static unless movement is absolutely necessary. Ensure all transitions happen within few second duration without mentioning the length or using the word video. Here, CAPTION is replaced with the target caption specific to the image. Additionally, as explained, in-context learning is employed to provide the VLM with examples alongside the instruction. Before processing the desired source image and edit prompt, the instruction is presented to the VLM nine times, each paired with distinct example consisting of source image, target caption, and corresponding temporal editing caption. Examples of these are illustrated in Figure S9. Figure S9. In Context Learning Examples. 12 To assess the impact of the Temporal Editing Caption, we conduct an ablation experiment comparing its use against directly using the target editing captions from the TEdBench benchmark. Apart from this modification, we adhere to the same protocols as described in the original experiment in Section 5.2. As shown in Table S4, this setup preserves similar resemblance to the source image but underperforms in terms of image editing performance."
        },
        {
            "title": "Model",
            "content": "Source Target LPIPS CLIP-I CLIP"
        },
        {
            "title": "Original Captions\nTemporal Captions",
            "content": "0.21 0.22 0.89 0.89 0.60 0.63 Table S4. Temporal Editing Captions Ablation. C. Frame Selection C.1. VLM Instruction As detailed in Section 3.3, our method selects the frame that best aligns with the intended edit from each generated video. To automate this process, inspired by [22], we create collage of uniformly sampled frames from the video, along with the source image and target editing caption, and prompt VLM to identify the optimal frame. The model is instructed to select the earliest frame (i.e., with the lowest index) that satisfies the editing intent, minimizing deviation from the original image. In both this process and the best seed selection process (applied across all methods), if none of the edited frames successfully fulfill the desired edit, the original image is retained as the final output. The instruction provided to the VLM is: The image displays the source photo at the top, with collage of 12 edited versions beneath it. The target edit image caption was: CAPTION. Your task is to choose the image from 1 to 12 that best follows this edit fully and naturally. If none of the images follows the edit, select image 0. If multiple images follow the edit equally, prioritize the one with the lowest number possible. Avoid selecting images that appear to follow the edit but are not edits of the original image. Additionally, avoid images where camera motion, zoom, or image quality differs significantly, or where the content does not appear stable relative to the original source. Respond with: The selected edit is:x where is the number of your chosen edit. Here, CAPTION refers to the target editing caption. Examples of the collages can be found in Figure S10. Figure S10. Frame Selection Collage. The target editing caption for this example is: photo of cat yawning.. Figure S11. Flux.1-dev Generations C.2. Ablation To validate the effectiveness of our approach, we compare it to the naive solution of using the last frame of the generated video as the edited output. The evaluation follows the same protocol described in Section 5.2. As can be seen in Table S5, this naive approach results in lower target CLIP score for the edited outputs, highlighting the advantages of our method."
        },
        {
            "title": "Model",
            "content": "Source Target LPIPS CLIP-I CLIP Last Frame Selected Frame 0.24 0.22 0.9 0.89 0.61 0. Table S5. Frame Selection Ablation. D. Editing Manifold Pathway As elaborated in Section 4, to simulate the image manifold, we generated 200 images across three distinct categories using FLUX.1-dev. Examples of these generated images are shown in Figure S11. E. PosEdit In Section 5.3, we outline the construction of human pose editing dataset. This dataset encompasses 58 editing tasks, distributed across 8 distinct action categories, featuring 8 different subjects. The source images consistently depict neutral standing pose with arms relaxed at the sides, while the target poses vary according to the edit category. Each editing category is paired with target caption and temporal caption. Figure S12 and Figure S13 illustrates examples for each action category. Figure S12. PosEdit Examples. 13 Figure S15. Survey Example G. Additional Vision Tasks Captions As outlined in Section 5.5 and demonstrated in Figure 6, we showcase our frameworks applicability for additional, more classic vision tasks that are not typically classified as image editing. For these tasks, we employ Runway Gen3 as our video generator. Empirically, these tasks required longer and more descriptive captions. The temporal editing captions used for each task are as follows: 1. Relighting: The scenes lighting shifts gradually, changing to night. The sun is setting, and artificial lights replace it. The camera is static. Time-lapse. Cinematic. The image expands, adding new sur2. Outpainting: roundings seamlessly beyond the original frame. 3. Denoising: The image clears up as noise fades away, revealing smoother, cleaner details. 4. Debluring: The camera comes into focus, revealing sharp details and enhanced clarity, as though camera lens has adjusted perfectly. Nothing moves. Static image. Figure S13. PosEdit Examples. F. Human Survey As detailed in Section 5.4, we conducted human evaluation survey to assess our methods performance based on real user preferences. Following the framework in [47], the survey questions evaluated (1) the accuracy of the edit relative to the prompt and (2) the quality of the edit, defined as the preservation of visual fidelity to the source image. Each participant reviewed 20 edits, comparing our method with LEDITS++. Examples of the pages shown to the evaluators are provided in Figures S14 and S15. Figure S14. Survey Example"
        }
    ],
    "affiliations": [
        "Technion - Israel Institute of Technology"
    ]
}