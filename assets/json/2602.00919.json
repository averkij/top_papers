{
    "paper_title": "Green-VLA: Staged Vision-Language-Action Model for Generalist Robots",
    "authors": [
        "I. Apanasevich",
        "M. Artemyev",
        "R. Babakyan",
        "P. Fedotova",
        "D. Grankin",
        "E. Kupryashin",
        "A. Misailidi",
        "D. Nerus",
        "A. Nutalapati",
        "G. Sidorov",
        "I. Efremov",
        "M. Gerasyov",
        "D. Pikurov",
        "Y. Senchenko",
        "S. Davidenko",
        "D. Kulikov",
        "M. Sultankin",
        "K. Askarbek",
        "O. Shamanin",
        "D. Statovoy",
        "E. Zalyaev",
        "I. Zorin",
        "A. Letkin",
        "E. Rusakov",
        "A. Silchenko",
        "V. Vorobyov",
        "S. Sobolnikov",
        "A. Postnikov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Green-VLA, a staged Vision-Language-Action (VLA) framework for real-world deployment on the Green humanoid robot while maintaining generalization across diverse embodiments. Green-VLA follows a five stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) reinforcement-learning (RL) policy alignment. We couple a scalable data-processing pipeline (3,000 hours of demonstrations) with temporal alignment and quality filtering, and use a unified, embodiment-aware action interface enabling a single policy to control humanoids, mobile manipulators, and fixed-base arms. At inference, the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance to improve safety and precise target selection. Experiments on Simpler BRIDGE WidowX and CALVIN ABC-D, as well as real-robot evaluations, show strong generalization and performance gains from RL alignment in success rate, robustness, and long-horizon efficiency."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 1 3 ] . [ 1 9 1 9 0 0 . 2 0 6 2 : r Green-VLA: Staged VisionLanguageAction Model for Generalist Robots Manipulation Team, Sber Robotics Center* *A detailed list of contributors in section 7 We introduce Green-VLA, staged VisionLanguageAction framework for real-world deployment on the humanoid Green robot, while maintaining generalization across diverse embodiments. Green-VLA follows five-stage curriculum: (L0) foundational VLMs, (L1) multimodal grounding, (R0) multi-embodiment pretraining, (R1) embodiment-specific adaptation, and (R2) RL-based policy alignment. Progression builds semantic and physical priors, learns shared affordances, and aligns policies for long-horizon execution beyond behavior cloning. At its core is unified data and control stack for robot fleets. scalable data-processing pipeline including DataQA and temporal-alignment filters and synchronizes 3,000 hours of demonstrations; unified, embodiment-aware action interface enables single policy to control humanoids, mobile manipulators, and fixedbase arms; and the VLA controller is enhanced with episode-progress prediction, out-of-distribution detection, and joint-prediction-based guidance module that generalizes to unseen objects. Optimized for the Green humanoid, Green-VLA generalizes in zero-shot manner to new embodiments and achieves state-of-the-art performance across bimanual systems and benchmarks, with RL alignment providing gains in success rate, robustness, and long-horizon efficiency. Sber Robotics Center Date: February 2026 Code: https://github.com/greenvla/GreenVLA Project Page: https://greenvla.github.io/"
        },
        {
            "title": "1 Introduction",
            "content": "VisionLanguageAction (VLA) models have recently emerged as promising foundation for embodied AI, extending the success of large-scale language and vision models into robotics. By conditioning robot control on multimodal context and natural language instructions, VLAs aim to unify perception, reasoning, and action within single end-to-end framework. This paradigm promises generalist robots capable of executing diverse, long-horizon tasks across heterogeneous environments, with the flexibility to adapt to novel objects, goals, and embodiments. Recent works such as ùúã0 [5], Gemini Robotics [35], GR00T N1 [4], and AgiBot GO-1 [1] highlight this trajectory, combining large-scale data aggregation with unified architectures and demonstrating strong progress on manipulation, reasoning, and evaluation benchmarks. Despite this rapid progress, scaling alone does not resolve the core challenges of real-world deployment. First, robotic datasets are inherently heterogeneous in terms of observations, action spaces, and sampling rates. Second, data quality varies drastically across sources, with trajectories suffering from jitter, blurry frames, inconsistent execution, and low scene diversity. Third, the predominant training paradigm remains behavior cloning (BC), which minimizes LBC = E(ùë†,ùëé)D (cid:2)ùúã ùúÉ (ùë†) ùëé2(cid:3), where ùë† denotes the state, ùëé the action, ùúã ùúÉ the policy for robotics control and the demonstration dataset, but this approach quickly saturates and fails to align policies to long-horizon objectives and task-level rewards. These limitations yield brittle models that generalize poorly across embodiments and environments, undermining the promise of scalable robotics foundation models. In parallel, growing body of work has begun to explore explicit reasoning within VLA models (e.g., EO-1 [23], WALL-OSS [40]), showing that integrating high-level decomposition or chain-of-thought reasoning improves long-horizon planning. Yet such approaches often rely on autoregressive reasoning loops that incur significant inference latency, preventing their use in real-time robotic control. In practice, system efficiency depends not only on success rate (SR) but also on throughput, where the time-to-completion of each task compounds over extended workflows. 1 Figure 1: Green-VLA architecture. multimodal visionlanguage model encodes instructions, camera views, and proprioception into tokens that feed flow-matching action expert. high-level task planner decomposes user goals into subtasks, queries the VLA loop, and uses auxiliary signals (episode end, OOD, and JPM-based guidance for precise target points) to ensure safe, instruction-faithful execution across embodiments. We introduce Green-VLA, framework that moves beyond data scaling by emphasizing quality alignment, action unification, and reinforcement learning refinement. Green-VLA unifies over 24M non-robotics, internet-scale multimodal samples and 3,000 hours of humanoid and manipulator demonstrations into normalized action space Aùë¢, achieved through frequency interpolation/extrapolation and explicit control-type prompting. DataQA pipeline automatically evaluates and filters episodes using trajectory smoothness ùêΩ, image sharpness ùëÜ, visual diversity ùê∑, and state variance ùúé2, with trajectory smoothing applied to reduce high-frequency noise. This ensures that scale is complemented by quality, yielding more stable foundation for generalist learning. Green-VLA supports multiple embodiments and control types under the same semantic layout, enabling positive transfer between humanoids, mobile manipulators, and collaborative arms. Combined with staged training recipe-from web-scale VLM pretraining through robotics pretraining, embodiment specialization, and RL refinementthis pipeline turns heterogeneous data into single, consistent policy that can be deployed across diverse real-world robots. While the framework is embodiment-agnostic, our primary target platform is the Green humanoid robot, where we control 32 DoF of the full upper body (head, torso, dual arms, and dexterous hands) through the unified action interface. This setting requires coordinated bimanual manipulation, whole upper-body motion, and fine-grained fingertip control, making it substantially harder than standard single-arm, parallel-gripper benchmarks. Despite this, the same Green-VLA policy successfully handles the full evaluation scopefrom single-arm manipulators and dual-arm platforms to the Green humanoidwithout architectural changes. This demonstrates that unified actions plus staged training are sufficient to bridge from conventional manipulators to high-DoF humanoid control. Training proceeds in five progressive stages: ùêø0 : ùêµùëéùë†ùëí ùëâ ùêøùëÄ ùêø1 : ùëä ùëíùëè ùëÉùëüùëíùë°ùëüùëéùëñùëõ ùëì ùëúùëü ùëÉ‚Ñéùë¶ùë†ùëñùëêùëéùëô ùëä ùëúùëüùëôùëë ùëàùëõùëëùëíùëü ùë†ùë°ùëéùëõùëëùëñùëõùëî ùëÖ0 : ùê∫ùëíùëõùëíùëüùëéùëô ùëÖùëúùëèùëúùë°ùëñùëêùë† ùëÉùëüùëíùë°ùëüùëéùëñùëõ ùëÖ1 : ùê∏ùëöùëèùëúùëëùëñùëöùëíùëõùë° ùëÜùêπùëá ùëÖ2 : ùëÖùêø ùê¥ùëôùëñùëîùëõùëöùëíùëõùë° This recipe first leverages large-scale multimodal data for common-sense grounding, then transfers to unified robot data, adapts to specific embodiments, and finally aligns policies with reinforcement learning objectives to improve long-horizon control. 2 Our contributions are as follows: 1. quality and temporal alignment module for diverse robotics datasets. DataQA pipeline (jitter ùêΩ, sharpness ùëÜ, diversity ùê∑, variance ùúé2) with trajectory smoothing and optical flowbased speed alignment, combined with balanced-to-target sampler ùë§ùëñ (ùõº) and simple speed-conditioned modulation for multi-scale (fast/precise) control. On top of this, joint prediction + guidance module (JPM) improving precise object targeting, especially in visually dense setups such as e-commerce shelves. 2. staged VLA training recipe bridging web-scale priors, robotics data, embodiment specialization, and RL alignment (L0L1R0R1R2), providing clear path from generic multimodal pretraining to real-world robot deployment. 3. Validation of the staged recipe across phases and embodiments. Green-VLA matches or exceeds prior pretrains at R0, is competitive with other VLA models after embodiment tuning, and achieves the largest gains after RL alignment (R2), especially on long-horizon success, recovery, and precise task followingshowing that careful unification, data curation, and guidance matter for high-quality VLA. 4. deployment-ready design for the Green humanoid robot, with unified upper-body control (arms, hands, head, torso) and task-planner integration, while remaining compatible with wide spectrum of other embodiments and standard simulators. Future work. To fully realize the potential of Green-VLA, several extensions are envisioned. First, incorporating multilingual instruction following (English, Russian, and others) will improve inclusivity and data efficiency in global deployments. Second, adding lightweight reasoning module for task decomposition, while preserving low-latency control, can combine the strengths of chain-of-thought planning with real-time execution. Third, integrating embodied memory and trajectory replay may further improve performance in long-horizon household or industrial tasks. Together, these directions highlight path toward practical, scalable, and generalist robotic intelligence."
        },
        {
            "title": "2 Why a Staged VLA Pipeline Matters",
            "content": "Scaling VisionLanguageAction (VLA) models is not only question of parameter count or dataset size. Evidence from recent works ùúã0.5 [6], EO-1 [23], WALL-OSS [40], and Gemini Robotics [34] suggests that strong performance depends on combining complementary data regimes: web-scale multimodal data for semantic and physical common sense, and large-scale robotics action data for grounded control. We argue that staged training is essential to balance generalization, efficiency, and real-world reliability. Base VLM (L0) is the underlying visionlanguage model we start from, already pretrained on large-scale image/videotext data. It has no robot actions yet and lacks the refined visual, physical, multi-view understanding that is crucial for robots; later stages (L1R2) adapt it to real-world execution. Web and multimodal pretraining (L1) builds general reasoning and semantic grounding. Models exposed to internet-scale video and multimodal corpora acquire priors about physics, object affordances, and task structure that cannot be recovered from robot data alone. EO-1 [23] demonstrated that adding L1-like knowledge improves zero-shot generalization to unseen objects and scenes, while WALL-OSS [40] showed that multimodal co-training with chain-of-thought signals benefits reasoning-intensive tasks. In Green-VLA we use multimodal web data for L1 pretraining. General robotics pretraining (R0) captures broad affordance priors: mapping goals, objects, and kinematics to feasible action distributions. Large-scale multi-embodiment data (humanoids, manipulators) encourage models to abstract away embodiment-specific quirks and learn cross-domain invariants. ùúã0.5 showed that cross-source pretraining was crucial for achieving long-horizon execution in unseen homes. Similarly, AgiBot GO-1s latent planner leveraged scale to gain robustness in dual-arm dexterity. In Green-VLA, R0 serves as the core repository of base manipulation skills, maximizing data efficiency across embodiments. Effective tuning (R1) converts capacity into competence. After general pretraining, careful adaptation to target embodiment yields immediate success rate (SR) gains without requiring new large-scale data. Key techniques include: (i) targeted hyperparameter and optimizer search; (ii) architectural adjustments such as embodiment-aware state/action heads; (iii) efficiency improvements for inference (e.g., SDPA attention [18] kernels, reducing denoising steps in flow matching). In WALL-OSS [40], MoE layers [28] and efficient inference design reduced overhead, highlighting that tuning must balance success rate (SR) improvements with real-time deployability. RL alignment (R2) closes the last-mile gap. Behavior cloning saturates quickly in long-horizon and contact-rich manipulation, as it struggles to assign credit over extended action chains. The notorious problem of out-of-distribution (OOD) actions is also difficult to mitigate through additional demonstrations alone, due to the cost of human labor and the complexity of predicting OOD states. Reinforcement learning methods reshape the objective to incorporate task rewards, failure recovery, and preference-like feedback. This stage improves both success rate and average chain length (ACL). To improve performance on hard, dexterous tasks, RL-style fine-tuning is required. In Green-VLA, R2 integrates BC priors with RL alignment, achieving both stability and long-horizon robustness. Figure 2: Green-VLAs robot-specific training stages use visual question answering (VQA) and robotics data and enable robot adaptation and specialization for new embodiments, spatial reasoning, task generalization, dexterous manipulation, and failure recovery. In summary, the staged pipeline matters because each stage addresses distinct bottleneck: L1 enriches semantic grounding, R0 captures affordance priors, R1 adapts efficiently to embodiments, and R2 injects reward-based alignment for real-world robustness."
        },
        {
            "title": "3.1 Data Pipeline",
            "content": "We train Green-VLA in stages, combining web/multimodal grounding with large-scale robotics data. For L1 we use 24M non-robotics, internet-scale multimodal samples to learn general visuallanguage priors with multi-view, physical, and environment understanding. For R0, we pretrain on 184M robotics-domain samples overall > 3,000 hours across humanoids and manipulators from web and our data collection pipeline. Each episode is annotated with language instruction. RGB streams and proprioception are temporally normalized so that comparable physical progress aligns across sources. We have DataQA pipeline that scores and filters trajectories using jitter ùêΩ, diversity ùê∑, sharpness ùëÜ, and state variance ùúé2, discarding low-quality segments. The result is high-quality, embodiment-agnostic corpus that preserves semantic consistency while enabling scalable, unified pretraining. An overall representation of our data collection, filtering and storage pipeline is presented in Figure 5."
        },
        {
            "title": "3.2 Dataset",
            "content": "3.2.1 Multi-modal Web Data For the pretraining (R0) phase, in addition to robotics data we utilized large corpus of web data (L1) (Figure 3), including general VQA, pointing, bounding box prediction, pixel-wise trajectory prediction, multi-view VQA, general captioning and combined pointing with spatial reasoning. more detailed view of the datasets is provided below: 4 RefSpatial [41] comprehensive dataset combining 2D and 3D images, pointing, mutli-view question answering. During training, we map points to PaliGemma special tokens, so the samples combine text tokens with spatial tokens in queries and answers. AgibotWorld [1] large-scale robotics dataset. Based on the Agibot dataset markup, we sample few images from the main camera, then using the sampled indices and the following subtask, we create VQA samples, such as predicting the next affordable subtask, task completion answering and answering which task is currently being executed. Additionally, we project end-effector poses to main camera frames, filter idle points and randomly choose 300K images for pixel-wise trajectory prediction. RoboPoint [39] dataset containing 1432 image-QA instances, including object references, synthetic free space reference, object detection and general-purpose VQA instances. ShareRobot [8] high-quality dataset including task planning, object affordance, and end-effector trajectory in pixel space. The dataset was generated synthetically from OXE data. Robo2VLM [8] synthetic dataset generated using Gemini-2.5-Pro to generate reasoning traces supporting the correct choice. PixMo-Points [11] PixMo-Points is dataset of images paired with referring expressions and points marking the locations the expression refers to in the image. It was collected using human annotators and contains diverse range of points and expressions, with many high-frequency (10+) expressions. MS COCO [16] large-scale object detection, segmentation, and captioning dataset. In our training pipeline, we used only the object detection subset. A-OKVQA [27] crowdsourced dataset composed of diverse set of about 25K questions requiring broad base of commonsense and world knowledge to answer. OpenSpaces [25] is synthetic dataset specifically designed for training vision-language models on spatial visual question answering (VQA), containing approximately 3.4 million question-answer pairs derived from localized narratives and spatial reasoning tasks. Sun RGB-D [32] is benchmark 3D scene understanding dataset comprising 10,335 RGB-D images (combining RGB color with depth information) annotated with 146,617 2D polygons, 64,595 precisely oriented 3D bounding boxes, and complete room layout annotations collected from four different types of RGB-D sensors. We use only 2D subset for pretraining. For consistency between different tasks we select custom weights while sampling the data during pretraining, see Figure 3. 3.2.2 Robotics data For the pretraining stage (R0), we assembled large corpus of data composed of open-source and internally collected datasets, as summarized in Figure 4. AgiBotWorld_twofinger [1] features dual-arm robot with two-finger grippers on the proprietary AgiBot A2D platform. It provides three camera views (hand_left, hand_right, and head) and records both Cartesian and joint states for the arms, as well as platform and torso configurations. The platform supports tilting motions, which enhance its manipulation capabilities. The dataset contains 774 hours of data, 41 unique tasks, and 18 distinct skills, primarily related to household chores and shopping activities. Droid [13] is collected using Franka Panda 7-DoF arm equipped with Robotiq 2F-85 gripper. It uses three cameras: two static diagonal views and one wrist-mounted view. The data is recorded in Cartesian coordinates and Euler angles of the end effector during teleoperation with an Oculus Quest 2. With 49,629 unique tasks, this dataset contains 512 hours of data and 37 distinct skills, offering substantial diversity across real-world household scenarios and abstract object interactions. Galaxea-Open-World-Dataset [33] utilizes mobile dual-arm R1 Lite manipulator with three cameras mounted on the head and wrists. It records only joint states, including platform and torso joints. The dataset comprises 477 hours of data, 9,768 unique tasks, and 85 distinct skills collected across 11 physical domains such as residential, catering, retail, and office environments. Action_net [12] collects joint-state data from humanoid Fourier robots equipped with two types of hands. Data acquisition is performed via teleoperation using Apple Vision Pro, covering 143 hours of data, 1,577 unique tasks, and 16 distinct skills. AgiBotWorld_dexhand [1] features robot with two five-fingered hands and provides both joint and Cartesian states. It includes three cameras (two wrist-mounted and one head-mounted) and contains 82 hours of data with 18 unique 5 Figure 3: Datasets mixture used in L1 training phase. Left: distribution of sample counts across sub-datasets. Right: sampling weight allocation across categories. The data corpus integrates diverse web sources covering spatial reasoning, pointing, robotics-related VQA, and multi-view QA. tasks and set of distinct manipulation skills (e.g., grasping and in-hand manipulation). Fractal [7] is collected on real robot developed by Everyday Robots equipped with two-finger grippers. It employs single camera mounted behind the manipulator at head level and records Cartesian states. The dataset contains 350 hours of data, 598 unique tasks, and 4 distinct skills. Robomind [38] is multi-embodiment dataset integrating data from several robots, including the Franka Emika Panda, the X-humanoid Tienkung, the Agilex cobot, and the UR5e. It uses up to three cameras and primarily features two-finger grippers, except for the humanoid robot, which has five-fingered hands. The dataset includes 33 hours of data, 389 unique tasks, and 45 distinct skills across domestic, industrial, kitchen, office, and retail domains. We used only humanoid subset from this dataset. RDT [19] is acquired using the ALOHA dual-arm robot with two-finger grippers and three cameras positioned at chest level and on both wrists. It records joint states and comprises 60 hours of data, 271 unique tasks, and 36 distinct skills. Bridge [37] is collected in real-world scenarios and involves WidowX 250 6-DoF robot arm with two-finger gripper. The data includes Cartesian coordinates, Euler angles, and up to four camera views, depending on the task. It provides 105 hours of data, 24 environments, 105 unique tasks, and 13 distinct skills. BiPlay [10] consists of 9.7 hours of bimanual data collected with an ALOHA robot. It contains joint states, 326 unique scenes, 2,440 unique tasks, and 48 distinct skills. There are 2 self-collected datasets in our pretraining: Green Humanoid dataset records 32 joint states with three fisheye cameras (two wrist-mounted and one on the head) over 48 hours, encompassing 5,099 unique tasks and 4 skills. ALOHA any_pick dataset is gathered on modified ALOHA Agilex dual-arm platform with differential drive. It captures joint states from two wrist cameras and one platform camera, covering 11.2 hours, 1,852 unique tasks, and two unique skills, namely Pick <object> from the table and Place <object> into the box. To increase the effective amount of humanoid data, we synthetically expand the raw 48 hours of the Green Humanoid 6 Figure 4: Left: Dataset sampling rates used during the R0 phase of GreenVLA training. Right: Number of data samples (frames) per dataset, illustrating relative temporal coverage. The corpus combines large-scale open datasets (e.g., AgibotWorld, DROID, Galaxea) with internally collected humanoid and dexterous-hand data. dataset up to 167 training hours using two structured augmentations. First, we exploit the approximate bilateral symmetry of the robot to build mirrored copy of each episode: wrist-camera streams are horizontally flipped and swapped (left right), the head camera is horizontally flipped, and joint trajectories are transformed by swapping left/right limbs and negating the relevant yaw/roll components of the torso, neck, and arms so that mirrored states remain kinematically valid. Task texts are updated by swapping left/right mentions. Second, we generate time-reversed demonstrations, but only for episodes whose tasks correspond to physically reversible interactions. Concretely, we select episodes whose task templates indicate reversible skills (e.g., pick, hand-over, moving an object between hands, take from the hand), and exclude inherently irreversible ones (e.g., placing with possible drop or irreversible release), so that the terminal state of the original trajectory is valid initial condition for the reversed one. For selected episode with image streams ùêºùë° , robot states ùë†ùë° , and actions ùëéùë° for ùë° = 0, . . . , ùëá 1, we construct reversed episode by reordering the sequence in time, ùêº ùë°+1. The corresponding language instructions are updated to match the reversed intent (e.g., pick <object> from the table place <object> on the table). These mirrored and time-reversed trajectories are then added back to the training mixture, yielding 167 hours of effective humanoid data from 48 hours of real-world recordings. ùë° = ùë†ùëá 1ùë° , and reassigning actions ùëé ùë° drives the system from ùë† ùë° so that each ùëé ùë° = ùêºùëá 1ùë° , ùë† ùë° to ùë† Together, these datasets provide extensive real-world data with rich multimodal inputs and diverse task sets, offering valuable resources for developing robust bimanual manipulation policies in the R0."
        },
        {
            "title": "3.3 Data Quality Assurance",
            "content": "To ensure high-quality training data, we employed the DataQA pipeline for robotic dataset curation and quality assessment. This pipeline enabled us to focus model training on high-quality demonstrations, improving both sample efficiency and final policy performance. We applied multi-stage filtering process to remove problematic episodes from our training dataset. The filtering criteria include: Missing cameras and missing frames. Too short and suspiciously long episodes. Motion activity thresholds to ensure meaningful robot movements. We identify erratic motions using tremble score: ùëÜtremble = (cid:164)ùë†smooth (cid:164)ùë† (cid:164)ùë†smooth + (cid:164)ùë† , where (cid:164)ùë† is the velocity trajectory, and (cid:164)ùë†smooth is its Gaussian-smoothed version. To estimate image sharpness, we first detect local regions with sharp boundaries using Laplacian std score in blocks of size 4 4 pixels. These local boundary scores are max-pooled with kernel of size 16 16 to estimate which regions of 64 64 pixels are sharp and which are blurry/low-textured, i.e., do not contain sharp boundaries. The median of these region sharpness scores is considered as the episode sharpness score: ùëÜsharp = median(MaxPool(stdblock(2ùêº))). Gripper open/closed action pattern validation. It is task-specific filter. For example pattern open-closed-open is expected for pick-and-place tasks. 7 Figure 5: Overview of the data pipeline for robot learning. Data collection and processing loop integrating robot-side teleoperation, cloud-based data verification, open-source dataset mining, and model training. The pipeline supports iterative model updates via RL fine-tuning and feedback from real-robot deployments. Beyond filtering, we evaluated dataset diversity and quality using quantitative metrics to weigh datasets in the training mixture. Visual diversity was quantified through analysis of DINOv3 [30] features ùëì as ùê∑vis = Eùëë [stdùë° (Eùë† [ ùëìùë° ,ùë†,ùëë])]. Here, features ùëì are averaged over spatial locations ùë†, then standard deviation is computed over time ùë°, and averaged across feature dimensions ùëë. State-space diversity was measured as the Frobenius norm of the covariance matrix of robot states in the dataset: ùê∑state = tr(Cov(ùë†)), where ùë† represents the concatenated proprioceptive signals. Dataset quality metrics and sampling probabilities are presented in Table 1. Outlier ùê∑state values arise from state spaces that vary in their units across embodiments. AgiBot twofinger received the highest sampling weight as it is largest (700+ hours), and is the most visually diverse among humanoid datasets. Among ALOHA datasets, Galaxea is the most visually diverse and the largest, so it received the highest weight. Galaxea contains the base movement which results in higher ùê∑state than other ALOHA datasets. Among single-arm datasets, DROID has the highest weight as its trajectories are smoother, scenes and tasks are more diverse, and it is larger than other datasets. Table 1: Dataset quality metrics and sampling probabilities. Dataset #Episodes Hours ùê∑vis ùê∑state RoboMind AgiBot dexhand AgiBot twofinger ActionNet Green Humanoid (ours) ALOHA any_pick (ours) BiPlay RDT Galaxea Bridge DROID Fractal n H L r 1 6K 6K 46K 30K 135K 7K 7K 6K 114K 53K 92K 87K 33 82 774 143 11 31 59 477 105 501 351 0.085 0.128 0.149 0.088 0.119 0.126 0.104 0.162 0.168 0.162 0.140 0.106 1.72 4391.79 3975.09 6.84 2. 2.62 1.63 2.70 37.85 0.69 3.10 0.95 ùëÜsharp 150.00 77.17 83.90 124.41 82.64 99.49 84.59 101.39 37.81 41.44 60.59 60.77 ùëÜtremble Sampling prob. 0.402 0.261 0.286 0.341 0.256 0.268 0.402 0.319 0.398 0.358 0.137 0.399 0.054 0.080 0.210 0.102 0.089 0.025 0.037 0.052 0.124 0.041 0.129 0."
        },
        {
            "title": "4 Green-VLA Model",
            "content": "We first present the architecture of Green-VLA in Figure 1 and describe the R0 and R1 training phases. Conceptually, during R0, we train the model using all available robotics data. During R1, we fine-tune the model in the same manner, but using high-quality dataset tailored to the target embodiment. We then describe the R2 reinforcement learning fine-tuning stage."
        },
        {
            "title": "4.1 Architecture",
            "content": "Green-VLA builds on unified transformer-based architecture that maps rich multimodal context to normalized actions in Aùë¢. visionlanguage encoder first fuses RGB observations, proprioceptive state, and natural language instructions into shared token sequence, augmented with an embodiment/control-type prompt ùëê that specifies the active effectors and action parameterization. On top of this representation, flow-matching action expert predicts unified action chunks in Aùë¢, respecting our semantic slot layout and masks for each robot, while optional guidance steers trajectories toward task-specific targets. We further optimize the architecture for real-time deployment through efficient attention (SDPA kernels), lightweight heads, and reduced denoising steps, enabling single, scalable policy that supports multi-embodiment control with low latency."
        },
        {
            "title": "4.2 Task planner module",
            "content": "Our task planner (Figure 1 left) is high-level GigaVision VLM based on GigaChat [20] that sits on top of Green-VLA and decides when and how the low-level policy should act. The planner first parses voice or text input and classifies whether the query requires physical action. If no action is needed (e.g., question about the environment or system status), high-level VLM is used purely as language model and replies via text or speech. Otherwise, the planner converts the users high-level goal (e.g., set the table for lunch, prepare an order for pickup) into an intermediate, symbolic code: sequence of atomic subtasks such as pick [item] with [left/right] hand, place [item] on [target], give [item]. This code is then translated into structured prompt that conditions Green-VLA. During execution, Green-VLA predicts an episode-end probability for the current subtask, when this score exceeds high threshold (e.g., >0.98), the planner queries feedback module to evaluate whether the subtask is truly complete. If the VLM judges the subtask as successful, it predicts the next subtask in the sequence; if not, it replans the current subtask and updates the prompt accordingly. Importantly, GigaVision is pretrained and kept frozen in all subsequent Green-VLA training; it is only used at inference time to drive this high-level, language-based task decomposition, feedback, and replanning loop."
        },
        {
            "title": "4.3 Unified Action Space for Multi-Embodiment Control",
            "content": "Most current VLA systems do not use large-scale multi-action space pretraining, or reduce it to naive padding of heterogeneous actions into single vector. In practice, this often destroys positive transfer. Consider embodiments ùëí with action spaces Aùëí Rùëëùëí , ùë° , ùëí)}, where ùë•ùëí ùë° Aùëí the corresponding action. common strategy is to embed all ùëéùëí ùë° into padded space = Rùëëmax via ùëéùëí ùë° ), where ùëÉùëí pads actions with zeros up to ùëëmax, and then train single policy ùúã ùúÉ with ùëëùëí varying across robots and control types, and dataset = {(ùë•ùëí ùë° denotes multimodal observations, and ùëéùëí ùë° = ùëÉùëí (ùëéùëí ùë° , ùëéùëí (ùúÉ) = E( ùë•ùëí ùë° ,ùëéùëí ùë° ) (cid:2)ùúã ùúÉ (ùë•ùëí ùë° ) ùëéùëí (cid:3) . ùë° 2 2 This formulation is mis-specified for diverse control spaces: 1. When two embodiments ùëí1, ùëí2 use overlapping coordinates differently, the optimal solution to averages in its native control space, it would still be penalized ùë° on the shared (but semantically different) dimensions. This encourages shortcut learning (e.g., incompatible targets. Even if ùúã ùúÉ were perfect in predicting ùëéùëí1 ùë° for not matching ùëéùëí2 inferring \"which dataset am in?\" and overfitting to spurious cues) instead of learning embodiment-robust structure. 2. In many aggregated datasets, the same robot ùëí is logged under multiple control parameterizations (e.g., joint-space torques, joint positions, Cartesian end-effector deltas, etc.), each mapped into differently. Under L, the policy is simultaneously trained to match incompatible targets for the same embodiment and state distribution. 9 Formally, let ùëöùëí {0, 1}ùëëmax be the indicator of valid dimensions for embodiment ùëí. Then: (cid:105) (ùúÉ) = E(cid:104) ùëöùëí (ùúã ùúÉ (ùë•ùëí (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:123)(cid:122) (cid:124) valid coords ùë° ) ùëéùëí ùë° )2 2 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) + (1 ùëöùëí) (ùúã ùúÉ (ùë•ùëí (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) ùë° ) ùëéùëí ùë° )2 2 (cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32)(cid:32) (cid:125) (cid:124) (cid:123)(cid:122) spurious penalty , where is Hadamard (element-wise) product. The second term is purely an artifact of padding and directly conflicts with cross-embodiment generalization. Moreover, we may have conflicts in the first valid coordinates across embodiments. Unified semantic layout. Green-VLA replaces naive padding with unified action space Aùë¢ R64 with fixed semantic layout (right side of Figure 1) where each index range has consistent physical meaning across all robots that implement it. Here we map native actions (joint, Cartesian, gripper, base, etc.) into the corresponding unified slots with Œ¶ùëí : Aùëí Aùë¢ and use binary mask ùëöùëí {0, 1}64, indicating which slots are used by ùëí. Training then minimizes masked BC objective: Luni (ùúÉ) = E(cid:104)(cid:13) (cid:13)ùëöùëí (cid:0)ùúã ùúÉ (ùë•ùëí ùë° , ùëêùëí) Œ¶ùëí (ùëéùëí ùë° )(cid:1)(cid:13) (cid:13) (cid:105) , 2 2 where ùëêùëí is an embodiment/control-type prompt. Crucially, no loss is applied to (1 ùëöùëí), eliminating spurious gradients from padding and maintaining consistent semantics: for example, the model may predict in joint and Cartesian space and not be penalized for these predictions. This gives our model the ability to learn multi-embodiment semantics, as training in one action space does not destroy semantics for others. Dynamic embodiment and control-type prompting. To make the action mapping explicit and controllable, we condition Green-VLA on structured control prompt ùëêùëí = (cid:0)#arms, #hands, gripper/dex-hand, joint/cartesian, mobile/static, slots used (cid:1), serialized as tokens. The policy thus predicts: ÀÜùë¢ùë° = ùúã ùúÉ (ùë•ùëí ùë° , ùëêùëí) Aùë¢, and the downstream controller of robot ùëí applies the inverse map Œ¶1 ùëí (cid:0)ùëöùëí ÀÜùë¢ùë° (cid:1). ÀÜùëéùëí ùë° = Œ¶1 ùëí to the active slots: This design (i) preserves identifiability of shared skills and (ii) allows control over which action modalities are produced during inference. To reduce conflicting noise/targets across embodiments on unused slots that introduce unnecessary variance at inference stage for embodiment ùëí, we localize the noise injection: ùúñùëí (0, ùêºùëòùëí ), ùúñ = ùëöùëí embed(ùúñùëí). Retargeting for target humanoid. Beyond unifying action coordinates, Green-VLA also performs explicit retargeting from diverse training robots to chosen target embodiment, such as humanoid with dexterous hands. Intuitively, retargeting means, given source robot (e.g., 7-DoF arm with gripper, or dual-arm platform without fingered hands), we transform its recorded actions into the configuration space of the target humanoid so that doing the same thing looks and feels as similar as possible. Concretely, we align semantically corresponding parts-left arm to left arm, right arm to right arm, gripper open/close to the humanoids dexterous hand grasp and map them into the appropriate slots of our unified action space. When the degrees of freedom differ, we do not naively pad or duplicate; instead, we choose the closest meaningful match (e.g., mapping 6-DoF end-effector pose to the nearest feasible joint configuration) so that the target humanoid can imitate the intent of the source trajectory rather than its exact joint layout. This way, demonstrations from heterogeneous anthropomorphic and non-anthropomorphic robots become usable as-if-humanoid experience, strengthening transfer and ensuring that every added dataset directly enriches the control repertoire of the deployed system. Stabilizing training To stabilize multi-embodiment training with flow matching, we treat dataset-embodiment sampling as scheduled mixture: each dataset ùëñ has target weight ùë§ùëñ (from embodiment relevance, size, and DataQA quality), but during training we sample according to: ùëä (ùë° ) ùëñ = ùë§ ùõºùë° ùëñ (cid:205) ùëó ùë§ ùõºùë° ùëó , ùõºùë° [0, 1], ùõº0 = 0, ùõºùëá = 1, 10 Thus, we start from uniform mixture (ùõº = 0) and gradually converge to the desired biased distribution (ùõº = 1). Intuitively, this prevents early collapse onto few dominant embodiments and lets the model first learn shared structure before specializing. This curriculum is especially important when using high-momentum optimizer (e.g., ùõΩ1 = 0.950.98) with large batch size. The effective update is long-term moving average, so if we begin with heavily skewed ùë§ùëñ, the gradients from large/target datasets quickly dominate the momentum buffer, and rare embodiments are washed out even if their ùë§ùëñ are initially large. This leads to mode forgetting and poor cross-robot transfer. Starting balanced ensures all embodiments contribute to the early representation; as ùõº increases, the momentum tracks smoother, better-conditioned shift toward the target distribution instead of locking onto single mode. Action alignment To further standardize heterogeneous demonstrations, Green-VLA performs action alignment to equalize the varying execution speeds across datasets. Different robots and operators move at different intrinsic speedssome execute trajectories quickly and smoothly, while others move slowly due to task complexity or teleoperation system properties. Mixing such datasets naively confuses the model about the expected magnitude of dynamics. We address this issue by resampling trajectories and interpolating actions using monotonic cubic splines to normalize the effective motion per step. To determine the appropriate resampling rate for each dataset, we estimate its execution speed by computing the mean optical flow magnitude from wrist cameras as proxy for visual motion (See Figure 6). For example, datasets with low capture frequency, like Bridge and Fractal, tend to have large between-frame optical flow, so we densify their actions with additional interpolated waypoints. On the other hand, we speed up datasets with high capture frequency and slow motion like AgiBot DexHand by downsampling actions. This flow-based resampling creates temporally aligned trajectories in the unified action space, where similar visual and geometric changes correspond to similar action increments, making cross-dataset generalization more robust. In practice, we apply this alignment both to open-source datasets and to our own teleoperation data to ensure consistent motion statistics across all sources. Figure 6: Mean optical flow magnitude per dataset used for temporal alignment (higher values correspond to faster apparent motion). Temporal Scale Conditioning for LongShort Horizon Control On top of temporally aligned trajectories, we introduce speed-conditioned augmentation that lets Green-VLA represent both fine-grained manipulation and faster, coarse motions within the same model. For each training sample, we draw scalar action speed factor ùë£ ùëù(ùë£) and use our interpolation / resampling procedure to warp the target trajectory: values ùë£ > 1 effectively slow the sequence down (more intermediate waypoints), while ùë£ < 1 compress it (fewer, coarser steps). This changes the effective temporal resolution of the supervision and augments the target distribution without breaking cross-dataset consistency. Concretely, let ‚Ñéùë° be the hidden state of the action expert. We apply ùë£-conditioned RMS-style modulation: ‚Ñéùë° = RMSNorm(‚Ñéùë° ), ÀÜ‚Ñéùë° = ùõæ(ùë£) ‚Ñéùë° + ùõΩ(ùë£), and predict actions from ÀÜ‚Ñéùë° , where ùõæ(ùë£), ùõΩ(ùë£) are small learned functions of ùë£. In this way, each sample in the batch is trained at different effective speed, and the model explicitly knows which regime it should operate in. Because all datasets have been normalized to common motion scale using optical-flowbased alignment, the meaning of ùë£ is consistent across embodiments: higher values emphasize slow, high-resolution adjustments, while lower values encourage faster progress along the trajectory. At inference time, ùë£ becomes simple hyperparameter that trades off local precision versus long-horizon efficiency, and in future work it can be set automatically by high-level robotics planner. This speed-conditioned modulation effectively teaches Green-VLA to operate on multiple temporal zoom levels, which is exactly what we want for balancing shortand long-horizon capabilities. When ùë£ is low, the model is encouraged to make small, incremental changes between stepscapturing detailed contact dynamics. When ùë£ is large, the same underlying representation is pushed to cover larger progress per step, learning stable shortcuts through easy segments like reaching, lifting, or base motion. At inference time, this translates into controllable continuum: the same Green-VLA can act as careful local controller in delicate phases and as an efficient high-level executor over long horizons, without retraining or separate planners. Now Green-VLA is controllable such that ùë£ is hyperparameter of inference, however, as future work, high-level robotics VLA may control this parameter. Figure 7: Robot state density is modeled with Gaussian Mixture Model (GMM). Actions that would cause the robot to enter an out-of-distribution state with low GMM density are corrected based on the GMM density gradient. Episode progress The flow-matching action expert jointly predicts normalized episode progress signal ÀÜùúåùë° [0, 1], trained from demonstrations with target ùúåùë° = ùë°/ùëá, where ùë° is the current step and ùëá is the episode length. This scalar is exposed to downstream planners, enabling them to request updated instructions or subgoals as long-horizon tasks unfold. OOD detector Inspired by [24], we further equip the architecture with an online out-of-distribution (OOD) detector over the predicted trajectory, modeled as mixture of Gaussians fitted on robot states from the Green-VLA training set: ùëùtrain(ùë†) = ùëò ùúôùëò (ùë† ùúáùëò, Œ£ùëò). At inference, if the predicted actions cause the robot to enter state ùë† such that ùëùtrain (ùë†) < ùúèood, we recompute the actions to instead reach corrected state ùë† + ùõºùëùtrain(ùë†), where ùõº = 0.2 is the gradient step size (see Figure 7). Overall, this module nudges the policy back toward the training-state distribution, improving its safety and stability in long tasks."
        },
        {
            "title": "4.4 Guidance with joint prediction module",
            "content": "In dynamic settings such as e-commerce shelves, new or rarely seen items may appear that are not recognized by VLA, yet are explicitly specified in the instruction (e.g., pick the blue 500ml bottle of X). To handle this, Green-VLA augments its policy with lightweight training-free guidance module. First, joint prediction head takes the current observation and language instruction and predicts target point ùëù in the robots workspace corresponding to the described item. Then, during action generation, we apply flow-matching guidance: instead of sampling from the unconditional velocity field ùë£ ùúÉ (ùë•), we bias the field toward trajectories that move the end-effector toward ùëù Joint prediction module To estimate the target point ùëù required by the guidance mechanism, we propose the Joint Prediction Module (JPM). The key idea is to decompose the manipulation instruction into subtasks and infer goal robot configuration directly from each subtasks textual description. Given an observation image and an instruction specifying which object should be manipulated, JPM first queries specialized Vision-Language Model to predict 2D affordance point (ùë¢, ùë£) on the image via pointing-based mechanism. This point corresponds to the location most relevant for grasping or interaction. We then lift this 2D affordance into the robots 3D workspace. For pixel (ùë¢, ùë£) with depth ùëë (ùë¢, ùë£), camera intrinsic matrix ùêæ, and camera pose ùëá ùë§ ùëê ùëÜùê∏ (3), the corresponding 3D point in the world (robot-base) frame is (cid:21) (cid:20)ùëù 1 = ùëá ùë§ ùëê (cid:20)ùëë (ùë¢, ùë£) ùêæ 1 [ùë¢, ùë£, 1] 1 (cid:21) . The resulting point ùëù R3 represents the desired target location in the robots workspace and is fully consistent with the scene geometry and the language query. 12 Finally, JPM computes feasible joint configuration ùëû that positions the end-effector at ùëù by solving an inverse kinematics (IK) problem with typical constraints on orientation, reachability, and collision avoidance. The resulting ùëû is then injected into the policy and used during flow-matching guidance to bias the velocity field ùë£ ùúÉ (ùë•) toward trajectories that move the end-effector toward ùëù. Guidance module For conditioning, we adopt pseudoinverse guidance scheme (Œ†GDM; [31]) on top of the learned flow-matching policy. At each denoising step, we modify the velocity field ùë£ with an additional guidance term that nudges the trajectory toward desired target value ùëå (e.g., the predicted grasp point or object-specific action). Concretely, Œ†GDM estimates how changes in the current noisy action Aùúè ùë° affect the final clean action and injects gradient-based correction in that direction, yielding guided field vŒ†GDM that preserves the prior while steering the generation so that the final output is consistent with the inferred target constraint."
        },
        {
            "title": "4.5 RL fine-tuning (R2)",
            "content": "A model that has completed the full L0L1R0R1 training cycle is further fine-tuned using reinforcement learning. We employ two approaches: Trajectory optimization with native fine-tuning inspired by [21], and Optimization of source distribution [36]. Trajectory optimization with native fine-tuning small separate critic estimating stateaction pair value (the Q-function) is trained on the R1 dataset with sparse rewards. We use Implicit Q-Learning [15] for its stability, elegant mitigation of the Q-value overestimation problem, and smooth offline-to-online transition. The critic is trained according to the following objectives: ùêøùëâ (ùúì) = E(ùë†,ùëé)ùê∑off (cid:2)ùêø ùúè 2 (ùëÑ ÀÜùúÉ (ùë†, ùëé) ùëâùúì (ùë†))(cid:3), (cid:2)(ùëü (ùë†, ùëé) + ùõæùëâùúì (ùë†) ùëÑ ùúÉ (ùë†, ùëé))2(cid:3) . ùêøùëÑ (ùúÉ) = E(ùë†,ùëé,ùë† )ùê∑off The parameter ùúè (0, 1) is the expectile of some random variable ùëã defined as solution to the asymmetric least squares problem (ùë¢) = ùúè 1(ùë¢ < 0)ùë¢2. arg min ùëöùúè where ùêø ùúè 2 Eùë•ùëã [ùêø ùúè 2 (ùë• ùëö ùúè)], Then, the base model that has undergone R1 fine-tuning generates trajectories in the environmentthis may be either simulator environment such as CALVIN [22] or Simpler, or real-robot setup. The recorded trajectories are improved using the trained Q-function: at each step of the trajectory, the gradient of the Q-function with respect to the action, ùëéùëÑ(ùë†, ùëé), is computed. This gradient is normalized and added to the original action generated by the base model at the current step, scaled by multiplier chosen as hyperparameter (which, in certain sense, is analogous to learning rate): ùëé ùëé + ùúÇ ùëéùëÑ(ùë†, ùëé) ùëéùëÑ(ùë†, ùëé) , The gradient computation and update are performed times (another hyperparameter), resulting in an optimized trajectory. Since the trained Q-function is not optimal, and the hyperparameters are tuned heuristically, the resulting action is not guaranteed to improve the trajectory (we consider an improvement to be reduction in task execution time) and may even worsen the outcome. To prevent adding low-quality data to the dataset, we perform validation of the optimized trajectories in the environment: the operator restores the environment to the state corresponding to the beginning of the original trajectory rollout, after which the improved trajectory is executed and the result is saved. The enriched data are added to the R1 dataset, after which native R1 fine-tuning is repeated starting from the weights obtained at the end of phase R0. The advantage of trajectory optimization with native fine-tuning is that we do not need to modify the weights of the base model using gradients obtained from executing any RL algorithm directly. Because the base models we use are primarily flow-matching models, traditional RL fine-tuning would face several challenges. First, on-policy PG methods like PPO or GRPO require estimating the log-probabilities of generated actions. There are different approaches to handle this, however, all of them have some limitations affecting either the wall-clock training time, training stability, or representational capability of model. Off-policy methods are not an easy alternative either, as they require backpropagating value gradients through the iterative action generation process which may be unstable and demand meticulous tuning. 13 Figure 8: Phase R2: RL alignment. Optimization of the source noise distribution: an actor ùúã ùúÉùëõùëúùëñùë†ùëí (ùúñ ùë†) learns to sample noise that improves the flow-matching policys actions. PARL-style trajectory optimization: experience and teleoperation data are iteratively refined with Q-function gradients and added to train set. Optimization of source distribution After improving model performance with optimized trajectories, we apply the second RL fine-tuning approach, which also has the advantage of preserving the base model weights from being altered directly. Since flow-matching model generates actions based on vectors sampled from some source distribution, e.g. Gaussian, the generation result depends on the initialization of these vectors or, more formally, on the distribution from which they are sampled. The goal of flow-matching is training velocity field neural network with some set of parameters ùúÉ that determines time-dependent flow, defined as dùë° ùúìùë° (ùë•) = ùë¢ ùúÉ (ùúìùë° (ùë•), ùë°), where ùúìùë° (ùë•) := ùúì(ùë°, ùë•) and ùúì0 (ùë•) = ùë•. Given this vector field, samples from the target distribution ùëã1 = ùúì1 (ùëã0), with ùëã0 ùëù0, can be obtained by solving the corresponding ODE [17]. The R1 phase of training uses isotropic Gaussian distribution as ùëù0 - the source of random vectors; accordingly, our base model learns transformation from this distribution to the action distribution in the R1 dataset. Using an actorcritic algorithm, we train small separate actor network that generates the random noise fed into the base model, which maximizes the return obtained by the base model in the environment. In other words, the actor provides new distribution ùëù 0 which purpose is improving the base model behavior. This is an online algorithm requiring RL-guided trajectory execution in the environment. However, compared to traditional online RL algorithms, it is safer in terms of controlled exploration: the actions generated by the base model tend to remain close to those in the training dataset, even if the distribution of random noise shifts."
        },
        {
            "title": "5 Experiment Metrics Across Phases",
            "content": "We evaluate Green-VLA across staged training phases (R0, R1, R2) and heterogeneous embodiments, focusing on task-following, long-horizon robustness, and efficiency. Overall, Green-VLA is mid-scale 4B-parameter VLA model. We use PaliGemma (3B) as the visionlanguage backbone, and couple it with dedicated flow-matching action expert and lightweight heads that account for the remaining parameters. Our architecture cleanly separates semantic perception from high-frequency control by attaching the action expert on top of the frozen or lightly tuned PaliGemma trunk. For the R0 robotics pretraining stage, we train Green-VLA for over 105 optimization steps on cluster of 48 H100 GPUs, which is sufficient to fully saturate the unified multi-embodiment robotic dataset while remaining substantially lighter in both data and compute than prior very large VLA systems. First, in the R0 phase, we benchmark table-cleaning, pick-and-place tasks on the AgileX Magic Cobot [2], measuring success rate (SR) and execution efficiency (time-to-clear and actions-to-clear) against ùúã0, GR00T N1, AgiBot GO-1, and WALL-OSS. Importantly, Green-VLA is trained on 3,000 hours of unified demonstrationssubstantially less 14 (a) Setup for \"pick tape\" task. (b) Setup for \"pick pliers\" task. (c) Cleaning table setup Figure 9: a), b) Examples of task-following setup. c) Example of full table cleaning setup Table 2: ALOHA table-cleaning task Policy ùúã0 GR00T N1 WALL-OSS AgiBot GO-1 Green-VLA(R0) Tape Screwdrivers Pliers First item SR AVG Time 46.3 38.9 27.4 57.8 83.1 29.7 35.4 14.2 48.6 52. 31.8 29.5 27.3 33.2 63.7 35.6 33.2 12.1 38.4 69.5 2m59s >5m >5m 3m57s 1m35s than the > 10,000 hours of data used in ùúã0allowing us to quantify the benefits of quality alignment and unified actions under constrained data. Second, we report R0 performance on standardized open benchmarks derived from WidowX and Google-robot, comparing SR against Flower, RT-1X, ùúã0, OpenVLA and the Dexbotic [9] version of MemoryVLA [29] to assess cross-embodiment generalization under identical protocols. Third, in the R1 phase, we fine-tune Green-VLA on the CALVIN environment using only our generic R0-pretrained checkpoint (i.e., without ever pretraining on CALVIN data), and measure multitask SR and compositional generalization isolating the effect of embodiment-specific adaptation. Bimanual cleaning table setup. For manipulation on Cobot Magic, we evaluate instruction-conditioned picking and table-cleaning in controlled, reproducible environment shared across ùúã0, WALL-OSS, AgiBot GO-1, GR00T N1, and Green-VLA from R0 phase only, without any fine-tuning. We additionally fine-tune all the models used as baselines for 20,000 iterations on dataset containing ALOHA trajectories for the table-cleaning task. In the single-target setting, we sample 4 to 6 objects on the table and provide natural-language instruction of the form \"Pick the [specified item] and place it in the box.\" For each candidate object, we run 10 episodes with randomized placements and distractors (identical across methods), see Figure 9, and report the fraction of trials where the correct item is grasped and successfully dropped into the box Table 2. In the full-task setting, we place 15 to 20 objects on the table and issue sequential instructions for multiple items and run the evaluation 10 times. We measure (i) first-attempt correctnessthe fraction of instructions where the first pick matches the requested itemand (ii) the mean time (or action steps) required to completely clear all instructed items into the box. This protocol jointly captures task following, object disambiguation under clutter, and execution efficiency, enabling direct comparison of our R0-stage Green-VLA against prior VLA baselines. We can see that Green-VLA achieves higher task-following accuracy and SR than ùúã0. Unexpectedly, we find that multi-embodiment pretraining yields strong results for almost all datasets in the training set, and the model does not require additional tuning for specific embodiment on the same data. Simpler. For the Simpler benchmarks on WidowX and Google Robot, we compare our R0-pretrained Green-VLA against range of existing foundation policies, including ùúã0 (R0 and SFT-fine-tuned), OpenVLA [14], Flower [26], and RT-1X (on RT-1 data). Our R0 model outperforms other methods at the same pure-pretrain stage and achieves performance comparable to several fine-tuned baselines. We run all experiments with the default Simpler horizon (80-200 steps for Google Robot and 80 steps for WidowX) and use our episode-end prediction head (Green-VLA EEP) to terminate execution once the task is judged complete. This is particularly important for Google Robot tasks: unnecessary motion after the goal is reached can easily turn successful configuration into failed one, mirroring real-world deployments where the robot should complete the task and then wait for new instruction instead of fidgeting in the scene. For Simpler, we report the mean result aggregated across 7 evaluation. On WidowX, we perform three-stage comparison using the same architecture: (i) R0 pretraining on the unfiltered BRIDGE mixture, (ii) R1-style SFT on filtered BRIDGE subset, and (iii) R2 RL fine-tuning starting from the filtered R1 checkpoint. We observe consistent increase in success rate across these stages, illustrating how dataset curation Table 3: SimplerEnv evaluation across different policies on Google Robot tasks for default number of Simpler episode steps. We report models that were pretrained on mixture-data including Rt-1 dataset or fine-tuned on it. Model ùúã0 (Fine-tune) OpenVLA RT-1-X Flower Green-VLA(R0) Green-VLA(R1) Green-VLA(R2) Visual Matching Variant Aggregation #Overall Drawer Move near Pick Coke AVG VM Drawer Move near Pick Coke AVG VA Average 38.3 35.6 59.7 - 62.9 47.0 61.0 65.3 46.2 31.7 - 61.2 58.7 50.8 72.7 16.3 56.7 - 90.4 95.0 98.1 58.8 27.7 53.4 - 71.4 66.9 69.9 25.6 17.7 49.0 33.5 34.1 51.6 63.7 47.7 32.3 38.1 42.9 71. 75.2 54.5 29.7 75.5 92.1 98.2 54.8 39.8 39.6 49.1 56.3 73.7 56.8 33.8 46.5 42.4 60.2 61.3 71.8 plus RL alignment builds on the unified pretrain to progressively improve performance on concrete target embodiment. For Flower, OpenVLA, and RT-1X, we report the officially published Simpler results from the FLOWER paper [26] and the OpenVLA report [3], rather than re-implementing these baselines. We evaluate DB-MemVLA on WidowX tasks under the same conditions as Green-VLA for 7 runs and aggregate results using default episode lengths of 80 steps. Table 4: SimplerEnv evaluation across different policies on WidowX Robot tasks. We report the results for model that was pretrained on mixture-data including the Bridge dataset. Model ùúã0 (Fine-tune) OpenVLA RT-1-X Flower DB-MemVLA Green-VLA (R0) Green-VLA (R1) Green-VLA (R2) Pick Success Spoon Cubes Eggplant Carrot AVG Grasp Spoon Cubes Eggplant Carrot AVG Success 45.8 4.1 16.7 91.7 66.7 75.0 87.5 50.0 12.5 8.3 83.3 91.7 91.7 95.8 91.6 8.3 0.0 79.2 91.7 87.5 91.7 25.0 33.0 20.8 100 50.0 50.0 91.6 53.1 14.5 11.5 88.6 75.0 76.1 91.7 29.1 0.0 0.0 71.0 85.1 33.3 66.7 90. 16.7 0.0 0.0 8.0 57.6 33.3 37.5 52.6 62.5 4.1 0.0 88.0 100 88.5 79.2 84.8 0.0 0.0 4.2 13.0 50.0 25.0 37.5 89.0 27.1 1.0 1.1 45.0 73.2 45.0 55.2 79."
        },
        {
            "title": "5.1 R1: Efficient Fine-Tuning",
            "content": "Guidance with JPM To comprehensively test the JPM with guidance system, we created an e-commerce store environment where the robot receives commands to pick an item from the shelf and place it in the shopping cart. Here, it is crucial to select the correct item exactly as specified in the instructions, so we conducted testing on in-domain and out-of-domain data with new items. We report metrics in Figure 11. Figure 10: First, we ground the affordance in 2D, and then, by lifting it to 3D, we initialize the target for Green-VLA guidance. 16 Notably, this e-commerce task is hard for end-to-end VLM policies: they must disambiguate near-identical products (e.g., orange vs. pineapple juice, different apple varieties) where the only reliable cues are small label text, subtle branding, or fine-grained color accents, and packaging often changes, creating OOD variants and near-duplicates. Our JPM predicts language-conditioned target point, and the guidance module steers flow matching toward that target, yielding significant success rate boost while preserving policy flexibility. We evaluate shelf picking under three recognition regimes using the same scene layouts for all policies. In in domain (ID)Coarse, the instruction specifies only brand/category (e.g., J7 juice), and any variant in that class is counted as correct; this measures basic category following under clutter. Figure 11: E-commerce shelf picking: top-1 success rate (%) for Green-VLA with and without JPM guidance. Columns report In-DomainCoarse (brand/category), In-DomainSKU (exact variant), and Out-of-Domain (unseen SKUs/packaging). Higher is better. In In-D omain Stock Keeping Unit IDSKU (Fine), the instruction targets an exact variant/SKU (e.g., J7 orange 0.5 L), and success requires grasping and placing that precise itempenalizing near-misses like the wrong flavor or size. In out-of-distribution (OOD) evaluation, we introduce unseen SKUs/packaging (new flavors, sizes, or rebranded labels) and assess generalization. For each regime, we randomize object poses and distractors, issue language instructions, and report first-pick correctness (top-1), task success rate (grasp place), and time-to-completion; mispicks and regrasps are recorded to diagnose failure modes. Humanoid For humanoid evaluation on our Green robot, we focus on instruction-conditioned pick-and-place ensuring robustness to out-of-distribution scene layouts. The humanoid is tasked with picking diverse items primarily packaged food, the SberBoom smart speaker, and the SberRing smart ring and sorting them into one of three target baskets according to natural-language commands. Figure 12: Quantitative evaluation of the humanoid policy. The figure summarizes performance across instruction-conditioned manipulation tasks, including pick, place, handover, fruit sorting, and full table cleaning. The final group shows average success across all tasks, for both in-domain and out-of-domain settings. The evaluation includes both leftand right-hand picking, correct basket selection, arm-to-arm handover when required for reachability, and handing objects directly to user upon request. We randomize object positions, distractor items, and background clutter across episodes to test stability under unseen configurations, and measure success by exact task following: using the instructed arm, selecting the correct object, executing the correct placement or handover, and maintaining safe, reliable behavior in OOD setups. We show several representative tasks captured from the robots onboard and external cameras, while the control policy operates the full upper body, including the head, arms, and torso. 17 Figure 13: Humanoid executing task-planned pick-and-place sequence. The high-level task planner decomposes the users request (sort apples and oranges into the basket) into subtasks such as pick the small green apple with your left hand, pick the large orange with your right hand, and place the big apple in the target basket. Green-VLA then executes each subtask, coordinating arm choice, precise placement, full upper-body control, and task-following to complete the full sorting task. In the main humanoid scenario, we evaluate the system along several core capabilities. First, we test whether the system reliably picks the correct item, in both in-distribution and out-of-distribution scenes, and report average results. For the humanoid scenario we evaluate set of instruction-conditioned capabilities that together characterize full-table interaction. Each episode is controlled by specifying which hand to use and providing explicit object and basket references. Concretely, we measure: (i) Pick success for commands of the form pick [item] from the [table/highground] with [left/right] hand; (ii) Place success for place [item] into the [basket] with [left/right] hand; (iii) Basket retrieval for pick [item] from the [basket] with [left/right] hand; (iv) Hand-over to user for give [item] to user; (v) Fruit sorting, where the robot must pick individual fruits from central basket and place them into the correct baskets based on language instructions; (vi) Average chain length (ACL), defined as the number of primitive actions in these composite sequences (pick place re-pick give, up to length 4), which captures how fast the robot completes multi-step tasks; and (vii) Table cleaning time, the average time to clear all relevant objects via repeated pickplace subtasks, serving as an overall efficiency metric that reflects how quickly the system completes long-horizon workflows that require repeated pickplace subtasks. Taken together, these metrics show that the humanoid system can robustly follow detailed language instructions, coordinate both arms, and complete complex sorting and cleaning tasks in direct and time-efficient manner, even under OOD scene layouts. 18 (a) Success rates for e-commerce items after R1 and R2 training stages. (b) CALVIN benchmark: ACL for ùúã0, Flower and Green-VLA across R1/R2. Figure 14: Comparison of Green-VLA performance in e-commerce setup and CALVIN ABCD benchmark."
        },
        {
            "title": "5.2 R2: RL Alignment",
            "content": "We evaluate the effectiveness of the R2 RL alignment phase on the Simpler BRIDGE WidowX and CALVIN ABCD benchmarks, along with pick-from-shelf tests in our e-commerce environment. On the CALVIN benchmark, we compare three models: (i) ùúã0 fine-tuned on CALVIN, (ii) Green-VLA after R1 embodiment fine-tuning on CALVIN, and (iii) Green-VLA after R2 RL alignment. We find that Green-VLA R1 and ùúã0 achieve comparable performance, with R1 slightly ahead on aggregate success rate and multi-step task chains. The largest gains come from R2 (see Figure 14b): RL alignment markedly improves long-horizon consistency, error recovery, and compositional task successraising average chain length (ACL). Overall, reward-shaped refinement helps overcome BC saturation and delivers substantive performance improvement over both ùúã0 fine-tuning and Green-VLA R1. For Green-VLA, we do not use the unified action space for this benchmark. We apply the same RL alignment procedure in the Simpler BRIDGE WidowX environments and report success rates for the Green-VLA R2 checkpoint in Table 4. R2 alignment improves the R1 models success rate by an absolute 24%, demonstrating high effectiveness of RL fine-tuning. Finally, we show results of R2 fine-tuning for the pick and place into tote task in our e-commerce environment in Figure 14a. While JPM and guidance modules are primarily focused on task following in this setup, R2 RL fine-tuning targets physically reliable grasping of challenging objects. We evaluate on set of items with difficult shapes, textures, sizes, and weights (e.g., cookies, shampoo), deformable packaging (pet food), and medium-difficulty rigid items (e.g., deodorant). R2 optimization of success-conditioned rewards and penalty-free recoveries leads to more accurate approach trajectories, improved contact geometry, and fewer drops or slips."
        },
        {
            "title": "6 Conclusion",
            "content": "We presented Green-VLA, staged visionlanguageaction framework that moves beyond raw scale toward quality alignment, action unification, and reinforcement learning fine-tuning. At the data level, our DataQA pipeline filters and smooths heterogeneous demonstrations and aligns temporal dynamics via optical flowbased resampling. At the policy level, unified action space with embodiment prompts resolves cross-robot inconsistencies and enables positive transfer. At the training level, target-balanced sampling schedule stabilizes multi-embodiment flow matching, while conservative RL fine-tuning boosts performance on difficult, long-horizon tasks requiring advanced dexterity. Finally, at inference, efficiency optimizations and guidance enable low-latency, instruction-following controleven for novel, language-specified items. Empirically, Green-VLA demonstrates strong pretrain-stage performance on Simpler and CALVIN, outperforming prior foundation policies at comparable stages and approaching fine-tuned baselines. On real robots, we observe successful application on bimanual setups, and reliable humanoid behavior under OOD layouts. With the R2 RL alignment phase, Green-VLA achieves state-of-the-art results on the Simpler BRIDGE WidowX setup and competitive, near-state-of-the-art performance on CALVIN ABCD. While promising, Green-VLAs performance still depends on retargeting fidelity, residual dataset bias, and adequate 19 coverage of dexterous skills. Future work will extend multilingual instruction following, strengthen the coupling between fast reasoning and real-time control, and integrate online data collection with safety-aware RL to further reduce failure modes. Overall, Green-VLA offers practical recipefrom web-scale grounding to unified robotics pretraining, embodiment adaptation, and RL alignmentfor building generalist, responsive, and reliable robot policies."
        },
        {
            "title": "7 Contributors and Acknowledgments",
            "content": "Contributors*: VLA: I. Apanasevich, M. Artemyev, R. Babakyan, P. Fedotova, D. Grankin, E. Kupryashin, A. Misailidi, D. Nerus, A. Nutalapati, G. Sidorov RL fine-tune: I. Efremov, M. Gerasyov, D. Pikurov, Y. Senchenko Data pipeline: S. Davidenko, D. Kulikov, M. Sultankin Control: K. Askarbek, O. Shamanin, D. Statovoy, E. Zalyaev, I. Zorin Data collection: A. Letkin, E. Rusakov, A. Silchenko, V. Vorobyov Benchmarks: .Sobolnikov Project supervisor: A. Postnikov *Authors are listed in alphabetical order."
        },
        {
            "title": "References",
            "content": "[1] AgiBot-World-Contributors, Q. Bu, J. Cai, L. Chen, X. Cui, Y. Ding, S. Feng, et al. Agibot world colosseo: large-scale manipulation platform for scalable and intelligent embodied systems, 2025. [2] AgileX Robotics. Cobot magic. https://global.agilex.ai/products/cobot-magic, 2024. Mobile ALOHA-based dual-arm mobile manipulator platform. [3] Anonymous. Discrete diffusion vla: Bringing discrete diffusion to action decoding in visionlanguageaction policies. https://openreview.net/attachment?id=YWeNCMxdhM&name=pdf, 2026. Under review as conference paper at ICLR 2026. [4] J. Bjorck, F. Casta√±eda, N. Cherniadev, X. Da, R. Ding, L. J. Fan, et al. Gr00t n1: An open foundation model for generalist humanoid robots, 2025. [5] K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter, et al. ùúã0: vision-language-action flow model for general robot control, 2024. [6] K. Black et al. ùúã0.5: vision-language-action model with open-world generalization, 2025. [7] A. Brohan, N. Brown, J. Carbajal, Y. Chebotar, X. Chen, K. Choromanski, T. Ding, D. Driess, A. Dubey, C. Finn, P. Florence, C. Fu, M. G. Arenas, K. Gopalakrishnan, K. Han, K. Hausman, A. Herzog, J. Hsu, B. Ichter, A. Irpan, N. Joshi, R. Julian, D. Kalashnikov, Y. Kuang, I. Leal, L. Lee, T.-W. E. Lee, S. Levine, Y. Lu, H. Michalewski, I. Mordatch, K. Pertsch, K. Rao, K. Reymann, M. Ryoo, G. Salazar, P. Sanketi, P. Sermanet, J. Singh, A. Singh, R. Soricut, H. Tran, V. Vanhoucke, Q. Vuong, A. Wahid, S. Welker, P. Wohlhart, J. Wu, F. Xia, T. Xiao, P. Xu, S. Xu, T. Yu, and B. Zitkovich. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In arXiv preprint arXiv:2307.15818, 2023. [8] K. Chen, S. Xie, Z. Ma, P. R. Sanketi, and K. Goldberg. Robo2vlm: Visual question answering from large-scale in-the-wild robot manipulation datasets. arXiv preprint arXiv:2505.15517, 2025. [9] D. Contributors. Dexbotic: Open-source vision-language-action toolbox, 2025. [10] S. Dasari, O. Mees, S. Zhao, M. K. Srirama, and S. Levine. The ingredients for robotic diffusion transformers. In Proceedings of the IEEE International Conference on Robotics and Automation (ICRA), Atlanta, USA, 2025. [11] M. Deitke, C. Clark, S. Lee, R. Tripathi, Y. Yang, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146, 2024. [12] Y. M. Fourier ActionNet Team. Actionnet: dataset for dexterous bimanual manipulation. 2025. [13] A. Khazatsky, K. Pertsch, S. Nair, A. Balakrishna, S. Dasari, S. Karamcheti, S. Nasiriany, M. K. Srirama, L. Y. Chen, K. Ellis, P. D. Fagan, J. Hejna, M. Itkina, M. Lepert, Y. J. Ma, P. T. Miller, J. Wu, S. Belkhale, S. Dass, 20 H. Ha, A. Jain, A. Lee, Y. Lee, M. Memmel, S. Park, I. Radosavovic, K. Wang, A. Zhan, K. Black, C. Chi, K. B. Hatch, S. Lin, J. Lu, J. Mercat, A. Rehman, P. R. Sanketi, A. Sharma, C. Simpson, Q. Vuong, H. R. Walke, B. Wulfe, T. Xiao, J. H. Yang, A. Yavary, T. Z. Zhao, C. Agia, R. Baijal, M. G. Castro, D. Chen, Q. Chen, T. Chung, J. Drake, E. P. Foster, J. Gao, V. Guizilini, D. A. Herrera, M. Heo, K. Hsu, J. Hu, M. Z. Irshad, D. Jackson, C. Le, Y. Li, K. Lin, R. Lin, Z. Ma, A. Maddukuri, S. Mirchandani, D. Morton, T. Nguyen, A. ONeill, R. Scalise, D. Seale, V. Son, S. Tian, E. Tran, A. E. Wang, Y. Wu, A. Xie, J. Yang, P. Yin, Y. Zhang, O. Bastani, G. Berseth, J. Bohg, K. Goldberg, A. Gupta, A. Gupta, D. Jayaraman, J. J. Lim, J. Malik, R. Mart√≠n-Mart√≠n, S. Ramamoorthy, D. Sadigh, S. Song, J. Wu, M. C. Yip, Y. Zhu, T. Kollar, S. Levine, and C. Finn. Droid: large-scale in-the-wild robot manipulation dataset. 2024. [14] M. J. Kim, K. Pertsch, S. Karamcheti, T. Xiao, A. Balakrishna, S. Nair, R. Rafailov, E. Foster, G. Lam, P. Sanketi, Q. Vuong, T. Kollar, B. Burchfiel, R. Tedrake, D. Sadigh, S. Levine, P. Liang, and C. Finn. Openvla: An open-source vision-language-action model, 2024. [15] I. Kostrikov, A. Nair, and S. Levine. Offline reinforcement learning with implicit q-learning, 2021. [16] T.-Y. Lin, M. Maire, S. Belongie, L. Bourdev, R. Girshick, J. Hays, P. Perona, D. Ramanan, C. L. Zitnick, and P. Doll√°r. Microsoft COCO: Common objects in context. In European Conference on Computer Vision (ECCV), 2014. [17] Y. Lipman, M. Havasi, P. Holderrieth, N. Shaul, M. Le, B. Karrer, R. T. Q. Chen, D. Lopez-Paz, H. Ben-Hamu, and I. Gat. Flow matching guide and code, 2024. [18] E. Litman. Scaled-dot-product attention as one-sided entropic optimal transport, 2025. URL https://arxiv. org/abs/2508.08369. [19] S. Liu, L. Wu, B. Li, H. Tan, H. Chen, Z. Wang, K. Xu, H. Su, and J. Zhu. Rdt-1b: diffusion foundation model for bimanual manipulation. arXiv preprint arXiv:2410.07864, 2024. [20] V. Mamedov, E. Kosarev, G. Leleytner, I. Shchuckin, V. Berezovskiy, D. Smirnov, D. Kozlov, S. Averkiev, L. Ivan, A. Proshunin, et al. Gigachat family: Efficient russian language modeling through mixture of experts architecture. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 93106, 2025. [21] M. S. Mark, T. Gao, G. G. Sampaio, M. K. Srirama, A. Sharma, C. Finn, and A. Kumar. Policy agnostic rl: Offline rl and online rl fine-tuning of any class and backbone, 2024. [22] O. Mees, L. Hermann, E. Rosete-Beas, and W. Burgard. Calvin: benchmark for language-conditioned policy learning for long-horizon robot manipulation tasks. IEEE Robotics and Automation Letters (RA-L), 7(3):73277334, 2022. [23] D. Qu et al. Eo-1: Interleaved vision-text-action pretraining for general robot control, 2025. [24] A. Reichlin, G. L. Marchetti, H. Yin, A. Ghadirzadeh, and D. Kragic. Back to the manifold: Recovering from out-of-distribution states. In 2022 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 86608666, 2022. doi: 10.1109/IROS47612.2022.9981315. [25] RemyxAI. Openspaces: Synthetic spatial vqa dataset. https://huggingface.co/datasets/remyxai/ OpenSpaces, 2024. Spatial VQA data generated with VQASynth. [26] M. Reuss, H. Zhou, M. R√ºhle, √ñ. E. Yaƒümurlu, F. Otto, and R. Lioutikov. Flower: Democratizing generalist robot policies with efficient vision-language-action flow policies, 2025. [27] D. Schwenk, A. Khandelwal, C. Clark, K. Marino, and R. Mottaghi. A-okvqa: benchmark for visual question answering using world knowledge. arXiv preprint arXiv:2206.01718, 2022. [28] N. Shazeer, A. Mirhoseini, K. Maziarz, A. Davis, Q. Le, G. Hinton, and J. Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer, 2017. URL https://arxiv.org/abs/1701.06538. [29] H. Shi, B. Xie, Y. Liu, L. Sun, F. Liu, T. Wang, E. Zhou, H. Fan, X. Zhang, and G. Huang. Memoryvla: Perceptual-cognitive memory in vision-language-action models for robotic manipulation, 2025. URL https: //arxiv.org/abs/2508.19236. [30] O. Sim√©oni, H. V. Vo, M. Seitzer, F. Baldassarre, M. Oquab, C. Jose, V. Khalidov, M. Szafraniec, S. Yi, M. Ramamonjisoa, F. Massa, D. Haziza, L. Wehrstedt, J. Wang, T. Darcet, T. Moutakanni, L. Sentana, C. Roberts, A. Vedaldi, J. Tolan, J. Brandt, C. Couprie, J. Mairal, H. J√©gou, P. Labatut, and P. Bojanowski. DINOv3, 2025. URL https://arxiv.org/abs/2508.10104. [31] J. Song, A. Vahdat, M. Mardani, and J. Kautz. Pseudoinverse-guided diffusion models for inverse problems, 2023. In International Conference on Learning Representations. [32] S. Song, S. P. Lichtenberg, and J. Xiao. SUN RGB-D: RGB-D scene understanding benchmark suite. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2015. [33] G. Team. Galaxea g0: Open-world dataset and dual-system vla model. arXiv preprint arXiv:2509.00576v1, 2025. [34] G. R. Team et al. Gemini robotics 1.5: Pushing the frontier of generalist robots with advanced embodied reasoning, thinking, and motion transfer, 2025. [35] G. R. Team et al. Gemini robotics 1.5: Pushing the frontier of generalist robots with advanced embodied reasoning, thinking, and motion transfer, 2025. [36] A. Wagenmaker, M. Nakamoto, Y. Zhang, S. Park, W. Yagoub, A. Nagabandi, A. Gupta, and S. Levine. Steering your diffusion policy with latent space reinforcement learning, 2025. [37] H. Walke, K. Black, A. Lee, M. J. Kim, M. Du, C. Zheng, T. Zhao, P. Hansen-Estruch, Q. Vuong, A. He, V. Myers, K. Fang, C. Finn, and S. Levine. Bridgedata v2: dataset for robot learning at scale. In Conference on Robot Learning (CoRL), 2023. [38] K. Wu, C. Hou, J. Liu, Z. Che, X. Ju, Z. Yang, M. Li, Y. Zhao, Z. Xu, G. Yang, et al. Robomind: Benchmark on multi-embodiment intelligence normative data for robot manipulation. In Robotics: Science and Systems (RSS) 2025. Robotics: Science and Systems Foundation, 2025. URL https://www.roboticsproceedings.org/ rss21/p152.pdf. [39] W. Yuan, J. Duan, V. Blukis, W. Pumacay, R. Krishna, A. Murali, A. Mousavian, and D. Fox. Robopoint: vision-language model for spatial affordance prediction for robotics. arXiv preprint arXiv:2406.10721, 2024. [40] A. Zhai et al. Igniting vlms toward the embodied space, 2025. [41] E. Zhou, J. An, C. Chi, Y. Han, S. Rong, C. Zhang, P. Wang, Z. Wang, T. Huang, L. Sheng, et al. Roborefer: Towards spatial referring with reasoning in vision-language models for robotics. arXiv preprint arXiv:2506.04308, 2025."
        }
    ],
    "affiliations": [
        "Sber Robotics Center"
    ]
}