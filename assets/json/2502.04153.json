{
    "paper_title": "UltraIF: Advancing Instruction Following from the Wild",
    "authors": [
        "Kaikai An",
        "Li Sheng",
        "Ganqu Cui",
        "Shuzheng Si",
        "Ning Ding",
        "Yu Cheng",
        "Baobao Chang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we propose a simple and scalable approach UltraIF for building LLMs that can follow complex instructions with open-source data. UltraIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints. Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions. This prompt composer allows us to synthesize complicated instructions as well as filter responses with evaluation questions. In our experiment, for the first time, we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instruction-following benchmarks without any benchmark information, using only 8B model as response generator and evaluator. The aligned model also achieved competitive scores on other benchmarks. Moreover, we also show that UltraIF could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating broader use cases for the method. Our code will be available at https://github.com/kkk-an/UltraIF."
        },
        {
            "title": "Start",
            "content": "ULTRAIF: Advancing Instruction Following from the Wild Kaikai An 1 2 * Li Sheng 1 3 * Ganqu Cui 1 Shuzheng Si 3 Ning Ding 3 Yu Cheng 1 Baobao Chang"
        },
        {
            "title": "Abstract",
            "content": "Instruction-following made modern large language models (LLMs) helpful assistants. However, the key to taming LLMs on complex instructions remains mysterious, for that there are huge gaps between models trained by open-source community and those trained by leading companies. To bridge the gap, we propose simple and scalable approach ULTRAIF for building LLMs that can follow complex instructions with open-source data. ULTRAIF first decomposes real-world user prompts into simpler queries, constraints, and corresponding evaluation questions for the constraints. Then, we train an UltraComposer to compose constraint-associated prompts with evaluation questions. This prompt composer allows us to synthesize complicated instructions as well as filter responses with evaluation questions. In our experiment, for the first time, we successfully align LLaMA-3.1-8B-Base to catch up with its instruct version on 5 instructionfollowing benchmarks without any benchmark information, using only 8B model as response generator and evaluator. The aligned model also achieved competitive scores on other benchmarks. Moreover, we also show that ULTRAIF could further improve LLaMA-3.1-8B-Instruct through self-alignment, motivating broader use cases for the method. Our code will be available at https: //github.com/kkk-an/UltraIF. 5 2 0 2 6 ] . [ 1 3 5 1 4 0 . 2 0 5 2 : r 1. Introduction Large language models (Meta, 2024; OpenAI, 2024) have demonstrated remarkable capabilities, especially in following complex instructions. While modeling such ability is crucial, the technical details and the instruction datasets used in state-of-the-art LLMs remain mysterious. For example, LLaMA3 (Meta, 2024) reportedly leverages instructionfollowing data at the tens of millions scale but has not been *Equal contribution 1Shanghai AI Lab 2Peking University 3Tsinghua University. Correspondence to: Ganqu Cui <cuiganqu@pjlab.org.cn>, Baobao Chang <chbb@pku.edu.cn>. Figure 1. Instruction-following performance comparison of ULTRAIF against baselines on IFEval and MultiIF. open-sourced. This lack of transparency has resulted in significant gap between the research community and the leading companies. Recent efforts in aligning LLMs to follow instructions have focused on creating high-quality instruction-following data. On the one hand, Wei et al. (2021); Rajani et al. (2023); Jiang et al. (2023) involve human annotators in developing instructions and manually crafting corresponding responses. While effective, these methods are label-intensive, heavily reliant on human expertise, and face challenges in scalability and cost efficiency. On the other hand, Xu et al. (2023); Wang et al. (2023); Sun et al. (2024a); Dong et al. (2024) attempt to leverage LLMs to automatically construct highquality instruction data. Specifically, Xu et al. (2023); Sun et al. (2024a) guide LLMs to generate constraints and evolve initial instructions into more complex forms. However, these LLMs-driven methods heavily rely on models instructionevolving capability and overemphasize instruction complexity, ultimately hindering the diversity of evolved instructions and the correctness of generated responses. To improve this, Wang et al. (2023); Dong et al. (2024) introduce handcrafted constraints inspired by human priors to guide LLMs. For instance, Dong et al. (2024) introduces constraints that can be verified by code execution to ensure response correctness. However, these handcrafted constraints introduce rigidity, 1 ULTRAIF leading to homogeneous instructions and making it narrow in encompassing more complex or diverse instructions (e.g., write in Shakespeares tone). As result, scaling such instruction with correct responses remains significant challenge, limiting the applicability of modeling the distribution of instructions from real-world users. In this paper, we propose ULTRAIF, simple and scalable method which synthesizes high-quality instructionfollowing data. The core idea of ULTRAIF is to decompose real-world user instructions for both constraints and evaluation questions, then train composer model to synthesize diverse and complex instructions with verification questions. To achieve this, we first utilize LLM to decompose human instructions into simplified instructions and their associated constraints. For each constraint, the LLM further generates the corresponding evaluation question to verify whether the upcoming response meets the requirement. With these components, we train UltraComposer, which takes the simplified instruction as input and outputs the original instruction along with its evaluation question. In this way, the composer learns to evolve instructions with verifiable constraints, and benefits from the generalization ability of LLMs rather than handcrafted rules. With the composer, ULTRAIF could make any instruction more complicated to synthesize large-scale and diverse dataset. The evaluation questions further help with quality control in rejection sampling and preference learning (Rafailov et al., 2024; Chen et al., 2024). Through comprehensive experiments, we demonstrate that ULTRAIF significantly enhances the instruction-following capabilities of LLMs with high scalability and cost efficiency. Our evaluation, conducted on the LLaMA-3.1-8B model across five instruction-following datasets, confirms ULTRAIFs strong alignment with general instructions. Notably, as shown in Figure 1, by scaling up the training data, we achieve milestone, optimizing the LLaMA-3.1-8BBase model to match the instruction-following ability of its instruct version. Additionally, we assess the generability of ULTRAIF by evaluating it on mathematical, reasoning, coding, and general conversation domains. Furthermore, we explore the potential of self-alignment in ULTRAIF by further optimizing the LLaMA-3.1-8B-Instruct model, and achieve substantial improvement. The main contributions of our paper are as follows: We introduce ULTRAIF, simple and scalable approach that leverages real-world user instructions to train composer model, UltraComposer, enabling the synthesis of complex and diverse instructions with correct responses. Our experiments demonstrate the strong performance of ULTRAIF in handling complex instructions, surpassing all baselines under the same data budget while retaining general capabilities in domains such as mathematics, coding, and conversational tasks. We reach new milestone by optimizing the LLaMA3.1-8B-Base model to match the instruction-following abilities of its Instruct counterpart with only 200k data, and showcase the self-alignment potential by further optimizing the LLaMA-3.1-8B-Instruct model on it own. 2. ULTRAIF 2.1. Overview ULTRAIF synthesizes high-quality instruction-following datasets through two-stage process. As shown in Figure 2, ULTRAIF first constructs the UltraComposer by decomposing user instructions into simplified ones and constraints, along with corresponding evaluation questions, as detailed in 2.2. This specialized composer facilitates the synthesis of instructions with more complex and diverse constraints, while the evaluation questions ensure the correctness and reliability of the generated responses. Then, we introduce the Generate-then-Evaluate process, described in 2.3. This framework first uses UltraComposer to incorporate constraints into instructions and then evaluates the generated responses using corresponding evaluation questions covering various quality levels. Additionally, we introduce our two training strategies in 2.4. 2.2. UltraComposer Previous studies (Xu et al., 2023; Sun et al., 2024a) that rely solely on LLMs are limited by the models instructionevolving ability, which restricts the diversity of synthetic instructions and compromises response accuracy. While Wang et al. (2023); Dong et al. (2024) address response correctness through handcrafted constraints, this approach further limits instruction diversity. In contrast, ULTRAIF focuses on generating diverse, complex instructions with correct responses. To achieve this, we propose the UltraComposer, specialized model to synthesize diverse instructions and generate corresponding evaluation questions. Building this composer model involves three key steps: instruction decomposition, evaluation question generation, and UltraComposer training. We provide the prompts about instruction decomposition and evaluation question generation in Appendix C.2. Instruction Decomposition The decomposition process leverages LLMs to decompose complex instructions into simplified components, such as those sourced from ShareGPT (Chiang et al., 2023). These components consist of set of simplified instructions paired with constraints that represent the underlying requirements of the original instruction. For example, as shown in Figure 2 (a), the instruction (X) In Shakespeares tone, recommend me ten Chinese books. can be decomposed into the simplified instruction (x1) Recommend me ten Chinese books. and the paired constraint (c1) In Shakespeares tone., etc. This step is 2 ULTRAIF Figure 2. The framework of ULTRAIF. Specifically, ULTRAIF begins by training the UltraComposer, which decomposes real-world user instructions and evaluation questions. For (a), the given instruction can be decomposed into several pairs, such as the numeric constraint ten books and content constraint Chinese books. Next, ULTRAIF adopts Generate-then-Evaluate process, where the composer iteratively adds multiple constraints to each collected instruction and then applies the evaluation questions for rejection sampling. essential for disentangling intricate objectives into more structured elements, extending beyond basic format or content constraints (Dong et al., 2024; Wang et al., 2023), and forming foundation to model the distribution of real-world user instructions effectively. (b). In this way, the UltraComposer can directly complicate instructions, streamlining the generation process into single step. Moreover, it enriches the diversity of constraints, extending beyond LLMs intrinsic knowledge to encompass the distribution of real-world scenarios. {(x1, c1, q1), ... , (xn, ci, qi)}, (1) Evaluation Question Generation While Xu et al. (2023); Wang et al. (2023) focus on improving the complexity of instructions, omitting the quality of generated responses often leads to low-quality samples. Inspired by Qin et al. (2024), we utilize LLM to generate an evaluation question for each identified constraint. For the above example, the evaluation question (q1) would be Is the response written in Shakespeares tone?. These questions are designed to be precise, enabling reliable assessment of generated responses for adherence to the constraints. This mechanism not only addresses the limitation of verifying only constraints that can be checked programmatically (Dong et al., 2024) but also enhances the reliability of the generated data and ensures alignment with the intent of the original instruction. UltraComposer Training With the decomposed instructions and evaluation questions, we train our UltraComposer to take simplified query (xi) as input and generate the original instruction (X) with its evaluation question (qi), denoted as Eq. 2. The training process is shown in Figure 2 ltraComposer(xi) (X, qi), (2) 2.3. Generate-then-Evaluate As shown in Figure 2, with UltraComposer, ULTRAIF efficiently generates high-quality instruction-following data through the Generate-then-Evaluate process, supporting both Supervised Fine-tuning and Preference Learning. This process comprises two key components, instruction generation and response evaluation. It begins by collecting user instructions from existing datasets, after which UltraComposer iteratively incorporates multiple constraints into each instruction, and then the corresponding evaluation questions are used to assess the responses generated. Thus, ULTRAIF could make any instruction more complex, facilitating diverse and large-scale instruction-following dataset. Instrucion Generation The UltraComposer adapts the augmentation process fully automated and aligns with human preferences. We start by collecting user instructions from existing datasets (Chiang et al., 2023; Teknium, 2023; 3 ULTRAIF Rajani et al., 2023), and then use the Composer to augment these instructions. As shown in Eq. 3, this process can be conducted iteratively, enabling the generation of more complex and realistic instructions (x) with multiple constraints, paired with corresponding evaluation questions (q). ltraComposer(x(n)) (x(n), q(n)), q(n+1) = q(n+1) q(n) x(n+1) = x(n), (3) Response Evaluation Next, we prompt LLMs to generate responses for each augmented instruction. The quality of the generated responses is then assessed by evaluating them against evaluation questions. This results in dataset Ddata comprising (x, q, ychosen, yrejected). Ideally, this process requires only three to four calls to the LLM, significantly reducing the computational cost. Since the added constraints are aligned with human instructions, ULTRAIF eliminates the need to verify whether the constraints are consistent with the original instructions (Dong et al., 2024; Katz-Samuels et al., 2024). Additionally, the evaluation questions replace the need for separate score-filtering stage. Consequently, ULTRAIF achieves greater efficiency and incurs minimal costs when constraining large-scale datasets compared to previous research (Xu et al., 2023; Dong et al., 2024). 2.4. Training Strategies ULTRAIF offers flexible training strategies for aligning model with instruction following capabilities. To thoroughly evaluate the effectiveness, we provide two approaches: Supervised Finetuing (SFT). Given the dataset Ddata, we apply standard Supervised Finetuning (SFT) objective on vanilla model π with parameters θ, as shown in Eq. 4: LSF (πθ) = (cid:88) log πθ(ycx) (4) (x,yc)Ddata where represents the augmented instruction, and rc denotes the corresponding chosen response. SFT + Iterative Online DPO. As ULTRAIF is equipped with evaluation questions, it facilitates quality control by enabling the generation of pairwise responses with varying quality levels. This property makes it particularly suitable for the application of Direct Perference Optimization (DPO, Rafailov et al. (2024)) to refine the fine-tuned model, πref . The DPO objective is formulated as Eq. 5: LDP O(πθ, πref ) = E(x,yc,yr )Ddata log σ(β ) πθ(ycx) πref (ycx) πθ(yrx) πref (yrx) = (log log ) where β is scaling hyperparameter, σ denotes the sigmoid function, and πθ is initialized from πref and further optimized during the DPO stage. In the context of ULTRAIF, the UltraComposer enables an iterative augmentation of instructions, transitioning from simpler to more complex tasks. This allows the DPO process to be formulated as an iterative curriculum. At each iteration, the model πref is replaced with the latest optimized model from the previous stage. Concurrently, more challenging instruction-following datasets are generated and utilized for further training. This iterative approach ensures continuous improvement in model performance and adaptability across increasingly complex scenarios. Moreover, during the iterative process, as observed by (Chen et al., 2024), the DPO objective primarily focuses on optimizing the margin between the chosen and rejected samples, rather than directly maximizing the probability of chosen samples and minimizing that of the rejected ones. To address this, we employ the Noise Contrastive Estimation (NCA, Chen et al. (2024)) loss in the final iteration, and the objective is defined in Eq. 6: LN CA(πθ, πref ) = E(x,yc,yr )Ddata (cid:20) log σ(β log πθ(ycx) πref (ycx) ) + 1 2 (cid:88) log σ(β log y{yc,yr } πθ(yx) πref (yx) (cid:21) ) (6) 3. Experiments 3.1. Experimental Setup Datasets and Baselines To train UltraComposer, we decompose instructions from ShareGPT (Chiang et al., 2023) and generate corresponding evaluation questions by LLaMA-3.1-70B-Instruct. In our experiments, we collect human instructions from existing open-source datasets, including ShareGPT, OpenHermes2.5, and No Robots (Teknium, 2023; Rajani et al., 2023; Chiang et al., 2023), and employ UltraComposer to complicate instructions and then generate responses. For baselines, we reimplement existing methods using either public datasets (Sun et al., 2024a; Xu et al., 2023) or available implementations (Dong et al., 2024), and include series of currently open and closed-source LLMs. More details are in Appendix A. Experimental Settings We first fine-tune LLaMA-3.18B-Instruct to build our UltraComposer. Subsequently, we explore two settings to implement our training strategies as written in 2.4. (5) Strong-to-Weak. In this setting, knowledge is distilled from larger model to smaller one. For ULTRAIF, we ULTRAIF Table 1. The main results on five instruction-following benchmarks. Pr. and Ins. stand for prompt and instruction levels, respectively. and represent strict and loose metrics for IFEval. For LiveBench, we only report the performance on the subset of instruction-following data. Results marked with are sourced from the original benchmarks, and represents we reimplement the methods. Method #Data IFEval Multi-IF InfoBench LiveBench FollowBench Pr(L) 79.30 74.86 Ins(S) Ins(L) Turn1 Turn2 Turn3 DRFR 83.60 77. 85.40 81.65 81.50 68.54 70.50 59.63 60.90 51.26 GPT-4 LLaMA-3.1-8B-Instruct LLaMA-3.1-8B (ShareGPT) Evol-Instruct (2023) Conifer (2024a) AUTOIF (2024) ULTRAIF + SFT + Iterative DPO ULTRAIF + SFT + Iterative DPO + SFT scale up + Iterative DPO Pr(S) 76.90 69.13 43.99 41.96 46.40 47.13 53.97 58.22 55.82 56.93 69.87 71.35 - - 10k 10k 13k 10k 10k 8k 10k 8k 175k 20k Strong-to-Weak (Supervisor: LLaMA-3.1-70B-Instruct) 54.34 45.66 51.02 56.93 58.59 65.25 54.32 54.44 58.51 57. 64.15 68.11 64.39 58.03 62.59 67.02 68.82 74.22 44.69 39.03 44.91 47.63 52.55 58.14 25.11 24.34 25.83 27. 29.34 35.65 18.50 19.14 17.95 20.53 22.29 26.55 Self-Alignment (Supervisor: LLaMA-3.1-8B-Instruct) 58.78 64.14 72.46 75.42 66.18 66.66 77.46 79. 69.54 73.02 80.22 83.09 55.59 58.63 66.24 69.63 36.72 42.04 53.66 58.28 28.07 31.20 42.19 46.86 89.40 81.33 81.56 75.74 75.73 80. 81.91 83.56 77.78 79.86 79.20 80.70 Score 69.40 57.10 33.20 44.90 45.60 40.50 42.20 49. 46.60 54.20 51.40 56.00 SSR 78.60 63.41 59.59 43.87 52.42 60.41 59.50 59.99 55.88 58.56 59.93 62. leverage LLaMA-3.1-70B-Instruct for response generation and evaluation and then train LLaMA-3.1-8B-Base. Self-Alignment. We replace the supervision model with Llama-3.1-8B-Instruct and then train the Base model. Our experiments are conducted on 8A100 GPUs (80GB) using mixed precision with bf16, DeepSpeed ZeRO Stage 3 (Rasley et al., 2020), and FlashAttention 2 (Dao, 2023). And we choose Xtuner (Contributors, 2023) as our training framework to implement experiments. More details can be found in Appendix C.1. across all datasets. By fine-tuning on our generated data, ULTRAIF achieves substantial improvements, particularly on IFEval and Multi-IF. When compared to strong baselines like AutoIF (Dong et al., 2024), ULTRAIF achieves scores of 53.97 (Pr(S)) and 64.15 (Ins(S)) on IFEval and 81.91 (DRFR) on InfoBench, surpassing AutoIF by margins ranging from 1.29% to 6.84%. These results underscore ULTRAIFs capability to effectively follow instructions, even with lower training data, representing significant advancement over state-of-the-art approaches. Evaluation We evaluate ULTRAIF on five instructionfollowing benchmarks, including IFEval (Zhou et al., 2023), Multi-IF (He et al., 2024), InfoBench (Qin et al., 2024), FollowBench (Jiang et al., 2023), and LiveBench (White et al., 2024). While IFEval and Multi-IF focus on testing verifiable instructions using functions, the others extend to more general instructions that need to be evaluated by LLMs. The details about benchmarks are provided in Appendix B. In addition, we further test the general ability of ULTRAIF such as mathematical (Chen et al., 2021), reasoning (Suzgun et al., 2022), coding (Cobbe et al., 2021), and general interaction capabilities (Li et al., 2024). 3.2. Main Results Table 1 shows the performance of ULTRAIF on five instruction-following benchmarks. ULTRAIF Outperforms All Previous Methods In the Strong-to-Weak setting, ULTRAIF demonstrates performance that is comparable to or exceeds previous methods Iterative DPO Boosts Performance Effectively As shown in Table 1, the iterative DPO process substantially enhances alignment with complex instructions. Specifically, in comparison to SFT, iterative DPO achieves an average improvement of 5% in the Strong-to-Weak setting and 3.8% in the Self-Alignment setting for multi-turn instruction-following tasks. Furthermore, this process enables ULTRAIF to surpass state-of-the-art methods in three benchmarks that require LLM-based evaluation, with an improvement of 1.5% on InfoBench, 4.6% on LiveBench, and 2.62% on FollowBench, demonstrating the importance of ULTRAIF in handling diverse instructions. Smaller Supervisor Yields Better Performance comparison between the self-alignment and strong-to-weak settings reveals that the self-alignment setting, which employs smaller model as supervisor, achieves superior performance. This observation is consistent with the findings of Hui et al. (2024). Notably, it is particularly evident during the SFT stage, where self-alignment outperforms strong-to-weak on Multi-IF and LiveBench. Although the 5 ULTRAIF Table 2. The general performance on mathematical, reasoning, coding, and conversational domains. We report Pass@1 on HumanEval, Acc on BBH and GSM8k, and Win Rate on Arena Hard. Method Code Reasoning Math Conversation General HumanEval LLaMA-3.1-8B-Instruct AutoIF (2024) ULTRAIF + SFT + Iterative DPO + SFT scale up + Iterative DPO 65.24 46.34 43.90 47.56 52.44 55.49 BBH 68.54 67.18 67.33 68.03 67.26 68.44 GSM8k Arena Hard LiveBench [All] 80.80 51.50 48.60 48.10 66.70 68.00 18.30 9.20 12.20 16.00 16.00 31. 25.90 17.50 21.30 21.70 22.80 23.10 Table 3. The performance and its difference compared to the SFT model across each iteration during the Iterative DPO process. Iteration IFEval Multi-IF LiveBench Pr(S) Pr(L) Ins(S) Ins(L) Turn Turn2 Turn3 Score Iter 1 Iter 2 Iter 3 Iter 3w.N CA 55.45+1.48 55.08+1.11 56.75+2.78 58.22+4.25 61.55+2.96 62.66+4.07 63.03+4.44 65.25+6. 65.10+0.95 65.47+1.32 66.79+2.64 68.11+3.96 70.74+1.92 71.82+3.00 72.42+3.60 74.22+5.40 56.13+3.58 57.26+4.71 57.10+4.55 58.14+5.59 32.11+2.77 34.92+5.58 34.87+5.53 35.65+6.31 24.38+2.09 26.28+4.00 26.11+3.82 26.55+4.26 42.20+0.00 47.20+5.00 45.70+3.50 49.50+7. improvements introduced by DPO are relatively minor, selfalignment continues to exhibit superior performance on these two benchmarks, further emphasizing the advantages of leveraging smaller supervisory model. ULTRAIF Achieves New Milestone By scaling up the training data, ULTRAIF achieves new milestone in instruction-following alignment. With 175k data in the SFT stage and 20k data in the DPO stage, ULTRAIF reaches impressive performance, with 71.35 (Pr(S)) and 79.38 (Ins(S)), while the LLaMA-3.1-8B-Instruct model only achieves 69.13 (Pr(S)) and 77.46 (Ins(S)), and comparable across the left benchmarks. This demonstrates that ULTRAIF, when optimized and trained on larger datasets, not only improves instruction-following capabilities but also comes closest to matching the performance of LLaMA-3.1-8B-Instruct, marking significant leap forward in model performance. 3.3. Cross-Domain Validation To verify the generalizability of ULTRAIF, we evaluate ULTRAIF across four general domains, including coding, reasoning, mathematical problem solving, and conversational tasks. Table 2 presents the performance of ULTRAIF compared to AutoIF and LLaMA-3.1-8B-Instruct. While ULTRAIF shows slightly lower performance than AutoIF in the mathematical domain, it achieves significant improvements in coding and conversational tasks. Additionally, scaling the training data leads to remarkable performance gains, and the DPO stage consistently improves results across all domains. Notably, ULTRAIF significantly improves the general capabilities of models, particularly on the comprehensive LiveBench benchmark (White et al., 2024) and the ArenaHard (Li et al., 2024) conversational task. Specifically, it outperforms AutoIF by notable margin of 4.2% on LiveBench, while achieving an impressive 15.4% improvement in conversational performance on ArenaHard. These results suggest that ULTRAIF facilitates the development of more general and versatile models, capable of addressing wide range of tasks effectively. 4. Analysis 4.1. Impact of the Iterative DPO Process Figure 3. Reward trajectories for the chosen and rejected samples across the steps of each iteration during the Iterative DPO process. As UltraComposer allows for iteratively adding constraints to instructions, we introduce the Iterative DPO process during training and gradually increase instruction complexity. Figure 3 illustrates the reward trajectories for the chosen and rejected samples during the DPO process. The reward 6 ULTRAIF margin progressively broadens with the training steps of each iteration. However, by iteration 3, the optimization objective deviates, focusing on maximizing the margin between the chosen and rejected samples rather than enhancing the reward for the chosen samples and minimizing the rejected ones. This shift causes rewards for both samples to fall below zero, resulting in performance degradation on Multi-IF and LiveBench benchmarks, as shown in Table 3. To mitigate this issue, we replace the DPO objective with NCA (Chen et al., 2024), stabilizing the training process and yielding more consistent and optimal results. 4.2. Scalability of ULTRAIF By capturing the distribution of real-world instructions, ULTRAIF facilitates the instruction augmentation process while reducing the risk of inconsistencies between added constraints and the original instructions. Table 4 reports the pass rate during dataset synthesis, where ULTRAIF significantly outperforms AutoIF. This indicates that for generating an equivalent amount of data, ULTRAIF reduces costs by factor of three to five. Furthermore, during the rejection sampling stage, while AutoIF necessitates rigorous function-based filtering for instruction synthesis and response generation, ULTRAIF achieves this with single LLM call, making it far more scalable and cost-efficient. Table 4. The pass rate of data synthesis. Method SFT Pass Rate DPO Pass Rate AutoIF ULTRAIF 20% 85% 26% 60% Figure 4. Scaling the training set on SFT stage. To further assess the scalability of ULTRAIF, we conduct experiments under the self-alignment setting. Figure 4 illustrates the effect of different training data sizes during the SFT stage. With about 175k, ULTRAIF demonstrates powerful performance compared to baselines, highlighting 7 Figure 5. The performance of exploring the potentiality of ULTRAIF on self-alignment. ULTRAIFs capacity to scale effectively with larger datasets. 4.3. Extension of Self Alignment In our main experiments, we distill knowledge from the Instruct version model to enhance the vanilla model, demonstrating the effectiveness of ULTRAIF. However, the potential for ULTRAIF to independently enhance strong model like Cheng et al. (2024) has not yet been explored. In this section, we conduct experiments to investigate the self-improvement capabilities of ULTRAIF. Under the SelfAlignment setting, we use data generated by LLaMA-3.18B-Instruct to enable the model to train itself. As shown in Figure 5, ULTRAIF significantly boosts the performance of the strong model across different size of training data, even without more powerful supervisor. Specifically, ULTRAIF improves performance on IFEval by 2.4%-5.9%, on Multi-IF by 3.74%-5.38%, on LiveBench by 2.5%, and on FollowBench by 2.29%, further validating the effectiveness of our approach. Table 5. The performance comparison of incorporating multi-turn data during the SFT stage. Method Multi-IF InfoBench LiveBench ULTRAIF + SFT w. multi turn 40.12 43.13 +3.01 77.78 79.86 +1. 46.60 54.20 +5.10 4.4. Analysis of Multi-Turn Data Building on prior work that emphasizes enhancing multiturn instruction-following capabilities (Sun et al., 2024b; He et al., 2024), our analysis reveals that incorporating multiturn data during the SFT stage significantly improves ULTRAIFs performance across various benchmarks. As shown in Table 5, the inclusion of multi-turn data results in performance gains of 3.01% on Multi-IF, 1.18% on InfoBench, and 5.10% on LiveBench, compared to the baseline SFT model without such data. These improvements highlight the critical role of multi-turn interactions in training, allowing ULTRAIF ability. Wang et al. (2023); Dong et al. (2024) introduce human priors such as verifiable constraints to ensure the quality of generated responses. However, they restrict the diversity of instructions compared to real-world scenarios. In contrast, ULTRAIF decomposes real-world user instructions for both constraints and evaluation questions, then trains composer model to synthesize diverse and complex instructions with correct responses, presenting an effective mechanism for generating instruction-following data. 5.2. Perference Learning with Instruction Following Perference learning has gained prominence as method to enhance instruction-following capabilities by refining models to generate high-quality responses based on feedback (Ouyang et al., 2022; Dong et al., 2024; Sun et al., 2024a; Gao et al., 2024). In this context, it is typically employed to optimize models that have been finetuned on instruction datasets, thus improving the accuracy and alignment in generating high-quality outputs. common approach leverages reward signals, which may be human-provided or automatically generated, to guide the models learning process. For instance, reinforcement learning from human feedback (RLHF) often employs Proximal Policy Optimization (PPO) to optimize models, but this approach requires ranked responses, which can be resource-intensive and dependent on human labor. Recent work by Rafailov et al. (2024) and Chen et al. (2024) introduces direct optimization of preferences, offering solution to these challenges and reducing the need for extensive human feedback. ULTRAIF enables the generation of evaluation questions to guide preference learning in more structured and efficient manner. It complements direct preference optimization by providing scalable approach to generating instructionfollowing data, reducing costs and enhancing the scalability for instruction-following tasks. 6. Conclusion In this paper, we propose ULTRAIF, scalable and effective approach for synthesizing high-quality instructionfollowing data. By decomposing human instructions into simplified queries, constraints, and corresponding evaluation questions, we train an UltraComposer that enables the efficient generation of constraint-aware instructions. Across two different settings, ULTRAIF demonstrates strong performance across five instruction-following benchmarks. Extensive experiments conducted on LLaMA-3.1-8B-Instruct further highlight ULTRAIFs potential for self-alignment. Most importantly, we are the first to optimize the LLaMA-3.1-8BBase model to match the instruction-following capabilities of its Instruct counterpart, underscoing the effectiveness and potential of our approach. Figure 6. Analysis of the number of added constraints and the evaluation question filter module during the SFT stage. the model to better understand conversational context and dependencies, thereby enhancing its instruction-following capabilities. Therefore, we incorporate multi-turn data in our scaling experiments. 4.5. Ablation Studies on ULTRAIF The iterative augmentation capability of our UltraComposer raises critical question for the SFT stage: should simple or complex instructions be prioritized for training? Figure 6 presents the results of using varying levels of instruction complexity during the SFT stage. The results demonstrate that as instruction complexity increases, performance correspondingly improves, reaching the peak after three iterations with an improvement ranging from 0.73% to 1.66%. Furthermore, we evaluate the effectiveness of our evaluation questions. Without filtering out low-quality responses, performance deteriorates significantly over 3.35%-5.36%. This mechanism becomes increasingly critical as instruction complexity grows, with the performance gap widening alongside the increasing complexity, underscoring the importance of this module in maintaining high-quality training data. 5. Related Work 5.1. Instruction Following Instruction following has become key area in LLMs, with focus on improving the ability to understand and execute complex human instructions. Early approaches (Wei et al., 2021; Rajani et al., 2023; Jiang et al., 2023) have relied heavily on curated datasets involving human-generated instructions and their corresponding responses. Recent work has turned to leveraging LLMs to automate the process. For example, Xu et al. (2023) prompts LLMs to evolve instructions through in-depth or in-breadth transformations. Sun et al. (2024a) directly guides LLMs to complicate them. But relying solely on LLMs leads to the inclusion of low-quality samples, as this is limited by LLMs instruction-evolving 8 ULTRAIF"
        },
        {
            "title": "Impact Statements",
            "content": "This paper introduces ULTRAIF, scalable approach to enhance the instruction-following abilities of LLMs. By utilizing open-source data, it fosters transparency, reproducibility, and improved interpretability. From an ethical perspective, ULTRAIF reduces dependency on proprietary datasets but may influence areas such as misinformation and biased decision-making. ULTRAIF contributes to the democratization of LLM development, enhancing accessibility to advanced models. While potential societal implications exist, none present immediate ethical concerns beyond those commonly addressed in Machine Learning research."
        },
        {
            "title": "References",
            "content": "Chen, H., He, G., Yuan, L., Cui, G., Su, H., and Zhu, J. Noise contrastive alignment of language models with explicit rewards. arXiv preprint arXiv:2402.05369, 2024. Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. D. O., Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman, G., et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Cheng, J., Liu, X., Wang, C., Gu, X., Lu, Y., Zhang, D., Dong, Y., Tang, J., Wang, H., and Huang, M. Spar: Selfplay with tree-search refinement to improve instructionfollowing in large language models. arXiv preprint arXiv:2412.11605, 2024. Chiang, W.-L., Li, Z., Lin, Z., Sheng, Y., Wu, Z., Zhang, H., Zheng, L., Zhuang, S., Zhuang, Y., Gonzalez, J. E., Stoica, I., and Xing, E. P. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality, March 2023. URL https://lmsys.org/blog/ 2023-03-30-vicuna/. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Contributors, X. Xtuner: toolkit for efficiently fine-tuning llm. https://github.com/InternLM/xtuner, 2023. Dao, T. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. W., Zhang, G., Zan, D., Lu, K., Yu, B., Liu, D., Cui, Z., Yang, J., Sha, L., Wang, H., Sui, Z., Wang, P., Liu, T., and Chang, B. Towards unified view of preference learning for large language models: survey. arXiv preprint arXiv:2409.02795, 2024. He, Y., Jin, D., Wang, C., Bi, C., Mandyam, K., Zhang, H., Zhu, C., Li, N., Xu, T., Lv, H., et al. Multi-if: Benchmarking llms on multi-turn and multilingual instructions following. arXiv preprint arXiv:2410.15553, 2024. Hui, T., Zhao, L., Dong, G., Zhang, Y., Zhou, H., and Su, S. Smaller language models are better instruction evolvers. arXiv preprint arXiv:2412.11231, 2024. Jiang, Y., Wang, Y., Zeng, X., Zhong, W., Li, L., Mi, F., Shang, L., Jiang, X., Liu, Q., and Wang, W. Followbench: multi-level fine-grained constraints following benchmark for large language models. arXiv preprint arXiv:2310.20410, 2023. Katz-Samuels, J., Li, Z., Yun, H., Nigam, P., Xu, Y., Petricek, V., Yin, B., and Chilimbi, T. Evolutionary contrastive distillation for language model alignment. arXiv preprint arXiv:2410.07513, 2024. Li, T., Chiang, W.-L., Frick, E., Dunlap, L., Wu, T., Zhu, B., Gonzalez, J. E., and Stoica, I. From crowdsourced data to high-quality benchmarks: Arena-hard and benchbuilder pipeline. arXiv preprint arXiv:2406.11939, 2024. Loshchilov, I. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. Meta. The llama 3 herd of models, 2024. URL https: //arxiv.org/abs/2407.21783. OpenAI. Gpt-4 technical report, 2024. URL https:// arxiv.org/abs/2303.08774. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., Schulman, J., Hilton, J., Kelton, F., Miller, L., Simens, M., Askell, A., Welinder, P., Christiano, P. F., Leike, J., and Lowe, R. Training language models to follow instructions with human feedback. In Koyejo, S., Mohamed, S., Agarwal, A., Belgrave, D., Cho, K., and Oh, A. (eds.), Advances in Neural Information Processing Systems, volume 35, pp. 2773027744. Curran Associates, Inc., 2022. Dong, G., Lu, K., Li, C., Xia, T., Yu, B., Zhou, C., and Zhou, J. Self-play with execution feedback: Improving instruction-following capabilities of large language models. arXiv preprint arXiv:2406.13542, 2024. Qin, Y., Song, K., Hu, Y., Yao, W., Cho, S., Wang, X., Wu, X., Liu, F., Liu, P., and Yu, D. Infobench: Evaluating instruction following ability in large language models. arXiv preprint arXiv:2401.03601, 2024. Gao, B., Song, F., Miao, Y., Cai, Z., Yang, Z., Chen, L., Hu, H., Xu, R., Dong, Q., Zheng, C., Quan, S., Xiao, Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: 9 ULTRAIF White, C., Dooley, S., Roberts, M., Pal, A., Feuer, B., Jain, S., Shwartz-Ziv, R., Jain, N., Saifullah, K., Naidu, S., et al. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314, 2024. Xu, C., Sun, Q., Zheng, K., Geng, X., Zhao, P., Feng, J., Tao, C., and Jiang, D. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244, 2023. Zhou, J., Lu, T., Mishra, S., Brahma, S., Basu, S., Luan, Y., Zhou, D., and Hou, L. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Rajani, N., Tunstall, L., Beeching, E., Lambert, No robots. N., Rush, A. M., https://huggingface.co/datasets/ HuggingFaceH4/no_robots, 2023. and Wolf, T. Rasley, J., Rajbhandari, S., Ruwase, O., and He, Y. Deepspeed: System optimizations enable training deep learning models with over 100 billion parameters. In Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining, pp. 35053506, 2020. Sun, H., Liu, L., Li, J., Wang, F., Dong, B., Lin, R., and Huang, R. Conifer: Improving complex constrained instruction-following ability of large language models. arXiv preprint arXiv:2404.02823, 2024a. Sun, Y., Liu, C., Zhou, K., Huang, J., Song, R., Zhao, X., Zhang, F., Zhang, D., and Gai, K. Parrot: Enhancing multi-turn instruction following for large language models. In Ku, L.-W., Martins, A., and Srikumar, V. (eds.), Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 97299750, Bangkok, Thailand, August 2024b. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.525. URL https: //aclanthology.org/2024.acl-long.525/. Suzgun, M., Scales, N., Schärli, N., Gehrmann, S., Tay, Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi, E. H., Zhou, D., et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Teknium. Openhermes 2.5: An open dataset of llm assistants, 2023. https://huggingface.co/datasets/ synthetic data for generalist URL teknium/OpenHermes-2.5. Wang, Y., Kordi, Y., Mishra, S., Liu, A., Smith, N. A., Khashabi, D., and Hajishirzi, H. Self-instruct: Aligning language models with self-generated instructions. In Rogers, A., Boyd-Graber, J., and Okazaki, N. (eds.), Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 1348413508, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/ 2023.acl-long.754. URL https://aclanthology. org/2023.acl-long.754. Wei, J., Bosma, M., Zhao, V. Y., Guu, K., Yu, A. W., Lester, B., Du, N., Dai, A. M., and Le, Q. V. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. 10 A. Datasets and Baselines ULTRAIF ShareGPT 1 is an open-source and multi-turn conversation dataset that contains over 52K user-shared chatting histories with GPT-4. We decompose the human instructions from ShareGPT into around 200K data pairs to train our UltraComposer. For main experiment, we randomly select 10K human instructions to generate augmented instructions. OpenHermes (Teknium, 2023) is large-scale, diverse, and high-quality compilation consisting of around 1M synthetically generated instruction and chat samples. We randomly select subset of 150K instructions from OpenHermes-2.5 for the scale-up experiment. No Robots (Rajani et al., 2023) is high-quality dataset of 10k instructions and demonstrations created by skilled human annotators. We use all instructions from No Robots for the scale-up experiment. B. Evaluation Benchmarks IFEval (Zhou et al., 2023) is an easy-to-produce benchmark designed to evaluate the instruction-following capability of LLMs. IFEval constructs around 500 prompts that contain 25 types of verifiable instructions. We use both loose and strict accuracy metrics at prompt and instruction levels in our evaluation. Multi-IF (He et al., 2024) is benchmark designed to assess LLMs proficiency in following multi-turn and multilingual instructions. Based on IFEval, Multi-IF contains 4,501 multilingual conversations, where each has three turns. We report the average accuracy across all languages for each of the three rounds in the experiment. InfoBench (Qin et al., 2024) is benchmark comprising 500 diverse instructions and 2,250 decomposed questions across multiple constraint categories, adopting new metric Decomposed Requirements Following Ratio (DRFR) for evaluating LLMs ability to follow instructions. We use GPT-4-1106-preview as the evaluator in our assessments. FollowBench (Jiang et al., 2023) is multi-level fine-grained constraints following benchmark for LLMs. FollowBench incorporates five distinct fine-grained constraint types (Content, Situation, Style, Format, and Example) and underscores multi-level mechanism when building instruction prompts. In our experiment, we prompt the GPT-4-1106-preview to assess whether LLMs outputs have satisfied each individual constraint. LiveBench (White et al., 2024) is LLM benchmark that contains wide variety of challenging tasks (math, coding, reasoning, language, instruction-following, and data analysis) and automatically scores answers according to objective ground-truth values. When assessing the instruction-following skills of LLMs, we employ the instruction-following subset, while the entire dataset is utilized to gauge their overall capabilities. GSM8K (Cobbe et al., 2021) comprises 8.5K high-quality, multilingual grade school math word problems, specifically designed to assess the multi-step mathematical reasoning proficiency of language models. We report the overall accuracy in the experiment. HumanEval (Chen et al., 2021) consists of 164 programming problems with function signatures, docstrings, bodies, and unit tests, averaging 7.7 tests per problem. It is utilized to evaluate the coding abilities of LLMs. HumanEval assesses the capability of LLMs in program synthesis from docstrings, testing language comprehension, reasoning, algorithms, and elementary mathematics skills. We report Pass@1 on HumanEval in the experiment. BBH (Suzgun et al., 2022) is clean, challenging and tractable subset benchmark filtered from Big Bench, which includes 23 types of difficult tasks and 6,511 evaluation examples in total. BBH primarily assesses the models reasoning capacities and problem-solving skills comprehensively. In the experiment we report the accuracy metrics on BBH. Arena Hard (Li et al., 2024) is an automatic LLM benchmark consisting of 500 challenging challenging user queries, which is curated to evaluate the comprehensive performance of LLMs in user dialogue scenarios. In the experiment, we adopt GPT-4-1106-preview as the judge model and report the win rate of our models against the baseline model (GPT-4-0314). 1We use the version from https://huggingface.co/datasets/shibing624/sharegpt_gpt4. 11 C. Experimental Details C.1. Implementation Details ULTRAIF In the SFT stage, we perform full fine-tuning with learning rate of 1e-5. The maximum token length is set to 8192 and variable-length packing is enabled. We use AdamW (Loshchilov, 2017) as the optimizer with warmup ratio of 0.03 and employ LinearLR scheduler at the beginning, transitioning to CosineAnnealingLR towards the end. In the DPO stage, the configuration is similar, with the only difference being lower learning rate of 5e-7. Additionally, the beta parameter of DPO loss is set to 0.1. C.2. Prompts of ULTRAIF To train our UltraComposer, we prompt LLM to perform Instruction Decomposition and Eval Question Generation. We use the following prompt template to decompose human instructions:"
        },
        {
            "title": "Prompt Template of Instruction Decomposition",
            "content": "You are an expert in extracting instruction constraints from given query. Definition of Constraint: The smallest unit of restriction or requirement that the instruction imposes on the task. Query: {query} If the query is not question, or is simple or straightforward without any constraints, please only respond with the following JSON, indicating that no constraints are present. { } \"Complex\": False If constraints are present, follow these steps: 1. Identify the Basic Query: Clearly understand the primary goal of the query, stripping away any constraints. The Basic Query should be the essential task without any added conditions or restrictions. 2. Extract and Categorize Constraints: Identify and classify constraints based on the following types: Content Constraints: * Specific Terms or Symbols: Mandatory use of certain terms or symbols with their exact placement (e.g., must include the word beautiful). * Required Elements or Concepts: Mandates for including specific elements or concepts in responses, reflecting scenario or object (e.g., highlights the Great Wall). * Thematic Directives: Instructions related to thematic content, perspective, or tone, emphasizing response significance (e.g., write poem about London). Numerical Constraints: * Constraints on quantities related to the content, such as the number of points, sentences, paragraphs, response length, or examples (e.g., within single paragraph with three sentences). Stylistic Constraints: * Desired tone and style for the response (e.g., formal, informal, conversational). * Specific language or terminology to be used or avoided (e.g., encyclopedic style). Format Constraints: * Required structure or format for the response (e.g., list, JSON, bullet points, Java language). * Presentation styles or formatting requirements (e.g., electronic medical record format). Linguistic Constraints: * Language use in specific contexts, such as discourse, dialects, sociolects, and language policies (e.g., in English). 12 ULTRAIF * Sentence structure, including phrases, constituents, and the use of imperatives (e.g., with nouns and verbs). * Internal structure of words, including roots, affixes, and morphological changes (e.g., lowercase, single-rhyme). 3. Response Format: Do not consider details that are part of the content itself, such as those used in descriptions, scenarios, or examples, unless they directly impose restriction of the response. The Basic Query should represent the querys core goal, free from any constraints. Ensure that extracted constraints do not overlap with the Basic Query. Present each constraint as dictionary within list, where each dictionary contains: * constraint: The specific restriction or requirement. * simplified query: The query after removing this constraint, polished for coherence and correctness. Exclude any constraint types not present in the query. { } \"Complex\": True, \"Basic Query\": ..., \"Content Constraints\": [ \"constraint\": \"...\", \"simplified query\": \"...\" \"constraint\": \"...\", \"simplified query\": \"...\" { }, { }, ], ... Please only provide the response in JSON format. We use the following prompt template to generate evaluation questions for instructions: Prompt Template of Eval Question Generation You are an expert in crafting questions to evaluate whether response to query adheres to specific constraints. For the given constraint, please design question that human evaluators can use to assess if the response meets the specified constraint. The question should focus solely on the given constraint and not other constraints present in the original query. Specifically, if the given constraint is meaningless or is part of the content itself, such as those used in descriptions, scenarios, or examples, you can respond with an empty string. Query: {query} Constraint: {constraint} Please design question for the specified constraint for the given query, and respond in the JSON format without explanation. { } \"question\": \"string\", 13 ULTRAIF For Generate-then-Evaluate process, we prompt LLM to perform Response Generation and Response Evaluation. First we use the following prompt template to generate responses for the augmented instructions:"
        },
        {
            "title": "Prompt Template of Response Generation",
            "content": "You are an expert tasked with answering the given query. Please provide clear and concise response directly, without introductory phrases such as What great question, Here is the answer, or similar expressions. Focus solely on addressing the query. Now please answer the given query while strictly following its inside constraints. [Query] {query} Then we use the following prompt template to evaluate the quality of those generated responses:"
        },
        {
            "title": "Prompt Template of Response Evaluation",
            "content": "You are an expert that is good at judging whether the response to given query meets the specified evaluator questions. Your task is to carefully examine the response to determine if it adheres to each requirement outlined in the evaluator questions. [Query] {query} [Response] {response} [Evaluator Question] {question} For each question, please provide justification for your evaluation, explaining how the response does or does not satisfy the criteria and score (YES or NO) indicating whether the answer satisfies each constraint. You should only respond in the following JSON format: { } \"Question 1\": { \"explanation\": \"\", \"score\": \"YES\" or \"NO\" }, \"Question 2\": { \"explanation\": \"\", \"score\": \"YES\" or \"NO\" }, C.3. Case Study Table 6 shows some examples of augmented instructions and evaluation questions generated by ULTRAIF. The original queries come from ShareGPT dataset. 14 ULTRAIF Eval Question Is the explanation of Merkle tree in the context of blockchain presented in way that 10-year-old can understand? Does the response evoke dark, eerie, and ominous atmosphere, characteristic of gothic horror? Does the HTML form include form tags for exactly three user inputs: first_name, last_name, and date_of_birth? Does the response recommend activities in Okinawa that focus on nature, food, and local culture? Is the response to the question explained in exactly 5 paragraphs? Is the homepage for the translation business designed using WordPress? Original Query Explain merkle tree in blockchain. Table 6. Examples of ULTRAIFs data pair. Augmented Instruction Explain merkle tree in blockchain to 10 years old. We are driving in car. It is cold outside, windshield is frozen, and we are on secluded road to small village. This is scene from horror movie, and its just starting. Describe scene in great detail. We are driving in car. It is cold outside, windshield is frozen, and we are on secluded road to small village. This is scene from horror movie, and its just starting. Describe scene in great detail, and write it in the style of gothic horror author. Im planning to visit Okinawa Japan from April 7th to April 10th. Do you have any recommendation on what to do while Im there? Design html form with form tags. Design html form with form tags for the following 3 user inputs: first_name, last_name, date_of_birth. Im planning to visit Okinawa Japan from April 7th to April 10th. Do you have any recommendation on what to do while Im there? Id like to focus on nature, food, and local culture. What is the meaning of life? Explain it in 5 paragraphs. Write me homepage for translation business in wordpress. Write homepage for translation business. What is the meaning of life?"
        }
    ],
    "affiliations": [
        "Peking University",
        "Shanghai AI Lab",
        "Tsinghua University"
    ]
}