{
    "paper_title": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following",
    "authors": [
        "Qingyu Ren",
        "Qianyu He",
        "Jingwen Chang",
        "Jie Zeng",
        "Jiaqing Liang",
        "Yanghua Xiao",
        "Han Xia",
        "Zeye Sun",
        "Fei Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose a logic-structured training framework LSRIF that explicitly models instruction logic. We first construct a dataset LSRInstruct with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRIF including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators."
        },
        {
            "title": "Start",
            "content": "LSRIF: Logic-Structured Reinforcement Learning for Instruction Following Qingyu Ren1, Qianyu He1, Jingwen Chang1, Jie Zeng1, Jiaqing Liang2*, Yanghua Xiao1* Han Xia3, Zeye Sun3, Fei Yu3 1Shanghai Key Laboratory of Data Science, College of Computer Science and Artificial Intelligence, Fudan University, 2School of Data Science, Fudan University, 3Ant Group {qyren24,qyhe21,jwchang24, jzeng23}@m.fudan.edu.cn, {liangjiaqing, shawyh}@fudan.edu.cn 6 2 0 2 4 1 ] . [ 2 1 3 4 6 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Instruction-following is critical for large language models, but real-world instructions often contain logical structures such as sequential dependencies and conditional branching. Existing methods typically construct datasets with parallel constraints and optimize average rewards, ignoring logical dependencies and yielding noisy signals. We propose logicstructured training framework LSRIF that explicitly models instruction logic. We first construct dataset LSRINSTRUCT with constraint structures such as parallel, sequential, and conditional types, and then design structure-aware rewarding method LSRM including average aggregation for parallel structures, failure-penalty propagation for sequential structures, and selective rewards for conditional branches. Experiments show LSRIF brings significant improvements in instruction-following (in-domain and out-of-domain) and general reasoning. Analysis reveals that learning with explicit logic structures brings parameter updates in attention layers and sharpens token-level attention to constraints and logical operators."
        },
        {
            "title": "Introduction",
            "content": "Instruction following is core capability of large language models (LLMs) and is essential for their use in real-world applications (Zhang et al., 2025; Lu et al., 2025; Ye et al., 2025). User instructions are often complex and may span multiple turns or agent-based interactions (Qi et al., 2025; Deshpande et al., 2025). Beyond producing fluent text, effective instruction following requires models to correctly understand and satisfy multiple constraints, which are often expressed through structured and interdependent conditions (He et al., 2024; An et al., 2025). In essence, complex instructions are composed of multiple constraints connected by logical struc- * Corresponding author. Figure 1: Essentially, the complex instruction is the logical composition of constraints. tures. Correct instruction following therefore requires not only satisfying individual constraints, but also adhering to the logical relationships between them. As shown in Fig. 1, the complex instruction contains three common types of logical relationships. Parallel (And) structures require all constraints to be satisfied simultaneously. Sequential (FirstThenFinally) structures impose an execution order, where later constraints depend on the successful completion of earlier ones. Conditional (IfElse) structures introduce branching logic, where the model must first evaluate condition and then follow the correct branch. Existing approaches for improving instruction following still face clear limitations when dealing with logically structured instructions. From the perspective of data construction, most training data simplify instructions by treating all constraints as parallel (Sun et al., 2024; Huang et al., 2025). Although some datasets include logical structure, they are mainly used for evaluation rather than training (Wen et al., 2024; Wang et al., 2025). In terms of reward modeling, the reward for the entire instruction is often computed as the average of the rewards for individual constraints (Qin et al., Figure 2: Our framework LSRIF consists of two components: (LSRINSTRUCT) logic-structured dataset construction, and (LSRM) structure-aware reward modeling with corresponding methods. 2025). This assumes that constraints are independent. However, for sequential or conditional instructions, failure at an early step makes later constraints irrelevant, and simple averaging can produce incorrect training signals. Finally, regarding interpretability for performance improvements, prior work typically shows gains in instructionfollowing performance and the preservation of general reasoning abilities (Peng et al., 2025), yet the underlying reasons remain unexplored. Furthermore, it remains unclear whether gains in logically structured instruction following actually transfer to reasoning ability. To address these limitations, we propose logicstructured training framework LSRIF that explicitly models instruction logic in both data construction and reward design. (1)Logic-Structured Data (LSRINSTRUCT). We define instruction structures using three basic logical forms: parallel, sequential, and conditional. Based on these forms, we construct dataset of multi-constraint instructions (2) Logiccovering multiple logical structures. Structured Reward Modeling (LSRM). We design reward modeling methods that reflect the execution semantics of different structures. For parallel structures, rewards are aggregated by averaging. For sequential structures, we apply decay mechanism so that failures in earlier steps reduce rewards for later ones. For conditional structures, rewards are assigned only to the constraints in the correct branch. (3) Interpretability for Performance Improvements. We further analyze how logic-structured training affects the model. We observe larger parameter updates in attention layers than in MLP layers. At the token level, trained models place more attention on logical connectors and constraint-related tokens. These changes also appear in general reasoning tasks, indicating that the learned ability transfers beyond instruction following. Our contributions are summarized as follows: (1) We propose LSRIF, logic-structured training framework. (2) LSRIF includes LSRINSTRUCT, an instruction dataset capturing parallel, sequential, and conditional constraint logic structures, and LSRM, structure-aware reward modeling that aligns reward signals with logical execution semantics. (3) LSRIF improves both in-domain and out-of-domain instruction-following performance and general reasoning ability, with attention and token-level interpretability analysis."
        },
        {
            "title": "2 Related Work",
            "content": "2."
        },
        {
            "title": "Instruction Following Data Construction",
            "content": "Existing work constructs datasets with multiconstraint instructions to improve instructionfollowing capabilities (Qin et al., 2025; Cheng et al., 2024). However, these approaches directly concatenate constraints, ignoring potential structures among them, which fails to simulate realworld user instructions. While some datasets consider logical structures (Wen et al., 2024; Wang et al., 2025), they are primarily designed for evaluation rather than training. In contrast, we construct training dataset where constraints show explicit logical structures."
        },
        {
            "title": "Following",
            "content": "Training paradigms for instruction following have evolved from supervised fine-tuning (Sun et al., 2024) to Direct Preference Optimization (Huang et al., 2025; Qi et al., 2024) and Reinforcement Learning with Verifiable Rewards (RLVR) (Peng et al., 2025; Qin et al., 2025). Existing RLVR methods aggregate constraint-level rewards through simple averaging. However, this averaging strategy fails when constraint logical structures are not parallel (e.g., sequential or conditional). We propose structure-aware reward modeling, where different structures employ distinct reward modeling methods."
        },
        {
            "title": "3 Method",
            "content": "two main comOur approach consists of logic-structured dataset construction ponents: (LSRINSTRUCT) and structure-aware reward modeling (LSRM). As illustrated in Fig. 2, we organize instructions into three logical structuresParallel, Sequential, and Conditional and employ structureaware reward model with three corresponding methods: Average Aggregation for parallel structures, Penalty Propagation for sequential structures, and Branch Selection for conditional structures."
        },
        {
            "title": "3.1 Logic-Structured Dataset Construction",
            "content": "To move beyond flat constraint concatenation, we formalize three logic structure types: Parallel Structure. set of constraints = {c1, c2, . . . , cn} that must all be satisfied simultaneously. This structure corresponds to the flat assumption commonly adopted in prior work, where constraints are treated as independent(e.g., Respond in English and use no commas and limit the length to 100 words). Sequential Structure. An ordered sequence of constraints = (c1, c2, . . . , cn), where each constraint ct is meaningful only if all preceding constraints (c1, . . . , ct1) are successfully satisfied (e.g., First generate an outline, then write summary, finally translate it into English). Logic Type # Inst. # Cons. Types # Cons. Evaluation"
        },
        {
            "title": "17510\nParallel\nSequential\n10435\nConditional 10574",
            "content": "48 25 25 52106 31295 42152 Table 1: Statistics of LSRINSTRUCT. #Inst., #Cons. Types, #Cons. and Evaluation refer to the number of instructions, constraint types, total constraints, and evaluation methods. Conditional Structure. branching structure governed by trigger constraint cp. The active execution branch is determined by whether cp is satisfied: if cp holds, the model must satisfy the true-branch constraint ctrue; else, it must satisfy the false-branch constraint cfalse (e.g., If the input text contains code, explain its functionality; else, summarize the text). We construct the dataset by collecting seed (Li et al., instructions from Infinity-Instruct (Köpf et al., 2024), 2025), Open Assistant Self-Instruct (Wang et al., 2022a) and SuperNatural (Wang et al., 2022b), defining constraint types (hard constraints in Tab. 5, soft constraints in Tab. 6), and using GPT-4.1 to generate multiconstraint instructions that instantiate these logical structures. Each instruction follows logical structure with multiple constraints organized accordingly, enabling controlled analysis and structureaware training. Detailed statistics of LSRINSTRUCT are shown in Tab. 1."
        },
        {
            "title": "3.2 Structure-Aware Reward Modeling",
            "content": "We adopt the Group Relative Policy Optimization (GRPO) (Shao et al., 2024) training, where model optimization is driven by automatically computed signals indicating constraint satisfaction. For hard constraints, we use programmatic verification. For soft constraints, we employ reward model to assess adherence. We train Qwen2.5-7B-Instruct as the reward model, where we exploit the natural partial order in and-type multi-constraint instructions to construct binary preference pairs and train the model via supervised fine-tuning with binary classification objective following (Ren et al., 2025). Given constraint-level verification results, we aggregate these rewards according to the logical structure of each instruction. Formally, let denote model output and denote an atomic constraint. We define binary verification function r(o, c) {0, 1}, where r(o, c) = 1 if output satisfies constraint c, and 0 otherwise. The aggregation of rewards according to logical structures is described as follows. Reward for Parallel Structure (Average Aggregation). For parallel constraint set = {c1, . . . , cn}, we define: Rpar(o, C) = 1 (cid:88) ciC r(o, ci). (1) This coincides with standard RLVR aggregation under flat constraint assumptions. Reward for Sequential Structure (Penalty Propagation). For sequential structure = (c1, . . . , cn), we introduce penalty propagation that discounts downstream rewards when earlier steps fail. The adjusted reward for ci is: i(o, S) = r(o, ci) (cid:89) j<i γ(1r(o,cj )), (2) where γ [0, 1) is decay coefficient. The overall reward is: Rseq(o, S) = 1 S (cid:88) i=1 i(o, S). (3) Reward for Conditional Structure (Branch Selection). For conditional structure with trigger cp and branches ctrue, cfalse: Rcond(o, cp, ctrue, cfalse) = (cid:40) r(o, ctrue), r(o, cfalse), r(o, cp) = 1, r(o, cp) = 0. (4) This ensures optimization focuses exclusively on the logically valid branch."
        },
        {
            "title": "4.1 Set-up",
            "content": "Models. We conduct experiments on models of different scales from 1.5B to 14B to evaluate the effectiveness of our method across different architectures and parameter scales. Specifically, we evaluate on: (1) 1.5B: Qwen2.5-1.5B-Instruct; (2) 7B: Qwen2.5-7B-Instruct and Distill-Qwen-7B; (3) 8B: Llama-3.1-8B-Instruct and Qwen3-8B; (4) 14B: Distill-Qwen-14B. This diverse set of models allows us to assess the generalizability of our approach across different model families and scales. and Baselines. We compare against both strong general-purpose models specialized instruction-following optimized models. Generalpurpose baselines include GPT-4o and QwQ-32B. Specialized instruction-following baselines include RAIF-7B, Self-Supervised-7B, VERIF-8B, SPAR8B-DPO, Conifer-7B-DPO, and Crab-7B-DPO, which are specifically optimized for instruction following tasks using various training paradigms including supervised fine-tuning, self-supervised learning, verification-based reinforcement learning training, and direct preference optimization. Training Methods. We compare three training methods: Base uses the original model directly without any additional training; SFT fine-tunes the model on the dataset generated by the strong model GPT-4.1 using supervised fine-tuning; LSRIF is our logic-structured reinforcement learning training method that employs structure-aware reward modeling to align optimization signals with logical constraint structure execution semantics. For each model scale, we evaluate all three methods to demonstrate the effectiveness of our approach. Evaluation Benchmarks. We evaluate models on both in-domain and out-of-domain instruction following benchmarks. In-domain benchmarks include IFEval (Zhou et al., 2023) (Pr.(L)), CFBench (Zhang et al., 2024) (ISR), and FollowBench (Jiang et al., 2023) (HSR). Out-of-domain benchmarks include ComplexBench (Wen et al., 2024) (Overall), WritingBench (Wu et al., 2025) (Avg.), Collie (Yao et al., 2023) (Avg.), AgentIF (Qi et al., 2025) (CSR), and MultiChallenge (Deshpande et al., 2025) (Overall). Details of the experiment set-up are provided in Appx. A.4."
        },
        {
            "title": "4.2 Performance",
            "content": "Instruction Following Performance. As shown in Tab. 2, LSRIF significantly improves instruction following capabilities across different models on both in-domain and out-of-domain benchmarks. LSRIF consistently outperforms Base and SFT across all model scales, with improvements on various metrics. On in-domain benchmarks, LSRIF achieves substantial gains across all model scales. For smaller models, Qwen2.5-1.5B-Instruct shows remarkable improvements, improving by 25.2 on IFEval and 6.0 on CFBench. For 7B models, Qwen2.5-7BInstruct improves by 5.8 on IFEval and 7.0 on CFBench. For stronger models, Qwen3-8B achieves Models Method IFEval CFBench FollowBench ComplexBench WritingBench Collie AgentIF MultiChallenge In-Domain Out-of-Domain GPT-4o QwQ-32B Self-Supervised-7B VERIF-8B RAIF-7B SPAR-8B-DPO Crab-7B-DPO Conifer-7B-DPO Qwen2.5-1.5B-Instruct Baseline Baseline Baseline Baseline Baseline Baseline Baseline Baseline Base SFT Pr.(L) 84.8 83.9 78.9 87.1 74. 82.4 57.7 52.3 43.6 64.0 ISR 65.3 68.0 52.0 41.0 43.0 37. 25.0 25.0 22.0 24.0 HSR Overall 70.4 62.2 57.5 56.9 56.2 56. 49.4 50.0 34.6 37.4 71.6 73. 68.7 54.7 68.7 63.8 59.0 48. 45.9 49.8 Avg. 75.5 79.1 58. 50.8 61.7 47.0 45.4 32.2 44. 44.4 Avg. CSR Overall 49.8 52. 38.0 28.3 20.2 27.7 19.6 17. 13.0 16.1 58.5 58.1 56.7 56. 51.9 53.6 47.2 44.3 42.8 46. 12.9 38.5 15.6 15.0 14.4 17. 14.1 8.0 12.0 10.2 LSRIF 68.8 (+25.2) 28.0 (+6.0) 38.9 (+4.3) 52.4 (+6.5) 46.8 (+2.0) 19.3 (+6.3) 51.5 (+8.7) 14.4 (+2.4) Qwen2.5-7B-Instruct Base SFT 73.9 75.2 47. 43.0 55.1 55.7 66.1 68.5 57. 51.2 36.3 30.5 54.2 55.5 15. 14.5 LSRIF 79.7 (+5.8) 54.0 (+7.0) 57.5 (+2.4) 70.0 (+3.9) 63.2 (+6.0) 37.3 (+1.0) 56.5 (+2.3) 18.7 (+3.5) Distill-Qwen-7B Base SFT 61.7 65.1 36.0 40.0 41.7 43. 55.2 55.8 53.0 53.6 25.2 28. 47.2 44.2 13.9 14.2 LSRIF 71.5 (+9.8) 47.0 (+11.0) 44.0 (+2.3) 61.1 (+5.9) 55.0 (+2.0) 30.0 (+4.8) 46.7 (-0.5) 15.0 (+1.1) Llama-3.1-8B-Instruct Base SFT 73.8 77. 34.0 36.0 53.8 52.2 63.6 61. 47.5 46.9 46.5 34.5 53.4 55. 16.2 14."
        },
        {
            "title": "LSRIF",
            "content": "81.5 (+7.7) 40.0 (+6.0) 58.4 (+4.6) 63.9 (+0.3) 48.0 (+0.5) 47.6 (+1.1) 57.8 (+4.4) 18.7 (+2.5) Distill-Qwen-14B"
        },
        {
            "title": "Base",
            "content": "SFT 74.9 79.3 55.0 56.0 51. 56.8 72.7 70.5 61.0 59.2 34. 36.1 54.5 59.2 17.2 16.4 LSRIF 82.1 (+7.2) 60.0 (+5.0) 58.2 (+7.0) 75.5 (+2.8) 63.8 (+2.8) 38.8 (+4.4) 61.7 (+7.2) 18.3 (+1.1) Qwen3-8B"
        },
        {
            "title": "SFT",
            "content": "87.8 80.6 66.0 62.0 56.4 53. 78.5 74.3 75.1 74.7 45.5 35. 64.4 63.3 29.8 25.6 LSRIF 90.2 (+2.4) 68.0 (+2.0) 58.1 (+1.7) 79.2 (+0.7) 75.6 (+0.5) 48.1 (+2.6) 65.0 (+0.6) 32.3 (+2.5) Table 2: Model performance on in-domain and out-of-domain instruction following benchmarks. strong performance with improvements of 2.4 on IFEval and 2.0 on CFBench. On out-of-domain benchmarks, LSRIF demonstrates consistent improvements across diverse evaluation scenarios. Qwen2.5-7B-Instruct improves by 6.0 on WritingBench and 3.5 on MultiChallenge. Qwen2.51.5B-Instruct shows improvements of 6.5 on ComplexBench and 8.7 on AgentIF. initially underperforms. Notably, LSRIF enables models to outperform specialized baseline models even while the base model For instance, Qwen2.5-7B-Instruct underperforms RAIF-7B and Self-Supervised-7B, but after LSRIF training exceeds both baselines with substantial improvements. After LSRIF, Qwen3-8B achieves 90.2 on IFEval, higher than GPT-4o (84.8) and VERIF-8B (87.1), demonstrating state-of-the-art performance on this benchmark. Logical Reasoning Performance. We evaluate logical reasoning capabilities using Enigmata (Chen et al., 2025), comprehensive benchmark suite designed to assess logical reasoning abilities of large language models. Enigmata comprises 36 tasks distributed across seven categories, with each task equipped with generators that can produce infinite examples and rule-based verifiers. The benchmark evaluates four key reasoning subcategories: Logic (formal logical inference), Arithmetic (mathematical computation and reasoning), Graph (graph-based problem solving) and Search (path-finding task). As shown in Tab. 3, LSRIF effectively enhances both logical reasoning and general capabilities. On Enigmata, LSRIF outperforms base models across all subcategories, with particularly strong gains on Arithmetic. For Distill-Qwen-7B, Arithmetic improves by 10.6, while Logic increases by 2.7 and Graph by 6.4. For Distill-Qwen-14B, Arithmetic shows the most substantial improvement, increasing by 18.0, with Logic improving by 3.7 and Graph by 2.2. The significant improvements on Arithmetic suggest that LSRIFs structure-aware reward modeling effectively captures mathematical constraint satisfaction, enabling models to better follow numerical and computational requirements in instructions."
        },
        {
            "title": "Model",
            "content": "Logic Reasoning (Enigmata)"
        },
        {
            "title": "General Capabilities",
            "content": "Logic Arithmetic Graph Search Overall AIME2024 AIME2025 GPQA-Diamond MT-Bench AlpacaEval2.0 Distill-Qwen-7B Distill-Qwen-7B-LSRIF 10.9 13."
        },
        {
            "title": "44.7\nDistill-Qwen-14B\nDistill-Qwen-14B-LSRIF 48.4",
            "content": "3.7 14.3 21.0 39.0 11.1 17.5 31.1 33.3 4.4 4.6 10.5 14. 9.9 12.4 22.4 24.4 53.4 55.1 69.3 70.2 38.7 41.2 49.0 49. 49.1 52.5 58.6 60.1 5.9 6.3 6.6 7.0 5.0 5.8 26.7 30. Table 3: Model performance on logic reasoning (Enigmata) and general capabilities benchmarks. We evaluate AIME using Avg@30 method. Bolded value indicates the best result for each model on the benchmark."
        },
        {
            "title": "Config",
            "content": "Performance IFEval CFBench AIME2024 Enigmata Distill-Qwen-7B Distill-Qwen-7B-LSRIF w/o LSRM w/o Sequential Data w/o Conditional Data 61.7 71.5 68.6 69.9 69.7 36.0 47.0 42.0 44.0 44.0 53.4 55.1 52.4 54.0 51.6 9.9 12.4 10.5 11.0 10. Table 4: Ablation study results on different abilities. Bolded values indicate the best performance. Figure 3: LSRIF performance on different reward forms. Const.-Level and Inst.-Level refer to constraint-level and instruction-level, respectively. On general capabilities benchmarks, which encompass mathematics (AIME2024, AIME2025), science (GPQA-Diamond), and general instruction following (MT-Bench, AlpacaEval2.0), LSRIF brings consistent improvements across all evaluated benchmarks. These results demonstrate that LSRIF not only enhances logical reasoning capabilities but also improves general model performance across diverse evaluation domains."
        },
        {
            "title": "4.3 Ablation Studies",
            "content": "As shown in Tab. 4, removing any component degrades performance compared to the full LSRIF. Removing the LSRM, which ignores logical structure and averages rewards across all constraints, results in the largest drop, indicating its critical importance. Specifically, without LSRM, performance decreases by 2.9 on IFEval, 5.0 on CFBench, and 2.7 on AIME2024. This demonstrates that structure-aware reward modeling is essential for effectively capturing logical constraint relationships. Removing sequential data from LSRINSTRUCT also leads to performance decreases, with drops of 1.6 on IFEval and 3.0 on CFBench. Similarly, removing conditional data results in decreases of 1.8 Figure 4: Performance on nested structures from (Wen et al., 2024). on IFEval, 3.0 on CFBench, and 3.5 on AIME2024. All ablation variants still outperform the base model. This indicates that even partial components of LSRIF provide substantial benefits over the base model. These results demonstrate that each componentthe logic-structured reward modeling and logic-structured dataset construction play crucial role in the overall effectiveness of LSRIF."
        },
        {
            "title": "4.4.1 Robustness to Reward Modeling",
            "content": "We compare our reward model with alternative reward methods to demonstrate robustness of our method to different reward forms. As shown in Fig. 3, all reward methods outperform the baseline, indicating that our method is robust to reward forms. LLM-as-a-Judge (Qwen2.5-7B-Instruct) with constraint-level rewards shows improvements over the base model on IFEval and CFBench. Our reward model with instruction-level rewards further tains better performance across all depths compared to Base models. These results indicate that the improvements gained from training on nonnested structures generalize effectively to nested constraint structures, with the benefits becoming pronounced at higher nesting depths."
        },
        {
            "title": "5.1 Parameter Change Patterns",
            "content": "Fig. 5 presents the relative parameter change rates across layers and modules after LSRIF training. The change rate is measured using the normalized Frobenius norm: = Wafter WbeforeF WbeforeF 100%, (5) where Wbefore and Wafter denote the parameters before and after training. For model with layers, let (l) denote the change rate for module at layer l. Attention vs. MLP Modules. clear pattern observed in Fig. 5 is that attention modules undergo substantially larger parameter changes than MLP modules across most layers. In particular, the query and key projection matrices exhibit the highest change rates, while MLP up and down projections show comparatively smaller and more uniform updates: (l) attn.q, (l) attn.k > (l) mlp.up, (l) mlp.down, (6) Layer-wise Trends. This discrepancy between attention and MLP updates is consistent across layers. Although both module types display some variation along depth, attention-related parameters consistently dominate the overall magnitude of change, especially in lower and upper layers. In contrast, MLP parameters remain relatively stable throughout the network. Model Consistency. The same trend holds for both Qwen2.5-7B-Instruct and Distill-Qwen-7B. While the distilled model shows larger absolute change magnitudes, the relative dominance of attention parameter updates over MLP updates remains consistent. Overall, these results indicate that LSRIF primarily induces stronger updates in attention mechanisms, whereas MLP layers are affected to much lesser extent. (a) Qwen2.5-7B-Instruct (b) Distill-Qwen-7B Figure 5: Parameter change rates of LLMs to the original ones across different modules. Darker orange colors indicate larger parameter changes. improves performance on IFEval and CFBench, while our constraint-level variant achieves the best performance across all evaluated benchmarks. Furthermore, our RM consistently outperforms LLM-as-a-Judge, demonstrating the superior effectiveness of our reward model. The constraintlevel variant achieves substantial improvements over LLM-as-a-Judge on both IFEval and CFBench. Both instruction-level and constraint-level variants of our RM achieve competitive performance, with the constraint-level variant achieving the best overall results, indicating that our method is effective for different reward granularity. The superior performance of constraint-level rewards suggests that fine-grained constraint evaluation enables more precise optimization signals compared to instructionlevel aggregation."
        },
        {
            "title": "4.4.2 Generalization to Nested Structures",
            "content": "We conduct experiments to evaluate the performance of our method under nested logical-structure constraints. Although our training data only contains non-nested structures, LSRIF still improves performance on nested constraint structures: Selection_1 (depth 1), Selection_and_Chain_2 (depth 2), and Selection_and_Chain_3 (depth 3) from ComplexBench. As shown in Fig. 4, LSRIF main- (a) Instruction Following More Attention on constraints and their underlying logic (b) Logic Reasoning More Attention on logical connectors Figure 6: Comparison of attention importance changes for each token position in Qwen2.5-7B-Instruct before and after training on instruction following and logic reasoning tasks. Darker colors indicate greater increases."
        },
        {
            "title": "5.2 Token-Level Information Flow Analysis",
            "content": "We analyze token-level information flow using gradient-based saliency attribution to quantify how training redirects attention to semantically critical tokens. For token xi with embedding Ei, the attribution score is defined as Si = (cid:12) (cid:12) (cid:88) (cid:12) (cid:12) (cid:12) d=1 Ei,d Ei,d (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) . (7) The sequence-level loss function is defined as This hierarchical pattern indicates that the model prioritizes structural elements encoding logical relationships, aligning with structure-aware reward modeling. The substantial updates to (l) attn.q and (l) attn.k enable query and key representations that prioritize tokens encoding logical structures. The attention mechanism computes query-key similarities where query and key projections are updated to maximize attention weights for structural tokens, validating that LSRIF adapts attention mechanisms to capture constraint relationships rather than merely adjusting output representations. L(x, y) = (cid:88) t=1 log (yt y<t, x). (8)"
        },
        {
            "title": "The change in attention importance is measured as",
            "content": "Si = Safter Sbefore , (9) where higher values indicate greater increases in attention importance. As shown in Fig. 6, training shifts attention from diffuse to concentrated patterns, directly corresponding to parameter changes in attention query and key modules  (Fig. 5)  . For instruction following tasks, we observe hierarchical attention increase across three token categories: logical connectors (First, then, else) show the highest increase, constraint tokens (bullet, lowercase, bolded) show moderate increase, and action verbs (apply, formal) show lower increase. For logic reasoning tasks, we observe similar hierarchical pattern: logical operators (or, and) show the highest increase, followed by choice/negation terms (either, not) and descriptive predicates (attends). In this work, we propose LSRIF, logic-structured training framework. We construct LSRINSTRUCT, multi-constraint instruction dataset covering parallel, sequential, and conditional constraint logic structures, and design LSRM, structure-aware reward modeling that aligns training signals with logical execution semantics. LSRIF improves instruction following in both in-domain and outof-domain settings, while also enhancing general reasoning ability. We also conduct attention and token-level interpretability analysis for model performance improvements."
        },
        {
            "title": "7 Limitations",
            "content": "Our study has following main limitations. First, due to computational constraints, we do not evaluate our method on larger models such as 70B+, and validation at this scale would further strengthen the credibility and robustness of our approach. Second, our training data is primarily English. While results on CFBench indicate that logic-structured training can generalize to other languages, we encourage the community to construct multilingual logic-structured instruction datasets to more systematically assess and extend cross-lingual generalization."
        },
        {
            "title": "References",
            "content": "Kaikai An, Li Sheng, Ganqu Cui, Shuzheng Si, Ning Ding, Yu Cheng, and Baobao Chang. 2025. Ultraif: Advancing instruction following from the wild. arXiv preprint arXiv:2502.04153. Jiangjie Chen, Qianyu He, Siyu Yuan, Aili Chen, Zhicheng Cai, Weinan Dai, Hongli Yu, Qiying Yu, Xuefeng Li, Jiaze Chen, and 1 others. 2025. Enigmata: Scaling logical reasoning in large language arXiv models with synthetic verifiable puzzles. preprint arXiv:2505.19914. Jiale Cheng, Xiao Liu, Cunxiang Wang, Xiaotao Gu, Yida Lu, Dan Zhang, Yuxiao Dong, Jie Tang, Hongning Wang, and Minlie Huang. 2024. Spar: Self-play with tree-search refinement to improve instructionfollowing in large language models. arXiv preprint arXiv:2412.11605. Kaustubh Deshpande, Ved Sirdeshmukh, Johannes Baptist Mols, Lifeng Jin, Ed-Yeremai HernandezCardona, Dean Lee, Jeremy Kritz, Willow Primack, Summer Yue, and Chen Xing. 2025. Multichallenge: realistic multi-turn conversation evaluation benchIn Findings of mark challenging to frontier llms. the Association for Computational Linguistics: ACL 2025, pages 1863218702. Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori Hashimoto. 2024. Length-controlled alpacaeval: simple way to debias automatic evaluators. arXiv preprint arXiv:2404.04475. Simeng Han, Hailey Schoelkopf, Yilun Zhao, Zhenting Qi, Martin Riddell, Wenfei Zhou, James Coady, David Peng, Yujie Qiao, Luke Benson, and 1 others. 2022. Folio: Natural language reasoning with first-order logic. arXiv preprint arXiv:2209.00840. Qianyu He, Jie Zeng, Qianxi He, Jiaqing Liang, and Yanghua Xiao. 2024. From complex to simple: Enhancing multi-constraint complex instruction following ability of large language models. arXiv preprint arXiv:2404.15846. Hui Huang, Jiaheng Liu, Yancheng He, Shilong Li, Bing Xu, Conghui Zhu, Muyun Yang, and Tiejun Zhao. 2025. Musc: Improving complex instruction following with multi-granularity self-contrastive training. arXiv preprint arXiv:2502.11541. Yuxin Jiang, Yufei Wang, Xingshan Zeng, Wanjun Zhong, Liangyou Li, Fei Mi, Lifeng Shang, Xin Jiang, Qun Liu, and Wei Wang. 2023. Followbench: multi-level fine-grained constraints following benchmark for large language models. arXiv preprint arXiv:2310.20410. Andreas Köpf, Yannic Kilcher, Dimitri von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, and 1 others. 2024. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36. Jijie Li, Li Du, Hanyu Zhao, Bo-wen Zhang, Liangdong Wang, Boyan Gao, Guang Liu, and Yonghua Lin. 2025. Infinity instruct: Scaling instruction selection and synthesis to enhance language models. arXiv preprint arXiv:2506.11116. Ke-Han Lu, Zhehuai Chen, Szu-Wei Fu, ChaoHan Huck Yang, Jagadeesh Balam, Boris Ginsburg, Yu-Chiang Frank Wang, and Hung-yi Lee. 2025. Developing instruction-following speech language model without speech instruction-tuning data. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE. MAA. 2024. American invitational mathematics examination - aime. Accessed in February 2024. MAA. 2025. American invitational mathematics examination - aime. Accessed in February 2025. Hao Peng, Yunjia Qi, Xiaozhi Wang, Bin Xu, Lei Hou, and Juanzi Li. 2025. Verif: Verification engineering for reinforcement learning in instruction following. arXiv preprint arXiv:2506.09942. Yunjia Qi, Hao Peng, Xiaozhi Wang, Amy Xin, Youfeng Liu, Bin Xu, Lei Hou, and Juanzi Li. 2025. Agentif: Benchmarking instruction following of large language models in agentic scenarios. arXiv preprint arXiv:2505.16944. Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, and Juanzi Li. 2024. Constraint back-translation improves complex instruction following of large language models. arXiv preprint arXiv:2410.24175. Yulei Qin, Gang Li, Zongyi Li, Zihan Xu, Yuchen Shi, Zhekai Lin, Xiao Cui, Ke Li, and Xing Sun. 2025. Incentivizing reasoning for advanced instructionfollowing of large language models. arXiv preprint arXiv:2506.01413. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Qingyu Ren, Qianyu He, Bowei Zhang, Jie Zeng, Jiaqing Liang, Yanghua Xiao, Weikang Zhou, Zeye Sun, and Fei Yu. 2025. Instructions are all you need: Self-supervised reinforcement learning for instruction following. arXiv preprint arXiv:2510.14420. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, and 1 others. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Tao Zhang, Chenglin Zhu, Yanjun Shen, Wenjing Luo, Yan Zhang, Hao Liang, Fan Yang, Mingan Lin, Yujing Qiao, Weipeng Chen, and 1 others. 2024. Cfbench: comprehensive constraintsarXiv preprint following benchmark for llms. arXiv:2408.01122. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, and 1 others. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in neural information processing systems, 36:4659546623. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient finetuning of 100+ language models. arXiv preprint arXiv:2403.13372. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911. Haoran Sun, Lixin Liu, Junjie Li, Fengyu Wang, Baohua Dong, Ran Lin, and Ruohui Huang. 2024. Conifer: Improving complex constrained instructionfollowing ability of large language models. arXiv preprint arXiv:2404.02823. Chenglin Wang, Yucheng Zhou, Qianning Wang, Zhe Wang, and Kai Zhang. 2025. Complexbench-edit: Benchmarking complex instruction-driven image editing via compositional dependencies. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 1339113397. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022a. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560. Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Anjana Arunkumar, Arjun Ashok, Arut Selvan Dhanasekaran, Atharva Naik, David Stap, and 1 others. 2022b. Super-naturalinstructions: Generalization via declarative instructions on 1600+ nlp tasks. arXiv preprint arXiv:2204.07705. Bosi Wen, Pei Ke, Xiaotao Gu, Lindong Wu, Hao Huang, Jinfeng Zhou, Wenchuang Li, Binxin Hu, Wendy Gao, Jiaxing Xu, and 1 others. 2024. Benchmarking complex instruction-following with multiple constraints composition. Advances in Neural Information Processing Systems, 37:137610137645. Yuning Wu, Jiahao Mei, Ming Yan, Chenliang Li, Shaopeng Lai, Yuran Ren, Zijia Wang, Ji Zhang, Mengyue Wu, Qin Jin, and 1 others. 2025. Writingbench: comprehensive benchmark for generative writing. arXiv preprint arXiv:2503.05244. Shunyu Yao, Howard Chen, Austin Hanjie, Runzhe Yang, and Karthik Narasimhan. 2023. Collie: Systematic construction of constrained text generation tasks. arXiv preprint arXiv:2307.08689. Junjie Ye, Caishuang Huang, Zhuohan Chen, Wenjie Fu, Chenyuan Yang, Leyi Yang, Yilong Wu, Peng Wang, Meng Zhou, Xiaolong Yang, and 1 others. 2025. multi-dimensional constraint framework for evaluating and improving instruction following in large language models. arXiv preprint arXiv:2505.07591. Junjie Zhang, Ruobing Xie, Yupeng Hou, Xin Zhao, Leyu Lin, and Ji-Rong Wen. 2025. Recommendation as instruction following: large language model empowered recommendation approach. ACM Transactions on Information Systems, 43(5):137."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Dataset A.1.1 Constraint Types As shown in Tab. 5 and Tab. 6, we distinguish between soft and hard constraints on LLM outputs. Soft constraints cannot be reliably verified by fixed symbolic rules, as they target high-level, often subjective properties such as semantic focus, tone and emotion, stylistic form, audienceor authorspecific style, and syntactic patterns. In contrast, hard constraints are explicitly rule-checkable: they specify concrete requirements on keywords and their frequencies, lengths (in words, sentences, or paragraphs), detectable formats (e.g., numbered bullets, titles, JSON), presence of placeholders or postscripts, and strict start/end markers or punctuation usage. Together, these constraint types provide comprehensive taxonomy for characterizing both high-level communicative behavior and strictly verifiable surface properties in our instruction formulations. A.1.2 Prompt for constructing logically structured multi-constraint instructions As shown in Tab. 7, the template takes seed question and reference list of 25 constraint types. The model selects atomic constraints and combines them using three logical composition types (And, Chain, Selection): And requires satisfying multiple constraints simultaneously, Chain requires sequential task completion, and Selection requires conditional branch selection. The model generates three composite constraints in JSON format, each specifying its composition type and sub-constraints. These constraints are added to the seed question to form complex multi-constraint instructions. A.2 Reward Model Training We fine-tune Qwen2.5-7B-Instruct for binary classification task to determine whether response satisfies given constraint. Training data consists of response-constraint pairs. Each sample is tokenized by concatenating the response and constraint into single text sequence. We use fullparameter fine-tuning (not LoRA) with the HuggingFace Trainer framework. Training hyperparameters: learning rate 5e-6, batch size 1 per device, gradient accumulation steps 1, 3 epochs, FP16 precision, gradient checkpointing enabled, and DeepSpeed optimization configured via JSON. We use accuracy as the evaluation metric, computed by comparing predicted labels with ground truth labels. The training is performed on 8 NVIDIA H200 GPUs. A.3 SFT Training We perform supervised fine-tuning (SFT) on six models: Qwen2.5-1.5B-Instruct, Qwen2.5-7BInstruct, Llama-3.1-8B-Instruct, Distill-Qwen-7B, Distill-Qwen-14B, and Qwen3-8B. The training is performed on 8 NVIDIA H200 GPUs. The training data consists of instruction-response pairs where responses are generated by the teacher model GPT-4.1. Training is conducted using LLaMAFactory (Zheng et al., 2024) with LoRA fine-tuning (rank=8, targeting all linear layers). We use maximum sequence length of 20480 tokens, and employ 16 preprocessing workers and 4 dataloader workers. Training hyperparameters include: batch size of 1 per device with gradient accumulation of 8 steps, learning rate of 1.0e-4, 3 training epochs, cosine learning rate scheduler with 10% warmup ratio, and bfloat16 precision. Model-specific templates are applied according to each models architecture. A.4 RL Training"
        },
        {
            "title": "Implementation Details",
            "content": "A.4.1 We apply GRPO training using the VeRL framework. Training is conducted on 8 NVIDIA H200 GPUs. Maximum prompt length is 2048 tokens, and maximum response length is 8192 tokens. The rollout batch size is set to 384, and data is shuffled with random seed of 1. The algorithm employs the GRPO advantage estimator with KL penalty enabled. We use low-variance KL penalty formulation with coefficient of 1.0e-2. Training batches are organized with global batch size of 96, micro-batches of size 4 per device for updates, and micro-batches of size 8 per device for experience generation. Gradient clipping is applied with max norm of 1.0. The optimizer uses learning rate of 1.0e-6 with weight decay of 1.0e-2, and no learning rate warm-up. We leverage FSDP with full sharding enabled. For rollouts, we generate responses with temperature of 1.0 and group size of 5. Tensor parallelism of size 2 is applied. The maximum number of batched tokens is set to 16000. Different models are trained for varying numbers of epochs: Qwen2.5-7B-Instruct, Llama3.1-8B-Instruct and Distill-Qwen-14B are trained for 1 epoch; Distill-Qwen-7B and Qwen2.5-1.5BInstruct are trained for 5 epochs; and Qwen3-8B is trained for 4 epochs. For the sequential rewards, we set the decay coefficient γ to 0.5, which represents moderate penalty propagation strength. With γ = 0.5, each failed earlier step reduces the effective reward of subsequent steps by half, encouraging correct early decisions while still allowing partial credit for later successes. Empirically, this choice provides stable training dynamics and avoids overly aggressive reward suppression observed with smaller γ values. A.4.2 Baselines RAIF-7B (Qin et al., 2025): RAIF-7B (Incentivizing Reasoning) proposes systematic approach to enhance large language models ability to handle complex instructions by incentivizing reasoning processes during test-time computation scaling. The method encourages models to engage in explicit reasoning steps when processing complex instructions, thereby improving instruction-following performance through enhanced computational reasoning capabilities. Conifer-7B-DPO (Sun et al., 2024): Conifer addresses complex constrained instruction-following through two-stage training pipeline. The method first constructs curriculum dataset organized from simple to complex instructions and performs supervised fine-tuning (SFT) on this dataset. Subsequently, it applies Direct Preference Optimization (DPO) training using an open-source preference dataset to further refine the models ability to follow complex constraints. Crab-7B-DPO (Qi et al., 2024): Crab employs constraint back-translation strategy to improve complex instruction following. The method leverages Llama3-70B-Instruct as strong teacher model to back-translate constraints into highquality instruction-response pairs. This process creates comprehensive dataset with complex constraints, which is then used for DPO training to enhance the models instruction-following capabilities. SPAR-8B-DPO (Cheng et al., 2024): SPAR (Self-play with tree-search refinement) introduces self-play framework that integrates tree-searchbased self-refinement mechanisms. The framework enables an LLM to play against itself, employing tree-search strategies to iteratively refine responses with respect to given instructions. This approach generates valid and comparable preference pairs while minimizing unnecessary variations, facilitating effective DPO training for instructionfollowing tasks. VERIF (Peng et al., 2025): VERIF (Verification Engineering for Reinforcement Learning) combines multiple verification approaches to enhance instruction following through reinforcement learning. The method integrates rule-based code verification with LLM-based verification from large reasoning models (RLVR), providing comprehensive verification signals that guide the reinforcement learning process toward better instructionfollowing performance. Self-Supervised-7B (Ren et al., 2025): SelfSupervised-7B presents self-supervised reinforcement learning framework for instruction following that eliminates the need for external supervision. The method extracts reward signals directly from instructions and generates pseudo-labels for reward model training, thereby removing dependencies on human-annotated preference data. The framework introduces constraint decomposition strategies and efficient constraint-level binary classification methods to address sparse reward problems while maintaining computational efficiency. Experimental results demonstrate significant performance improvements across multiple datasets, including complex agentic tasks and multi-turn instruction-following scenarios. A.4.3 Benchmarks We evaluate instruction-following ability on various benchmarks: IFEval (Zhou et al., 2023): IFEval (InstructionFollowing Evaluation) focuses on verifiable instructions that can be automatically checked for compliance. The benchmark includes instructions such as \"write in more than 400 words\" and \"mention the keyword of AI at least 3 times\", covering 25 distinct types of verifiable instructions across approximately 500 prompts. Each prompt may contain one or more verifiable instructions, enabling systematic evaluation of models ability to follow explicit, rule-based constraints. CFBench (Zhang et al., 2024): CFBench (Comprehensive Constraints Following Benchmark) is large-scale benchmark featuring 1,000 carefully curated samples that span more than 200 real-life scenarios and over 50 natural language processing tasks. The benchmark systematically compiles constraints from real-world instructions and establishes comprehensive framework for constraint categorization, including 10 primary categories and over 25 subcategories. Each constraint is seamlessly integrated within instructions to reflect realistic usage scenarios. FollowBench (Jiang et al., 2023): FollowBench provides comprehensive evaluation framework covering five distinct types of fine-grained constraints: Content, Situation, Style, Format, and Example. To enable precise constraint-following assessment across varying difficulty levels, the benchmark introduces multi-level mechanism that incrementally adds single constraint to the initial instruction at each increased level, allowing for granular analysis of model performance. ComplexBench (Wen et al., 2024): ComplexBench is designed to comprehensively evaluate LLMs ability to follow complex instructions composed of multiple constraints. The benchmark proposes hierarchical taxonomy for complex instructions, encompassing 4 constraint types, 19 constraint dimensions, and 4 composition types. It includes manually collected high-quality dataset that systematically covers various constraint combinations and logical structures. WritingBench (Wu et al., 2025): WritingBench is comprehensive benchmark designed to evaluate LLMs across diverse writing domains. The benchmark covers 6 core writing domains and 100 subdomains, encompassing creative, persuasive, informative, and technical writing. It provides systematic evaluation of models ability to generate high-quality written content that adheres to various stylistic and content requirements. Collie (Yao et al., 2023): Collie (Constrained Language Generation) employs grammar-based framework that enables the specification of rich, compositional constraints across multiple generation levels, including word, sentence, paragraph, and passage levels. The benchmark encompasses diverse modeling challenges such as language understanding, logical reasoning, counting, and semantic planning, providing systematic approach to evaluating constraint-following capabilities. AgentIF (Qi et al., 2025): AgentIF is the first benchmark specifically designed for systematically evaluating LLM instruction-following ability in agentic scenarios. The benchmark features three key characteristics: (1) Realistic: constructed from 50 real-world agentic applications; (2) Long: averaging 1,723 words with maximum of 15,630 words; (3) Complex: averaging 11.9 constraints per instruction, covering diverse constraint types including tool specifications and condition constraints. MultiChallenge (Deshpande et al., 2025): MultiChallenge is pioneering benchmark evaluating large language models on conducting multi-turn conversations with human users, crucial yet underexamined capability. The benchmark identifies four categories of challenges in multi-turn conversations that are both common in real-world humanLLM interactions and challenging to current frontier LLMs. All four challenge categories require accurate instruction-following, context allocation, and in-context reasoning simultaneously. We assess general reasoning and knowledge capabilities with the following datasets: GPQA-Diamond (Rein et al., 2024): GPQADiamond is specialized subset of the GPQA (Graduate-Level Google-Proof Q&A) benchmark, comprising 198 meticulously crafted multiplechoice questions across biology, chemistry, and physics. These questions are designed to be exceptionally challenging, requiring domain expertise that makes them rigorous test for AI models scientific knowledge and reasoning capabilities. 2024) AIME2024 and (MAA, AIME2025 (MAA, 2025): The AIME (American Invitational Mathematics Examination) datasets consist of problems from the 2024 and 2025 AIME competitions, respectively. These datasets are commonly used to evaluate the mathematical reasoning ability of large language models. The AIME is prestigious mathematics competition for high school students in the United States, and its problems require sophisticated mathematical reasoning and problem-solving skills. FOLIO (Han et al., 2022): FOLIO (First-Order Logic Inference Over Text) is benchmark dataset developed to assess the logical reasoning capabilities of large language models. It consists of human-annotated examples that require deductive reasoning grounded in first-order logic (FOL). The benchmark evaluates models ability to perform formal logical inference over natural language text, bridging natural language understanding and formal reasoning. Enigmata (Chen et al., 2025): Enigmata is comprehensive suite designed to enhance logical reasoning capabilities of large language models. The benchmark comprises 36 tasks distributed across seven categories, with each task equipped with generators that can produce infinite examples and rule-based verifiers. This generator-verifier design supports scalable multi-task reinforcement learning training, fine-grained analysis, and seamless integration with reinforcement learning with verifiable rewards (RLVR). Models trained on Enigmata demonstrate strong performance across multiple puzzle reasoning benchmarks and exhibit good generalization to advanced mathematical and STEM reasoning tasks. MT-Bench (Zheng et al., 2023): MT-Bench (Multi-Turn Benchmark) is an evaluation framework designed to assess chat assistants performance in multi-turn conversations. The benchmark contains 80 high-quality multi-turn openended questions covering diverse topics such as writing, role-playing, mathematics, and coding. Model responses are scored by GPT-4, providing direct scores without requiring pairwise comparisons. MT-Bench enables systematic evaluation of models conversational abilities and their capacity to maintain context and coherence across multiple interaction turns. AlpacaEval 2.0 (Dubois et al., 2024): AlpacaEval 2.0 is an automated evaluation benchmark designed to assess instruction-following capabilities of language models. The benchmark leverages GPT-4 as an automated annotator to compare model-generated responses with reference outputs, evaluating how well models adhere to user instructions. The benchmark is characterized by its efficiency, low cost, and reliability, enabling rapid assessment of model performance. AlpacaEval 2.0 provides standardized evaluation protocol for comparing instruction-following models across diverse tasks and scenarios. A.4.4 Case Study Tab. 8 illustrates the models transformation in constraint-aware reasoning and output generation. Before training, the models reasoning process shows awareness of constraints (e.g., \"Let me make sure its clear without commas\") but fails to translate this awareness into compliant output: the generated riddle contains commas and uses keywords inappropriately (e.g., \"afternoon embroidered\" rather than natural keyword integration). The constraint compliance is [False, False]. After training, the models reasoning becomes more systematic, explicitly planning constraint satisfaction (e.g., \"Ill have to structure the sentences carefully to avoid [commas]\") and naturally incorporating keywords. This improved reasoning directly translates to compliant output: the riddle contains no commas and integrates both keywords naturally. The constraint compliance improves to [True, True], demonstrating effective alignment between reasoning and constraint satisfaction. A.4.5 Training Dynamics Analysis As shown in Fig. 7, we present the reward and response length dynamics during training across four models: Qwen2.5-7B-Instruct, Distill-Qwen7B, Distill-Qwen-14B, and Qwen3-8B. For reward scores, all models exhibit consistent pattern: an initial rapid increase followed by stabilization with oscillations. Qwen2.5-7B-Instruct shows the steepest initial improvement, rising from 0.4 to 0.6 within 20 steps, while Distill-Qwen models demonstrate more gradual increases over 200-400 steps, reaching stable scores around 0.65-0.7. Qwen38B displays higher volatility with scores fluctuating between 0.62 and 0.74. In contrast, response length shows high variability across all models with no clear monotonic trend. Response lengths vary substantially by model scale: Qwen2.5-7BInstruct generates shorter responses (340-400 tokens), while Distill-Qwen models and Qwen3-8B produce longer outputs (700-850 and 900-1200 tokens, respectively). The high variance in response length suggests that the training process maintains flexibility in output generation while improving constraint satisfaction, as evidenced by the stable reward trends. A.4.6 Full Parameter Change Patterns As shown in Fig. 8, we extend the parameter change analysis to three additional models: Qwen2.51.5B-Instruct, Distill-Qwen-14B, and Qwen3-8B. The analysis reveals consistent patterns across all models: attention query (attn.q) and key (attn.k) modules exhibit the highest parameter change rates, particularly concentrated in the bottom and top layers, while attention value (attn.v) and output (attn.o) modules consistently show minimal changes across all layers. MLP modules (mlp.up, mlp.down, mlp.gate) demonstrate moderate change rates, falling between the high changes in attn.q/attn.k and the low changes in attn.v/attn.o. A.4.7 Full Token-Level Information Flow"
        },
        {
            "title": "Analysis",
            "content": "As shown in Fig. 9, we extend the token-level information flow analysis to Distill-Qwen-7B and Qwen3-8B models on complex instructionfollowing tasks. Before training, both models exhibit relatively diffuse attention patterns, with only subset of tokens (e.g., \"instructional\", \"paragraph\", \"checklist\", \"error\") showing moderate importance. After training, both models demonstrate dramatic shift towards uniformly high attention importance across virtually all tokens in the prompt, including conjunctions, prepositions, and specific constraints."
        },
        {
            "title": "Number Sentences",
            "content": "Number Paragraphs + First Word"
        },
        {
            "title": "Change Cases",
            "content": "Frequency of All-capital Words Start with / End with"
        },
        {
            "title": "No Commas",
            "content": "(e.g., specified keywords Response must include {keyword1}, {keyword2}). particular word should appear certain number of times ({N} times). Prohibits the inclusion of specified keywords ({forbidden words}). Requires specific letter to appear certain number of times ({N} times). Entire response must be in specified language ({language}) and no other. Specifies the exact number of paragraphs ({N}), separated by markdown divider ***. Constraint on the number of words: at least / around / at most {N} words. Constraint on the number of sentences: at least / around / at most {N} sentences. Requires {N} paragraphs (separated by two line breaks), with the {i}-th paragraph starting with specified word ({first_word}). Requires an explicit postscript at the end, starting with specified marker ({postscript marker}). Response must contain at least {N} placeholders in square brackets (e.g., [address]). Requires exactly {N} bullet points using markdown format (e.g., * This is point.). Answer must include title wrapped in double angular brackets (e.g., poem of joy). Response must be one of the provided options ({options}). Requires at least {N} sections highlighted using markdown (e.g., *highlighted section*). Response must have {N} sections, with each sections beginning marked by splitter (e.g., {section_splitter} X). Entire output must be wrapped in JSON format. First repeat the request without change, then provide the answer. Requires two different responses, separated by six asterisk symbols (******). Entire response must be in English, using only capital letters. Entire response must be in English, using only lowercase letters, with no capital letters allowed. Words with all capital letters should appear at least / around / at most {N} times. Response must end with specific phrase ({end_phrase}), with no other words following it. Entire response must be wrapped in double quotation marks. Prohibits the use of any commas in the entire response. Table 5: Hard Constraint Types (Zhou et al., 2023)."
        },
        {
            "title": "Element constraint",
            "content": "Requires specific terms or symbols with precise placement. Requires inclusion of specific entities or scenarios. ...must include the word beautiful. ...highlights the Great Wall."
        },
        {
            "title": "Semantic constraint",
            "content": "Focuses on themes, tone, or stance. Write poem about London."
        },
        {
            "title": "Document Count",
            "content": "Limits the number of words. 50-word poem. Limits the number of sentences. ...three sentences. Limits the number of paragraphs. ...divided into 3 sections. Limits the number of documents. ...list 3 articles."
        },
        {
            "title": "Tone and emotion",
            "content": "Conforms to specific emotional tone."
        },
        {
            "title": "Form and style",
            "content": "Uses specified stylistic form and perception. Write letter in an angry and sarcastic tone. Write passage in an encyclopedic style. Audience-specific Tailored to specific audience group. Write poem for 6-year-old."
        },
        {
            "title": "Authorial style",
            "content": "Emulates specific authors styles. Write passage in the style of Shakespeare."
        },
        {
            "title": "Fundamental format",
            "content": "Follows standard formats like JSON, HTML, etc. Output in JSON format."
        },
        {
            "title": "Bespoke format",
            "content": "Uses custom formatting protocols. Bold the main idea and output in unordered list."
        },
        {
            "title": "Syntactic constraint",
            "content": "Tailored for specific applications or domains. Convert to electronic medical record format. Adapts to context like dialects or language policy. Output in English, classical Chinese, etc. Follows specific phrase and clause structures. Use imperatives with nouns and verb phrases."
        },
        {
            "title": "Phonological constraint",
            "content": "Controls affixes, roots, and word formation. Focuses on sounds, tone, and intonation. Output all content in lowercase English. Single-syllable tongue twisters. Role-based constraint Responds with specific role identity. You are Confucius, how do you decide? Task-specific constraint Addresses defined situational task. Work from home, how to report?"
        },
        {
            "title": "Rule constraint",
            "content": "Involves multi-faceted and nested reasoning. Conforms to patterns from example pairs. Narrows response space via exclusions. Combines requirements that are hard to satisfy simultaneously. Follows symbolic or logical operation rules. On the left, 10 total, what to do? input:x..., output:{...}; input:y..., output? No responses about political topics. five-character quotation, 1000 words. Each answer adds 1+1=3, then 2+2=5. Table 6: Soft Constraint Types (Zhang et al., 2024). /* Task Description */ 1. currently have seed question, but the seed questions are relatively simple. To make the instructions more complex, want you to identify and return three composition constraints that can be added to the seed question. 2. will provide [Seed Question] and [Constraint References], and you can use these references to propose the composition constraint that would increase the difficulty of the seed question. 3. You may choose one or more constraints from the [Constraint References] list, and combine them using the following composition rules. 4. Do not modify or rewrite the seed question. Your task is only to generate the new composite constraint that can be added to it. 5. Return the added constraint(s) in the JSON format described below, including all sub-constraints and their logical composition types. 6. Do not return anything else. No explanation, no reformulated question, no analysisonly the JSON structure. /* Logical Composition Types */ And: The output is required to satisfy multiple constraints simultaneously. Template: C1 and C2 and C3. Example: summarize the news in bullet points and within 100 words. Chain: The output is required to complete multiple tasks sequentially, each with its own constraints. Template: first C1, then C2, finally C3. Example: introduce Mona Lisa: year of creation, then background, then impact. Selection: The output is required to select different branches according to conditions, fulfilling the constraints of the corresponding branch. Template: if C1 then C2 otherwise C3. Example: if the painting has an animal, describe it in Chinese; otherwise, give year, background, and impact. /* JSON Output Format */ Return { \"composite_constraints\": [ ... ] } where each element contains \"composite_constraint\" with fields \"type\": \"<And/Chain/Selection>\" and \"sub_constraints\" (\"c1\", \"c2, \"c3\") each holding \"constraint\" string that specifies one atomic constraint. /* Constraint References*/ 1. Lexical content constraint: must include specific terms or symbols with precise placement. 2. Element constraint: include specific entities or scenarios. 3. Semantic constraint: focus on themes, tone, or stance. 4. Word Count: limit the number of words. 5. Sentence Count: limit the number of sentences. 6. Paragraph Count: limit the number of paragraphs. 7. Document Count: limit the number of documents. 8. Tone and emotion: conform to specific emotional tone. 9. Form and style: use specified stylistic form and perception. 10. Audience-specific: tailored to specific audience group. 11. Authorial style: emulate specific authors styles. 12. Fundamental format: follow standard formats like JSON, HTML, etc. 13. Bespoke format: use custom formatting protocols. 14. Specialized format: tailored for specific applications or domains. 15. Pragmatic constraint: adapt to context like dialects or language policy. 16. Syntactic constraint: follow specific phrase and clause structures. 17. Morphological constraint: control over affixes, roots, and word formation. 18. Phonological constraint: focus on sounds, tone, and intonation. 19. Role-based constraint: respond with specific role identity. 20. Task-specific constraint: address defined situational task. 21. Complex context constraint: involve multi-faceted and nested reasoning. 22. Example constraint: conform to patterns from example pairs. 23. Inverse constraint: narrow response space via exclusions. 24. Contradictory constraint: combine requirements that are hard to satisfy simultaneously. 25. Rule constraint: follow symbolic or logical operation rules. /* Seed Question */ [Seed Question]: {} /* Modified Question */ [Modified Question]: (the seed question plus one of the generated composite constraints). Table 7: Prompt template for constructing logically structured multi-constraint instructions."
        },
        {
            "title": "After Training",
            "content": "Content Input: Write riddle about embroidery that has the answer \"needle\". Include keywords afternoon and distressed in the response. Dont use any commas in your answer. Think: Alright, need to create riddle about embroidery where the answer is \"needle.\" The riddle should include the keywords afternoon and distressed. Let me break this down. First, Ill think about what embroidery entails. Embroidery involves stitching, so maybe using words like \"stitch,\" \"needle,\" or \"cloth.\" Since the answer is \"needle,\" need to hint at it without stating it directly. For afternoon, perhaps can use something related to the time of day, like \"quietly\" or \"evening,\" but in way that ties into embroidery. Maybe \"afternoon\" can relate to when someone is stitching, so \"quietly\" could work, as its gentle activity. Distressed can be tricky. Maybe can imply it through the feeling of the fabric or the process. Perhaps something like \"snag\" or \"torn,\" but need to tie it back to embroidery. Maybe the needle moves too fast, causing something to snag, making the work look distressed. Putting it together, want riddle that flows naturally. Let me try: \"What does an afternoon embroidered with needle bring?\" Hmm, thats close. Maybe tweak it to add the distressed element. Wait, \"What does an afternoon embroidered with needle bring?\" The answer is \"a needle,\" but maybe can add clue about the distress. So, \"What does an afternoon embroidered with needle bring? stitched, but maybe little snag, sewing thats bit hurried, so small, worried look.\" That uses afternoon as embroidered and needle as the answer. The distressed part is hinted at by snag and hurried, which can make someone worried about the work. think that works. Let me make sure its clear without commas. The response is: \"What does an afternoon embroidered with needle bring? stitched, but maybe little snag, sewing thats bit hurried, so small, worried look.\" That includes both keywords and hints at the needle being the answer. </think> Output: What does an afternoon embroidered with needle bring? stitched, but maybe little snag, sewing thats bit hurried, so small, worried look. Constraint Compliance: [False, False] Input: Write riddle about embroidery that has the answer \"needle\". Include keywords afternoon and distressed in the response. Dont use any commas in your answer. Think: Alright, need to help the user create riddle about embroidery with the answer \"needle.\" They specifically want me to include the keywords afternoon and distressed. Hmm, let me break this down. First, should understand what riddle entails. Its question or statement that requires thinking to solve, often with clever twist. Since the answer is \"needle,\" need to think about how to incorporate that into riddle. The user mentioned not using any commas, so Ill have to structure the sentences carefully to avoid that. Let me start brainstorming some ideas. Embroidery is about stitching, so maybe can relate needle to the tool used. Including afternoon... perhaps something about the time of day. Maybe the needle is used in the afternoon? Or the result of using it in the afternoon. Distressed could relate to something thats been repaired or fixed, like hole thats been mended. Putting it together, think of the process of sewing. If someone is sewing in the afternoon and notices hole, they might use needle. So, maybe the riddle is about something thats fixed in the afternoon with needle. That way, the keyword distressed can describe the hole thats being mended. So, the riddle could go like: \"What is used in the afternoon to mend hole thats been distressed?\" The answer would be the needle. Let me make sure it flows without commas and makes sense. Yes, that should work. Ill present it as riddle, so its question that leads to the answer with the given keywords. </think> Output: What do you use in the afternoon to mend hole thats been distressed? The answer is needle. Constraint Compliance: [True, True] Table 8: Comparison of the outputs of R1-Distill-Qwen-7B before and after training for the instruction in IFEval. (a) Reward Dynamics of Qwen2.5-7B-Instruct. (b) Response Length Dynamics of Qwen2.5-7BInstruct. (c) Reward Dynamics of Distill-Qwen-7B. (d) Response Length Dynamics of Distill-Qwen7B. (e) Reward Dynamics of Distill-Qwen-14B. (f) Response Length Dynamics of Distill-Qwen14B. (g) Reward Dynamics of Qwen3-8B. (h) Response Length Dynamics of Qwen3-8B. Figure 7: Training dynamics of reward and response length. (a) Qwen2.5-1.5B-Instruct (b) Distill-Qwen-14B (c) Qwen3-8B Figure 8: Parameter change rates of LLMs to the original ones across different modules. (a) Before Training - Distill-Qwen-7B (b) After Training - Distill-Qwen-7B (c) Before Training - Qwen3-8B Figure 9: Token-level information flow analysis. Darker orange indicates higher attention importance. (d) After Training - Qwen3-8B"
        }
    ],
    "affiliations": [
        "Ant Group",
        "School of Data Science, Fudan University",
        "Shanghai Key Laboratory of Data Science, College of Computer Science and Artificial Intelligence, Fudan University"
    ]
}