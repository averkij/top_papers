{
    "paper_title": "General-Reasoner: Advancing LLM Reasoning Across All Domains",
    "authors": [
        "Xueguang Ma",
        "Qian Liu",
        "Dongfu Jiang",
        "Ge Zhang",
        "Zejun Ma",
        "Wenhu Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the \"Zero\" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks."
        },
        {
            "title": "Start",
            "content": "General-Reasoner: Advancing LLM Reasoning Across All Domains Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen University of Waterloo, Vector Institute, TikTok, Singapore, M-A-P x93ma@uwaterloo.ca,wenhuchen@uwaterloo.ca https://tiger-ai-lab.github.io/General-Reasoner/"
        },
        {
            "title": "Abstract",
            "content": "Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the Zero reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering wide range of disciplines; and (2) developing generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train series of models and evaluate them on wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks. 5 2 0 2 0 2 ] . [ 1 2 5 6 4 1 . 5 0 5 2 : r Figure 1: Effectiveness of our General-Reasoner trained with diverse verifiable reasoning questions using model-based verifier compared to baseline methods on various reasoning tasks. The first two authors have equal contribution. Technical Report. Work in progress."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large language models (LLMs) have demonstrated strong potential in many expert-level tasks by following user instructions. One of the most significant breakthroughs in recent work is the use of reinforcement learning (RL), such as PPO and GRPO, to improve the reasoning capabilities of LLMs. particularly noteworthy development is the Zero reinforcement learning setting proposed by DeepSeek-R1-Zero [1], which shows that training base LLM directly via RL can unlock powerful reasoning capabilities without relying on supervised fine-tuning step. RL for LLM reasoning has inspired multiple open-source efforts, such as SimpleRL [2], DAPO [3], and DeepScaleR [4, 5], which leverage variants of GRPO [6] to further enhance reasoning performanceeither in the zero RL setting or via continuous reinforcement learning on models with supervised fine-tuning or distillation. However, existing methods primarily focus on training and evaluation in mathematical reasoning or coding tasks. This narrow focus can be attributed to two main reasons: (1) DATA ABUNDANCE: it is easier to harvest large-scale mathematical data from the Internet due to the abundance of international math and coding competitions and exams; (2) ANSWER VERIFICATION: mathematical domains allow for easy and reliable answer verification using straightforward rule-based methods (e.g., exact numeric matching, symbolic equation comparison), which provide accurate reward signals. However, this reliance on rule-based verifiers restricts the generalizability of the resulting models. Real-world reasoning often spans multiple disciplinessuch as science, finance, and the humanitiesand involves complex, long-tailed answer representations that cannot be reliably verified through heuristic rules. In fact, training solely on mathematical or coding data does not guarantee generalization to other reasoning domains. For example, although S1/S1.1 [7] significantly improves mathematical scores, it degrades performance on MMLU-Pro [8] by 46%. To address these challenges and broaden the applicability and robustness of RL-based reasoning models, we propose new training paradigm designed to enhance the reasoning capabilities of LLMs across diverse, non-mathematical domains, while preserving their strengths in mathematics. Our approach tackles the two aforementioned problems: All-Domain Reasoning Dataset (WebInstruct-verified): major bottleneck for scaling reasoning beyond mathematics is the lack of publicly available, high-quality reasoning datasets that span multiple domains and offer reliably verifiable answers. To address this, we construct large-scale, diverse-domain dataset by carefully crawling and filtering high-quality reasoning questions from web resources, based on WebInstruct [9]. We then employ state-of-the-art LLMs to automatically select questions whose answers can be reliably verified, significantly expanding the training scope to disciplines such as physics, chemistry, social sciences, and finance. Our carefully curated dataset contains approximately 230K high-quality reasoning questions, providing robust foundation for general reasoning training across multiple complex domains. All-Domain Answer Verifier (General-Verifier): In addition to scaling data, existing rule-based verifiers struggle with the diverse answer types commonly found in broader domainsincluding short string answers, LaTeX expressions, and other structured formswhich limits the effectiveness of reasoning training beyond mathematics. We introduce compact generative verifier model (1.5B parameters) explicitly trained to verify short answers in context-aware, chain-of-thought manner, thereby providing robust and reliable reward signals for RL training. This approach improves the flexibility and scalability of RL training across diverse reasoning tasks. Using this diverse, verifiable reasoning data along with the generative model-based verifier, we are able to train series of General-Reasoner models in the Zero-RL setting from various base models. To validate the effectiveness of our approach, we conduct comprehensive evaluations across 12 challenging reasoning benchmarks beyond mathematics, including MMLU-Pro [8], GPQA [10], SuperGPQA [11], TheoremQA [12], and BBEH [13], as well as standard mathematical reasoning benchmarks such as MATH-500 [14], GSM8K [15], and Olympiad [16]. General-Reasoner typically boosts performance on general benchmarks like MMLU-Pro and SuperGPQA by approximately 10%. On math benchmarks, General-Reasoner can even slightly outperform math-focused RL frameworks such as SimpleRL [2], benefiting from cross-domain generalization. Our best model General-Reasoner-Qw3-14B is able to match or beat GPT-4o in various benchmarks. In summary, our work makes the following key contributions: 2 1. We construct and release large-scale, high-quality dataset (WebInstruct-verified) of verifiable reasoning questions with short-form answers spanning diverse domains, significantly broadening the training resources available for general reasoning. 2. We introduce compact, generative model-based verifier (General-Verifier) specifically trained for chain-of-thought answer verification, effectively replacing rule-based methods and enabling robust RL training across multiple complex reasoning domains. 3. Through extensive evaluations, we empirically demonstrate the superior performance and generalization capabilities of General-Reasoner in the Zero-RL setting, providing strong baseline and valuable insights for future advancements in RL-driven general reasoning."
        },
        {
            "title": "2 Related Works",
            "content": "2.1 Large Language Models for Reasoning Advancements in large language models (LLMs) have demonstrated substantial potential in effectively performing diverse tasks by following user instructions. However, expert-level taskssuch as solving complex mathematical problems [12] or addressing practical STEM challenges [8]-require robust reasoning capabilities. Recent work on chain-of-thought reasoning has shown that LLMs can significantly enhance their performance by explicitly decoding their thought processes during inference [17]. Subsequent studies have further investigated this method, termed test-time scaling, to better understand and enhance the reasoning abilities of LLMs. Following this direction, commercial models such as OpenAIs O1 series have exhibited impressive performance on reasoning-intensive evaluations [18], including mathematical Olympiad problems. Meanwhile, recent open-source modelssuch as Qwen [19], QWQ [20], and Deepseek R1 [1]have also achieved competitive results, narrowing the performance gap with state-of-the-art commercial counterparts. This progress opens new avenues for academia to explore advanced optimization strategies aimed at further improving the reasoning capabilities of LLMs [7, 3, 21]. 2.2 Zero Reinforcement Learning for LLMs Typical methods for enhancing the reasoning capabilities of LLMs usually involve first performing supervised fine-tuning with chain-of-thought (CoT) data on base model [22], followed by reinforcement learning (RL) to further improve reasoning and generalization performance. However, recent studies exemplified by Deepseek-R1-Zero [4] have demonstrated that directly applying reinforcement learning to strong base model can effectively uncover significant reasoning capabilities without initial supervised fine-tuning. This Zero Reinforcement Learning approach has attracted considerable attention within the research community, inspiring efforts to replicate and enhance the method on other powerful base models such as Qwen [23]. key advantage of zero RL is its efficiency in collecting verifiable questionanswer pairs, which eliminates the need for complete reasoning chains as training targets. This efficiency provides greater flexibility in enhancing reasoning performance without complex data collection efforts, enabling models to self-improve more effectively. However, existing works on zero RL have mostly focused on mathematical reasoning [2]. While concurrent works like CrossThink [24] and RLVR [25] aim to expand reasoning capabilities to broader domains, our work seeks to do so by leveraging modelbased verifier to scale diverse reasoning data across domains and provide comprehensive evaluations to demonstrate effectiveness."
        },
        {
            "title": "3 General Reasoner",
            "content": "3.1 Diverse Verifiable Reasoning Tasks To facilitate robust reasoning capabilities across wide range of domains beyond mathematical problems, we construct large-scale, diverse, and high-quality dataset composed of verifiable reasoning tasks. Our dataset-building pipeline is illustrated in Figure 2. The initial data source is from WebInstruct dataset [9], which consists of around 5 million naturally occurring, web-crawled instructions from high-quality resource websites, including platforms like 3 Figure 2: Data creation pipeline: It consists of QA mining, Extraction and Verification. StackExchange and various educational portals. Despite WebInstructs suitability for general instruction tuning, the majority of documents are not directly usable as reasoning tasks due to the lack of explicit verifiable answers or require reasoning process. To address this, we first trace the entries in WebInstruct back to its original web page to re-crawl precise question-answer pairs. During this process, we remove the questions lacking clearly identifiable human-written answers on the original source websites. Many websites require membership or complex interaction to show the answers, which will be filtered out. This careful selection aims to ensure retained entries are verified by humans, enhancing the datasets reliability and correctness. Next, we further leverage Gemini-1.5-Pro [26], state-of-the-art LLM, to extract questions explicitly identified as having clearly verifiable short answers for single-turn questions. This step yields an intermediate dataset of approximately 1 million verifiable reasoning questions across various disciplines. Subsequently, we apply Gemini-2.0-Flash to annotate each question with metadata, including the answer type, subject category, and difficulty level. Recognizing the skewed ratio of mathematical tasks, we specifically filter out mathematics problems that are labeled as easier than university-level to ensure more balanced and challenging dataset distribution. (a) Answer Type Distribution (b) Domain Distribution Additionally, recognizing that web-crawled data inherently contains noise, such as questions that are either unsolvable or trivially easy, we implement further rigorous filtering to refine the dataset quality. Specifically, Gemini-2.0-Flash generates eight candidate solutions for each question, allowing us to apply the following quality control criteria: We exclude questions for which all eight candidate solutions fail, effectively removing ambiguous or noisy questions that likely arise from crawling errors or incomplete source content. We also exclude overly simplistic questions for which all eight candidate solutions are correct, ensuring the dataset maintains sufficient complexity and presents meaningful challenges for robust reasoning and generalization during RL training. The Gemini-2.0-flash generated solutions are also later utilized to train our proposed model-based verifier, which will be discussed in detail in the next section. Eventually, the processed dataset contains approximately 230k reasoning questions spanning diverse answer formats, including multiple-choice, numerical expressions, matrices as highlighted in Figure 3a. Figure 3b further illustrates the balanced domain distribution of our curated dataset, encompassing disciplines such as mathematics, physics, chemistry, finance, and various other humanities and social sciences fields. This rigorous data curation process produces challenging but reliable dataset for training generalizable reasoning capabilities in large language models. 4 Table 1: Examples of reasoning questions where the model provides correct answers, but the rulebased verifier fails to recognize their correctness, while the model-based verifier succeeds. Question Example 1 Example 2 Example 3 Consider the line perpendicular to the surface = x2 + y2 at the point where = 4 and = 1. Find vector parametric equation for this line in terms of the parameter t. Find the partial pressure in solution containing ethanol and 1propanol with total vapor pressure of 56.3 torr. The pure vapor pressures are 100.0 torr and 37.6 torr, respectively, and the solution has mole fraction of 0.300 of ethanol. What is the work done to push 1 kg box horizontally for 1 meter on surface with coefficient of friction of 0.5? Ground Truth Answer = 4 + 8t, = 1 + 2t, = 17 - 30.0 torr, 26.3 torr Student Answer 4 + 8t, 1 + 2t, 17 - The partial pressure of ethanol is 30.0 torr and the partial pressure of 1-propanol is 26.32 torr. Rule Based Verifier Model Based Verifier False True False True 4.9 4.9 Nm False True 3.2 Model-Based Verifier for GRPO Preliminary. We adopt Group Relative Policy Optimization (GRPO) [6] following the recent advancements such as DeepSeek-R1 [1]. Given question-answer pair (q, a), behavior policy πθold samples group of individual responses {oi} i=1. The GRPO objective updates model parameters θ as follows [3]: (q,a)D, {oi}G i=1πθold (q) JGRPO(θ) = oi t=1 i=1 1 oi 1 [ ( min (ri,t(θ) ˆAi,t, clip(ri,t(θ), 1 ϵ, 1 + ϵ) ˆAi,t) βDKL(πθπref))], where ri,t(θ) = πθ(oi,t q, oi,<t) πθold (oi,t q, oi,<t) , ˆAi,t = ri mean({Ri} std({Ri}G i=1) i=1) . (1) (2) In this work, our design on the model-based verifier specifically affects how the reward is computed. Limitations of Existing Reward Model. Traditional reward models are trained through human feedback or preference assessment, returning scalar values based on the entire output to indicate overall quality. Although intuitive, these models are suffering from being hacked by the policy model, and usually require the reward model to have large parameter size to be effective and robust. In contrast, rule-based verifiers, widely used in mathematical reasoning due to simplicity, evaluate only the final answer, allowing models greater freedom to explore diverse reasoning paths. However, these rule-based approaches encounter several critical limitations when extending beyond mathematics: Rigid Matching Criteria: Rule-based methods typically require exact matches or adherence to rigid structures, failing to recognize semantically equivalent answers that differ in representation. Semantic Insensitivity: They are ineffective at interpreting answers that vary semantically, such as equivalent expressions or answers expressed in different units or formats. Lack of Generality: Adapting rule-based verification to wide range of disciplines and diverse answer formats can be difficult, limiting their applicability and scalability. Generative Model-Based Verifier. We introduce compact generative model-based verifier specifically trained to robustly assess answer equivalence across diverse domains. Ideally, state-of-the-art large language model (LLM) like Gemini-2.0 could verify answer equivalence; however, such solutions are computationally expensive and impractical for large-scale RL training. Instead, we leverage our dataset creation pipeline, specifically Gemini-2.0-generated candidate answers and verification annotations, to train compact 1.5B-parameter generative verifier model. 5 This verifier, initialized from Qwen2.5-Math-1.5B [23], is fine-tuned to assess student-generated short answers (extracted from the response) against ground-truth references in generative manner, whose inference process is formulated as: ˆy (y q, g, s), ˆy = ycot, [sep], ylabel (3) where ˆy is chain-of-thought reasoning process ycot with final binary prediction ˆylabel (true/false) on whether the student answer is equivalent to the ground-truth in the question context. This verifier integrates seamlessly into our reinforcement learning pipeline, providing robust, accurate reward signals. Empirical analysis confirms that our model-based verifier achieves high agreement with Gemini-2.0-Flash, substantially outperforming traditional rule-based approaches."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Training We follow the Zero RL setting, directly conducting reinforcement learning (RL) from base large language models without an intermediate supervised fine-tuning stage. Specifically, we initialize our models using the base model from Qwen2.5 family (7B and 14B) and the newer Qwen3 family (4B and 14B) [19], and apply the GRPO algorithm. Please note that we pick the general Qwen model instead of the Qwen-math model to maximize the general performance, which could lead to seemingly lower performance on mathematical benchmarks. Reward scores during training are calculated as follows: If the solution extraction fails (e.g., no boxed answer or summarization such as the solution is:), the reward is -0.5. If the solution passes verification, the base reward is 1, with length-based penalty applied to discourage excessively long generations: penalty = -0.05 min(10, abs(len_of_ground_truth - len_of_answer)) Training is conducted on 4 nodes with 8H100 GPUs per node for up to 700 steps for Qwen2.5 series models. For model initialzied with Qwen3-Base, we train up to 400 steps. Please see Appendix A.5 for detailed hyperparameters of each model checkpoint. During training, the average model response length increases from approximately 700 tokens to around 1000 tokens. The total training time is around 2 days for the 4B/7B model and around 4 days for the 14B model. Our implementation is based on the verl repository.2 4.2 Evaluation To evaluate the models general reasoning capabilities, we conduct comprehensive assessment across several challenging benchmarks: MMLU-Pro [8]: robust and challenging massive multi-task understanding dataset tailored to more rigorously benchmark large language models capabilities. SuperGPQA [11]: large-scale benchmark targeting graduate-level reasoning across 285 diverse disciplines. All the questions are verified to be not found on Google Search. BBEH [13]: new benchmark that extends BIG-Bench Hard [27] by introducing more challenging tasks for better evaluation of complex reasoning. GPQA [10]: Graduate-level question answering designed to be resistant to shallow patternmatching or memorization. We use the diamond split in GPQA. TheoremQA [12]: Graduate-level question answering designed to require knowing corresponding theorems. It covers math, physics, EE&CS, and Finance. Its general reasoning benchmark. Math-Related Tasks: suite of standard math reasoning benchmarks, including MATH500 [14], Olympiad [16], Minerva [28], GSM8K [15], AMC, AIME24, and AIME25. We use the simple-evals3 evaluation framework, and use GPT4o [29] to check the answer equivalence. 2https://github.com/volcengine/verl 3https://github.com/openai/simple-evals 6 Table 2: Accuracy comparison of our General-Reasoner with baseline methods on general reasoning benchmarks. MMLU-Pro, SuperGPQA, TheoremQA and BBEH contain multiple subfields. Backbone MMLU-Pro GPQA-D Model Name Metric MiMo-RL QwQ-32B GPT-4o o1-mini DeepSeek-R1 MiMo-Base Qwen2.5-32B-Inst - - DeepSeek-V3 Qwen3-4B-Base Qwen3-4B-Instruct (non-think) General-Reasoner-4B - Qwen3-4B-Base Qwen3-4B-Base Qwen2.5-7B-Base Qwen2.5-7B-Instruct Open-Reasoner-Zero Nemotron-CrossThink SimpleRL-Qwen2.5-7B-Zoo General-Reasoner-7B - Qwen2.5-7B-Base Qwen2.5-7B-Base Qwen2.5-7B-Base Qwen2.5-7B-Base Qwen2.5-7B-Base Micro 58.6 52.0 74.6 80.3 84.0 4B Models 51.6 61.8 62.8 7B Models 47.7 57.0 59.4 57.8 51.5 58.9 14B Models - Qwen2.5-14B-Base Qwen2.5-14B-Base Qwen2.5-14B-Instruct Qwen2.5-14B-Base SimpleRL-Qwen2.5-14B-Zoo General-Reasoner-Qw2.5-14B Qwen2.5-14B-Base Qwen3-14B-Base Qwen3-14B-Instruct (non-think) General-Reasoner-Qw3-14B - Qwen3-14B-Base Qwen3-14B-Base 53.3 62.7 64.0 66.6 64.2 70.9 70.3 Acc 54.4 54.5 50.0 60.0 71.5 26.3 41.7 42.9 29.3 33.8 36.6 38.5 24.2 38.8 32.8 41.4 39.4 43.4 45.9 54.8 56.1 SuperGPQA Macro (discipline) TheoremQA BBEH Micro Acc 40.5 43.6 46.3 45.2 59.9 25.4 32.1 32.5 26.7 30.7 32.8 29.1 29.9 34.2 30.7 35.8 35.7 39.5 36.5 39.8 39.9 38.8 48.4 43.6 53.1 59. 34.8 42.0 48.3 29.1 36.6 37.4 - 38.0 45.3 33.0 41.9 40.8 44.3 44.0 42.4 54.4 11.4 22.6 22.3 - 34.9 8.1 14.9 12.2 8.0 12.2 12.2 - 11.9 12. 10.8 15.2 13.6 15.2 13.0 19.2 17.3 Table 3: Math task accuracy across datasets. Most evaluations are based on greedy decoding except for AIME24 and AIME25 (averaged over 32 sampling runs following SimpleRL [2]). Model Name AVG MATH-500 Olympiad Minerva GSM8K AMC AIME24 AIME25 Qwen3-4B-Base Qwen3-4B-Instruct (non-think) General-Reasoner-4B Qwen2.5-7B-Base Qwen2.5-7B-Instruct SimpleRL-Qwen2.5-7B-Zoo General-Reasoner-7B Qwen2.5-14B-Base Qwen2.5-14B-Instruct SimpleRL-Qwen2.5-14B-Zoo General-Reasoner-Qw2.5-14B Qwen3-14B-Base Qwen3-4B-Instruct (non-think) General-Reasoner-Qw3-14B 22.5 54.2 53.4 34.7 46.3 48.4 48.5 37.0 49.9 50.7 53.9 49.9 57.0 58.8 4B Models 16.4 49.0 47. 7B Models 28.6 39.4 41.9 37.9 14B Models 33.5 44.7 44.6 42.1 44.3 52.4 51.9 38.8 80.4 80.6 60.2 75.0 74.0 76. 65.4 77.4 77.2 78.6 74.6 82.0 83.8 18.4 57.0 57.7 36.0 45.2 49.6 54.0 24.3 52.2 54.0 58.1 55.9 59.9 68.0 39.6 92.0 92.2 83.1 90.9 90.7 92. 91.6 94.5 94.2 94.2 93.2 93.9 94.4 27.5 62.5 60.0 30.0 52.5 60.0 55.0 37.5 57.5 60.0 70.0 55.0 57.5 70.0 10.3 22.5 20.0 3.8 12.5 15.2 13. 3.6 12.2 12.9 17.5 14.7 28.5 24.4 6.7 16.1 15.4 1.4 8.5 7.5 10.4 2.9 11.0 11.8 16.9 11.4 25.1 19.2 4.3 Baselines Our main baselines are listed as follows: 1) Qwen2.5 and Qwen3 family [19]: We include these model results to understand the gains of our RL training. 2) SimpleRL [2]: comprehensive collection of math-based RL models, 3) Open-Reasoner-Zero [30]: strong RL reasoning model to replicate DeepSeek-R1, which also focused on math training, 4) Nemotron-CrossThink [24]: an all-domain reasoning model. This model is the closet to our general-purpose reasoning model. 4.4 Main Results Table 2 summarizes the performance of our General-Reasoner across general reasoning benchmarks. Overall, General-Reasoner with Zero RL consistently outperforms both base and supervised fine-tuned models across the Qwen2.5 and Qwen3 backbones. 7 For models initialized with Qwen2.5-7B-Base, General-Reasoner achieves 58.9% on MMLU-Pro, surpassing both the base model at 47.7% and the instructed model at 57.0%. These gains also extend to GPQA and SuperGPQA. Similar improvements appear with the 14B backbone: General-ReasonerQw2.5-14B reaches 66.6% on MMLU-Pro, outperforming Qwen2.5-14B-Base at 53.3% and Qwen2.514B-Instruct at 62.7%. It also shows strong results on math-related benchmarks, achieving high average scores for both 7B and 14B variants, as shown in Table 3. Compared to other reinforcement learning methods, General-Reasoner consistently outperforms both SimpleRL and NemotraonCrossThink across MMLU-Pro, GPQA, SuperGPQA, and BBEH. The trend holds with 14B models, where General-Reasoner achieves the best overall results. Stronger results are observed when initializing General-Reasoner with Qwen3 backbones. For example, General-Reasoner-4B surpasses Qwen2.5-7B after Zero RL, reaching 62.8% on MMLUPro versus 58.9%. This demonstrates the efficiency and transferability of our training method across model scales. The best performing version is General-Reasoner-Qw3-14B, which achieves 56.1% on GPQA and 54.4% on TheoremQA, matching the performance of commercial models like GPT4o, which scores 50.0% and 43.6% respectively, despite relying solely on Zero RL. Compared to Qwen3-14B-Instruct (non-think), which undergoes post-training via distillation from much bigger teacher model, our model maintains an advantage on many benchmarks. Notably, even in non-think mode, the Qwen3-Instruct model continues to generate CoT outputs. While some performance gaps remain relative to closed-source or closed-data systems, our results underscore the promise of Zero reinforcement learning when combined with domain-diverse reasoning data and compact, modelbased verifier. Although performance gap remains on some benchmarks compared to closed-source or closed-data models, our results underscore the promise of Zero reinforcement learning when combined with domain-diverse reasoning data and compact generative verifier. Notably, our model does not exhibit overthinking. During training, the average response length grows to around 1,000 tokens, which is significantly shorter than methods like DeepScaleR [4], where outputs can be around 32k tokens. As an example, on the Computer Science split of MMLU-Pro, DeepScaleR-1.5B-Preview requires 18 minutes on 4H100 GPUs to achieve 35% accuracy. In contrast, our General-Reasoner-4B finishes in just 1.5 minutes with higher accuracy of 61%."
        },
        {
            "title": "5 Analysis and Ablation Study",
            "content": "5.1 Impact of Data Abundance Table 4: Model performance trained with the diverse domain reasoning data vs. math-only data. Backbone Data MMLU-Pro GPQA SuperGPQA Math-Related Qwen2.5-7B-Base Qwen2.5-7B-Base Full Math Only Qwen2.5-14B-Base Qwen2.5-14B-Base Math Only Full 58.9 56.9 66.6 64. 34.3 32.8 43.4 38.9 34.2 29.8 39.5 35.6 48.5 49.1 53.9 48. To quantify how domain diversity in the training data affects reasoning performance, we compare two variants of our Zero RL setup: one trained on the full, diverse-domain dataset and one trained exclusively on math-related questions. Table 4 reports results for both Qwen2.5-7B-Base and Qwen2.5-14B-Base backbones under these two conditions. For the 7B backbone, restricting training to math tasks yields one point gain on the Math-Related benchmark (49.1% vs. 48.5%), but comes at the expense of general reasoning: MMLU-Pro, GPQA and SuperGPQA each drop by roughly 2 percentage points when compared to the full-data model. In contrast, the model trained on the full dataset achieves stronger performance across all benchmarks, demonstrating more general reasoning capabilities. The 14B backbone exhibits an even clearer benefit from data diversity. With full data, General-Reasoner-Qw2.5-14B outperforms its mathonly counterpart on every metric, improving both general benchmarks. These results confirm that training on diverse reasoning domains enhances general reasoning while maintaining or improving mathematical reasoning. 8 Table 5: Zero RL training using our modelbased verifier versus the rule-based verifier on the Qwen3-4B-Base model for 120 step. Dataset Model-Based Rule-Based MMLU-Pro GPQA SuperGPQA Math-Related 60.1 39.4 30.5 50.4 58.1 37.9 30.1 50.0 Figure 4: MMLU-Pro evaluation score at different training step using model-based verifier and rule-based verifier. Figure 5: Agreement comparison between rule-based and model-based verifiers against Gemini-2.0Flash, conditioned on Gemini verifying an answer as correct. Left: Agreement by answer type. Right: Agreement by task subject. 5. Impact of General-Verifier on Model Training We conduct an ablation study to verify the effectiveness of our model-based verifier by comparing models trained using either our model-based verifier or traditional rule-based verifier4. Both setups train the Qwen3-4B-Base model for 120 steps under identical conditions. As shown in Table 5, training with the model-based verifier achieves higher accuracy across all evaluated benchmarks compared to the rule-based verifier. Figure 4 provides detailed comparison of performance progression on the MMLU-Pro dataset throughout training. We observe that the rule-based verifier reaches an early plateau at around step 60 (approximately 58%), while the model-based verifier continues improving, achieving about 60% accuracy by step 120. This demonstrates that the model-based verifier effectively guides reinforcement learning, enabling the model to leverage diverse training data better and ultimately achieve stronger overall reasoning performance. 5.3 Verifier Agreement with Gemini-2.0-Flash To further assess the advantage of the our model-based verifier over the rule-based verifier, we study their agreement with Gemini-2.0-Flash. As discussed earlier, critical limitation of the rule-based verifier is its high false-negative rate. In sample of 50k answer-verification pairs that Gemini deemed correct, the rule-based verifier achieved only 22.2% agreement on average, whereas our model-based verifier achieved 78.7%. We further analyze agreement across different answer types as shown in Figure 5-left. False negatives are most prevalent in unstructured answer types such as Lists or Strings. Even in structured formats like multiple choice, variations like answers being expressed as textual descriptions rather than option letters can cause discrepancies. In Figure 5-right, domain-level analysis shows that the model-based verifier is particularly beneficial for non-math STEM fields like Physics and Engineering, where answer formats are diverse. In contrast, fields such as Economics tend to use more structured answers like multiple choices, which narrows the performance gap between the two verifiers. 4The rule-based verifier is implemented based on https://github.com/huggingface/Math-Verify"
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we introduce paradigm for enhancing the general reasoning capabilities of large language models by leveraging model-based verifier to scale verifiable training data across diverse domains. By applying GRPO training directly to base LLMs using our curated, high-quality, verifiable dataset and generative model-based verifier, we demonstrate competitive reasoning performance against models that require an additional supervised fine-tuning stage. Our approach achieves strong generalization across various challenging domains while preserving superior effectiveness in mathematical reasoning tasks."
        },
        {
            "title": "References",
            "content": "[1] DeepSeek-AI Team. DeepSeek-R1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. [2] Weihao Zeng, Yuzhen Huang, Qian Liu, Wei Liu, Keqing He, Zejun Ma, and Junxian He. Simplerl-zoo: Investigating and taming zero reinforcement learning for open base models in the wild, 2025. URL https://arxiv.org/abs/2503.18892. [3] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, Haibin Lin, Zhiqi Lin, Bole Ma, Guangming Sheng, Yuxuan Tong, Chi Zhang, Mofan Zhang, Wang Zhang, Hang Zhu, Jinhua Zhu, Jiaze Chen, Jiangjie Chen, Chengyi Wang, Hongli Yu, Weinan Dai, Yuxuan Song, Xiangpeng Wei, Hao Zhou, Jingjing Liu, Wei-Ying Ma, Ya-Qin Zhang, Lin Yan, Mu Qiao, Yonghui Wu, and Mingxuan Wang. DAPO: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv.org/abs/2503.14476. [4] Michael Luo, Sijun Tan, Justin Wong, Xiaoxiang Shi, William Y. Tang, Manan Roongta, Colin Cai, Jeffrey Luo, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepscaler: Surpassing o1-preview with 1.5b model by scaling rl, 2025. Notion Blog. [5] Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. Deepcoder: fully open-source 14b coder at o3-mini level, 2025. Notion Blog. [6] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. [7] Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. [8] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, Tianle Li, Max Ku, Kai Wang, Alex Zhuang, Rongqi Fan, Xiang Yue, and Wenhu Chen. MMLU-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. URL https:// openreview.net/forum?id=y10DM6R2r3. [9] Xiang Yue, Tianyu Zheng, Ge Zhang, and Wenhu Chen. MAmmoTH2: Scaling instructions from the web. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. URL https://openreview.net/forum?id=yVu5dnPlqA. [10] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. GPQA: graduate-level google-proof q&a benchmark, 2023. URL https://arxiv.org/abs/2311.12022. [11] M-A-P. SuperGPQA: Scaling LLM evaluation across 285 graduate disciplines, 2025. URL https://arxiv.org/abs/2502.14739. 10 [12] Wenhu Chen, Ming Yin, Max Ku, Pan Lu, Yixin Wan, Xueguang Ma, Jianyu Xu, Xinyi Wang, and Tony Xia. TheoremQA: theorem-driven question answering dataset. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 78897901, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.489. URL https://aclanthology.org/2023.emnlp-main.489/. [13] Mehran Kazemi, Bahare Fatemi, Hritik Bansal, John Palowitch, Chrysovalantis Anastasiou, Sanket Vaibhav Mehta, Lalit K. Jain, Virginia Aglietti, Disha Jindal, Peter Chen, Nishanth Dikkala, Gladys Tyen, Xin Liu, Uri Shalit, Silvia Chiappa, Kate Olszewska, Yi Tay, Vinh Q. Tran, Quoc V. Le, and Orhan Firat. Big-bench extra hard, 2025. URL https://arxiv.org/ abs/2502.19187. [14] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe. [15] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/ abs/2110.14168. [16] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems, 2024. URL https://arxiv.org/abs/2402.14008. [17] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. [18] OpenAI Team. Openai o1 system card, 2024. URL https://arxiv.org/abs/2412.16720. [19] Qwen Team. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. [20] Qwen Team. QwQ-32B: Embracing the power of reinforcement learning, March 2025. URL https://qwenlm.github.io/blog/qwq-32b/. [21] Xiaomi LLM-Core Team. Mimo: Unlocking the reasoning potential of language model from pretraining to posttraining. arXiv:2505.07608, 2025. URL https://arxiv.org/abs/2505. 07608. [22] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. [23] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. Qwen2.5-math technical report: Toward mathematical expert model via self-improvement, 2024. URL https://arxiv.org/abs/2409.12122. [24] Syeda Nahida Akter, Shrimai Prabhumoye, Matvei Novikov, Seungju Han, Ying Lin, Evelina Bakhturina, Eric Nyberg, Yejin Choi, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-CrossThink: Scaling self-learning beyond math reasoning, 2025. URL https://arxiv.org/abs/2504.13941. [25] Yi Su, Dian Yu, Linfeng Song, Juntao Li, Haitao Mi, Zhaopeng Tu, Min Zhang, and Dong Yu. Crossing the reward bridge: Expanding rl with verifiable rewards across diverse domains, 2025. URL https://arxiv.org/abs/2503.23829. [26] Gemini. Gemini: family of highly capable multimodal models, 2024. URL https://arxiv. org/abs/2312.11805. [27] Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them, 2022. URL https://arxiv.org/abs/2210.09261. [28] Aitor Lewkowycz, Anders Johan Andreassen, David Dohan, Ethan Dyer, Henryk Michalewski, Vinay Venkatesh Ramasesh, Ambrose Slone, Cem Anil, Imanol Schlag, Theo Gutman-Solo, Yuhuai Wu, Behnam Neyshabur, Guy Gur-Ari, and Vedant Misra. Solving quantitative reasoning problems with language models. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho, editors, Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=IFXTZERXdM7. [29] OpenAI. GPT-4o system card, 2024. URL https://arxiv.org/abs/2410.21276. [30] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. URL https://arxiv.org/abs/2503.24290."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Limitation This work focuses on the Zero RL setting, where models are trained directly from base language models without intermediate supervised fine-tuning. However, the proposed integration of modelbased answer verification and diverse-domain reasoning data is not tied to Zero RL. Exploring its impact when combined with intermediate stages, such as supervised fine-tuning or distillation is promising to have further performance gain. Additionally, our study targets general language reasoning tasks across diverse domains. We do not specifically focus on code reasoning or olympiad-level math competitions. Integrating these specialized reasoning domains into our framework is valuable avenue for future extension. A.2 Broader Impact This work improves the general reasoning capabilities of large language models, making them more applicable to real-world scenarios where complex, multi-domain reasoning is essential. By enabling LLMs to go beyond math and code, our approach allows AI systems to better support humans in tasks such as decision-making, analysis, and problem-solving across diverse fields, ultimately increasing their utility in production settings and high-impact applications. A.3 Detailed Results We provide detailed evaluation results on MMLU-pro, SuperGPQA and BBEH are listed in below. Following previous work, the average reported in MMLU-Pro is micro-average, and for SuperGPQA and BBEH, we report macro-average across subtask/domain. Table 6: Per-domain accuracy comparison of different models on MMLU-Pro. Model Name Qwen3-4B-Base General-Reasoner-4B Qwen2.5-7B-Base Qwen2.5-7B-Instruct SimpleRL-Qwen2.5-7B-Zoo General-Reasoner-7B Avg 51.6 62.8 47.7 57.0 51.5 58.9 53.3 Qwen2.5-14B-Base 64.2 Qwen3-14B-Base 62.7 Qwen2.5-14B-Instruct SimpleRL-Qwen2.5-14B-Zoo 64.0 General-Reasoner-Qw2.5-14B 66.6 General-Reasoner-Qw3-14B 70.3 CS Math Chem Eng Law Bio Health Phys Bus Phil Econ Other Psy Hist 49.3 61.7 51.7 57.1 54.9 61.5 54.6 70.2 66.6 66.1 69.8 73.9 67.8 81.6 58.9 71.4 55.4 75.5 63.0 77.4 75.3 75.8 78.8 86. 59.6 69.9 45.1 57.3 48.7 62.1 52.8 67.3 63.0 66.9 67.5 76.4 44.8 50.9 34.5 43.1 40.9 48.1 36.0 48.7 39.7 49.8 54.8 55. 26.0 29.2 25.5 31.3 30.8 32.1 31.9 33.2 37.4 37.2 39.7 39.9 67.2 79.8 64.6 71.4 68.3 71.7 71.3 80.8 79.6 79.5 81.7 83. 49.1 61.0 46.6 55.4 53.1 57.8 56.5 65.5 65.2 64.1 65.3 70.5 58.1 70.0 50.0 60.7 53.4 62.4 52.9 68.7 63.9 67.7 71.4 76. 59.2 71.4 55.4 64.8 58.0 66.2 61.1 70.7 69.3 69.5 71.9 76.6 33.1 48.9 34.9 44.9 41.3 44.3 46.1 53.5 53.5 55.5 56.7 58. 58.5 71.1 57.1 67.1 62.6 67.2 64.8 74.4 72.0 73.8 74.4 78.7 40.3 54.3 47.6 54.8 52.2 54.3 50.1 60.7 63.4 61.4 64.0 66. 53.8 66.3 54.8 63.0 60.7 63.5 61.0 69.7 72.1 70.9 73.2 73.3 34.4 47.8 39.1 48.3 42.5 47.0 44.4 54.6 59.1 53.8 59.1 58. Table 7: Per-domain accuracy comparison of different models on SuperGPQA. Model Name Avg. Eng. Med. Qwen3-4B-Base General-Reasoner-4B Qwen2.5-7B-Base Qwen2.5-7B-Instruct SimpleRL-Qwen2.5-7B-Zoo General-Reasoner-7B 25.4 32. 26.7 30.7 29.9 34.2 Qwen2.5-14B-Base 30.7 Qwen3-14B-Base 36.5 Qwen2.5-14B-Instruct 35.8 35.7 SimpleRL-Qwen2.5-14B-Zoo General-Reasoner-Qw2.5-14B 39.5 General-Reasoner-Qw3-14B 39.9 27.3 34.6 25.1 29.2 28.0 32.3 29.2 36.1 35.6 34.2 36.5 42.6 26.1 32. 26.7 31.2 31.3 34.5 31.2 38.8 37.1 36.7 41.6 44.9 Sci. 26.7 34.5 23.8 27.9 26.0 31.1 27.9 34.4 34.1 33.0 35.6 42. Phil. Mil. Sci. Econ. Mgmt. Socio. Lit./Arts Hist. Agron. Law Edu. 30.7 37.8 29.2 32.9 32.7 38.3 32.9 41.8 41.8 39.9 44.7 47.0 29.3 33. 31.7 33.7 31.5 36.5 33.7 40.3 39.5 41.3 42.9 42.7 23.1 36.4 28.0 36.4 34.3 41.3 36.4 38.5 39.2 38.5 42.7 41.9 19.9 24. 21.5 24.8 25.0 25.1 24.8 28.8 30.7 30.8 32.8 29.2 15.3 19.9 18.2 20.5 23.1 23.3 20.5 26.7 26.6 26.7 29.7 28.2 23.1 31. 25.6 27.4 27.2 29.5 27.4 33.6 32.2 31.1 37.9 34.5 29.3 29.4 27.7 31.3 29.4 34.2 31.3 36.4 36.1 38.1 39.6 34.5 28.7 34. 31.6 35.1 33.5 39.9 35.1 37.2 37.4 38.2 45.3 43.0 26.5 38.0 29.7 32.3 34.9 36.3 32.3 37.8 38.6 36.0 41.8 41.5 23.9 35. 28.8 36.1 32.2 42.4 36.1 44.4 36.1 40.0 41.9 45.9 13 Table 8: Per sub-task accuracy comparison of different models on BBEH. Model Qwen3-4B-Base General-Reasoner-4B Qwen2.5-7B-Base Qwen2.5-7B-Inst SimpleRL-7B-Zoo General-Reasoner-7B Avg 8.1 12.2 8.0 12.2 11.9 12.5 10.8 Qwen2.5-14B-Base 13.0 Qwen3-14B-Base 15.2 Qwen2.5-14B-Inst SimpleRL-14B-Zoo 13.6 General-Reasoner-Qw2.5-14B 15.2 General-Reasoner-Qw3-14B 17.3 BGQ Bool Bug Caus Disam Dyck Geom Hyp Ling Movie MultiA NYCC ObjC ObjP Sarc Shuf Spat Sport Temp TimeA Web Sort Zebra 36.0 33.5 26.5 36.0 29.5 28.5 34.5 36.0 43.5 29.0 35.0 33.0 12.5 14.0 10.0 5.5 9.5 15. 13.0 10.5 22.5 7.0 24.0 21.5 1.5 1.0 0.0 0.5 0.5 1.0 1.5 1.0 1.5 0.0 0.0 2.0 41.5 47.0 39.0 48.5 45.0 42. 48.5 40.5 50.5 50.0 50.5 49.5 18.3 40.8 18.3 37.5 44.2 45.8 41.7 45.0 50.0 48.3 45.8 53.3 0.0 4.5 3.0 0.5 0.5 2. 0.5 5.0 2.0 4.0 2.5 4.0 3.5 23.5 9.5 24.5 27.0 19.5 9.0 19.5 39.0 33.0 22.5 24.5 0.5 0.5 1.0 1.5 0.0 0. 2.5 2.5 9.0 1.0 0.5 2.5 1.5 4.0 1.5 4.0 3.0 3.5 3.0 5.0 7.0 3.5 6.0 9.0 28.5 39.0 30.0 37.0 28.5 34. 31.0 41.0 43.0 42.5 53.5 49.0 0.0 5.0 0.0 0.0 1.0 1.0 0.5 3.5 3.0 0.5 4.5 18.5 10.0 12.5 6.0 9.0 9.5 12. 11.5 12.0 13.5 13.5 14.0 10.5 0.5 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 0.0 1.5 0.5 1.5 0.0 0.5 3.5 2.0 1. 0.0 1.0 2.0 2.5 2.0 1.5 6.0 16.0 11.5 17.0 16.5 18.0 17.0 25.0 29.0 25.0 22.0 25.5 3.0 1.5 2.0 9.0 8.5 8. 4.5 7.0 6.0 1.0 7.5 10.0 0.5 7.0 5.0 8.0 8.0 9.5 0.5 5.5 10.0 8.5 13.0 13.5 10.5 16.5 3.5 15.5 12.0 15. 10.0 14.0 19.5 19.5 18.5 22.0 2.5 3.0 4.5 0.5 3.0 1.5 1.0 1.5 2.0 2.0 2.0 2.0 11.0 19.0 11.0 21.0 16.0 18. 16.0 19.5 28.5 23.5 10.5 43.5 8.0 12.0 5.5 9.0 14.0 8.0 8.0 9.0 13.0 13.0 18.0 17.0 0.5 6.5 0.5 2.5 5.5 4. 6.0 6.5 4.5 8.0 9.0 8.5 9.5 0.5 16.5 16.0 14.5 22.0 15.0 13.5 19.0 6.0 16.5 6.5 A.4 Verifier Input-Output Example Verifier Input: User: ### Question: Solve the Bernoulli equation ### Ground Truth Answer: ### Student Answer: dy dx + = x2y2 1 = x2 + Cx = 1 Cx x2 For the above question, please verify if the students answer is equivalent to the ground truth answer. You dont need to redo the question yourself, just check if the students answer equivalent to the ground truth answer. If the students answer is correct, output \"Final Decision: Yes\". If the students answer is incorrect, output \"Final Decision: No\". Verifier Output: Assistant: 1. Ground Truth Answer: 2. Student Answer: We can rewrite the ground truth answer as: 1 = x2 + Cx = 1 Cx x2 1 = Cx x2 = 1 Cx x2 Comparing this with the students answer, we see that they are the same. Final Decision: Yes A.5 Detailed Hyper-Parameters We provide the detailed hyperparameters for training our General-Reasoner variants in Table 9. The difference in batch size configurations between the Qwen2.5 and Qwen3 series is due to the limitations in earlier versions of vLLM (e.g., v0.6.3), which did not support engine sleep and wake mechanisms for properly loading and unloading the verifier model. As result, we had to dedicate 2 GPUs to the verifier and 6 GPUs to policy model for each node. With the newer vLLM version (v0.8.5), we are able to share GPU resources more efficiently by leveraging proper parameter loading and unloading. 14 Table 9: Hyperparameter settings for General-Reasoner variants. parameter General-Reasoner-7B General-Reasoner-Qw2.5-14B General-Reasoner-4B General-Reasoner-Qw3-14B n_nodes n_gpu backbone_hgf_id vllm_version train_batch_size max_prompt_length max_response_length learning_rate ppo_mini_batch_size ppo_micro_batch_size clip_ratio_low clip_ratio_high kl_loss_coef kl_loss_type temperature rollout_n kl_coef 4 8H100 Qwen/Qwen2.5-7B 0.6.3 768 1024 4096 5e-7 192 4 0.3 0.3 0.0001 low_var_kl 1.0 8 0. 4 8H100 Qwen/Qwen2.5-14B 0.6.3 768 1024 4096 5e-7 192 4 0.3 0.3 0.0001 low_var_kl 1.0 8 0.001 4 8H100 Qwen/Qwen3-4B-Base 0.8.5 1024 1024 4096 5e-7 256 4 0.2 0.3 0.0001 low_var_kl 0.7 8 0.001 4 8H100 Qwen/Qwen3-14B-Base 0.8.5 1024 1024 4096 5e-7 256 4 0.2 0.3 0.0001 low_var_kl 0.7 8 0."
        }
    ],
    "affiliations": [
        "M-A-P",
        "Singapore",
        "TikTok",
        "University of Waterloo",
        "Vector Institute"
    ]
}