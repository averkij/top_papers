{
    "paper_title": "3AM: Segment Anything with Geometric Consistency in Videos",
    "authors": [
        "Yang-Che Sun",
        "Cheng Sun",
        "Chin-Yang Lin",
        "Fu-En Yang",
        "Min-Hung Chen",
        "Yen-Yu Lin",
        "Yu-Lun Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2's appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++'s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/"
        },
        {
            "title": "Start",
            "content": "3AM: 3egment Anything with Geometric Consistency in Videos Yang-Che Sun1 Cheng Sun2 Chin-Yang Lin1 Fu-En Yang2 Min-Hung Chen2 Yen-Yu Lin1 Yu-Lun Liu1 2NVIDIA Research 1National Yang Ming Chiao Tung University 6 2 0 2 3 ] . [ 1 1 3 8 8 0 . 1 0 6 2 : r Figure 1. Overview of 3AM. Given an input video or an unconstrained photo collection, 3AM takes user-provided prompt, e.g., mask, point, or box, and produces consistent object track across all views. Our method maintains cross-view correspondence even under large viewpoint changes, cluttered scenes, and variations in capture conditions, enabling robust object tracking from both videos and casual multi-view image sets."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, training-time enhancement that integrates 3Daware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2s appearance features, the model achieves geometryconsistent recognition grounded in both spatial position and visual similarity. We propose field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++s Selected Subset, improving over state-of-theart VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/ Video object segmentation (VOS) identifies and tracks target objects throughout video sequence with consistent masks across frames. This is fundamental to autonomous driving, robotics, augmented reality, and video editing. critical challenge is maintaining object identity under large viewpoint variations, temporary disappearances, or similar distractors. Traditional VOS methods rely on 2D appearance features and fail when the object appears dramatically different from varying viewpoints. For embodied AI in dynamic 3D environments, consistently recognizing objects across diverse viewpoints is essential for robust scene understanding. Recent video object segmentation advances follow two parallel lines. First, 2D foundation models with memorybased architectures: SAM2 [46] introduced promptable segmentation with streaming memory and spatio-temporal attention. SAM2Long [16] employs memory trees for long-range identity preservation, while DAM4SAM [56] incorporates distractor-aware updates. However, purely 2D approaches fail under large viewpoint changes as appearance features cannot establish reliable correspondence. Second, 3D instance segmentation methods: proposal-centric approaches like Mask3D [48] and OneFormer3D [28] operate on point clouds, while projection-based methods like Open3DIS [39], SAM3D [76], and SAM2Object [84] lift 2D masks to 3D. 1 Figure 2. Limitations of traditional VOS and 3D segmentation approaches, and an overview of our capability. (a) Traditional VOS methods such as SAM2 [46] lose track when the camera undergoes large viewpoint changes, causing masks to drift or disappear. (b) 3D segmentation approaches rely on accurate camera poses and explicit 3D mask merging; they often propagate errors when the 3D reconstruction is incomplete or noisy. (c) Our 3AM consistently tracks object instances across drastic viewpoint changes without requiring camera poses or 3D ground-truth masks, demonstrating robust cross-view correspondence purely from geometry-aware 2D tracking. These require camera poses, depth maps, preprocessing, and super-linear computational scaling. 3AM achieves 90.6% IoU and 71.7% Positive IoU versus SAM2Longs 74.7% and 41.3% (+15.9 and +30.4 points). Both paradigms have critical gaps  (Fig. 2)  . 2D methods like SAM2 excel at efficiency but lack geometric awareness, failing under viewpoint variation (Fig. 2a). MOSEv2 [15] shows significant degradation under wide-baseline changes. Conversely, 3D methods achieve better consistency but require explicit 3D inputs (Fig. 2b) and struggle to generalize. 2D-to-3D lifting suffers from view-inconsistent predictions [23, 42, 53, 70, 84]. End-to-end approaches like PanSt3R [85] require offline processing. Can we achieve 3D-aware, viewpoint-robust segmentation while preserving promptability and efficiency, without explicit 3D supervision? (Fig. 2c) We introduce 3AM  (Fig. 1)  , training-time enhancement to SAM2 that integrates 3D-aware features from reconstruction models. Our key insight is that MUSt3R [7] encodes implicit geometric correspondence through features learned from multi-view consistency. By fusing multi-level MUSt3R features with SAM2s appearance features via lightweight Feature Merger, we enable geometry-consistent recognition based on spatial position. This geometric awareness is learned from posed RGB video during training, but at inference requires only RGB input and prompts. We introduce field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions. On ScanNet++ [82] and Replica [50] with wide-baseline motion, In summary, our contributions are: key observation on the gap between 2D tracking and 3D consistency. 2D tracking is fundamentally limited under large viewpoint variations. Existing 3D approaches require camera poses, depth, or 3D masks, often unavailable in practice. This motivates our geometry-aware framework, which achieves cross-view consistency without 3D supervision at inference. 3D-aware feature integration framework fusing geometric features from 3D reconstruction models with SAM2s appearance features, and field-of-view aware training strategy ensuring geometric consistency for effective 3D correspondence learning. Comprehensive experimental validation demonstrating substantial improvements over SAM2 on challenging datasets (ScanNet++, Replica), particularly on object reappearance and viewpoint changes. 2. Related Work 2D Video Object Segmentation. Video object segmentation (VOS) segments and tracks target objects across video sequences with temporally coherent masks [8, 37, 43, 57, 64]. Early approaches relied on appearance propagation or optical flow but struggled with long-term consistency and occlu2 Figure 3. 3AM Pipeline Overview. Our Feature Merger fuses multi-level MUSt3R features, learned from multi-view consistency to encode implicit geometric correspondence, with SAM2s appearance features via cross-attention and convolutional refinement. These merged geometry-aware representations then undergo memory attention with previous frames and mask decoding, enabling spatially-consistent object recognition that maintains identity across large viewpoint changes without requiring camera poses at inference. sion [4, 40, 74]. Recent memory-based architectures leverage spatio-temporal attention and feature retrieval for improved object association [1113, 31, 35, 41, 69, 7779]. SAM2 [46], promptable model with streaming memory supporting points, boxes, and masks, achieves strong results across benchmarks. Follow-up efforts extend SAM2 [16, 56, 72, 75, 81]: SAM2Long [16] introduces memory trees for long-range identity preservation, while DAM4SAM [56] incorporates distractor-aware updates for cluttered scenes. Despite these advances, large viewpoint changes, occlusion, and disappearance-reappearance events cause significant degradation, as shown by MOSEv2 [15, 45, 68]. To address these limitations, our method integrates 3D-aware representations into SAM2 for object-consistent segmentation under challenging viewpoint and appearance variations. 3D Instance Segmentation. Mainstream 3D instance segmentation follows two tracks. Proposal-centric methods operate natively in 3D, predicting instance masks from point clouds with learned proposals, often requiring large-scale 3D supervision and suffering from domain shifts [19, 24, 28, 38, 44, 48, 52, 53, 59]. complementary line lifts 2D results to 3D by projecting per-view masks and merging across views, but still depends on posed RGB-D and point clouds for mask consolidation [3, 5, 21, 2527, 32, 39, 55, 58, 66, 70, 76]. Pure 2D promptable segmentation rarely suffices for objectaligned 3D instances because cross-view association and temporal consistency are not enforced by image-space inference alone. Consequently, multi-view fusion often exhibits view-inconsistent masks, error accumulation, and fragmented 3D instances [23, 42, 53, 70, 84]. In contrast, our approach imposes 3D-aware self-consistency directly on video, preserving identity and geometry without 3D annotations or point-cloud mask merging. End-to-End 3D-Aware Methods. Recent progress in endto-end 3D reconstruction shows modern models can directly infer geometric structure from 2D inputs, enhancing downstream 3D perception tasks [7, 9, 10, 17, 29, 33, 36, 49, 51, 6063, 65, 73, 80]. This has fueled end-to-end 3D-aware segmentation frameworks that unify geometric reasoning and object parsing [1, 22, 30]. PanSt3R [85] integrates 3D-aware features from MUSt3R [7] with transformer decoder to jointly predict geometry and panoptic segmentation, achieving multi-view consistent segmentation through learnable quadratic fusion. However, it is incompatible with promptable segmentation backbones (e.g., SAM2), limiting interactive inputs and open-vocabulary segmentation. Moreover, it requires offline access to entire image sequences, preventing online or streaming use. Our method introduces 3D-aware features into the SAM2 framework, pairing geometric consistency with promptability and broad generalization to enable object-consistent segmentation and tracking without additional 3D supervision. 3. Method 3.1. Problem Setting Given an RGB image and user-provided prompt (which can be mask, box, or point), our goal is to identify the same object throughout video sequence while maintaining consistent segmentation, i.e., avoiding false correspondences or identity switches across frames. Previous approaches often fail to achieve this goal. 2D-based methods (e.g., SAM2[46]) rely solely on image-space appearance features, and thus are sensitive to camera viewpoint or distance changes. They also struggle with visually similar or confusing objects, as partially addressed by DAM4SAM[56], leading to frequent tracking failures as the frame sequence grows longer. 3D-based methods (e.g., Mask3D[48], OpenYOLO3D[5], EmbodiedSAM[71]) attempt to localize objects in space using 3D mask proposals or dense 2D masks combined with camera poses, but this requires running structure-from-motion (SfM) or similar pipelines. As the number of video frames increases, both runtime and computational complexity grow rapidly, making these approaches impractical for long sequences or large scenes. To address these limitations, we aim to exploit 3Daware information while maintaining 2D segmentation framework. Specifically, we encourage the model to perform feature matching that is geometry-consistent, enabling it to identify the same object by recognizing its correspondence 3 Figure 4. Illustration of Features for Feature Merging. The heat map is computed using the cosine similarity between the red query point and the target frame. As illustrated in the lower row, vanilla SAM2 fails under large viewpoint changes. In contrast, as the MUSt3R feature hierarchy gradually shifts from semantic correspondence toward the point-cloud domain, we select intermediate layers to preserve both semantic relevance and geometric structure. By combining MUSt3Rs geometric cues with SAM2s visual semantics, the merged feature Fmerged provides significantly more reliable localization of the target object. in 3D space. 3.2. Architecture The overall pipeline is shown in Fig. 3. For each video frame i, our model extracts two complementary feature streams that capture appearance cues and multi-view geometric consistency. First, the frame is processed by the SAM2 vision encoder, producing 2D appearance feature map (i) 2D. In parallel, the same frame is passed into MUSt3R, which attends to its own multi-view memory through MUSt3Rs internal cross-attention mechanism, yielding geometry-aware fea2D and ture map 2D are fused into refined representation merged. The merged feature is processed by the memory attention module and the mask decoder, and is finally encoded by the memory encoder to produce memory features that will be referenced by future frames. We provide comparison of different memory selection methods in Sec. 4. 3D. Next, (i) Feature Merging. As shown in Fig. 4, SAM2 becomes unreliable when the object reappears or when the viewpoint changes drastically. The heat map in the lower row demonstrates that SAM2 cannot reliably associate the query point with its target under large angular differences, revealing its limited capacity to maintain object identity when the visual appearance changes substantially. To address this issue, we enhance SAM2 with MUSt3R features. The upper row of Fig. 4 visualizes MUSt3R activations across depth. Early layers preserve clearer correspondence pattern with the query point, while deeper layers produce more diffuse responses. This shift reflects MUSt3Rs progression toward point-cloud decoding: the representation becomes more geometry-oriented but less semantically aligned with the 2D query. Consequently, relying solely on either early or late layers is insufficient; early layers lack explicit geometric structure, whereas late layers lose semantic clarity. We therefore sample MUSt3R features from both early and late stages to capture complementary information. These multi-level MUSt3R features are processed by our Feature Merger, which jointly performs cross-attention fusion and convolutional refinement. The cross-attention blocks first integrate the sampled MUSt3R layers into single geometryaware feature, aligning information from different depths. Specifically, as shown in Fig. 4, we take the MUSt3R encoder feature at the shallowest sampled depth, i.e., the most semantically rich layer, as the initial input to the Feature Merger. It first passes through self-attention block to establish an initial semantic representation. The remaining sampled MUSt3R features are then incorporated one by one through sequence of cross-attention layers, where the current merged representation acts as the query and each additional MUSt3R feature provides the keyvalue pairs. In this way, the Feature Merger progressively integrates information from both shallow and deeper stages of MUSt3R. The output is then fed into convolutional stage that merges it with SAM2s 2D feature F2D for restoring fine spatial detail. We further compare different 3D foundation models in the ablation study and more architecture details and comparison are provided in the supplementary materials. The final merged feature, Fmerged, retains SAM2s high-resolution, segmentation-aware appearance cues while incorporating MUSt3Rs geometry-aware consistency, enabling robust object localization even under severe viewpoint changes. Mask Decoding. The output of the memory-attended features is fed into the SAM2 mask decoder to produce the segmentation mask for frame i. The resulting merged features are then encoded and appended to the memory, enabling temporally consistent object identification in subsequent frames. 3.3. Training Frame Sampling key challenge in training our model is the limited capacity of the memory module (SAM2 maintains at most eight memory slots). To enable the model to learn robust object identification across diverse camera viewpoints, we must carefully design how training frames are sampled from each video sequence. Naıve Random Sampling. straightforward strategy is to randomly sample frames from the video as in Fig. 5b. This exposes the model to broad distribution of camera viewpoints and object appearances. Random sampling also 4 (a) Vanilla continuous sampling strategy (b) Naive random sampling without considering field-of-view (c) Field-of-viewaware sampling strategy (Ours) Figure 5. Overview of our sampling strategy during training. (a) Continuous sampling provides densely spaced frames but offers limited viewpoint diversity. (b) Naıve random sampling introduces viewpoint variation but may select frames that observe spatially disjoint regions of the same object. For example, frame 0 shows the left side of the couch while frame 1 shows the right side. Because these regions are far apart in 3D space, treating them as the same supervisory signal forces the model to match inconsistent geometry and leads to ambiguous learning. (c) Our field-of-viewaware sampling retains only frames whose masked 3D points lie within the reference camera frustum over threshold, ensuring consistent geometric overlap while preserving natural pose and occlusion variation. naturally introduces distracting or visually similar objects in the scene , which encourages the model to rely on 3D-aware alignment rather than purely local 2D similarity. Object Spanning Problem However, naıve random sampling introduces failure mode when an object occupies large spatial extent. Consider bed, table, or cabinet: two randomly sampled frames may both contain the same instance, yet correspond to spatially distant regions (e.g., one frame at the headboard, the other at the footboard). Since our objective is to enforce 3D location consistency, which is to encourage the model to identify an object by recognizing that it lies in the nearby 3D position; thus, these wide-baseline views of large object may contradict the intended training signal. Although the two frames belong to the same instance, their projected 3D locations are far apart, which can confuse the learning objective. Field-of-View Sampling. For each training iteration, the first sampled frame is designated as the reference frame. To select the remaining 1 frames, we apply FOV-based filter using camera poses and depth. For every candidate frame, we back-project its object mask into 3D, transform the points into the reference frame, and re-project them onto the reference image. frame is kept only if sufficient fraction of its masked 3D points fall inside the reference camera frustum: Table 1. Datasets used for training. We apply FOV-aware sampling only when ground-truth geometry is available. ScanNet++ [82] ASE [2] MOSE [14] Real Dynamic FOV sampling prob #Scenes / Videos 0.8 0.8 2612 0.0 1453 This ensures that selected frames observe overlapping physical regions of the object, avoiding degenerate cases where two views of the same instance correspond to distant, nonoverlapping parts. We do not filter out partially occluded frames: frustum overlap reflects alignment in viewing direction, not full visibility, and retaining occlusion cases helps the model learn to differentiate viewpoint changes from true object absence. 4. Experiments Training Setup. We train our model for 1M iterations using the AdamW optimizer with batch size of 1, where we only set the Memory Attention, the Mask Decoder, and the Feature Merger to trainable with learning rates of 5e-6, 5e-6, and 1e-5, respectively. All loss coefficients and memory frames, which are set to 8, follow the original SAM2 configuration. #{candidate masked points inside reference frustum} #{masked points in candidate frame} > τ. Datasets. Intuitively, to equip the model with the ability to segment objects consistently across scene, it is necessary to 5 train on 3D-based datasets that record long video sequences covering diverse camera viewpoints, along with corresponding segmentation masks and RGB frames. However, the 2D masks in such 3D-based datasets are often projected from point cloud segmentations, which can be degraded due to the sparsity of the point cloud and projection errors. To mitigate the imperfect 2D mask annotations common in 3D-based datasets, we train on hybrid mixture of synthetic, real, and video segmentation datasets. Specifically, the Aria Synthetic Dataset(ASE) [2] provides clean geometric supervision, ScanNet++ [82] offers realistic indoor 3D consistency, and MOSE[14] maintains the original temporally coherent masks. This combination enables balanced spatial and temporal learning. Note that to remain consistent with our sampling strategy described in Sec. 3.3, we apply continuous sampling on MOSE, as it does not provide camera poses or depth maps. For ASE and ScanNet++, which include calibrated geometry, we adopt mixed policy: continuous sampling with probability 0.2 and our FOV-aware sampling with probability 0.8. The FOV threshold τ is set to 0.25. To apply our sampling strategy, we first run MUSt3R inference on every scene in ScanNet++ and MOSE and store the resulting features. These precomputed features are then used during training to enable FOV-aware sampling. For continuous sampled batches, MUSt3R is executed on the fly. 4.1. 2D Evaluation. To demonstrate the capability of our model, we evaluate its performance on 2D object tracking under challenging camera motion. Specifically, we focus on scenarios where the camera undergoes significant translation and rotation, and where objects may disappear and later reappear. Traditional VOS benchmarks such as LVOS [20], VOST [54], DAVIS [45], and YTOS [68] primarily assume relatively fixed camera and stable scene surroundings, making them insufficient for testing robustness under large viewpoint changes. To address this limitation, we use 3D-based datasets, ScanNet++ [82] and Replica [50], which naturally provide extensive camera trajectory variation due to their 3D reconstruction requirements. Their wide-baseline viewpoints make them suitable for evaluating consistency under severe pose changes. For each scene in both datasets, we use ground-truth camera poses to project 3D instance masks onto the 2D frames. We then select the frame with the largest visible mask area as the conditioning frame and perform forward and backward tracking from that reference. Compared Methods. For comparison, we use SAM2 as the state-of-the-art VOS baseline. We also include the following works: SAM2Long [16] and DAM4SAM [56], which enhance SAM2 by improving its memory selection for more robust tracking. For fair comparison, we additionally finetune the original SAM2 under our dataset setting. Note that our method, 3AM, uses the original SAM2 memory-selection mechanism in all reported results unless specified otherwise. Metrics. We use three complementary metrics to evaluate tracking quality. IoU is computed over all frames, including those where the object is absent, reflecting overall stability. Positive IoU measures accuracy only on frames where ground-truth mask is present. Successful IoU further restricts to visible frames where the prediction has non-zero overlap with ground truth, capturing accuracy conditioned on successful localization. These metrics assess robustness to disappearance and accuracy when tracking succeeds. 4.1.1. ScanNet++ The results are shown in Tab. 2, and we provide visualization at Fig. 8. On the ScanNet++ dataset, we further evaluate performance on Selected Subset specifically constructed to emphasize object reappearance. For each object track, we count the number of non-continuous visible segments, where segment is defined as any contiguous span of visible frames whose length ℓ exceeds minimal threshold ℓmin. Objects with frequent reappearance are typically associated with large viewpoint changes, since each disappearancereappearance cycle often corresponds to observing the object from substantially different angle or position. This subset therefore provides focused evaluation of robustness under severe pose variation. On the Whole Set, 3AM achieves the highest performance across all metrics, with an IoU of 0.8898, outperforming SAM2 (0.4392), SAM2Long (0.8233), and DAM4SAM (0.8205). 3AM also attains the best Positive IoU (0.5630) and Successful IoU (0.7155). On the more challenging Selected Subset, 3AM maintains clear advantage. It reaches an IoU of 0.9061 and Positive IoU of 0.7168, substantially higher than SAM2Long (0.7474 / 0.4133) and DAM4SAM (0.7648 / 0.4356). 3AM achieves the best Successful IoU at 0.7737, highlighting its ability to maintain object identity even when the object reappears under large changes in viewpoint and spatial configuration. One thing to note is that SAM2 performs worse after finetuning. We attribute this degradation to the absence of grounding information from MUSt3R, which causes the memory attention to be mistrained due to insufficient reference cues for distinguishing different objects. Two-view Matching Comparison with SegMASt3R. We further compare Our 3AM with SegMASt3R [1], twoview matching approach built on MASt3R that associates segment masks between two images under large viewpoint changes. SegMASt3R is not directly prompable and does not natively support > 2 multiview tracking. Nevertheless, we include it as strong two-view baseline by adopting the following protocol: we use the ground-truth initial mask as the source-frame candidate, and for each subsequent frame we independently perform two-view mask matching against 6 Table 2. Video Object Segmentation results on the ScanNet++ [82] dataset. Positive IoU is computed over frames with ground-truth masks. Successful IoU is computed over frames where ground-truth object exists and the IoU of prediction and GT = 0. Method Whole Set Selected Subset IoU Positive IoU Successful IoU IoU Positive IoU Successful IoU SAM2 [46] SAM2 (Finetuned) SAM2Long [16] DAM4SAM [56] 3AM (Ours) 0.4392 0.7815 0.8233 0.8205 0.8898 0.0235 0.0106 0.4166 0.4193 0.5630 0.0831 0.0657 0.6855 0.6783 0.7155 0.3397 0.6628 0.7474 0.7648 0. 0.0179 0.0038 0.4133 0.4356 0.7168 0.0395 0.0276 0.6382 0.6650 0.7737 Figure 6. Visual comparison of VOS methods. The leftmost frame is used as the conditioned frame and provides the reference mask. Table 3. Quantitative comparison between 3AM and SegMASt3R under the two-view matching evaluation protocol on ScanNet++ Whole Set. Method IoU Positive IoU Successful IoU SegMASt3R [1] 3AM (Ours) 0.6800 0.8915 0.3628 0.5115 0.4053 0. Table 4. Video Object Segmentation results on the Replica[50] dataset. Positive IoU is computed over frames with ground-truth masks. Successful IoU is computed over frames where groundtruth object exists and the IoU of prediction and GT = 0. Method IoU Positive IoU Successful IoU SAM2 [46] SAM2Long [16] DAM4SAM [56] 3AM (Ours) 0.4424 0.7691 0.7744 0.8119 0.1432 0.5195 0.5135 0.6381 0.2188 0.6273 0.6124 0.6793 the source frame. To fairly compare the two approaches, when tracking with our 3AM, we dont use any memory other than the source frame. Under this protocol, 3AM achieves Reference ALL IoU of 0.8915, POS IoU of 0.5115, and SUC IoU of 0.6405, whereas SegMASt3R obtains 0.6800, 0.3628, and 0.4053, respectively. These results demonstrate 3AM not only provides stronger functionality, but also outperforms SegMASt3R even when restricted to two-view matching. 4.1.2. Replica On the Replica dataset, 3AM again achieves the best performance across all three evaluation metrics as shown in Tab. 4. Compared to SAM2, which attains an IoU of 0.4424, SAM2Long (0.7691) and DAM4SAM (0.7744) provide substantial improvements due to their enhanced memoryselection strategies. However, 3AM surpasses all prior methods, reaching an IoU of 0.8119."
        },
        {
            "title": "The gains are even more pronounced on the metrics that",
            "content": "7 isolate visible-object performance. For Positive IoU, 3AM obtains 0.6381, clearly higher than SAM2Long (0.5195) and DAM4SAM (0.5135). Similarly, 3AM achieves the highest Successful IoU at 0.6793, indicating stronger consistency in frames where the object is correctly localized. These results confirm 3AM provides robust object tracking under the large viewpoint variations characteristic of the Replica dataset. These results demonstrate that 3AM consistently surpasses prior methods and is particularly effective in scenarios involving object reappearance and large viewpoint variation, despite not relying on the explicit memory-selection mechanisms used in previous approaches. Due to space limit, more visualization is provided in the supplementary materials. 4.2. 3D Evaluation We next evaluate our method on 3D task. We adopt the class-agnostic 3D instance segmentation setting of ScanNet200, as it directly reflects models ability to produce object-consistent predictions in 3D scenes. Prior approaches typically require explicit 3D fusion[71] or merging[76], regardless of whether the proposals originate from 2D masks[26] or 3D detectors[6, 39]. Our goal is to demonstrate that this additional 3D-space merging is unnecessary: if 2D tracking is geometrically consistent across views, then reliable 3D instances can be obtained simply by projecting the tracked 2D masks into 3D. Concretely, we follow prior methods[84] and generate 2D proposals from SAM2 on keyframes selected by stridebased sampling. These proposals are then propagated with 3AM, whose improved cross-view consistency enables stable object identification across multiple camera viewpoints. We perform merging using IoU and inter-mask precision to associate proposals over time, and project the resulting 2D tracks into 3D. In contrast, previous SAM2-based methods[84] rely heavily on 3D merging because their tracking often drifts or fails under large viewpoint changes. Further implementation details are provided in the supplementary material. As shown in Table 5, 3AM achieves the highest performance among online methods, with an AP of 47.3 and strong AP50 and AP25 scores (59.7 and 75.3). These results highlight central finding of our work: robust 3D instance segmentation can emerge from geometry-aware tracking without heavy 3D supervision. 4.3. Ablation Study Memroy Selection. We further analyze the role of memory selection by combining 3AM with existing strategies proposed for SAM2. Table 6 reports results on the ScanNet++ Whole Set. 3AM, which uses the original SAM2 memory-selection mechanism, already achieves strong performance with an IoU of 0.8898. When we incorporate alternative memory-selection policies, such as those from DAM4SAM [56] or SAM2Long [16], we observe modest imTable 5. Class-agnostic 3D instance segmentation results of different methods on ScanNet200 dataset. Method Type 3D GT AP AP50 AP25 SAMPro3D [67] Offline Open3DIS [39] Offline Offline SAI3D [83] SAM2Object [84] Offline SAM3D [76] ESAM [18] 3AM (Ours) Online Online Online Yes Yes Yes Yes Yes Yes No 18.0 34.6 28.2 34.0 20.2 42.2 47. 32.8 43.1 47.2 52.7 35.7 63.7 59.7 56.1 48.5 67.9 70.3 55.5 79.6 75. Table 6. Video Object Segmentation results between different memory selection methods on ScanNet++ Whole Set. Positive IoU is computed over frames with ground-truth masks. Successful IoU is computed over frames where ground-truth object exists and the IoU of prediction and GT = 0. Method IoU Positive IoU Successful IoU 3AM DAM4SAM-3AM [56] SAM2Long-3AM [16] 0.8898 0.8941 0.9004 0.5630 0.5471 0.5498 0.7155 0.7204 0.7361 Table 7. Comparison of different 3D reconstruction backbones. Model Online Frame Num ScanNet++ Selected Subset Positive IoU VGGT π3 CUT3R MUSt3R 200 300 > 10,000 > 10,000 0.2751 0.7168 provements: DAM4SAM-3AM slightly increases Successful IoU to 0.7204, while SAM2Long-3AM achieves the highest IoU (0.9004) and Successful IoU (0.7361). These results suggest that 3AM already delivers strong and stable performance without relying on specialized memory-selection policies. While alternative strategies such as those from SAM2Long or DAM4SAM offer small additional gains, their impact is relatively limited compared to the improvements brought by 3AM itself. We view the design of memory-selection mechanisms tailored specifically for 3AM as an interesting direction for future work. 3D Foundataion Models. We compare 3D foundation models in Table 7. VGGT and π3 operate offline on frame batches, unsuitable for online tracking. CUT3R supports online operation, but yields limited instance-level alignment as shown in Fig. 7, reflected by its Positive IoU of 0.2751 on the ScanNet++ Selected Subset. MUSt3R is fully online with substantially stronger object alignment across viewpoints. This alignment is essential: consistent 3D alignment enables stable 2D cross-view correspondence, allowing reliable mask propagation without explicit 3D merging. 5. Conclusion We introduce 3AM, integrating 3D-aware features into SAM2 for viewpoint-robust video object segmentation. 8 Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, and Fahad Shahbaz Khan. Open-yolo 3d: Towards fast and accurate open-vocabulary 3d instance segmentation. arXiv preprint arXiv:2406.02548, 2024. 3 [6] Mohamed El Amine Boudjoghra, Angela Dai, Jean Lahoud, Hisham Cholakkal, Rao Muhammad Anwer, Salman Khan, and Fahad Shahbaz Khan. Open-YOLO 3d: Towards fast and accurate open-vocabulary 3d instance segmentation. In The Thirteenth International Conference on Learning Representations, 2025. 8 [7] Yohann Cabon, Lucas Stoffl, Leonid Antsfeld, Gabriela Csurka, Boris Chidlovskii, Jerome Revaud, and Vincent Leroy. Must3r: Multi-view network for stereo 3d reconstruction. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 10501060, 2025. 2, [8] Sergi Caelles, Kevis-Kokitsi Maninis, Jordi Pont-Tuset, Laura Leal-Taixe, Daniel Cremers, and Luc Van Gool. One-shot video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 221230, 2017. 2 [9] David Charatan, Sizhe Lester Li, Andrea Tagliasacchi, and Vincent Sitzmann. pixelsplat: 3d gaussian splats from image pairs for scalable generalizable 3d reconstruction. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1945719467, 2024. 3 [10] Zhuoguang Chen, Minghui Qin, Tianyuan Yuan, Zhe Liu, and Hang Zhao. Long3r: Long sequence streaming 3d reconstruction. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 52735284, 2025. 3 [11] Ho Kei Cheng and Alexander Schwing. Xmem: Long-term video object segmentation with an atkinson-shiffrin memory model. In European conference on computer vision, pages 640658. Springer, 2022. 3 [12] Ho Kei Cheng, Yu-Wing Tai, and Chi-Keung Tang. Rethinking space-time networks with improved memory coverage for efficient video object segmentation. Advances in neural information processing systems, 34:1178111794, 2021. [13] Ho Kei Cheng, Seoung Wug Oh, Brian Price, Joon-Young Lee, and Alexander Schwing. Putting the object back into video object segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 31513161, 2024. 3 [14] Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Philip HS Torr, and Song Bai. MOSE: new dataset for video object segmentation in complex scenes. In ICCV, 2023. 5, 6, 13 [15] Henghui Ding, Kaining Ying, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang, Philip HS Torr, and Song Bai. Mosev2: more challenging dataset for video object segmentation in complex scenes. arXiv preprint arXiv:2508.05630, 2025. 2, [16] Shuangrui Ding, Rui Qian, Xiaoyi Dong, Pan Zhang, Yuhang Zang, Yuhang Cao, Yuwei Guo, Dahua Lin, and Jiaqi Wang. Sam2long: Enhancing sam 2 for long video segmentation with training-free memory tree. arXiv preprint arXiv:2410.16268, 2024. 1, 3, 6, 7, 8 [17] Cheng-De Fan, Chen-Wei Chang, Yi-Ruei Liu, Jie-Ying Lee, Jiun-Long Huang, Yu-Chee Tseng, and Yu-Lun Liu. SpecFigure 7. Visual comparison of different 3D reconstruction backbones. (Top) CUT3Rs reconstruction lacks stable object alignment; the same table appears at inconsistent 3D locations. Such geometric instability weakens feature distinctiveness, making reliable tracking difficult. (Bottom) In contrast, MUSt3R provides coherent and stable object alignment across viewpoints, yielding features that preserve object identity and enable robust tracking Through our Feature Merger and field-of-view aware sampling, 3AM achieves geometry-consistent tracking requiring only RGB at inference. On wide-baseline datasets, 3AM substantially outperforms state-of-the-art VOS methods, achieving gains of +15.9 and +30.4 points on the ScanNet++ Selected Subset. Acknowledgements. This work was supported by NVIDIA Taiwan AI Research & Development Center (TRDC). This research was funded by the National Science and Technology Council, Taiwan, under Grants NSTC 112-2222-E-A49-004MY2 and 113-2628-E-A49-023-. Yu-Lun Liu acknowledges the Yushan Young Fellow Program by the MOE in Taiwan."
        },
        {
            "title": "References",
            "content": "[1] Anonymous Authors. Segmast3r: Geometry grounded segment matching. In Advances in Neural Information Processing Systems (NeurIPS), 2025. 3, 6, 7 [2] Armen Avetisyan, Christopher Xie, Henry Howard-Jenkins, Tsun-Yi Yang, Samir Aroudj, Suvam Patra, Fuyang Zhang, Duncan Frost, Luke Holland, Campbell Orme, et al. Scenescript: Reconstructing scenes with an autoregressive structured language model. In European Conference on Computer Vision, pages 247263. Springer, 2024. 5, 6, 13 [3] Miguel Angel Bautista, Pengsheng Guo, Samira Abnar, Walter Talbott, Alexander Toshev, Zhuoyuan Chen, Laurent Dinh, Shuangfei Zhai, Hanlin Goh, Daniel Ulbricht, et al. Gaudi: neural architect for immersive 3d scene generation. Advances in Neural Information Processing Systems, 35:2510225116, 2022. 3 [4] Goutam Bhat, Felix Jaremo Lawin, Martin Danelljan, Andreas Robinson, Michael Felsberg, Luc Van Gool, and Radu Timofte. Learning what to learn for video object segmentation. In European Conference on Computer Vision, pages 777794. Springer, 2020. 3 [5] Mohamed El Amine Boudjoghra, Angela Dai, Jean Lahoud, tromotion: Dynamic 3d reconstruction of specular scenes. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2132821338, 2025. 3 [18] Vivek Gupta, KW Bannister, Chris Flynn, and Clancy James. Efficient summation of arbitrary masksesam. Publications of the Astronomical Society of Australia, pages 112, 2024. 8 [19] Lei Han, Tian Zheng, Lan Xu, and Lu Fang. Occuseg: Occupancy-aware 3d instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 29402949, 2020. 3 [20] Lingyi Hong, Wenchao Chen, Zhongying Liu, Wei Zhang, Pinxue Guo, Zhaoyu Chen, and Wenqiang Zhang. Lvos: benchmark for long-term video object segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1348013492, 2023. 6 [21] Peng-Hao Hsu, Ke Zhang, Fu-En Wang, Tao Tu, Ming-Feng Li, Yu-Lun Liu, Albert YC Chen, Min Sun, and Cheng-Hao Kuo. Openm3d: Open vocabulary multi-view indoor 3d object detection without human annotations. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 86888698, 2025. 3 [22] Ayush Jain, Pushkal Katara, Nikolaos Gkanatsios, Adam Harley, Gabriel Sarch, Kriti Aggarwal, Vishrav Chaudhary, and Katerina Fragkiadaki. Odin: single model for 2d and 3d segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 3564 3574, 2024. 3 [23] Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, Xuesong Niu, Tengyu Liu, Qing Li, and Siyuan Huang. Sceneverse: Scaling 3d vision-language learning for grounded scene understanding. In European Conference on Computer Vision, pages 289310. Springer, 2024. 2, [24] Li Jiang, Hengshuang Zhao, Shaoshuai Shi, Shu Liu, ChiWing Fu, and Jiaya Jia. Pointgroup: Dual-set point groupIn Proceedings of the ing for 3d instance segmentation. IEEE/CVF conference on computer vision and Pattern recognition, pages 48674876, 2020. 3 [25] Li Jiang, Shaoshuai Shi, and Bernt Schiele. Open-vocabulary 3d semantic segmentation with foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2128421294, 2024. 3 [26] Sanghun Jung, Jingjing Zheng, Ke Zhang, Nan Qiao, Albert YC Chen, Lu Xia, Chi Liu, Yuyin Sun, Xiao Zeng, Hsiang-Wei Huang, et al. Details matter for indoor openvocabulary 3d instance segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 96279637, 2025. 8 [27] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1972919739, 2023. 3 [28] Maxim Kolodiazhnyi, Anna Vorontsova, Anton Konushin, and Danila Rukhovich. Oneformer3d: One transformer In Proceedings of for unified point cloud segmentation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2094320953, 2024. 1, 3 [29] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with mast3r. In European Conference on Computer Vision, pages 7191. Springer, 2024. 3 [30] Hongyang Li, Jinyuan Qu, and Lei Zhang. Ovseg3r: Learn open-vocabulary instance segmentation from 2d via 3d reconstruction. arXiv preprint arXiv:2509.23541, 2025. [31] Minghan Li, Shuai Li, Xindong Zhang, and Lei Zhang. Univs: Unified and universal video segmentation with prompts as In Proceedings of the IEEE/CVF conference on queries. computer vision and pattern recognition, pages 32273238, 2024. 3 [32] Ming-Feng Li, Yueh-Feng Ku, Hong-Xuan Yen, Chi Liu, YuLun Liu, Albert YC Chen, Cheng-Hao Kuo, and Min Sun. Genrc: Generative 3d room completion from sparse image collections. In European Conference on Computer Vision, pages 146163. Springer, 2024. 3 [33] Chin-Yang Lin, Cheng Sun, Fu-En Yang, Min-Hung Chen, Yen-Yu Lin, and Yu-Lun Liu. Longsplat: Robust unposed 3d gaussian splatting for casual long videos. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2741227422, 2025. 3 [34] Tsung-Yi Lin, Piotr Dollar, Ross Girshick, Kaiming He, Bharath Hariharan, and Serge Belongie. Feature pyramid networks for object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 21172125, 2017. 13 [35] Yong Liu, Ran Yu, Fei Yin, Xinyuan Zhao, Wei Zhao, Weihao Xia, and Yujiu Yang. Learning quality-aware dynamic memory for video object segmentation. In European Conference on Computer Vision, pages 468486. Springer, 2022. 3 [36] Yu-Lun Liu, Chen Gao, Andreas Meuleman, Hung-Yu Tseng, Ayush Saraf, Changil Kim, Yung-Yu Chuang, Johannes Kopf, and Jia-Bin Huang. Robust dynamic radiance fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1323, 2023. 3 [37] K-K Maninis, Sergi Caelles, Yuhua Chen, Jordi Pont-Tuset, Laura Leal-Taixe, Daniel Cremers, and Luc Van Gool. Video IEEE object segmentation without temporal information. transactions on pattern analysis and machine intelligence, 41 (6):15151530, 2018. [38] Ishan Misra, Rohit Girdhar, and Armand Joulin. An end-toend transformer model for 3d object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 29062917, 2021. 3 [39] Phuc Nguyen, Tuan Duc Ngo, Evangelos Kalogerakis, Chuang Gan, Anh Tran, Cuong Pham, and Khoi Nguyen. Open3dis: Open-vocabulary 3d instance segmentation with 2d mask guidance. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 40184028, 2024. 1, 3, 8 [40] Seoung Wug Oh, Joon-Young Lee, Ning Xu, and Seon Joo Kim. Video object segmentation using space-time memory In Proceedings of the IEEE/CVF international networks. conference on computer vision, pages 92269235, 2019. 3 [41] Kwanyong Park, Sanghyun Woo, Seoung Wug Oh, In So Kweon, and Joon-Young Lee. Per-clip video object segmen10 tation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 13521361, 2022. [42] Songyou Peng, Kyle Genova, Chiyu Jiang, Andrea Tagliasacchi, Marc Pollefeys, Thomas Funkhouser, et al. Openscene: 3d scene understanding with open vocabularies. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 815824, 2023. 2, 3 [43] Federico Perazzi, Jordi Pont-Tuset, Brian McWilliams, Luc Van Gool, Markus Gross, and Alexander Sorkine-Hornung. benchmark dataset and evaluation methodology for video object segmentation. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 724732, 2016. 2 [44] Jens Piekenbrinck, Christian Schmidt, Alexander Hermans, Narunas Vaskevicius, Timm Linder, and Bastian Leibe. Opensplat3d: Open-vocabulary 3d instance segmentation using gaussian splatting. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52465255, 2025. 3 [45] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv:1704.00675, 2017. 3, 6 [46] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, Chao-Yuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 1, 2, 3, 7 [47] Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, Jitendra Malik, Yanghao Li, and Christoph Feichtenhofer. Hiera: hierarchical vision transformer without the bells-and-whistles. ICML, 2023. [48] Jonas Schult, Francis Engelmann, Alexander Hermans, Or Litany, Siyu Tang, and Bastian Leibe. Mask3d: Mask transformer for 3d semantic instance segmentation. arXiv preprint arXiv:2210.03105, 2022. 1, 3 [49] Meng-Li Shih, Ying-Huan Chen, Yu-Lun Liu, and Brian Curless. Prior-enhanced gaussian splatting for dynamic scene reconstruction from casual video. In Proceedings of the SIGGRAPH Asia 2025 Conference Papers, pages 113, 2025. 3 [50] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan, June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler, Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele, Steven Lovegrove, and Richard Newcombe. The Replica dataset: digital replica of indoor spaces. arXiv preprint arXiv:1906.05797, 2019. 2, 6, 7 [51] Chih-Hai Su, Chih-Yao Hu, Shr-Ruei Tsai, Jie-Ying Lee, Chin-Yang Lin, and Yu-Lun Liu. Boostmvsnerfs: Boosting mvs-based nerfs to generalizable view synthesis in large-scale scenes. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. 3 [52] Jiahao Sun, Chunmei Qing, Junpeng Tan, and Xiangmin Xu. Superpoint transformer for 3d scene instance segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 23932401, 2023. [53] Ayca Takmaz, Elisabetta Fedele, Robert Sumner, Marc Pollefeys, Federico Tombari, and Francis Engelmann. Openmask3d: Open-vocabulary 3d instance segmentation. arXiv preprint arXiv:2306.13631, 2023. 2, 3 [54] Pavel Tokmakov, Jie Li, and Adrien Gaidon. Breaking the In Proceedings of object in video object segmentation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2283622845, 2023. 6 [55] Tao Tu, Shun-Po Chuang, Yu-Lun Liu, Cheng Sun, Ke Zhang, Donna Roy, Cheng-Hao Kuo, and Min Sun. Imgeonet: Imageinduced geometry-aware voxel representation for multi-view 3d object detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 69967007, 2023. 3 [56] Jovana Videnovic, Alan Lukezic, and Matej Kristan. distractor-aware memory for visual object tracking with sam2. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2425524264, 2025. 1, 3, 6, 7, 8 [57] Paul Voigtlaender, Yuning Chai, Florian Schroff, Hartwig Adam, Bastian Leibe, and Liang-Chieh Chen. Feelvos: Fast end-to-end embedding learning for video object segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 94819490, 2019. 2 [58] Suhani Vora, Noha Radwan, Klaus Greff, Henning Meyer, Kyle Genova, Mehdi SM Sajjadi, Etienne Pot, Andrea Tagliasacchi, and Daniel Duckworth. Nesf: Neural semantic fields for generalizable semantic segmentation of 3d scenes. arXiv preprint arXiv:2111.13260, 2021. 3 [59] Thang Vu, Kookhoi Kim, Tung Luu, Thanh Nguyen, and Chang Yoo. Softgroup for 3d instance segmentation on point clouds. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 27082717, 2022. [60] Hengyi Wang and Lourdes Agapito. 3d reconstruction with spatial memory. arXiv preprint arXiv:2408.16061, 2024. 3 [61] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 52945306, 2025. [62] Qianqian Wang, Yifei Zhang, Aleksander Holynski, Alexei Efros, and Angjoo Kanazawa. Continuous 3d perception model with persistent state. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 10510 10522, 2025. [63] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 20697 20709, 2024. 3 11 in Neural Information Processing Systems, 35:3632436336, 2022. 3 [78] Zongxin Yang, Yunchao Wei, and Yi Yang. Associating objects with transformers for video object segmentation. Advances in Neural Information Processing Systems, 34:2491 2502, 2021. [79] Zongxin Yang, Jiaxu Miao, Yunchao Wei, Wenguan Wang, Xiaohan Wang, and Yi Yang. Scalable video object segmentation with identification mechanism. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(9):62476262, 2024. 3 [80] Botao Ye, Sifei Liu, Haofei Xu, Xueting Li, Marc Pollefeys, Ming-Hsuan Yang, and Songyou Peng. No pose, no problem: Surprisingly simple 3d gaussian splats from sparse unposed images. arXiv preprint arXiv:2410.24207, 2024. 3 [81] Mingqiao Ye, Seoung Wug Oh, Lei Ke, and Joon-Young Lee. Entitysam: Segment everything in video. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2423424243, 2025. 3 [82] Chandan Yeshwanth, Yueh-Cheng Liu, Matthias Nießner, and Angela Dai. Scannet++: high-fidelity dataset of 3d indoor scenes. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1222, 2023. 2, 5, 6, 7, 14 [83] Yingda Yin, Yuzheng Liu, Yang Xiao, Daniel Cohen-Or, Jingwei Huang, and Baoquan Chen. Sai3d: Segment any instance in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 32923302, 2024. 8 [84] Jihuai Zhao, Junbao Zhuo, Jiansheng Chen, and Huimin Ma. Sam2object: Consolidating view consistency via sam2 for zero-shot 3d instance segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1932519334, 2025. 1, 2, 3, [85] Lojze Zust, Yohann Cabon, Juliette Marrie, Leonid Antsfeld, Boris Chidlovskii, Jerome Revaud, and Gabriela Csurka. Panst3r: Multi-view consistent panoptic segmentation. arXiv preprint arXiv:2506.21348, 2025. 2, 3 [64] Chun-Hung Wu, Shih-Hong Chen, Chih-Yao Hu, Hsin-Yu Wu, Kai-Hsin Chen, Yu-You Chen, Chih-Hai Su, Chih-Kuo Lee, and Yu-Lun Liu. Denver: Deformable neural vessel representations for unsupervised video vessel segmentation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1568215692, 2025. 2 [65] Yuqi Wu, Wenzhao Zheng, Jie Zhou, and Jiwen Lu. Point3r: Streaming 3d reconstruction with explicit spatial pointer memory. arXiv preprint arXiv:2507.02863, 2025. 3 [66] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Yi Wang, and Zhangyang Wang. Neurallift-360: Lifting an in-the-wild 2d photo to 3d object with 360deg views. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 44794489, 2023. 3 [67] Mutian Xu, Xingyilang Yin, Lingteng Qiu, Yang Liu, Xin Tong, and Xiaoguang Han. Sampro3d: Locating sam prompts in 3d for zero-shot instance segmentation. In 2025 International Conference on 3D Vision (3DV), pages 12221232. IEEE, 2025. 8 [68] Ning Xu, Linjie Yang, Yuchen Fan, Dingcheng Yue, Yuchen Liang, Jianchao Yang, and Thomas Huang. Youtube-vos: large-scale video object segmentation benchmark. arXiv preprint arXiv:1809.03327, 2018. 3, [69] Xiaohao Xu, Jinglu Wang, Xiao Li, and Yan Lu. Reliable propagation-correction modulation for video object segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 29462954, 2022. 3 [70] Xiuwei Xu, Huangxing Chen, Linqing Zhao, Ziwei Wang, Jie Zhou, and Jiwen Lu. Embodiedsam: Online segment any 3d thing in real time. arXiv preprint arXiv:2408.11811, 2024. 2, 3 [71] Xiuwei Xu, Huangxing Chen, Linqing Zhao, Ziwei Wang, Jie Zhou, and Jiwen Lu. Embodiedsam: Online segment any 3d thing in real time. arXiv preprint arXiv:2408.11811, 2024. 3, 8 [72] Cheng-Yen Yang, Hsiang-Wei Huang, Wenhao Chai, Zhongyu Jiang, and Jenq-Neng Hwang. Samurai: Adapting segment anything model for zero-shot visual tracking with motion-aware memory. arXiv preprint arXiv:2411.11922, 2024. 3 [73] Jianing Yang, Alexander Sax, Kevin Liang, Mikael Henaff, Hao Tang, Ang Cao, Joyce Chai, Franziska Meier, and Matt Feiszli. Fast3r: Towards 3d reconstruction of 1000+ images in one forward pass. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2192421935, 2025. 3 [74] Linjie Yang, Yuchen Fan, and Ning Xu. Video instance segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 51885197, 2019. [75] Qiushi Yang, Yuan Yao, Miaomiao Cui, and Liefeng Bo. Mosam: Motion-guided segment anything model with spatialtemporal memory selection. arXiv preprint arXiv:2505.00739, 2025. 3 [76] Yunhan Yang, Xiaoyang Wu, Tong He, Hengshuang Zhao, and Xihui Liu. Sam3d: Segment anything in 3d scenes. arXiv preprint arXiv:2306.03908, 2023. 1, 3, 8 [77] Zongxin Yang and Yi Yang. Decoupling features in hierarchical propagation for video object segmentation. Advances 12 A. Overview This supplementary material presents additional results to complement the main manuscript. First, we provide all the implementation details, including network architectures in Sec. B. Next, we elaborate on training details, such as the FoV sampling and datasets in Sec. C. Then, we describe how we perform class-agnostic instance segmentation in Sec. D. Finally, we provide more qualitative comparisons on both video object tracking and class-agnostic instance segmentation in Sec. E. In addition to this document, we provide additional video results to compare with state-of-the-art methods. B. Implementation Details Through all experiments, we use SAM 2.1-Large as our baseline due to computational resource constraints. Hiera [47] image encoder produces multi-scale features, where stride-16 and stride-32 outputs (Stages 3/4) are fused via an FPN [34] with convolutions to obtain frame embeddings, while shallow features (stride 4/8) from Stages 1/2 are injected only into the mask decoder to recover fine boundaries. Memory attention uses sinusoidal absolute positional embeddings together with 2D RoPE; object pointer tokens do not use RoPE, and we adopt = 4 layers. The prompt encoder follows SAM, and the mask decoder uses the mask token as the object pointer stored in the memory bank. An additional token predicts object visibility via an MLP head, and occluded frames receive learned occlusion embedding in memory. As in SAM 2, the decoder outputs multiple mask candidates and selects the one with the highest predicted IoU when ambiguity persists. The memory encoder reuses Hiera features without separate backbone; memory features are projected to 64 dimensions, and the 256-dim pointer is split into four 64-d tokens for cross-attention. For multi-object videos, the image encoder is shared while each object maintains an independent memory bank and decoder. B.1. Feature Merger As illustrated in Fig. 4, our method uses multi-layer features from the MUSt3R decoder. We keep MUSt3Rs memory mechanism unchangedi.e., we do not modify its viewcoveragebased memory selection. We empirically select the layer indices [encoder, 4, 7, 11] to incorporate both semantic cues from early layers and geometry-aware signals from later layers, as deeper decoder layers in MUSt3R increasingly capture 3D structure and directly decode point clouds. After selecting the MUSt3R features, we merge them using sequence of cross-attention operations. We first construct positional embeddings: PE3D is obtained by processing the point map and ray map produced by MUSt3R, while PE2D is taken from MUSt3Rs 2D positional encoding. The encoder feature is projected from 1024 to 768 channels using 11 convolution, added with PE3D, and then passed through self-attention layer with PE2D to form an initial coarse feature Fcoarse. For each selected MUSt3R layer Fi with {4, 7, 11}, we update Fcoarse using: Fcoarse FFN(cid:0)CrossAttn(cid:0)SelfAttn(Fcoarse + PE3D), Fi + PE3D(cid:1)(cid:1) . (1) We omit normalization and skip connections for clarity. After the attention fusion, Fcoarse is upsampled and refined by 3 3 convolution, then concatenated with the Hiera image-encoder feature F2D from SAM 2, followed by an additional convolution block to match the feature dimensions. The resulting feature Fmerged is used for memory attention and memory encoding. We leave the shallow Hiera features from Stages 1 and 2 untouched so that the mask decoder can still rely on strong low-level visual cues for producing high-resolution segmentation outputs. C. Training Details C.1. Field-of-View Sampling We do not use 100% FOV-aware sampling because we observe that applying FOV filtering to every batch degrades the models original feature-matching ability inherited from SAM2. When all training samples come from drastically different viewpoints, the model is over-regularized toward cross-view matching and loses its within-view correspondence skills, leading to form of feature collapse. C.2. Settings Loss We follow SAM2s loss design without modification. Specifically, mask prediction is supervised using weighted combination of focal loss (20) and dice loss (1); the IoU head is trained with an ℓ1 loss (1); and the occlusion prediction head uses cross-entropy loss (1). C.3. Datasets Aria Synthetic Environments (ASE) [2] ASE is largescale synthetic dataset of 100,000 procedurally generated indoor scenes rendered with simulated Aria-glass sensors. Each scene contains realistic 3D object layouts, simulated trajectories, and aligned 2D/3D annotations. ASE enables large-scale training of 3D scene understanding, object detection, and tracking, particularly in data-hungry settings where real-world labelled data is scarce. We sample total of 2,612 scenes whose number of views falls within practical range: scenes with too few views do not provide sufficient viewpoint variation for training, while scenes with excessively many views lead to memory overflow when running MUSt3R. MOSE [14] The MOSE dataset (Complex Video Object Segmentation) targets video-object segmentation in cluttered and occluded real-world scenes. It comprises 2,149 video clips with 5,200 objects from 36 categories and 431, 13 them into consolidated 3D instances. Two scores are computed for every fragment: (1) 3D overlap score in the point-cloud domain, and (2) 2D temporal-overlap score based on the propagated masks along the video. Because our model maintains consistent object identity across the video, an object visible earlier will be recognized again whenever it reappears, providing reliable temporal evidence for merging. Finally, after merging, duplicate assignments are resolved by majority voting at the superpoint level: each superpoint is assigned to the instance in which it is observed the greatest number of times. E. Qualitative Comparison Video Comparisoin We provide video object tracking visualization in Figs. 8 to 14. Class-Agnostic Instance Segmentation. We provide additional visual results on class-agnostic instance segmentation in Fig. 15. high-quality masks. Unlike previous VOS benchmarks that feature large, salient, isolated targets, MOSE features heavy occlusion, object disappearance and re-appearance, and small/inconspicuous objects. MOSE is primarily used to preserve the core VOS capability of our model. While our geometric integration encourages strong cross-view consistency, we do not want the model to over-rely on geometry and hallucinate object tracks without sufficient visual evidence; MOSE helps anchor the model to appearance-driven cues. Although MUSt3R supports dynamic scenes, we find that training on highly dynamic and diverse datasets such as SAV Train leads to instability in practice. MOSE provides more controlled level of motion and scene variation, allowing us to maintain stable learning while still leveraging dynamicobject supervision. ScanNet++ [82] ScanNet++ is high-quality indoor RGB reconstruction dataset that provides accurate camera poses, dense trajectories, and detailed geometric annotations. The public release contains over 1,000 reconstructed scenes, representing an expanded version of the initial smaller release[82]. Its combination of reliable geometry and viewpoint diversity makes it well suited for studying viewconsistent object tracking under large camera motions. However, the 2D masks in ScanNet++ are generated by projecting 3D point-cloud instance labels into the image plane, which inevitably introduces reprojection noise from depth and pose inaccuracies. Despite this, ScanNet++ remains valuable because it provides realistic indoor scenes with rich geometric structure and large viewpoint variation. Together with the other two datasets, which supply clean, frame-aligned 2D masks, this forms complementary training combination that balances realistic geometry with high-quality appearance supervision. D. Class-Agnostic Instance Segmentation For class-agnostic 3D instance segmentation, we proceed as follows. Given set of keyframes, either sampled at fixed interval or selected using view-coverage criterion, we first generate 2D instance masks using SAM2s automatic mask generator. For each keyframe i, we then propagate its instance masks forward to all subsequent frames using our model; no backward tracking is performed. The propagated masks are lifted into 3D by back-projecting their pixels using the depth map and camera pose. practical issue is the presence of reprojection errors near object boundaries, caused by depth noise and view-dependent misalignment. To mitigate this, we (1) compute reprojection score using the agreement between projected depth and ground-truth depth, and (2) erode the 2D masks slightly before projection, which reduces artifacts concentrated near object edges. After obtaining the per-frame 3D fragments, we merge 14 Figure 8. Visual comparison of different VOS methods"
        },
        {
            "title": "21\nFigure 14. Visual comparison of different VOS methods",
            "content": "Figure 15. Visual results on class-agnostic instance segmentation."
        }
    ],
    "affiliations": [
        "NVIDIA Research",
        "National Yang Ming Chiao Tung University"
    ]
}