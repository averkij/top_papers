{
    "paper_title": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots",
    "authors": [
        "Ting Huang",
        "Dongjian Li",
        "Rui Yang",
        "Zeyu Zhang",
        "Zida Yang",
        "Hao Tang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Grounding natural-language instructions into continuous control for quadruped robots remains a fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real world. To address these issues, we present MobileVLA-R1, a unified vision-language-action framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, a large-scale dataset of multi-granularity chain-of-thought (CoT) for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce a two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately a 5% improvement. Real-world deployment on a quadruped robot validates robust performance in complex environments. Code: https://github.com/AIGeeksGroup/MobileVLA-R1. Website: https://aigeeksgroup.github.io/MobileVLA-R1."
        },
        {
            "title": "Start",
            "content": "MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots Ting Huang Dongjian Li Rui Yang Zeyu Zhang Zida Yang Hao Tang Peking University Equal contribution. Project lead. Corresponding author: bjdxtanghao@gmail.com. 5 2 0 2 2 2 ] . [ 1 9 8 8 7 1 . 1 1 5 2 : r Figure 1. Real-world demonstration of MobileVLA-R1. Upon receiving natural-language instructions, MobileVLA-R1 processes RGB video streams through visionlanguage model to perform spatial reasoning and generate continuous locomotion commands, enabling the quadruped robot to accomplish complex tasks in real-world environments."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Grounding natural-language instructions into continuous control for quadruped robots remains fundamental challenge in vision language action. Existing methods struggle to bridge high-level semantic reasoning and low-level actuation, leading to unstable grounding and weak generalization in the real-world. To address these issues, we present MobileVLA-R1, unified visionlanguageaction framework that enables explicit reasoning and continuous control for quadruped robots. We construct MobileVLA-CoT, large-scale dataset of multi-granularity CoT for embodied trajectories, providing structured reasoning supervision for alignment. Built upon this foundation, we introduce two-stage training paradigm that combines supervised CoT alignment with GRPO reinforcement learning to enhance reasoning consistency, control stability, and long-horizon execution. Extensive evaluations on VLN and VLA tasks demonstrate superior performance over strong baselines, with approximately 5% improvement. Real-world deployment on quadruped robot validates robust performance in complex environments. Code: https://github.com/ AIGeeksGroup/MobileVLA-R1. Website: https: //aigeeksgroup.github.io/MobileVLA-R1. Vision-language navigation (VLN) and embodied manipulation are fundamental capabilities for intelligent robotic systems, enabling agents to ground natural-language instructions into perception, reasoning, and continuous control in real-world environments [1, 4, 5, 7, 19, 31, 60]. Such competencies are essential for service robotics, assistive systems, and general-purpose autonomous agents operating in human-centric spaces, where natural-language interaction and reliable execution are required. Recent advances in multimodal foundation models have significantly improved perception and language understanding, motivating research on extending vision-language models (VLMs) to embodied tasks. Despite this progress, two major gaps still limit current approaches. The first lies in bridging semantic reasoning and motor control: direct language-to-action mapping often leads to weak interpretability and unstable grounding. The second involves lack of transparent reasoning structures: methods relying on latent intermediate embeddings achieve stability but obscure semantic logic, limiting compositional reasoning and error traceability. Our motivation is to build an embodied foundation model that can bridge high-level semantic reasoning and low-level motor execution, enabling interpretable planning and robust control across diverse environments. To this end, we introduce MobileVLA-R1, hierarchical vision language action (VLA) framework designed for quadruped robots. Rather than predicting motor commands directly from language, MobileVLA-R1 generates structured Chain-of-Thought (CoT) action plans conditioned on multimodal observations and then translates them into continuous control commands through an action decoder. This reasoning-then-execution design enhances interpretability, improves grounding stability, and enables flexible adaptation across diverse environments. The training process follows two-stage paradigm that first aligns reasoning ability through supervised learning on CoT-annotated embodied data and then refines action grounding and execution fidelity via Group Relative Policy Optimization (GRPO) reinforcement learning. We evaluate MobileVLA-R1 across two complementary embodied AI benchmarks: R2R-CE and RxR-CE for language-guided navigation, and the QUARD dataset for quadruped control and manipulation. On VLN-CE, MobileVLA-R1 achieves state-of-the-art performance with an average improvement of 5% in success rate over strong baselines, while on QUARD it consistently outperforms recent visionlanguageaction models across all six control tasks, demonstrating enhanced reasoning-aligned control. Real-world deployment on the Unitree Go2 further confirms robust performance under cluttered and partially observable conditions. In this work, we make three key contributions: We propose MobileVLA-R1, hierarchical vision language action framework that explicitly connects semantic reasoning and motor control through Chain-of-Thought generation and continuous quadruped execution, addressing the core semanticcontrol gap. We design two-stage training framework that integrates supervised CoT alignment with GRPO reinforcement learning, improving reasoning consistency, control robustness, and long-horizon stability. We construct MobileVLA-CoT, multi-granularity CoT dataset for embodied trajectories, and demonstrate measurable gains on embodied AI benchmarks, achieving approximately 5% performance improvement and reliable deployment on the Unitree Go2 platform. 2. Related Work Mobile robot navigation and manipulation. Vision language navigation (VLN) studies how agents follow naturallanguage instructions in 3D environments. R2R [4] established the benchmark with human-annotated indoor trajectories, and RxR [37] extended it with multilingual, longer, and semantically richer descriptions. Early approaches relied on sequence-to-sequence models [20, 43], while later works introduced attention and transformer-based architectures [11, 24, 75] to improve cross-modal grounding and long-horizon reasoning. With large-scale pretrained VLMs [2830], recent VLN systems [23, 25, 32, 40, 47, 48, 51, 70, 76] further enhance instruction following and generalization across diverse scenes. In parallel, language-conditioned robot manipulation has shown that natural language can directly guide physical actions [41, 55, 56, 68]. Systems such as SayTap [57], RT2 [5], OpenVLA [33], and generalist embodied frameworks like Octo [21] demonstrate robust instruction-conditioned locomotion and manipulation in real-world settings. These developments collectively point toward unified embodied agents capable of grounding language in both navigation and manipulation behaviors. Reinforcement learning on LLMs. Transformer-based language models [16, 49, 58] exhibit strong generalization through large-scale pretraining, while RLHF [46] further improves instruction following. Chain-of-thought (CoT) prompting [62] enhances multi-step reasoning, and recent models such as DeepSeek-R1 combine CoT with GRPO [54] to improve long-horizon reasoning. CoT has also been extended to spatial and embodied settings [12], suggesting its potential for grounded decision making. In embodied AI, reinforcement learning is increasingly used to align multimodal perception, reasoning, and control. RL-based frameworks like RLVR-World [64] improve semantic consistency and robustness in real-world tasks, while multimodal reasoning systems such as MMHELIX [74], Octo [21], and OpenVLA [33] demonstrate the promise of integrating high-level reasoning with continuous control. Overall, these directions reveal growing convergence between CoT reasoning, reinforcement learning, and embodied policy optimization, motivating unified frameworks that connect abstract reasoning with low-level robotic execution. 3. Datasets 3.1. Public Datasets R2R [4] provides instructiontrajectory pairs in realistic Matterport3D [6] indoor scenes and serves as canonical benchmark for instruction-following navigation. RxR [38] extends this setting with multilingual supervision and dense temporal grounding, offering longer and more instructiondense descriptions that enrich spatial semantics. To supply continuous control and manipulation signals on legged platforms, we additionally use QUARD [18], quadruped dataset encompassing both locomotion and manipulation episodes in diverse environments. Together, these sources provide complementary supervision for language grounding, spatial reasoning, and low-level action execution. Figure 2. Architecture of MobileVLA-R1. MobileVLA-R1 is an end-to-end framework that integrates natural-language instructions with multimodal perception. It processes RGB, depth, and point cloud observations together with textual commands to generate continuous locomotion actions, enabling mobile robots to follow complex instructions and adapt to diverse environments in real time. Statistics of public VLN and embodied-control Table 1. datasets used for synthesizing MobileVLA-CoT. Nav. indicates navigation capability, Emb-Ctl. indicates embodied control capability, and CoT Anno. indicates whether the dataset provides chain-of-thought annotations. Dataset R2R [4] RxR [38] QUARD [18] MobileVLA-CoT-Episode MobileVLA-CoT-Step MobileVLA-CoT-Nav 3.2. Synthetic Dataset Nav. Emb-Ctl. CoT Anno. Scale 50K 58 262K 18K 78K 38K Building on R2R, RxR, and QUARD, we synthesize MobileVLA-CoT, large-scale chain-of-thought corpus aligned with multimodal observations and control targets. Unlike datasets that only specify final goals, MobileVLACoT explicitly records the reasoning process that connects instructions to actions, enabling interpretable and step-bystep decision traces. It contains episode-level CoT, which summarizes task outcomes and high-level execution plans; step-level CoT, which specifies the next action and its rationale; and navigation-level CoT, which connects global instructions with multi-step reasoning across the trajectory. In total, the corpus includes 18K episode-level samples, 78K step-level samples, and navigation-focused subset, MobileVLA-CoT-Nav, with 38K CoT annotations, as summarized in Table 1. 3.3. CoT Data Engine As illustrated in Figure 3, we design unified CoT data engine to synthesize multi-granularity reasoning traces for navigation and embodied control tasks. The engine takes paired RGBDepth observations, natural-language instructions, and corresponding stateaction histories as inputs and leverages the reasoning capability of large multimodal model, Gemini-2.5-Flash [14], guided by structured prompt templates. Each prompt describes the current environment, specifies the task format, and instructs the model to reason before answering. The model first generates the reasoning process enclosed in <think>...</think> tags, followed by the corresponding control command or action in <answer>...</answer> format. strategies. To cover different reasoning granularities, we construct three complementary datasets: MobileVLA-CoT-Nav provides long-horizon navigation-level reasoning across full trajectories, MobileVLA-CoT-Step captures local control reasoning and next-action prediction, and MobileVLACoT-Episode summarizes trajectory-level outcomes and semi-automatic verification high-level pipeline, combining rule-based filtering and manual inspection, is applied to remove malformed or unsafe annotations before training; additional details on data generation and quality control are provided in the Supp. Mat. section 8. Through this process, the engine generates high-quality, reasoning-aligned embodied trajectories that serve as the foundation for supervised CoT alignment in MobileVLA-R1, enabling structured reasoning and reliable control grounding before reinforcement learning. 4. The Proposed Method 4.1. Overview MobileVLA-R1 follows hierarchical reasoningexecution paradigm composed of two sequential training stages. Built upon the MobileVLA-CoT dataset (Section 3), the framework first performs supervised fine-tuning (SFT) on CoTannotated embodied data to establish structured reasoning and semantic alignment between visual observations and Figure 3. CoT Data Engine. We construct the MobileVLA-CoT by defining navigation and step-level instructions, integrating RGBDepth visual inputs, and specifying structured reasoning prompts. These inputs are fed into Gemini-2.5-Flash, which generates multi-granularity Chain-of-Thought (CoT) annotations with corresponding action outputs. textual commands. The second stage further refines action grounding and continuous control through Group Relative Policy Optimization (GRPO) [54], as depicted in Figure 4. For architecture, as illustrated in Figure 2, MobileVLAR1 follows the LLaVA [39] design, adopting unified multimodal perception front-end that integrates RGB, depth, and point cloud inputs. We initialize the model from NaVILA [13] and jointly train it with SFT and GRPO, achieving tight coupling between explicit reasoning and continuous control within unified vision language action framework. Problem setups. The objective of MobileVLA-R1 is to generate continuous sequence of actions for closed-loop quadruped control, conditioned on multimodal perception and natural-language instructions. At each timestep t, the agent receives an observation st = {xrgb } tot gether with an instruction I, and predicts an action at according to policy πθ(at st, i). Formally, the mapping can be expressed as: , xdepth , xpoint fθ : (X rgb, depth, point) A, at = [Vx, Vy, ωyaw, α], (1) where Vx and Vy denote the translational velocities along the xand y-axes, ωyaw represents the angular velocity around the yaw axis, and α is discrete high-level action sampled from predefined action set. This formulation unifies continuous locomotion control and discrete embodied behaviors under reasoning-conditioned policy, enabling interpretable and robust quadruped operation in real-world environments. 4.2. Cold Start Stage Recent works such as DeepSeek-R1 [15] highlight the potential of reinforcement learning for reasoning; yet direct end-to-end optimization often causes instability in multimodal settings. To mitigate this, we introduce cold-start stage based on supervised fine-tuning (SFT) to align the output format and initialize reasoning before reinforcement learning. This stage involves two steps. First, the model is finetuned on the MobileVLA-CoT-Episode and MobileVLACoT-Nav datasets to learn structured reasoning in the format <think>...</think><answer>...</answer>, thereby improving coherence in long-horizon tasks. Then, it is further trained on 10K-sample subset of MobileVLACoT-Step, where answers contain executable velocity and action commands. These are deterministically parsed into control signals, enabling smooth transition from language reasoning to physical actuation. 4.3. Reinforcement Learning To further enhance reasoning-driven action generation, we employ the GRPO framework [54], which has recently shown remarkable success in DeepSeek-R1 [15]. Unlike traditional gradient-based reinforcement learning algorithms such as Proximal Policy Optimization (PPO) [53], GRPO focuses on reward alignment within groups of sampled responses, allowing for more stable and efficient learning from comparative feedback rather than absolute reward magnitudes. Its objective is to iteratively refine the models policy through group-wise reward optimization, thereby improving both reasoning coherence and action precision. Policy sample. Given an input pair (x, i), where encodes multi-modal scene representation composed of RGB images, depth maps, and point cloud features, and represents natural language instruction, the policy πθ generates responses {o1, o2, , oN } for each sample. Each response is evaluated using set of reward functions that encourage accurate, executable, and well-formatted outputs. Movement reward. This reward evaluates the consistency between the predicted and ground-truth motion directions, Figure 4. The pipeline of RL policy. The model generates responses from given input, rewards are then computed for each response. After normalizing and clipping, these rewards are conflated with KL-divergence term, which prevents the model from over-updating, to update the policy. encouraging smooth and stable trajectories. The policy outputs control vector = (Vx, Vy, Vyaw, a), where the first three components represent continuous velocities. Given yaw, a), the movement the ground truth = (V , reward is computed as the cosine similarity between the two velocity vectors: , Rm = oo o2 o2 , (2) where = (Vx, Vy, Vyaw). This formulation promotes directionally aligned and dynamically consistent control, while discrete actions are supervised separately by the action reward. Action reward. This reward supervises discrete control behaviors by checking whether the predicted action matches the ground-truth label a. It provides simple yet effective signal for aligning high-level decisions with target actions: Raction = (cid:40) if = 1, 0, otherwise . Such binary formulation offers clear feedback on action correctness and complements the continuous guidance from the movement reward. Format reward. To ensure structural consistency, the format reward Rf ormat checks whether the model output strictly follows the reasoningexecution template <think>...</think><answer>...</answer>. Formally, Rf ormat = (cid:40) 1, 0, if output satisfies format otherwise . (4) This constraint guarantees machine-parseable outputs and enforces consistent mapping from reasoning traces to executable control commands. Policy update. Given responses sampled from the current policy πθ, we compute individual rewards = (r1, r2, . . . , rN ) and advantages Ai = ri 1 (cid:88) j=1 rj, (5) which are normalized as ˆAi = Ai icy is then optimized using the clipped GRPO objective: σA+ϵ for stability. The pol- (3) (cid:34) (cid:32) JGRPO(θ) = EG j=1 min ˆAj, πθ(oj I) πθold (oj I) (cid:33) (cid:19) clip (cid:18) πθ(oj I) πθold (oj I) , 1 ϵ, 1 + ϵ ˆAj (cid:35) (6) β DKL(πθ πref) where = (x, i) is the multimodal observationinstruction pair, oj is the j-th generated output, and πref denotes the frozen reference policy. Table 2. Comparison with SOTA methods on the Val-Unseen split of VLN-CE [35, 37]. Pano stands for panorama and Odo stands for odometry. indicates methods using the waypoint predictor [26]. MobileVLA-R1 outperforms all methods that do not rely on simulator pre-trained waypoint predictors, even when those methods leverage additional inputs such as depth, panoramic views, and odometry. Method CMA [26] Sim2Sim [34] GridMM [61] Ego2-Map [69] DreamWalker [59] Reborn [2] ETPNav [3] HNR [77] AG-CMTP [9] R2R-CMTP [9] InstructNav [42] LAW [52] CM2 [22] WS-MGMap [10] AO-Planner [8] Seq2Seq [36] CMA [36] NaVid [72] Uni-NaVid [32] NaVILA [13] VLN-R1 [48] OctoNav [21] StreamVLN [63] CorrectNav [71] MobileVLA-R1 (Ours) 5. Experiments Observation R2R-CE Val-Unseen RxR-CE Val-Unseen S.RGB Pano. Depth Odom. NE OS SR SPL NE SR SPL nDTW 6.20 6.07 5.11 5.54 5.53 5.40 4.71 4. 7.90 7.90 6.89 6.83 7.02 6.28 5.55 7.77 7.37 5.47 5.58 5.22 7.00 - 4.98 4.24 4.05 52.0 52.0 61.0 56.0 59.0 57.0 65.0 67.0 39.0 38.0 - 44.0 41.0 47.0 59.0 37.0 40.0 49.0 53.5 62.5 41.2 42.9 64.2 67.5 41.0 43.0 49.0 47.0 49.0 50.0 57.0 61.0 23.0 26.0 31.0 35.0 34.0 38.0 47.0 25.0 32.0 37.0 47.0 54.0 30.2 37.1 56.9 65. 69.7 68.3 36.0 36.0 41.0 41.0 44.0 46.0 49.0 51.0 19.0 22.0 24.0 31.0 27.0 34.0 33.0 22.0 30.0 35.0 42.7 49.0 21.8 33.6 51.9 62.3 65.2 8.76 - - - - 5.98 5.64 5. - - - 10.90 - - 7.06 12.10 - - 6.24 6.77 9.10 - 6.22 4.09 3.92 26.5 - - - - 48.6 54.7 56.3 - - - 8.0 - - 43.3 13.9 - - 48.7 49.3 22.7 - 52.9 69.3 71.5 22.1 - - - - 42.0 44.8 46. - - - 8.0 - - 30.5 11.9 - - 40.9 44.0 17.6 - 46.0 63.3 66.8 47.0 - - - - 63.3 61.9 63.5 - - - 38.0 - - 50.1 30.8 - - - 58.8 - - 61.9 75.2 76.1 Benchmarks. To comprehensively evaluate both the navigation and embodied control capabilities of MobileVLAR1, we adopt two complementary settings. For high-level navigation reasoning, we evaluate MobileVLA-R1 on the VLN-CE benchmarks [35, 37], which provide continuous 3D environments reconstructed from Matterport3D [6]. Compared with R2R [4] and RxR [38], VLN-CE supports continuous interaction with the environment, posing greater challenges for long-horizon spatial reasoning and low-latency control. We follow standard protocols and report results on the val-unseen splits of VLN-CE-R2R and VLN-CE-RxR to measure navigation accuracy, success rate, and trajectory efficiency. To further assess the low-level actuation and reasoningaligned control ability, we conduct action-only evaluations on the QUARD dataset [18], which contains diverse quadruped locomotion and manipulation tasks. While VLN-CE focuses on high-level navigation under linguistic instructions, QUARD emphasizes the precise execution of continuous control commands and discrete behaviors. Evaluation metrics. For the VLN-CE benchmarks, we adopt standard navigation metrics, including navigation error (NE), oracle success rate (OS), success rate (SR), success-weighted path length (SPL), and normalized dynamic time warping (nDTW). These metrics collectively evaluate the agents spatial reasoning accuracy, trajectory efficiency, and overall instruction-following success. For embodied control evaluation on QUARD [18], we follow the official protocol and report the average success rate across six quadruped control tasks. Each value represents the success ratio over 25 evaluation episodes, and tasks are grouped into three difficulty levels (Easy, Medium, Hard) based on motion complexity and interaction difficulty. Network architecture. We initialize MobileVLA-R1 from the pretrained NaVILA [13] to leverage its embodied visionlanguage alignment. Since NaVILA was originally designed for RGB-only perception, we extend its architecture by introducing additional encoders for depth [66] and point cloud modalities. Specifically, we incorporate DepthAnything V2 [67] as the depth encoder and Point Transformer v3 [65] for 3D geometric representation. The multi-modal features from RGB, depth, and point inputs are fused through lightweight projection module before being aligned with the LLaMA3-8B language backbone. Parameter efficient tuning. To enable scalable fine-tuning under limited computation, we adopt LoRA [27] as the primary parameter-efficient tuning strategy. LoRA modules are inserted into the projection and attention layers of the LLaMA3-8B backbone, while the vision encoders remain frozen. This design preserves pretrained multimodal representations from NaVILA while allowing efficient adaptaTable 3. Overall performance on the QUARD benchmark. We evaluate MobileVLA-R1 and baselines on six quadruped control tasks from QUARD [18], grouped by difficulty into Easy, Medium, and Hard. Each value denotes the average success rate over 25 evaluation episodes. Our model achieves consistently higher performance across both locomotion and manipulation tasks, demonstrating robust reasoning-aligned control in challenging embodied settings. Method CLIP [50] VC-1 [44] QUART [17] MoRE [73] MobileVLA-R1 Easy Medium Hard Distinguish Go to Go avoid Go through Crawl Unload 0.44 0.46 0.66 0.82 0.92 0.43 0.43 0.60 0.80 0.89 0.45 0.45 0.53 0. 0.71 0.19 0.31 0.41 0.57 0.65 0.00 0.00 0.32 0.49 0.58 0.00 0.00 0.12 0. 0.44 Average 0.25 0.28 0.44 0.60 0.73 tion to embodied reasoning and control tasks. We first perform supervised alignment on approximately 20K steps using the full MobileVLA-CoT-Episode and MobileVLA-CoT-Nav datasets, along with small subset of the MobileVLA-CoT-Step dataset. The goal is to establish structured reasoning and decision-making abilities. We adopt the LoRA configuration with = 16, α = 32, and cosine learning-rate scheduler. Training is conducted on 4H20 (96 GB) GPUs with learning rate of 2 104, using the AdamW optimizer. Following SFT, we refine the model via Group Relative Policy Optimization (GRPO) [54] to improve reasoning-to-action consistency and execution stability. In each update, the model generates 8 responses per sample, and 5 samples are used for each optimization step. Training runs for 1K steps on single H20 (96 GB) GPU with β = 0.04, clip param = 0.2, and learning rate of 1 106. The policy is optimized with AdamW, using KL regularization to stabilize updates and maintain alignment with the SFT reference model. 5.1. Main Results Vision language navigation. As shown in Table 2, MobileVLA-R1 consistently outperforms prior methods on both R2R-CE [35] and RxR-CE [37] benchmarks. It achieves higher success rates and SPL scores with lower navigation errors, demonstrating superior reasoning alignment and generalization across unseen environments. Quadruped control and manipulation. As shown in Table 3, MobileVLA-R1 achieves the best overall performance on the QUARD benchmark. It consistently surpasses strong baselines such as QUART [17] and MoRE [73] across both locomotion and manipulation tasks, demonstrating stable reasoning-to-control grounding and robust execution under complex conditions. 5.2. Real World Evaluation Robot type settings. As shown in Fig. 5 (a), we use the Unitree Go2 quadruped robot as the physical platform for real-world evaluation. The robot is equipped with an L2 LiDAR for 3D environmental perception, an RGB-D camTable 4. Real-world experiments on quadruped (Unitree Go2) conducted in different environments (Workspace, Corridor, and Outdoor). Simple and Complex refer to simple and complex instruction-following tasks, respectively. Workspace Corridor Outdoor Method Simple Complex Simple Complex Simple Complex NE SR NE SR NE SR NE SR NE SR NE SR GPT-4o [45] NaVILA [13] MobileVLA-R1 2.01 1. 1.03 0.67 0.83 0.93 2.38 1.76 1.23 0.33 0. 0.91 1.49 1.15 0.96 0.53 0.89 1.00 3.00 1. 1.23 0.00 0.67 0.86 - - - 0.67 0. 1.00 - - - 0.50 0.83 0.96 era for panoramic visual sensing, and an NVIDIA Jetson Orin computation module for onboard inference. As illustrated in Fig. 5 (b), the sensory inputs are synchronized and fused into unified pointdepth representation, which is processed by MobileVLA-R1 in real time. When onboard resources are limited, the inference pipeline operates in hybrid mode: the Go2 transmits sensory data to remote H20 server, where MobileVLA-R1 performs reasoning and sends continuous velocity commands back for execution. This closed-loop design ensures low-latency control while maintaining consistent coordination between perception, reasoning, and actuation. Scene setup and task types. To evaluate generalization under diverse real-world conditions, we conduct experiments in three representative environments: Workspace, Corridor, and Outdoor. For each, we define two levels of instruction complexity: Simple tasks consist of one or two short navigation or manipulation commands, whereas Complex tasks require multi-step reasoning and long-horizon spatial planning. This setup enables comprehensive assessment of robustness, reasoning-to-action grounding, and adaptability to environmental diversity. Quantitative real-world evaluation. MobileVLA-R1 consistently outperforms prior models, including NaVILA [13] and GPT-4o [45], across Workspace, Corridor, and Outdoor environments  (Table 4)  . It achieves higher success rates and lower navigation errors under both simple and complex instructions, demonstrating robust reasoning-to-control alignment and stable execution in real-world scenarios. Qualitative real-world evaluation. In Fig. 1 and Fig. 5 (c), MobileVLA-R1 demonstrates smooth and coherent locomotion across various real-world scenes. It accurately follows complex natural-language commands, maintaining stable motion and spatial consistency under dynamic and cluttered conditions. These results highlight the effectiveness of our reasoning-aligned framework in bridging highlevel intent and low-level control for robust quadruped execution. More qualitative visualizations will be included in the Supp. Mat. section 10. 5.3. Ablation Study Effect of reward design. To verify the effectiveness of our reward formulation in GRPO training, we ablate the three reward components: the movement reward Rm, the action Figure 5. (a) Hardware platform: the Unitree Go2 quadruped robot is equipped with Jetson Orin Nano (on-board PC) as the computation module, an L2 LiDAR for 3D environment perception, and an Intel RealSense D435i RGB-D camera for visual sensing. (b) Deployment process: RGBDepth and 3D point cloud data are transmitted to MobileVLA-R1, which performs multimodal reasoning and action generation. The resulting velocity and motion commands are sent back to the on-board PC for real-time execution on the robot. (c) RealWorld qualitative results: MobileVLA-R1 effectively integrates RGB, depth, and map observations to follow long-horizon language instructions with coherent spatial reasoning. Table 5. Reward decomposition on R2R-CE Val-Unseen. denotes inclusion of the reward. The first row is the SFT-only baseline, while subsequent rows apply GRPO with different subsets of movement (Rm), action (Raction), and format (Rf ormat) rewards. Removing any term degrades SR and SPL, and using all three yields the best navigation success and path efficiency. complementary optimization signals for reasoning-aligned and control-stable policy learning. Additional ablation. We further conduct incremental modality encoder ablations to analyze the contribution of each perception stream in the Supp. Mat. section 7. Rm Raction Rf ormat SR SPL 58.0 60.7 61.9 59.6 64.5 63.4 65.2 68.3 53. 55.5 56.8 54.7 60.2 59.1 61.0 65.2 reward Raction, and the format reward Rf ormat. Table 5 reports the performance on the R2R-CE Val-Unseen split. Removing any rewards causes noticeable degradation in navigation success and trajectory efficiency. Without Rm, the model exhibits unstable locomotion and oscillatory motion. Excluding Raction weakens high-level decision consistency, while dropping Rf ormat leads to malformed or unstructured outputs that cannot be reliably parsed into executable commands. Combining all three rewards yields the highest success rate and SPL, validating that they provide 6. Conclusion In this work, we introduce MobileVLA-R1, unified visionlanguageaction framework that bridges high-level reasoning and low-level control for quadruped robots. By decoupling structured chain-of-thought reasoning from continuous motor execution, our model achieves interpretable decision-making and robust control across diverse environments. The two-stage training paradigm, which combines supervised CoT alignment with GRPO-based reinforcement learning, effectively enhances reasoning consistency, control stability, and long-horizon execution. Extensive experiments on the VLN-CE and QUARD benchmarks, as well as real-world deployments on the Unitree Go2 robot, demonstrate the superior performance and adaptability of MobileVLA-R1 compared to existing methods. These results highlight the effectiveness of integrating structured reasoning with continuous control, advancing the development of generalizable embodied agents."
        },
        {
            "title": "References",
            "content": "[1] Michael Ahn, Anthony Brohan, et al. Do as can and not as say: Grounding language in robotic affordances. In arXiv preprint arXiv:2204.01691, 2022. 1 [2] Dong An et al. 1st place solutions for rxr-habitat vision-andlanguage navigation competition. In CVPRW, 2022. 6 [3] Dong An et al. Etpnav: Evolving topological planning for vision-language navigation in continuous environments. PAMI, 2024. 6 [4] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko Sunderhauf, Ian Reid, Stephen Gould, and Anton van den Hengel. Vision-and-language navigation: Interpreting visually-grounded navigation instructions in real environments. In 2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 36743683, 2018. 1, 2, 3, 6 [5] Anthony Brohan, Noah Brown, et al. Rt-2: Vision-languageaction models transfer web knowledge to robotic control. arXiv preprint arXiv:2307.15818, 2023. 1, 2 [6] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran Song, Andy Zeng, and Yinda Zhang. Matterport3D: Learning from RGBD data in indoor environments. International Conference on 3D Vision (3DV), 2017. 2, 6 [7] Devendra Singh Chaplot, Dhiraj Gandhi, Abhinav Kumar Gupta, and Ruslan Salakhutdinov. Object goal navigation using goal-oriented semantic exploration. ArXiv, abs/2007.00643, 2020. [8] Jiaqi Chen et al. Affordances-oriented planning using foundation models for continuous vision-language navigation. arXiv preprint arXiv:2407.05890, 2024. 6 [9] Kevin Chen et al. Topological planning with transformers for vision-and-language navigation. In CVPR, 2021. 6 [10] Peihao Chen et al. Weakly-supervised multi-granularity map In NeurIPS, learning for vision-and-language navigation. 2022. 6 [11] Shizhe Chen, Pierre-Louis Guhur, Cordelia Schmid, and Ivan Laptev. History aware multimodal transformer for vision-and-language navigation. Advances in neural information processing systems, 34:58345847, 2021. 2 [12] Yanjun Chen, Yirong Sun, Xinghao Chen, Jian Wang, Xiaoyu Shen, Wenjie Li, and Wei Zhang. Integrating chainof-thought for multimodal alignment: study on 3d visionlanguage learning. ArXiv, abs/2503.06232, 2025. 2 [13] An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, and Xiaolong Wang. Navila: Legged robot visionlanguage-action model for navigation. In RSS, 2025. 4, 6, [14] Gheorghe Comanici et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, arXiv preprint and next generation agentic capabilities. arXiv:2507.06261, 2025. 3 [15] DeepSeek-AI et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv, abs/2501.12948, 2025. 4 [16] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pages 4171 4186, 2019. 2 [17] Pengxiang Ding, Han Zhao, Wenjie Zhang, Wenxuan Song, Min Zhang, Siteng Huang, Ningxi Yang, and Donglin Wang. Quar-vla: Vision-language-action model for quadruped robots. In ECCV 2024, page 352367, Berlin, Heidelberg, 2024. Springer-Verlag. 7 [18] Pengxiang Ding, Han Zhao, Wenjie Zhang, Wenxuan Song, Min Zhang, Siteng Huang, Ningxi Yang, and Donglin Wang. Quar-vla: Vision-language-action model for quadruped robots. In European Conference on Computer Vision, pages 352367. Springer, 2025. 2, 3, 6, [19] Danny Driess, F. Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan Wahid, Jonathan Tompson, Quan Ho Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet, Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff, Andy Zeng, Igor Mordatch, and Peter R. Florence. Palm-e: An embodied multimodal In International Conference on Machine language model. Learning, 2023. 1 [20] Daniel Fried, Ronghang Hu, Volkan Cirik, Anna Rohrbach, Jacob Andreas, Louis-Philippe Morency, Taylor BergKirkpatrick, Kate Saenko, Dan Klein, and Trevor Darrell. Speaker-follower models for vision-and-language navigation. In Neural Information Processing Systems (NeurIPS), 2018. 2 [21] Chen Gao et al. Octonav: Towards generalist embodied navigation. arXiv preprint arXiv:2506.09839, 2025. 2, 6 [22] Georgios Georgakis et al. Cross-modal map learning for vision and language navigation. In CVPR, 2022. 6 [23] Weituo Hao, Chunyuan Li, Xiujun Li, Lawrence Carin, and Jianfeng Gao. Towards learning generic agent for visionand-language navigation via pre-training. Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2 [24] Yicong Hong, Cristian Rodriguez-Opazo, Yuankai Qi, Qi Wu, and Stephen Gould. Language and visual entity relationship graph for agent navigation. In NeurIPS 2020, pages 112, 2020. 2 [25] Yicong Hong, Qi Wu, Yuankai Qi, Cristian RodriguezOpazo, and Stephen Gould. recurrent vision-and-language bert for navigation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 16431653, 2021. [26] Yicong Hong, Zun Wang, Qi Wu, and Stephen Gould. Bridging the gap between learning in discrete and continuous environments for vision-and-language navigation. 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1541815428, 2022. 6 [27] J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan AllenZhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. Lora: ArXiv, Low-rank adaptation of large language models. abs/2106.09685, 2021. 6 [28] Ting Huang, Zeyu Zhang, and Hao Tang. 3d-r1: Enhancing reasoning in 3d vlms for unified scene understanding. arXiv preprint arXiv:2507.23478, 2025. 2 [29] Ting Huang, Zeyu Zhang, Yemin Wang, and Hao Tang. 3d coca: Contrastive learners are 3d captioners. arXiv preprint arXiv:2504.09518, 2025. [30] Ting Huang, Zeyu Zhang, Ruicheng Zhang, and Yang Zhao. Dc-scene: Data-centric learning for 3d scene understanding. arXiv preprint arXiv:2505.15232, 2025. 2 [31] Wenlong Huang, P. Abbeel, Deepak Pathak, and Igor Mordatch. Language models as zero-shot planners: ExtractArXiv, ing actionable knowledge for embodied agents. abs/2201.07207, 2022. 1 [32] Zhang Jiazhao et al. Uni-navid: video-based visionlanguage-action model for unifying embodied navigation tasks. RSS, 2025. 2, [33] Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. 2 [34] Jacob Krantz and Stefan Lee. Sim-2-sim transfer for visionand-language navigation in continuous environments. In ECCV, 2022. 6 [35] Jacob Krantz, Erik Wijmans, Arjun Majundar, Dhruv Batra, and Stefan Lee. Beyond the nav-graph: Vision and language navigation in continuous environments. In European Conference on Computer Vision (ECCV), 2020. 6, 7 [36] Jacob Krantz et al. Beyond the nav-graph: Vision-andlanguage navigation in continuous environments. In ECCV, pages 104120. Springer, 2020. 6 [37] Alexander Ku, Peter Anderson, Roma Patel, Eugene Ie, and Jason Baldridge. Room-across-room: Multilingual visionand-language navigation with dense spatiotemporal groundIn Proceedings of the 2020 Conference on Empirical ing. Methods in Natural Language Processing (EMNLP), pages 43924412, 2020. 2, 6, 7 [38] Alexander Ku et al. Room-across-room: Multilingual visionand-language navigation with dense spatiotemporal grounding. In EMNLP, 2020. 2, 3, [39] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. ArXiv, abs/2304.08485, 2023. 4 [40] Qingxiang Liu, Ting Huang, Zeyu Zhang, and Hao Tang. Nav-r1: Reasoning and navigation in embodied scenes. arXiv preprint arXiv:2509.10884, 2025. 2 [41] Zeting Liu, Zida Yang, Zeyu Zhang, and Hao Tang. Evovla: Self-evolving vision-language-action model. arXiv preprint arXiv:2511.16166, 2025. 2 [42] Yuxing Long et al. Instructnav: Zero-shot system for generic arXiv instruction navigation in unexplored environment. preprint arXiv:2406.04882, 2024. [43] Chih-Yao Ma, Jiasen Lu, Zuxuan Wu, Ghassan AlRegib, Zsolt Kira, Richard Socher, and Caiming Xiong. Selfmonitoring navigation agent via auxiliary progress estimaIn Proceedings of the International Conference on tion. Learning Representations (ICLR), 2019. 2 [44] Arjun Majumdar, Karmesh Yadav, Sergio Arnaud, et al. Where are we in the search for an artificial visual cortex for embodied intelligence? arXiv preprint arXiv:2303.18240, 2023. 7 [45] OpenAI. Hello gpt-4o, 2024. 7 [46] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke E. Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Francis Christiano, Jan Leike, and Ryan J. Lowe. Training language models to follow instructions with human feedback. ArXiv, abs/2203.02155, 2022. 2 [47] Yuankai Qi, Zizheng Pan, Yicong Hong, Ming-Hsuan Yang, Anton van den Hengel, and Qi Wu. The road to knowwhere: An object-and-room informed sequential bert for indoor vision-language navigation. 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 1635 1644, 2021. 2 [48] Zhangyang Qi et al. Vln-r1: Vision-language navigation via reinforcement fine-tuning. arXiv preprint arXiv:2506.17221, 2025. 2, 6 [49] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. OpenAI, 2019. Accessed: 2024-1115. [50] Alec Radford, Jong Wook Kim, Chris Hallacy, et al. Learning transferable visual models from natural language supervision. arXiv preprint arXiv:2103.00020, 2021. 7 [51] Abhinav Rajvanshi, Karan Sikka, Xiao Lin, Bhoram Lee, Han-Pang Chiu, and Alvaro Velasquez. Saynav: grounding large language models for dynamic planning to navigation in new environments. In Proceedings of the Thirty-Fourth International Conference on Automated Planning and Scheduling. AAAI Press, 2024. 2 [52] Sonia Raychaudhuri et al. Language-aligned waypoint (law) supervision for vision-and-language navigation in continuous environments. In EMNLP, 2021. 6 [53] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. ArXiv, abs/1707.06347, 2017. 4 [54] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Jun-Mei Song, Mingchuan Zhang, Y. K. Li, Yu Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. ArXiv, abs/2402.03300, 2024. 2, 4, 7 [55] Zirui Song, Guangxian Ouyang, Meng Fang, Hongbin Na, Zijing Shi, Zhenhao Chen, Fu Yujie, Zeyu Zhang, Shiyu Jiang, Miao Fang, et al. Hazards in daily life? enabling robots to proactively detect and resolve anomalies. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 73997415, 2025. [56] Zirui Song, Guangxian Ouyang, Mingzhe Li, Yuheng Ji, Chenxi Wang, Zixiang Xu, Zeyu Zhang, Xiaoqing Zhang, Qian Jiang, Zhenhao Chen, et al. Maniplvm-r1: Rein- [73] Han Zhao, Wenxuan Song, Donglin Wang, Xinyang Tong, Pengxiang Ding, Xuelian Cheng, and Zongyuan Ge. More: Unlocking scalability in reinforcement learning for quadruped vision-language-action models, 2025. 7 [74] Xiangyu Zhao, Junming Lin, Tianhao Liang, Yifan Zhou, Wenhao Chai, Yuzhe Gu, Weiyun Wang, Kai Chen, Gen Luo, Wenwei Zhang, Junchi Yan, Hua Yang, Haodong Duan, and Xue Yang. Mm-helix: Boosting multimodal long-chain reflective reasoning with holistic platform and adaptive hybrid policy optimization. arXiv preprint arXiv:2510.08540, 2025. 2 [75] Fengda Zhu, Yi Zhu, Xiaojun Chang, and Xiaodan Liang. Vision-language navigation with self-supervised auxiliary reasoning tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 10012 10022, 2020. 2 [76] Ziyu Zhu et al. Move to understand 3d scene: Bridging visual grounding and exploration for efficient and versatile embodied navigation. ICCV, 2025. 2 [77] Wang Zihan et al. Lookahead exploration with neural radiance representation for continuous vision-language navigation. In CVPR, 2024. forcement learning for reasoning in embodied manipulaarXiv preprint tion with large vision-language models. arXiv:2505.16517, 2025. 2 [57] Yujin Tang, Wenhao Yu, Jie Tan, Heiga Zen, AleksanSaytap: Language to dra Faust, and Tatsuya Harada. quadrupedal locomotion. ArXiv, abs/2306.07580, 2023. 2 [58] Ashish Vaswani, Noam M. Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Neural Information Processing Systems, 2017. 2 [59] Hanqing Wang et al. Dreamwalker: Mental planning for continuous vision-language navigation. In ICCV, 2023. 6 [60] Xin Eric Wang, Qiuyuan Huang, Asli Celikyilmaz, Jianfeng Gao, Dinghan Shen, Yuan fang Wang, William Yang Wang, and Lei Zhang. Reinforced cross-modal matching and selfsupervised imitation learning for vision-language navigation. 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 66226631, 2018. 1 [61] Zihan Wang et al. Gridmm: Grid memory map for visionand-language navigation. In ICCV, 2023. [62] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2022. Curran Associates Inc. 2 [63] Meng Wei et al. Streamvln: Streaming vision-and-language navigation via slowfast context modeling. arXiv preprint arXiv:2507.05240, 2025. 6 [64] Jialong Wu, Shaofeng Yin, Ningya Feng, and Mingsheng Long. Rlvr-world: Training world models with reinforcement learning. arXiv preprint arXiv:2505.13934, 2025. 2 [65] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler, faster, stronger. In CVPR, 2024. 6 [66] Zhengri Wu, Yiran Wang, Yu Wen, Zeyu Zhang, Biao Wu, and Hao Tang. Stereoadapter: Adapting stereo depth estimation to underwater scenes. arXiv preprint arXiv:2509.16415, 2025. 6 [67] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. ArXiv, abs/2406.09414, 2024. 6 [68] Angen Ye, Zeyu Zhang, Boyuan Wang, Xiaofeng Wang, Dapeng Zhang, and Zheng Zhu. Vla-r1: Enhancing reaarXiv preprint soning in vision-language-action models. arXiv:2510.01623, 2025. [69] Hong Yicong et al. Learning navigational visual representations with semantic map supervision. In ICCV, 2023. 6 [70] Bangguo Yu, Hamidreza Kasaei, and Ming Cao. L3mvn: Leveraging large language models for visual target navigation. 2023 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS), pages 35543560, 2023. 2 [71] Zhuoyuan Yu et al. Correctnav: Self-correction flywheel empowers vision-language-action navigation model. arXiv preprint arXiv:2508.10416, 2025. 6 [72] Jiazhao Zhang et al. Navid: Video-based vlm plans the next step for vision-and-language navigation. In RSS, 2024. 6 MobileVLA-R1: Reinforcing Vision-Language-Action for Mobile Robots"
        },
        {
            "title": "Supplementary Material",
            "content": "7. More Ablation Study Effect of multimodal perception. We perform an incremental modality encoder ablation to examine the contributions of each sensory modality. Starting from the text and image encoder, we progressively add depth and point cloud encoders to the model. As shown in Table 6, each additional modality leads to consistent improvements in navigation success (SR) and trajectory efficiency (SPL) on both R2R-CE and RxR-CE. Depth cues enhance local geometric understanding and obstacle awareness, while point cloud features further strengthen 3D spatial reasoning and path planning. The full multimodal configuration achieves the best overall performance, demonstrating that rich 3D perception substantially benefits reasoning-grounded embodied navigation. Table 6. Incremental modality encoder ablation on VLN-CE benchmarks. Starting from the text and image encoder, we incrementally add depth and point cloud encoders. Each additional modality consistently improves navigation success and trajectory efficiency on both R2R-CE and RxR-CE. The full configuration MobileVLA-R1 achieves the best performance, demonstrating the benefit of rich 3D perception for reasoning-grounded navigation. Setting R2R-CE RxR-CE SR SPL SR SPL Text & Image Encoder + Depth Encoder + Point Encoder 62.5 66.0 67. MobileVLA-R1 (Ours) 68.3 59.0 62.0 63.5 65.2 66.0 69.0 70.2 71. 61.5 64.0 65.2 66.8 8. CoT Data Generation and Quality Control Avoiding data leakage. We generate CoT annotations only for the official training splits of R2R, RxR, and QUARD, and do not synthesize data on any validation or test episodes. All quantitative results are obtained on the original benchmarks using their human-provided instructions and evaluation protocols, and the teacher model (Gemini-2.5-Flash) is never queried with ground-truth answers for the evaluated trajectories. Semi-automatic verification pipeline. To control annotation quality, we adopt semi-automatic verification pipeline. We first apply rule-based parsing and regular-expression checks to filter out malformed outputs that violate the <think>...</think> and <answer>...</answer> formats, contain unparsable control commands, or propose actions outside the QUARD action space. The remaining samples are then manually reviewed on randomly sampled subset from each of the three CoT subsets. During this pass, two authors independently check whether (i) the reasoning is logically consistent with the visual observations and instructions, (ii) the proposed action is feasible given the state-action history, and (iii) there are no obvious safety issues, such as collisions. Samples that fail any of these criteria are either removed or corrected, and we additionally spot-check long-horizon and cluttered episodes to identify systematic failure modes. In practice, the most common errors are overly generic reasoning or slightly mis-scaled velocity commands, which our filtering removes before training. 9. Test-Time Efficiency and Practical Considerations Test-time efficiency is critical for deploying embodied models on real quadruped platforms. In our current implementation, MobileVLA-R1 runs on remote H20 GPU server and communicates with the robot via wired connection through local computer. The vision-language backbone has 8B parameters, which allows for strong reasoning capabilities but also introduces non-trivial latency. On this setup, one forward pass of MobileVLA-R1 takes about 10 seconds per decision step, and the end-to-end closed loop (including perception I/O and network transmission) requires roughly 15 seconds per control step. Despite this latency, the current system is adequate for our experimental settings, which focus on indoor navigation and short-range manipulation in quasi-static environments, where the robot moves cautiously and the scene changes slowly. However, we acknowledge that such latency is too high for highly dynamic scenarios, fast locomotion, or finegrained real-time interaction, and we view this as key limitation of the present prototype. There are several practical trade-offs and mitigation strategies. First, the action decoder in MobileVLA-R1 already converts CoT outputs into continuous commands over short horizon, allowing the robot to execute multiple lowlevel steps per high-level reasoning cycle. This suggests regime where CoT-based planning runs at lower frequency (e.g., when the instruction or scene changes), while lightweight on-board controllers track the issued velocity commands at higher frequency. Second, the visual encoders and language backbone can, in principle, be replaced by smaller architectures or compressed via distillation and quantization to trade modest amount of accuracy for substantial reductions in latency and memory footprint. Third, onds of end-to-end delay per control step, which restricts deployment to relatively slow and quasi-static scenarios. This level of latency is inadequate for fast locomotion or highly dynamic humanrobot interaction and calls for more efficient model and system designs. Future work. Future work could expand the action space and improve the granularity of action decoding, enabling the model to execute more diverse and context-dependent skills. Beyond supervised learning, self-supervised and lifelong learning strategies would allow the agent to autonomously acquire new abilities and adapt to evolving environments without extensive human annotation. On the systems side, we believe the latency gap can be addressed by combining MobileVLA-R1 with model compression techniques (e.g., distillation and quantization), smaller backbones, and more aggressive hierarchical control schemes, where high-level CoT planning runs at low frequency while compact low-level policies execute onboard. In addition, integrating richer multi-modal sensing, enhancing cross-domain transfer through large-scale pretraining, and extending the framework to dexterous manipulation and collaborative tasks represent promising directions toward more adaptive, intelligent, and general-purpose embodied agent capable of reliable operation in complex, unstructured real-world settings. caching perception features and sharing CoT prefixes across consecutive steps can amortize the cost of long-horizon reasoning. systematic exploration of these design choices is beyond the scope of this work, but our results highlight that achieving strong CoT-based reasoning on quadruped robots is currently possible, even under relatively high latency, and point toward model compression and hierarchical control as promising directions for practical deployment. 10. Visualization We provide qualitative visualizations in both simulato illustrate the perceptor and real-world settings tionreasoningaction process of MobileVLA-R1 across diverse environments. As shown in Figure 6 and Figure 7, the robot receives natural-language instructions and performs step-bystep reasoning before executing continuous actions in the R2R-CE and RxR-CE validation splits. Each trajectory demonstrates coherent visual perception, spatial reasoning, and action grounding under unseen scenes and linguistic variations, highlighting the models robust cross-lingual understanding and spatial consistency. Finally, Figure 8 - Figure 10 present real-world deployments on the Unitree Go2 robot. Under complex indoor and outdoor environments, MobileVLA-R1 accurately follows long-horizon commands, maintaining stable motion and continuous spatial grounding. These results validate that MobileVLA-R1 generalizes from simulated training to physical environments with robust reasoning-aligned control. 11. Limitation and Future Work Limitations. While MobileVLA-R1 achieves strong performance in both simulation and real-world environments, several limitations remain that highlight directions for future research. First, the current framework mainly focuses on navigation and basic manipulation, where the action decoder maps high-level language instructions to fixed set of predefined behaviors. This design limits the systems ability to cope with highly dynamic or novel situations that demand fine-grained motor control and continuous adaptation and constrains the diversity and compositionality of learned skills. Second, MobileVLA-R1 still relies heavily on supervised datasets and semi-scripted environments for training reasoning and control policies. Such dependence constrains the models generalization to open-ended, long-horizon tasks and truly unstructured environments. third limitation lies in test-time latency. Running an 8B vision-language backbone on remote server results in roughly 10 seconds of model inference and about 15 secFigure 6. Qualitative visualization on R2R-CE. MobileVLA-R1 executes English navigation instructions in the simulator. Figure 7. Qualitative visualization on RxR-CE. MobileVLA-R1 follows multilingual instructions with complex spatial semantics, maintaining coherent reasoning and stable motion across unseen environments. Figure 8. Real-world qualitative results. The robot demonstrates spatially grounded perceptionreasoningaction behaviors, such as approaching targets, interacting with objects, and avoiding obstacles in realistic indoor environments. Figure 9. Outdoor real-world qualitative results. MobileVLA-R1 delivers highly reliable long-horizon reasoning and control in diverse outdoor environments, showcasing its strong real-world generalization. Figure 10. Additional outdoor real-world qualitative results. MobileVLA-R1 exhibits fine-grained obstacle avoidance, target-aware spatial reasoning, and high-precision locomotion control in diverse real-world outdoor scenes."
        }
    ],
    "affiliations": [
        "Peking University"
    ]
}