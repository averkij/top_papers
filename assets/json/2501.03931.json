{
    "paper_title": "Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers",
    "authors": [
        "Yuechen Zhang",
        "Yaoyang Liu",
        "Bin Xia",
        "Bohao Peng",
        "Zexin Yan",
        "Eric Lo",
        "Jiaya Jia"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present Magic Mirror, a framework for generating identity-preserved videos with cinematic-level quality and dynamic motion. While recent advances in video diffusion models have shown impressive capabilities in text-to-video generation, maintaining consistent identity while producing natural motion remains challenging. Previous methods either require person-specific fine-tuning or struggle to balance identity preservation with motion diversity. Built upon Video Diffusion Transformers, our method introduces three key components: (1) a dual-branch facial feature extractor that captures both identity and structural features, (2) a lightweight cross-modal adapter with Conditioned Adaptive Normalization for efficient identity integration, and (3) a two-stage training strategy combining synthetic identity pairs with video data. Extensive experiments demonstrate that Magic Mirror effectively balances identity consistency with natural motion, outperforming existing methods across multiple metrics while requiring minimal parameters added. The code and model will be made publicly available at: https://github.com/dvlab-research/MagicMirror/"
        },
        {
            "title": "Start",
            "content": "Magic Mirror: ID-Preserved Video Generation in Video Diffusion Transformers Yuechen Zhang1* Yaoyang Liu2* Bin Xia1 Bohao Peng1 Zexin Yan4 Eric Lo1 Jiaya Jia1,2,3 1CUHK 2HKUST 3SmartMore 4CMU *https://julianjuaner.github.io/projects/MagicMirror/ 5 2 0 J 7 ] . [ 1 1 3 9 3 0 . 1 0 5 2 : r Figure 1. Magic Mirror generates text-to-video results given the ID reference image. Each video pair shows 24 frames (from total of 49) with its corresponding face reference displayed in the bottom-left corner. Please use Adobe Acrobat Reader for video playback to get optimal viewing experience. Complete videos are available on the project page."
        },
        {
            "title": "Abstract",
            "content": "We present Magic Mirror, framework for generating identity-preserved videos with cinematic-level quality and dynamic motion. While recent advances in video diffusion models have shown impressive capabilities in textto-video generation, maintaining consistent identity while producing natural motion remains challenging. Previous methods either require person-specific fine-tuning or struggle to balance identity preservation with motion diversity. Built upon Video Diffusion Transformers, our method introduces three key components: (1) dual-branch facial feature extractor that captures both identity and structural features, (2) lightweight cross-modal adapter with Con- *Equal contribution. ditioned Adaptive Normalization for efficient identity integration, and (3) two-stage training strategy combining synthetic identity pairs with video data. Extensive experiments demonstrate that Magic Mirror effectively balances identity consistency with natural motion, outperforming existing methods across multiple metrics while requiring minimal parameters added. The code and model will be made publicly available at: https://github.com/dvlabresearch/MagicMirror/. 1. Introduction Human-centered content generation has been focal point in computer vision research. Recent advancements in image generation, particularly through Diffusion Models [1 4], have propelled personalized content creation to the forefront of computer vision research. While significant progress has been made in preserving personal identity (ID) in image generation [510], achieving comparable fidelity in video generation remains challenging. Existing ID-preserving video generation methods show promise but face limitations. Approaches like MagicMe and ID-Animator [11, 12], utilizing inflated UNets [13] for fine-tuning or adapter training, demonstrate some success in maintaining identity across frames. However, they are ultimately restricted by the generation models inherent capabilities, often failing to produce highly dynamic videos (see Fig. 2). These approaches make static copy-andpaste instead of generating dynamic facial motions. Another branch of methods combines image personalization methods with Image-to-Video (I2V) generation [1416]. While these two-stage solutions preserve ID to some extent, they often struggle with stability in longer sequences and require separate image generation step. To address current shortcomings, we present Magic Mirror, single-stage framework designed to generate high-quality videos while maintaining strong ID consistency and dynamic natural facial motions. Our approach leverages native video diffusion models [14] to generate ID-specific videos, aiming to empower individuals as protagonists in their virtual narratives, and bridge the gap between personalized ID generation and high-quality video synthesis. The generation of high-fidelity identity-conditioned videos poses several technical challenges. primary challenge stems from the architectural disparity between image and video generation paradigms. State-of-the-art video generation models, built upon full-attention Diffusion Transformer (DiT) architectures [14, 17], are not directly compatible with conventional cross-attention conditioning methods. To bridge this gap, we introduce lightweight identityconditioning adapter integrated into the CogVideoX [14] framework. Specifically, we propose dual-branch facial embedding that simultaneously preserves high-level identity features and reference-specific structural information. Our analysis reveals that current video foundation models optimize for text-video alignment, often at the cost of spatial fidelity and generation quality. This trade-off manifests in reduced image quality metrics on benchmarks such as VBench [18], particularly affecting the preservation of finegrained identity features. To address it, we develop Conditioned Adaptive Normalization (CAN) that effectively incorporates identity conditions into the pre-trained foundation model. This module, combined with our dual facial guidance mechanism, enables identity conditioning through attention guidance and feature distribution guidance. Another significant challenge lies in the acquisition of high-quality training data. While paired image data with consistent identity is relatively abundant, obtaining highFigure 2. Magic Mirror generates dynamic facial motion. IDAnimator [11] and Video Ocean [19] exhibit limited motion range due to strong identity-preservation constraint. Magic Mirror achieves more dynamic facial expressions while maintaining reference identity fidelity. fidelity image-video pairs that maintain identity consistency remains scarce. To address this limitation, we develop strategic data synthesis pipeline that leverages identity preservation models [5] to generate paired training data. Our training methodology employs progressive approach: initially pre-training on image data to learn robust identity representations, followed by video-specific fine-tuning. This two-stage strategy enables effective learning of identity features while ensuring temporal consistency in facial expressions across video sequences. We evaluate our method on multiple general metrics by constructing human-centric video generation test set and comparing it with the aforementioned competitive IDpreserved video generation methods. Extensive experimental and visual evaluations demonstrate that our approach successfully generates high-quality videos with dynamic content and strong facial consistency, as illustrated in Fig. 1. Magic Mirror represents the first approach to achieve customized video generation using Video DiT without requiring person-specific fine-tuning. Our work marks an advancement in personalized video generation, paving the way for enhanced creative expression in the digital domain. (1) We introduce Magic Mirror, novel fine-tuning free framework for generating ID-preserving videos; (2) We design lightweight adapter with conditioned adaptive normalizations for effective integration of face embeddings in full-attention Diffusion Transformer architectures; (3) We develop dataset construction method that combines synthetic data generation with progressive training strategy to address data scarcity challenges in personalized video generation. Our main contributions are: 2 2. Related Works Diffusion Models. Since the introduction of DDPM [3], diffusion models have demonstrated remarkable capabilities across diverse domains, spanning NLP [20, 21], medical imaging [22, 23], and molecular modeling [24, 25]. In computer vision, following initial success in image generation [26, 27], Latent Diffusion Models (LDM) [1] significantly reduced computational requirements while maintaining generation quality. Subsequent developments in conditional architectures [28, 29] enabled fine-grained concept customization over the generation process. Video Generation via Diffusion Models. Following the emergence of diffusion models, their superior controllability and diversity in image generation [30] have led to their prominence over traditional approaches based on GANs [3133] and auto-regressive Transformers [3436]. The Video Diffusion Model (VDM) [37] pioneered video generation using diffusion models by extending the traditional U-Net [38] architecture to process temporal information. Subsequently, LVDM [39] demonstrated the effectiveness of latent space operations, while AnimateDiff [13] adapted text-to-image models for personalized video synthesis. significant advancement came with the Diffusion Transformer (DiT) [17], which successfully merged Transformer architectures [40, 41] with diffusion models. Building on this foundation, Latte [42] emerged as the first open-source text-to-video model based on DiT. Following the breakthrough of SORA [43], several open-source initiatives including Open-Sora-Plan [44], Open-Sora [45], and CogVideoX [14] have advanced video generation through DiT architectures. While current research predominantly focuses on image-to-video translation [15, 16, 46] and motion control [47, 48], the critical challenge of ID-preserving video generation remains relatively unexplored. ID-Preserved Generation. ID-preserved generation, or identity customization, aims to maintain specific identity characteristics in generated images or videos. Initially developed in the GAN era [31] with significant advances in face generation [33, 49, 50], this field has evolved substantially with diffusion models, demonstrating enhanced capabilities in novel image synthesis [28, 51]. Current approaches to ID-preserved image generation fall into two main categories: Tuning-based Models: These approaches fine-tune models using one or more reference images to generate identityconsistent outputs. Notable examples include Textual Inversion [51] and Dreambooth [28]. Tuning-free Models: Addressing the computational overhead of tuning-based approaches, these models maintain high ID fidelity through additional conditioning and trainable parameters. Starting with IP-adapter [52], various methods like InstantID, PhotoMaker [510] have emerged to enable efficient, high-quality personalized generation. ID-preserved video generation introduces additional complexities, particularly in synthesizing realistic facial movements from static references while maintaining identity consistency. Current approaches include MagicMe [12], tuning-based method requiring per-identity optimization, and ID-Animator [11], tuning-free approach utilizing face adapters and decoupled Cross-Attention [52]. However, these methods face challenges in maintaining dynamic expressions while preserving identity, and are constrained by base model limitations in video quality, duration, and prompt adherence. The integration of Diffusion Transformers presents promising opportunities for advancing IDpreserved video generation. 3. Magic Mirror An overview of Magic Mirror is illustrated in Fig. 3. This dual-branch framework (Sec. 3.2) extracts facial identity features from one or more reference images r. These embeddings are subsequently processed through DiT backbone augmented with lightweight cross-modal adapter, incorporating conditioned adaptive normalization (Sec. 3.3). This architecture enables Magic Mirror to synthesize identity-preserved text-to-video outputs. The following sections elaborate on the preliminaries of diffusion models (Sec. 3.1) and each component of our method. 3.1. Preliminaries Latent Diffusion Models (LDMs) generate data by iteratively reversing noise corruption process, converting random noise into structured samples. At time step {0, . . . , }, the model predicts latent state xt conditioned on xt+1: pθ(xtxt+1) = (xt; (cid:101)µt, (cid:101)βtI ), where θ represents the model parameters, (cid:101)µt denotes the predicted mean, and (cid:101)βt is the variance schedule. (1) The training objective typically employs mean squared error (MSE) loss on the noise prediction ˆϵθ(xt, t, ctxt): ϵ ϵθ(xt, t, ctxt)2(cid:105) Lnoise = Et,ctxt,ϵN (0,1) (cid:104) , (2) where ctxt denotes the text condition. Recent studies on controllable generation [7, 5254] extend this framework by incorporating additional control signals, such as image condition cimg. This is achieved through feature extractor τimg that processes reference image r: cimg = τimg(r). Consequently, the noise prediction function in Eq. (2) becomes ϵθ(xt, t, ctxt, cimg). 3.2. Decoupled Facial Feature Extraction The facial feature extraction component of Magic Mirror is illustrated in the left part of Fig. 3. Given an ID reference 3 Figure 3. Overview of Magic Mirror. The framework employs dual-branch feature extraction system with ID and face perceivers, followed by cross-modal adapter (illustrated in Fig. 4) for DiT-based video generation. By optimizing trainable modules marked by the flame, our method efficiently integrates facial features for controlled video synthesis while maintaining model efficiency. image R(hw3), our model extracts facial condition embeddings cface = {xface, xfuse} using hybrid query-based feature perceivers. These embeddings capture both highlevel identity features and facial structural information: xface = τface(qface, ) (3) xfuse = Ffuse(xid, qtxt), where xid = τid(qid, ) (4) where = Fface(r) represents dense feature maps extracted from pre-trained CLIP ViT Ffeat [55]. Both perceivers τface and τid utilize the standard Q-Former[56] architecture with distinct query conditions qface and qid. Here, qface is learnable embedding for facial structure extraction, while qid = Fid(r) represents high-level facial features extracted by face encoder Fid [5, 57]. Each perceiver employs crossattention between iteratively updated queries and dense features to obtain compressed feature embeddings. The compressed embeddings are integrated through decoupled mechanism. Following recent approaches in newconcept customization [5, 28], we fuse facial embeddings with text embeddings at identity-relevant tokens (e.g., man, woman) in the input prompt qtxt, as expressed in Eq. (4). fusion MLP Ffuse projects xid into the text embedding space. The final text embedding for DiT input is computed as ˆxtxt = mxfuse + (1 m)xtxt, where denotes tokenlevel binary mask indicating fused token positions qtxt. 3.3. Conditioned Adaptive Normalization Having obtained the decoupled ID-aware conditions cface, we address the challenge of their efficient integration into the video diffusion transformer. Traditional Latent Diffusion Models, exemplified by Stable Diffusion [1], utilize isolated cross-attention mechanisms for condition injection, facilitating straightforward adaptation to new conditions through decoupled cross-attention [6, 52]. This approach is enabled by uniform conditional inputs (specifically, text condition ctxt) across all cross-attention layers. However, our framework builds upon CogVideoX [14], which implements cross-modal full-attention paradigm with layerwise distribution modulation experts. This architectural choice introduces additional complexity in adapting to new conditions beyond simple cross-attention augmentation. Leveraging CogVideoXs layer-wise modulation [14], we propose lightweight architecture that incorporates additional conditions while preserving the models spatiotemporal relationship modeling capacity. As illustrated in Fig. 4, facial embedding xface is concatenated with text and video features (xtxt and xvid) through full-self attention. CogVideoX employs modal-specific modulation, where factors mvid and mtxt are applied to their respective features through adaptive normalization modules φ{txt, vid}. To accommodate the facial modality, we introduce dedicated adaptive normalization module φface, normalizing facial features preceding the self-attention and feed-forward network (FFN). The corresponding set of modulation factors mface is computed as: face, γ2 face, γ1 face, µ2 face, σ1 face, σ2 mface = {µ face} = φface(t, l), (5) where denotes the time embedding and represents the layer index. Let denotes in-block operations, where 1 represents attention and 2 represents the FFN. The feature transformation after operation is computed followed scale σ, shift µ, and gating γ, represented as: xn = xn1 (1 + σn) + µn, then xn = xn + γnF n(xn), with modality-specific subscripts omitted for brevity. Furthermore, to enhance the distribution learning capability of text and video latents from specific reference IDs, we introduce Conditioned Adaptive Normalization (CAN), inspired by class-conditioned DiT [17] and StyleGANs [33] approach to condition control. CAN predicts distribution shifts for video and text modalities: ˆmvid, ˆmtxt = φcond(t, l, µ1 vid, xid). (6) Here, µ1 vid acts as distribution identifier for better initialization of the CAN module, and xid from Eq. (4) represents the facial embedding prior to the fusion MLP. The final modulation factors are computed through residual addition: mvid = ˆmvid + φvid(t, l), mtxt = ˆmtxt + φtxt(t, l). 4 Figure 5. Overview of our training datasets. The pipeline includes image pre-training data (A-D) and video post-training data (D). We utilize both self-reference data (A, B) and filtered synthesized pairs with the same identity (C, D). Numbers of (images + synthesized images) are reported. utilize the SFHQ [59] dataset, which applies self-reference techniques with standard text prompts. To prevent overfitting and promote the generation of diverse face-head motion, we use the FFHQ [33] dataset as base. From this, we random sample text prompts from prompt pool of human image captions, and synthesize ID-conditioned image pairs using PhotoMaker-V2 [5], ensuring both identity similarity and diversity through careful filtering. For video post-training, we leverage the high-quality Pexels and Mixkit datasets, along with small collection of self-collected videos from the web. Similarly, synthesized image data corresponding to each face reference of keyframes are generated as references. The combined dataset offers rich visual content for training the model across images and videos. The objective function combines identity-aware and general denoising loss: = Lnoise + λ (1 cos(qface, D(x0))) , where D() represents the latent decoder for the denoised latent x0, and λ is the balance factor. Following PhotoMaker [5], we compute the denoising loss specifically within the face area for 50% of random training samples. 4. Experiments 4.1. Implementation Details Dataset preparation. As illustrated in Fig. 5, our training pipeline leverages both self-referenced and synthetically paired image data [33, 58, 59] for identity-preserving alignment in the initial training phase. For synthetic data pairs (denoted as and in Fig. 5), we employ ArcFace [57] for Figure 4. Cross-modal adapter in DiT blocks, featuring Conditioned Adaptive Normalization (CAN) for modal-specific feature modulation and decoupled attention integration. We found this conditional shift prediction φcond is suitable for an MLP implementation. Complementing the conditioned normalization, we augment the joint full self-attention TSA with cross-attention mechanism TCA [11, 52] to enhance ID modality feature aggregation. The attention output xout is computed as: xout = TSA(Wqkv(xfull)) + TCA(Wq(xfull), Wkv(xface)), (7) where TSA and TCA utilize the same query projection Wq(xfull), while the key-value projections Wkv in crossattention are re-initialized and trainable. 3.4. Data and Training Training zero-shot customization adapter presents unique data challenges compared to fine-tuning approaches, like Magic-Me [12]. Our models full-attention architecture, which integrates spatial and temporal components inseparably, necessitates two-stage training strategy. As shown in Fig. 5, we begin by training on diverse, high-quality datasets to develop robust identity preservation capabilities. Our progressive training pipeline leverages diverse datasets to enhance model performance, particularly in identity preservation. For image pre-training, we first utilize the LAION-Face [58] dataset, which contains web-scale real images and provides rich source for generating selfreference images. To further increase identity diversity, we 5 Models Dynamic Degree Text Alignment Inception Score Average ID Similarity Similarity Decay Face Motion FMref Face Motion FMinter Overall Preference DynamiCrafter [15] EasyAnimate-I2V [16] CogVideoX-I2V [14] ID-Animator [11] Magic Mirror 0.455 0.155 0.660 0.140 0.705 0.168 0.177 0.213 0.211 0.240 8.20 9.55 9.85 7.57 10.59 0.896 0.903 0.901 0.923 0. 0.002 0.022 0.029 0.005 0.002 0.237 0.262 0.413 0.652 0.704 1.388 2.875 2.930 2.111 3.040 5.402 5.935 6.404 5.693 7.315 Table 1. Quantitative comparisons. We report results with Image-to-Video and ID-preserved models. ID similarities are evaluated on the corresponding face-enhanced prompts to avoid face missing caused by complex prompts. face, qb facial recognition and detection to extract key attributes including age, bounding box coordinates, gender, and facial embeddings. Reference frames are then generated using PhotoMakerV2 [5]. We implement quality control process by filtering image pairs {a, b} based on their facial embedding cosine similarity, retaining pairs where d(qa face) > 0.65. For text conditioning, we utilize MiniGemini-8B [60] to caption all video data, to form diverse prompt pool containing 29K prompts, while CogVLM [61] provides video descriptions in the second training stage. Detailed data collection procedures are provided in Appendix A.1. Training Details. Our Magic Mirror framework extends CogVideoX-5B [14] by integrating facial-specific modal adapters into alternating DiT layers (i.e., adapters in all layers with even index l). We adopt the feature extractor Fface and ID perceiver τid from pre-trained PhotoMakerV2 [5]. In the image pre-train stage, we optimize the adapter components for 30K iterations using global batch size of 64. Subsequently, we perform video fine-tuning for 5K iterations with batch size of 8 to enhance temporal consistency in video generation. Both phases employ decayed learning rate starting from 105. All experiments were conducted on single compute node with 8 NVIDIA A800 GPUs. Evaluation and Comparisons. We evaluate our approach against the state-of-the-art ID-consistent video generation model ID-Animator [11] and leading Image-toVideo (I2V) frameworks, including DynamiCrafter [15], CogVideoX [14], and EasyAnimate [16]. Our evaluation leverages standardized VBench [18], for video generation assessment that measures motion quality and textmotion alignment. For identity preservation, we utilize facial recognition embedding similarity [62] and facial motion metrics. Our evaluation dataset consists of 40 singlecharacter prompts from VBench, ensuring demographic diversity, and 40 action-specific prompts for motion assessment. Identity references are sampled from 50 face identities from PubFig [63], generating four personalized videos per identity across varied prompts. 4.2. Quantitative Evaluation The quantitative results are summarized in Tab. 1. We evaluate generated videos using VBenchs and EvalCrafters Models Visual Quality Text Alignment Dynamic Degree ID Similarity DynamiCrafter [15] EasyAnimate-I2V [16] CogVideoX-I2V [14] ID-Animator [11] Magic Mirror 6.03 6.62 6.86 5.63 6.97 7.29 8.21 8.31 6.37 8.88 4.85 5.57 6.55 4.06 7. 5.87 6.01 6.22 6.70 6.39 Table 2. User study results. general metrics [18, 64], including dynamic degree, textprompt consistency, and Inception Score [65] for video quality assessment. For identity preservation, we introduce Average Similarity instead of similarity with the reference image, measuring the distance between generated faces and the average similarity of reference images for each identity. This prevents models from achieving artificially high scores through naive copy-paste behavior, as illustrated in Fig. 2. Face motion is quantified using two metrics: FMref (relative distance to reference face) and FMinter (inter-frame distance), computed using RetinaFace [66] landmarks after position alignment, and the L2 distance between the normalized coordinates are reported as the metric. Our method achieves superior facial similarity scores compared to I2V approaches while maintaining competitive performance to ID-Animator. We demonstrate strong text alignment, video quality, and dynamic performance, attributed to our decoupled facial feature extraction and crossmodal adapter with conditioned adaptive normalizations. Besides, we analyze facial similarity drop across uniformly sampled frames from each video to assess temporal identity consistency, report as the similarity decay term in Tab. 1. Standard I2V models (CogVideoX-I2V [14], EasyAnimate [16]) exhibit significant temporal decay in identity preservation. While DynamiCrafter [15] shows better stability due to its random reference strategy, it compromises fidelity. Both ID-Animator [11] and our Magic Mirror maintain consistent identity preservation throughout the video duration. Figure 6. Qualitative comparisons. Captions and reference identity images are presented in the top-left corner for each case. 4.3. Qualitative Evaluation Beyond the examples shown in Fig. 1, we present comparative results in Fig. 6. Our method maintains high text coherence, motion dynamics, and video quality compared to conventional CogVideoX inference. When compared to existing image-to-video approaches [5, 1416], Magic Mirror demonstrates superior identity consistency across frames while preserving natural motion. Our method also achieves enhanced dynamic range and text alignment compared to ID-Animator [11], which exhibits limitations in motion variety and prompt adherence. To complement our quantitative metrics, we conducted comprehensive user study to evaluate the perceptual quality of generated results. The study involved 173 participants Figure 7. Examples for ablation studies. Left: Ablation on modules. Right: Ablation on and training strategies. who assessed the outputs across four key aspects: motion dynamics, text-motion alignment, video quality, and identity consistency. Participants rated each aspect on scale of 1-10, with results summarized in Tab. 2. As shown in the overall preference scores in Tab. 1, Magic Mirror consistently outperforms baseline methods across all evaluated dimensions, demonstrating its superior perceptual quality in human assessment. 4.4. Ablation Studies Condition-related Modules. We evaluate our key architectural components through ablation studies, shown in the left section of Fig. 7. Without the reference feature embedding branch, the model loses crucial high-level attention guidance, significantly degrading identity fidelity. The conditioned adaptive normalization (CAN) proves vital for distribution alignment, enhancing identity preservation across frames. The effectiveness of CAN for facial condition injection is further demonstrated in Fig. 8, showing improved training convergence for identity information capture during the image pre-train stage. Training Strategy. The right section of Fig. 7 illustrates the impact of different training strategies. Image pre-training is essential for robust identity preservation, while video posttraining ensures temporal consistency. However, training exclusively on image data leads to color-shift artifacts during video inference. This artifact is caused by modulation factor inconsistencies in different training stages. Our twostage training approach achieves optimal results by leveraging the advantages of both phases, generating videos with high ID fidelity and dynamic facial motions. Figure 8. CAN speeds up the convergence. Without the Conditioned Adaptive Normalization, the model cannot fit the simplest appearance features like hairstyle in the image pre-train stage. Model Memory Parameters CogVideoX-5B [14] Magic Mirror 24.9 GiB 28.6 GiB 10.5B 12.8B Time 204s 209s Table 3. Computation overhead of Magic Mirror. 4.5. Discussions Computation Overhead. Compared with the baseline, we analyze the computational requirements in GPU memory utilization, parameter count, and inference latency for generating 49-frame 480P video. Most additional parameters are concentrated in the embedding extraction stage, which requires only single forward pass. Consequently, as shown in Tab. 3, Magic Mirror introduces minimal computational overhead in both GPU memory consumption and inference time compared to the base model. Feature Distribution Analysis. To validate our condi8 Figure 9. Different modalities scale distribution using t-SNE. Each point represents the scale with unique timestep-layer index. We also illustrate shift variant on text and videos adaptive scale using different colors. tioned adaptive normalization mechanism, we visualize the predicted modulation scale factors σ using t-SNE [67] in Fig. 9. The analysis reveals distinct distribution patterns across transformer layers and is not sensitive to timestep input. The face modality exhibits its characteristic distribution. The conditioned residual ˆσ introduces targeted distribution shifts from the baseline, which empirically improves model convergence with ID conditions. Limitations and Future Work. While Magic Mirror demonstrates strong performance in ID-consistent video generation, several challenges remain. First, the current framework lacks support for multi-identity customized generation. Second, our method primarily focuses on facial features, leaving room for improvement in preserving finegrained attributes such as clothing and accessories. Extending identity consistency to these broader visual elements represents promising direction for practical multishot customized video generation. 5. Conclusion In this work, we presented Magic Mirror, zero-shot framework for identity-preserved video generation. Magic Mirror incorporates dual facial embeddings and Conditional Adaptive Normalization (CAN) into DiT-based architectures. Our approach enables robust identity preservation and stable training convergence. Extensive experiments demonstrate that Magic Mirror generates high-quality personalized videos while maintaining identity consistency from single reference image, outperforming existing methods across multiple benchmarks and human evaluations. Acknowlegdement This work was supported in part the Areas of by the Research Grants Council under scheme grant AoE/E-601/22-R and the Excellence Shenzhen Science and Technology Program under No. KQTD20210811090149095."
        },
        {
            "title": "Appendix",
            "content": "This appendix provides comprehensive technical details and additional results for Magic Mirror, encompassing dataset preparation, architectural specifications, implementation, and extensive experimental validations. We include additional qualitative results and in-depth analyses to support our main findings. We strongly encourage readers to examine https://julianjuaner.github.io/ projects/MagicMirror/ for dynamic video demonstrations. The following contents are organized for efficient navigation."
        },
        {
            "title": "Appendix Contents",
            "content": "A. Experiment Details A.1. Training Data Preparation . . . . . . . . . A.2. Test Data Preparation . . . . . . . . . . . . . . . . . . . . . . . . . A.3. Comparisons A.4. Evaluation Metrics . . . . . . . . . . . . A.5. Implementation Details . . . . . . . . . . . . Additional Discussions B.1. Why Distribution is Important B.2. Limitation Analysis . . . . . . . . . . . . . . . . . . C. Additional Results & Applications C.1. Additional Applications . . . . . . . . . . C.2. Image Generation Results . . . . . . . . . C.3. Video Generation Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . D. Acknowledgments A. Experiment Details A.1. Training Data Preparation 9 9 10 11 12 12 12 13 13 13 13 13 13 Our training dataset is constructed through rigorous preprocessing pipeline, as illustrated in Fig. 10. We start with LAION-face [58], downloading 5 million images, which undergo strict quality filtering based on face detection confidence scores and resolution requirements. The filtered subset of 107K images is then processed through an image captioner [60], where we exclude images containing textual elements. This results in curated set of 50K high-quality face image-text pairs. To enhance identity diversity, we incorporate the synthetic SFHQ dataset [59]. To fit the model output, we standardize these images by adding black borders and pairing them with consistent prompt template: squared ID photo of ..., with pure black on two sides. This preprocessing ensures uniformity while maintaining the datasets diverse identity characteristics. For FFHQ [33], we leverage state-of-the-art identitypreserving prior PhotoMakerV2 [5] to generate synthetic images. We filter redundant identities using pairwise facial similarity metrics, with prompts sampled from our 50K video keyframe captions. We utilize the Pexels-400K and Figure 10. Detailed training data processing pipeline. Building upon Fig. 5, we illustrate comprehensive filtering criteria, prompt examples, and processing specifications. The data flow is indicated by blue arrows, while filtering rules leading to data exclusion are marked with red arrows. Mixkit datasets from [44] for image-video pairs. The videos undergo systematic preprocessing pipeline, including face detection and motion-based filtering to ensure high-quality dynamic content. We generate video descriptions using CogVLM video captioner [61]. Following our FFHQ processing strategy, we employ PhotoMakerV2 to synthesize identity-consistent images from the detected faces, followed by quality-based filtering. A.2. Test Data Preparation Face Images Preparation We construct comprehensive evaluation set for identity preservation assessment across video generation models. Our dataset comprises 50 distinct identities across seven demographic categories: man, woman, elderly man, elderly woman, boy, girl, and baby. The majority of faces are sourced from PubFig dataset [63], supplemented with public domain images for younger categories. Each identity is represented by 1-4 reference images to capture variations in pose and expression. 10 Figure 11. Impact of prompt length on image-to-video generation. We demonstrate how image-to-video models perform differently with concise versus enhanced prompts. Frames with large artifacts are marked in red. First frame images are generated from enhanced prompts. Prompt Preparation Our test prompts are derived from VBench [18], focusing on human-centric actions. For detailed descriptions, we sample from the initial 200 GPT4-enhanced prompts and select 77 single-person scenarios. Each prompt is standardized with consistent subject descriptors and augmented with the img trigger word for model compatibility. We assign four category-appropriate prompts to each identity, ensuring demographic alignment. For the baby category, which lacks representation in VBench, we craft four custom prompts to maintain evaluation consistency across all categories. A.3. Comparisons ID-Animator [11] We utilize enhanced long prompts for evaluation, although some undergo partial truncation due to CLIPs 77-token input constraint. CogVideoX-5B-I2V [14] For this image-to-video variant, we first generate reference images using PhotoMakerV2 [5] for each prompt-identity pair. These images, combined with enhanced long prompts, serve as input for video generation. [16] We EasyAnimate same PhotoMakerV2-generated reference images as in our CogVideoX-5B-I2V experiments. evaluate using the DynamiCrafter [15] Due to model-specific resolution requirements, we create dedicated set of reference images using PhotoMakerV2 that conform to the models specifications. In image-to-video baselines, through reference images generated by enhanced prompts, we deliberately use original short concise prompts for video generation. This choice stems from our empirical observation that image-to-video models exhibit strong semantic bias when processing lengthy prompts. Specifically, these models tend to prioritize text alignment over reference image fidelity, leading to degraded video quality and compromised identity preservation. This trade-off is particularly problematic for our face preservation objectives. We provide visual evidence of this phenomenon in Fig. 11. Figure 12. Face Motion (FM) calculation. FMinter follows similar computation across consecutive video frames. A.4. Evaluation Metrics Our evaluation framework combines standard video metrics with face-specific measurements. From VBench [18], we utilize Dynamic Degree for motion naturality and Overall Consistency for text-video alignment. Video quality is assessed using Inception Score from EvalCrafter [64]. For facial fidelity, we measure identity preservation using facial recognition embedding similarity [62] and temporal stability through frame-wise similarity decay. We propose novel facial dynamics metric to address the limitation of static face generation in existing methods. As shown in Fig. 12, we extract five facial landmarks using RetinaFace [66] and compute two motion scores: FMref measures facial motion relative to the reference image (computed on aspect-ratio-normalized frames to eliminate positional bias), while FMinter quantifies maximized inter-frame facial motion (computed on original frames to preserve translational movements). This dual-score approach enables comprehensive assessment of facial dynamics. A.5. Implementation Details Decoupled Facial Embeddings. Our architecture employs two complementary branches: an ID embedding branch based on pre-trained PhotoMakerV2 [5] with two-token IDembedding query qid, and facial structural embedding branch that extracts detailed features from the same ViTs penultimate layer. The latter initializes 32 token embeddings as facial query qface input. We use projection layer to align facial modalities before diffusion model input. Conditioned Adaptive Normalization. This paragraph elaborates on the design details of the Conditioned Adaptive Normalization (CAN) module, complementing the overview provided in Sec. 3.3 and Fig. 4. For predicting facial modulation factors mface, we employ two-layer MLP architecture, following the implementation structure of the Figure 13. Detailed implementation of Conditioned Adaptive Normalization. We present the expanded architecture of φcond (illustrated in the unmasked region above) with comprehensive annotations of input-output tensor dimensions at each transformation. original normalization modules φtxt, text. The detailed implementation of CAN is illustrated in Fig. 13. Given the facial ID embedding xid R2c containing two tokens, we first apply one global projection layer for dimensionality reduction, mapping it to dimension c1. Subsequently, in each adapted layer, we concatenate this projected embedding with the time embedding and the predicted shift factor µ1 vid along the channel dimension. An MLP then processes this concatenated representation to produce the final modulation factors. To ensure stable training, all newly introduced modulation predictors are initialized with zero. B. Additional Discussions B.1. Why Distribution is Important Besides the cross-modal modulation factor distribution plots presented in Fig. 9, we investigate the critical role of distribution in the convergence of the Magic Mirror network through another perspective. Our experiments reveal that modality-aware data distributions significantly impact generation quality. Using two distinct datasets of CelebVText [68] and our video data from Pexels, we fine-tuned only the normalization layers φvid, txt on the CogVideoX base model. The results demonstrate that distributionspecific fine-tuning substantially influences the spatial fidelity of the generated videos, as evidenced in Fig. 14. This finding not only underscores the importance of distribution alignment but also validates the high quality of our collected video dataset. 12 Figure 14. Modulation layers reflect data distribution. Finetuning solely the modulation layer weights demonstrates adaptation to distinct data distributions, affecting both spatial fidelity and temporal dynamics. Figure 15. Limitations of Magic Mirror. (a) Fine-grained feature preservation failure in facial details and accessories. (b) Motion artifacts in generated videos showing temporal inconsistencies. B.2. Limitation Analysis As discussed in Sec. 4.5, our approach faces several limitations, particularly in handling multi-person scenarios and preserving fine-grained features. Fig. 15 illustrates two representative failure cases: incomplete transfer of reference character details (such as accessories) and motion artifacts caused by the base model. These limitations highlight critical areas for future research in controllable personalized video generation, particularly in maintaining temporal consistency and fine detail preservation. C. Additional Results & Applications C.1. Additional Applications Fig. 16 demonstrates two extended capabilities of Magic Mirror. First, beyond realistic customized video generation, our framework effectively handles stylized prompts, leveraging CogVideoXs diverse generative capabilities to produce identity-preserved outputs across various artistic styles and visual effects. Furthermore, we show that our method can generate high-quality, temporally consistent multi-shot sequences when maintaining coherent character and style descriptions. We believe these capabilities have significant implications for automated video content creation. C.2. Image Generation Results Magic Mirror demonstrates strong capability in IDpreserved image generation with the image-pre-trained stage. Notably, it achieves even superior facial identity fidelity compared to video-finetuned variants, primarily due to optimization constraints in video training (e.g., limited batch sizes and dataset scope). Representative examples are presented in Fig. 17. C.3. Video Generation Results Additional video generation results and comparative analyses are provided in Figs. 18 and 19, highlighting our methods advantages. Fig. 18 specifically demonstrates the benefits of our one-stage approach over I2V, including superior handling of occluded initial frames, enhanced dynamic range, and improved temporal consistency during facial rotations. In Fig. 19, we provide more results with human faces on different scales. D. Acknowledgments Social Impact. Magic Mirror is designed to facilitate creative and research-oriented video content generation while preserving individual identities. We advocate for responsible use in media production and scientific research, explicitly discouraging the creation of misleading content or violation of portrait rights. As our framework builds upon the 13 Figure 16. Additional applications of Magic Mirror. We can generate identity-preserved videos across artistic styles and can generate multi-shot videos with consistent characters. More results are presented in the project page. DiT foundation model, existing diffusion-based AI-content detection methods remain applicable. Data Usage. The training data we used is almost entirely sourced from known public datasets, including all image data and most video data. All video data was downloaded and processed through proper channels (i.e., download requests). We implement strict NSFW filtering during the training process to ensure content appropriateness. Figures 17-19 are presented on the following pages Figure 17. Image generation using Magic Mirror. Model in the image pre-train stage captures ID embeddings of the reference ID (Ref-ID), yet over-fits on some low-level distributions such as image quality, style, and background. Figure 18. Advantages over I2V generation. Magic Mirror successfully handles challenging scenarios including partially occluded initial frames and maintains identity consistency through complex facial dynamics, addressing limitations of traditional I2V approaches. 15 Figure 19. Video generation results. We demonstrate Magic Mirrors capability across varying facial scales and compositions. Additional examples and comparative analyses are available in the project page."
        },
        {
            "title": "References",
            "content": "[1] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 1, 3, 4 [2] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 35:3647936494, 2022. [3] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. 3 [4] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, Wesam Manassra, Prafulla Dhariwal, Casey Chu, Yunxin Jiao, and Aditya Ramesh. Improving image generation with better captions. Technical report, OpenAI, 2023. 2 [5] Zhen Li, Mingdeng Cao, Xintao Wang, Zhongang Qi, MingMing Cheng, and Ying Shan. Photomaker: Customizing reIn CVPR, alistic human photos via stacked id embedding. pages 86408650, 2024. 2, 3, 4, 5, 6, 7, 9, 11, 12 [6] Zinan Guo, Yanze Wu, Zhuowei Chen, Lang Chen, and Qian He. Pulid: Pure and lightning id customization via contrastive alignment. NeurIPS, 2024. [7] Qixun Wang, Xu Bai, Haofan Wang, Zekui Qin, and Anthony Chen. Instantid: Zero-shot identity-preserving generation in seconds. arXiv preprint arXiv:2401.07519, 2024. 3 [8] Jiehui Huang, Xiao Dong, Wenhui Song, Hanhui Li, Jun Zhou, Yuhao Cheng, Shutao Liao, Long Chen, Yiqiang Yan, Shengcai Liao, et al. Consistentid: Portrait generation with multimodal fine-grained identity preserving. In arXiv preprint arXiv:2404.16771, 2024. [9] Li Chen, Mengyi Zhao, Yiheng Liu, Mingxu Ding, Yangyang Song, Shizun Wang, Xu Wang, Hao Yang, Jing Liu, Kang Du, et al. Photoverse: Tuning-free image customization with text-to-image diffusion models. arXiv preprint arXiv:2309.05793, 2023. [10] Xu Peng, Junwei Zhu, Boyuan Jiang, Ying Tai, Donghao Luo, Jiangning Zhang, Wei Lin, Taisong Jin, Chengjie Wang, and Rongrong Ji. Portraitbooth: versatile portrait model for fast identity-preserved personalization. In CVPR, pages 2708027090, 2024. 2, 3 [11] Xuanhua He, Quande Liu, Shengju Qian, Xin Wang, Tao Hu, Ke Cao, Keyu Yan, Man Zhou, and Jie Zhang. Id-animator: Zero-shot identity-preserving human video generation. arXiv preprint arXiv:2404.15275, 2024. 2, 3, 5, 6, 7, 11 [12] Ze Ma, Daquan Zhou, Chun-Hsiao Yeh, Xue-She Wang, Xiuyu Li, Huanrui Yang, Zhen Dong, Kurt Keutzer, and Jiashi Feng. Magic-me: Identity-specific video customized diffusion. arXiv preprint arXiv:2402.09368, 2024. 2, 3, 5 [13] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toimage diffusion models without specific tuning. ICLR, 2024. 2, 3 [14] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 3, 4, 6, 7, 8, 11 [15] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. ECCV, 2024. 3, 6, 11 [16] Jiaqi Xu, Xinyi Zou, Kunzhe Huang, Yunkuo Chen, Bo Liu, MengLi Cheng, Xing Shi, and Jun Huang. Easyanimate: high-performance long video generation method based on transformer architecture. arXiv preprint arXiv:2405.18991, 2024. 2, 3, 6, 7, 11 [17] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, pages 41954205, 2023. 2, 3, 4 [18] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, et al. Vbench: Comprehensive benchIn CVPR, pages mark suite for video generative models. 2180721818, 2024. 2, 6, 11, 12 [19] LuChen. Video ocean - filmmaking for everyone. 2 [20] Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. NeurIPS, 35:43284343, 2022. 3 [21] Shansan Gong, Mukai Li, Jiangtao Feng, Zhiyong Wu, and LingPeng Kong. Diffuseq: Sequence to sequence text generation with diffusion models. ICLR, 2022. [22] Hyungjin Chung and Jong Chul Ye. Score-based diffusion models for accelerated mri. Medical image analysis, 80:102479, 2022. 3 [23] Hyungjin Chung, Byeongsu Sim, and Jong Chul Ye. Come-closer-diffuse-faster: Accelerating conditional diffusion models for inverse problems through stochastic contraction. In CVPR, pages 1241312422, 2022. 3 [24] Gabriele Corso, Hannes Stark, Bowen Jing, Regina Barzilay, and Tommi Jaakkola. Diffdock: Diffusion steps, twists, and turns for molecular docking. ICLR, 2023. 3 [25] Bowen Jing, Gabriele Corso, Jeffrey Chang, Regina Barzilay, and Tommi Jaakkola. Torsional diffusion for molecular conformer generation. NeurIPS, 35:2424024253, 2022. 3 [26] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 34:87808794, 2021. 3 [27] Diederik Kingma, Tim Salimans, Ben Poole, and Jonathan Ho. Variational diffusion models. NeurIPS, 34:21696 21707, 2021. [28] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. Dreambooth: Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR, pages 2250022510, 2023. 3, 4 [29] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. PMLR, 2021. 3 17 [30] Zhen Xing, Qijun Feng, Haoran Chen, Qi Dai, Han Hu, Hang Xu, Zuxuan Wu, and Yu-Gang Jiang. survey on video diffusion models. ACM Computing Surveys, 2023. 3 [31] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. 3 [32] Tero Karras, Samuli Laine, Miika Aittala, Janne Hellsten, Jaakko Lehtinen, and Timo Aila. Analyzing and improving the image quality of stylegan. In CVPR, pages 81108119, 2020. [33] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In CVPR, pages 44014410, 2019. 3, 4, 5, 9 [34] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pages 1287312883, 2021. [35] Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pages 8821 8831. Pmlr, 2021. [36] Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for content-rich text-to-image generation. ICLR, 2(3):5, 2024. 3 [37] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. NeurIPS, 35:86338646, 2022. 3 [38] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, pages 234241. Springer, 2015. 3 [39] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 3 [40] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. NeurIPS, 2017. 3 [41] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In ICLR, 2021. [42] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 3 [43] OpenAI. Video generation models as world simulators. 3 [44] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, April 2024. 3, 10 [45] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, March 2024. 3 [46] Yan Zeng, Guoqiang Wei, Jiani Zheng, Jiaxin Zou, Yang Wei, Yuchen Zhang, and Hang Li. Make pixels dance: Highdynamic video generation. In CVPR, 2024. 3 [47] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In SIGGRAPH, pages 111, 2024. 3 [48] Wangbo Yu, Jinbo Xing, Li Yuan, Wenbo Hu, Xiaoyu Li, Zhipeng Huang, Xiangjun Gao, Tien-Tsin Wong, Ying Shan, and Yonghong Tian. Viewcrafter: Taming video diffusion models for high-fidelity novel view synthesis. arXiv preprint arXiv:2409.02048, 2024. [49] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel Cohen-Or. Encoding in style: stylegan encoder for image-to-image translation. In CVPR, pages 22872296, 2021. 3 [50] Xintao Wang, Yu Li, Honglun Zhang, and Ying Shan. Towards real-world blind face restoration with generative facial prior. In CVPR, pages 91689178, 2021. 3 [51] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel Cohen-Or. An image is worth one word: Personalizing text-to-image generation using textual inversion. ICLR, 2023. 3 [52] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 3, 4, 5 [53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pages 38363847, 2023. [54] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, MingChang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. [55] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PMLR, 2021. 4 [56] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. arXiv preprint arXiv:2301.12597, 2023. 4 [57] Jiankang Deng, Jia Guo, Niannan Xue, and Stefanos Zafeiriou. Arcface: Additive angular margin loss for deep face recognition. In CVPR, pages 46904699, 2019. 4, 5 [58] Christoph Schuhmann, Robert Kaczmarczyk, Aran Komatsuzaki, Aarush Katta, Richard Vencu, Romain Beaumont, Jenia Jitsev, Theo Coombes, and Clayton Mullis. Laion400m: Open dataset of clip-filtered 400 million image-text In NeurIPS Workshop Datacentric AI, number FZJpairs. 2022-00923. Julich Supercomputing Center, 2021. 5, 9 [59] David Beniaguev. Synthetic faces high quality (sfhq) dataset, 2022. 5, 9 [60] Yanwei Li, Yuechen Zhang, Chengyao Wang, Zhisheng Zhong, Yixin Chen, Ruihang Chu, Shaoteng Liu, and Jiaya Jia. Mini-gemini: Mining the potential of multi-modality 18 vision language models. arXiv preprint arXiv:2403.18814, 2024. 6, [61] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. 6, 10 [62] Adam Geitgey. face recognition, 2017. 6, 12 [63] Neeraj Kumar, Alexander Berg, Peter Belhumeur, and Shree Nayar. Attribute and simile classifiers for face verification. In ICCV, pages 365372. IEEE, 2009. 6, 10 [64] Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In CVPR, pages 22139 22149, 2024. 6, 12 [65] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. NeurIPS, 29, 2016. 6 [66] Jiankang Deng, Jia Guo, Evangelos Ververas, Irene Kotsia, and Stefanos Zafeiriou. Retinaface: Single-shot multi-level In CVPR, pages 52035212, face localisation in the wild. 2020. 6, 12 [67] Laurens Van der Maaten and Geoffrey Hinton. Visualizing data using t-sne. JMLR, 9(11), 2008. [68] Jianhui Yu, Hao Zhu, Liming Jiang, Chen Change Loy, Weidong Cai, and Wayne Wu. Celebv-text: large-scale facial text-video dataset. In CVPR, pages 1480514814, 2023."
        }
    ],
    "affiliations": [
        "CMU",
        "CUHK",
        "HKUST",
        "SmartMore"
    ]
}