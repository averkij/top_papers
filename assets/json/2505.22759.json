{
    "paper_title": "FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian",
    "authors": [
        "Sara Papi",
        "Marco Gaido",
        "Luisa Bentivogli",
        "Alessio Brutti",
        "Mauro Cettolo",
        "Roberto Gretter",
        "Marco Matassoni",
        "Mohamed Nabih",
        "Matteo Negri"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The development of speech foundation models (SFMs) like Whisper and SeamlessM4T has significantly advanced the field of speech processing. However, their closed nature--with inaccessible training data and code--poses major reproducibility and fair evaluation challenges. While other domains have made substantial progress toward open science by developing fully transparent models trained on open-source (OS) code and data, similar efforts in speech remain limited. To fill this gap, we introduce FAMA, the first family of open science SFMs for English and Italian, trained on 150k+ hours of OS speech data. Moreover, we present a new dataset containing 16k hours of cleaned and pseudo-labeled speech for both languages. Results show that FAMA achieves competitive performance compared to existing SFMs while being up to 8 times faster. All artifacts, including code, datasets, and models, are released under OS-compliant licenses, promoting openness in speech technology research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 9 5 7 2 2 . 5 0 5 2 : r : The First Large-Scale Open-Science Speech Foundation Model for English and Italian"
        },
        {
            "title": "Sara Papi",
            "content": ", Marco Gaido , Luisa Bentivogli, Alessio Brutti, Mauro Cettolo, Roberto Gretter, Marco Matassoni, Mohamed Nabih, Matteo Negri Fondazione Bruno Kessler (FBK), Italy {spapi,mgaido,bentivo,brutti,cettolo gretter,matasso,mnabih,negri}@fbk.eu denotes equal contribution FAMA-medium (878M): https://hf.co/FBK-MT/fama-medium FAMA-small (479M): https://hf.co/FBK-MT/fama-small FAMA Data: https://hf.co/datasets/FBK-MT/fama-data FAMA Code: https://github.com/hlt-mt/FBK-fairseq"
        },
        {
            "title": "Abstract",
            "content": "The development of speech foundation models (SFMs) like Whisper and SeamlessM4T has significantly advanced the field of speech processing. However, their closed naturewith inaccessible training data and codeposes major reproducibility and fair evaluation challenges. While other domains have made substantial progress toward open science by developing fully transparent models trained on open-source (OS) code and data, similar efforts in speech processing remain limited. To fill this gap, we introduce FAMA, the first family of open science SFMs for English and Italian, trained on 150k+ hours of OS speech data. Moreover, we present new dataset containing 16k hours of cleaned and pseudolabeled speech for both languages. Results show that FAMA achieves competitive performance compared to existing SFMs while being up to 8 times faster. All artifacts, including codebase, datasets, and models, are released under OS-compliant licenses, promoting openness in speech technology research. 1 FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian"
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 The FAMA Framework 2.1 Training and Evaluation Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.2 Model Architecture . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Training and Evaluation Procedures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.4 Terms of Comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Results 3.1 Pre-training and Catastrophic Forgetting . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 Comparison with Existing SFMs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.3 Computational Time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Conclusions 2 3 4 4 5 5 6 6 6 7 8 9 FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian"
        },
        {
            "title": "1 Introduction",
            "content": "The development of speech foundation models (SFMs) has significantly advanced speech processing in the last few years, particularly in areas such as automatic speech recognition (ASR) and speech translation (ST). Popular SFMs such as OpenAI Whisper (Radford et al., 2023) and Meta SeamlessM4T (Barrault et al., 2023) have been released to the public in various sizes and with extensive language coverage. However, these models completely lack comprehensive accessibility to their training codebases and datasets, hindering their reproducibility and raising concerns about potential data contamination (Dong et al., 2024), thereby complicating fair evaluation. In other domains, multiple efforts towards building models that are more accessible, reproducible, and free from proprietary constraints have been made (BigScience Workshop et al., 2022; Biderman et al., 2023; Liu et al., 2024; Sun et al., 2024; Deitke et al., 2024; Dai et al., 2024; Martins et al., 2025). For instance, the OLMO project (Groeneveld et al., 2024) has demonstrated the feasibility of training large language models (LLMs) using only open-source (OS) data (Soldaini et al., 2024), realizing an open-science1 system (White et al., 2024) for text processing. However, such comprehensive approaches are still lacking in the field of speech processing. Recent works towards this direction are represented by OWSM (Peng et al., 2023) and its subsequent versions (Peng et al., 2024). OWSM, whose model weights and the codebase used for the training are released open source, reproduces Whisper-style training using publicly available data. Despite representing valuable initiative toward building an open-science system, there is still step missing for creating the first SFM of this kind: leveraging only data that is not only publicly available but also released under an OS-compliant license (Gaido et al., 2024a). Such effort would allow users complete access and control over the data used at every stage of the scientific process, promoting reproducibility (Belz et al., 2023), fair evaluation (Balloccu et al., 2024), and the ability to build upon prior research without any barriers (Chesbrough, 2015). Besides transparency and collaboration, these efforts also foster users trust by ensuring that data is not leveraged to build tools that can be used under conditions/purposes (e.g., commercial) for which the data was not intended (White et al., 2024). To fill this gap, we create FAMA,2 the first family of large-scale open-science SFMs for English and Italian trained on over 150k hours of exclusively OS-compliant speech data. We leverage both already available OS datasets and create new collection of ASR and ST psuedolabels for Italian and English comprising more than 16k hours of OS-compliant speech, along with automatically generated Italian and English translations for an additional 130k+ hours of speech. We also detail training and evaluation procedures and provide full access to training data to have complete control of the model creation and avoid data contamination issues. FAMA models achieve remarkable results, with up to 4.2 WER and 0.152 COMET improvement on average across languages compared to OWSM and remaining competitive in terms of ASR performance with the Whisper model family while being up to 8 times faster. All the artifacts used for realizing FAMA models, including codebase, datasets, and models themself, are released under OS-compliant licenses, promoting more responsible creation of models in our community. Our approach would not only facilitate fair evaluation and comparison of SFMs but also encourage broader participation in speech technology development, leading to more inclusive and diverse applications. 1Open science involves ensuring transparency and accessibility at all stages of the scientific process (Vicente-Saez and Martinez-Fuentes, 2018), including publishing OS research papers, data, code, and any information needed to replicate the research. 2Fama (from the Latin fari meaning to speak) is the personification of the public voice in Roman mythology. FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian"
        },
        {
            "title": "2.1 Training and Evaluation Data",
            "content": "In compliance with the open-science ideology, we train and test our models only on OS-compliant data. The training set comprises both already publicly available OS datasets, presented in Table 1, and new pseudolabels created for this work. Dataset CommonVoice v18 (Ardila et al., 2020) CoVoST2 (Wang et al., 2021b) FLEURS (Conneau et al., 2023) LibriSpeech (Panayotov et al., 2015) MOSEL (Gaido et al., 2024a) MLS (Pratap et al., 2020) VoxPopuli-ASR (Wang et al., 2021a) Total #hours en 1746 420 7 358 66,301 44,600 519 113,951 it 250 28 9 - 21,775 247 74 22,383 Label G G+A Table 1: List of the publicly available training speech data for English (en) and Italian (it). stands for gold labels while for automatically generated labels. To create the new pseudolabels, we leveraged the speech content of YouTube-Commons,3 dataset collecting YouTube videos released with the permissive CC-BY 4.0 license. The videos are automatically converted into wav files with one channel and sampling rate of 16k Hz. Then, the audio is cleaned from music and non-speech phenomena and segmented using silero (Team, 2024), lightweight VAD having low computational requirements. Lastly, the audio is split using SHAS (Tsiamas et al., 2022) to obtain segments suitable for training of around 16 seconds on average. The resulting dataset contains automatic transcripts, which we created with Whisper large-v3,4 for 14,200k hours of speech for English (en) and 1,828k for Italian (it). Including data in Table 1, the final ASR training set comprises 128,152 hours of en speech and 24,211 hours of it speech, with total of 152,363 hours of speech data, including 48,259 gold-labeled hours. Being composed of speech-transcript pairs, the data mentioned so far is suitable for ASR. For ST, instead, only CoVoST2 and FLEURS contain translations from and into en and it. For this reason, we automatically translated the transcripts of all the speech data (including the original CoVoST2) with MADLAD-400 3B-MT (Kudugunta et al., 2023).5 Following (Gaido et al., 2022; Alam and Anastasopoulos, 2024), we additionally filter out samples based on the ratio between the source and target text lengths (in characters) for each language pair based on their distribution (rmin = 0.75, rmax = 1.45 for en-it, and rmin = 0.65, rmax = 1.35 for it-en), resulting into 1.24% of data filtering for en-it and 3.08% for it-en. The final training set comprises the automatically translated speech data and the gold CoVoST2 and FLEURS datasets, resulting in 149,564 hours for en-it and 24,211 hours for it-en. For training, validation, and testing, we use gold-labeled benchmarks. ASR evaluation is conducted on CommonVoice, MLS, and VoxPopuli, with CommonVoice also serving as the validation set for both en and it. For translation, we use CoVoST2 for it-en and FLEURS dev and test sets for en-it. 3https://hf.co/datasets/PleIAs/YouTube-Commons 4https://hf.co/openai/whisper-large-v3 5https://hf.co/google/madlad400-3b-mt FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian"
        },
        {
            "title": "2.2 Model Architecture",
            "content": "FAMA models are two-sized encoder-decoder architectures, small and medium. Both models are composed of Conformer encoder (Gulati et al., 2020) and Transformer decoder (Vaswani et al., 2017). FAMA small has 12 encoder layers and 6 decoder layers, while FAMA medium has 24 encoder layers and 12 decoder layers. Our decision to use an encoder twice as deep as the decoderunlike Whisper and OWSM, which have an equal number of encoder and decoder layersis driven by two key motivations: i) since autoregressive models perform multiple decoder passes during output generation, shallower decoder speeds up inference by making each pass faster, and ii) since many approaches integrate SFMs with LLMs by leveraging the encoder (Gaido et al., 2024b), deeper encoder helps preserve more of the SFMs processing capabilities in such integrations. Each layer has 16 attention heads, an embedding dimension of 1,024, and an FFN dimension of 4,096. The Conformer encoder is preceded by two 1D convolutional layers with stride of 2 and kernel size of 5. The kernel size of the Conformer convolutional module is 31 for both the pointand depth-wise convolutions. The vocabulary is built using SentencePiece unigram model (Kudo and Richardson, 2018) with size 16,000 trained on en and it transcripts. Two extra tokens<lang:en> and <lang:it>are added to indicate whether the target text is in en or it. The input audio is represented by 80 Mel-filterbank features extracted every 10 ms with window of 25 ms."
        },
        {
            "title": "2.3 Training and Evaluation Procedures",
            "content": "We train both models using combination of three losses. First, label-smoothed cross-entropy loss (LCE) is applied to the decoder output, using the target text as the reference (transcripts for ASR and translations for ST). Second, CTC loss (Graves et al., 2006) is computed using transcripts as reference (LCTCsrc) on the output of the 8th encoder layer for small and the 16th for medium. Third, CTC loss on the final encoder output (LCTCtgt) is applied to predict the target text. The final loss is the weighted sum of the above-mentioned losses: = λ1LCE + λ2LCTCsrc + λ3LCTCtgt where λ1, λ2, λ3 = 5.0, 1.0, 2.0, and the label smoothing factor of the CE is 0.1. FAMA models are trained using two-stage approach, where the model is pre-trained first on ASR data only (ASR pre-training) and then trained on both ASR and ST data (ASR+ST training). Both training stages lasted 1M steps, corresponding to 6 epochs over the training data. For the ASR pre-training, the learning rate (lrS1) scheduler adopted to train the small model is the Noam scheduler (Vaswani et al., 2017) with peak of 2e-3 and 25,000 warm-up steps. To cope with convergence issues similar to (Peng et al., 2024), for the medium model we adopted piece-wise warm-up on the Noam scheduler, with the learning rate first increasing linearly to 2e-5 for 25k steps and then to 2e-4 for an additional 25k steps, followed by the standard inverse square root function. For the ASR+ST training, we sample the ASR target with probability pASR=0.5 and use the ST target otherwise. Training settings are the same as for ASR pre-training, except for the learning rate that is set to constant value lrS2=1e-4. Experiments on how pASR and lrS2 are determined for the small model are discussed in Section 3.1. For the medium model, similarly to the first stage, the lrS2 is scaled down by one order of magnitude compared to the small model, i.e., constant value lrS2=1e-5 is used. The optimizer is AdamW with momentum β1, β2 = 0.9, 0.98, weight decay of 0.001, dropout of 0.1, and clip normalization of 10.0. We apply SpecAugment (Park et al., 2019) during both ASR pre-training and ASR+ST training. We use mini-batches of 10,000 tokens for FAMA small and 4,500 for FAMA medium with an update frequency of, respectively, 2 and 6 on 16 NVIDIA A100 GPUs (64GB FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian 6 RAM), save checkpoints every 1,000 steps and average the last 25 checkpoints to obtain the final model. The inference is performed using single NVIDIA A100 GPU with batch size of 80,000 tokens. We use beam search with beam 5, unknown penalty of 10,000, and no-repeat n-gram size of 5. Additionally, we report the results using the joint CTC rescoring (Yan et al., 2023), leveraging the CTC on the encoder output with weight 0.2. Both training and inference are done using the bug-free Conformer implementation (Papi et al., 2024) available in FBK-fairseq,6 which is built upon fairseq-S2T (Wang et al., 2020). ASR performance is evaluated with word error rate (WER) using the jiWER library7 with the text normalized using Whisper normalizer8. ST performance is evaluated using COMET (Rei et al., 2020) version 2.2.4, with the default Unbabel/wmt22-comet-da model."
        },
        {
            "title": "2.4 Terms of Comparison",
            "content": "As first term of comparison, we use Whisper (Radford et al., 2023) in both medium9 and large-v3 configurations as the first is comparable with FAMA medium in terms of size and the secondtrained on more than 4M hoursis the best performing model of the Whisper family. The comparison is made for en and it ASR and it-en ST, as Whisper does not cover the en-to-many translation directions. Whisper models are released under Apache 2.0 license and, therefore, open weights. For both ASR and ST, we also compare with SeamlessM4T medium10 and v2-large11 covering ASR and both ST language directions (Barrault et al., 2023). The model is non-commercial and, therefore, not open. We also compare with OWSM v3.1 medium12, the best performing model of the OWSM family, also covering both ASR and ST language directions and released open source (Peng et al., 2024). To ensure fair comparison, we perform the inference with HuggingFace transformers13 version 4.48.1 using the standard settings and beam search with beam 5, except for OWSM, which is not supported on HuggingFace, and for which the original ESPNet14 inference code is used with beam size of 3."
        },
        {
            "title": "3.1 Pre-training and Catastrophic Forgetting",
            "content": "Catastrophic forgetting is well-known problem in machine learning (McCloskey and Cohen, 1989) that arises when system is trained sequentially on multiple languages or tasks, leading to degradation in performance on original domains or languages (Kar et al., 2022). As we follow two-stage approach, which is commonly employed in SFMs training (Radford et al., 2023), we analyze the conditions in which this phenomenon arises during the ASR+ST training. Figure 1 shows the perplexity (ppl) behavior during the first 100/500k steps of the FAMA small model training on the validation sets. We present the results of different systems obtained by varying both the learning rate lrS2 and the sampling probability pASR discussed in Section 2.3. Lower values of 6https://github.com/hlt-mt/FBK-fairseq 7https://pypi.org/project/jiwer/ 8https://pypi.org/project/whisper-normalizer/ 9https://hf.co/openai/whisper-medium 10https://hf.co/facebook/hf-seamless-m4t-medium 11https://hf.co/facebook/seamless-m4t-v2-large 12https://hf.co/espnet/owsm_v3.1_ebf 13https://pypi.org/project/transformers/ 14https://github.com/espnet/espnet/tree/master/egs2/owsm_v3.1/s2t1 15We attempted to use beam size of 5 but the model had out-of-memory issues even when reducing the batch size. FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian 7 1. 1.7 1.6 1.5 50 100 150 200 250 300 350 400 450 steps (k) (a) ASR 9 8 p 7 6 5 50 100 150 200 250 300 350 400 450 500 steps (k) (b) ST lrS2=1e-3, pASR=0.2 lrS2=1e-3, pASR=0.5 lrS2=1e-4, pASR=0.2 lrS2=1e-4, pASR=0.5 Figure 1: Average ASR and ST perplexity (ppl) on both English and Italian up to 500k steps of the training. Due to the evident worse results achieved by using lr of 1e-3, we stopped the training curves after 100k steps. The black dashed line is the ppl of the ASR model from which the training is started. lrS2 (e.g., 1e-5) lead to worse performance and are not included in the results. Since the computational budget for our experiments is limited, we analyze two cases for the sampling probability: 1) pASR=0.5 to obtain system equally trained on both ASR and ST tasks, and 2) pASR=0.2 to obtain system trained more on the unseen task during pre-training i.e., the ST task. As we can see from the curves, lrS2 of 1e-3 seems to be too high for maintaining good ASR performance while learning new task (ST). Both in the case in which the ST training is more boosted (pASR=0.2) and in the case in which ASR and ST training is balanced (pASR=0.5), we notice significant increase in the ASR ppl of up to 0.25 that corresponds to drop in performance of 3-4 WER on both languages which, moreover, it is not recovered later on in the training. Therefore, to avoid catastrophic forgetting arising just in the first steps, we exclude lrS2=1e-3 and use 1e-4 for the two-stage training. Regarding the ASR sampling, we look at the behavior of the curves for 500k steps (half of the secondstage training) and notice that the ASR ppl curve with pASR=0.5 slowly approaches the original model ppl value while the one with pASR=0.2, despite improving, is not able to approach the original ppl value. This is counterbalanced by lower (hence, better) ppl of the pASR=0.2 curve on ST compared to that of the pASR=0.5 curve. However, this difference, which is about 0.2 ppl, is not reflected in the ST Instead, the difference in performance, which only improves by 0.005 COMET points on average. terms of WER is significant, with quality drop of 0.8 WER across en and it. As result, we conclude that we avoid catastrophic forgetting in the two-stage training only by evenly sampling the ASR and ST tasks during the second step."
        },
        {
            "title": "3.2 Comparison with Existing SFMs",
            "content": "In Table 2, we show the results for both ASR and ST of our FAMA models and SFMs presented in Section 2.4. For FAMA models, we provide the scores of the ASR-only model (FAMA-ASR), obtained after pre-training, and of the final ASR+ST model, as well as the results obtained through joint CTC rescoring. FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian 8 Model #params CV MLS VP AVG ASR (WER ) Whisper medium Whisper large-v3 OWSM v3.1 medium SeamlessM4T medium SeamlessM4T v2-large FAMA-ASR small + joint CTC rescoring FAMA-ASR medium + joint CTC rescoring FAMA small + joint CTC rescoring FAMA medium + joint CTC rescoring 769M 1550M 1020M 1200M 2300M 475M 878M 475M 878M en 14.5 11.2 11.9 10.7 7.7 13.8 13.9 11.7 11.7 13.7 13.6 11.5 11. it 10.4 6.5 12.5 7.8 5.0 8.9 8.9 7.1 7.0 8.6 8.5 7.0 7.7 en 14.2 5.0 6.6 8.8 6.4 5.8 5.8 5.1 5.1 5.8 5.8 5.2 5.2 it 15.9 8.8 19.3 11.3 8.5 12.6 12.4 12.2 12.2 12.8 12.8 13.9 13.5 en 8.1 7.1 8.4 10.2 6.9 7.2 7.0 7.0 7.0 7.3 7.2 7.2 7.1 it 26.8 18.8 24.0 18.2 16.6 15.7 14.6 15.9 14.6 15.6 14.8 15.9 14.9 en 12.3 7.8 9.0 9.9 7.0 8.9 8.9 7.9 7.9 8.9 8.9 8.0 7. it 17.7 11.4 18.6 12.4 10.0 12.4 12.0 11.7 11.3 12.3 12.0 12.3 12.0 ST (COMET ) FLRS CVST2 en-it it-en - 0.801 - 0.825 0.337 0.636 0.820 0.831 0.855 0.852 - - - - - - - - 0.807 0.774 0.804 0.777 0.821 0.787 0.818 0.791 Table 2: ASR and ST performance of FAMA models and existing SFMs as terms of comparison. The results are reported on CommonVoice (CV), Multilingual LibriSpeech (MLS), and VoxPopuli (VP) for ASR, and on CoVoST (CVST2), and FLEURS (FLRS) for ST. Best values are in bold. Looking at the results of FAMA-ASR, we observe that the medium model outperforms the small one, with 0.8 WER improvements on average both with and without the joint CTC rescoring. Compared to Whisper medium, FAMA achieves better results with FAMA medium outperforming Whisper by 4.4 WER on en and 6.4 on it while having similar number of model parameters. Remarkable performance is achieved by FAMA medium also compared to OWSM v3.1 medium, with improvements of up to 1.1 WER on en and 7.3 on it, but also compared to Whisper large-v3, where similar WER scores are achieved. Instead, SeamlessM4T models, leveraging large pretrained models such as wav2vecBERT 2.0 (which is trained on 4.5 million hours) and NLLB (which is trained on more than 43 billion sentences), still outperform FAMA, with the v2-large scoring an incredibly low WER on CommonVoice also compared to strong competitor as Whisper large-v3. Looking at the ASR results of the final FAMA models, we observe that the WER remained almost unaltered compared to the ASR-only model, as already discussed in Section 3.1. Regarding ST results, we notice that FAMA models outperform OWSM v3.1 medium, with an improvement of up to 0.141 COMET by FAMA small and 0.152 by FAMA medium while still struggling to achieve the performance of Whisper and SeamlessM4T. These mixed outcomescompetitive ASR performance even against larger non-open models but lower ST performancedemonstrate both the feasibility of building high-quality open-science SFMs and the need for initiatives dedicated to creating OS-compliant ST datasets with human references to bridge the gap with non-open models."
        },
        {
            "title": "3.3 Computational Time",
            "content": "As an additional comparison, we evaluate the throughput of the SFMs on single NVIDIA A40 40GB. The throughput, measured in xRTF (the inverse of the real-time factor),16 is calculated as the number of seconds of processed audio divided by the compute time in seconds. The test set used for this performance evaluation is CommonVoice on both en and it with total duration of, respectively, 26.9 and 26.4 hours. For each model, we report the maximum batch size possible spanning in the range 2, 4, 8, and 16, as higher values resulted in out-of-memory issues with all models. The results are 16https://github.com/NVIDIA/DeepLearningExamples/blob/master/Kaldi/SpeechRecognition/ README.md#metrics FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian 9 Model Whisper medium Whisper large-v3 SeamlessM4T medium SeamlessM4T v2-large FAMA small FAMA medium Batch Size 8 4 2 2 16 8 en 13.3 7.9 28.5 13.7 57.4 39.5 xRTF () it 10.9 6.5 26.2 13.3 56.0 41.2 AVG 12.1 7.2 27.4 13.5 56.7 40.4 Table 3: Computational time and maximum batch size for Whisper, SeamlessM4T, and FAMA models. Best values are in bold. reported in Table 3. We notice that Whisper models are the slowest ones, with an average xRTF of 12.1 for medium and 7.2 for large-v3, making them 3-6 times slower than FAMA medium and 5-8 than FAMA small. These results can be attributed to the architectural design of Whisper models that apply an 2 audio subsampling compared to the commonly used 4 (as in FAMA) and introduce lot of padding in shorter sequences to achieve the fixed 30-second length. The Seamless models, despite having no extra padding (as FAMA) and greater audio subsampling of 8, are 2 times faster than Whisper ones but still 1.5-3 times slower for, respectively, medium and v2-large, compared to FAMA medium and 2-4 compared to FAMA small, making the FAMA model family the fastest by large margin."
        },
        {
            "title": "4 Conclusions",
            "content": "In this paper, we addressed the challenges posed by the closed nature of existing SFMs, such as limited accessibility to training data and codebases, by introducing FAMA, the first large-scale open-science SFM for English and Italian. Trained on over 150k hours of exclusively OS speech, FAMA ensures full transparency, with all artifacts released under OS-compliant licenses. Additionally, we contributed new collection of ASR and ST pseudolabels for about 16k hours of speech data, and more than 130k hours of English and Italian automatic translations. Results show that FAMA models outperform OWSM on both ASR and ST and also achieve comparable ASR results to Whisper while being up to 8 times faster. By providing the community with fully accessible resources, FAMA bridges the gap between advances in speech technology and open science principles, enabling fair evaluation, broader participation, and inclusivity. Future work will focus on extending FAMA to additional languages with the ultimate goal of further expanding the open science ecosystem to speech technologies."
        },
        {
            "title": "Acknowledgments",
            "content": "This paper has received funding from the PNRR project FAIR - Future AI Research (PE00000013), under the NRRP MUR program funded by the NextGenerationEU, and from the European Unions Horizon research and innovation programme under grant agreement No 101135798, project Meetween (My Personal AI Mediator for Virtual MEETings BetWEEN People). We acknowledge CINECA for the availability of high-performance computing resources and support. FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian"
        },
        {
            "title": "References",
            "content": "Md Mahfuz Ibn Alam and Antonios Anastasopoulos. 2024. case study on filtering for end-to-end speech translation. arXiv preprint arXiv:2402.01945. R. Ardila, M. Branson, K. Davis, M. Henretty, M. Kohler, J. Meyer, R. Morais, L. Saunders, F. M. Tyers, and G. Weber. 2020. Common voice: massively-multilingual speech corpus. In Proceedings of the 12th Conference on Language Resources and Evaluation (LREC 2020), pages 42114215. Simone Balloccu, Patrícia Schmidtová, Mateusz Lango, and Ondrej Dusek. 2024. Leak, cheat, repeat: Data contamination and evaluation malpractices in closed-source LLMs. In Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 6793, St. Julians, Malta. Association for Computational Linguistics. Loïc Barrault, Yu-An Chung, Mariano Cora Meglioli, David Dale, Ning Dong, Paul-Ambroise Duquenne, Hady Elsahar, Hongyu Gong, Kevin Heffernan, John Hoffman, et al. 2023. Seamlessm4t: Massively multilingual & multimodal machine translation. arXiv preprint arXiv:2308.11596. Anya Belz, Craig Thomson, Ehud Reiter, and Simon Mille. 2023. Non-repeatable experiments and non-reproducible results: The reproducibility crisis in human evaluation in NLP. In Findings of the Association for Computational Linguistics: ACL 2023, pages 36763687, Toronto, Canada. Association for Computational Linguistics. Stella Biderman, Hailey Schoelkopf, Quentin Anthony, Herbie Bradley, Kyle OBrien, Eric Hallahan, Mohammad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, Aviya Skowron, Lintang Sutawika, and Oskar Van Der Wal. 2023. Pythia: suite for analyzing large language models across training and scaling. In Proceedings of the 40th International Conference on Machine Learning, ICML23. JMLR.org. BigScience Workshop, Teven Le Scao, Angela Fan, Christopher Akiki, Ellie Pavlick, Suzana Ilic, Daniel Hesslow, Roman Castagné, Alexandra Sasha Luccioni, François Yvon, et al. 2022. Bloom: 176bparameter open-access multilingual language model. arXiv preprint arXiv:2211.05100. Henry Chesbrough. 2015. From open science to open innovation. Institute for Innovation and Knowledge Management, ESADE. Alexis Conneau, Min Ma, Simran Khanuja, Yu Zhang, Vera Axelrod, Siddharth Dalmia, Jason Riesa, Clara Rivera, and Ankur Bapna. 2023. Fleurs: Few-shot learning evaluation of universal representations of speech. In 2022 IEEE Spoken Language Technology Workshop (SLT), pages 798805. Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuolin Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. 2024. Nvlm: Open frontier-class multimodal llms. arXiv preprint arXiv:2409.11402. Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. 2024. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. arXiv preprint arXiv:2409.17146. Yihong Dong, Xue Jiang, Huanyu Liu, Zhi Jin, Bin Gu, Mengfei Yang, and Ge Li. 2024. Generalization or memorization: Data contamination and trustworthy evaluation for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1203912050, Bangkok, Thailand. Association for Computational Linguistics. FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian 11 Marco Gaido, Sara Papi, Luisa Bentivogli, Alessio Brutti, Mauro Cettolo, Roberto Gretter, Marco Matassoni, Mohamed Nabih, and Matteo Negri. 2024a. MOSEL: 950,000 hours of speech data for opensource speech foundation model training on EU languages. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1393413947, Miami, Florida, USA. Association for Computational Linguistics. Marco Gaido, Sara Papi, Dennis Fucci, Giuseppe Fiameni, Matteo Negri, and Marco Turchi. 2022. Efficient yet competitive speech translation: FBK@IWSLT2022. In Proceedings of the 19th International Conference on Spoken Language Translation (IWSLT 2022), pages 177189, Dublin, Ireland (in-person and online). Association for Computational Linguistics. Marco Gaido, Sara Papi, Matteo Negri, and Luisa Bentivogli. 2024b. Speech translation with speech foundation models and large language models: What is there and what is missing? In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1476014778, Bangkok, Thailand. Association for Computational Linguistics. Alex Graves, Santiago Fernández, Faustino Gomez, and Jürgen Schmidhuber. 2006. Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks. In Proceedings of the 23rd International Conference on Machine Learning, ICML 06, page 369376, New York, NY, USA. Dirk Groeneveld, Iz Beltagy, Evan Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, Shane Arora, David Atkinson, Russell Authur, Khyathi Chandu, Arman Cohan, Jennifer Dumas, Yanai Elazar, Yuling Gu, Jack Hessel, Tushar Khot, William Merrill, Jacob Morrison, Niklas Muennighoff, Aakanksha Naik, Crystal Nam, Matthew Peters, Valentina Pyatkin, Abhilasha Ravichander, Dustin Schwenk, Saurabh Shah, William Smith, Emma Strubell, Nishant Subramani, Mitchell Wortsman, Pradeep Dasigi, Nathan Lambert, Kyle Richardson, Luke Zettlemoyer, Jesse Dodge, Kyle Lo, Luca Soldaini, Noah Smith, and Hannaneh Hajishirzi. 2024. OLMo: Accelerating the science of language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1578915809, Bangkok, Thailand. Association for Computational Linguistics. Anmol Gulati, James Qin, Chung-Cheng Chiu, Niki Parmar, Yu Zhang, Jiahui Yu, Wei Han, Shibo Wang, Zhengdong Zhang, Yonghui Wu, and Ruoming Pang. 2020. Conformer: Convolution-augmented Transformer for Speech Recognition. In Proc. Interspeech 2020, pages 50365040. Sudipta Kar, Giuseppe Castellucci, Simone Filice, Shervin Malmasi, and Oleg Rokhlenko. 2022. Preventing catastrophic forgetting in continual learning of new natural language tasks. In Proceedings of the 28th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 31373145. Taku Kudo and John Richardson. 2018. SentencePiece: simple and language independent subword tokenizer and detokenizer for neural text processing. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 6671, Brussels, Belgium. Association for Computational Linguistics. Sneha Kudugunta, Isaac Caswell, Biao Zhang, Xavier Garcia, Derrick Xin, Aditya Kusupati, Romi Stella, Ankur Bapna, and Orhan Firat. 2023. Madlad-400: multilingual and document-level large audited In Proceedings of the 37th International Conference on Neural Information Processing dataset. Systems, NIPS 23, Red Hook, NY, USA. Curran Associates Inc. FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian 12 Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, et al. 2024. Llm360: Towards fully transparent open-source llms. In First Conference on Language Modeling. Pedro Henrique Martins, Patrick Fernandes, João Alves, Nuno M. Guerreiro, Ricardo Rei, Duarte M. Alves, José Pombal, Amin Farajian, Manuel Faysse, Mateusz Klimaszewski, Pierre Colombo, Barry Haddow, José G.C. de Souza, Alexandra Birch, and André F.T. Martins. 2025. Eurollm: Multilingual language models for europe. Procedia Computer Science, 255:5362. Proceedings of the Second EuroHPC user day. Michael McCloskey and Neal J. Cohen. 1989. Catastrophic interference in connectionist networks: The sequential learning problem. volume 24 of Psychology of Learning and Motivation. Vassil Panayotov, Guoguo Chen, Daniel Povey, and Sanjeev Khudanpur. 2015. Librispeech: An ASR corpus based on public domain audio books. In 2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 52065210. Sara Papi, Marco Gaido, Andrea Pilzer, and Matteo Negri. 2024. When good and reproducible results are giant with feet of clay: The importance of software quality in NLP. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 36573672, Bangkok, Thailand. Association for Computational Linguistics. Daniel S. Park, William Chan, Yu Zhang, Chung-Cheng Chiu, Barret Zoph, Ekin D. Cubuk, and Quoc V. Le. 2019. SpecAugment: Simple Data Augmentation Method for Automatic Speech Recognition. In Proc. Interspeech 2019, pages 26132617. Yifan Peng, Jinchuan Tian, William Chen, Siddhant Arora, Brian Yan, Yui Sudo, Muhammad Shakeel, Kwanghee Choi, Jiatong Shi, Xuankai Chang, Jee weon Jung, and Shinji Watanabe. 2024. Owsm v3.1: Better and faster open whisper-style speech models based on e-branchformer. In Interspeech 2024, pages 352356. Yifan Peng, Jinchuan Tian, Brian Yan, Dan Berrebbi, Xuankai Chang, Xinjian Li, Jiatong Shi, Siddhant Arora, William Chen, Roshan Sharma, Wangyou Zhang, Yui Sudo, Muhammad Shakeel, Jee-Weon Jung, Soumi Maiti, and Shinji Watanabe. 2023. Reproducing whisper-style training using an opensource toolkit and publicly available data. In 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), pages 18. Vineel Pratap, Qiantong Xu, Anuroop Sriram, Gabriel Synnaeve, and Ronan Collobert. 2020. MLS: Large-Scale Multilingual Dataset for Speech Research. In Proc. Interspeech 2020, pages 2757 2761. Alec Radford, Jong Wook Kim, Tao Xu, Greg Brockman, Christine McLeavey, and Ilya Sutskever. 2023. Robust speech recognition via large-scale weak supervision. In International Conference on Machine Learning, pages 2849228518. PMLR. Ricardo Rei, Craig Stewart, Ana Farinha, and Alon Lavie. 2020. COMET: neural framework for MT evaluation. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 26852702, Online. Association for Computational Linguistics. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, Valentin Hofmann, Ananya Jha, Sachin Kumar, Li Lucy, Xinxi Lyu, Nathan Lambert, Ian Magnusson, Jacob Morrison, Niklas Muennighoff, FAMA: The First Large-Scale Open-Science Speech Foundation Model for English and Italian Aakanksha Naik, Crystal Nam, Matthew Peters, Abhilasha Ravichander, Kyle Richardson, Zejiang Shen, Emma Strubell, Nishant Subramani, Oyvind Tafjord, Evan Walsh, Luke Zettlemoyer, Noah Smith, Hannaneh Hajishirzi, Iz Beltagy, Dirk Groeneveld, Jesse Dodge, and Kyle Lo. 2024. Dolma: an open corpus of three trillion tokens for language model pretraining research. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1572515788, Bangkok, Thailand. Association for Computational Linguistics. Qiang Sun, Yuanyi Luo, Sirui Li, Wenxiao Zhang, and Wei Liu. 2024. OpenOmni: collaborative open source tool for building future-ready multimodal conversational agents. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 4652, Miami, Florida, USA. Association for Computational Linguistics. Silero Team. 2024. Silero vad: pre-trained enterprise-grade voice activity detector (vad), number detector and language classifier. https://github.com/snakers4/silero-vad. Ioannis Tsiamas, Gerard I. Gállego, José A. R. Fonollosa, and Marta R. Costa-jussà. 2022. Shas: In Interspeech 2022, pages Approaching optimal segmentation for end-to-end speech translation. 106110. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz In Advances in Neural Information Kaiser, and Illia Polosukhin. 2017. Attention is all you need. Processing Systems, volume 30. Ruben Vicente-Saez and Clara Martinez-Fuentes. 2018. Open science now: systematic literature review for an integrated definition. Journal of Business Research, 88:428436. Changhan Wang, Morgane Riviere, Ann Lee, Anne Wu, Chaitanya Talnikar, Daniel Haziza, Mary Williamson, Juan Pino, and Emmanuel Dupoux. 2021a. VoxPopuli: large-scale multilingual speech In Proceedings of corpus for representation learning, semi-supervised learning and interpretation. the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 9931003, Online. Association for Computational Linguistics. Changhan Wang, Yun Tang, Xutai Ma, Anne Wu, Dmytro Okhonko, and Juan Pino. 2020. fairseq S2T: Fast speech-to-text modeling with fairseq. In Proceedings of the 2020 Conference of the Asian Chapter of the Association for Computational Linguistics (AACL): System Demonstrations. Changhan Wang, Anne Wu, Jiatao Gu, and Juan Pino. 2021b. CoVoST 2 and Massively Multilingual Speech Translation. In Proc. Interspeech 2021, pages 22472251. Matt White, Ibrahim Haddad, Cailean Osborne, Xiao-Yang Yanglet Liu, Ahmed Abdelmonsef, Sachin Varghese, and Arnaud Le Hors. 2024. The model openness framework: Promoting completeness and openness for reproducibility, transparency, and usability in artificial intelligence. arXiv preprint arXiv:2403.13784. Brian Yan, Siddharth Dalmia, Yosuke Higuchi, Graham Neubig, Florian Metze, Alan Black, and Shinji Watanabe. 2023. CTC alignments improve autoregressive translation. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 1623 1639, Dubrovnik, Croatia. Association for Computational Linguistics."
        }
    ],
    "affiliations": [
        "Fondazione Bruno Kessler (FBK), Italy"
    ]
}