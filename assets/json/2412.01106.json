{
    "paper_title": "One Shot, One Talk: Whole-body Talking Avatar from a Single Image",
    "authors": [
        "Jun Xiang",
        "Yudong Guo",
        "Leipeng Hu",
        "Boyang Guo",
        "Yancheng Yuan",
        "Juyong Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing a whole-body talking avatar from a single image. We propose a novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-to-video diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce a tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of a photorealistic, precisely animatable, and expressive whole-body talking avatar from just a single image."
        },
        {
            "title": "Start",
            "content": "One Shot, One Talk: Whole-body Talking Avatar from Single Image Jun Xiang1,2 Yudong Guo1 Yancheng Yuan2 Leipeng Hu1 Juyong Zhang1 Boyang Guo1 1University of Science and Technology of China 2The Hong Kong Polytechnic University 4 2 0 2 2 ] . [ 1 6 0 1 1 0 . 2 1 4 2 : r Figure 1. Given one-shot image (e.g., your favorite photo) as input, our method reconstructs fully expressive whole-body talking avatar that captures personalized details and supports realistic animation, including vivid body gestures and natural expression changes. Project page: https://ustc3dv.github.io/OneShotOneTalk/"
        },
        {
            "title": "Abstract",
            "content": "single image. Building realistic and animatable avatars still requires minutes of multi-view or monocular self-rotating videos, and most methods lack precise control over gestures and expressions. To push this boundary, we address the challenge of constructing whole-body talking avatar from single image. We propose novel pipeline that tackles two critical issues: 1) complex dynamic modeling and 2) generalization to novel gestures and expressions. To achieve seamless generalization, we leverage recent pose-guided image-tovideo diffusion models to generate imperfect video frames as pseudo-labels. To overcome the dynamic modeling challenge posed by inconsistent and noisy pseudo-videos, we introduce tightly coupled 3DGS-mesh hybrid avatar representation and apply several key regularizations to mitigate inconsistencies caused by imperfect labels. Extensive experiments on diverse subjects demonstrate that our method enables the creation of photorealistic, precisely animatable, and expressive whole-body talking avatar from just Corresponding author: Yudong Guo. 1. Introduction Realistic rendering and precise control of gestures and expressions for whole-body talking avatars hold significant potential for AR/VR applications, such as telepresence and immersive remote conferencing. While extensively studied in the fields of 3D Vision and Graphics, most productionlevel body avatars still require light stage [9, 53] to capture hours or minutes of multi-view frames, making them unsuitable for typical consumer usage. To address this, we focus on the challenging task of constructing an animatable whole-body avatar that supports both photo-realistic rendering and precise 3D control of gestures and expressions, from only single image. This one-shot pipeline primarily faces the following challenges. Complex dynamic modeling. Humans exhibit complex gestures and facial movements when communicating with each other. To model the motion space of the entire body, SMPL-X [40] integrates several previous mod1 els [31, 34, 46] to capture body, hand, and facial movements. With the help of these parametric geometric models, recent personalized human avatars learn to model dynamic geometry and appearance by integrating neural textures [11, 32], neural radiance fields [23, 41, 52, 67], and 3D Gaussians [16, 17, 27, 29, 43] for dynamic photo-realistic rendering. To capture the full appearance of whole-body avatar, these methods require complete observations, typically relying on dense input data for supervision, such as multi-view videos or self-rotating monocular videos. Moreover, these approaches primarily focus on body motion and novel view synthesis, falling short in accurately capturing and animating diverse facial expressions and hand movements, which limits their practical applicability. Due to the reliance on dense inputs, realistic and expressive wholebody modeling from single image remains an unsolved challenge in this field. Generalization to novel gestures and expressions. Another key challenge lies in the limited ability to generalize to wide variety of gestures and facial expressions, especially those that deviate significantly from the poses seen during training. This limitation arises from the data-driven nature of current methods, where the training data typically encompasses only finite set of gestures and expressions. Consequently, when animating avatars, regions or motions that were underrepresented in the training datasuch as dynamic clothing, inner mouth movements, or intricate hand gesturesare often poorly synthesized or omitted entirely. These issues are further compounded when working with single image input, as the limited visual cues often fail to capture the full range of motion and expression. Recently, several approaches [15, 59, 65, 71] have leveraged image and video diffusion models [3, 45] to achieve imageto-video generation guided by whole-body landmarks [61]. Although various model architectures and training techniques have been explored to improve generation quality, temporal consistency remains challenge, and these methods have several key limitations. For example, body landmarks fail to disentangle identity and pose, leading to poor cross-identity animation results, with facial and body distortions and identity mismatches. Moreover, the sparse 2D landmarks are insufficient for precisely controlling gestures and facial movements, as they do not accurately capture the underlying dynamic geometry. In this work, we propose novel pipeline to address both challenges in the one-shot setting. To generalize to diverse gestures and facial movements, we preprocess the large-scale TED Gesture Dataset [62] to build comprehensive whole-body motion space for people talking. We then use these motion sequences as guidance to drive the input single image with pre-trained whole-body video diffusion model [65] and 3D face animation model [5]. This enables us to generate various video sequences of the target person performing different gestures and expressions. However, directly using these pseudo labels to train body avatar leads to unsatisfactory results. First, as noted above, current diffusion-based body animation methods struggle with temporal consistency, identity preservation, and motion alignment. Directly employing them as labels results in significant blurring, distortion, and identity degradation. Furthermore, as we adapt two individual approaches for generating pseudo labels for body gestures and facial expressions, respectively. Due to inconsistencies in camera spaces and rendering procedures, merging them into unified avatar representation poses additional challenges. To tackle the challenge of complex dynamic modeling, we utilize both the single input image and imperfect pseudo labels to train hybrid mesh-3DGS avatar representation, constrained by several carefully designed regularizations. The single input image provides an accurate, though incomplete, appearance for the body avatar, while the pseudo videos offer imperfect but more complete visual cues. For the pseudo video labels, instead of using per-pixel losses, we employ perceptual-based loss term [64], which helps achieve reasonable appearance modeling while alleviating misalignment in the pseudo labels. To further alleviate the inconsistencies caused by the pseudo labels, we adopt tightly coupled mesh-3DGS hybrid avatar representation. By introducing Laplacian smoothing and normal-consistency regularization on the deformed body mesh, we ensure that the structure of the 3D Gaussians used for rendering is well-constrained. Finally, we supervise the avatar representation using both gesture and head video pseudo labels, enabling the creation of photorealistic, precisely animatable, and expressive wholebody avatar. In summary, our contributions include the following aspects: We introduce novel pipeline that overcomes the key challenges of building whole-body expressive talking avatar from single image. Our diffusion guidance strategy effectively extracts valuable knowledge from imperfect diffusion outputs and combines it with the limited information from the input image, enabling complete modeling of the talking avatar. Our carefully designed 3DGS-mesh coupled avatar representation, along with essential regularization techniques, facilitates accurate modeling of diverse subjects and stabilizes the optimization process. 2. Related Work Human Gaussian Splatting. 3D Gaussian Splatting [25] is the state-of-the-art method for scene reconstruction and novel view synthesis (NVS), offering superior rendering speed and visual quality. This has significantly influenced human avatar studies. Methods using multi-view 2 videos [24, 37, 39, 43, 72] demonstrate excellent performance, with unique designs such as ASH [39], which achieves efficient Gaussian learning via mesh UV parameterization. For monocular video input, most human Gaussian splatting methods [14, 16, 17, 29, 36, 49] link Gaussian fields to parametric mesh models, often using additional regularization terms. ExAvatar [36] applies connectivitybased regularizers to short, casually captured videos. For sparse-view images, GPS-Gaussian [69] achieves real-time human NVS by encoding human priors into the network, while HumanSplat [38] generates high-quality static reconstructions from one-shot inputs. In contrast, our method is the first human Gaussian approach capable of recovering realistic, animatable talking avatar from just single image. Avatar Reconstruction from Few-Shot Images. Some works [6, 22, 55] apply 3D GAN inversion for one-shot human reconstruction, but they struggle with preserving personal details and generalization. PIFu [47] and subsequent works [4, 48, 56, 57, 70] introduce pixel-aligned features and neural fields for image-based human reconstruction. An alternative approach leverages diffusion priors to fill in missing details, such as training human-centered diffusion models [2, 13, 38], using novel-view diffusion results for additional supervision [30, 33, 66], and employing Score Distillation Sampling (SDS) [42] to generate 3D avatars from 2D priors [19, 58, 60, 63]. However, these methods focus on static scenes and overlook dynamic human motion, limiting their ability to capture human dynamics. ELICIT [18] uses CLIP [44] for semantic understanding, but it fails to handle hand and facial motions, restricting expressive animation capabilities. In contrast, our approach leverages priors from pose-guided human video diffusion model, capturing both human appearance and dynamics, and enabling expressive full-body animations, particularly in the hands and face. Pose-Guided Human Video Diffusion. Pose-guided human video diffusion models [7, 15, 59, 65, 71] directly generate animated videos from reference image and pose sequence, bypassing traditional 3D reconstruction and rendering processes. The success of these models depends on the quality of training data, model design, and pose guidance. Some approaches [7, 15, 59, 71] incorporate temporal layers to ensure smooth transitions, inspired by AnimateDiff [10], while others [65] use video diffusion models [3] for dynamic sequences. Pose guidance is typically provided by OpenPose [15, 65], DensePose [59], depth maps [7], or SMPL [71]. MimicMotion [65] improves pose accuracy with confidence-aware strategy, and Make-YourAnchor [20] personalizes outputs by fine-tuning models on identity-specific images. We adopt MimicMotion for its superior performance in handling hand regions. Despite their strengths, these 2D models still face challenges, such as image distortion, identity changes, and pose misalignment, due to the lack of 3D understanding. To address these, we leverage optimized avatar representations with carefully designed constraints, improving consistency and naturalness in the generated animations. 3. Method Given single image of the target person, we aim to reconstruct 3D talking avatar that fully inherits the identity and enables natural animation. To address the challenge of complex dynamic modeling from imperfect pseudo videos, we adopt tightly coupled 3DGS-mesh hybrid avatar representation (Sec. 3.1). To generalize well to diverse gestures and facial movements, we generate imperfect video sequences of the target person driven by various motion sequences (Sec. 3.2). Finally, we introduce the carefully designed constraints and loss terms to train the representation from noisy videos effectively (Sec. 3.3). The entire pipeline is illustrated in Fig. 2. 3.1. Coupled 3DGS-Mesh Avatar Whole-body parametric mesh models facilitate human animation and provide good initialization, while 3DGS offers enhanced expressiveness and realistic rendering. To address the challenges of the one-shot task, we design novel coupled 3DGS-mesh representation, which effectively integrates the geometric priors and surface regularization of the mesh without diminishing the expressive capability of the Gaussian field. We couple the 3DGS field with the typical SMPL-X model, which is formulated as follows: (β, θ, ϕ) = (T (β, θ, ϕ), J(β), θ, W), (β, θ, ϕ) = + BS(β) + BE(ϕ) + BP (θ), (1) (2) where BS(), BE() and BP () denote shape, expression, and pose blend functions respectively, with β, ϕ and θ representing the corresponding parameters. is the template mesh, and () is the standard LBS function that rotates the vertices in around the estimated joints J, smoothed by the blend weights W. Since the shape code β is fixed once registered, we will omit it in the later statements and denote = (β, θ, ϕ) and = J(β) for simplicity. Inspired by [1, 39, 54], we initialize the 3D Gaussians on the canonical mesh surface using UV parameterization. For 3D Gaussian located on triangle = {V1, V2, V3} of mesh with barycentric coordinates (u, v), the surface position is give by: Bary(T ) = V(k, u, v) = uV1+vV2+(1uv)V3. (3) Coupled 3DGS-Mesh Deformation. To model clothes, haircuts, and other complex regions that the SMPL-X mesh fails to handle, the deformation of 3D Gaussians is crucial. 3 Figure 2. Overview. Our method constructs an expressive whole-body talking avatar from single image. We begin by generating pseudo body and head frames using pre-trained generative models, driven by collected video dataset with diverse poses. Per-pixel supervision on the input image, perceptual supervision on imperfect pseudo labels, and mesh-related constraints are then applied to guide the 3DGS-mesh coupled avatar representation, ensuring realistic and expressive avatar reconstruction and animation. While previous works can achieve correct deformation with sufficient supervision, we need to impose additional constraints on the deformation field to prevent overfitting to the single input image and imperfect pseudo-labels, while also facilitating the integration of other modules. Based on these considerations, we design two deformation fields in the canonical space and enforce their proximity: one represents the conventional Gaussian deformation, and the other represents the critical mesh deformation. This approach allows us to indirectly influence the Gaussian deformation by applying soft constraints on the deformed mesh. Moreover, since there is no strict binding between the deformed mesh surface and the Gaussians, complex regions are still effectively handled. The two deformation fields, together with the full-body animation and Gaussian Splatting process, are formulated as follows: G(θ, ϕ, P) = SP (W (Bary(T )+dX, J, θ, Bary(W)), P). (4) (θ, ϕ) = (T + dT, J, θ, W), (5) where G() represents the final rendered image, SP () and denote the rendering process and the remaining properties of 3D Gaussians, and dX and dT represent the optimized deformations of the Gaussians and mesh vertices, respectively. We apply key regularizers on dT and propagate soft constraints to dX by encouraging dX to align with Bary(dT ). 3.2. Pseudo Labels Generation In the one-shot image setting, many regions are unseen or occluded. To construct complete avatar and ensure generalization to novel gestures and expressions, we turn our attention to recent advances in human motion diffusion models [15, 65] and head animation techniques [5, 8]. For leveraging diffusion-based generative models, the SDS loss [42] is widely used in text/image-to-3D works [19, 51, 60, 63]. Although diffusion models may introduce moderate deviations, the SDS technique remains effective in these studies. However, for tasks such as realistic dynamic avatar animation from one-shot image, pose alignment is critical [14, 36], and the misalignment introduced by 2D diffusion models cannot be overlooked. Therefore, directly applying SDS in our setting is suboptimal. Instead, we focus on extracting accurate information from pseudo frames synthesized by these generative models. i=1}K To generate whole-body pseudo labels, we collect set of SMPL-X pose sequences {(θNi , ϕNi)F j=1 from the TED Gesture Dataset [62] as input to the generative models. Since no unified generative model exists for both body and head simultaneously, we employ two separate approaches to generate pseudo labels for body gestures and facial expressions, respectively. Given source image IS registered with the SMPL-X parameters (βS, θS, ϕS), we adopt MimicMotion [65] with the collected pose sequences to generate 4 Figure 3. Qualitative comparisons with representative methods [18, 36, 65] in the cross-identity motion reenactment task. Our method achieves accurate and realistic animation with almost all fine details preserved and identity unchanged. various pseudo body frames: IN = otion(IS, D(βS, θN , ϕN )), (6) where D() denotes the mapping from SMPL-X parameters to DWPose [61]. We randomly set the root body pose in θN for each segment to increase viewpoint generalization, with pitch (30, 30) and yaw (10, 10). After that, we re-track IN to obtain more accurate pose parameters (ˆθN , ˆϕN ), while keeping the shape code βS fixed. The re-tracking process is crucial, as it overcomes the misalignment introduced by 2D diffusion models and helps achieve precise and realistic results. For the head region, we adopt Portrait4D-v2 [5] to generate various pseudo frames of the target person performing diverse expressions, also driven by the videos from the TED Gesture Dataset. 3.3. Objective Functions Assisted by our novel hybrid 3DGS-mesh avatar representation, we introduce several carefully designed regularization terms to stabilize the avatar reconstruction process and effectively extract the correct information from both the oneshot input and the imperfect pseudo-labels. Mesh-related Constraints. We use the following loss functions to apply soft constraints on the Gaussian field based on the mesh: LSC =λnormalLnormal + λMLM +λMGCLMGC + λlapLlap. (7) Here, Lnormal is the normal consistency loss applied to the deformed mesh surface, ensuring surface normal consistency post-deformation. LM is the mask loss, which measures the discrepancy between the ground truth mask and the mask of the deformed mesh rendered via [28]. These two losses work together to regulate the behavior of dT , while influencing Gaussian deformation dX through the mesh-Gaussian consistency loss: LMGC = dX Bary(dT )1. (8) Additionally, we compute the Laplacian smoothing loss Llap for dX along with the scaling and RGBs of the 3D Gaussians, based on their initial connectivity state on the canonical mesh surface. Perceptual Supervision of Pseudo-Labels. The synthesized pseudo-labels exhibit noticeable artifacts, such as im5 age distortion and lack of 3D consistency, especially in the results generated with MimicMotion. These artifacts cannot be fully corrected through pose alignment alone. As result, pixel-aligned losses like L1 or MSE often lead to issues like texture flickering, blurring, and identity changes. However, the high-level human structure is consistently wellpreserved in the source image, which aids human recognition and is empirically referred to as perceptual similarity. Previous works [35, 64] have used deep neural networks to capture these perceptual structure features. We adopt the LPIPS perceptual loss [64], using VGG [50] as the backbone. By learning deep perceptual human features from dynamic poses and preserving detailed information from the source image, our method is able to conduct realistic and complete talking avatar animations. Thus, for the pseudo frames, we primarily use the following perceptual loss: Ldiff = λLPIPSLLPIPS(IN , G(ˆθN , ˆϕN , P)). (9) Per-pixel Supervision of the Input Image. For the source image IS with pose (θS, ϕS), we use the regular L1 loss, SSIM loss, and mask loss between IS and the splatted image G(θS, ϕS, P): LS = λRGBLRGB + λSSIMLSSIM + λalphaLalpha. (10) The final loss function is the sum of all the aforementioned losses. 3.4. Implementation Details Gaussian Field. Similar to [16], we adopt an isotropic Gaussian field for better generalization to novel viewpoints and poses. Specifically, we fix the opacity α = 1, the rotation quaternion = (1, 0, 0, 0), and only use scalar scaling factor ˆs for the Gaussians. We set the UV map resolution to 512 and use approximately 150,000 Gaussians in total. Optimization. We train our models using the Adam optimizer [26] with β = (0.9, 0.999). The learning rates for the Gaussian parameters are the same as those in the official implementation, while the learning rates for both the Gaussian and mesh deformation fields are set to η = 1e4. For the loss weights, we set λnormal, λM, λMGC, λlap, λLPIPS, λRGB, λSSIM, λalpha to 1e2, 1e1, 1e1, 1e2, 2e1, 8e1, 1e1, and 4e1, respectively. We start adding the perceptual loss at the 2000th step, while the other losses are used throughout the entire training process. We empirically find that longer training results in better quality and does not lead to training collapse. 4. Experiments 4.1. Dataset We use the poses and expressions processed from 100 videos of the TED Gesture Dataset [62] as the motion sequences during training. For evaluation, the one-shot input and driving poses are primarily sourced from the ActorsHQ [21] and the Casual Conversations Dataset [12]. All videos and images are cropped and resized to fixed aspect ratio of 9:16, with videos sampled at 30 FPS. For foreground segmentation, we use BiRefNet [68], and for human pose tracking, we employ the custom fitting procedure from ExAvatar [36]. 4.2. Comparison with Representative Methods We compare our method with several representative works, including: (1) ExAvatar [36], recent SOTA work that models human avatars with mesh-based 3D Gaussian field. (2) ELICIT [18], one-shot NeRF-based animatable avatar. (3) MimicMotion [65], general 2D pose-guided human video diffusion model. (4) Make-Your-Anchor [20], 2D diffusion-based method that is fine-tuned on one-minute video clip of the individual to enhance identity information. Note that for ExAvatar, since it takes short videos as input, we compare with two versions of it: ExAvatar-40shot, which uses 40-shot images, and ExAvatar-1shot, which uses one-shot images as input. For Make-Your-Anchor, as we find it does not perform well on one-shot input, we only compare it with available video input by fine-tuning it on short video clip. Qualitative Comparisons. Fig. 3 and Fig. 4 present qualitative comparison between our method and the other representative approaches. For Fig. 3, we use the pose of different identity as the driving signal. For Fig. 4, we compare the performance of these methods on the self-driven pose reenactment task, using subjects that have corresponding video data. As observed, ExAvatar-1shot tends to overfit the input image and fails to recover accurate textures in regions occluded by hands. Even with 40-shot input, ExAvatar still struggles with incomplete knowledge and fails to handle novel gestures effectively. MimicMotion generates relatively reasonable results but is constrained by its training distribution and struggles with identity consistency across frames, often leading to appearance mismatches and identity changes. ELICIT, which uses NeRF-based representation, ensures 3D consistency but relies on SMPL for geometry, which neglects the hand region. This coarse semantic proxy fails to support complex hand reconstruction or animation. Make-Your-Anchor, although pre-trained on multiple identities to learn human motion priors, requires long video with sufficient movement and appearance data to adapt to new identities. It struggles with short fine-tuning videos and fails to recover fine details, especially for gestures outside its fine-tuning distribution. In contrast, our method generates animatable and expressive talking avatars from single input image. Our results preserve fine human details and achieve natural animation 6 Figure 4. Qualitative comparisons with representative methods [18, 20, 36, 65] in the self-driven motion reenactment task. Our method well models facial and hand regions, which match the input image most in global identity preservation and local details modeling, even compared with some methods trained on captured videos. with excellent rendering quality. We provide additional examples of cross-identity pose reenactment in Fig. 5. Using the same driving pose, identities with completely different attributes can be driven in the same way, thanks to the SMPL-X model and the 3DGS-mesh coupled representation. Quantitative Comparison. Tab. 1 presents quantitative comparison between our model and other methods. Although the one-shot image animation task typically lacks strict test set for quantitative comparison, we perform the evaluation on the self-driven task and use five common metrics for comparison: Mean Squared Error (MSE), L1 distance, PSNR, SSIM, and LPIPS. Our method outperforms all others across these metrics, demonstrating superior realism and 3D consistency in the results. However, we believe that these metrics do not fully capture the quality or capabilities of the methods, especially for the one-shot image animation task. We encourage readers to refer to the video results of our method for more comprehensive and objective evaluation. 4.3. Ablation Studies We conduct ablations studies on several critical components of the proposed method. Mesh-related Constraints. Our designed hybrid 3DGSmesh avatar representation and the corresponding regularizations significantly enhance the expressiveness and integrity of the final results, as demonstrated in Fig. 7. When soft constraints for mesh deformation are omitted, geometric artifacts appear in specific regions. Similarly, without Figure 5. More examples of cross-identity pose reenactment. Different subjects can be accurately animated with the same poses. Metrics MSE(103) L1(102) PSNR SSIM(101) LPIPS(102)"
        },
        {
            "title": "ELICIT ExAvatar MimicMotion Ours",
            "content": "5.65 1.65 22.56 9.21 6.60 3.93 1.24 24.22 9.24 4.09 2.69 1.41 25.84 9.18 3.89 1.22 0.84 29.31 9.43 2.99 Table 1. Quantitative comparisons with representative methods [18, 36, 65] on self-driven data. Our method outperforms others in pixel-wise error metrics, realism evaluation metrics and perceptual quality metrics. (ExAvatar here denotes ExAvatar-40shot.) the Gaussian Laplacian loss, the Gaussian field fails to capture fine details, further compromising the overall quality. Perceptual-based Pseudo-Labels Guidance. Pseudo7 Figure 6. Perceptual diffusion guidance is of great importance to inpainting unseen regions and modeling natural and realistic textures. diffusion guidance we employ not only preserves the full personalized attributes of the input image but also inpaints unseen regions with more natural and consistent textures. Re-Tracking Step. The re-tracking procedure applied to the pseudo-output of motion diffusion models helps mitigate the misalignment introduced by 2D diffusion models, preventing texture errors and detail loss. As shown in Fig. 8, the re-tracking step successfully recovers more accurate texture structures. Figure 7. Soft mesh constraints together with Gaussian Laplacian help preserve geometric integrity and model fine details. 5. Conclusion In this paper, we introduce novel pipeline for creating expressive talking avatars from single image. We propose coupled 3DGS-Mesh avatar representation, incorporating several key constraints and carefully designed hybrid learning framework that combines information from both the input image and pseudo frames. Experimental results demonstrate that our method outperforms existing techniques, with our one-shot avatar even surpassing stateof-the-art methods that require video input. Considering its simplicity in construction and ability to generate vivid, realistic animations, our method shows significant potential for practical applications of talking avatars across various fields. Limitations. The approach relies on accurate registration between the input image and the parametric human mesh, and severe mismatches, especially in regions like fingers, can cause optimization issues and result in incorrect textures. Additionally, rendering large views or extending to full 360 human reconstruction remains difficult due to current limitations in human motion diffusion models and the lack of data for large, novel viewpoints. Future work will explore integrating semantic information from large language models and static priors from 3D reconstruction to address these limitations. Figure 8. Re-Tracking step preserves better texture structures and avoid texture loss. labels guidance is the core concept behind this work, enabling the challenging task of generating animatable and expressive talking avatars from single input image. As shown in Fig. 6, without diffusion guidance, we are unable to inpaint unseen regions effectively, and the result suffers from severe overfitting to the input image. Additionally, due to the misalignment and inconsistency of the pseudooutput generated by motion diffusion models, using pixelbased losses like L1 for diffusion guidance results in overly smooth outputs and struggles to capture fine details, especially in facial and hand regions. In contrast, the perceptual"
        },
        {
            "title": "References",
            "content": "[1] Rameen Abdal, Wang Yifan, Zifan Shi, Yinghao Xu, Ryan Po, Zhengfei Kuang, Qifeng Chen, Dit-Yan Yeung, and Gordon Wetzstein. Gaussian shell maps for efficient 3d human In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 9441 9451, 2024. 3 [2] Badour AlBahar, Shunsuke Saito, Hung-Yu Tseng, Changil Kim, Johannes Kopf, and Jia-Bin Huang. Single-image 3d In SIGhuman digitization with shape-guided diffusion. GRAPH Asia 2023 Conference Papers, pages 111, 2023. 3 [3] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 2, 3 [4] Enric Corona, Mihai Zanfir, Thiemo Alldieck, Eduard Gabriel Bazavan, Andrei Zanfir, and Cristian Sminchisescu. Structured 3d features for reconstructing controllable avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 16954 16964, 2023. 3 [5] Yu Deng, Duomin Wang, and Baoyuan Wang. Portrait4d-v2: Pseudo multi-view data creates better 4d head synthesizer. In Proceedings of the European conference on computer vision (ECCV), 2024. 2, 4, 5 [6] Zijian Dong, Xu Chen, Jinlong Yang, Michael Black, Otmar Hilliges, and Andreas Geiger. Ag3d: Learning to generate 3d avatars from 2d image collections. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1491614927, 2023. 3 [7] Mengyang Feng, Jinlin Liu, Kai Yu, Yuan Yao, Zheng Hui, Xiefan Guo, Xianhui Lin, Haolan Xue, Chen Shi, Xiaowen Li, et al. Dreamoving: human video generation framework based on diffusion models. arXiv e-prints, pages arXiv 2312, 2023. [8] Jianzhu Guo, Dingyun Zhang, Xiaoqiang Liu, Zhizhou Zhong, Yuan Zhang, Pengfei Wan, and Di Zhang. Liveportrait: Efficient portrait animation with stitching and retargeting control. arXiv preprint arXiv:2407.03168, 2024. 4 [9] Kaiwen Guo, Peter Lincoln, Philip Davidson, Jay Busch, Xueming Yu, Matt Whalen, Geoff Harvey, Sergio OrtsEscolano, Rohit Pandey, Jason Dourgarian, et al. The relightables: Volumetric performance capture of humans with realistic relighting. ACM Transactions on Graphics (ToG), 38(6):119, 2019. 1 [10] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized text-toimage diffusion models without specific tuning, 2023. 3 [11] Marc Habermann, Lingjie Liu, Weipeng Xu, Michael Zollhoefer, Gerard Pons-Moll, and Christian Theobalt. Real-time deep dynamic characters. ACM Transactions on Graphics (ToG), 40(4):116, 2021. 2 the casual conversations dataset. measuring fairness in ai: IEEE Transactions on Biometrics, Behavior, and Identity Science, 4(3):324332, 2021. 6 [13] Ho, Jie Song, Otmar Hilliges, et al. Sith: Single-view textured human reconstruction with image-conditioned diffusion. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 538549, 2024. 3 [14] Hezhen Hu, Zhiwen Fan, Tianhao Wu, Yihan Xi, Seoyoung Lee, Georgios Pavlakos, and Zhangyang Wang. Expressive gaussian human avatars from monocular rgb video. In NeurIPS, 2024. 3, 4 [15] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. 2, 3, [16] Liangxiao Hu, Hongwen Zhang, Yuxiang Zhang, Boyao Zhou, Boning Liu, Shengping Zhang, and Liqiang Nie. Gaussianavatar: Towards realistic human avatar modeling from single video via animatable 3d gaussians. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, 3, 6 [17] Shoukang Hu, Tao Hu, and Ziwei Liu. Gauhuman: Articulated gaussian splatting from monocular human videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2041820431, 2024. 2, 3 [18] Yangyi Huang, Hongwei Yi, Weiyang Liu, Haofan Wang, Boxi Wu, Wenxiao Wang, Binbin Lin, Debing Zhang, and Deng Cai. One-shot implicit animatable avatars with modelIn IEEE Conference on Computer Vision based priors. (ICCV), 2023. 3, 5, 6, 7 [19] Yangyi Huang, Hongwei Yi, Yuliang Xiu, Tingting Liao, Jiaxiang Tang, Deng Cai, and Justus Thies. Tech: Text-guided reconstruction of lifelike clothed humans. In 2024 International Conference on 3D Vision (3DV), pages 15311542. IEEE, 2024. 3, 4 [20] Ziyao Huang, Fan Tang, Yong Zhang, Xiaodong Cun, Juan Cao, Jintao Li, and Tong-Yee Lee. Make-your-anchor: diffusion-based 2d avatar generation framework. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69977006, 2024. 3, 6, 7 [21] Mustafa Isık, Martin Runz, Markos Georgopoulos, Taras Khakhulin, Jonathan Starck, Lourdes Agapito, and Matthias Nießner. Humanrf: High-fidelity neural radiance fields for humans in motion. ACM Transactions on Graphics (TOG), 42(4):112, 2023. 6 [22] Suyi Jiang, Haoran Jiang, Ziyu Wang, Haimin Luo, Wenzheng Chen, and Lan Xu. Humangen: Generating human radiance fields with explicit priors. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1254312554, 2023. [23] Wei Jiang, Kwang Moo Yi, Golnoosh Samei, Oncel Tuzel, and Anurag Ranjan. Neuman: Neural human radiance field from single video. In Proceedings of the European conference on computer vision (ECCV), 2022. 2 [12] Caner Hazirbas, Joanna Bitton, Brian Dolhansky, Jacqueline Pan, Albert Gordo, and Cristian Canton Ferrer. Towards [24] Yuheng Jiang, Zhehao Shen, Penghao Wang, Zhuo Su, Yu Hong, Yingliang Zhang, Jingyi Yu, and Lan Xu. Hifi4g: 9 High-fidelity human performance rendering via compact In Proceedings of the IEEE/CVF Congaussian splatting. ference on Computer Vision and Pattern Recognition, pages 1973419745, 2024. 3 [25] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuhler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. [26] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6 [27] Muhammed Kocabas, Jen-Hao Rick Chang, James Gabriel, Oncel Tuzel, and Anurag Ranjan. HUGS: Human gaussian splatting. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2 [28] Samuli Laine, Janne Hellsten, Tero Karras, Yeongho Seol, Jaakko Lehtinen, and Timo Aila. Modular primitives for high-performance differentiable rendering. ACM Transactions on Graphics, 39(6), 2020. 5 [29] Jiahui Lei, Yufu Wang, Georgios Pavlakos, Lingjie Liu, and Kostas Daniilidis. Gart: Gaussian articulated template models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1987619887, 2024. 2, 3 [30] Peng Li, Wangguandong Zheng, Yuan Liu, Tao Yu, Yangguang Li, Xingqun Qi, Mengfei Li, Xiaowei Chi, Siyu Xia, Wei Xue, et al. Pshuman: Photorealistic single-view human reconstruction using cross-scale diffusion. arXiv preprint arXiv:2409.10141, 2024. [31] Tianye Li, Timo Bolkart, Michael Black, Hao Li, and Javier Romero. Learning model of facial shape and expression from 4d scans. ACM Transactions on Graphics (ToG), 36 (6):1941, 2017. 2 [32] Lingjie Liu, Marc Habermann, Viktor Rudnev, Kripasindhu Sarkar, Jiatao Gu, and Christian Theobalt. Neural actor: Neural free-view synthesis of human actors with pose control. ACM Transactions on Graphics (TOG), 40(6):116, 2021. 2 [33] Zhibin Liu, Haoye Dong, Aviral Chharia, and Hefeng Wu. Human-vdm: Learning single-image 3d human gaussian arXiv preprint splatting from video diffusion models. arXiv:2409.02851, 2024. 3 [34] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. Smpl: skinned multiperson linear model. ACM Transactions on Graphics (ToG), 34(6), 2015. 2 [35] Roey Mechrez, Itamar Talmi, and Lihi Zelnik-Manor. The contextual loss for image transformation with non-aligned In Proceedings of the European conference on comdata. puter vision (ECCV), pages 768783, 2018. 6 [36] Gyeongsik Moon, Takaaki Shiratori, and Shunsuke Saito. Expressive whole-body 3D gaussian avatar. In ECCV, 2024. 3, 4, 5, 6, [37] Arthur Moreau, Jifei Song, Helisa Dhamo, Richard Shaw, Yiren Zhou, and Eduardo Perez-Pellitero. Human gaussian splatting: Real-time rendering of animatable avatars. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 788798, 2024. 3 [38] Panwang Pan, Zhuo Su, Chenguo Lin, Zhen Fan, Yongjie Zhang, Zeming Li, Tingting Shen, Yadong Mu, and Yebin Humansplat: Generalizable single-image human Liu. arXiv preprint gaussian splatting with structure priors. arXiv:2406.12459, 2024. 3 [39] Haokai Pang, Heming Zhu, Adam Kortylewski, Christian Theobalt, and Marc Habermann. Ash: Animatable gaussian splats for efficient and photoreal human rendering. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 11651175, 2024. 3 [40] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3D hands, face, and body from single image. In Proceedings IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), pages 1097510985, 2019. 1 [41] Sida Peng, Yuanqing Zhang, Yinghao Xu, Qianqian Wang, Qing Shuai, Hujun Bao, and Xiaowei Zhou. Neural body: Implicit neural representations with structured latent codes In Proceedfor novel view synthesis of dynamic humans. ings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 90549063, 2021. 2 [42] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. 3, 4 [43] Zhiyin Qian, Shaofei Wang, Marko Mihajlovic, Andreas Geiger, and Siyu Tang. 3dgs-avatar: Animatable avatars In Proceedings of via deformable 3d gaussian splatting. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 3 [45] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 2 [46] Javier Romero, Dimitrios Tzionas, and Michael J. Black. Embodied hands: modeling and capturing hands and bodies together. ACM Transactions on Graphics (ToG), 36(6), 2017. 2 [47] Shunsuke Saito, Zeng Huang, Ryota Natsume, Shigeo Morishima, Angjoo Kanazawa, and Hao Li. Pifu: Pixel-aligned implicit function for high-resolution clothed human digitization. In Proceedings of the IEEE/CVF international conference on computer vision, pages 23042314, 2019. 3 [48] Shunsuke Saito, Tomas Simon, Jason Saragih, and Hanbyul Joo. Pifuhd: Multi-level pixel-aligned implicit function for In Proceedings of high-resolution 3d human digitization. the IEEE/CVF conference on computer vision and pattern recognition, pages 8493, 2020. 3 [49] Zhijing Shao, Zhaolong Wang, Zhuang Li, Duotun Wang, Xiangru Lin, Yu Zhang, Mingming Fan, and Zeyu Wang. SplattingAvatar: Realistic Real-Time Human Avatars with In Proceedings of Mesh-Embedded Gaussian Splatting. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 3 [50] Karen Simonyan. Very deep convolutional networks arXiv preprint large-scale image recognition. for arXiv:1409.1556, 2014. 6 [51] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. arXiv preprint arXiv:2309.16653, 2023. [52] Chung-Yi Weng, Brian Curless, Pratul Srinivasan, Jonathan Barron, and Ira Kemelmacher-Shlizerman. Humannerf: Free-viewpoint rendering of moving people from In Proceedings of the IEEE/CVF conmonocular video. ference on computer vision and pattern Recognition, pages 1621016220, 2022. 2 [53] Donglai Xiang, Fabian Prada, Zhe Cao, Kaiwen Guo, Chenglei Wu, Jessica Hodgins, and Timur Bagautdinov. Drivable avatar clothing: Faithful full-body telepresence with dynamic clothing driven by sparse rgb-d input. In SIGGRAPH Asia 2023 Conference Papers, pages 111, 2023. 1 [54] Jun Xiang, Xuan Gao, Yudong Guo, and Juyong Zhang. Flashavatar: High-fidelity head avatar with efficient gaussian In Proceedings of the IEEE/CVF Conference embedding. on Computer Vision and Pattern Recognition, pages 1802 1812, 2024. 3 [55] Zhangyang Xiong, Di Kang, Derong Jin, Weikai Chen, Linchao Bao, Shuguang Cui, and Xiaoguang Han. Get3dhuman: Lifting stylegan-human into 3d generative model using In Proceedings of the pixel-aligned reconstruction priors. IEEE/CVF International Conference on Computer Vision, 2023. 3 [56] Yuliang Xiu, Jinlong Yang, Dimitrios Tzionas, and Michael Icon: Implicit clothed humans obtained from norBlack. In 2022 IEEE/CVF Conference on Computer Vimals. sion and Pattern Recognition (CVPR), pages 1328613296. IEEE, 2022. 3 [57] Yuliang Xiu, Jinlong Yang, Xu Cao, Dimitrios Tzionas, and Michael Black. Econ: Explicit clothed humans optimized via normal integration. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 512523, 2023. [58] Yuliang Xiu, Yufei Ye, Zhen Liu, Dimitrios Tzionas, and Michael Black. Puzzleavatar: Assembling 3d avatars from personal albums. ACM Transactions on Graphics (TOG), 2024. 3 [59] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human imIn Proceedings of age animation using diffusion model. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. 2, 3 [60] Xihe Yang, Xingyu Chen, Daiheng Gao, Shaohui Wang, Xiaoguang Han, and Baoyuan Wang. Have-fun: Human avatar reconstruction from few-shot unconstrained images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 742752, 2024. 3, 4 11 [61] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with two-stages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42104220, 2023. 2, 5 [62] Youngwoo Yoon, Woo-Ri Ko, Minsu Jang, Jaeyeon Lee, Jaehong Kim, and Geehyuk Lee. Robots learn social skills: End-to-end learning of co-speech gesture generation for In 2019 International Conference on humanoid robots. Robotics and Automation (ICRA), pages 43034309. IEEE, 2019. 2, 4, [63] Jingbo Zhang, Xiaoyu Li, Qi Zhang, Yanpei Cao, Ying Shan, and Jing Liao. Humanref: Single image to 3d human genIn Proceedings of eration via reference-guided diffusion. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18441854, 2024. 3, 4 [64] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. 2, 6 [65] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation arXiv preprint with confidence-aware pose guidance. arXiv:2406.19680, 2024. 2, 3, 4, 5, 6, 7 [66] Zechuan Zhang, Zongxin Yang, and Yi Yang. Sifu: Side-view conditioned implicit function for real-world usIn Proceedings of the able clothed human reconstruction. IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 99369947, 2024. [67] Fuqiang Zhao, Wei Yang, Jiakai Zhang, Pei Lin, Yingliang Zhang, Jingyi Yu, and Lan Xu. Humannerf: Efficiently generated human radiance field from sparse inputs. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 77437753, 2022. 2 [68] Peng Zheng, Dehong Gao, Deng-Ping Fan, Li Liu, Jorma Laaksonen, Wanli Ouyang, and Nicu Sebe. Bilateral reference for high-resolution dichotomous image segmentation. CAAI Artificial Intelligence Research, 3:9150038, 2024. 6 [69] Shunyuan Zheng, Boyao Zhou, Ruizhi Shao, Boning Liu, Shengping Zhang, Liqiang Nie, and Yebin Liu. Gpsgaussian: Generalizable pixel-wise 3d gaussian splatting for In Proceedings of real-time human novel view synthesis. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1968019690, 2024. 3 [70] Zerong Zheng, Tao Yu, Yebin Liu, and Qionghai Dai. Pamir: Parametric model-conditioned implicit representation for image-based human reconstruction. IEEE transactions on pattern analysis and machine intelligence, 2021. 3 [71] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Qingkun Su, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image arXiv preprint animation with 3d parametric guidance. arXiv:2403.14781, 2024. 2, 3 [72] Wojciech Zielonka, Timur Bagautdinov, Shunsuke Saito, Michael Zollhofer, Justus Thies, and Javier Romero. Drivable 3d gaussian avatars. arXiv preprint arXiv:2311.08581, 2023. 3 One Shot, One Talk: Whole-body Talking Avatar from Single Image"
        },
        {
            "title": "Supplementary Material",
            "content": "Figure 9. detailed illustration of our pipeline. A. Detailed Pipeline For comprehensive understanding, we present detailed illustration of our pipeline in Fig. 9. B. Limitation Tracking and Large-View Rendering. Accurate SMPL-X tracking is essential for mesh-based avatar representation. Our method relies on precise registration between the input image or video and the parametric human mesh, which can be compromised by tracking inaccuracies. Additionally, self-intersection may occur when we conduct crossidentity animation, particularly observed in the finger area, as illustrated in Fig. 10. Furthermore, our method demonstrates robust performance within view range of -30 to 30 degrees. However, its performance degrades for larger viewing angles, as demonstrated in Fig. 11. C. Broader Impact Our work enables the reconstruction of expressive wholebody talking avatars from single photo, allowing for realistic animations with vivid body gestures and natural expression changes. We consider this significant advancement in the research and practical applications of multimodal digital humans. However, this technology carries the risk of misuse, such as generating fake videos of individuals to spread false information or harm reputations. We strongly condemn such unethical applications. While it may Figure 10. Inaccurate tracking and finger self-intersection during cross-identity animation. Figure 11. Novel view results across diverse angles. not be possible to entirely prevent malicious use, we believe that conducting research in an open and transparent manner can help raise public awareness of potential risks. Additionally, we hope our work can inspire further advancements in forgery detection technologies."
        }
    ],
    "affiliations": [
        "The Hong Kong Polytechnic University",
        "University of Science and Technology of China"
    ]
}