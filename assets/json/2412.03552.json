{
    "paper_title": "Imagine360: Immersive 360 Video Generation from Perspective Anchor",
    "authors": [
        "Jing Tan",
        "Shuai Yang",
        "Tong Wu",
        "Jingwen He",
        "Yuwei Guo",
        "Ziwei Liu",
        "Dahua Lin"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "$360^\\circ$ videos offer a hyper-immersive experience that allows the viewers to explore a dynamic scene from full 360 degrees. To achieve more user-friendly and personalized content creation in $360^\\circ$ video format, we seek to lift standard perspective videos into $360^\\circ$ equirectangular videos. To this end, we introduce Imagine360, the first perspective-to-$360^\\circ$ video generation framework that creates high-quality $360^\\circ$ videos with rich and diverse motion patterns from video anchors. Imagine360 learns fine-grained spherical visual and motion patterns from limited $360^\\circ$ video data with several key designs. 1) Firstly we adopt the dual-branch design, including a perspective and a panorama video denoising branch to provide local and global constraints for $360^\\circ$ video generation, with motion module and spatial LoRA layers fine-tuned on extended web $360^\\circ$ videos. 2) Additionally, an antipodal mask is devised to capture long-range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres. 3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames. Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art $360^\\circ$ video generation methods. We believe Imagine360 holds promise for advancing personalized, immersive $360^\\circ$ video creation."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 ] . [ 1 2 5 5 3 0 . 2 1 4 2 : r Imagine360: Immersive 360 Video Generation from Perspective Anchor Jing Tan1, Shuai Yang2,4, Tong Wu1(cid:0), Jingwen He1, Yuwei Guo1, Ziwei Liu3, Dahua Lin1,4(cid:0) 1 The Chinese University of Hong Kong 3 Nanyang Technological University 2 Shanghai Jiao Tong University 4 Shanghai AI Laboratory https://ys-imtech.github.io/projects/Imagine360 Imagine360 lifts standard perspective video into 360 video, enabling dynamic scene experience Figure 1. Overview of Imagine360. from full 360 degrees. Compared to Follow-Your-Canvas, which focuses only on perspective visual and motion patterns, our approach achieves more plausible spherical video patterns. Best viewed with Acrobat Reader for the animated 360 videos."
        },
        {
            "title": "Abstract",
            "content": "360 videos offer hyper-immersive experience that allows the viewers to explore dynamic scene from full 360 degrees. To achieve more user-friendly and personalized content creation in 360 video format, we seek to lift standard perspective videos into 360 equirectangular videos. To this end, we introduce Imagine360, the first perspective-to-360 * : Equal contribution. (cid:0) : Corresponding author. video generation framework that creates high-quality 360 videos with rich and diverse motion patterns from video anchors. Imagine360 learns fine-grained spherical visual and motion patterns from limited 360 video data with several key designs. 1) Firstly we adopt the dual-branch design, including perspective and panorama video denoising branch to provide local and global constraints for 360 video generation, with motion module and spatial LoRA layers fine-tuned on extended web 360 videos. 2) Additionally, an antipodal mask is devised to capture long1 range motion dependencies, enhancing the reversed camera motion between antipodal pixels across hemispheres. 3) To handle diverse perspective video inputs, we propose elevation-aware designs that adapt to varying video masking due to changing elevations across frames. Extensive experiments show Imagine360 achieves superior graphics quality and motion coherence among state-of-the-art 360 video generation methods. We believe Imagine360 holds promise for advancing personalized, immersive 360 video creation. 1. Introduction Imagine taking on journey in the heart of bustling city, serene beach, or cherished place of your own. As you turn your head, you will see the evolving world at new viewpoints, allowing for dynamic experience in full 360 degrees. 360 video offers an interactive, immersive viewing experience that creates living, breathing world as if the viewer is part of the experience. With the rapid development of head-mounted spatial computing systems, the demand for 360 videos is increasing, driven by extensive applications across entertainment, education, and communication. Recent advancements in 360 video generation have focused on text-guided [46] and image-guided [30] models. While these methods produce plausible 360 videos, they require panoramic optical flow [46] or high-quality panoramic images [30] as guidance, which are hard to obtain for average users. In contrast, perspective videos are much more accessible, easily captured with smartphone cameras, or generated by advanced video generation models. To enable more user-friendly and personalized 360 video creation, we propose new task: perspective-to-360 video generation, which transforms perspective video inputs into 360 equirectangular videos. Specifically, we take perspective video with narrow FOV as the anchor video, project it to 360 180 FOV video canvas with perspective-to-equirectangular (P2E) mapping and create the unknown surrounding pixels. With dynamic and diverse motion condition information from anchor videos, this video-based guidance offers more effective paradigm to generate high-quality 360 videos with richer and more complex motions compared to textor image-based guidance. One relevant task is video outpainting, which aims to fill in missing regions outside the edges of video frames in larger canvas, typically in the perspective domain with fixed video masking across frames. Simply applying standard video outpainting methods does not achieve satisfactory results, as our perspective-to-360 video generation exhibits more challenges. First, due to the large domain gap between perspective and panorama videos, learning the spherical visual and motion patterns requires sophisticated design when trained on limited panorama video data. Second, as videos exhibit different elevation angles across frames, after the P2E mapping, the masking continuously changes in shape, size, and location, shown in Fig. 1, requiring elevationaware designs for robust generation. To address these challenges, we introduce Imagine360, the first perspective-to-360 video generation framework, that creates high-quality panoramic videos from perspective anchor videos. Directly fine-tuning pre-trained video diffusion model from the perspective domain proves inadequate for generating high-quality panoramic patterns. To tackle this issue, we adapt the dual-branch design [54] from image-level to conditioned video generation. Specifically, we devise two parallel branches for perspective and panoramic video denoising, with each branch comprising spatial layers, motion modules, and cross-domain attention layers. The two branches are tightly coupled via bidirectional direct mapping (P2E and E2P) through cross-domain attention, ensuring the generation of plausible spherical motion patterns. However, direct mapping alone is insufficient for capturing long-range dependencies essential for 360 videos. It often fails to account for critical characteristic of panoramic videos, where each pixel undergoes reverse camera motion of its antipodal counterpart. Hence, we improve the cross-domain attention with antipodal mask, to facilitate information exchange between each pixel and its antipodal counterparts on the opposite hemisphere. In this regard, the receptive field of each pixel is extended from the directly mapped neighborhood to the antipodal neighborhood, making it easier to learn long-range panoramic motion dependencies. Moreover, taking general videos as anchors for personalized creation requires the framework to cater to different yaw and pitch angles in the anchor video. Benefiting from the 360 close-loop property, we can assume the viewer rotates with the camera, and without loss of generality, only consider the influence of the changing pitch angles. In practice, we incorporate elevation-aware designs, including elevation-aware data sampling in training and an elevation estimation module in inference, to ensure robust generation for customized video inputs. With the three key designs, Imagine360 pioneers the end-to-end, high-quality 360 video generation from perspective video inputs. Extensive experiments show that our model generates 360 videos with the best frame quality and motion quality among state-of-the-art 360 video generation methods. We also show bonus advantage of our pipeline for achieving superior panorama image outpainting results. We believe that Imagine360 is able to lead the 360 video generation community in creating personalized, hyper-immersive experiences for downstream applications. 2 2. Related Works 2.1. Diffusion Models Diffusion models [21, 37, 38] have achieved remarkable success in image generation [10, 33, 35], leading to advancements in video diffusion models [35, 16, 22, 45, 52]. The first video diffusion model (VDM) [23] adopts space-time factorized U-Net in pixel space to model lowresolution videos. Imagen-Video [22] proposes to use cascaded DMs for generating high-definition videos. Subsequent research [35, 16, 45] adapts existing text-to-image (T2I) models to text-to-video (T2V) models by incorporating temporal layers, including both convolution and attention layers. More recently, several works [13, 52] directly use 3D full attention to model space-time information for more unified video representation. On the other hand, image-to-video (I2V) [2, 15, 17, 50, 56, 57] has arisen great attention as it enables more precise control on video generation. Some works achieve I2V by incorporating the image condition into the pretrained T2V models and finetuning newly added modules [17, 50] or the inherited weights [2, 56], while plug-to-play methods [15, 57] aims to turn any text-to-image models into image animators. 2.2. Video Outpainting Video Outpainting aims to fill in the missing regions at the edges of source videos. Compared to advanced imagelevel outpainting, video-level outpainting remains underexplored due to its challenges in maintaining both spatial and temporal fidelity and consistency. Recent video outpainting methods leverage diffusion to generate highquality pixels in the missing regions. M3DDM [11] proposes frame-guided Masked 3D diffusion model and coarse-to-fine inference strategy to tackle artifact accumulation in long video outpainting. MOTIA [41] employs per-case optimization strategy to learn the data-specific patterns of source video for better outpainting quality. FollowYour-Canvas [6] divides the canvas into multiple windows and achieves outpainting of different sizes and resolutions by merging each outpainted window. These methods focus on handling perspective video outpainting with fixed masking in each frame. Despite their appealing outpainting results, it remains difficult for them to handle perspective360 video generation that requires high-quality outpainting in panoramic distribution and continuously changing video masks from varying elevations. In contrast, our Imagine360 handles the perspective-to-360 video generation with global and local constraints from dual-branch diffusion and elevation-aware designs to handle changing video masks, producing high-quality 360 video generations from perspective video anchors. 2.3. 360 Panorama Generation Early methods [1, 7, 8, 27, 32, 42, 48] exploit GANbased framework for panorama image generation. OmniDreamer [1] proposes transformer-based framework for 360-degree outpainting and devises circular inference to obtain 360 close-loop continuity. Recently, diffusion-based methods [12, 28, 29, 40, 43, 44, 49, 54, 55] have dominated image-level panorama generation. Due to the data scarcity of large panorama image datasets, methods [43, 44] that directly fine-tune LDM to generate the panorama results in low-quality images with simple structures and sparse assets. PanFusion [55] introduces the dual-branch structure, consisting of panorama and perspective branch to leverage the synergy from both global and local constraints for high-quality text-to-panorama generation. Despite numerous efforts in image-level panorama genthere are few works [30, 31, 46] focusing on eration, panorama video generation. 360DVD [46] takes text prompts and additional panorama video optical flow as guidance and learns 360-Adapter on standard T2V models to generate plausible 360 videos. 4K4DGen [30] animates static panoramic image at user-selected regions with I2V pre-trained prior in training-free manner. These fine-tuning-based or training-free approaches struggle to bridge the distribution gap between panoramic and perspective videos, resulting in simple natural perturbations, such as clouds moving and water running. In contrast, our Imagine360 benefits from the dual-branch video denoising structure with antipodal relation modeling and elevation-aware designs, resulting in more dynamic 360 videos with rich and structured motions. 3. Our Approach Given perspective video, we aim to generate an equirectangular panoramic (ERP) video that replicates its visual appearance and motion, and extends to the complete 360 180 field of view. We propose Imagine360, the first perspective-to-360 video generation framework, featuring three key designs to create high-quality 360 videos from perspective video anchor, as illustrated in Fig. 2. First, to learn the spherical visual and motion patterns based on pre-trained perspective generative prior, we employ the dual-branch design, consisting of panorama branch and perspective branch to jointly denoise the 360 spacetime latent. Second, to obtain more fine-grained and plausible panoramic motion, we refine the cross-domain attention to highlight antipodal masking that captures long-range motion dependencies in antipodal directions. Finally, to handle diverse video inputs with varying elevation angles, we propose elevation-aware training and inference designs to obtain robust generations from different video anchors. 3 Figure 2. Pipeline of Imagine360. Given perspective anchor video guidance, Imagine360 leverages dual-branch video noising structure, with parallelled panorama and perspective branches to denoise 360 videos with plausible panoramic patterns. Additionally, we devise the cross-domain spherical attention with antipodal masking to capture long-range dependencies for reversed antipodal motion. Finally, we introduce elevation-aware designs to handle diverse video inputs of changing elevations. 3.1. Workflow Setup Perspective-to-360 video mask construction. Similar to video outpainting, our task also requires video masking on the 360 video canvas. For given perspective video of frames and its camera pose (F OV, θ1:T , ϕ1:T ), where θ denotes the yaw angle (azimuth) and ϕ denotes the pitch angle (elevation), we project it into the equirectangular domain via P2E projection to construct the 360 video mask Mpano. By stacking the frame masks, we have the video masking 1:T pano. With this projection, we obtain the masks and masked latents to feed into the panorama UNet along with the latents. Anchor video preparation. Our training and inference have slightly different procedures to obtain the anchor videos. During training, given the target panorama video with set of camera poses of different elevations, we can obtain the corresponding video projections in the perspective domain and obtain masked panorama video indicating where to outpaint and where to keep. The anchor video for perspective domain 1:T P,anc is the perspective video projections and the anchor video for panormama domain 1:T E,anc is the maximum inscribed rectangle crop of the kept region in the target panorama video. During inference, we use the input perspective video as the perspective anchor, generate its text prompt with video LLM, and estimate the corresponding camera pose for mask construction and build the panorama anchor video as in training. Video-conditioned generation. The perspective-to-360 video task requires precise guidance over the newly generated pixels, as the unknown pixels far outnumber the given ones. To prepare the guidance signals once had anchor videos and text prompts, following [6], we use SAMs visual encoder [26] to extract features from the anchor video. These features are processed by query-based Transformer, which distills high-level visual and motion cues into compact latent tokens. To guide the massive spacetime pixel generation, we adopt the IP cross-attention from [53], to decouple cross-attention for text prompts and visual embeddings in U-Net layers. This allows our model to extend visual content and motion patterns effectively to other views and unmasked areas of the spacetime panorama. 3.2. Dual-Branch 360 Video Denoising Dual-branch design. Extending perspective video to 360 canvas requires careful model design due to the large differences between panorama and perspective distributions. Mainstream methods either train Latent Diffusion Model (LDM) to directly denoise panorama frames or project panorama frames to multiple perspective views for joint denoising. The former results in plain visuals and mild motion from limited panorama data, while the latter easily produces local, short-range motions in each view. Inspired by the dual-branch design for image-level panorama generation [54], we introduce dual-branch video denoising structure. It consists of global panorama branch and local perspective branch, both using U-Nets based on AnimateDiff [16], where each U-Net layer consists of spatial layers initialized from SD weights and motion module initialized from [6] weights. Aiming to generate 360 panorama video of shape RT 3HW (W = 2H), the noisy latent input is of shape RT 9HW from 4 latents, binary masks and masked latents. The panorama and perspective branch share the same noisy latents such that they could be aligned in the subsequent denoising steps. The panorama branch directly takes the noisy global latents as input, whereas the perspective branch projects the latent into = 20 perspective views according to the icosahedron modeling [34, 54], and each view gets latent of shape RT 9H/2H/2 through projection. LoRA layers are employed at spatial self-attention and IP cross-attention on each branch to cater to different resolutions and the new panorama distribution. Denoising on the same noisy latents, the global panorama branch provides holistic understanding of the 360 spacetime canvas to maintain the global consistency, while the local perspective branch leverages good pretrained perspective generative prior to preserve rich local details at each view in every frame. At the end of each UNet block, the latents from both branches are aligned via cross-domain attention module to enhance the local-global synergy. The attention module works bi-directionally. Panorama pixels and perspective window pixels from the same frame are flattened into two sequences and feed into the attention module as the key-value token or query token in alternation, with spherical positional encoding [54] added to indicate the relative spherical location. The information exchange is highlighted for directly mapped pixel pairs between two domains. Specifically, as illustrated in Fig. 3, at each frame, we use P2E projections to map the pixels from the panorama branch to the directly mapped pixels (orange) in the perspective branch, and vice versa. This bidirectional mapping is reflected via an attention mask, denoted spherical mask, and added to the attention module. Gaussian blur is also applied on the spherical mask to enable neighboring activations in the attention. Resource-friendly fine-tune strategy. With only thousands of training panorama videos, we seek to fine-tune limited proportion of parameters to yield good generation results. Thanks to the space-time disentangled design from AnimateDiff [16], we do not need to fine-tune the whole model to learn the distributions of panoramic videos. In practice, we add LoRA on spatial attention layers for both branches. To learn panoramic motion, we show in Sec. B.1 that employing motion LoRA layers does not have sufficient influence on the pretrained prior to generate good panorama motion. Therefore, the whole motion module is fine-tuned for panorama branch to learn the necessary prior for generating spherical motion patterns. Note that, to fully exploit the perspective generative prior and prevent our limited panorama video data messing up the good perspective motion from pretrained weights, we do not fine-tune the motion module in the perspective branch. Experimental results show that our resource-friendly fine-tuning strategy achieves stunning results for 360 video generation despite Cross-domain Spherical Attention (perspective Figure 3. branch) highlights interaction for direct-mapped pixels (spherical mask) and antipodal-mapped pixels (antipodal mask) between panorama and perspective domains. the limited data and GPU resources. The training objectives of the trainable modules in the two branches are: LE = E(x1:T ),y,ϵ1:T ,t (cid:104)(cid:13) (cid:13)ϵE ϵE,Θ (cid:0)z1:T E,t , t, τΘ(y), 1:T E,anc (cid:105) , (cid:1)(cid:13) 2 (cid:13) 2 (1) LP = (cid:104)(cid:13) (cid:13)ϵP ϵP,Θ (cid:0)z1:T P,t , t, τΘ(y), 1:T (cid:105) 2 (cid:1)(cid:13) (cid:13) , (2) 2 P,anc ,t ),y,ϵ1:T P(x1:T where x1:T and ϵE,Θ represent the target panorama video and the predicted noise from the panorama branch, respectively, while x1:T and ϵP,Θ denote the target perspective video and the predicted noise from the perspective branch. To facilitate the synchronous exchange of information between two branches, we combine the two branches losses as the final training objective = LE + 1 i=1 Li . (cid:80)m 3.3. Encouraging Fine-grained Panoramic Patterns The dual-branch design generates plausible spherical patterns for spatial appearance and motion, however, to achieve fine-grained panoramic videos, i.e. 360 close-loop continuity and reversed antipodal motion, we introduce additional advanced techniques, including circular padding and antipodal mask in cross-domain spherical mask. Circular padding for close-loop continuity. The 360 close-loop continuity is an important property for panorama media, which refers to the left-most and right-most edges of the panorama video seamlessly connected to form continuous loop. Following [51, 54], we employ the circular padding before the convolutional layer in each U-Net block and unpad to its original resolution afterward to mitigate the loop inconsistency from local convolutions. For better 360 immersive experience, in addition to the main framework, we also implement circular padding in the optional video super-resolution post-processing [19] to pad and unpad the upsampled latents in the decoding process for SR close-loop continuity. Antipodal mask for reversed motion. One particular property of panoramic motion is that the antipodal pixel always exhibits the reversed camera motion. That is to say, if the camera is moving forward in the forward viewing direction, then if the viewer turns to the backward viewing direction, the scene should look like moving away from the 5 gle, we also compute the sinusoidal embedding for θ and concatenate with the mask positional encoding as the final positional encoding. Elevation estimation for inference. During inference, to construct pitch sequence for each test video, we employ PerspectiveFields [25] to estimate the pitch angle for each input frame. As PerspectiveFields is developed for single image calibration, estimating the pitch sequence frame by frame causes inconsistency in the projected mask sequence. Therefore, we leverage LinearRegression to fit the pitch sequence and produce smooth masking on the 360 video canvas. 3.5. Training Data Collection Due to the significant domain gap between panoramic and perspective motions, fine-tuning requires extensive textvideo paired data in the panoramic domain. Although the WEB360 [46] dataset offers 2,114 panoramic videos of outdoor landscapes, it remains limited in scale and lacks diverse, structured motions typical in urban environments, such as vehicle traffic and pedestrian activities. To address this, we collected 8,630 360-degree videos from YouTube, covering virtual city tours, wildlife documentaries, and VR game captures, to supplement training. The videos were converted to equirectangular format, segmented into consecutive shots using TransNet-v2 [39], and processed into 5-second clips at 20 fps with 2 speed-up. We extracted optical flow using PanoFlow [36], to filter out static clips based on maximum optical flow values. Text captions were generated as two-sentence summaries for each clip using VideoLLaMa-2 [9]. We conduct our training on combination of the WEB360 and our dataset with 10,744 samples. 4. Experiments 4.1. Implementation Details Training settings. The spatial and motion modules in our model initializations are respectively based on Stable Diffusion v2.1 and Follow-Your-Canvas [6]. Training is conducted on 8 NVIDIA A100 GPUSs with 50k training steps using our proposed dataset, with the spatial LoRA rank and αLoRA set to 32 and 1.0. The resolution is set to 5121024, the length of frames to 40, the batch size to 1, and the learning rate to 1 105. Evaluation metrics. We employ video quality assessment (VQA) metric from Q-Align [47] to measure the general In addition, we project the 360 quality of 360 videos. videos into 4 perspective views with ϕ = 0, OV = 90, θ = [0, 90, 180, 270] and employ imaging quality, aesthetic quality, motion smoothness, and subject consistency metrics from Vbench [24] to measure the graphics quality and motion consistency of projected perspective videos as well as examine the panoramic structure plausiFigure 4. Elevation-aware sampling augments the training samples with diverse elevation trajectories. viewer. In both the panoramic branch and the perspective branch, the antipodal pixels are usually distant from each other in the token sequence, whereas the attention modules often tend to attend more on neighboring pixels, we observe that the reversed motion in the antipodal directions is mild compared to the motion in the neighboring pixels of the projected guidance video. To emphasize the antipodal relations in the 360 videos, we highlight the antipodal activations in the cross-domain spherical mask and denote this set of activations as the antipodal mask. As shown in Fig. 3, we find each pixel in the perspective domain the antipodal pixel (blue) of its panorama counterpart, and vice versa. The antipodal pairs are assigned high activation on the masking and added to the attention map. Gaussian blur is also applied on the mask for antipodal neighbors. 3.4. Elevation-robustness over General Video Input Commonly, we assume the video input to be upright with fixed camera pose, but in-the-wild perspective videos are of various azimuthal angles θ1:T and elevation angles ϕ1:T . As the ERP panorama exhibits 360 close-loop continuity, without loss of generality, we can set the azimuthal angle to zero by assuming the viewer is rotating with the camera to simplify computation. However, we must specifically investigate ϕ as the change of elevation angle significantly impacts the shape and location of the masking, as shown in Fig. 4. To create an elevation-augmented generation pipeline, we propose elevation-augmented data sampling, an elevation estimator for inference. Elevation-aware data sampling. We create flexible masking for each frame with an elevation-aware data sampling strategy. As illustrated in Fig. 4, for each sample, we randomly sample ϕs (20, 20) and slope ratio (0.5, 0.5), then construct smooth pitch trajectory sequence with ϕs as the starting angle and assign frame (1, ) with ϕ = ϕs + t. As the masks exhibit non-gridded structure from the spherical P2E projection, we compute the positional encoding by calculating the maximum inscribed rectangle of each mask, shown with red dashed box. The center coordinates x, y, and size h, are combined with the 360 canvas size H, to compute the sinusoidal embedding as the mask positional encoding on the panorama branch. Moreover, to highlight the pitch an6 Figure 5. Qualitative comparisons on 360 video generations among state-of-the-art methods. Imagine360 generates 360 video generation with superior visual quality and plausible panoramic patterns. Method Imaging Quality Aesthetic Quality Motion Smoothness Subject Consistency VQA Animatediff [16]+LatentLab360 Follow-Your-Canvas [6] 360DVD [46] Ours 0.7159 0.6978 0.5537 0.7487 0.5840 0.6432 0.4745 0.6439 0.9780 0.9771 0.9701 0.9806 0.9338 0.9529 0.9629 0. 0.7860 0.8444 0.5867 0.8672 Table 1. Quantitative comparison with SoTA approaches on Vbench [24] metrics and VQA from Q-Align [47]. bility of the 360 videos. 4.2. Quantitative Comparisons Comparison methods. Since we are the first perspectiveto-360 video generation framework, its infeasible to find method that has the exactly same input condition as ours. We compare with methods that produces 360 videos from other kinds of conditions. 360DVD [46] is the advanced text-guided 360 video generation method, that takes text prompts and panorama video optical flow as input for 360 video generation. Here, we feed the same text prompt as ours with its default optical flow into 360DVD for comparison. Follow-Your-Canvas [6] is the state-of-the-art video outpainting method that employs tile-based outpainting to handle outpainting of arbitrary size and resolution. Here, we project our perspective anchor video into the panorama canvas and feed the masked canvas into [6] for evaluation. AnimateDiff+LatentLab360 [16] takes image inputs and animates with the LatentLab360 LoRA. For comparison, we feed the first frame of our generations into AnimateDiff+LatentLab360 and take its animated videos for evaluation. Tab. 1 provides the quantitative comparison of Imagine360 with other models on total of 100 test cases. Imagine360 achieves the best performance in all metrics. This indicates that our approach excels other methods not only in general panorama video quality but also in panorama structure plausibility. We also involve human evaluation to examine the quality of generated 360 videos. We invite 26 users with expertise in video and 3D generation to assess the results across three dimensions: graphics quality, structure plausibility, and temporal coherence. Tab. 3 reports the average user ranking of all four methods, and our method achieves the best performance in all three dimensions. Please refer to Sec. for more details regarding the setup and metrics. Moreover, we show that our method also achieves superior panorama image outpainting performance. Please refer to Sec. for comparison. 4.3. Qualitative Comparisons We present the qualitative comparison between Imagine360 and other SoTA models in Fig. 5. At each frame, 7 Method Imaging Quality Aesthetic Quality Motion Smoothness Subject Consistency VQA Ours(Full model) w/o persbranch w/o panobranch w/o antipodal mask w/o elevation-aware designs 0.7487 0.7050 0.5854 0.7446 0.6978 0.6439 0.6124 0.4549 0.6360 0.6296 0.9806 0.9744 0.9700 0.9802 0. 0.9710 0.9599 0.9546 0.9652 0.9529 0.8672 0.8435 0.6168 0.8580 0.7663 Table 2. Quantitative ablation studies on dual-branch designs, antipodal mask, and elevation-aware designs. Method Graphics Quality Structure Plausibility Temporal Coherence Animatediff [16]+LatentLab360 360DVD [46] Follow-Your-Canvas [6] Ours 2.3488 1.2067 2.7692 3.6827 2.3894 1.7588 2.1298 3.7260 2.1433 1.4279 3.0385 3.3942 Table 3. Human evaluation results on 360 video generation. Figure 6. Qualitative ablation on the dual-branch design shows that dual-branch design generates plausible panoramic patterns compared to single-branch. we present the panorama frame with four projected perspective views (ϕ = 0, θ = [0, 90, 180, 270]) to make it easier to examine the panoramic structure plausibility. Fig. 5 shows that AnimateDiff+LatentLab360 and Follow-Your-Canvas fail to achieve 360 close-loop continuity (orange box) and produce mild-scale motion by observing the change of the canoe location (left-side) and animal size (right-side). 360DVD produces more distorted patterns and blurry visual appearance in both cases. In contrast, our Imagine360 achieves superior visual quality and plausible, obvious motion in the generated 360 videos. We highly recommend readers visit our project page for more stunning results. 4.4. Ablative Studies Ablation on dual-branch design. We show the qualitative results of our model with dual-branch design (full model), with single panorama branch and single perspective branch in Fig. 6. Either single panorama branch or single perspective branch alone is insufficient to achieve good spherical visual structure and panoramic motion patterns with limited high-quality 360 videos. Quantitative results in Tab. 2 also demonstrate the effectiveness of the proposed dual-branch video denoising branch. Further ablation on dual-branch fine-tuning can be found in Sec. B.1. Figure 7. Qualitative ablation on the antipodal mask shows improved reversed motion in the antipodal view from the antipodal activations. Figure 8. Qualitative ablation on the elevation-aware designs shows reduced artifacts from the elevation-aware designs . Ablation on antipodal mask. Fig. 7 illustrates the impact of the antipodal mask on 360 video generations. We show the perspective videos of θ = 0 and θ = 180 at frame 0, /3, 2T /3, . Thanks to the antipodal mask, as the camera moves forward in the input direction (θ = 0), we can observe clear backward motion in the antipodal views (θ = 180). Quantitative result in Tab. 2 also proves the effectiveness of adding antipodal activations in the crossdomain spherical mask. Ablation on elevation-aware designs. In Fig. 8, we show that not addressing changing elevations in the input anchor video leads to artifacts in the 360 videos scene geometry. Without elevation-aware handling, the generated video fails to cater to the elevation in the input anchor video, causing distortion in scene elements like mountains (yellow box). 8 Quantitative results in Tab. 2 further validates the benefits of elevation-aware designs. 5. Conclusion In this paper, we propose Imagine360, the first perspectiveto-360 video generation framework using video-based control to create 360 videos with rich and structured panoramic motions. Our contributions are three-fold: (1) we introduce dual-branch video denoising structure with panorama and perspective branches for 360 video generation with both global and local constraints; (2) we design an antipodal mask in cross-domain spherical attention to model long-range motion dependencies in panoramic videos; and (3) elevation-aware designs are introduced to handle varying elevation angles in diverse video inputs. Experiments show that Imagine360 produces 360 videos with superior video quality and panoramic motion plausibility. Limitations and future works. Imagine360 leverages offthe-shelf models to estimate elevation for inference, therefore the generated video could contain artifacts from inIn the next step, we plan accurate elevation estimations. to train specialized elevation estimation module with our 360 video dataset to improve the estimation accuracy."
        },
        {
            "title": "References",
            "content": "[1] Naofumi Akimoto, Yuhi Matsuo, and Yoshimitsu Aoki. Diverse plausible 360-degree image outpainting for efficient 3dcg background creation. CVPR, 2022. 3 [2] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. 3 [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. 3 [4] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. [5] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. arXiv preprint arXiv:2401.09047, 2024. 3 [6] Qihua Chen, Yue Ma, Hongfa Wang, Junkun Yuan, Wenzhe Zhao, Qi Tian, Hongmei Wang, Shaobo Min, Qifeng Chen, and Wei Liu. Follow-your-canvas: Higher-resolution video outpainting with extensive content generation. arXiv preprint arXiv:2409.01055, 2024. 3, 4, 6, 7, [7] Zhaoxi Chen, Guangcong Wang, and Ziwei Liu. Text2light: Zero-shot text-driven hdr panorama generation. ACM Transactions on Graphics (TOG), 41(6):116, 2022. 3 [8] Yen-Chi Cheng, Chieh Hubert Lin, Hsin-Ying Lee, Jian Ren, Sergey Tulyakov, and Ming-Hsuan Yang. Inout: Diverse image outpainting via GAN inversion. In CVPR, pages 11421 11430. IEEE, 2022. 3 [9] Zesen Cheng, Sicong Leng, Hang Zhang, Yifei Xin, Xin Li, Guanzheng Chen, Yongxin Zhu, Wenqi Zhang, Ziyang Luo, Deli Zhao, et al. Videollama 2: Advancing spatialtemporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. 6 [10] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 3 [11] Fanda Fan, Chaoxu Guo, Litong Gong, Biao Wang, Tiezheng Ge, Yuning Jiang, Chunjie Luo, and Jianfeng Zhan. Hierarchical masked 3d diffusion model for video outpainting. In Proceedings of the 31st ACM International Conference on Multimedia, pages 78907900, 2023. 3 [12] Mengyang Feng, Jinlin Liu, Miaomiao Cui, and Xuansong Xie. Diffusion360: Seamless 360 degree panoramic image generation based on diffusion models, 2023. 3, [13] Peng Gao, Le Zhuo, Ziyi Lin, Chris Liu, Junsong Chen, Ruoyi Du, Enze Xie, Xu Luo, Longtian Qiu, Yuhang Zhang, et al. Lumina-t2x: Transforming text into any modality, resolution, and duration via flow-based large diffusion transformers. arXiv preprint arXiv:2405.05945, 2024. 3 [14] Leon Gatys, Alexander Ecker, and Matthias Bethge. ImIn age style transfer using convolutional neural networks. Proceedings of the IEEE conference on computer vision and pattern recognition, pages 24142423, 2016. 11 [15] Xun Guo, Mingwu Zheng, Liang Hou, Yuan Gao, Yufan Deng, Chongyang Ma, Weiming Hu, Zhengjun Zha, Haibin Huang, Pengfei Wan, et al. I2v-adapter: general imageto-video adapter for video diffusion models. arXiv preprint arXiv:2312.16693, 2023. 3 [16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 3, 4, 5, 7, 8 [17] Yuwei Guo, Ceyuan Yang, Anyi Rao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Sparsectrl: Adding sparse controls to text-to-video diffusion models. In European Conference on Computer Vision, pages 330348. Springer, 2025. 3 [18] Takayuki Hara, Yusuke Mukuta, and Tatsuya Harada. Spherical image generation from few normal-field-of-view imIEEE Transactions ages by considering scene symmetry. on Pattern Analysis and Machine Intelligence, pages 115, 2022. 11 [19] Jingwen He, Tianfan Xue, Dongyang Liu, Xinqi Lin, Peng Gao, Dahua Lin, Yu Qiao, Wanli Ouyang, and Ziwei Liu. Venhancer: Generative space-time enhancement for video generation. arXiv preprint arXiv:2407.07667, 2024. 5, 9 [20] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In EMNLP (1), pages 75147528. Association for Computational Linguistics, 2021. 11 [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. 3 [22] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Imagen Poole, Mohammad Norouzi, David Fleet, et al. video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 3 [23] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. 3 [24] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. In CVPR, pages 2180721818. IEEE, 2024. 6, 7 [25] Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver Wang, Kevin Blackburn-Matzen, Matthew Sticha, and David Fouhey. Perspective fields for single image camera calibration. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17307 17316, 2023. 6, 12, [26] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 4 [27] Dilip Krishnan, Piotr Teterwak, Aaron Sarna, Aaron Maschinot, Ce Liu, David Belanger, and William T. Freeman. Boundless: Generative adversarial networks for image extension. In ICCV, pages 1052010529. IEEE, 2019. 3 [28] Yuseung Lee, Kunho Kim, Hyunjin Kim, and Minhyuk Sung. Syncdiffusion: Coherent montage via synchronized joint diffusions, 2023. 3, 11 [29] Jialu Li and Mohit Bansal. Panogen: Text-conditioned panoramic environment generation for vision-and-language navigation. In NeurIPS, 2023. 3 [30] Renjie Li, Panwang Pan, Bangbang Yang, Dejia Xu, Shijie Zhou, Xuanyang Zhang, Zeming Li, Achuta Kadambi, Zhangyang Wang, and Zhiwen Fan. 4k4dgen: Panoramic 4d generation at 4k resolution. arXiv preprint arXiv:2406.13527, 2024. 2, 3 [31] Jingwei Ma, Erika Lu, Roni Paiss, Shiran Zada, Aleksander Holynski, Tali Dekel, Brian Curless, Michael Rubinstein, and Forrester Cole. Vidpanos: Generative panoramic videos from casual panning videos. In SIGGRAPH Asia 2024 Conference Papers, 2024. 3 [32] Changgyoon Oh, Wonjune Cho, Yujeong Chae, Daehee Park, Lin Wang, and Kuk-Jin Yoon. BIPS: bi-modal indoor panorama synthesis via residual depth-aided adversarial learning. In ECCV (16), pages 352371. Springer, 2022. 3 [33] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: improving latent diffusion models for high-resolution image synthesis. CoRR, 2023. [34] Manuel Rey-Area, Mingze Yuan, and Christian Richardt. 360monodepth: High-resolution 360deg monocular depth In Proceedings of the IEEE/CVF Conference estimation. on Computer Vision and Pattern Recognition, pages 3762 3772, 2022. 5 [35] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3 [36] Hao Shi, Yifan Zhou, Kailun Yang, Xiaoting Yin, Ze Wang, Yaozu Ye, Zhe Yin, Shi Meng, Peng Li, and Kaiwei Wang. Panoflow: Learning 360 optical flow for surrounding temIEEE Transactions on Intelligent poral understanding. Transportation Systems, 24(5):55705585, 2023. 6 [37] Yang Song and Stefano Ermon. Generative modeling by esIn Advances in timating gradients of the data distribution. Neural Information Processing Systems. Curran Associates, Inc., 2019. 3 [38] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. 3 [39] Tomaˇs Souˇcek and Jakub Lokoˇc. Transnet v2: An effective deep network architecture for fast shot transition detection. arXiv preprint arXiv:2008.04838, 2020. [40] Gabriela Ben Melech Stan, Diana Wofk, Scottie Fox, Alex Redden, Will Saxton, Jean Yu, Estelle Aflalo, Shao-Yen Tseng, Fabio Nonato, Matthias Muller, and Vasudev Lal. Ldm3d: Latent diffusion model for 3d, 2023. 3 [41] Fu-Yun Wang, Xiaoshi Wu, Zhaoyang Huang, Xiaoyu Shi, Dazhong Shen, Guanglu Song, Yu Liu, and Hongsheng Li. Be-your-outpainter: Mastering video outpainting through input-specific adaptation. In European Conference on Computer Vision, pages 153168. Springer, 2025. 3 [42] Guangcong Wang, Yinuo Yang, Chen Change Loy, and Ziwei Liu. Stylelight: Hdr panorama generation for lighting estimation and editing, 2022. 3 [43] Hai Wang, Xiaoyu Xiang, Yuchen Fan, and Jing-Hao Xue. Customizing 360-degree panoramas through text-to-image diffusion models. In WACV, pages 49214931. IEEE, 2024. 3 [44] Jionghao Wang, Ziyu Chen, Jun Ling, Rong Xie, and Li Song. 360-degree panorama generation from few unregistered nfov images. In ACM Multimedia, pages 68116821. ACM, 2023. 3 [45] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3 [46] Qian Wang, Weiqi Li, Chong Mou, Xinhua Cheng, and Jian Zhang. 360dvd: Controllable panorama video generation In Proceedings of with 360-degree video diffusion model. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 69136923, 2024. 2, 3, 6, 7, 8, 12, 10 [47] Haoning Wu, Zicheng Zhang, Weixia Zhang, Chaofeng Chen, Liang Liao, Chunyi Li, Yixuan Gao, Annan Wang, Erli Zhang, Wenxiu Sun, Qiong Yan, Xiongkuo Min, Guangtao Zhai, and Weisi Lin. Q-align: Teaching lmms for visual scoring via discrete text-defined levels. In ICML. OpenReview.net, 2024. 6, 7, 11 [48] Songsong Wu, Hao Tang, Xiao-Yuan Jing, Haifeng Zhao, Jianjun Qian, Nicu Sebe, and Yan Yan. Cross-view panorama image synthesis. IEEE Trans. Multim., 25:35463559, 2023. 3 [49] Tianhao Wu, Chuanxia Zheng, and Tat-Jen Cham. Panodiffusion: 360-degree panorama outpainting via diffusion, 2024. 3, 11 [50] Jinbo Xing, Menghan Xia, Yong Zhang, Haoxin Chen, Xintao Wang, Tien-Tsin Wong, and Ying Shan. Dynamicrafter: Animating open-domain images with video diffusion priors. arXiv preprint arXiv:2310.12190, 2023. 3 [51] Shuai Yang, Jing Tan, Mengchen Zhang, Tong Wu, Yixuan Li, Gordon Wetzstein, Ziwei Liu, and Dahua Lin. Layerpano3d: Layered 3d panorama for hyper-immersive scene generation. arXiv preprint arXiv:2408.13252, 2024. 5 [52] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [53] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. arXiv preprint arXiv:2308.06721, 2023. 4 [54] Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, Xiaoshui Huang, Dinh Phung, Wanli Ouyang, and Jianfei Cai. Taming stable diffusion for text to 360 panorama image In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 6347 6357, 2024. 2, 3, 4, 5, 13 [55] Cheng Zhang, Qianyi Wu, Camilo Cruz Gambardella, Xiaoshui Huang, Dinh Phung, Wanli Ouyang, and Jianfei Cai. Taming stable diffusion for text to 360 panorama image In Proceedings of the IEEE/CVF Conference generation. on Computer Vision and Pattern Recognition, pages 6347 6357, 2024. 3 [56] Shiwei Zhang, Jiayu Wang, Yingya Zhang, Kang Zhao, Hangjie Yuan, Zhiwu Qin, Xiang Wang, Deli Zhao, and I2vgen-xl: High-quality image-to-video Jingren Zhou. arXiv preprint synthesis via cascaded diffusion models. arXiv:2311.04145, 2023. 3 [57] Yiming Zhang, Zhening Xing, Yanhong Zeng, Youqing Fang, and Kai Chen. Pia: Your personalized image animator via plug-and-play modules in text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 77477756, 2024. 3 A. Discussion on Panorama Image Outpaint bonus advantange of Imagine360 is that apart from panorama video outpainting, we also achieves superior performance for panorama image outpainting. We compared our method with state-of-the-art panorama image outpainting approaches, including Diffusion360 [12], PanoDiffusion [49] and SIG-SS [18]. We use the first frame of video as the input image and extract the first frame of our outpainted video as our result for panorama image outpainting. For quantitative comparison, we adopt Intra-Style [14, 28] metric to evaluate the panorama style coherence; CLIP [20] to measure the alignment between the panorama and the input text prompts; IQA [47] measures the overall image quality. Tab. shows our method achieves the best performance among the compared methods across all metrics. We also show the qualitative comparison in Fig. A, with red dashed box indicating the input image. The results of Diffusion360 [12] show less consistency that its newly generated pixels having different style with the know pixels. SIG-SS [18] is Gan-based methods and its generations exhibits over-smoothness compared diffusion-based approaches. PanoDiffusion [49] focuses on indoor scenes and does not generalize well to outdoor scene. In contrast, Imagine360 outpaints more consistent, high-quality and aesthetic panorama compared to other approaches. Method Intra-Style(103) CLIP IQA Diffusion360 [12] PanoDiffusion [49] SIG-SS [18] Ours 3.40 3.46 2.06 0.99 27.49 23.19 26.26 29.12 0.77 0.72 0.48 0.78 Table A. Quantitative comparison with the state-of-the-art Panorama Outpainting methods. B. Extended Ablations B.1. Ablation on Dual-branch Fine-tuning Strategy We ablate on the fine-tuning strategy that trains our framework to produce high-quality generations with limited data and computational resources. As shown in Tab. B, due to the gap between the domain of the pretrained model and the panorama video domain, removing the spatial LoRA layer in any branch will reduce the video quality to certain extent. Thus, compared with directly fine-tuning all spatial modules, applying LoRA to both branches proves to be more cost-effective choice. For motion modeling, to effectively capture panoramic motion patterns, it is intuitive to add motion LoRA layer or finetune the motion module on the panorama branch. We further conduct comparative experiment between directly fine-tuning motion module and applying motion LoRA. Results show that compared to the spatial domain gap, the temporal domain gap between the pretrained model and panoramic videos is more substantial. Consequently, simply utilizing LoRA for fine-tuning can result in overfitting, ultimately degrading the quality of generated video. 11 Figure A. Qualitative comparisons between Imagine360 and the state-of-the-art panorama outpainting methods. Pano Spatial LoRA Pers Spatial LoRA Pano Motion Module Motion LoRA #1 (ours) #2 #3 # IQ AQ MS SC VQA 0.7487 0.7474 0.7305 0. 0.6439 0.6235 0.6246 0.6067 0.9806 0.9756 0.9720 0.9758 0.9710 0.9664 0.9657 0.9558 0.8672 0.8584 0.8553 0.8385 Table B. Ablative study on dual-branch fine-tune strategy across Image Quality (IQ), Aesthetic Quality (AQ), Motion Smoothness (MS), Subject Consistency (SC) and VQA metrics. because the adopted off-the-shelf elevation predictor [25] is primarily designed for images rather than videos, and it handles videos as sequence of images without handling the noise and temporal inconsistency between frames. Consequently, it produces inconsistent and oscillatory predictions for videos without additional smoothing. By incorporating this smoothing technique, the adverse effects of these fluctuations are mitigated to significant extent, thereby enhancing the overall quality of the generated video. B.2. Ablation on Extended Web Data As we collect additional panorama video data from web in our training, we also ablate on the effect of the extended data. We train 360DVD [46] on our training data and report the performance in Tab. C. Results show that the performance of 360DVD [46] improves across all metrics using our data but remains weak compared to Imagine360, demonstrating the effectiveness of both our data and our proposed framework designs. B.3. Ablation on Elevation Estimation Smoothing As demonstrated in Tab. D, the absence of elevation estimation smoothing leads to noticeable decline in the quality of the generated video on each quantitative metric. This is 12 Method Imaging Quality Aesthetic Quality Motion Smoothness Subject Consistency VQA Ours 360DVD [46] 360DVD [46] + Our Data 0.7487 0.5537 0. 0.6439 0.4745 0.5381 0.9806 0.9701 0.9739 0.9710 0.9629 0.9696 0.8672 0.5867 0.7573 Table C. Ablative study on our extended panorama video dataset. Method Imaging Quality Aesthetic Quality Motion Smoothness Subject Consistency VQA Ours w/o smoothing 0.7487 0.7408 0.6439 0.6082 0.9806 0.9706 0.9710 0.9512 0.8672 0.8443 Table D. Ablative study on Elevation Estimation Smoothing. elevation estimation with model that do not use both of the designs to see the effect from elevation handling. C. Additional Implementation Details C.1. Data Collection As the collected web videos contain noisy content, we further explain the details of our data clearning strategies as follows. We filter out static videos based on extracted optical flow. The flow values are first normalized to the range of [0, 1], with an average flow value calculated for each frame. The videos that contain less than 10% of frames with > 0.1 average flow value are considered static and removed from the dataset. C.2. Inference Settings. We choose PerspectiveFields [25] to estimate the elevation of per-frame and use LinearRegression to smooth the pitch sequence. Additionally, we modified the Venhancer [19] to keep the 360 close-loop property to super-resolution the output panorama video for better 360 VR immersive experience in the webpage. Note that we do not use video SR of any kind in our comparison and ablation experiments. C.3. Experiment Settings Due to space limit in the main paper, we further add explainations of our experiment settings. For human evaluation, we ask the users to evaluate the 360 videos across three dimensions: graphics quality, structure plausibility, and temporal coherence. We provide both the 360 videos and its four perspective projection videos with ϕ = 0, θ = [0, 90, 180, 270]. Graphics quality refers to the clarity and detail richness of the panorama and perspective video frames. Structure plausibility refers to the level of distortion in each perspective projections. Temporal coherence refers to the motion consistency and subject consistency: whether theres object suddenly appears or disappears, etc. For ablations, in the ablation on antipodal mask, we compare the model variant using both the antipodal mask and the directly-mapped mask with the model variant using only the directly-mapped mask. Note that we do not ablate on the directly mapped mask because its effectiveness in dualdesign structure was already addressed in PanFusion [54]. In the ablation on elevation-aware design ablation, we compare our full model using both the elevation sampling and"
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Shanghai AI Laboratory",
        "Shanghai Jiao Tong University",
        "The Chinese University of Hong Kong"
    ]
}