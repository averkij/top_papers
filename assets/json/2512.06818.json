{
    "paper_title": "MeshSplatting: Differentiable Rendering with Opaque Meshes",
    "authors": [
        "Jan Held",
        "Sanghyun Son",
        "Renaud Vandeghen",
        "Daniel Rebain",
        "Matheus Gadelha",
        "Yi Zhou",
        "Anthony Cioppa",
        "Ming C. Lin",
        "Marc Van Droogenbroeck",
        "Andrea Tagliasacchi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, a mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On Mip-NeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2x faster and using 2x less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/."
        },
        {
            "title": "Start",
            "content": "MeshSplatting: Differentiable Rendering with Opaque Meshes Jan Held1,2 Sanghyun Son3 Renaud Vandeghen1 Daniel Rebain4 Matheus Gadelha6 Yi Zhou6 Anthony Cioppa1 Ming C. Lin3 Marc Van Droogenbroeck1 Andrea Tagliasacchi2,5 1University of Li`ege 4University of British Columbia 2Simon Fraser University 5University of Toronto 3University of Maryland 6Adobe Research 5 2 0 2 7 ] . [ 1 8 1 8 6 0 . 2 1 5 2 : r Figure 1. MeshSplatting produces connected mesh composed only of opaque triangles, achieving high-quality novel view synthesis through end-to-end optimization, with 2 training speed-up and 2 lower memory usage over current state-of-the-art methods. (a) Our representation is compatible with standard game engines, requiring no a-posteriori conversion and/or custom rendering routines for transparency, and natively supports (b) physical interactions, (c) interactive walkthroughs, and (d) ray tracing. (e) MeshSplatting enables straightforward object extraction, allowing scene elements to be directly exported and imported into game engines."
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Primitive-based splatting methods like 3D Gaussian Splatting have revolutionized novel view synthesis with real-time rendering. However, their point-based representations remain incompatible with mesh-based pipelines that power AR/VR and game engines. We present MeshSplatting, mesh-based reconstruction approach that jointly optimizes geometry and appearance through differentiable rendering. By enforcing connectivity via restricted Delaunay triangulation and refining surface consistency, MeshSplatting creates end-to-end smooth, visually high-quality meshes that render efficiently in real-time 3D engines. On MipNeRF360, it boosts PSNR by +0.69 dB over the current state-of-the-art MiLo for mesh-based novel view synthesis, while training 2 faster and using 2 less memory, bridging neural rendering and interactive 3D graphics for seamless real-time scene interaction. The project page is available at https://meshsplatting.github.io/. Recent advances in novel view synthesis, like 3D Gaussian Splatting [26], have enabled photo-realistic reconstruction of extremely complex scenes. 3D Gaussian Splatting encodes scenes with millions of 3D Gaussian primitives, achieving real-time rendering while also training efficiently. However, while 3DGS renders with high visual fidelity, Gaussian primitives are not immediately compatible with classical graphics pipelines used in simulators, games, and AR/VR applications, as these are typically based on polygonal meshes. Therefore, integrating 3DGS in classical pipelines either requires engineering rendering engines [30, 38, 49] and simulators [36] to support them. However, this is non-trivial since 3DGS relies on sorting and alpha blending, preventing the use of standard techniques like depth buffers and occlusion culling [1, 22]. Another line of work is converting Gaussian radiance fields into meshes [14, 20, 54]. Although 1 somewhat effective, all of these conversion approaches rely on complex post-processing and typically lead to loss of visual quality, as the conversion step is non-differentiable. Rather than relying on conversion a-posteriori, we consider optimizing mesh directly by making the rasterization process differentiable. Early approaches, such as Kato et al. [25] and Liu et al. [32], enabled differentiable optimization of solid polygonal surfaces, but they required very careful initialization for optimization to converge effectively and are mainly limited to object-level setups rather than large realistic scenes. Instead, Held et al. [16] recently proposed to optimize (potentially transparent) triangles via volumetric rendering, therefore effectively replacing Gaussians with triangles. However, when their triangles are rendered in game engine, noticeable drop in visual quality occurs, as game engines assume triangles to be opaque. Moreover, Held et al. [16] outputs soup of triangles, rather than connected polygonal mesh, often needed for physics-based simulation. Key Contributions. We introduce MeshSplatting to address all the aforementioned limitations: (i) an end-to-end optimization of mesh-based scene representations that retains visual quality while training 2 faster than current state-of-the-art methods; (ii) rather than polygon soup, we generate connected mesh by refining the vertex locations of restricted Delaunay triangulation; (iii) triangles are naturally connected to each other, and quantities stored within vertices are smoothly interpolated across each triangle; (iv) the optimization is aware that the triangles should be opaque, and therefore allowing direct high-quality rendering in standard game engines (see Fig. 1), opening the door for classical techniques like the use of depth buffers and occlusion culling [1, 22]. MeshSplatting achieves higher visual fidelity and captures finer geometric detail compared to modern mesh-based novel-view synthesis approaches [14, 20, 53, 54]. It is the first method to reconstruct large-scale real-world meshes end-to-end, directly producing connected, opaque, and colored triangle meshes without post-hoc extraction. Our representation can be directly imported into standard game engines, enabling wide range of downstream applications including physics-based simulation, interactive walkthroughs, ray tracing, and scene editing, some of which are illustrated in Figure 1. 2. Related work Differentiable rendering enables end-to-end optimization by propagating image-based losses back to scene parameters, allowing for the learning of explicit representations such as point clouds [12, 25], voxel grids [10], polygonal meshes [25, 32, 33], and more recently, Gaussian primitives [26]. The advent of 3D Gaussian Splatting [26] showed that it is possible to fit millions of anisotropic Gaussians in minutes, enabling real-time rendering with high fidelity. Since then, various directions have been explored to improve the Gaussian primitive, including the use of 2D Gaussians [20], generalized Gaussians [15, 45], alternative kernels [21], and learnable basis functions [6]. Other works moved beyond Gaussians entirely, investigating different primitives such as smooth 3D convexes [17], linear primitives [47], sparse voxel fields [44], or radiance foams [11]. More recently, Held et al. [16] advocated for the comeback of triangles, the most classical primitive in computer graphics. Several researchers have explored this direction, proposing triangle-based representations for efficient scene modeling [4, 24]. However, existing triangle-based methods usually result in an unstructured triangle soup, with no connectivity between adjacent triangles, and they fail to produce opaque triangles at the end of training, limiting usability in downstream applications. We propose method that enforces connectivity, resulting in mesh of opaque triangles directly compatible with game engines. Mesh reconstruction from images. Implicit and explicit methods have made significant progress in reconstructing 3D scenes, but they remain largely incompatible with traditional game engines that primarily rely on mesh-based rasterization with depth buffers. Some methods propose strategies to convert implicit radiance fields into meshes. BakedSDF [51] learns neural signed distance field and appearance and then bakes them into textured triangle mesh. Binary Opacity Fields [40] drives densities toward near binary opacities so surfaces can be extracted as mesh, and MobileNeRF [7] distills NeRF into compact set of textured polygons. However, these methods introduce overhead and increase the overall training time. Several methods have built upon 3DGS and proposed ways to extract mesh from an optimized Gaussian scene. 2DGS [20] and RaDe-GS [54] rely on Truncated Signed Distance Fields for mesh extraction. Other approaches extract meshes by sampling surface-aligned Gaussian level set followed by Poisson reconstruction [13], or by defining Gaussian opacity level set and applying Marching Tetrahedra on Gaussian-induced tetrahedral grids [53]. All of these, however, treat mesh extraction as separate postprocessing step, decoupled from the optimization process. More recently, MiLo [14] integrates surface mesh extraction directly into the optimization, jointly refining both the mesh and the Gaussian representation. However, while MiLo optimizes the mesh geometry during training, color still has to be learned separately. Another line of work employs differentiable meshes for 3D reconstruction [42, 43], but these methods primarily target synthetic objects and do not generalize to real-world scenes. In contrast, MeshSplatting directly optimizes opaque triangles together with their vertex colors, making the result immediately compatible 2 with any game engine without additional post-processing steps. 3. Methodology We now overview the key components of our method. Section 3.1 reviews Triangle Splatting [16], which is used as volume-renderable primitives for differentiable rendering in this work. We give an overview of our MeshSplatting representation in Section 3.2, and describe the optimization stages to convert the triangle soup into connected mesh in Section 3.3. For the representation to be natively compatible with game engines, triangles need to be opaque, and the process to achieve this objective is detailed in Section 3.4. We conclude by detailing other optimization details in Section 3.5, such as densification/pruning, and training losses. 3.1. Background In Triangle Splatting [16], each triangle Tm is defined by three vertices viR3, color cm, smoothness parameter σm and an opacity om. The rasterization process begins by projecting each 3D vertex vi of triangle Tm onto the image plane using standard pinhole camera model1. To determine the influence of triangle on pixel and make the splatting process differentiable, the signed distance field ϕ of the 2D triangle in image space is defined as: ϕ(p) = max i{1,2,3} Li(p), Li(p) = ni + di, (1) where ni are the unit normals of the triangle edges pointing outside the triangle, and di are offsets such that the triangle is given by the zero-level set of the function ϕ. The signed distance field ϕ thus takes positive values outside the triangle, negative values inside, and equals zero on its boundary. The window function is then defined as: (cid:18) I(p) ="
        },
        {
            "title": "ReLU",
            "content": "(cid:19)(cid:19)σ (cid:18) ϕ(p) ϕ(s) (2) with sR2 be the incenter of the projected triangle (i.e., the point inside the triangle with minimum signed distance). Note how the indicator evaluates to 1 at the triangle incenter, 0 at the boundary and 0 outside the triangle. σ is smoothness parameter that controls the transition between the incenter and boundary of the triangle. More specifically, as σ0, the representation converges to solid triangle, while larger values of σ yield smooth window function that gradually increases from zero at the boundary to one at the center. This triangle parameterization has two main limitations: triangles remain isolated without vertex sharing, and treating σ and as independent free parameters prevents them from becoming fully opaque after training. 1Unlike 3DGS, linearization of Gaussian projections is required [55], since perspective projection preserves linearity, 3D triangles remain triangles in 2D screen space. Figure 2. Mesh parametrization. (left) In triangle soup, each triangle Tm is defined independently by three vertices vi, vj, vk, color cm, smoothness parameter σm, and an opacity om, without sharing vertices with neighboring triangles. (right) MeshSplatting parameterize triangle Tm through shared vertex set, where each vertex vi stores xi, yi, zi, ci, and oi. Each triangle is defined by the three indices in the vertex set that compose it. During the backward pass, gradients from all adjacent triangles are accumulated at shared vertices. The smoothness parameter σ is shared across all triangles. 3.2. Vertex-sharing triangle representation In MeshSplatting, we define our mesh vertices as = {vi R3 = 1, . . . , }, (3) with denoting their cardinality. Similarly to Son et al. [42] each vertex is parameterized as vi = (xi, yi, zi, ci, oi), where (xi, yi, zi) R3 denotes its 3D position, ci R3 the vertex color and oi [0, 1] the vertex opacity.2 triangle is defined by triplet of vertices Tm = {vi, vj, vk}, its opacity is set to oTm = min(oi, oj, ok), and its color at point inside the triangle is obtained by interpolating vertex colors with barycentric coordinates. During differentiation, vertex positions, colors, and opacities therefore receive the accumulated gradients from all triangles connected to it, as shown in Figure 2 (right). 3.3. From soups to meshes Our optimization executes in two stages that gradually convert an unstructured into structured representation, as illustrated in Figure 3. This design exploits the fact that, early in training, unstructured representations are easier to optimize, as they impose less constraints on the representation. Stage 1. Triangle soup optimization . We start by taking as input set of posed images and corresponding camera parameters obtained from structure-from-motion (SfM) [41], which also provides sparse point cloud. For each 3D point, we initialize an equilateral triangle centered at that point, with its size proportional to the average distance to its three 2After training, the opacity parameter is discarded, and all triangles are treated as fully opaque for compatibility with standard game engines. 3 Stage 1. Triangle soup optimization Stage 2. Mesh creation & refinement R l o a. SfM point cloud initialization b. transparent & disconnected triangles a. restricted Delaunay triangulation b. opaque & connected triangles Figure 3. From triangle soups to meshes. (1a) We initialize semi-transparent triangles and scale them based on local density. (1b) We optimize semi-transparent triangle soup without shared vertices, leading to disconnected triangles. (2a) Applying restricted Delaunay triangulation restores global connectivity but introduces geometric artifacts and loss of visual quality, as vertex colors no longer accurately align with the underlying geometry. (2b) The final fine-tuning stage refines the connected mesh, producing smooth surfaces, accurate geometry, and restoring the visual fidelity lost during triangulation. Using only opaque triangles, our method achieves high visual quality compared to the semi-transparent and isolated triangle soup. nearest neighbors, and random orientation. All triangles are initially defined as semi-transparent (we selected oi=0.28 via hyper-parameter tuning). We begin the optimization with this unstructured triangle soup, i.e. without any connectivity or manifold constraints between triangles. Each triangle is optimized independently and can move freely under the influence of image-space gradients. This unconstrained formulation behaves similarly to point-based splatting, enabling rapid coverage of the visible scene and fast adaptation to local geometry and appearance. Note that, while similar, this is not the same as executing Held et al. [16], as we optimize interpolated vertex quantities within triangle, rather than keeping them uniform. Stage 2. Mesh creation & refinement. To transform our triangle soup into connected mesh, we execute restricted Delaunay triangulation on the triangle soup [8]. This operation first compute standard Delaunay tetrahedralization, and then identifies tetrahedral faces whose dual Voronoi edges intersect the surface of the input triangle soup. The restricted Delaunay triangulation generates mesh that approximates the surface, while at the same time, maintaining Delaunay properties (high quality meshing) locally restricted to it. Note that, in contrast to other reconstruction [28, 34, 35], restricted Delaunay triangulation does not introduce new vertices or modify their position. Instead, it reuses the optimized vertices directly, preserving both spatial accuracy and learned appearance. Given the mesh connectivity from our restricted Delaunay triangulation, we then continue optimization so to fine-tune both vertex locations, as well as their appearance. As vertices are shared among adjacent triangles, gradients from neighboring faces are accumulated at shared vertices, ensuring that each vertex is updated consistently according to all incident triangles; see Figure 2. We do not need to introduce additional facets or vertices, as the unstructured triangle soup optimization has already produced sufficiently dense set of triangles to capture all spatial regions accurately. After this fine-tuning stage, we obtain fully opaque mesh capable to render the scene at high photometric quality in conventional rendering engines. In the final iterations of training, we enable supersampling, allowing even small triangles to receive gradients and be properly optimized. 3.4. Optimizing meshes with opaque triangles Optimizing meshes composed only of opaque triangles introduces new challenges compared to classical NeRF/3DGS optimization. To enable gradient propagation through occlusions and allow the representation to be optimized effectively, the representation must remain semi-transparent in the early phases of training. There are two degrees of freedom that we can control in this regard: the per-vertex opacity parameter o, and the smoothness parameter σ. Opacity parameter scheduling. We optimize opacity freely during the initial 5k iterations. Subsequently, we reparameterize opacity so that the optimizer is encouraged to make triangles more opaque. We achieve this by reparameterizing opacity values as o(o) = Ot + (1 Ot) sigm(o), (4) where sigm(.) is the sigmoid function, and scheduling Ot 4 Figure 4. Window parameter scheduling. To ensure stable gradient flow during training, we begin with smooth triangles (σ=1.0, left) and linearly decrease σ throughout training, resulting in sharper triangles by the end. We visualize σ for prototypical triangle at the beginning and end of each optimization stage. over time. Note that if Ot = 0 the sigmoid parameterizes opacities smoothly between 0 and 1. Conversely, if Ot = 1 all the opacities are mapped to value of 1 (fully opaque triangles). By linearly increasing Ot from 0 to 1 during optimization, we can control this behavior smoothly. Window parameter scheduling. We initialize the window parameter σ=1.0, corresponding to linear transition from the incenter to the triangle boundary, and treat it as single parameter, shared across all triangles. Throughout the training stages detailed in Section 3.3, σ is linearly annealed from 1.0 0.0001, ensuring strong gradient flow in the early stages while converging to opaque triangles at the end of the optimization process; see Figure 4. Note that this is in direct contrast to Triangle Splatting, as Held et al. [16] lets every triangle optimize its own window parameter. 3.5. Optimization details We now detail our densification/pruning strategies, training losses, and observations about our rendering process. Densification. As the initial triangle soup may not be sufficiently dense, we adapt the ideas from 3DGS-MCMC [29] to spawn additional triangles. At each densification step, candidate triangles are selected by sampling from probability distribution constructed directly from their opacity using Bernoulli sampling. Following Held et al. [16], new triangles are generated through midpoint subdivision: the midpoints of the three edges of selected triangle are connected, splitting it into four smaller triangles. The new midpoints are added to the vertex set and assigned the average color and opacity of their two adjacent vertices. We already exploit the connectivity in the early stages, as it drastically reduces the number of newly created vertices, resulting in only 6 new vertices after split when connected, compared to 12 in triangle-soup setting. Pruning. At iteration 5k (Stage 1, just before opacity scheduling starts), we prune all triangles with opacity o<0.2, eliminating roughly 70% of primitives. During the rest of Stage 1, we monitor the maximum volume rendering blending weight w=T across views, and prune whenever w<Ot, hence eliminating occluded triangles as the representation becomes more opaque. While pruning is disabled during Stage 2, we perform final pruning pass over all training views at the end of training to remove triangles that were never rendered. Training losses. We optimize the 3D vertex positions vi, opacity oi, and spherical harmonic color coefficients ci of all vertices. Our training loss combines the photometric L1 and LD-SSIM terms from 3DGS [26], the opacity loss Lo from Kheradmand et al. [29], the depth alignment loss Lz (detailed below), the normal loss Ln from [20], and the depth loss Ld from Kerbl et al. [27]: = L3DGS + βoLo + βzLz + βnLn + βdLd . (5) We follow Kerbl et al. [27], and employ Depth Anything v2 [50] to align the predicted depths using their scale-andshift procedure. The normal loss Ln can be supervised either by an external normal estimation network [18], or the self-supervised normal regularization from 2DGS [20], or both. We employ both supervision sources in all experiments, except for the mesh quality evaluation (Section 4.3), where we demonstrate that MeshSplatting also performs effectively even in self-supervised setting. Depth alignment loss. To promote the creation of manifolds, we align triangles to the observed depth map using vertex-to-surface depth loss. To achieve this, for each rendered vertex vi with predicted depth zi and screen coordinates (xi, yi), we sample the predicted depth from the rendered depth map, and (robustly via L1 losses) penalize the depth difference: Lz = 1 . Unlike losses like normal consistency [3, 9] or Laplacian [3, 9, 48] that rely on local mesh connectivity, this formulation acts on each vertex independently. Rendering equation. The final color of each image pixel is computed by accumulating contributions from all overlapping triangles in depth order: i=1 zi (cid:80)N C(p) = (cid:80)N n=1 cTn oTn I(p) . Analogously to Chen et al. [7], at the end of training, this simplifies to C(p)=cTn I(p), so that only single evaluation per pixel is required, significantly accelerating the rendering process (i.e. over-drawing is zero). i=1 (1 oTiI(p)) (cid:16)(cid:81)n1 (cid:17) 4. Experiments We compare our method to concurrent approaches on MipNeRF360 [2] and Tanks and Temples (T&T) [31]. We evaluate the visual quality using standard metrics: SSIM, PSNR, and LPIPS. Our the total number of used vertices and training times are reported on an NVIDIA A100 (40GB). Following Guedon et al. [14], we focus our evaluation on the task of Mesh-Based Novel View Synthesis, whose evaluation protocol attempts to measure how well reconstructed meshes reproduce complete scenes. Existing"
        },
        {
            "title": "Characteristics",
            "content": "Mip-NeRF360 dataset Mesh Color Connect Ready SN LP IP SSIM SN LP IP SSIM Tanks & Temples 3DGS[26] Triangle Splatting [16] 2DGS [20] GOF [53] RaDe-GS [54] MiLo [14] Triangle Splatting [16]"
        },
        {
            "title": "MeshSplatting",
            "content": "- - - 27.21 27.16 15.36 20.78 23.56 24.09 21.05 24.78 0.214 0.191 0.474 0.465 0.361 0.323 0.462 0. 0.815 0.814 0.498 0.573 0.668 0.688 0.558 0.728 - 23.14 23.14 2M 14.23 33M 21.69 31M 20.51 21.46 7M 17.27 3M 3M 20.52 0.183 0.143 0.485 0.326 0.344 0.348 0.402 0.287 0.841 0. 0.569 0.690 0.659 0.706 0.600 0.745 - 16M 12M 10M 4M 6M 2M Table 1. Mesh-based novel view synthesis on the Mip-NeRF360 dataset. MeshSplatting significantly outperforms all concurrent methods both in visual quality and in compactness, requiring far fewer vertices to achieve superior results. Mesh indicates whether method directly produces mesh (vs. requiring post-processing). Color denotes whether the mesh is already colored or requires some form of post-processing (e.g., coloring by fine-tuning). Connect specifies whether the final mesh consists of connected component. Ready means the output is directly usable in standard game engines without custom rendering shaders. with only opaque triangles. datasets such as DTU, MipNeRF360, and Tanks&Temples are limited: MipNeRF360 lacks ground-truth geometry, DTU only contains simple objects, and T&T provides sparse annotations with only foreground regions. As solution, we measure novel view synthesis quality through the visual consistency between mesh renderings and reference images, capturing (i) geometric alignment and surface artifacts, (ii) mesh completeness, and (iii) background reconstruction, even when ground-truth geometry is unavailable. Finally, to quantitatively assess the mesh quality of our method, we compute the Chamfer Distance on DTU. Baselines. We compare our method against Triangle Splatting [16], and meshes derived from MiLo [14], 2DGS [20], Gaussian Opacity Fields (GOF) [53], and RaDe-GS [54]. For MiLo, surface mesh extraction is integrated into the optimization itself, so no additional post-processing is reIn contrast, 2DGS, GOF, and RaDe-GS rely quired. on mesh extraction as post-processing operation. Furthermore, all these methods require an additional postprocessing stage to color the mesh, achieved by training neural color field for 5k iterations; see MiLo for details [14]. For Triangle Splatting, we use the opaque triangles version, as it produces game engine outputs without additional post-processing needed. For reference, we also compare against 3DGS [26] to highlight the contrast in rendering quality between volumetric and mesh-based novel view synthesis. Qualitatively, we compare against current state-of-the-art MiLo [14] and Triangle Splatting [16], which represents the closest line of work to ours. Implementation details. We set the spherical harmonics to degree 3, which yields 51 parameters per vertex (48 from the SH coefficients and 3 from the vertex position) and 3 parameters per triangle (the vertex indices). In comparison, single 3D Gaussian requires 59 parameters. 4.1. Mesh-based NVS Table 1 and Figure 5 Table 1 reports quantitative results on the Mip-NeRF360 and Tanks & Temples datasets for mesh-based novel view synthesis. Compared to 2DGS and Triangle Splatting, it our method uses similar number of vertices, yet achieves 410 dB higher PSNR and significantly lower LPIPS. Compared to GOF, RaDe-GS and MiLo, MeshSplatting uses 2 to 10 times fewer vertices while obtaining significantly higher SSIM and lower LPIPS. In terms of LPIPS (the metric that best correlates with human visual perception), MeshSplatting significantly outperforms all concurrent methods. On the Mip-NeRF360 dataset, our method achieves substantially higher PSNR than GOF, RaDe-GS and MiLo. On T&T, GOF and MiLo attain higher PSNR but exhibit noticeably lower SSIM and higher LPIPS scores. This suggests that although GOF and MiLo produce highly detailed meshes, their renderings contain more artifacts, which degrade perceptual quality and lead to worse LPIPS and SSIM scores. We illustrate this qualitatively in Figure 5 and provide more examples in the supplementary material. MeshSplatting reconstructs fine structures and details more accurately and produces less noisy renderings. Additionally, note that 2DGS, GOF, and RaDe-GS require two additional post-processing steps after training: first extracting mesh, and then coloring it. MiLo directly outputs mesh, but still relies on post-processing stage to texture the mesh. These extra steps limit the practicality of such methods, and increase their overall complexity. In contrast, we directly produce colored opaque meshes that are immediately compatible with any game engine, without requiring additional steps. 4.2. Training speed & memory Table 2 We report the average training time and final mesh size on the Mip-NeRF360 dataset. MeshSplatting trains in only 48 6 Ground Truth MeshSplatting MiLo Triangle Splatting y k T r Figure 5. Qualitative results. Comparison of our method with ground truth, the current state-of-the-art MiLo [14] and opaque Triangle Splatting [16]. Our approach produces renderings that are closer to the ground truth, with sharper details and finer structures (see the Bicycle spokes), and with fewer artifacts (see the table in the Truck scene). More visualizations are available in the supplementary material. Method GOF RaDe-GS MiLo Train FPS (HD) FPS (Full HD) Memory 74m 84m 106m OOM OOM 170 220 OOM OOM 160 190 1.5GB 1.1GB 253MB 100MB MeshSplatting 48m Table 2. Speed & memory on MipNeRF-360. MeshSplatting achieves faster training and lower memory usage than concurrent methods. FPS were measured on costumer MacBook M4. minutes on average, achieving 3555% speedup over concurrent mesh-yielding methods. Our optimized restricted Delaunay triangulation runs in under two minutes, contributing negligibly to the total training time. MiLo performs Delaunay triangulation at every iteration, whereas we run it only once. This results in training time of 106 minutes for MiLo, compared to just 48 minutes for MeshSplatting. The final mesh representation produced by MeshSplatting is substantially more compact, with only 100 MB, which corresponds to 2.515 reduction compared to concurrent methods. GOD and RaDe-GS require 1.5 GB and 1.1 GB of memory, respectively, making them impractical for real-world applications. This compactness enables MeshSplatting to run efficiently even on consumer hardware, significantly opening its applicability to lightweight rendering and real-time use-cases. MeshSplatting renders 25% faster on consumer M4 MacBook compared to existing approaches. GOF and RaDe-GS exceed the devices Figure 6. Object segmentation. With MeshSplatting, objects can be easily extracted or removed from reconstructed scene, in this case the flower pot from the Garden scene. From left to right: generated RGB image of the object, estimated surface normals, and resulting mesh representation. memory capacity, resulting in an out-of-memory error. 4.3. Surface reconstruction For evaluation on the DTU dataset, we use only selfsupervised regularization (βd to zero) to ensure fair comparison with other methods and to show that MeshSplatting also performs well in self-supervised setting. Although we designed MeshSplatting for mesh-based novel-view synthesis in large and complex real scenes, rather than surface reconstruction, it achieves mesh quality comparable to concurrent methods. Across the 15 scenes, MeshSplatting attains the lowest Chamfer distance in 5 of them. detailed comparison table is provided in the supplementary material. 7 0 1 3 4+ Mean PSNR LPIPS SSIM"
        },
        {
            "title": "Restricted Delaunay\nWith pruning\nFinal mesh",
            "content": "1% 2% 5 % 11% 81% 9% 22% 25% 19% 25% 2% 9% 16% 20% 53% 6.1 2.5 3.7 Table 3. Connectivity. Distribution of triangle connectivity on the Garden scene. The final mesh mostly consists of triangles connected to three or more neighboring triangles, indicating wellconnected mesh."
        },
        {
            "title": "Baseline",
            "content": "w/o SH w/o Ld w/o Lz w/o Ln 24.78 -2.07 +0.05 +0.02 +0.10 0.31 0.728 +0.06 -0.04 -0.01 -0. -0.069 +0.006 +0.002 +0.004 Table 4. Ablations (Mip-NeRF360). We assess the impact of each design choice by removing each one of them individually. 4.4. Applications We now illustrate few application demos that are quite difficult to implement with semi-transparent representations, yet almost trivial once the scene is represented with opaque meshes like in MeshSplatting. Physical simulation. As our representation contains no semi-transparent primitives, the triangles may directly be interpreted as hard surfaces for the purpose of physical simulation. We demonstrate this by using an off-the-shelf nonconvex mesh collider: the one provided in the Unity game engine. With no post-processing, our mesh can be loaded into Unity, and used for physics interaction with dynamic objects and characters. Figure 1 showcases selection of downstream applications, with additional visualizations in the supplementary material. Object segmentation. Current 3D Gaussian Splatting methods for object extraction or removal [5, 52] face fundamental challenge: single pixel is rendered by accumulating the contributions of many primitives. This makes it non-trivial to decide whether primitive belongs to given object. To address this, prior work learns object associations during optimization [5, 52]. In contrast, MeshSplatting requires no additional tweak. Since each pixel is covered by exactly one triangle, mapping objects from image space to 3D space becomes straightforward: given 2D mask of an object, all triangles contributing to pixels within that mask are directly identified as part of the object. By iterating over all training views, we recover the complete set of triangles belonging to the object. The object masks are generated using SAMv2 [39], which enables the selection of single or multiple objects that can then be removed or extracted from scene with minimal complexity. Figure 6 shows qualitative example of the extracted flower pot from the Garden scene of the Mip-NeRF360 dataset. Beyond object removal and extraction, this capability allows the scene to be segmented into distinct sub-meshes by disconnecting triangles belonging to different objects, enabling structured and object-aware 3D scene editing. 4.5. Analysis Mesh connectivity Table 3. We report the distribution of triangles according to their connectivity with neighboring (a) no regularization (b) w/o Ld (c) ours (baseline) (d) stronger regularization Figure 7. Regularization vs. mesh quality. (a) Without any regularization, the rendered views have high visual quality, but the underlying geometry is inaccurate. (b) The normal loss Ln encourages smoother surfaces, yet without the depth regularization Ld, few local regions show minor geometric inaccuracies. (c) Our baseline model achieves both smooth and geometrically consistent surfaces. (d) Increasing the regularization strength yields even smoother geometry, but the visual fidelity decreases as spherical harmonics fail to capture fine appearance details. triangles. After the restricted Delaunay triangulation, most triangles are already well connected, with about 92% having at least three neighbors. If pruning is applied immediately after this step, large number of triangles are removed because the Delaunay mesh contains many very small triangles that do not contribute to rendering, significantly reducing the overall connectivity. Instead, pruning is performed only after training. We make final pass over all training views and remove all triangles that did not contribute to any rendered image. On average, each triangle is connected to approximately 3.7 triangles, and fewer than 2% of triangles remain isolated. Overall, most triangles are connected to exactly three neighboring triangles. More analyzes are provided in the supplementary material. 8 Ablations Table 4 and Figure 7. While the regularization terms Ld, Lz, and Ln slightly reduce visual quality, they significantly improve geometric accuracy and yield smoother surfaces. The decrease in visual fidelity arises because the spherical harmonics representation cannot fully compensate for the reduced geometric flexibility, nor can it cheat by positioning triangles in non-physical configurations to better reproduce local colors or textures. This is further confirmed by replacing spherical harmonics with simple RGB colors, which results in an average drop of about 2 PSNR. This highlights that expressive color representations are essential for maintaining high visual fidelity with fully opaque triangles. Future work could decouple geometry and appearance. By employing more expressive appearance model, such as neural textures, the representation could achieve smoother and more accurate geometry without losing appearance information, thereby preserving high visual fidelity even under strong smoothness constraints. More ablations are provided in the supplementary material. 5. Conclusions We introduce differentiable framework for end-to-end optimization of mesh-based scene representations. By reformulating the triangle parameterization to enable vertex sharing, our method produces connected meshes while maintaining high visual fidelity. redefined training strategy moves the optimization toward opaque triangles and connectivity, resulting in unified representation that combines high quality appearance and accurate geometry within compact, real-time-renderable mesh. MeshSplatting bridges radiance field optimization with traditional graphics pipelines, paving the way for the practical integration of neural scene representations into interactive VR applications, game engines, and simulation environments. Acknowledgments. We thank Bernhard Kerbl and George Kopanas for their helpful feedback and for proofreading the paper. J. Held is funded by the F.R.S.-FNRS. The present research benefited from computational resources made available on Lucia, the Tier-1 supercomputer of the Walloon Region, infrastructure funded by the Walloon Region under the grant agreement n1910247."
        },
        {
            "title": "References",
            "content": "[1] Tomas Akenine-Moller, Eric Haines, Naty Hoffman, Angelo Pesce, Michał Iwanicki, and Sebastien Hillaire. Real-Time Rendering. AK Peters/CRC Press, Boca Raton, Florida, USA, 4 edition, 2018. 1, 2 [2] Jonathan T. Barron, Ben Mildenhall, Dor Verbin, Pratul P. Srinivasan, and Peter Hedman. Mip-NeRF 360: Unbounded anti-aliased neural radiance fields. In IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 54605469, New Orleans, LA, USA, 2022. 5 [3] Mark Boss, Zixuan Huang, Aaryaman Vasishta, and Varun Jampani. SF3D: Stable fast 3D mesh reconstruction with UV-unwrapping and illumination disentanglement. In IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 1624016250, Nashville, TN, USA, 2025. 5 [4] Nathaniel Burgdorfer and Philippos Mordohai. Radiant triangle soup with soft connectivity forces for 3D reconstruction and novel view synthesis. arXiv, abs/2505.23642, 2025. 2 [5] Jiazhong Cen, Jiemin Fang, Chen Yang, Lingxi Xie, Xiaopeng Zhang, Wei Shen, and Qi Tian. Segment any 3D In AAAI Conf. Artif. Intell., pages 19711979, Gaussians. Philadelphia, PA, USA, 2025. 8 [6] Haodong Chen, Runnan Chen, Qiang Qu, Zhaoqing Wang, Tongliang Liu, Xiaoming Chen, and Yuk Ying Chung. Beyond Gaussians: Fast and high-fidelity 3D splatting with linear kernels. arXiv, abs/2411.12440:114, 2024. [7] Zhiqin Chen, Thomas Funkhouser, Peter Hedman, and Andrea Tagliasacchi. MobileNeRF: Exploiting the polygon rasterization pipeline for efficient neural field rendering on mobile architectures. In IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 1656916578, Vancouver, Can., 2023. 2, 5, 15 [8] Siu-Wing Cheng, Tamal Krishna Dey, and Jonathan Shewchuk. Delaunay Mesh Generation. Chapman and Hall/CRC, 2013. 4 [9] Jaehoon Choi, Rajvi Shah, Qinbo Li, Yipeng Wang, Ayush Saraf, Changil Kim, Jia-Bin Huang, Dinesh Manocha, Suhib Alsisan, and Johannes Kopf. LTM: Lightweight textured mesh extraction and refinement of large unbounded scenes for efficient storage and real-time rendering. In IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 5053 5063, Seattle, WA, USA, 2024. 5 [10] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 54915500, New Orleans, LA, USA, 2022. 2 [11] Shrisudhan Govindarajan, Daniel Rebain, Kwang Moo Yi, and Andrea Tagliasacchi. Radiant foam: Real-time differentiable ray tracing. arXiv, abs/2502.01157, 2025. [12] Markus Gross and Hanspeter Pfister. Point-Based Graphics. Morgan Kauffmann Publ. Inc., San Francisco, CA, USA, 2007. 2 [13] Antoine Guedon and Vincent Lepetit. SuGaR: Surfacealigned Gaussian splatting for efficient 3D mesh reconstruction and high-quality mesh rendering. In IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 53545363, Seattle, WA, USA, 2024. 2 [14] Antoine Guedon, Diego Gomez, Nissim Maruani, Bingchen Gong, George Drettakis, and Maks Ovsjanikov. MILo: Mesh-in-the-loop Gaussian splatting for detailed and efficient surface reconstruction. arXiv, abs/2506.24096, 2025. 1, 2, 5, 6, 7, 15 [15] Abdullah Hamdi, Luke Melas-Kyriazi, Jinjie Mai, Guocheng Qian, Ruoshi Liu, Carl Vondrick, Bernard Ghanem, and Andrea Vedaldi. GES: Generalized exponential splatting for efficient radiance field rendering. In IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 1981219822, Seattle, WA, USA, 2024. [16] Jan Held, Renaud Vandeghen, Adrien Deli`ege, Daniel Hamdi, Abdullah Rebain, Silvio Giancola, Anthony Cioppa, Andrea Vedaldi, Bernard Ghanem, Andrea Tagliasacchi, and Marc Van Droogenbroeck. Triangle splatting for real-time radiance field rendering. arXiv, abs/2505.19175, 2025. 2, 3, 4, 5, 6, 7, 15 [17] Jan Held, Renaud Vandeghen, Abdullah Hamdi, Adrien Deli`ege, Anthony Cioppa, Silvio Giancola, Andrea Vedaldi, Bernard Ghanem, and Marc Van Droogenbroeck. 3D convex splatting: Radiance field rendering with 3D smooth convexes. In IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 2136021369, Nashville, TN, USA, 2025. 2 [18] Mu Hu, Wei Yin, Chi Zhang, Zhipeng Cai, Xiaoxiao Long, Hao Chen, Kaixuan Wang, Gang Yu, Chunhua Shen, and Shaojie Shen. Metric3D v2: versatile monocular geometric foundation model for zero-shot metric depth and surface normal estimation. IEEE Trans. Pattern Anal. Mach. Intell., 46(12):1057910596, 2024. 5 [19] Yuanming Hu, Tzu-Mao Li, Luke Anderson, Jonathan Ragan-Kelley, and Fredo Durand. Taichi. ACM Trans. Graph., 38(6):116, 2019. 13 [20] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2D Gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH Conf. Pap., pages 111, Denver, CO, USA, 2024. 1, 2, 5, 6, 15 [21] Yi-Hua Huang, Ming-Xian Lin, Yang-Tian Sun, Ziyi Yang, Xiaoyang Lyu, Yan-Pei Cao, and Xiaojuan Qi. Deformable radial kernel splatting. In IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 2151321523, Nashville, TN, USA, 2025. 2 [22] John Hughes, Andries van Dam, Morgan McGuire, David Sklar, James D. Foley, Steven Feiner, and Kurt Akeley. Computer Graphics: Principles and Practice. Addison-Wesley, 3 edition, 2014. 1, [23] Rasmus Jensen, Anders Dahl, George Vogiatzis, Engil Tola, and Henrik Aanaes. Large scale multi-view stereopsis evaluation. In IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 406413, Columbus, OH, USA, 2014. 15 [24] Changjian Jiang, Kerui Ren, Linning Xu, Jiong Chen, Jiangmiao Pang, Yu Zhang, Bo Dai, and Mulin Yu. HaloGS: Loose coupling of compact geometry and Gaussian splats for 3D scenes. arXiv, abs/2505.20267, 2025. 2 [25] Hiroharu Kato, Yoshitaka Ushiku, and Tatsuya Harada. Neural 3D mesh renderer. In IEEE/CVF Conf. Comput. Vis. Pat10 tern Recognit. (CVPR), pages 39073916, Salt Lake City, UT, USA, 2018. 2 Int. Conf. Learn. Represent. (ICLR), pages 144, Singapore, 2025. [26] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkuehler, and George Drettakis. 3D Gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):114, 2023. 1, 2, 5, 6, 15 [27] Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer, Alexandre Lanvin, and George Drettakis. hierarchical 3D Gaussian representation for real-time rendering of very large datasets. ACM Trans. Graph., 43(4): 115, 2024. 5 [28] Muhammad Umar Karim Khan and Chong-Min Kyung. Poisson mixture model for high speed and low-power background subtraction. Smart Sensors and Systems, pages 123, 2020. 4 [29] Shakiba Kheradmand, Daniel Rebain, Gopal Sharma, Weiwei Sun, Jeff Tseng, Hossam Isack, Abhishek Kar, Andrea Tagliasacchi, and Kwang Moo Yi. 3D Gaussian splatting as Markov chain Monte Carlo. In Adv. Neural Inf. Process. Syst. (NeurIPS), pages 8096580986, Vancouver, Can., 2024. 5 [30] KIRI Innovations. 3DGS Render: 3D Gaussian splatting renderer for Blender. https://github.com/KiriInnovation / 3dgs - render - blender - addon, 2025. 1 [31] Arno Knapitsch, Jaesik Park, Qian-Yi Zhou, and Vladlen Koltun. Tanks and temples: benchmarking large-scale scene reconstruction. ACM Trans. Graph., 36(4):113, 2017. 5 [32] Shichen Liu, Weikai Chen, Tianye Li, and Hao Li. Soft rasterizer: differentiable renderer for image-based 3D reasoning. In IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pages 77077716, Seoul, South Korea, 2019. [33] Matthew M. Loper and Michael J. Black. OpenDR: An approximate differentiable renderer. In Eur. Conf. Comput. Vis. (ECCV), pages 154169, Zurich, Switzerland, 2014. 2 [34] Nissim Maruani, Roman Klokov, Maks Ovsjanikov, Pierre Alliez, and Mathieu Desbrun. VoroMesh: Learning watertight surface meshes with Voronoi diagrams. In IEEE/CVF Int. Conf. Comput. Vis. (ICCV), pages 1451914528, Paris, Fr., 2023. 4 [35] Nissim Maruani, Maks Ovsjanikov, Pierre Alliez, and Mathieu Desbrun. PoNQ: neural QEM-based mesh representation. In IEEE/CVF Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 36473657, Seattle, WA, USA, 2024. 4 [36] Vismay Modi, Nicholas Sharp, Or Perel, Shinjiro Sueda, and David I. W. Levin. Simplicits: Mesh-free, geometry-agnostic elastic simulation. ACM Trans. Graph., 43(4):111, 2024. 1 [37] Tomas Moller and Ben Trumbore. Fast, minimum storage ray/triangle intersection. ACM SIGGRAPH 2005 Courses, page 7, 2005. 13 [38] Aras Pranckeviˇcius. UnityGaussianSplatting: 3D Gaussian splatting in Unity. https://github.com/aras-p/ UnityGaussianSplatting, 2023. 1 [39] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, Eric Mintun, Junting Pan, Kalyan Vasudev Alwala, Nicolas Carion, ChaoYuan Wu, Ross Girshick, Piotr Dollar, and Christoph Feichtenhofer. SAM 2: Segment anything in images and videos. In [40] Christian Reiser, Stephan Garbin, Pratul Srinivasan, Dor Verbin, Richard Szeliski, Ben Mildenhall, Jonathan Barron, Peter Hedman, and Andreas Geiger. Binary opacity grids: Capturing fine geometric detail for mesh-based view synthesis. ACM Trans. Graph., 43(4):114, 2024. 2 [41] Johannes L. Schonberger and Jan-Michael Frahm. Structurefrom-motion revisited. In IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 41044113, Las Vegas, NV, USA, 2016. [42] Sanghyun Son, Matheus Gadelha, Yang Zhou, Matthew Fisher, Zexiang Xu, Yi-Ling Qiao, Ming C. Lin, and Yi Zhou. DMesh++: An efficient differentiable mesh for complex shapes. arXiv, abs/2412.16776, 2024. 2, 3 [43] Sanghyun Son, Matheus Gadelha, Yang Zhou, Zexiang Xu, Ming C. Lin, and Yi Zhou. DMesh: differentiable mesh representation. In Adv. Neural Inf. Process. Syst. (NeurIPS), pages 1203512077, Vancouver, Can., 2024. 2 [44] Cheng Sun, Jaesung Choe, Charles Loop, Wei-Chiu Ma, and Yu-Chiang Frank Wang. Sparse voxels rasterization: RealIn IEEE/CVF time high-fidelity radiance field rendering. Conf. Comput. Vis. Pattern Recognit. (CVPR), pages 16187 16196, Nashville, TN, USA, 2025. 2 [45] Maria Taktasheva, Lily Goli, Alessandro Fiorini, Zhen Li, Daniel Rebain, and Andrea Tagliasacchi. 3D Gaussian flats: arXiv, Hybrid 2D/3D photometric scene reconstruction. abs/2509.16423, 2025. 2 [46] Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, Stefan J. van der Walt, Matthew Brett, Joshua Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C. J. Carey, Ilhan Polat, Yu Feng, Eric W. Moore, Jake VanderPlas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian Henriksen, E. A. Quintero, Charles R. Harris, Anne M. Archibald, Antˆonio H. Ribeiro, Fabian Pedregosa, Paul van Mulbregt, Aditya Vijaykumar, Alessandro Pietro Bardelli, Alex Rothberg, Andreas Hilboll, Andreas Kloeckner, Anthony Scopatz, Antony Lee, Ariel Rokem, C. Nathan Woods, Chad Fulton, Charles Masson, Christian Haggstrom, Clark Fitzgerald, David A. Nicholson, David R. Hagen, Dmitrii V. Pasechnik, Emanuele Olivetti, Eric Martin, Eric Wieser, Fabrice Silva, Felix Lenders, Florian Wilhelm, G. Young, Gavin A. Price, Gert-Ludwig Ingold, Gregory E. Allen, Gregory R. Lee, Herve Audren, Irvin Probst, Jorg P. Dietrich, Jacob Silterra, James T. Webber, Janko Slaviˇc, Joel Nothman, Johannes Buchner, Johannes Kulick, Johannes L. Schonberger, Jose Vinıcius de Miranda Cardoso, Joscha Reimer, Joseph Harrington, Juan Luis Cano Rodrıguez, Juan Nunez-Iglesias, Justin Kuczynski, Kevin Tritz, Martin Thoma, Matthew Newville, Matthias Kummerer, Maximilian Bolingbroke, Michael Tartre, Mikhail Pak, Nathaniel J. Smith, Nikolai Nowaczyk, Nikolay Shebanov, Oleksandr Pavlyk, Per A. Brodtkorb, Perry Lee, Robert T. McGibbon, Roman Feldbauer, Sam Lewis, Sam Tygier, Scott Sievert, Sebastiano Vigna, Stefan Peterson, Surhud More, Tadeusz Pudlik, Takuya Oshima, Thomas J. Pingel, Thomas P. Robitaille, Thomas Spura, Thouis R. Jones, Tim Cera, Tim Leslie, Tiziano Zito, Tom Krauss, Utkarsh Upadhyay, Yaroslav O. Halchenko, and Yoshiki Vazquez-Baeza. SciPy 1.0: fundamental algorithms for scientific computing in Python. Nat. Methods, 17(3):261272, 2020. 13 [47] Nicolas von Lutzow and Matthias Nießner. LinPrim: Linear primitives for differentiable volumetric rendering. arXiv, abs/2501.16312, 2025. 2 [48] Nanyang Wang, Yinda Zhang, Zhuwen Li, Yanwei Fu, Wei Liu, and Yu-Gang Jiang. Pixel2Mesh: Generating 3D mesh models from single RGB images. arXiv, abs/1804.01654, 2018. 5 [49] XVERSE. XScene-UEPlugin: 3D Gaussian splatting plugin for Unreal Engine 5. https://github.com/xverseengine/XScene-UEPlugin, 2024. 1 [50] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything V2. In Adv. Neural Inf. Process. Syst. (NeurIPS), pages 137, Vancouver, Can., 2024. 5 [51] Lior Yariv, Peter Hedman, Christian Reiser, Dor Verbin, Pratul P. Srinivasan, Richard Szeliski, Jonathan T. Barron, and Ben Mildenhall. BakedSDF: Meshing neural SDFs for real-time view synthesis. In ACM SIGGRAPH Conf. Proc., pages 19, Los Angeles, CA, USA, 2023. [52] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit anything in 3D scenes. In Eur. Conf. Comput. Vis. (ECCV), pages 162179, 2024. 8 [53] Zehao Yu, Torsten Sattler, and Andreas Geiger. Gaussian opacity fields: Efficient adaptive surface reconstruction in unbounded scenes. ACM Trans. Graph., 43(6):113, 2024. 2, 6, 15 [54] Baowen Zhang, Chuan Fang, Rakesh Shrestha, Yixun Liang, Xiaoxiao Long, and Ping Tan. RaDe-GS: Rasterizing depth in Gaussian splatting. arXiv, abs/2406.01467, 2024. 1, 2, 6, 15 [55] Matthias Zwicker, Hanspeter Pfister, Jeroen van Baar, and IEEE Trans. Vis. Comput. Markus Gross. EWA splatting. Graph., 8(3):223238, 2002. 3 12 MeshSplatting: Differentiable Rendering with Opaque Meshes"
        },
        {
            "title": "Supplementary Material",
            "content": "5.1. Methodology. 1 and 1 and Mesh creation & refinement - Figure 8. We provide an additional visualization that illustrates both the visual and geometric improvements achieved across the different stages. Figure 8 shows how the geometry evolves by rendering the mesh without color in Blender. Fine-tuning the output of the restricted Delaunay triangulation not only enhances visual fidelity but also leads to more accurate geometry. Restricted Delaunay triangulation.. As mentioned earlier, to construct the restricted Delaunay triangulation we first compute the standard Delaunay tetrahedralization of the input vertices, using the implementation in SciPy [46]. For each triangular face in this tetrahedralization, we identify the two tetrahedra 2 that are adjacent to ; faces on the boundary of the tetrahedralization that have only single incident tetrahedron are discarded. We then compute the circumcenters of 2 , which are vertices of the dual Voronoi diagram of the Delaunay tetrahedralization. The dual edge associated with is obtained by connecting the circumcenters of 1 and 2 . After computing the dual edges associated with the Delaunay faces, we determine which of these edges intersect the triangles in our optimized triangle soup. To perform intersection tests efficiently, we build bounding volume hierarchy over these triangles. During traversal, when testing an edge in against BVH node, we first check for overlap between their axis-aligned bounding boxes. If the node is internal, we continue the traversal to its children; if it is leaf node, we apply precise raytriangle intersection test based on the MollerTrumbore algorithm [37]. Whenever an intersection is detected, we mark the corresponding Delaunay face as part of the output mesh. We implement this detection pipeline in Taichi [19], whose automatic CPU/GPU parallelization yields substantial speedup. Hyerparameters - Table 5. The code will be released together with all hyperparameters used, ensuring that all results are fully reproducible. For completeness, we also list the most important hyperparameters here. 5.2. Addition experimental results Detailed results - Table 6 & Table 7 & Table 8 & Figure 10 & Figure 11. We provide the per-scene results of MeshSplatting to facilitate detailed comparison and reproducibility. In addition, we provide further qualitative results demonstrating that MeshSplatting produces renderings with fewer artifacts and less noise compared to the current state of the art, MiLo. While MiLo generates high-quality meshes and therefore achieves strong PSNR on the T&T"
        },
        {
            "title": "Indoor",
            "content": "feature learning rate opacity learning rate vertex pos. learning rate densification start densification end densification interval start pruning pruning threshold mesh creation 0.0016 0.03 0.0015 500 10000 500 4000 0.235 11000 0.004 0.05 0.0015 500 10000 500 3000 0.235 βz βn βd βo 0.00025 0.0001 0.01 2e-06 0.00025 0.0001 0.0 0.0 Table 5. Hyperparameters List of the most important hyperparameters."
        },
        {
            "title": "Truck Train",
            "content": "PSNR 22.32 18.72 LPIPS 0.237 0.337 SSIM 0.799 0.693 Table 6. Per scene LPIPS, PSNR, and SSIM scores for the Truck and Train scenes of the T&T dataset."
        },
        {
            "title": "Bicycle Flowers Garden Stump Treehill",
            "content": "PSNR LPIPS SSIM 23.04 0.348 0.641 19.34 0.417 0.480 24.70 0.217 0.762 24.78 0.316 0.678 20.53 0.428 0. Table 7. Per-scene PSNR, LPIPS, and SSIM scores for outdoor MipNeRF360 scenes."
        },
        {
            "title": "Room Counter Kitchen Bonsai",
            "content": "PSNR 28.52 LPIPS 0.271 SSIM 0.873 26.51 0.279 0.846 27.42 0.227 0.858 28.19 0.294 0.876 Table 8. Per-scene PSNR, LPIPS, and SSIM scores for indoor MipNeRF360 scenes. dataset, its outputs exhibit more noise, leading to higher LPIPS and lower SSIM compared to MeshSplatting. 13 Stage 1. Triangle soup optimization Stage 2. Mesh creation & refinement h M structure-from-motion point cloud transparent & disconnected triangles restricted Delaunay triangulation opaque & connected triangles Figure 8. From soups to triangle meshes. The geometry improves significantly after refining the mesh. Surface reconstruction - Table 10. MeshSplatting attains the best Chamfer distance on 5 of the 15 scenes. It achieves significantly better mean score than Triangle Splatting and 3DGS. Compared to methods specifically tailored for mesh extraction, our results remain competitive and are on par with 2DGS. Even though our focus is mesh-based novel view synthesis, these results show that MeshSplatting also produces accurate surface meshes. 2M 3M 4M 5M PSNR 22.14 +0.15 +0.36 +0.46 LPIPS 0.39 -0.02 -0.05 -0.06 Table 9. Number of vertices vs visual quality. MeshSplatting scales effectively with the number of vertices, showing consistent improvements in visual quality as the vertex count increases. All improvements are shown relative to 2M. Impact of triangle count on visual quality - Table 9. We evaluate the visual quality across different triangle counts on the outdoor scenes of MipNeRF360, as these scenes offer more room for improvement compared to indoor ones. Increasing the number of triangles consistently enhances visual quality, with clear gains in both PSNR and especially in LPIPS. The results show that MeshSplatting scales well with triangle count, with visual quality improving consistently as more triangles are used. Object extraction - Figure 9. Given 2D mask from SAMv2, we identify all visible triangles contributing to the masked pixels and aggregate them across all training views, yielding the complete set of triangles belonging to the selected object. The entire process takes up to two minutes, depending on the scene and the number of input views. Concretely, we first store the object masks for each training Figure 9. Object extraction. Additional visual examples demonstrating the object extraction capabilities of MeshSplatting. From left to right: generated RGB image of the object, estimated surface normals, and resulting mesh representation. image, then iterate over all views to mark triangles as part of the object whenever they render at least one pixel within the mask. The extracted triangles can then be removed or exported as standalone submesh without retraining. 5.3. Additional ablations - Table 11 Hard pruning step. An effective pruning strategy is crucial for achieving high visual quality with opaque primitives. Without the hard pruning step at iteration 5k, visual quality degrades noticeably. Unnecessary triangles remain in the scene and can no longer be removed. W/o stage 2. Running restricted Delaunay triangulation directly on the SfM point cloud produces connected mesh that is not yet geometrically consistent, leaving many regions, particularly in the background, uncovered. Training with only connected triangles restricts the degrees of freedom of both vertices and triangles, making optimization considerably harder and leading to drop in visual quality. 14 Method 37 40 55 63 65 83 97 105 106 110 118 122 Mean 3DGS [26] 2DGS [20] GOF [53] RaDe-GS [54] MiLo [14] Triangle Splatting [16] 2.14 0.48 0.50 0.46 0.43 0.98 1.53 0.91 0.82 0.73 0.74 1.07 2.08 0.39 0.37 0.33 0.34 1. 1.68 0.39 0.37 0.38 0.37 0.51 3.49 1.01 1.12 0.79 0.80 1.67 2.21 0.83 0.74 0.75 0.74 1.44 1.43 0.81 0.73 0.76 0.70 1.17 2.07 1.36 1.18 1.19 1.21 1.32 2.22 1.27 1.29 1.22 1.22 1. 1.75 0.76 0.68 0.62 0.66 0.98 1.79 0.70 0.77 0.70 0.62 0.96 2.55 1.40 0.90 0.78 0.80 1.11 1.53 0.40 0.42 0.36 0.37 0.56 1.52 0.76 0.66 0.68 0.76 0.93 1.50 0.52 0.49 0.47 0.48 0. MeshSplatting 0.77 0.72 0.74 0.60 0. 1.00 0.81 1.09 1.19 0.58 0. 0.93 0.63 0.66 0.59 1.96 0.80 0.74 0.68 0.68 1.06 0. Table 10. Chamfer distance on the DTU dataset [23]. MeshSplatting achieves performance comparable to concurrent methods. Ground Truth MeshSplatting MiLo Triangle Splatting r n T r Figure 10. More qualitative results on the Tanks and Temples dataset. MeshSplatting reconstructs more details and finer structures compared to concurrent works. While MiLo achieves higher PSNR, MeshSplatting produces fewer artifacts and less noisy renderings, which results in significantly lower LPIPS and higher SSIM. PSNR LPIPS SSIM"
        },
        {
            "title": "Baseline",
            "content": "w/o hard pruning w/o stage 2 w/o supersampling w/o pruning w/o SH prune w/o conn. w/o Ld w/o Lz w/o Ln w/o sigma decay cosine opacity schedule cosine σ schedule 24.78 -0.67 -8.56 -0.80 -0.62 -2.07 -0.19 +0.05 +0.02 +0. -7.96 -0.20 -0.76 0.31 0.728 +0.02 +0.25 +0.04 +0.05 +0.06 +0.01 -0.04 -0.01 -0.02 +0.27 +0.01 +0. -0.021 -0.260 -0.040 0.045 -0.07 -0.01 +0.006 +0.002 +0.004 -0.329 -0.01 -0.028 Table 11. Detailed ablations (Mip-NeRF360). We isolate the impact of each design choice by removing them individually. W/o supersampling. To reduce aliasing from opaque triangles, similar to Chen et al. [7], we render at the target resolution, and then downsample to the final resolution using area interpolation, which averages over input pixel regions to implement box anti-aliasing filter. By disabling supersampling during the final training iterations and testing, visual quality decreases. This occurs because the representation uses fully opaque triangles, and metrics such as PSNR penalize small pixel-level shifts. In contrast, when supersampling followed by average downsampling is applied, transitions between triangles become smoother, leading to more continuous transitions and higher visual quality. We applied anti-aliased rendering for both training and testing. W/o pruning. When pruning is based only on opacity rather than the maximum blending weight w, many unnecessary triangles remain in the scene. As these triangles become opaque, they introduce artifacts and reduce visual quality. 15 Ground Truth MeshSplatting MiLo Triangle Splatting y B s p S r Figure 11. More qualitative results on the MipNeRf-360. MeshSplatting is able to reconstruct fine details with only opaque triangles. W/o SH. When training with only RGB colors, the visual quality drops significantly. The opaque and connected triangles struggle to accurately reproduce fine texture details. Real-world scenes exhibit complex spatial variations in color and illumination, which would require an impractically large number of triangles to model using RGB alone. Incorporating spherical harmonics enables each triangle to capture part of this variation, resulting in noticeably improved appearance. This observation suggests that more expressive appearance model could further enhance visual fidelity. Future work could explore learning per-triangle textures either jointly during training or as post-processing refinement, as done in recent mesh-based methods. Such an approach could further narrow the visual quality gap between 3D Gaussian-based representations and fully opaque triangle-based ones. Pruning w/o connectivity.. During the first stage, we exploit triangle connectivity by applying midpoint subdivision while ensuring that all four newly created triangles remain connected. This approach reduces the number of introduced vertices by half: subdividing each triangle independently would yield 12 vertices, whereas maintaining shared connectivity requires only 6. By connecting triangles, we can (a) Without Lz (b) With Lz Figure 12. Impact of the Lz regularization term. With the regularization we obtain smoother surfaces. allocate larger portion of the memory budget to rendering more vertices and triangles, which leads to improved visual quality. W/o Lz - Figure 12. Even with normal regularization, some triangles remain misoriented. By pushing the vertices closer to the surface, we improve both mesh smoothness and normal consistency. The vertex regularization term Lvertex 5.5. Limitations Limitations. MeshSplatting achieves high visual quality and accurate reconstruction in regions where the initial point cloud is dense. In contrast, background areas with sparse coverage still exhibit incomplete geometry and reduced fidelity. Moreover, when moving outside the orbit of training views, the visual quality degrades. While the softness and opacity of Gaussian-based approaches may still provide slightly plausible results in such cases, our use of opaque triangles makes the artifacts more pronounced. Future work could address those limitation by initializing with more complete point cloud, or by incorporating alternative additional representation such as triangulated sky dome. Furthermore, transparent objects such as glasses or bottles remain difficult to represent using only opaque triangles, as illustrated in Fig. 13. Finally, MeshSplatting does not explicitly enforce watertightness; however, the resulting meshes can be directly used in many downstream applications, offering an effective trade-off between simplicity, usability, and high visual quality. Future work could incorporate additional regularization terms to constrain vertex motion and prevent self-intersections or overlaps. Figure 13. Limitations. Accurately recovering backgrounds (left), particularly under limited viewpoints, and handling transparent objects (right) remain challenging. reduces artifacts and promotes smoother surfaces, particularly in background regions where supervision is typically weaker. Cosine scheduler. Compared to linear schedule for increasing opacity and decreasing σ, we also experimented with cosine scheduler. However, performance, particularly for σ, drops noticeably. The main reason is that the linear schedule maintains higher σ values for longer, allowing gradients to remain stable during the final iterations, whereas the cosine scheduler reaches low σ values earlier, causing gradients to vanish prematurely. Finally, we also analyze the impact of initialization. Instead of starting from soft triangles (i.e., σ = 1.0) that gradually converge toward solid ones, we evaluate the case where training begins directly with fully solid triangles. In this configuration, gradients can only flow through the opacity term, causing optimization to stagnate. As result, both performance and visual quality degrade noticeably due to severe vanishing gradients. 5.4. Mesh connectivity. The resulting mesh exhibits vertexface ratio of 0.48, which is close to the theoretical 0.5 expected for closed manifold surface, indicating that the global mesh topology is already compact and near-manifold despite local nonmanifold regions remaining. For reference, completely isolated mesh with no vertices-sharing, would have ratio of 1.5. We further analyze vertex connectivity by measuring the number of incident triangles (vertex valence) across the reconstructed mesh. The mean valence is 6.2 with median of 5, which aligns with the expected value of 6 for regular triangular tessellations but with broader distribution. Roughly 35% of vertices exhibit low valence ( 4), indicating remaining boundary regions and small disconnected fragments, while about 30% fall within the regular interior range (58). small fraction (< 10%) shows high valence (> 12), corresponding to locally dense or merged zones. This distribution shows that while the mesh is largely connected and compact, it remains not fully watertight."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Simon Fraser University",
        "University of British Columbia",
        "University of Liège",
        "University of Maryland",
        "University of Toronto"
    ]
}