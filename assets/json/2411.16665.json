{
    "paper_title": "Edge Weight Prediction For Category-Agnostic Pose Estimation",
    "authors": [
        "Or Hirschorn",
        "Shai Avidan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Category-Agnostic Pose Estimation (CAPE) localizes keypoints across diverse object categories with a single model, using one or a few annotated support images. Recent works have shown that using a pose graph (i.e., treating keypoints as nodes in a graph rather than isolated points) helps handle occlusions and break symmetry. However, these methods assume a static pose graph with equal-weight edges, leading to suboptimal results. We introduce EdgeCape, a novel framework that overcomes these limitations by predicting the graph's edge weights which optimizes localization. To further leverage structural priors, we propose integrating Markovian Structural Bias, which modulates the self-attention interaction between nodes based on the number of hops between them. We show that this improves the model's ability to capture global spatial dependencies. Evaluated on the MP-100 benchmark, which includes 100 categories and over 20K images, EdgeCape achieves state-of-the-art results in the 1-shot setting and leads among similar-sized methods in the 5-shot setting, significantly improving keypoint localization accuracy. Our code is publicly available."
        },
        {
            "title": "Start",
            "content": "Edge Weight Prediction For Category-Agnostic Pose Estimation"
        },
        {
            "title": "Tel Aviv University",
            "content": "https://orhir.github.io/edge_cape/ 4 2 0 2 5 2 ] . [ 1 5 6 6 6 1 . 1 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Category-Agnostic Pose Estimation (CAPE) localizes keypoints across diverse object categories with single model, using one or few annotated support images. Recent works have shown that using pose graph (i.e., treating keypoints as nodes in graph rather than isolated points) helps handle occlusions and break symmetry. However, these methods assume static pose graph with equal-weight edges, leading to suboptimal results. We introduce EdgeCape, novel framework that overcomes these limitations by predicting the graphs edge weights which optimizes localization. To further leverage structural priors, we propose integrating Markovian Structural Bias, which modulates the self-attention interaction between nodes based on the number of hops between them. We show that this improves the models ability to capture global spatial dependencies. Evaluated on the MP-100 benchmark, which includes 100 categories and over 20K images, EdgeCape achieves state-of-the-art results in the 1shot setting and leads among similar-sized methods in the 5-shot setting, significantly improving keypoint localization accuracy. Our code is publicly available. 1. Introduction 2D pose estimation, which involves identifying the locations of key semantic parts within an image, is fundamental problem in computer vision. From human pose estimation [4, 11, 45] to animal tracking and vehicle localization [30, 37], accurate pose estimation is essential for both academic research and industrial applications. Traditional approaches mainly focused on category-specific models that are tailored to predefined keypoints and predefined object categories. These models achieve high accuracy in some domains but struggle when encountering categories that lack annotated training data. This limitation has sparked interest in flexible models that generalize beyond the fixed categories and keypoints seen in training. To address these challenges, Category-Agnostic Pose Support Data CapeFormer [36] (Only Nodes) GraphCape [14] (Unweighted Graph) Ours (Weighted Graph) Figure 1. EdgeCape. Given support image, keypoints definition, and skeletal relations (purple), our model localizes the keypoints on query image (green). Previous methods treat keypoints as isolated (CapeFormer) or use unweighted graphs (GraphCape). We, in contrast, predict weighted graphs that lead to better localization. Estimation (CAPE) has emerged as promising solution [42]. CAPE seeks to enable keypoint localization for arbitrary keypoints and for any object category using only few annotated support images, allowing single model to generalize across diverse object types. By significantly reducing the need for extensive data collection and retraining for each new category or keypoint definition, CAPE offers versatile and cost-effective approach to pose estimation. While early CAPE methods treat keypoints as isolated entities, recent works [14, 33] have leveraged user-defined structural pose graphs to break symmetry, manage occlusions, and preserve object structure. However, these methods rely on unweighted graphs, limiting their effectiveness. 1 ization, we draw on recent advances in Graph Neural Networks [9, 40, 48, 54], which integrate graph structures into the self-attention mechanism of Transformers. Specifically, based on Ma et al. [22] we treat the normalized adjacency matrix as Markov transition matrix, representing transitions between keypoints as stochastic process. This probabilistic view allows us to define bias term that adjusts self-attention weights according to the structural distance between nodes in the graph, supporting more complex spatial relations. We evaluate our approach on the MP-100 benchmark, comprehensive dataset comprising over 20,000 images spanning 100 diverse object categories. Our method outperforms the previous state-of-the-art in the 1-shot setting and outperforms similar-sized methods in the 5-shot setting, significantly improving keypoint localization accuracy. We also demonstrate the robustness of our approach under challenging conditions and through extensive ablation study, such as super cross-category keypoint matching, where traditional methods often fail. In summary, we introduce novel approach to CAPE that treats object structure as dynamic and adaptive, leveraging predicted edge weights to improve localization accuracy. Our contributions can be summarized as follows: We propose an adaptive edge weight prediction mechanism, enabling the model to learn complex real-valued instance-specific pose graphs, rather than relying on fixed binary skeleton definitions. We propose an enhanced graph-based architecture, with Markovian Attention Bias, allowing the model to better capture complex spatial dependencies between keypoints. We achieve SOTA results on the MP-100 benchmark, demonstrating the effectiveness of our approach in both 1-shot and 5-shot settings. 2. Related Works 2.1. Category-Agnostic Pose Estimation Category-agnostic pose estimation (CAPE), introduced by Xu et al. [42], aims to extend conventional category-specific pose estimation [4, 11, 45, 47, 49] and multi-category pose estimation [43, 49] to unseen categories. This approach focuses on developing models that generalize beyond category-specific training, providing flexibility and robustness across various object types. Several key models have contributed to the field of CAPE. POMNet [42], based on regression-based approaches [18, 25, 56], utilized transformer to encode query images and support keypoints to predict similarity. Building on this, CapeFormer [35] adopted DETR-like framework [5, 12, 39, 50] to address unreliable matching outcomes, refining initial predictions. Later, different approaches were suggested to improve various aspects of CapeFormer. ESCAPE[24], introduced (a) (b) Figure 2. Graph Definition: (a) Graph structure serves as strong prior, representing an objects 3D structure/anatomy, and remains valid across viewing angles for both rigid and non-rigid objects. (b) As Edge placement can be ambiguous, we aim to learn the optimal graph for keypoint localization. We visualize different input graphs (left) and unnormalized output predicted graphs (right). Our network disconnects symmetric parts, which can hurt localization, and converges to similar refined skeletons for both inputs. For instance, for locating humans right elbow, it is effective to consider the positions of the right hand and shoulder, though their relative contributions may differ in magnitude. These structural priors hold for any viewing angle, and both for rigid and non-rigid objects. We aim to advance this line of work by introducing more complex graphs with realvalued edge weights. However, assigning weights to edges, rather than just defining their presence, is difficult even for humans. In addition, determining the optimal skeletal structure for an object is an ill-posed problem, as different skeletal annotations can lead to varying performance [14]. Our contribution lies in predicting graph edge weights that optimize keypoint localization. Figure 1 highlights the differences between recent CAPE methods. In this paper, we present EdgeCape, novel approach to CAPE that extends the graph-based framework by incorporating adaptive edge weight prediction. Predicting the full structure of unseen object categories can be extremely challenging, as it requires 3D understanding of the objects structure/anatomy. Instead, we learn to refine input prior graphs and assign edge weights. This approach combines user-provided prior knowledge with predicted edge weight, producing structure-aware skeleton optimized for localization. This way, our model can handle complex and varied object geometries. During training, we learn edge weights prediction using user-annotated graphs [14]. At test time, user can provide any prior graph, which we refine to improve keypoint localization. An illustration of different graph priors and predictions is shown in Figure 2. To further exploit the graph structure for improved local2 super-keypoints that capture the statistics of semantically related keypoints from different categories, tackling variability in object appearances and poses. Chen et al. [7] proposed to predict meta-points independently of support annotations, later refining them to align with desired keypoints based on support inputs. These meta-points are similar to object proposal tokens [1, 15, 57] in object detection models and provide structural information. X-Pose [44] introduced bottom-up approach, capable of simultaneously working on several instances in an image. PPM [27] used large-scale text-to-image diffusion models [32] to locate keypoints. At test-time, pseudo-prompts, corresponding to the keypoints, are learned and then used for localization using the diffusion models cross-attention maps. Moreover, recent works found object structure is crucial for effective CAPE localization. GraphCape[14] leveraged graph convolutional networks (GCNs) [16] to model the structural relations between keypoints, to better handle symmetry and occlusions. Later, CapeX [33] expanded this method by using textual point explanations, enabling the model to infer object poses from natural language descriptions. SCAPE [19] suggested enhancing the modeling of structural details by altering keypoints correlations. They refine the attention maps in self-attention blocks by using learnable attention filters. SDPNet [31] also explored graph-based methods by using GCNs to facilitate information sharing between structural keypoints. Their approach predicts adjacency matrices through keypoints self-attention, and is supervised by secondary GCN along with mask-reconstruction task for self-supervised learning. However, there are several limitations to this method. First, it relies solely on keypoint features, while object structure might be better inferred from the entire object. Second, predicting structures for unseen categories without prior knowledge is extremely challenging and requires 3D understanding of the objects anatomy. Finally, some structural information might be encoded in the auxiliary GCN rather than the adjacency matrix, causing it to be lost as it is only used during training. In contrast, we use both image and keypoint features to refine the users prior structural knowledge and leverage the models decoder for supervision, constructing weighted graph optimized for localization. 2.2. Structural Encoding in GNNs Graph Transformers adapt the Transformer architecture for In the context of GNNs, transgraph-structured data. formers self-attention mechanism can be viewed as message passing between all nodes, regardless of the graph connectivity. Three main strategies exist for incorporating structural information into Transformers: Positional Embedding from Graph Structure. Mialon et al. and Feldman et al. [13, 23] introduce positional encodings based on heat kernels and other graph kernels. Dwivedi et al. [9] employed Laplacian eigenvectors as positional encodings. Building upon this, Kreuzer et al. [17] utilized the complete spectrum of the Laplacian matrix to learn more expressive positional encodings. Recently, Ma et al. [22] employed random walk probabilities as relative positional encodings. Combining Graph Neural Networks (GNNs) with Transformers. Wu et al. [41] used GNNs to extract local structural node features, which Transformer then processes to model long-range interactions. Rampaˇsek et al. [28] introduced hybrid model that combines GNN layers with self-attention layers to capture both local and global dependencies. Similarly, Hirschorn et al. [14] replaced the feed-forward layers of the transformers with GCNs. Incorporating Graph Structural Bias into SelfAttention. This strategy transforms graph structural features into biases that directly modify the self-attention mechanism, allowing the Transformer to capture graphspecific characteristics better. Ying et al. [48] proposed spatial encodings based on the shortest path lengths between nodes to model structural similarity. Zhao et al. [54] enhanced the attention matrix by incorporating proximitybased relationships across different node neighborhoods. Dwivedi et al. [9] extended the graph Transformer model by encoding edge features into the self-attention mechanism. Wu et al. [40] introduced topological information as relational biases, improving the attention matrixs fidelity. Among these methods, we advocate incorporating graph structural biases into the self-attention matrix and combining GNNs with Transformers, allowing the Transformer to naturally understand and leverage node connectivity. 3. Method The complete framework of our method is illustrated in Figure 3, and is based on GraphCape [14]. In this setup, the model extracts support keypoints and query image features, and refines them by transformer encoder through selfattention. similarity proposal generator provides initial coordinate localization, and graph transformer decoder then refines these initial predictions. In the following section, we describe the key components of our proposed approach, EdgeCape. We first present the skeleton prediction network, the core idea of our paper. Then, we discuss the supervision strategy for adjacency matrix prediction. Next, we detail our enhanced feature extraction process. Finally, we describe our use of Markovian Attention Bias, which incorporates graph-based biases into the Transformers self-attention mechanism. More details on the complete framework and about the training scheme are in the supplementary. Figure 3. Framework Overview. Our model consists of three main components: feature extraction module, skeleton predictor, and graph-based keypoint predictor. The feature extraction module processes multi-scale image features, while the skeleton predictor refines the prior graph input by predicting residual connections. The graph-based keypoint predictor then utilizes the keypoint relations based on the refined structure, improving keypoint localization across diverse object geometries. 3.1. Edge Weight Prediction The core idea of our work is to learn adaptive weighted skeletons optimized for localization and tailored to each input instance. well-designed graph emphasizes localized, relevant connections that adapt to the unique structure of each object. Weighted connections are crucial because they enable the model to capture varying levels of influence between keypoints. For example, complete graphs, where all keypoint are connected, introduce unnecessary complexity and dilute the models focus with irrelevant relations. In contrast, structural or anatomical nearest-neighbor connections are beneficial for extrapolating locations, especially in cases of occlusion. For instance, in human pose estimation, weighted pose graph provides strong prior for locating persons knee based on the thigh and foot positions, though their relative contributions may differ in magnitude. Moreover, training models with structural priors also helps in learning asymmetrical features. Therefore, instead of relying on unweighted, fixed graphs, we aim to create adaptive weighted skeletons. To achieve this, we predict only the residual graph structure rather than the entire graph. By refining reliable unweighted graph prior, the model leverages the structural or anatomical relations provided by the user, feature that becomes especially valuable for unseen object categories with unfamiliar 3D structures. Formally, we define learnable function, fθ to learn the residual graph A. This function leverages input prior skeleton knowledge denoted Aprior, as well as the support image features Fs and support keypoint features , both of which are crucial for structure learning. The function fθ is defined as: = fθ(Aprior, Fs, ) (1) 4 The architecture of our skeleton prediction network is illustrated in Figure 4. Our first step is to refine the keypoints features to enhance their structural understanding. We find the graph-decoder proposed in [14] effective for this purpose, as it is designed to consider structural dependencies. We observe that further refinement of the input image features, in conjunction with the keypoint features, can significantly improve structure learning. To achieve this, we add cross-attention mechanism, where the input image features are updated by attending to the keypoint features. We refer to this module as dual-attention graph decoder, which encourages the exchange of structural information between the image and keypoint features. Once the feature refinement is complete, we apply multi-headed self-attention mechanism to predict the residual adjacency matrix A. To ensure smooth combination of the prior and residual matrices, while minimizing disruptions in early training, we incorporate the zero-convolution operator used in ControlNet [53]. The resulting combined matrix is then passed through an activation function to ensure positivity: = ReLU (Aprior + ciA) (2) where ci is learnable scalar initialized to zero, so the output skeleton begins as the prior adjacency matrix Aprior, stabilizing the model early in training. As training progresses, the model gradually learns to balance the contributions of Aprior and A. enforce to adjacency matrix symmetry and interpretation ensure Symas valid skeleton, metry is enforced by averaging the matrix with its row-wise: transpose and right-stochasticity. normalization normalization Lastly, we applied proper and is ing an auxiliary GCN, we use the keypoint prediction module for supervision. Optimization is done as follows: We first randomly mask portion of the input support keypoint features using the vector mask , and replace them with learnable masking token Fmask. Then, using the masked input, we predict the query keypoint locations: = gθ(M Fs + (1 ) Fmask, A, 0) (5) where is the Hadamard product, gθ is the graph-decoder of the keypoint prediction module, 0 are the predicted coordinates from the proposal generator, and is the predicted adjacency matrix. The adjacency loss is the L1 distance between the predicted keypoints using masked features and the ground-truth locations ˆP : Ladj = (cid:88) i= i ˆPi (6) We want to encode this data into the adjacency matrix. Therefore, during the backward pass for this loss, we freeze the decoders weights and all the inputs except A, to optimize only the adjacency predictor. In addition, we follow [19] and drop the heatmap loss used in GraphCape [14] as we empirically notice that this loss term doesnt improve performance. Thus, training is done using: = Lof set + λadjLadj, (7) where Lof set is the L1 localization loss as in [14]. More details about our training scheme are in the supplementary. 3.3. Feature Extraction Recent visual foundation models like Dino [6, 26] have demonstrated that their self-supervised pre-trained features can serve as powerful descriptors for semantic correspondence [51]. Trained on extensive datasets, these models are highly suited for category-agnostic applications, as they capture generalizable features that are not bound to specific categories. Recent efforts have focused on identifying features best suited for semantic correspondence [2, 52]. We aim to leverage these analyses for the task of CAPE (the differences between the two tasks are discussed in [19, 36]). However, GraphCape [14] showed that DinoV2 backbone performed worse than SwinV2, indicating that Swin backbone is better suited for the suggested architecture (aligning with Lin et al. [20]). Nonetheless, we advocate the use of Dino features, as Banani et al. [10] demonstrated that Dinov2 possesses an intrinsic 3D structural awareness. This structural awareness is beneficial for capturing skeletal structures the focus of our work. As Dino models may capture distinct properties across various network layers, relying solely on the final layers features may not be optimal for localization Figure 4. Skeleton Predictor Model. Our skeleton predictor contains feature refinement module that uses the prior input graph Aprior and the support image features Fs to enhance the keypoint features . Self-attention is then applied to predict the residual adjacency output A. = + AT 2 (3) Aij = Aij Aij (cid:80) (4) 3.2. Adjacency Supervision Optimizing the skeleton predictor using only localization loss leads to sub-optimal results. Thus, we seek to add to the predicted adjacency matrix dedicated supervision signal. However, even for humans, identifying the correct graph structure is not always straightforward, and determining appropriate edge weights is particularly challenging. Given this uncertainty, an unsupervised approach is necessary to learn these relations effectively. SDPNet [31] suggested using GCN with masking strategy to supervise the skeleton prediction. We advocate this strategy, as we aim for an adjacency matrix that can help the model overcome occlusions using structure. However, this requires additional GCN training which is not used during inference. Therefore, some of the structure data may be encoded in the GCN weights and not only in the adjacency matrix. Thus, we adopt the masking strategy, but instead of us-"
        },
        {
            "title": "Support",
            "content": "GT CapeFromer [36] SCAPE [19] GraphCape [14] EdgeCape Figure 5. Qualitative Comparison. We visualize keypoint predictions for the 1-shot setting. The left column shows the support data, followed by ground-truth query keypoints, and results from different methods. tasks. Following [10], we adopt dense, multi-scale probing approach similar to the DPT decoder [29] to harness the nuanced features at different scales. By freezing the pretrained backbone and training only the DPT layers, we preserve the generalization capabilities of self-supervised features, which are crucial for category-agnostic tasks. This is compared to previous methods, which fine-tuned the pretrained backbone. 3.4. Markovian Attention Bias In category-agnostic pose estimation (CAPE), encoding spatial relations between keypoints enhances robust localization, especially when dealing with novel objects [14, 33]. While Transformer models provide global receptive field, this flexibility lacks structural constraints, requiring explicit encoding of these relations. We build on GraphCape [14], which integrates GCN layers to propagate structural information, but these are limited to nearest-neighbor connections. To capture more complex and distant dependencies, we incorporate bias term in the Transformers self-attention mechanism based on graph connectivity [48]. To support weighted edges that represent varying strengths in keypoint relations, we adopt continuous formulation. We follow Ma et al. [21] and treat the normalized adjacency matrix (cid:101)A as stochastic matrix representing Markov process, where ( (cid:101)Ak)ij indicates the probability of transitioning from node vi to vj in k-hops. This enables both direct and multi-hop relations to influence attention. We start by constructing the matrix : Pij = [I, (cid:101)A, (cid:101)A2, . . . , (cid:101)Ak1]i,j RK (8) where is the maximum hops considered. Then, we use the following self-attention term: aij = (hiWQ)(hjWK)T + MLP(Pij) (9) where the MLP: RK modulates the influence of keypoints based on graph distance. The full formulation and derivation of this bias term are in the supplementary. 4. Experiments In line with prior CAPE works, we use the MP-100 dataset for training and evaluation. This dataset consists of over 20,000 images drawn from various category-specific pose estimation datasets, covering 100 object categories. Each category includes varying numbers of keypoints, with up to 68 keypoints per category. The dataset is divided into five distinct, mutually exclusive splits, ensuring that categories used for training, validation, and testing do not overlap. To evaluate our models performance, we use the Probability of Correct Keypoint (PCK) [46] metric, following established practices. We set the PCK threshold to 0.2, as commonly done in previous works. In the supplementary, we comprehensively compare our method by including two additional metrics: Area Under the ROC Curve (AUC) and Normalized Mean Error (NME). 4.1. Implementation Details For fair comparison, training parameters, data augmentations, and data pre-processing are kept the same as in previous works. In addition, we use the smallest version of DinoV2 to match the backbone size of GraphCape [14]. More implementation details are in the supplementary. 6 Model Backbone Split 1 Split 2 Split Split 4 Split 5 Avg Split 1 Split 2 Split Split 4 Split 5 Avg 1-Shot 5-Shot POMNet [42] CapeFormer [35] ESCAPE [24] MetaPoint+ [7] SDPNet [31] X-Pose [44] PPM [27] SCAPE1 [19] GraphCape [14] ResNet-50 ResNet-50 ResNet-50 ResNet-50 HRNet-32 Swin Stable Diffusion DinoV2 SwinV2 Ours DinoV2 + DPT 84.23 89.45 86.89 90.43 91.54 89.07 91.03 91.47 91.19 93.69 78.25 84.88 82.55 85.59 86.72 85.05 88.06 86.29 87. 89.27 78.17 83.59 81.25 84.52 85.49 85.26 84.48 87.23 85.68 87.85 78.68 83.53 81.72 84.34 85.77 85.52 86.73 87.07 85.87 86.67 79.17 85.09 81.32 85.96 87.26 85.79 87.40 86.94 85. 79.70 85.31 82.74 86.17 87.36 86.14 87.54 87.80 87.23 87.59 89.01 84.72 91.94 91.41 92.58 93.68 - 93.64 94.33 94.24 95.51 79.61 88.92 87.43 89.63 90.23 - 92.71 90.53 91. 91.94 78.00 89.40 85.33 89.98 89.67 - 91.76 91.49 90.15 91.33 80.38 88.01 87.27 88.70 89.08 - 92.85 90.68 90.37 90.36 80.85 88.25 86.76 89.20 89.46 - 91.94 89.80 89. 80.71 89.30 87.63 90.02 90.42 - 92.58 91.37 91.16 91.92 92.21 Table 1. MP-100 Results. PCK performance under 1-shot and 5-shot settings. Our approach outperforms other methods on the 1-shot setting. In addition, on the 5-shot setting, we outperform similar-sized methods and reduce the performance gap with PPM [27], method that uses huge pre-trained diffusion model and test-time optimization. The best results are bold, and the second-best are underlined. Input Graph Predicted Graph Input SwinV2 DinoV2 DinoV2 + DPT Probing Figure 7. Feature Extraction. PCA visualization comparing different feature extraction models. Our approach captures finegrained details and asymmetrical features that aid localization. graph-based keypoint prediction module. As can be seen, our network weakens symmetric parts connections, which can hurt localization. For example, the table has stronger connections between the base and legs than between the different corners of the base. Alternatively, it creates new helpful connections in the human face. Finally, in Figure 7 we show PCA visualizations of our feature extraction method compared to previous methods. Our approach captures fine-grained details and learns asymmetrical features that enhance localization. In the supplementary, we also demonstrate the instance adaptability of our graph prediction module by presenting graph examples from different instances within the same category. Additionally, we provide additional predicted graphs across various categories, more comparison examples, and PCA visualizations of features from different backbones. 4.3. Quantitative Results We report results on the MP-100 dataset under 1-shot and 5shot settings in Table 1. For fair comparison, we show results using similar-sized backbones. It is worth noting, however, that PPM [27] differs from other methods as it is based 1Evaluated without keypoint identifiers, which were shown to be inapplicable for real-world scenarios [14, 24, 44] Figure 6. Predicted Skeleton. We visualize the unnormalized graph outputs. The left column denotes the input Aprior and the right column is the refined adjacency matrix. Line width corresponds to edge weight. The model disconnects symmetric parts and creates new connections that are helpful for localization. 4.2. Qualitative Results Figure 5 provides qualitative comparison between our method and previous CAPE works, including CapeFormer [36], SCAPE [19], and GraphCape [14]. Our methods incorporation of predicted weighted pose graphs acts as stronger prior for keypoint localization, effectively breaking symmetry and enhancing localization accuracy. Additionally, we provide few examples of graph predictions. Figure 6 shows input prior graphs Aprior and the predicted refined graphs, which serve as inputs to the 7 Method Human Body Human Face Vehicle Furniture POMNet [42] CapeFormer [35] ESCAPE [24] MetaPoint+ [7] SDPNet [31] PPM [27] SCAPE [19] GraphCape [14] Ours 73.82 83.44 80.60 84.32 83.84 85.13 84.24 88.38 88. 79.63 80.96 84.13 82.21 81.24 82.44 85.98 83.28 89.62 34.92 45.40 41.39 46.51 45.53 52.08 45.61 44.06 45.55 47.27 52.49 55.49 53.67 53.08 60.59 54.13 45.56 64. Table 2. Super Cross-Category. PCK results for the super crosscategory setting. Our method outperforms others in this challenging setting across most splits, demonstrating robust generalization. The best results are bold, and the second-best are underlined. on the significantly larger Stable Diffusion model [32]. Moreover, it requires test-time optimization, which is unnecessary for the other methods. Nevertheless, we include their results for completeness. As can be seen, our method outperforms the state-of-the-art by an average of 1.21% under the 1-shot setting, and achieves the highest performance among similar-sized methods under the 5-shot setting, with margin of 0.84%. 4.4. Ablation Study Following standard practice, we conducted several ablation studies on the MP-100 split-1 in the 1-shot setting. First, we present quantitative comparison in the challenging supercategory setting. Next, we analyze the contribution of each component within our framework. Finally, we examine the impact of different input graphs Aprior on performance. In the supplementary, we provide additional ablation experiments including masking query images to evaluate occlusion handling, quantitative analysis and comparisons of our adjacency matrix supervision strategy, histograms of predicted adjacency weight changes, and evaluations with various backbones. Super Cross-Category. We conduct cross-supercategory experiment following prior works to assess our models generalization capacity. Splitting the MP-100 dataset into eight super-categories, we hold out each supercategory in turn and train on the rest. As shown in Table 2, our model outperforms other methods across most splits. We attribute much of this improvement to our proposed feature extraction, which combines frozen DinoV2 with DPT decoder. This harnesses its fine-grained features for precise localization without impairing generalization. This outcome highlights our frameworks effectiveness in capturing category-agnostic features. Method Ablation PCK0.2 GraphCape [14] + Dino Backbone + Dino Probing + Remove Heatmap Loss + Graph Weight Prediction + Attention Bias 91.19 90.34 (-0.85%) 92.28 (+1.94%) 92.80 (+0.52%) 93.54 (+0.74%) 93.69 (+0.15%) Table 3. Method Ablation. We showcase the contribution of each of our modified components. Simple backbone replacement reduces accuracy, while learnable multi-resolution fusion, attention bias, and edge weight prediction each boost performance, highlighting the value of complex weighted graphs for localization. Prior Graph Input Aprior PCK0.2 Self-Connections Only Fully-Connected Graph Annotated Graph 93.10 92.91 93.69 Table 4. Different Prior Graph Inputs. Results using different Aprior inputs. Testing with disconnected or fully connected graphs shows performance drop, as the model is designed to refine rather than fully predict the structure. Components Contribution. We base our architecture on GraphCape [14], with Table 3 demonstrating the performance impact of each proposed component as they are added incrementally. Replacing the backbone initially results in drop in accuracy, but incorporating our suggested feature extraction method yields substantial performance gain. Noticeable improvements are achieved by integrating our graph prediction model, with an additional boost provided by the Markov Attention Bias. Together, these results underscore the value of using complex weighted graphs for accurate localization. The Impact of Prior Graph Input Aprior. To evaluate the impact of the input graph structure prior, we test our method, trained on annotated graphs, using different graph inputs. Results are shown in Table 4. Evaluating the model with disconnected or fully connected graphs leads to performance drop, as our model is designed to refine the structure rather than predict it entirely. In the supplementary material, we provide qualitative examples and illustrate changes in adjacency matrix weights for different Aprior. 5. Conclusion We present EdgeCape, novel CAPE method that improves keypoint localization through predicted refined skeleton treat keypoints graphs. Unlike previous methods that as isolated or use unweighted graphs, EdgeCape predicts weighted graphs that yield superior localization accuracy. 8 Our approach integrates edge weight prediction with Markovian Attention Bias to capture complex structural dependencies, improving accuracy under occlusions and symmetry challenges. Evaluated on the MP-100 dataset, EdgeCape achieves state-of-the-art results in the 1-shot setting and leads among similar-sized methods in the 5-shot setting, advancing versatile, structure-aware CAPE methods."
        },
        {
            "title": "References",
            "content": "[1] Bogdan Alexe, Thomas Deselaers, and Vittorio Ferrari. Measuring the objectness of image windows. IEEE transactions on pattern analysis and machine intelligence, 34(11):2189 2202, 2012. 3 [2] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep vit features as dense visual descriptors. ECCVW What is Motion For?, 2022. 5 [3] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 61546162, 2018. 17 [4] Z. Cao, G. Hidalgo Martinez, T. Simon, S. Wei, and Y. A. Sheikh. Openpose: Realtime multi-person 2d pose estimation using part affinity fields. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2019. 1, 2 [5] Nicolas Carion, Francisco Massa, Gabriel Synnaeve, Nicolas Usunier, Alexander Kirillov, and Sergey Zagoruyko. End-toend object detection with transformers. In European conference on computer vision, pages 213229. Springer, 2020. 2 [6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96509660, 2021. 5 [7] Junjie Chen, Jiebin Yan, Yuming Fang, and Li Niu. Metapoint learning and refining for category-agnostic pose estiIn Proceedings of the IEEE/CVF Conference on mation. Computer Vision and Pattern Recognition, pages 23534 23543, 2024. 3, 7, [8] MMPose Contributors. Openmmlab pose estimation toolbox and benchmark. https://github.com/openmmlab/mmpose, 2020. 14 [9] Vijay Prakash Dwivedi and Xavier Bresson. generalization of transformer networks to graphs. arXiv preprint arXiv:2012.09699, 2020. 2, 3 [10] Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas Guibas, Justin Johnson, and Varun Jampani. Probing the 3d awareness of visual foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2179521806, 2024. 5, 6, 14 [11] Hao-Shu Fang, Jiefeng Li, Hongyang Tang, Chao Xu, Haoyi Zhu, Yuliang Xiu, Yong-Lu Li, and Cewu Lu. Alphapose: Whole-body regional multi-person pose estimation IEEE Transactions on Pattern and tracking in real-time. Analysis and Machine Intelligence, 2022. 1, 2 [12] Yuxin Fang, Wen Wang, Binhui Xie, Quan Sun, Ledell Wu, Xinggang Wang, Tiejun Huang, Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale. arXiv preprint arXiv:2211.07636, 2022. 2 [13] Or Feldman, Amit Boyarski, Shai Feldman, Dani Kogan, Avi Mendelson, and Chaim Baskin. Weisfeiler and leman go infinite: Spectral and combinatorial pre-colorings. arXiv preprint arXiv:2201.13410, 2022. 3 [14] Or Hirschorn and Shai Avidan. Pose anything: graphbased approach for category-agnostic pose estimation. arXiv preprint arXiv:2311.17891, 2023. 1, 2, 3, 4, 5, 6, 7, 8, 12, 13, 14, 16, 17, [15] Jan Hosang, Rodrigo Benenson, Piotr Dollar, and Bernt Schiele. What makes for effective detection proposals? IEEE transactions on pattern analysis and machine intelligence, 38(4):814830, 2015. 3 [16] Thomas Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. arXiv preprint arXiv:1609.02907, 2016. 3 [17] Devin Kreuzer, Dominique Beaini, Will Hamilton, Vincent Letourneau, and Prudencio Tossou. Rethinking graph transformers with spectral attention. Advances in Neural Information Processing Systems, 34:2161821629, 2021. 3 [18] Jiefeng Li, Siyuan Bian, Ailing Zeng, Can Wang, Bo Pang, Wentao Liu, and Cewu Lu. Human pose regression with In Proceedings of the residual log-likelihood estimation. IEEE/CVF international conference on computer vision, pages 1102511034, 2021. 2 [19] Yujia Liang, Zixuan Ye, Wenze Liu, and Hao Lu. Scape: simple and strong category-agnostic pose estimator, 2024. 3, 5, 6, 7, 8, 13, 14, 16, 18 [20] Yutong Lin, Yuhui Yuan, Zheng Zhang, Chen Li, Nanning Zheng, and Han Hu. Detr does not need multi-scale or locality design. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 65456554, 2023. 5 [21] Liheng Ma, Chen Lin, Derek Lim, Adriana Romero-Soriano, K. Dokania, Mark Coates, Philip H.S. Torr, and Ser-Nam Lim. Graph Inductive Biases in Transformers without MesIn Proc. Int. Conf. Mach. Learn., 2023. 6, sage Passing. 18 [22] Liheng Ma, Chen Lin, Derek Lim, Adriana Romero-Soriano, Puneet Dokania, Mark Coates, Philip Torr, and Ser-Nam Lim. Graph inductive biases in transformers without mesIn International Conference on Machine sage passing. Learning, pages 2332123337. PMLR, 2023. 2, [23] Gregoire Mialon, Dexiong Chen, Margot Selosse, and Julien Mairal. Graphit: Encoding graph structure in transformers. arXiv preprint arXiv:2106.05667, 2021. 3 [24] Khoi Duc Nguyen, Chen Li, and Gim Hee Lee. Escape: Encoding super-keypoints for category-agnostic pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2349123500, 2024. 2, 7, 8, 14 [25] Markus Oberweger and Vincent Lepetit. Deepprior++: Improving fast and accurate 3d hand pose estimation. In Proceedings of the IEEE international conference on computer vision Workshops, pages 585594, 2017. 2 9 [26] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. arXiv preprint arXiv:2304.07193, 2023. 5 [27] Duo Peng, Zhengbo Zhang, Ping Hu, Qiuhong Ke, Yau, and Jun Liu. Harnessing text-to-image diffusion models for category-agnostic pose estimation. In European Conference on Computer Vision. Springer, 2024. 3, 7, [28] Ladislav Rampaˇsek, Michael Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique Beaini. Recipe for general, powerful, scalable graph transformer. Advances in Neural Information Processing Systems, 35: 1450114515, 2022. 3 [29] Rene Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. ViIn Proceedings of sion transformers for dense prediction. the IEEE/CVF International Conference on Computer Vision (ICCV), pages 1217912188, 2021. 6 [30] Dinesh Reddy, Minh Vo, and Srinivasa Narasimhan. Carfusion: Combining point tracking and part detection for In Proceedings of dynamic 3d reconstruction of vehicles. the IEEE conference on computer vision and pattern recognition, pages 19061915, 2018. 1 [31] Pengfei Ren, Yuanyuan Gao, Haifeng Sun, Qi Qi, Jingyu Wang, and Jianxin Liao. Dynamic support information mining for category-agnostic pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19211930, 2024. 3, 5, 7, 8, 13 [32] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. 3, 8 [33] Matan Rusanovsky, Or Hirschorn, and Shai Avidan. Capex: Category-agnostic pose estimation from textual point explanation, 2024. 1, 3, 6, [34] Min Shi, Hao Lu, Chen Feng, Chengxin Liu, and Zhiguo Cao. Represent, compare, and learn: similarity-aware In Proceedings of framework for class-agnostic counting. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95299538, 2022. 17 [35] Min Shi, Zihao Huang, Xianzheng Ma, Xiaowei Hu, and Zhiguo Cao. Matching is not enough: two-stage framework for category-agnostic pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 73087317, 2023. 2, 7, 8, 14 [36] Min Shi, Zihao Huang, Xianzheng Ma, Xiaowei Hu, and Zhiguo Cao. Matching is not enough: two-stage framework for category-agnostic pose estimation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73087317, 2023. 1, 5, 6, 7, 12, 13, 16 [37] Xibin Song, Peng Wang, Dingfu Zhou, Rui Zhu, Chenye Guan, Yuchao Dai, Hao Su, Hongdong Li, and Ruigang Yang. Apollocar3d: large 3d car instance understanding benchmark for autonomous driving. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 54525462, 2019. 1 [38] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In Computer VisionECCV transforms for optical flow. 2020: 16th European Conference, Glasgow, UK, August 23 28, 2020, Proceedings, Part II 16, pages 402419. Springer, 2020. [39] Wenhai Wang, Jifeng Dai, Zhe Chen, Zhenhang Huang, Zhiqi Li, Xizhou Zhu, Xiaowei Hu, Tong Lu, Lewei Lu, Hongsheng Li, et al. Internimage: Exploring large-scale vision foundation models with deformable convolutions. arXiv preprint arXiv:2211.05778, 2022. 2 [40] Qitian Wu, Wentao Zhao, Zenan Li, David Wipf, and Junchi Yan. Nodeformer: scalable graph structure learning transformer for node classification. Advances in Neural Information Processing Systems, 35:2738727401, 2022. 2, 3 [41] Zhanghao Wu, Paras Jain, Matthew Wright, Azalia Mirhoseini, Joseph Gonzalez, and Ion Stoica. Representing longrange context for graph neural networks with global attention. Advances in Neural Information Processing Systems, 34:1326613279, 2021. 3 [42] Lumin Xu, Sheng Jin, Wang Zeng, Wentao Liu, Chen Qian, Wanli Ouyang, Ping Luo, and Xiaogang Wang. Pose for everything: Towards category-agnostic pose estimation. In European Conference on Computer Vision, pages 398416. Springer, 2022. 1, 2, 7, 8 [43] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose++: Vision transformer foundation model for generic arXiv preprint arXiv:2212.04246, body pose estimation. 2022. 2 [44] Jie Yang, Ailing Zeng, Ruimao Zhang, and Lei Zhang. Xpose: Detection any keypoints. ECCV, 2024. 3, 7, 14 [45] Sen Yang, Zhibin Quan, Mu Nie, and Wankou Yang. Transpose: Keypoint localization via transformer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1180211812, 2021. 1, [46] Yi Yang and Deva Ramanan. Articulated human detection with flexible mixtures of parts. IEEE transactions on pattern analysis and machine intelligence, 35(12):28782890, 2012. 6 [47] Yuxiang Yang, Junjie Yang, Yufei Xu, Jing Zhang, Long Lan, and Dacheng Tao. Apt-36k: large-scale benchmark for animal pose estimation and tracking. Advances in Neural Information Processing Systems, 35:1730117313, 2022. 2 [48] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? Advances in neural information processing systems, 34: 2887728888, 2021. 2, 3, 6, 18 [49] Hang Yu, Yufei Xu, Jing Zhang, Wei Zhao, Ziyu Guan, and Dacheng Tao. Ap-10k: benchmark for animal pose esIn Thirty-fifth Conference on Neural timation in the wild. Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. 2, 12 [50] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum. Dino: Detr 10 with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022. 2 [51] Junyi Zhang, Charles Herrmann, Junhwa Hur, Eric Chen, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. Telling left from right: Identifying geometry-aware semantic correIn Proceedings of the IEEE/CVF Conference spondence. on Computer Vision and Pattern Recognition, pages 3076 3085, 2024. [52] Junyi Zhang, Charles Herrmann, Junhwa Hur, Luisa Polania Cabrera, Varun Jampani, Deqing Sun, and Ming-Hsuan Yang. tale of two features: Stable diffusion complements dino for zero-shot semantic correspondence. Advances in Neural Information Processing Systems, 36, 2024. 5 [53] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 4 [54] Jianan Zhao, Chaozhuo Li, Qianlong Wen, Yiqi Wang, Yuming Liu, Hao Sun, Xing Xie, and Yanfang Ye. Gophormer: Ego-graph transformer for node classification. arXiv preprint arXiv:2110.13094, 2021. 2, 3 [55] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transarXiv preprint formers for end-to-end object detection. arXiv:2010.04159, 2020. 17 [56] Christian Zimmermann and Thomas Brox. Learning to estimate 3d hand pose from single rgb images. In Proceedings of the IEEE international conference on computer vision, pages 49034911, 2017. 2 [57] Lawrence Zitnick and Piotr Dollar. Edge boxes: LoIn Computer Vision cating object proposals from edges. ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 391405. Springer, 2014."
        },
        {
            "title": "Supplementary Material",
            "content": "A. Supplementary Index Section presents additional experimental results for our method, including: Additional metrics results on MP-100 dataset. Quantitative evaluations on single-category setting. Predicted adjacency matrix change - histogram showing the matrices change given different Aprior inputs, demonstrating the effect of our graph prediction module. Method scalability - performance using different backbone models in various sizes. Query image masking performance comparison, demonstrating occlusion handling. Adjacency matrix supervision strategy ablation study. Graph prediction module design ablation - the importance of support image features for graph prediction. More qualitative results - including predicted graphs, and qualitative comparisons. Implementation Details. Section provides additional details about the different components in our method: Framework Overview. Markovian Attention Bias - Complete description and derivation. Training Scheme. B. Further Experiments B.1. Quantitative Results - Additional Metrics For more complete comparison of our method, we report two additional metrics - Area Under ROC Curve (AUC) and Normalized Mean Error (NME) on the MP-100 dataset 1-shot setting. We evaluate CapeFormer, SCAPE, GraphCape, and our method. As shown in Table 5, our method achieves state-of-the-art also in these metrics, demonstrating the superiority of our method. B.2. Ablation Study Single-Category Setting. We report PCK performance on single-category pose estimation benchmark using the AP-10K dataset [49], large-scale animal pose estimation benchmark comprising 23 animal families and 60 species. This dataset features unified keypoint definition and skeletal structure across all categories. Notably, the training and testing category splits are not mutually exclusive, meaning categories may appear in both sets. This setup allows us to evaluate our methods effectiveness in single-category context, where categories share common anatomical structures but exhibit species-specific variations. The results are presented in Table 6. Our method outperforms CapeFormer [36] and GraphCape [14]. Notably, the compared methods fine-tune their backbones, which provides an advantage in this single-category setting where only fixed set of keypoints is relevant. We present two versions of our method: one using fine-tuned SwinV2 backbone (as in GraphCape) and another employing the feature extractor proposed in this paper. As the results show, leveraging predicted weighted graphs and Markovian Attention Bias leads to performance gain. Moreover, although our proposed feature extractor uses frozen backbone, it achieves slightly better performance than the SwinV2 finetuned version. Adjacency Matrix Change. We assess how the weights of the adjacency matrices change. We create histogram of the difference between normalized Aprior and the normalized predicted matrix. Results are shown in Figure 8, demonstrating that our network effectively refines the input graph prior. This refinement involves creating new connections and adjusting the weights of existing ones, enabling the model to better represent structural relationships. Method Scalability. We assess our models performance using backbones of varying sizes to explore the trade-off between accuracy and efficiency. Results are shown in table 7. As anticipated, larger backbones yield improved performance due to their increased capacity for feature extraction and representation. However, these improvements come at the cost of greater computational requirements and model size. For comparison, we include results using GraphCapes [14] original SwinV2 backbone. This comparison underscores the advantages of our weighted graph refinement approach. Nevertheless, as hypothesized in the main paper, the performance gap is less pronounced because Dino features, utilized in our model, offer inherent 3D structural awareness. This capability is crucial for accurately predicting and refining structural relationships, making Dino particularly suitable choice for our task. Handling Occlusions. To demonstrate the effect of weighted graph information in handling occlusions, we applied random partial masking to the query images before executing our algorithm. As shown in Figure 9, our method consistently surpasses GraphCape when parts of the query images are masked, accurately predicting keypoints. Adjacency Matrix Supervision. To evaluate the effectiveness of our unsupervised masking strategy, we first test 12 Model Split 1 Split Split 3 Split 4 Split 5 Avg Split 1 Split Split 3 Split 4 Split 5 Avg AUC NME CapeFormer [36] SCAPE (DinoV2) [19] GraphCape [14] Ours 88.64 89.85 89.08 90.88 86.39 87.41 87.69 88. 86.18 87.85 86.97 88.17 85.81 87.09 87.01 87.69 86.51 87.72 86.67 86.70 87.98 87. 88.00 88.62 0.088 0.074 0.083 0.064 0.110 0.099 0.097 0. 0.111 0.093 0.103 0.090 0.116 0.096 0.104 0.097 0.108 0.096 0.107 0.106 0.092 0. 0.093 0.088 Table 5. MP-100 Additional Metrics. AUC and NME performance under 1-shot setting. Our approach outperforms other methods. The best results are bold. (a) Annotated Graph Prior (b) Self-loops Prior (c) Fully-Connected Prior Figure 8. Adjacency Matrix Change Histogram. histogram showing the weight changes of our predicted weighted graph. We show the change given different Aprior inputs. The x-axis shows the change and the y-axis is the log-frequency. The dashed red line shows no change in edge weight. Model Metric PCK AUC NME CapeFormer [36] GraphCape [14] Ours (SwinV2) Ours (DinoV2 + DPT) 87.44 89.54 89.90 89. 86.41 87.22 87.29 87.63 0.094 0.086 0.085 0.081 Table 6. AP-10K Results. PCK, AUC, and NME results for the single-category AP-10K benchmark. [] methods fine-tune their backbone. The best results are bold. Backbone GraphCape [14] Ours SwinV2 DinoV2 + DPT DinoV2-B + DPT 91.19 92.80 - 91.52 93.69 94.10 Table 7. Different Backbones. PCK results using different backbones for GraphCape [14] and our method. Our method outperforms GraphCape with the same backbone and demonstrates scalability with larger backbones. Results also highlight the benefits of Dino features, which exhibit 3D structural awareness. the performance using the GCN reconstruction approach proposed by Ren et al. [31], which utilizes an auxiliary Figure 9. Handling Occlusions. Quantitative results when masking the query image. Our method consistently surpasses GraphCape, leveraging cues provided by the weighted graph structure to overcome information gaps in the query images. GCN to reconstruct masked inputs. With this approach, we achieve 92.89% accuracy, compared to 93.69% using our decoder-based reconstruction strategy decrease of 0.8%. This drop supports our hypothesis that structural information is embedded within the auxiliary GCN weights and thus lost during inference. Graph Prediction Module Design. We highlight the importance of utilizing whole-image features for skeleton prediction rather than just keypoint features. Incorporating cross-attention with support image features in the skele-"
        },
        {
            "title": "Zoom In",
            "content": "Figure 10. Instance Adaptability. We visualize an example of instance adaptability. The left column denotes the input Aprior and the right column is the refined adjacency matrix. In the top row, we see graph input of panda body, where all keypoints are visible. In the bottom row, as some keypoints are occluded (node 15), the input graph includes isolated nodes (node 16). Our predicted graph connects this isolated node to enhance localization. vious works. In addition, the backbone size was matched to GraphCape, thus we used the smallest (20M parameters) version of DinoV2. The graph predicting network, encoder, and keypoint prediction decoder have 3 layers. For the Markovian Bias Attention, we use maximum of = 4 hops, and for Ladj we mask 50% of keypoints, using λadj = 1. The model is built upon MMPose framework [8], trained using Adam optimizer with batch size of 16, the learning rate is 105, and decays by 10 on the 160th and 180th epoch. Each phase is trained for 100 epochs. Training takes 1 day on single Nvidia A100 GPU. The feature extraction DPT decoder is based on the official code from [10]. Evaluation of SCAPE [19] was done by removing the keypoint identifiers (which were shown to be inapplicable for real-world scenarios [14, 24, 44]) and training network using their official code. We also used GraphCape and CapeFormer official code for evaluation and visualizations. ton prediction network achieves 93.54% PCK. In contrast, excluding image features results in performance drop to 93.43% (-0.11%), demonstrating the value of leveraging global context for accurate graph refinement. B.3. Additonal Qualitative Results Figure 10 illustrates the instance adaptability of our graph Specifically, we present two input prediction module. graphs of panda body: one where all keypoints are visible, and another where some keypoints are occluded, resulting in isolated nodes in the input graph. As shown, our graph prediction module generates different graphs for instances of the same category based on the input. Notably, the predicted graph connects the isolated node, enhancing localization accuracy. In addition, Figure 11 illustrates additional skeleton predictions. We also present in Figure 12 additional qualitative comparison of Capeformer [35], SCAPE [19], GraphCape [14], and our model on various categories. B.4. Implementation Details For fair comparison, training parameters, data augmentations, and data pre-processing are kept the same as in pre-"
        },
        {
            "title": "Predicted Graph",
            "content": "Figure 11. Predicted Skeleton. We visualize the unnormalized graph outputs. The left column denotes the input Aprior and the right column is the refined adjacency matrix. Line width corresponds to edge weight. The model disconnects symmetric parts and creates new connections that are helpful for localization."
        },
        {
            "title": "Support",
            "content": "GT CapeFromer [36] SCAPE [19] GraphCape [14] EdgeCape Figure 12. Qualitative Comparison. We visualize keypoints predictions for the 1-shot setting. The left column denotes the support image with its corresponding skeleton. The second column is the ground-truth query keypoints. The following columns are results from CapeFormer, SCAPE, GraphCape, and our method. 16 C. Method Details C.1. Framework Overview We base our method on GraphCape [14], introducing three main architectural modifications: Updating the feature extraction module. Incorporating Markov Bias attention in the graphdecoder. Adding graph-prediction module. These modifications are detailed in the main paper, while below we provide brief overview of GraphCapes architecture for completeness. For full details, please refer to the original paper. Feature Extractor: pre-trained model extracts features from support and query images, resulting in the query feature map ˆFq RhwC and support keypoint features ˆFs RKC. Support keypoint features are derived by element-wise multiplication between the support images feature map and keypoint masks, created using Gaussian kernels centered at the support keypoints. In multi-shot scenarios (e.g., 5-shot), the average of support keypoint features across images in feature space is taken. Transformer Encoder: The transformer encoder fuses information between support keypoint and query patch features. Support keypoints and query features are concatenated before entering the self-attention layer and separated afterward. The output is the refined query feature map Fq and refined support keypoint features Fs. Similarity-Aware Proposal Generator: GraphCape builds on CapeFormers two-stage approach, first generating initial keypoint predictions, which are then refined via DETR-based transformer decoder. The proposal generator aligns support keypoint features with query features, producing similarity maps from which peaks are selected as similarity-aware proposals. To enhance efficiency and adaptability, trainable inner-product mechanism [34] is used to explicitly model similarity. Graph Transformer Decoder: transformer decoder network decodes keypoint locations from the query feature map. Each layer contains self-attention, crossattention, and feed-forward blocks. In this design, GraphCape replaces the original transformer decoders feedforward network with GCN-based module to incorporate structural priors. To prevent excessive smoothinga common issue in deep GCNs that can blur node distinctions and degrade performanceGraphCape adds linear layer for each node following the GCN layer. Each decoder layer predicts coordinate deltas for prior predictions using an iterative refinement strategy [3, 38, 55]. The updated coordinates are created as follows: l+1 = σ(σ1(P l) + LP (F l+1 )) (10) where σ and σ1 are the sigmoid and its inverse. AdFigure 13. Graph Decoder Prediction Layer. Overview of the Transformer decoder architecture, adapted from the GraphCape design. The decoder consists of self-attention, cross-attention, and graph-based feed-forward network. We incorporate Markovian Attention Bias into the self-attention mechanism to encourage structural keypoint interactions. Self-attention facilitates adaptive interactions among support keypoints, while cross-attention extracts localization information from the input features. Finally, the decoder refines keypoint features and outputs location predictions. ditionally, the decoder leverages predicted coordinates to provide enhanced reference points for feature pooling from the image feature map. The keypoints positions from the last layer are used as the final prediction. C.2. Markovian Attention Bias In category-agnostic pose estimation, encoding spatial relations between keypoints is beneficial for robust localization, especially when dealing with novel objects [14, 33]. Transformer models naturally have global receptive field, allowing each node (or token) to attend to all others within layer. However, this flexibility introduces challenge: Transformers lack inherent structural constraints, so positional dependencies must be explicitly encoded to reflect local relations. While this is often done in sequence data through absolute or relative positional encodings, graphs pose different challenge, as nodes are not arranged lin17 early and connectivity is defined by edges. term: aij = (hiWQ)(hjWK)T + LP (Pij) (13) where MLP: RK R, modulates the influence of keypoints based on their distance in the graph (i.e., the number of hops between them). This formulation results in continuous and learnable structure-based bias term. C.3. Training Scheme When we attempt to directly integrate the adjacency matrix predictor and the self-attention bias mechanism, we observe only small impact on the models performance. Changing the adjacency matrix while training the bias attention MLP results in unstable training. Thus, we first train our base model and fine-tune each added component. We begin by training the base model and establishing strong foundational features necessary for robust localization. We follow [19] and drop the heatmap loss used in GraphCape [14] as we empirically notice that this loss term doesnt improve performance. Thus, optimization is done using Lof set which is the L1 localization loss as in [14]. Once the model has converged, we freeze the feature extractor module and integrate the skeleton predictor. This stage allows the model to further adapt by incorporating specific structural insights provided by the skeleton predictor. For this phase, we add the adjacency loss, resulting in: = Lof set + λadjLadj, (14) In the final phase, we maintain the frozen feature extractor and freeze the skeleton predictor. Then, we integrate the Markovian bias attention. This final stage allows the model to strengthen the models capacity to interpret spatial dependencies between keypoints. This three-phase approach allows each component to integrate structural encoding progressively, enhancing accuracy through stable framework. Our approach aims to better utilize the structural dependencies between keypoints for CAPE. We build on the foundations set by GraphCape [14], which incorporates GCN layers into the feed-forward layers to propagate structural information. However, the GCN layers used in that model were limited by their fixed, nearest-neighbor receptive field, which restricts the models ability to capture more complex or distant connections between keypoints. Thus, we further integrate the graph-prior into the architecture. We follow Ying et al. [48], adding bias term based on graph connectivity to the self-attention mechanism in the decoder. Denote aij as the (i, j)-element of the Query-Key product attention matrix a, resulting in: aij = (hiWQ)(hjWK)T + bϕ(vi,vj ) (11) where bϕ(vi,vj ) is learnable scalar indexed by ϕ(vi, vj), and is unique for each attention head. Unlike the limited receptive field in GCNs, using bϕ(vi,vj ) enables each node in single Transformer layer to adaptively attend to all other nodes based on the graphs structural information. ϕ(vi, vj) is usually the distance of the shortest path (SPD) between vi and vj if the two nodes are connected. This bias term is highly effective in capturing general structure, boosting the performance of GraphCape by around 0.5%. However, it assumes discrete distance between nodes and is applicable for unweighted adjacency matrices. Our primary objective is to predict real-valued edges, representing the strength of the structural keypoints connections. Thus, we follow Ma et al. [21] and treat the normalized adjacency matrix (cid:101)A as right stochastic matrix. We differentiate their implementation by using the bias attention mechanism instead of relative positional encoding. stochastic matrix is commonly used to describe the transitions in Markov chain, where each element ( (cid:101)A)ij represents the probability of moving from one state (or node) vi to another state vj in single step. Specifically, the entry ( (cid:101)Ak)ij gives the probability of transitioning from node vi to node vj in exactly k-hops. This process allows us to capture both direct and indirect relations between keypoints, enabling the model to consider more distant keypoints that may influence localization. Thus, like Ma et al. [21], we build the following matrix: Pij = [I, A, A2, ..., Ak1]i,j RK (12) where is the identity matrix and the parameter controls the maximum number of hops considered. We incorporate this graph characteristic into our models attention mechanism to enable more nuanced and flexible structural priors. The complete decoder layer is illustrated in Figure 13. Based on Equations 11 and 12, we use the following bias"
        }
    ],
    "affiliations": []
}