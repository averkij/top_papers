{
    "paper_title": "From Data to Rewards: a Bilevel Optimization Perspective on Maximum Likelihood Estimation",
    "authors": [
        "Abdelhakim Benechehab",
        "Gabriel Singer",
        "Corentin Léger",
        "Youssef Attia El Hili",
        "Giuseppe Paolo",
        "Albert Thomas",
        "Maurizio Filippone",
        "Balázs Kégl"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Generative models form the backbone of modern machine learning, underpinning state-of-the-art systems in text, vision, and multimodal applications. While Maximum Likelihood Estimation has traditionally served as the dominant training paradigm, recent work have highlighted its limitations, particularly in generalization and susceptibility to catastrophic forgetting compared to Reinforcement Learning techniques, such as Policy Gradient methods. However, these approaches depend on explicit reward signals, which are often unavailable in practice, leaving open the fundamental problem of how to align generative models when only high-quality datasets are accessible. In this work, we address this challenge via a Bilevel Optimization framework, where the reward function is treated as the optimization variable of an outer-level problem, while a policy gradient objective defines the inner-level. We then conduct a theoretical analysis of this optimization problem in a tractable setting and extract insights that, as we demonstrate, generalize to applications such as tabular classification and model-based reinforcement learning. We release the code at https://github.com/abenechehab/nll_to_po ."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 1 ] . s [ 3 4 2 6 7 0 . 0 1 5 2 : r Preprint. Under review. FROM DATA TO REWARDS: BILEVEL OPTIMIZATION PERSPECTIVE ON MAXIMUM LIKELIHOOD ESTIMATION Abdelhakim Benechehab 1,2, Gabriel Singer 1, Corentin Leger1, Youssef Attia El Hili1, Giuseppe Paolo3, Albert Thomas1, Maurizio Filippone4, Balazs Kegl1 1 Huawei Noahs Ark Lab, Paris 2 Department of Data Science, EURECOM 3 Cognizant AI Lab, Paris 4 Statistics Program, KAUST Equal contribution. abdelhakim.benechehab@gmail.com Correspondence:"
        },
        {
            "title": "ABSTRACT",
            "content": "Generative models form the backbone of modern machine learning, underpinning state-of-the-art systems in text, vision, and multimodal applications. While Maximum Likelihood Estimation has traditionally served as the dominant training paradigm, recent work have highlighted its limitations, particularly in generalization and susceptibility to catastrophic forgetting compared to Reinforcement Learning techniques, such as Policy Gradient methods. However, these approaches depend on explicit reward signals, which are often unavailable in practice, leaving open the fundamental problem of how to align generative models when only highquality datasets are accessible. In this work, we address this challenge via Bilevel Optimization framework, where the reward function is treated as the optimization variable of an outer-level problem, while policy gradient objective defines the inner-level. We then conduct theoretical analysis of this optimization problem in tractable setting and extract insights that, as we demonstrate, generalize to applications such as tabular classification and model-based reinforcement learning. We release the code at https://github.com/abenechehab/nll to po."
        },
        {
            "title": "INTRODUCTION",
            "content": "Generative models have become central to modern machine learning research, driving advances in text (Brown et al., 2020; DeepSeek-AI et al., 2025), image (Rombach et al., 2021; Ramesh et al., 2021), and multimodality (Zhang et al., 2024; Bai et al., 2025; Fu et al., 2025; Łajszczak et al., 2024; Yin et al., 2024) under the umbrella of Generative AI (GenAI). Their ability to synthesize realistic content has made them foundational in applications ranging from decision making (Shi et al., 2025; Kim et al., 2024; Intelligence et al., 2025) to scientific discovery (Manica et al., 2023; Lu et al., 2024). Traditionally, such models are trained via Maximum Likelihood Estimation (MLE), where the parameters of the generative model are optimized to maximize the probability of observed data. This approach provides principled framework for fitting models to large datasets and remains the backbone of many generative learning pipelines. Notably, this approach is omnipresent in todays Large Language Models (LLMs) through the next token prediction paradigm (Vaswani et al., 2023; Brown et al., 2020; DeepSeek-AI et al., 2025). However, recent breakthroughs in LLMs research, demonstrate the limitations of MLE alone. Techniques based on Policy Gradient (PG) methods (Bellman, 1958), such as Reinforcement Learning from Human Feedback (Christiano et al., 2017; Stiennon et al., 2020) and more recently Reinforcement Learning from Verifiable Rewards (Shao et al., 2024; DeepSeek-AI et al., 2025), have proven more effective than supervised fine-tuning at aligning models with human preferences and improving generation quality (Shenfeld et al., 2025; Lai et al., 2025; Swamy et al., 2025). These methods leverage explicit or implicit reward signals to guide training beyond likelihood objectives. In many real-world scenarios, explicit reward functions for the tasks we aim to solve are not readily available. Instead, we often have access to high-quality datasets that we wish to use for aligning our 1 Preprint. Under review. models. Depending on the structure of these datasets, several techniques have been proposed to derive reward functions, such as from preference data (Rafailov et al., 2023) or from demonstrations (Finn et al., 2016a;b) when framed within Markov Decision Process (MDP) formalism. Despite these advances, the fundamental question surrounding this problem remains unresolved: Can we learn an implicit reward function from unlabeled data, and exploit the well-developed policy optimization literature to train models more effectively than with MLE? In this paper, we propose the following contributions toward addressing this question: Bilevel optimization perspective on MLE: We reinterpret the MLE training objective as Bilevel Optimization (Bi-O) problem, where the outer-level problem optimizes over the reward function, while the inner-level problem is defined by PG objective with respect to the model parameters. Theoretical analysis: We study this formulation under Gaussian data distribution with the reward given by negatively scaled distance in the output space, deriving insights into the theoretically optimal parameters of the reward function. Practical algorithms: Guided by the theoretical analysis and leveraging implicit differentiation solvers, we propose two practical algorithms for addressing the bilevel optimization problem. We evaluate these algorithms on two MLE applications: tabular classification and model-based reinforcement learning. The remainder of the paper is organized as follows. Section 2 situates our work within the relevant literature, and Section 3 introduces the problem setup and motivates our approach. In Section 4, we address the bilevel optimization problem in the Gaussian case, while Section 5 considers the general setting using implicit differentiation. We then present experimental results in Section 6 and conclude with discussion in Section 7."
        },
        {
            "title": "2 RELATED WORK",
            "content": "PG vs MLE for Generative models. Generative models aim to capture the underlying distribution of observed data, with the goal of synthesizing realistic samples afterwards, e.g. text generation (Brown et al., 2020) and image generation (Rombach et al., 2021; Ramesh et al., 2021). Many of the existing generative modeling approaches such as Autoregressive models (Radford & Narasimhan, 2018; Vaswani et al., 2023; Radford et al., 2019), Variational AutoEncoders (Kingma & Welling, 2013; Higgins et al., 2017), Generative Adversarial Networks (Goodfellow et al., 2014; Arjovsky et al., 2017), Diffusion Models (Sohl-Dickstein et al., 2015; Rombach et al., 2021), can be framed through the lens of MLE or its approximations. However, and especially in the context of sequence generation, MLE in autoregressive models has been proven to suffer from compounding errors and exposure bias, among other problems (Tan et al., 2019; Bahdanau et al., 2017; Ranzato et al., 2016; Bengio et al., 2015; Venkatraman et al., 2015; Benechehab et al., 2024). As an alternative approach, PG methods have emerged as more effective way to sample the output space when reward function is available (Bahdanau et al., 2017). Beyond vanilla PG, more sophisticated methods have been developed, such as Reward-Augmented Maximum Likelihood (Norouzi et al., 2016; Volkovs et al., 2011), where reward-based stationary sampling distribution is defined, Softmax Policy Gradient (Ding & Soricut, 2017), an intermediate approach between sampling the model and sampling reward-based distribution, and MIXER (Ranzato et al., 2016), scheduling approach that gradually transitions from MLE to PG using the REINFORCE algorithm (Williams, 1992). Besides autoregressive models, policy gradient methods have also been used to train (or finetune) Diffusion models (Black et al., 2024; Uehara et al., 2024; Zekri & Boulle, 2025), and GANs (Paria et al., 2017; Yu et al., 2017). Reward models. Policy Gradient methods constitute one class of algorithms for solving MDPs (Bellman, 1958), the central formalism underpinning the RL field. Training generative models with PG methods builds on the formulation of the task as an MDP. In this setting, the reward function plays pivotal role. The most direct way of learning reward model is via supervised learning from past interactions, as done in Model-based Reinforcement Learning (Chua et al., 2018; Janner et al., 2019; Yu et al., 2020; Hafner et al., 2021; Kegl et al., 2021; Benechehab et al., 2025). Beyond the supervised approach, several other paradigms for reward learning have been developed. 2 Preprint. Under review. Learning from Demonstrations includes Inverse RL methods (Abbeel & Ng, 2004; Ziebart et al., 2008; Finn et al., 2016a;b) that learn reward model Rθ under which demonstrations of the form (s, a, snext) are optimal. Another paradigm, Learning from Goals, defines the reward function with respect to reaching goal in the state space (Liu et al., 2022). In this setting, goal attainment has been modeled in terms of spatial distances (Nachum et al., 2018; Mazzaglia et al., 2024), temporal distances (Hartikainen et al., 2020; Wang et al., 2025), and semantic similarity (Sontakke et al., 2023; Fan et al., 2022). The Learning from Preferences approach relies on transforming preference data of the form (τ0 τ1), where τi is trajectory (s1, a1, . . . , sτi, aτi) and is preference relationship, into reward model using the Bradley-Terry model (Bradley & Terry, 1952). Reward models learned from preference data have enabled significant progress in post-training generative models (Kim et al., 2023; Touvron et al., 2023; Rafailov et al., 2023; Song et al., 2024). Starting with InstructGPT (Ouyang et al., 2022), this approach has become standard for improving targeted aspects of LLMs, e.g. safety (Dai et al., 2024), as well as for applications such as mathematical reasoning (Xin et al., 2025; Shao et al., 2024; Luong et al., 2024) and code generation (DeepSeek-AI et al., 2025). Bilevel optimization. Bi-O was originally introduced in economics and game theory by von Stackelberg (1934) to model hierarchical decision-making problems between leader and follower. More broadly, Bi-O offers framework for addressing problems with hierarchical structures, where the task is to optimize two interdependent objective functions: an inner-level objective and an outer-level objective. In machine learning, Bi-O was first applied to feature selection (Bennett et al., 2006) and was later extended to wide spectrum of applications, including hyperparameter optimization (Mackay et al., 2019; Franceschi et al., 2017; Pedregosa, 2016), reinforcement learning (Hong et al., 2022; Nikishin et al., 2021), and meta-learning (Franceschi et al., 2018). Various Bi-O solvers have been proposed to address different regularity conditions on the innerand outer-level objectives. Among these, automatic differentiation-based approaches compute gradients of the outer-level objective by differentiating through the iterative steps of the inner-level optimization algorithm (Wengert, 1964; Linnainmaa, 1976; Domke, 2012; Franceschi et al., 2017). In parallel, implicit differentiation methods (Bengio, 2000) leverage the implicit differentiation theorem to approximately estimate the gradient of the outer loss by solving linear system (Pedregosa, 2016; Chen et al., 2021; Ji et al., 2021; Arbel & Mairal, 2022). Beyond alternating methods, Dagreou et al. (2024) introduce framework where innerand outer-level variables evolve jointly within single training loop. Bi-O has also been generalized to functional settings (Petrulionyte et al., 2024), where the inner-level optimization is carried out over functions in infinite-dimensional spaces. In the context of generative models, some approaches enhance the training efficiency of energy-based latent variable models through bilevel formulations (Bao et al., 2020; Kan et al., 2022), while Xiao et al. (2025) propose bilevel framework for tuning hyperparameters and noise schedules in diffusion models. Bilevel Reinforcement Learning. Bilevel RL optimizes an outer-level objective in the reward parameters, often policy alignment signal, while an inner loop learns policy under that reward (Gaur et al., 2025; Shen et al., 2024; Yang et al., 2025). This framework has been applied in areas such as reward shaping (Zou et al., 2019) and RLHF (Chakraborty et al., 2024). The closest work to ours is (Zeng et al., 2022), which combines MLE with the Maximum-Entropy inverse RL framework. However, they focus on control tasks in the episodic RL setting while we aim at providing general framework for any data modality and any MLE task."
        },
        {
            "title": "3 PRELIMINARIES",
            "content": "In Section 3.1, we begin by motivating the idea of learning reward functions from data, outlining scenarios in which PG methods may be preferred over MLE. We then formally define the problem setup in Section 3.2. 3.1 MOTIVATION In Reinforcement Learning, PG methods are traditionally viewed as producing unbiased yet highvariance gradient estimates, especially in long-horizon or high-dimensional tasks (Greensmith et al., 2001). In contrast, MLE has historically served as the dominant paradigm in supervised learning and probabilistic modeling (Akaike, 1973). However, in the current era of large pretrained models and 3 Preprint. Under review. advanced RL algorithms, these limitations have become less restrictive, giving rise to many cases where PG methods are more advantageous than MLE. first phenomenon is the mismatch between training objectives and evaluation metrics. In sequence prediction, for instance, evaluation scores such as BLEU or ROUGE (widely used for machine translation) do not decompose into token-level likelihoods. While for the widely used autoregressive models MLE is restricted to maximizing token-level likelihoods, PG methods directly optimize sequence-level rewards and naturally account for this discrepancy (Norouzi et al., 2016; Ding & Soricut, 2017; Ranzato et al., 2016). Another key phenomenon is catastrophic forgetting. When adapting large language models to downstream tasks through post-training, it is often desirable to preserve prior knowledge while specializing to new distributions. Recent studies (Shenfeld et al., 2025; Lai et al., 2025; Swamy et al., 2025) suggest that on-policy RL fine-tuning achieves this balance more effectively than supervised fine-tuning, since its updates converge to solutions closest in KL divergence to the original policy. Taken together, these observations motivate our approach: rather than maximizing the likelihood directly, we propose general framework that interprets data signals as reward functions, thereby also enabling PG optimization. 3.2 PROBLEM SETUP Let (Ω, F, P) be probability space, and let : Ω and : Ω be two random variables, with Rm and Rn, where (n, m) N2 . Consider maximum likelihood estimation problem where we observe iid realizations = {(xi, yi)}N i=0 from fixed unknown distribution over Y. The goal is to model the conditional distribution using parametric model (cid:98)Y ˆpθ where θ Θ := Rdθ are parameters spanning finite dimensional space with dimension dθ. In the MLE formalism, we optimize the parameters θ by maximizing the log-likelihood, equivalently seen as Kullback-Leibler divergence minimization (Akaike, 1973) (denoted as dKL): θ = arg min θΘ EX [dKL(q(X)ˆpθ(X))] = arg max θΘ EX EY Xq[log ˆpθ(Y X)] (MLE) parallel approach, based on reinforcement learning, consists in maximizing reward function : that evaluates the quality of generated ˆy against the true observations y, resulting in the Policy Gradient (PG) objective. Here we state the entropy-regularized PG objective, variant that is commonly considered in RL algorithms (Haarnoja et al., 2017; 2018; Wen et al., 2024): θ = arg max θΘ EX EY Xq (cid:104) (cid:98)Y ˆpθ (cid:104) (cid:105) r( (cid:98)Y , ) (cid:105) + λH(ˆpθ) , (PG) where λ > 0 is parameter controlling the strength of the regularization, and denotes the entropy. In this work, we ask whether the reward function itself can be seen as an optimization variable over Hilbert space H. The optimal reward function is then determined based on the MLE objective, which now represents the outer-level of the following bilevel optimization problem: Definition 3.1 (Bilevel Optimization problem). EX EY Xq (cid:2)log ˆpθ (Y X)(cid:3) s.t. max rH θ = arg max θΘ EX EY Xq (cid:104) (cid:98)Y ˆpθ (cid:2)r(Y , )(cid:3) + λH(ˆpθ) (cid:105) (Bi-O)"
        },
        {
            "title": "4 SOLVING BI-O IN A TRACTABLE CASE",
            "content": "The objective of this section is to analyze the bilevel optimization problem Bi-O under specific assumptions on the data-generating distribution and the reward parametrization, in which both the innerand outer-level problems admit closed-form solutions. 4 Preprint. Under review."
        },
        {
            "title": "4.1 THEORETICAL ANALYSIS",
            "content": "We start our analysis by stating the following assumptions, which will prove useful in the establishment of our main results: Assumption 4.1 (Gaussian density model). We assume that both the true conditional density and the model density ˆpθ are Gaussian distributions with linear mean functions and fixed covariance matrices: := (ΛX, Σ), (cid:98)Y ˆpθ := (AX, B), where Λ Rnn, Σ S++ Assumption 4.2 (Reward model). Let S++ form: ( (cid:98)Y , ) Rn Rn, rU( (cid:98)Y , ) = ( (cid:98)Y )T U( (cid:98)Y ). (R)1, and θ := (A, B) Θ := Rnn S++ (R). (R), we define the reward model as the following quadratic We first notice that this choice of parametrization is valid as the resulting reward function is maximized in . Furthermore, this parametrization enables that for any Rnn, rU is an element of the Hilbert space of square-integrable real-valued functions with weighted measure. We refer the interested reader to Appendix A.4 for technical definition of and proof of this statement. We now state the main results, showcasing closed-form solutions of the Bi-O problem under the previous assumptions. Proposition 4.1 (Closed-form solution). Under assumption 4.1 and 4.2, the Bi-O problem = arg max (R) S++ EX EY Xq (cid:104) log ˆpθ (cid:105) (Y X) s.t. θ = arg max θΘ EX EY Xq (cid:104) (cid:98)Y ˆpθ (cid:104) (cid:105) ( (cid:98)Y )T U( (cid:98)Y ) (cid:105) + λH(ˆpθ) has exactly one solution that writes: The proof of proposition 4.1 is deferred to Appendix A.1. = λ 2 Σ1. Corollary 4.1.1 (Isotropic case). For = σ2In, the set of solutions of Bi-O is: (cid:26) (cid:27) Fλ,Σ := S++ (R) Tr(U) = (cid:12) (cid:12) (cid:12) (cid:12) λn2 2 Tr(Σ) . Note that, for any given λ > 0, Σ S++ to reward functions we consider for the empirical experiments in Section 4.2. (R), Fλ,Σ = since λn 2 Tr(Σ) In Fλ,Σ which corresponds Interpretation as Mahalanobis distance. We observe that the optimal reward function is charn (R) that is inversely proportional to the covariance matrix of the acterized by matrix S++ data-generating process. With this choice, the reward admits clear interpretation as the negative squared Mahalanobis distance (Mahalanobis, 1936), which measures the distance between the sample (cid:98)Y and the Gaussian distribution centered at with covariance matrix Σ. This perspective suggests that, in practice, the noisier the data, the less strongly the model should be penalized for deviations from samples. Finally, the scaling by λ serves to balance the two competing optimization objectives, reward maximization and entropy regularization. Interpretation as reverse KL minimization. An interesting observation arises when substituting the optimal reward parametrization into the inner-level objective: the PG formulation becomes equivalent to minimizing the reverse KL divergence between the model distribution ˆpθ and the data-generating distribution q. This connection provides an explanation for the empirical results presented in the next section. We therefore state it formally as corollary, with the proof deferred to Appendix A.3: 1We denote by S++ (R) the set of real symmetric positive definite matrices. Preprint. Under review. Figure 1: Synthetic data experiment. The PG loss when paired with the optimal reward function matches the NLL-trained baseline in terms of NLL (left panel), all while having faster convergence in terms of moment matching (center and right panels for the mean and variance, respectively). Corollary 4.1.2. Under the assumptions of proposition 4.1, the optimal parameters θ the lower-level problem with = λ 2 Σ1 minimize the reverse KL divergence between ˆpθ and q: obtained from θ = arg minθΘ EX [dKL (ˆpθ(X) q(X))] . 4.2 EMPIRICAL VALIDATION In this section, we empirically evaluate the theoretical results from Section 4.1. To this end, we generate synthetic data that satisfy assumption 4.1: = {(xi, yi)}N i=0, where xi U([5, 5]n) (U denotes the uniform distribution), and yi q(.xi) := (Λxi, Σ) with diagonal covariance matrix Σ = β2In and β > 0. For the model ˆpθ, we relax the linearity and homoscedasticity assumptions by considering neural network that parametrizes Gaussian distribution, in which both the mean function and the diagonal covariance matrix depend on the input: ˆpθ(.xi) := (µθ(xi), σ2 θ (xi)In). We compare baselines trained with the negative log-likelihood (NLL) and mean squared error (MSE) losses against PG variants, using either negative squared distance reward = In or the optimal reward function derived in corollary 4.1.1 with = λn 2 Tr(Σ) In. Fig. 1 shows how the validation NLL and moment-matching errors (mean and covariance) change over training. Consistent with theory, we observe that adjusting the reward function with the optimal matrix yields learning curve nearly identical to the NLL baseline (yellow and red curves in the left panel of Fig. 1). Moreover, the PG variant with the optimal matrix converges faster than the NLL baseline in matching the moments of the data-generating distribution (center and right panel). Finally, we note that the vanilla PG method (with = In) suffers from diverging NLL due to the variance shrinking to zero for some values of λ which leads to numerical instabilities. Figure 2: Learned distributions comparison on single data point. The PG loss paired with the optimal reward function in corollary 4.1.1 shows optimal convergence, even when compared with the baseline directly optimizing the NLL. To gain further insight, Fig. 2 shows the evolution of the learned distributions for single training data point. In this illustrative example, the PG variant with the optimal reward displays the most natural behavior in fitting the target distribution, unlike the NLL baseline, which initially causes the variance to increase sharply before reducing it to match the target variance. This behavior can be explained by corollary 4.1.2 since minimizing dKL (ˆpθ q) is known to induce mode seeking behavior, thus encouraging ˆpθ to concentrate directly on the mode of q. 6 Preprint. Under review."
        },
        {
            "title": "5 SOLVING BI-O IN GENERAL",
            "content": "In contrast to the previous section, where we assumed access to the data-generating distribution and provided closed-form solution to problem Bi-O, real-world applications typically do not satisfy such assumptions. Consequently, solving the bilevel optimization problem Bi-O by directly optimizing the outer objective offers more general approach applicable to broader class of problems. Bilevel optimization solvers can generally be divided into three categories. Explicit gradient methods treat the gradient update as differentiable mapping and backpropagate through the unrolled optimization path of the inner-level problem (Franceschi et al., 2017). Gradient-free methods instead rely on evolutionary strategies, optimizing the outer objective while considering the inner problem as black-box function (Song et al., 2020; Feng et al., 2021). Finally, implicit differentiation methods leverage the implicit function theorem to reformulate gradient estimation as the solution to linear system (Dagreou et al., 2024; Petrulionyte et al., 2024). In this work, we focus on implicit differentiation, as explicit gradient methods often encounter memory issues from storing long computational graphs, while gradient-free approaches are generally limited by the curse of dimensionality. 5.1 IMPLICIT DIFFERENTIATION Consider reward parametrization rϕ with ϕ Φ := Rdϕ, where dϕ denotes the dimension of the reward parameter space. The optimization of the outer-level problem can thus be restricted to the Hilbert space of reward functions spanned by parameters ϕ Φ (see the appendix for an explicit construction in the case of the Mahalanobis parametrization). Within this setup, implicit differentiation treats the solution of the inner problem, θ, as an implicit function of ϕ and allows one to compute the best-response derivatives ϕθ(ϕ) analytically via the implicit function theorem. To proceed, we define an operator : Φ Θ Θ := Rdθ whose roots characterize the inner-level optimal solution θ(ϕ). That is, for all ϕ Φ, we have f(ϕ, θ(ϕ)) = 0. Leveraging this property, the derivative of interest ϕθ(ϕ) can be determined by solving for ϕf(ϕ, θ(ϕ)) = 0: ϕ Φ, θf (ϕ, θ(ϕ)) ϕθ(ϕ) + ϕf (ϕ, θ(ϕ)) = 0, (1) where ϕθ(ϕ) is obtained by solving the linear system in Eq. (1), enabling gradient descent on the outer problem via the chain rule. In our bilevel optimization formulation, the operator arises naturally from the fixed-point characterization of the gradient update: f(ϕ, θ) = θ + αθLin(ϕ, θ) θ = αθLin(ϕ, θ) where Lin is the inner-level objective and α is learning rate. Under this definition, the first-order optimality condition holds whenever the inner-level optimization converges to local minimum θ(ϕ), where the gradient vanishes, which we assume is plausible hypothesis given any modern stochastic optimizer (e.g. Adam (Kingma & Ba, 2017)). 5.2 EMPIRICAL VALIDATION In practice, we use TorchOpt (Ren et al., 2023), python package that enables differentiable optimization solvers that can be integrated with Pytorch (Paszke et al., 2019) neural network implementations. Precisely, we run the implicit differentiation-based solvers using the Conjugate Gradient algorithm for the linear system resolution, as in (Rajeswaran et al., 2019). We now compare the obtained results, in the same setup as Section 4.2, to get insights into the effectiveness of this kind of bilevel optimization solvers against MLP-based policies. Fig. 3 presents the results of running an implicit differentiation solver for 100 outer iterations, each with 50 inner iterations, and learning rate of 102 for both optimization loops. The outer optimization variable is single parameter (panel -c-) initialized at 1, which defines the diagonal Mahalanobis matrix for the reward: > 0 s.t. = In. Panel -aillustrates the evolution of the outer loss (NLL evaluated on the optimal policy from the inner PG loop), showing clear improvement relative to the initialization at 1 (which corresponds to the Euclidean distance). Additionally, the optimization parameter converges to value close to the theoretical optimum = λn 2 Tr(Σ) , as derived in corollary 4.1.1. This convergence is further supported by panel -e-, which plots the outer 7 Preprint. Under review. Figure 3: Implicit differentiation solver on synthetic data experiment. From left to right: -aouter loss (NLL), -binner reward optimization loop, -ctrajectory of the reward parameter u, -dgradient of the outer loss with respect to u, -ethe outer loss landscape. loss landscape as function of the reward parameter u, revealing roughly convex landscape with global minimum near the theoretical optimum. These results validate our intuition from the tractable case discussed in Section 4, even in the more general setting of MLP-based policies and stochastic optimization solvers within the implicit differentiation framework."
        },
        {
            "title": "6 APPLICATIONS",
            "content": "The goal of this section is to use intuition gained from the previous analysis to derive practical algorithms that we can validate on common MLE tasks from the literature. he) - heuristic Algorithm 1 PG(U Input: Data = {(xi, yi)}N 1. Estimate cov matrix ˆΣ = cov({(yi)}N he(λ, ˆΣ)) 2. loss PG(U 3. train policy(ˆpθ, D, loss) Return: learned model ˆpθ i=0, model ˆpθ, λ i=0) im) - implicit differentiation i=0, model ˆpθ, λ Algorithm 2 PG(U Input: Data = {(xi, yi)}N 1. 2. loss PG(U 3. train policy(ˆpθ, D, loss) Return: learned model ˆpθ im) im imp diff solver(D, λ) We build on the theoretical analysis in Section 4.1 to suggest realistic way to estimate the optimal reward parametrization U. The main challenge with this approach lies in estimating the covariance matrix-dependent term. As stated in Algorithm 1, we propose an approximate approach that estimates an empirical covariance matrix ˆΣ from the training data. In parallel, we use the implicit differentiationbased bilevel solver to provide gradient-based approach (Algorithm 2). Such an approach, is more general as its not sensitive to the estimation error on the covariance matrix, nor requires the validity of the assumptions under which we derive our theoretical results. In the following, we use both Algorithm 1 and Algorithm 2 to benchmark our method against vanilla PG and NLL losses in two real-world applications: tabular classification, and model-based reinforcement learning. Note that, in the experiments, we are effectively solving the inner-level problem of the Bi-O formulation, while substituting the reward function either with the optimal matrices im, or with the identity In for the squared-distance baseline. he and 6.1 TABULAR CLASSIFICATION We evaluate our framework on two tabular classification datasets from the UCI repository (Wang, 2023). Specifically, we train multiclass logistic regression model with the PG loss, where the reward is defined using the distance between the one-hot encoded ground-truth labels and generations sampled from the models softmax distribution. We consider both Dataset Method Accuracy/102 Credit default Poker NLL PG(In) PG(U he) NLL PG(In) PG(U he) 79.8 .012 75.5 .023 82.0 .001 48.6 .001 38.2 .039 52.4 .001 Table 1: Accuracy. 8 Preprint. Under review. multiclass (Poker) and imbalanced binary classification (Credit default). In the case of unbalanced datasets, accuracy alone can be misleading, in which case we additionally report the Area Under the ROC curve (AUC). Table 1 shows the accuracy results, while Table 2 extends this to the AUC for the binary classification task. On the imbalanced Credit default dataset, accuracy is high across methods but AUC reveals that PG(U he) better separates classes. The Poker dataset remains challenging for all methods, yet PG(U Method AUC/102 NLL PG(In) PG(U he) 70.5 .005 57.7 .632 71.3 .001 Table 2: AUC. on the binary classification task Credit default. he) still provides the best performance."
        },
        {
            "title": "6.2 MODEL-BASED REINFORCEMENT LEARNING",
            "content": "t, si t, ai t+1)}N Model-Based Reinforcement Learning (MBRL) addresses the supervised learning problem of estimating the (possibly stochastic) transition function of MDP. Typically, we assume access to data of the form = {(si i=0, consisting of trajectories of states and actions collected by an unknown policy. The goal is to approximate the next-state distribution St+1 St, At q. In practice, the dynamics model is often Gaussian probabilistic model trained via log-likelihood (Chua et al., 2018; Janner et al., 2019), which makes it directly applicable to our experimental setup. We consider three D4RL (Fu et al., 2021) HalfCheetah tasks, each from different data-collecting policy: simple, medium, and expert, accessible through the Minari project (Younis et al., 2024). All models train for 400 epochs with Adam optimizer (learning rate = 103) and λ = 1. Table 3 presents MSE and NLL results across the different losses under evaluation. As expected, NLL-optimized models achieve the strongest performance on NLL. However, and consistent with the synthetic data experiments in Section 4.2, we observe that the PG loss with the optimal reward heuristic PG(U he) delivers significant NLL improvements compared to PG with the negative squared-distance reward PG(In). Moreover, the optimal reward obtained from the implicit differentiation solver PG(U im) achieves the second-best NLL performance while simultaneously achieving optimal MSE, an important property in the context of MBRL, particularly when using deterministic planners. Task Metrics MSE/102 NLL/102 425 3 459 3 539 3 199 1 241 4 174 3 230 1 274 1 198 1 47 1 73 1 49 528 18 796 70 420 24 267 1 286 1 290 1 NLL simple medium expert PG(In) simple medium expert PG(U simple medium expert PG(U simple medium expert he) im) These findings support our intuition that PG methods with an optimal reward can enhance NLL (in this case also MSE), as guaranteed by our bilevel optimization framework. It is worth emphasizing that, in the context of MBRL, the metric of ultimate interest is the policy performance derived from these models, typically quantified by the return (e.g. , the cumulative reward up to the task horizon). We defer exploration of this direction to future work, as the focus in the present paper is on the MLE task. Table 3: MBRL experiment. The PG loss with optimal reward comes second to the NLL baseline in terms of NLL, and ranks first in terms of MSE. 208 1 232 2 208 2 190 1 231 1"
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this paper, we investigated how to learn reward functions that, when used within policy gradient algorithm, produce models that are optimal in the sense of maximum likelihood with respect to observed data. To address this question, we introduced bilevel optimization framework and derived closed-form solutions under specific assumptions on the reward model and the data-generating distribution. Finally, we validated our approach against practical applications, showing that our 9 Preprint. Under review. framework facilitates more effective use of the advantages of PG methods through an optimal choice of the reward function. Limitations & Future Work. The reward parametrization used in our work is somewhat restrictive, potentially limiting its flexibility across wider range of tasks. While we have validated the framework in both synthetic and practical environments, further large-scale experiments are necessary to more thoroughly assess its generalizability to complex applications. Additionally, our experiments have predominantly focused on tabular data, we therefore aim to extend our approach to domains where Maximum Likelihood Estimation is known to encounter challenges, such as compounding errors, exposure bias, and limited exploration. These include areas like LLM fine-tuning, structured prediction tasks (e.g. , machine translation), and time series forecasting. We intend to actively explore these directions in future research."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "The authors extend their gratitude to Ambroise Odonnat, Hamza Cherkaoui, and Michael Arbel for insightful discussions on the initial drafts of this project. This work was made possible thanks to open-source software, including Python (Van Rossum & Drake Jr, 1995), PyTorch (Paszke et al., 2019), and TorchOpt (Ren et al., 2023)."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "In order to ensure reproducibility we release the code at https://github.com/abenechehab/nll to po. Implementation details and relevant hyperparameters are provided in each experiment section of the main text."
        },
        {
            "title": "REFERENCES",
            "content": "Pieter Abbeel and Andrew Y. Ng. Apprenticeship learning via inverse reinforcement learning. In Proceedings of the Twenty-First International Conference on Machine Learning, ICML 04, pp. 1, New York, NY, USA, 2004. Association for Computing Machinery. ISBN 1581138385. doi: 10.1145/1015330.1015430. URL https://doi.org/10.1145/1015330.1015430. Akaike. Information Theory and an Extension of the Maximum Likelihood Principle. In 2nd International Symposium on Information Theory, 1973, pp. 268281. Publishing House of the Hungarian Academy of Sciences, 1973. Michael Arbel and Julien Mairal. Amortized implicit differentiation for stochastic bilevel optimization. In International Conference on Learning Representations, 2022. URL https://openreview. net/forum?id=3PN4iyXBeF. Martin Arjovsky, Soumith Chintala, and Leon Bottou. Wasserstein gan, 2017. URL https: //arxiv.org/abs/1701.07875. Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An actor-critic algorithm for sequence prediction, 2017. URL https://arxiv.org/abs/1607.07086. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. URL https://arxiv.org/abs/2502.13923. Fan Bao, Chongxuan Li, Kun Xu, Hang Su, Jun Zhu, and Bo Zhang. Bi-level score matching for learning energy-based latent variable models. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. 10 Preprint. Under review. Richard Bellman. Dynamic programming and stochastic control processes. Information and Control, 1(3):228239, 1958. https://doi.org/10. 1016/S0019-9958(58)80003-0. URL https://www.sciencedirect.com/science/ article/pii/S0019995858800030. ISSN 0019-9958. doi: Abdelhakim Benechehab, Albert Thomas, Giuseppe Paolo, Maurizio Filippone, and Balazs Kegl. multi-step loss function for robust learning of the dynamics in model-based reinforcement learning, 2024. URL https://arxiv.org/abs/2402.03146. Abdelhakim Benechehab, Youssef Attia El Hili, Ambroise Odonnat, Oussama Zekri, Albert Thomas, Giuseppe Paolo, Maurizio Filippone, Ievgen Redko, and Balazs Kegl. Zero-shot model-based reinforcement learning using large language models. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id= uZFXpPrwSh. Samy Bengio, Oriol Vinyals, Navdeep Jaitly, and Noam Shazeer. Scheduled sampling for sequence prediction with recurrent neural networks, 2015. URL https://arxiv.org/abs/1506. 03099. Yoshua Bengio. Gradient-based optimization of hyperparameters. Neural Computation, 12(8): 18891900, 2000. doi: 10.1162/089976600300015187. K.P. Bennett, Jing Hu, Xiaoyun Ji, G. Kunapuli, and Jong-Shi Pang. Model selection via bilevel optimization. In The 2006 IEEE International Joint Conference on Neural Network Proceedings, pp. 19221929, 2006. doi: 10.1109/IJCNN.2006.246935. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=YCWjhGrJFD. Ralph Allan Bradley and Milton E. Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. ISSN 00063444, 14643510. URL http://www.jstor.org/stable/2334029. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language models are few-shot learners, 2020. URL https: //arxiv.org/abs/2005.14165. Souradip Chakraborty, Amrit Bedi, Alec Koppel, Huazheng Wang, Dinesh Manocha, Mengdi Wang, and Furong Huang. PARL: unified framework for policy alignment in reinforcement learning from human feedback. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=ByR3NdDSZB. Tianyi Chen, Yuejiao Sun, and Wotao Yin. Closing the gap: Tighter analysis of alternating stochastic gradient methods for bilevel problems. In M. Ranzato, A. Beygelzimer, Y. Dauphin, P.S. Liang, and J. Wortman Vaughan (eds.), Advances in Neural Information Processing Systems, volume 34, pp. 2529425307. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2021/ 2021. file/d4dd111a4fd973394238aca5c05bebe3-Paper.pdf. Paul Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario In I. Guyon, U. Von Amodei. Deep reinforcement learning from human preferences. Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 30. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2017/ 2017. file/d5e2c0adad503c91f91df240d0cd4e49-Paper.pdf. Preprint. Under review. Kurtland Chua, Roberto Calandra, Rowan McAllister, and Sergey Levine. Deep reinforcement In Advances in Neural learning in handful of trials using probabilistic dynamics models. Information Processing Systems 31, pp. 47544765. Curran Associates, Inc., 2018. Mathieu Dagreou, Pierre Ablin, Samuel Vaiter, and Thomas Moreau. framework for bilevel optimization that enables stochastic and global variance reduction algorithms, 2024. URL https: //arxiv.org/abs/2201.13409. Josef Dai, Xuehai Pan, Ruiyang Sun, Jiaming Ji, Xinbo Xu, Mickel Liu, Yizhou Wang, and Yaodong Yang. Safe RLHF: Safe reinforcement learning from human feedback. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum? id=TyFrPOKYXw. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Nan Ding and Radu Soricut. Cold-start reinforcement learning with softmax policy gradient. In Neural Information Processing Systems, 2017. URL https://api.semanticscholar. org/CorpusID:4206469. Justin Domke. Generic methods for optimization-based modeling. In Neil D. Lawrence and Mark Girolami (eds.), Proceedings of the Fifteenth International Conference on Artificial Intelligence and Statistics, volume 22 of Proceedings of Machine Learning Research, pp. 318326, La Palma, Canary Islands, 2123 Apr 2012. PMLR. URL https://proceedings.mlr.press/v22/ domke12.html. Linxi Fan, Guanzhi Wang, Yunfan Jiang, Ajay Mandlekar, Yuncong Yang, Haoyi Zhu, Andrew Tang, De-An Huang, Yuke Zhu, and Anima Anandkumar. Minedojo: Building open-ended embodied agents with internet-scale knowledge. In Thirty-sixth Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2022. URL https://openreview.net/forum? id=rc8o_j8I8PX. Xidong Feng, Oliver Slumbers, Ziyu Wan, Bo Liu, Stephen McAleer, Ying Wen, Jun Wang, and Yaodong Yang. Neural auto-curricula, 2021. URL https://arxiv.org/abs/2106. 02745. 12 Preprint. Under review. Chelsea Finn, Paul Christiano, Pieter Abbeel, and Sergey Levine. connection between generative adversarial networks, inverse reinforcement learning, and energy-based models, 2016a. URL https://arxiv.org/abs/1611.03852. Chelsea Finn, Sergey Levine, and Pieter Abbeel. Guided cost learning: Deep inverse optimal control via policy optimization. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 4958, New York, New York, USA, 2022 Jun 2016b. PMLR. URL https://proceedings.mlr.press/v48/finn16.html. Luca Franceschi, Michele Donini, Paolo Frasconi, and Massimiliano Pontil. Forward and reverse gradient-based hyperparameter optimization. In Doina Precup and Yee Whye Teh (eds.), Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pp. 11651173. PMLR, 0611 Aug 2017. URL https://proceedings.mlr.press/v70/franceschi17a.html. Luca Franceschi, Paolo Frasconi, Saverio Salzo, Riccardo Grazzi, and Massimiliano Pontil. Bilevel programming for hyperparameter optimization and meta-learning. In Jennifer Dy and Andreas Krause (eds.), Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 15681577. PMLR, 1015 Jul 2018. URL https://proceedings.mlr.press/v80/franceschi18a.html. Chaoyou Fu, Haojia Lin, Xiong Wang, Yi-Fan Zhang, Yunhang Shen, Xiaoyu Liu, Haoyu Cao, Zuwei Long, Heting Gao, Ke Li, Long Ma, Xiawu Zheng, Rongrong Ji, Xing Sun, Caifeng Shan, and Ran He. Vita-1.5: Towards gpt-4o level real-time vision and speech interaction, 2025. URL https://arxiv.org/abs/2501.01957. Justin Fu, Aviral Kumar, Ofir Nachum, George Tucker, and Sergey Levine. D4rl: Datasets for deep data-driven reinforcement learning, 2021. URL https://arxiv.org/abs/2004.07219. Mudit Gaur, Utsav Singh, Amrit Singh Bedi, Raghu Pasupathu, and Vaneet Aggarwal. On the sample complexity bounds in bilevel reinforcement learning, 2025. URL https://arxiv.org/abs/ 2503.17644. Ian J. Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks, 2014. URL https: //arxiv.org/abs/1406.2661. Evan Greensmith, Peter Bartlett, and Jonathan Baxter. Variance reduction techniques for gradient estimates in reinforcement learning. In T. Dietterich, S. Becker, and Z. Ghahramani (eds.), Advances in Neural Information Processing Systems, volume 14. MIT Press, URL https://proceedings.neurips.cc/paper_files/paper/2001/ 2001. file/584b98aac2dddf59ee2cf19ca4ccb75e-Paper.pdf. Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. 2017. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: Off-policy maximum entropy deep reinforcement learning with stochastic actor, 2018. URL https: //arxiv.org/abs/1801.01290. Danijar Hafner, Timothy Lillicrap, Mohammad Norouzi, and Jimmy Ba. Mastering atari with discrete world models. In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=0oabwyZbOu. Kristian Hartikainen, Xinyang Geng, Tuomas Haarnoja, and Sergey Levine. Dynamical distance learning for semi-supervised and unsupervised skill discovery. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=H1lmhaVtvr. Irina Higgins, Loic Matthey, Arka Pal, Christopher Burgess, Xavier Glorot, Matthew Botvinick, Shakir Mohamed, and Alexander Lerchner. beta-VAE: Learning basic visual concepts with constrained variational framework. In International Conference on Learning Representations, 2017. URL https://openreview.net/forum?id=Sy2fzU9gl. 13 Preprint. Under review. Mingyi Hong, Hoi-To Wai, Zhaoran Wang, and Zhuoran Yang. two-timescale framework for bilevel optimization: Complexity analysis and application to actor-critic, 2022. URL https: //arxiv.org/abs/2007.05170. Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Manuel Y. Galliker, Dibya Ghosh, Lachy Groom, Karol Hausman, Brian Ichter, Szymon Jakubczak, Tim Jones, Liyiming Ke, Devin LeBlanc, Sergey Levine, Adrian Li-Bell, Mohith Mothukuri, Suraj Nair, Karl Pertsch, Allen Z. Ren, Lucy Xiaoyang Shi, Laura Smith, Jost Tobias Springenberg, Kyle Stachowicz, James Tanner, Quan Vuong, Homer Walke, Anna Walling, Haohuan Wang, Lili Yu, and Ury Zhilinsky. π0.5: vision-language-action model with open-world generalization, 2025. URL https://arxiv. org/abs/2504.16054. Michael Janner, Justin Fu, Marvin Zhang, and Sergey Levine. When to trust your model: Modelbased policy optimization. In H. Wallach, H. Larochelle, A. Beygelzimer, F. d'Alche-Buc, E. Fox, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 32. Curran Associates, Inc., 2019. Kaiyi Ji, Junjie Yang, and Yingbin Liang. Bilevel optimization: Convergence analysis and enhanced design, 2021. URL https://arxiv.org/abs/2010.07962. Ge Kan, Jinhu Lu, Tian Wang, Baochang Zhang, Aichun Zhu, Lei Huang, Guodong Guo, and Hichem Snoussi. Bi-level doubly variational learning for energy-based latent variable models, 2022. URL https://arxiv.org/abs/2203.14702. Balazs Kegl, Gabriel Hurtado, and Albert Thomas. Model-based micro-data reinforcement learning: In International Conferwhat are the crucial model properties and which model to choose? ence on Learning Representations, 2021. URL https://openreview.net/forum?id= p5uylG94S68. Changyeon Kim, Jongjin Park, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. Preference transformer: Modeling human preferences using transformers for RL. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum? id=Peot1SFDX0. Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, Quan Vuong, Thomas Kollar, Benjamin Burchfiel, Russ Tedrake, Dorsa Sadigh, Sergey Levine, Percy Liang, and Chelsea Finn. Openvla: An open-source vision-language-action model, 2024. URL https://arxiv.org/abs/2406. 09246. Diederik P. Kingma and Jimmy Ba. Adam: method for stochastic optimization, 2017. URL https://arxiv.org/abs/1412.6980. Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. CoRR, abs/1312.6114, 2013. URL https://api.semanticscholar.org/CorpusID:216078090. Song Lai, Haohan Zhao, Rong Feng, Changyi Ma, Wenzhuo Liu, Hongbo Zhao, Xi Lin, Dong Yi, Min Xie, Qingfu Zhang, Hongbin Liu, Gaofeng Meng, and Fei Zhu. Reinforcement fine-tuning naturally mitigates forgetting in continual post-training, 2025. URL https://arxiv.org/ abs/2507.05386. Seppo Linnainmaa. Taylor expansion of the accumulated rounding error. BIT, 16(2):146160, June 1976. ISSN 0006-3835. doi: 10.1007/BF01931367. URL https://doi.org/10.1007/ BF01931367. Minghuan Liu, Menghui Zhu, and Weinan Zhang. Goal-conditioned reinforcement learning: Problems and solutions, 2022. URL https://arxiv.org/abs/2201.08299. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI Scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292, 2024. Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning, 2024. URL https://arxiv.org/abs/2401.08967. 14 Preprint. Under review. Matthew Mackay, Paul Vicol, Jonathan Lorraine, David Duvenaud, and Roger Grosse. Self-tuning networks: Bilevel optimization of hyperparameters using structured best-response functions. In International Conference on Learning Representations, 2019. URL https://openreview. net/forum?id=r1eEG20qKQ. P.C. Mahalanobis. On the generalised distance in statistics. Proceedings of the National Institute of Sciences of India, 2:4955, 1936. Matteo Manica, Jannis Born, Joris Cadow, Dimitrios Christofidellis, Ashish Dave, Dean Clarke, Yves Gaetan Nana Teukam, Giorgio Giannone, Samuel C. Hoffman, Matthew Buchan, Vijil Chenthamarakshan, Timothy Donovan, Hsiang Han Hsu, Federico Zipoli, Oliver Schilter, Akihiro Kishimoto, Lisa Hamada, Inkit Padhi, Karl Wehden, Lauren McHugh, Alexy Khrabrov, Payel Das, Seiji Takeda, and John R. Smith. Accelerating material design with the generative toolkit for scientific discovery. npj Computational Materials, 9(1), 2023. ISSN 2057-3960. doi: 10.1038/ s41524-023-01028-1. URL http://dx.doi.org/10.1038/s41524-023-01028-1. Pietro Mazzaglia, Tim Verbelen, Bart Dhoedt, Aaron Courville, and Sai Rajeswar. Genrl: Multimodalfoundation world models for generalization in embodied agents, 2024. URL https://arxiv. org/abs/2406.18043. Ofir Nachum, Shixiang Gu, Honglak Lee, and Sergey Levine. Data-efficient hierarchical reinforcement learning, 2018. URL https://arxiv.org/abs/1805.08296. Evgenii Nikishin, Romina Abachi, Rishabh Agarwal, and Pierre-Luc Bacon. Control-oriented modelbased reinforcement learning with implicit differentiation, 2021. URL https://arxiv.org/ abs/2106.03273. Mohammad Norouzi, Samy Bengio, zhifeng Chen, Navdeep Jaitly, Mike Schuster, Yonghui Reward augmented maximum likelihood for neural strucWu, and Dale Schuurmans. tured prediction. In D. Lee, M. Sugiyama, U. Luxburg, I. Guyon, and R. Garnett (eds.), Advances in Neural Information Processing Systems, volume 29. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2016/ 2016. file/2f885d0fbe2e131bfc9d98363e55d1d4-Paper.pdf. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback, 2022. URL https://arxiv.org/abs/2203.02155. Biswajit Paria, Avisek Lahiri, and Prabir Kumar Biswas. Policygan: Training generative adversarial networks using policy gradient. In 2017 Ninth International Conference on Advances in Pattern Recognition (ICAPR), pp. 16, 2017. doi: 10.1109/ICAPR.2017.8593063. Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, Alban Desmaison, Andreas Kopf, Edward Yang, Zach DeVito, Martin Raison, Alykhan Tejani, Sasank Chilamkurthy, Benoit Steiner, Lu Fang, Junjie Bai, and Soumith Chintala. PyTorch: an imperative style, high-performance deep learning library. Curran Associates Inc., Red Hook, NY, USA, 2019. Fabian Pedregosa. Hyperparameter optimization with approximate gradient. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of The 33rd International Conference on Machine Learning, volume 48 of Proceedings of Machine Learning Research, pp. 737746, New York, New York, USA, 2022 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/ pedregosa16.html. Ieva Petrulionyte, Julien Mairal, and Michael Arbel. Functional bilevel optimization for machine learning. In NeurIPS (Spotlight Poster), 2024. arXiv preprint arXiv:2403.20233. Alec Radford and Karthik Narasimhan. Improving language understanding by generative pre-training. 2018. URL https://api.semanticscholar.org/CorpusID:49313245. 15 Preprint. Under review. Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language models are unsupervised multitask learners. 2019. URL https://api.semanticscholar. org/CorpusID:160025533. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=HPuSIXJaa9. Aravind Rajeswaran, Chelsea Finn, Sham Kakade, and Sergey Levine. Meta-learning with implicit gradients, 2019. URL https://arxiv.org/abs/1909.04630. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In Marina Meila and Tong Zhang (eds.), Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pp. 88218831. PMLR, 1824 Jul 2021. URL https://proceedings.mlr.press/v139/ramesh21a.html. MarcAurelio Ranzato, Sumit Chopra, Michael Auli, and Wojciech Zaremba. Sequence level training with recurrent neural networks, 2016. URL https://arxiv.org/abs/1511.06732. Jie Ren, Xidong Feng, Bo Liu, Xuehai Pan, Yao Fu, Luo Mai, and Yaodong Yang. Torchopt: An efficient library for differentiable optimization. Journal of Machine Learning Research, 24(367): 114, 2023. URL http://jmlr.org/papers/v24/23-0191.html. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2021. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models, 2024. URL https://arxiv.org/abs/ 2402.03300. Han Shen, Zhuoran Yang, and Tianyi Chen. Principled penalty-based methods for bilevel reinforcement learning and rlhf, 2024. URL https://arxiv.org/abs/2402.06886. Idan Shenfeld, Jyothish Pari, and Pulkit Agrawal. Rls razor: Why online reinforcement learning forgets less, 2025. URL https://arxiv.org/abs/2509.04259. Lucy Xiaoyang Shi, Brian Ichter, Michael Equi, Liyiming Ke, Karl Pertsch, Quan Vuong, James Tanner, Anna Walling, Haohuan Wang, Niccolo Fusai, Adrian Li-Bell, Danny Driess, Lachy Groom, Sergey Levine, and Chelsea Finn. Hi robot: Open-ended instruction following with hierarchical vision-language-action models, 2025. URL https://arxiv.org/abs/2502.19417. Jascha Sohl-Dickstein, Eric A. Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics, 2015. URL https://arxiv.org/abs/ 1503.03585. Feifan Song, Bowen Yu, Minghao Li, Haiyang Yu, Fei Huang, Yongbin Li, and Houfeng Wang. Preference ranking optimization for human alignment, 2024. URL https://arxiv.org/ abs/2306.17492. Xingyou Song, Wenbo Gao, Yuxiang Yang, Krzysztof Choromanski, Aldo Pacchiano, and Yunhao Tang. Es-maml: Simple hessian-free meta learning. In International Conference on Learning Representations, 2020. URL https://openreview.net/forum?id=S1exA2NtDB. Sumedh Anand Sontakke, Jesse Zhang, Seb Arnold, Karl Pertsch, Erdem Biyik, Dorsa Sadigh, Chelsea Finn, and Laurent Itti. RoboCLIP: One demonstration is enough to learn robot policies. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=DVlawv2rSI. 16 Preprint. Under review. Nisan Stiennon, Long Ouyang, Jeff Wu, Daniel M. Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei, and Paul Christiano. Learning to summarize from human feedback. In Proceedings of the 34th International Conference on Neural Information Processing Systems, NIPS 20, Red Hook, NY, USA, 2020. Curran Associates Inc. ISBN 9781713829546. Gokul Swamy, Sanjiban Choudhury, Wen Sun, Zhiwei Steven Wu, and J. Andrew Bagnell. All roads lead to likelihood: The value of reinforcement learning in fine-tuning, 2025. URL https: //arxiv.org/abs/2503.01067. Bowen Tan, Zhiting Hu, Zichao Yang, Ruslan Salakhutdinov, and Eric Xing. Connecting the dots between mle and rl for sequence prediction, 2019. URL https://arxiv.org/abs/1811. 09740. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, Brian Fuller, Cynthia Gao, Vedanuj Goswami, Naman Goyal, Anthony Hartshorn, Saghar Hosseini, Rui Hou, Hakan Inan, Marcin Kardas, Viktor Kerkez, Madian Khabsa, Isabel Kloumann, Artem Korenev, Punit Singh Koura, Marie-Anne Lachaux, Thibaut Lavril, Jenya Lee, Diana Liskovich, Yinghai Lu, Yuning Mao, Xavier Martinet, Todor Mihaylov, Pushkar Mishra, Igor Molybog, Yixin Nie, Andrew Poulton, Jeremy Reizenstein, Rashi Rungta, Kalyan Saladi, Alan Schelten, Ruan Silva, Eric Michael Smith, Ranjan Subramanian, Xiaoqing Ellen Tan, Binh Tang, Ross Taylor, Adina Williams, Jian Xiang Kuan, Puxin Xu, Zheng Yan, Iliyan Zarov, Yuchen Zhang, Angela Fan, Melanie Kambadur, Sharan Narang, Aurelien Rodriguez, Robert Stojnic, Sergey Edunov, and Thomas Scialom. Llama 2: Open foundation and fine-tuned chat models, 2023. URL https://arxiv.org/abs/2307.09288. Masatoshi Uehara, Yulai Zhao, Ehsan Hajiramezanali, Gabriele Scalia, Gokcen Eraslan, Avantika Lal, Sergey Levine, and Tommaso Biancalani. Bridging model-based optimization and generative modeling via conservative fine-tuning of diffusion models, 2024. URL https://arxiv.org/ abs/2405.19673. Guido Van Rossum and Fred Drake Jr. Python reference manual. Centrum voor Wiskunde en Informatica Amsterdam, 1995. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv.org/ abs/1706.03762. Arun Venkatraman, Martial Hebert, and J. Andrew Bagnell. Improving multi-step prediction of learned time series models. In Proceedings of the Twenty-Ninth AAAI Conference on Artificial Intelligence, AAAI15, pp. 30243030. AAAI Press, 2015. ISBN 0262511290. Maksims N. Volkovs, Hugo Larochelle, and Richard S. Zemel. Loss-sensitive training of probabilistic conditional random fields, 2011. URL https://arxiv.org/abs/1107.1805. Heinrich von Stackelberg. Marktform und Gleichgewicht. Springer-Verlag, Berlin, 1934. English translation: The Theory of the Market Economy, Oxford University Press, 1952. Jingcong Wang. Uci datasets, 2023. URL https://dx.doi.org/10.21227/g4y0-sw34. Yucen Wang, Rui Yu, Shenghua Wan, Le Gan, and De-Chuan Zhan. FOUNDER: Grounding foundation models in world models for open-ended embodied decision making. In Forty-second International Conference on Machine Learning, 2025. URL https://openreview.net/ forum?id=UTT5OTyIWm. Muning Wen, Junwei Liao, Cheng Deng, Jun Wang, Weinan Zhang, and Ying Wen. Entropyregularized token-level policy optimization for language agent reinforcement, 2024. URL https: //arxiv.org/abs/2402.06700. R. E. Wengert. simple automatic derivative evaluation program. Commun. ACM, 7(8):463464, August 1964. ISSN 0001-0782. doi: 10.1145/355586.364791. URL https://doi.org/10. 1145/355586.364791. Preprint. Under review. R. J. Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine Learning, 8:229256, 1992. Quan Xiao, Hui Yuan, Saif, Gaowen Liu, Ramana Rao Kompella, Mengdi Wang, and Tianyi Chen. first-order generative bilevel optimization framework for diffusion models. In Fortysecond International Conference on Machine Learning, 2025. URL https://openreview. net/forum?id=3qL4LRUaJ8. Huajian Xin, Z.Z. Ren, Junxiao Song, Zhihong Shao, Wanjia Zhao, Haocheng Wang, Bo Liu, Liyue Zhang, Xuan Lu, Qiushi Du, Wenjun Gao, Haowei Zhang, Qihao Zhu, Dejian Yang, Zhibin Gou, Z.F. Wu, Fuli Luo, and Chong Ruan. Deepseek-prover-v1.5: Harnessing proof assistant feedback for reinforcement learning and monte-carlo tree search. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum? id=I4YAIwrsXa. Yan Yang, Bin Gao, and Ya xiang Yuan. Bilevel reinforcement learning via the development of hyper-gradient without lower-level convexity, 2025. URL https://arxiv.org/abs/2405. 19697. Shukang Yin, Chaoyou Fu, Sirui Zhao, Ke Li, Xing Sun, Tong Xu, and Enhong Chen. survey on multimodal large language models. National Science Review, 11(12), November 2024. ISSN 2053714X. doi: 10.1093/nsr/nwae403. URL http://dx.doi.org/10.1093/nsr/nwae403. Omar G. Younis, Rodrigo Perez-Vicente, John U. Balis, Will Dudley, Alex Davey, and Jordan Terry. Minari, September 2024. URL https://doi.org/10.5281/zenodo.13767625. Lantao Yu, Weinan Zhang, Jun Wang, and Yong Yu. Seqgan: Sequence generative adversarial nets with policy gradient, 2017. URL https://arxiv.org/abs/1609.05473. Tianhe Yu, Garrett Thomas, Lantao Yu, Stefano Ermon, James Zou, Sergey Levine, Chelsea Finn, and Tengyu Ma. Mopo: Model-based offline policy optimization. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 1412914142. Curran Associates, Inc., 2020. URL https://proceedings.neurips.cc/paper/2020/file/ a322852ce0df73e204b7e67cbbef0d0a-Paper.pdf. Oussama Zekri and Nicolas Boulle. Fine-tuning discrete diffusion models with policy gradient methods, 2025. URL https://arxiv.org/abs/2502.01384. Siliang Zeng, Chenliang Li, Alfredo Garcia, and Mingyi Hong. Maximum-likelihood inverse reinforcement learning with finite-time guarantees. Advances in Neural Information Processing Systems, 35:1012210135, 2022. Jingyi Zhang, Jiaxing Huang, Sheng Jin, and Shijian Lu. Vision-language models for vision tasks: survey, 2024. URL https://arxiv.org/abs/2304.00685. Brian D. Ziebart, Andrew Maas, J. Andrew Bagnell, and Anind K. Dey. Maximum entropy inverse reinforcement learning. In Proceedings of the 23rd National Conference on Artificial Intelligence - Volume 3, AAAI08, pp. 14331438. AAAI Press, 2008. ISBN 9781577353683. Haosheng Zou, Tongzheng Ren, Dong Yan, Hang Su, and Jun Zhu. Reward shaping via meta-learning. arXiv preprint arXiv:1901.09330, 2019. Mateusz Łajszczak, Guillermo Cambara, Yang Li, Fatih Beyhan, Arent van Korlaar, Fan Yang, Arnaud Joly, Alvaro Martın-Cortinas, Ammar Abbas, Adam Michalski, Alexis Moinet, Sri Karlapati, Ewa Muszynska, Haohan Guo, Bartosz Putrycz, Soledad Lopez Gambino, Kayeon Yoo, Elena Sokolova, and Thomas Drugman. Base tts: Lessons from building billion-parameter text-to-speech model on 100k hours of data, 2024. URL https://arxiv.org/abs/2402.08093. 18 Preprint. Under review."
        },
        {
            "title": "TABLE OF CONTENTS",
            "content": "A Theoretical analysis A.1 Proof of proposition 4.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.1 useful lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.1.2 An intermediate proposition: solution of the inner-level problem . . . . . . A.1.3 Concluding the proof: solution of the outer-level problem . . . . . . . . . A.2 Proof of corollary 4.1.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 Proof of corollary 4.1.2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.4 On the definition of in assumption 4.2 . . . . . . . . . . . . . . . . . . . . . . . Additional experiments B.1 Distribution comparison . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 20 20 23 25 25 26 29 19 Preprint. Under review."
        },
        {
            "title": "A THEORETICAL ANALYSIS",
            "content": "A.1 PROOF OF PROPOSITION 4.1 A.1.1 USEFUL LEMMA This lemma will be useful to show to concavity of the objective function in the following proposition. Lemma A.1. Let (A, B, C) S+ S+ Rnn, then one has that: Tr(ACBC ) 0. Proof. Since is symmetric positive semidefinite, there exists symmetric matrix A1/2 such that Then, Then, it follows that : Let Then, = (A1/2)2. Tr(ACBC ) = Tr(A1/2A1/2CBC ). Tr(A1/2A1/2CBC ) = Tr(A1/2CBC A1/2). = A1/2CBC A1/2. Tr(ACBC ) = Tr(M ). So now it suffices to show that is definite semipositive. The matrix is symmetric. For any vector Rn, xM = xA1/2CBC A1/2x = (C A1/2x)B(C A1/2x). Since is positive semidefinite, (C A1/2x)B(C A1/2x) 0. Hence, is positive semidefinite. A.1.2 AN INTERMEDIATE PROPOSITION: SOLUTION OF THE INNER-LEVEL PROBLEM We start by proving the following proposition on the closed-form solution of the inner level problem in Bi-O: Proposition A.1. Under Assumptions 4.1 and 4.2, the inner-level optimization problem θ = arg max θΘ (cid:104) EX,Y (cid:98)Y ˆpθ (cid:105) (cid:104) ( (cid:98)Y )T ( (cid:98)Y ) (cid:105) + λH(ˆpθ) . admits exactly one solution that writes as θ(U ) = (cid:18) Λ, (cid:19) . λU 1 2 Proof of Proposition A.1. We prove the proposition by deriving closed-form expression for the objective θ (cid:55) J(θ) and its gradient, then we show that is strictly concave in θ = (A, B) over Rnn S++ (R) which guarantee that there is exactly one solution. The objective function is: J(θ) = EX EY (cid:104) (cid:98)Y Pθ (cid:104) ( (cid:98)Y )T ( (cid:98)Y ) + λH( (cid:98)Y X) (cid:105)(cid:105) . 20 Preprint. Under review. First, we compute the inner expectation over (cid:98)Y for fixed and . Since (cid:98)Y (AX, B), the entropy of (cid:98)Y is: H( (cid:98)Y X) = log(2πe det(B)). 1 2 Now, define: (cid:98)Y := (cid:98)Y Pθ (cid:104) ( (cid:98)Y )T ( (cid:98)Y ) + λH( (cid:98)Y X) (cid:105) . (cid:98)Y = (cid:124) (cid:98)Y Pθ (cid:104) (cid:105) ( (cid:98)Y )T ( (cid:98)Y ) + (cid:123)(cid:122) (cid:125) λ log(2πe det(B)). For fixed and , let = (cid:98)Y . We show that : = (cid:2)X AT AX + Tr(U B) 2Y (AX) + Y (cid:3) . Expanding the quadratic form: Z = (cid:98)Y (cid:98)Y 2Y (cid:98)Y + Y. Taking expectations: (cid:104) (cid:98)Y (cid:98)Y (cid:98)Y 2Y (cid:98)Y + Y (cid:105) = E[ (cid:98)Y (cid:98)Y ] 2Y E[ (cid:98)Y ] + Y. Since (cid:98)Y (AX, B), we have E[ (cid:98)Y X] = AX. Using the formula for the expectation of quadratic form, for random vector with mean µ and covariance K: E[W ] = µT µ + Tr(U K). Here, = (cid:98)Y , µ = AX, = B, so: Thus, we get the desired expression for A. It follows that, E[ (cid:98)Y (cid:98)Y X] = (AX)T (AX) + Tr(U B). (cid:98)Y = AT AX Tr(U B) + 2Y AX Y + λ 2 log(2πe det(B)). Now, for fixed X, we compute: JX (A, B) = EY Since (ΛX, Σ), we have E[Y X] = ΛX. Using quadratic form expectation again: (cid:2)I (cid:98)Y (cid:3) . Thus, EY [Y ] = ΛT ΛX + Tr(U Σ). JX (A, B) = EY (cid:2)X AT AX Tr(U B) + 2Y AX Y (cid:3) + λ log(2πe det(B)) = AT AX Tr(U B) + 2EY [Y ]T AX EY [Y ] + λ 2 log(2πe det(B)) = AT AX Tr(U B) + 2(ΛX)T AX (cid:0)X ΛT ΛX + Tr(U Σ)(cid:1) + λ log(2πe det(B)) = (cid:0)AT 2ΛT + ΛT Λ(cid:1) Tr(U (B + Σ)) + λ 2 log(2πe det(B)). Since is symmetric: One has that: AT 2ΛT + ΛT Λ = (A Λ)T (A Λ), J(θ) = EX (cid:2)X (A Λ)T (A Λ)X(cid:3) Tr(U (B + Σ)) + λ 2 log(2πe det(B)) Preprint. Under review. Let MA = (A Λ)T (A Λ). MAX R): EX (cid:2)X MAX(cid:3) = EX it follows that (since Tr(x) = for any and here (cid:2)Tr(X MAX)(cid:3) = EX (cid:2)Tr(MAXX )(cid:3) = Tr(MAEX [XX ]). Let ΣX = EX [XX ]. Thus, EX (cid:2)X MAX(cid:3) = Tr (cid:0)(A Λ)T (A Λ)ΣX (cid:1) . J(A, B) = Tr (cid:0)(A Λ)T (A Λ)ΣX (cid:1) Tr(U (B + Σ)) + λ 2 log(2πe det(B)) (2) Let and u1 = (A1, B1) and u2 = (A2, B2) such that u1 + tu2 Rnn S++ show that : (cid:55) J(u1 + tu2) is concave function on = {t R, Let , one has . It suffices to u1 + tu2 Rnn S++ }. u1 + tu2 = (A1 + tA2, B1 + tB2), (cid:1) (cid:125) g(t) = Tr(U (B1 + tB2 + Σ) Tr(cid:0)(A1 + tA2 Λ) (A1 + tA2 Λ) ΣX (cid:124) λ 2 (cid:124) + log (2πe det(B1 + tB2)) (cid:123)(cid:122) (cid:125) :=H2(t) . (cid:123)(cid:122) :=H1(t) First, 2 (N, R). Regarding H1, straightforward calculation shows that where: (cid:55) H1(t) = αt2 + βt + γ, 2 ), α = Tr(U A2ΣX β = 2 Tr(U (A1 Λ)ΣX γ = Tr(U (A1 Λ)ΣX (A1 Λ)) Tr(U B1 + 2Σ). 2 ) Tr(U B2), The second derivative is: (cid:55) 1 (t) = 2 Tr(U A2ΣX 2 ). Since U, ΣX S+ Tr(U A2ΣX 2 ) 0. Thus: H1 is concave. (R), and A2 Rnn we apply the lemma A.1 which gives us immediately that For H2 one can show, using Jacobis formulas that H 2(t) = dt log(det(B1 + ()B2)) = Tr (cid:0)(B1 + tB2)1B2 (cid:1) Since B1, B2 S++ on can find two basis B1 and B2 such that they are diagonal in these bases, λ = (λ1, . . . , λn) Rn {0n}, such that (B1)B1 = Diag(λ) µ = (µ1, . . . , µn) Rn {0n} and (B2)B2 = Diag(µ) So so H 2(t) = (cid:88) 1in µi λi + tµi , H 2 (t) = (cid:88) 1in µ2 (λi + tµi)2 < 0. 22 Preprint. Under review. So is sum of concave and strictly concave function so its strictly concave function and thus is strictly concave, thus the problem A.1 admits exactly one solution on Rnn S++ (R); which is solution of J(A, B) = 0. (3) Lets find in closed form the solutions of the previous equation. It follows that: Then, Finally: AJ(A, B) = 2U (A Λ)ΣX BJ(A, B) = + λ 2 ln(det)(B) = (B1)T = + λ B1 θ(U ) = (cid:18) Λ, (cid:19) . λU 1 2 A.1.3 CONCLUDING THE PROOF: SOLUTION OF THE OUTER-LEVEL PROBLEM We restate the proposition before proceeding with the proof: Proposition A.2. Given the above assumptions and the solution in Proposition A.1, the outerlevel problem = arg min (R) S++ EX,Y (cid:2)log ˆpθ (Y X)(cid:3), has exactly one solution: = λΣ1 2 . Proof. Let S++ (cid:16) (cid:17) Λ, λU 1 . 2 (R) and (λ, n) + by the previous proposition one has θ(U ) = Lets check that is convex function of . φ : (cid:55) EX DKL (cid:0)q( X) pθ ( X)(cid:1) To show the convexity of φ, we show the convexity of g, which is defined as follow. Let U, (R) and IU,V := {u + uV S++ S++ (R)}. Define IU,V g(t) = φ(tU + ). One has that 2 (IU,V , R). Since both q( X) and pθ divergence has closed-form expression: ( X) are Gaussian with the same mean ΛX, the KullbackLeibler DKL (cid:0)q pθ (cid:1) = (cid:20) 1 2 Tr (cid:0)Σ1 Σ(cid:1) + ln (cid:18) det(Σp) det(Σ) (cid:19)(cid:21) , where Σpθ = λ 2 1. It follows that, Σ1 pθ = 2 λ U, and det(Σ1 pθ ) = (cid:19)n (cid:18) λ 2 det(U )1. 23 Preprint. Under review. Substituting these in, we find: (cid:20)"
        },
        {
            "title": "DKL",
            "content": "(cid:0)q pθ (cid:1) = Tr (cid:19) Σ + ln (cid:18) 2 λ (cid:18) Tr(U Σ) 2 + 2 ln (cid:18) λ 2 1 2 1 λ (cid:19)(cid:21) (λ/2)n det(U ) det(Σ) (cid:19) ln(det(Σ)) 1 2 = 1 2 ln(det(U )). This expression is independent of X, so its expectation is itself: 1 λ where is constant independent of . φ(U ) = Tr(U Σ) 1 2 ln(det(U )) + C, Now, we express g(t) explicitly and set IU,V A(t) = + tV : IU,V g(t) = φ(A(t)) = 1 λ Tr(A(t)Σ) 1 ln(det(A(t))) + C. Clearly, 2(IU,V , R), lets show that its second derivative is positive. The first derivative is: IU,V , g(t) = 1 λ Tr(U Σ) 1 2 dt ln(det(A(t))). Using the identity that follows from Jacobis formula: we get: IU,V , dt ln(det(A(t))) = Tr (cid:0)A(t)1A(t)(cid:1) , IU,V , g(t) = 1 λ Tr(U Σ) 1 2 Tr(A(t)1U ). Differentiating again, one has that: Therefore, IU,V , g(t) = 1 2 dt Tr(A(t)1U ). IU,V , g(t) = 1 Tr(A(t)1U A(t)1U ). Let IU,V and = A(t)1/2U A(t)1/2, where A(t)1/2 is the symmetric positive definite square root of A(t). Since is positive definite, is also positive definite. We have: Tr(A(t)1U A(t)1U ) = Tr(A(t)1/2A(t)1/2U A(t)1/2A(t)1/2U ) = Tr(A(t)1/2U A(t)1/2A(t)1/2U A(t)1/2) = Tr(BB) = Tr(B2). Thus, g(t) = 1 Tr(B2). Since is symmetric and positive definite, its eigenvalues λ1, . . . , λn are positive. Therefore, Tr(B2) = (cid:88) i=1 λ2 > 0, which implies g(t) > 0 for all IU,V . Since the second derivative of is strictly positive on its domain, is strictly convex. Thus φ is strictly convex on S++ (R). The existence and the uniqueness is shown. By the moment matching principle for KullbackLeibler divergence one find that = λΣ1 2 . 24 Preprint. Under review. A.2 PROOF OF COROLLARY 4.1.1 Corollary A.2.1 (Isotropic case). If we assume that = σ2In, the set of solutions to the outer-level problem is characterized by: Fλ,Σ := (cid:26) S++ (R) (cid:12) (cid:12) (cid:12) (cid:12) Tr(U) = λn2 2 Tr(Σ) (cid:27) . Proof. The proof follows the same calculations and arguments as those used in the proof of Proposition A.1 and Proposition 4.1. Specifically, we show that the objective function (A, σ2) (cid:55) J(A, σ2) is concave in (A, σ2) and solve the first-order optimality conditions. This leads to the solution (cid:18) (cid:19) θ(U ) = Λ, λn 2 Tr(U ) . Substituting this solution into the DKL expression and following the same arguments leads to Fλ,Σ. A.3 PROOF OF COROLLARY 4.1.2 We first restate the corollary on the reverse KL minimization equivalence: Corollary A.2.2. Under the assumptions of Proposition 4.1, the optimal parameters θ the lower-level problem with = λ 2 Σ1 minimize the reverse KL divergence between ˆpθ and q: obtained from θ = arg minθΘ EX [dKL (ˆpθ(X) q(X))] . Proof. Substituting = λ 2 Σ1 gives: J(θ) = EX EY Xq (cid:20)(cid:20) (cid:98)Y ˆpθ(X) (cid:20) λ 2 ( (cid:98)Y )Σ1( (cid:98)Y ) (cid:21)(cid:21)(cid:21) + λH(ˆpθ). Since q(Y X) is Gaussian with mean AX and covariance Σ, write = AX + ε with ε (0, Σ). Remark that: (cid:98)Y = ( (cid:98)Y AX) ε. Thus, ( (cid:98)Y )Σ1( (cid:98)Y ) = ( (cid:98)Y AX)Σ1( (cid:98)Y AX) 2( (cid:98)Y AX)Σ1ε + εΣ1ε. Taking the conditional expectation EY : (cid:105) (cid:104) ( (cid:98)Y )Σ1( (cid:98)Y ) EY = ( (cid:98)Y AX)Σ1( (cid:98)Y AX) 0 + E[εΣ1ε] = ( (cid:98)Y AX)Σ1( (cid:98)Y AX) + tr(Σ1Σ) = ( (cid:98)Y AX)Σ1( (cid:98)Y AX) + n, where is the dimension of . Therefore, EY (cid:20) λ 2 (cid:21) ( (cid:98)Y )Σ1( (cid:98)Y ) = (cid:104) λ 2 ( (cid:98)Y AX)Σ1( (cid:98)Y AX) + (cid:105) . Now, the log-likelihood of (cid:98)Y under q(X) is: Thus, log q( (cid:98)Y X) = 1 ( (cid:98)Y AX)Σ1( (cid:98)Y AX) 1 2 log ((2π)nΣ) . ( (cid:98)Y AX)Σ1( (cid:98)Y AX) = 2 log q( (cid:98)Y X) log ((2π)nΣ) . 25 Preprint. Under review. one has: EY (cid:20) λ 2 (cid:21) ( (cid:98)Y )Σ1( (cid:98)Y ) = (cid:104) λ 2 2 log q( (cid:98)Y X) log ((2π)nΣ) + (cid:105) = λ log q( (cid:98)Y X) + λ 2 (cid:124) [log ((2π)nΣ) n] (cid:123)(cid:122) (cid:125) :=cn . The term cn is constant with respect to (cid:98)Y and θ. Therefore, EXq (cid:20) (cid:98)Y ˆpθ(X) (cid:20) EY (cid:20) λ ( (cid:98)Y )Σ1( (cid:98)Y ) (cid:21)(cid:21)(cid:21) = λEXq (cid:104) (cid:104) (cid:98)Y ˆpθ(X) log q( (cid:98)Y X) (cid:105)(cid:105) +cn The entropy term is: λH(ˆpθ) = λEXq (cid:104) (cid:98)Y ˆpθ(X) (cid:104) log ˆpθ( (cid:98)Y X) (cid:105)(cid:105) . Thus, the objective function becomes: J(θ) = λEXq (cid:104) (cid:98)Y ˆpθ(X) (cid:104) log q( (cid:98)Y X) log ˆpθ( (cid:98)Y X) (cid:105)(cid:105) + cn = λEXq [dKL (ˆpθ(X) q(X))] + cn. So, maximizing J(θ) is equivalent to minimizing the reverse KL divergence, which completes the proof. A.4 ON THE DEFINITION OF IN ASSUMPTION 4.2 Let where := (cid:16) L2(cid:16) Rn Rn, R, eXX 2 dλ(X, ) (cid:17) ; , (cid:17) , f, f, gH = (cid:90) RnRn (X, ) g(X, ) eXX 2 dλ(X) dλ(X ) and dλ denotes the Lebesgue measure. Lemma A.2. Let Rnn, then rU as defined in 4.2 is an element of (cid:17) Rn Rn, R, eXX 2 dλ(X, ) . := L2(cid:16) Proof. We denote by , Rn the usual Euclidean scalar product on Rn. X, Rn and any matrix Rnn, we have by the Cauchy-Schwarz inequality In particular, for any (X )U (X ) = (cid:10)X , (X )(cid:11) Rn 2 (X ) (4) and notice that for any Rnn and Rn, X2 = (cid:88) (cid:88) 2 Uk,jXj n2 1kn j=1 It leads to: (cid:18) max (k,j)[1,n]2 Uk,j2 X2. (5) (cid:19) (cid:125) (cid:124) (cid:123)(cid:122) :=C(n,U )>0 Preprint. Under review. (cid:90) (cid:0) (X )U (X )(cid:1)2 eXX 2 dλ(X, ) RnRn (cid:90) (cid:124)(cid:123)(cid:122)(cid:125) (4) RnRn 2 (X )2eXX 2 dλ(X, ) (cid:90) RnRn C(n, ) (cid:124)(cid:123)(cid:122)(cid:125) (5) < . 4eXX 2 dλ(X, ) Since; (cid:90) RnRn 4eXX 2 dλ(X, ) = (cid:90) Rn Z4eZ dZ = Vol(Sn1) (cid:90) 0 rn1 r4er2 dr < . rU H. The following two lemmas justifies the reparametrization search space by S++ (R). Lemma A.3. The set {rU : S++ (R)} is in bijection with S++ (R). Proof. Denote := {rU : S++ (R)} and φ : S++ defined by S++ The surjectivity of φ is straitghforward since the image of φ is I. Let U1, U2 S++ ϕ(U1) = ϕ(U2), which is φ(U ) = rU . and assume X, Rn, (X )T (U1 U2)(X ) = 0. (6) But one can find GLn(R) such that Then (6) reads: U1 U2 = DP , = diag(λ1, . . . , λn). = (w1, ..., wn) Rn (cid:88) i= λiw2 (cid:124) (cid:123)(cid:122) (cid:125) 0 = 0. Thus [1, n] λi = 0, so U1 = U2. Lemma A.4. Let and be two non-empty sets and let ϕ : be bijection. Let : and : R, such that g(ϕ(x)) = (x) (7) Then the optimization problems: (P1) max xX (x) and (P2) max yY g(y), are equivalent in the sense that: If is solution of (P1), then = ϕ(x) is solution of (P2). 27 Preprint. Under review. Conversely, if is solution of (P2), then = ϕ1(y) is solution of (P1). Proof. First; lets show that (7) implies that f (ϕ1(y)) = g(y). (8) Let , since ϕ is bijection one can find such that = ϕ(x) and = ϕ1(y) so using (7): ϕ1(y) = (x) = (cid:124)(cid:123)(cid:122)(cid:125) (7) ϕ(x) = (cid:124)(cid:123)(cid:122)(cid:125) bijectivity of ϕ ϕ ϕ1(y) = g(y). (9) Lets now proove that the sup of the two problems are equals. Since ϕ is bijection, every element can be uniquely written as = ϕ(x) for some X. By assumption, we then have g(y) = g(ϕ(x)) = (x) supxX (x) := Mf . So g(y) := Mg Mf sup yY (10) Let X, by the bijection, one can find such that = ϕ1(y) so (x) = (ϕ1(y)) = g(y) Mg. It follows that Combining (10) and (11): Mf Mg (11) sup yY g(y) = sup xX (x). Furthermore, if is point where attains its maximum, then for = ϕ(x), we have: g(y) = (x) = max xX (x) = max yY g(y), so is solution of (P2). Conversely, if is point where attains its maximum, then let = ϕ1(y). We have: (x) = g(ϕ(x)) = g(y) = max yY g(y) = max xX (x), so is solution of (P1). Proposition A.3. For each S++ (R), define (U ) = EX EY Xq[log ˆpθU (Y X)]. For each function I, where = (cid:8)rU : S++ (R)(cid:9) , with rU ( (cid:98)Y , ) = ( (cid:98)Y )U ( (cid:98)Y ), define g(r) = EX EY Xq log ˆpθr (Y X). Then the optimization problems max S++ (R) (U ) and max rI g(r) are equivalent. Proof. Its straightforward consequence of the lemma A.4 with := S++ map ϕ : defined by ϕ(U ) = rU , for which we now that, by Lemma A.3, is bijection. (R) and := and the 28 Preprint. Under review."
        },
        {
            "title": "B ADDITIONAL EXPERIMENTS",
            "content": "B.1 DISTRIBUTION COMPARISON Figure 4: Distribution comparison, different value of λ"
        }
    ],
    "affiliations": [
        "Cognizant AI Lab, Paris",
        "Department of Data Science, EURECOM",
        "Huawei Noahs Ark Lab, Paris",
        "Statistics Program, KAUST"
    ]
}