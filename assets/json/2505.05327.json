{
    "paper_title": "ICon: In-Context Contribution for Automatic Data Selection",
    "authors": [
        "Yixin Yang",
        "Qingxiu Dong",
        "Linli Yao",
        "Fangwei Zhu",
        "Zhifang Sui"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Data selection for instruction tuning is essential for improving the performance of Large Language Models (LLMs) and reducing training cost. However, existing automated selection methods either depend on computationally expensive gradient-based measures or manually designed heuristics, which may fail to fully exploit the intrinsic attributes of data. In this paper, we propose In-context Learning for Contribution Measurement (ICon), a novel gradient-free method that takes advantage of the implicit fine-tuning nature of in-context learning (ICL) to measure sample contribution without gradient computation or manual indicators engineering. ICon offers a computationally efficient alternative to gradient-based methods and reduces human inductive bias inherent in heuristic-based approaches. ICon comprises three components and identifies high-contribution data by assessing performance shifts under implicit learning through ICL. Extensive experiments on three LLMs across 12 benchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of ICon. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICon-selected data outperform full datasets by 5.42% points and exceed the best performance of widely used selection methods by 2.06% points. We further analyze high-contribution samples selected by ICon, which show both diverse tasks and appropriate difficulty levels, rather than just the hardest ones."
        },
        {
            "title": "Start",
            "content": "ICON: In-Context Contribution for Automatic Data Selection Yixin Yang, Qingxiu Dong, Linli Yao, Fangwei Zhu,Zhifang Sui State Key Laboratory of Multimedia Information Processing, Peking University {yangyx,dqx,linliyao,zhufangwei2022}@stu.pku.edu.cn, szf@pku.edu.cn 5 2 0 2 8 ] . [ 1 7 2 3 5 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Data selection for instruction tuning is essential for improving the performance of Large Language Models (LLMs) and reducing training cost. However, existing automated selection methods either depend on computationally expensive gradient-based measures or manually designed heuristics, which may fail to fully exploit the intrinsic attributes of data. In this paper, we propose In-context Learning for Contribution Measurement (ICON), novel gradient-free method that takes advantage of the implicit fine-tuning nature of in-context learning (ICL) to measure sample contribution without gradient computation or manual indicators engineering. ICON offers computationally efficient alternative to gradientbased methods and reduces human inductive bias inherent in heuristic-based approaches. ICON comprises three components and identifies high-contribution data by assessing performance shifts under implicit learning through ICL. Extensive experiments on three LLMs across 12 benchmarks and 5 pairwise evaluation sets demonstrate the effectiveness of ICON. Remarkably, on LLaMA3.1-8B, models trained on 15% of ICON-selected data outperform full datasets by 5.42% points and exceed the best performance of widely used selection methods by 2.06% points. We further analyze highcontribution samples selected by ICON, which show both diverse tasks and appropriate difficulty levels, rather than just the hardest ones."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in large language models (LLMs), such as GPT-4o (Hurst et al., 2024), have demonstrated strong capabilities in both understanding and generation. These models are capable of performing wide range of tasks (Liu et al., 2023b; Dong et al., 2023; Team et al., 2024; Guo 1Project page: ICon_Data_Selection/ https://annayang2020.github.io/ et al., 2025), often showing impressive flexibility and adaptability. Instruction tuning (Mishra et al., 2021; Wei et al., 2021; Sanh et al., 2022) has emerged as powerful approach to enhance the performance of such models by finetuning them to better follow human instructions. While traditional instruction tuning relies on amassing vast datasets (Köpf et al., 2023; Chung et al., 2024), recent study (Zhou et al., 2023a) manually selecting data has shown that training with carefully selected subsets of high-quality data can achieve superior performance with reduced computational cost. Automated data selection methods are essential, as manually selecting the most valuable training data is time-consuming and impractical. Previous studies on automated data selection (Xia et al., 2024; Zhang et al., 2024) have explored gradientbased approaches to estimate sample value, but these methods are often computationally expensive and inefficient. Other works propose lightweight alternatives based on manually designed textual features (Liu et al., 2023c; Bukharin and Zhao, 2023; Zhang et al., 2025) or simple heuristics such as difficulty (Li et al., 2023a, 2024b) and uncertainty (Kung et al., 2023). However, these methods fail to directly measure the training impact of samples and introduce human inductive bias into the selection process. Specifically, manually defined indicators, such as difficulty, uncertainty, or topic coverage, can not accurately capture the true contribution of sample, and reliance on predefined metrics limits the ability to fully exploit the potential of the training data. In this paper, we introduce In-Context Learning for Contribution Measurement (ICON), novel selection method that captures the contribution of individual samples while avoiding human inductive bias and costly gradient computation. Inspired by in-context learning (ICL), our method takes advantadge of recent findings (Dong et al., 2022; Dai et al., 2023; Zheng et al., 2023; Yin et al., 2024) Figure 1: Overview of the ICON Method. The diagram illustrates the two key components of the method: Contribution Quantification with the ICON Score and ICON-guided Selection Paradigm Training. This approach enables gradient-free, bias-reduced data selection with scalable inference. that ICL simulates parameter updates via the attention mechanism, effectively acting as implicit fine-tuning without the need for explicit gradient updates. Our key intuition is that this implicit finetuning nature allows us to directly assess the contribution of individual samples, without the need for manually defined indictor engineering. Additionally, since ICON does not require explicit training or gradient computation, it enables highly efficient selection, significantly reducing computational overhead compared to traditional gradientbased methods. Our approach leverages ICL to measure sample contribution through three components: 1) an assessment set to provide reliable performance reference, 2) ICON scores that quantify contribution via ICLs implicit fine-tuning, and 3) lightweight selection paradigm for scalable data selection. The ICON score captures each samples contribution without costly gradient computation or humandesigned heuristics. higher positive score indicates greater contribution, while lower negative score reflects more detrimental effects. Finally, the ICON-guided selection paradigm is trained efficiently with LoRA (Hu et al., 2021) based on the target model, reducing the selection complexity to linear inference calls. We demonstrate the effectiveness of ICON across multiple models and evaluation settings. Experiments on LLaMA3.1-8B (Grattafiori et al., 2024), Qwen2.5-3B (Yang et al., 2024), and LLaMA2-7B (Lyu et al., 2024) are conducted using 12 widely used benchmarks and 5 pairwise comparison test sets. Models trained on small fraction of ICON-selected data outperform their full-data counterparts. Notably, with only 15% of the data, the LLaMA3.1-8B and Qwen2.5-3B models improve by 5.42 and 1.24 percentage points, respectively; LLaMA2-7B achieves 0.65-point gain using just 5%. ICON also surpasses widely used selection methods. For example, ICON improves LLaMA3.1-8Bs average benchmark score by 2.06 points over the best prior method. Furthermore, we also analyze the impact of data scale, cross-dataset generalization, and the properties of high-contribution samples selected by ICON. The main contributions of our paper are as follows: We propose ICON, novel data selection method inspired by ICL. By leveraging its implicit fine-tuning nature, ICON enables gradient-free and bias-reduced measurement of each samples contribution to model performance. Our approach avoids gradient updates and auxiliary models, reducing computational cost and design complexity. Once trained the ICON-guided selection with LoRA, paradigm enables data selection through linear-complexity inference calls, making it highly scalable. We demonstrate the effectiveness of ICON through experiments on multiple models, 12 widely used benchmarks, and 5 pairwise test sets, showing consistent performance gains over baselines."
        },
        {
            "title": "2 Related Work",
            "content": "Data Selection Data selection aims to construct optimal datasets to improve model performance (Murphy, 2012), reduce training cost (Suárez et al., 2019; Sorscher et al., 2022), mitigate undesirable model behaviors (Longpre et al., 2024), and ensure evaluation quality (Oren et al., 2024). With the rise of LLMs, it plays central role across various training stages, including pretraining, instruction tuning, alignment, in-context learning, and task-specific finetuning. Pretraining selection focuses on filtering large-scale raw data (Soldaini et al., 2024; Penedo et al., 2023); Instruction tuning often uses manual (Zhou et al., 2023a) or automated methods, with the latter detailed in the next section; alignment involves manual curation (Bai et al., 2022), model-based evaluation (Wang et al., 2023b; Cui et al., 2023), and reward model re-weighting (Touvron et al., 2023; Pace et al., 2024); in-context learning emphasizes demonstration choice (Xu and Zhang, 2024; Luo et al., 2024) and order (Lu et al., 2021); and taskspecific tuning leverages utility-based (Ivison et al., 2023) or empirical methods (Grangier and Iter, 2022). Data Selection for Instruction Tuning Instruction tuning (Wei et al., 2021; Liu et al., 2023a; Xu et al., 2024) is common approach to enhance instruction-following capabilities. Recent work (Zhou et al., 2023a) shows that carefully selecting smaller, higher quality dataset can improve both the effectiveness and efficiency of instruction tuning. Existing automatic data selection methods fall into three main categories: (1) selection based on textual features like helpfulness, quality, or complexity (Zhang et al., 2025; Cao et al., 2023; Chen et al., 2023; Liu et al., 2023c; Bukharin and Zhao, 2023; Minghao et al., 2024); (2) selection using training-time gradients to capture model-specific characteristics (Xia et al., 2024; Zhang et al., 2024; Pan et al., 2024; Chang et al., 2024); (3) selection guided by feedback during inference, which is model-aware and much more efficient (Du et al., 2023; Kung et al., 2023; Li et al., 2023a, 2024b; Liu et al., 2024; Wang et al., 2024; Li et al., 2024a). The first and third approaches rely on handcrafted heuristics for scoring and filtering, introducing human inductive bias, while the second demands substantial computational cost. ICON addresses the limitations of existing approaches by avoiding both costly gradient computations and manually designed heuristics. It leverages ICL to directly measure sample contribution in modelaware, efficient manner."
        },
        {
            "title": "3 Methodology",
            "content": "To efficiently assess the contribution of each sample, we draw on recent advances in in-context learning (ICL), which has been recognized as form of implicit finetuning without explicit gradient updates (Dai et al., 2023; Yin et al., 2024). Specifically, ICL leverages the models attention mechanism to simulate parameter updates by processing demonstration examples, thereby adapting the models behavior within the given context. The key intuition is that in-context learning can effectively simulate the training impact of individual samples, enabling an assessment of their influence on model performance without the high computational cost of finetuning. Building on this insight, we propose the ICON method, an efficient data selection method that harnesses ICLs implicit finetuning nature to assess each samples contribution to overall model performance. As shown in Figure 1, ICON comprises three phases: (1) constructing representative assessment set, (2) computing the proposed ICON score to quantify sample contribution, and (3) learning an ICON-guided selection paradigm to curate data for final instruction tuning."
        },
        {
            "title": "3.1 Assessment Set Construction",
            "content": "The goal of this phase is to construct an assessment set that reliably reflects the models overall capabilities. To ensure diversity and reduce bias, we sample data from three sources: ChatGPT-generated, GPT-4-generated, and human-authored data, resulting in high-quality set covering range of task types and instruction complexities. The assessment set is used solely for assessing model performance, with no overlap with training data or downstream benchmarks, ensuring independent evaluation and fair comparison. Formally, we define the assessment set as Da, consisting of instruction tuning task samples, where each is represented as pair (xa, ya). Here, = map(Instruction, [Input]) represents the full instruction, which is formed by concatenating the instruction and optional input, and denotes the corresponding response. Similarly, we also define the candidate training set as Dt = {(xt m)}, where is the number of candidate training instructionresponse pairs. 2), . . . , (xt 1), (xt m, yt 2, yt 1, yt"
        },
        {
            "title": "Score",
            "content": "In this phase, we introduce the ICON score, metric for estimating the contribution of individual samples to overall model performance. The core idea is to leverage ICL to evaluate contribution of sample. ICL enables models to adapt to new information presented within prompt, providing an efficient approach to assess the contribution of individual samples on task performance. We adopt perplexity as the assessment metric, as it directly evaluates the likelihood of response tokens and serves as smoother indicator of model performance compared to accuracy (Li et al., 2023a, 2024b; Jiang et al., 2024). For task sample Si = (xa ) Da, where Da is the assessment set, ya contains tokens and the k-th token is denoted as ya i,k, the models performance under parameters θ on task is defined as: , ya PPLθ(Si) = PPLθ(ya xa ) (cid:32) = exp 1 (cid:88) k=1 log pθ(ya i,kxa , ya i,1, ..., ya i,k1) (cid:33) (1) j, yt Then, We select sample Tj = (xt j) from t, subset of the training set Dt. This sample is inserted into the context during inference to assess the models performance on an assessment sample Si after implicitly learning from Tj. The models performance on Si, given Tj in context is denoted as PPLθ(SiTj) = PPLθ(ya ). Since peri plexity is affected by both content and input length, we introduce fairness adjustment to neutralize length effects. Specifically, we define the baseline performance on Si without the influence of Tj by using randomly generated meaningless sequence rand , which matches the length of Tj. The models baseline performance on Ti is then denoted ) = PPLθ(ya as PPLθ(SiT rand Tj, xa rand , xa ). j Using these values, we define the task-level ICON score of sample Sj on task under parameters θ as shown in Equation 2, where ϵ is small constant to avoid division by zero. positive tasklevel ICON score indicates that Sj contributes positively to model performance on task i, while negative score suggests detrimental effect. We Figure 2: Pairwise comparison results (win-tie-lose) between models trained on ICON-selected subsets and models trained on the full Alpaca dataset across five evaluation benchmarks: Vicuna, Koala, WizardLM, SInstruct, and LIMA. ICON-selected data enhances instruction-following ability with fewer samples, as evidenced by pairwise comparisons. then compute the global ICON score by aggregating the task-level scores across all assessment tasks, as shown in Equation 3. This global score allows us to rank samples based on their overall contribution. higher positive global ICON score implies stronger positive impact on model overall performance, while lower negative score indicates the opposite. task-ICONθ(Tj Si) = PPLθ(SiT rand ) PPLθ(SiTj) PPLθ(Si) + ϵ (2) global-ICONθ(Sj) = 1 (cid:88) SiDa task-ICONθ(Tj Si) (3) 3.3 ICON-guided Selection Paradigm Training While the ICON score effectively captures samples contribution, it requires multiple inference calls per candidate training sample. To further improve efficiency, we train an ICON-guided selection paradigm using the computed global ICON scores. This allows us to reduce the selection complexity to O(m) inference calls, where is the total number of candidate samples. Specifically, we use subset of the training data and label the top K% of samples by global ICON score as highcontribution. selection model is then trained using LoRA on the target model to classify highvalue samples. This paradigm is applied to the full candidate set to select the most contributive samples for instruction tuning, thereby enhancing the models ability to follow instructions."
        },
        {
            "title": "4.1 Datasets",
            "content": "Training Dataset In this paper, we use the classic Alpaca dataset (Taori et al., 2023), one of the original instruction datasets created by Stanford University. The Alpaca dataset consists of 52,002 instruction-following samples, generated with TextDavinci-003 through the self-instruction approach (Wang et al., 2022). The LLM trained on this dataset demonstrate good instructionfollowing capability. However, its heavy dependence on TextDavinci-003 has raised concerns among researchers regarding the quality of instruction data. The WizardLM dataset (Xu et al., 2023) utilizes the Evol-Instruct algorithm to enhance the quality and complexity of instruction data. We conduct experiments using WizardLM-70K to verify the generalization of our method across different datasets. Assessment Dataset We consider the impact of training candidate sample on the models performance over the assessment dataset as an indicator of its overall influence on model capability, as constructed in Section 3.1. To reduce bias and ensure diversity, we sample data from three sources: OpenOrca-GPT3.5 (Lian et al., 2023) containing ChatGPT-generated data, OpenOrca-GPT4 containing GPT-4-generated data, and Dolly-15K (Mike et al., 2023) containing human-generated data. From these three datasets, we randomly select 1,020 instructions to construct the assessment dataset. 4."
        },
        {
            "title": "Implementation Details",
            "content": "For experiments on the LLaMA3.1-8B (Grattafiori et al., 2024), Qwen2.5-3B (Yang et al., 2024), and LLaMA2-7B (Lyu et al., 2024) models, we all follow the original Alpaca2 training configuration using the Alpaca codebase. Models are trained for three epochs with the Adam optimizer (Kingma and Ba, 2014), learning rate of 2 105, and batch size of 128. The maximum input length is set to 512 for Alpaca dataset and 2048 for WizardLM dataset."
        },
        {
            "title": "4.3.1 Benchmark Evaluation",
            "content": "To evaluate the effectiveness of ICON, we report performance on several widely used benchmarks. These benchmarks fall into three cateinstrucgories, assessing different capabilities: Intion following, knowledge, and reasoning. struction following evaluation includes two key benchmarks: IFEval (Zhou et al., 2023c) and AlpacaEval (Li et al., 2023b). AlpacaEval provides an LLM-based automatic evaluation on the AlpacaFarm set (Dubois et al., 2023), where we use GPT-4 (Achiam et al., 2023) as both the response generator and the evaluator comparing model outputs with reference responses. Knowledge evaluation includes four key benchmarks: GLUE (Wang et al., 2018), GPQA (Rein et al., 2024), MMLU (Hendrycks et al., 2020), and TruthfulQA (Lin et al., 2021). Reasoning evaluation includes six key benchmarks: ARC (Clark et al., 2018), BBH (Suzgun et al., 2022), HellaSwag (Zellers et al., 2019), LogiQA (Liu et al., 2020), MuSR (Sprague et al., 2023), and Winogrande (Sakaguchi et al., 2021)."
        },
        {
            "title": "4.3.2 Pairwise Comparison",
            "content": "The performance on several pairwise comparison test sets for instruction following is also provided, including WizardLM (Xu et al., 2023), SelfInstruct (Wang et al., 2022), Vicuna (Chiang et al., 2023), Koala (Vu et al., 2023), and LIMA (Zhou et al., 2023b). These sets contain 218, 252, 80 and 300 human-curated instruction data, respectively, across domains such as math, coding, writing, and general knowledge. We use GPT-4 (Hurst et al., 2024) as the judge. For each instruction in the test sets, candidate models generate responses, which are then scored by GPT-4 in pairwise fashion. The judge rates each response from 1 to 10 based on factors such as relevance and accuracy. To mitigate positional bias (Wang et al., 2023a), we submit each response pair twice with reversed order. 2https://github.com/tatsu-lab/stanford_alpaca Instruction Following Knowledge Reasoning IFEval AlpacaEval GLUE GPQA MMLU TQA ARC BBH HS LQA MuSR WG Avg FLOPs (1012) FULL 15.68 ICON (1%) 8.31 16.21 ICON (5%) ICON (10%) 16.08 ICON (15%) 21.16 32.34 FULL 27.04 ICON (1%) ICON (5%) 20.85 ICON (10%) 25.28 ICON (15%) 23.07 15.48 FULL 5.25 ICON (1%) ICON (5%) 7.78 ICON (10%) 11.86 8.06 ICON (15%) 18.66 28.62 28.96 27.99 29. 28.74 34.55 36.25 37.05 30.11 21.76 30.35 31.47 29.76 26.83 54.48 58.47 56.91 57.86 60.58 68.98 67.22 68.34 69.26 70.46 53.98 54.51 51.93 50.62 54.67 27.27 31.31 27.27 26.76 30. 31.82 31.82 27.88 34.85 34.34 26.26 23.23 23.23 25.76 27.78 LLaMA3.1-8B 43.26 55.77 53.70 53.57 52.30 39.60 40.53 36.46 51.42 26.73 37.51 63.77 43.11 49.91 44.51 61.19 23.35 39.37 71.03 42.91(+4.96) 40.51 47.18 44.23 58.44 24.42 37.75 69.46 42.09(+4.14) 42.63 45.48 42.81 56.27 26.57 42.01 66.93 42.08(+4.13) 43.16 44.80 44.15 55.56 28.42 41.64 67.96 43.37(+5.42) 37. Qwen2.5-3B 61.71 64.63 64.73 63.67 64.09 45.42 46.42 45.05 55.13 35.79 44.51 63.85 44.08 53.16 47.35 56.19 34.71 41.05 68.51 47.53(+0.88) 43.26 53.50 47.80 58.10 32.10 39.72 69.06 46.80(+0.15) 43.35 50.00 47.00 57.85 35.79 40.25 68.27 47.72(+1.07) 46.90 51.71 47.68 58.16 38.40 41.96 67.80 47.89(+1.24) 46.65 LLaMA2-7B 43.69 40.55 36.95 37.30 36. 40.48 41.72 36.18 53.98 25.03 41.11 62.82 45.82 42.24 38.34 59.20 25.19 37.68 67.96 39.19(+0.65) 45.72 40.53 36.07 57.57 23.96 40.62 66.77 38.55(+0.01) 45.19 41.72 35.38 57.39 24.88 39.82 66.38 38.84(+0.30) 43.89 42.66 34.74 57.01 23.50 39.80 66.06 38.45(-0.09) 38.54 40.12 0.59 3.06 5.62 8.37 36.46 0.44 2.27 5.28 6.85 48.72 0.66 3.26 6.16 9.18 Table 1: Performance of LLaMA3.1-8B, Qwen2.5-3B, and LLaMA2-7B in terms of FLOPs and evaluation benchmarks for instruction following, knowledge, and reasoning. The datasets TQA, HS, LQA, and WG correspond to TruthfulQA, HellaSwag, LogicQA, and WinoGrande, respectively. Models trained on ICON-selected data outperform those trained on full datasets on evaluation benchmarks, achieving better performance with fewer samples. model is considered to win only if it does not lose in both orders, following these rules: Win: wins both, or wins one and ties the other; Tie: ties both, or wins one and loses one; Lose: loses both, or loses one and ties the other."
        },
        {
            "title": "5.1 Main Results",
            "content": "In this section, we present the evaluation benchmark results shown in Table 1. The models are trained using different proportions of Alpaca data selected by the ICON method, including 1%, 5%, 10%, and 15%, corresponding to 520, 2,600, 5,200, and 7,800 samples, respectively. \"All\" denotes models trained on the full, unfiltered dataset. Experiments were conducted on multiple models, including LLaMA3.1-8B, Qwen2.5-3B, and LLaMA27B. Across all benchmarks, models trained on small fraction of ICON-selected data often match or surpass the performance of those trained on the full dataset, and consistently outperform them in overall metrics. For example, the LLaMA3.1-8B model trained on just 15% of ICON-selected data exceeds the full-data baseline by 5.42 percentage points. Similarly, the Qwen2.5-3B model with 15% ICONselected data and the LLaMA2-7B model with only 1% achieve overall gains of 1.24% and 0.65%, respectively. Notably, these ICON-trained models require only about 1/5 to 1/100 of the FLOPs compared to full-data training, demonstrating significantly lower resource consumption. These results underscore the effectiveness of ICON-guided selection in enhancing instruction tuning with substantially less data. The results of the pairwise evaluation, including detailed win-tie-lose statistics on the Vicuna, Koala, WizardLM, SInstruct, and LIMA test sets, are presented in Figure 2. We focus on the best-performing models from the evaluation benchmarks: LLaMA3.1-8B, Qwen2.5-3B, and LLaMA2-7B, each trained on 7,800, 7,800, and 520 ICON-selected samples, respectively. All three models consistently outperform their counterparts trained on the full Alpaca dataset across all five test sets. These results further validate the effectiveness of ICON-guided selection in improving instructionfollowing ability, even with significantly less training data."
        },
        {
            "title": "5.2 Comparison with Other Methods",
            "content": "In this subsection, we compare our method with several widely used data selection baselines on the Alpaca dataset, as shown in Table 2 and Table 4. \"Random\" selects samples at random. \"Low PPL\" selects samples with the lowest perplexity, while \"Top PPL\" selects those with the highest perplexity. \"Alpagasus\" uses GPT-3.5-Turbo to score sample responses based on dimensions such as helpfulness Comparison Test Set ICON vs. Random ICON vs. Low PPL ICON vs. Top PPL ICON vs. Alpagasus (Chen et al., 2023) ICON vs. Deita (Liu et al., 2023c) ICON vs. Superfilter (Li et al., 2024b) Pairwise Winning Score Vicuna WizardLM LIMA SInstruct Koala 1.3389 1.2125 1.3333 1.3375 1.7778 1.9126 1.3389 1.1250 1.3889 1.3375 1.0944 1.0375 1.3233 1.4533 1.8567 1.1033 1.2733 1.0100 1.4541 1.3945 1.9125 1.1789 1.2661 1.0780 1.5159 1.3690 1.8452 1.3611 1.3730 1. Overall 1.3922 1.3903 1.8556 1.2252 1.3214 1.0786 Table 2: Comparison with other methods on Vicuna, WizardLM, LIMA, SInstruct, and Koala test set. The pairwise Winning Scores are calculated between models using our method and other methods. All the comparisons are performed by GPT-4, and the values that are greater than 1.0 represent our models are better and vice versa. The models are trained on LLaMA3.1-8B with 7,800 selected samples. ICON consistently outperforms widely used data selection methods on pairwise comparison test sets. and accuracy. \"Deita\" uses set of models finetuned from LLaMA and Mistral to evaluate sample quality, complexity, and diversity. \"Superfilter\" selects samples using IFD scores that reflect instruction difficulty, computed using GPT-2. Superfilter assumes that IFD scores from smaller models are as reliable as those from larger ones. Table 2 presents the pairwise winning scores of the ICON model compared to other data selection methods. The ICON model used here is trained on 7,800 ICON-selected samples using LLaMA3.1-8B as the base model. The pairwise winning score is calculated as (Num(Win) Num(Lose))/Num(All) + 1, offering direct comparison between ICON and every other method. score greater than 1 indicates that the ICON model outperforms the compared model, with higher values reflecting stronger advantages. ICON demonstrates consistent and clear advantages over Random, Low PPL, Top PPL, Alpagasus, Deita, and Superfilter across all five test sets as well as in overall performance. Table 4 further compares ICON with other methods on 13 evaluation benchmark tasks. We report the average scores for instruction tuning, knowledge, reasoning, and overall evaluation, where higher scores indicate better performance. ICON achieves superior results across all three categories and overall evaluation, confirming its effectiveness in selecting high-contribution data for instruction tuning. The detailed results for different methods on evaluation benchmarks are shown in Appendix A."
        },
        {
            "title": "6 Analysis",
            "content": "Figure 3: Model performance (average score) on evaluation benchmarks with varying proportions of ICONselected Alpaca data. The models are trained based on LLaMA3.1-8B. The dataset is Alpaca dataset. As data scale grows, performance generally improves and then declines, with the best result at 15%. results of training on ICON-selected subsets of the Alpaca dataset at varying proportions: 1%, 5%, 10%, 15%, 20%, 25%, 30%, 50%, 75%, and 100%. The average score across all evaluation benchmarks generally rises and then declines, peaking at 15%, demonstrating that small, carefully selected subset can outperform models trained on much larger datasets. Based on this observation, we select the model trained on 15% (7,800 samples) of ICONselected data as the representative model for subsequent comparisons. Detailed results for each selection scale are provided in Appendix B. These results show that adding more data beyond 15% does not lead to further improvements, highlighting the efficiency of ICON in identifying the most contributive samples. Optimal Data Scale for Instruction Tuning We further investigate how the scale of selected data affects model performance. Using the LLaMA3.18B model as case study, Figure 3 presents the Generalization of ICON on different Datasets In addition to the main experiments conducted on the Alpaca dataset, we also evaluated the generalization of the ICON method to different datasets. FULL ICON (1%) ICON (5%) ICON (10%) ICON (15%) IF 33.61 34.57 43.21 36.89 35.24 KN 45.46 46.39 48.28 48.15 46.29 RS 44.37 49.57 48.36 47.18 45.76 Avg 42.94 46.01 47.48 45.79 44.18 Table 3: Evaluation benchmark results of models trained on WizardLM dataset and its ICON-selected subsets. The IF, KN, and RS correspond to average scores on Instruction Following, Knowledge, and Reasoning, respectively. Results on WizardLM dataset confirm the generalization of ICON across datasets. Low PPL Top PPL Alpagasus Deita Superfilter ICON (Ours) IF 19.35 3.28 16.39 15.20 20. 25.52 KN 44.45 41.72 45.14 45.46 44.76 46.71 RS 46.53 44.69 45.29 45.92 45. 47.09 Avg 41.31 36.80 40.42 40.65 41.02 43.37 Table 4: Performance of different methods on evaluation benchmarks. The IF, KN, and RS correspond to average scores on Instruction Following, Knowledge, and Reasoning, respectively. The models are trained on LLaMA3.1-8B with 7,800 selected samples. ICON outperforms widely used data selection methods on pairwise evaluation benchmarks. Specifically, we performed experiments on the WizardLM dataset. The results, shown in Table 3 and detailed in Appendix C, cover selection scales of 1%, 5%, 10%, 15%, and ALL, corresponding to 520, 2,600, 5,200, 7,800, and 70K samples. As shown, models trained on ICON-selected subsets consistently outperform those trained on the full dataset across the Instruction Following, Knowledge, and Reasoning tasks, achieving higher average scores and overall performance. The model trained on just 5% of the data achieves the best results, with 4.54 percentage point improvement in average evaluation benchmark score. These findings demonstrate the effectiveness of ICON across datasets. Notably, the selection paradigm, trained on the Alpaca subset, is reused here without modification, demonstrating its robust generalization. Characteristics of High-Contribution ICON Samples We further analyzed the characteristics of the high-contribution samples selected by the ICON method. Figure 4 shows the distribution of high-contribution samples in the t-SNE space, where we observe both cross-cluster dispersion and local density. This pattern aligns with the notion Figure 4: Visualization using t-SNE on sample embeddings from the Alpaca dataset. Red points represent samples with the top 15% ICON high-contribution scores and gray points represent other samples from the dataset. High-contribution ICON samples exhibit diverse characteristics. Figure 5: Difficulty distribution of top 15% highcontribution samples vs. full Alpaca dataset. Highcontribution samples (red bars), selected via ICON for instruction tuning, exhibit distinct difficulty patterns compared to the full dataset (gray bars). Highcontribution ICON samples tend to fall within an appropriate difficulty range. in previous studies (Liu et al., 2023c; Bukharin and Zhao, 2023) that \"instruction data should be diverse\". However, the distribution is not uniformly scattered. Instead, some clusters exhibit higher density, suggesting that high-contribution samples tend to exhibit certain clustering behavior in the feature space. Figure 5 further compares the distribution of instruction difficulty between high-contribution samples and the full dataset. We use the IFD score (Li et al., 2023a, 2024b) as the difficulty metric. The t-test (Kendall, 1937) result (t = 2.24, = 0.0251) shows that high-contribution samples are significantly more difficult than the full dataset on average, indicating that the selection method tends to favor more challenging samples. However, as seen in the distribution, high-contribution samples are not exclusively concentrated in the highest difficulty range. Interestingly, while the method favors more difficult samples, some mid-level difficulty samples are retained, and extreme difficulty samples are avoided. This finding challenges the view that more difficult samples are always inherently more beneficial for model training (Du et al., 2023; Li et al., 2023a, 2024b,a; Zhang et al., 2025)."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we introduced ICON, novel method that uses ICL to efficiently assess the contribution of individual training samples without human inductive bias. The method is structured into three stages: constructing an assessment set, computing ICON scores, and training lightweight selection paradigm for scalable data selection. Extensive experiments on three LLMs across 12 benchmarks and 5 pairwise evaluation sets show that models trained on ICON-selected data outperform those trained on the full dataset, achieving superior performance with fewer samples. ICON also significantly outperforms other widely used data selection methods. We further analyzed the optimal data scale, generalization across datasets, and characteristics of high-contribution samples. As data scale increases, performance improves initially but later declines, indicating an optimal data size. ICON generalizes well across datasets, and its high-contribution samples display diversity while maintaining an appropriate level of difficulty for effective learning."
        },
        {
            "title": "Limitations",
            "content": "While ICL can be seen as form of implicit finetuning, it captures the impact of individual samples on the models behavior, which may not fully replicate the effect of batch training with gradient updates. As result, some differences in performance may arise. Nevertheless, we believe that ICL provides useful approximation of sample contribution and can still effectively guide data selection for instruction tuning despite these differences. Furthermore, while our method demonstrates promising results for instruction tuning, we believe it has the potential for broader applicability. For instance, ICON could be adapted to other tasks and domains by modifying the assessment set and the calculation of the ICON score. Exploring these extensions is beyond the scope of this paper and remains direction for future work."
        },
        {
            "title": "References",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Alon Albalak, Yanai Elazar, Sang Michael Xie, Shayne Longpre, Nathan Lambert, Xinyi Wang, Niklas Muennighoff, Bairu Hou, Liangming Pan, Haewon Jeong, et al. 2024. survey on data selection for language models. arXiv preprint arXiv:2402.16827. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jackson Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Cameron McKinnon, et al. 2022. Constitutional ai: Harmlessness from ai feedback. arXiv preprint arXiv:2212.08073. Alexander Bukharin and Tuo Zhao. 2023. Data diversity matters for robust instruction tuning. ArXiv, abs/2311.14736. Yihan Cao, Yanbin Kang, and Lichao Sun. 2023. Instruction mining: High-quality instruction data ArXiv, selection for abs/2307.06290. large language models. Ernie Chang, Pin-Jie Lin, Yang Li, Changsheng Zhao, Daeil Kim, Rastislav Rabatin, Zechun Liu, Yangyang Shi, and Vikas Chandra. 2024. Target-aware language modeling via granular data sampling. In Conference on Empirical Methods in Natural Language Processing. Lichang Chen, SHIYANG LI, Jun Yan, Hai Wang, Kalpa Gunaratna, Vikas Yadav, Zheng Tang, Vijay Srinivasan, Tianyi Zhou, Heng Huang, and Hongxia Jin. 2023. Alpagasus: Training better alpaca with fewer data. ArXiv, abs/2307.08701. Wei-Lin Chiang, Zhuohan Li, Ziqing Lin, Ying Sheng, Zhanghao Wu, Hao Zhang, Lianmin Zheng, Siyuan Zhuang, Yonghao Zhuang, Joseph Gonzalez, et al. 2023. Vicuna: An open-source chatbot impressing gpt-4 with 90%* chatgpt quality. See https://vicuna. lmsys. org (accessed 14 April 2023), 2(3):6. Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. 2024. Scaling instruction-finetuned language models. Journal of Machine Learning Research, 25(70):153. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. 2018. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168. Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun. 2023. Ultrafeedback: Boosting language models with high-quality feedback. J. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, and Weizhu Chen. 2021. Lora: Low-rank adaptation of large language models. ArXiv, abs/2106.09685. Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. 2023. Why can gpt learn in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the Association for Computational Linguistics: ACL 2023, pages 40054019. Qingxiu Dong, Li Dong, Ke Xu, Guangyan Zhou, Yaru Hao, Zhifang Sui, and Furu Wei. 2023. Large language model for science: study on vs. np. arXiv preprint arXiv:2309.05689. Qingxiu Dong, Lei Li, Damai Dai, Ce Zheng, Jingyuan Ma, Rui Li, Heming Xia, Jingjing Xu, Zhiyong Wu, Tianyu Liu, et al. 2022. survey on in-context learning. arXiv preprint arXiv:2301.00234. R. C. St. Johnn. R. Draper. 1975. D-optimality for regression designs: review. Technometrics, 17(1):15 23. Qianlong Du, Chengqing Zong, and Jiajun Zhang. 2023. Mods: Model-oriented data selection for instruction tuning. ArXiv, abs/2311.15653. Yann Dubois, Chen Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Alpacafarm: simulation framework for methods that learn from human feedback. Advances in Neural Information Processing Systems, 36:3003930069. David Grangier and Dan Iter. 2022. The trade-offs of domain adaptation for neural language models. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38023813. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Sariel Har-Peled and Akash Kushal. 2005. Smaller coresets for k-median and k-means clustering. In Proceedings of the twenty-first annual symposium on Computational geometry, pages 126134. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276. Hamish Ivison, Noah Smith, Hannaneh Hajishirzi, and Pradeep Dasigi. 2023. Data-efficient finetuning using cross-task nearest neighbors. In Findings of the Association for Computational Linguistics: ACL 2023, pages 90369061. Zhengbao Jiang, Zhiqing Sun, Weijia Shi, Pedro Rodriguez, Chunting Zhou, Graham Neubig, Xi Lin, Wen-tau Yih, and Srini Iyer. 2024. Instruction-tuned language models are better knowledge learners. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 54215434, Bangkok, Thailand. Association for Computational Linguistics. M. G. Kendall. 1937. Statistical methods for research workers. Nature, 139:737737. Diederik Kingma and Jimmy Ba. 2014. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980. Andreas Köpf, Yannic Kilcher, Dimitri Von Rütte, Sotiris Anagnostidis, Zhi Rui Tam, Keith Stevens, Abdullah Barhoum, Duc Nguyen, Oliver Stanley, Richárd Nagyfi, et al. 2023. Openassistant conversations-democratizing large language model alignment. Advances in Neural Information Processing Systems, 36:4766947681. Po-Nien Kung, Fan Yin, Di Wu, Kai-Wei Chang, and Nanyun Peng. 2023. Active instruction tuning: Improving cross-task generalization by training on prompt sensitive tasks. In Conference on Empirical Methods in Natural Language Processing. Ming Li, Lichang Chen, Jiuhai Chen, Shwai He, Jiuxiang Gu, and Tianyi Zhou. 2024a. Selective reflection-tuning: Student-selected data recycling for llm instruction-tuning. ArXiv, abs/2402.10110. Ming Li, Yong Zhang, Shwai He, Zhitao Li, Hongyu Zhao, Jianzong Wang, Ning Cheng, and Tianyi Zhou. 2024b. Superfiltering: Weak-to-strong data filtering for fast instruction-tuning. In Annual Meeting of the Association for Computational Linguistics. Ming Li, Yong Zhang, Zhitao Li, Jiuhai Chen, Lichang Chen, Ning Cheng, Jianzong Wang, Tianyi Zhou, and Jing Xiao. 2023a. From quantity to quality: Boosting llm performance with self-guided data selection for instruction tuning. ArXiv, abs/2308.12032. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. 2023b. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval. Wing Lian, Bleys Goodson, Eugene Pentland, Austin Cook, Chanvichet Vong, and Teknium. 2023. Openorca: An open dataset of gpt augmented flan reasoning traces. Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958. Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. 2023a. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. ArXiv, abs/2311.10774. Man Luo, Xin Xu, Yue Liu, Panupong Pasupat, and Mehran Kazemi. 2024. In-context learning with retrieved demonstrations for language models: survey. arXiv preprint arXiv:2401.11624. Kaifeng Lyu, Haoyu Zhao, Xinran Gu, Dingli Yu, Anirudh Goyal, and Sanjeev Arora. 2024. Keeping llms aligned after fine-tuning: The crucial role of prompt templates. arXiv preprint arXiv:2402.18540. Conover Mike, Hayes Matt, Mathur Ankit, Xie Jianwei, Wan Jun, Shah Sam, Ghodsi Ali, Wendell Patrick, Zaharia Matei, and Xin Reynold. 2023. Free dolly: Introducing the worlds first truly open instructiontuned llm. Wu Minghao, Thuy-Trang Vu, Lizhen Qu, and Gholamreza Haffari. 2024. The best of both worlds: Bridging quality and diversity in data selection with bipartite graph. arXiv preprint arXiv:2410.12458. Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. 2023b. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning. arXiv preprint arXiv:2311.10774. Swaroop Mishra, Daniel Khashabi, Chitta Baral, and Hannaneh Hajishirzi. 2021. Natural instructions: Benchmarking generalization to new tasks from natural language instructions. arXiv preprint arXiv:2104.08773, pages 839849. Jian Liu, Leyang Cui, Hanmeng Liu, Dandan Huang, Yile Wang, and Yue Zhang. 2020. Logiqa: challenge dataset for machine reading comprearXiv preprint hension with logical reasoning. arXiv:2007.08124. Liangxin Liu, Xuebo Liu, Derek F. Wong, Dongfang Li, Ziyi Wang, Baotian Hu, and Min Zhang. 2024. Selectit: Selective instruction tuning for llms via uncertainty-aware self-reflection. In Neural Information Processing Systems. Wei Liu, Weihao Zeng, Keqing He, Yong Jiang, and Junxian He. 2023c. What makes good data for alignment? comprehensive study of automatic data selection in instruction tuning. ArXiv, abs/2312.15685. Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. 2024. pretrainers guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 32453276. K. Lu, Hongyi Yuan, Zheng Yuan, Runji Lin, Junyang Lin, Chuanqi Tan, and Chang Zhou. 2023. Instruction tagging for analyzing super- #instag: vised fine-tuning of large language models. ArXiv, abs/2308.07074. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity. arXiv preprint arXiv:2104.08786. Kevin Murphy. 2012. Machine learning: probabilistic perspective. MIT press. Yonatan Oren, Nicole Meister, Niladri S. Chatterji, Faisal Ladhak, and Tatsunori Hashimoto. 2024. Proving test set contamination in black-box language In The Twelfth International Conference models. on Learning Representations. Alizée Pace, Jonathan Mallinson, Eric Malmi, Sebastian Krause, and Aliaksei Severyn. 2024. West-of-n: Synthetic preference generation for improved reward modeling. arXiv preprint arXiv:2401.12086. Xingyuan Pan, Luyang Huang, Liyan Kang, Zhicheng Liu, Yu Lu, and Shanbo Cheng. 2024. G-dig: Towards gradient-based diverse and high-quality instruction data selection for machine translation. ArXiv, abs/2405.12915. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Hamza Alobeidli, Alessandro Cappelli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. 2023. The refinedweb dataset for falcon llm: Outperforming curated corpora with web data only. Advances in Neural Information Processing Systems, 36:7915579172. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. 2024. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. 2021. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106. Victor Sanh, Albert Webson, Colin Raffel, Stephen Bach, Lintang Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, et al. 2022. Multitask prompted training enables zero-shot task generalization. In ICLR 2022-Tenth International Conference on Learning Representations. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. 2024. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159. Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. 2022. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:1952319536. Zayne Sprague, Xi Ye, Kaj Bostrom, Swarat Chaudhuri, and Greg Durrett. 2023. Musr: Testing the limits of chain-of-thought with multistep soft reasoning. arXiv preprint arXiv:2310.16049. Hongjin Su, Jungo Kasai, Chen Henry Wu, Weijia Shi, Tianlu Wang, Jiayi Xin, Rui Zhang, Mari Ostendorf, Luke Zettlemoyer, Noah Smith, et al. 2023. Selective annotation makes language models better few-shot learners. In International Conference on Learning Representations (ICLR 2023)(01/05/202305/05/2023, Kigali, Rwanda). Pedro Javier Ortiz Suárez, Benoît Sagot, and Laurent Romary. 2019. Asynchronous pipeline for processing huge corpora on medium to low resource infrastructures. In 7th Workshop on the Challenges in the Management of Large Corpora (CMLC-7). LeibnizInstitut für Deutsche Sprache. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. 2022. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261. Thuy-Trang Vu, Xuanli He, Gholamreza Haffari, and Ehsan Shareghi. 2023. Koala: An index for quantifying overlaps with pre-training corpora. arXiv preprint arXiv:2303.14770. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. 2018. Glue: multi-task benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461. Peiyi Wang, Lei Li, Liang Chen, Zefan Cai, Dawei Zhu, Binghuai Lin, Yunbo Cao, Qi Liu, Tianyu Liu, and Zhifang Sui. 2023a. Large language models are not fair evaluators. arXiv preprint arXiv:2305.17926. Tianlu Wang, Ping Yu, Xiaoqing Ellen Tan, Sean OBrien, Ramakanth Pasunuru, Jane Dwivedi-Yu, Olga Golovneva, Luke Zettlemoyer, Maryam FazelZarandi, and Asli Celikyilmaz. 2023b. Shepherd: critic for language model generation. arXiv preprint arXiv:2308.04592. Yejie Wang, Keqing He, Dayuan Fu, Zhuoma Gongque, Heyang Xu, Yanxu Chen, Zhexu Wang, Yujia Fu, Guanting Dong, Muxi Diao, Jingang Wang, Mengdi Zhang, Xunliang Cai, and Weiran Xu. 2024. How do your code llms perform? empowering code instruction tuning with high-quality data. ArXiv, abs/2409.03810. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. 2022. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc V. Le. 2021. Finetuned language models are zero-shot learners. ArXiv, abs/2109.01652. Sherry Wu, Hua Shen, Daniel Weld, Jeffrey Heer, and Marco Tulio Ribeiro. 2023. Scattershot: Interactive in-context example curation for text transformation. In Proceedings of the 28th International Conference on Intelligent User Interfaces, pages 353367. Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang, and Tatsunori Hashimoto. 2023. Stanford alpaca: an instruction-following llama model (2023). Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. 2024. Less: Selecting influential data for targeted instruction tuning. ArXiv, abs/2402.04333. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. 2024. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. 2023. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288. Can Xu, Qingfeng Sun, Kai Zheng, Xiubo Geng, Pu Zhao, Jiazhan Feng, Chongyang Tao, and Daxin Jiang. 2023. Wizardlm: Empowering large language models to follow complex instructions. arXiv preprint arXiv:2304.12244. Shangqing Xu and Chao Zhang. 2024. Misconfidencebased demonstration selection for llm in-context learning. arXiv preprint arXiv:2401.06301. Zhangchen Xu, Fengqing Jiang, Luyao Niu, Yuntian Deng, Radha Poovendran, Yejin Choi, and Bill Yuchen Lin. 2024. Magpie: Alignment data synthesis from scratch by prompting aligned llms with nothing. ArXiv, abs/2406.08464. Superfilter, and ICON. Overall, ICON outperforms existing methods across the evaluated benchmarks."
        },
        {
            "title": "Different Dataset",
            "content": "An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115. Qingyu Yin, Xuzheng He, Chak Tou Leong, Fan Wang, Yanzhao Yan, Xiaoyu Shen, and Qiang Zhang. 2024. Deeper insights without updates: The power of incontext learning over fine-tuning. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 41384151, Miami, Florida, USA. Association for Computational Linguistics. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can machine really finish your sentence? arXiv preprint arXiv:1905.07830. Dylan Zhang, Qirun Dai, and Hao Peng. 2025. The best instruction-tuning data are those that fit. Jipeng Zhang, Yaxuan Qin, Renjie Pi, Weizhong Zhang, Rui Pan, and Tong Zhang. 2024. Tagcos: Taskagnostic gradient clustered coreset selection for instruction tuning data. ArXiv, abs/2407.15235. Ce Zheng, Lei Li, Qingxiu Dong, Yuxuan Fan, Zhiyong Wu, Jingjing Xu, and Baobao Chang. 2023. Can we edit factual knowledge by in-context learning? arXiv preprint arXiv:2305.12740. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, L. Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023a. Lima: Less is more for alignment. ArXiv, abs/2305.11206. Chunting Zhou, Pengfei Liu, Puxin Xu, Srinivasan Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, et al. 2023b. Lima: Less is more for alignment. Advances in Neural Information Processing Systems, 36:5500655021. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. 2023c. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911."
        },
        {
            "title": "Different Methods",
            "content": "The detailed results for different methods on evaluation benchmarks are shown in Table 5. The methods include Low PPL, Top PPL, Alpagasus, Deita, Instruction Following Knowledge Reasoning Avg IFEval AE Avg GLUE GPQA MMLU TQA Avg ARC BBH HS LQA MuSR WG Avg Low PPL Top PPL Alpagasus Deita Superfilter 21.28 17.42 0.97 5.60 23.07 9.72 9.10 21.30 13.98 26.53 19.35 3.28 16.39 15.20 20.25 56.51 55.29 57.52 58.31 55. 29.80 26.77 30.81 27.28 27.78 53.18 44.82 53.82 54.47 53.66 38.30 44.45 44.97 40.51 56.32 30.88 37.37 69.14 46.53 41.31 39.98 41.72 43.69 37.01 54.55 23.04 44.10 65.75 44.69 36.80 38.39 45.14 44.20 41.04 56.13 27.19 37.74 65.43 45.29 40.42 41.77 45.46 42.66 42.94 55.67 26.57 39.90 67.79 45.92 40.65 41.94 44.76 45.65 41.31 56.56 25.81 37.12 66.22 45.45 41.02 ICON (Ours) 21.16 29.89 25.52 60. 30.81 52.30 43.16 46.71 44.80 44.15 55.56 28.42 41.64 67.96 47.09 43.37 Table 5: The detailed results of different methods on the evaluation benchmarks. The datasets AE, TQA, HS, LQA, and WG correspond to AlpacaEval, TruthfulQA, HellaSwag, LogicQA, and WinoGrande, respectively. All models are trained on LLaMA3.1-8B with 7,800 selected samples. The best results are highlighted in bold. Instruction Following Knowledge Reasoning Avg IFEval AE Avg GLUE GPQA MMLU TQA Avg ARC BBH HS LQA MuSR WG Avg 8.31 28.62 ICON (1%) 16.21 28.96 ICON (5%) 16.08 27.99 ICON (10%) 21.16 29.89 ICON (15%) 11.95 27.24 ICON (20%) 15.63 23.35 ICON (25%) 17.72 25.87 ICON (30%) 15.00 22.91 ICON (50%) ICON (75%) 15.17 23.79 FULL (100%) 15.68 18. 18.46 22.58 22.03 25.52 19.60 19.49 21.79 18.95 19.48 17.17 58.47 56.91 57.86 60.58 55.07 50.84 58.57 57.75 55.11 54.48 31.31 27.27 26.76 30.81 29.29 28.28 24.74 23.73 29.80 27.27 55.77 53.70 53.57 52.30 53.02 51.12 52.41 51.50 46.70 43.26 43.11 47.17 49.91 44.51 61.19 23.35 39.37 71.03 48.23 42.91 40.51 44.60 47.18 44.23 58.44 24.42 37.75 69.46 46.91 42.09 42.63 45.21 45.48 42.81 56.27 26.57 42.01 66.93 46.68 42.08 43.16 46.71 44.80 44.15 55.56 28.42 41.64 67.96 47.09 43.37 41.71 44.77 43.36 42.08 55.06 26.88 37.36 64.88 44.94 40.66 41.26 42.88 42.06 42.13 54.52 28.88 39.77 67.32 45.78 40.43 40.66 44.10 41.89 42.32 54.35 26.57 37.63 66.54 44.88 40.77 41.12 43.53 41.64 40.33 53.19 28.12 35.27 64.25 43.80 39.57 39.94 42.89 39.99 37.24 52.52 28.73 35.12 64.25 42.98 39.03 39.60 41.15 40.53 36.46 51.42 26.73 37.51 63.77 42.74 37.95 Table 6: The detailed results of different data scales on the evaluation benchmarks. The datasets AE, TQA, HS, LQA, and WG correspond to AlpacaEval, TruthfulQA, HellaSwag, LogicQA, and WinoGrande, respectively. All models are trained on LLaMA3.1-8B with 7,800 selected samples. The best results are highlighted in bold. Instruction Following Knowledge Reasoning IFEval AE Avg GLUE GPQA MMLU TQA Avg ARC BBH HS LQA MuSR WG Avg Avg FLOPs (1012) 25.43 41.79 ALL 20.70 48.44 ICON (1%) 26.90 59.52 ICON (5%) ICON (10%) 27.67 46.11 ICON (15%) 25.98 44.50 33.61 34.57 43.21 36.89 35.24 61.13 55.41 62.78 62.89 63.04 25.76 27.78 26.26 27.27 25.25 48.05 60.48 56.64 55.22 52. 46.91 45.46 39.76 40.56 51.22 27.65 42.41 64.64 44.37 42.94 41.87 46.39 50.17 46.61 60.08 27.34 41.24 71.98 49.57 46.01 47.44 48.28 48.29 44.23 57.73 27.50 41.07 71.35 48.36 47.48 47.21 48.15 43.94 40.57 56.13 29.19 44.18 69.06 47.18 45.79 44.25 46.29 43.52 42.00 54.56 28.11 38.72 67.64 45.76 44.18 191.95 4.85 13.06 23.77 30.45 Table 7: The detailed performance of LLaMA3.1-8B trained on WizardLM data in terms of FLOPs and evaluation benchmarks for instruction following, knowledge, and reasoning. The datasets AE, TQA, HS, LQA, and WG correspond to AlpacaEval, TruthfulQA, HellaSwag, LogicQA, and WinoGrande, respectively. The best results of ICON across different training data scales are highlighted in bold."
        }
    ],
    "affiliations": [
        "State Key Laboratory of Multimedia Information Processing, Peking University"
    ]
}