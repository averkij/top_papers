{
    "paper_title": "Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning",
    "authors": [
        "Hai-Long Sun",
        "Zhun Sun",
        "Houwen Peng",
        "Han-Jia Ye"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our re-implementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from a gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only a ~2% accuracy drop on MathVista's test-hard subset, revealing the model's textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), a strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems."
        },
        {
            "title": "Start",
            "content": "Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning Hai-Long Sun 1 2 3 Zhun Sun 3 Houwen Peng 3 Han-Jia Ye"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 5 2 0 2 7 1 ] . [ 1 0 6 3 3 1 . 3 0 5 2 : r Recent advancements in Large Language Models (LLMs) have demonstrated enhanced reasoning capabilities, evolving from Chain-of-Thought (CoT) prompting to advanced, product-oriented solutions like OpenAI o1. During our reimplementation of this model, we noticed that in multimodal tasks requiring visual input (e.g., geometry problems), Multimodal LLMs (MLLMs) struggle to maintain focus on the visual information, in other words, MLLMs suffer from gradual decline in attention to visual information as reasoning progresses, causing text-over-relied outputs. To investigate this, we ablate image inputs during long-chain reasoning. Concretely, we truncate the reasoning process midway, then re-complete the reasoning process with the input image removed. We observe only 2% accuracy drop on MathVistas test-hard subset, revealing the models textual outputs dominate the following reasoning process. Motivated by this, we propose Take-along Visual Conditioning (TVC), strategy that shifts image input to critical reasoning stages and compresses redundant visual tokens via dynamic pruning. This methodology helps the model retain attention to the visual components throughout the reasoning. Our approach achieves state-of-the-art performance on average across five mathematical reasoning benchmarks (+3.4% vs previous sota), demonstrating the effectiveness of TVC in enhancing multimodal reasoning systems. The project page is available at https://sun-hailong.github.io/ projects/TVC. 1School of Artificial Intelligence, Nanjing University 2National Key Laboratory for Novel Software Technology, Nanjing University 3Tencent. Correspondence to: Han-Jia Ye <yehj@lamda.nju.edu.cn>. 1 Large Language Models (LLMs) have achieved significant advancements in natural language processing (NLP), particularly in the area of reasoning. These models have evolved from simple prompt-based Chain-of-Thought (CoT) (Wei et al., 2022) techniques to sophisticated product-oriented solutions like OpenAIs o1 (OpenAI, 2024b), DeepSeekR1 (DeepSeek-AI, 2024), and Qwen-QVQ (QwenTeam, 2024), demonstrating iterative reasoning capabilities for complex multi-step tasks, which enables them to handle tasks that require multi-step thinking, logic, and knowledge integration. Recently, several works also extended the CoT reasoning process to MLLMs settings through data-centric innovations. For instance, Math-LLaVA (Shi et al., 2024) pioneers domain-specific training with the MathV360K dataset, while MAmmoTH-VL (Guo et al., 2024) scales up multimodal CoT data generation. While such progress is notable in text-based domains, extending these advancements to multimodal scenarios presents unique challenges that transcend traditional language model boundaries. Reasoning in MLLMs requires fused understanding across different modalities, for example, in geometric reasoning tasks the model should interpret and reason about images alongside text. Therefore, the models ability to integrate and maintain focus on both types of information is critical. Unlike text-only LLMs that reinforce problem context through linguistic recurrence, MLLMs struggle to sustain visual attention across reasoning steps. That is, as the length of the context increases, the model is more inclined to conduct the next step of reasoning based on the previously outputted text rather than the information of the image itself, which eventually leads to the continuation of the wrong text reasoning process and degraded model performance. We term this phenomenon as visual forgetting. In this work, we conduct diagnostic analysis of the visual forgetting effect within long-chain reasoning system. The system processes multimodal Q&A tasks through series of interconnected reasoning steps. We demonstrate significantly reduced attentional allocation to visual inputs during multi-stage reasoning processes. More formally, our analysis: 1) truncates the reasoning process midway and removes Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning the image embeddings; 2) regenerates subsequent reasoning trajectories; 3) evaluates the reasoning outcomes of the pre/post-ablation inference trajectories. Intuitively, the performance gap between normal reasoning and diagnostic reasoning reveals the models dependency on generated text over original visual evidence. Our results (See Section 2.1) on the MathVista-Hard datasets show that, removing the image midway through the reasoning only causes an insignificant performance degradation (i.e.2%), indicating that the model completes the reasoning process based primarily on its output text. More importantly, we also observe that the models dependency on the visual evidence diminishes over time since the early removal of the image inputs could hurt accuracy by 20%. This suggests models reasoning employs both visual and textual information in the early stage, then over-relying on text history which limits full visual reasoning potential. Motivated by this, we propose novel strategy to mitigate the visual forgetting effect and maintain visual attention throughout the reasoning process. Our methodology compresses and shifts the image input to the later stages of the reasoning process, ensuring the model integrates sufficient visual evidence into its reasoning. This approach results in improved performance and achieves state-of-the-art results on average across five mathematical reasoning benchmarks (i.e., +3.4% vs previous sota). Our findings highlight the effectiveness of this strategy in enhancing the performance of multimodal reasoning systems, providing robust solution to the problem of visual forgetting in long-chain reasoning tasks. 2. Take-along Visual Conditioning: Sustaining Visual Evidence for Multi-modal Long CoT Reasoning In this section, we first discuss our motivation and observations of the visual forgetting phenomenon in MLLM reasoning systems (Section 2.1). Then, we propose the Take-alone Visual Conditioning (TVC) approach to mitigate visual forgetting and enhance the models long-chain reasoning capabilities (Section 2.2). 2.1. Capturing the Visual Forgetting Text-based reasoning systems often repeat key terms or phrases (e.g., restating triangle ABC or equation (1)) to keep the problems context clear and focused. By restating these critical details, they create strong connections between each step of the reasoning process and the original problems requirements. This repetition ensures the logic stays on track, and consistent to follow. CMLLM = (V, T1, ..., Tn) (1) Figure 1: The visual forgetting phenomenon by removing the image at different reasoning stages. It shows that by the midpoint of the reasoning process, the model becomes less dependent on the image, causing text-over-relied outputs. On the other hand, MLLMs struggle with this approach due to their design. As formalized in Equation (1), visual inputs are confined to the initial processing stages. Unlike text, these visual evidence arent revisited or reinforced later in the reasoning process. Because theres no built-in way to keep visual information active or relevant throughout the task, the systems ability to connect visual details with text or logic weakens over time, leading to progressive visual attention decay. The model is more likely to reason with previously outputted text and becomes particularly problematic in visual reasoning tasks that require continuous validation of spatial relationships. We conduct two analytic analyses using the QVQ-72BPreview model (QwenTeam, 2024) to capture this visual forgetting phenomenon quantitatively and qualitatively. On one hand, we remove the visual inputs at eight different stages to observe their impact. On the other hand, we depict the attention matrix to directly track the attention decay of the visual evidence over time. Progressive Image Removing. To assess the extent to which the reasoning process depends on previously generated text, we first perform normal reasoning process, then reset the KV cache at various stages of the reasoning process. This effectively removed image tokens and forced subsequent steps to rely solely on text-based information. Specifically, the reasoning process was divided into = 8 intervals based on output token counts regardless of the length of the normal reasoning process, with visual input progressively masked by resetting the KV cache at different cutoff positions {0, 1, . . . , 1}. In other words, the first k/8 part of the normal reasoning process is now employed as prompt, and the model now re-complete the reasoning process without image inputs. Furthermore, we Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning changes in visual feature attention across different stages of reasoning, we investigate the layer-level attention weights at various response token positions and the token-level attention weights at layer 16. Given that MLLMs attention weights reflect the focus on tokens and influence the decision-making process, we first analyze the attention weights at each layer of the MLLM. Specifically, for each layer, we calculate the proportion of attention weights on all image tokens. As shown in Figure 2(a), we observe that at the 1/8 position of the reasoning process, the model effectively focuses on the visual inputs. However, as reasoning progresses, despite fluctuations in attention to visual tokens across layers, the models overall attention to visual evidence gradually decreases, leading to visual forgetting. Next, following the methodology of FastV (Chen et al., 2024a), we analyze the attention maps for several cases and find that the model predominantly focuses on previously generated text tokens rather than the input image. After approximately 20% tokens, the existence of image inputs on attention maps diminishes significantly, as illustrated in Figure 2(b). Both of the observations indicate phenomenon of visual memory degradation, revealing the models limitations in maintaining consistent attention to visual inputs throughout the reasoning process. 2.2. Take-along Visual Conditioning In this section, we introduce our solution to tackle this problem in detail. We propose Take-along Visual Conditioning (TVC), dynamic image retention mechanism that re-introduces visual inputs at strategic intervals throughout the reasoning process. TVC mitigates visual attention decay by periodically reaffirming visual information, akin to human problem-solving behaviors where individuals frequently refer back to visual inputs. Our approach enhances the models ability to incorporate visual information continuously, improving its long-chain reasoning capacity by ensuring that visual evidence is revisited during critical decision-making moments. The TVC method consists of two key stages: training and testing. In the training stage, we introduce Dynamic Visual Reaffirmation (DVR), which guides the model through iterative reinforcement of visual evidence during long reasoning chains. In the testing phase, we present Periodic Visual Calibration (PVC), where visual reactivation is periodically triggered at self-reflection intervals. To prevent the model from forgetting previous text-based reasoning steps due to an excessive number of image tokens, we adopt image compression through adaptive pooling to reduce the image token size while preserving spatial semantics. This dual-modality engagement mechanism ensures consistent interaction between textual reasoning and visual evidence. We present an Figure 2: Illustration of layer-level and token-level attention weights. (a) The layer-level attention weights of image tokens (b) The token-level across different response token positions. attention weights at the middle layer. It shows that the models attention to the image gradually decreases during the reasoning process. discovered that for some questions (30.9% of the MathVista dataset), the model could answer correctly using only the text-based prompt. Consequently, we excluded these questions and designated the remaining dataset as the MathVistaHard dataset. As illustrated in Figure 1, we observe that the performance is 40.9 at = 4 and 43.1 for the normal reasoning. This minimal 2.2% gap suggests that the model overly relies on text outputs during the later reasoning stages rather than on the image. This indicates that once the model completes the half stages of reasoning, it becomes overly reliant on its own generated CoT. Over-reliance on this self-generated reasoning worsens the models tendency to forget visual evidence over time, which can result in hallucinations during lengthy, multi-step reasoning tasks. This result also reveals near-exponential forgetting effect in performance as increases, which we formalize as: R(k) = Rfull visual(k), visual(k) ek, (2) where Rfull represents the full multimodal reasoning performance, and visual(k) quantifies the performance degradation caused by visual masking at position k. Visual Attention Decay. To more intuitively observe the Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning illustration of the TVC system in Figure 3. Dynamic Visual Reaffirmation. Our dynamic visual reaffirmation training strategy combines two key components: 1) Data Curation: We curate long-chain reasoning data using the pipeline described in Section 3, sourced from highquality academic datasets (e.g., MathV360K, Geo170K, and LLaVA-OneVision). This process yields high-quality dataset optimized for long-chain reasoning training. 2) Visual Content Injection: While the curated data ensures correctness, the QVQ model inherently lacks the ability to iteratively reference visual inputs during reasoning. Therefore, we manually re-inject the visual content (visual embeddings and bridging prompt) to triggers visual re-activation at predefined self-reflection intervals. Specifically, given the initial multimodal input M0 = (V, T0), DVR performs visual reactivation at self-reflection intervals {r1, ..., rm}: Mi = V, [Tprev; Prompt (cid:124) (cid:123)(cid:122) (cid:125) Re-activation ; Tnew] at step ri (3) where Tprev represents the previous reasoning steps and Tnew denotes the new reasoning steps that are based on prior reasoning and reintroduce focus on visual information. The bridging prompt is employed to hint the existence of the image, e.g., Let me see the image again. To improve efficiency, our initial implementation adopts midpoint reactivation (m = 1, r1 = 0.5L for L-step chains). During the self-reflection phase of the reasoning steps, we randomly select positions to inject visual content. Specifically, we inject reactivation prompts and regenerate visual captions using the models intrinsic capabilities. This forces joint attention to both textual reasoning and visual evidence. By leveraging the models intrinsic image captioning capability, we continuously reinforce visual information throughout the reasoning process. This ensures that the model incorporates image evidence during reflection, rather than relying solely on textual reasoning. Periodic Visual Calibration. Calibrating visual attention plays crucial role in enhancing long-chain reasoning capabilities. After training our model, we further design the periodic visual calibration process. Specifically, we coordinate operations during reactivation as follows: 1) Token Compression: We first compress visual tokens using average pooling to prevent text-based reasoning from forgetting visual information. 2) Visual Cache Reset: We then prepend an instruction (bridging prompt) to re-introduce the image and re-inject image tokens by resetting the KV cache of the generation process. We also provide an example of how PVC is implemented in the case study section (Section 4.5). PVC both improves reasoning efficiency and prevents the model from forgetting previous reasoning steps due to an overload of visual tokens. Figure 3: Overview of TVC System Design. We enable the model to have take-along visual conditioning capabilities through two stages: training and inference. 3. Data-Centric Implementation of Multimodal Reasoning System In this section, we briefly describe our implementation of the multimodal reasoning system through scalable curated data generation pipeline. 3.1. Long-Chain Reasoning Data Collection Prior research (Qin et al., 2024; Jiang et al., 2024) has identified two dominant paradigms for constructing longchain reasoning systems: (1) explicit search-based methods, which utilize structures such as Monte Carlo Tree Search (MCTS) combined with specially trained reward models to guide the search process toward optimal solutions, and (2) instruction distillation approaches, which fine-tune models on curated long chain-of-thought (CoT) datasets. To efficiently develop an MLLM with long-chain reasoning capabilities, we adopt the distillation paradigm. In this section, we describe the distillation process and present the data generation pipeline aimed at enhancing the reasoning capability of MLLM. Our distillation pipeline begins with aggregating publicly available reasoning datasets (e.g., MathV360K (Shi et al., 2024) and Geo170K (Gao et al., 2023)). Through an iterative process of distillation and response filtering, we progressively enhance the models long-chain reasoning capabilities. Specifically, the teacher model, QVQ-72B-Preview, generates long COT reasoning data, while the student model, Qwen2-VL, undergoes supervised fine-tuning on the filtered CoT data to enhance its reasoning performance. 3.2. Iterative Distillation with Reject Sampling After obtaining the long-chain reasoning responses, we employ an assessment pipeline to ensure data quality. Specifi4 Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning This two-stage sampling achieves partial error recovery while reducing the average token count to minimize meaningless self-reflection, effectively balancing correctness and efficiency.To further enhance the quality of the dataset, we also perform dynamic token truncation and reflection word pruning. This approach helps reduce the ineffective reflection parts in the dataset, thereby mitigating the issue of the model being unable to complete its answers. More details are in Section B. 4. Experiments We conducted comprehensive experiments across multiple vision-language benchmarks to demonstrate the effectiveness of our method. Section 4.1 provides implementation details for TVC. In Section 4.3, we present key results on visual reasoning tasks, supplemented with findings on general image understanding. Section 4.4 details ablation studies to evaluate the influence of critical design decisions. Finally, Section 4.5 presents visualizations and case studies to illustrate the methods operational characteristics and insights derived from it. 4.1. Training Recipe We integrated the TVC system with MLLMs of varying scales to demonstrate the effectiveness and generalizability of our approach. Initial implementation with Qwen2-VL7B-Instruct confirmed the methods validity. To further evaluate scalability and establish robust baselines against state-of-the-art MLLMs, we expanded the approach to 72B model. Prior to training TVC, we followed the longchain reasoning pipeline described earlier. We employed the LLaMA-Factory (Zheng et al., 2024) framework, using learning rate of 2e-5, batch size of 256, and 5 training epochs. During optimization, only the LLM parameters and cross-modal connector were trained, while the visual encoder remained frozen. The training process required 10 hours on 64H20 GPU setup for the 7B model and approximately 4 days for the 72B model. Additional details are presented in Table 4. 4.2. Evaluation Setup We conduct comprehensive experimental analysis across various visual reasoning benchmarks that require advanced visual reasoning skills. To ensure well-rounded evaluation, we select several widely recognized and representative benchmarks, including MathVista (Lu et al., 2024a), MathVerse (Zhang et al., 2024a), MathVision (Wang et al., 2024a), Dynamath (Zou et al., 2024), and OlympiadBench (He et al., 2024). MathVista consists of 6,141 examples that require fine-grained, deep visual understanding and compositional reasoning. MathVerse contains 2,612 multi-subject math problems from variety of Figure 4: Data Generation Pipeline of TVC. We use iterative distillation to collect long-chain reasoning data, followed by comprehensive response filtering process to ensure high-quality reasoning. cally, we introduce dual-temperature sampling mechanism to optimize data quality through variance exploitation. Deterministic Initial Sampling. For the first-stage data generation, we employ temperature τ = 0 to obtain the QVQs most confident reasoning paths: Dinit = {(x, yCoT) PQVQ(x; τ = 0)}, (4) where yCoT represents the response generated by QVQ. This ensures that the model generates the most probable reasoning path for each input. Subsequently, we use these highconfidence responses as foundation for further refinement in later stages of data generation. Answer-Centric Reject Sampling. To ensure the quality of the data, we employ an LLM-as-a-Judge approach for answer-centric reject sampling. Specifically, we use strong LLM (e.g., Qwen2.5-72B-Instruct) as the verifier and prompt the model to output yes or no in JSON format, indicating whether the long-chain reasoning answer is correct, i.e., Yvalid = {yCoTMatch(yCoT, ygt)}. After this process, we obtain 200K high-quality samples. Best-of-N Error Correction. While initial sampling with temperature τ = 0 yields set of data, there are substantial residual errors (Derror) where QVQs responses (yCoT) unmatched with ground truth answers (55.8% of Dinit). To fully leverage the available open-source data, we implement contrastive regeneration strategy using τ = 1: Dcorrected = (cid:26) (cid:91) xDerror arg min yiYvalid (cid:27)8 Length(yi) , (5) i=1 where Yvalid denotes the subset of 8 regenerated responses (at τ = 1) that satisfy answer correctness. For cases with multiple valid responses, we prioritize minimal reasoning token length to enhance conciseness and efficiency: Length(y) = (cid:88) t= I(wt / {[PAD], [SEP]}) (6) 5 Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning Table 1: Results on Visual Reasoning Tasks. We conduct evaluation experiments across 6 benchmarks, covering both general reasoning and task-specific reasoning assessments. TVC exhibits notable effectiveness and generalizability when applied to Qwen2-VL, surpassing other state-of-the-art MLLMs by large margin. Model Size MathVista MathVision MathVerse Dynamath OlympiadBench Average MiniCPM-V-2.6 (Yadav et al., 2025) VITA-1.5 (Fu et al., 2025) LLaVA-COT (Xu et al., 2024) Qwen2-VL (Wang et al., 2024b) InternVL2.5 (Chen et al., 2024b) POINTS1.5 (Liu et al., 2024b) Ovis1.6-Gemma2 (Lu et al., 2024b) InternVL2.5-COT (Chen et al., 2024b) LLaVA-OneVision (Li et al., 2024) Qwen2-VL (Wang et al., 2024b) QVQ-72B-preview (QwenTeam, 2024) TVC TVC 8B 8B 11B 7B 8B 8B 27B 78B 72B 72B 72B 7B 72B 60.8 66.2 52.5 60.9 64.5 66.4 70.2 71.4 67.1 69.7 71. 68.1 72.2 18.4 19.5 19.9 16.3 17.0 22.0 20.6 32.5 25.3 26.6 35.9 22.7 41.9 17.6 23.4 22.6 24.6 22.8 26.6 37.8 40.1 27.2 36.2 41.5 38.9 48.8 9.8 9.6 7.8 11.0 9.4 14.2 17.0 28.5 15.6 20.0 30. 15.1 30.0 - - - 3.2 0.1 - - - - 10.3 20.4 9.8 24.3 - - - 23.2 22.8 - - - - 32.6 40.0 30.9 43.4 Table 2: Ablations on the TVC System. TVC enhances reasoning capabilities, showing significant improvements on both general and task-specific reasoning benchmarks. Method MathVista MathVision MathVerse Avg Baseline Vanilla - Direct SFT TVC w/o PVC TVC w/o DVR TVC Full 60.9 63.5 66.7 66.2 68.1 16.3 19.8 21.8 22.3 22.7 24.6 31.6 35.6 34.7 38. 33.9 38.3 41.4 41.0 43.2 sources. MathVision includes 3,040 high-quality mathematical problems sourced from established mathematics competitions. OlympiadBench features 8,476 bilingual multimodal problems tailored to Olympic-level mathematics and physics competitions. These benchmarks evaluate problemsolving abilities in mathematics, and following standard practice, we use GPT-4o-mini as the evaluator. Following the VLMEvalKit guidelines, we exclude the text-only split from MathVerse and the theorem-proof sections from OlympiadBench. For fair comparison, we conduct evaluations using the testmini sets of MathVerse and MathVista. Fast evaluation is made possible through the use of the VLMEvalKit (Duan et al., 2024) and vLLM (Kwon et al., 2023). 4.3. Main Results on Visual Reasoning Revised Paragraph: The results in Table 1 demonstrate the effectiveness and generalizability of TVCacross multiple visual reasoning benchmarks. Notably, our model achieves 16.7% improvement over QVQ-72B-Preview on MathVision and 17.6% gain on MathVerse, highlighting enhanced reasoning capabilities. Unlike conventional datasets where textual descriptions may include implicit visual cues, MathVerse is an all-around visual math benchmark specifically designed for equitable, in-depth evaluation of MLLMs. The significant gains on MathVerse underscore the significance of TVC, given the benchmarks unique design principles. Furthermore, the TVC-7B model, despite its smaller size compared to counterparts, achieves competitive performance, even outperforming leading MLLMs in multiple cases. This demonstrates the robustness of our methodology even with more compact model configurations. Beyond task-specific visual reasoning, we extend our evaluation to general reasoning benchmarks (e.g., MathVista), where TVCconsistently delivers strong performance, achieving 3.6% improvement over the original Qwen2-VL-72B model. These results emphasize TVCs ability to excel in tasks requiring both perception and reasoning. Collectively, the findings indicate that TVC not only advances performance in specialized visual reasoning tasks but also offers substantial benefits in broader application scenarios. 4.4. Further Analysis In this section, we conduct comprehensive experiments to evaluate the design choices of TVC, emphasizing the key contributions of our approach. We also present case study to further illustrate the qualitative effectiveness of TVC. Effectiveness of TVC system. To evaluate the effectiveness of the TVC system, we conduct comprehensive ablation experiments on various components using the Qwen2-VL7B as the Baseline. We begin by performing supervised fine-tuning on the Qwen2-VL-7B model with the data from Section 3, referred to as Vanilla - Direct SFT. Furthermore, we apply the DVR training approach outlined in Section 2.2, which increases the focus on the visual information in the training data, enabling the model to implicitly learn visual conditioning capabilities. Additionally, during the testing phase, we experiment with resetting the visual KV cache 6 Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning Table 3: Ablations on Token Compression. Method MathVista MathVision MathVerse Avg TVC Baseline + 2x2 Avg Pooling + 4x4 Avg Pooling 68.3 67.8 68.1 21.5 22.9 22.7 39.6 38.3 38.9 43.1 43.0 43. training, and the results indicate that TVC benefits from increased data. To prevent underfitting from becoming performance bottleneck, we increase the number of training epochs with smaller datasets, which further improves model performance. This indicates that insufficient data or epochs lead to undertrained models, making it difficult to learn long-chain reasoning. Increasing both the data and training epochs can effectively alleviate this issue. 4.5. Case Study We present comprehensive case study in Figure 6 to illustrate the improvements of our TVC approach. Specifically, we provide an example that highlights advancements in the reasoning process. In this example, the model is tasked with carefully observing the objects within an image and, after eliminating certain shapes and attributes, providing the count of the remaining objects. During base CoT reasoning, the model fails to check the attributes of each object and only focuses on their shapes, leading to an incorrect final answer. Subsequently, the model learns to re-focus on the image and carefully describe the attributes of each object in detail. This process allows the model to identify the issue in its previous reasoning and provide the correct answer. 5. Conclusion In this paper, we introduce Take-along Visual Conditioning (TVC), novel strategy designed to enhance the reasoning capabilities of MLLMs by addressing the challenge of visual attention degradation during long-chain reasoning. By dynamically shifting the image input to critical stages of reasoning and compressing redundant visual tokens, we ensure that the model maintains focus on the visual information throughout the process. Our extensive evaluation on several mathematical reasoning benchmarks demonstrates the effectiveness of TVC in improving multimodal reasoning, providing robust approach to equip MLLMs with better visual grounding for complex tasks."
        },
        {
            "title": "Limitations",
            "content": "Despite advancements, our method may still exhibit several limitations. First, for highly complex reasoning tasks requiring sophisticated analytical capabilities, simply increasing visual revisits proves insufficient. In contrast, it is crucial Figure 5: Ablations on the amount of training data. TVC benefits from data scaling, continually improving the reasoning capabilities. midway through the reasoning process, and after compressing the visual tokens, we add them to the end of the reasoning steps. This strategy allows the model to further observe the image content during its thought process, mitigating the visual forgetting and suppressing hallucinations. As demonstrated in Table 2, the results highlight that the TVC system is crucial for enhancing visual reasoning capabilities. When using only the SFT training data without the DVR strategy in TVC (Vanilla - Direct SFT), improvements in visual reasoning tasks are limited, as the model lacks the ability to reflect on visual information. Furthermore, models trained with the DVR strategy alone still produce sub-optimal results, underscoring the importance of comprehensive approach that integrates PVC and DVR. The contributions of PVC and DVR are relatively balanced in enhancing the reasoning capabilities. To further validate the effectiveness of our visual token compression, we conduct experiments with different pooling methods. As shown in Table 3, the TVC Baseline represents the method without image compression. We observe that the use of pooling methods has little impact on the models capabilities. Utilizing 4x4 average pooling for compression not only enhances the models inference efficiency but also achieves slight performance improvement. Data Scaling Law. To validate the scalability of our approach, we also conduct data scaling law experiment for training. As shown in Figure 5, we compare models across various data size: 50K, 100K, 150K, and 200K samples, and present the performance relative to the base model (Qwen2VL-7B). Our findings show that as the dataset size increases, the models performance continues to improve. However, it is evident that with small dataset, the models reasoning ability cannot reach an optimal level solely through SFT. Therefore, data scaling proves highly effective during SFT Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning Figure 6: Case Study of TVC. TVC effectively re-examines the image during the reflection process to correct mistakes, guiding the model to the correct answer. to enhance the models inherent reasoning capacity. Second, our method assumes the availability of delayed visual processing, making it potentially unsuitable for real-time applications requiring instantaneous visual feedback, such as robotic navigation or time-sensitive decision-making scenarios."
        },
        {
            "title": "References",
            "content": "AI, M. Build the future of ai with meta llama 3. Technical report, Meta AI, 2024. URL https://llama.meta. com/llama3/. anthropic. Introducing the next generation of claude. Technical report, anthropic, 2024. URL https://www. anthropic.com/news/claude-3-family. Chen, Z., Wu, J., Wang, W., Su, W., Chen, G., Xing, S., Muyan, Z., Zhang, Q., Zhu, X., Lu, L., et al. Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks. arXiv:2312.14238, 2023. Chen, Z., Wang, W., Cao, Y., Liu, Y., Gao, Z., Cui, E., Zhu, J., Ye, S., Tian, H., Liu, Z., et al. Expanding performance boundaries of open-source multimodal models with model, data, and test-time scaling. arXiv preprint arXiv:2412.05271, 2024b. DeepSeek-AI. Incentivizing reasonDeepseek-r1: ing capability in llms via reinforcement learning. Technical report, DeepSeek, 2024. URL https: //github.com/deepseek-ai/DeepSeek-R1/ blob/main/DeepSeek_R1.pdf. Bai, J., Bai, S., Chu, Y., Cui, Z., Dang, K., Deng, X., Fan, Y., Ge, W., Han, Y., Huang, F., et al. Qwen technical report. arXiv:2309.16609, 2023. Chen, L., Zhao, H., Liu, T., Bai, S., Lin, J., Zhou, C., and Chang, B. An image is worth 1/2 tokens after layer 2: Plug-and-play inference acceleration for large visionlanguage models, 2024a. Duan, H., Yang, J., Qiao, Y., Fang, X., Chen, L., Liu, Y., Dong, X., Zang, Y., Zhang, P., Wang, J., et al. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models. arXiv preprint arXiv:2407.11691, 2024. Fu, C., Lin, H., Wang, X., Zhang, Y.-F., Shen, Y., Liu, X., Li, Y., Long, Z., Gao, H., Li, K., et al. Vita-1.5: Towards gpt8 Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning 4o level real-time vision and speech interaction. arXiv preprint arXiv:2501.01957, 2025. Gao, J., Pi, R., Zhang, J., Ye, J., Zhong, W., Wang, Y., Hong, L., Han, J., Xu, H., Li, Z., et al. G-llava: Solving geometric problem with multi-modal large language model. arXiv preprint arXiv:2312.11370, 2023. Guo, J., Zheng, T., Bai, Y., Li, B., Wang, Y., Zhu, K., Li, Y., Neubig, G., Chen, W., and Yue, X. Mammoth-vl: Eliciting multimodal reasoning with instruction tuning at scale. arXiv preprint arXiv:2412.05237, 2024. He, C., Luo, R., Bai, Y., Hu, S., Thai, Z. L., Shen, J., Hu, J., Han, X., Huang, Y., Zhang, Y., et al. Olympiadbench: challenging benchmark for promoting agi with olympiadlevel bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. Hu, Y., Shi, W., Fu, X., Roth, D., Ostendorf, M., Zettlemoyer, L., Smith, N. A., and Krishna, R. Visual sketchpad: Sketching as visual chain of thought arXiv preprint language models. for multimodal arXiv:2406.09403, 2024. Jiang, J., Chen, Z., Min, Y., Chen, J., Cheng, X., Wang, J., Tang, Y., Sun, H., Deng, J., Zhao, W. X., et al. Technical report: Enhancing llm reasoning with reward-guided tree search. arXiv preprint arXiv:2411.11694, 2024. Kwon, W., Li, Z., Zhuang, S., Sheng, Y., Zheng, L., Yu, C. H., Gonzalez, J. E., Zhang, H., and Stoica, I. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Li, B., Zhang, Y., Guo, D., Zhang, R., Li, F., Zhang, H., Zhang, K., Zhang, P., Li, Y., Liu, Z., et al. Llavaonevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. Li, C., Wong, C., Zhang, S., Usuyama, N., Liu, H., Yang, J., Naumann, T., Poon, H., and Gao, J. Llava-med: Training large language-and-vision assistant for biomedicine in one day. arXiv:2306.00890, 2023. Liu, H., Li, C., Li, Y., Li, B., Zhang, Y., Shen, S., Improved reasonJanuary 2024a. https://llava-vl.github.io/blog/ and Lee, Y. J. ing, ocr, URL 2024-01-30-llava-next/. and world knowledge, Llava-next: Lu, P., Peng, B., Cheng, H., Galley, M., Chang, K.-W., Wu, Y. N., Zhu, S.-C., and Gao, J. Chameleon: Plug-andplay compositional reasoning with large language models. Advances in Neural Information Processing Systems, 36: 4344743478, 2023. Lu, P., Bansal, H., Xia, T., Liu, J., Li, C., Hajishirzi, H., Cheng, H., Chang, K.-W., Galley, M., and Gao, J. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In ICLR, 2024a. Lu, S., Li, Y., Chen, Q.-G., Xu, Z., Luo, W., Zhang, K., and Ye, H.-J. Ovis: Structural embedding alignment for multimodal large language model. arXiv preprint arXiv:2405.20797, 2024b. McKinzie, B., Gan, Z., Fauconnier, J.-P., Dodge, S., Zhang, B., Dufter, P., Shah, D., Du, X., Peng, F., Weers, F., et al. Mm1: Methods, analysis & insights from multimodal llm pre-training. arXiv:2403.09611, 2024. OpenAI. Gpt-4o: Hello gpt-4o. Technical report, OpenAI, 2024a. URL https://openai.com/index/ hello-gpt-4o/. OpenAI. Learning to reason with llms. Technical report, OpenAI, 2024b. URL https://openai.com/ index/learning-to-reason-with-llms/. Qin, Y., Li, X., Zou, H., Liu, Y., Xia, S., Huang, Z., Ye, Y., Yuan, W., Liu, H., Li, Y., et al. O1 replication journey: strategic progress reportpart 1. arXiv preprint arXiv:2410.18982, 2024. QwenTeam. Qvq: To see the world with wisdom. Technical report, Alibaba, 2024. URL https://qwenlm. github.io/blog/qvq-72b-preview/. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural language supervision. In ICML, 2021. Reid, M., Savinov, N., Teplyashin, D., Lepikhin, D., Lillicrap, T., Alayrac, J.-b., Soricut, R., Lazaridou, A., Firat, O., Schrittwieser, J., et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv:2403.05530, 2024. Shi, W., Hu, Z., Bin, Y., Liu, J., Yang, Y., Ng, S.-K., Bing, L., and Lee, R. K.-W. Math-llava: Bootstrapping mathematical reasoning for multimodal large language models. arXiv preprint arXiv:2406.17294, 2024. Liu, Y., Tian, L., Zhou, X., Gao, X., Yu, K., Yu, Y., and Zhou, J. Points1. 5: Building vision-language model towards real world applications. arXiv preprint arXiv:2412.08443, 2024b. Sun, H.-L., Zhou, D.-W., Li, Y., Lu, S., Yi, C., Chen, Q.-G., Xu, Z., Luo, W., Zhang, K., Zhan, D.-C., et al. Parrot: Multilingual visual instruction tuning. arXiv preprint arXiv:2406.02539, 2024. 9 Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning Team, G., Anil, R., Borgeaud, S., Wu, Y., Alayrac, J.-B., Yu, J., Soricut, R., Schalkwyk, J., Dai, A. M., Hauth, A., et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Zhuang, W., Huang, X., Zhang, X., and Zeng, J. Math-puma: Progressive upward multimodal alignment arXiv preprint to enhance mathematical reasoning. arXiv:2408.08640, 2024. Tong, S., Brown, E., Wu, P., Woo, S., Middepogu, M., Akula, S. C., Yang, J., Yang, S., Iyer, A., Pan, X., et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. arXiv preprint arXiv:2406.16860, 2024. Zou, C., Guo, X., Yang, R., Zhang, J., Hu, B., and Zhang, H. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024. Wang, K., Pan, J., Shi, W., Lu, Z., Zhan, M., and Li, H. Measuring multimodal mathematical reasoning with mathvision dataset, 2024a. Wang, P., Bai, S., Tan, S., Wang, S., Fan, Z., Bai, J., Chen, K., Liu, X., Wang, J., Ge, W., Fan, Y., Dang, K., Du, M., Ren, X., Men, R., Liu, D., Zhou, C., Zhou, J., and Lin, J. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024b. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E., Le, Q., and Zhou, D. Chain of thought prompting elicits reasoning in large language models. arXiv:2201.11903, 2022. Xu, G., Jin, P., Hao, L., Song, Y., Sun, L., and Yuan, L. Llava-o1: Let vision language models reason step-bystep. arXiv preprint arXiv:2411.10440, 2024. Yadav, A., Liu, L., and Qi, Y. Exploring primitive visual measurement understanding and the role of output format in learning in vision-language models. arXiv preprint arXiv:2501.15144, 2025. Zhai, X., Mustafa, B., Kolesnikov, A., and Beyer, L. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 1197511986, 2023. Zhang, R., Jiang, D., Zhang, Y., Lin, H., Guo, Z., Qiu, P., Zhou, A., Lu, P., Chang, K.-W., Gao, P., et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? arXiv preprint arXiv:2403.14624, 2024a. Zhang, R., Wei, X., Jiang, D., Guo, Z., Li, S., Zhang, Y., Tong, C., Liu, J., Zhou, A., Wei, B., et al. Mavis: Mathematical visual instruction tuning with an automatic data engine. arXiv preprint arXiv:2407.08739, 2024b. Zheng, Y., Zhang, R., Zhang, J., Ye, Y., Luo, Z., Feng, Z., and Ma, Y. Llamafactory: Unified efficient finetuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. URL http://arxiv.org/abs/2403. 13372. Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning A. Related Work Multimodal Large Language Models. Multimodal Large Language Models (MLLMs) (Li et al., 2023; Liu et al., 2024a; Sun et al., 2024; Wang et al., 2024b; Lu et al., 2024b; McKinzie et al., 2024) integrate vision encoders (Radford et al., 2021; Zhai et al., 2023) with LLMs (AI, 2024; Bai et al., 2023), endowing them with robust capabilities across wide range of domains. These include general visual understanding(OpenAI, 2024a; Li et al., 2024), mathematical reasoning (Shi et al., 2024; Gao et al., 2023), and answering college-level questions (Chen et al., 2023), demonstrating their versatility in real-world tasks. The rapid advancements in open-source models have also spurred the development of proprietary models, such as GPT-4o (OpenAI, 2024a), Gemini (Team et al., 2023; Reid et al., 2024), Qwen2-VLMAX (Wang et al., 2024b), and Claude3 (anthropic, 2024). These models have demonstrated remarkable performance in both evaluation benchmarks and practical applications, solidifying their position at the forefront of AI research and deployment. Reasoning with MLLMs. Recent advancements in MLLMs have significantly enhanced performance in reasoning tasks across both text and multimodal scenarios (OpenAI, 2024b; DeepSeek-AI, 2024; QwenTeam, 2024). Current methods primarily rely on CoT (Wei et al., 2022) to train MLLMs for step-by-step reasoning. Data-driven approaches include Math-LLaVA (Shi et al., 2024), which introduced the MathV360K dataset, and MAmmoTH-VL (Guo et al., 2024), which curates large-scale multimodal CoT dataset in scalable manner. Another line of research explores vision-text alignment. MAVIS (Zhang et al., 2024b) finetunes math-specific vision encoder with curated caption data, while Math-PUMA (Zhuang et al., 2024) leverages the Kullback-Leibler (KL) divergence of next-token prediction distributions for modality alignment. In different paradigm, MLLMs act as coordinators, utilizing external tools such as LLMs, web search engines, and computer programs for complex reasoning. Chameleon (Lu et al., 2023) orchestrates tool-call sequences, and Visual Sketchpad (Hu et al., 2024) enables models to generate visual sketches to aid reasoning. B. More Details of Reasoning Dataset In this section, we provide detailed description of dynamic token truncation and reflection word pruning in the process of constructing the reasoning dataset. We also provide detailed information about the training data in Table 5. B.1. Dynamic Token Truncation To further improve the dataset quality, we analyze the distribution of token lengths after the answer-centric rejection sampling. We find that many samples are close to the maximum token limit, and manual checks show that these long reasoning chains often have problemssuch as logical errors, mistakes in multi-step calculations, and reliance on shortcuts that dont work in general cases (e.g., substituting specific values). Motivated by the correlation between extreme token lengths and compromised solution quality, we implement adaptive truncation thresholds to keep the answers within the 200-8000 token range. This dynamic filtering not only eliminates the invalid cases (overly verbose or terse responses) but also enhances the overall quality of the data. The final length distribution matches how human experts solve problems and keeps the important reasoning steps intact. B.2. Reflection Word Pruning Our analysis reveals critical flaw in distilled reasoning chains: excessive metacognitive loops caused by uncontrolled reflection markers (e.g., Alternatively, Wait), which led to performance degradation through infinite loops or ungrounded speculation. Term frequency analysis of reflection density shows heavy-tailed distribution95% of samples contained fewer than 10 reflection markers per chain, while 1% exhibited over 50 markers, with this extreme group strongly correlating to hallucination rates. To address this, we introduce reflection token quota system that automatically prunes samples exceeding 25 reflection markers while retaining core reasoning logic using semanticaware span detection. As shown in Figure 4, this approach significantly reduced infinite-loop instances in validation tasks while improving answer accuracy. The refined reflection pattern mirrors expert human problem-solving strategies, wherein targeted self-correction enhances, rather than disrupts, the continuity of the reasoning process. C. Discussion In this work, we investigate the phenomenon of visual information forgetting in MLLMs during long-chain reasoning. Through comprehensive analysis and experiments, we show that as reasoning chains progressively lengthen, models exhibit gradual deterioration in retaining visual inputs, ultimately undermining their multimodal reasoning capabilities and exacerbating hallucination issues. To address this challenge, we propose take-along visual conditioning mechanism that enables models to dynamically revisit visual inputs during reasoning steps, thereby enhancing content fidelity throughout the inference process. Our work represents an initial exploration into mitigating visual forgetting in extended multimodal reasoning chains. We envision future research directions including: (1) Developing hybrid architectures that synergistically enhance both visual retention and intrinsic reasoning capabilities; (2) 11 Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning Figure 7: The token and reflection word distribution of the long-chain reasoning dataset. Table 4: The detailed training hyperparameters. Config Deepspeed Epoch Warmup Ratio Max Grad Norm Optimizer Learning rate Learning rate scheduler Text max length Batch size per GPU Gradient Accumulation Steps GPU Precision SFT Zero3 5 0.1 1.0 AdamW 2e-5 Cosine 8192 1 4 64H20-96G Bf16 Investigating adaptive attention mechanisms for real-time multimodal applications; (3) Exploring curriculum learning strategies to progressively strengthen long-chain reasoning capacities. We hope this foundational study will inspire further advances in understanding and improving multimodal reasoning systems for complex real-world applications. Table 5: Details on the TVCs training data, which is derived from publicly available datasets. Datasets Samples MathV360K (Shi et al., 2024) Geo170K (Gao et al., 2023) LLaVA-OneVision (Li et al., 2024) Cambrian-1 (Tong et al., 2024) 221K 22K 97K 1K Mitigating Visual Forgetting via Take-along Visual Conditioning for Multi-modal Long CoT Reasoning Figure 8: Qualitative Results of TVC."
        }
    ],
    "affiliations": [
        "National Key Laboratory for Novel Software Technology, Nanjing University",
        "School of Artificial Intelligence, Nanjing University",
        "Tencent"
    ]
}