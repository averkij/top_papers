{
    "paper_title": "SepLLM: Accelerate Large Language Models by Compressing One Segment into One Separator",
    "authors": [
        "Guoxuan Chen",
        "Han Shi",
        "Jiawei Li",
        "Yihang Gao",
        "Xiaozhe Ren",
        "Yimeng Chen",
        "Xin Jiang",
        "Zhenguo Li",
        "Weiyang Liu",
        "Chao Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have exhibited exceptional performance across a spectrum of natural language processing tasks. However, their substantial sizes pose considerable challenges, particularly in computational demands and inference speed, due to their quadratic complexity. In this work, we have identified a key pattern: certain seemingly meaningless special tokens (i.e., separators) contribute disproportionately to attention scores compared to semantically meaningful tokens. This observation suggests that information of the segments between these separator tokens can be effectively condensed into the separator tokens themselves without significant information loss. Guided by this insight, we introduce SepLLM, a plug-and-play framework that accelerates inference by compressing these segments and eliminating redundant tokens. Additionally, we implement efficient kernels for training acceleration. Experimental results across training-free, training-from-scratch, and post-training settings demonstrate SepLLM's effectiveness. Notably, using the Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the GSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in streaming settings, SepLLM effectively processes sequences of up to 4 million tokens or more while maintaining consistent language modeling capabilities."
        },
        {
            "title": "Start",
            "content": "SepLLM: Accelerating Large Language Models by Compressing One Segment into One Separator Guoxuan Chen 1 2 Han Shi 1 Jiawei Li 1 Yihang Gao 2 Xiaozhe Ren 1 Yimeng Chen 3 Xin Jiang 1 Zhenguo Li 1 Weiyang Liu 4 Chao Huang 2 Project page: sepllm.github.io 4 2 0 2 6 1 ] . [ 1 4 9 0 2 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) have exhibited exceptional performance across spectrum of natural language processing tasks. However, their substantial sizes pose considerable challenges, particularly in computational demands and inference speed, due to their quadratic complexity. In this work, we have identified key pattern: certain seemingly meaningless special tokens (i.e., separators) contribute disproportionately to attention scores compared to semantically meaningful tokens. This observation suggests that information of the segments between these separator tokens can be effectively condensed into the separator tokens themselves without significant information loss. Guided by this insight, we introduce SepLLM, plug-and-play framework that accelerates inference by compressing these segments and eliminating redundant tokens. Additionally, we implement efficient kernels for training acceleration. Experimental results across trainingfree, training-from-scratch, and post-training settings demonstrate SepLLMs effectiveness. Notably, using the Llama-3-8B backbone, SepLLM achieves over 50% reduction in KV cache on the GSM8K-CoT benchmark while maintaining comparable performance. Furthermore, in streaming settings, SepLLM effectively processes sequences of up to 4 million tokens or more while maintaining consistent language modeling capabilities. 1. Introduction Transformer-based models (Vaswani et al., 2017) have exhibited exceptional performance across wide range of 1Huawei Noahs Ark Lab 2The University of Hong Kong 3Center of Excellence for Generative AI, KAUST 4Max Planck Institute for Intelligent Systems, Tubingen. Correspondence to: Han Shi <shi.han@huawei.com>. Technical Report Figure 1. The loss comparison between vanilla Transformer and proposed SepLLM. SepLLM achieves lower loss at different computation costs and different training time consistently. tasks, including natural language processing (Zhang et al., 2020; Raffel et al., 2020), computer vision (Dosovitskiy et al., 2020), and scientific machine learning (Geneva & Zabaras, 2022). However, vanilla Transformers that rely on next-token prediction face significant computational challenges, particularly when scaling to larger models and longer contexts. These computational inefficiencies significantly impact both inference speed and training time. The core challenge underlying these efficiency issues is the self-attention module, which exhibits quadratic complexity with respect to the number of input tokens. Research on efficient Transformers in LLMs primarily follows two major directions. The first approach focuses on linear attention (Katharopoulos et al., 2020; Schlag et al., 2021), replacing the vanilla self-attention module with alternatives that achieve linear complexity. However, these architectural modifications are significantly different from traditional self-attention, preventing direct utilization of powerful pretrained Transformer models. The second approach emphasizes KV cache optimization (Xiao et al., 2024a; Zhu et al., 2024; Xiao et al., 2024b; Li et al., 2024b), aiming to eliminate redundant KV cache to accommodate longer input contexts. For example, Xiao et al. (2024a) introduced an adaptive mechanism that selectively retains essential tokens and their key values based on cumulative attention scores. Similarly, Zhu et al. (2024) proposed token selection strategy with controlled sparsity, achieving near-lossless acceleration. While promising, these training-free methods adapt poorly to the training stage, resulting in discrepancies between training and inference performance. StreamingLLM (Xiao Accelerating LLMs by Compressing One Segment into One Separator est attention weights. Such empirical findings motivate us to propose SepLLM, simple yet effective framework to accelerate inference. Through targeted masking experiments on well-trained LLMs, we demonstrate that separator tokens contain crucial information and are essential for model performance. This finding suggests that sequences are initially segmented by separators, with segment information being compressed into these frequently attended separator tokens while redundant specific tokens are discarded. We conduct comprehensive experiments to validate SepLLMs effectiveness across various tasks and datasets, examining performance in training-free, training-fromscratch, and post-training settings. We have made our implementation publicly available at sepllm.github.io. Our codebase supports efficient multinode distributed training with accelerated attention module Sep-Attention and also includes numerous existing Fusion Operators to accelerate the training process, such as fused rope (Su et al., 2023), fused layer norm, etc. 2. Related Work KV Cache Compression. Recent research has focused on overcoming LLMs limitations in processing extensive contextual inputs. FastGen (Ge et al., 2024) proposes an adaptive KV cache management method, optimizing memory usage by customizing retention strategies for different attention heads. SnapKV (Li et al., 2024b) enhances efficiency through KV cache compression, utilizing attention scores to select and cluster significant positions. H2O (Zhang et al., 2024b) implements dynamic token retention policy, balancing recent and historically important information to optimize memory use. StreamingLLM (Xiao et al., 2024b) expands LLMs capabilities to handle infinite sequence lengths without fine-tuning, by reserving attention sinks and local tokens. QuickLLaMA (Li et al., 2024a) proposes to evict the query-aware KV caches for inference acceleration. PyramidInfer (Yang et al., 2024) and PyramidKV (Zhang et al., 2024a) modify the KV cache capacity throughout various layers, with larger allocation in the lower sections and reduced amount in the upper portions. However, most works in this category cannot be applied into training phase. Sparse Attention. Sparse attention involves creating sparse attention matrices by limiting attention to predefined patterns, such as local windows or fixed-stride block patterns. Beltagy et al. (2020) combine dilated local window attention with task-specific global attention. BigBird (Zaheer et al., 2020) proposes linear complexity attention alternative using global tokens, local sliding window attention, and random attention. In comparison, SparseBERT (Shi et al., 2021) proposes differentiable attention mask algorithm to learn the attention mask in an end-to-end manner. Note Figure 2. The visualization of attention scores among different layers given the input Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. .... Note that the separator tokens like , and . contribute massive attentions. et al., 2024b) represents notable attempt to address these limitations by preserving attention sinks and local tokens to reduce computation and memory overhead. However, it omits many intermediate tokens, resulting in performance degradation compared to standard Transformers. To gain better understanding of the intrinsic mechanism of LLMs, we analyze attention patterns across different samples. Figure 2 illustrates the attention distribution when Llama-3-8B-instruct (Dubey et al., 2024) processes math problem (complete results are presented in Appendix A). Surprisingly, rather than focusing on semantically meaningful tokens (such as nouns and verbs), LLMs tend to prioritize attention to seemingly meaningless special tokens (like . or n) for information retrieval. This observation suggests that segment information is compressed and embedded into these separator tokens, enabling efficient information retrieval without direct extraction from content tokens. Inspired by this observation, we introduce SepLLM, an efficient transformer architecture featuring data-dependent sparse attention mechanism that selectively retains only initial, neighboring, and separator tokens while dropping other tokens. The training-free SepLLM performs comparably to vanilla Transformer, which validates our hypothesis that segment information is effectively compressed into separator tokens. More importantly, we integrate SepLLM to the training stage (including both training from scratch or finetuning) and implement hardware-efficient kernel based on FlexAttention (PyTorch, 2024). This integration reduces the discrepancies between training and inference that are present in previous approaches. As demonstrated in Figure 1, SepLLM consistently achieves lower loss compared to vanilla Transformer given the same computational costs or training time. Moreover, SepLLM reduces computational costs by 28% and training time by 26% while achieving the same training loss. Our contributions are summarized as follows: We analyze attention patterns by visualizing token-level attention scores, revealing that initial tokens, neighboring tokens, and separator tokens consistently receive the highAccelerating LLMs by Compressing One Segment into One Separator that most works about sparse attention are fixed and built on BERT (Devlin et al., 2019) families. In comparison, our proposed SepLLM are mainly built on GPT (Brown et al., 2020) series and its attention masks are data-dependent. 3. Method 3.1. Fundamental Design From Figure 2, we can observe that within given input context, seemingly meaningless separator tokens receive higher attention scores compared to tokens with actual semantic meanings. Therefore, we propose novel Transformer architecture where, for certain layer of the transformer (i.e., self-attention layer), each token in the input can only see portion (not all) of the hidden states of tokens preceding the current token, outputted by the previous transformer layer. This subset of tokens includes the initial few words (i.e., attention sinks (Xiao et al., 2024b)), all separator tokens before the current token, and the closest tokens to the current token. Details are as follows. Initial Tokens. When using the sliding window mechanism (Beltagy et al., 2020) for generation, removing the keyvalue (KV) pairs corresponding to the initial tokens in the KV Cache results in noticeable increase in the perplexity of generated tokens, phenomenon mentioned by Xiao et al. (2024b). The initial few tokens are also referred to as attention sinks. We retained this setup and further validated the role of initial tokens in subsequent experiments. Usually, initial tokens are kept. Separator Tokens. From Figure 2, we can observe that within given input context, seemingly meaningless separator tokens (such as commas, periods, exclamation marks, semicolons, etc.) that segment sequences receive higher attention scores compared to semantically meaningful tokens (such as nouns or verbs). Therefore, we hypothesize that these separators may compress the information of the text segments naturally segmented by them, such that when the Transformer generates new tokens, it only needs to reference the information contained in these separators to extract the information pertaining to those text segments. Hence, in training-free scenario, we employed this strategy and achieved similar results to the original model based on full attention across many tasks. Furthermore, to reinforce the effect of using separators to compress information within their respective segments, we employed training-from-scratch and post-training approaches to compel the model during training to restrict the current token from accessing all information from distant preceding text, i.e., in each segment, only the separator representing its segment is visible to the current token (with other tokens being masked, see Figure 3). After training in this manner, the information within segments is forced to be condensed into the separators, leading the Transformers probability distribution for predicting Figure 3. The overall paradigm of SepLLM. The left side illustrates the attention mask in the training or pre-filling stage given the input ABC,DE.FGn. The right side illustrates the KV cache management in the generation stage. the next word closely resembling that of the original Transformer with full attention. Neighboring Tokens. Language tasks usually exhibit strong local dependencies and interactions, since adjacent tokens often form coherent phrases or have dependencies that are required to be captured. Neighboring tokens usually help form locally smooth and coherent contexts, allowing the model to generate sentences that are reasonable within the immediate context. The neighboring tokens, also referred to as local attention or sliding window attention, are considered in various efficient Transformers (Xiao et al., 2024b; Zhang et al., 2024b) and we have also adopted this approach, with the number of preceding tokens closest to the current token denoted as n. 3.2. Overall Pipeline We split the overall pipeline of our proposed SepLLM into training/pre-filling stage and generating stage. Training/Pre-filling. During the training/pre-filling stage of SepLLM architecture, we do not need to multiply all query vectors corresponding to tokens in the input context with all the key vectors. It is sufficient to just multiply the vectors of the query-key pairs corresponding to the highlighted elements in the mask matrix shown in Figure 3. The formulation can be illustrated in the following. Mul (cid:0)Q, K(cid:12) = Softmax (Λ) , Λ = (cid:12) M(cid:1) dk = (1) where Rmdk , Rmdk are the matrices of query and key for one attention layer, in which each row vector Qi, Kj correspond to the query of i-th token and the key of j-th token in the input context with sequence length m. dk denotes the dimension for key and query vectors. Λ, Rmm are the raw and final attention maps, respectively. Rmdv is value matrix of dimension dv 3 Accelerating LLMs by Compressing One Segment into One Separator Figure 4. Overall framework of the proposed SepLLM tailored for streaming applications. The KV pairs are storaged in four cache blocks (displayed as four columns), and are updated in each iteration (shown in single row). Once the runtime usage Sizerun reach the max capacity c, SepLLM move KV caches of separator tokens in Past Window Cache into Separator Cache and drop other KV caches. and Rmdv deonotes the output for the current attention layer. Mul() represents sparse matrix multiplication function which can be optimized by methods like Zhu et al. (2024) and we also implement our own module named SegAttention to accelerate this process. Bmm is binary mask matrix1 used as parameter for Mul(): cache by retaining only the KV for separators, neighboring and initial tokens. However, as the number of input tokens increases, the number of separators in KV cache will also accumulate endlessly, which is not feasible for streaming settings. Therefore, we propose Tailored Streaming Design for streaming scenarios. (cid:40) Λi,j = Kj/ (cid:112) dk , , if Mi,j = 1 if Mi,j = 0 . (2) where Λi,j, Ai,j, Mi,j are the elements in the i-th row and j-th column of matrices Λ, A, M, respectively. Since Ai,j = 0 if Λi,j = , the tokens that are not Initial, Separator, and Neighboring tokens will be masked by in Equation 1. This strategy (Equation 1) applies to all heads of multi-head attention (Vaswani et al., 2017). Generation. The management of the KV cache during the generation stage for this Fundamental Design (Section 3.1) is also intuitive. As shown in the right side of Figure 3, when generating new token, we only preserve the KV caches for the Initial, Separator, and Neighboring tokens. Therefore, KV caches in the proposed SepLLM is much smaller and require less memory. Ideally, based on SepLLM, the perplexity of generating the next word is comparable to that of the original Transformer with full attention. 3.3. Tailored Streaming Design In real-world scenarios, there are numerous streaming applications such as multi-round dialogues, where long interactions are expected (Xiao et al., 2024b). Hence, we expect SepLLM to handle infinite input without significantly sacrificing efficiency and performance, especially for the streaming applications. As discussed in Fundamental design (Section 3.1), SepLLM can save substantial amount of KV 1B := {0, 1}, which is binary set. 4 Framework. Figure 4 illustrates the SepLLMs processing architecture for streaming inference applications. The diagram depicts multiple iterations, with each row representing distinct processing cycle. The system simultaneously maintains four specialized cache blocks: Initial Cache, Separator Cache, Past Window Cache, and Local Window Cache. Specifically, Initial Cache captures the attention sink proposed by Xiao et al. (2024b). Local Window and Past Window Caches store the KVs for consecutive tokens, with Past Window Cache serving as an overflow buffer for the Local Window Cache. Separator Cache retains the KVs for separators which contain condensed segment information. To describe the cache management policies, we denote the runtime usage of the four caches as Sizeinit, Sizesep, Sizepast w, and Sizelocal w, respectively. The runtime usage across all KV caches is defined as Sizerun := Sizeinit + Sizesep + Sizepast + Sizelocal w, which satisfies Sizerun c. The number of contiguous Neighboring tokens is defined as := Sizepast +Sizelocal w. Notably, is function of the input sequence length rather than fixed hyperparameter for streaming setting. For clarity, we detail the preset hyperparameters of this caching system as follows. c: The maximum capacity of the entire KV caches. a: The maximum capacity of Initial Cache. s: The maximum capacity of Separator Cache. w: The maximum capacity of Local Window Cache. Notably, is also the minimum value of after the runtime KV caches usage Sizerun reaches c. Accelerating LLMs by Compressing One Segment into One Separator During streaming sequence generation, SepLLM will first fill Initial Cache and then Local Window Cache. After Sizelocal reaches w, subsequent tokens are directed to Past Window Cache. Compression is triggered when Sizerun reaches (iteration 1 in Figure 4), where separator tokens in Past Window Cache are moved to Separator Cache and other tokens are discarded. When Separator Cache reaches its capacity at some sequence length m0, enters periodic pattern. Specifically, for > m0, follows periodically linear function bounded by and s. The detailed evolution of KV caches is illustrated in Appendix B. To analyze the average usage of KV caches, we define nm := 1 k=1 n(k). According to the periodicity, we have (cid:80)m lim nm = + 2 . (3) For the average runtime KV cache usage of infinite-length sequence generation, we have lim nm + + Sizerun = lim + + + 2 = < c. (4) Positional Encoding. Our positional encoding strategy for streaming settings is the same as the state-of-the-art StreamingLLM (Xiao et al., 2024b), designed specifically for infinite-length inputs, where we focus on positions within the caches instead of those in the original text. 4. Experiments and Results 4.1. Experimental Settings We evaluate our proposed SepLLM on the following tasks, i.e., training-free, training from scratch, posting-training and streaming applications. Model. Two popular model families, i.e., Pythia (Biderman et al., 2023) and Llama-3 (Dubey et al., 2024), are employed for evaluation. Specifically, Pythia-160m-deduped is used in the training from scratch tasks since the model, data, configurations, and checkpoints are all open-source and the training results are reproducible. As for post-training settings, we take Pythia-1.4B-deduped as our base model. Even though Llama-3 exhibits powerful performance on various downstream task, these training experimental details are not available. Therefore, we only use Llama-3 for training-free and streaming tasks. Training Datasets. In the training from scratch and posttraining tasks, the deduplicated Pile (Gao et al., 2020) is utilized for training, which contains about 207B tokens. And all other configurations are the same as the corresponding settings as Pythia (Biderman et al., 2023). Specifically, the training epoch is set to 1.5 epoch (143000 steps with 5 the global batch size as 1024), which means about 300B tokens in total are utilized for training from scratch, which is identical to Pythia (Biderman et al., 2023). Parameter Setting. The official 93,000-step checkpoint of Pythia-1.4B-deduped model is used to conduct posttraining, which corresponds to just completing one epoch of training on the deduped Pile dataset (Gao et al., 2020). And [., ,, ?, !, ;, :, , t, n] are separator tokens used for all evaluations. More specific experimental settings are introduced in the respective experiment sections. 4.2. Training-free We evaluate the proposed SepLLM architecture in the training-free tasks based on the popular Llama-3-8BInstruct model (Dubey et al., 2024). Benchmarks. The representative and commonly-used GSM8K-CoT (Cobbe et al., 2021) and MMLU (Hendrycks et al., 2021)) are adopted. GSM8K-CoT (Cobbe et al., 2021) tests models ability to solve mathematical problems by evaluating its reasoning and step-by-step problem-solving skills. CoT (Wei et al., 2022) means the ability to simulate reasoning process by breaking down complex problems into series of logical steps. And the commonly-used 8 shots are adopted. MMLU (Hendrycks et al., 2021) assesses models general knowledge and reasoning across wide range of subjects, such as history, science, mathematics and so on. The common 5-shot setting is used for MMLU. Results. The experimental results for training-free are shown in Table 1. Vanilla represents the original Llama3 model with full attention, while StrmLLM represents StreamingLLM (Xiao et al., 2024b). means the number of KV for neighboring tokens we retain. For SepLLM, all the KV for Special Tokens are kept and for the setting SepLLM (n=256), we find that SepLLM exhibits comparable performance in both multi-step mathematical CoT task and multidisciplinary knowledge reasoning tasks, when compared to the full-attention Llama-3. SepLLM achieves this using only 47.36% of KV utilized by the original Llama-3 for reasoning, indicating SepLLMs capability of modeling contexts requiring both multi-step logical analysis and those involving multi-domain knowledge reasoning while retaining only 50% original KV. StrmLLM (n=256) setting corresponds to removing all separators KV from SepLLM (n=256) setting, except for those in Neighboring and Initial tokens. We observe noticeable decrease in both mathematical analysis and multidisciplinary knowledge reasoning abilities of StrmLLM (n=256). StrmLLM (n=256) utilizes only 26.00% and 37.73% of the KV for the GSM8K and MMLU tasks, respectively, which are less than SepLLM (n=256) (47.36% and 44.61% reAccelerating LLMs by Compressing One Segment into One Separator GSM8K-CoT strict r.KV(%) flexible humanities stem MMLU social other Overall r.KV (%) Vanilla StrmLLM (n=380) StrmLLM (n=256) SepLLM (n=256) 60.49 57.73 62.10 57.66 Table 1. Evaluation results and average running time KV cache usage for training-free experiments on GSM8K-CoT 8-shots and MMLU 5-shots. For SepLLM and StreamingLLM, three initial tokens KV is kept for this experiment. r.KV(%) here represents the ratio of KV usage at runtime for the respective method compared to Vanilla. 100.00 52.50 37.73 44.61 100.00 47.54 26.00 47.36 76.50 74.39 73.06 76. 65.72 63.39 62.10 64.68 77.26 71.42 68.61 77.18 72.19 70.13 69.78 72.19 56.61 54.46 54.49 56.49 77.79 70.89 69.67 77.18 Method Vanilla StrmLLM(n=64) SepLLM(n=64) SepLLM(n=128) SepLLM(n=64,H) SepLLM(n=64,H/T) ARC-c 20.14 20.65 19.62 19.97 20.73 21.42 ARC-e 46.80 47.39 46.46 47.35 48.44 47.26 LBD-ppl 34.83 44.03 40.08 30.16 36.54 33.41 LBD-acc 33.28 26.74 28.97 33.18 30.45 32.80 LogiQA 23.81 21.97 26.42 22.73 25.35 22.73 PIQA 62.84 63.82 63.82 64.64 64.36 63. SciQ 81.50 75.80 80.10 82.60 80.60 81.20 Attn(%) 100.00 16.58 25.83 35.64 32.01 38.18 r.KV(%) 100.00 15.28 25.40 32.27 31.58 37.75 Table 2. The performance of downstream tasks and the usage of running-time KV cache in the training from scratch setting. 2018), LAMBADA (Paperno et al., 2016) (for Perplexity and Accuracy), LogiQA (Liu et al., 2021), PIQA (Bisk et al., 2020), SciQA (Welbl et al., 2017). From the loss curves depicted in Figure 5 and the downstream performance in Table 2, we draw the following analysis. Neighboring Token Benefits. Based on the experiments with the settings SepLLM (n=64) and SepLLM (n=128), we find that during training, increasing Neighboring Tokens (n) leads to faster decrease in the training loss curve (Figure 5). Furthermore, models trained with larger exhibit stronger performance in downstream tasks  (Table 2)  . This highlights the important role of neighboring tokens in contextual language modeling and downstream task inference. Hybrid Layer Benefits. We find that employing certain hybrid architecture is beneficial for both the training loss and the performance on downstream tasks. For instance, by modifying only the first self-attention layer to full attention in the experiment corresponding to SepLLM (n=64) (denoted as SepLLM (n=64,H)), there is moderate optimization in both the training process and downstream tasks. If both the first and last attention layers are changed to full attention (denoted as SepLLM (n=64,H/T)), this optimization becomes more pronounced. For example, LAMBADA perplexity decreases from 40.08 for SepLLM (n=64) to 36.54 for SepLLM (n=64,H) and 33.41 for SepLLM (n=64,H/T). Separators Role. The experiment with the setting StrmLLM (n=64) corresponds to SepLLM (n=64), but does not consider separators other than those in Neighboring and Initial tokens. We observe significant slowdown in the training loss decrease for StrmLLM (n=64), and its performance deteriorates across various downstream tasks. This (a) Loss w.r.t steps (b) Loss Ratio w.r.t FLOPs Figure 5. Training loss curves for training from scratch. 5(b) shows the ratio of the loss values of different methods to that of Vanilla with respect to FLOPs. spectively). Consequently, we increase the of StrmLLM to 380, aligning the kept KV on GSM8K to be equal to SepLLM (n=256) (approximately 47%, while on MMLU task, StrmLLM (n=380) retains 52.5% of the KV, significantly higher than SepLLM (n=256)). This leads to improved performance compared to StrmLLM (n=256). However, it still remains lower than the full-attention Llama-3 and SepLLM (n=256). This indicates that the KV of separators indeed encapsulate information contained within their respective segments, and removing them significantly impacts the Transformers understanding and reasoning abilities. 4.3. Training from Scratch We train the original Pythia-160m-deduped model as well as the Pythia-160m-deduped model modified with the SepLLM (and StreamingLLM) architecture on the Pile dataset for 143,000 steps using global batch size of 1024 (involving approximately 300B tokens in total for training). All training configurations are consistent with Pythia (Biderman et al., 2023) (see Section 4.1). And following Pythia (Biderman et al., 2023), we conduct testing on the following downstream tasks: ARC-Challenge and ARC-Easy (Clark et al., 6 Accelerating LLMs by Compressing One Segment into One Separator Arch. Setting StrmLLM n= SepLLM Vanilla n=64 n=128 n=64,H n=64,H/T full FLOPs(%) Attn.(%) 70.11 6.43 71.77 72.58 72.83 17.21 22.48 24. 73.90 31.01 100.0 100.0 Table 3. The comparison of FLOPs and Attention Map Ratio. indicates that the KV corresponding to separators contain information about the segment they belong to, which is beneficial for predicting subsequent tokens. We also investigate the FLOPs and Attention Map Ratio (indicating the proportion of 1s in the lower triangle of the attention mask) required by the different architectures when trained on the same input data. As shown in Table 3, We find that SepLLM can significantly reduce FLOPs by approximately 30%. After plotting the loss ratios between SepLLM and Vanilla under the same FLOPs (see Figure 5(b)), we observe that SepLLM has lower loss than Vanilla. This indicates that our SepLLM architecture at least has comparable ability to extract useful information from the dataset during training as Vanilla. Besides, the detailed wall-clock time per iteration and the real-time speed up are illustrated in Appendix and Figure 1. 4.4. Post-training Since training from scratch is time-consuming, we also conduct post-training experiment using 93000-step Pythia1.4B-deduped checkpoint officially released by Pythia (see Section 4.1 for details). Figure 6 displays the loss curves for post-training, where SepLLM (n=64, larger lr) denotes that we employ an entire cosine learning rate scheduler (including warm-up process starting from 0) identical to that of original Pythia-1.4B-deduped from step 0. SepLLM (n=64) and SepLLM (n=128) utilize cosine learning rate scheduler that continues to decay from the 93000th step. From Figure 6, it is evident that increasing and appropriately raising the learning rate both facilitate the decrease in loss. Moreover, this also illustrates that SepLLM for swift transformation from full-attention Transformer checkpoint to model that aligns with the requirements of the SepLLM architectures embedding distribution through post-training. Figure 6. Training loss curves for the post-training. PG19 1M 1.5M 2M 2.5M 3M 3.5M 4M StrmLLM 39.5 38.2 38.3 37.6 36.4 35.8 36.1 SepLLM (s=32) 37.7 36.6 36.6 36.0 34.9 34.2 34.5 SepLLM (s=64) 37.1 36.0 36.1 35.4 34.3 33.7 33.9 Table 4. The perplexity comparison on the PG19 test set (Rae et al., 2020). For fair evaluation, we keep the KV cache capacity as 324 and keep Sink Cache as 4 for both StreamingLLM and SepLLM. next token through SepLLM remains consistently lower than that of streamingLLM (Xiao et al., 2024b) within the range from 1M to 4M. This once again verifies the ability of KV corresponding to separators to compress segment information and their impact on predicting the probability distribution of the next token. We also test the end-to-end inference time of Vanilla, StreamingLLM and our SepLLM on PG19 (Rae et al., 2020) test set based on LlaMA-3-8B (Dubey et al., 2024). Based on the aforementioned settings, we used these LLMs to generate 20K and 64K tokens to evaluate their total inference time (wall-clock time), average perplexity, and average KV cache usage. For both SepLLM and StreamingLLM, the maximum whole KV cache capacity was set to 800 (i.e., c=800), and the initial KV size was set to 4 (i.e., a=4). For SepLLM, we additionally set s=64 and w=256. The results are shown in Table 5. Above results demonstrate that our SepLLM can achieve lower perplexity with less wall-clock time as well as lower average runtime KV usage, especially for longer sequences, given the same max KV cache capacity c. 4.5. Streaming Applications 4.6. Ablation Study SepLLM can also adapt well to streaming applications, where infinite-length interactions may occur. Here, we follow StreamingLLM (Xiao et al., 2024b) to validate scenarios of infinite-length interactions using our Tailored Streaming Design on the commonly used PG19 dataset (Rae et al., 2020), which comprises 100 extensive literary works. The results are shown in Table 4. We can observe that for the same KV cache capacity c, the perplexity of predicting the We conduct various ablation experiments specifically for long streaming applications. This includes detailed study of the impact of various hyperparameters across different text lengths (5K to 20K). The experimental results about and (w,c) pair are illustrated in Table 6 and Table 7 respectively. The conclusions are: s: From Table 6, the capacity of Separator Cache affects the perplexity of long-text inference, as we find that in7 Accelerating LLMs by Compressing One Segment into One Separator Length Methods 20K 64K 20K Vanilla StrmLLM 800 800 SepLLM Vanilla 64K StrmLLM 800 800 SepLLM r.KV 10K 800 562 32K 800 562 ppl 302.6 31.5 28.3 1090.8 37.9 33.4 time (s) 523.8 341.2 325.8 3380.6 1096.0 1049.7 Method 320 StrmLLM 512 796 224 SepLLM 320 512 324 516 800 324 516 800 r.KV 324 516 800 308 452 690 5K 13.18 12.87 11.96 13.01 12.91 12.09 10K 15K 20K 8.91 8.85 11.51 8.78 8.74 11.37 8.72 8.67 11.01 8.72 8.67 11.17 8.72 8.67 11.26 8.62 8.56 11.03 Table 5. The perplexity and runing time comparison on the PG19 test set (Rae et al., 2020). r.KV means the average runtime KV cache usage in the generation process. Table 7. Average downstream performance (ppl) over different input lengths and average runtime KV usage with different c,w on WikiText, in which a=4 for both methods and s=64 for SepLLM. 32 48 64 5K 13.11 13.03 13.01 10K 11.31 11.26 11.17 15K 8.74 8.70 8.67 20K 8.79 8.76 8. r.KV 292 300 308 Table 6. The perplexity and average runtime KV cache usage of SepLLM with respect to different Separator Cache sizes (s) on WikiText (Merity et al., 2017), in which a=4, w=224, c=324. creasing leads to certain degree of perplexity reduction. and w: As can be seen in Table 7, and can impact the perplexity in the scenario of long streaming input with lengthy text. Moreover, as they increase, the perplexity decreases accordingly. We also perform the following experiments to validate the effectiveness of Initial Tokens and PEs shifting for both StreamingLLM and SepLLM. The experimental results are shown in Table 8 and the discussions are as follows. Initial Tokens: Initial Tokens are crucial for modeling context in long streaming inputs, whether for SepLLM or StreamingLLM. Removing them has significant impact on the perplexity of long texts. This conclusion is consistent with the paper (Xiao et al., 2024b). Positional Encodings Shifting. Following streamingLLM, we conduct Positional Shifting for streaming applications, i.e., we focus on positions within the cache rather than those in the original text. Table 8 shows that this shifting plays crucial role, as removing it significantly increases the perplexity (StreamingLLM increases from around 13 to over 400). It is noteworthy that SepLLM, even without employing this shifting, only sees perplexity increase to around 200, which is much lower than StreamingLLM. This further underscores the separators role for the stability in predicting tokens. 4.7. Generalization and Information Retrieval To verify the generalization of our SepLLM, we adapt our method to models with different architectures and different scales. The results in Appendix can validate the generalization of our proposed framework. Specifically, we adapt our proposed SepLLM to different models ininitial shift Method StrmLLM StrmLLM StrmLLM StrmLLM SepLLM SepLLM SepLLM SepLLM 8.9 10.8 5K 8.9 13.2 10.9 14.6 425.5 513.1 509.5 506.8 409.4 540.5 527.5 558.2 8.8 13.1 12.5 14.9 192.7 214.6 175.0 174.4 226.4 264.7 227.5 228.8 10K 15K 20K r.KV 324 11.5 324 13.2 324 324 292 290 292 290 11.3 14. 8.7 12.4 Table 8. The perplexity and average runtime KV cache usage of SepLLM and StreamingLLM tested on WikiText (Merity et al., 2017). c=324, a=0/4 for both methods. s=32,w=224 for SepLLM . cluding Pythia-6.9B, Pythia-12B (Biderman et al., 2023), Llama-3-8B-Base/Instruct (Dubey et al., 2024) and Falcon40B (Almazrouei et al., 2023). Moreover, we also conduct the Needle In Haystack experiment, which further demonstrates the compression capability of separator tokens for segment information. As illustrated in Appendix E, SepLLM can retrieve the needle in most scenarios. In comparison, StreamingLLM (Xiao et al., 2024b) cannot complete this task. Appendix discusses the effect of separators. 5. Concluding Remarks In this paper, we focus on efficient neural architecture modification to address the computational and storage challenges associated with processing long contexts in large language models. From the visualization of attention maps, we find that certain separator tokens consistently contribute high attention scores. Inspired by this observation, we introduce SepLLM, sparse attention mechanism , which focuses attention computation on initial tokens, neighboring tokens and separator tokens. To achieve real-time acceleration, we also implement hardware-efficient kernels. Our training-free studies suggest that these separator tokens effectively compress segment information, enabling efficient information retrieval. Unlike previous training-free methods, SepLLM can be incorporated into the training process, including training from scratch or post-training, thus reducing disparities between training and inference. We have conducted extensive experiments across various tasks and datasets to demonstrate SepLLMs practical effectiveness. Accelerating LLMs by Compressing One Segment into One Separator"
        },
        {
            "title": "References",
            "content": "Almazrouei, E., Alobeidli, H., Alshamsi, A., Cappelli, A., Cojocaru, R., Debbah, M., Goffinet, E., Hesslow, D., Launay, J., Malartic, Q., Mazzotta, D., Noune, B., Pannier, B., and Penedo, G. The Falcon Series of Open Language Models. Preprint arXiv:2311.16867, 2023. Beltagy, I., Peters, M., Longformer: The Long-Document Transformer. Preprint arXiv:2004.05150, 2020. and Cohan, A. Biderman, S., Schoelkopf, H., Anthony, Q., Bradley, H., OBrien, K., Hallahan, E., Khan, M., Purohit, S., Prashanth, U., Raff, E., Skowron, A., Sutawika, L., and Wal, O. Pythia: Suite for Analyzing Large Language Models Across Training and Scaling. In International Conference on Machine Learning, 2023. Bisk, Y., Zellers, R., Bras, R., Gao, J., and Choi, Y. PIQA: Reasoning about Physical Commonsense in Natural LanIn AAAI conference on artificial intelligence, guage. 2020. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G., Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J., Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M., Gray, S., Chess, B., Clark, J., Berner, C., McCandlish, S., Radford, A., Sutskever, I., and Amodei, D. Language Models are Few-Shot Learners. In Neural Information Processing Systems, 2020. Chen, Y., Qian, S., Tang, H., Lai, X., Liu, Z., Han, S., and Jia, J. LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models. In International Conference on Learning Representations, 2024. Clark, P., Cowhey, I., Etzioni, O., Khot, T., Sabharwal, A., Schoenick, C., and Tafjord, O. Think you have Solved Question Answering? Try ARC, the AI2 Reasoning Challenge. Preprint arXiv:1803.05457, 2018. Cobbe, K., Kosaraju, V., Bavarian, M., Chen, M., Jun, H., Kaiser, L., Plappert, M., Tworek, J., Hilton, J., Nakano, R., et al. Training Verifiers to Solve Math Word Problems. Preprint arXiv:2110.14168, 2021. Devlin, J., Chang, M., Lee, K., and Toutanova, K. BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding. In North American Chapter of the Association for Computational Linguistics, 2019. Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., and Houlsby, N. An Image is Worth 16x16 Words: Transformers for 9 Image Recognition at Scale. In International Conference on Learning Representations, 2020. Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Yang, A., Fan, A., et al. The Llama 3 Herd of Models. Preprint arXiv:2407.21783, 2024. Gao, L., Biderman, S., Black, S., Golding, L., Hoppe, T., Foster, C., Phang, J., He, H., Thite, A., Nabeshima, N., et al. The Pile: An 800GB Dataset of Diverse Text for Language Modeling. Preprint arXiv:2101.00027, 2020. Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J. Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs. In International Conference on Learning Representations, 2024. Geneva, N. and Zabaras, N. Transformers for Modeling Physical Systems. Neural Networks, 2022. He, Z., Feng, G., Luo, S., Yang, K., Wang, L., Xu, J., Zhang, Z., Yang, H., and He, D. Two Stones Hit One Bird: Bilevel Positional Encoding for Better Length Extrapolation. In International Conference on Machine Learning, 2024. Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., and Steinhardt, J. Measuring Massive Multitask Language Understanding. In International Conference on Learning Representations, 2021. Katharopoulos, A., Vyas, A., Pappas, N., and Fleuret, F. Transformers are RNNs: Fast Autoregressive Transformers with Linear Attention. In International Conference on Machine Learning, 2020. Li, J., Shi, H., Jiang, X., Li, Z., Xu, H., and Jia, J. QuickLLaMA: Query-aware Inference Acceleration for Large Language Models. Preprint arXiv:2406.07528, 2024a. Li, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli, A., Ye, H., Cai, T., Lewis, P., and Chen, D. SnapKV: LLM Knows What You are Looking for Before Generation. Preprint arXiv:2404.14469, 2024b. Liu, J., Cui, L., Liu, H., Huang, D., Wang, Y., and Zhang, Y. LogiQA: Challenge Dataset for Machine Reading Comprehension with Logical Reasoning. In International Joint Conferences on Artificial Intelligence, 2021. Merity, S., Xiong, C., Bradbury, J., and Socher, R. Pointer Sentinel Mixture Models. In International Conference on Learning Representations, 2017. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, N., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernandez, R. The LAMBADA dataset: Word prediction Accelerating LLMs by Compressing One Segment into One Separator Yang, D., Han, X., Gao, Y., Hu, Y., Zhang, S., and Zhao, H. PyramidInfer: Pyramid KV Cache Compression for Highthroughput LLM Inference. Preprint arXiv:2405.12532, 2024. Zaheer, M., Guruganesh, G., Dubey, K., Ainslie, J., Alberti, C., Ontanon, S., Pham, P., Ravula, A., Wang, Q., Yang, L., and Ahmed, A. Big Bird: Transformers for Longer Sequences. In Neural Information Processing Systems, 2020. Zhang, J., Zhao, Y., Saleh, M., and Liu, P. PEGASUS: Pretraining with Extracted Gap-sentences for Abstractive Summarization. In International Conference on Machine Learning, 2020. Zhang, Y., Gao, B., Liu, T., Lu, K., Xiong, W., Dong, Y., Chang, B., Hu, J., Xiao, W., et al. PyramidKV: Dynamic KV Cache Compression based on Pyramidal Information Funneling. Preprint arXiv:2406.02069, 2024a. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., et al. H2O: HeavyHitter Oracle for Efficient Generative Inference of Large In Neural Information Processing Language Models. Systems, 2024b. Zhu, Q., Duan, J., Chen, C., Liu, S., Li, X., Feng, G., Lv, X., Cao, H., Xiao, C., Zhang, X., et al. SampleAttention: Near-Lossless Acceleration of Long Context LLM Inference with Adaptive Structured Sparse Attention. Preprint arXiv:2406.15486, 2024. requiring broad discourse context . In Association for Computational Linguistics, 2016. PyTorch. Flexattention: The flexibility of pytorch with the performance of flashattention, 2024. URL https: //pytorch.org/blog/flexattention/. Rae, J., Potapenko, A., Jayakumar, S., Hillier, C., and Lillicrap, T. Compressive Transformers for Long-Range In International Conference on Sequence Modelling. Learning Representations, 2020. Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S., Matena, M., Zhou, Y., Li, W., and Liu, P. Exploring the Limits of Transfer Learning with Unified Text-to-Text Transformer. Journal of Machine Learning Research, 2020. Schlag, I., Irie, K., and Schmidhuber, J. Linear transformIn Internaers are secretly fast weight programmers. tional Conference on Machine Learning, pp. 93559366. PMLR, 2021. Shi, H., Gao, J., Ren, X., Xu, H., Liang, X., Li, Z., and Kwok, J. SparseBERT: Rethinking the Importance Analysis in Self-attention. In International Conference on Machine Learning, 2021. Su, J., Lu, Y., Pan, S., Murtadha, A., Wen, B., and Liu, Y. Roformer: Enhanced transformer with rotary position embedding, 2023. URL https://arxiv.org/abs/ 2104.09864. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A., Kaiser, L., and Polosukhin, I. Attention is All you Need. In Neural Information Processing Systems, 2017. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q., Zhou, D., et al. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models . In Neural Information Processing Systems, 2022. Welbl, J., Liu, N., and Gardner, M. Crowdsourcing Multiple Choice Science Questions. In Workshop on Noisy Usergenerated Text, 2017. Xiao, C., Zhang, P., Han, X., Xiao, G., Lin, Y., Zhang, Z., Liu, Z., and Sun, M. InfLLM: Training-Free LongContext Extrapolation for LLMs with an Efficient Context In Neural Information Processing Systems, Memory. 2024a. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient Streaming Language Models with Attention Sinks. In International Conference on Learning Representations, 2024b. Accelerating LLMs by Compressing One Segment into One Separator"
        },
        {
            "title": "Appendix",
            "content": "D. The Performance of Different Models A. Visualization of Attention Scores We take Llama-3-8B-instruct (Dubey et al., 2024) as the model for visualization. The input sentence is Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May? Answer Natalia sold 48 clips in April. In May, she sold half as many clips as she did in April, so she sold 48/2=24 clips in May. Therefore, Natalia sold total of 48+24=72 clips in April and May. The answer is 72 . and the visualization of different attention maps are shown in Figure 12,13,14. B. The Evolution of KV Caches To explain the dynamic design for streaming setting better, we illustrate the detailed evolution of KV caches in Figure 7. As can be seen, and Sizerun are both periodic functions after m0 tokens. And the average KV caches used is much less than the maximum capacity c. Different Architectures. Concerning different decoderonly models, we test our SepLLM on Llama3 and Pythia backbones on PG19 test dataset (generating 64K tokens). The results are shown in Table 10 (For SepLLM, = 4, = 64, = 256, = 800). Backbone Arch. Vanilla r.KV ppl 64K 32K 1037. Pythia-6.9B StrmLLM 800 SepLLM 800 Vanilla 800 562 15.9 15.8 64K 32K 1090.8 Llama-3-8B StrmLLM 800 SepLLM 800 800 37.9 33.4 time(s) 4160.7 1522.6 1456.0 3380.6 1096.0 1049.7 Table 10. The comparison of SepLLM adapted to different architectures. From above table, it can be seen that for models with similar size, setting similar KV retention rate can yield similarly good performance. Different Scales. To learn the generalization to different scales, we test our SepLLM on Pythia-6.9B and Pythia-12B backbones on PG19 test dataset (generating 20K tokens). The results are illustrated in Table 11. Backbone 4 Pythia-6.9B 4 4 4 Pythia-12B 64 64 64 64 256 800 928 256 800 1024 1280 800 r.KV ppl 13.0 562 12.7 946 12.7 1138 12.1 time(s) 445.0 450.4 454.4 577.0 Figure 7. The evolution of KV caches in the streaming setting. C. Training Acceleration We list the detailed wall-clock time per iteration and throughput in Table 9 and the speed-up ratio is 1.53. time per iteration (ms) samples / second Vanilla (Full Attention) 2524.45 405. SepLLM (n=64) 1648.11 622.31 SepLLM (n=128) 1653.11 620.3 Table 9. The details about training acceleration. Table 11. The comparison of SepLLM adapted to Pythia (Biderman et al., 2023) with different scales. Compared to Pythia-12B, the smaller model Pythia-6.9B will have higher perplexity if the capacity of the KV caches is the same (c=800). Therefore, it is necessary to increase to achieve lower perplexity close to that of Pythia-12B (c=800). On the other hand, the larger models will have lower perplexity but require longer inference time. Larger Model Falcon-40B (Almazrouei et al., 2023) is an another larger architecture we adapted for evaluation the scalability of our proposed SepLLM. The experiment results are shown in Table 12, where we set a=4,s=64,w=512/720, c=800/1024 for SepLLM, and a=4, c=800/1024 for StreamingLLM. And the conclusions are similar with previous parts. We can see that the speed of SepLLM(n=128) and SepLLM(n=64) is almost the same, which is attributed to the excellent parallelization capability of Sep-Attention module. Base or Instruct. In general, whether it is the base model or the instruction-tuned model, we can condense the segment information into the corresponding Key-Value pairs 11 Accelerating LLMs by Compressing One Segment into One Separator"
        },
        {
            "title": "Length Methods",
            "content": "c 20K 64K StrmLLM 1024 StrmLLM 800 SepLLM 1024 SepLLM 800 StrmLLM 1024 StrmLLM 800 SepLLM 1024 SepLLM 800 r.KV 1024 800 906 690 1024 800 906 690 ppl 8.98 9.02 8.92 9.00 11.01 11.09 10.96 11. time (s) 1512.88 1430.59 1440.89 1368.07 4844.79 4623.90 4619.63 4414.72 Table 12. The comparison of SepLLM adapted to Falcon-40B (Almazrouei et al., 2023). of the separator tokens. To illustrate, we fine-tune Llama3-8B-instruct and Llama-3-8B-base models (Dubey et al., 2024) on LongAlpaca dataset (Chen et al., 2024) for only 200 and 500 steps, respectively. After fine-tuning process, we take GSM8K-CoT (Cobbe et al., 2021) as the benchmark for reasoning ability evaluation. We find that both the base model and the instruction-tuned model exhibit excellent performance (matching or surpassing the vanilla model with original attention mechanism). The only difference is that for Llama-3-8B-base, we need to fine-tune for more steps to achieve such performance. In comparison, Llama3-8B-instruct requires fewer fine-tuning steps. Even in training-free scenario, Llama-3-8B-instruct demonstrates decent performance. This indicates that the embeddings of Llama-3-8B-instruct align with the distribution required by the SepLLM architecture better. Training from scratch During our training process, we enforce that each current token can only see its preceding neighboring tokens, separator tokens, and initial tokens such that the model is compelled to condense the information of each segment into the Key-Value pairs corresponding to the separators through the self-attention mechanism. Therefore, the hidden embedding of separator is functionally similar to the state space of an RNN, even though the computation method differs as we utilize the attention mechanism. Furthermore, since the length of each segment is typically finite, short and balanced (He et al., 2024), the compressed information is efficient and less likely to be forgotten. Training-free First, these separators (commas, periods) are extremely high-frequency tokens (both in the pretraining text and in the text generated by the language model). Thus, during the pre-training process, these separators are the most common context for all other tokens in the vocabulary. Consequently, their embeddings exhibit greater similarity, resulting in larger attention values when multiplied with other tokens. Furthermore, from logical standpoint, it is evident that separators need to be generated by the language model very frequently. Therefore, their attention values with respect to any other token cannot be too small; otherwise, they would not be generated frequently by the language model. Tokens with larger mutual attention values will incorporate more information from other tokens. Hence, from semantic perspective, generating separator serves as summarization of the current segment, naturally dividing and summarizing the contextual semantics. Backbone Algorithm Base Vanilla SepLLM ft. Vanilla SepLLM ft. GSM8K-CoT r.KV (%) 54.44 55.95 77.26 77.63 100 47.36 100 47.36 Instruct Table 13. The comparison of SepLLM adapted to Llama-38B (Dubey et al., 2024) with base or instruct versions. E. Needle In Haystack To evaluate the long-context ability of our proposed SepLLM, we take Needle In Haystack as the benchmark and compare the performance of SepLLM and StreamingLLM. The results are shown in Figure 8,9,10,11 and SepLLM can achieve more scores compared with StreamingLLM. F. Discussions on Separators We provide the following assumptions and discussions on why keeping the KV caches corresponding to separators can maintain the performance of the original model. 12 Accelerating LLMs by Compressing One Segment into One Separator Figure 8. Needle In Haystack test results for streamingLLM (n=64) based on Pythia-160M-deduped. Figure 9. Needle In Haystack test results for our SepLLM(n=64, H/T) based on Pythia-160M-deduped. Figure 10. Needle In Haystack test results for our SepLLM(n=2048; first/last 2 layers (4 layers in total): full attention) based on Llama-3-8B-instruct. 4 initial tokens are kept. Figure 11. Needle In Haystack test results for our SepLLM(n=2048; first/last 2 layers (4 layers in total): full attention) based on Llama-3-8B-instruct. 32 initial tokens are kept. 13 Accelerating LLMs by Compressing One Segment into One Separator Figure 12. An example of attention map in Llama-3-8B-Instruct (Layer 0 and Head 0). 14 Accelerating LLMs by Compressing One Segment into One Separator Figure 13. An example of attention map in Llama-3-8B-Instruct (Layer 1 and Head 0). 15 Accelerating LLMs by Compressing One Segment into One Separator Figure 14. An example of attention map in Llama-3-8B-Instruct (Layer 2 and Head 0)."
        }
    ],
    "affiliations": [
        "Center of Excellence for Generative AI, KAUST",
        "Huawei Noahs Ark Lab",
        "Max Planck Institute for Intelligent Systems, Tubingen",
        "The University of Hong Kong"
    ]
}