{
    "paper_title": "DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers",
    "authors": [
        "Zitong Wang",
        "Hang Zhao",
        "Qianyu Zhou",
        "Xuequan Lu",
        "Xiangtai Li",
        "Yiren Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Diffusion models have recently motivated great success in many generation tasks like object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we delve into a novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, a diffusion Transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: https://github.com/Wangzt1121/DiffDecompose."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 2 1 4 5 1 2 . 5 0 5 2 : r DiffDecompose: Layer-Wise Decomposition of Alpha-Composited Images via Diffusion Transformers Zitong Wang1 Hang Zhao1 Qianyu Zhou1 Xuequan Lu2 Xiangtai Li3 Yiren Song"
        },
        {
            "title": "2 University of Western Australia, Australia\n4 National University of Singapore, Singapore",
            "content": "Figure 1: We propose novel generative task, Layer-Wise Decomposition of Alpha-Composited Images, to recover constituent layers from single overlapped images under the condition of semitransparent or transparent layer non-linear occlusion. We introduce the AlphaBlend dataset, the first large-scale dataset for transparent and semi-transparent layer decomposition to support six real-world subtasks. (a) shows generation results on alpha layer removal (III), semi-transparent and transparent layer separation (IIIIV), and complex non-linear alpha-blend decomposition (VVI). (b) highlights the datasets broad coverage across categories e.g., flare, fog, glassware, X-ray contraband."
        },
        {
            "title": "Abstract",
            "content": "Diffusion models have recently achieved great success in various generation tasks, such as object removal. Nevertheless, existing image decomposition methods struggle to disentangle semi-transparent or transparent layer occlusions due to mask prior dependencies, static object assumptions, and the lack of datasets. In this paper, we explore novel task: Layer-Wise Decomposition of Alpha-Composited Images, aiming to recover constituent layers from single overlapped images under the condition of semi-transparent/transparent alpha layer non-linear occlusion. To address challenges in layer ambiguity, generalization, and data scarcity, we first introduce AlphaBlend, the first large-scale and high-quality dataset for transparent and semi-transparent layer decomposition, supporting six real-world subtasks (e.g., translucent flare removal, semi-transparent cell decomposition, glassware decomposition). Building on this dataset, we present DiffDecompose, diffusion transformer-based framework that learns the posterior over possible layer decompositions conditioned on the input image, semantic prompts, and blending type. Rather than regressing alpha mattes directly, DiffDecompose performs In-Context Decomposition, enabling the model to predict one or multiple layers without per-layer supervision, and introduces Layer Position Encoding Cloning to maintain pixel-level correspondence across layers. Extensive experiments on the proposed Equal contribution. Corresponding author. Preprint. Under review. AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose. The code and dataset will be available upon paper acceptance. Our code will be available at: https://github.com/Wangzt1121/DiffDecompose."
        },
        {
            "title": "Introduction",
            "content": "With the development of diffusion models, image-to-image [42, 16, 62, 65] and text-to-image [6, 18, 10] generation methods have been extensively studied and have wide applications in image inpainting [12, 49, 61, 59], video generation [5, 58, 76, 60], and image segmentation [36, 2, 22] to obtain high-quality images and user-friendly interfaces. Most existing approaches rely on mask-based constraints to guide the model toward modifying or preserving specific image regions, or assume temporal consistency by treating moving object information as static to infer future or missing frames. Despite their success, these methods face two major limitations when applied to more complex and realistic scenarios. Firstly, the assumption of known separation regions or static objects is too simple, making it difficult to apply to more complex scenarios. Secondly, the noise in the diffusion models forward phase may destroy the images original information, reducing the image contexts usability. Recently, new type of method, termed layer-wise decomposition [53, 64, 71, 30, 13, 3, 21], has gradually entered the field of computer vision and become potential method to solve the problem of these generative models. The core objective of layer-wise decomposition is to decompose single composited image into its constituent layers, each containing the foreground object, its associated alpha matte (transparency), and potential depth ordering. Such decomposition enables more granular control over individual layers, allowing for other tasks such as image inpainting [44], matting [35], scene understanding [66], and content creation [52]. Nonetheless, these decomposing methods will fail to recover the accurate background when the foreground itself is layer-level semi-transparent or transparent, since they still rely on region-based assumptions. Most importantly, current methods, such as image inpainting [69, 39, 17, 26] and layer decomposition [71, 64], cannot generalize to the real world to tackle arbitrary alpha-blended images with diverse contents and complex layer structures. In this paper, our goal is to delve into novel task that focuses on semi-transparent or transparent layer-wise decomposition. As shown in Figure 1 (a), this novel task does not require any mask-based information for background assumption and no clear visual contrast to provide depth information, and under such conditions, natural question is how to decompose these transparent or semitransparent layers from the background ? Naturally, single composited image often corresponds to multiple plausible decompositions due to the complex entanglement of color and transparency across layers, leading to additional difficulties in reconstructing the original scene. Therefore, this proposed task faced three main challenges not addressed in prior works: (1) Layer ambiguity and coupling of color and transparency: foreground and background occupy the same visual plane and lack the separability via depth or edge contrast, thereby introducing significant layer ambiguity. Furthermore, critical foreground details may be partially embedded in the alpha channel, rendering them inaccessible through RGB information alone. (2) Generalization in real-world scenarios: The above methods directly remove the pixel values of specific area and use the network to generate new pixel values based on background reasoning. Such manner largely overlooks the exploration of pixel-level correlations between the occluded area and the background, leading to the failure of generalization in various realistic scenarios. (3) Lack of large-scale dataset: significant challenge in layer decomposition is the lack of large-scale and high-quality datasets. Current datasets [53, 64] are often generated from AI, and the final overlapped images tend to have pixel-level difference from the initial foreground and background, resulting in the failure of layer-wise decomposition. Motivated by the above fact, we propose the first large-scale, high-quality dataset, namely AlphaBlend, specifically designed for the decomposition of semi-transparent and transparent layers, where class of scenarios frequently encountered in real-world applications but not previously presented in existing benchmarks. As shown in Figure 1 (b), AlphaBlend has two characteristics: Firstly, it is the first dataset to support six different semi-transparent/transparent decomposition subtasks, and comprehensively constructed in modeling nonlinear compositional behavior, layer entanglement, and alpha-driven visual ambiguity. Secondly, this dataset contains various styles and content, such as security, glass replacement, cell separation, etc., and each domain contains around 5000-10000 training images and 300-500 test images, which is closer to the real-world scenarios. The dataset will be released to the public to support further exploration of this new task upon paper acceptance. 2 Based on on the novel task and the AlphaBlend dataset, we propose diffusion Transformerbased framework DiffDecompose to probabilistically infer multiple plausible layer decompositions conditioned on context, which has two novel points: (1) Rather than explicitly regressing alpha mattes, we reformulate the task as learning the posterior distribution over compositional layers, conditioned on the observed image, semantic prompts, and blending type. (2) We design the InContext Decomposition to enable the model to effectively predict single-layer or multi-layer results under given alpha-composited image conditions across various alpha compositional contexts without explicit supervision for each individual layer, and devise Layer Position Encoding Cloning (LPEC) to preserve the pixel-level correspondence. Our main contributions are three-fold: We present novel generative task, namely Layer-Wise Decomposition of Alpha-Composited Images for the semi-transparent/transparent scenarios, and reformulate this task from novel perspective as probabilistic problem of generating plausible layer decompositions. We introduce the AlphaBlend dataset, the first large-scale, high-quality dataset that supports six semi-transparent/transparent layer-wise decomposition subtasks, i.e., translucent flare removal, translucent occlusion removal, semi-translucent watermark removal, transparent glassware decomposition, semi-transparent cell decomposition, X-ray contraband decomposition. We propose diffusion Transformer-based framework, namely DiffDecompose, and we are the first, to our best knowledge, to leverage the DiT for in-context decomposition without any implicit supervision. Concretely, we devise the Layer Position Encoding Cloning to realize the pixel-level alignment between different layers. Extensive experiments on the proposed AlphaBlend dataset and public LOGO dataset verify the effectiveness of DiffDecompose."
        },
        {
            "title": "2 Related Work",
            "content": "Diffusion Model. Diffusion Models (DMs) [46, 43] have significantly advanced image generation and editing, surpassing GANs [14, 34, 38, 34, 33, 55] in tasks such as artistic synthesis [9], instruction-guided transformation [7], and controllable editing [25, 68]. Recent extensions have explored applications in 3D modeling [31] and scene-consistent generation [37], showcasing the broad generative capacity of diffusion models. Despite these advances, diffusion models remain an open and ill-posed challenge, particularly in scenarios involving transparency, semi-transparency, and nonlinear blending. In this work, we present novel generative task, namely Layer-Wise Decomposition of Alpha-Composited Images, and propose diffusion-based framework DiffDecompose, to decompose complex semi-transparent/transparent layer-level composited images into plausible foreground and background layers. To support this novel task, we construct first, large-scale and high-quality dataset AlphaBlend, which mainly contains layer-level semi-transparent or transparent images. Controllable Generation. Recent advances in controllable diffusion models [72, 67, 39, 40, 60] have enabled powerful image editing capabilities, particularly in inpainting. Existing approaches are typically categorized into text-based and mask-based methods. Text-based methods [70, 77, 61] rely on prompt engineering to achieve multi-task unification, but their performance is highly sensitive to prompt accuracy, often resulting in degraded outputs when instructions are imprecise [63]. Maskbased methods [69, 39, 17] focus on preserving background integrity via accurate region masks, yet they introduce noise to masked areas and perform iterative denoising, which weakens the conditioning signal and limits performance in complex blending scenarios. To this end, we extend noise-free conditioning paradigm inspired by OminiControl [50], from novel point to reformulate the object occlusion task [70, 17, 49] or layer decomposition task [53, 64] as probabilistic generation task, where the goal is to model the posterior distribution over plausible layer decompositions conditioned on the observed composite image. Under this definition, not only can we solve the common objectlevel or channel-level occlusion task, but also extend this idea to our proposed layer-level semitransparent/transparent task. In-context Learning. In-context learning, originally developed in large language models [8, 74], has recently been extended to vision tasks such as object editing and region manipulation [74, 54, 47, 4, 73, 11, 48, 20]. While prior works leverage contextual cues for object insertion or style transfer, none have addressed layer-wise decomposition, particularly in semi-transparent or transparent settings. In this paper, we introduce In-Context Decomposition (ICD), which enables the model to infer 3 multiple compositional layers based on contextual prompts without explicit supervision. Furthermore, we propose Layer Position Encoding Cloning (LPEC) strategy to preserve spatial alignment and reinforce information consistency between the decomposed layers and the original image."
        },
        {
            "title": "3.1 AlphaBlend Dataset Construction",
            "content": "To support the study of image compositing under complex transparency conditions, we introduced the AlphaBlend Dataset, which contains six sub-datasets for six different subtasks, including: Xray contraband decomposition, Transparent glassware decomposition, Semi-transparent cell decomposition, Semi-transparent watermark removal, Translucent occlusion removal, and Translucent flare removal. Each sub-dataset contains around 5000-10000 training images and 300500 test images. Each dataset simulates the visual phenomena of transparent and semi-transparent objects in the real world, such as occlusion, feature overlap, glare reflection, etc., by performing taskspecific mixing of foreground RGBA image with an alpha channel and natural or structured RGB background. The image composition process follows the general formulation: = AαBlendB, where Aα denotes the foreground image with an associated alpha transparency map, is the background image, and Blend is task-specific blending operator (e.g. alpha blending or screen mode). X-ray Contraband Decomposition sub-dataset. To emulate the physical property of X-ray in synthetic data, we formulate its process as: = (1 α) + α (cid:0) Aα 255 255(cid:1), where α controls foreground transparency. This nonlinear blending mimics the cumulative darkening seen in overlapping X-ray materials. 255 Transparent Glassware Decomposition sub-dataset. To simulate transparent and refractive materials like glassware, we formulate its process as: = Aα(x, y) B(x, y), when the alpha channel of B(x, y) is lower than 0.5 and = 1 (1 Aα(x, y)) (1 B)(x, y), when B(x, y) is higher than 0.5 during foreground-background composition. Unlike X-rays, such condition should reflect real-world optical properties of transparent glass, allowing highlights to be intensified while preserving subtle background attenuation, thus producing visually realistic rendering of refractive materials. Semi-transparent Watermark Removal sub-dataset. To simulate real-world visual effects of semitransparent overlays such as watermarks, logos, or textual elements, we formulate the compositing process as: = (1 α(x, y)) Aα(x, y) + α(x, y) B(x, y). linear alpha blending approach effectively reflects the smooth visual interference watermarks imposed on natural scenes while preserving contextual consistency for downstream layer decomposition. Semi-transparent Cell Decomposition sub-dataset. For microscopic imaging, we simulate overlapping semi-transparent cell structures using simple additive model: = Aα(x, y) + B(x, y). Unlike X-ray or glass-like materials that rely on light attenuation or refractive blending, this additive formulation captures the cumulative intensity build-up commonly observed in microscopy. Translucent Occlusion Removal sub-dataset. To simulate natural environmental occlusions such as fogged or rainy windows, this dataset overlays translucent layers onto outdoor or indoor background scenes. In contrast to object-level foreground overlays, this process is full-image, diffuse blending, where translucent elements span across the entire scene without distinct semantic boundaries. Translucent Flare Removal sub-dataset. The foreground lens glare or light reflection images are also synthesized with natural background images by using full-image diffuse blending. Overall, the translucent occlusion removal and the translucent glare removal datasets can be used as benchmark for assessing generalizable, mask-free layer separation under ambient environmental interference. Collectively, these datasets provide comprehensive benchmark for evaluating models in realistic scenes. More details of our proposed AlphaBlend dataset can be found in the appendix."
        },
        {
            "title": "3.2 Problem Formulation and Overview",
            "content": "Problem Formulation. As shown in blue background of Figure 2, to obtain the background from the observed image RHW 3, the current method [69, 39, 17] regards this problem as an image edition task or common mask-based layer-wise decomposition task, which can be written as: pθ(y z, m) = (cid:90) pθ(y z, m)pθ (z0 zt, m) dz0, (1) Figure 2: The comparison of conventional inpainting methods with our proposed DiffDecompose. The conventional inpainting approach (blue background) relies on predefined object masks and predicts missing regions, often causing semantic errors in transparent scenes. In contrast, DiffDecompose (green background) conditions on the full composited image and jointly predicts foreground and background via layer-level decomposition. This formulation removes the need for explicit masks and enables more accurate separation in the presence of transparency and complex blending. where represents the region that should be foreground split from the background, and the whole process can be viewed as the prediction of region m. The goal is to Unfortunately, since the semitransparent/transparent object will refract the background information, including translucent object perception (e.g., glass or plastic), biological imaging (e.g., semi-transparent cells), and X-ray security scans, the pixel-level values of occluded region are no longer the simple coverage or replace, it is more of nonlinear blend of multiple layers, which is totally different from existing tasks. Directly deleting the occlusion pixel and utilizing the background information to predict new pixel is unreasonable and will result in false pixel-level restoration. Considering such common condition, we define it as novel task, namely Layer-Wise Decomposition of Alpha-Composited Images, where the problem is decomposing an observed image that results from the layered composition of semi-transparent/transparent foreground and background. Formally, let RHW 4 denote the foreground RGBA image, and RHW 3 denote the RGB background image. These are combined through (possibly nonlinear) composition function G, resulting in an observed image RHW 3: = G(x, y). The function may range from simple alpha blending to more complex operators such as additive, multiply, screen, or overlay composition. In practice, the exact formulation of may be unknown or variable across domains. Thus, the decomposing background and foreground can be defined as layer-wise decomposition process (the green background in Figure 2). Given the observed image z, our goal is to recover plausible pair of images (x, y) such that the composition function G(x, y) z. This inverse problem is highly ill-posed, particularly when is nonlinear and information is entangled across layers. To address this challenge, we propose to train conditional diffusion model that learns the joint posterior distribution over foreground and background conditioned on the composed image, which can be defined as: pθ(x, z, τ ) = (cid:90) pθ(x, z, τ )pθ (z0 z, τ ) dz0. (2) During the training, we assume access to dataset of triplets (x, y, z), where = G(x, y). The model is trained to sample decomposed pairs that are both semantically plausible and compositionally consistent under G. Framework Overview. As shown in Figure 3, the overview architecture of DiffDecompose can be divided into two steps: (1) Utilize the frozen VAE [27] to respectively encode semitransparent/transparent foreground, common RGB background, and condition alpha-blend image, to obtain their latent spatial-level features. In the meantime, the prompts are fed into the frozen T5XXL [41] to extract the text-level features. (2) The process of decomposition is realized by In-context Decomposition (ICD) by concatenating the clean condition token and noise tokens, and utilizing the bidirectional attention to achieve conditional generation. During the process of ICD, we also devise Layer Position Encoding Clone strategy to achieve consistency between the specific layer and the original layer information, and utilize the MMA for the interaction between different modalities. 5 Figure 3: The DiffDecompose framework comprises two steps: (1) VAE encodes the source image into condition token and concatenates it with noisy latent token, controlling the generation of layer decomposition. (2) In-Context Decomposition constructs an image-conditioned base model to decompose multiple layers. Among them, the LPEC clones the position encoding to ensure the alignment between different layers, and MMAttention are introduced to process the multi-modulation features. 3.3 In-Context Decomposition Since the disentanglement of alpha-composited images is non-linear, directly learning the alpha maps is too difficult for decomposition. Thus, we develop an in-context decomposition to process layer decomposition as context-aware spatial separation problem, where the model leverages both visual latent representations fx, fy, and fz, and the textual latent representations ft to infer the spatial organization and semantic consistency of foreground and background components. To successfully realize this process, we develop the Layer Position Encoding Clone (LPEC) and introduce Multimodality Attention (MMA) to achieve the process of in-context decomposition. Layer Position Encoding Cloning. To enforce spatial coherence across compositional layers and enhance the disentanglement of foreground and background during generation, we propose the LPEC mechanism. Specifically, we compute the positional encoding PEz from the composited image z, and inject it into the token representations of the background image and composited observation z: cy = cy + PEz, cz = cz + PEz. (3) = c(i,j) c(i,j) c(i,j) c(i,j) This operation ensures that both the background and the composite tokens share unified coordinate frame, i.e., (i, j), preserves relative positional structure during attention computation. In contrast, the foreground tokens cx are excluded from this positional cloning, i.e.,cx = cx. This separation avoids spatial entanglement between layers and prevents unwanted feature blending. In transparent or semi-transparent compositions, foreground and background often overlap spatially but differ in semantic meaning and transparency. Maintaining disjoint positional encodings: PEx PEz, the model prevents spatial interference between foreground and background layers. By maintaining distinct positional space for the foreground, the model is encouraged to preserve its semantic and spatial independence, thereby enhancing the fidelity of layer disentanglement in semi-transparent or fully transparent scenes. Multi-Modality Attention. After applying LPEC, the latent tokens cx, cy, and cz are then concatenated along the sequence dimension to perform joint attention. The Multi-modal attention mechanisms [18] are utilized to provide conditional information for the denoising of the alpha-composed image. Furthermore, by maintaining in noise-free state, we ensure the retention of high-frequency textures and fine structural details from the original image, thereby preventing degradation during iterative denoising. This process can be written as: MMA ([cz; cx; cy; cT ]) = softmax V, (4) (cid:19) (cid:18) QK where Q, K, are the query, key, and value projections of the concatenated sequence [cz; cx; cy; cT ], is the feature dimension, and cT represents the textual condition tokens extracted from task6 specific prompt. In particular, the description consists of the foreground image x, the background image y, and their composited observation z, (e.g., Three sub-images. <image-1>: transparent glass, <image-2>: this is dinning room with some chairs, <image-3>: The overlapped of <image-1> and <image-2>.), which guide the semantic understanding of the inter-layer relationships."
        },
        {
            "title": "4.1 Experimental Settings",
            "content": "Implementation Details. We initialize our model using the pre-trained parameters of the Flux.1 [28] architecture and fine-tune it across six representative subtasks. The model is trained using LoRA [18] rank of 128, batch size of 1, and learning rate of 104 on single H20 GPU for 30,000 steps. For the subtasks of translucent flare removal, translucent occlusion removal, semi-transparent watermark and removal, all input images are resized to 512512. For the subtasks of transparent glassware decomposition, semi-transparent cell decomposition, and X-ray contraband decomposition, input images are resized to 5121024, and the training is conducted for 50,000 steps under the same hyperparameter configuration unless otherwise specified. Baseline. We compare our method with the state-of-the-art inpainting methods, which include the Inpainting Anything [69], diffusion-based models SDXL-Inpaint [39], Flux-control-Inpaint [51], PowerPaint [77] and ClipAway [17]. To ensure fair comparison and reduce inconsistencies or boundary artifacts, we apply mask dilation for five iterations using 55 kernel prior to inpainting. Benchmark. To provide rigorous and comprehensive evaluation, we evaluate our DiffDecompose and baselines on two datasets: our proposed AlphaBlend dataset and the public LOGO dataset [15]. For our proposed AlphaBlend dataset (described in Sec. 3.1), DiffDecompose infers its six scenarios, including: X-ray contraband decomposition, transparent glassware decomposition, semitransparent cell decomposition, semi-transparent watermark removal, translucent occlusion removal, and translucent flare removal, with 300-500 test images. For LOGO evaluation, we utilize its three test datasets, LOGO-H, LOGO-L, and LOGO-G."
        },
        {
            "title": "4.2 Main Results",
            "content": "Figure 4 showcases the performance of DiffDecompose across six presented subtasks. It can effectively disentangle complex compositional layers, ranging from window raindrops and lens flares to watermarks, glassware, X-ray contraband, and overlapping cells without reliance on explicit masks. It accurately reconstructs background content while preserving fine foreground structures under diverse transparency patterns and blending behaviors. These results demonstrate the methods strong generalization to real-world transparent and semi-transparent layer decomposition scenarios. Figure 4: Our DiffDecompose shows impressive layer-level decomposition results of the image. It can solve the layer-level decomposition (i.e., Subtask II and Subtask III), the nonlinear alpha-blend layer removal and decomposition (i.e., Subtask and Subtask VI), and the semi-transparent/transparent object-level decomposition (i.e., Subtask IV and Subtask VI), demonstrating its generalization and application in various scenarios. 7 Qualitative Evaluation. Figure 5 compares DiffDecompose with recent image inpainting-based SOTA methods across various semi-transparent layer removal tasks. While traditional inpainting approaches work for small, localized object-level occlusions, they struggle when foreground regions expand or exhibit transparency, often resulting in blurred textures or semantic inconsistencies. Methods like SDXL, ClipAway, and PowerPaint generate visually plausible but structurally inaccurate content, particularly in complex cases like transparent glass, X-ray contraband, or dense cell overlaps. In contrast, our DiffDecompose leverages layer-wise decomposition to disentangle foreground and background more accurately, preserving high-frequency textures and semantic continuity. Our method consistently outperforms other baseline methods in range of challenging alpha-synthesized image scenarios, accurately recovering fine structural details, especially in watermark removal and transparent object decomposition. Furthermore, we found that these inpainting methods fail to remove the layer-level transparent occlusion, the details can be seen in Appendix. Figure 5: Qualitative comparisons between our method and other methods. Quantitative Evaluation. Table 1 quantitatively demonstrates the superiority of our DiffDecompose framework across both synthetic and public datasets, outperforming the second-best methods by an average margin of 36.3% in RMSE, +1.2% in SSIM, and 52.8% in LPIPS. In complex real-world scenarios such as X-ray contraband removal and translucent window occlusions, our model achieves up to 4 lower RMSE and 3 better LPIPS than inpainting-based baselines, which struggle with color entanglement and global transparency. In particular, for challenging global occlusions like \"Rain\" and \"Light\", our method significantly surpasses all baselines, while others fail without explicit masks. Although our FID in LOGO-L is slightly higher than Inpaint Anything [69] (24.20 vs 23.71), which is attributed to the highly structured and mask-friendly layout of LOGO-L, providing strong mask priors. Importantly, our model is mask-free and shows consistently better structural accuracy across all LOGO datasets (e.g., RMSE 1.6, SSIM 0.002, LPIPS 0.011 over Inpaint Anything on LOGO-H). These results confirm that DiffDecompose not only generalizes across our six diverse datasets but also outperforms existing methods on public benchmarks, without requiring additional spatial priors. 8 Table 1: Comparison results with previous methods on LOGO and AlphaBlend dataset. Dataset Models PowerPoint [77] Flux-ControlNet [51] SDXL-inpainting [39] ClipAaway [17] Inpaint Anything [69] Ours Dataset LOGO-H LOGO-L LOGO-G RMSE SSIM LPIPS FID RMSE SSIM LPIPS FID RMSE SSIM LPIPS FID 10.0330 11.7016 8.1936 11.3593 4.8252 3.2321 Semi-transparent Watermark Removal Transparent Glassware Decomposition 0.0360 0.0508 0.0302 0.0463 0.0143 0.0092 0.9726 0.9640 0.9781 0.9690 0.9849 0.9923 6.4480 8.7095 4.9184 7.5353 3.0461 2.1655 0.9870 0.9800 0.9899 0.9851 0.9931 0.9962 0.0212 0.0351 0.0172 0.0275 0.0072 0. 30.49 61.22 30.57 32.40 23.71 24.20 34.64 64.67 35.24 36.55 27.50 27.25 32.42 8.1397 37.28 14.5943 32.37 6.3711 33.21 9.3021 25.90 3.8823 24.78 2.7940 Semi-transparent Cell Decomposition 0.0272 0.0705 0.0221 0.0356 0.0101 0.0063 0.9811 0.9670 0.9848 0.9785 0.9898 0.9945 Models RMSE SSIM LPIPS FID RMSE SSIM LPIPS PowerPoint [77] Flux-ControlNet [51] SDXL-inpainting [39] ClipAaway [17] Inpaint Anything [69] Ours Dataset 31.4009 35.9988 21.7313 35.3515 11.5273 2.9976 0.8808 0.8430 0.9163 0.8641 0.9527 0.9894 59.70 82.21 50.89 60.80 34.58 10.89 X-ray Contraband Decomposition 0.0571 0.1848 0.0923 0.1561 0.0484 0.0050 10.6886 17.3372 14.4375 13.9394 8.2335 7.9938 0.9727 0.9551 0.9596 0.9649 0.9775 0. 0.0236 0.0451 0.0421 0.0333 0.0189 0.0134 FID 33.35 68.41 73.36 47.64 31.30 25.55 RMSE SSIM LPIPS 9.55 12.08 9.70 7.58 3.3028 2. 0.8504 0.9701 0.9764 0.9590 0.9862 0.9901 0.1526 0.0407 0.0274 0.0180 0.0093 0.0055 FID 36.46 89.47 79.18 49.79 36.42 27.01 Translucent Flare Removal Translucent Occlusion Removal Models RMSE SSIM LPIPS PowerPoint [77] Flux-ControlNet [51] SDXL-inpainting [39] ClipAaway [17] Inpaint Anything [69] Ours 7.1830 7.6561 7.0060 9.9123 4.8260 3.8903 0.9826 0.9808 0.9845 0.9789 0.9856 0. 0.0134 0.1550 0.0125 0.0200 0.0080 0.0049 FID 39.90 45.49 39.38 55.92 38.13 23.73 RMSE SSIM LPIPS FID RMSE SSIM LPIPS FID 90.4829 96.0870 63.1518 97.5711 83.3769 16.4373 0.1319 0.2158 0.3539 0.2416 0.3058 0.8189 0.8150 0.8016 0.7269 0.8362 0.9040 0. 404.70 253.40 229.96 255.26 462.58 23.15 89.1031 87.8764 64.6848 96.2468 83.2172 18.9561 0.1336 0.1991 0.3296 0.2344 0.3090 0.7653 0.8092 0.8126 0.7351 0.8427 0.9046 0.1445 421.63 239.67 209.20 235.00 477.38 43."
        },
        {
            "title": "4.3 Ablation study and Analysis",
            "content": "Effectiveness of LPEC. As shown in Table 2, it can be seen that although LPEC is simple, it is crucial for different layer decomposition tasks. For the subtask of translucent occlusion removal at the layerlevel, after studying the LPEC, it can improve 2.3/0.0242/0.0227 in terms of RMSE/SSIM/LPIPS. Apart from the layer-level decomposition, the object-level decomposition can also be viewed as the process of layer-level decomposition, and even more effectively. For the object-level semi-transparent Security, which is the most difficult for color distanglement, the LPEC can effectively separate the goods from their baggage. Additionally, the object-level transparent task of glass can improve the ICD results by 16.6106/0.2398/0.056 in terms of RMSE, SSIM, and LPIPS objective indicators. Table 2: Ablation of LPEC on different subtasks. X-ray Contraband Semi-transparent glassware Translucent occlusion Translucent flare Subtask RMSE SSIM LPIPS RMSE SSIM LPIPS RMSE SSIM LPIPS RMSE SSIM LPIPS w/o LPEC 16.5379 3.8903 Ours 0.7922 0.9882 0.0489 0.0049 24.6044 7. 0.7361 0.9759 0.0694 0.013 21.2568 18.9561 0.7411 0.7653 0.1672 0.1445 32.4708 16. 0.7040 0.8189 0.2352 0.0945 To further verify the effectiveness of LPEC, Figure 6 shows the results of DiffDecompose in four sub-tasks. As can be seen, when we remove LPEC, the layers of transparent glass and the most challenging subtask, X-ray contraband decomposition, will cause information distanglement. For the color of persons face and the body of car. More ablation studies are provided in the Appendix. Effectiveness of ICD. The ICD can be regarded as the key to the task of decomposition. As shown in Figure 7, when we totally remove the ICD, the framework fails to decompose the different layers and even results in false prediction, which can be regarded as the failure of layer decomposition. In contrast, as we place the ICD on our baseline, the results can not only recover the accurate background but also output different layers. More ablation studies are provided in the Appendix. Figure 6: The visualization of LPEC ablation. Full settings ensure the information alignment of different layers, while removals degrade performance. Figure 7: The visualization of ICD ablation. It shows that only the full setting enables effective decomposition."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose novel layer decomposition task for the semi-transparent/transparent layer-wise decomposition of alpha-composited images. To support this new task, we propose large-scale and high-quality AlphaBlend dataset that contains multiple layer-level transparent/semitransparent foreground alpha-blend images. To cope with this complex task, we present novel point that redefines the problem as learning posterior over potential layer configurations conditioned on contextual cues. In the process of training, we propose transformer diffusion-based framework, DiffDecompose, to learn the in-context decomposition to effectively predict single-layer or multilayer results under given synthetic image conditions and achieve layer-by-layer decomposition. Furthermore, we also devise the LPEC for the process of ICD to effectively align the information between certain layer and the original layer. Extensive experiments verify the effectiveness of semi-transparent/transparent layer-level decomposition task."
        },
        {
            "title": "References",
            "content": "[1] Mohammad Mahmudul Alam and Mohammad Tariqul Islam. Machine learning approach of automatic identification and counting of blood cells. Healthcare Technology Letters, 6(4):103108, 2019. [2] Tomer Amit, Tal Shaharbany, Eliya Nachmani, and Lior Wolf. Segdiff: Image segmentation with diffusion probabilistic models. arXiv preprint arXiv:2112.00390, 2021. [3] Jingqi Bai, Jingkai Zhou, Benzhi Wang, Weihua Chen, Yang Yang, Zhen Lei, and Fan Wang. Layer-animate for transparent video generation. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 15, 2025. [4] Yutong Bai, Xinyang Geng, Karttikeya Mangalam, Amir Bar, Alan Yuille, Trevor Darrell, Jitendra Malik, and Alexei Efros. Sequential modeling enables scalable learning for large vision models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2286122872, 2024. [5] Omer Bar-Tal, Hila Chefer, Omer Tov, Charles Herrmann, Roni Paiss, Shiran Zada, Ariel Ephrat, Junhwa Hur, Guanghui Liu, Amit Raj, et al. Lumiere: space-time diffusion model for video generation. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [6] James Betker, Gabriel Goh, Li Jing, Tim Brooks, Jianfeng Wang, Linjie Li, Long Ouyang, Juntang Zhuang, Joyce Lee, Yufei Guo, et al. Improving image generation with better captions. Computer Science, 2(3):8, 2023. [7] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. [8] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in Neural Information Processing Systems, 33:18771901, 2020. [9] Bowei Chen, Yifan Wang, Brian Curless, Ira Kemelmacher-Shlizerman, and Steven Seitz. Inverse painting: Reconstructing the painting process. In SIGGRAPH Asia 2024 Conference Papers, pages 111, 2024. [10] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Zhongdao Wang, James T. Kwok, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-α: Fast training of diffusion transformer for photorealistic text-to-image synthesis. In The Twelfth International Conference on Learning Representations, 2024. [11] Lan Chen, Qi Mao, Yuchao Gu, and Mike Zheng Shou. Edit transfer: Learning image editing via vision in-context relations. arXiv preprint arXiv:2503.13327, 2025. [12] Xi Chen, Yutong Feng, Mengting Chen, Yiyang Wang, Shilong Zhang, Yu Liu, Yujun Shen, and Hengshuang Zhao. Zero-shot image editing with reference imitation. Advances in Neural Information Processing Systems, 37:8401084032, 2024. [13] Xuewei Chen, Zhimin Chen, and Yiren Song. Transanimate: Taming layer diffusion to generate rgba video. arXiv preprint arXiv:2503.17934, 2025. [14] Yizhen Chen and Haifeng Hu. An improved method for semantic image inpainting with gans: progressive inpainting. Neural Processing Letters, 49:13551367, 2019. [15] Xiaodong Cun and Chi-Man Pun. Split then refine: stacked attention-guided resunets for blind single image visible watermark removal. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 35-2, pages 11841192, 2021. [16] Zhicheng Ding, Panfeng Li, Qikai Yang, and Siyang Li. Enhance image-to-image generation with llavagenerated prompts. In 5th International Conference on Information Science, Parallel and Distributed Systems, pages 7781, 2024. [17] Yigit Ekin, Ahmet Burak Yildirim, Erdem Eren Çaglar, Aykut Erdem, Erkut Erdem, and Aysegul Dundar. Clipaway: Harmonizing focused embeddings for removing objects via diffusion models. Advances in Neural Information Processing Systems, 37:1757217601, 2024. [18] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, pages 1260612633, 2024. [19] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. [20] Zhongbin Fang, Xiangtai Li, Xia Li, Joachim Buhmann, Chen Change Loy, and Mengyuan Liu. Explore in-context learning for 3d point cloud understanding. Advances in Neural Information Processing Systems, 36:4238242395, 2023. [21] Zeqi Gu, Wenqi Xian, Noah Snavely, and Abe Davis. Factormatte: Redefining video matting for recomposition tasks. ACM Transactions on Graphics, 42(4):114, 2023. [22] Zhangxuan Gu, Haoxing Chen, and Zhuoer Xu. Diffusioninst: Diffusion model for instance segmentation. In IEEE International Conference on Acoustics, Speech and Signal Processing, pages 27302734, 2024. [23] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017. [24] Timothy Hodson. Root mean square error (rmse) or mean absolute error (mae): When to use them or not. Geoscientific Model Development Discussions, 2022:110, 2022. [25] Mude Hui, Siwei Yang, Bingchen Zhao, Yichun Shi, Heng Wang, Peng Wang, Yuyin Zhou, and Cihang Xie. Hq-edit: high-quality dataset for instruction-based image editing. arXiv preprint arXiv:2404.09990, 2024. [26] Longtao Jiang, Zhendong Wang, Jianmin Bao, Wengang Zhou, Dongdong Chen, Lei Shi, Dong Chen, and Houqiang Li. Smarteraser: Remove anything from images using masked-region guidance. arXiv preprint arXiv:2501.08279, 2025. [27] Diederik P. Kingma and Max Welling. Auto-encoding variational bayes. In 2nd International Conference on Learning Representations, 2014. [28] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [29] Yunwei Lan, Zhigao Cui, Chang Liu, Jialun Peng, Nian Wang, Xin Luo, and Dong Liu. Exploiting diffusion prior for real-world image dehazing with unpaired training. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39-4, pages 44554463, 2025. [30] Yao-Chih Lee, Erika Lu, Sarah Rumbley, Michal Geyer, Jia-Bin Huang, Tali Dekel, and Forrester Cole. Generative omnimatte: Learning to decompose video into layers. arXiv preprint arXiv:2411.16683, 2024. [31] Xinyu Li, Qi Yao, and Yuanda Wang. Garmentdiffusion: 3d garment sewing pattern generation with multimodal diffusion transformers. arXiv preprint arXiv:2504.21476, 2025. [32] Beibei Lin, Yeying Jin, Yan Wending, Wei Ye, Yuan Yuan, and Robby Tan. Nighthaze: Nighttime image dehazing via self-prior learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39-5, pages 52095217, 2025. [33] Guilin Liu, Aysegul Dundar, Kevin Shih, Ting-Chun Wang, Fitsum Reda, Karan Sapra, Zhiding Yu, Xiaodong Yang, Andrew Tao, and Bryan Catanzaro. Partial convolution for padding, inpainting, and image synthesis. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(5):60966110, 2022. [34] Guilin Liu, Fitsum Reda, Kevin Shih, Ting-Chun Wang, Andrew Tao, and Bryan Catanzaro. Image inpainting for irregular holes using partial convolutions. In Proceedings of the European Conference on Computer Vision, pages 85100, 2018. [35] Zhiyuan Lu, Hao Lu, and Hua Huang. Efficient portrait matte creation with layer diffusion and connectivity priors. arXiv preprint arXiv:2501.16147, 2025. [36] Shervin Minaee, Yuri Boykov, Fatih Porikli, Antonio Plaza, Nasser Kehtarnavaz, and Demetri Terzopoulos. Image segmentation using deep learning: survey. IEEE Transactions on Pattern Analysis and Machine Intelligence, 44(7):35233542, 2021. [37] Yifang Pan, Karan Singh, and Luiz Gustavo Hafemann. Model see model do: Speech-driven facial animation with style control. arXiv preprint arXiv:2505.01319, 2025. [38] Deepak Pathak, Philipp Krahenbuhl, Jeff Donahue, Trevor Darrell, and Alexei Efros. Context encoders: Feature learning by inpainting. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 25362544, 2016. 12 [39] Suraj Patil."
        },
        {
            "title": "Sdxl",
            "content": "inpainting. https://huggingface.co/spaces/diffusers/ stable-diffusion-xl-inpainting/tree/main, 2024. [40] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Cision, pages 41954205, 2023. [41] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. [42] Elad Richardson, Yuval Alaluf, Or Patashnik, Yotam Nitzan, Yaniv Azar, Stav Shapiro, and Daniel CohenOr. Encoding in style: stylegan encoder for image-to-image translation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22872296, 2021. [43] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. [44] Chaohua Shi, Xuan Wang, Si Shi, Xule Wang, Mingrui Zhu, Nannan Wang, and Xinbo Gao. Foodfusion: novel approach for food image composition via diffusion models. arXiv preprint arXiv:2408.14135, 2024. [45] Jake Snell, Karl Ridgeway, Renjie Liao, Brett Roads, Michael Mozer, and Richard Zemel. Learning to generate images with perceptual similarity metrics. In 2017 IEEE International Conference on Image Processing, pages 42774281. IEEE, 2017. [46] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In 9th International Conference on Learning Representations, 2021. [47] Wensong Song, Hong Jiang, Zongxing Yang, Ruijie Quan, and Yi Yang. Insert anything: Image insertion via in-context editing in dit. arXiv preprint arXiv:2504.15009, 2025. [48] Yiren Song, Cheng Liu, and Mike Zheng Shou. Makeanything: Harnessing diffusion transformers for multi-domain procedural sequence generation. arXiv preprint arXiv:2502.01572, 2025. [49] Zeyi Sun, Ye Fang, Tong Wu, Pan Zhang, Yuhang Zang, Shu Kong, Yuanjun Xiong, Dahua Lin, and Jiaqi Wang. Alpha-clip: clip model focusing on wherever you want. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1301913029, 2024. [50] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098, 2024. [51] AlimamaCreative Team. Flux.1-dev-controlnet-inpainting-alpha. https://github.com/ alimama-creative/FLUX-Controlnet-Inpainting.git, 2024. [52] Xie Tianyidan, Rui Ma, Qian Wang, Xiaoqian Ye, Feixuan Liu, Ying Tai, Zhenyu Zhang, Lanjun Wang, and Zili Yi. Anywhere: multi-agent framework for user-guided, reliable, and diverse foreground-conditioned image generation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39-7, pages 74107418, 2025. [53] Petru-Daniel Tudosiu, Yongxin Yang, Shifeng Zhang, Fei Chen, Steven McDonagh, Gerasimos Lampouras, Ignacio Iacobacci, and Sarah Parisot. Mulan: multi layer annotated dataset for controllable text-to-image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2241322422, 2024. [54] Chaoyang Wang, Xiangtai Li, Henghui Ding, Lu Qi, Jiangning Zhang, Yunhai Tong, Chen Change Loy, and Shuicheng Yan. Explore in-context segmentation via latent diffusion models. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, pages 75457553, 2025. [55] Chaoyang Wang, Xiangtai Li, Lu Qi, Henghui Ding, Yunhai Tong, and Ming-Hsuan Yang. Semflow: Binding semantic segmentation and image synthesis via rectified flow. Advances in Neural Information Processing Systems, 37:138981139001, 2024. [56] Zhou Wang and Alan Bovik. universal image quality index. IEEE Signal Srocessing Letters, 9(3):8184, 2002. [57] Yanlu Wei, Renshuai Tao, Zhangjie Wu, Yuqing Ma, Libo Zhang, and Xianglong Liu. Occluded prohibited items detection: An x-ray security inspection benchmark and de-occlusion attention module. In Proceedings of the 28th ACM International Conference on Multimedia, pages 138146, 2020. 13 [58] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76237633, 2023. [59] Jianzong Wu, Xiangtai Li, Chenyang Si, Shangchen Zhou, Jingkang Yang, Jiangning Zhang, Yining Li, Kai Chen, Yunhai Tong, Ziwei Liu, et al. Towards language-driven video inpainting via multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1250112511, 2024. [60] Jianzong Wu, Xiangtai Li, Yanhong Zeng, Jiangning Zhang, Qianyu Zhou, Yining Li, Yunhai Tong, and Kai Chen. Motionbooth: Motion-aware customized text-to-video generation. arXiv preprint arXiv:2406.17758, 2024. [61] Shaoan Xie, Zhifei Zhang, Zhe Lin, Tobias Hinz, and Kun Zhang. Smartbrush: Text and shape guided object inpainting with diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2242822437, 2023. [62] Yanwu Xu, Yang Zhao, Zhisheng Xiao, and Tingbo Hou. Ufogen: You forward once large scale text-toimage generation via diffusion gans. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81968206, 2024. [63] Ziyang Xu, Kangsheng Duan, Xiaolei Shen, Zhifeng Ding, Wenyu Liu, Xiaohu Ruan, Xiaoxin Chen, and Xinggang Wang. Pixelhacker: Image inpainting with structural and semantic consistency. arXiv preprint arXiv:2504.20438, 2025. [64] Jinrui Yang, Qing Liu, Yijun Li, Soo Ye Kim, Daniil Pakhomov, Mengwei Ren, Jianming Zhang, Zhe Lin, Cihang Xie, and Yuyin Zhou. Generative image layer decomposition with visual effects. arXiv preprint arXiv:2411.17864, 2024. [65] Lehan Yang, Lu Qi, Xiangtai Li, Sheng Li, Varun Jampani, and Ming-Hsuan Yang. Unified dense prediction of video diffusion. arXiv preprint arXiv:2503.09344, 2025. [66] Shuai Yang, Jing Tan, Mengchen Zhang, Tong Wu, Yixuan Li, Gordon Wetzstein, Ziwei Liu, and Dahua Lin. Layerpano3d: Layered 3d panorama for hyper-immersive scene generation. arXiv preprint arXiv:2408.13252, 2024. [67] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721, 2023. [68] Qifan Yu, Wei Chow, Zhongqi Yue, Kaihang Pan, Yang Wu, Xiaoyang Wan, Juncheng Li, Siliang Tang, Hanwang Zhang, and Yueting Zhuang. Anyedit: Mastering unified high-quality image editing for any idea. arXiv preprint arXiv:2411.15738, 2024. [69] Tao Yu, Runseng Feng, Ruoyu Feng, Jinming Liu, Xin Jin, Wenjun Zeng, and Zhibo Chen. Inpaint anything: Segment anything meets image inpainting. arXiv preprint arXiv:2304.06790, 2023. [70] Yongsheng Yu, Ziyun Zeng, Hang Hua, Jianlong Fu, and Jiebo Luo. Promptfix: You prompt and we fix the photo. arXiv preprint arXiv:2405.16785, 2024. [71] Lvmin Zhang and Maneesh Agrawala. Transparent image layer diffusion using latent transparency. arXiv preprint arXiv:2402.17113, 2024. [72] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 38363847, 2023. [73] Yuanhan Zhang, Kaiyang Zhou, and Ziwei Liu. What makes good examples for visual in-context learning? Advances in Neural Information Processing Systems, 36:1777317794, 2023. [74] Zechuan Zhang, Ji Xie, Yu Lu, Zongxin Yang, and Yi Yang. In-context edit: Enabling instructional image editing with in-context generation in large scale diffusion transformer. arXiv preprint arXiv:2504.20690, 2025. [75] Xin Zheng, Yong Wang, Guoyou Wang, and Jianguo Liu. Fast and robust segmentation of white blood cell images by self-supervised learning. Micron, 107:5571, 2018. [76] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022. [77] Junhao Zhuang, Yanhong Zeng, Wenran Liu, Chun Yuan, and Kai Chen. task is worth one word: Learning with task prompts for high-quality versatile image inpainting. In European Conference on Computer Vision, pages 195211, 2024. 15 Overview In the Appendix, we first present more details about our proposed dataset, AlphaBlend. Then, we introduce more information about the training and inference of DiffDecompose. Subsequently, we offer more competing results upon our proposed AlphaBlend dataset and public dataset, LOGO, and also include additional quantitative experiments and complexity analyses. Finally, we present more visual results from generative results of DiffDecompose. The supplementary includes the following sections: A.AlphaBlend dataset. B.Training Loss Function. C.Inference Algorithm. D.More Comparison Experiments and Generative Results. E.User Study and Broader Impacts. F.Limitations."
        },
        {
            "title": "A AlphaBlend dataset",
            "content": "As can be seen from Figure 8, the properties of the AlphaBlend dataset are different from the existing datasets that are intended to generate foreground layer object information that is completely opaque but has an alpha channel. The AlphaBlend datasets consider different semi-transparent conditions, from layer-level color distanglement to simple region-level transparent object occlusion. The task is no longer the same as the tasks of object removal [69, 39, 17], downstream image restoration [29, 32], or layer generation [71, 64], and is also common scenario in the real world. For tasks and II, the rain, fog, light, and dirt can always occlude the whole background. Different from the common image dehazing and deraining, the pixel of the background has not been covered or fuzzy, but makes itself present semi-transparent. For task III, we further consider the more complex watermark condition in the real world, the current watermark dataset always presents lower opacity, and smaller occlusion, providing sufficient background information for inpainting methods to predict their background. Thus, we introduce bigger coverage with the size of 96128, and the opacity below 0.25, which is more difficult to recognize. Tasks III, V, and VI, respectively, represent different pixel-level non-linear occlusions. These subtasks utilize part of the dataset [75, 1, 57, 50]."
        },
        {
            "title": "B Training Loss Function",
            "content": "The loss function is formulated within the conditional flow matching [19] framework as the joint expectation over timesteps, noisy latent variables, and noise samples: Llce = Et,pt(xtϵx),pt(ytϵy),p(ϵx),p(ϵy) vΘ (z, t, x, y, cT ) ut(z ϵx, ϵy)2 (5) where vΘ (z, t, x, y) are the neural networks predicted conditional velocity fields for the noisy foreground and background latent variables xt and yt, respectively, conditioned on the timestep t, the observed composite image z, and conditioning information τ . The true velocities ut(z ϵx, ϵy), are derived from the diffusion process and noise samples ϵx, ϵy. This loss enables the model to learn an optimal probabilistic path from noise to the true decomposed images, thereby jointly inferring foreground and background layers under complex nonlinear composition."
        },
        {
            "title": "C Inference Algorithm",
            "content": "Algorithm 1 demonstrates the inference process of DiffDecompose. The composited image is regarded as the condition input of model D() without adding any noise, and foreground and background will be first randomly sampled by Gaussian distribution noise, and gradually generate their corresponding information guided by the prompt and condition image. 16 Figure 8: The presentation of the six subtasks dataset. Each foreground has its respective properties. Algorithm 1 DiffDecompose Inference Algorithm 1: Input: Composited Image z, Trained DiffDecompose model D() at sampling timestep 2: Output: foreground image and background image 3: for epoch = 1 to do xt (0, I) 4: yt (0, I) 5: ˆvx, ˆvy = vθ(xt, yt, t, z, τ ) 6: (cid:113) 1 αt1 1 αt σ2 = η αt1 1 αt 1 αtˆvx αt 1 αtˆvy αt + (cid:112)1 αt1 σ2 + (cid:112)1 αt1 σ ˆvx + σtzt ˆvy + σtzt xt1 = (cid:16) xt (cid:16) yt αt 8: 7: 9: (cid:17) (cid:17) (cid:17) (cid:16) αt1 yt1 = Iout D(ZT , t) 10: 11: end for"
        },
        {
            "title": "D More Comparison Experiments and Generative Results",
            "content": "D.1 Evaluation Metric. We use the Frechet Inception Distance (FID) [23] to assess the photorealism of the generated images by comparing the source image distribution with the inpainted image distributions. To measure the accuracy of correct object removal, we introduce root mean squared error (RMSE)[24], structural similarity index measurement (SSIM)[56], and Learned Perceptual Image Patch Similarity (LPIPS) [45]. 17 D.2 More Comparisons We present comprehensive comparison of our DiffDecompose and other methods on series of tasks within our newly established benchmark AlphaBlend and publicly watermark dataset LOGO. We present two competing results in each subtask. For our proposed AlphaBlend, since the current inpainting methods [69, 39, 17] mainly rely on the mask-based method to achieve image inpainting, the task of occlusion and light removal is not reasonable for them to edit the whole image as shown in Figure 9, no matter how we tune the strengthen and change the prompt, it is still difficult to remove the rain and light. Thus, we only compare the results of the cell, glassware, and watermark, which can also be regarded as the regionlevel task instead of the layer-level task for these competing methods. It can be seen that when facing the transparent scenarios, the current methods directly predict the mask region with the background or prompt instead of fully utilizing the information of its transparent region. Additionally, when the occlusion region is pixel-additive, the competing method is prone to utilize the background to generate an unreasonable object to fix the removal cell. These results suggest that when the target object is no longer simple pixel-level coverage, introducing the region-based methods for image editing is difficult to recover accurately. In comparison, our proposed DiffDecompose can better remove the foreground layer and maintain the background layer successfully. To further verify the effectiveness Figure 9: The SDXL-inpainting results under different strengths. We input the prompt like \"Remove the light and make the scenario darker\" and \"Remove the foreground rain and fog\", it is difficult for the model to process the layer-level edition. of DiffDecompose, we extend the way to the common task of watermark removal. As shown in the latest three rows, the publicly available LOGO-G, LOGO-L, and LOGO-H, represent progressively challenging benchmarks for watermark removal, distinguished primarily by the transparency and size attributes of their embedded watermarks, these three public datasets can be concluded as: LOGO-G. This dataset consists 2,000 testing samples, characterized by gray-scale watermarks. The watermark transparency in LOGO-Gray varies broadly from 35% to 85%, encompassing both relatively faint and highly opaque watermarks. The wide transparency range introduces substantial variability, simulating real-world scenarios where watermarks may range from subtle overlays to dominant visual artifacts. This dataset thus serves as comprehensive testbed for evaluating models robustness across diverse watermark visibility levels. LOGO-L. LOGO-L restricts the watermark transparency to narrower interval of 35% to 60%. Furthermore, the watermark size is constrained between 35% to 60% of the host image width. These constraints reflect moderate watermark prominence both in opacity and spatial extent, providing balanced challenge for watermark removal techniques. The reduced transparency range relative to LOGO-Gray focuses the evaluation on semi-transparent watermarks, common in practical applications where watermarks are designed to be visible but non-intrusive. LOGO-H. LOGO-H represents more difficult subset derived from LOGO-L. It targets harder cases by increasing both watermark transparency and size, randomly selecting values from 60% to 85%. These higher transparency and larger size ranges imply that watermarks are more visually prominent and occlusive, significantly complicating the watermark removal task. Models evaluated on LOGO-H must contend with nearly opaque and spatially extensive watermarks, testing their ability to recover underlying image content under severe occlusion. As shown in Figure 10, our method outperforms existing approaches across challenging subsets, particularly LOGO-H and LOGO-L, which involve large occlusions and foregrounds with colors closely resembling the background. Compared methods often introduce artifacts or fail to fully remove watermarks under such conditions. In contrast, our approach achieves clearer and more coherent 18 Figure 10: Qualitative results of our and compared methods on the proposed AlphaBlend dataset and the publicly LOGO dataset. Inpainting models often replace the object by predicting new pixel information instead of removing it, which fails to generate realistic background. Our method is the only one that effectively removes objects and fills the regions in accurate manner. reconstructions by jointly inferring foreground and background through layer-wise decomposition framework. This enables effective disentanglement of semi-transparent overlays without relying on explicit masks, making our method especially robust in complex, real-world scenarios involving transparency and subtle blending. D.3 More Generative Results Figure 11 presents additional qualitative results for the first three subtasks of the AlphaBlend dataset: Subtask (Translucent Flare Removal), Subtask II (Translucent Occlusion Removal), and Subtask III (Semi-transparent Watermark Removal). These extended visualizations further validate the capability of our DiffDecompose framework to disentangle complex semi-transparent and transparent layers in diverse real-world image compositions. DiffDecompose effectively removes complex lens flare while preserving scene structure and detail, separates translucent glass and reflection artifacts without disrupting background consistency, and accurately eliminates semi-transparent watermarks while maintaining underlying textures. As shown in Figure 12, we present more qualitative results of our proposed DiffDecompose framework on three key semi-transparent/transparent layer-wise decomposition subtasks from the AlphaBlend dataset: Subtask IV (Transparent Glassware Decomposition), Subtask (Semi-transparent Cell Decomposition), and Subtask VI (X-ray Contraband Decomposition). These additional visualizations provide deeper insights into the robustness and generalization capabilities of our method in handling complex alpha-composited images characterized by nonlinear blending and ambiguous layer interactions. DiffDecompose isolates glass objects from varied backgrounds, accurately separates individual cells even in densely overlapping regions, and distinguishes hidden items from complex scan content while preserving contextual detail without relying on explicit masks. As shown in Table 3, DiffDecompose achieves low RMSE and LPIPS scores alongside high SSIM across all three subtasks, indicating accurate structural and perceptual foreground reconstruction. These results confirm the methods effectiveness in preserving fine-grained details and layer integrity, even under semi-transparent and complex blending conditions. By introducing probabilistic layered decomposition framework, our model recovers visually coherent and semantically aligned foregrounds. This highlights the advantage of modeling compositional structures without requiring explicit pixel-level supervision. Table 3: Quantitative Evaluation of Foreground Separation Quality. Performance metrics, including RMSE, SSIM, LPIPS, and FID, demonstrate DiffDecomposes effectiveness in accurately extracting foreground layers across transparent glassware, X-ray contraband, and semi-transparent cell decomposition subtasks."
        },
        {
            "title": "Subtask Transparent Glassware Decomposition",
            "content": "X-ray Contraband Decomposition Semi-transparent Cell Decomposition"
        },
        {
            "title": "Models",
            "content": "RMSE SSIM LPIPS FID RMSE SSIM LPIPS FID RMSE SSIM LPIPS FID"
        },
        {
            "title": "Ours",
            "content": "0.7570 0.9993 0.0019 1.5463 0.7784 0. 0.0030 6.0607 0.2507 0.9997 0.0003 1. 20 Figure 11: Additional Qualitative Results for Subtasks IIII. Extended visualizations demonstrating DiffDecomposes capability in removing complex semi-transparent artifacts, including lens flares, translucent occlusions, and semi-transparent watermarks. The results illustrate faithful layer-wise decomposition, preserving fine details and semantic consistency across diverse real-world scenarios. 21 Figure 12: Additional Qualitative Results for Subtasks IVVI. Extended results showcasing DiffDecomposes effectiveness in transparent glassware decomposition, X-ray contraband separation, and semi-transparent cell decomposition. The visualizations highlight the frameworks ability to disentangle complex overlapping layers, preserving structural details and semantic fidelity in challenging transparency scenarios. D.4 User Study To further evaluate the perceptual quality of our model compared to the baseline methods, we conducted user study with 30 subjects using an online voting interface, the voting interface and voting results are shown in Figures 13 and 14, respectively. For each visual scene, we presented the outputs of the six methods in random order. Participants were asked to select the best result in terms of removal effectiveness, background integrity, and result plausibility for six representative subtasks. Our method received the highest number of votes in all six categories, demonstrating superiority over other competing baseline methods in terms of both realism and semantic coherence, without introducing unnecessary changes. 22 Figure 13: User study results. The voting results of DiffDecompose and the baseline method are compared on different subtasks, including removal effectiveness, background integrity, and result plausibility. Figure 14: user study voting interface was provided to participants. We present the performance of five competing methods on the task of watermark removal, cell separation, and glassware removal. The participants can click the alphabet to choose which method is the best and accord with their requirements."
        },
        {
            "title": "E Limitations and Broader Impacts",
            "content": "Limitations As shown in Figure 15, although DiffDecompose performs well in various challenging semi-transparent and transparent layered decomposition tasks, it still has some limitations that deserve further study. Since DiffDecompose does not have an explicit supervision layer and fine-grained masks in the process of foreground and background separation, the pixels in the image repair area will have pixel drift that is imperceptible to the naked eye. That is, the overall separated foreground and background information have good visual contrast, but lack pixel-level position accuracy. Therefore, we will introduce Diffusion Guidance Masks in the future to improve the networks attention to image boundaries, thereby improving the accuracy of pixel restoration. Figure 15: Results of failed decomposition by DiffDecompose. Broader Impacts. This work may benefit applications in image editing, scientific visualization, and digital restoration by enabling accurate decomposition of semi-transparent and transparent layers. The public release of the AlphaBlend dataset and DiffDecompose code promotes reproducibility and supports broader research efforts. However, the ability to decompose image layers may also enable misuse in image manipulation or privacy invasion. We encourage responsible use and recommend the development of safeguards, particularly when extending this work to sensitive data. No personally identifiable or human subject data is involved in our experiments."
        }
    ],
    "affiliations": []
}