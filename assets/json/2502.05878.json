{
    "paper_title": "Enhancing Financial Time-Series Forecasting with Retrieval-Augmented Large Language Models",
    "authors": [
        "Mengxi Xiao",
        "Zihao Jiang",
        "Lingfei Qian",
        "Zhengyu Chen",
        "Yueru He",
        "Yijing Xu",
        "Yuecheng Jiang",
        "Dong Li",
        "Ruey-Ling Weng",
        "Min Peng",
        "Jimin Huang",
        "Sophia Ananiadou",
        "Qianqian Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Stock movement prediction, a critical task in financial time-series forecasting, relies on identifying and retrieving key influencing factors from vast and complex datasets. However, traditional text-trained or numeric similarity-based retrieval methods often struggle to handle the intricacies of financial data. To address this, we propose the first retrieval-augmented generation (RAG) framework specifically designed for financial time-series forecasting. Our framework incorporates three key innovations: a fine-tuned 1B large language model (StockLLM) as its backbone, a novel candidate selection method enhanced by LLM feedback, and a training objective that maximizes the similarity between queries and historically significant sequences. These advancements enable our retriever, FinSeer, to uncover meaningful patterns while effectively minimizing noise in complex financial datasets. To support robust evaluation, we also construct new datasets that integrate financial indicators and historical stock prices. Experimental results demonstrate that our RAG framework outperforms both the baseline StockLLM and random retrieval methods, showcasing its effectiveness. FinSeer, as the retriever, achieves an 8% higher accuracy on the BIGDATA22 benchmark and retrieves more impactful sequences compared to existing retrieval methods. This work highlights the importance of tailored retrieval models in financial forecasting and provides a novel, scalable framework for future research in the field."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 2 8 7 8 5 0 . 2 0 5 2 : r Enhancing Financial Time-Series Forecasting with Retrieval-Augmented Large Language Models Mengxi Xiao School of Computer Science, Wuhan University Wuhan, China Zihao Jiang School of Computer Science, Wuhan University Wuhan, China"
        },
        {
            "title": "Yueru He\nColumbia University\nUnited States",
            "content": "Dong Li School of Computer Science, Wuhan University China Zhengyu Chen School of Computer Science, Wuhan University Wuhan, China"
        },
        {
            "title": "Yuecheng Jiang\nStevens Institute of Technology\nUnited States",
            "content": "Min Peng School of Computer Science, Wuhan University China"
        },
        {
            "title": "Yijing Xu\nColumbia University\nUnited States",
            "content": "Ruey-Ling Weng Yale University United States"
        },
        {
            "title": "Sophia Ananiadou\nUniversity of Manchester\nUnited Kingdom",
            "content": "Qianqian Xie School of Computer Science, Wuhan University China xieq@whu.edu.cn Abstract Stock movement prediction, critical task in financial time-series forecasting, relies on identifying and retrieving key influencing factors from vast and complex datasets. However, traditional texttrained or numeric similarity-based retrieval methods often struggle to handle the intricacies of financial data. To address this, we propose the first retrieval-augmented generation (RAG) framework specifically designed for financial time-series forecasting. Our framework incorporates three key innovations: fine-tuned 1B large language model (StockLLM) as its backbone, novel candidate selection method enhanced by LLM feedback, and training objective that maximizes the similarity between queries and historically significant sequences. These advancements enable our retriever, FinSeer, to uncover meaningful patterns while effectively minimizing noise in complex financial datasets. To support robust evaluation, we also construct new datasets that integrate financial indicators and historical stock prices. Experimental results demonstrate that our RAG framework outperforms both the baseline StockLLM and random retrieval methods, showcasing its effectiveness. FinSeer, as the retriever, achieves an 8% higher accuracy on the BIGDATA22 benchmark and retrieves more impactful sequences compared to existing retrieval methods. This work highlights the importance of tailored retrieval models in financial forecasting and provides novel, scalable framework for future research in the field. Corresponding author. CCS Concepts Information systems Top-k retrieval in databases; Similarity measures; Novelty in information retrieval. Keywords Retrieval-augmented generation, Financial Time-series forecasting, Stock Movement Prediction, Large Language Model"
        },
        {
            "title": "1 Introduction\nFinancial time-series forecasting plays a pivotal role in ensuring\nmarket stability and efficiency, directly impacting critical areas such\nas investment strategies, risk management, and the formulation\nof economic policies [4]. One of the most prominent tasks in this\ndomain is stock movement prediction [29, 30, 32], which focuses\non forecasting the direction of price changes, i.e. whether a stock\nwill rise or fall. Accurate predictions of financial metrics, including\nstock movements, interest rates, and economic indicators, are es-\nsential for a wide range of stakeholders, from individual investors\nto large financial institutions. However, the inherent complexity\nand volatility of financial markets pose significant challenges to\nforecasting, necessitating the use of advanced methodologies to\neffectively analyze vast and often noisy datasets.",
            "content": "Conference17, July 2017, Washington, DC, USA Mengxi Xiao et al. Traditional stock movement prediction methods [6, 18] relied on analyzing sequences of stock prices and financial indicators1 to identify patterns, but they often failed to capture the semantic connections between these sequences, task at which large language models (LLMs) excel [9]. However, LLM-based methods [2931] have primarily shifted focus to leveraging textual data such as news and tweets, often using only the past several days closing prices as reference while overlooking the rich insights embedded in historical time-series data [1]. This highlights the need to effectively integrate financial time-series data with LLMs. Directly working with such data presents challenge due to its vast scale, encompassing both the breadth of influencing variables and the depth of historical trends. To address this, retrieval-based approach is necessary to efficiently sift through extensive time-series data, extracting meaningful information to enhance LLMs ability to deliver more accurate and robust stock movement predictions. Although retrieval-augmented generation (RAG) methods have been applied for various tasks [2, 8, 13], it is challenging to directly apply them to financial time-series retrieval. First, most embeddingbased retrieval methods (retrievers) [14, 22, 24, 28, 39] are trained on textual data and struggle to process numeric time-series data, which lacks explicit semantic information. Second, distance-based retrieval methods like Dynamic Time Warping (DTW) [35] focus on numeric similarities and fail to capture deeper semantic relationships or contextual nuances essential for stock movement prediction. These limitations underscore the need for more tailored retrieval framework designed for financial time-series data. To address these challenges, we introduce retrieval-augmented framework for stock movement prediction, featuring the first dedicated retriever for financial time-series forecasting, Financial TimeSeries Retriever (FinSeer). Unlike previous RAG methods, which rely on pre-trained encoders or distance-based metrics, our framework is the first to train dedicated retriever tailored for continuous and complex temporal sequences. The whole retrieval framework is shown in Figure 1. To address the limitations of existing datasets, which often contain only price data, and to better align with professional financial analysis, we construct new datasets by integrating 20 groups of financial indicators. These indicators provide critical insights into market behavior that stock prices alone cannot capture2. We begin by selecting high-trade-volume U.S. stocks, and the indicators are chosen through domain expert consultation and mutual information analysis. These indicators are then serialized and divided into train, valid and test sets. By training the retriever on datasets enriched with diverse financial indicators, our retrieval framework incorporates financial domain expertise, enabling it to interpret the implications of trending signals that uncover deeper patterns beyond surface-level trend similarities. The testing set, designed to mimic real-world economic analysis, ensures robust evaluation of model performance in practical financial scenarios. 1Financial indicators [10] are professional market analysis metrics derived from price data, where their values or crossover points signal fluctuations in the stock market, indicating rising or falling trends. However, relying on one or two indicators can introduce bias, and if an indicator only provides meaningful information at crossover points, it may fail to offer valuable insights during periods without crossings. 2For instance, the overbought area in the KDJ indicator group [26], where 𝐾 > 80, 𝐷 > 70, and 𝐽 > 90, signals potential falling trend. To train our retriever to identify semantically related sequences and align the retrievers priorities with those of the LLM, we introduce novel retrieval mechanism with three key innovations: new backbone LLM, novel method for selecting positive and negative candidates, and new training objective. First, to activate the LLMs inherent knowledge and ensure instruction-following capabilities, we fine-tune 1B parameter LLM (LLaMA3.2-1B Instruct) [7], creating StockLLM as the backbone model of our retriever. By using smaller LLM, we establish more challenging experimental setup, ensuring performance improvements are attributable to FinSeer rather than the LLMs capacity. Next, StockLLM identifies timeseries segments that enhance the generative process, feedbacks from StockLLM are used to select the most beneficial sequences that could lead the model to make the correct decision as positive candidates and the least beneficial as negative candidates. This could bridge the gap between the retriever and the LLM and ensure that retrieved data aligns with the LLMs forecasting priorities. Finally, the training objective, inspired by Zhang et al. [39], ensures the retriever distinguishing historically significant sequences (positive candidates) from noisy sequences (negative candidates) by maximizing the similarity between the query and candidate sequences. This prioritizes meaningful patterns while minimizing irrelevant data, outperforming traditional methods that struggle to separate signal from noise in complex financial time series. Based on FinSeer, the inference process of our RAG framework consists of three steps: (1) using FinSeer to retrieve relevant sequences from the candidate pool, (2) incorporating these sequences into StockLLMs input context, and (3) combining this enriched context with the original query to perform forecasting. This approach effectively integrates salient historical patterns with current data, improving prediction accuracy. To thoroughly evaluate the performance of our RAG framework and the retriever, we rely solely on temporal information, excluding additional textual data like news. For the RAG framework evaluation, StockLLM serves as the backbone model, integrated with different retrieval methods for comparison. Experimental results demonstrate that using our RAG framework outperforms bare StockLLM, indicating that the RAG framework enhances LLM performance by incorporating relevant time-series data as financial context. However, RAG with retrieval models trained on text data does not always improve performance compared to bare StockLLM. In some cases, performance even declines. This suggests that these retrieval models cannot effectively identify beneficial time-series data that could enhance the models predictions, which highlights fundamental gap between text-based retrieval and time-series data. Furthermore, RAG framework with FinSeer consistently surpasses all other retrieval models, such as Instructor [22], BGE (BAAI General Embedding) [28], LLM-Embedder [39], UAE [14], and E5mistral-7b-instruct [24], demonstrating the effectiveness of our trained FinSeer in identifying relevant time-series financial data to enhance stock movement prediction. Further analysis of retrieval results reveals that FinSeer uniquely identifies relevant financial indicators that enhance LLM performance, while other retrievers predominantly select sequences based on superficial similarities, such as close price and adjusted close price. This behavior underscores critical limitation of text-trained models: they focus on surface-level patterns and fail to discern truly valuable time-series Enhancing Financial Time-Series Forecasting with Retrieval-Augmented Large Language Models Conference17, July 2017, Washington, DC, USA Figure 1: Overview of our time-series RAG framework. information. In contrast, FinSeer effectively bridges this gap, demonstrating its ability to uncover underlying relevance in financial data. Our contributions are summarized as follows: (1) We propose the first RAG framework for financial time-series forecasting, in which the retrieval mechanism features three key innovations: fine-tuned 1B parameter LLM (StockLLM) as the backbone, novel candidate selection method leveraging LLM feedback to identify beneficial sequences, and training objective that maximizes similarity between queries and historically significant sequences, enabling the retriever to uncover meaningful patterns while minimizing noise in complex financial data. (2) We construct new training and testing datasets to address the limitations of existing ones containing only price data, and better align with real-world economic analysis. Besides price data, these datasets integrate financial indicators that signal upward or downward trends, carefully selected through domain expert consultation and mutual information analysis. By using new training datasets, we train the retriever to uncover deeper patterns beyond surface-level trends during training. While the testing set, designed to mimic real-world scenarios, ensures robust evaluation of model performance in practical financial applications. (3) We propose new retriever, FinSeer, specifically designed for time-series forecasting, trained on newly constructed datasets that integrate financial indicators alongside historical stock prices. By leveraging LLM feedback and prioritizing historically significant sequences, FinSeer uncovers deeper patterns beyond surface-level trends, setting it apart from traditional methods. (4) Experimental results demonstrate that our RAG framework outperforms bare StockLLM and random retrieval, highlighting its effectiveness, while our retriever, FinSeer, surpasses existing RAG methods in stock movement prediction across three datasets, achieving an 8% higher accuracy on BIGDATA22 compared to general-purpose LLM-feedbackbased retriever. Furthermore, FinSeer retrieves more dynamic and impactful sequences, showcasing its ability to identify and leverage the most relevant patterns for forecasting."
        },
        {
            "title": "2 Problem Definition and Goals\nRetrieval-augmented financial time-series forecasting [19] involves\npredicting future values or trends (𝐺) based on a given query se-\nquence (𝑞) and a set of retrieved historical sequences (𝑐). These\nsequences are collected over time at regular intervals. In the re-\ntrieval process, the goal of the retrieval model (𝑅) is to efficiently\nidentify and extract the most useful historical sequences from a\nlarge pool of candidates. By providing relevant context, the retrieval\nmodel enhances the forecasting model’s ability to make accurate\nand reliable predictions.\nStock Movement Prediction Task. In the specific task of stock\nmovement prediction [34], the problem is framed as a binary classi-\nfication task: predicting whether a stock’s price will rise or fall on\nthe next trading day. Given a query sequence 𝑞, which represents\nthe stock’s price over the previous 𝑡 days, the model retrieves rele-\nvant sequences as context and predicts the stock’s movement 𝑀𝑞,𝑑\nfor the next trading day 𝑑. Table 1 defines the major symbols used\nin this paper.\nRise/Fall Threshold Settings. To classify daily movements as rise\nor fall, we first calculate returns 𝑅𝑡 , which represents the percentage",
            "content": "Conference17, July 2017, Washington, DC, USA Mengxi Xiao et al. Table 1: The definition of symbols. General Time-series Symbol 𝑞 𝐺 (𝑞) Stock Movement Prediction Symbol 𝑞 = {𝑞𝑑 𝑡 , ..., 𝑞𝑑 1} 𝐺 (𝑞, 𝑑) {rise, fall} 𝑃 (𝑐) = 𝐿𝐿𝑀 (𝑂 𝑞, 𝑐) 𝑃 (𝑐) = 𝐿𝐿𝑀 (𝑀𝑑 𝑞, 𝑐) CP = {𝑐𝑖 𝑖 = 1, . . . , 𝑘 } CN = {𝑐𝑖 𝑖 = 𝑘 + 1, . . . , 𝑛} 𝑤𝑖 𝑅 Definitions The query time-series data. In stock movement prediction, 𝑞 refers to the query stock price sequence of length 𝑡, containing stock price data from trading day 𝑑 𝑡 to trading day 𝑑 1. The final output 𝐺 given the query 𝑞. In stock movement prediction, 𝐺 (𝑞, 𝑑) shows the generation 𝐺 of the query stock 𝑞 on the query trading day 𝑑, belonging to rise or fall. The probability 𝑃 of the LLM to generate an accurate output 𝑂 given the query sequence 𝑞 and candidate sequence 𝑐. In stock movement prediction, 𝑃 (𝑐) refers to generating the accurate movement 𝑀 on the query trading day 𝑑. The set of top-k retrieved sequences as positive examples, where 𝑃 (𝑐𝑖 ) 𝑃 (𝑐𝑖+1). The set of negative retrieved sequences, where 𝑃 (𝑐𝑖 ) 𝑃 (𝑐𝑖+1). The soft weight of the 𝑖𝑡ℎ retrieved sequences, where 𝑤𝑖 = 𝑃 (𝑐𝑖 ), 𝑖 = 1, ..., 𝑘. The retrieve model. change in the closing price over consecutive days. 𝑅𝑡 = adj_close𝑑 adj_close𝑑 1 adj_close𝑑 1 100 (1) Following Yoo et al. [37] and Soun et al. [21], we classify the movement as rise if return exceeds 0.55, fall if it is below -0.5. In line with previous experimental settings [29, 31], we do not evaluate freeze cases. However, we include sequences with 𝑅𝑡 [0.5, 0.55] as freeze candidates in the candidate pool, ensuring diverse and comprehensive set of historical data for context. 𝑀𝑞,𝑑 = 𝑅𝑡 > 0.55 rise, 𝑅𝑡 < 0.5 fall, freeze, 0.5 𝑅𝑡 0.55 (2) Rationale for Unbalanced Thresholds. The asymmetrical thresholds for classifying daily movements (0.55 for rises and -0.5 for falls) reflect the inherent dynamics of stock market behavior. Stock prices typically rise gradually due to sustained investor optimism but fall sharply during panic selling or profit-taking. The stricter rise threshold prevents minor upward fluctuations from being misclassified as significant increases, while the more lenient fall threshold ensures meaningful downward trends are captured. This approach aligns with market realities, improving the reliability of movement classifications."
        },
        {
            "title": "3.1 Dataset Construction\nTo address the limitations of existing datasets and better align\nwith real-world economic analysis, we construct new datasets by\nintegrating financial indicators alongside historical stock prices, en-\nabling deeper insights into market behavior that stock prices alone\ncannot capture. This section details stock selection, raw data col-\nlection (Collection of Basic Price Indicators) and financial indicator\nselection process(Calculation and Selection of Financial Indicators).\nSpecifically, our datasets are built using high-trade-volume U.S.\nstocks, while the indicators are selected through domain expert\nconsultation and mutual information analysis.\nStock Selection. To construct our datasets, we select high-trade-\nvolume U.S. stocks across three periods: 2014-2015, 2017-2018, and\n2022-2023. The first two periods align with two benchmark datasets:\nfor 2014-2015, we use the same stocks as the ACL18 dataset [34],\nand for 2017-2018, we use the same stocks as the BIGDATA22\ndataset [21]. To incorporate more recent data, we manually select\nhigh-trade-volume stocks from 2022-2023 to create the STOCK23\ndataset. This ensures our training and evaluation reflect recent mar-\nket conditions and provide a robust benchmark for stock movement\nprediction.\nCollection of Basic Price Indicators. Stock data is collected using\nthe Yahoo Finance API [33], which provides basic price indicators\nsuch as the opening price, highest price, lowest price, adjusted\nclosing price, and trading volume for each trading day. The daily\nprice movement is then calculated as the reference answer for model\ntraining and evaluation.",
            "content": "We then divide the stock data into training, validation, and test sets. To ensure temporal integrity and prevent information leakage, we partition the dataset by stocks rather than by time. This approach maintains consistent time ranges across splits while ensuring that stocks in the validation and test sets are entirely unseen during training. Unlike time-based splits, stock-based partitioning provides more robust evaluation of the models ability to generalize to new stocks, better reflecting real-world scenarios where models are applied to unseen entities. Table 2 summarizes the stock counts for each split in our datasets. Calculation and Selection of Financial Indicators. In our datasets, we integrate financial indicators to provide critical insights into market behavior that stock prices alone cannot capture. Following analyzing methods in the book Machine learning for trading [10], we Enhancing Financial Time-Series Forecasting with Retrieval-Augmented Large Language Models Conference17, July 2017, Washington, DC, USA Table 2: Dataset statistics. Dataset Stock amount Trading dates train valid test all sequences query sequences ACL18 [34] BIGDATA22 [21] STOCK23 33 22 5 3 3 33 22 2014.06.02-2015.12.31 2015.06.03-2015.12.31 2019.04.01-2020.12.31 2020.04.09-2020.12.31 2022.01.03-2023.12.31 2023.01.03-2023.12. conduct an analysis using query sequences in all training sets to explore the relationship between commonly used financial indicators and returns. To measure the relationship between these indicators and returns, we utilize Mutual Information (MI), non-linear measure of dependency that quantifies the amount of information one variable contains about another. Specifically, the MI between an indicator 𝑋 and returns 𝑅𝑡 is calculated as: 𝐼 (𝑋 ; 𝑅𝑡 ) = 𝑥 𝑟 𝑝 (𝑥, 𝑟 ) log (cid:16) 𝑝 (𝑥,𝑟 ) 𝑝 (𝑥 )𝑝 (𝑟 ) (cid:17) 𝑑𝑥 𝑑𝑟, (3) where 𝑝 (𝑥, 𝑟 ) represents joint probability density function of 𝑋 (indicator) and 𝑅𝑡 (returns), 𝑝 (𝑥) shows marginal probability density function of 𝑋 , and 𝑝 (𝑟 ) is marginal probability density function of 𝑅𝑡 . We compute MI scores for each indicator and select the top-20 indicator groups with the highest scores as our candidates, with the detailed score ranking provided in Appendix B.1. Additionally, an example of data piece from our datastore is illustrated in Appendix A.1."
        },
        {
            "title": "3.2 Sequence Serialization\nSince stock movement prediction depends on the changes in related\nfeatures rather than their exact values, we serialize stock prices\nand financial indicators into a time-series format. We use JSON to\nrepresent these sequences, as it has been demonstrated to effectively\nsupport LLMs in interpreting time-series data [5, 20, 36].\nQuery Sequence Serialization We select query sequences where\nthe query date is at least one year after the start date of the cor-\nresponding split in the dataset. This ensures that each query has\nenough candidate sequences for retrieval. For example, in the ACL18\ndataset, which spans from 2014-06-02 to 2015-12-31, the query se-\nquences are defined as those from 2015-06-03 to 2015-12-31.",
            "content": "Each query sequence contains the adjusted close price from the previous five trading days along with basic stock information. The indicator is represented as five-day sequence, with one-day sliding window applied across trading days. For example, when inquerying about stock MO on 2015-06-02, the query sequences contains stock name, query date, last five trading dates and their corresponding adjusted close prices. The serialized sequence is shown below: { \" query_stock \": \" MO \" , \" query_date \" : \" 2015 -06 -02 \" , \" recent_date_list \" : [\" 2015 -05 -26 \" , \" 2015 -05 -27 \" , \" 2015 -05 -28 \" , \" 2015 -05 -29 \" , \" 2015 -06 -01 \" ], \" adjusted_close_list \": [29.669 , 29.9872 , 29.8657 , 29.6227 , 29.6227]} Candidate Sequence Serialization As new information becomes available, the candidate pool dynamically incorporates sequences from the most recent trading days. For instance, when the query date is 2015-06-03, the candidate pool includes sequences from 201406-02 to 2015-06-02. When the query date advances to 2015-06-04, the sequence for 2015-06-03 is added to the candidate pool without the need for additional training. This demonstrates the scalability of our method. For each candidate stock on specific date, we select 6 basic price indicators and 20 groups of financial indicators. Each candidate sequence includes typical financial indicator along with basic stock information. As the same as query serialization, the indicator is represented as five-day sequence, with one-day sliding window applied across trading days. For example, candidate represent stock MO on date 2014-07-02, with highest price as its indicator. The sequence includes basic information containing stock name, date, movement, last five trading dates and their corresponding highest prices. The serialized sequence is shown below: { \" candidate_stock \": \" MO \" , \" candidate_date \": \" 2014 -07 -02 \" , \" candidate_movement \": \" freeze \" , \" recent_date_list \": [\" 2014 -06 -25 \" , \" 2014 -06 -26 \" , \" 2014 -06 -27 \" , \" 2014 -06 -30 \" , \" 2014 -07 -01 \"], \" high_list \": [42.2 , 42.0 , 41.86 , 42.28 , 42.0]}"
        },
        {
            "title": "3.3 Retriever Training\nTo identify valuable information from vast amounts of sequences\nand uncover semantic relationships between them, we introduce a\nnovel retrieval mechanism featuring three key innovations: a fine-\ntuned 1B parameter LLM (StockLLM) as the backbone, a candidate\nselection method leveraging LLM feedback to identify beneficial se-\nquences and a training objective that maximizes similarity between\nqueries and historically significant sequences, enabling the trained\nretriever to uncover meaningful patterns while minimizing noise\nin complex financial data. While inspired by [39], our approach is\nuniquely tailored to financial time-series data, addressing the chal-\nlenges of relevance assessment and alignment in a domain where\ntraditional retrieval methods often fall short. This section details\nthe training process of our framework, including the training of the\nLLM backbone, candidate scoring and selection, and the training of\nthe retriever, which contains the training objective and knowledge\ndistillation.\nThe LLM Backbone. To activate the LLM’s inherent knowledge\nand ensure instruction-following capabilities, we fine-tune a 1B\nparameter LLM (LLaMA 3.2-1B-Instruct) [7] using the LoRA tech-\nnique for efficient low-rank adaptation, resulting in StockLLM.\nBy intentionally using a smaller backbone model, we establish a\nmore challenging experimental setup, ensuring that performance\nimprovements are attributable to FinSeer’s retrieval capabilities\nrather than the LLM’s capacity. The fine-tuning process is imple-\nmented using the LlamaFactory framework [40], with the following\nconfiguration: a learning rate of 5e-5, a cosine scheduler, gradient\naccumulation over 8 steps, mixed-precision (fp16) training, and\n5 epochs of training with regular evaluation to log metrics and\nselect the best-performing checkpoint. This setup demonstrates\nthe robustness of our framework in extracting meaningful insights\neven under constrained model size, highlighting the effectiveness\nof FinSeer in enhancing financial forecasting tasks.\nCandidate Scoring. To determine whether a candidate sequence\nassists in predicting the movement of the query, we use LLM feed-\nback to score each candidate. Specifically, for a given query 𝑞, we\nintegrate the query sequence and each candidate sequence 𝑐𝑖 from\nthe candidate pool as concurrent inputs to the LLM. The LLM out-\nputs logits, which are unnormalized scores representing the model’s",
            "content": "Conference17, July 2017, Washington, DC, USA Mengxi Xiao et al. confidence for each possible class (e.g., \"rise\" or \"fall\"). These logits are transformed into probabilities 𝑃 (𝑐) using the softmax function: 𝑃 (𝑐) = 𝑒𝑧𝑐 (cid:205)𝑗 𝑒𝑧 𝑗 , (4) where 𝑧𝑐 is the logit for the correct class (e.g., \"rise\" if the true movement is upward) and 𝑧 𝑗 represents the logits for all possible classes. The resulting probability 𝑃 (𝑐) serves as the score for the candidate 𝑐𝑖 with respect to the query 𝑞. Candidate Selection. We rank the candidate sequences in descending order based on their scores 𝑃 (𝑐). The top-1 sequence is selected as positive candidate, while the bottom 15 sequences are chosen as negative candidates. The sets of selected positive and negative sequences are denoted as CP and CN, respectively. Training Objective. Our retriever 𝑅(𝑞) is designed to intelligently distinguish between historically significant sequences CP and noisy sequences CN. The training objective is to ensure that 𝑅(𝑞) prioritizes sequences from CP while minimizing attention to those from CN. This is achieved by maximizing similarity measure 𝑠𝑢𝑝 (𝑞, 𝑠) between the query sequence 𝑞 and candidate sequences 𝑠. Mathematically, the retrievers objective is formulated as: 𝑠𝑢𝑝 (𝑞, 𝑠). 𝑅(𝑞) = arg max𝑠 CPCN (5) By focusing on sequences that maximize 𝑠𝑢𝑝 (𝑞, 𝑠), the retriever ensures that the most informative and contextually relevant historical sequences are identified. Knowledge Distillation. To leverage the scoring derived from the LLM, we employ knowledge distillation, which transfers knowledge from the teacher model (LLM) to the student model (retriever) by mimicking the teachers output distribution. This approach effectively captures nuanced patterns and predictions from the LLM. Specifically, we minimize the Kullback-Leibler (KL) divergence between the candidate distributions computed using the LLMs rewards and those predicted by the embedding model. For each query 𝑞 and its candidate list {CP, CN}, we derive the LLMs rewards for the candidates, denoted as {𝑃 (𝑐𝑖 ), 𝑖 = 1, ..., 𝑛}. To make these rewards suitable for distillation, we normalize them using softmax function with temperature 𝛼: 𝑤𝑖 = softmax𝑅 (cid:18) 𝑃 (𝑐𝑖 ) 𝛼 (cid:19) . The KL divergence is then computed as follows: min 𝑐 𝑤𝑖 log (cid:32) exp (cid:0)(cid:10)𝒆𝑞, 𝒆𝑐𝑖 (cid:11) /𝜏 (cid:1) (cid:205)𝑐 exp (cid:0)(cid:10)𝒆𝑞, 𝒆𝑐 (cid:11) /𝜏 (cid:1) (cid:33) , (6) (7) where 𝒆𝑞 and 𝒆𝑐𝑖 are the embeddings of the query 𝑞 and candidate 𝑐𝑖 , respectively, and 𝜏 is temperature parameter. This loss function optimizes the similarity between the query embedding and the embeddings of the top-ranked candidates, enhancing the retrievers ability to accurately predict stock price movements."
        },
        {
            "title": "3.4 Inference\nThe inference process of our RAG framework, built on FinSeer, con-\nsists of three key steps to enhance stock movement prediction. First,\nFinSeer retrieves relevant sequences from the candidate pool, lever-\naging its ability to identify historically significant patterns. Second,\nthese retrieved sequences are incorporated into StockLLM’s input",
            "content": "context, enriching the models understanding of temporal dynamics. Finally, the enriched context is combined with the original query to generate forecasts. By integrating salient historical patterns with current data, this approach bridges the gap between past trends and present conditions, significantly improving prediction accuracy and robustness in real-world financial scenarios."
        },
        {
            "title": "4.2 Main Results\nTable 3 summarizes the performance of stock movement prediction\nacross the three datasets.",
            "content": "We analyze our framework and retriever separately to evaluate their contributions. First, our RAG framework outperforms both bare StockLLM and random retrieval across all datasets, demonstrating its effectiveness in enhancing stock movement prediction. Second, among all evaluated retrieval methods, FinSeer consistently achieves the best performance when integrated with StockLLM, showcasing its ability to retrieve valuable sequences that improve forecasting accuracy. Notably, FinSeer is the only retriever that Enhancing Financial Time-Series Forecasting with Retrieval-Augmented Large Language Models Conference17, July 2017, Washington, DC, USA Table 3: Results of stock movement predictions using LLMs and retrieval models. The asterisk (*) indicates the LLM employed while using retrieval models. Retrieving Methods ACL BIGDATA22 STOCK23 (+ StockLLM) ACC MCC ACC MCC ACC MCC w/o Retrieval Random Retrieve DTW Instructor UAE BGE LLM Embedder 0.496 0.481 0.518 0. 0.482 0.510 0.489 0.499 -0.007 -0. 0.045 -0.008 -0.036 0.028 -0.019 0. 0.497 0.521 0.505 0.497 0.495 0. 0.487 0.459 -0.008 0.045 0.021 -0. -0.008 0.008 -0.026 -0.083 0.509 0. 0.492 0.505 0.494 0.499 0.488 0. 0.021 -0.004 -0.007 0.010 -0.009 -0. -0.014 0.007 FinSeer 0.521 0.042 0. 0.079 0.542 0.085 achieves stable improvements over bare StockLLM on all datasets, while other methods exhibit inconsistent results, sometimes enhancing predictions and other times degrading them. Furthermore, our experiments reveal that incorporating retrieval process does not always guarantee performance improvements, as retrieval errors can mislead the LLM and harm predictions. This underscores the critical need for retrieval models that align closely with the LLMs objectives, challenge that FinSeer effectively addresses through its tailored design. The minimal performance difference between the instructionfinetuned retriever (Instructor) and the no-retrieval baselines highlights the inherent challenge of time-series retrieval. Unlike text retrieval, time-series retrieval does not solely rely on task understanding, as candidate sequences often appear similar, making it difficult to distinguish valuable ones. The angle-optimized retriever (UAE) shows inconsistent performance across datasets, as its ability to differentiate similarity between query and candidate sequences does not fully align with the LLMs perception of importance. Similarly, LLM Embedder, our backbone model trained with LLM feedback, struggles to generalize to time-series retrieval tasks, further underscoring the complexity of the problem. The best-performing baseline is E5, which achieves competitive results despite its significantly larger model size and embedding dimensions (five times larger than FinSeer). However, FinSeer outperforms E5 on all datasets. This demonstrates that FinSeer successfully learns the preferences of LLMs and effectively enhances their time-series forecasting ability through retrieval, even under challenging conditions. In summary, while RAG methods do not guarantee performance improvements due to the risk of retrieval errors, FinSeer stands out by consistently retrieving high-quality sequences that align with the LLMs forecasting objectives, leading to superior results compared to other retrievers."
        },
        {
            "title": "4.3 Ablation Study\nIndicator Occurrences In this part, we analyze what indicators all\nRAG models retrieve. We calculate indicator occurrences on ACL18\ndataset and the results are shown in Figure 2.",
            "content": "As shown in the figure, FinSeer is the only model that successfully extracts diverse and comprehensive set of indicators while achieving superior performance. This clearly demonstrates its advanced temporal retrieval capabilities. Specifically, while other models like LLM Embedder and Instructor predominantly focus on basic indicators such as close price and adjusted close price, FinSeer effectively identifies and retrieves wide range of technical indicators, including kdj crossover, MACD Histogram, Bollinger Bands, and various alpha factors. This richer set of retrieved indicators provides FinSeer with more comprehensive auxiliary information, enabling more accurate and reliable predictions. Candidate Movement Correlation In this section, we investigate whether the LLM relies on the movement trends of retrieved sequences to make predictions. To this end, we compute the correlation between the movements of retrieved sequences and the LLMs generated results. The detailed calculation process is provided in Appendix B.2. The results in Table 4 reveal no significant correlation between the movement direction (rise or fall) of the retrieved candidate sequences and the final predictions of StockLLM. This finding indicates that StockLLM does not simply mirror the movement trends of the retrieved sequences but instead analyzes their specific content to infer the querys movement direction. This ability highlights the LLMs capacity to extract meaningful insights from complex time-series data. Table 4: Correlation between the movements of retrieved sequences and LLMs generated results. Dataset ACL BigData22 STOCK23 RAG Model Instructor UAE E5 BGE LLM Embedder FinSeer Instructor UAE E5 BGE LLM Embedder FinSeer Instructor UAE E5 BGE LLM Embedder FinSeer Correlation -0.098 -0.068 -0.108 0.175 0.014 0. P-Value 0.000 0.000 0.000 0.000 0.457 0.004 -0.072 0.009 -0.040 0.093 0.060 0.077 -0.114 -0.093 -0.064 0.131 0.015 0.032 0.000 0.633 0.032 0.000 0.001 0.000 0.000 0.000 0.000 0.000 0.350 0.041 This observation underscores the critical role of retrieval quality in RAG models, as the relevance and informativeness of retrieved sequences directly influence prediction performance. Notably, StockLLM is relatively small 1B parameter LLM, making this particularly challenging setting. Despite this, our time-series-tailored RAG model, FinSeer, successfully enhances StockLLMs performance by retrieving rich and relevant time-sensitive information. This demonstrates the effectiveness of our approach in improving prediction accuracy and showcases its potential for financial time-series forecasting."
        },
        {
            "title": "4.4 Case Study\nThis case study illustrates the critical importance of alignment be-\ntween the retriever and the LLM’s forecasting preferences in finan-\ncial time-series analysis. We examine the stock XOM on 2015-06-25",
            "content": "Conference17, July 2017, Washington, DC, USA Mengxi Xiao et al. Figure 2: Indicator occurrences of different RAG models on ACL18 dataset. from the ACL18 dataset, where the adjusted close price exhibited pronounced downward trend. The query sequence is as follows: { \" query_stock \": \" XOM \" , \" query_date \" : \" 2015 -06 -25 \" , \" recent_date_list \" : [\" 2015 -06 -18 \" , \" 2015 -06 -19 \" , \" 2015 -06 -22 \" , \" 2015 -06 -23 \" , \" 2015 -06 -24 \" ], \" adjusted_close_list \": [58.0813 , 57.8979 , 57.8707 , 57.8027 , 57.5377]} While multiple retrievers were evaluated, only FinSeer successfully enabled StockLLM to predict the correct movement as fall. Specifically, FinSeer retrieved five diverse indicatorsclose price, adjusted close price, alpha021, alpha054, and the highest priceproviding comprehensive view of the stocks behavior. Alpha021 identifies trends based on shortand long-term price averages and volume conditions, while alpha054 combines price and volume rankings to assess performance within specific time window. These indicators allowed StockLLM to accurately assess whether the downward trend would persist or reverse, demonstrating the value of retrieving contextually relevant and diverse features. In contrast, other retrievers, such as Instructor, BGE, LLM Embedder, and E5, extracted sequences dominated by close or adjusted close prices, all reflecting similar downward trends. While these sequences aligned with the current trend, they failed to provide actionable insights for forecasting future movements, leading StockLLM to misinterpret them as noise and incorrectly predict rise. Similarly, UAE retrieved sequences indicating overbought and oversold conditions, including three rise and two freeze trends. Although overbought signals often suggest potential downturn, the retrieved sequences themselves exhibited rising or frozen trends, confusing StockLLM and resulting in an erroneous prediction. This case study underscores the superiority of FinSeer in retrieving meaningful and diverse indicators that align with the LLMs forecasting logic, enabling more accurate and reliable predictions."
        },
        {
            "title": "4.5 Indicator Sequence Visualization\nTo intuitively examine how FinSeer embeds indicator sequences,\nwe use LargeVis [23] to reduce the embeddings to two dimensions.",
            "content": "In Figure 3, we visualize3 the vector space of nine indicators, including adjusted close price, open price, high price, low price, close price, volume, movement, returns, and WAP. The results show that the green points, representing movement information, are wellclustered, indicating that FinSeer effectively captures meaningful patterns. However, this also suggests that deeper relationships between stock movements and indicators remain to be explored, highlighting the potential for further research in this area. Figure 3: Visualization of indicator sequence embeddings on ACL18 dataset."
        },
        {
            "title": "5 Related Work\n5.1 Stock Movement Prediction\nNon-LLM Methods. Traditional approaches to stock movement\nprediction have focused on various aspects of financial data. One\nprominent category of methods analyzes stock price sequences and\ntheir corresponding technical indicators [6, 18] to identify patterns\nin historical data for predicting future movements. However, due\nto the complexity of factors influencing stock prices, subsequent\nmethods have incorporated additional contextual information, such",
            "content": "3https://medviz.org/app/ Enhancing Financial Time-Series Forecasting with Retrieval-Augmented Large Language Models Conference17, July 2017, Washington, DC, USA as news articles [3, 16] or social media posts [25, 34]. Despite efforts, these methods are highly susceptible to noise and struggle to analyze the vast and diverse nature of financial information effectively. LLM-based Methods. Recent studies have explored using LLMs for financial prediction tasks, either by fine-tuning open-source models or prompting advanced models like GPT-4. However, even stateof-the-art models, including GPT-4, have achieved only randomguessing-level accuracy in stock movement prediction [2931]. This highlights the inherent challenges in identifying and analyzing meaningful patterns in domain as volatile and multifaceted as stock market prediction."
        },
        {
            "title": "6 Conclusion\nIn this work, we introduce the first RAG framework for finan-\ncial time-series forecasting, featuring a novel retrieval mechanism\nwith three key innovations: a fine-tuned 1B parameter LLM (Stock-\nLLM), a candidate selection method leveraging LLM feedback, and\na training objective that maximizes similarity between queries and\nhistorically significant sequences. We also construct new datasets\nintegrating financial indicators and historical stock prices, selected\nthrough expert consultation and mutual information analysis, to\ntrain the retriever and ensure robust evaluation. Our retriever, Fin-\nSeer, leverages these datasets and LLM feedback to uncover deeper\npatterns beyond numeric similarities. Experiments demonstrate\nthat our RAG framework outperforms bare StockLLM and ran-\ndom retrieval, while FinSeer surpasses existing retrieval methods,",
            "content": "achieving an 8% higher accuracy on BIGDATA22 and retrieving more impactful sequences. This work highlights the importance of tailored retrieval models in financial forecasting and provides robust framework for future research. References [1] Oscar Bustos and Alexandra Pomares-Quimbaya. 2020. Stock market movement forecast: systematic review. Expert Systems with Applications 156 (2020), 113464. [2] Lingxi Cui, Huan Li, Ke Chen, Lidan Shou, and Gang Chen. 2024. Tabular data augmentation for machine learning: Progress and prospects of embracing generative ai. arXiv preprint arXiv:2407.21523 (2024). [3] Xiao Ding, Yue Zhang, Ting Liu, and Junwen Duan. 2015. Deep learning for event-driven stock prediction. In Twenty-fourth international joint conference on artificial intelligence. [4] Eugene F. Fama and Kenneth R. French. 2000. Forecasting Profitability and Earnings. The Journal of Business 73, 2 (2000), 161175. [5] Xi Fang, Weijie Xu, Fiona Anting Tan, Jiani Zhang, Ziqing Hu, Yanjun Qi, Scott Nickleach, Diego Socolinsky, Srinivasan Sengamedu, and Christos Faloutsos. 2024. Large Language Models on Tabular DataA Survey. arXiv preprint arXiv:2402.17944 (2024). [6] Fuli Feng, Huimin Chen, Xiangnan He, Ji Ding, Maosong Sun, and Tat-Seng Chua. 2018. Enhancing stock movement prediction with adversarial training. arXiv preprint arXiv:1810.09936 (2018). [7] Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, et al. 2024. The Llama 3 Herd of Models. arXiv:2407.21783 [cs.AI] https://arxiv.org/abs/2407.21783 [8] Xiaoxin He, Yijun Tian, Yifei Sun, Nitesh Chawla, Thomas Laurent, Yann LeCun, Xavier Bresson, and Bryan Hooi. 2024. G-retriever: Retrieval-augmented generation for textual graph understanding and question answering. arXiv preprint arXiv:2402.07630 (2024). [9] Jie Huang and Kevin Chen-Chuan Chang. 2022. Towards reasoning in large language models: survey. arXiv preprint arXiv:2212.10403 (2022). [10] Stefan Jansen. 2020. Machine Learning for Algorithmic Trading: Predictive models to extract signals from market and alternative data for systematic trading strategies with Python. Packt Publishing Ltd. [11] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. 2023. Mistral 7B. arXiv preprint arXiv:2310.06825 (2023). [12] Ming Jin, Shiyu Wang, Lintao Ma, Zhixuan Chu, James Zhang, Xiaoming Shi, Pin-Yu Chen, Yuxuan Liang, Yuan-Fang Li, Shirui Pan, et al. 2023. Time-llm: Time series forecasting by reprogramming large language models. arXiv preprint arXiv:2310.01728 (2023). [13] Pankaj Joshi, Aditya Gupta, Pankaj Kumar, and Manas Sisodia. 2024. Robust multi model rag pipeline for documents containing text, table & images. In 2024 3rd International Conference on Applied Artificial Intelligence and Computing (ICAAIC). IEEE, 993999. [14] Xianming Li and Jing Li. 2023. AnglE-optimized Text Embeddings. arXiv preprint arXiv:2309.12871 (2023). [15] Chenxi Liu, Qianxiong Xu, Hao Miao, Sun Yang, Lingzheng Zhang, Cheng Long, Ziyue Li, and Rui Zhao. 2024. TimeCMA: Towards LLM-Empowered Time Series Forecasting via Cross-Modality Alignment. arXiv preprint arXiv:2406.01638 (2024). [16] Qikai Liu, Xiang Cheng, Sen Su, and Shuguang Zhu. 2018. Hierarchical complementary attention network for predicting stock price movements with news. In Proceedings of the 27th ACM International Conference on Information and Knowledge Management. 16031606. [17] Brian Matthews. 1975. Comparison of the predicted and observed secondary structure of T4 phage lysozyme. Biochimica et Biophysica Acta (BBA)-Protein Structure 405, 2 (1975), 442451. [18] Yao Qin, Dongjin Song, Haifeng Chen, Wei Cheng, Guofei Jiang, and Garrison Cottrell. 2017. dual-stage attention-based recurrent neural network for time series prediction. arXiv preprint arXiv:1704.02971 (2017). [19] Omer Berat Sezer, Mehmet Ugur Gudelek, and Ahmet Murat Ozbayoglu. 2020. Financial time series forecasting with deep learning: systematic literature review: 20052019. Applied soft computing 90 (2020), 106181. [20] Ananya Singha, José Cambronero, Sumit Gulwani, Vu Le, and Chris Parnin. 2023. Tabular representation, noisy operators, and impacts on table structure understanding tasks in LLMs. arXiv preprint arXiv:2310.10358 (2023). [21] Yejun Soun, Jaemin Yoo, Minyong Cho, Jihyeong Jeon, and Kang. 2022. Accurate Stock Movement Prediction with Self-supervised Learning from Sparse Noisy Tweets. In 2022 IEEE International Conference on Big Data (Big Data). IEEE, 1691 1700. [22] Hongjin Su, Weijia Shi, Jungo Kasai, Yizhong Wang, Yushi Hu, Mari Ostendorf, Wen-tau Yih, Noah A. Smith, Luke Zettlemoyer, and Tao Yu. 2023. One Embedder, Any Task: Instruction-Finetuned Text Embeddings. In ACL (Findings). Association Conference17, July 2017, Washington, DC, USA Mengxi Xiao et al. for Computational Linguistics, 11021121. [23] Jian Tang, Jingzhou Liu, Ming Zhang, and Qiaozhu Mei. 2016. Visualizing largescale and high-dimensional data. In Proceedings of the 25th international conference on world wide web. 287297. [24] Liang Wang, Nan Yang, Xiaolong Huang, Linjun Yang, Rangan Majumder, and Furu Wei. 2023. Improving text embeddings with large language models. arXiv preprint arXiv:2401.00368 (2023). [25] Huizhe Wu, Wei Zhang, Weiwei Shen, and Jun Wang. 2018. Hybrid deep sequential modeling for social text-driven stock prediction. In Proceedings of the 27th ACM international conference on information and knowledge management. 16271630. [26] Mingyuan Wu and Xiaotian Diao. 2015. Technical analysis of three stock oscillators testing MACD, RSI and KDJ rules in SH & SZ stock markets. In 2015 4th International Conference on Computer Science and Network Technology (ICCSNT), Vol. 1. IEEE, 320323. [27] Shitao Xiao, Zheng Liu, Yingxia Shao, and Zhao Cao. 2022. RetroMAE: PreTraining Retrieval-oriented Language Models Via Masked Auto-Encoder. In EMNLP. https://arxiv.org/abs/2205.12035 [28] Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-Pack: Packaged Resources To Advance General Chinese Embedding. arXiv:2309.07597 [cs.CL] [29] Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, et al. 2024. The finben: An holistic financial benchmark for large language models. arXiv preprint arXiv:2402.12659 (2024). [30] Qianqian Xie, Weiguang Han, Yanzhao Lai, Min Peng, and Jimin Huang. 2023. The wall street neophyte: zero-shot analysis of chatgpt over multimodal stock movement prediction challenges. arXiv preprint arXiv:2304.05351 (2023). [31] Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023. PIXIU: Large Language Model, Instruction Data and Evaluation Benchmark for Finance. arXiv preprint arXiv:2306.05443 (2023). [32] Qianqian Xie, Dong Li, Mengxi Xiao, Zihao Jiang, Ruoyu Xiang, Xiao Zhang, Zhengyu Chen, Yueru He, Weiguang Han, Yuzhe Yang, et al. 2024. Open-finllms: Open multimodal large language models for financial applications. arXiv preprint arXiv:2408.11878 (2024). [33] Selene Yue Xu and CU Berkely. 2014. Stock price forecasting using information from Yahoo finance and Google trend. UC Brekley (2014), 122. [34] Yumo Xu and Shay Cohen. 2018. Stock movement prediction from tweets and historical prices. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers). 19701979. [35] Silin Yang, Dong Wang, Haoqi Zheng, and Ruochun Jin. 2024. TimeRAG: BOOSTING LLM Time Series Forecasting via Retrieval-Augmented Generation. arXiv preprint arXiv:2412.16643 (2024). [36] Yuwei Yin, Yazheng Yang, Jian Yang, and Qi Liu. 2023. FinPT: Financial Risk Prediction with Profile Tuning on Pretrained Foundation Models. arXiv preprint arXiv:2308.00065 (2023). [37] Jaemin Yoo, Yejun Soun, Yong-chan Park, and Kang. 2021. Accurate multivariate stock movement prediction via data-axis transformer with multi-level contexts. In Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining. 20372045. [38] Xinli Yu, Zheng Chen, Yuan Ling, Shujing Dong, Zongyi Liu, and Yanbin Lu. 2023. Temporal Data Meets LLMExplainable Financial Time Series Forecasting. arXiv preprint arXiv:2306.11025 (2023). [39] Peitian Zhang, Shitao Xiao, Zheng Liu, Zhicheng Dou, and Jian-Yun Nie. 2023. Retrieve anything to augment large language models. arXiv preprint arXiv:2310.07554 (2023). [40] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations). Association for Computational Linguistics, Bangkok, Thailand. http://arxiv.org/ abs/2403.13372 Data examples A.1 Examples in the Datastore An example in our datastore is shown below. { \" stock_name \": \" ABBV \" , \" query_date \": \" 2014 -06 -05 \" , \" query_movement \":\" rise \" , \" open \" :54.549999 , \" high \" :55.32 , \" low \" :54.360001 , \" close \" :55.299999 , \" adj_close \" :36.980961 , \" volume \" :4847300 , \" MACD_Histogram \" :0.0621141602 , \" macd_crossover \": null , \" bollinger_bands \": null , \" exceeding_upper \": null , \" exceeding_lower \": null , \" overbought_and_oversold_conditions \": null , \" kdj_crossover \": null , \" Returns \" :0.0131913094 , \" VWAP \" :54.9933333333 , \" alpha_smr \" :0.000549858 , \" alpha_mom \" :0.0121661224 , \" alpha002 \": null , \" alpha006 \": null , \" alpha009 \" : -0.481476 , \" alpha012 \" : -0.481476 , \" alpha021 \" :1 , \" alpha023 \" :0.0 , \" alpha024 \" : -0.769035 , \" alpha028 \": null , \" alpha032 \": null , \" alpha041 \" : -0.1554335256 , \" alpha046 \" : -0.481476 , \" alpha049 \" : -0.481476 , \" alpha051 \" : -0.481476 , \" alpha053 \": null , \" alpha054 \" :126.4264005641 , \" alpha101 \" : -18.282056485 } Addtional Experimental Results B.1 Score Calculation of Financial Indicators We compute the MI scores for each indicator using nonparametric estimation methods provided by mutual_info_regression from the Scikit-learn library. Then we normalize them into range (0, 1). These scores provide insight into the strength of the dependency between each financial indicator and the forward return. Higher MI scores indicate stronger relationships, highlighting which indicators are most predictive of future price movements. We select the top-20 indicator groups with the highest MI scores in our candidates, as is shown in Figure 4. Figure 4: Mutual Information(MI) scores between indicators and forward return. B.2 Candidate Movement Correlation For each query with retrieved candidates, we convert the movements of retrieved sequences and LLMs prediction to numerical Enhancing Financial Time-Series Forecasting with Retrieval-Augmented Large Language Models Conference17, July 2017, Washington, DC, USA representations. final prediction of the query is recorded as another numerical value: 𝑀𝑐 𝑗 , 𝑀𝑞 = 1, 0, -1, Then, we calculate the average value of the 𝑀𝑐𝑖 from the five retrieved sequences to represent the RAG-provided result, and the rise freeze fall 𝑀𝑐 𝑗 , 4 𝑥𝑖 = 1 5 𝑗=0 𝑦𝑖 = 𝑀𝑞𝑖 . Lastly, we adopt the Pearson correlation coefficient to compute the correlation: 𝑟 = (cid:205)(𝑥𝑖 𝑥)(𝑦𝑖 𝑦) (cid:205)(𝑥𝑖 𝑥)2 (cid:205)(𝑦𝑖 𝑦)2 ."
        }
    ],
    "affiliations": [
        "School of Computer Science, Wuhan University",
        "Yale University"
    ]
}