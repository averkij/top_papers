{
    "paper_title": "PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design",
    "authors": [
        "Jiazhe Wei",
        "Ken Li",
        "Tianyu Lao",
        "Haofan Wang",
        "Liang Wang",
        "Caifeng Shan",
        "Chenyang Si"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Graphic design forms the cornerstone of modern visual communication, serving as a vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, a framework that advances layout reasoning and controllable editing for professional graphic design. Specifically, we introduce a progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop a complete workflow that couples the trained LMM-based design model with generative models, enabling layer-controllable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geometrically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 2 8 0 4 0 . 2 1 5 2 : r PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design Jiazhe Wei1, Ken Li1, Tianyu Lao2, Haofan Wang2, Liang Wang1,3, Caifeng Shan1, Chenyang Si1 3 Institute of Automation, Chinese Academy of Sciences 1PRLab, Nanjing University 2LibLib.ai Project Page: https://postercopilot.github.io/ Figure 1. Generated results from our PosterCopilot. PosterCopilot exhibits exceptional graphic design capabilities by creating artworks with professional-grade layout, compelling visuals, and cohesive themes."
        },
        {
            "title": "Abstract",
            "content": "Graphic design forms the cornerstone of modern visual communication, serving as vital medium for promoting cultural and commercial events. Recent advances have explored automating this process using Large Multimodal Models (LMMs), yet existing methods often produce geometrically inaccurate layouts and lack the iterative, layer-specific editing required in professional workflows. To address these limitations, we present PosterCopilot, framework that advances *Equal Contribution Corresponding author (chenyang.si@nju.edu.cn) layout reasoning and controllable editing for professional graphic design. Specifically, we introduce progressive three-stage training strategy that equips LMMs with geometric understanding and aesthetic reasoning for layout design, consisting of Perturbed Supervised Fine-Tuning, Reinforcement Learning for Visual-Reality Alignment, and Reinforcement Learning from Aesthetic Feedback. Furthermore, we develop complete workflow that couples the trained LMMbased design model with generative models, enabling layercontrollable, iterative editing for precise element refinement while maintaining global visual consistency. Extensive experiments demonstrate that PosterCopilot achieves geomet1 rically accurate and aesthetically superior layouts, offering unprecedented controllability for professional iterative design. 1. Introduction Graphic design serves as fundamental medium for visual communication [36], translating abstract ideas into clear and engaging visuals. It brings together images, text, and graphic elements in deliberate way to create layouts that are both informative and visually appealing, bridging creativity with effective communication [1]. Recently, growing interest has emerged in automating the graphic design process through artificial intelligence. One major line of work explores diffusion-based generative models, which leverage their strong image synthesis capabilities to create visually rich posters [17, 31, 55]. However, because these models generate all image regions simultaneously, they struggle to preserve the structural integrity, texture fidelity, and stylistic consistency of user-provided assets, making local refinements prone to distortion [16, 35]. Another line leverages Large Multimodal Models (LMMs) to reason over design elements and predict their spatial and layer-wise arrangements, determining each elements position, scale, and ordering within the composition [18, 43, 57]. These methods preserve the authenticity of visual assets and introduce interpretability and controllability into the design process, representing promising step toward layout-centric and automationoriented graphic design. Despite these advances, current LMM-based methods still exhibit notable limitations when applied to professional design workflows: 1) when handling complex and numerous assets, existing methods often produce inaccurate and unaesthetic layouts [34, 56] as shown in Fig. 2. We identify that existing methods rely on supervised fine-tuning (SFT) over discrete textual tokens to represent continuous spatial coordinates, creating mismatch between the models symbolic representation and the true Euclidean geometry of layout design [24, 30]. This mismatch leads to misalignment, distortion, and suboptimal compositions. Moreover, these models lack visual feedback during training, which limits their ability to perceive and reason about aesthetic layouts [43, 44]. 2) More critically, current LMM-based approaches merely generate initial drafts and lack interactive editing capabilities [26, 37]. However, professional designers refine the drafts through multiple rounds of precise, layer-specific adjustments [3, 20, 39]. Therefore, enabling iterative refinement is crucial requirement for advancing AI-assisted graphic design toward practical applications [25, 40]. To address these challenges, we propose PosterCopilot, which advances the field toward layout reasoning and controllable editing for professional graphic design. Specifically, to mitigate the inaccurate and unaesthetic layouts resulting Figure 2. Some failure cases created by existing design models in real-world, multi-asset scenarios, producing severe misalignments and visual discord. from token-based coordinate representations, we propose Perturbed Supervised Fine-Tuning (PSFT), which reformulates coordinate regression into distribution-based learning paradigm by introducing controlled perturbations to groundtruth coordinates. Compared to point-wise regression, learning coordinate distribution allows the model to reason over continuous spatial relationships rather than memorizing discrete positions [29, 61], leading to more coherent and aesthetically balanced layouts. To further address the lack of visual feedback and aesthetic understanding, we introduce two-stage reinforcement learning (RL) strategy. In the first stage, Reinforcement Learning for Visual-Reality Alignment (RL-VRA) introduces verifiable geometric reward signals to explicitly correct residual spatial inaccuracies after SFT. In the second stage, Reinforcement Learning from Aesthetic Feedback (RLAF) employs learned aesthetic reward model to encourage the model to generate aesthetically coherent and diverse compositions that extend beyond the ground truth. Finally, to enable iterative and controllable refinement beyond initial generation, we develop complete workflow that couples the trained LMM-based design model with the generative models, seamlessly integrating asset creation with precision editing. This workflow supports layer-specific, iterative editing, allowing precise modification of individual elements while maintaining global visual consistency. It empowers designers with multi-round, high-fidelity editing capabilities, enabling flexible adjustments to specific layers without altering surrounding content. Experimental results indicate that the design model trained via our three-stage method produces layouts that are both accurate and visually appealing, even rivaling or surpassing the Nano-Banana. More significantly, PosterCopilots integration of generative agent provides precise layer-wise editing. This transforms it into powerful assistant, allowing designers to take well-composed draft as starting point and have it further optimized for enhanced aesthetics and practical application. Our main contributions are summarized as follows: 2 We propose PosterCopilot, the first framework to decouple complex poster design into layout reasoning and multiround lossless editing, demonstrating exceptional capabilities in both aspects. We introduce progressive alignment training paradigm (PSFT, RL-VRA, RLAF) that enables LMMs to reason over continuous spatial relationships while instilling design principles and human aesthetics. We design generative agent that supports iterative, controllable refinement beyond the initial generation, empowering PosterCopilot to serve as powerful assistant for real-world editing scenarios. We contribute large-scale, high-quality multi-layer poster dataset with rational granularity, along with its construction pipeline, addressing critical gaps in data scarcity and layer segmentation to benefit future research and applications. 2. Related Work Multi-layer Graphic Layout Planning prioritizes real-world practicality by first inferring layouts, then assembling layers for optimal flexibility. LMM-assisted approaches (LayoutPrompter [32], LayoutNUWA [52], PosterLLaVA [63]) employed in-context learning, while others specialized in asset integration (Graphist [9]), typography (POSTA [4]), or external generation (CreatiPoster [68], COLE [23]). Crucially, these methods mimic static datasets rather than learning from aesthetic outcomes. Our approach transcends limitations by internalizing layout principles and visual aesthetics through direct generative feedback. More discussion is in supplementary material. 3. Methodology In this section, we will first detail the training paradigm for the design model, and subsequently present the complete PosterCopilot pipeline. Our three-stage design model training paradigm is illustrated in Fig. 3. 3.1. Task Formulation Our objective is to automatically arrange user-provided elements = {e1, . . . , eN } of types = {image, text, shape} on canvas, achieving aesthetic coherence while preserving asset fidelity. Text elements are rasterized into image layers for unified processing. The input elements and canvas dimensions (Hc, Wc) are encoded into multimodal prompt , which our design model processes to generate the final layout: Hc,Wc M(P Hc,Wc ) (1) where = {(bi, li)}N box bi and layer order li. i=1 specifies each elements bounding (a) Perturbed Supervised Fine-Tuning (PSFT) (b) Reinforcement Learning for Visual-Reality Alignment (RL-VRA) (c) Reinforcement Learning from Aesthetic Feedback (RLAF) Figure 3. Overview of the training paradigm of PosterCopilot. Rather than formulating the training process as simple regression task, we endow PosterCopilot with outstanding layout capabilities and human-like aesthetics through three-stage training paradigm. 3.2. Perturbed Supervised Fine-Tuning We posit that the standard LMM practice of quantizing continuous coordinates into discrete text tokens fundamentally warps the optimization spaces geometry [11, 19, 48], hindering precise localization. To validate this, we visualize the local geometric uniformity using det(S), the determinant of the Structure Tensor [2, 15]. As shown in Fig. 4, the ideal Euclidean space (a) has det(S) 1, whereas the text-represented numerical space (b) is geometrically broken. Critically, (c) confirms that neighborhood averagingour core insighteffectively repairs this distortion and recovers stable optimization signal. Based on this finding, we propose Perturbed Supervised Fine-Tuning (PSFT). Instead of point-wise regression on ground-truth layout Ggt = {(bi, li)}N i=1, we sample perturbed variants G(i) pert by injecting Gaussian noise specifically on the bounding box values bi: G(i) pert (Ggt, σ2I), = 1, 2, . . . , (2) 3 Figure 4. Geometric instability of text-based coordinate representations. (a) Euclidean Space: The ideal baseline, showing perfect, uniform geometry (det(S) 1). (b) Text-Based Space: Suffers from signal collapse (near-zero det(S)) and geometric noise, creating chaotic landscape unstable for optimization. (c) Reconstructed Space via Neighborhood Averaging: This method suppresses noise, recovering smooth, uniform geometry that is far more stable than (b). Figure 5. Our motivation for visual-reality alignment and aesthetic feedback stems from the observation that design models frequently produce works that violate fundamental graphic design principles, as well as exhibit serious aesthetic flaws. We use red, green, and blue boxes to mark the error areas in the figure. where σ is small standard deviation. Our training objective, LPSFT, combines the standard cross-entropy loss on the original layout with an averaged loss over perturbations: LPSFT = LCE( ˆG, Ggt) 1 + λPerturbed (cid:88) i=1 LCE( ˆG, G(i) pert) (3) where ˆG is the models prediction. This formulation compels the model to learn continuous spatial distribution centered on the ground truth, rather than memorizing discrete token positions, thereby mitigating the limitations of text-based regression. 3.3. Reinforcement Learning for Visual-Reality"
        },
        {
            "title": "Alignment",
            "content": "While PSFT offers robust spatial prior, its dependence on supervised learning without visual feedback results in geometric flaws, such as bounding box drift and aspect ratio distortion. Critically, these rendering-stage errors, evident in Fig. 5, cannot be captured easily within the SFT paradigm itself. To bridge this visual-reality gap and align model outputs with graphic design principles, we introduce the Reinforcement Learning for Visual-Reality Alignment (RL-VRA) phase. We frame RL-VRA as an online policy optimization task under single-step Markov Decision Process (MDP). The state corresponds to the input prompt , while the action represents the layout generation = {(bi, li)}N i=1. Our objective is to refine the pre-trained SFT policy πref(G s) into an enhanced policy πθ(G s) by maximizing the expected return under geometry-aware reward signal: Hc,Wc JV RA(θ) = EGπθ(s) [r(G)] βDKL(πθ( s) πref( s)) (4) where JV RA(θ) balances reward maximization against policy conservatism, with πref serving as the frozen reference policy, β controlling the KL regularization strength [47], and r(G) providing dense verifiable geometric visual feedback. To ensure stable policy updates for high-dimensional discrete action spaces, we employ Group Relative Policy Optimization (GRPO) [49], which operates without explicit value function estimation. For each group of policy rollouts, we compute: Ai = r(Gi)"
        },
        {
            "title": "1\nK",
            "content": "K (cid:88) j=1 r(Gj) ri(θ) = πθ(Gi s) πθold(Gi s) (5) where Ai represents the advantage of action Gi relative to the group, and ri(θ) is the probability ratio between the new and old policies. Our reward function r(G) = rSpatial + rElement + rformat provides multi-scale geometric supervision, decomposing layout quality into spatial coherence and element-level fidelity components."
        },
        {
            "title": "The spatial reward rSpatial addresses layout misalignment",
            "content": "through Distance Intersection over Union (DIoU) [71]: rSpatial = rDIoU = (cid:88) (cid:18) IoU(bi, bgt ) (cid:19) ρ2(bi, bgt ) c2 (6) where ρ denotes the center distance, represents the diagonal of the minimal enclosing box, and bgt is the ground-truth box from Ggt. The element-level reward rElement = rAR + rsize penalizes geometric distortions that compromise visual integrity. The aspect ratio reward: rAR = (cid:12) (cid:12) (cid:12) (cid:12) (cid:88) log (cid:18) wi/hi /hgt wgt (cid:19)(cid:12) (cid:12) (cid:12) (cid:12) (7) preserves element proportions, while the size reward: rsize = (cid:88) (cid:20) smoothδ (cid:19) (cid:18) wi wgt wgt + smoothδ (cid:19)(cid:21) (cid:18) hi hgt hgt (8) 4 Figure 6. Overview of PosterCopilots Inference and Editing Pipeline. The standard inference and multi-round editing pipelines are marked by blue and red numbers, respectively. Before layout design, PosterCopilot can supplement new assets when design materials are insufficient. Generative Agent first processes user requirements, undergoes professional planning, and delivers complete assets. Design Master then generates optimal compositions based on the assets and requirements, ultimately rendering the Draft design. The draft design will be revised into the final design after multiple rounds of editing by the collaboration of both generative agent and design master. maintains original dimensions using the Huber loss [13]: smoothδ(d) = (cid:40) 0.5d2/δ 0.5δ < δ otherwise (9) where δ controls the transition between quadratic and linear regimes, preventing reward domination during extreme size distortions. We further incorporate rformat to enforce JSON-structured outputs. The complete reward formulation: r(G) = rDIoU (cid:124) (cid:123)(cid:122) (cid:125) Spatial Coherence + λsizersize + λARrAR (cid:123)(cid:122) (cid:125) Element Fidelity (cid:124) +rformat (10) where λsize, λAR > 0 balance reward components. This geometrically-grounded reward structure injects explicit visual-reality constraints directly into the policy gradient updates, enabling the model to learn corrective behaviors that transcend the limitations of previous methods that lack visual feedback during training. 3.4. Reinforcement Learning from Aesthetic Feedback While prior stages enforce graphic design rules based on single ground-truth, this is just one of many aesthetically valid solutions. To align with broader human aesthetic preferences, we introduce the Reinforcement Learning from Aesthetic Feedback (RLAF) stage. This stage explores wider design space using new subjective reward, raes(G), provided by pre-trained LMM (acting as an aesthetic judge) that evaluates the final rendered image. This aesthetic score is combined with our format reward rformat: rRLAF(G) = rformat + λaesraes(G) (11) where λaes > 0. This stage encourages the model to discover novel, high-appeal layouts that may surpass the ground-truth. 3.5. Unleashing the Creative Flow: Generative Asset Synthesis and Iterative Refinement With our design model, we now unleash its creative potential by integrating generative agent that completes the PosterCopilot framework. This integration transforms the model from pure layout planner into comprehensive design partner, capable of both asset synthesis and iterative editing. As shown in Fig. 6, this agent first addresses the issue of missing assets: when provided with only partial assets, it can adaptively generate new, style-consistent elements to complete the layout. Specifically, we utilize trained LMM called the reception model to generate textual descriptions for each missing layer, which are then combined with existing assets as style reference images to be fed together into text-to-image (T2I) model to generate the corresponding assets. More importantly, the generative agent supports fine-grained, multi-round editing required in professional workflows by accepting user instructions to perform targeted modifications on corresponding layers. This enables designers to perform stable, iterative cycles be5 lollipop sale to ice cream promotion) while perfectly preserving the original layout and decorative elements. 4.3.3. Poster Reframe Leveraging the design models powerful reasoning capability, PosterCopilot can intelligently reframe and regenerate appropriate layouts simply by modifying the canvas size specification in the input requirements. Fig. 11 presents examples of poster reframing by PosterCopilot. 5. Experimental Details 5.1. PosterCopilot Datasets long-standing challenge in constructing high-quality, multi-layer poster datasets is over-segmentation, where single visual element is fragmented across multiple independent layers [73] (e.g., shoe decomposed into separate layers for its laces, sole, and body). To solve this, we developed novel construction pipeline. As illustrated in Fig. 12, we employ OCR-based fine-granularity bounding box to merge overly fine-grained layers and filter out redundant ones. The refined dataset comprises 160K posters, encompassing total of 2.6M layers (1.2M text and 1.4M image/decorative). 5.2. Experimental Setup Implementation: Our design model employs Qwen-2.5VL-7B-Instruct [54] as backbone; the generative agent employs Qwen-Image-Edit-2509 [58] as T2I model; the reception model uses Qwen-2.5-7B [53]; and RLAF utilizes VisualQuality-R1 [59] as reward model. All experiments run on 8RTX H20 GPUs. Baselines: We compare against: (1) commercial platforms (Microsoft Designer, Nano-Banana); (2) academic SOTAs (LaDeCo [33], CreatiPoster [68]); and (3) reasoning models (Gemini 2.5 Pro [10], Qwen-VL-2.5-72B-Instruct [54]). Metrics: Following expert consultation, we evaluate the quality of the posters generated via ratings on key metrics for graphic design: Layout Rationality [12, 69], Text Legibility [5], Element Preservation [31], Style Consistency [51], Instruction Following [46] and Visual Appeal [46] for holistic poster quality evaluation, complemented by quantitative IoU, Inverse order pair ratio (IOPR) [9], and Aspect Ratio Distortion (ARD) [72] for ablation study. Evaluation Procedure: We performed human evaluation, supplemented by GPT-5 [41] as an extra reliable evaluator. For human evaluation, we conducted pairwise, binary-choice comparisons against each baseline. We sampled 25 examples per baseline, all generated from identical prompts and fully-provided assets. We collected 5 judgments per example, totaling 750 responses from over 40 evaluators with graphic design backgrounds. For GPT-5 evaluation, we used in-context learning to align the model with our scoring criteria, ensuring strict and fair assessment of all designs. We prompted GPT-5 to evaluate all results ten times, taking the average of its ratings as the final score for each method. Figure 7. Poster generated from fully-provided assets by PosterCopilot. tween precise single-layer asset editing and global layout re-arrangement, while effectively mitigating common challenges in traditional editing methods, such as asset distortion and uncontrollable edit scopes. 4. Application Harnessing its powerful reasoning capability and finegrained layer-wise architecture, PosterCopilot unlocks diverse applications in professional design scenarios. 4.1. Poster Generation from Fully-provided Assets As shown in Fig. 7, PosterCopilot excels at arranging complete set of user-provided assets into an aesthetically pleasing, professional-grade design, while guaranteeing every asset is faithfully preserved without alteration. 4.2. Poster Generation from insufficient Assets PosterCopilots generative agent handles incomplete assets by synthesizing missing layers, such as background or foreground layers, with stylistic consistency. This capability, as shown in Fig. 8, accelerates the initial design phase by enabling rapid drafts generation where synthesized elements blend harmoniously with user-provided assets. 4.3. Multi-round fine-grained Edit PosterCopilot supports precise, multi-round editing of poster drafts. This functionality encompasses diverse range of edit types, which we demonstrate in the following. 4.3.1. Single Layer Edit As shown in Fig. 9, PosterCopilot supports multiple, varied edits on single, fine-grained layer (e.g., modifying cameras material or characters pose). This high-fidelity process strictly confines the edit scope to the target layer, ensuring precise modification while preserving all other elements. This approach avoids the distortion common in diffusion-based methods that edit the entire poster. 4.3.2. Theme Switch Fig. 10 demonstrates the Theme Switch capability, enabling holistic theme migration through targeted, multiround edits. For instance, users can swap lollipop elements for ice cream, transforming the posters theme (e.g., 6 Figure 8. Posters generated from insufficient assets by our PosterCopilot. Figure 9. Multi-round refinement for single layer by our PosterCopilot. Figure 10. Multi-round refinement for theme switch by our PosterCopilot. More information about Experimental Details can be found in supplementary material. 6. Results and Analysis 6.1. Comparison with baselines Results of human evaluation is as shown in Fig. 13, PosterCopilots average win rate is well above 74% across all baselines. While LMM-based methods such as LaDeCo perform poorly on Layout Rationality and T2I models like Nanobanana struggle with Element Preservation, PosterCopilot preserves all user-provided elements while delivering harmonious, aesthetically pleasing designs. For GPT-5 evaluation, while GPT-5 excels at holistic quality assessment, it struggles with instruction following and element preservation as it cannot reliably process the source assets for these tasks. Consequently, these metrics were omitted from our GPT5 evaluation. PosterCopilots superiority in these specific areas was instead validated through our user study, which confirmed its high-fidelity performance with dominant win rate exceeding 87% on both. The results of GPT-5 evaluation is shown in Fig. 14 . We can see that PosterCopilot decisively outperforms other methods across most metrics. PosterCopilot is slightly deficient in Text Legibility compared to Nano-Banana, because PosterCopilot prioritizes faithfully preserving all user-requested text, scaling it as needed for harmonious layout. Nano-Banana, conversely, often achieves its legibility by simply discarding user elementsa flaw confirmed by its low Element Preservation score in our user study. 6.2. Ablation Study The RL-VRA and RLAF phases instill professional design principles to address SFT-stage issues, including bounding box drift, element distortion, and aspect ratio errors. Evaluated using IoU, IOPR, and ARD metrics (Tab. 1a), RL-VRA significantly improves layout accuracy over PSFT, with further IOPR/ARD gains in RLAF. The slight IoU drop in RLAF reflects its shifted focus from ground-truth fitting to aesthetic exploration. As detailed in Sec. 3.3, the RL7 Figure 11. PosterCopilot intelligently reframes posters to new canvas sizes while maintaining layout harmony. All figures are scaled to uniform height for presentation in this paper. Figure 12. Dataset construction pipeline for our PosterCopilot. We merged numerous scattered layers with OCR-based fine-granularity bounding box rather than simply parsing the original PSD file. ID"
        },
        {
            "title": "I\nII\nIII",
            "content": "ID"
        },
        {
            "title": "Training Stages",
            "content": "PSFT RL-VRA RLAF IOU IOPR ARD rformat 0.311 0.347 0.342 3.38 1.72 0.56 0.699 0.061 0.045 (a) Ablation on training stages."
        },
        {
            "title": "Layout Rewards",
            "content": "rDIOU rAR + rsize IOU IOPR ARD 0.317 0.339 0.347 3.29 1.95 1.72 0.707 0.734 0.061 (b) Ablation on reward components of RL-VRA. Table 1. Comprehensive ablation studies for training stages and reward components. We highlight the best results in red. VRA reward comprises three components: Spatial Coherence (rDIoU), Element Fidelity (rsize + rAR), and format reward. Our ablation study results in Tab. 1b on the first two rewards reveal their distinct contributions: the Spatial Coherence reward substantially enhances layout accuracy, while the Element Fidelity reward improves preservation of element sizes and proportions. Their combination yields optimal performance. Figure 13. Results of User-study. Figure 14. Results of GPT-5 evaluation. 7. Conclusion PosterCopilot revolutionizes automated poster design by decoupling creation into layout design and multi-round editing. Our progressive training paradigm forges the design model with geometric precision and human-like aesthetics, while generative agent enables multi-round, layer-wise editing mirroring professional workflows. Limitations include the lack of poster-specific aesthetic reward model and the use of standard blend modes, pointing to future work."
        },
        {
            "title": "References",
            "content": "[1] Jonathan Baldwin and Lucienne Roberts. Visual communication: from theory to practice. Ava Publishing, 2006. 2 [2] Josef Bigun, Goesta H. Granlund, and Johan Wiklund. Multidimensional orientation estimation with applications to texture analysis and optical flow. IEEE Transactions on pattern analysis and machine intelligence, 13(8):775790, 2002. 3 [3] Inha Cha and Richmond Wong. Understanding sociotechnical factors configuring ai non-use in ux work practices. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pages 117, 2025. 2 [4] Haoyu Chen, Xiaojie Xu, Wenbo Li, Jingjing Ren, Tian Ye, Songhua Liu, Ying-Cong Chen, Lei Zhu, and Xinchao Wang. Posta: go-to framework for customized artistic poster generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2869428704, 2025. 3, 1 [5] Jingye Chen, Yupan Huang, Tengchao Lv, Lei Cui, Qifeng Chen, and Furu Wei. Textdiffuser: Diffusion models as text painters. Advances in Neural Information Processing Systems, 36:93539387, 2023. 6 [6] Liuqing Chen, Qianzhi Jing, Yixin Tsang, and Tingting Zhou. Iris: multi-constraint graphic layout generation system. Frontiers of Information Technology & Electronic Engineering, 25(7):968987, 2024. 1 [7] Yan Chen, Long Li, Teng Xi, Long Zeng, and Jingdong Wang. Perception before reasoning: Two-stage reinforcement learning for visual reasoning in vision-language models. arXiv preprint arXiv:2509.13031, 2025. 1 [8] Yutao Cheng, Zhao Zhang, Maoke Yang, Nie Hui, Chunyuan Li, Xinglong Wu, and Jie Shao. Graphic design with large multimodal model. arXiv preprint arXiv:2404.14368, 2024. [9] Yutao Cheng, Zhao Zhang, Maoke Yang, Hui Nie, Chunyuan Li, Xinglong Wu, and Jie Shao. Graphic design with large multimodal model. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 24732481, 2025. 3, 6, 1 [10] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. 6, 5 [11] Alex Davies, Roussel Nzoyem, Nirav Ajmeri, et al. Language models do not embed numbers continuously. arXiv preprint arXiv:2510.08009, 2025. 3 [12] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems, 36, 2024. 6 [13] Ross Girshick. Fast R-CNN. In Proceedings of the IEEE International Conference on Computer Vision (ICCV), pages 14401448, 2015. 5 9 [14] Kamal Gupta, Justin Lazarow, Alessandro Achille, Larry Davis, Vijay Mahadevan, and Abhinav Shrivastava. Layouttransformer: Layout generation and completion with selfIn Proceedings of the IEEE/CVF International attention. Conference on Computer Vision, pages 10041014, 2021. 1 [15] Chris Harris, Mike Stephens, et al. combined corner and edge detector. In Alvey vision conference, pages 105244. Manchester, UK, 1988. 3 [16] Chen Hou, Guoqiang Wei, and Zhibo Chen. High-fidelity diffusion-based image editing. In Proceedings of the AAAI Conference on Artificial Intelligence, pages 21842192, 2024. [17] Runhui Huang, Kaixin Cai, Jianhua Han, Xiaodan Liang, Renjing Pei, Guansong Lu, Songcen Xu, Wei Zhang, and Hang Xu. Layerdiff: Exploring text-guided multi-layered composable image synthesis via layer-collaborative diffusion model. In European Conference on Computer Vision, pages 144160. Springer, 2024. 2, 1 [18] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 2 [19] Naoto Inoue, Kotaro Kikuchi, Edgar Simo-Serra, Mayu Otani, and Kota Yamaguchi. Layoutdm: Discrete diffusion model for controllable layout generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1016710176, 2023. 3, 1 [20] Mikołaj Janusz, Tomasz Wojnar, Yawei Li, Luca Benini, and Kamil Adamczewski. One shot vs. iterative: Rethinking pruning strategies for model compression. arXiv preprint arXiv:2508.13836, 2025. 2 [21] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards better evaluation metric for image generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93079315, 2024. 3 [22] Binbin Ji, Siddharth Agrawal, Qiance Tang, and Yvonne Wu. Enhancing spatial reasoning in vision-language models via chain-of-thought prompting and reinforcement learning. arXiv preprint arXiv:2507.13362, 2025. [23] Peidong Jia, Chenxuan Li, Yuhui Yuan, Zeyu Liu, Yichao Shen, Bohan Chen, Xingru Chen, Yinglin Zheng, Dong Chen, Ji Li, et al. Cole: hierarchical generation framework for multi-layered and editable graphic design. arXiv preprint arXiv:2311.16974, 2023. 3, 1 [24] Qing Jiang, Junan Huo, Xingyu Chen, Yuda Xiong, Zhaoyang Zeng, Yihao Chen, Tianhe Ren, Junzhi Yu, and Lei Zhang. Detect anything via next point prediction. arXiv preprint arXiv:2510.12798, 2025. 2 [25] Pegah Karimi, Jeba Rezwana, Safat Siddiqui, Mary Lou Maher, and Nasrin Dehbozorgi. Creative sketching partner: an analysis of human-ai co-creativity. In Proceedings of the 25th international conference on intelligent user interfaces, pages 221230, 2020. 2 [26] Abidullah Khan, Atefeh Shokrizadeh, and Jinghui Cheng. Beyond automation: How designers perceive ai as creative partner in the divergent thinking stages of ui/ux design. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems, pages 112, 2025. 2 scene control, and refinement in professional image generation case study. In International Conference on Computational Creativity, 2024. [27] Xiang Kong, Lu Jiang, Huiwen Chang, Han Zhang, Yuan Hao, Haifeng Gong, and Irfan Essa. Blt: Bidirectional layout transformer for controllable layout generation. In European Conference on Computer Vision, pages 474490. Springer, 2022. 1 [28] Hsin-Ying Lee, Lu Jiang, Irfan Essa, Phuong Le, Haifeng Gong, Ming-Hsuan Yang, and Weilong Yang. Neural design network: Graphic layout generation with constraints. In European conference on computer vision, pages 491506. Springer, 2020. 1 [29] Chen Li, Xiaoling Hu, Shahira Abousamra, Meilong Xu, and Chao Chen. Spatial diffusion for cell layout generation. In International Conference on Medical Image Computing and Computer-Assisted Intervention, pages 481491. Springer, 2024. 2 [30] Sha Li. Llms as layout designers: spatial reasoning perspective. arXiv e-prints, pages arXiv2509, 2025. 2 [31] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2251122521, 2023. 2, 6, 1 [32] Jiawei Lin, Jiaqi Guo, Shizhao Sun, Zijiang Yang, Jian-Guang Lou, and Dongmei Zhang. Layoutprompter: Awaken the design ability of large language models. Advances in Neural Information Processing Systems, 36:4385243879, 2023. 3, 1 [33] Jiawei Lin, Shizhao Sun, Danqing Huang, Ting Liu, Ji Li, and Jiang Bian. From elements to design: layered approach for automatic graphic design composition. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 81288137, 2025. 6, [34] Xiao Liu, Tianjie Zhang, Yu Gu, Iat Long Iong, Yifan Xu, Xixuan Song, Shudan Zhang, Hanyu Lai, Xinyi Liu, Hanlin Zhao, et al. Visualagentbench: Towards large multimodal models as visual foundation agents. arXiv preprint arXiv:2408.06327, 2024. 2 [35] Ishaan Malhi, Praneet Dutta, Ellie Talius, Sally Ma, Brendan Driscoll, Krista Holden, Garima Pruthi, and Arunachalam Narayanaswamy. Preserving product fidelity in large scale image recontextualization with diffusion models. arXiv preprint arXiv:2503.08729, 2025. 2 [36] Philip Meggs, Alston Purvis, Sandra Maxa, and Mark Sanders. Meggs history of graphic design. John Wiley & Sons, 2025. 2 [37] Marie Muehlhaus and Jurgen Steimle. Interaction design with generative ai: An empirical study of emerging stratearXiv preprint gies across the four phases of design. arXiv:2411.02662, 2024. 2 [38] Thanh Thi Nguyen, Campbell Wilson, and Janis Dalins. Aligning large vision-language models by deep reinforcement learning and direct preference optimization. arXiv preprint arXiv:2509.06759, 2025. [39] Rodolfo Ocampo Blanco and Oliver Bown. Integrating generative ai into creative workflows: Dealing with consistency, [40] Joel Oksanen. Bridging the integrity gap: Towards ai-assisted design research. In Extended abstracts of the CHI conference on human factors in computing systems, pages 15, 2024. 2 [41] OpenAI. Introducing gpt-5, 2025. 6 [42] Peter ODonovan, Aseem Agarwala, and Aaron Hertzmann. Learning layouts for single-pagegraphic designs. IEEE transactions on visualization and computer graphics, 20(8):1200 1213, 2014. 1 [43] Sohan Patnaik, Rishabh Jain, Balaji Krishnamurthy, and Mausoom Sarkar. Aesthetiq: Enhancing graphic layout design via aesthetic-aware preference alignment of multi-modal large language models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2370123711, 2025. 2, 1 [44] Leigang Qu, Haochuan Li, Wenjie Wang, Xiang Liu, Juncheng Li, Liqiang Nie, and Tat-Seng Chua. Silmm: Selfimproving large multimodal models for compositional text-toimage generation. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1849718508, 2025. 2 [45] Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. [46] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. 6 [47] John Schulman, Filip Wolski, Prafulla Dhara, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 4 [48] Karthick Panner Selvam. Why large language models fail at precision regression, 2025. 3 [49] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 4 [50] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. [51] Kihyuk Sohn, Nataniel Ruiz, Kimin Lee, Daniel Castro Chin, Irina Blok, Huiwen Chang, Jarred Barber, Lu Jiang, Glenn Entis, Yuanzhen Li, et al. Styledrop: Text-to-image generation in any style. arXiv preprint arXiv:2306.00983, 2023. 6 [52] Zecheng Tang, Chenfei Wu, Juntao Li, and Nan Duan. Layoutnuwa: Revealing the hidden layout expertise of large language models. arXiv preprint arXiv:2309.09506, 2023. 3, 1 10 [66] Yufei Zhan, Yousong Zhu, Shurong Zheng, Hongyin Zhao, Fan Yang, Ming Tang, and Jinqiao Wang. Vision-r1: Evolving human-free alignment in large vision-language models via vision-guided reinforcement learning. arXiv preprint arXiv:2503.18013, 2025. 1 [67] Hui Zhang, Dexiang Hong, Maoke Yang, Yutao Cheng, Zhao Zhang, Jie Shao, Xinglong Wu, Zuxuan Wu, and YuGang Jiang. Creatidesign: unified multi-conditional diffusion transformer for creative graphic design. arXiv preprint arXiv:2505.19114, 2025. 1 [68] Zhao Zhang, Yutao Cheng, Dexiang Hong, Maoke Yang, Gonglei Shi, Lei Ma, Hui Zhang, Jie Shao, and Xinglong Wu. Creatiposter: Towards editable and controllable multi-layer graphic design generation. arXiv preprint arXiv:2506.10890, 2025. 3, 6, 1, [69] Guangcong Zheng, Xianpan Zhou, Xuewei Li, Zhongang Qi, Ying Shan, and Xi Li. Layoutdiffusion: Controllable diffusion In Proceedings of model for layout-to-image generation. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2249022499, 2023. 6, 1 [70] Xinru Zheng, Xiaotian Qiao, Ying Cao, and Rynson WH Lau. Content-aware generative modeling of graphic design layouts. ACM Transactions on Graphics (TOG), 38(4):115, 2019. 1 [71] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and Dongwei Ren. Distance-IoU loss: Faster and better In Proceedings of learning for bounding box regression. the AAAI Conference on Artificial Intelligence, pages 12993 13000, 2020. 4 [72] Zhaohui Zheng, Ping Wang, Wei Liu, Jinze Li, Rongguang Ye, and Dongwei Ren. Distance-iou loss: Faster and better In Proceedings of learning for bounding box regression. the AAAI conference on artificial intelligence, pages 12993 13000, 2020. 6, 4 [73] Xingxing Zou, Wen Zhang, and Nanxuan Zhao. From fragment to one piece: review on ai-driven graphic design. Journal of Imaging, 11(9):289, 2025. 6, 3 [53] Qwen Team. Qwen2.5: party of foundation models, 2024. [54] Qwen Team. Qwen2.5-vl, 2025. 6, 5 [55] Xierui Wang, Siming Fu, Qihan Huang, Wanggui He, and Hao Jiang. Ms-diffusion: Multi-subject zero-shot image personalization with layout guidance. arXiv preprint arXiv:2406.07209, 2024. 2, 1 [56] Yuqing Wang, Zhijie Lin, Yao Teng, Yuanzhi Zhu, Shuhuai Ren, Jiashi Feng, and Xihui Liu. Bridging continuous and discrete tokens for autoregressive visual generation. arXiv preprint arXiv:2503.16430, 2025. 2 [57] Zhenyu Wang, Enze Xie, Aoxue Li, Zhongdao Wang, Xihui Liu, and Zhenguo Li. Divide and conquer: Language models can plan and self-correct for compositional text-to-image generation. arXiv preprint arXiv:2401.15688, 2024. 2 [58] Chenfei Wu, Jiahao Li, Jingren Zhou, Junyang Lin, Kaiyuan Gao, Kun Yan, Sheng ming Yin, Shuai Bai, Xiao Xu, Yilei Chen, Yuxiang Chen, Zecheng Tang, Zekai Zhang, Zhengyi Wang, An Yang, Bowen Yu, Chen Cheng, Dayiheng Liu, Deqing Li, Hang Zhang, Hao Meng, Hu Wei, Jingyuan Ni, Kai Chen, Kuan Cao, Liang Peng, Lin Qu, Minggang Wu, Peng Wang, Shuting Yu, Tingkun Wen, Wensen Feng, Xiaoxiao Xu, Yi Wang, Yichang Zhang, Yongqiang Zhu, Yujia Wu, Yuxuan Cai, and Zenan Liu. Qwen-image technical report, 2025. 6 [59] Tianhe Wu, Jian Zou, Jie Liang, Lei Zhang, and Kede Ma. Visualquality-r1: Reasoning-induced image quality assessment via reinforcement learning to rank. arXiv preprint arXiv:2505.14460, 2025. 6, 1, 3 [60] Xiaoshi Wu, Yiming Hao, Keqiang Sun, Yixiong Chen, Feng Zhu, Rui Zhao, and Hongsheng Li. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341, 2023. 1 [61] Xiao-Kun Wu, Min Chen, Wanyi Li, Rui Wang, Limeng Lu, Jia Liu, Kai Hwang, Yixue Hao, Yanru Pan, Qingguo Meng, et al. Llm fine-tuning: Concepts, opportunities, and challenges. Big Data and Cognitive Computing, 9(4):87, 2025. [62] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36:1590315935, 2023. 1 [63] Tao Yang, Yingmin Luo, Zhongang Qi, Yang Wu, Ying Shan, and Chang Wen Chen. Posterllava: Constructing unified multi-modal layout generator with llm. arXiv preprint arXiv:2406.02884, 2024. 3, 1 [64] Xuyong Yang, Tao Mei, Ying-Qing Xu, Yong Rui, and Shipeng Li. Automatic generation of visual-textual presentation layout. ACM Transactions on Multimedia Computing, Communications, and Applications (TOMM), 12(2):122, 2016. 1 [65] Ning Yu, Chia-Chih Chen, Zeyuan Chen, Rui Meng, Gang Wu, Paul Josel, Juan Carlos Niebles, Caiming Xiong, and Ran Xu. Layoutdetr: detection transformer is good multimodal layout designer. In European Conference on Computer Vision, pages 169187. Springer, 2024. 1 11 PosterCopilot: Toward Layout Reasoning and Controllable Editing for Professional Graphic Design"
        },
        {
            "title": "Supplementary Material",
            "content": "8. Related Work 8.1. Intelligent Graphic Design System Single-layer Graphic Design Generation initially relied on rule-based methods and human aesthetic constraints [42, 64, 70], or framed the task as constrained optimization problem [6, 28]. The paradigm shifted with the advent of text-to-image (T2I) models, driving research into enhancing the compositional capabilities of diffusion models by integrating layout information. Examples include GLIGEN [31], LayerDiff [17], and MS-Diffusion [55], with LayoutDiffusion [69] specifically using layout as conditioning modality. CreatiDesign [67] integrates user assets but requires pre-defined layouts. However, this single-layer approach inherently limits iterative refinement and editability, often leading to visual inconsistency and distortion in unmodified regions [19], which increases user burden and limits usability. Multi-layers Graphic Layout Planning has gained attention due to its focus on real-world practicality, operating by first inferring layout and then assembling multiple layers to offer high flexibility and editability. Early Transformerbased methods, including LayoutTransformer [14], BLT [27], and LayoutDETR [65], reframed generation as layout prediction task, but their flat, sequential representations lacked the necessary hierarchical structure for complex designs. The subsequent rise of Vision-Language Models (VLMs) led to VLM-assisted approaches like LayoutPrompter [32] and LayoutUWNA [52], which use in-context learning for layout inference. PosterLLAVA [63] guides generation through Vision Supervised Fine-Tuning on layered designs. Other methods focus on asset integration (Graphist [9]), typography (POSTA [4]), or external generative capabilities (CreatiPoster [68], COLE [23]). Crucially, these models primarily mimic static datasets rather than learning from the aesthetic quality of their own outputs. Our strategy moves beyond simple mimicry, internalizing fundamental principles of layout generation and visual aesthetics from direct generative feedback. 8.2. Reinforcement Learning for Visually Grounded"
        },
        {
            "title": "Layout Generation",
            "content": "Reinforcement Learning (RL) has significantly advanced the alignment of Vision-Language Models (VLMs) with human preferences [38, 66] and enhanced their reasoning capabilities [7, 22]. Various VLM-based visual reward models, such as HPSv2 [60], ImageReward [62], and VisualQuality-R1 [59], are trained on human preference datasets to provide aesthetic feedback. However, the feedback signals from these models are often overly general and holistic, lacking specific assessments of crucial graphic design elements like layout and alignment. While AesthetiQ [43] utilizes DirectPreference-Optimization (DPO) [45] to embed preferences in layout models, its basic feedback mechanism struggles with complex, nuanced preference signals. Our approach addresses these limitations by proposing multi-stage Reinforcement Learning framework that directly integrates finegrained layout principles and human aesthetic feedback into the models learning process. 9. Implementation details for three-stage training process Training is conducted in three stages: (1) initial PSFT phase, training for 3 epochs on 160K high-quality samples from our PosterCopilot datasets; (2) RL-VRA phase on 20K samples exhibiting complex layout rules; and (3) the final RLAF phase on 1k expert-validated samples. 9.1. Implementation details for PSFT phase As shown in Eq.5 in the main text, we perturb the bounding box values of each element in the ground truth layout to conduct our PSFT training phase. This process transforms them from single, precise values into Gaussian-like distribution, using the original value as the mean and small parameter as the variance. Subsequently, we sample values from this distribution and then calculate the PSFT loss. Prior to the PSFT training, we conducted grid analysis on the hyperparameters: (1) the standard deviation σ of the applied perturbation, and (2) the PSFT sampling number n. We evaluated wide range of σ and combinations. The quality of layouts generated by design models trained with these different parameter combinations was measured using IoU, ARD, and IOPR. The resulting impact of these parameters on the PSFT stage is illustrated in Fig. 15. It is evident that when the standard deviation σ of the added perturbation is below 3.0, the models overall performance in the PSFT phase improves as the perturbation magnitude increases. This is because adding perturbation effectively mitigates the numerical gap caused by text tokens performing regression tasks. Learning distribution (rather than single point) allows the design model to better grasp key layout patterns. When the standard deviation σ exceeds 3.0, the models performance shows slight degradation as σ increases (for fixed sampling number n). This is 1 Figure 15. Visualization of the hyperparameter analysis for the PSFT phase. because the spread of the perturbed distribution becomes excessively large, which interferes with the models learning of the ground truth layout.Conversely, increasing the sampling number consistently improves the design models performance, regardless of the σ value. However, this improvement becomes marginal once exceeds 5, and larger also incurs significant computational burden. Ultimately, to strike balance between model performance and computational efficiency, we adopt σ = 2.5 and = 5 as our final parameters. 9.2. Implementation details for RL-VRA 9.2.1. Implementation details for reward design We use verl [50] for our reinforcement learning training phase. In RL-VRA phase we design verifiable geometric reward as shown in Eq.10 in the main text: r(G) = rDIoU (cid:124) (cid:123)(cid:122) (cid:125) Spatial Coherence + λsizersize + λARrAR (cid:123)(cid:122) (cid:125) Element Fidelity (cid:124) +rformat (12) where λsize, λAR > 0. In practice, we empirically set the weights λsize = 0.6 and λAR = 0.4. In future work, more fine-grained method for automatically determining individual reward weights based on the training stage may further improve the training effectiveness of RL-VRA. The specific calculations of several rewards in RL-VRA during the actual training process are as follows: For rDIoU, the raw DIoU metric is calculated for each element in each data sample, with native value range of [-1.0, 1.0]. These values are then averaged to get ean DIoU . This average is transformed using the formula: rDIoU = (M ean DIoU + 1)/2) 10. (13) This mapping scales the original [-1.0, 1.0] range directly to the [0, 10] reward range, where value of -1.0 (worst) corresponds to 0 points and +1.0 (perfect) corresponds to 10 points. For rAR, its calculated from normalized penalty. The function first computes the absolute log-difference between the predicted and ground truth aspect ratios for each layer, capping this penalty value at 1.0 (defined as cap in the following illustration). It then calculates the average negative penalty as shown in Eq.7 in the main text to get roriginal , which lies in the range [-1.0, 0]. This penalty is converted into the score using the formula: AR rAR = ((roriginal AR + cap)/cap) 10 (14)"
        },
        {
            "title": "This inverts the roriginal",
            "content": ", mapping the worst-case penalty (-1.0) to score of 0.0 and the no-penalty case (0.0) to full score of 10.0. AR The computation of the size accuracy reward (rsize) parallels the methodology used for the aspect ratio reward. First, the size inaccuracy for each layer in each data sample is quantified as shown in Eq.8 in the main text. This resulting penalty is capped at maximum value of 1.0 (denoted as cap). The average of these individual penalties is then calculated across all layers, and its negative is taken, yielding rsize smooth. This ensures rsize smooth is bounded within the range [1.0, 0], where -1.0 represents the maximum penalty. Finally, rsize smooth is linearly transformed from its penalty-based range to the final 010 reward scale. This transformation is expressed in the following equation: rsize = (rsize smooth + cap) cap (15) The format reward rformat is binary score designed to ensure the prediction layout ˆG is valid JSON. It receives full score of 10.0 if ˆG can be successfully parsed as JSON object. If the string is malformed and results in JSONDecodeError or other parsing failure, the function immediately returns 0.0, effectively penalizing any syntactically incorrect outputs. In summary, we have obtained reward function that is dense, provides multi-dimensional geometric feedback, and has maximum score of 30. This balanced reward structure is designed to provide effective visual feedback while simultaneously mitigating reward hacking. Furthermore, it prevents any single component from dominating the optimization process, which would otherwise lead to the neglect of other crucial objectives. 9.2.2. GRPO Hyperparameter Settings for RL-VRA As shown in Tab. 2."
        },
        {
            "title": "Hyperparameter\nLearning Rate\nKL Loss Coefficient\nClip Ratio\nActor Entropy Coefficient\nTraining Batch Size\nGRPO Group Size\nTotal Epochs\nLearning Rate Optimizer",
            "content": "Value 1 106 0.01 0.2 0.01 96 8 1 Adam Table 2. GRPO hyperparameter settings for RL-VRA 9.3. Implementation details for RLAF 9.3.1. Implementation details for reward design rRLAF(G) is defined as Eq.11 in the main text: (16) rRLAF(G) = rformat + λaesraes(G) where λaes > 0. We adopt the same calculation method for rformat as in the RL-VRA stage. We employ VisualQualityR1 [59], an evaluation model meticulously trained to align with human aesthetic preferences, as the judge model for the RLAF stage. Similarly, we modulate the contribution of raes(G) via the hyperparameter λaes to ensure balanced configuration of reward scores. In our experiments, we set λaes = 2. 9.3.2. GRPO hyperparameter settings for RLAF"
        },
        {
            "title": "Hyperparameter\nLearning Rate\nKL Loss Coefficient\nClip Ratio\nActor Entropy Coefficient\nTraining Batch Size\nGRPO Group Size\nTotal Epochs\nLearning Rate Optimizer",
            "content": "Value 5 107 0.01 0.4 0.01 64 4 1 Adam Table 3. GRPO hyperparameter settings for RLAF 10. More Details For Evaluation Metrics 10.1. Aesthetic Evaluation Metrics It has become prevailing consensus among researchers in the field of graphic wdesign that relying on traditional AIGC metrics to gauge design quality is fundamentally unreasonable. While metrics like Frechet Inception Distance (FID) and Structural Similarity Index (SSIM) are highly effective in natural image synthesis tasks, they prove inadequate when assessing the quality of graphic design [19, 21, 68]. This inadequacy stems from fundamental divergence in evaluation dimensions: FID and other metrics focus primarily on pixel-level fidelity and the statistical similarity of feature distributions. However, the core value of poster design lies not in the pixel-level replication of training data, but in layout topology, visual hierarchy, typographic aesthetics, and the semantic interaction among multi-modal elements [73]. Specifically, vast majority of existing literature in the domain has critically argued that traditional AIGC metrics suffer from severe limitations. First, they lack the capability to perceive design rules. generated poster might exhibit texture and color distributions highly consistent with the training set (yielding favorable FID score), yet contain severe design accidents such as text occluding key image subjects, misalignment of elements, or imbalanced white space. While these errors are intolerable to human designers, they are often overlooked by evaluation systems based on convolutional features. Second, the calculation of these metrics is heavily influenced by the generative models fit to the training data distribution. low FID score merely indicates that the generated images are statistically similar to the training set, without measuring whether they are good in terms of visual appeal, the core of the graphic design domain. If the training data itself contains mediocre designs, traditional metrics may even reward outputs that mimic this mediocrity while penalizing high-quality designs that are innovative but deviate from the statistical mean. Consequently, directly applying traditional AIGC metrics fails to objectively evaluate the aesthetic value and layout quality of poster designs. There is an urgent need in this field to establish novel evaluation system based on geometric constraints and human aesthetic perception. Building upon the foundation of numerous distinguished prior works, we further consulted diverse panel of expertsspanning from professional graphic designers to AI researchers. Through this process, we finalized set of human evaluation metrics that are most suitable for assessing poster design. While the metrics are enumerated in Sec. 5.2 in the main text, owing to the limited space, their detailed descriptions are presented in Tab. 4. These metrics cover all critical aspects of poster quality assessment, enabling fair and comprehensive measurement of the final design quality. 10.2. Layout evaluation metrics In the training phase, our method takes multiple layers decomposed from complete poster as input, generates layout in JSON format, and subsequently renders this layout into poster image using rendering code. Following mainstream practices in prior literature, we employed three metricsIoU, IOPR, and ARDin the ablation study of the main text to directly measure the discrepancy between the predicted JSON layout and the ground truth layout. Here, we first provide the detailed calculation methods for these three metrics. For IoU metric, we clarify that all references to this"
        },
        {
            "title": "Design Utility",
            "content": "Layout Rationality evaluates the global compositional coherence, rational element placement, clarity of visual hierarchy, and minimal occlusion of critical content. Text Legibility assesses the readability of the text design (determined by font choice, size, line spacing, and color) and the faithfulness of its rendering (sharp edges, no distortion, artifacts, or garbled characters). Asset Preservation evaluates if all user-provided visual assets are fully retained and unaltered in the final result. Style Consistency assesses the coherence of stylistic treatment across all elements and the appropriateness of the overall visual style to the stated theme. Instruction following evaluates the fidelity to the textual specification, including the requested theme, style, layout, color scheme, and any required elements. Overall Visual Appeal assesses the immediate aesthetic appeal and the ability to attract attention at first glance. Design Utility assesses the suitability of the poster to be adopted as an initial design when facing the same practical brief (e.g., promoting the same product or theme. Table 4. Aesthetic evaluation metrics. metric throughout both the main text and the supplementary material denote the average IoU. Specifically, we compute the IoU between each element in the layout generated by the design model and its corresponding element in the ground truth layout. The final IoU score for poster sample is then derived by averaging the IoU values of all its constituent elements. The calculation of IoU is formally defined as follows: IoU(Bpred, Bgt) = Area(Bpred Bgt) Area(Bpred Bgt) (17) where denotes the number of elements in the poster, and B(i) represent the predicted and ground truth bounding boxes of the i-th element, respectively. pred and B(i) gt For the IOPR [8] metric, we evaluate the correctness of the predicted layer order, which is essential for maintaining visual hierarchy. IOPR quantifies the ratio of overlapping element pairs that violate the ground truth depth sequence. For single sample with layers, it is calculated as: (cid:80)n1 i=1 (cid:80)n j=i+1 IOPR = 1 (Oj < Oi overlap (i, j)) 1 (cid:80)n , j=i+ (cid:80)n1 i=0 (18) where is the number of layers in the hierarchical structure. 1 is an indicator function that returns 1 if the argument condition is true and 0 otherwise. denotes the output order or predicted order of the layers as determined by the model. Oi and Oj correspond to the predicted order positions of the ith and jth layers, respectively. overlap(i, j) is predicate function that determines whether the ith and jth layers overlap. For ARD metric, its utilized to measure the aspect ratio distortion of the predicted bounding boxes relative to the ground truth. It is derived from the term of the Complete IoU (CIoU) [72] metric, which is widely adopted in the industry: = (cid:18) 4 π arctan wgt hgt arctan (cid:19)2 (19) where wgt and hgt denote the ground truth bounding box values, w, denote the predicted bounding box values, and arctanh is one of the three tangent functions. In practice, we omitted the leading normalization term 4 π2 to make the metric differences more pronounced: (cid:18) ARD = arctan wgt hgt arctan (cid:19) (20) Although these quantitative layout evaluation metrics are less suited for assessing overall poster image quality compared to the aesthetic metrics introduced earlier, and are not directly applicable to single-layer generation or text-toimage models, they provide more direct quantification of the discrepancy between generated layouts and the ground truth. Consequently, we employ these metrics specifically in the ablation study of the main text, rather than calculating them for all baselines. 11. Supplementary Ablation Study Due to the limited space, the ablation study in the main text details only the primary training procedure and the reward component ablation results, demonstrating the necessity of each component and training phase. Here, we present additional ablation studies to directly validate the superiority of"
        },
        {
            "title": "Method",
            "content": "IOU IOPR ARD SFT PSFT (Ours) 0.285 0.311 4.12 3.38 0.851 0. Table 5. Quantitative comparison between standard SFT and our proposed PSFT. Best results are highlighted in red. our PSFT phase over conventional SFT paradigms. Furthermore, we conduct human evaluation to verify that RLAF guides the model to generate layouts more aligned with human aesthetics. This serves as an intuitive complement to the quantitative metrics presented in the main text. Except for the specific modules being ablated, all experimental settings for the ablation studies in both the main text and the supplementary material are identical to the training procedure described in Sec. 9. 11.1. Ablation study for PSFT phase We evaluated the design model trained solely with PSFT against the one trained with standard SFT. The latter was trained exclusively on ground truth layouts without the introduction of perturbations or other augmentation measures. The results is as shown in Tab. 5. We can see that the design model trained via PSFT significantly outperforms the standard SFT baseline across IoU, IOPR, and ARD metrics. This demonstrates that the PSFT strategy, by incorporating perturbations, effectively mitigates the numerical-semantic gap caused by treating numerical coordinates as text tokens for regression. 11.2. Ablation study for RLAF phase Fig. 16 visually demonstrates the critical role of RLAF. Given that poster design is inherently driven by human aesthetics, training design model solely to replicate ground truth layouts is insufficient. The model often generates layouts that deviate significantly from the ground truth yet remain aesthetically pleasing. In fact, layouts exhibiting greater divergence from the ground truth can sometimes yield superior aesthetic quality. We conducted an human evaluation on models trained via three progressive stages: only PSFT, PSFT + RL-VRA, and PSFT+RL-VRA+RLAF (PosterCopilot). We collected 10 inference poster samples, each of which was assessed by panel of 15 ranging from professional graphic designers to individuals with diverse interdisciplinary backgrounds. The assessment was strictly based on the human evaluation metrics defined in the main text. Fig. 17 presents the evaluation results across various metrics. It is evident that the RL-VRA stage significantly enhances the layout quality and consistency of the generated designs. Building upon the previous stages, RLAF further substantially improves the visual appeal. Regarding the instruction-following capability, since the design model Figure 16. Poster samples generated by the design model via multiple inference runs. The IoU scores against the ground truth layout are 0.87, 0.43, and 0.21, respectively. Notably, despite the varying degrees of deviation from the ground truth, all three posters align well with human aesthetics. has already achieved satisfactory level via training on the high-quality large-scale PosterCopilot dataset, the improvement in this metric is relatively marginal compared to other key indicators. Figure 17. Human evaluation comparison of design quality metrics across different stages of our training paradigm. PosterCopilot is trained via complete three stages. 12. More Details About Evaluation Procedure In the field of poster design, recent, open-source, and highperforming baselines capable of handling user-supplied assets are notably scarce. To ensure methodological diversity and comparison against state-of-the-art (SOTA) solutions from both academia and industry, we selected the following baselines: (1) commercial platforms (Microsoft Designer, Nano-Banana); (2) academic SOTAs (LaDeCo [33], CreatiPoster [68]); and (3) reasoning models (Gemini 2.5 Pro [10], Qwen-VL-2.5-72B-Instruct [54]). As demonstrated in the main text, our comparative analysis conditions all models on identical user assets input and design prompts to generate posters. Since the baselines encompass both text-to-image (T2I) models and non-end-to-end layout generation frameworks (similar to PosterCopilot), their inference pipelines exhibit slight variations. In this section, we provide detailed elaboration of these specific testing protocols. 12.1. Evaluation procedure for T2I models Among the selected baselines, Microsoft Designer and NanoBanana (formally known as Gemini 2.5 Flash Image) belongs to the T2I category. Notably, since its debut, Nano5 Banana has garnered widespread attention within the graphic design community, distinguished by its unparalleled capabilities in multi-asset conditioned generation and multi-turn iterative editing. The evaluation procedure for T2I models is relatively straightforward. We condition the models on all provided user assets, specify the target canvas dimensions, and input the design prompt to generate the corresponding poster samples for comparison. 12.2. Evaluation procedure for layout generation models The remaining methodsCreatiPoster [68], LaDeCo [33], Qwen-VL-2.5-72B-Instruct [54], Gemini 2.5 Pro [10], and our own PosterCopilotfall under the category of Layout Generation models. For these models, consistent with the T2I evaluation, we provide user assets and design prompts. However, we explicitly instruct the models to output the layout in JSON format. Upon obtaining the generated JSON files, we employ unified high-precision lossless rendering script to convert the text-based layouts into final poster images for each test sample. Its worth noting that CreatiPoster requires precise predefined layouts for foreground elements. To accommodate this, we provided the ground truth foreground layouts during its evaluation. Although this setup places our method at comparative disadvantage, PosterCopilot still achieved significant lead across all metrics in both GPT-5 evaluations and multi-dimensional human assessments. This further demonstrates PosterCopilots robust layout reasoning capabilities while requiring minimal manual input. 13. More Qualitative Comparisons We provide in Fig. 18 some examples of the setting where various methods assemble posters based on complete assets. Fig. 19 presents additional examples of precise single-layer editing. 14. More details about PosterCopilot datasets The main text provided key description of the PosterCopilot dataset construction pipeline. Here, we further offer more details regarding the dataset construction process and the dataset composition. Our datasets construction pipeline begins with the ingestion of approximately 160,000 professionally designed PSD source files collected from online stock platforms. In the initial phase, OCR Document Parsing, each PSD is exhaustively analyzed to extract all valid layers as independent PNG files. Concurrently, JSON annotation is generated for each poster, capturing low-level metadata such as bounding boxes, stacking order, and layer type, which provides the foundation for structured supervision. 6 To mitigate the fragmentation problem, the pipeline proceeds to the Parse stage, where the initial raw layers are prepared for semantic grouping. This is followed by the core Layers Merger phase. Here, the semantic cues provided by the initial OCR-based document parsing are leveraged as data-cleaning mechanism. The merger process intelligently groups and combines excessively fine layers and concurrently discards visually insignificant ones. This crucial refinement step effectively aligns the fragmented raw layers with human visual perception, resulting in refined annotation space focused on genuine visual elements. We present key statistics of the PosterCopilot dataset in Fig. 20. To facilitate understanding, we also provide an example of parsed JSON file for representative poster instance:"
        },
        {
            "title": "Example of parsed JSON file",
            "content": "{ \"psd_file\": \"c:/desktop/dataset-images/ freepik/freepik/Medical Poster/40858 9341-world-cancer-day-awarenesstemplate/11575324.psd\", \"ocr_file\": \"c:/desktop/user-workspace/ anonymous/psd-parsed-with-ocr/ Medical Poster-408589341-11575324/ ocr/11575324_ocr.json\", \"canvas_size\": { \"width\": 1748, \"height\": 2480 }, \"layers\": [ { \"src\": \"World cancer day\", \"category\": \"type\", \"x\": 144, \"y\": 537, \"w\": 1468, \"h\": 368, \"order\": 0, \"blend_mode\": \"BlendMode.NORMAL\", \"opacity\": 255, \"text_info\": [ { \"text\": \"WORLD CANCER DAY\", \"text_type\": \"PARAGRAPH\", \"font_size_px\": 50.31, \"font_family\": \"Jost-ExtraBold \", \"color_css\": \"rgba(96, 0, 146, .0)\", \"text_align\": \"center\", \"leading\": 0.99, \"warp\": { \"warpStyle\": \"bwarpNone\", \"warpRotate\": \"bHrzn\", \"warpValue\": 0.0, \"warpPerspective\": 0.0, \"warpPerspectiveOther\": 0.0 }, \"font-weight\": \"normal\", \"font-style\": \"normal\", Figure 18. Visual comparison of poster composition results across all methods. Each column corresponds to specific method, demonstrating its generation performance based on various user assets and prompts. \"tracking\": 0.0, \"transform\": [ 4.166666666666667, 0.0, 0.0, 4.166666666666667, -33255.49755600113, -32887.51407877605 ] } ], \"group\": [ [ ] \"Text\", \"World cancer day\" ], \"merged_layers_names\": [ \"World cancer day\" ], \"merged_layers_num\": 1, \"merged_layers_indices\": [ 0 ], 7 (a) The first set of qualitative comparisons on single-layer editing between PosterCopilot and Nano-Banana. (b) The second set of qualitative comparisons on single-layer editing between PosterCopilot and Nano-Banana. Figure 19. Comparison of single-layer editing performance between PosterCopilot and Nano-Banana. Among all the baselines, only Nano-Banana and our PosterCopilot support the precise editing of arbitrary layers within poster. Others either lack editing capabilities entirely or are limited to manual repositioning via dragging. Both PosterCopilot and Nano-Banana are fed with identical user assets and prompts for poster generation and multi-round edit. In each comparison, the top row shows the generation and multi-turn editing results of PosterCopilot, while the bottom row displays those of Nano-Banana. In the cases presented, the objective is to exclusively modify the background layer or the womans appearance while leaving the rest of the poster intact. As observed, PosterCopilot faithfully preserves non-target regions throughout multi-turn editing sessions while precisely modifying the target layer. In contrast, although Nano-Banana produces impressive results initially, severe distortion occurs in other parts of the poster after just one or two refinement iterations, and unintended attributes of the subject are also altered. \"is_single_layer\": true, \"files\": { \"layer\": \"c:/desktop/userworkspace/anonymous/psd-parsed 8 0.0, 4.166666666666667, -33253.49994542471, -32886.160441080734 ] } ], \"group\": [ [ ] \"Text\", \"entry free\" ], \"merged_layers_names\": [ \"entry free\" ], \"merged_layers_num\": 1, \"merged_layers_indices\": [ 1 ], \"is_single_layer\": true, \"files\": { \"layer\": \"c:/desktop/userworkspace/anonymous/psd-parsed -with-ocr/Medical Poster-40858 9341-11575324/merged/11575324_ 10_merged.png\" }, \"ocr_info\": { \"bbox\": [ 69, 112, 234, ], \"category\": \"Text\", \"text\": \"entrynfree\" } }, { \"src\": \"4/02\", \"category\": \"type\", \"x\": 697, \"y\": 122, \"w\": 319, \"h\": 128, \"order\": 3, \"blend_mode\": \"BlendMode.NORMAL\", \"opacity\": 255, \"text_info\": [ { \"text\": \"4/02\", \"text_type\": \"PARAGRAPH\", \"font_size_px\": 32.59, \"font_family\": \"MontserratExtraBold\", \"color_css\": \"rgba(96, 0, 146, 1 .0)\", \"text_align\": \"start\", \"leading\": 1.2, \"warp\": { \"warpStyle\": \"bwarpNone\", \"warpRotate\": \"bHrzn\", \"warpValue\": 0.0, \"warpPerspective\": 0.0, Figure 20. Key statistics of the PosterCopilot dataset. -with-ocr/Medical Poster-40858 9341-11575324/merged/11575324_ 11_merged.png\" }, \"ocr_info\": { \"bbox\": [ 136, 534, 1613, 907 ], \"category\": \"Title\", \"text\": \"# WORLD CANCER DAY\" } }, { \"src\": \"entry free\", \"category\": \"type\", \"x\": 74, \"y\": 119, \"w\": 156, \"h\": 117, \"order\": 1, \"blend_mode\": \"BlendMode.NORMAL\", \"opacity\": 255, \"text_info\": [ { \"text\": \"entry free\", \"text_type\": \"PARAGRAPH\", \"font_size_px\": 11.4, \"font_family\": \"MontserratSemiBold\", \"color_css\": \"rgba(96, 0, 146, 1 .0)\", \"text_align\": \"start\", \"leading\": 1.2, \"warp\": { \"warpStyle\": \"bwarpNone\", \"warpRotate\": \"bHrzn\", \"warpValue\": 0.0, \"warpPerspective\": 0.0, \"warpPerspectiveOther\": 0.0 }, \"font-weight\": \"normal\", \"font-style\": \"normal\", \"tracking\": 0.0, \"transform\": [ 4.166666666666667, 0.0, 9 \"warpPerspectiveOther\": 0.0 }, \"font-weight\": \"normal\", \"font-style\": \"normal\", \"tracking\": 0.0, \"transform\": [ 4.166666666666667, 0.0, 0.0, 4.166666666666667, -33255.50039401008, -32886.97428385417 ] } ], \"group\": [ [ ] \"Text\", \"4/02\" ], \"merged_layers_names\": [ \"4/02\" ], \"merged_layers_num\": 1, \"merged_layers_indices\": [ 3 ], \"is_single_layer\": true, \"files\": { \"layer\": \"c:/desktop/userworkspace/anonymous/psd-parsed -with-ocr/Medical Poster-40858 9341-11575324/merged/11575324_ 8_merged.png\" }, \"ocr_info\": { \"bbox\": [ 693, 118, 1021, 251 ], \"category\": \"Text\", \"text\": \"4/02\" } }, { \"src\": \"@cancer_day\", \"category\": \"type\", \"x\": 364, \"y\": 2119, \"w\": 295, \"h\": 52, \"order\": 5, \"blend_mode\": \"BlendMode.NORMAL\", \"opacity\": 255, \"text_info\": [ { \"text\": \"@cancer_day\", \"text_type\": \"PARAGRAPH\", \"font_size_px\": 12.29, \"font_family\": \"Jost-Medium\", \"color_css\": \"rgba(96, 0, 146, 1 10 .0)\", \"text_align\": \"start\", \"leading\": 1.2, \"warp\": { \"warpStyle\": \"bwarpNone\", \"warpRotate\": \"bHrzn\", \"warpValue\": 0.0, \"warpPerspective\": 0.0, \"warpPerspectiveOther\": 0.0 }, \"font-weight\": \"normal\", \"font-style\": \"normal\", \"tracking\": 0.0, \"transform\": [ 4.166666666666667, 0.0, 0.0, 4.166666666666667, -33254.49951986482, -32887.92683919271 ] } ], \"group\": [ [ ] \"Text\", \"@cancer_day\" ], \"merged_layers_names\": [ \"@cancer_day\" ], \"merged_layers_num\": 1, \"merged_layers_indices\": [ 5 ], \"is_single_layer\": true, \"files\": { \"layer\": \"c:/desktop/userworkspace/anonymous/psd-parsed -with-ocr/Medical Poster-40858 9341-11575324/merged/11575324_ 6_merged.png\" }, \"ocr_info\": { \"bbox\": [ 361, 2116, 658, ], \"category\": \"Text\", \"text\": \"@cancer_day\" } }, { \"src\": \"Healthy Life Avenue, 8842 Melrose st., LA,California\", \"category\": \"type\", \"x\": 71, \"y\": 2248, \"w\": 982, \"h\": 104, \"order\": 6, \"blend_mode\": \"BlendMode.NORMAL\", \"opacity\": 255, \"text_info\": [ { \"text\": \"Healthy Life Avenue, 88 42 Melrose st., LA, California\", \"text_type\": \"PARAGRAPH\", \"font_size_px\": 12.29, \"font_family\": \"Jost-Medium\", \"color_css\": \"rgba(96, 0, 146, 1 .0)\", \"text_align\": \"start\", \"leading\": 1.2, \"warp\": { \"warpStyle\": \"bwarpNone\", \"warpRotate\": \"bHrzn\", \"warpValue\": 0.0, \"warpPerspective\": 0.0, \"warpPerspectiveOther\": 0.0 }, \"font-weight\": \"normal\", \"font-style\": \"normal\", \"tracking\": 0.0, \"transform\": [ 4.166666666666667, 0.0, 0.0, 4.166666666666667, -33255.49974608525, -32887.958521327215 ] } ], \"group\": [ [ ] \"Text\", \"Healthy Life Avenue, 8842 Melrose st., LA,California\" ], \"merged_layers_names\": [ \"Healthy Life Avenue, 8842 Melrose st., LA,California\" ], \"merged_layers_num\": 1, \"merged_layers_indices\": [ 6 ], \"is_single_layer\": true, \"files\": { \"layer\": \"c:/desktop/userworkspace/anonymous/psd-parsed -with-ocr/Medical Poster-40858 9341-11575324/merged/11575324_ 5_merged.png\" }, \"ocr_info\": { \"bbox\": [ 69, 2245, 1054, 2357 Melrose st., LA,-nCalifornia \" } }, { \"src\": \"www.cancerday.com\", \"category\": \"type\", \"x\": 1222, \"y\": 2118, \"w\": 459, \"h\": 52, \"order\": 7, \"blend_mode\": \"BlendMode.NORMAL\", \"opacity\": 255, \"text_info\": [ { \"text\": \"www.cancerday.com\", \"text_type\": \"PARAGRAPH\", \"font_size_px\": 12.29, \"font_family\": \"Jost-Medium\", \"color_css\": \"rgba(96, 0, 146, 1 .0)\", \"text_align\": \"right\", \"leading\": 1.2, \"warp\": { \"warpStyle\": \"bwarpNone\", \"warpRotate\": \"bHrzn\", \"warpValue\": 0.0, \"warpPerspective\": 0.0, \"warpPerspectiveOther\": 0. }, \"font-weight\": \"normal\", \"font-style\": \"normal\", \"tracking\": 0.0, \"transform\": [ 4.166666666666667, 0.0, 0.0, 4.166666666666667, -33254.49970463595, -32889.040771484375 ] } ], \"group\": [ [ ] \"Text\", \"www.cancerday.com\" ], \"merged_layers_names\": [ \"www.cancerday.com\" ], \"merged_layers_num\": 1, \"merged_layers_indices\": [ 7 ], \"is_single_layer\": true, \"files\": { \"layer\": \"c:/desktop/userworkspace/anonymous/psd-parsed -with-ocr/Medical Poster-40858 9341-11575324/merged/11575324_ 4_merged.png\" ], \"category\": \"Text\", \"text\": \"Healthy Life Avenue, 8842 }, 11 \"ocr_info\": { \"bbox\": [ 1219, 2116, 1680, ], \"category\": \"Text\", \"text\": \"www.cancerday.com\" } }, { \"src\": \"cancer knows no gender or age. get check-up regulary.\", \"category\": \"type\", \"x\": 366, \"y\": 362, \"w\": 1004, \"h\": 97, \"order\": 8, \"blend_mode\": \"BlendMode.NORMAL\", \"opacity\": 255, \"text_info\": [ { \"text\": \"CANCER KNOWS NO GENDER OR AGE. GET CHECK-UP REGULARY.\", \"text_type\": \"PARAGRAPH\", \"font_size_px\": 12.0, \"font_family\": \"MontserratBoldItalic\", \"color_css\": \"rgba(96, 0, 146, 1 .0)\", \"text_align\": \"center\", \"leading\": 1.2, \"warp\": { \"warpStyle\": \"bwarpNone\", \"warpRotate\": \"bHrzn\", \"warpValue\": 0.0, \"warpPerspective\": 0.0, \"warpPerspectiveOther\": 0.0 }, \"font-weight\": \"normal\", \"font-style\": \"normal\", \"tracking\": 0.0, \"transform\": [ 4.166666666666667, 0.0, 0.0, 4.166666666666667, -33255.501571969085, -32886.8654327771 ] } ], \"group\": [ [ ] \"Text\", \"cancer knows no gender or age. get check-up regulary.\" ], \"merged_layers_names\": [ \"cancer knows no gender or age. get check-up regulary.\" 12 ], \"merged_layers_num\": 1, \"merged_layers_indices\": [ 8 ], \"is_single_layer\": true, \"files\": { \"layer\": \"c:/desktop/userworkspace/anonymous/psd-parsed -with-ocr/Medical Poster-40858 9341-11575324/merged/11575324_ 3_merged.png\" }, \"ocr_info\": { \"bbox\": [ 361, 355, 1374, 463 ], \"category\": \"Text\", \"text\": \"CANCER KNOWS NO GENDER OR AGE.nGET CHECK-UP REGULARY .\" } }, { \"src\": \"Anual Scientific Cancer congress\", \"category\": \"type\", \"x\": 373, \"y\": 993, \"w\": 920, \"h\": 116, \"order\": 9, \"blend_mode\": \"BlendMode.NORMAL\", \"opacity\": 255, \"text_info\": [ { \"text\": \"ANNUAL SCIENTIFIC CANCER CONGRESS\", \"text_type\": \"PARAGRAPH\", \"font_size_px\": 14.24, \"font_family\": \"MontserratExtraBold\", \"color_css\": \"rgba(96, 0, 146, 1 .0)\", \"text_align\": \"center\", \"leading\": 1.2, \"warp\": { \"warpStyle\": \"bwarpNone\", \"warpRotate\": \"bHrzn\", \"warpValue\": 0.0, \"warpPerspective\": 0.0, \"warpPerspectiveOther\": 0.0 }, \"font-weight\": \"normal\", \"font-style\": \"normal\", \"tracking\": 0.0, \"transform\": [ 4.166666666666667, 0.0, 0.0, 4.166666666666667, -33255.50023252936, -32886.372521938516 ] } ], \"group\": [ [ ] \"Text\", \"Anual Scientific Cancer congress\" ], \"merged_layers_names\": [ \"Anual Scientific Cancer congress\" ], \"merged_layers_num\": 1, \"merged_layers_indices\": [ ], \"is_single_layer\": true, \"files\": { \"layer\": \"c:/desktop/userworkspace/anonymous/psd-parsed -with-ocr/Medical Poster-40858 9341-11575324/merged/11575324_ 2_merged.png\" }, \"ocr_info\": { \"bbox\": [ 370, 986, 1296, 1113 ], \"category\": \"Text\", \"text\": \"## ANNUAL SCIENTIFIC CANCER CONGRESS\" } }, { \"src\": \"Vector Smart Object\", \"category\": \"smartobject\", \"x\": 129, \"y\": 1161, \"w\": 1355, \"h\": 889, \"order\": 10, \"blend_mode\": \"BlendMode.NORMAL\", \"opacity\": 255, \"text_info\": {}, \"group\": [ [ ] \"Design\", \"Vector Smart Object\" ], \"merged_layers_names\": [ \"Vector Smart Object\" ], \"merged_layers_num\": 1, \"merged_layers_indices\": [ 11 ], \"is_single_layer\": true, \"files\": { \"layer\": \"c:/desktop/userworkspace/anonymous/psd-parsed -with-ocr/Medical Poster-40858 9341-11575324/merged/11575324_ 1_merged.png\" }, \"ocr_info\": { \"bbox\": [ 127, 1156, 1489, 2051 ], \"category\": \"Picture\" } }, { \"src\": \"Background Layer\", \"category\": \"background\", \"x\": 0, \"y\": 0, \"w\": 1748, \"h\": 2480, \"order\": 11, \"blend_mode\": \"BlendMode.NORMAL\", \"opacity\": 255, \"text_info\": {}, \"group\": [ [ ], [ ], [ ] \"Social Media\", \"Vector Smart Object\" \"Design\", \"Vector Smart Object\" \"Background\", \"Background\" ], \"merged_layers_names\": [ \"Vector Smart Object\", \"Vector Smart Object\", \"Background\" ], \"merged_layers_num\": 3, \"merged_layers_indices\": [ 10, 12, 13 ], \"is_single_layer\": false, \"files\": { \"layer\": \"c:/desktop/userworkspace/anonymous/psd-parsed -with-ocr/Medical Poster-40858 9341-11575324/merged/11575324_ 0_merged.png\" } } ], \"statistics\": { \"original_layers\": 17, \"valid_layers\": 14, \"merged_groups\": 12, \"excluded_layers\": 0, \"out_of_bounds_layers\": } }"
        }
    ],
    "affiliations": [
        "Institute of Automation, Chinese Academy of Sciences",
        "LibLib.ai",
        "PRLab, Nanjing University"
    ]
}