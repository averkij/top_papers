{
    "paper_title": "On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking",
    "authors": [
        "Jianliang He",
        "Leda Wang",
        "Siyu Chen",
        "Zhuoran Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We present a comprehensive analysis of how two-layer neural networks learn features to solve the modular addition task. Our work provides a full mechanistic interpretation of the learned model and a theoretical explanation of its training dynamics. While prior work has identified that individual neurons learn single-frequency Fourier features and phase alignment, it does not fully explain how these features combine into a global solution. We bridge this gap by formalizing a diversification condition that emerges during training when overparametrized, consisting of two parts: phase symmetry and frequency diversification. We prove that these properties allow the network to collectively approximate a flawed indicator function on the correct logic for the modular addition task. While individual neurons produce noisy signals, the phase symmetry enables a majority-voting scheme that cancels out noise, allowing the network to robustly identify the correct sum. Furthermore, we explain the emergence of these features under random initialization via a lottery ticket mechanism. Our gradient flow analysis proves that frequencies compete within each neuron, with the \"winner\" determined by its initial spectral magnitude and phase alignment. From a technical standpoint, we provide a rigorous characterization of the layer-wise phase coupling dynamics and formalize the competitive landscape using the ODE comparison lemma. Finally, we use these insights to demystify grokking, characterizing it as a three-stage process involving memorization followed by two generalization phases, driven by the competition between loss minimization and weight decay."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 8 1 ] . [ 1 9 4 8 6 1 . 2 0 6 2 : r On the Mechanism and Dynamics of Modular Addition: Fourier Features, Lottery Ticket, and Grokking"
        },
        {
            "title": "Zhuoran Yang",
            "content": "Department of Statistics and Data Science, Yale University {jianliang.he, leda.wang, siyu.chen.sc3226, zhuoran.yang}@yale.edu"
        },
        {
            "title": "Abstract",
            "content": "We present comprehensive analysis of how two-layer neural networks learn features to solve the modular addition task. Our work provides full mechanistic interpretation of the learned model and theoretical explanation of its training dynamics. While prior work has identified that individual neurons learn single-frequency Fourier features and phase alignment, it does not fully explain how these features combine into global solution. We bridge this gap by formalizing diversification condition that emerges during training when overparametrized, consisting of two parts: phase symmetry and frequency diversification. We prove that these properties allow the network to collectively approximate flawed indicator function on the correct logic for the modular addition task. While individual neurons produce noisy signals, the phase symmetry enables majority-voting scheme that cancels out noise, allowing the network to robustly identify the correct sum. Furthermore, we explain the emergence of these features under random initialization via lottery ticket mechanism. Our gradient flow analysis proves that frequencies compete within each neuron, with the winner determined by its initial spectral magnitude and phase alignment. From technical standpoint, we provide rigorous characterization of the layer-wise phase coupling dynamics and formalize the competitive landscape using the ODE comparison lemma. Finally, we use these insights to demystify grokking, characterizing it as three-stage process involving memorization followed by two generalization phases, driven by the competition between loss minimization and weight decay."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 1.1 Related Work ."
        },
        {
            "title": "2 Preliminaries",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3"
        },
        {
            "title": "3 Empirical Findings",
            "content": "6 3.1 Mechanistic Pattern: Experimental Observations on Learned Weights . . . . . . . . . 6 9 3.2 Dynamical Perspective: Phase Alignment and Feature Emergence . . . . . . . . . . . 3.3 Grokking: From Memorization to Generalization . . . . . . . . . . . . . . . . . . . . . 11 1Our code is available at GitHub. For interactive visualizations and further experimental results, see our Hugging Face Space at Hugging Face."
        },
        {
            "title": "4 Mechanistic Interpretation of Learned Model",
            "content": ""
        },
        {
            "title": "5 Training Dynamics for Feature Emergence",
            "content": "14 5.1 Background: Discrete Fourier Transform . . . . . . . . . . . . . . . . . . . . . . . . . 14 5.2 Dynamical Perspective on Feature Emergence . . . . . . . . . . . . . . . . . . . . . 15 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 5.3 Properties at the Initial Stage 5.4 Preservation of Single-Frequency Pattern . . . . . . . . . . . . . . . . . . . . . . . . . 17 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 5.5 Neuron-Wise Phase Alignment"
        },
        {
            "title": "6 Theoretical Extensions",
            "content": "20 6.1 Theoretical Underpinning of Lottery Ticket Mechanism . . . . . . . . . . . . . . . . . 20 6.2 Dynamics Beyond Quadratic Activation . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "7 Conclusion",
            "content": ""
        },
        {
            "title": "26\nA.1 Detailed Interpretation of Grokking Dynamics in Section 3.3 . . . . . . . . . . . . . . 26\nA.2 Ablations Studies for Fully-Diversified Parametrization . . . . . . . . . . . . . . . . . 28\nA.3 Training Dynamics with Quadratic Activation . . . . . . . . . . . . . . . . . . . . . . 30",
            "content": "B Proof of Results in Section 4 and 5 30 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 B.1 Proof of Proposition 4.2 . B.2 Preliminary: Gradient Computation . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 B.3 Main Flow Approximation under Small Parameter Scaling . . . . . . . . . . . . . . . 33 B.3.1 Proof Overview: Simplified Dynamics under Approximation . . . . . . . . . 34 B.3.2 Proof of Lemma B.3: Main Flow of Decoupled Neurons . . . . . . . . . . . . . 35 B.4 Proof of Theorem 5.2: Single-Frequency Preservation . . . . . . . . . . . . . . . . . . 38 B.4.1 Proof of Auxiliary Lemma B.5 . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 . . . . . . . . . . . . . . . . . . . . . . . . . . 44 B.5.1 Proof of Auxiliary Lemma B.8, B.9 and B.10 . . . . . . . . . . . . . . . . . . . . 48 B.5 Proof of Theorem 5.3: Phase Alignment Proof of Results for Theoretical Extensions in Section"
        },
        {
            "title": "D Comparison with Existing Results",
            "content": ""
        },
        {
            "title": "1 Introduction",
            "content": "A central mystery in deep learning is how neural networks learn to generalize. While these models are trained to find patterns in data, the precise way they build internal representations through gradient-based training and make predictions on new, unseen data is not fully understood. The sheer complexity of modern networks often obscures the fundamental principles at work. To gain clearer view, researchers often simplify the problem by studying how networks solve simple but rich tasks that can be precisely analyzed. By meticulously analyzing the learning process in these controlled \"toy\" settings, we can uncover basic mechanisms that may apply more broadly. The modular addition task, (x, y) (cid:55) (x + y) mod has emerged as canonical problem for this approach, as it is simple to define yet reveals surprisingly complex and insightful learning dynamics. Figure 1: An illustration of the primary analytical technique and results. Discrete Fourier Transform (DFT) is utilized to quantitatively interpret the mechanism of learned models within the feature space, revealing the training dynamics that result in consistent feature learning. Figure (a) shows the neural network architecture we adopt two-layer fully connected neural network to learn the modular addition task. The inputs and are represented as one-hot vectors in Rp, σ() denotes the activation function, and the width of the neural network is denoted by . Figure (b) illustrates the technique of DFT. We apply DFT to the weights at the input and output layers, respectively. Each neuron involves two weight vectors, which lead to two magnitudes and phases. (See Observation 1 in 3.) Figure (c) illustrates some of our key empirical observations phase alignment (Observation 2), phase symmetry (Observation 3), and lottery ticket mechanism (Observation 6). Prior work has established that neural networks trained on modular arithmetic discover Fourier feature representation, embedding inputs onto circle to transform addition into geometric rotation (Nanda et al., 2023; Zhong et al., 2023). These studies have also highlighted the intriguing grokking phenomenon, where model suddenly generalizes long after it has memorized the training data (Power et al., 2022; Liu et al., 2022). While these observations are foundational, prior work has not yet offered conclusive, end-to-end explanation of the learning process. Existing theoretical accounts 3 often rely on mean-field approximations (Wang and Wang, 2025) or analyze non-standard loss functions (Morwani et al., 2023; Tian, 2024), leaving gap in our understanding of the finite-neuron dynamics under standard training. This leaves fundamental questions unanswered: (Q1) Mechanistic Interpretability: How does the trained network leverage its learned Fourier features to implement the modular addition algorithm precisely? (Q2) Training Dynamics: How do these specific Fourier features reliably emerge from gradientbased training with random initialization? In this paper, we provide comprehensive answers to these questions via systematic experiments and rigorous theoretical analysis of two-layer networks. For (Q1), while prior work has identified that neurons learn single-frequency features and exhibit phase alignment, we quantitatively characterize how these local features are synthesized into global mechanism. Specifically, we demonstrate that the network develops collective diversification condition (see Observation 3 and 4, formalized in Definition 4.1) characterized by two key properties: (i) frequency diversification: The network ensures that the full spectrum of necessary Fourier components is represented across the neuron population. (ii) phase symmetry: Within each frequency group, neurons exhibit high-order symmetry to ensure the balance required for noise cancellation. We rigorously prove that this dual condition allows the network to aggregate the noisy, biased signals of individual neurons into collective approximation of flawed indicator function (see Theorem 4.2) and how these patterns emerge from gradient training from mean-field perspective driven by the layer-wise phase coupling dynamics (see Theorem 5.2, 5.3 and Proposition 6.3 with proof sketch). To address (Q2), we explain the emergence of these features via lottery ticket mechanism (see Observation 6). Our analysis of the gradient flow reveals competitive dynamic in which multiple frequency components compete within each individual neuron during training. Specifically, by applying the ODE comparison lemma, we prove that the frequency component with the largest initial magnitude and the smallest phase misalignment grows exponentially faster than its competitors, eventually becoming the single dominant winner (see Corollary 6.1). This provides rigorous, neuron-wise explanation for the learned single-frequency structure, demonstrating how random initialization determines which specific Fourier features the network ultimately adopts. Finally, having established the underlying mechanism and training dynamics, we can address the final bonus question regarding the grokking phenomenon: (Q3) Memorization to Generalization: How do these mechanisms and dynamics explain the full timeline of grokking, from memorization to delayed generalization? We characterize it as three-stage process driven by the competition between loss minimization and weight decay. We demonstrate that the model first memorizes training data through perturbed version of the lottery ticket mechanism, followed by two generalization stages where weight decay prunes residual noise and refines the learned features into the sparse Fourier representation required for generalization. By providing complete, end-to-end theoretical and empirical account of this learning problem, our work offers concrete foundation for understanding the interplay between feature learning, training dynamics, and generalization in neural networks."
        },
        {
            "title": "1.1 Related Work",
            "content": "Modular Addition and Grokking Phenomenon. Studying simple tasks like modular addition has revealed deep insights into neural network mechanisms (e.g., Power et al., 2022). Reverseengineering has shown models learn Fourier feature, converting addition into geometric rotation by embedding numbers on circle (Nanda et al., 2023; Zhong et al., 2023; Gromov, 2023; Doshi 4 et al., 2024; Yip et al., 2024; McCracken et al., 2025). This discovery is central to understanding grokking, phenomenon where generalization suddenly emerges long after overfitting, which these papers study using specific train-test data splits (e.g., Liu et al., 2022; Doshi et al., 2023; Yip et al., 2024; Mallinar et al., 2024; Wu et al., 2025). Theoretical understanding of this modular addition task, however, remains incomplete. Morwani et al. (2023) characterize the loss landscape under the max-margin framework using non-standard ℓ2,3-regularization. The work Tian (2024) further analyzes the landscape of modified ℓ2-loss within the Fourier space, generalized these results to data with semi-ring structures on Abelian groups, and provided heuristic derivation for the mean-field dynamics of frequencies. Recently, Wang and Wang (2025) formalize and extende these mean-field results by analyzing the Wasserstein gradient flow under geometric equivariance constraint, and Kunin et al. (2025) characterize the Fourier feature emergence as trade-off between maximizing utility function over the dormant neurons and minimizing cost function over active ones. While Tian (2024) and Wang and Wang (2025) provide characterization of simpler, mean-field dynamics, full analytical result explaining the alignment and competition dynamics at the finite, neuron-wise level remains an open problem. different approach studies grokking modular arithmetic via the average gradient outer product for backpropagation-free models (Mallinar et al., 2024). Another line of research focuses on grokking dynamics and frames it as two-phase process, transitioning from an initial lazy (kernel) regime to later rich (feature) regime (Kumar et al., 2024; Lyu et al., 2023; Mohamadi et al., 2024; Ding et al., 2024), which are broadly related to our work. Recently, the work of Tian (2025) proposes three-stage theoretical framework for grokking dynamics that includes lazy learning, independent feature learning, and interactive feature learning. This three-stage process echoes our own observations for modular addition in 3.3, A.1. more detailed comparison with related work is provided in D. Training Dynamics of Neural Networks. To understand how neural networks perform feature learning, significant body of work has analyzed the training dynamics of neural networks under gradient-based optimization. This research typically focuses on settings where the target function exhibits low-dimensional structure, such as single-index (Ba et al., 2022; Lee et al., 2024; Berthier et al., 2024; Chen et al., 2025) and multi-index models (Damian et al., 2022; Arnaboldi et al., 2024; Ren et al., 2025). Taking step further, Allen-Zhu and Li (2019); Shi et al. (2022, 2023) have considered more general cases, analyzing function classes that encode latent features rather than relying on the explicit structure of index models. While insightful, these works assume well-structured target functions and clearly defined features, leaving the feature learning from natural data largely unclear. Notation. For any positive integer N+, let [n] = {i : 1 n}. Let Zp denote the set of integers modulo p. The ℓp-norm is denoted by p. For vector ν Rd, its i-th entry is denoted by υ[i]. The softmax operator, smax(), maps vector to probability distribution, where the i-th component is given by smax(υ)i = exp(υi)/ (cid:80) exp(υj). For two non-negative functions (x) and g(x) defined on R+, we write (x) g(x) or (x) as O(g(x)) if there exists two constants > 0 such that (x) g(x), and write (x) g(x) or (x) if there exists two constants > 0 such that (x) g(x). We write (x) g(x) or (x) = Θ(g(x)) if (x) g(x) and g(x) (x)."
        },
        {
            "title": "2 Preliminaries",
            "content": "In modular addition task, we aim to learn whose form is given by (x, y) (cid:55) Modular Addition. (x + y) mod for (x, y) Z2 . The complete dataset is given by Dfull = {(x, y, z) x, Zp, = (x + y) mod p} which consists of all possible input pairs (x, y) and their corresponding modular 5 sums z. This dataset is then partitioned into training set for learning and disjoint test set for evaluation. The performance of learned model is assessed on the test set by evaluating how accurately it predicts (x + y) mod for unseen input pairs. Such training setup is widely used in the literature to study phase transition phenomena, such as grokking (e.g., Nanda et al., 2023), and feature learning (e.g., Morwani et al., 2023) in modular arithmetic tasks. Two-Layer Neural Network. We consider two-layer neural network with hidden neurons and no bias terms. Each input is assigned to embedding vectors hx Rd, where : Zp (cid:55) Rd is an embedding function of dimension N. The embedding can be either the canonical embedding ex Rp in which case = or trainable one {hx}xZp Rd. Let θ = {θm}m[M ] and ξ = {ξm}m[M ] denote the parameters, where θm Rd is the parameter vector of the m-th hidden neuron and ξm Rp is its corresponding output-layer weight. The network output is then given by (x, y; ξ, θ) = (cid:88) m=1 ξm σ(hx + hy, θm) Rp, (2.1) where σ() is nonlinear activation. In this paper, we primarily focus on the ReLU activation σ(x) = max{x, 0} for experiments and the quadratic activation σ(x) = x2 for theoretical interpretations. Since the modular addition is essentially classification problem, we apply the softmax function smax : Rd (cid:55) Rd to the network output and consider the cross-entropy (CE) loss: ℓD(ξ, θ) = (cid:88) (x,y)D (cid:10) log smax (x, y; ξ, θ), e(x+y) mod (cid:11). (2.2) Here, log() is applied entrywise and e(x+y) mod is the one-hot vector that corresponds to the correct label. Intuitively, each input pair (x, y) is mapped to hidden representation by σ(hx + hy, θm) for each neuron m, then linearly combined by ξms to produce the logits (x, y; ξ, θ), and finally processed via the softmax function to yield categorical distribution for classification."
        },
        {
            "title": "3 Empirical Findings",
            "content": "In this section, we present the empirical findings. We set = 23 without loss of generality, and use two-layer neural network with width = 512 and ReLU activation. The network is trained using the AdamW optimizer with constant step size of η = 104. We initialize all parameters using PyTorchs default method (Paszke et al., 2019). For stable training, we then normalize these initial values and use the average loss over the dataset. We note that all of our empirical findings below are robust to the choice of p, and they appear as long as is sufficiently large and the neural network is properly optimized. Following prior work (Morwani et al., 2023; Tian, 2024), we primarily focus on training the model with the complete dataset Dfull (without train-test splitting), as this yields more stable training dynamics and enhances model interpretability. While the train-test split setup exhibits the intriguing grokking behavior (e.g., Nanda et al., 2023; Doshi et al., 2023; Gromov, 2023), wherein models suddenly achieve generalization after extensive training despite initial overfitting, we defer this analysis to 3.3, building upon the foundational results presented in subsequent sections."
        },
        {
            "title": "3.1 Mechanistic Pattern: Experimental Observations on Learned Weights",
            "content": "We first summarize the main empirical findings of our experiments using ReLU activation (see Figures 2 and 3), formalized as four key observations. The first two trigonometric parameterization 6 (a) Heatmap of Learned Parameters. (b) Actual Learned and Fitted Parameters of Each Neuron. Figure 2: Learned parameters under the full random initialization with = 23 and ReLU activation using AdamW. Figure (a) plots heatmap of the learned parameters for the first 10 neurons after Discrete Fourier Transform (DFT, see 5.1) grouped with frequency. Each row in the heatmap corresponds to the Fourier components of single neurons parameters. The plot clearly reveals single-frequency pattern: each neuron exhibits large, non-zero value focused on only one specific frequency component, confirming highly sparse and specialized frequency encoding. We remark that since only 10 out of 512 neurons are shown, not all (p 1)/2 = 11 frequencies appear in this sample. The same single-frequency pattern holds across all 512 neurons, which collectively cover all 11 frequencies (see Observation 3). Figure (b) further examines the periodicity by plotting line plots of the learned parameters for three neurons, each overlaid with trigonometric curve fitted via DFT. The fitted curve aligns almost perfectly with the actual one. and phase alignment have been previously explored in the literature (Gromov, 2023; Nanda et al., 2023; Yip et al., 2024), and are included for completeness. For clarity, we focus on the case where inputs are one-hot embedded. We begin with the most striking observation: global trigonometric pattern in parameters that consistently emerges across all training runs with random initialization. Observation 1 (Fourier Feature). There exists frequency mapping φ : [M ] [ p1 magnitudes αm, βm R+ and phases ϕm, ψm [π, π), such that 2 ], along with θm[j] = αm cos(ωφ(m)j + ϕm), ξm[j] = βm cos(ωφ(m)j + ψm), (m, j) [M ] [p], (3.1) where we denote ωk = 2πk/p for all [ p1 2 ]. This observation shows that the parameter vectors θm and ξm simplify during training into clean trigonometric pattern. In the frequency domain, this corresponds to sparse signal. After applying Discrete Fourier Transform (DFT, see 5.1), each neuron is represented by single active frequency φ(m). Given this single-frequency structure, we will henceforth refer to αm and ϕm as the input magnitude and phase, and to βm and ψm the output magnitude and phase for neuron m. This observation is illustrated in Figure 2. In Figure 2b, we zoom in on the learned parameters of the first three neurons, with each entry corresponding to the input or output value Zp. The plots show that these parameters are well approximated by cosine curves, shifted by phases ϕm and ψm, and scaled by magnitudes αm and βm, respectively. This suggests that the trained neural network learns to solve modular addition by embedding trigonometric structure into its parameters, where 7 average value ι cos(ιϕm) sin(ιϕm) 1 2 3 4 5 0.0123 -0.0234 -0.0531 -0.0235 -0.0505 -0.0500 0.0319 -0.0032 -0.0451 -0.0372 (a) Scatter of (2ϕm, ψm). (b) Phase Symmetry within Frequency Group Nk. (c) Distribution of αm, βm. Figure 3: Visualizations of learned phases with = 512 neurons. Figure (a) plots the relationship among the normalized 2ϕm and ψm, with all points lying around the line = x. Figure (b) shows the uniformity of the learned phases within specific group Nk. The left panel displays ιϕm for ι {1, 2, 3, 4} on unit circles, and the points are nearly uniformly distributed. The right panel quantifies this symmetry by computing the averages of cos(ιϕm) and sin(ιϕm), all of which are close to zero. Figure (c) presents violin plots of the magnitudes αm and βm. The tight distribution of these values around their mean suggests that the neurons learn nearly identical magnitudes. each dimension [p] corresponds to the value of cosine function at j. Next, we examine the local structure of individual neurons, and observe highly structured phase alignment behavior. Observation 2 (Doubled Phase). For each neuron [M ], the parameter exhibits doubled phase relationship, where the output phase is twice the input phase, i.e., (2ϕm ψm) mod 2π = 0. We visualize the relationship between ϕm and ψm in Figure 3a. Specifically, the dots represent the pairs (2ϕm, ψm), which lie precisely on the line = x, confirming the claim made in Observation 2. This indicates that the first-layer θm and second-layer ξm learns to couple in the feature space, specifically the Fourier space, through training. Having studied both global and neuron-wise local parameter patterns, we now examine how neurons coordinate their collective operation. Consider network with sufficiently large number of neurons, then the phases exhibit clear within-group uniformity and the magnitudes display nearly homogeneous scaling across neurons. Observation 3 (Model Symmetry). Let Nk be the set of neurons for frequency k, defined as Nk = {m [M ] : φ(m) = k}. For large , (i) phases are approximately uniform over (π, π) i.i.d. within frequency group Nk, i.e., ϕm, ψm Unif(π, π), (ii) every frequency is represented among the neurons, and (iii) the magnitudes αms and βm remains close across all neurons. Figure 3b illustrates the uniformity of phases within specific frequency group Nk by examining the higher-order symmetry, i.e., the symmetry of ιϕm for ι {1, 2, 3, 4}. Both the visualizations and the quantitative averages of sine and cosine values support the within-group uniformity claim stated in Observation 3. In addition, the learned magnitudes are similar across all neurons, preventing any single neuron from becoming dominant (see Figure 3c). Although large is not required for successful model training, it significantly aids in interpreting the mechanism of the learned model (see 4 for details). While previous work (e.g., Kumar et al., 2024), has introduced the phase uniformity to provide constructive model that solves modular addition, our findings significantly refine the understanding. Through empirical validations, we show that this phase uniformity is consistent when is large. Furthermore, in 4, we derive and utilize substantially 8 σ(x) max{x, 0}"
        },
        {
            "title": "Loss\nAccuracy",
            "content": "1.194 108 1.000 0.000 1.000 x2 0.000 1.000 x4 3.1 105 1. x8 0.051 1.000 log(1 + e2x) ex 1.2 103 1.000 6.5 104 1.000 4.246 0.041 x3 3.891 0.036 Table 1: Adaptivity of the learned parameterization. We evaluate the robustness of the trained model by replacing the ReLU with various alternative functions at test time. As shown in the table, the model maintains perfect prediction accuracy when using the absolute value function, even-order polynomials, or the exponential function. This demonstrates that the learned features are not strictly dependent on the original activation but rather on its underlying even-order components. weaker condition than strict uniformity to enable more precise, joint analysis of noise cancellation across diversified, finite set of neurons. Finally, we report surprising adaptivity in the learned parametrization: the network continues to perform perfectly when ReLU is replaced by broad class of alternative activations at the test time. See Table 1 for details. As shown in Table 1, when we replace the ReLU activation to other activation functions that has nonzero even-order components, e.g., x, x2, and x4, the resulting models still have perfect prediction accuracy. However, suppose we replace ReLU to an activation wihout any even-order component, e.g., and x3, the prediction accuracy is close to zero. This suggests that the key property of ReLU activation is that it has even-order components. 2 Observation 4 (Robustness to Activation Swapping). model trained with ReLU is robust to changes of activation function at inference time. This is because learning good solution only relies on the activations dominant even-order components. Consequently, functions with strong even components, such as the absolute value and quadratic, can be used interchangeably after training, all while maintaining perfect accuracy with negligible change in loss. Motivated by this key observation, in the sequel, we analyze the training dynamics of how two-layer neural networks solve modular addition using the more tractable quadratic activation."
        },
        {
            "title": "3.2 Dynamical Perspective: Phase Alignment and Feature Emergence",
            "content": "We conduct an analysis of training dynamics in an analytically tractable setting, using quadratic activation with small random initialization, and focus on the early stages of training. Motivated by Observation 1, our analysis hinges on studying the training dynamics within the frequency domain. To do this, we use the Discrete Fourier Transform (DFT), which is formalized in 5.1, to decompose the models parameters. Without loss of generality, any random initial parameter vector can be exactly represented by its frequency components magnitudes (αk m)s. This allows us to express the parameters as: for each entry [p], m)s and phases (ϕk m, ψk m, βk θm[j] = α0 + (p1)/2 (cid:88) k=1 cos(ωkj + ϕk αk m), ξm[j] = β0 + (p1)/2 (cid:88) k=1 cos(ωkj + ψk βk m), (3.2) As we will show in 5, under small initialization, the neurons and frequencies are fully decoupled. That is, the evolution of each neurons Fourier frequency components (magnitudes and phases) 2Due to phase symmetry, the output of ReLU neural network is fundamentally determined by the 1 follows from the identity ReLU(x) = 1 networks operations, leaving the absolute value term as the primary contribution to the output. 2 term. This 2 (x + x). Under phase symmetry, the linear components cancel out across the 9 (a) Evolution of misalignment level Dk neuron under gradient flow under small random initialization. and magnitude βk of specific (b) Leaned magnitude βk different initializations. under Figure 4: Illustration of the lottery-ticket mechanism under the fully random initialization. Figure (a) plots the dynamics of every frequency for specific neuron, with the red curve tracing the trajectory of the frequency that eventually dominates. In the left-hand plot, misalignment levels Dk are rescaled to [π, π) for clarity. Typically, the winning frequency is the one that starts with comparatively larger initial magnitude and smaller misalignment. Figure (b) plots the contour of the magnitude βk m(0)) after 10, 000 steps. The contours are symmetric about π, and reproduce the trend seen in Figure (a): initial states with larger magnitude and lower phase misalignment yield higher final magnitudes after the same training duration. with various (βk m(0), Dk only depends on the dynamics of themselves. This results in the parallel growth of the magnitudes and phases for each neuron-frequency pair (m, k). The central question is how the training process evolves this complex, multi-frequency initial state into the simple, single-frequency pattern observed at the end of training. Our finding is surprising: The final, dominant frequency learned by each neuron is entirely determined by small subset of Fourier components in its initial parameters. It arises from competitive dynamics among frequencies, as shown in Figure 4a. frequencys success is determined by its initial conditions, primarily two key factors: its initial magnitudes and its initial phase misalignment level. To gain more detailed understanding of the dynamics, we begin by tracking the evolution of phases. Motivated by the double phase phenomenon in Observation 2, we monitor the normalized phase difference Dk m) mod 2π [0, 2π). This quantity plays central role throughout our analysis: as we will show in 5, the magnitude growth rate is governed by cos(Dk m), making it the key variable that simultaneously controls both alignment and amplification. In the left-hand side of Figure 4a, we plot the dynamics of this phase difference, rescaling its range to (π, π] for visual clarity. This analysis leads to the following observation. m) and the phase rotation speed by sin(Dk , defined as Dk = (2ϕk ψk Observation 5 (Dynamics of Phase-Aligning). The phase difference Dk m(t) for each frequency converges monotonically to zero without crossing the axis. Generally, frequencies that start with an initial phase difference Dk m(0) closer to zero converge faster. = max{Dk To formalize the closeness of phase difference to zero, we define the phase misalignment (cid:101)Dk as (cid:101)Dk m}. In the following, we outline the core dynamics of the training process. It reveals that the single-frequency pattern in Observation 1 is the direct result of frequency competition, process governed by the interplay of phase misalignment and magnitude. m, 2π Dk 10 Observation 6 (Lottery Ticket Mechanism). Under small random initialization, neurons are m(0), decoupled. Each frequency draws lottery ticket specified by its initial magnitudes αk m(0) and misalignment level (cid:101)Dk m(0). All frequencies grow in parallel, and the one with the largest βk m(0) ultimately wins dominating the feature of specific αk m(0) and βk neuron due to the rapid acceleration once magnitudes become larger and (cid:101)Dk m(t) reaches zero. m(0) and the smallest (cid:101)Dk Figure 4a provides clear empirical illustration of the mechanism. The winning frequency, highlighted in red, begins with highly advantageous initialization: competitively large magnitude and misalignment value close to zero. While other frequencies exhibit slow growth, the holder of this winning ticket undergoes distinct phase of rapid, exponential acceleration in its magnitude. Figure 4b plots the magnitude under different initializations after fixed time = 10, verifying that frequencies with larger magnitude and smaller misalignment take advantage."
        },
        {
            "title": "3.3 Grokking: From Memorization to Generalization",
            "content": "In this section, we provide empirical insights into grokking by analyzing the models training dynamics using progress measure designed based on our prior observations. Prior work, such as Nanda et al. (2023), identifies two key factors for inducing grokking: distinct train-test data split and the application of weight decay. Here, we randomly partition the entire dataset of p2 points, using training fraction of 0.75, and apply weight decay of 2.0. The experimental results with detailed progress measure is plotted in Figure 5 As shown in Figure 5a, this elicits clear grokking phenomenon: the training loss drops quickly to zero. In contrast, the test loss initially remains high before gradually decreasing, signaling delayed generalization. We track four key progress measures: (a) Train-Test Loss and Accuracy. Standard indicators used to differentiate between the memorization phase and the onset of generalization; (b) Phase Difference. We monitor sin(D degree of layer-wise phase alignment; m), where m := 2ϕ ψ mod 2π, to evaluate the (c) Frequency Sparsity. Measured via the Inverse Participation Ratio (IPR), defined as IPR(ν) = (ν2r/ν2)2r with = 2, to capture the single-frequency emergence of Fourier coefficients; (d) ℓ2-norm of parameters. Utilized as proxy to monitor the structural evolution of weights and the specific influence of weight decay on the models complexity. Building upon Figure 5, we identify two primary driving forces behind the dynamics: loss minimization and weight decay. These forces guide the training process through an initial memorization phase followed by two generalization stages. The memorization phase is dominated by loss minimization, causing the model to fit the training data with its parameter norms increasing rapidly. As result, the model achieves perfect accuracy on the training data and their symmetric counterparts in the test set (due to the exchangability of the two input numbers), but completely fails to generalize to truly unseen test points (see Figure 10). At this phase, all the frequency components in one neuron keep growing but at different pace similar to the lottery ticket mechanism described previously, resulting in perturbed Fourier solution that overfits the training data. Next, the model enters the first generalization stage, which is characterized by precise interplay between the two forces. We conclude that both forces are active because the parameter norms continue to grow, which is clear indicator of ongoing loss minimization. At the same time, weight decay induces sparsification effect in the frequency domain. Specifically, the one frequency 11 (a) Train-test Loss. (b) Train-test Accuracy. (c) Phase Difference. (d) Norm & Freq. Sparsity. Figure 5: Progress measure of grokking behavior. The shaded regions mark three distinct phases: an initial memorization phase, followed by two generalization phases. Figures (a) and (b) plot the train-test loss and accuracy curve, where the network first overfits the training data to achieve near-zero training loss while the test loss remains high. Figure (c) visualizes the dynamics of average phase alignment level, measured by m1 (cid:80)M m). Figure (d) tracks the evolution of the average neuron-wise frequency sparsity level, as measured by the inverse participation ratio (IPR) of the Fourier coefficients, alongside the ℓ2-norm of the parameter. m=1 sin(D component that dominates in the lottery ticket mechanism continues growing, while weight decay refines the learned sparse features by pruning the remaining components, making it closer to the clean single-frequency solution for each neuron and causing the test loss to drop sharply. Specifically, the weight decay refines the learned sparse features, making it closer to the clean single-frequency solution for each neuron, causing the test loss to drop sharply. This dynamic culminates in turning point around step 10,000, which marks the onset of the second and final generalization stage. From this point, weight decay becomes the dominant force, slowly pushing the test accuracy toward perfect score. Figure 6: An illustration of the three stages of grokking dynamics and their main driving force. Principle of Memorization: Common-to-Rare. Early in training, as training accuracy rises, test accuracy falls from an initial 5% (due to small random initialization) to 0% (see Figure 5b). By Step 1000, when training accuracy peaks, the first phase is evident: the model prioritizes memorizing common data, specifically symmetric pairs where both (i, j) and its counterpart (j, i) are in the training set. This intense focus comes at cost, as the model actively suppresses performance on rare examples within the same training set, driving their accuracy to zero. Only after mastering the common data does the model shift its focus to the second phase: memorizing these rare examples that appear only once. Please refer to A.1 for more detailed interpretation of grokking dynamics."
        },
        {
            "title": "4 Mechanistic Interpretation of Learned Model",
            "content": "In this section, we first tackle the interpretability question in slightly idealized setting, leveraging the trigonometric patterns in Observations 1-3 and, motivated by Observation 4, adopting quadratic activation for analytical convenience. We show that the trained model effectively approximates an indicator function via majority-voting scheme within the Fourier space. Single-Neuron Contribution and Majority Voting. Under the parametrization of (3.1) in Observation 1 and the phase-alignment condition 2ϕm ψm = 0 mod 2π for all in Observation 2, the contribution of each neuron m, i.e., [m](x, y) = ξm σ(ex + ey, θm), to the logit at dimension [p] can be expressed as: [m](x, y; ξ, θ)[j] cos(ωφ(m)(x y)/2)2 {cos(ωφ(m)(x + j)) (cid:125) (cid:124) (cid:123)(cid:122) primary signal + 2 cos(ωφ(m)j + 2ϕm) + cos(ωφ(m)(x + + j) + 4ϕm)}. (4.1) Here, cos(ωk(x+y j)) provides the primary signal, whose value peaks exactly at = (x+y) mod p, while the remaining terms act as residual noise whose amplitude and sign depend on the chosen frequency k, phase ϕm, and input pair (x, y). Similar results have also been reported in Gromov (2023); Zhong et al. (2023); Nanda et al. (2023); Doshi et al. (2023). Although each neurons contribution is biased by its own frequency-phase view, the network as whole can attain perfect accuracy via majority-voting mechanism: every neuron votes based on its individual view, the model then aggregates these biased yet diverse votes to distill the correct answer. Despite this intuitive diversification argument, two questions remain unanswered: (a) How should we define diversification? (b) To what extent can the residual noise be canceled by aggregating over diverse set of frequency-phase pairs (φ(m), ϕm)? Majority-Voting Approximates Indicator via Overparameterization. Motivated by Observation 3, when is sufficiently large, the model naturally learns completely diversified neurons: every frequency is represented, and the phases exhibit uniform symmetry. We formalize this below. Definition 4.1 (Full Diversification). Neurons is called fully diversified if the frequency-phase pairs {(φ(m), ϕm)}m[M ] satisfy the following properties: (i) for every frequency [ p1 2 ], there are exactly neurons with φ(m) = k, (ii) there exists constant > 0 such that αmβ2 = for all [M ], and (iii) for each and ι {2, 4}, exp (cid:0)i ι (cid:80) (cid:1) = 0. ϕm mNk Note that Definition 4.1 is primarily formal restatement of Observation 3. In particular, Condition (ii) follows from the homogeneous scaling of magnitudes, and Condition (iii) captures the high-order phase symmetry implied by the uniformity within the frequency group. Condition (i) assumes an exact frequency balance an idealization that holds approximately under random initialization (see 6.1). We are now ready to present the main results regarding the interpretation of the learned model. Figure 7: Heatmap of the output logits with quadratic activation, with boxes indicating predicted higher values. 13 Proposition 4.2. Suppose that the neurons are completely diversified as per Definition 4.1. Under the parametrization in (3.1) and the phase-alignment condition 2ϕm ψm = 0 mod 2π for all [M ], the output logit at dimension [p] takes the form: (x, y; ξ, θ)[j] = aN/2 (cid:8) 1 + p/2 1(x + mod = j) (cid:125) (cid:124) (cid:123)(cid:122) signal term +p/4 (cid:88) 1(2z mod = j) (cid:9). (4.2) z{x,y} (cid:124) (cid:123)(cid:122) noise terms (cid:125) For any ϵ (0, 1), by taking (N p)1 log(p/ϵ), it holds that smax (, ; ξ, θ) emp(,)1, ϵ. Please refer to B.1 for detailed proof of Proposition 4.2. The proposition states that although each neuron individually implements trigonometric mechanism as shown in (4.1), the diversified neurons indeed collectively approximate the indicator function 1(x + mod = j). As noted in Zhong et al. (2023), the cos(ωφ(m)(x y)/2)2 term in (4.1) is the Achilles heel of this strategy. We show that even under complete diversification, it would still introduce spurious peaks at 2x mod and 2y mod p. However, from (4.2), we see that the true-signal peak exceeds these noise peaks by aN p/8. Hence, after the softmax operation, the models output would concentrate on the correct sum + mod as long as the magnitude grows large enough during the training. In A.2, we present ablation studies on full diversification, evaluating the performance of neural network predictors with limited frequencies and non-uniformly distributed phases under the same neuron budget constraint. The results show that fully diversified parameterization is the most parameter-efficient approach, yielding the largest logit gap between the ground-truth index and incorrect labels."
        },
        {
            "title": "5 Training Dynamics for Feature Emergence",
            "content": "In this section, we provide theoretical understanding of how features emerge during standard gradient-based training. Unlike previous theoretical works that focused on loss landscape analysis (e.g., Morwani et al., 2023), we offer more complete view from the perspective of training dynamics. To achieve this, we track the evolution of the models parameters directly in the Fourier space."
        },
        {
            "title": "5.1 Background: Discrete Fourier Transform",
            "content": "Motivated by empirical observations in 3, it is natural to apply the Fourier transform to model parameters and to track the evolution of the Fourier coefficients throughout the training process. This allows us to investigate how these Fourier features are learned. We begin by defining the Fourier basis matrix over Zp by Bp = [b1, . . . , bp] Rpp, where each column is given by b1 = 1p , b2k = (cid:114) 2 [cos(ωk), . . . , cos(ωkp)], b2k+1 = (cid:114) 2 [sin(ωk), . . . , sin(ωkp)], 2 ]3. We then project the model parameters, ξms and θms, onto where wk = 2kπ/p for all [ p1 this basis. This change of basis is equivalent to applying the Discrete Fourier Transform (DFT, Sundararajan, 2001), yielding the Fourier coefficients: gm = p θm, rm = ξm, [M ]. 3We choose as prime number greater than 2 to simplify the analysis. To better interpret these coefficients, we group the sine and cosine components for each frequency = (gm[2k], gm[2k + 1]) and and reparameterize them by their magnitude and phase. Denote by gk = (rm[2k], rm[2k + 1]) the coefficient vector in correspondence to frequency k. Their magnitudes rk and phases are defined as follows. For the input layer, αk the phase denotes the magnitude and ϕk of the k-th frequency component of θm. For the output layer, βk are the corresponding magnitude and phase of ξm. These can be formalized as and ψk αk = (cid:114) 2 gk m, = atan(gk ϕk m), βk = (cid:114) 2 rk m, = atan(rk ψk m). Here, atan(x) = atan2(x[2], x[1]) where atan2 : (cid:55) (π, π] is the 2-argument arc-tangent. This polar representation is intuitive, as it directly relates the coefficients to phase-shifted cosine, e.g., gm[2k] b2k[j] + gm[2k + 1] b2k+1[j] = αk m). By setting constant coefficients as α0 = gm[1]/ p, we can recover the expanded form in (3.2). cos(wkj + ϕk = rm[1]/ and β"
        },
        {
            "title": "5.2 A Dynamical Perspective on Feature Emergence",
            "content": "In the following, we provide theoretical explanation of how the features single-frequency and phase alignment patterns, i.e, Observation 1 and 2, emerge during training. For theoretical convenience, we adopt the quadratic activation (Arous et al., 2025) and focus on the training over complete dataset Dfull, familiar setting in prior work (e.g., Morwani et al., 2023; Tian, 2024). To better understand the training dynamics using the gradient-based optimization methods, we analyze the continuous-time limit of gradient descent gradient flow, which is introduced below. Gradient Flow. Consider training two-layer neural network as defined in (2.1) with one-hot input embeddings, i.e., hx = ex Rp, parameterized by Θ = {ξ, θ}, and the loss ℓ is given by the cross-entropy (CE) loss in (2.2), evaluated over the full dataset Dfull. When training the parameter Θ using the gradient flow, the dynamics are governed by the following ODE: tΘt = ℓ(Θt), ℓ(Θ) = (cid:88) (cid:88) xZp yZp (cid:10) log smax (x, y; ξ, θ), e(x+y) mod (cid:11). We consider gradient flow under an initialization that satisfies the following conditions. Assumption 5.1 (Initialization). For each neuron [M ], the network parameters (ξm, θm) are initialized as θm κinit (cid:112)p/2 (ϱ1[1] b2k + ϱ1[2] b2k+1) and ξm κinit (cid:112)p/2 (ϱ2[1] b2k + ϱ2[2] b2k+1) where ϱ1, ϱ 2 ]) and κinit > 0 denotes sufficiently small initialization scale. i.i.d. Unif(S1), Unif([ p1 Assumption 5.1 posits that each neuron is initialized randomly but contains singlem(0) = κinit. This specialized frequency component, all at the same small scale, i.e., αk initialization is adopted for theoretical convenience, allowing us to sidestep the chaotic frequency competition induced by entirely random initialization and study the evolution of one specific frequency. Specifically, the single-frequency is sufficient to capture the overall behavior as each frequency component evolves within its own orthogonal subspace. In 6.1, we will extend to the case where each neuron is initialized with multiple frequencies. m(0) = βk"
        },
        {
            "title": "5.3 Properties at the Initial Stage",
            "content": "Given sufficiently small initialization in Assumption 5.1, key property at the initial stage is that the parameter magnitudes remain small, resulting in the softmax output being nearly uniform over. 15 Formally, θm and ξm are small such that the following equality holds approximately: smax (x, y; ξ, θ) 1 1p. (5.1) While (5.1) suggests that the neural network behaves as poorly performing uniform predictor at the initial stage due to the small parameter magnitudes, this does not imply that the model learns nothing. Instead, the model can learn the \"feature direction\" of the data under the guidance of the gradient. In what follows, we examine the key components of the gradient and define the time threshold tinit to ensure all parameters remain within small scale. Neuron Decoupling. We first show that the neurons are decoupled at the initial stage, meaning the evolution of parameters θm and ξm depends solely on (θm, ξm)the parameters of neuron itselfby using the approximation in (5.1). To establish this, we compute the gradient and simplify it using periodicity. We derive that the gradient flow for each neuron [M ] at the initial stage admits the following simplified form: for each entry [p], we have tθm[j](t) 2p (p1)/2 (cid:88) m(t) βk αk m(t) cos(cid:0)ωkj + ψk m(t) ϕk m(t)(cid:1), tξm[j](t) k=1 (p1)/2 (cid:88) k=1 m(t)2 cos(ωkj + 2ϕk αk m(t)). (5.2a) (5.2b) Here, we use the Fourier expansion of parameters θm(t) and ξm(t) as given in (3.2). In words, the first equation states that θm evolves as superposition of cosines, where each frequency contributes with rate proportional to the product of the input and output magnitudes αk , modulated by between the two layers. The second equation shows that ξm evolves the phase difference ψk . squared, with phase of 2ϕk similarly, but its rate depends only on the input magnitude αk Crucially, the dynamics, i.e., tθm(t) and tξm(t), only depends on {(αk m, ψk m)}k[(p1)/2] and rm[1] that corresponds to neuron m. This demonstrates decoupled evolution among neurons. Hence, in the remaining section, we can focus on fixed neuron m. Similar decoupling technique with similar small output scale is also seen in Lee et al. (2024); Chen et al. (2025) for ℓ2-loss. ϕk βk m, βk m, ϕk Remark 5.1 (Equivalence to Margin Maximization under Small Initialization). Notice that the modular addition task is multi-class classification problem. To understand the feature emergence, Morwani et al. (2023) considers an average margin maximization problem, where the margin is defined by max ξ,θ ℓAM(ξ, θ) with ℓAM(ξ, θ) = (cid:88) (cid:88) (cid:26) xZp yZp (x, y; ξ, θ)[(x + y) mod p] (cid:27) (x, y; ξ, θ)[j] . 1 (cid:88) jZp In comparison, given the small scale of parameters during the initial stage, we can show that, similar to the approximation in (5.1), the loss takes the approximate form: ℓ(ξ, θ) = (cid:88) (cid:88) xZp yZp (cid:88) (cid:88) xZp yZp (cid:124) (x, y; ξ, θ)[(x + y) mod p] + (x, y; ξ, θ)[(x + y) mod p] + (cid:88) (cid:88) (cid:18) (cid:88) log xZp yZp j=1 exp(f (x, y; ξ, θ)[j]) (cid:19) 1 (cid:88) (cid:88) (cid:88) xZp yZp j=1 (x, y; ξ, θ)[j] +p2 log p, (cid:123)(cid:122) = ℓAM(ξ, θ) 16 (cid:125) where we use the first-order approximations exp(x) 1 + and log(1 + x) for small x. Following this, we observe that during the initial stage, minimizing the loss in (2.2) is equivalent to optimizing the average margin. This connection underpins the theoretical insights in Morwani et al. (2023), which links the margin maximization problem to empirical observations. Section Roadmap. With slight abuse of notation, we let denote the initial frequency of each neuron (see Assumption 5.1) and use the superscript instead of to simplify the notation further. In the following, we aim to show that (i) the single-frequency pattern, i.e., gm[j] = rm[j] = 0 for all = 2k, 2k + 1, is preserved throughout the gradient flow (see 5.4), and (ii) the phases of the first and second layers will align such that 2ϕ m(t) mod 2π converges to 0 (see 5.5). m(t) ψ"
        },
        {
            "title": "5.4 Preservation of Single-Frequency Pattern",
            "content": "Recall that the dynamics of the parameters are approximately given by the entry-wise ODEs in (5.2a) and (5.2b). Our goal is to lift these entry-wise dynamics into the Fourier domain and show that the single-frequency pattern is preserved. The argument proceeds in three steps: (i) project the entry-wise ODEs onto the Fourier basis Bp to obtain the dynamics of the Fourier coefficients gm and rm; (ii) convert to polar coordinates (αk m) via the chain rule; and (iii) show that m) and (βk the orthogonality of the Fourier basis ensures different frequencies decouple, so that non-feature frequencies initialized at zero remain negligible. We begin with the constant component. Note the constant frequency, i.e., gm[1] and rm[1], remains almost 0 due to the centralized dynamics: m, ψk m, ϕk tθm[j](t), tξm[j](t) span({bτ }p τ =2), [p]. (5.3) By definition, we can show that tgm[1](t) = b1, tθm(t) and trm[1](t) = b1, tξm(t). Given the zero-initialization gm[1] = rm[1] = 0 (see Assumption 5.1), and utilizing (5.3), it follows that tgm[1](t) trm[1](t) 0 s.t. gm[1](t) rm[1](t) 0, (5.4) holds throughout the first stage. Moreover, to establish frequency preservation, we track the magnitudes of each frequency, i.e., {αk m}k[(p1)/2]. Thanks to the orthogonality m}k[(p1)/2] and {βk of the Fourier basis, by applying the chain rule, for each frequency k, it holds that tαk tβk m(t) 2p αk m(t) αk m(t) βk m(t)2 cos (cid:0)2ϕk m(t) cos (cid:0)2ϕk m(t) ψk m(t) ψk m(t)(cid:1), m(t)(cid:1), where the evolution of the magnitudes for frequency only depends on (αk m(0) = 0 for = (see Assumption 5.1), we have the initial value αk m(0) = βk m, βk m, ϕk m, ψk m). Given m(t) βk αk m(t) 0, = k. (5.5) Recall that we define αk m. By combining (5.4) and (5.5), we can establish the preservation of single-frequency pattern (see Figure 14 for experimental results): and βk = (cid:112)2/p gk = (cid:112)2/p rk gm[j](t) rm[j](t) 0, = 2k, 2k + 1. Based on (5.6), we can further simplify (5.2a) and (5.2b) as follows tθm[j](t) 2p α tξm[j](t) α m(t) β m(t)2 cos(ωj + 2ϕ m(t) cos(ωj + ψ m(t)). m(t) ϕ m(t)), 17 (5.6) (5.7) +(D m(t) π 2 ) sin(D m) 0 m(t) π 0 (D m(t) π 2 ) sin(D m) < m(t) (a) Illustration of Phase Alignment Behavior. (b) Dynamics of Magnitudes and Phases for Neuron m. Figure 8: Visualizations of the alignment behavior and neuron evolution dynamics with κinit = 0.02. Figure (a) illustrates the dynamics of the normalized phase difference m(t) given by (5.9). Initialized randomly on the unit circle, the gradient flow will always drive m(t) to 0, regardless of the initial half-space. Figure (b) plots the dynamics of magnitudes and phases of the feature frequency for evolves to align, and magnitudes specific neuron during the initial stage of training. 2ϕ α starts growing rapidly once the phases are well-aligned. and ψ and β For each neuron, its evolution can be approximately characterized by four-particle dynamical system consisting of magnitudes α m(t). We formalize the m(t) and β result in (5.6) and the approximate arguments above into the following theorem. m(t) and phases ϕ m(t) and ψ Theorem 5.2 (Informal). Under the initialization in Assumption 5.1, for given threshold Cend > 0, we define the initial stage as (0, tinit], where tinit := inf{t R+ : maxm[M ] θm(t) ξm(t) Cend}. Suppose that log M/M c1/2 (1 + o(1)), κinit = o(M 1/3) and Cend κinit, given sufficiently small κinit, we have maxk=k inf t(0,tinit] αk m(t) = o(κinit). m(t) βk The formal statement and proof of Theorem 5.2 is provided in B.4. The theorem states that under small random initialization, during the initial training stage where the feature magnitudes remain within constant factor of their starting values, the non-feature frequencies, which are initialized at zero, will not grow beyond negligible o(κinit). We remark that the initial stage is sufficient to understand the dynamics of feature emergence. As we will show in the next section (5.5), constant-order growth of the parameter norms, i.e., maxm[M ] θm(t) ξm(t) κinit, is sufficient to achieve the desired phase alignment."
        },
        {
            "title": "5.5 Neuron-Wise Phase Alignment",
            "content": "We proceed to investigate the emergence of the phase alignment phenomenon. To build intuition, . According to the dynamics given by (5.7), it we first consider special stationary point ψ = 2ϕ is straightforward to observe the stationarity, as: tθm[j](t) cos(ωj + ϕ m(t)), tξm[j](t) cos(ωj + 2ϕ m(t)) = cos(ωj + ψ m(t)). This implies that at the stationary point where θm[j](t) cos(ωj + ϕ m(t)) and ξm[j](t) cos(ωj + m(t)), θm[j](t) and ξm[j](t) evolve in the same direction as themselves. Hence, the phases cease to ψ 18 rotate and remain stationary. Formally, by applying the chain rule over (5.7), we have m(t) π/2}) , m(t)(cid:1) exp (i {ψ exp(iϕ exp(iψ m(t)) 2p β m(t)) α m(t) ψ m(t) sin (cid:0)2ϕ m(t)(cid:1) exp (i {ϕ m(t) ψ m(t) sin (cid:0)2ϕ m(t)2/β . Both are The first line tracks the input phase ϕ m): when phases are misaligned this factor driven by the shared misalignment factor sin(2ϕ is nonzero and drives rotation, while at alignment it vanishes and both phases freeze. The π/2 versus +π/2 in the exponential indicates that the two phases rotate in opposite directions on the unit and ψ circle, converging toward each other. See Figure 8a for an illustration. Thus, phases ϕ evolve in the opposite directions, with rotation speed primarily determined by the magnitudes and will eventually misalignment level, quantified by sin(2ϕ meet ψ m(t)ψ m(t) mod 2π [0, 2π). Using (5.8), the chain rule gives that m(t)). This suggests that 2ϕ m(t) = 2ϕ . To understand the dynamics of the alignment behavior, we track and the second tracks the output phase ψ m(t) + π/2}) . m(t) ψ ψ (5.8) m(t)2/β m(t) α exp(iD m(t)) (cid:0)4β m(t)(cid:1) exp (i{D (cid:123)(cid:122) zero-attractor term Notably, though {0, π} are both stationary points of (5.9), the evolution of m(t) is consistently directed toward 0. This is due to the sign of sin(D m(t)) converges only to zero (see Figure 8a). Thus, we can establish the phase alignment behavior below: m(t)), which adaptively ensures exp(iD m(t) π/2}) (cid:125) m(t)(cid:1) sin (cid:0)D (5.9) (cid:124) . 2ϕ m(t) ψ m(t) mod 2π 0 when . Magnitude Remains Small after Alignment. Note the above analysis hinges on the parameter scale being sufficiently small, ensuring that the dynamics can be fully decoupled neuron-wise and that the approximation error remains negligible, as discussed in 5.3. To complete the argument, it remains to show that α m(t) remain small even after the phase is well-aligned. m(t) and β Under the initialization specified in Assumption 5.1, we can establish the following relationship: m(t)2 1)}1, where m(t) := β m(t)/κinit. m(t)) = sin(D sin(D m(t) measures how much the output magnitude β m(0)) {R m(t) (2R (2R2 Here, has grown relative to its initial value κinit. The identity is an exact conservation law that couples phase alignment to magnitude growth: the product 1) on the right-hand side is monotonically increasing in , so increase in magnitude must be accompanied by proportional decrease in misalignment sin(D m). Therefore, when misalignment level sin(D m(t) is bounded by {sin(D m(t), when the neuron is well-aligned, the parameter scales remain on the same order as at initialization. This aligns with experimental results in Figure 8b. We summarize these findings in the theorem below. m(t)) reaches small threshold δ > 0, the ratio m(0))/δ}1/3. Since α m(t) β Theorem 5.3. Consider the main flow dynamics under the initialization in Assumption 5.1. For any initial misalignment m(0) [0, 2π) and small tolerance level δ (0, 1), the minimal time tδ required for the phase to align such that m(t) δ satisfies that tδ (pκinit)1 (cid:0)1 {sin(D and the magnitude at this time is given by β regime , let ρt = Law(cid:0)ϕ m(t), ψ m(0))/δ}1/3 + max{π/2 m(tδ) κinit {sin(D m(0) π, 0}(cid:1), m(0))/δ}1/3. Moreover, in the mean-field unif, we have m(t)(cid:1) for all R+. Then, given ρ0 = λ where we let λunif denote the uniform law on (0, 2π]. ρ = T#λunif with : φ (cid:55) (φ, 2φ) mod 2π, 19 m(0)), (ii) the extent to which 2 , 3π Theorem 5.3 provides two key insights into the learning dynamics. First, it establishes that the convergence time depends on three key factors: (i) the initial misalignment level, measured by sin(D for (cid:1), and (iii) the initialization scale κinit and modulus p. Second, the theorem provides m(0) (cid:0) π theoretical justification for the emergence of phase symmetry (Observation 3) in the mean-field regime. For the formal theorem, proof sketch, and the complete proof, see Theorem B.7, B.3.1, and B.5, respectively. This result is derived from an analysis of simplified \"main flow\" of the dynamics, which neglects approximation errors and represents the limiting case as κinit (cid:55) 0. This simplified flow is compared visually to the full training dynamics in Figures 8b and 15b. m(0) deviates from the intermediate stage π or 3π"
        },
        {
            "title": "6 Theoretical Extensions",
            "content": "In this section, we extend the results from 5 to two more general scenarios: lottery mechanism under multi-frequency initialization in 6.1 and the dynamics with ReLU activation in 6.2."
        },
        {
            "title": "6.1 Theoretical Underpinning of Lottery Ticket Mechanism",
            "content": "To understand why single frequency pattern emerges from random, multi-frequency initialization (Observation 1), we can analyze the training dynamics for each frequency within specific neuron. The ODEs capture the dynamics of competition in (6.1), which are fully derived in 5.2. m(t)), tαk tβk tDk m(t) tβ0 m(t) βk m(t)2 cos(Dk m(t) αk m(t) 2p αk m(t) αk m(t) (cid:0)4βk m(t) 0. In words, the first two equations state that the magnitudes αk m(t) cos(Dk m(t)), m(t)2/βk m(t)(cid:1) sin(Dk and tα0 grow at rates proportional to cos(Dk and magnitudes grow rapidly, when misaligned (Dk equation governs the misalignment itself: Dk attracted toward zero. Together, these form self-reinforcing loop: and βk m) 1 < π/2), magnitudes decrease. The third m), so it is m): when the phases are well-aligned (Dk decreases at rate proportional to sin(Dk 0), cos(Dk = 0, m(t)), (6.1) Better alignment accelerates growth, and larger magnitudes speed up alignment. key insight from (6.1) is that the dynamics are fully decoupled. The evolution of each frequency is self-contained, proceeding orthogonally without cross-frequency interaction. This structural independence establishes the competitive environment required for the lottery ticket mechanism. The ODEs also reveal powerful reinforcing dynamic: the growth rate, proportional to the alignment term cos(Dk m(t)), is amplified by the magnitudes This creates larger-grows-fasterpositive feedback loop that drives the winners dominance. As introduced in 3.2, this process is not chaotic but is instead predictable competition governed by \"Lottery Ticket Mechanism\". Applying an ODE comparison lemma (Smith, 1995), we can compare the evolution of frequency magnitudes based on their initial conditions. This allows us to formally prove that the \"lottery ticket\" drawn at initialization determines which frequency will ultimately dominate. We formalize the results into the following corollary. Corollary 6.1. Consider multi-frequency initialization akin to Assumption 5.1. For given dominance level ε (0, 1) and fixed neuron m, let tε be the minimal time required for the winning frequency to dominate all others, such that maxk=k βk m(t) ε. Then, it holds that = min (cid:101)Dk m(0), + (c + 1) log + log 1 1ε pκinit {1 2c2π2 (log p/p)2} , m(t)/β tε π2p(2c+3) κinit 20 (a) Heatmaps of parameters after discrete Fourier transform for the first 20 neurons with ReLU activation at the intial stage. (b) Dynamics of magnitude and phase for Neuron with ReLU activation. Figure 9: Learned feature and dynamics of parameters initialized at Assumption 5.1 with = 23 and ReLU activation. Figure (a) shows heatmaps of the parameters after DFT at initialization and at the end of the initial stage. Similar to the quadratic activation (see Figure 14), the single-frequency pattern is approximately maintained, with small values emerging at frequencies 3k, 5k for θm, and 2k, 3k for ξm. Figure (b) plots the dynamics of specific neuron m. Here, the phase grow rapidly and synchronously. quickly aligns, i.e., ψ , and the magnitudes α 2ϕ and β where the bound holds under mild conditions and with high probability of at least 1 (cid:101)Θ(pc). The proof is deferred to C.1. Corollary 6.1 formalizes our Lottery Ticket Mechanism in Observation 6. It states that under multi-frequency random initialization where all frequencies start with identical magnitudes, the frequency with the smallest initial misalignment (cid:101)D will inevitably dominate. This dominance occurs rapidly, on timescale of (cid:101)O(cid:0) log p/(pκinit)(cid:1)."
        },
        {
            "title": "6.2 Dynamics Beyond Quadratic Activation",
            "content": "So far, we have focused on quadratic activation for more precise interpretation. However, experimental results indicate that quadratic activation is not essential or can be even problematic. In practice, quadratic activation often leads to unstable training with highly imbalanced neurons.4 In contrast, ReLU activation consistently leads to the emergence of desired features, as shown in 3. In this section, we investigate the training dynamics of ReLU activation. In parallel, we adopt an experimental setup identical to Training Dynamics of ReLU Activation. that of Figure 14 using the single-frequency initialization specified in Assumption 5.1, with the only 4The failure of the quadratic activation stems from the significant disparity in growth rates among neurons due to the nature of the quadratic function. Specifically, few neurons with more well-aligned initial phases grow faster in magnitude and come to dominate the output, leaving an insufficient growth of other neurons. This issue can be mitigated using techniques such as normalized GD (Cortés, 2006) or spherical GD. 21 modification being the replacement of quadratic activation with ReLU activation. The experimental results are shown in Figure 9, and the key observation is summarized below. Observation 7 (ReLU Leakage). For ReLU activation, although each neuron is initialized with single frequency k, such pattern is preserved approximately with small leakage during the training, with small values emerging at other frequencies. For θm, the values emerges at frequencies 3k, 5k and higher odd multiples, with magnitudes decaying gradually. In contrast, for ξm, these appear at 2k, 3k, and others, which also exhibit decay with increasing multiplicative factors. As shown in Observation 5, ReLU mostly preserves the single-frequency pattern but still exhibits small leakage at other frequencies. For instance, in Figure 9a, Neuron 3 is initialized with dominant frequency 1. After 30,000 training steps, small values emerge at frequencies 3 and 5 in θm, and at 2 and 3 in ξm. In what follows, we first formalize the multiplicative relationship among frequencies. Definition 6.2 (Frequency Multiplication). Given k, τ [ p1 under modulo if τ = rk mod or τ = rk mod for some [ p1 2 ], we say frequency τ is r-fold multiple of 2 ], denoted by τ = rk. Now we are ready to present the main result for training dynamics of ReLU activation. To state , which measures the magnitude of the gradient component at frequency the result, we introduce υ for parameter υ {θm, ξm}. In other words, captures how strongly single gradient step υ pushes energy into frequency k. The proposition compares this push at non-feature frequency to that at the dominant frequency k, where rk denotes the harmonic order of relative to k. Proposition 6.3. Consider gradient update with respect to the decoupled loss ℓm and assume that (θm, ξm) υ = (cid:112)υℓm, b2k2 + υℓm, b2k+12 denote the incremental scale for frequency satisfying (3.1). Let [ p1 2 ]. Under the asymptotic regime where , it holds that θm / / ξm = Θ(r = Θ(r2 = rkk; = (cid:80) ) and ξm (i) θm (ii) ) 1(r is odd), where kυℓm υ for υ {θm, ξm} when ψm = 2ϕm mod p, where j1,j=2k,2k+1 bjb . See C.2 for detailed proof. In words, Part (i) states that the leakage to non-feature frequency k, i.e., the rk-th harmonic of k, decays as 1/r2 relative to the dominant frequency. For the output layer ξm, an additional parity constraint holds: only odd harmonics of receive any leakage, while even harmonics receive zero. Part (ii) states that the gradient component at the feature frequency itself is proportional to the parameter vector, so the feature direction is reinforced without phase rotation consistent with the phase alignment observed for quadratic activation. Here, is the projection onto the Fourier subspace spanned by frequency (i.e., it filters out all other frequencies from the gradient). This provide quantitative explanation of the emergence dynamics of single frequency and phase alignment pattern in Observation 1 and 2."
        },
        {
            "title": "7 Conclusion",
            "content": "In this paper, we provide an end-to-end reverse engineering of how two-layer neural networks learn modular addition, from training dynamics to the final learned model. First, we show that trained networks implement majority-voting algorithm in the Fourier domain through phase alignment and model symmetry. Second, we explain how these features emerge from lottery-like mechanism where frequencies compete within each neuron, with the winner determined by initial magnitude and phase misalignment. Third, we characterize grokking as three-stage process where weight decay prunes non-feature frequencies, transforming perturbed Fourier representation into clean, 22 generalizable solution. These findings offer insights into the dynamics of feature learning in neural networks, mechanism that may extend to more general tasks."
        },
        {
            "title": "References",
            "content": "Allen-Zhu, Z. and Li, Y. (2019). What can resnet learn efficiently, going beyond kernels? Advances in Neural Information Processing Systems, 32. 5 Arnaboldi, L., Dandi, Y., Krzakala, F., Pesce, L., and Stephan, L. (2024). Repetita iuvant: Data repetition allows sgd to learn high-dimensional multi-index functions. arXiv preprint arXiv:2405.15459. 5 Arous, G. B., Erdogdu, M. A., Vural, N. M., and Wu, D. (2025). Learning quadratic neural networks in high dimensions: Sgd dynamics and scaling laws. arXiv preprint arXiv:2508.03688. 15 Ba, J., Erdogdu, M. A., Suzuki, T., Wang, Z., Wu, D., and Yang, G. (2022). High-dimensional asymptotics of feature learning: How one gradient step improves the representation. Advances in Neural Information Processing Systems, 35:3793237946. Berthier, R., Montanari, A., and Zhou, K. (2024). Learning time-scales in two-layers neural networks. Foundations of Computational Mathematics, pages 184. 5 Chen, S. and Li, Y. (2024). Provably learning multi-head attention layer. arXiv:2402.04084. 30 arXiv preprint Chen, S., Wu, B., Lu, M., Yang, Z., and Wang, T. (2025). Can neural networks achieve optimal computational-statistical tradeoff? an analysis on single-index model. In The Thirteenth International Conference on Learning Representations. 5, Cortés, J. (2006). Finite-time convergent gradient flows with applications to network consensus. Automatica, 42(11):19932000. 21 Damian, A., Lee, J., and Soltanolkotabi, M. (2022). Neural networks can learn representations with gradient descent. In Conference on Learning Theory, pages 54135452. PMLR. 5 Ding, X. D., Guo, Z. C., Michaud, E. J., Liu, Z., and Tegmark, M. (2024). Survival of the fittest representation: case study with modular addition. arXiv preprint arXiv:2405.17420. Doshi, D., Das, A., He, T., and Gromov, A. (2023). To grok or not to grok: Disentangling generalization and memorization on corrupted algorithmic datasets. arXiv preprint arXiv:2310.13061. 5, 6, 13, 26 Doshi, D., He, T., Das, A., and Gromov, A. (2024). Grokking modular polynomials. arXiv preprint arXiv:2406.03495. 4 Gromov, A. (2023). Grokking modular arithmetic. arXiv preprint arXiv:2301.02679. 4, 6, 7, 13 Hirsch, M. W. (1982). Systems of differential equations which are competitive or cooperative: I. limit sets. SIAM Journal on Mathematical Analysis, 13(2):167179. Kamke, E. (1932). Zur theorie der systeme gewöhnlicher differentialgleichungen. ii. Acta Mathematica, 58(1):5785. 55 Kramer, B. and MacKinnon, A. (1993). Localization: theory and experiment. Reports on Progress in Physics, 56(12):1469. 26 23 Kumar, T., Bordelon, B., Gershman, S. J., and Pehlevan, C. (2024). Grokking as the transition from lazy to rich training dynamics. In The Twelfth International Conference on Learning Representations. 5, Kunin, D., Marchetti, G. L., Chen, F., Karkada, D., Simon, J. B., DeWeese, M. R., Ganguli, S., and Miolane, N. (2025). Alternating gradient flows: theory of feature learning in two-layer neural networks. arXiv preprint arXiv:2506.06489. 5 Lee, J. D., Oko, K., Suzuki, T., and Wu, D. (2024). Neural network learns low-dimensional polynomials with sgd near the information-theoretic limit. Advances in Neural Information Processing Systems, 37:5871658756. 5, 16 Liu, Z., Kitouni, O., Nolte, N. S., Michaud, E., Tegmark, M., and Williams, M. (2022). Towards understanding grokking: An effective theory of representation learning. Advances in Neural Information Processing Systems, 35:3465134663. 3, 5 Lyu, K., Jin, J., Li, Z., Du, S. S., Lee, J. D., and Hu, W. (2023). Dichotomy of early and late phase implicit biases can provably induce grokking. arXiv preprint arXiv:2311.18817. 5 Mallinar, N., Beaglehole, D., Zhu, L., Radhakrishnan, A., Pandit, P., and Belkin, M. (2024). Emergence in non-neural models: grokking modular arithmetic via average gradient outer product. arXiv preprint arXiv:2407.20199. McCracken, G., Moisescu-Pareja, G., Letourneau, V., Precup, D., and Love, J. (2025). Uncovering universal abstract algorithm for modular addition in neural networks. arXiv preprint arXiv:2505.18266. 5 Mohamadi, M. A., Li, Z., Wu, L., and Sutherland, D. J. (2024). Why do you grok? theoretical analysis of grokking modular addition. arXiv preprint arXiv:2407.12332. 5 Morwani, D., Edelman, B. L., Oncescu, C.-A., Zhao, R., and Kakade, S. (2023). Feature emergence via margin maximization: case studies in algebraic tasks. arXiv preprint arXiv:2311.07568. 4, 5, 6, 14, 15, 16, 17 Nanda, N., Chan, L., Lieberum, T., Smith, J., and Steinhardt, J. (2023). Progress measures for grokking via mechanistic interpretability. arXiv preprint arXiv:2301.05217. 3, 4, 6, 7, 11, Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z., Gimelshein, N., Antiga, L., et al. (2019). Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32. 6 Power, A., Burda, Y., Edwards, H., Babuschkin, I., and Misra, V. (2022). Grokking: Generalization beyond overfitting on small algorithmic datasets. arXiv preprint arXiv:2201.02177. 3, 4 Ren, Y., Nichani, E., Wu, D., and Lee, J. D. (2025). Emergence and scaling laws in sgd learning of shallow neural networks. arXiv preprint arXiv:2504.19983. 5 Shi, Z., Wei, J., and Liang, Y. (2022). theoretical analysis on feature learning in neural networks: Emergence from inputs and advantage over fixed features. arXiv preprint arXiv:2206.01717. Shi, Z., Wei, J., and Liang, Y. (2023). Provable guarantees for neural networks via gradient feature learning. Advances in Neural Information Processing Systems, 36:5584855918. 5 24 Smith, H. L. (1995). Monotone dynamical systems: an introduction to the theory of competitive and cooperative systems: an introduction to the theory of competitive and cooperative systems. Number 41. American Mathematical Soc. 20, 55 Sundararajan, D. (2001). The discrete Fourier transform: theory, algorithms and applications. World Scientific. Tian, Y. (2024). Composing global optimizers to reasoning tasks via algebraic objects in neural nets. arXiv preprint arXiv:2410.01779. 4, 5, 6, 15, 59, 60 Tian, Y. (2025). framework on dynamics of feature emergence and delayed generalization. arXiv preprint arXiv:2509.21519. 5 Wang, P. and Wang, Z. (2025). Why neural network can discover symbolic structures with gradientbased training: An algebraic and geometric foundation for neurosymbolic reasoning. arXiv preprint arXiv:2506.21797. 4, 5, 59, 60 Wu, W., Jaburi, L., jacob drori, and Gross, J. (2025). Towards unified and verified understanding of group-operation networks. In The Thirteenth International Conference on Learning Representations. Yip, C. H., Agrawal, R., Chan, L., and Gross, J. (2024). Modular addition without black-boxes: Compressing explanations of mlps that compute numerical integration. arXiv preprint arXiv:2412.03773. 5, 7 Zhong, Z., Liu, Z., Tegmark, M., and Andreas, J. (2023). The clock and the pizza: Two stories in mechanistic explanation of neural networks. Advances in neural information processing systems, 36:2722327250. 3, 4, 13,"
        },
        {
            "title": "A Additional Experimental Details and Results",
            "content": "A.1 Detailed Interpretation of Grokking Dynamics in Section 3.3 Inverse Participation Ratio (IPR). To quantitatively characterize the concentration of Fourier coefficients at specific frequency k, or equivalently, the sparsity level of the learned parameters in the Fourier domain, we introduce the inverse participation ratio (IPR). This metric, originally used in physics as localization measure (Kramer and MacKinnon, 1993), was recently adopted in Doshi et al. (2023) as progress measure to understand the generalization behavior in machine learning. Specifically, given ν Rd, the IPR is defined as IPR(ν) = (ν2r/ν2)2r for some integer > 1. We calculate the IPR for all {θm}m[M ] and {ξm}m[M ], and take the average. Definition of Progress Measure. Here, we provide formal definition of the progress measure for grokking used in Figure 5, which is defined over the model output and parameters θms and ξms. - Loss : ℓD = (cid:88) (cid:10) log smax (x, y; ξ, θ), e(x+y) mod (cid:11); 1 (cid:88) (x,y)D IPRθ,ξ = - Accuracy : AccD = - IPR : - ℓ2-norm : (x,y)D 1 (cid:8)argmax(cid:0)smax (x, y; ξ, θ)(cid:1) = (x + y) mod p(cid:9); 1 2M (cid:32) (cid:88) m=1 B θm4 θm (cid:33)4 + 1 2M (cid:32) (cid:88) B ξm4 ξm2 (cid:33)4 ; ℓ2-normθ,ξ = 1 2M (θm2 + ξm2). m=1 (cid:88) m=1 Three-Phase Dynamics of Grokking. As discussed in 3.3, the grokking process is governed by the interplay between two primary forces: loss minimization and weight decay. The dynamics unfold across three major phases: an initial memorization stage dominated by the loss gradient, followed by two distinct generalization stages where the balance between these forces shifts. Below, we provide more detailed account of each phase by examining our key progress measures. - Phase I: Memorization. Initially, the network quickly memorizes the training data, reaching 100% accuracy. Test accuracy also improves to around 70%, aided by the models symmetric architecture. Figure 10 provide clear empirical evidence for this perfect memorization. The model achieves flawless accuracy and high confidence on the training data (dark blue entries) and test data whose symmetric counterparts were part of the training set (light blue entries). Note that the model completely fails on the truly \"unseen\" held-out test data (white entries outlined in red), confirming it has learned to exploit symmetry rather than achieving true generalization at this stage. During this time, feature frequencies become roughly aligned (see Figure 5c) and their sparsity increases significantly (see Figure 5d). While these dynamics resemble full-data setup, the incomplete data yields perturbed Fourier solution that overfits the training set. - Phase II: Loss-Driven Norm Growth with Rapid Feature Cleanup. After reaching perfect training accuracy, the models parameters continue evolving to further reduce the loss. Instead of naively amplifying parameter magnitudes, weight decay actively steers their direction. As shown in Figure 5d, the dynamic is thus balancing act: the loss gradient pushes to scale up parameters, while weight decay prunes unnecessary frequencies to decelerate the growth of norm. Figure 10: Heatmaps of trained model from Figure 5 at the end of the memorization stage. The left panel displays the data distribution: dark blue entries represent training data, light blue entries are test data whose symmetric counterparts are in the training set, and white entries (outlined in red) are the remaining held-out test data. The middle panel shows the models accuracy, demonstrating that it has perfectly memorized all training data and their symmetric variants but completely fails to generalize to the held-out data. Finally, the right panel visualizes the models post-softmax output on the correct answer for each data point, further confirming the accuracy results. Figure 11: Data distribution during the memorization stage. The first panel illustrates the data partitioning, which, unlike in Figure 10, uses the following scheme: white entries denote test data, dark brown entries represent common (symmetric) training data, and light brown entries (outlined in red) denote rare (asymmetric) training data. The remaining three plots track the models accuracy, demonstrating two-stage memorization scheme. At initialization, the model performs at low, chance-level accuracy. However, after approximately 1000 steps, it masters the common symmetric training data, but its performance on rare asymmetric data drops to zero, overwriting any initially correct random predictions. By the end of the memorization stage, the model finally memorizes these rare data points, achieving 100% training accuracy - Phase III: Slow Cleanup Driven Solely by Weight Decay. By the end of Phase II, training loss is near-zero and test accuracy approaches 100%. Thus, in the final stage, the diminished loss gradient allows weight decay to dominate, causing the parameter norm to decrease (see Figure 5d). Without the main driving force of the loss, this final cleanup\" phase is extremely slow (see Figure 5b), during which test accuracy gradually converges to 100%. 27 Figure 12: Heatmaps of parameters after applying discrete Fourier transform along training epoches for the first 20 neurons with = 23 under train-test split setup. At the end of the memorization stage (step 2200), single-frequency pattern has started to emerge, accompanied by noisy perturbations in other frequencies. This initial \"perturbed Fourier solution\" is subsequently refined, as weight decay prunes the noisy, non-feature frequencies to reveal the final, clean pattern. A.2 Ablations Studies for Fully-Diversified Parametrization In this section, we present comprehensive ablation studies investigating the efficiency of the fully diversified parametrization as defined in Definition 4.1. We evaluate the models based on the CE loss defined in Equation 2.2 while maintaining fixed, equivalent computational budget. All predictors share fixed neuron constraint = 128 and scale αmβ2 = 1 for all [M ]. The ablation is performed across two distinct dimensions of the diversification strategy: Ablation of Frequency Diversification. We examine the impact of restricting the number 2 ] with = {1, 2, 4, 8}. of learned frequencies. We use only subset of frequencies [ p1 The phases for each selected frequency are kept uniformly distributed over [0, 2π). Ablation of Phase Uniformity. We investigate the effect of restricting the range of the phase distribution. The model utilizes the full set of frequencies, but the phase for each frequency is uniformly distributed over restricted interval [0, ιπ) with ι {0.4, 0.8, 1.2, 1.6}. The ablation study results in Table 2 confirm that full frequency and phase diversification is essential for maximizing parametrization efficiency under fixed constraints. Part shows that the CE loss decreases rapidly as the number of frequencies increases, dropping from 1.64 at = 1 to 7.41 1015 for the full frequency set, underscoring the critical role of spectral richness. Part II reveals that restricting the phase distribution range significantly degrades performance. For instance, the loss is 4.82 for [0, 0.4π) but achieves the minimum of 7.41 1015 only when the phases span the full [0, 2π) interval. These findings collectively validate that the fully diversified parametrization achieves the maximum efficiency. Visually, this maximum efficiency is confirmed in Figure 13, where the fully diversified parametrization generates the highest confidence prediction 28 Part I: Frequency Diversity Ablation."
        },
        {
            "title": "Full Freqs",
            "content": "1.64 6.02 101 2.88 102 2.99 108 7.41 1015 2.01 102 8.79 102 1.55 102 1.07 107 Part II: Phase Diversity Ablation. [0, 0.4π) [0, 0.8π) [0, 1.2π) [0, 1.6π) [0, 2π)"
        },
        {
            "title": "Loss",
            "content": "4.82 2.00 103 1.19 109 3.54 107 7.41 1015 Table 2: Performance of the predictor under different ablation configurations. For the frequency ablation study, the average and standard deviation of the loss are reported across all possible combinations of frequencies of the specified size K. The results show that the fully diversified parametrization achieves the lowest CE loss, confirming its maximum efficiency under the fixed = 1 and neuron budget = 128. constraints of model scale αmβ2 Figure 13: Output logits for the predictor under different ablation configurations, evaluated across four distinct query points (x, y). The true prediction label is indicated by the dashed vertical line in each panel. The fully diversified parametrization yields the largest logit gap between the ground truth and incorrect labels, signifying maximal prediction confidence. by creating the largest logit gap between the ground truth label and all incorrect alternatives. Please refer to Figure 13 for visualizations of model outputs under different ablation configurations. A.3 Training Dynamics with Quadratic Activation To under the training dynamics with quadratic activation, we set = 23 and use two-layer neural network with width = 512. The network is trained using SGD optimizer with step size η = 104, initialized under Assumption 5.1 with initial scale κinit = 0.02. Figure 14: Heatmaps of parameters after applying discrete Fourier transform along training epoches for the first 20 neurons initialized under Assumption 5.1 with = 23 and quadratic activation. At the initial stage, these neurons preserve the single-frequency pattern by evolving only the Fourier coefficients corresponding to the initial frequency k, while keeping the others 0 throughout. As shown in Figure 14, single-frequency pattern is preserved throughout the training process. This empirical result aligns with our theoretical findings in Theorem 5.2, which states that under sufficiently small initialization, the single-frequency structure will remain stable during the initial stage of training. In other words, the neurons are fully decoupled and the main flow dominates. Proof of Results in Section 4 and 5 B.1 Proof of Proposition 4. We first introduce useful lemma about the softmax operation. Lemma B.1. Let ν Rd. If = argmaxi νi and νi νi τ for all = i, then smax(ν) ei1 1 exp(τ ) + (d 1) . Proof of Lemma B.1. See Lemma 3.6 in Chen and Li (2024) for detailed proof. Now we are ready to present the proof of Proposition 4.2. 30 Proof of Proposition 4.2. Let [m] be the logit contributed by neuron m, and fix [p]. Under the parametrization in (3.1) and the phase-alignment condition 2ϕm ψm = 0 mod 2π, we have [m](x, y; ξ, θ)[j] cos(ωφ(m)j + 2ϕm) (cid:0)cos(ωφ(m)x + ϕm) + cos(ωφ(m)y + ϕm)(cid:1)2 = αmβ2 = 2a cos(ωφ(m)(x y)/2)2 cos(ωφ(m)j + 2ϕm) {1 + cos(ωφ(m)(x + y) + 2ϕm)} = cos(ωφ(m)(x y)/2)2 {2 cos(ωφ(m)j + 2ϕm) + cos(ωφ(m)(x + j)) + cos(ωφ(m)(x + + j) + 4ϕm)}, where the second equality uses the homogeneous scaling, i.e., condition (ii) in Definition 4.1. Next, summing over all neurons in the frequency-group Nk, gives [m](x, y; ξ, θ)[j] (cid:88) mNk = cos(ωk(x y)/2)2 cos(ωk(x + j)) (cid:125) (cid:123)(cid:122) (cid:124) condition (i): Nk = (cid:8)2 cos(ωkj + 2ϕm) + cos(ωk(x + + j) + 4ϕm)(cid:9) + cos(ωk(x y)/2)2 (cid:88) mNk (cid:124) = aN/2 cos(ωk(x + j)) + aN/4 {cos(ωk(2x j)) + cos(ωk(2y j))}, (cid:123)(cid:122) = 0 due to condition (iii) (cid:125) (B.1) where the second equality follows from the balanced-frequency and the high-order phase-symmetry conditions (i) and (iii) in Definition 4.1. Summing (B.1) over all frequency yields (x, y; ξ, θ)[j] = (p1)/2 (cid:88) (cid:88) k=1 mNk [m](x, y; ξ, θ)[j] = aN/2 (p1)/2 (cid:88) k=1 cos(ωk(x + j)) (cid:26) (p1)/2 (cid:88) + aN/4 cos(ωk(2x j)) + (p1)/2 (cid:88) k=1 cos(ωk(2y j)) . (cid:27) (B.2) k=1 By symmetry, for any fixed N, (cid:80)(p1)/2 k=1 cos(ωkz) = (p 1)/2 if = 0 mod else 1/2. Then, (p1)/2 (cid:88) k=1 cos(ωkz) = 1 2 + 2 1(z mod = 0). (B.3) Thus, by combining (B.2) and (B.3), we can conclude that (x, y; ξ, θ)[j] = aN/2 (cid:8) 1 + p/2 1(x + mod = j) + p/4 (cid:88) 1(2z mod = j)(cid:9), [p]. z{x,y} Note that when = y, the true-signal logit at = (x + y) mod exceeds all others by aN p/8, and when = y, the margin is even larger. Applying Lemma B.1 yields smax (x, y; ξ, θ) e(x+y) mod p1 1 exp(aN p/8) + 1 exp(aN p/8). Hence, to achieve error ϵ, it suffices to choose (N p)1 log(p/ϵ), which completes the proof. 31 B.2 Preliminary: Gradient Computation Recall the logit of the two-layer neural network in (2.1) takes the form: (x, y) := (x, y; ξ, θ) = (cid:88) m= ξm σ(ex + ey, θm) Rp. (B.4) For theoretical analysis, we consider the training dynamics over the full dataset Dfull = {(x, y, z) x, Zp, = (x + y) mod p} and the corresponding CE loss, defined in (2.2), can be written as ℓ := ℓ(ξ, θ; D) = (cid:88) (cid:88) xZp yZp (cid:10) log smax (x, y; ξ, θ), e(x+y) mod (cid:11) (cid:32) = (cid:88) (cid:88) log xZp yZp exp(f (x, y)[(x + y) mod p]) j=1 exp(f (x, y)[j]) (cid:80)p (cid:33) (cid:88) (cid:88) = xZp yZp (cid:124) (x, y)[(x + y) mod p] + (cid:88) (cid:88) (cid:18) (cid:88) log exp(f (x, y)[j]) . (cid:19) (B.5) (cid:123)(cid:122) := (cid:101)ℓ yZp xZp (cid:124) (cid:125) j=1 (cid:123)(cid:122) := ℓ (cid:125) Following the loss decomposition in (B.5), we compute the gradients of these two parts respectively. Recall that the two-layer neural network is parametrized by ξ = {ξm}m[M ] and θ = {θm}m[M ] with ξm, θm Rp. By substituting the form of in (B.4) into (cid:101)ℓ and ℓ, we have (cid:88) (cid:88) (cid:88) (cid:101)ℓ = ξm[(x + y) mod p] σ(ex + ey, θm), xZp yZp m=1 (cid:32) (cid:88) (cid:88) (cid:88) ℓ = log (cid:18) (cid:88) exp ξm[j] σ(ex + ey, θm) (cid:19)(cid:33) . xZp yZp j=1 m=1 Fix neuron [M ]. First, we calculate the gradients for (cid:101)ℓ. By direct calculation, we have ξm (cid:101)ℓ = (cid:88) (cid:88) xZp yZp e(x+y) mod σ(ex + ey, θm). Following this, the entry-wise derivative with respect to ξm[j] satisfies that (cid:101)ℓ ξm[j] = (cid:88) σ(ex + ey, θm) := x,yZp:(x+y) mod p=j (cid:88) (x,y)Sp σ(ex + ey, θm). (B.6) Here, we define compute the gradient with respect to θm, following that = {x, Zp : (x + y) mod = j} for notational simplicity. Similarly, we can θm (cid:101)ℓ = (cid:88) (cid:88) ξm[(x + y) mod p] (ex + ey) σ(ex + ey, θm) yZp xZp (cid:88) = (cid:88) ex ξm[(x + y) mod p] σ(ex + ey, θm), xZp yZp where the last equality uses the symmetry of and y. Hence, the entry-wise derivative follows (cid:101)ℓ θm[j] = 2 (cid:88) xZp ξm[mp(x, j)] σ(ex + ej, θm), (B.7) where we re-index = and to simplify the form. Next, we compute the gradients for ℓ. Following similar argument in (B.7) and (B.6), based on the chain rule, it holds that ℓ ξm[j] = (cid:88) (cid:88) xZp yZp exp (cid:0) (cid:80)M i=1 exp (cid:0) (cid:80)M (cid:80)p m=1 ξm[j] σ(ex + ey, θm)(cid:1) m=1 ξm[i] σ(ex + ey, θm)(cid:1) σ(ex + ey, θm). In addition, by direct calculation, we can obtain that ℓ θm[j] = 2 (cid:88) (cid:88) exp (cid:0) (cid:80)M i=1 exp (cid:0) (cid:80)M m=1 ξm[τ ] σ(ex + ej, θm)(cid:1) m=1 ξm[i] σ(ex + ej, θm)(cid:1) ξm[τ ] (cid:80)p τ =1 xZp σ(ex + ej, θm), (B.8) (B.9) where the last equality results from re-indexing = j, x, and i. Throughout the section, we consider quadratic activation σ(x) = x2 for theoretical convenience. B.3 Main Flow Approximation under Small Parameter Scaling The key property used in Stage is that the scale of parameters is relatively small due to the small initialization and sufficiently small constant a. Following this, we have the approximation below: (cid:0)smax (x, y; ξ, θ)(cid:1)[j] = exp (cid:0) (cid:80)M i=1 exp (cid:0) (cid:80)M m=1 ξm[j] ex + ey, θm2(cid:1) m=1 ξm[i] ex + ey, θm2(cid:1) (cid:80)p 1 , [p]. (B.10) To formalize the approximation above, we introduce the following approximation error terms: (cid:32) (cid:88) (cid:88) Err(1) m,j = xZp yZp (cid:88) (cid:88) Err(2) m,j = xZp τ =1 (cid:32) (cid:80)p exp (cid:0) (cid:80)M i=1 exp (cid:0) (cid:80)M exp (cid:0) (cid:80)M i=1 exp (cid:0) (cid:80)M (cid:80)p m=1 ξm[j] ex + ey, θm2(cid:1) m=1 ξm[i] ex + ey, θm2(cid:1) m=1 ξm[τ ] ex + ej, θm2(cid:1) m=1 ξm[i] ex + ej, θm2(cid:1) (cid:33) 1 ex + ey, θm2, (cid:33) 1 ξm[τ ] ex + ey, θm, for all (j, m) [p] [M ]. The approximation result is formalized in the following lemma. Lemma B.2. Denote θ = maxm θm and ξ = maxm] ξm. For all (j, m) [p] [M ], the approximation error is upper bounded by Err(1) m,j Err(2) m,j 8p θm max{ξm, θm} (exp(8M ξ θ2 ) 1). Proof of Lemma B.2. Let sj(x, y) = (cid:80)M m=1 ξm[j] ex + ey, θm2 denote the score given by the neural network for the j-th entry. Then, for fixed (x, y), the softmax vector for j-th entry is given by p(x, y)[j] = exp(s(x, y)[j])/ (cid:80)p i=1 exp(s(x, y)[i]). Note that, for any (m, j) [M ] [p], we have Err(1) m,j = (cid:88) (cid:88) xZp yZp (cid:18) p(x, y)[j] (cid:19) 1 ex + ey, θm2 4p2 max (x,y)Z2 (cid:12) (cid:12) (cid:12) (cid:12) p(x, y)[j] (cid:12) (cid:12) (cid:12) (cid:12) 1 θm2 . (B.11) 33 Let x,y = maxj[p] s(x, y)[j] minj[p] s(x, y)[j] > 0 for any (x, y) Z2 that x,y can be effectively bounded by the scales of θms and ξms, following that . It is straightforward to see x,y 2 (cid:88) ξm[j] ex + ey, θm (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) m=1 (cid:13) (cid:13) (cid:13) (cid:13) (cid:13) 8M ξ θ2 , (x, y) Z2 p. (B.12) Following this, we upper bound the difference between the softmax-induced distribution and the uniform distribution using the small-scale score vector. By simple algebra, we can show that (cid:12) (cid:12) p(x, y)[j] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 max j[p] = (cid:12) (cid:12) min (cid:12) (cid:12) j[p] (cid:12) (cid:12) (cid:12) (cid:12) (cid:95) 1 max j[p] p(x, y)[j] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 (cid:12) (cid:12) 1 + (p 1) exp(x,y) (cid:12) (cid:26) exp(x,y) 1 1 exp(x,y) + p(x, y)[j] (cid:12) (cid:12) (cid:12) (cid:12) 1 (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:95) 1 1 + (p 1) exp(x,y) 1 (cid:95) 1 exp(x,y) (cid:27) (cid:12) (cid:12) (cid:12) (cid:12) 1 exp(x,y) + 1 1 (exp(x,y) 1) max {exp(x,y), 1} 1 (exp(x,y) 1). (B.13) By combining (B.11), (B.12) and (B.13), we can reach the conclusion that Err(1) m,j 4p θm2 (exp(8M ξ θ ) 1). Building upon similar argument, it holds that Err(2) m,j = 2 (cid:88) (cid:88) xZp τ =1 (cid:12) (cid:12) p(x, j)[τ ] (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) 1 ξm[τ ] ex + ey, θm 8p θm ξm (exp(8M ξ θ2 ) 1). Hence, we complete the proof of bounded approximation error. Lemma B.2 formalizes key technical tool for analyzing the dynamics during the initial stage: given small-scale parameters θms and ξms, and specified small constant (introduced for technical convenience), the softmax components in the gradient can be effectively approximated by uniform vector, with controllable and small approximation error. In the following sections, we denote Err(1) = (Err(1) m,j)j[p] Rp for notational simplicity and we remark that the error vectors would vary along the grdient flow. m,j)j[p] Rp and Err(2) = (Err(2) B.3.1 Proof Overview: Simplified Dynamics under Approximation Before delving into the technical details, we provide brief summary of the approximate dynamics of parameters and their transformations along gradient flow in Table 3. This overview characterizes the training during the initial phase, when parameter magnitudes are small. We use to highlight the central flow, omitting the perturbations introduced by approximation errors as defined in (B.10). The simplification of the approximate dynamics leverages two key features that arise under the specialized initialization in Assumption 5.1: neuron-wise decoupled loss landscapemeaning the evolution of each neuron depends only on itselfand preservation of single-frequency structure i.e., the parameters exhibit only one frequency component in the Fourier domain. These properties hold during the early stage of training. Refer to 5.2 for detailed illustration and proof sketch. With slight abuse of notation, we let denote the initial frequency of each neuron and we use the superscript instead of to simplify the notation in Table 3. 34 In Part I, we present the dynamics of original parameters{θm}m[M ] and {ξm}m[M ] Roadmap. with calculation details provided in B.3.2 and B.4. In Part II, building on the results from B.4, we shift focus to the dynamics of the discrete Fourier coefficients, defined in 5.1, to better understand the evolution of parameters in the Fourier domain. Finally, based on the results in Part and Part II, we analyze the dynamics of the magnitudes and phases of the Fourier signals (see 5.1 for definitions), to interpret the alignment behavior between θm and ξm, and the detailed derivations are provided in B.5. The auxiliary equalities naturally arise from the definition of discrete Fourier coefficients and their transformations. Part I: Dynamics of Original Parameters. θm[j](t) ξm[j](t) tθm[j](t) 2p α m(t) β tξm[j](t) α m(t) cos(ωkj + ψ m(t)2 cos(ωkj + 2ϕ m(t) ϕ m(t)), m(t)), [p] [p] Part II: Dynamics of Dicrete Fourier Coefficients. tgm[2k](t) tgm[2k + 1](t) gm[2k](t) gm[2k + 1](t) rm[2k](t) rm[2k + 1](t) Part III: Dynamics of Magnitudes and Phases. trm[2k](t) p3/2/ trm[2k + 1](t) p3/2/ 2 p3/2 α 2 p3/2 α m(t) β m(t) ϕ m(t) cos (cid:0)ψ m(t)(cid:1) m(t) ϕ m(t)(cid:1) m(t)2 sin (cid:0)2ϕ m(t) sin (cid:0)ψ m(t)2 cos (cid:0)2ϕ 2 α m(t) β 2 α m(t)(cid:1) m(t)(cid:1) α m(t) β m(t) ϕ m(t) ψ m(t) m(t)1 m(t)(cid:1) m(t) cos (cid:0)2ϕ tα m(t) β m(t) α m(t) 2p α tβ m(t) sin (cid:0)2ϕ m(t)) 2p β m(t) sin (cid:0)2ϕ m(t)2 m(t)) α β m(t)2 m(t) + α 4β m(t)) β m(t) m(t)2 cos (cid:0)2ϕ m(t) ψ m(t) ψ (cid:17) sin(D (cid:16) m(t) ψ m(t)(cid:1) m(t) ψ m(t)(cid:1) exp (i {ϕ m(t)(cid:1) exp (i {ψ m(t)) exp (i {D exp(iϕ exp(iψ exp(iD m(t) π/2}) m(t) + π/2}) m(t) π/2}) Part IV: Auxiliary Equalities. cos(ϕ cos(ψ m(t)) = (cid:112)2/p gm[2k](t)/α m(t)) = (cid:112)2/p rm[2k](t)/β m(t) denote the phase misalignment level defined as m(t), m(t), m(t)) = (cid:112)2/p gm[2k + 1](t)/α m(t)) = (cid:112)2/p rm[2k + 1](t)/β m(t) = 2ϕ m(t) mod 2π. m(t) ψ sin(ψ sin(ϕ m(t), m(t). 1 We use Table 3: Summarization of the approximate dynamics during the initial stage. Please refer to B.3.2, B.4 and B.5 for formalized arguments and detailed derivations. B.3.2 Proof of Lemma B.3: Main Flow of Decoupled Neurons Lemma B.3 (Main Flow). Consider the discrete Fourier coefficients, as well as the signal magnitudes and phases, defined over {θm}m[M ] and {ξm}m[M ] (see 5.1 for definitions). Then, at each time R+ and [M ], the gradient dynamics takes the following form: tξm[j](t) = (p1)/2 (cid:88) k= m(t)2 cos(ωkj + 2ϕk αk m(t)) Err(1) m,j(t), 35 tθm[j](t) = 2p (p1)/2 (cid:88) k=1 m(t) βk αk m(t) cos(ωkj + ψk m(t) ϕk m(t)) Err(2) m,j(t), where the approximation errors Err(1) m,j(t) and Err(2) m,j(t) are defined in B.3 Lemma B.3 indicates that the dynamics of θm(t)s and ξm(t)s only depend on θm(t) and ξm(t) such that the neurons are almost fully decoupled with small approximation errors. Proof of Lemma B.3. Consider fixed neuron m. By combining the gradient computations in (B.6) and (B.8), we can write the complete form of derivative of loss ℓ with respect to ξm[j] as ℓ ξm[j] = (cid:101)ℓ ξm[j] (cid:88) = + ℓ ξm[j] (x,y)Sp σ(ex + ey, θm) + 1 (cid:88) (cid:88) xZp yZp σ(ex + ey, θm) + Err(1) m,j. (B.14) Similarly, by combining (B.7) and (B.9), we have the derivative of ℓ with respect to θm[j]: ℓ θm[j] = (cid:101)ℓ θm[j] + ℓ θm[j] = 2 (cid:88) xZp ξm[mp(x, j)] σ(ex + ej, θm) + 2 (cid:88) (cid:88) xZp τ =1 ξm[τ ] σ(ex + ej, θm) + Err(2) m,j. (B.15) Motivated by Lemma B.3, we focus on the dominant terms of the gradient and carefully manage the error terms to characterize the central flow that determines the main dynamics in the initial stage. Step 1: Deriving Gradient of ξm. By switching from the standard canonical basis to the Fourier basis, we can write θm using form of discrete Fourier expansion, as shown in (3.2). Then, we have (cid:88) (x,y)Sp σ(ex + ey, θm) = (cid:32) (cid:88) 2α0 + (p1)/2 (cid:88) (cid:88) αk (x,y)Sp k=1 z{x,y} (cid:33)2 cos(ωkz + ϕk m) = 4p (α m)2 + (p1)/2 (cid:88) (αk m)2 (i) + (cid:88) mατ αk (ii) k=1 1k=τ (p1)/2 + 2α0 (p1)/2 (cid:88) k=1 αk (iii). (B.16) where we denote each term as (i) = (ii) = (iii) = (cid:32) (cid:88) (cid:88) (cid:33) cos(ωkz + ϕk m) , (cid:88) z{x,y} cos(ωτ + ϕτ m), (x,y)Sp (cid:88) z{x,y} (cid:88) cos(ωkz + ϕk m) (x,y)Sp (cid:88) z{x,y} (cid:88) (x,y)Sp z{x,y} cos(ωkz + ϕk m). xZp m) = (cid:80) In the following, we compute (i), (ii) and (iii) respectively using trigonometric identities and the periodicity of the module addition task over the full space Z2 . First, note that (i) = 2 (cid:88) xZp cos(ωkx + ϕk m)2 + 2 (cid:88) cos(ωkx + ϕk m) cos(ωky + ϕk m) = + (cid:88) xZp cos(ω2kx + 2ϕk m) + (x,y)Sp (cid:88) (x,y)Sp cos(ωk(x + y) + 2ϕk m) + (cid:88) (x,y)Sp cos(ωk(x y)) = (1 + cos(ωkj + 2ϕk m)), where the last equality uses the fact that (cid:80) cos(ωk(x + y) + 2ϕk m) = cos(ωkj + 2ϕk (x,y)Sp cos(ωk(x y)) = (cid:80) cos(ωkx) = 0 and xZp . Following similar argument, we have m) for all (x, y) (cid:88) cos(ωkx + ϕk m) cos(ωτ + ϕτ m) + 2 cos(ωkx + ϕk m) cos(ωτ + ϕτ m) (B.17) cos((ωkx + ωτ y) + ϕk + ϕτ m) + (x,y)Sp (cid:88) + xZp cos((ωk + ωτ )x + ϕk + ϕτ m) + (x,y)Sp (cid:88) cos((ωkx ωτ y) + ϕk ϕτ m) (x,y)Sp (cid:88) cos((ωk ωτ )x + ϕk ϕτ m) = 0, (B.18) (ii) = = (cid:88) xZp (cid:88) where we use (cid:80) m) in + ϕτ the last inequality for the first term and similar arguent for the second one. In addition, it is easy to show that (iii) = 2 (cid:80) m) = 0. By combining (B.16), (B.17) and (B.18), we have cos((ωk ωτ ) + ωτ + ϕk cos((ωkx + ωτ y) + ϕk cos(ωkx + ϕk + ϕτ (x,y)Sp xZp xZp (cid:88) (x,y)Sp σ(ex + ey, θm) = 4p (α0 m)2 + (p1)/2 (cid:88) (αk m)2 (1 + cos(ωkj + 2ϕk m)). k=1 Following this, based on (B.14), the simplified derivative of each entry takes the form ℓ ξm[j] Err(1) m,j = (cid:88) (x,y)Sp σ(ex + ey, θm) + 1 (cid:88) (cid:88) j=1 (x,y)Sp σ(ex + ey, θm) = (p1)/2 (cid:88) (αk m)2 cos(ωkj + 2ϕk m), [p]. k= Step 2: Deriving Gradient of θm. Next, we calculate the gradient of θm, following procedure analogous to the one in Step 1. To begin, we consider the expression: ξm[mp(x, j)] σ(ex + ej, θm) = 2 (cid:88) xZp (cid:88) xZp (cid:124) ξm[mp(x, j)] θm[x] +2 θm[j] (cid:123)(cid:122) (iv) (cid:125) (cid:124) (cid:88) xZp (cid:123)(cid:122) (v) ξm[x] . (B.19) (cid:125) Term (iv) can be decomposed and simplified using the fourier expansions of ξm and θm in (3.2). By carefully applying cosine product identities and rearranging the terms, we have (iv) = (cid:32) (cid:88) xZp β0 + (p1)/2 (cid:88) k=1 cos(ωk mp(x, j) + ψk βk m) (cid:33) 37 (cid:32) α + cos(ωkx + ϕk αk m) (cid:33) (p1)/2 (cid:88) k= = α0 β0 + (p1)/2 (cid:88) k=1 mβk αk (iv.1) + α0 (p1)/2 (cid:88) k=1 βk (iv.2) (cid:88) + 1k=τ (p1)/2 mβτ αk (iv.3) + β0 (p1)/2 (cid:88) k=1 αk (iv.4). where we denote each term as (iv.1) = (iv.3) = (cid:88) xZp (cid:88) xZp cos(ωk mp(x, j) + ψk m) cos(ωkx + ϕk m), cos(ωτ mp(x, j) + ϕτ m) cos(ωkx + ψk m), (iv.2) = (iv.4) = (cid:88) xZp (cid:88) xZp cos(ωk mp(x, j) + ψk m), cos(ωk mp(x, j) + ϕk m). Analogous to (B.17) and (B.18), using the trigonometric identities and periodicity of the module addition task, we have (iv.2) = (iv.3) = (iv.4) = 0, and for the first term we can show that (iv.1) = (cid:88) xZp cos(ωk mp(x, j) + ψk m) cos(ωkx + ϕk m) = cos(ωkj + ψk ϕk m). By combining the arguments above, we can conclude that (iv) = α0 β + (p1)/2 (cid:88) k=1 mβk αk cos(ωkj + ψk ϕk m). Besides, by substituting the fourier expansions of ξm into (v), it holds that (v) = θm[j] (cid:88) xZp (cid:32) β0 + (p1)/2 (cid:88) cos(ωkx + ψk βk m) (cid:33) = θm[j] β0 = β k=1 (cid:32) α0 + (p1)/2 (cid:88) k=1 cos(ωkj + ϕk αk m) (cid:33) . (B.20) (B.21) By combining (B.19), (B.20), (B.21) and substituting them back into (B.15), by simple calculation, we can show that constant frequencies are cancelled and we have ℓ θm[j] Err(2) m,j = 2 (cid:88) xZp ξm[mp(x, j)] σ(ex + ej, θm) + 2 (cid:88) (cid:88) xZp τ =1 ξm[τ ] σ(ex + ej, θm) = 2p (p1)/2 (cid:88) k=1 mβk αk cos(ωkj + ψk ϕk m), [p]. Recall that the gradient flow is defined as tΘ(t) = ℓ(Θ(t)). Following this, we have tθm(t) = θmℓ and tξm(t) = ξmℓ for all [M ]. Then, by combining Step 1 and Step 2 and using the definition of gradient flow, we complete the proof. B.4 Proof of Theorem 5.2: Single-Frequency Preservation Theorem B.4 (Formal Statement of Theorem 5.2). Let the model be initialized according to Assumption 5.1 with scale κinit > 0. For given threshold Cend > 0, we define the initial stage as the time interval (0, tinit], where tinit is the first hit time: tinit := inf{t R+ : max m[M ] θm(t) ξm(t) Cend}. (B.22) 38 Suppose the following conditions hold: (i) log M/M c1/2 (1 + o(1)), κinit = o(M 1/3) and Cend = Θ(κinit), and (ii) scale κinit is sufficiently small such that the event Ephase = {m [M ] s.t. cos(2ϕ m(t) ψ m(t)) 1 (M 1 log )2, (0, tinit]} holds with probability greater than 1 for some constant > 0. Then, we have maxk=k inf t(0,tinit] αk m(t) = o(κinit). m(t) βk In Theorem B.4, the initial time interval (0, tinit) is defined by imposing that the parameters remain substantially small, upper bounded by Cend as stated in (B.22). Ephase assumes during the initial stage, there exists at least one well-aligned neuron whose phase difference 2ϕ m(t) has uniformly lower-bounded cosine value. This should hold with high probability under the random initialization in Assumption 5.1, jointly resulting from the concentration (see Lemma B.6) and the consistent decrease of phase difference for well-initialized neurons when κinit (cid:55) 0 (see Lemma B.9). Since the difference between the real dynamics for θm(t), ξm(t) and the central flow can be bounded by some error uniformly over (0, tinit], where the error is monotone function with respect to κinit, and the real dynamics for ϕ m(t) is continuous function of the real dynamics for θm(t), ξm(t), this claim holds. m(t) ψ m(t), ψ Proof of Theorem B.4. Based on Lemma B.2 and (B.22), throughout the training, we can uniformly upper bound the approximation errors by sup t(0,tinit) max m,j Err(1) m,j(t) Err(2) m,j(t) 8p sup t(0,tinit) end, C5 max θm(t) max{ξm(t), θm(t)} (exp(8M ξ(t) θ(t)2 ) 1) (B.23) where the last inequality uses exp(x)1 for [0, 1] and (B.22) implies 8M ξ(t)θ(t)2 1 for all (0, tinit) under the scaling that C3 init 1. In the following, we show that the evolution of non-feature frequencies is governed by the bounded error terms, and the feature coefficient can grow rapidly even when perturbed by noise. end κ3 Step 1: Derive the Dynamics with Approximation Errors. Consider fixed neuron m. By applying the chain rule, we have tgm(t) = tθm(t) and trm(t) = tξm(t) such that tgm[j](t) = bj, tθm(t), trm[j](t) = bj, tξm(t), [p]. Hence, the time derivatives of constant frequency, based on Lemma B.3, satisfy that trm[1](t) = Err(1) (t), b1, tgm[1](t) = Err(2) (t), b1, (B.24) where the the RHS of (B.24) can be controlled by Err(1) (t), b1 Err(1) (t)2 b12 Err(1) (t), Err(2) (t), b1 Err(2) (t). Based on Lemma B.3 and the orthogonality of the Fourier basis, by simple calculation, it holds that trm[2k](t) = (cid:88) j= (cid:114) 2 cos(ωkj) (p1)/2 (cid:88) k=1 m(t)2 cos(ωkj + 2ϕk αk m(t)) (cid:88) j=1 b2k[j] Err(1) m,j(t) = (cid:112)2p αk m(t)2 (cid:88) j=1 cos(ωkj) cos(ωkj + 2ϕk m(t)) Err(1) (t), b2k = p3/2 2 and similarly, we have αk m(t)2 cos (cid:0)2ϕ m(t)(cid:1) Err(1) (t), b2k, trm[2k + 1](t) = p3/2 2 αk m(t)2 sin(2ϕk m(t)) Err(1) (t), b2k+1. Following this, by applying the chain rule, we have rm[2k](t)2 + rm[2k + 1](t)2 trm[2k](t) + rm[2k + 1](t) βk m(t) trm[2k + 1](t) (cid:27) tβk m(t) = = = (cid:112) 2 (cid:114) 2 (cid:26) rm[2k](t) βk m(t) (cid:114) p3/2 2 2 m(t)2 cos (cid:0)2ϕk αk 2 = αk m(t)2 (cid:8) cos(ψk m(t)) cos(2ϕk m(t)) + sin(ψk m(t)) sin(2ϕk m(t))(cid:9) + (cid:102)Err (1) (t) m(t) ψk m(t)(cid:1) + (cid:102)Err (1) (t), where we define the approximation-induced error term as: (1) (t) := (cid:102)Err 2 (cid:26) rm[2k](t) βk m(t) Err(1) (t), b2k rm[2k + 1](t) βk m(t) Here, notice that the error terms can be upper bounded by (B.25) (cid:27) Err(1) (t), b2k+1 . (cid:102)Err (1) (t) (cid:114) 2 (cid:114) 2 (cid:113) Err(1) (t), b2k2 + Err(1) (t), b2k+12 Err(1) (t)2 (cid:113) b2k2 2 + b2k+12 2 2Err(1) (t), where the first inequality uses the Cauchy-Schwarz inequality and the fact that rm[2k](t)2 + rm[2k + 1](t)2 = p/2 βk m(t)2 by definition. Moreover, following similar argument above, we have tgm[2k](t) = 2p3/2 αk m(t) βk m(t) cos(ψk m(t) ϕk m(t)) + Err(2) (t), b2k, and also tgm[2k + 1](t) = 2p3/2 αk m(t) βk m(t) sin(ψk m(t) ϕk m(t)) + Err(2) (t), b2k+1. Thus, by applying the chain rule, we can reach that tαk m(t) = (cid:114) 2 (cid:112) gm[2k](t)2 + gm[2k + 1](t)2 = 2p αk m(t) βk m(t) cos (cid:0)2ϕk m(t) ψk m(t)(cid:1) + (cid:102)Err (2) (t), (B.26) where the approximation error satisfies that (cid:102)Err (2) (t) = (cid:12) (cid:12) (cid:12) (cid:12) 2 gm[2k](t) m(t) αk Err(2) (t), b2k gm[2k + 1](t) m(t) αk Err(2) (cid:12) (cid:12) (t), b2k+1 (cid:12) (cid:12) 2Err(2) (t). 40 Step 2.1: Bound the Growth of Non-feature Frequency. By combining (B.24), (B.25) and (B.26), since cos (cid:0)2ϕk m(t)(cid:1), we can upper bound the growth of non-feature frequencies as m(t) ψk m(t) + (cid:102)Err tαk m(t) 2p αk m(t) αk tβk trm[1](t) m(t) βk m(t)2 + (cid:102)Err Err(1) (1) (t), (t), (2) (t), tgm[1](t) Err(2) (t), (cid:102)Err (i) (t) Err(i) (t), {0, 1}. for all = and [M ]. For the growth of constant coefficients, (B.27c) indicates that α0 m(t) β0 m(t) = 1/ gm[1](t) rm[1](t) (t) Err(2) Err(1) max t(0,tinit] (t) C end t, (B.27a) (B.27b) (B.27c) (B.27d) (B.28) where the inequality results from (B.23). Following this, by combining (B.27a), (B.27b), (B.27c) and (B.27d), it holds that t{αk m(t)/ 2 + βk m(t)} αk m(t) {αk 2p Cend {αk m(t) + m(t) + 2βk m(t)} + (cid:102)Err (1) (t) + (cid:102)Err 2βk m(t)} + (cid:102)Err (1) (t) + (cid:102)Err (2) (t)/ (2) (t)/ 2 2, where the last inequality uses (B.22) and θm(t)2 2 = α 2 (cid:80)(p1)/2 k=1 m(t)2 such that αk m(t) (cid:112)2/p θm(t)2 αk 2 θm(t) 2 Cend, (0, tinit), (B.29) m(t)2 + for all frequency and similarly we have βk m(t) αk m(t)/ 2 + βk m(t) {αk 2 + βk m(0)} exp( 2 Cend. For = k, Lemma B.5 shows that { (cid:102)Err (1) (s) + (cid:102)Err (2) (s)/ 2p Cend (t s))ds , (B.30) 2p Cend t) 2} exp( m(0)/ (cid:90) + 0 (cid:124) (cid:123)(cid:122) (i) (cid:125) where the first term can be eliminated due to the zero initialization for non-feature frequencies as specified in Assumption 5.1. To upper bound (B.30), we can show that (i) (cid:90) 0 {2Err(1) (s) + 2Err(1) (s)} exp( 2p Cend (t s))ds 4 sup t(0,tinit) max Err(1) (t) Err(2) (t) (cid:90) exp( 2p Cend (t s))ds C5 end exp( (cid:90) 0 2p Cend (t s))ds C5 end t, (B.31) where the first inequality follows (B.27d) and the last inequality results from exp(x) 1 2x for (0, 1). By combining (B.30) and (B.31), we can conclude that m(t) βk αk m(t) C5 end max{p Cend t, 1} C5 end t, (B.32) for all non-feature frequencies = if we consider time ( of this analysis, we will adhere to this interval, and we will later show that tinit (p Cend)1. 2p Cend)1 tinit. For the remainder 41 Step 2.2: Bound the Time of Initial Stage. Based on (B.25) and (B.26), we first show that during the initial stage, the change in the quantity α m(t)2 remains small. Note that m(t)2 2β t{α m(t)2 2β m(t)2} = 2α m(t) tα = 4p α m(t)2 β m(t) tβ m(t) 4β m(t) cos (cid:0)2ϕ m(t) m(t) ψ m(t)(cid:1) + 2α m(t) (cid:102)Err (2) (t) 4p α m(t)2 β m(t) cos (cid:0)2ϕ m(t)(cid:1) 4β m(t) (cid:102)Err = 2α m(t) (cid:102)Err (2) (t) 4β m(t) (cid:102)Err m(t) ψ (1) (t). Following this, by integrating on both sides, we can show that m(t)2 2β α m(t)2 α m(0)2 2β m(0)2 (cid:90) t{α m(s)2 2β m(s)2}ds κ2 init 2 Cend sup t(0,tinit) (cid:102)Err (1) (t) (cid:102)Err (2) (t) κ init O(M C6 end) t, (1) (t) (B.33) where the second inequality uses (B.29). Recall that we choose sufficiently small κinit such that Ephase holds. Thus, there exists neuron such that inf t(0,tinit) cos(2ϕ m(t)) CD. Leveraging this result along with (B.25) and (B.33), it follows that: m(t) ψ tβ (1) (t) m(t) CD α = 2p CD β 2p CD β 2p CD β 2p CD β m(t)2 + (cid:102)Err m(t)2 + CD {α m(t)2 CD κ2 m(t)2 {κ2 m(t)2 (1 + o(1)) κ2 init + O(M C5 init, m(t)2 2β init O(M p2 end)} m(t)2} + (cid:102)Err (1) (t) end) CD O(M C5 end) (B.34) where the second inequality results from (B.27d), the third is guaranteed by the time interval constraint ( 2p Cend)1 tinit, and the last one uses κ3 init = o(1) and Cend = Θ(κinit). Given the Riccati ODE in (B.34) and the initialization β m(t) is monotone increasing as long as 2CD 1 + o(1), which can be guaranteed by choosing sufficiently large such that log M/M c1/2 (1 + o(1)). Following this, we can further show that m(0) = κinit, β tβ m(t) 2pκinit CD β m(t) (1 + o(1)) κ2 init, ( 2p Cend)1 tinit. (B.35) By combining (B.35) and Lemma B.5, we can get β m(t) κinit exp(2pκinit CD t) (1 + o(1)) κinit/(2CD) {exp(2pκinit CD t) 1}. Recall that, by definition β m(tinit) Cend κinit. Thus, we can upper bound the hitting time tinit by 1 2pκinit CD (cid:18) Cend/κinit (1 + o(1))/(2CD) 1 (1 + o(1))/(2CD) (pκinit)1. (B.36) log (cid:19) tinit Step 3: Conclude the Proof. Based on (B.28), (B.32) and (B.36), it holds that max k=k inf t(0,tinit] which completes the proof. m(t) βk αk m(t) C5 end tinit o(κinit), 42 B.4.1 Proof of Auxiliary Lemma B.5 Lemma B.5. Let ι = 0 denote non-zero constant and ζ : [0, ) (cid:55) Rn denote continuous function. For any initial condition x(0) Rn, the unique solution of tx(t) = ιx(t) + ζ(t) is given by x(t) = x(0) exp(ιt) + (cid:90) 0 ζ(s) exp(ι(t s))ds. In particular, if ζ(t) ζ is constant, then x(t) = x(0) exp(ιt) + ζ/ι (exp(ιt) 1). Proof of Lemma B.5. Note that, by chain rule, we have t{xt exp(ιt)} = ιx(t) exp(ιt) + tx(t) exp(ιt) = ζ(t) exp(ιt). By integrating both sides from 0 to t, we can obtain the desired result. Lemma B.6. Under the initialization in Assumption 5.1, with probability greater that 1 c, it holds that maxm[M ] cos(D m) > 1 c2π2 2(log )2, where > 0 is constant. Proof of Lemma B.6. Throughout the proof, we drop the initial time (0) for simplicity. Recall that, as specified in Assumption 5.1, the parameters are initialized as below θm κinit (cid:112)p/2 (ϱ1[1] b2k + ϱ1[2] b2k+1), ξm κinit (cid:112)p/2 (ϱ2[1] b2k + ϱ2[2] b2k+1). By definition, we have cos(ϕ m) = ϱ1[1] and sin(ϕ m) = ϱ1[2]. Thus, it holds that (cos(ϕ m), sin(ϕ m)) = (ϱ1[1], ϱ1[2]) d= (ϱ1[1], ϱ1[2]), following the symmetry of the uniform distribution on the unit circle. Hence, ϕ Similarly, we have ψ this, the tail probability takes the form: Unif(π, π) such that m(0) Unif(π, π). mod 2π Unif(0, 2π). Following = 2ϕ ψ (cid:16) max m[M ] cos(D m) > 1 c2π2 2(log )2(cid:17) (cid:16) [M ], cos(D = 1 = 1 (cid:0)1 arccos (cid:0)1 c2π2 2(log )2(cid:1)/π(cid:1)M . m) 1 c2π2 2(log )2(cid:17) (B.37) Suppose > cπ log such that cπ 1 log (0, 1), then we have arccos (cid:0)1 c2π2 2(log )2)(cid:1) arccos (cid:0) cos(cπ 1 log )(cid:1) = cπ 1 log M, (B.38) where the inequality follows from cos(x) 1 x2 for all and fact that arccos() is monotonely decreasing on [1, 1]. By combining (B.37) and (B.38), we obtain (cid:16) max m[M ] cos(D m) > 1 c2π2 2(log )2(cid:17) 1 (cid:0)1 1 log (cid:1)M 1 exp(c log ) = 1 c. Here, we use (1 x)M exp(xM ) for all [0, 1] and then complete the proof. 43 B.5 Proof of Theorem 5.3: Phase Alignment In this section, due to the inherent difficulty of tracking multi-particle dynamical system with error termseven when the approximation errors are provably smallwe focus on the central flow dynamics presented in Lemma B.3, directly omitting the error terms caused by unpredictable drift. In summary, the resulting dynamical system can be described by the following ODEs: tθm[j](t) = 2p (p1)/2 (cid:88) k=1 m(t) βk αk m(t) cos(ωkj + ψk m(t) ϕk m(t)), tξm[j](t) = (p1)/2 (cid:88) k=1 m(t)2 cos(ωkj + 2ϕk αk m(t)), (B.39a) (B.39b) for fixed neuron and all [p]. We formalize the phase alignment in the following theorem. Theorem B.7 (Formal Statement of Theorem 5.3). Consider the main flow dynamics defined in (B.39a) and (B.39b), under the initialization in Assumption 5.1. Let δ = o(1) be sufficiently small tolerance. For any m(0) (0, 2π], define the convergence time tδ = inf{t R+ : tδ (pκinit)1 (cid:8)1 (sin(D m(0))/δ}1/3 + max{π/2 m(t) δ}. Then, tδ satisfies m(0) π, 0}(cid:1), Furthermore, the magnitude at this time is given by β mean-field regime , let ρt = Law(cid:0)ϕ on (π, π]. Then, ρ0 = λ2 m(t), ψ m(tδ) κinit {sin(D m(0))/δ}1/3. Moreover, in the m(t)(cid:1) for all R+ and let λunif denote the uniform law unif and ρ = T#λunif, where : φ (cid:55) (φ, 2φ) mod 2π. Before presenting the proof of Theorem B.7, we first introduce several key intermediate results that help elucidate the dynamics. We begin with lemma that characterizes the simplified dynamics of the system, leveraging the Fourier domain and the single-frequency initialization. Lemma B.8 (Main Flow under Fourier Domain). Under the initialization in Assumption 5.1, let denote the initial frequency of each neuron, and we use the superscript for notational simplicity. We define m(t) mod 2π, then the main flow can be equivalently described as m(t) = 2ϕ m(t) ψ tα m(t) = 2p α m(t) cos(D m(t) β (cid:18) exp(iD m(t)) = 4β m(t) + m(t)), (cid:19) m(t)2 α β m(t) tβ m(t) = α m(t)(cid:1) exp (i{D sin (cid:0)D m(t)2 cos(D m(t)), m(t) π/2}) . (B.40) This lemma allows us to largely simplify the analysis, reducing it from tracking 2p-dimensional system to three-particle dynamical system of α m(t)). Building on this, the next m(t), β two lemmas further show that the dynamics is indeed one-dimensional, and the trajectory exhibits symmetry property that aids in understanding the evolutions under different initializations. m(t) and Lemma B.9. Consider the ODE in (B.40), the following quantities remain constant: m(t)2 2β α m(t)2 = Cdiff, sin(D m(t)) β m(t) α m(t)2 = Cprod, Building upon this, we can further simplify the dynamics of tD m(t) = (cid:0)4β m(t) + α m(t)2/β m(t)) in as m(t)(cid:1) sin (D m(t)) , R+. (B.41) due to its well-regularized behavior ensured by the constant relationship. 44 (a) Simplified Dynamics with m(0) (π/2, π). (b) Simplified Dynamics with m(0) (0, π/2). m(0) and different phase difference Figure 15: Training dynamics of specific decoupled neuron characterized by (B.47a) and (B.47b) m(0). Figure (a) plots the with identical initial scales α m(0) = β dynamics of phases, phase difference, and the magnitudes with m(0) (π/2, π), whose behavior is detailedly characterized in Theorem B.7. The difference decreases monotonically to 0, while the magnitudes first decay slightly when m(t) falls below π/2. Figure (b) plots the dynamics under is initialized closer to the convergence point, resulting in shorter convergence time compared to the case in Figure (a). Moreover, the simplified dynamics shown in Figure (b) align well with the full dynamics in Figure 8a with the same initialization, indicating the effectiveness of the approximation. m(t) (π/2, π) and then increase rapidly when m(0) (0, π/2) where We highlight that (B.41) is not direct corollary from (B.40) due to the potential jump from 0 to 2π in the discontinuous definition of mod 2π. However, thanks to the constant relationship revealed in Lemma B.9, we can show that m(t) is well-behaved\" by staying in the half-space where it is initialized, and consistently approaching zero throughout the gradient flow. Lemma B.10. Consider the ODE given in (B.40) with initial condition the hit time that m(tπ/2) = π/2, then for any (0, tπ/2), we have m(0) (π/2, π). Let tπ/2 denote m(tπ/2 t) = β β m(tπ/2 + t), m(tπ/2 t) + m(tπ/2 + t) = π. Proof of Lemma B.8, B.9 and B.10. Please refer to B.5.1 for detailed proof. Now we are ready to present the proof of Theorem B.7. Proof of Theorem B.7. Without loss of generality, we focus on the case where case established in Lemmas B.8 and B.9. Specifically, the trajectories of α m(0) (0, π). The m(0) (π, 0) can be extended identically owing to the symmetry of dynamics in (B.40) as m(t) are invariant m(t) and β 45 under sign flip of from (0, π) to (π, 0) at each time t. m(t) such that the entire dynamics evolves symmetrically, with m(t) mirrored m(0) (0, π/2) and Roadmap. casesD Lemma B.10, we only need to characterize two time intervals (i) the traveling time from for any time from In the following, we establish the convergence time by further dividing into two m(0) (π/2, π). Notably, thanks to the symmetry established in m(0) to π/2 m(0) = κinit, (ii) the convergence This is because, m(0) (π/2, π), denoted by t(cid:1)π/2 , both initialized at β m(0) (0, π/2), denoted by t(cid:1)δ For For m(0) to 0 for an arbitrary initial phase m(0) (0, π/2), the convergence time can be captured by t(cid:1)δ m(0) (π/2, π), the time is given by 2t(cid:1)π/2 + t(cid:1)δ denote the time traveling from π we let t(cid:1)δ Lemma B.10, as it takes equal time for π such that the remaining convergence time is equal to t(cid:1)δ m(0). Also, when m(t) reaches π m(t) to travel from π m(0), we have β . , where with slight abuse of notation m(0) to 0. Such argument is supported by m(0) to π/2 and from π/2 to m(t) = κinit due to the symmetry, Below are some useful properties. Under the initialization in Assumption 5.1, Lemma B.9 ensures m(t)2 = 2β α m(t)2 κ2 init, R+. Following this, we can characterize the dynamics as follows: tβ tD m(t) = (2β m(t) = (cid:0)6β m(t)2 κ2 m(t) κ2 init) cos(D init/β m(t)), m(t)(cid:1) sin (D m(t)) . (B.42) (B.43a) (B.43b) Hence, we have and increases thereafter. Besides, it follows from (B.42) that β m(t) is monotonely decreasing, and β m(t) first decreases when m(t) (π/2, π) m(t) κinit/ 2 for all R+. Part I: Travelling time from where we define t(cid:1)π/2 = min{t R+ : m(0) to π/2 with m(0) (π/2, π). We consider (0, t(cid:1)π/2] m(t) π/2}. Based on (B.43b), by definition, we have tD m(t) (cid:0)6β m(t) κ2 init/β m(t)(cid:1) 5p κinit, where the last inequality uses 6β [κinit/ can lower bound 2, κinit] since β m(t) by m(t) m(t) is monotonically decreasing throughout the stage. Following this, we m(t) is monotonically increasing on R+ and β m(t) κ init/β m(t) m(0) 5p κinit for all tϵ 1 . Thus, we have t(cid:1)π/2 m(0) m(t(cid:1)π/2) 5p κinit = m(0) π/2 5p κinit , On the other side, (B.43b) implies that tD m(t) (cid:0)6β m(t) κ2 init/β m(t) 0 such that m(t)(cid:1) sin(D m(0)) tD m(t) m(0). Then, we have 2p κinit sin(D m(0)). Similarly, we can upper bound t(cid:1)π/2 . By combining the arguments above, we have t(cid:1)π/2 (p κinit)1 {D m(0) π/2}. 46 Part II: Convergence time from δ > 0, and the convergence time is formalized as t(cid:1)δ = min{t R+ : sin(D m(0) (0, π/2). Consider small error level m(t)) δ}. Note that m(t) is monotonically increasing in this stage. Also, m(t) is monotonically decreasing and β m(0) to 0 with sin(D m(t)) β m(t) α m(t)2 = sin(D m(t)) β m(t) (2β m(t)2 κ2 init) = sin(D m(0)) κ3 init, following (B.42), Lemma B.9 and β m(0) = κinit as specified in Assumption 5.1. By definition, m(t(cid:1)δ ) (2β m(t(cid:1)δ )2 κ init) β m(t(cid:1)δ )3. m(0))/δ κ3 sin(D init = β m(t(cid:1)δ )/κinit 3(cid:112)sin(D (cid:33) Hence, we have β m(0))/δ. Following (B.43a), it holds that (cid:32) log β m(t) κinit/ β m(t) + κinit/ 2 2 = 2 κinit tβ m(t)2 κ2 2β init m(t) = 2 2 κinit cos(D m(t)) κinit p, since cos(D m(t)) [cos(D m(0)), 1]. Hence, by integrating over time (0, t(cid:1)δ ], we can show that (cid:32) log β m(t(cid:1)δ ) κinit/ β m(t(cid:1)δ ) + κinit/ (cid:33) 2 + log(3 + 2 2) κinit t(cid:1)δ . (B.44) Next, we bound the scale of the term within the logarithm. For small tolerance δ = o(1), we have β m(t(cid:1)δ ) κinit/ β m(t(cid:1)δ ) + κinit/ 2 2 = 1 2(cid:0) 2 β m(t(cid:1)δ )/κinit + 1(cid:1)1 = 1 Θ(cid:0) 3(cid:112)δ/ sin(D m(0))(cid:1). (B.45) Thus, by combing the arguments in (B.44) and (B.45), we can conclude that t(cid:1)δ (p κinit)1 (cid:8)1 3(cid:112)δ/ sin(D m(0))(cid:9), where we use the fact that log(1 x) for small > 0. Based on the results in Part and Part II, for any initial phase difference m(0) (0, π) and sufficiently small error tolerance δ (0, 1), by symmetry, the convergence time is of level tδ (pκinit)1 (cid:8)1 (sin(D m(0))/δ}1/3 + max{π/2 m(0) π, 0}(cid:1), where we let (x)+ = max{x, 0} denote the ReLU function. Part III: Preservation of Uniform Phase Distribution and Double-Phase Convergence. Recall that Lemma B.9 gives there exists constant Cprod such that sin(D m(t)) β m(t) α m(t)2 = Cprod. Following this, we can write the dynamics of ϕ m(t) and ψ m(t) as exp(iϕ exp(iψ m(t)) = 2p Cprod α m(t)) = Cprod β m(t)2 exp (i {ϕ m(t)2 exp (i {ψ m(t) π/2}) , m(t) + π/2}) . (B.46) m(t), tend to As established previously, the magnitudes of the learned parameters, α infinity as . This divergence drives the convergence of the corresponding phases to fixed m(), which are determined by the initialization. Furthermore, Theorem 5.3 values, ϕ m() and ψ m(t) and β 47 proves that the misalignment term phases must satisfy the phase alignment condition: 2ϕ m(t) converges to zero. This directly implies that the limiting m(). m() = ψ Let exp(iϕ m(t)) = z(t). By (B.46), z(t) is continuously differentiable with respect to t. Consider Φ m(t) = ϕ m(0) + (cid:90) 0 ℑ(z(s) sz(s))ds, then we can check that Φ also check that it satisfies exp(iΦ m(t)) = z(t) since m(t) is continuously differentiable. By differentiating both sides, we can exp(iΦ m(t)) = exp(iΦ m(t)) (cid:90) ℑ(z(s) sz(s))ds = exp(i(Φ = 2p Cprod α = 2p Cprod α m(t) π/2)) ℑ(z(t) tz(t)) m(t)2 exp(i(Φ m(t)2 exp(i(Φ where the third equality results from (B.46) and the last line we use the fact that z(t) = 1 by definition. Using the uniqueness of ODE and initial condition exp(iΦ m(0)) = z(0), we can conclude that exp(iΦ m(t) π/2)) ℑ(z(t) z(t) (i)) m(t) π/2)), m(t) mod 2π. By direct calculation, we have m(t)) = z(t) and thus ϕ m(t) mod 2π = Φ tΦ m(t) = 2p Cprod α m(t)2, which indicates that Φ m(t) = ϕ m(0) 2p Cprod (cid:90) 0 m(s)2ds. α m(t) is jointly given by (B.42), (B.43a) and (B.43b). Following this, Recall that the dynamics of α given {α m(0), m(0), β m(t) mod 2π = Φ ϕ m(0)}, we can write m(t) mod 2π := ϕ m(0) By simple calculation, we can show that ϕ these arguments and applying similar one to ψ m(0) and ϕ m(t) establishes that m(0) m(0) mod 2π + G(α m(0), m(0), β i.i.d. (π, π] for all m. Combining m(0)) mod 2π. ϕ m(t) m() and ψ i.i.d. Unif(π, π), ψ m(t) i.i.d. Unif(π, π), (t, m) R+ [M ]. m() are both uniformly distributed over [0, 2π). Recall that 2ϕ Thus, ϕ m() m()) degenerates on the (periodic) for any given initialization, then the joint measure of (ϕ line 2ϕ = ψ inside the support. Since the marginals of them are both uniform, the joint limiting measure is given by ρ = T#λunif with : φ (cid:55) (φ, 2φ) mod 2π, which completes the proof. m() = ψ m(), ψ B.5.1 Proof of Auxiliary Lemma B.8, B.9 and B.10 Proof of Lemma B.8. Following the same argument in the proof of Theorem 5.2, by pushing the approximation error to 0, we can show an exact single-frequency pattern: m(t) = βk αk m(t) 0, R+, = k. Formally, this result holds under the initialization in Assumption 5.1, which can be justified using m(t)) with zero initial value. Then, the dynamics of matrix ODE argument over uk the original parameter can be simplified to coefficient only related to k. For all [p], we have m(t) = (αk m(t), βk tθm[j](t) = 2p α m(t) β m(t) cos(ωkj + ψ m(t) ϕ m(t)), (B.47a) tξm[j](t) = α m(t)2 cos(ωkj + 2ϕ m(t)). (B.47b) Recall tgm[j](t) = bj, tθm(t), by simple calculation, it holds that m(t) cos (cid:0)ψ m(t) sin (cid:0)ψ tgm[2k](t) = tgm[2k + 1](t) = 2 p3/2 α 2 p3/2 α m(t) β m(t) β m(t) ϕ m(t) ϕ m(t)(cid:1), m(t)(cid:1), and similarly, by using trm[j](t) = bj, tξm(t), we can obtain that m(t)2 cos (cid:0)2ϕ m(t)2 sin (cid:0)2ϕ trm[2k](t) = p3/2/ trm[2k + 1](t) = p3/2/ 2 α 2 α m(t)(cid:1), m(t)(cid:1), where the additional (cid:112)2/p arises from the normalization factor in bjs (see 5.1). Since the magnitudes follows α m, by applying the chain rule, then and β = (cid:112)2/p tα tβ m(t) = 2p α m(t) = α = (cid:112)2/p m(t) cos (cid:0)2ϕ m(t) ψ m(t) β m(t)2 cos (cid:0)2ϕ m(t)(cid:1), m(t) ψ m(t)(cid:1). (B.48a) (B.48b) Next, we understand the evolution of phases by tracking the dynamics of exp(iϕ m(t)) via Eulers formula. Note that ϕ m(t) cannot be directly tracked via ODEs due to abrupt m(t) and ψ jumps from π to π, which arise from the use of atan2() function in definitions (see 5.1). By definition and the chain rule, it follows that m(t)) and exp(iψ cos(ϕ m(t)) = (cid:19) (cid:114) 2 (cid:114) 2 = 2p β m(t) (cid:18) gm[2k](t) α (cid:26) tgm[2k](t) α m(t) m(t) cos (cid:0)ψ = 2p β m(t) cos(ϕ tα m(t) α m(t) m(t)(cid:1) m(t) ϕ m(t)) cos (cid:0)2ϕ = 2p β m(t)) sin (cid:0)2ϕ m(t) sin(ϕ m(t)) = (cid:112)2/p gm[2k](t)/α (cid:27) gm[2k](t) m(t) α m(t) ψ m(t)(cid:1), m(t) ψ m(t)(cid:1) where the second equality uses cos(ϕ the trigonometric indentity. Similarly, we have m(t) and the last one results from sin(ϕ m(t)) = 2p β m(t) cos(ϕ m(t)) sin (cid:0)2ϕ m(t) ψ m(t)(cid:1), which gives that exp(iϕ m(t)) = 2p β m(t) sin (cid:0)2ϕ m(t) ψ m(t)(cid:1) exp (i {ϕ m(t) π/2}) . Following similar argument, we can show that exp(iψ m(t)) = α m(t)2 β m(t) sin (cid:0)2ϕ m(t) ψ m(t)(cid:1) exp (i {ψ m(t) + π/2}) . (B.49) (B.50) Thanks to the initialization and preservation of the single-frequency, the 2p-dimensional dynamical system can be tracked via four-particle system with α , whose dynamics are , ϕ given by (B.48a), (B.48b), (B.49) and (B.50). Furthermore, note that , and ψ , β exp(2iϕ m(t)) = 2 exp(iϕ m(t)) exp(iϕ m(t)) 49 = 4p β m(t) sin (cid:0)2ϕ m(t) ψ m(t)(cid:1) exp (i {2ϕ m(t) π/2}) . Based on (B.50) and (B.51), by denoting m(t) = 2ϕ m(t) ψ m(t) mod 2π, we obtain that exp(iD m(t)) = = 4p β exp(2iϕ exp(iψ m(t)) m(t)) exp(2iϕ m(t)) exp(iψ exp(2iψ m(t)) m(t) sin (cid:0)D m(t)(cid:1) exp (i {D m(t)2 α β m(t) m(t)(cid:1) exp (i {D (cid:19) sin (cid:0)D m(t)) m(t) π/2}) m(t) + π/2}) (cid:18) = 4β m(t) + sin (cid:0)D m(t)(cid:1) exp (i{D m(t) π/2}) . m(t)2 α β m(t) (B.51) (B.52) By combining (B.48a), (B.48b) and (B.52), we complete the proof. Proof of Lemma B.9. Following the simplified main flow in the Fourier domain (see Lemma B.8), it is easy to show that α m(t) is constant throughout the gradient flow since m(t)2 2β t{α m(t)2 2β m(t)2} = 2α m(t) tα m(t) 4β m(t) tβ m(t) = 0. Hence, there exists an initialization-dependent constant Cdiff such that Moreover, by applying the chain rule, we can deduce that m(t)2 = 2β α m(t)2 + Cdiff, R+. t{α m(t)2 β m(t)2 tβ m(t)2 tβ m(t) + t{α m(t) + 2t{β m(t)2} β m(t)2} β m(t) m(t) m(t)} = α = α = α = α m(t)4 cos(D m(t)2 {α m(t)) + 4β m(t)2 α m(t)2} cos(D m(t)2 cos(D m(t)). m(t)2 + 4β m(t)) Following this, we can compute the time derivative of sin(D m(t)) β m(t) α m(t)2, following that t{sin(D m(t)) β = sin(D = cos(D m(t) α m(t)) β m(t)) α m(t)2} m(t) α m(t)2 (cid:0)4β m(t)2 {α m(t)2 + sin(D m(t)) t{α m(t)2 β m(t)) m(t)2(cid:1) sin (D m(t)} m(t)2 + α m(t)2 + 4β m(t)2} cos(D m(t)) = 0, + sin(D m(t)) α m(t) α m(t)) β Finally, we show that where the second equality uses (B.40) in Lemma B.8. Therefore, there exists constant Cprod such that sin(D Lemma B.9, we always have sin(D no jump behavior occurs for applying chain rule over (B.52), we can reach that m(t)2 = Cprod for all R+. m(t) remains within the half-space where it is initialized, which means m(0) (ιπ, (ι + 1)π). By m(t) will never reach ιπ for any ι. This ensures m(t), allowing us to directly track its dynamics. Following this, by m(t)) = 0, so m(t) (ιπ, (ι + 1)π) for ι {1, 0} determined by the initial state tD m(t) = (cid:0)4β m(t) + α m(t)2/β m(t)(cid:1) sin (D m(t)) , which completes the proof. 50 Proof of Lemma B.10. Based on the results in Lemma B.8 and B.9, we reduce the main flow into one-dimensional dynamical system characterized by β m(t). Specifically, we have tβ m(t) = α m(t)2 cos(D m(t)) = (2β m(t)2 + Cdiff) sign{cos(D m(t))} := ς(β m(t)) sign{cos(D m(t))}. (cid:115) 1 C2 prod m(t)2 + Cdiff)2 m(t)2 (2β β As given in (B.41), due to the nonnegativity of the magnitudes, we can show that decreasing if (0, tπ/2], where tπ/2 denote the hit time that m(t) is monotonely m(0) (π/2, π). We consider = tπ/2 for [tπ/2, 2tπ/2) and = tπ/2 for m(tπ/2) = π/2. Following this, we have rβ sβ m(s)), m(s) = tβ m(t tπ/2) = ς(β m(r) = tβ Here, we decompose tβ m(t) within time [0, 2tπ/2] into backward process within time (0, tπ/2] and forward process within time [tπ/2, 2tπ/2] respectively. Starting from time = = 0, where the m(tπ/2), since ς is locally Lipschitz, by the uniqueness of the ODE initial value is both given by β m(tπ/2 t) for all [0, tπ/2). solution, for = r, we have β m(s) = β Furthermore, by combining Lemma B.9, the monotonicity of m(t) and the arguments above, we can show that m(tπ/2 t) = ς(β m(tπ/2 + t) = π, which completes the proof. m(tπ/2 t) + m(tπ/2 + t) = β m(r), i.e., β m(r)). Proof of Results for Theoretical Extensions in Section 6 C.1 Proof of Corollary 6.1: Phase Lottery Ticket We first formalize the random multiple frequency initialization as follows. Assumption C.1. For each neuron [M ], the parameters (ξm, θm) are initialized as θm(0) κinit (cid:112)p/2 ξm(0) κinit (cid:112)p/2 (p1)/2 (cid:88) k=1 (p1)/2 (cid:88) k=1 (ϱ1,k[1] b2k + ϱ1,k[2] b2k+1) , (ϱ2,k[1] b2k + ϱ2,k[2] b2k+1) , where ϱr,k i.i.d. Unif(S1) for all and {1, 2}, and κinit > 0 denotes small initialization scale. This is the natural extension of Assumption 5.1 to multiple frequencies, and the arguments in B, i.e., Lemma B.8, B.9 and B.10, go through with only routine modifications thanks to the neuron decoupling and the orthogonality of frequencies. We first state the formal version of Corollary C.2. Corollary C.2 (Formal Statement of Corollary 6.1). Consider random initialization following Assumption C.1, and let denote the winning frequency given by = mink (cid:101)Dk m(0). For given ε (0, 1), define the dominance time tε as tε := inf{t R+ : max k=k m(t)/β βk m(t) ε}. Then, with probability at least 1 (cid:101)Θ(pc), where > 0 satisfying c4π2e2(1c), it holds that tε π2p(2c+3) κinit + (c + 1) log + log 1 1ε pκinit {1 2c2π2 (log p/p)2} . 51 Before delving into the proof, we first establish key property of the decoupled dynamics under this initializationorder preservationunder the initialization specified in C.1. Lemma C.3. Let σ be the permutation that sorts the initial phase differences in non-decreasing order: (cid:101)Dσ(1) (0) (cid:101)Dσ(2) (0) (cid:101)Dσ( p1 2 ) (0), m(0) = min{Dk where (cid:101)Dk Under the initialization in Assumption C.1, the rank-ordering of the corresponding magnitudes βk inverted and preserved for all time 0: m(0)} represents the shortest circular distance for the initial phase. m(t) is m(0), 2π Dk (t) βσ(2) βσ(1) (t) β σ( p1 2 ) (t). Proof of Lemma C.3. Please refer to C.1.1 for detailed proof. Lemma C.3 states that, when neurons are decoupled and each frequency is initialized at the same scale κinit > 0, the ordering of frequencies by magnitude βk within each neuron remains fixed throughout the gradient flow, with larger magnitudes corresponding to smaller initial phase difference. Now we are ready to present the proof of Corollary C.2. i.i.d. Unif(S1) Proof of Corollary C.2. As specified in Assumption C.1, for all [M ], we initialize ϱr,k for all {1, 2} and [ p1 2 ]. Thanks to the orthogonality among frequencies, each frequency evolves independently, so Lemmas B.8, B.9 and B.10 apply to every frequency k, not just the feature frequency k. For fixed neuron m, by defining (cid:101)Dk m(0) = min{Dk m(0)}, we have m(0), 2π Dk tβk (cid:101)Dk m(t) = (2βk m(t) = (cid:0)6βk m(t)2 κ2 m(t) κ2 init) cos( (cid:101)Dk init/βk m(t)), m(t)(cid:1) sin (cid:0) (cid:101)Dk m(t)(cid:1). (C.1a) (C.1b) and βk Step 1: Deriving Winning Frequency and Initial Phase Gap. By Lemma C.3, the dynamics preserves the ordering of (cid:101)Dk throughout the gradient flow. Specifically, at any time R+, the ordering remains unchanged. Thus, the lottery ticket winner, i.e., frequency such that m(t) βτ βk m(t) for all τ = k, is given by = argmink (cid:101)Dk To demystify the dominance phenomenon, it suffices to focus on the growth of the magnitude of the winning frequency and the second-dominant frequency = argmink=k (cid:101)Dk m(0). Under the initialization as specified in Assumption C.1, with probability greater than 1 (cid:101)Θ(pc) for some constant (0, 1), we have the following good initialization: m(0). Einit = 1 := (cid:8) init 3 init init 2 (cid:101)D {cos( (cid:101)D m(0) < π/2(cid:9) (cid:8) cos( (cid:101)D m(0)) cos( (cid:101)D m(0)) 1 2c2π2 (log p/p)2}. m(0)) + π2p2(c+1)} (C.2) m(0) are respectively the firstand the secondorder statistics of p1 2 i.i.d. Unif(0, π) based on similar argument in Lemma B.6, and thus (cid:101)D m(0) i.i.d copies of Unif(0, π), This is because (cid:101)Dk and (cid:101)D denoted by U(i)s. Notice that m(0) P(cid:0)E 1,c init (cid:1) = P(cid:0)i, U(i) π/2(cid:1) + P(cid:0)i > 1, U(i) π/2, U(1) π/2(cid:1) = (p + 1) 2 p+1 2 pc. (C.3) Furthermore, if c4π2e2(1c), it holds that P(cid:0)E 2,c init (cid:1) + P(cid:0)E 1,c (cid:1) P(cid:0){cos(U(1)) cos(U(2)) + π2p2(c+1)} 1 init init P(cid:0){U 2 (cid:1) + pc (1) 4 (2)/12 2π2p2(c+1)} 1 init P(cid:0){U 2 (1) 2π2p2(c+1) + 2(cπ/p log p)4} 1 init (2) 2 (2) 2 (cid:1) (cid:1) + P(cid:0){U 4 (2) 2 (2) 24 (cπ/p log p)4} 1 init (1) 8π2p2(c+1)) + P(U(2) 2cπ/p log p) + pc, (cid:1) + pc P(U 2 (C.4) where the second inequality uses 1 x2/2 cos(x) 1 x2/2 + x4/24 for (0, π/2). Moreover, to bound the RHS of (C.4), we can show that P(U 2 (2) 2 (1) 8π2p2(c+1)) P(U(1) (U(2) U(1)) 4π2p2(c+1)) P(U(1) 2πp(c+1)) + P(U(2) U(1) 2πp(c+1)) = 2 2(1 2p(c+1)) 2 pc, (C.5) where the second inequality follows U(1) d= U(2) U(1). Furthermore, it holds that P(U(2) 2cπ/p log p) = (1 2c/p log p) p1 2 + c(p 1)/p log (1 2c/p log p) p3 (1 + log p) (1 2c/p log p) p3 2 pc log p. (C.6) By combining (C.4), (C.5) and (C.6), we have P(cid:0)E 2,c init (cid:1) pc log p. Similarly, we can derive that P(cid:0)E 3,c init (cid:1) = P(cid:0) cos(U(1)) 1 2c2π2 (log p/p)2(cid:1) P(U(1) 2cπ/p log p) = (1 2c/p log p) p1 2 pc, (C.7) where the inequality also uses cos(x) 1 x2/2 for (0, π/2) Based on (C.3),(C.4) and (C.7), the good initialization event Einit holds with probability of at least 1 Θ(pc log p). In the subsequent analysis, we assume that this event occurs. Step 2: Growth of Gap between Winning Frequency and Others. Based on (C.1a), the dynamics for the log-magnitude follows log βk m(t) = tβk m(t) βk m(t) = (2βk m(t) κ init/βk m(t)) cos( (cid:101)Dk m(t)). To compare the winning frequency () against the runner-up (), we examine the dynamics of their log-ratio log β m(t) β m(t) , which measures the exponential rate at which the winner pulls ahead: log β m(t) β m(t) = (2β m(t) κ2 init/β m(t)) cos( (cid:101)D m(t)) (2β m(t) κ2 init/β m(t)) cos( (cid:101)D m(t)) = (β m(t) β + (2β 2p cos(( (cid:101)D m(t)) {2 + κ2 init/β m(t) β m(t) κ2 m(0)) (β m(t) β init/(β m(t)) {cos( (cid:101)D m(t)) cos( (cid:101)D m(t)) + κinit {cos( (cid:101)D m(t))} cos( (cid:101)D m(t)) m(t))} m(t)) cos( (cid:101)D m(t))}. (C.8) 53 m(t) β m(t) (cid:101)D m(t) and (cid:101)D Here, we use (i) β property in Lemma C.3, and (ii) under the good initialization Einit where (cid:101)D (cid:101)D β m(t) β in Assumption C.1. Let ρm(t) = β m(0), (cid:101)D m(t) > 0 for all (, t) {, }R+. Therefore, we have cos( (cid:101)D m(t) for all R+ based on the order preservation , we have m(0)), m(0) = κinit under the initialization m(t) 2β m(0)κ2 m(t). Following (C.8), we have m(0) π 2 m(t)) cos( (cid:101)D m(0) = κinit and 2β m(t) < 0 and tβ init/β m(t)/β m(t)κ2 init/β log ρm(t) 2p κinit cos( (cid:101)D m(0)) (ρm(t) 1) κinit {cos( (cid:101)D m(t)) cos( (cid:101)D m(t))}, Based on the first term in the right-hand side, simple calculation shows that the dynamics satisfy: log (cid:19) (cid:18) ρm(t) 1 ρm(t) 2p κinit cos( (cid:101)D m(0)) > 0. Thus, we can integrate this result over any interval [s, t] to obtain lower bound: ρm(t) {1 + (1/ρm(s) 1) exp(2p cos( (cid:101)D m(0)) κinit (t s))}1, (0, t]. (C.9) Following this, once the ratio ρm(t) is larger than 1, the ratio ρm(t) surpasses 1, it begins to grow super-exponentially, accelerating rapidly towards infinity. Motivated by this dynamics, our analysis proceeds in two stages: first, we show that ρm(t) does not get stuck at the initial stationary point ρm(t) 1, and second, we quantify its rate of growth using (C.9). Step 2.1. Initial Growth of the Ratio Beyond Unity. Consider short initial time interval (0, t1], during which the model parameters remain close to their initial values while the ratio ρm(t) quickly exceeds 1. Based on (C.1b), we have cos( (cid:101)D m(t)) cos( (cid:101)D m(t)) cos( (cid:101)D m(0)) + cos( (cid:101)D m(0)) 2 max {,} cos( (cid:101)D m(t)) cos( (cid:101)D m(0)) 2 max {,} cos( (cid:101)D m(t)) = 2 max {,} (cid:90) 0 cos( (cid:101)D m(s))ds = 2p max {,} 6p max {,} (cid:90) 0 (cid:90) 0 (cid:0)6β m(s) κ2 init/β m(s)(cid:1) sin( (cid:101)D m(s))2ds β m(s)ds 6pt max {,} max 0st m(s) = 6pt β β m(t), (C.10) where the last inequality results from β property, i.e., β m(t) β m(t) at any time t, as shown in Lemma C.3. Following (C.1a), we get m(s) β m(t) for all (0, t] and the rank preservation tβ m(t) (2β m(t)2 κ2 init) = β m(t) κinit/ 2 coth( 2pκinit ι1), R+, (C.11) where we denote ι1 = arccoth( t1 := inf (cid:8)s (0, t] : 2pκinit coth( 2pκinit ι1) > cg π2p2(c+1)(cid:9). 2). By choosing cg (0, 1), we define Here, we choose sufficiently small cg to ensure that t1 is well-defined and finite before the system explodes. This choice makes t1 correspondingly small and the following asymptotic result holds: coth( 2pκinit t1 ι1) 2 + pκinit t1 = t1 cg π2p(2c+3)/κinit. (C.12) Recall from (C.2) that under the good initialization Einit, the initial cosine gap cos(D is lower bounded by π2p2(c+1). By combining (C.10), (C.11) and definition of t1, we have m(0))cos(D m(0)) cos( (cid:101)D m(t)) m(t)) cos( (cid:101)D m(0)) cos( (cid:101)D cos( (cid:101)D π2p2(c+1) 6p sup t(0,t1] m(0)) cos( (cid:101)D m(t)) cos( (cid:101)D m(t) (1 cg) π2p2(c+1), β m(t)) cos( (cid:101)D m(0)) + cos( (cid:101)D m(0)) (C.13) for all (0, t1]. Building upon (C.12) and (C.13), we can show that log ρm(t1) = log ρm(0) + (cid:90) t1 0 cos(D m(s)) cos(D m(s))ds cg(1 cg) pκinit t1 π2p2(c+1) π4p4(c+1), and thus ρm(t1) exp(1 + π4p4(c+1)) 1 + π4p4(c+1) for sufficiently large p. Step 2.2. Super-exponential Growth. Let ε > 0 be the dominance threshold. We now derive the time t2 required for the lower bound of the ratio to exceed this threshold, i.e., ρm(t2) > 1/ε, such that tε t2b. Our starting point is the state at time t1, after which we have ρm(t1) 1 + π4p4(c+1). Following (C.9), we have ρm(t)1 1 + (1/ρm(t1) 1) exp(2p cos( (cid:101)D m(0)) κinit (t t1)) 1 π4p4(c+1) exp(2p {1 2c2π2 (log p/p)2} κinit (t t1)), where the last inequality results from 1/ρm(t1) 1 1 ρm(t1) given ρm(t1) is close to 1, and the good initialization cos( (cid:101)D m(0)) 1 2c2π2 (log p/p)2 in (C.2). By choosing t2 = t1 + 4(c + 1) log + log 1 1ε 4 log π 2pκinit {1 2c2π2 (log p/p)2} π2p(2c+3) κinit + (c + 1) log + log 1 1ε pκinit {1 2c2π2 (log p/p)2} , we can guarantee that ρm(tε)1 < ε, which completes the proof. C.1.1 Proof of Auxiliary Lemma C. We begin by recalling the foundational results for celebrated class of dynamical systemsknown as cooperative systemswhich enjoy useful rank-preservation property (e.g., Smith, 1995). Before stating this formally, let us give precise definition. Definition C.4 (Cooperative System). Consider p-convex set Rd such that tx + (1 t)xy for all [0, 1] whenever x, and x+. Suppose : (cid:55) is continuously differentiable. The dynamical system, defined by txt = (xt), is called cooperative if fi xj (x) 0 for all = j. In other words, cooperative systems Jacobian has nonnegative off-diagonal entries, so increasing any coordinate of the state cannot decrease another in the next iteration. With this definition in hand, we can now state the key monotonicity property of cooperative systems. Lemma C.5. Consider cooperative system txt = (xt), and write for x, Rd if xi yi for all Rd. Given two initial values x1 at all times R+. 0, then we have x1 0 x2 x2 Proof of Lemma C.5. Please refer to Kamke (1932); Hirsch (1982) for detailed proof. 55 In what follows, we prove Lemma C.3, which is direct application of Lemma C.5. Proof of Lemma C.3. Recall that, by Lemmas B.9 and B.10, together with the orthogonality of the frequency basis, for every [ p1 2 ], the dynamical system is given by (C.1a) and (C.1b) with initial condition βk m(0) = κinit for every frequency k."
        },
        {
            "title": "We first show that the evolution of Dk",
            "content": "m(t) consistently shares the symmetric trajectory at any time if initialized symmetrically. Let x(t) = (βk m(t)) and denote by ς(x(t)) right-hand side of (C.1a), (C.1b), such that tx(t) = ς(x(t)). Define the involution I(β, D) = (β, 2π D) with its Jacobian following dI diag(1, 1). direct calculation shows that m(t), Dk ς I(βk m(t), Dk m(t)) dI ς(βk m(t), Dk m(t)) 0, i.e., the system is equivariant under I. By uniqueness of solutions, the solution with initial x(0) = (βk m(t)(cid:1), so the two trajectories remain symmetric. m(0)) satisfies x(t) = I(cid:0)βk m(0), 2π Dk Hence, it suffices to consider the dynamics with standardized initialization min{Dk m(0), 2π m(0)} (0, π]. Following similar argument in Lemma B.9, under the standardized initialization, and rewrite m(t) (0, π) at all time t. To verify cooperativeness, we introduce (cid:101)βk m, Dk m). From (C.1a) and (C.1b) one obtains Dk we have Dk the dynamics in the new coordinates ( βk = βk m(t), Dk (cid:101)βk tDk m(t) = (2 (cid:101)βk m(t) = (cid:0)6 (cid:101)βk m(t)2 κ2 m(t) κ2 init) cos(Dk init/ (cid:101)βk m(t)(cid:1) sin (cid:0)Dk m(t)) := ς1(βk m(t), Dk m(t)(cid:1) := ς2(βk m(t)), m(t), Dk m(t)), and it is easy to check that the vector field is cooperative by ς1 Dk = sin(Dk m(t)) > 0, ς2 (cid:101)βk = (cid:0)6 + κ2 init/ (cid:101)βk m(t)2(cid:1) > 0. Thus, (βk m(0) = κinit for all and phase difference Dk βk m, Dk m) is cooperative, and by Lemma C.5, it preserves the initial ordering. Since m(0)s are distinct, it follows that Dk m(0) Dτ m(0) = R+, (cid:101)βk m(t) (cid:101)βτ m(t) = R+, βk m(t) βτ m(t), for every pair k, τ [ 2 ], which completes the proof. C.2 Proof of Proposition 6.3: Dynamics of ReLU Activation Proof of Proposition 6.3. We begin by recalling from B.2 that, for each fixed index m, the gradient with respect to the decoupled loss ℓm takes the form ℓ θm[j] = 2 (cid:88) xZp ξm[mp(x, j)] 1(ex + ej, θm 0) + 2 (cid:88) (cid:88) xZp τ =1 ξm[τ ] 1(ex + ej, θm 0), (C.14a) ℓm ξm[j] (cid:88) = (x,y)Sp max{ex + ey, θm, 0} + 1 (cid:88) (cid:88) j=1 (x,y)Sp max{ex + ey, θm, 0}, (C.14b) for all [p]. We first evaluate these gradients at the single-frequency θm[j] = α m) for all j, and then to extract the DFT coefficients. and ξm[j] = β cos(ωkj + ψ cos(ωkj + ϕ m) 56 Step 1: Gradient of ξm. First observe that max{x, 0} = (x + x)/2. Then, we have (cid:88) (x,y)Sp σ(ex + ey, θm) = 1 2 (cid:88) ex + ey, θm + 1 2 (cid:88) (x,y)Sp ex + ey, θm (x,y)Sp (cid:88) = α 2 (x,y)Sp cos(ωkx + ϕ m) + cos(ωky + ϕ m), (C.15) Moreover, by applying the sum-to-product trigonometric identities, we can show that 1 (cid:88) (x,y)Sp cos(ωkx + ϕ m) + cos(ωky + ϕ m) (cid:88) = (x,y)Sp cos(ωk(x + y)/2 + ϕ m) cos(ωk(x y)/2) = cos(ωkj/2 + ϕ m) cos(ωkx/2) = 2p π (cid:88) xZp cos(ωkj/2 + ϕ m). (C.16) The last inequality uses the fact that for an odd prime p, {ωkx}xZp = {2kxπ/p}xZp = {2πx/p}xZp which is uniform sample of [0, 1]. Thus, in the limit , we have , 1 (cid:88) xZp cos(ωkx/2) = (cid:90) 1 0 cos(πx)dx = 1 π (cid:90) π 0 cos(u)du = 2 π . By putting these two asymptotic expressions (C.15) and (C.16) into (C.14b), we obtain that ℓm ξm[j] = pα π (cid:18) cos(ωkj/2 + ϕ m) cos(ωki/2 + ϕ m) (cid:19) , [p]. 1 (cid:88) i=1 Next, we apply DFT with respect to ξmℓm in the asymptotic regime . Let rk [p] denote the multiplication factor in Definition 6.2, i.e., rkk = mod for k, [ p1 2 ]. Then, we have 1 2p (cid:88) j=1 cos(ωkj/2 + ϕ m) exp(i ωkj) = (1)rk+1 π(4r2 1) (cid:123)(cid:122) (cid:125) (cid:124) := ςrk exp(2rkϕ i). (C.17) cosine derivation of (C.17) proceeds as follows: 1 p (cid:88) j=1 cos(ωkj/2 + ϕ m) cos(ωkj) = (cid:90) 0 cos(πkx + ϕ m) cos(2rkπkx)dx = = (cid:90) π 1 π 0 cos(2rkϕ π m) cos(u) cos(2rk (u ϕ m))du = cos(u) cos(2rku)du cos(2rkϕ π m) (cid:90) π 0 (cid:32)(cid:90) π 0 cos((2rk + 1)u)du + (cid:90) π 2 0 cos((2rk 1)u)du = (cid:33) 2(1)rk+1 π(4r2 1) cos(2rkϕ m), where the third equality follows from trigonometric identities, evenness of sin(2ru), and periodicity. similar calculation applies to the sine, and combining both real and imaginary parts yields (C.17). Therefore, we have ξmℓm, b2k = 2 2 α m/π p3/2 ςrk cos(2rkϕ m), ξmℓm, b2k+1 = 2 2 α m/π p3/2 ςrk sin(2rkϕ m), and thus ξm / ξm = ςrk /ςrk = Θ(r2 ). Moreover, it follows by simple calculation (P kξmℓm)[j] = ξmℓm, b2k b2k[j] + ξmℓm, b2k+1 b2k+1[j] cos(2kj + 2ϕ m), for all [p] such that we have kξmℓm ξm. Step 2: Gradient of θm. Following (C.14a), first notice that ξm[mp(x, j)] 1(ex + ej, θm) (cid:88) xZp = β (cid:88) xZp cos(ωk(x + j) + ψ m) 1(cos(ωkx + ϕ m) + cos(ωkj + ϕ m) 0) = pβ π sin(ωkj + ϕ m) cos(ωkj + ψ ϕ m), where the last equality results from the following calculation under the asymptotic regime: 1 (cid:88) xZp cos(ωk(x + j) + ψ m) 1(cos(ωkx + ϕ m) + cos(ωkj + ϕ m) 0) cos(2πx + ωkj + ψ m) 1(cos(2πx + ϕ m) + cos(ωkj + ϕ m))dx (cid:90) 1 = 0 (cid:90) 1 2π 1 2π = = = m+2π muϕ ϕ cos(u) cos(ωk j+ϕ m) ϕ cos(ωkj + ψ (cid:90) m) 1 π sin(arccos( cos(ωkj + ϕ (cid:124) (cid:123)(cid:122) = sin(ωkj + ϕ m) m))) (cid:125) cos(u + ωkj + ψ ϕ m)du cos(u)du 0u2π cos(u) cos(ωk j+ϕ m) cos(ωkj + ψ ϕ m). By applying DFT over θmℓm in the asymptotic regime , we can show that 1 (cid:88) j= sin(ωkj + ϕ m) cos(ωkj + ψ ϕ m) exp(i ωkj) = 1 π (cid:26) exp({ψ (rk + 2)ϕ rk(rk + 2) m} i) + exp({ψ + (rk 2)ϕ rk(rk 2) m} i) (cid:27) 1(rk is odd), (C.18) where rkk = mod p. The above results follow the calculation below: 1 (cid:88) j=1 sin(ωkj + ϕ m) cos(ωkj + ψ ϕ m) cos(ωkj) (cid:90) 1 = = 1 2π 0 (cid:90) 2πϕ ϕ sin(2πkx + ϕ m) cos(2πkx + ψ ϕ m) cos(2πrkkx)dx sin(u) cos(u + ψ 2ϕ m) cos(rk(u ϕ m))du 58 (cid:90) 2π = 1 4π 0 1 4π (cid:90) 2π 0 + sin(u) cos((rk + 1)u + ψ (rk + 2)ϕ m)du sin(u) cos((rk 1)u ψ (rk 2)ϕ m)du, (C.19) m/ ψ (cid:90) 2π where for h1 = rk 1 and h2 = ψ (rk + 2)ϕ (rk 2)ϕ , we can further show show (cid:90) 2π 0 sin(u) cos(h1u + h2)du = cos(h2) sin(u) cos(h1u)du = (1 + (1)h1) cos(h2) sin(u) cos(h1u)du = (cid:90) π 0 4 1 h2 1 cos(h2) 1(h1 is even). (C.20) By combining (C.19) and (C.20), and performing similar calculation for the sine component, we obtain the result in (C.18) This implies that for even rk, we have θmℓm, b2k = 2β m/π p3/2 θmℓm, b2k+1 = 2β m/π p3/2 (cid:110) cos(ψ (cid:110) sin(ψ m) (rk + 2)ϕ rk(rk + 2) (rk + 2)ϕ rk(rk + 2) m) + cos(ψ sin(ψ + (rk 2)ϕ rk(rk 2) + (rk 2)ϕ rk(rk 2) m) (cid:111) , m) (cid:111) , Hence, k(θm)/(θm) = Θ(r ) 1(rk is even) and for all [p] (P kθmℓm)[j] = θmℓm, b2k b2k[j] + θmℓm, b2k+1 b2k+1[j] cos(wkj + ϕ m), which gives that kθmℓm θm and completes the proof."
        },
        {
            "title": "D Comparison with Existing Results",
            "content": "Our work is closely related to that of Tian (2024) and Wang and Wang (2025), who studied two-layer network for learning group multiplication on an Abelian group, which is generalization of the standard modular addition task. For theoretical convenience, they adopt modified ℓ2-loss to mitigate noisy interactions induced by the constant frequency. Let 11 denote the mean-zero projection, then the loss is defined as 1 = 1 (cid:101)ℓ(ξ, θ) = (cid:88) (cid:88) xZp yZp (cid:16) (cid:13) (cid:13) (cid:13) 1 1/2p (x, y; ξ, θ) e(x+y) mod (cid:17)(cid:13) 2 (cid:13) (cid:13) , (D.1) where the output of the network is normalized by 1/2p within loss calculation. Unlike (D.1), we show that minimizing standard CE loss with small initialization naturally decouples the dynamics of each frequency (see Theorem 5.2), with the constant frequency having zero gradient throughout training and therefore remaining zero under zero-initialization (see Corollary 6.1). Notation Clarifications. We begin by explaining the notation used in Tian (2024). In their analysis, the (modified) complex Fourier coefficients of the weights are given by zqkm C, where the indices {ξ, θ}, [M ] and [p 1] {0} correspond to the layer, neuron, and frequency, respectively. This complex representation is equivalent to the real-valued cosine-sine pairs used in our DFT definition in 5.1. Specifically, for all (p 1)/2, we can show that zθkm = αk m/ 2 exp(iϕk m), zξkm = βk m/ 2 exp(iψk m). 59 By the conjugate symmetry of the DFT coefficients, our single real component at frequency determines the complex coefficients for both and k. Therefore, for the higher frequencies (p + 1)/2 p, the relationship is given by zθkm = zθ(pk)m = αk m/ 2 exp(iϕk m), zξkm = zξ(pk)m = βk m/ 2 exp(iψk m), which completes the one-to-one correspondence between our basis and the one used by Tian (2024). Loss Landscape within Fourier Domain. Tian (2024) expresses the loss (cid:101)ℓ from (D.1) in the Fourier domain using {zqkm}. In Theorem 1, they show that the loss (cid:101)ℓ decouples into per-frequency terms (cid:101)ℓ = p1 (cid:80) k=0 (cid:101)ℓk + (p 1)/p, where (cid:101)ℓk is quadratic polynomial whose variables {ρk1k2k}k1,k2[p1] are third-order monomials of the Fourier coefficients. Formally, we have (cid:101)ℓk = poly(cid:0){ρk1k2k}k1,k2[p1] (cid:1), where ρk1k2k = (cid:88) m=1 zθk1mzθk2mzξkm. (D.2) Mean-Field Dynamics. Building on their analysis of the loss, Theorem 7 in Tian (2024) presents heuristic result for the gradient dynamics. By considering truncated loss polynomial from (D.2), symmetric Gaussian initialization, and the mean-field limit , they show that tρk1k2k(t) = 2 ζk1k2k(t) {1(k1 = k2 = k) ρk1k2k(t)}, (D.3) where ζk1k2k(t) is term of constant order along the training. The solution to the ODE in (D.3) provides more high-level theoretical basis for the emergence of the key structural properties we identified in our work. Consider the case k1 = k2 = k, we have ρkkk(t) = (cid:88) m=1 z2 θkm(t) zξkm(t) (cid:88) m=1 m(t)2 βk αk m(t) exp(i{ψk m(t) 2ϕk m(t)}) 1. For this to hold, the imaginary part of ρkkk(t) should converge to 0: ℑ(ρkkk(t)) (cid:88) m=1 m(t)2 βk αk m(t) sin(ψk m(t) 2ϕk m(t)) 0. This convergence is direct consequence of the phase alignment dynamic (2ϕk 0 as revealed in 5.5. Moreover, if we consider k1, k2 = k, then we have m(t)ψk m(t)) mod 2π ρk1k2k(t) (cid:88) m=1 (t) αk2 αk1 (t) βk m(t) exp(i{ψk m(t) ϕk (t) ϕk2 (t)}) 0. sufficient condition for this is that the product of amplitudes αk1 m(t) goes to zero for all [M ]. This corresponds precisely to the single-frequency sparsity we observed in 6.1. Beyond these, Tian (2024) also discussed data with general algebraic structure and its relationship with properties of global optimizers. Recently, Wang and Wang (2025) formalized these mean-field dynamics by modeling the networks parameters as continuous distribution. This approach allows the training process to be rigorously described as Wasserstein gradient flow on the measure space. (t) αk2 (t) βk"
        }
    ],
    "affiliations": [
        "Yale University"
    ]
}