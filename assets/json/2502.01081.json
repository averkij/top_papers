{
    "paper_title": "The Jumping Reasoning Curve? Tracking the Evolution of Reasoning Performance in GPT-[n] and o-[n] Models on Multimodal Puzzles",
    "authors": [
        "Vernon Y. H. Toh",
        "Yew Ken Chia",
        "Deepanway Ghosal",
        "Soujanya Poria"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The releases of OpenAI's o1 and o3 mark a significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of o1 comes at nearly 750 times the computational cost of GPT-4o, raising concerns about its efficiency. Our results reveal a clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to o1. Nonetheless, we observe that the o1 model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available https://github.com/declare-lab/LLM-PuzzleTest."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 1 8 0 1 0 . 2 0 5 2 : r THE JUMPING REASONING CURVE? TRACKING THE EVOLUTION OF REASONING PERFORMANCE IN GPT- [N] AND O-[N] MODELS ON MULTIMODAL PUZZLES Vernon Y.H. Toh Yew Ken Chia Deepanway Ghosal"
        },
        {
            "title": "Soujanya Poria",
            "content": "Singapore University of Technology and Design (SUTD) Figure 1: The performance of GPT-[n] and o-[n] series models on PUZZLEVQA and ALGOPUZZLEVQA, illustrating how multimodal reasoning evolves over time with model releases and inference cost. The size of each circle roughly represents the inference cost per puzzle."
        },
        {
            "title": "ABSTRACT",
            "content": "The releases of OpenAIs o1 and o3 mark significant paradigm shift in Large Language Models towards advanced reasoning capabilities. Notably, o3 outperformed humans in novel problem-solving and skill acquisition on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI). However, this benchmark is limited to symbolic patterns, whereas humans often perceive and reason about multimodal scenarios involving both vision and language data. Thus, there is an urgent need to investigate advanced reasoning capabilities in multimodal tasks. To this end, we track the evolution of the GPT-[n] and o-[n] series models on challenging multimodal puzzles, requiring fine-grained visual perception with abstract or algorithmic reasoning. The superior performance of o1 comes at nearly 750 times the computational cost of GPT-4o, raising concerns about its efficiency. Our results reveal clear upward trend in reasoning capabilities across model iterations, with notable performance jumps across GPT-series models and subsequently to o1. Nonetheless, we observe that the o1 model still struggles with simple multimodal puzzles requiring abstract reasoning. Furthermore, its performance in algorithmic puzzles remains poor. We plan to continuously track new models in the series and update our results in this paper accordingly. All resources used in this evaluation are openly available 1. 1https://github.com/declare-lab/LLM-PuzzleTest"
        },
        {
            "title": "INTRODUCTION",
            "content": "Recent advances in large language models (LLMs) have demonstrated impressive capabilities in language understanding and generation, as seen in OpenAIs GPT-[n] series of models (Brown et al., 2020). Yet, true artificial general intelligence (AGI) requires robust reasoning abilities across different modalities (Fei et al., 2021). For instance, models such as OpenAIs new o-[n] series demonstrate jumping reasoning curve through dramatic improvements on the Abstraction and Reasoning Corpus for Artificial General Intelligence (ARC-AGI) (Chollet, 2019). However, the current evaluations in Figure 2 mainly focus on symbolic patterns, whereas humans often reason over complex data involving vision and language. Thus, the ability to perceive, understand, and reason about multimodal inputs remains crucial component of human-like intelligence, deserving urgent investigation. Figure 2: ARC-AGI semi-private scores of the OpenAI models over time. To this end, puzzles often serve as effective measures of cognitive abilities such as pattern recognition and step-by-step reasoning. Notably, such measures typically do not require specific domain knowledge, allowing individuals from diverse backgrounds to engage with them. One prominent example is Ravens Progressive Matrices (Raven, 1989), non-verbal assessment tool designed to evaluate abstract reasoning and fluid intelligence. In this test, participants are presented with abstract patterns containing missing element and must identify the correct piece to complete the pattern. Thus, inspired by abstract puzzles as measures of intelligence, recent multimodal benchmarks have enabled systematic evaluation across specific cognitive abilities, including visual perception, inductive reasoning, deductive reasoning, and algorithmic problem solving (Chia et al., 2024; Ghosal et al., 2024). Compared to previous measures, they require general understanding of spatial relationships, pattern recognition, and reasoning across visual and language elements, thus providing more holistic measure of artificial general intelligence. Our research addresses several key questions: (1) How do current state-of-the-art models perform on visual reasoning tasks? (2) What types of pattern recognition and reasoning are particularly challenging? (3) How can we systematically evaluate and compare different models multimodal reasoning capabilities? 2 Figure 3: Case study on an abstract puzzle from the Colors & Shapes (left) category and Colors & Numbers (right) category in PUZZLEVQA. In our evaluation, we assess the performance of GPT-[n] and o-[n] models on abstract multimodal puzzles from PuzzleVQA, which primarily test abstract reasoning. Additionally, we evaluate the models on AlgoPuzzleVQA, which require an algorithmic approach rather than brute-force solving. To ensure comprehensive evaluation, we present the puzzles in both multiple-choice and openended question answering formats. Our findings indicate that despite their sophisticated capabilities in standard benchmarks, current models still struggle with seemingly simple multimodal puzzles (Figure 3). Contrary to previous benchmarks such as ARC-AGI, we observe less dramatic reasoning curve without extreme jumps in performance. This limitation highlights the substantial gap between current artificial intelligence and human-like reasoning abilities. As the models continue to rapidly advance and scale as in Figure 1, this benchmark will serve as critical indicator of progress toward more robust and generalized artificial intelligence. Overall, here are the key findings of our study: TL;DR 1. Performance steadily improves from GPT-4-Turbo to GPT-4o to o1. While the jump from GPT-4-Turbo to GPT-4o is moderate, the transition from GPT-4o to o1 marks significant advancement but it comes at cost of 750x more inference cost. 2. Although o1 exhibits notable improvement in reasoning performance, it still falls far short of human performance on the simple visual abstract reasoning dataset, PUZZLEVQA. 3. GPT-4-Turbo and GPT-4o both face significant bottlenecks in perception and inductive reasoning. 4. o1s primary bottleneck lies in perception. With ground truth perception provided, o1 shows strong reasoning capabilities, outperforming GPT-4-Turbo and GPT-4o by 18-20%. 5. In particular, o1 struggles with reasoning based on visual shapes and sizes. 6. As the complexity of multimodal puzzles increasesfor instance, in puzzles from ALGOPUZZLEVQA or dual-concept puzzles in PUZZLEVQA that combine multiple dimensions such as colors and numbersall models experience noticeable performance decline."
        },
        {
            "title": "2 PUZZLEVQA & ALGOPUZZLEVQA",
            "content": "Understanding the capabilities and limitations of large multimodal models in visual reasoning tasks requires datasets that challenge their cognitive capabilities in nuanced ways. In this study, we employ PUZZLEVQA (Chia et al., 2024) and ALGOPUZZLEVQA (Ghosal et al., 2024) to evaluate abstract visual reasoning and algorithmic problem-solving capabilities. Multimodal puzzles serve as crucial benchmark for evaluating large multimodal models because they require unique combination of perception, reasoning, and abstraction. Unlike other abstract reasoning benchmarks such as ARC-AGI, where test examples are input to the model as textual context, multimodal puzzles requires the integration of visual and textual information to solve the problem. They also provide an ideal setting for probing systematic reasoning and generalization, as their structured yet diverse nature tests the abilities to infer patterns and apply them across novel contexts. Examples from PUZZLEVQA and ALGOPUZZLEVQA are shown in Figure 4 and Figure 5. These two datasets were chosen for their complementary characteristics: while PUZZLEVQA emphasizes basic visual abstract reasoning, requiring pattern recognition to solve puzzles, ALGOPUZZLEVQA features more complex puzzles that demand deducing algorithms for their solutions."
        },
        {
            "title": "2.1 PUZZLEVQA COMPOSITION",
            "content": "Figure 4: Example single-concept and dual-concept abstract puzzles in PUZZLEVQA, designed around fundamental concepts such as numbers, colors, size, and shapes. PUZZLEVQA consists of 2,000 test instances, organized into 10 puzzle categories. Four of these categories focus on single-concept patterns, such as numbers, colors, sizes, and shapes, while the remaining six categories emphasize dual-concept patterns, which combine two distinct concepts. We present some puzzle examples in Figure 4. Each category includes two multimodal templates, with each template capable of generating variety of unique puzzle instances. 4 Each puzzle is formulated with the following core components: 1. Objects: Conceptual elements like numbers, colors, shapes, and sizes. 2. Layout: The spatial arrangement of objects, which provides necessary visual context. 3. Pattern: The underlying rules governing object interactions (e.g., spatially opposite parts must share the same color). 4. Demonstrations: Multiple instances of interacting objects that represent the underlying pattern. 5. Query: natural language question prompting the model to solve the puzzle by reasoning about the missing element. For clarity, detailed example of puzzle formulation is provided in Appendix A.1. Using these components, 100 unique puzzle instances were generated from each multimodal template, resulting in total of 2,000 test instances. PUZZLEVQA is designed to evaluate the reasoning capabilities of large multimodal models, focusing on their ability to interpret abstract patterns that require both visual and textual understanding. By encompassing diverse range of singleand dual-concept puzzles, the dataset aims to reveal the strengths and weaknesses of current large multimodal models."
        },
        {
            "title": "2.2 ALGOPUZZLEVQA COMPOSITION",
            "content": "Figure 5: Example of puzzles from ALGOPUZZLEVQA with visual features represented in the top row and algorithmic features in the bottom two rows. For each feature, at least one puzzle instance from each category is presented. Note that the header categories are not exhaustive, as some puzzles may belong to additional categories not listed in the headers. The complete categorization can be found in Appendix B.1. ALGOPUZZLEVQA consists of 18 distinct puzzles, each with 100 test instances, resulting in total of 1,800 test instances. These puzzles cover wide range of topics, combining both visual and algorithmic categories. Each puzzle includes at least one visual category and one algorithmic category. We present some puzzle examples in Figure 5. 5 Visual categories: 1. Colors: Puzzles where understanding the colour of the puzzle components is crucial for solving the question. 2. Position: In some puzzles, understanding spatial positioning of the puzzle components is necessary for solving the question. 3. Shape/Size: This category includes the understanding of both absolute and relative shapes and sizes of the puzzle components. 4. Text: Certain puzzles incorporate optical characters or embedded text that provide important information that must be used to correctly solve the question. Algorithmic categories: 1. Arithmetic: These puzzles require basic mathematical operations, such as addition, multiplication, counting, and modular arithmetic, to solve the problem. 2. Boolean Logic: Some puzzles require the application of Boolean logic, such as checking conditions like equality or inequality between different components or states. 3. Combinatorics: These puzzles involve counting combinations and permutations of the components or states. The questions typically ask about the number of unique configurations that can be achieved after performing sequence of operations. 4. Graphs: Puzzles in this category can be represented as graph data structures, where graph algorithms can be applied to find the solution. 5. Optimization: Optimization puzzles focus on finding the best solution, whether it involves minimizing time, steps, or maximizing given outcome (e.g., summation or sorting). 6. Search: These puzzles require the use of search algorithms, including breadth-first search or exhaustive search, to explore possible solutions or configurations. 7. Sets: In these puzzles, solving the problem requires considering the identical nature of some objects and the equivalence of some positions or configurations. The algorithmic categories are not mutually exclusive, and puzzles may contain two or more categories in order to derive the answer. The primary goal of ALGOPUZZLEVQA is to assess the gap between visual data interpretation and algorithmic problem-solving skills. The puzzles are designed to challenge and evaluate large multimodal models, testing their ability to solve algorithmic problems that require visual understanding, language comprehension, and complex algorithmic reasoning."
        },
        {
            "title": "3.1 EVALUATION PIPELINE",
            "content": "To ensure comprehensive evaluation, we present the puzzles to the models in both multiple-choice and open-ended formats. The original datasets consist of puzzles in multiple-choice format. Below, we provide detailed explanation of both the multiple-choice and open-ended setups."
        },
        {
            "title": "3.1.1 MULTIPLE CHOICE SETUP",
            "content": "(First stage) CoT Prompting. We leverage zero-shot chain of thought (CoT) prompting (Kojima et al., 2022) with prompt similar to Lets think step by step to elicit reasoning steps from GPT-[n] models. For the o-[n] model, we do not use CoT prompting since these models are trained to perform reasoning internally. If the letter answer can be extracted during the first prompting stage with regular expressions, we skip the second stage. However, if the letter answer cannot be extracted, we proceed to the answer extraction stage. (Second stage) Answer Extraction. We take the initial prompt from the first stage and the generated output, then append the text \"Therefore, among (A) (B) (C) (D), the answer is:\" for puzzles with four options, or \"Therefore, among (A) (B) (C), the answer is:\" for puzzles with three options. This allows us to extract the final letter answer"
        },
        {
            "title": "MULTI CHOICE",
            "content": "GPT-4-Turbo GPT-4o E U V Z G A"
        },
        {
            "title": "Colors\nNumbers\nShapes\nSize",
            "content": "Colors & Numbers Colors & Shapes Colors & Size Numbers & Shapes Numbers & Size Size & Shapes"
        },
        {
            "title": "Clock\nColour Hue\nMap Colour\nMaze Solve",
            "content": "Move Box N-Queens Number Slide Rotten Fruits Rubiks Cube Think Dot Tower of Hanoi"
        },
        {
            "title": "Average",
            "content": "51.0 82.5 32.5 19.0 54.5 30.0 31.5 31.5 24.5 28.5 38.6 46.0 43.0 1.0 3.0 0.0 5.0 10.0 16.0 20.0 17.0 14.0 32. 32.0 36.0 0.0 8.0 14.0 0.0 16.5 72.5 84.5 51.5 39.0 48.0 45.5 21.5 20.0 34.5 50.5 46. 46.0 52.0 3.0 7.0 3.0 10.0 22.0 8.0 23.0 16.0 32.0 53.0 44.0 41.0 2.0 23.0 29.0 1.0 23. o1 80.5 96.5 54.5 54.5 97.0 75.0 30.0 78.0 41.5 55.0 66.3 51.0 83.0 1.0 34.0 6.0 15.0 21.0 17. 23.0 16.0 71.0 43.0 54.0 32.0 39.0 42.0 31.0 0.0 32.2 GPT-4-Turbo GPT-4o 42.5 85.0 59.5 37.5 64.5 61.5 50.0 54.5 32.5 55.0 54.2 49.0 63.0 29.0 25.0 27.0 36.0 38.0 40.0 36.0 35.0 45.0 36. 52.0 47.0 15.0 29.0 40.0 15.0 36.5 77.0 87.0 71.0 44.0 64.5 66.0 58.5 55.5 30.5 60.5 60. 52.0 66.0 39.0 30.0 33.0 28.0 49.0 47.0 36.0 35.0 46.0 56.0 48.0 50.0 35.0 68.0 44.0 23.0 43. o1 91.5 99.0 66.5 77.5 99.5 80.5 50.0 92.5 49.0 86.5 79.2 47.0 92.0 61.0 52.0 83.0 23.0 50.0 50. 30.0 20.0 89.0 56.0 74.0 60.0 68.0 49.0 67.0 25.0 55.3 Table 1: Accuracy scores of GPT-[n] and o-[n] models on PUZZLEVQA and ALGOPUZZLEVQA. and compare it to the ground truth. The accuracy of predicting the correct final answer is used as the evaluation metric."
        },
        {
            "title": "3.1.2 OPEN ENDED SETUP",
            "content": "(First stage) CoT Prompting. Similar to the setup described in Section 3.1.1, we use CoT prompting for GPT-[n] models. However, for o-[n] models, we do not use CoT prompting. In the openended setup, instead of performing answer extraction, we use GPT-4o to directly match the generated answer with the ground truth answer. (Second stage) Answer Matching. For open-ended responses, we use GPT-4o to compare the generated responses from the first stage with the ground truth answers. Specifically, GPT-4o is prompted to evaluate whether the generated response aligns with the ground truth answer. The exact prompt used for this evaluation is provided in Appendix C. Similar to the multiple-choice setup, the accuracy of predicting the correct final answer is used as the evaluation metric."
        },
        {
            "title": "3.2 MODELS",
            "content": "We investigate the performance of GPT-[n] and o-[n] models: (1) GPT-4-Turbo (turbo-2024-04-09), (2) GPT-4o (2024-08-06), (3) o1 (2024-12-17). We selected these two model series from OpenAI due to their rapid advancements and significant contributions to the field of large language models (LLMs). Each version has introduced innovative techniques that have shaped the LLM landscape. For example, GPT-4-Turbo has set benchmarks in understanding visual inputs, while GPT-4o is highly efficient model designed for multimodal inputs and outputs. The o1 model, recent addition, is trained with step-by-step reasoning objective and reinforcement learning, making it powerful reasoner capable of handling wide range of tasks effectively. We use the high reasoning mode for o1. Please note that our study can easily be expanded to other closed-sourced and open-sourced models."
        },
        {
            "title": "4.1 SCALING TRENDS",
            "content": "To investigate the evolution of reasoning performance, we present the average accuracy on PUZZLEVQA and ALGOPUZZLEVQA over time, along with the inference cost per puzzle, as shown in Figure 1. The reported accuracy corresponds to the open-ended setting, while the inference cost per puzzle is estimated based on the average API cost for processing 200 puzzle questions. We observe more significant jump in performance from the GPT-[n] to o-[n] models, highlighting their enhanced reasoning capabilities. However, this reasoning advancement comes at more than 750x inference cost compared to GPT-4o, likely due to more extensive reasoning steps or hidden processes (Wei et al., 2022)."
        },
        {
            "title": "4.2 EXPANDED EVALUATION",
            "content": "To assess the holistic reasoning capabilities of multimodal models, we present results for both openended and multiple-choice answer formats in Table 1. Overall, we observe that all models generally perform better in the multiple-choice setting compared to the open-ended setting. Particularly, the o1 model experiences the largest performance decline, with 23.1% drop in score on ALGOPUZZLEVQA between multiple-choice and open-ended settings. Conversely, the o1 model shows the smallest performance decline on PUZZLEVQA, with score drop of 12.9% between the two formats. PUZZLEVQA. PUZZLEVQA is relatively simple dataset designed to evaluate the abstract reasoning abilities of large multimodal models. According to (Chia et al., 2024), human performance on subset of this dataset in the multiple-choice setting reaches score of 91.4%. However, GPT-[n] and o-[n] models still fall significantly short of this benchmark, with o1 achieving the highest score among them at 79.2% in the multiple-choice setting. Among single-concept puzzles, most models find the size and shape categories to be the most challenging. Specifically in the multiple-choice setting, o1 achieves scores of only 66.5% for shapes and 77.5% for size categories, in contrast to other single-concept categories like colors and numbers, which achieve significantly higher scores of 91.5% and 99.0%, respectively. Performance declines further in dual-concept puzzles compared to single-concept ones. In particular, models struggle more with those involving combinations such as Numbers & Size, Size & Shapes, and Colors & Size. The lowest scores were observed with the Numbers & Size puzzle, where GPT-4-Turbo, GPT-4o, and o1 achieved only 32.5%, 30.5%, and 49.0%, respectively. Overall, we observe moderate improvement in performance from GPT4-Turbo to GPT-4o and substantial leap from GPT-4o to o1 across majority puzzle categories in PUZZLEVQA. This highlights the effectiveness of the specialized reasoning enhancements introduced in o1. Another key finding is the inability of GPT-4-Turbo to effectively perceive and reason with colorsa challenge that GPT4o and o1 overcome to some extent, outperforming GPT-4-Turbo by approximately 22% to 29% in this area in the open-ended setting. Additionally, o1 demonstrates superior performance in numerical reasoning and puzzles within the size category. Interestingly, o1 does not improve GPT-4os performance on puzzles requiring reasoning about shapes. In fact, in the multiple-choice setting, o1 underperforms GPT-4o for the shapes category by 4.5%. ALGOPUZZLEVQA. ALGOPUZZLEVQA is more challenging visual puzzle reasoning dataset that demands algorithmic problem-solving abilities. The performance of all models remains relatively low on this dataset, achieving score of 36.5%, 43.6%, and 55.3% for GPT-4-Turbo, GPT-4o, and o1 respectively in the multiple-choice setting. However, similar to PUZZLEVQA, we observe notable improvement in performance with o1 compared to GPT-4-Turbo and GPT-4o. Specifically for puzzles such as calendar, clock, and number slide in the multiple-choice setting, o1 demonstrates significant improvements over GPT-4o, with performance gains of 26%, 50%, and 43%, respectively. In the open-ended setting, we observe significant drop in performance compared to the multiple-choice setting, particularly on puzzles like Chain Link and Wood Slide, where performance across all models is close to 0%. For example, o1 achieves scores of 1.0% on Chain Link and 0.0% on Wood Slide in the open-ended setting, while it achieves 61.0% and 25.0%, respectively, in the multiple-choice setting. On the other hand, o1 performs well on puzzles like Calendar and Number Slide, reaching scores of 92% and 89%, showing considerable improvement over GPT-4o, which scores 66% and 46%, respectively. 8 Figure 6: Case study on Clock in ALGOPUZZLEVQA on multiple-choice and open-ended setting. Figure 7: Case study on Chain Link in ALGOPUZZLEVQA on multiple-choice and open-ended setting."
        },
        {
            "title": "4.3 DISCUSSIONS",
            "content": "Multiple Choice vs Open Ended Problems. We evaluate the models in two settings: multiplechoice setup and an open-ended setup. As shown in Table 1, all models experience significant performance decline in the open-ended setup. In PUZZLEVQA, the average drop is relatively mild (ranging from 8 to 15%), whereas in ALGOPUZZLEVQA, the average decline is more pronounced (ranging from 20 to 28%), indicating the increased difficulty of the task. 9 Specifically, for the Clock puzzle in ALGOPUZZLEVQA, we observed 77% drop in accuracy. To illustrate this, we present case study in Figure 6. Clock puzzles require fine-grained visual perception, as accurately determining the exact time displayed on the clock is crucial for answering correctly. In the open-ended setting, we found that o1 lacked the necessary precision in visual perception, misreading the clock as 2:45 instead of 2:43. Although the underlying concept was correctly applied, this small discrepancy led to an incorrect answer. However, in the multiple-choice format, the presence of the answer choices provided helpful cue, guiding the model toward greater precision in interpreting the clock. As result, it correctly selected option (C) 4:23. Additionally, we present another case study on the Chain Link puzzle, where o1 scored only 1% in the open-ended setting compared to 61% in the multiple-choice setting (Figure 7). In the multiplechoice setting, although the model selected the correct answer, its reasoning was incorrect. The correct solution to this problem requires 4 cuts and 7 joins, totaling 34 minutes. However, o1 incorrectly outputted 2 cuts and 12 joins. Despite this mistake, the sum still resulted in 34, which coincidentally matched the correct answer. Notably, the model seemed to settle on 34 early in the reasoning process rather than considering larger value, such as 69. This could be because 69 was not among the provided answer choices. Interestingly, in the open-ended setting where multiplechoice options were not provided, the model instead arrived at an answer of 69. Figure 8: Case study on Number Slide in ALGOPUZZLEVQA across GPT-[n] and o-[n] models. Gap between GPT-[n] and o-[n] models. Based on the results in Table 1, we observed significant performance gap between the GPT-[n] and o-[n] models. One particular case is the Number Slide puzzle, where the o1 model achieves 71% accuracy, while GPT-4o and GPT-4-Turbo score only 32% and 14%, respectively. To further analyze this discrepancy, we present case study on specific test instance of the Number Slide puzzle in the open-ended setting (Figure 8). Both GPT-4o and GPT-4-Turbo incorrectly perceive the open position as being in the bottom-right corner of the 44 grid, leading to the wrong answer. In contrast, the o1 model accurately interprets the visual layout, correctly identifying the blank space in row 2, column 4. This demonstrates its superior visual perception capabilities, allowing it to ultimately arrive at the correct answer. Reasoning Bottlenecks. To analyze reasoning bottlenecks, following Chia et al. (2024), we incrementally provided the models with additional ground truth guidance, namely perception and induction steps (Appendix D). We conducted this analysis on the open-ended setting and the detailed results of this experiment are reported in Table 2. We observe that perception is the primary bottle-"
        },
        {
            "title": "Original",
            "content": "GPT-4-Turbo w/ p. w/ p. & i. Original"
        },
        {
            "title": "OPEN ENDED",
            "content": "GPT-4o w/ p. w/ p. & i. Original E U P"
        },
        {
            "title": "Colors\nNumbers\nShapes\nSize",
            "content": "Colors & Numbers Colors & Shapes Colors & Size Numbers & Shapes Numbers & Size Size & Shapes"
        },
        {
            "title": "Average",
            "content": "51.0 82.5 32.5 19.0 54.5 30.0 31.5 31.5 24.5 28.5 38.6 75.0 77.0 71.5 64.5 67.0 81.0 53.5 29.5 70.0 97.5 68. 97.0 98.5 97.5 95.5 89.5 64.5 75.5 84.5 63.0 93.0 85.8 72.5 84.5 51.5 39.0 48.0 45.5 21.5 20.0 34.5 50.5 46. 80.0 88.5 63.5 62.5 52.0 77.5 78.0 33.5 73.0 92.5 70.1 92.0 99.5 97.5 96.5 89.5 77.0 94.5 85.5 73.5 92.5 89. 80.5 96.5 54.5 54.5 97.0 75.0 30.0 78.0 41.5 55.0 66.2 o1 w/ p. 94.0 98.0 55.5 98.0 95.0 81.5 99.0 86.0 81.5 98. 88.6 w/ p. & i. 99.0 97.0 100.0 100.0 100.0 89.5 94.0 91.0 77.5 99.5 94.8 Table 2: Bottleneck analysis of GPT-[n] and o-[n] models on PUZZLEVQA. Original refers to our main setting where only question and an image are provided as input. To reveal the specific multimodal reasoning bottlenecks, we progressively inject ground-truth explanations in the input for visual perception (p.) and inductive reasoning (i.). We provide an example of the different prompts used in the bottleneck analysis in Figure 11. Figure 9: Case study on an abstract puzzle from the Numbers & Size category in PUZZLEVQA. neck across all models. By injecting visual details of the puzzle in the input prompt as guidance, the results improve by 22% to 30% for all models. GPT-4-Turbo and GPT-4o show weaknesses in inductive reasoning as we observe that after injecting inductive reasoning in the input prompt, the performance improves by further 16% to 19% over the original with perception setting. Furthermore, the inductive reasoning superiority of GPT-4o over GPT-4-Turbo on PUZZLEVQA is minimal, as GPT-4o only outperforms GPT-4-Turbo by 1.5% even with visual perception guidance. On the other hand, o1 demonstrates strong inductive reasoning capabilities, its performance improves only moderately by 6% with visual perception and inductive reasoning guidance, suggesting that perception is its primary limitation. With accurate visual perception guidance, o1 can effectively perform inductive reasoning and achieve high score. However, there are still instances where o1 fails, even with perception and induction step guidance (Figure 9). In the original open-ended setting, o1 reasons that the puzzles pattern involves pairing opposite circles and summing their values. When additional visual details are provided in the perception setting, o1 instead concludes that the pattern is based on the sums of adjacent numbers. 11 Even in the perception and induction setting, where both visual details and the underlying pattern are explicitly provided, o1 still incorrectly interprets the numbers as primes. In each case, these misinterpretations lead to an incorrect final answer due to an inaccurate prediction of the underlying pattern."
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this study, we evaluated and analyzed the multimodal reasoning capabilities of GPT-[n] and o-[n] models on PUZZLEVQA and ALGOPUZZLEVQA. Our experiments highlight significant improvements in multimodal reasoning performance from GPT-[n] to o-[n] models, with o1 demonstrating the most substantial gains. However, these advancements come at considerably higher inference cost. Across both multiple-choice and open-ended settings, models consistently perform better in the multiple-choice setting, with o1 experiencing the largest performance drop on ALGOPUZZLEVQA in the open-ended setting. In PUZZLEVQA, o1 outperforms previous models, especially in numerical reasoning tasks, although it still faces difficulties with shape-related puzzles. Similarly, in the more challenging ALGOPUZZLEVQA, overall performance remains low, but o1 demonstrates significant improvements over GPT-4o, particularly in puzzles like Number Slide and Calendar. Despite these advancements, visual perception remains key limitation across all models. Providing explicit details about visual perception significantly improves performance, highlighting that accurately interpreting visual input is still major challenge. While o1 demonstrates strong inductive reasoning abilities, its dependence on precise perception suggests that further improvements in visual understanding are needed."
        },
        {
            "title": "REFERENCES",
            "content": "Tom Brown, Benjamin Mann, Nick Ryder, et al. Language models are few-shot learners. In H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (eds.), Advances in Neural Information Processing Systems, volume 33, pp. 18771901. Curran Associates, Inc., URL https://proceedings.neurips.cc/paper_files/paper/2020/ 2020. file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf. Yew Ken Chia, Vernon Toh Yan Han, Deepanway Ghosal, Lidong Bing, and Soujanya Poria. Puzzlevqa: Diagnosing multimodal reasoning challenges of language models with abstract visual patterns, 2024. URL https://arxiv.org/abs/2403.13315. Francois Chollet. On the measure of intelligence, 2019. URL https://arxiv.org/abs/ 1911.01547. Nanyi Fei, Zhiwu Lu, Yizhao Gao, Guoxing Yang, Yuqi Huo, Jing Wen, Haoyu Lu, Ruihua Song, Xin Gao, Tao Xiang, Haoran Sun, and Jiling Wen. Towards artificial general intelligence via multimodal foundation model. Nature Communications, 13, 2021. URL https://api. semanticscholar.org/CorpusID:249314857. Deepanway Ghosal, Vernon Toh Yan Han, Chia Yew Ken, and Soujanya Poria. Are language models puzzle prodigies? algorithmic puzzles unveil serious challenges in multimodal reasoning, 2024. URL https://arxiv.org/abs/2403.03864. Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. Large language models are zero-shot reasoners. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https://openreview.net/forum?id=e2TBb5y0yFf. John Raven. The raven progressive matrices: review of national norming studies and ethnic and socioeconomic variation within the united states. Journal of Educational Measurement, 26(1):1 16, 1989. ISSN 00220655, 17453984. URL http://www.jstor.org/stable/1434619. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc Le, and Denny Zhou. Chain of thought prompting elicits reasoning in large language In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Admodels. vances in Neural Information Processing Systems, 2022. URL https://openreview.net/ forum?id=_VjQlMeSB_J."
        },
        {
            "title": "A PUZZLEVQA DETAILS",
            "content": "A.1 PUZZLEVQA COMPONENTS Figure 10: Illustration example of components for abstract puzzles in PUZZLEVQA. To construct each puzzle instance, we first define the layout and pattern of multimodal template, and populate the template with suitable objects that demonstrate the underlying pattern. A.2 PUZZLEVQA STATISTICS We report the dataset statistics of PUZZLEVQA in Table 3."
        },
        {
            "title": "Test\nInstances",
            "content": "Numbers Colors Shapes Size Numbers & Shapes Numbers & Colors Numbers & Size Shapes & Colors Shapes & Size Colors & Size 2 2 2 2 2 2 2 2 2 2 200 200 200 200 200 200 200 200"
        },
        {
            "title": "Total",
            "content": "20 2000 Table 3: Dataset statistics of PUZZLEVQA."
        },
        {
            "title": "B ALGOPUZZLEVQA DETAILS",
            "content": "B.1 ALGOPUZZLEVQA ONTOLOGY Puzzle Board Tiling Calendar Chain Link Checker Move Clock Colour Hue Map Colour Maze Solve Move Box N-Queens Number Slide Rotten Fruits Rubiks Cube Think Dot Tower of Hanoi"
        },
        {
            "title": "Water Jugs\nWheel of Fortune\nWood Slide",
            "content": "Visual Features Colour Position Shape/Size Text Arithmetic Boolean Logic Algorithmic Features Combinatorics Graphs Optimization Search Sets Table 4: Ontological categorization of the puzzles in ALGOPUZZLEVQA. GPT-4O EVALUATION PROMPT GPT-4o Evaluation Prompt Evaluate the candidate answer against the correct answer. If the candidate answer is correct, output [correct]; otherwise, output [incorrect]. Question: {question} Candidate Answer: {candidate_answer} Correct Answer: {correct_answer} Evaluation:"
        },
        {
            "title": "D PUZZLEVQA BOTTLENECK ANALYSIS SETUP",
            "content": "Figure 11: An example of prompts used in the bottleneck analysis: Perception includes the visual details of the puzzle, while induction includes an explanation of the underlying pattern within it."
        }
    ],
    "affiliations": [
        "Singapore University of Technology and Design (SUTD)"
    ]
}