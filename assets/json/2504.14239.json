{
    "paper_title": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners",
    "authors": [
        "Yuhang Liu",
        "Pengxiang Li",
        "Congkai Xie",
        "Xavier Hu",
        "Xiaotian Han",
        "Shengyu Zhang",
        "Hongxia Yang",
        "Fei Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires a shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, a reasoning-centric, two-stage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing a basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into a deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-to-error steps. Experimental results show InfiGUI-R1 achieves strong performance in GUI grounding and trajectory tasks. Resources at https://github.com/Reallm-Labs/InfiGUI-R1."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 1 ] . [ 1 9 3 2 4 1 . 4 0 5 2 : r InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners Yuhang Liu Zhejiang University liuyuhang@zju.edu.cn Pengxiang Li Dalian University of Technology lipengxiang@mail.dlut.edu.cn Xavier Hu Zhejiang University xavier.hu.research@gmail.com Xiaotian Han xiaotian.sky.han@gmail.com Congkai Xie Reallm Labs xieck13@gmail.com Shengyu Zhang Zhejiang University sy_zhang@zju.edu.cn Hongxia Yang The Hong Kong Polytechnic University hongxia.yang@polyu.edu.hk Fei Wu Zhejiang University wufei@zju.edu.cn Abstract Multimodal Large Language Models (MLLMs) have powered Graphical User Interface (GUI) Agents, showing promise in automating tasks on computing devices. Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches rely on manually designed reasoning templates, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, some existing agents continue to operate as Reactive Actors, relying primarily on implicit reasoning that may lack sufficient depth for GUI tasks demanding planning and error recovery. We argue that advancing these agents requires shift from reactive acting towards acting based on deliberate reasoning. To facilitate this transformation, we introduce InfiGUI-R1, an MLLM-based GUI agent developed through our Actor2Reasoner framework, reasoning-centric, twostage training approach designed to progressively evolve agents from Reactive Actors to Deliberative Reasoners. The first stage, Reasoning Injection, focuses on establishing basic reasoner. We employ Spatial Reasoning Distillation to transfer cross-modal spatial reasoning capabilities from teacher models to MLLMs through trajectories with explicit reasoning steps, enabling models to integrate GUI visual-spatial information with logical reasoning before action generation. The second stage, Deliberation Enhancement, refines the basic reasoner into deliberative one using Reinforcement Learning. This stage introduces two approaches: Sub-goal Guidance, which rewards models for generating accurate intermediate sub-goals, and Error Recovery Scenario Construction, which creates failure-and-recovery training scenarios from identified prone-toerror steps. These approaches enhance the agents planning abilities and self-correction capabilities. Experimental results confirm that InfiGUI-R1 achieves strong performance in both cross-platform GUI grounding and trajectory tasks, proving competitive against previous agents, even those with significantly larger parameters. Resources are available at https://github.com/Reallm-Labs/InfiGUI-R1. Both authors contributed equally to this research. Corresponding author. Keywords GUI Agents, MLLMs, Reinforcement Learning Figure 1: Performance comparison of various GUI agents on the ScreenSpot-Pro benchmark. Our model, InfiGUI-R1-3B marked with star, demonstrates competitive performance against models with larger parameter counts."
        },
        {
            "title": "1 Introduction\nGraphical User Interface (GUI) agents, increasingly powered by\nadvances in Multimodal Large Language Models (MLLMs) [5, 22,\n28, 39, 48] hold significant promise for automating a wide range of\ntasks on computing devices such as mobile phones and computers\n[9, 42]. These agents interact with digital environments through\nvisual interfaces, aiming to enhance user productivity and broaden\nthe scope of automated task completion.",
            "content": "Recent works have begun exploring reasoning in GUI tasks with encouraging results. However, many current approaches either rely on manually designed reasoning templates or lack GUI-specific optimization, which may result in reasoning that is not sufficiently robust and adaptive for complex GUI environments. Meanwhile, many existing MLLM-based GUI agents continue to operate as Preprint, Under review, April 2025 Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu Reactive Actors [10, 29], relying primarily on implicit reasoning. This implicit reasoning often lacks the sufficient depth required for complex, multi-step GUI tasks demanding sophisticated planning and adaptive error recovery. Such tasks necessitate not only precise spatial understanding of dense visual layouts but also the ability to effectively integrate cross-modal information (visual-spatial understanding into textual reasoning) and engage in the deliberative processes crucial for robust, long-horizon task execution. We argue that fundamentally advancing GUI agent capabilities requires paradigm shift: moving beyond reactive execution towards agents that function as Deliberative Reasoners. These agents should explicitly incorporate reasoning processes between perception and action (Perception Reasoning Action), enabling them to plan ahead, decompose complex goals, understand spatial relationships deeply, and reflect upon past actions to correct mistakes. This transition is crucial for handling the complexities and dynamic nature of real-world GUI environments. To enable this transformation, we introduce the Actor2Reasoner framework, reasoning-centric methodology designed to progressively evolve GUI agents from Reactive Actors to Deliberative Reasoners. Our framework culminates in InfiGUI-R1-3B, an MLLMbased agent demonstrating enhanced reasoning and robustness. The Actor2Reasoner framework tackles two core challenges: 1) reliably injecting foundational reasoning capabilities, particularly bridging the critical cross-modal gap between visual-spatial perception and textual reasoning, to achieve the initial leap from Actor to Reasoner; and 2) refining and elevating the reasoning quality of this foundational Reasoner to instill advanced planning and reflection capabilities, ultimately reaching the Deliberative stage. The Actor2Reasoner framework unfolds in two distinct stages: Stage 1: Reasoning Injection (Laying the Foundation for the Reasoner): This stage focuses on the pivotal transition from Actor to Reasoner. We employ Spatial Reasoning Distillation, leveraging trajectories from powerful reasoning teacher model that include explicit spatial reasoning steps. By training the base MLLM on this distilled data via Supervised Fine-Tuning (SFT), we guide it to break the direct Perception Action link and explicitly incorporate reasoning, especially spatial reasoning crucial for GUI tasks. This establishes the foundational (Perception Reasoning Action) pattern, overcoming key limitation of standard MLLMs in integrating visual-spatial understanding into their reasoning flow. Stage 2: Deliberation Enhancement (Refining into Deliberative Reasoner): Building upon the Reasoner established in Stage 1, this stage uses Reinforcement Learning (RL) to refine its capabilities towards deliberation. This refinement strategically enhances the two core facets of deliberative reasoning: planning and reflection. Two key innovations drive this process: Sub-goal Guidance: To enhance the agents forward-looking planning and task decomposition abilities, we guide it to generate explicit intermediate sub-goals during its reasoning process. The alignment of these generated sub-goals with ground truth provides an intermediate reward signal, effectively training the agents capacity for proactive planning (\"Total Goal Sub-goal Action\"). Error Recovery Scenario Construction: Complementing the planning focus, this innovation cultivates the agents ability to look backward and adjust through reflective self-correction hallmark of deliberation. We actively construct scenarios within the RL environment that simulate error states or recovery moments (e.g., having just executed an incorrect action or needing to get \"back on track\" after an error). Training within these scenarios, using targeted rewards, compels the agent to learn adaptive strategies like escaping error states (e.g., using back action) and adjusting plans after recognizing mistake. This directly shapes the agents ability to reflect on its actions and recover from failures, enhancing its robustness. Together, our framework provides pathway to imbue GUI agents with the reasoning, planning, and reflection capabilities necessary for task automation. We validate the effectiveness of InfiGUI-R1-3B, trained using our Actor2Reasoner framework, on suite of challenging benchmarks designed to assess core GUI agent competencies. These include tasks requiring precise GUI element grounding across platforms (e.g., ScreenSpot, ScreenSpot-Pro [21, 24]) and those demanding complex, long-horizon task execution with planning and adaptation (e.g., AndroidControl[26]). Our experimental results demonstrate significant improvements. InfiGUI-R1-3B achieves state-of-the-art cross-platform grounding capabilities (87.5% avg on ScreenSpot) and strong performance in executing complex, long-horizon tasks (71.1% success rate on AndroidControl-High) among models with comparable parameter counts. These findings confirm our frameworks ability to cultivate sophisticated planning and reflection abilities, substantially advancing the agents capacity for deliberate, robust, and effective GUI task automation. Our main contributions are threefold: We propose the Actor2Reasoner framework, novel twostage training methodology designed to systematically transform MLLM-based GUI agents from Reactive Actors into Deliberative Reasoners by progressively injecting and refining reasoning capabilities. We introduce three key technical innovations within this framework: Spatial Reasoning Distillation to establish foundational cross-modal reasoning, Sub-goal Guidance to enhance planning reasoning, and Error Recovery Scenario Construction to cultivate reflective error correction abilities through targeted RL. We develop InfiGUI-R1-3B, an MLLM-based GUI agent trained via our framework, and demonstrate its effectiveness through comprehensive experiments."
        },
        {
            "title": "2 Related Works\n2.1 Multimodal LLMs\nLarge Language Models (LLMs) [6, 13, 47, 53] have significantly\nenhanced the capabilities of AI systems in tackling a wide range of\ntasks [17, 25], thanks to their exceptional ability to process complex\nsemantic and contextual information. The remarkable power of\nLLMs has also inspired exploration into their potential for process-\ning multimodal data, such as images. Typically, the architecture of\nMultimodal Large Language Models (MLLMs) consists of three main\ncomponents: a pre-trained large language model, a trained modality\nencoder, and a modality interface that connects the LLM with the\nencoded modality features. Various vision encoders, such as ViT",
            "content": "InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners Preprint, Under review, April 2025 [12], CLIP [41], and ConvNeXt [34], extract visual features, which are integrated using techniques like adapter networks [31], crossattention layers [3], and visual expert modules [49]. These methods have facilitated the development of high-performing MLLMs, such as Qwen-VL [7], GPT-4 Vision [36], BLIP-2 [23] and InfiMM [32], thus opening new avenues for LLMs in processing GUI tasks."
        },
        {
            "title": "2.2 MLLM-based GUI Agents\nAgents are AI systems that perceive their environments, make de-\ncisions, and take actions to complete specific tasks. The emergence\nof LLMs with human-level reasoning ability has significantly ad-\nvanced the development of agents. For GUI tasks, earlier systems\nrelied on LLMs to read and interpret structured representations\nsuch as HTML code [50]. However, recent works have demonstrated\nthat directly interacting with the visual form of GUIs leads to better\nperformance [16]. Consequently, MLLM-based GUI agents have\nbeen proposed, leveraging visual perception alongside language\nunderstanding.",
            "content": "Several representative systems have pioneered this area. ILuvUI [20] fine-tuned LLaVA to enhance general GUI comprehension, while AppAgent [58] explored mobile app usage through autonomous interactions. CogAgent [15] introduced high-resolution encoders to better capture UI detail, and Ferret-UI-anyres [57] supported flexible screen resolutions to handle diverse device settings. More recent works have introduced modular and lightweight architectures aimed at improving generalization and deployment efficiency. InfiGUIAgent [33] proposed two-stage approach, combining general pretraining on grounding and QA tasks with synthetic fine-tuning for hierarchical planning and reasoning. UI-TARS [40] extended this by using unified vision-language interface across mobile, web, and desktop environments, incorporating reflection and milestone tracking mechanisms to boost task success rates. In parallel, AgentS2 [1] adopted generalist-specialist framework, decoupling high-level reasoning from domain-specific grounding modules and enabling long-horizon planning with Mixture of Grounding mechanisms. In terms of input, recent agents prioritize screenshot-level visual understanding, optionally enhanced with layout or OCR-based textual cues. Techniques such as set-of-mark prompting [54] and chain-of-action reasoning [38] have been employed to improve grounding accuracy and task planning. To further improve interaction efficiency, agents such as UI-R1 [35], GUI-R1 [52] replace large-scale supervision with rule-based reinforcement learning, achieving competitive performance with minimal expert data. Moreover, to support real-world usability, newer agents are tested on increasingly complex environments. UI-TARS and AgentS2 report strong performance on OSWorld and AndroidWorld benchmarks, showing robust cross-platform generalization. GUI-Xplore [44] further introduces one-shot adaptation setting, encouraging agents to build structural UI maps via autonomous exploration before task execution."
        },
        {
            "title": "3 Actor2Reasoner\nWe introduce the Actor2Reasoner framework, a reasoning-centric,\nprogressive training methodology designed to systematically en-\nhance the capabilities of Multimodal Large Language Model (MLLM)",
            "content": "Figure 2: Overview of the Actor2Reasoner framework, twostage methodology for progressively transforming Reactive Actor into Deliberative Reasoner. Stage 1: Reasoning Injection uses Supervised Fine-Tuning (SFT) with Spatial Reasoning Distillationidentifying reasoning bottlenecks (Pinpoint) and leveraging teacher model (Distill)to instill foundational cross-modal reasoning and transition the agent into Basic Reasoner (Perception Reasoning Action). Stage 2: Deliberation Enhancement applies RL to refine planning and reflection capabilities, using Sub-goal Guidance (Reward) for forward-looking task decomposition and Error Recovery Scenario Construction (Reflect) for backward-looking self-correction, culminating in Deliberative Reasoner. based GUI agents. The core objective is to transition agents from reactive behavior towards deliberative reasoning for GUI task automation. This framework comprises two stages designed to first establish foundational reasoning and then refine it towards advanced deliberation. Section 3.1 details the methodology for Stage 1, focusing on reasoning injection via Spatial Reasoning Distillation. Subsequently, Section 3.2 details the methodology for Stage 2, where RL is employed to enhance deliberation through Sub-goal Guidance and Error Recovery Scenario Construction."
        },
        {
            "title": "3.1 Stage 1: Reasoning Injection\nThe primary objective of Stage 1 is to accomplish the fundamen-\ntal transition from a Reactive Actor (Perception â†’ Action) to a\nfoundational Reasoner (Perception â†’ Reasoning â†’ Action). This\ntransition is critical because standard MLLMs often struggle to\neffectively integrate the rich visual-spatial information present in\nGUI screenshots into their textual reasoning processes. This lim-\nitation hinders their ability to handle the GUI tasks that demand\nprecise spatial understanding and grounding. To address this, Stage\n1 employs Spatial Reasoning Distillation, which is designed to\nexplicitly inject spatial reasoning capabilities into the agent.",
            "content": "Spatial Reasoning Distillation leverages the reasoning capabilities of powerful teacher model to generate high-quality reasoning trajectories, which are then used to train the target MLLM (the student). The core idea is to guide the student model to learn not just the correct action, but also the intermediate reasoning stepsparticularly those involving spatial logicthat lead to that action. This process is implemented through the following steps:"
        },
        {
            "title": "3.1.1 Pinpointing Reasoning Bottleneck Samples. To maximize the\nefficiency of distillation, we first identify interaction steps where\nthe base MLLMâ€™s failure is most likely attributable to a lack of\nreasoning, rather than fundamental perception or action execution\ndeficits. We term these Reasoning Bottleneck Samples. This",
            "content": "Preprint, Under review, April 2025 Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu identification employs two-step criterion for each step ğ‘  in given trajectory: (i) The base MLLM ğ‘€, when given the current GUI screenshot ğ¼ğ‘  and the overall task goal ğº, fails to predict the correct action. Let ğ‘high = ğ‘€ (ğ¼ğ‘ , ğº). (ii) However, when provided with the additional ground-truth sub-goal ğ‘”ğ‘  for that specific step, the same model ğ‘€ successfully predicts the correct action. Let ğ‘low = ğ‘€ (ğ¼ğ‘ , ğº, ğ‘”ğ‘  ). Formally, the set of reasoning bottleneck steps ğ‘†bottleneck is defined as: ğ‘†bottleneck = {ğ‘  IsCorrect(ğ‘high) = False IsCorrect(ğ‘low) = True} These samples represent steps where the primary difficulty lies in inferring the immediate task (ğ‘”ğ‘  ) from the overall goal (ğº) based on the visual context (ğ¼ğ‘  ), making them ideal candidates for reasoning injection. We use base MLLM such as Qwen2.5-VL-3B-Instruct for this filtering process."
        },
        {
            "title": "3.1.2 Generating Spatial Reasoning Trajectories. For each step ğ‘  âˆˆ\nğ‘†bottleneck, we generate a detailed reasoning trajectory using a high-\ncapability teacher model. This involves:",
            "content": "Spatial Information Extraction and Compression. We extract relevant structural and spatial information (e.g., element types, text content, coordinates, hierarchy) from the accessibility tree (a11y tree) associated with the GUI screenshot ğ¼ğ‘  . Irrelevant attributes and elements are filtered out. powerful MLLM (e.g., Qwen2.5-VL-32BInstruct) is then employed to compress this processed information into concise textual description ğ·spatial, which consists of detailed description of the GUI page, including all relevant elements coordinate information and descriptions for the specific step, capturing the essential spatial layout and key element details. Reasoning Trajectory Generation. The compressed spatial description ğ·spatial, the available action space description, and the overall goal ğº are fed as input to powerful large language model with strong reasoning capabilities (e.g., QwQ-32B [46]). This teacher model is prompted to generate both an explicit reasoning text ğ‘…teacher and the corresponding action ğ‘teacher. Crucially, ğ‘…teacher is guided to articulate the logical steps, including using the spatial information in ğ·spatial for element localization, relationship assessment, and action justification. Injecting Reasoning via SFT. The generated pairs (ğ‘…teacher, 3.1.3 ğ‘teacher) are first filtered to ensure quality via rejection sampling based on the correctness of the predicted action ğ‘teacher. The highquality pairs are then used to fine-tune the base MLLM. The SFT objective trains the student model to predict the teachers reasoning and action when given the GUI screenshot and the overall goal: (ğ¼ğ‘ , ğº) (ğ‘…teacher, ğ‘teacher). By learning to explicitly generate or implicitly simulate these reasoning steps before outputting the action, the student model internalizes the Perception Reasoning Action pattern. Upon completion of Stage 1, the resulting model is foundational Reasoner equipped with enhanced spatial understanding and the basic ability to connect perception to action through an intermediate reasoning process."
        },
        {
            "title": "3.2.1 Reinforcement Learning Setup. We utilize RL to further opti-\nmize the agentâ€™s policy beyond supervised learning. Specifically, we\nadopt the REINFORCE Leave-One-Out (RLOO) algorithm [2],\nwhich effectively reduces the variance of policy gradient estimates\nby employing the average reward of other samples within the same\nbatch as a baseline for the current sample. This \"leave-one-out\"\nbaseline strategy obviates the need for training a separate value\nor critic model, thereby simplifying the training architecture. The\nRLOO policy gradient âˆ‡ğœƒ ğ½ (ğœƒ ) is estimated as:",
            "content": "ğœƒ ğ½ (ğœƒ ) 1 ğ‘˜ ğ‘˜ ğ‘–=1 ğ‘…(ğ‘¦ (ğ‘– ), ğ‘¥) 1 ğ‘˜ ğ‘—ğ‘– ğ‘…(ğ‘¦ ( ğ‘— ), ğ‘¥) ğœƒ log ğœ‹ğœƒ (ğ‘¦ (ğ‘– ) ğ‘¥) where ğ‘˜ is the number of output sequences ğ‘¦ (ğ‘– ) sampled from the current policy ğœ‹ğœƒ given input ğ‘¥, and ğ‘…(ğ‘¦, ğ‘¥) is the reward function evaluating the quality of output ğ‘¦. The design of the reward function ğ‘…(ğ‘¦, ğ‘¥) is crucial for guiding the agents learning trajectory. Our total reward ğ‘…total integrates assessments of both output format correctness and task execution accuracy: ğ‘…total = ğ‘¤ ğ‘“ ğ‘…format + ğ‘¤ğ‘ ğ‘…acc Here, ğ‘…format checks if the models output ğ‘¦ conforms to the expected format (e.g., putting the reasoning process within <think> </think> tags), yielding 1 if valid and 0 otherwise. ğ‘…acc measures the accuracy of the content, and is calculated only if ğ‘…format = 1, ensuring the agent first learns to generate structurally valid outputs. ğ‘¤ ğ‘“ and ğ‘¤ğ‘ are weighting hyperparameters (ğ‘¤ ğ‘“ + ğ‘¤ğ‘ = 1). The accuracy reward ğ‘…acc is tailored to the specific task type: Agent Trajectory Task Reward (ğ‘…agent): For evaluating sequences of GUI actions, we provide fine-grained feedback by combining rewards for the action type and its parameters: ğ‘…agent = ğ‘¤ğ‘¡ ğ‘…type + ğ‘¤ğ‘ ğ‘…param where ğ‘¤ğ‘¡ + ğ‘¤ğ‘ = 1. ğ‘…type grants reward of 1 if the predicted action type (e.g., click, type) matches the ground truth, and 0 otherwise. ğ‘…param provides stricter reward, granting 1 only if both the action type and all its parameters match the ground truth, and 0 otherwise. (Note: This reward is further refined by Sub-goal Guidance in Section 3.2.2). InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners Preprint, Under review, April 2025 Grounding Task Rewards: For evaluating GUI element localization: Point Localization Reward (ğ‘…point): Given predicted point coordinate (ğ‘¥ğ‘, ğ‘¦ğ‘ ) and the ground-truth bounding box ğµgt of the target element, the reward is 1 if the point falls within the box, and 0 otherwise: ğ‘…point = I((ğ‘¥ğ‘, ğ‘¦ğ‘ ) ğµgt) Bounding Box Reward (ğ‘…bbox): We compute the Intersection over Union (IoU) between the predicted bounding box ğµpred and the ground-truth box ğµgt. To avoid penalizing minor deviations excessively while encouraging significant overlap, we use threshold ğœIoU. The reward is 1 if the IoU meets or exceeds the threshold, otherwise it is the IoU scaled by the threshold: ğ‘…bbox = (cid:40)"
        },
        {
            "title": "1\nIoU(ğµpred,ğµgt )\nğœIoU",
            "content": "if IoU(ğµpred, ğµgt) ğœIoU if IoU(ğµpred, ğµgt) < ğœIoU Other Task Rewards (ğ‘…other): For auxiliary tasks potentially included in the training mix (e.g., VQA, multiple-choice questions), we use Exact Match (EM) or mathematical expression verification against the ground truth ğ‘¦gt to determine correctness: ğ‘…other = I(ExactMatch(ğ‘¦ans, ğ‘¦gt) MathVerify(ğ‘¦ans, ğ‘¦gt)) To ensure the agent enhances its GUI-specific deliberation skills without compromising its general multimodal understanding and visual grounding foundations, the RL training phase utilizes diverse mixture of data. This includes the core GUI trajectory data (e.g., from AndroidControl [26]), GUI element grounding data (e.g., from widget captioning datasets [27]), general-purpose multimodal question-answering datasets, and object detection datasets (e.g., from COCO [30]). Following established practices for eliciting reasoning [43, 56], we employ system prompt that explicitly instructs the model to first articulate its reasoning process internally before providing the final action. The specific prompt used is:"
        },
        {
            "title": "System Prompt for Reasoning",
            "content": "You FIRST think about the reasoning process as an internal monologue and then provide the final answer. The reasoning process MUST BE enclosed within <think> </think> tags. Sub-goal Guidance. To elevate the foundational Reasoner to3.2.2 wards Deliberative Reasoner capable of sophisticated planning, core aspect of Stage 2 focuses on enhancing its task decomposition ability. Standard MLLMs often falter when required to independently infer the necessary intermediate steps from high-level objective within complex GUI environment. Sub-goal Guidance is specifically designed to address this limitation within the RL framework by incentivizing the agent to formulate and pursue accurate sub-goals, thereby fostering more structured and effective planning. This is achieved by assessing the quality of the sub-goal implied within the agents reasoning process. Sub-goal Quality Assessment. We incentivize accurate sub-goal formulation by integrating its assessment into the agents reward structure during RL training. We assess the quality of the implicitly generated sub-goal within the reasoning text. ğ‘” . This extracted sub-goal ğ‘ extracted During training, for each step, we employ lightweight scoring LLM to analyze the agents reasoning output (the text within <think>...</think> tags) and attempt to extract the implied subgoal, denoted as ğ‘ extracted is then compared against the corresponding ground-truth sub-goal ğ‘ gt ğ‘” (obtained from dataset annotations1). Based on the degree of semantic ğ‘” , raw score ğ‘†raw is assigned on match between ğ‘ extracted scale of 1 to 10. If the scoring LLM fails to extract sub-goal from the reasoning text, ğ‘†raw is set to 0. This raw score is then normalized to the range [0, 1] to produce the final sub-goal reward: and ğ‘ gt ğ‘” ğ‘” ğ‘…subgoal = ğ‘†raw This normalized score, ğ‘…subgoal, serves as an intermediate reward signal reflecting the quality of the agents planning for the current step. To specifically encourage correct planning even when the final action execution fails, we integrate ğ‘…subgoal into the Agent Trajectory Task Reward ğ‘…agent (introduced in Section 3.2.1). The formulation is modified as follows, incorporating dedicated weight ğ‘¤ğ‘  : ğ‘…agent = (cid:40)ğ‘¤ğ‘¡ ğ‘…type + ğ‘¤ğ‘ ğ‘…param ğ‘¤ğ‘¡ ğ‘…type + ğ‘¤ğ‘  ğ‘…subgoal if ğ‘…param = 1 if ğ‘…param = 0 where ğ‘¤ğ‘¡ , ğ‘¤ğ‘, ğ‘¤ğ‘  are non-negative weights, and typically ğ‘¤ğ‘  is set lower than ğ‘¤ğ‘ to prioritize full action correctness when achievable. This conditional reward structure provides targeted feedback on the planning quality when the agent struggles with accurate action execution, thereby guiding the learning process towards better intermediate reasoning and task decomposition."
        },
        {
            "title": "3.2.3 Error Recovery Scenario Construction. While Sub-goal Guid-\nance enhances forward-looking planning, developing a Deliber-\native Reasoner also necessitates the ability to reflect upon and\nrecover from errorsâ€”a capability often missing in standard GUI\nagents prone to irrecoverable failures. To cultivate robustness and\nadaptability, we utilize Error Recovery Scenario Construction,\na technique that directly targets the agentâ€™s reflective and cor-\nrective reasoning abilities by integrating specific failure-recovery\nsituations into the RL training process. This mechanism comple-\nments planning by strengthening the agentâ€™s capacity for backward-\nlooking adjustment.",
            "content": "Identify Prone-to-error Steps: To maximize training efficiency, we first identify interaction steps where the agent demonstrates instability. For given step ğ‘ , we employ the base model (e.g., Qwen2.5VL-3B-Instruct) to sample ğ‘ğ‘ ğ‘ğ‘šğ‘ğ‘™ğ‘’ action sequences at heightened temperature ğ‘‡ . Steps whose success rate ğ‘ƒsuccess (ğ‘ ) falls between 0 and 1 (0 < ğ‘ƒsuccess (ğ‘ ) < 1) are designated as Prone-to-error Steps, forming the set ğ‘†error_prone. These steps signify situations where the agent possesses the capacity for correct action but is also susceptible to errors, presenting optimal opportunities for learning corrective strategies. Training on steps the agent always masters or always fails is less efficient for learning recovery; the former 1https://github.com/google-research/google-research/tree/master/android_control Preprint, Under review, April 2025 Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu Model Proprietary Models GPT-4o [37] Claude Computer Use [4] Gemini 2.0 (Project Mariner) [11] General Open-source Models Qwen2-VL-7B [48] Qwen2.5-VL-3B [8] Qwen2.5-VL-7B [8] GUI-specific Models CogAgent [15] SeeClick [10] UGround-7B [14] OS-Atlas-7B [51] ShowUI-2B [29] Aguvis-7B [18] UI-R1-3B [35] GUI-R1-3B [52] GUI-R1-7B [52] UI-TARS-2B [40] Ours InfiGUI-R1-3B Mobile Accuracy (%) Desktop Web Avg. Text Icon Text Icon Text Icon 30.5 - - 61.3 - - 67.0 78.0 82.8 93.0 92.3 95.6 - - - 93.0 23.2 - - 39.3 - - 24.0 52.0 60.3 72.9 75.5 77.7 - - - 75. 20.6 - - 52.0 - - 74.2 72.2 82.5 91.8 76.3 93.8 90.2 93.8 91.8 90.7 19.4 - - 45.0 - - 20.0 30.0 63.6 62.9 61.1 67.1 59.3 64.8 73.6 68. 11.1 - - 33.0 - - 70.4 55.7 80.4 90.9 81.7 88.3 85.2 89.6 91.3 84.3 7.8 - - 21.8 - - 28.6 32.5 70.4 74.3 63.6 75.2 73.3 72.1 75.7 74. 18.8 83.0 84.0 42.9 55.5 84.7 47.4 53.4 73.3 82.5 75.1 84.4 - - - 82.3 97.1 81.2 94. 77.1 91.7 77.6 87.5 Table 1: Performances on various platforms (Mobile, Desktop, Web) on ScreenSpot. All experiments were conducted using raw screenshot information. Results marked in bold represent the best performance, and those underlined indicate the second-best performance. needs no correction, while the latter might indicate deeper issues potentially confounded by naive recovery training. ğ‘  Constructing Recovery Scenarios: For each prone-to-error step ğ‘  ğ‘†error_prone, we construct two distinct types of scenarios for RL training, each designed to teach specific aspect of error handling: Error Escape Scenario. The primary objective here is to train the agent to recognize it has entered an erroneous state and execute an appropriate \"escape\" action (e.g., pressing the back button). To simulate this, we select an incorrect action ğ‘err sampled during the identification phase, which leads to an unintended subsequent observation ğ¼ err ğ‘ +1. The RL agent is then presented with this error observation ğ¼ err ğ‘  = ğ»ğ‘  1 ğ‘err ğ‘ +1 alongside modified history ğ» err (where ğ»ğ‘  1 is the history prior to step ğ‘ , and denotes concatenation). The desired behavior for the agent in this context is to output predefined escape action, ğ‘escape. Back on Track Scenario. This scenario aims to train reflective adjustment, enabling the agent to resume the intended task flow after recovering from an error. We assume the agent has just executed the escape action ğ‘escape from the error state, returning it to the original observation ğ¼ğ‘  encountered at step ğ‘ . The agent is presented with this original observation ğ¼ğ‘  , but its history reflects the recent detour: ğ» recover ğ‘  ğ‘escape. The desired behavior in this \"back on track\" state is for the agent to perform the originally correct action ğ‘ ğ‘  for step ğ‘ , demonstrating its ability to re-evaluate the situation and proceed correctly despite the preceding failure. = ğ»ğ‘  1 ğ‘err ğ‘  ğ‘  The constructed Error Escape and Back on Track scenario samples are incorporated into the data used for RL training in Stage 2. When the agent encounters these scenarios as input ğ‘¥ and generates an output ğ‘¦, its performance is evaluated using the same comprehensive reward function ğ‘…total (ğ‘¦, ğ‘¥). By rewarding successful escape actions in the first scenario type and correct subsequent actions in the second, the RL process specifically reinforces the agents adaptive strategies for handling failures. This targeted training solidifies the agents transition towards Deliberative Reasoner, together with the task decomposition ability."
        },
        {
            "title": "4.1 Setup",
            "content": "Implementation Details. Our model, InfiGUI-R1-3B, is built upon Qwen2.5-VL-3B-Instruct and trained using the proposed Actor2Reasoner Framework, which consists of two main stages. For the RL reward function ğ‘…total = ğ‘¤ ğ‘“ ğ‘…format + ğ‘¤ğ‘ ğ‘…acc, we set the weights ğ‘¤ ğ‘“ = 0.1 and ğ‘¤ğ‘ = 0.9. Within the agent trajectory accuracy reward ğ‘…acc_agent, the weights are ğ‘¤ğ‘¡ = 0.2 for type matching and ğ‘¤ğ‘ = 0.8 for exact parameter matching. For bounding box InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners Preprint, Under review, April 2025 Model Text Icon Avg. Text Icon Avg. Text Icon Avg. Text Icon Avg. Text Icon Avg. Text Icon Avg. Text Icon Avg. CAD Development Creative Scientific Office OS Avg. Proprietary Models 2.0 GPT-4o [19] Claude Computer Use [4] 14.5 General Open-source Models Qwen2-VL-7B [48] Qwen2.5-VL-3B [8] Qwen2.5-VL-7B [8] Kimi-VL [45] 0.5 - - - GUI-specific Models SeeClick [10] CogAgent-18B [15] Aria-UI [55] OS-Atlas-4B [51] OS-Atlas-7B [51] ShowUI-2B [29] UGround-7B [14] UGround-V1-7B [14] UI-R1-3B [35] GUI-R1-3B [52] GUI-R1-7B [52] UI-TARS-2B [40] UI-TARS-7B [40] Ours InfiGUI-R1-3B 0.0 3.7 0.0 - - - 0.0 3.1 1.6 0.0 4.7 0.0 1.6 - 6.3 7.8 6.3 4.7 9.4 1.3 1.5 11.9 22.0 0.4 - - - 2.6 - - - 0.0 3.9 0.0 - - - 0.6 1.9 14.9 6.1 16.2 6.1 1.5 7.1 10.3 33.1 1.9 16.9 11.1 26.6 13.5 - - - 0.0 0.7 0.0 0.0 1.4 1.4 2.1 - - 4.1 22.7 4.8 33.8 4.8 49.4 4.1 14.6 47.4 18.0 58.4 12.4 1.3 - - - 0.3 8.0 8.4 3.7 17.7 9.4 14.7 35.5 - - - 26.4 36. 2.5 7.1 7.6 2.0 12.2 2.5 14.2 - 11.2 26.4 23.9 17.8 20.8 0.7 12.6 1.0 25.9 0.0 3.4 0.0 - - - 2. 0.0 0.6 16.8 33.9 15.8 25.8 30.1 16.3 26.9 11.0 0.0 1.2 1.1 0.0 0. 0.9 - - - 6.3 - - - 0.0 - - - 3.5 - - - 3.4 - - - 1.9 - - - 3.0 - - - 0.9 - - - 0.0 4.5 0.0 - - - 0.0 8.1 0.5 - - - 1.3 23.4 2.5 - - - 0.0 7.1 0.2 - - - 0.8 17.1 1.6 23.9 29.0 34. 1.5 - - - 0.0 1.8 6.4 5.5 7.3 7.3 2.7 - 0.0 0.0 2.1 1.4 2.8 0.0 2.8 - 3.5 5.6 8.4 6.3 3.5 0.6 22.2 5.6 14.7 27.1 2.3 9.0 17.9 37.5 5.3 13.2 17.0 31.9 27.8 - - - 1.0 1.1 7.7 9.6 11.3 23.7 3.7 3.0 28.8 18.9 7.7 9.1 16.5 27.3 31.1 - 17.8 27.3 - 40.9 - 38.9 27.7 42.9 50.0 9.1 32.8 63.9 31.8 50.0 63.3 20.8 53.5 30.8 16.9 24.5 47.8 16.2 35.7 2.8 0.9 0.0 2.0 1.1 5.6 10.0 0.0 13.4 13.0 4.7 16.1 1.9 18.1 20.3 4.8 5.6 3.8 7.5 5.1 27.4 27.1 5.7 24.4 33.9 10.6 15.3 13.5 10.3 7.5 19.3 31.6 11.3 27.0 17.8 38.8 - - - - 0.0 0.0 0.0 0.0 4.5 2.2 0.0 - - 4.5 13.1 28.1 5.6 42.1 16.9 5.6 1.8 1.5 12.0 3.1 17.1 2.6 3.1 5.0 16.8 28.1 10.8 6.6 25.0 9.7 - 26.1 - - - - - - 27.6 56.9 17.3 39.8 50.3 17.0 42.6 21.5 0.0 0.8 2.0 1.7 4.0 2.6 2.8 - - - - 8.4 32.2 11.3 53.6 17.0 58.7 26.4 42.4 11.8 61.8 17.3 55.6 11. 48.8 - - - 14.3 39.6 - - 33.0 14.1 28.4 51.3 12.4 32. 44.9 7.0 29.0 58.3 20.0 41.7 65.5 28.3 57.0 43.9 12.4 29.6 49.1 14.1 35.7 Table 2: Performance comparison of different agent models across various task categories based on Text, Icon, and Average scores on ScreenSpot-Pro. Results marked in bold represent the best performance, and those underlined indicate the second-best performance. rewards (ğ‘…bbox), the IoU threshold is ğœIoU = 0.7. When using subgoal similarity as reward (ğ‘…subgoal) in cases where the action parameters are incorrect (ğ‘…param = 0), we use weight ğ‘¤ğ‘  = 0.2. diverse platforms (Mobile, Desktop, Web). ScreenSpot-Pro specifically increases the difficulty with complex desktop applications and high-resolution screens. Training Data. To ensure both strong GUI capabilities and general multimodal understanding, we train InfiGUI-R1-3B on diverse dataset mixture: AndroidControl (10k trajectories + 2k reflection-focused trajectories), GUI Grounding data (5k samples aggregated from RicoSCA, Widget Caption, etc.), MathV360K (11k samples for general reasoning), and COCO (4k samples for general visual grounding and understanding). Training Parameters. All experiments were conducted using 16 NVIDIA H800 GPUs. For the SFT stage (Stage 1), we used learning rate of 2.0e-6, global batch size of 32, and warmup ratio of 0.1. For the RL stage (Stage 2), we used learning rate of 1.0e-6, batch size of 256 for training updates, rollout batch size of 256, and generated 16 rollouts per sample during policy exploration."
        },
        {
            "title": "4.2 Evaluation Benchmarks\nTo comprehensively evaluate InfiGUI-R1-3B, we utilize several\nkey benchmarks targeting different facets of GUI agent capabilities:",
            "content": "ScreenSpot & ScreenSpot-Pro: These benchmarks assess fundamental GUI understanding and element grounding accuracy across AndroidControl: This benchmark evaluate the agents ability to execute complex, multi-step tasks within realistic Android environments. They directly test the higher-level reasoning capabilities crucial for Deliberative Reasoner, including planning, and state tracking over long interaction trajectories. We report results on the Low-level (Low) and High-level (High) splits of AndroidControl."
        },
        {
            "title": "4.3 Results\nWe compare InfiGUI-R1-3B against a range of state-of-the-art\nopen-source and proprietary GUI agents. The results demonstrate\nthe effectiveness of our Actor2Reasoner framework in advancing\nGUI agents towards deliberative reasoning.",
            "content": "Performance on ScreenSpot. Table 1 summarizes the results on the ScreenSpot benchmark, evaluating grounding across Mobile, Desktop, and Web platforms. InfiGUI-R1-3B achieves state-of-theart performance among all compared models, including proprietary ones like Gemini 1.5 Pro and Claude, with an impressive average accuracy of 87.5%. It consistently ranks first across all platforms and both text-based and icon-based grounding tasks (Mobile: 97.1/81.2, Desktop: 94.3/77.1, Web: 91.7/77.6). This outstanding performance Preprint, Under review, April 2025 Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu Model Claude* GPT-4o Aria-UI OS-Atlas-4B Aguvis-7B Aguvis-72B UI-R1 GUI-R1-3B GUI-R1-7B UI-TARS-2B Ours InfiGUI-R1-3B AndroidControl-Low AndroidControl-High Type Grounding SR Type Grounding SR 74.3 74.3 91.9 94.3 - - 98.1 0.0 0.0 87.7 83.8 82.6 - - 87.3 19.4 19.4 67.3 80.6 80.5 84.4 - - - 89.3 63.7 66.3 84.7 - 58.0 71.6 81.2 0.0 0.0 43.2 73.8 - 56.2 65.6 78.4 12.5 20.8 10.2 67.5 61.5 66.4 - 46.6 51.7 68. 96.0 93.2 92.1 82.7 74.4 71. Table 3: Performance comparison of different agent models on AndroidControl benchmarks. SR stands for Success Rate. Results marked in bold represent the best performance, and those underlined indicate the second-best performance. underscores the robustness and generalization ability of InfiGUIR1-3Bs visual understanding and grounding capabilities. Performance on ScreenSpot-Pro. As shown in Table 2, InfiGUIR1-3B achieves competitive performance on the demanding ScreenSpot-Pro benchmark, which focuses on complex, high-resolution desktop GUI grounding. With an overall average score of 35.7, it performs comparably to the larger UI-TARS-7B model (35.7) and significantly outperforms other baselines like OS-Atlas-7B (18.9) and UGround-7B (16.5). Our model shows particular strength in categories like CAD (28.4 avg), Office (57.0 avg) and OS (29.6 avg), demonstrating robust grounding capabilities even in specialized software environments. While not universally outperforming the top model in every category, the strong overall performance validates the effectiveness of our approach. Performance on AndroidControl. Table 3 presents the results on the AndroidControl benchmark. InfiGUI-R1-3B achieves high Success Rate (SR) of 92.1% on AndroidControl-Low and 71.1% on AndroidControl-High. This surpasses the previous state-of-the-art model with similar parameters, UI-TARS-2B (SR: 89.3% / 68.9%). Furthermore, it also outperforms larger GUI-specific models such as Aguvis-72B (SR: 84.4% / 66.4%). This highlights the effectiveness of the training focused on planning capabilities in our Stage 2. In summary, the experimental results across AndroidControl, ScreenSpot-Pro, and ScreenSpot demonstrate that InfiGUI-R13B significantly advances the capabilities of GUI agents. Our Actor2Reasoner framework, combining Spatial Reasoning Distillation and RL-based Deliberation Enhancement (Sub-goal Guidance, Error Recovery), successfully transforms base MLLM into more effective Deliberative Reasoner, achieving state-of-the-art among models with similar parameter counts in trajectory-based tasks and element grounding across different platforms and resolutions, even with relatively small 3B parameter model. Figure 3: Reward curves during reinforcement learning training. The plot shows the overall reward and the rewards for individual task types (Low-level, High-level, Grounding) over training steps."
        },
        {
            "title": "5 Conclusion\nWe present InfiGUI-R1-3B, a multimodal GUI agent that bridges\nthe gap between reactive execution and deliberative reasoning.\nThrough the Actor2Reasoner framework, our approach systemati-\ncally injects and refines reasoning capabilities in two stages: Spatial\nReasoning Distillation to build foundational cross-modal reason-\ning, and Deliberation Enhancement via reinforcement learning to\nsupport sub-goal planning and error recovery. Empirical results\nacross diverse benchmarks demonstrate that InfiGUI-R1-3B not\nonly matches or surpasses larger models in grounding accuracy\nbut also excels in long-horizon task execution with robust planning\nand reflection.",
            "content": "References [1] Saaket Agashe, Kyle Wong, Vincent Tu, Jiachen Yang, Ang Li, and Xin Eric Wang. 2025. Agent S2: Compositional Generalist-Specialist Framework for Computer Use Agents. arXiv:2504.00906 [cs.AI] https://arxiv.org/abs/2504.00906 [2] Arash Ahmadian, Chris Cremer, Matthias GallÃ©, Marzieh Fadaee, Julia Kreutzer, Olivier Pietquin, Ahmet ÃœstÃ¼n, and Sara Hooker. 2024. Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint arXiv:2402.14740 (2024). [3] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur Mensch, Katie Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob Menick, Sebastian Borgeaud, Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira, Oriol Vinyals, Andrew Zisserman, and Karen Simonyan. 2022. Flamingo: Visual Language Model for Few-Shot Learning. In Advances in Neural InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners Preprint, Under review, April 2025 Information Processing Systems 35: Annual Conference on Neural Information Processing Systems 2022, NeurIPS 2022, New Orleans, LA, USA, November 28 - December 9, 2022, Sanmi Koyejo, S. Mohamed, A. Agarwal, Danielle Belgrave, K. Cho, and A. Oh (Eds.). http://papers.nips.cc/paper_files/paper/2022/hash/ 960a172bc7fbf0177ccccbb411a7d800-Abstract-Conference.html [4] Anthropic. 2024. Developing computer use model. https://www.anthropic.com/ news/developing-computer-use. Accessed: 2025-04-12. [5] Anas Awadalla, Irena Gao, Josh Gardner, Jack Hessel, Yusuf Hanafy, Wanrong Zhu, Kalyani Marathe, Yonatan Bitton, Samir Gadre, Shiori Sagawa, et al. 2023. Openflamingo: An open-source framework for training large autoregressive vision-language models. arXiv preprint arXiv:2308.01390 (2023). [6] Jinze Bai, Shuai Bai, Yunfei Chu, Zeyu Cui, Kai Dang, Xiaodong Deng, Yang Fan, Wenhang Ge, Yu Han, Fei Huang, Binyuan Hui, Luo Ji, Mei Li, Junyang Lin, Runji Lin, Dayiheng Liu, Gao Liu, Chengqiang Lu, K. Lu, Jianxin Ma, Rui Men, Xingzhang Ren, Xuancheng Ren, Chuanqi Tan, Sinan Tan, Jianhong Tu, Peng Wang, Shijie Wang, Wei Wang, Shengguang Wu, Benfeng Xu, Jin Xu, An Yang, Hao Yang, Jian Yang, Jian Yang, Shusheng Yang, Yang Yao, Bowen Yu, Yu Bowen, Hongyi Yuan, Zheng Yuan, Jianwei Zhang, Xing Zhang, Yichang Zhang, Zhenru Zhang, Chang Zhou, Jingren Zhou, Xiaohuan Zhou, and Tianhang Zhu. 2023. Qwen Technical Report. ArXiv (2023). https://doi.org/10.48550/arXiv.2309.16609 [7] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. 2023. Qwen-VL: Frontier Large VisionLanguage Model with Versatile Abilities. ArXiv (2023). https://doi.org/10.48550/ arXiv.2308.12966 [8] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923 (2025). [9] Rogerio Bonatti, Dan Zhao, Francesco Bonacci, Dillon Dupont, Sara Abdali, Yinheng Li, Yadong Lu, Justin Wagle, Kazuhito Koishida, Arthur Bucker, et al. 2024. Windows agent arena: Evaluating multi-modal os agents at scale. arXiv preprint arXiv:2409.08264 (2024). [10] Kanzhi Cheng, Qiushi Sun, Yougang Chu, Fangzhi Xu, Yantao Li, Jianbing Zhang, and Zhiyong Wu. 2024. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935 (2024). [11] Google DeepMind. 2024. Gemini-2.0 (Project Mariner). https://deepmind.google/ technologies/project-mariner. Accessed: 2025-04-12. [12] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. https://openreview.net/forum? id=YicbFdNTTy [13] Luciano Floridi and Massimo Chiriatti. 2020. GPT-3: Its nature, scope, limits, and consequences. Minds and Machines 30 (2020), 681694. [14] Boyu Gou, Ruohan Wang, Boyuan Zheng, Yanan Xie, Cheng Chang, Yiheng Shu, Huan Sun, and Yu Su. 2024. Navigating the digital world as humans do: Universal visual grounding for gui agents. arXiv preprint arXiv:2410.05243 (2024). [15] Wenyi Hong, Weihan Wang, Qingsong Lv, Jiazheng Xu, Wenmeng Yu, Junhui Ji, Yan Wang, Zihan Wang, Yuxiao Dong, Ming Ding, et al. 2024. Cogagent: visual language model for gui agents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 1428114290. [16] Xueyu Hu, Tao Xiong, Biao Yi, Zishu Wei, Ruixuan Xiao, Yurun Chen, Jiasheng Ye, Meiling Tao, Xiangxin Zhou, Ziyu Zhao, Yuhuai Li, Shengze Xu, Shawn Wang, Xinchen Xu, Shuofei Qiao, Kun Kuang, Tieyong Zeng, Liang Wang, Jiwei Li, Yuchen Eleanor Jiang, Wangchunshu Zhou, Guoyin Wang, Keting Yin, Zhou Zhao, Hongxia Yang, Fan Wu, Shengyu Zhang, and Fei Wu. 2024. OS Agents: Survey on MLLM-Based Agents for General Computing Devices Use. Preprints (December 2024). doi:10.20944/preprints202412.2294.v1 [17] Xueyu Hu, Ziyu Zhao, Shuang Wei, Ziwei Chai, Qianli Ma, Guoyin Wang, Xuwu Wang, Jing Su, Jingjing Xu, Ming Zhu, Yao Cheng, Jianbo Yuan, Jiwei Li, Kun Kuang, Yang Yang, Hongxia Yang, and Fei Wu. 2024. InfiAgent-DABench: Evaluating Agents on Data Analysis Tasks. arXiv preprint arXiv:2401.05507 (2024). [18] Xu Huang, Weiwen Liu, Xiaolong Chen, Xingmei Wang, Hao Wang, Defu Lian, Yasheng Wang, Ruiming Tang, and Enhong Chen. 2024. Understanding the planning of LLM agents: survey. arXiv preprint arXiv:2402.02716 (2024). [19] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276 (2024). [20] Yue Jiang, Eldon Schoop, Amanda Swearngin, and Jeffrey Nichols. 2023. ILuvUI: Instruction-tuned LangUage-Vision modeling of UIs from Machine Conversations. arXiv preprint arXiv:2310.04869 (2023). [21] Marko Jurmu, Sebastian Boring, and Jukka Riekki. 2008. ScreenSpot: Multidimensional resource discovery for distributed applications in smart spaces. In Proceedings of the 5th Annual International Conference on Mobile and Ubiquitous Systems: Computing, Networking, and Services. 19. [22] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. 2024. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326 (2024). [23] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. 2023. Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning. PMLR, 1973019742. [24] Kaixin Li, Hongzhan Lin, Ziyang Luo, Yuchen Tian, Jing Ma, Zhiyong Huang, Tat-Seng Chua, et al. 2025. Screenspot-pro: Gui grounding for professional highresolution computer use. In Workshop on Reasoning and Planning for Large Language Models. [25] Linyi Li, Shijie Geng, Zhenwen Li, Yibo He, Hao Yu, Ziyue Hua, Guanghan Ning, Siwei Wang, Tao Xie, and Hongxia Yang. 2024. InfiBench: Evaluating the Question-Answering Capabilities of Code Large Language Models. arXiv preprint arXiv:2404.07940 (2024). [26] Wei Li, William Bishop, Alice Li, Chris Rawles, Folawiyo Campbell-Ajala, Divya Tyamagundlu, and Oriana Riva. 2024. On the effects of data scale on computer control agents. arXiv e-prints (2024), arXiv2406. [27] Yang Li, Luheng Li, Gangaand He, Jingjie Zheng, Hong Li, and Zhiwei Guan. 2020. Widget Captioning: Generating Natural Language Description for Mobile User Interface Elements. arXiv preprint arXiv:2010.04295 (2020). [28] Zijing Liang, Yanjie Xu, Yifan Hong, Penghui Shang, Qi Wang, Qiang Fu, and Ke Liu. 2024. Survey of Multimodel Large Language Models. In Proceedings of the 3rd International Conference on Computer, Artificial Intelligence and Control Engineering. 405409. [29] Kevin Qinghong Lin, Linjie Li, Difei Gao, Zhengyuan Yang, Zechen Bai, Weixian Lei, Lijuan Wang, and Mike Zheng Shou. 2024. Showui: One vision-languageaction model for generalist gui agent. In NeurIPS 2024 Workshop on Open-World Agents. [30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr DollÃ¡r, and Lawrence Zitnick. 2014. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13. Springer, 740 755. Instruction Tuning. [31] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2023. Visual Information Processing In Advances in Neural Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (Eds.). http://papers.nips.cc/paper_files/paper/2023/hash/ 6dcf277ea32ce3288914faf369fe6de0-Abstract-Conference.html [32] Haogeng Liu, Quanzeng You, Yiqi Wang, Xiaotian Han, Bohan Zhai, Yongfei Liu, Wentao Chen, Yiren Jian, Yunzhe Tao, Jianbo Yuan, Ran He, and Hongxia Yang. 2024. InfiMM: Advancing Multimodal Understanding with an Open-Sourced Visual Language Model. In Annual Meeting of the Association for Computational Linguistics. [33] Yuhang Liu, Pengxiang Li, Zishu Wei, Congkai Xie, Xueyu Hu, Xinchen Xu, Shengyu Zhang, Xiaotian Han, Hongxia Yang, and Fei Wu. 2025. InfiGUIAgent: Multimodal Generalist GUI Agent with Native Reasoning and Reflection. arXiv preprint arXiv:2501.04575 (2025). [34] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. 2022. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 1197611986. [35] Zhengxi Lu, Yuxiang Chai, Yaxuan Guo, Xi Yin, Liang Liu, Hao Wang, Guanjing Xiong, and Hongsheng Li. 2025. UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning. arXiv preprint arXiv:2503.21620 (2025). [36] OpenAI. 2023. GPT-4V(ision) System Card. https://cdn.openai.com/papers/ GPTV_System_Card.pdf [37] OpenAI. 2024. GPT-4o. 2025-01-03. https://openai.com/index/hello-gpt-4o/ Accessed: [38] Zhenyu Pan, Haozheng Luo, Manling Li, and Han Liu. 2024. Chain-of-action: Faithful and multimodal question answering through large language models. arXiv preprint arXiv:2403.17359 (2024). [39] Zhiliang Peng, Wenhui Wang, Li Dong, Yaru Hao, Shaohan Huang, Shuming Ma, and Furu Wei. 2023. Kosmos-2: Grounding multimodal large language models to the world. arXiv preprint arXiv:2306.14824 (2023). [40] Yujia Qin, Yining Ye, Junjie Fang, Haoming Wang, Shihao Liang, Shizuo Tian, Junda Zhang, Jiahao Li, Yunxin Li, Shijue Huang, et al. 2025. UI-TARS: Pioneering Automated GUI Interaction with Native Agents. arXiv preprint arXiv:2501.12326 (2025). [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning. PMLR, 87488763. [42] Christopher Rawles, Sarah Clinckemaillie, Yifan Chang, Jonathan Waltz, Gabrielle Lau, Marybeth Fair, Alice Li, William Bishop, Wei Li, Folawiyo Campbell-Ajala, et al. 2024. Androidworld: dynamic benchmarking environment for autonomous agents. arXiv preprint arXiv:2405.14573 (2024). [43] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2024. HybridFlow: Flexible and Efficient RLHF Framework. arXiv preprint arXiv: 2409.19256 (2024). Preprint, Under review, April 2025 Yuhang Liu, Pengxiang Li, Congkai Xie, Xavier Hu, Xiaotian Han, Shengyu Zhang, Hongxia Yang, and Fei Wu [44] Yuchen Sun, Shanhui Zhao, Tao Yu, Hao Wen, Samith Va, Mengwei Xu, Yuanchun Li, and Chongyang Zhang. 2025. GUI-Xplore: Empowering Generalizable GUI Agents with One Exploration. arXiv preprint arXiv:2503.17709 (2025). [45] Kimi Team, Angang Du, Bohong Yin, Bowei Xing, Bowen Qu, Bowen Wang, Cheng Chen, Chenlin Zhang, Chenzhuang Du, Chu Wei, et al. 2025. Kimi-VL Technical Report. arXiv preprint arXiv:2504.07491 (2025). [46] Qwen Team. 2025. QwQ-32B: Embracing the Power of Reinforcement Learning. https://qwenlm.github.io/blog/qwq-32b/ [47] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, TimothÃ©e Lacroix, Baptiste RoziÃ¨re, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. 2023. LLaMA: Open and Efficient Foundation Language Models. ArXiv (2023). https://doi.org/10.48550/arXiv.2302.13971 [48] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. 2024. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191 (2024). [49] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. 2023. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079 (2023). [50] Hao Wen, Yuanchun Li, Guohong Liu, Shanhui Zhao, Tao Yu, Toby Jia-Jun Li, Shiqi Jiang, Yunhao Liu, Yaqin Zhang, and Yunxin Liu. 2023. AutoDroid: LLMpowered Task Automation in Android. arXiv preprint arXiv:2308.15272 (2023). [51] Zhiyong Wu, Zhenyu Wu, Fangzhi Xu, Yian Wang, Qiushi Sun, Chengyou Jia, Kanzhi Cheng, Zichen Ding, Liheng Chen, Paul Pu Liang, et al. 2024. Osatlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218 (2024). [52] Xiaobo Xia and Run Luo. 2025. GUI-R1: Generalist R1-Style Vision-Language Action Model For GUI Agents. arXiv preprint arXiv:2504.10458 (2025). [53] Chaojun Xiao, Xueyu Hu, Zhiyuan Liu, Cunchao Tu, and Maosong Sun. 2021. Lawformer: pre-trained language model for chinese legal long documents. AI Open 2 (2021), 7984. [54] Jianwei Yang, Hao Zhang, Feng Li, Xueyan Zou, Chunyuan Li, and Jianfeng Gao. 2023. Set-of-mark prompting unleashes extraordinary visual grounding in gpt-4v. arXiv preprint arXiv:2310.11441 (2023). [55] Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, and Junnan Li. 2024. Aria-UI: Visual Grounding for GUI Instructions. arXiv preprint arXiv:2412.16256 (2024). [56] Shenzhi Wang Zhangchi Feng Dongdong Kuang Yuwen Xiong Yaowei Zheng, Junting Lu. 2025. EasyR1: An Efficient, Scalable, Multi-Modality RL Training Framework. https://github.com/hiyouga/EasyR1. [57] Keen You, Haotian Zhang, Eldon Schoop, Floris Weers, Amanda Swearngin, Jeffrey Nichols, Yinfei Yang, and Zhe Gan. 2025. Ferret-ui: Grounded mobile ui understanding with multimodal llms. In European Conference on Computer Vision. Springer, 240255. [58] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. Appagent: Multimodal agents as smartphone users. arXiv preprint arXiv:2312.13771 (2023)."
        }
    ],
    "affiliations": [
        "Dalian University of Technology",
        "Reallm Labs",
        "The Hong Kong Polytechnic University",
        "Zhejiang University"
    ]
}