{
    "paper_title": "$π_\\texttt{RL}$: Online RL Fine-tuning for Flow-based Vision-Language-Action Models",
    "authors": [
        "Kang Chen",
        "Zhihao Liu",
        "Tonghe Zhang",
        "Zhen Guo",
        "Si Xu",
        "Hao Lin",
        "Hongzhi Zang",
        "Xiang Li",
        "Quanlu Zhang",
        "Zhaofei Yu",
        "Guoliang Fan",
        "Tiejun Huang",
        "Yu Wang",
        "Chao Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision-Language-Action (VLA) models enable robots to understand and perform complex tasks from multimodal input. Although recent work explores using reinforcement learning (RL) to automate the laborious data collection process in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based VLAs (\\eg, $π_0$, $π_{0.5}$) remains challenging due to intractable action log-likelihoods from iterative denoising. We address this challenge with $π_{\\texttt{RL}}$, an open-source framework for training flow-based VLAs in parallel simulation. $π_{\\texttt{RL}}$ implements two RL algorithms: (1) \\textbf{Flow-Noise} models the denoising process as a discrete-time MDP with a learnable noise network for exact log-likelihood computation. (2) \\textbf{Flow-SDE} integrates denoising with agent-environment interaction, formulating a two-layer MDP that employs ODE-to-SDE conversion for efficient RL exploration. We evaluate $π_{\\texttt{RL}}$ on LIBERO, ManiSkill, and MetaWorld benchmarks. On LIBERO, $π_{\\texttt{RL}}$ boosts few-shot SFT models $π_0$ and $π_{0.5}$ from 57.6\\% to 97.6\\% and from 77.1\\% to 98.3\\%, respectively. On ManiSkill, we train $π_{\\texttt{RL}}$ in 320 parallel environments, improving $π_0$ from 38.4\\% to 78.8\\% and $π_{0.5}$ from 40.1\\% to 90.8\\% across 4352 variations of pick-and-place task. On MetaWorld, RL is conducted over 50 different manipulation tasks and yields performance gains of 35.0\\% and 26.9\\% for $π_0$ and $π_{0.5}$ models, respectively. Overall, $π_{\\texttt{RL}}$ achieves significant performance gains and stronger generalization over SFT-models, validating the effectiveness of online RL for flow-based VLAs."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 2 9 8 8 5 2 . 0 1 5 2 : r a"
        },
        {
            "title": "Preprint Version",
            "content": "πRL: ONLINE RL FINE-TUNING FOR FLOW-BASED VISION-LANGUAGE-ACTION MODELS Kang Chen,,, Zhihao Liu,,, Tonghe Zhang,,, Zhen Guo, Si Xu, Hao Lin, Hongzhi Zang, Xiang Li, Quanlu Zhang, Zhaofei Yu, Guoliang Fan, Tiejun Huang, Yu Wang,, Chao Yu,, Equal Contributions Work completed while Tonghe was at Tsinghua University Corresponding Authors: zoeyuchao@gmail.com,yu-wang@tsinghua.edu.cn Tsinghua University Peking University Institute of Automation, Chinese Academy of Sciences Carnegie Mellon University Infinigence AI Zhongguancun Academy https://github.com/RLinf/RLinf https://huggingface.co/RLinf Figure 1: Overview of πRL. πRL, an online RL framework featuring Flow-Noise and Flow-SDE two approaches, is designed to enhance the performance and generalization of SFT-aligned flowbased VLAs, represented by the π0 and π0.5. Experiments conducted on LIBERO, ManiSkill, and MetaWorld benchmarks demonstrate that πRL achieves significant gains over SFT models."
        },
        {
            "title": "ABSTRACT",
            "content": "Vision-Language-Action (VLA) models enable robots to understand and perform complex tasks from multimodal input. Although recent work explores using reinforcement learning (RL) to automate the laborious data collection process in scaling supervised fine-tuning (SFT), applying large-scale RL to flow-based VLAs (e.g., π0, π0.5) remains challenging due to intractable action log-likelihoods from iterative denoising. We address this challenge with πRL, an open-source framework for training flow-based VLAs in parallel simulation. πRL implements two RL algorithms: (1) Flow-Noise models the denoising process as discrete-time MDP with learnable noise network for exact log-likelihood computation. (2) Flow-SDE integrates denoising with agent-environment interaction, formulating two-layer MDP that employs ODE-to-SDE conversion for efficient RL exploration. We evaluate πRL on LIBERO, ManiSkill, and MetaWorld benchmarks. On"
        },
        {
            "title": "Preprint Version",
            "content": "LIBERO, πRL boosts few-shot SFT models π0 and π0.5 from 57.6% to 97.6% and from 77.1% to 98.3%, respectively. On ManiSkill, we train πRL in 320 parallel environments, improving π0 from 38.4% to 78.8% and π0.5 from 40.1% to 90.8% across 4352 variations of pick-and-place task. On MetaWorld, RL is conducted over 50 different manipulation tasks and yields performance gains of 35.0% and 26.9% for π0 and π0.5 models, respectively. Overall, πRL achieves significant performance gains and stronger generalization over SFT-models, validating the effectiveness of online RL for flow-based VLAs."
        },
        {
            "title": "INTRODUCTION",
            "content": "Vision-Language-Action (VLA) models (Din et al., 2025) have emerged as leading solution for general-purpose robots, effectively bridging the gap between high-level multimodal reasoning and low-level physical control (Firoozi et al., 2025). Conditioned on sensor inputs and language commands, VLAs (Team et al., 2024; Kim et al., 2024; Black et al., 2024; Intelligence et al., 2025) can translate abstract instructions into executable robotic actions, thereby enabling intuitive and flexible human-robot interaction. The training methodology for VLAs follows the standard pre-training and supervised fine-tuning (SFT) paradigm as shown in Fig. 1. Building on the pretrained Vision-Language Model (VLM) (Touvron et al., 2023; Beyer et al., 2024), VLAs are fine-tuned on large-scale, heterogeneous human demonstration datasets (ONeill et al., 2024; Khazatsky et al., 2024), followed by SFT on the target task to align their capabilities with the specific embodiment and environment. However, reliance on SFT introduces critical challenge: curating large-scale, high-quality expert trajectories is both laborious and costly (Din et al., 2025), and the models obtained via SFT tend to overfit to expert demonstrations (Fei et al., 2025). Recent efforts (Zang et al., 2025; Li et al., 2025a; Tan et al., 2025; Liu et al., 2025b) have explored expanding the VLA training process with reinforcement learning (RL), establishing pre-training, SFT, and RL paradigm as shown in Fig. 1, allowing VLAs to improve their performance beyond initial expert demonstrations through active environmental interaction and the development of more generalizable policies. However, these RL advances have been largely confined to autoregressive VLAs, featuring OpenVLA (Kim et al., 2024) and OpenVLA-OFT (Kim et al., 2025), which employ discrete action decoders that generate output in an autoregressive or parallel fashion. This stands in stark contrast to diffusionor flow-based VLAs, exemplified by the π series models π0 (Black et al., 2024) and π0.5 (Intelligence et al., 2025), which generate actions through iterative refinement in flow matching (Lipman et al., 2022), offering the advantages of generating action chunks in high-frequency and performing highly dexterous tasks (Black et al., 2024). Consequently, previous VLA-RL algorithms are incompatible with flow-based VLAs, and the fundamental challenge lies in how to characterize logarithmic likelihood (Hutchinson, 1989; Chen et al., 2018) for the executed actions. Contribution: We introduce πRL, the first open-source framework for fine-tuning flow-based VLAs π0 and π0.5 with parallel online RL. To address the intractable log-likelihood estimation problem in flow matching, we propose two solutions: Flow-Noise and Flow-SDE. Flow-Noise integrates learnable noise network into the denoising process and models this stage as discrete-time Markov decision process (MDP) for exact log-likelihood estimation. Flow-SDE converts the ordinary differential equation (ODE) denoising process into stochastic differential equation (SDE) while maintaining equivalent marginal distributions for exploration, and builds two-layer MDP that couples the denoising process with policy-environment interaction, along with hybrid ODE-SDE sampling technique for training acceleration. Given the formulated MDP and the exact log-likelihood computation, πRL undergoes further optimization via the proximal policy optimization (PPO) (Schulman et al., 2017) algorithm."
        },
        {
            "title": "Preprint Version",
            "content": "We conduct extensive experiments on the challenging multi-task benchmarks LIBERO (Liu et al., 2023) and high-fidelity simulator ManiSkill (Tao et al., 2024) to evaluate the effectiveness of πRL optimization on the π0 and π0.5 models, with comprehensive findings summarized in Fig. 1. Results on LIBERO. πRL demonstrates substantial performance gains over the SFT baselines, with the average success rate of π0 improving from 57.6% to 97.6%, and π0.5 from 77.1% to 98.3%. Notably, on the LIBERO-Long task suite, πRL boosts the performance of the π0.5 one-trajectory SFT model from 43.9% to 94.0%, surpassing the 92.4% performance of the all-trajectories SFT model. Moreover, we compare against group relative policy optimization (GRPO) (Shao et al., 2024) as an alternative policy gradient algorithm, with the comparison showing that PPO consistently outperforms GRPO across all task suites. Results in ManiSkill. We train the policy to pick 16 types of objects and place them on 17 different receptacles in 16 photorealistic scenes, with total of 4,352 combinations. πRL boosts the average success rate from 41.6% to 85.7% for π0 and 41.9% to 84.8% for π0.5, demonstrating πRLs ability to support large-scale multi-task RL. Additionally, we also conduct experiments on the SIMPLER benchmark (Li et al., 2024), the success rate was elevated from 67.2% to 86.7% for π0 and from 59.24% to 79.1% for π0.5. Results in MetaWorld. Beyond the pick-and-place tasks detailed previously, we evaluate πRL on the MetaWorld MT50 benchmark to assess the capabilities of πRL across 50 diverse manipulation tasks of varying difficulty. With RL, the π0 and π0.5 models achieve success rates of 85.8% and 70.7%, exceeding the performance of the leading baseline SmolVLA (68.2%) (Shukor et al., 2025). To sum up, our contributions are: RL for flow-based VLAs. We introduce πRL, the first online RL fine-tuning framework for flow-based π-series VLAs, featuring Flow-Noise and Flow-SDE, two distinct technical solutions that allow exact log-likelihood estimation in flow matching. Superior Performance. We demonstrate significant performance improvements and enhanced generalization of πRL on the multi-task benchmarks LIBERO and ManiSkill. Comprehensive Ablation. We conduct thorough ablation studies on RL algorithms, critic designs, noise injection strategies, MDP formulations, and hyperparameters within flowbased VLAs, providing empirical insights for future research on RL for flow-based VLAs. Open-source Code and Models. We release all codes and model checkpoints to ensure reproducibility, hope thating that our study helps to advance further research in this field."
        },
        {
            "title": "2 RELATED WORK",
            "content": "2.1 VISION-LANGUAGE-ACTION MODELS VLA models have recently achieved remarkable progress in robotics by integrating multimodal inputs to enable unified perception, reasoning, and control. This development has led to series of architectures, including Octo (Team et al., 2024), RT (Brohan et al., 2022), OpenVLA, OpenVLAOFT, π0, π0.5, and GR00T (Bjorck et al., 2025). OpenVLA, which exemplifies the autoregressive VLA architecture, discretizes the action space into tokenized representations. This enables language-conditioned control by treating actions as part of the VLMs vocabulary, but it inherently limits the resolution required for fine-grained motion. To achieve more dexterous and continuous physical behaviors, π0 and π0.5, as representatives of flow-based VLA architectures, introduce an action chunking architecture based on flow matching. This allows VLAs to model complex continuous action distributions, thereby achieving more dexterous physical behaviors. In this work, we further fine-tune the π-series models with online RL algorithms, enhancing their performance and generalization capabilities through online interaction with the environment. 2.2 ONLINE RL FINE-TUNING FOR VLA MODELS Recent research has increasingly focused on enhancing the performance and generalization of VLAs with online RL. For example, SimpleVLA-RL (Li et al., 2025a), building on the OpenVLA-OFT and GRPO, demonstrated that RL can improve long-horizon planning of VLA models under data"
        },
        {
            "title": "Preprint Version",
            "content": "scarcity. RL4VLA (Liu et al., 2025b) empirically evaluated PPO, GRPO, and direct preference optimization (DPO) (Rafailov et al., 2023) with stage-based sparse rewards, finding PPO to yield superior performance. VLA-RL (Lu et al., 2025) proposed specialized robotic process reward model and enhanced the data processing pipeline. iRe-VLA (Guo et al., 2025b) proposed framework that iterates between RL exploration and SFT updates. RIPT-VLA (Tan et al., 2025) applied the REINFORCE leave-one-out (RLOO) (Kool et al., 2018) algorithm to the QueST (Mete et al., 2024) and OpenVLA-OFT architectures. RLinf-VLA (Yu et al., 2025; Zang et al., 2025) provides unified and efficient framework for scalable RL training of VLA models, supporting diverse VLA architectures such as OpenVLA and OpenVLA-OFT, multiple RL algorithms like PPO and GRPO, and various simulators including LIBERO and ManiSkill. These works demonstrate the effectiveness of RL fine-tuning VLA models. While these approaches demonstrate the potential of applying online RL to VLAs, their application to flow-based VLAs is hindered by the challenge of exact log-likelihood estimation."
        },
        {
            "title": "2.3 RL FINE-TUNING FOR FLOW MODELS",
            "content": "Integrating RL with flow models is promising way to transcend the limitations of imitation learning. To this end, Flow-GRPO (Liu et al., 2025a) converts the deterministic ODE into an equivalent SDE to enable stochasticity exploration, foundation upon which subsequent works like Mix-GRPO (Li et al., 2025b) and TempFlow-GRPO (He et al., 2025) further accelerate training through hybrid ODE-SDE rollouts. ReinFlow (Zhang et al., 2025) injects learnable noise into the flow path and transforms it into discrete-time Markov process with tractable likelihood for stable policy gradient updates. Flow policy optimization (FPO) (McAllister et al., 2025) reframes policy optimization as maximizing the advantage-weighted ratio of the conditional flow matching loss. Policy-Agnostic RL (PA-RL) Mark et al. (2024) effectively fine-tunes diverse diffusion and Transformer architectures by distilling critic-optimized actions into the policy via supervised learning. Diffusion steering via reinforcement learning (DSRL) (Wagenmaker et al., 2025) refines the flow policy by performing RL in its latent-noise space, rather than modifying the policy parameters themselves. While prior work has mostly focused on non-robotic tasks or small-scale, single-task robotics, we address the more challenging problem of fine-tuning large-scale flow-based VLAs for complex, multi-task robotic scenarios."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "3.1 PROBLEM FORMULATION We formulate the task as an MDP, defined by tuple = (S, A, P0, PENV, RENV, γ). The state st is defined as the robot observation ot and P0 denotes the initial state distribution. Given the state, the flow policy predicts an action at π(st) A, resulting in the state transition st+1 PENV(st, at) and reward RENV(st, at). The objective is to learn policy πθ that maximizes the expected γ-discounted return over horizon of + 1: (πθ) = Eπθ,P0 (cid:34) (cid:88) (cid:35) γtRENV(st, at) . t=0 (1) With the policy gradient surrogate (Williams, 1992), the gradient of the return expectation can be approximated from sampled trajectories: θJ (πθ) = Eπθ,P0 (cid:34) (cid:88) t=0 θ log πθ(atst)A(st, at) . (2) (cid:35) The advantage function, A(st, at) = Q(st, at) (st), measures the relative merit of the action value Q(st, at) over the state value (st), providing low-variance signal for the policy update. 3.2 FLOW-BASED VISION-LANGUAGE-ACTION MODEL flow-based VLA model πθ is designed to map the observation ot comprising RGB images, language tokens, and robot proprioception to sequence of future actions At = [at,0, ..., at,H1],"
        },
        {
            "title": "Preprint Version",
            "content": "Figure 2: Two optimization methods in πRL. Flow-Noise adds learnable noise in one-layer MDP  (Fig. 3)  , using the denoised joint likelihood for policy gradient. Flow-SDE builds two-layer MDP with ODE-to-SDE conversion, and computes the likelihood directly. formulated as p(Atot). Within the model, the VLM extracts features from the visual and language inputs, while the flow matching expert is tasked with generating the actions. Specifically, the model learns conditional vector field vθ that transforms standard Gaussian noise distribution into the target action At. This is achieved by minimizing the Conditional Flow Matching (CFM) loss, which aligns the predicted vector field vθ with the ground-truth vector field u: LCFM = Eτ,p(At,ot),q(Aτ At) (cid:104) vθ(Aτ , ot) u(Aτ At)2 2 (cid:105) . (3) At) generates noisy action1 Aτ Here, the conditional probability path q(Aτ = τ At + (1 τ )ϵ from an action At, random noise ϵ (0, I), and continuous time τ [0, 1] in flow matching. For this specific path, the corresponding ground-truth vector field is defined as u(Aτ At) = At ϵ. During the inference, the action sequence is generated by first sampling noise vector A0 (0, I), which is further iteratively refined by integrating the learned vector field vθ over fixed number of steps based on the forward Euler method: Aτ +δ = Aτ + vθ(Aτ , ot) δ."
        },
        {
            "title": "4 METHODOLOGY",
            "content": "Existing VLA-RL approaches leverage base models such as OpenVLA for discrete actions and OpenVLA-OFT for continuous actions. To compute the action log-likelihood log πθ(atst), discrete models (Liu et al., 2025b) apply softmax to the output logits, while continuous models (Li et al., 2025a) treat the action as Gaussian distribution, employing prediction head to estimate the variance. As for the flow-based VLAs, directly computing the exact likelihood (Hutchinson, 1989) is inaccurate with few denoising steps. Moreover, the deterministic nature of its ODE sampling process precludes exploration, making its implementation within RL non-trivial. To this end, we propose Flow-Noise and Flow-SDE, two technical approaches that make flow-based VLAs amenable to RL, as depicted in Fig. 2. 4.1 FLOW-NOISE Inspired by Reinflow (Zhang et al., 2025), we incorporate learnable noise network into the flow matching denoising process and solve the problem within the standard one-layer MDP framework detailed in Sec. 3.1. By modeling the denoising stage as discrete MDP, we can directly compute the log-likelihood of the denoised sequence, enabling equivalent policy optimization via RL. 1Aτ incorporates two temporal indices, denotes the discrete time step for environment interaction and τ represents the continuous time variable in flow matching."
        },
        {
            "title": "Preprint Version",
            "content": "Figure 3: Illustration for the noise injection on the flow matching, exemplified by π0.5, which integrates image, language, and state information for unified VLM input. 4.1.1 STOCHASTICITY INJECTION In Flow-Noise, we parameterize the noise schedule with neural network, allowing the magnitude of the injected noise to be learned dynamically during training for greater flexibility, as shown in Fig. 3. We focus on the generation process within single environment timestep t. For notational simplicity, we omit the time subscript t, e.g., writing Aτ , and denote the predicted velocity vθ(Aτ , o) as vτ . The step transition during the denoising process is modeled as an isotropic Gaussian distribution p(Aτ +δAτ ) (µτ , Στ ), where the mean is determined by the forward Euler update of the original ODE and the variance is controlled by the learnable noise network θ: (cid:26)µτ = Aτ + vτ δ Στ = diag(σ2 θ) . (4) Here, σθ() is the standard deviation learned from the noise injection network, conditioned on the action Aτ , and the observation o. The noise network is trained jointly with the velocity but discarded after fine-tuning, leaving deterministic policy for inference. 4.1.2 LOG-LIKELIHOOD ESTIMATION The primary challenge in applying policy gradient methods to flow-based VLAs stems from the intractable log-likelihood of the final executed action. In Flow-Noise, we address it by substituting the gradient of the joint log-likelihood of the entire denoising process into the policy optimization objective in Eq. (2), which is theoretically grounded in Reinflow (Zhang et al., 2025). The inference process for action generation is discretized into uniform steps, which defines sequence of time points {τ0, τ1, . . . , τK}. With the step interval defined as δ = 1/K, the discrete timestep at the k-th point is τk = δ, starting from τ0 = 0 and culminating at τK = 1. Given the observation o, the exact and tractable log probability for the entire denoising sequence = (A0, . . . , A1) is depicted in Fig. 2 and formulated as: (cid:32) log π(Ao) = log π(A0o) (cid:33) π(Aτk+1 Aτk , o) . K1 (cid:89) k=0 (5) Building on this, we can treat flow-based policy optimization within standard MDP framework. 4.2 FLOW-SDE Inspired by Flow-GRPO (Liu et al., 2025a), we enhance stochastic exploration by converting the denoising process from ODE into an SDE formulation. We further construct two-layer MDP to"
        },
        {
            "title": "Preprint Version",
            "content": "couple the denoising process with the policy-environment interaction following DPPO (Ren et al., 2024), while leveraging the hybrid ODE-SDE sampling technique to accelerate the training process."
        },
        {
            "title": "4.2.1 STOCHASTICITY INJECTION",
            "content": "In Flow-SDE, we convert the deterministic ODE into an equivalent SDE that preserves the marginal probability density of the generated actions, as shown in Fig. 3. The deterministic ODE sampling trajectory of the flow matching, especially the Rectified Flow (Liu et al., 2022), is described by the forward Euler method: dAτ = vτ dτ. (6) Building on the connection between the probability flow ODE and SDE (Song et al., 2020), we can transform the deterministic ODE in Eq. (6) into an equivalent SDE, with drift term that corrects the original velocity and diffusion term that introduces noise: (cid:18) (cid:19) dAτ = vτ g2(τ ) log qτ (Aτ ) 1 2 (cid:124) (cid:123)(cid:122) Drift Term dτ (cid:125) + g(τ )dw (cid:124) (cid:123)(cid:122) (cid:125) Diffusion Term , (7) where g(τ ) is scalar function controlling the noise schedule, log qτ (Aτ ) is the score function of the marginal distribution qτ and dw denotes Wiener process. As established in Flow-GRPO, the score function and the velocity field are critically linked by log qτ (Aτ ) = Aτ τ 1τ τ vτ . By substituting the score function with the velocity field in Eq. (7) and setting the noise schedule g(τ ) to στ = 1τ with controlling the noise level, we derive the final SDE formulation for the flow-matching sampler: (cid:113) τ dAτ = (cid:20) vτ + σ2 τ 2τ (Aτ + (1 τ )vτ ) (cid:21) dτ + στ dwτ . (8) Discretizing this SDE reveals that the transition probability p(Aτ +δAτ ) (µτ , Στ ) is an isotropic Gaussian distribution, with the mean and variance formulated as: (cid:40) µτ = Aτ + Στ = σ2 τ δ (cid:104) vτ + σ2 τ 2τ (Aτ + (1 τ )vτ ) (cid:105) δ . (9) 4.2.2 MDP FORMULATION While Flow-Noise substitutes the joint log-likelihood of the entire denoising sequence for the likelihood of the final executed action, we couple the denoising process of the flow matching with environmental interaction in Flow-SDE. Specifically, we embed the inner MDP defined during the denoising process into the high-level, outer-loop MDP with the environment MENV in Sec. 3.1, formulating two-layer MDP as shown in Fig. 2, with components defined with respect to the environment time and denoising time τ . State sτ Action aτ action for the outer loop: = (ot, Aτ ) is the tuple of the observation ot and the action state Aτ . is defined as the next sampled denoised action in the inner-loop and the executed where Aτ +δ Transition (sτ = µτ + στ sτ , aτ aτ = (cid:26)Aτ +δ A1 if τ < 1 if τ = 1 , δ ϵ, ϵ (0, I) is the randomly sampled noise. ) defines how the state evolves, formulated as: (10) (11) sτ = (cid:26)(ot, aτ ) (ot+1, A0 t+1) if τ < 1 if τ = 1 ."
        },
        {
            "title": "Preprint Version",
            "content": "For τ < 1, the inner loop transition PFLOW() occurs between different denoised action = Aτ +δ states, where the observation ot remains fixed and the next action state is set by aτ . For τ = 1, the final action aτ interacts with the outer-loop environment, resulting in new observation ot+1 according to the environment dynamics PENV(). Concurrently, the action state is reset from standard normal distribution A0 = A1 t+1 (0, I). Reward R(sτ , aτ tion with the environment: ) is granted only upon completion of the denoising process and interacR(sτ , aτ ) = (cid:26) RENV(ot, A1 ) if τ < 1 if τ = 1 . (12) Within the two-layer MDP framework, log π(atst) is transformed into estimating log π(aτ to the Gaussian nature of the transitions. the problem of estimating the action log-likelihood ), which is straightforward to compute due sτ"
        },
        {
            "title": "4.2.3 HYBRID ODE-SDE SAMPLING",
            "content": "In the formulated two-layer MDP framework, the effective trajectory length is the product of the environment interaction steps and the number of flow matching denoising steps. While this formulation enables RL training for flow-based VLAs, it significantly extends the MDP horizon compared to non-iterative VLA methods, which substantially increases both the training difficulty and the computational time required for optimization. To this end, we adopt the mixed ODE-SDE rollout strategy, drawing inspiration from the text-toimage generation methods such as Mix-GRPO (Li et al., 2025b) and TempFlow-GRPO (He et al., 2025). Specifically, during the denoising process, single step is randomly sampled as stochastic SDE transition governed by p(Aτ +δAτ ) (µτ , Στ ), while the remaining steps follow deterministic ODE transitions defined by the update rule Aτ +δ = Aτ + vτ δ. Under this formulation, we treat the deterministic ODE transition between states as an environmentlevel wrapper and revise the state transition function of the previous two-layer MDP. Specifically, at each environment step t, denoising time τt is randomly selected for the policys stochastic = (ot, Aτt injection. The policy π acts on this state sτt according to Eq. (10). The environment wrappers then execute all subsequent deterministic steps, ultimately transitioning to the next observation ot+1 and the next action state sτt+1 t+1 ) at newly sampled time τt+1. During this process, the state input and action output of the policy remain consistent with the previous two-layer MDP formulation, thus ensuring theoretical consistency. ), sampling the action Aτt+δ t+1 = (ot+1, Aτt+1 4.3 POLICY OPTIMIZATION 4.3.1 ALGORITHM Given the formulated flow policy MDP, our objective is to learn the optimal parameters θ for the policy πθ that maximizes the expected discounted return (πθ). To this end, we apply the widely adopted policy gradient algorithm PPO to optimize the policy. π-series models (Black et al., 2024; Intelligence et al., 2025) adopt chunk-based approach for action generation. Specifically, the policy outputs an entire sequence of future actions At = [at,0, ..., at,H1] in response to each observation. In this approach, we treat the entire sequence as single macro-step and define its corresponding reward Rt = (cid:80)H1 j=0 rt,j as the sum of the per-step rewards rt,j, referred to as the chunk-level formulation in RLinf-VLA (Zang et al., 2025). To effectively guide policy updates, PPO employs Generalized Advantage Estimation (GAE) (Schulman et al., 2015) to compute low-variance estimate of the advantage, estimated as: ˆAt = t (cid:88) (γλ)kTt+k, k=0 (13) where the TD-error is Tt = Rt + γV (st+1) (st). Here, () is the state-value function derived from the critic network, γ is the discount factor, and λ is the parameter that balances the trade-off between bias and variance in the advantage estimate."
        },
        {
            "title": "Preprint Version",
            "content": "(a) Critic with the action expert, exemplified by π0. (b) Critic with the VLM, exemplified by π0.5. Figure 4: Illustration of the two critic placement configurations. PPO constrains policy updates to small trust region to prevent large, destabilizing updates, with the objective function: (πθ) = Et (cid:104) min (cid:16) ρt(θ) ˆAt, clip(ρt(θ), 1 ϵ, 1 + ϵ) ˆAt (cid:17)(cid:105) , (14) where the clip function, governed by hyperparameter ϵ, restricts the ratio ρt(θ) to the interval [1 ϵ, 1 + ϵ] to ensure training stability. Here, the probability ratio ρt(θ) between the updated and old policies takes the form of either: ρt(θ) = πθnew(atst) πθold(atst) or ρt(θ) = πθnew (aτ πθold(aτ sτ ) sτ ) , (15) for the one-layer and two-layer MDP formulations, respectively. 4.3.2 CRITIC DESIGN Following VLA-PPO works (Zang et al., 2025; Liu et al., 2025b), we employ shared actor-critic architecture for memory-efficient value prediction as shown in Fig. 4. However, the two flow-based VLAs process the proprioceptive state differently: in π0, the state is fed into the action expert model, whereas in π0.5, it is merged with prompt embeddings within the VLM. To this end, for the π0.5 variant, we attach the critic network directly to the VLM output, providing the value estimate Vvlm(ot) conditioned on the integrated image, language, and state inputs. Conversely, for the π0 variant, achieving the value prediction is non-trivial due to the coupled input structure, where the action expert requires both the noisy action Aτ and the state. To this end, we approximate Vexpert(ot) by averaging the value estimates across the entire denoising trajectory, formulated as: Vexpert(ot) Eτ [0,1][Vexpert(ot, Aτ )]. (16)"
        },
        {
            "title": "5 EXPERIMENTAL RESULTS",
            "content": "5.1 SETUP Benchmarks. We perform experiments based on LIBERO (Liu et al., 2023), ManiSkill (Tao et al., 2024) and MetaWorld (McLean et al., 2025) benchmarks. LIBERO (Liu et al., 2023) is built on CPU-based simulation platform MuJoCo and assesses knowledge transfer in robotic multi-task and lifelong learning across four manipulation task suites: Spatial, Object, Goal, and Long. ManiSkill serves as high-fidelity, GPU-parallelized simulation platform. Within ManiSkill, we adopt the SIMPLER benchmark (Li et al., 2024) as our primary testbed. To further evaluate the generalization capability of πRL, we follow the setup of RL4VLA (Liu et al., 2025b) and construct 4,352 pick-and-place task combinations as an extended benchmark."
        },
        {
            "title": "Preprint Version",
            "content": "Table 1: Evaluation results on the LIBERO benchmark, evaluated based on the success rate (%). Model # Full Dataset SFT Octo OpenVLA πfast OpenVLA-OFT π0 π0.5 # Few-shot SFT + RL π0 SFT Flow-SDE Flow-Noise # Few-shot SFT + RL π0.5 SFT Flow-SDE Flow-Noise Spatial Object Goal Long Avg. Avg. LIBERO 78.9 84.7 96.4 91.6 96.8 98.8 65.3 98.4 99.0 84.6 99.6 99. 85.7 88.4 96.8 95.3 98.8 98.2 64.4 99.4 99.2 95.4 100 100 84.6 79.2 88.6 90.6 95.8 98.0 49.8 96.2 98.2 84.6 98.8 99. 51.1 53.7 60.2 86.5 85.2 92.4 51.2 90.2 93.8 43.9 93.0 94.0 75.1 76.5 85.5 91.0 94.2 96.9 57.6 96.1 97.6 77.1 97.9 98. +38.5 +40.0 +20.8 +21.2 MetaWorld (McLean et al., 2025) is multi-task evaluation benchmark built upon the MuJoCo simulator. To evaluate performance on broad spectrum of manipulation skills beyond pick-and-place, we utilize the MT50 task set as our testbed. Flow-based VLAs. We conduct experiments based on π0 and π0.5. π0 introduces the flow-matching action expert (300M) built upon pre-trained PaliGemma (3B) to leverage broad semantic knowledge from internet-scale data. π0.5 further utilizes co-training across heterogeneous data sources (e.g., multi-robot data, web data, and high-level semantic predictions) for broader generalization. In addition to the π-series models, we also conduct experiments on GR00T (Bjorck et al., 2025) in Appendix Sec. C, validating that our algorithm is applicable to other flow-based VLAs. Implementation Details. Given that pre-trained models often struggle to generalize to task-specific benchmarks, we initiate our process with SFT on expert demonstrations. For the SFT stage, we finetune the entire 3.3B model following the official setting. In the subsequent RL stage, we freeze the VLM parameters and exclusively fine-tune the 300M action expert model, driven by GPU memory efficiency and the findings from RL4VLA that RL contributes more significantly to action generalization. We build the whole framework upon the RLinf (Yu et al., 2025) codebase, where we adopt shared, co-located GPU allocation strategy that places the environment, rollout model, and actor model on the same GPU and executes them serially. For the model configurations, we adhere to the official setting provided by openpi (Black et al., 2024; Intelligence et al., 2025). In these settings, π0 utilizes image, language, and proprioceptive states as input, whereas π0.5 notably omits state information for the LIBERO benchmark2. Following this precedent, we consistently omit the state input for π0.5 during both SFT and RL phases on LIBERO and ManiSkill. Our experiments are conducted on 8 NVIDIA H100 80GB GPUs, and detailed training hyperparameters are available in Appendix Tabs. 7 and 8. 5.2 MAIN RESULTS 5.2.1 LIBERO SFT Procedure. The LIBERO benchmark comprises four task suites, each consisting of 10 distinct subtasks. To facilitate few-shot SFT on LIBERO, minimum of 40 expert demonstration trajectories is necessary to ensure positive success rate for each subtask across four task suites, thereby guaranteeing positive optimization signal for the subsequent RL phase. 2https://github.com/Physical-Intelligence/openpi/issues/"
        },
        {
            "title": "Preprint Version",
            "content": "We perform few-shot SFT following the official training configs provided by openpi. For the π0 model, we utilized subset of 58 trajectories, sampled from the total of 1,692 trajectories spanning the four task suites in the official LIBERO SFT dataset3, to perform SFT, which served as the initial checkpoint4 for subsequent RL training on LIBERO-Spatial, LIBERO-Object and LIBERO-Object task suites. Additionally, larger pool of 208 trajectories was employed for the LIBERO-Long few-shot SFT5 due to the long-horizon and more challenging nature of these tasks. For the π0.5 model, given its better pretrained checkpoint and training config, we only leveraged 40 trajectories for few-shot SFT, providing unified checkpoint6 across task suites. RL Procedure. In RL, the VLA model receives multi-modal input state comprising: an agent-view and wrist-view (both 224 224 RGB images), natural language guidance, the robot end effector pose, and the gripper state. The model outputs an action to interact with the LIBERO environment, which provides binary reward of 1 for successful task completion and 0 otherwise. Experiments. We benchmark the performance of πRL, which fine-tunes the few-shot SFT π0 and π0.5 models with our proposed Flow-Noise and Flow-SDE, against several state-of-the-art VLAs trained on the entire LIBERO dataset, including Octo, OpenVLA, OpenVLA-OFT, πfast (Pertsch et al., 2025), π0, and π0.5. We conduct experiments on four LIBERO task suites and report performance as the success rate across all 500 initial states (10 sub-tasks 50 states each). Analysis. As detailed in Tab. 1, our proposed two solutions, Flow-Noise and Flow-SDE, not only achieve comparable performance but also establish new state-of-the-art by significantly boosting the performance of the few-shot π0 and π0.5 SFT models. For the few-shot π0 model, the SFT baseline performs poorly, with an average success rate of only 57.6%, indicating that the model struggles with limited demonstration data. Our proposed πRL substantially boosts performance, with Flow-SDE and Flow-Noise reaching 96.1% and 97.6%, respectively, and surpassing the full-dataset π0 SFT baseline of 94.2%. While the π0.5 few-shot SFT baseline achieves decent average performance of 77.1%, it struggles with the challenging LIBERO-Long task, scoring only 43.9%. Our proposed πRL framework rectifies this deficiency, boosting the LIBERO-Long success rate from 43.9% to 94.0%, constituting 50.1% improvement. Notably, despite using only single trajectory for SFT, πRL reaches 98.3% final performance, surpassing the 96.9% full-dataset SFT model. Discussion on two methods. Flow-SDE and Flow-Noise differ primarily in their noise injection strategy and MDP formulation, with experiments indicating that Flow-Noise marginally outperforms Flow-SDE, result we attribute to two factors: Noise Injection: Flow-Noise employs noise network for exploration, complemented by relative entropy bonus for noise magnitude adaptation, which affords the model finer control during convergence, thus achieving better performance. MDP Formulation: Flow-Noise adopts one-layer MDP formulation where the logprobability of the executed action is derived from the joint log-probability of the denoised sequence. This formulation endows Flow-Noise with higher data utilization efficiency, leading to faster convergence, as demonstrated in Fig. 8. Despite this, the performance discrepancy is still marginal (e.g., 1.5% in π0 and 0.4% in π0.5). Additionally, Flow-Noise requires recomputing the entire denoising trajectory for log-likelihood computation. Consequently, the update time per RL training step scales with the number of denoising steps, whereas it remains constant for Flow-SDE due to its mixed ODE-SDE rollout strategy. 5.2.2 MANISKILL SFT Procedure. Since the SFT dataset provided by RL4VLA lacks the state information required for π0, we re-synthesized trajectories following their setting using the MPLib motion planning 3https://huggingface.co/datasets/physical-intelligence/libero 4https://huggingface.co/RLinf/RLinf-Pi0-SFT-Spatial-Object-Goal 5https://huggingface.co/RLinf/RLinf-Pi0-SFT-Long 6https://huggingface.co/RLinf/RLinf-Pi05-SFT"
        },
        {
            "title": "Preprint Version",
            "content": "Table 2: Evaluation results on the WidowX SIMPLER benchmark for π0 and π0.5. Model SIMPLER Carrot Eggplant Spoon Cube Avg. π0 π0.5 SFT Flow-Noise SFT Flow-Noise 82.7 95.7 +13.0 70.6 82.0 +11. 87.5 96.7 +9.2 91.9 98.2 +6.3 61.7 91.6 +29.9 43.5 82.8 +39.3 37.1 63.0 +25.9 31.0 53.3 +22. 67.2 86.7 +19.5 59.2 79.1 +19.9 suite (Guo et al., 2025a), with the final 15 additional frames appended to reinforce the concept of completing motion. RL Procedure. In RL, the VLA model receives an input comprising single 480 640 RGB thirdperson view, short language instruction, and the current joint pose. The model also receives structured reward signal from the environment: 1.0 for correct object placement and 0.1 for successful attachment of the gripper to the object, mitigating unwanted throwing behaviors. Experiments. Based on the models π0 and π0.5 models, we empirically validate the performance of Flow-SDE and Flow-Noise against SFT baselines in SIMPLER and pick-and-place generalization benchmark. Following the default settings of π0 and π0.5s official code base, we include proprioception information in the input to the action expert of π0 and omit the state input to the VLM of π0.5. In SIMPLER, the experimental setup comprises an 8-DoF WidowX-250S arm evaluated on four standard tasks: (1) Spoon: placing spoon on cloth, (2) Carrot: placing carrot on plate, (3) Eggplant: placing an eggplant in basket, and (4) Cube: stacking cube. For the SFT stage, we employ curated dataset in which each task is trained with 144 demonstration episodes. In the generalization test, the policy is prompted to pick from 16 different object types and place them onto 17 different receptacles, distributed across 16 unique table scenes, yielding total of 4,352 unique task combinations. Given the high complexity of this setting, the SFT data set was prepared with 16,384 episodes, scale substantially larger than that for SIMPLER tasks. Analysis. As detailed in Tab. 2 and Tab. 3, πRL achieves substantial performance improvements In the SIMPLER environment, πRL inin both the SIMPLER and generalization environments. creases the average success rate of the π0 model from 67.2% to 86.7%, with three tasks (carrot, eggplant, and spoon) exceeding 90% success. In the training environment of the generalization test, which comprises 4352 task compositions, the performance of π0 increases from 41.6% to 85.7%, while the π0.5 model improves from 40.1% to 84.8%. These results demonstrate the effectiveness of πRL in photorealistic environment. Generalization Tests. Following RL4VLA, we further evaluate the models generalization across three challenging out-of-distribution (OOD) scenarios: (1) Vision, challenging the model with novel backgrounds and textures; (2) Semantics, probing comprehension with unseen objects, varied instructions, and confounding elements like extra objects or receptacles; (3) Execution, assessing robustness against varied initial states, unseen robot poses, and dynamic disturbances, such as moving target object during execution. In the OOD scenarios detailed in Tab. 3, we observe that the π0-SFT model demonstrates strong generalization for visual information. This can be attributed to the robust foundation of its VLM, which allows it to better handle visual disturbances. However, the semantic performance of π0 drops dramatically. This degradation is less pronounced when switching to the π0.5 baseline, benefit likely stemming from the knowledge generalization of the pre-trained π0.5 model. Regarding action execution, π0 exhibits larger performance drop than π0.5. We hypothesize that this discrepancy arises from the inclusion of joint angle states as input"
        },
        {
            "title": "Preprint Version",
            "content": "Table 3: Evaluation results on the Generalization Test of ManiSkill. Model π0 π0.5 SFT Flow-SDE Flow-Noise SFT Flow-SDE Flow-Noise IND 38.4 78.8 77.8 +40.4 40.1 90.9 89.7 +50.8 OOD Vision Semantic Execution Avg. 32.6 61.1 63.4 +30.8 40.2 68.0 69.9 +29.7 8.4 25.4 23.1 +16.8 16.6 34.5 35.5 +18.9 13.2 31.5 24.2 +18.3 22.4 45.4 54.9 +32. 18.1 39.3 36.9 +21.3 26.4 49.3 53.4 +27.1 (a) Spatial (b) Object (c) Goal (d) Long Figure 5: Visual comparison of PPO and GRPO with Flow-SDE π0 on the LIBERO, demonstrating that PPO outperforms GRPO in terms of convergence performance and training speed. in π0, leading to severe overfitting in the control task. In contrast, π0.5 omits these inputs, thereby avoiding the same degree of performance degradation. Furthermore, while RL yields significant improvements on in-distribution tasks, we observe its gains are limited in OOD scenarios. We attribute this discrepancy to two factors we aim to address in future work. First, the SFT baseline model itself exhibits substantial performance degradation in OOD settings, which inherently caps the generalization potential achievable by the subsequent RL finetuning. Second, freezing the VLM during the RL stage for training efficiency prevents the model from adapting its visual features to the environment, consequently hindering its visual generalization capabilities. 5.2.3 METAWORLD SFT Procedure. We perform SFT on the π0 and π0.5 models using the official dataset7, which consists of 2500 trajectories across 50 different manipulation tasks. RL Procedure. During the RL procedure, the VLA model processes multi-modal input comprising 480 480 RGB agent-view image, language guidance, the robots end-effector position, and its gripper state. Based on this input, the model outputs an action to interact with the environment, which in turn provides sparse reward: 1 for successful task completion and 0 otherwise. Experiments. We benchmark the performance of πRL against Diffusion Policy (Chi et al., 2025), TinyVLA (Wen et al., 2025), and SmolVLA (Shukor et al., 2025). For the performance evaluation, we follow the setup from SmolVLA, i.e., classifying 50 tasks into easy, medium, hard, and very hard four categories according to their difficulties. Analysis. As detailed in Tab. 4, RL fine-tuning substantially boosts performance. The π0 and π0.5 models achieve average success rates of 85.8% and 70.7%, respectively. This marks significant improvement over their SFT-only counterparts and surpasses the best-performing baseline SmolVLA of 68.2%, confirming that RL can effectively enhance model capabilities across diverse range of manipulation task types. 7https://huggingface.co/datasets/lerobot/metaworld_mt"
        },
        {
            "title": "Preprint Version",
            "content": "Table 4: Evaluation results on the MetaWorld MT50 benchmark. Methods"
        },
        {
            "title": "Diffusion Policy\nTinyVLA\nSmolVLA",
            "content": "SFT Flow-SDE Flow-Noise SFT Flow-SDE Flow-Noise Easy Medium Hard Very Hard Avg. Avg. MetaWorld 23.1 77.6 87.1 77.9 92.1 91.1 68.2 86.4 86. 10.7 21.5 51.8 51.8 74.6 81.8 37.3 55.5 58.1 1.9 11.4 70.0 53.3 61.7 78.3 41.7 75.0 63. 6.1 15.8 64.0 20.0 84.0 92.0 28.0 66.0 56.0 10.5 31.6 68.2 50.8 78.1 85.8 43.8 70.7 66. +27.3 +35.0 +26.9 +22.3 Table 5: Comparison of the PPO and GRPO with Flow-SDE on the LIBERO. Spatial Object Goal Long LIBERO SFT +GRPO +PPO SFT +GRPO +PPO 65.3 97.8 98. 84.6 97.4 99.6 64.4 97.8 99.4 95.4 99.8 100 49.8 83.2 96.2 84.6 91.2 98.8 51.2 81.4 90. 43.9 77.6 93.0 Avg. 57.6 90.0 96.0 77.1 91.5 97.9 Avg. +32.4 +38. +14.4 +20.8 π0 π0.5 Model π0 π0. 5.3 ABLATION STUDY Given that Flow-SDE achieves performance comparable to Flow-Noise while offering higher computational efficiency, we conduct ablation studies with the Flow-SDE method on the LIBERO benchmark to investigate the impact of the RL algorithm, critic design, stochasticity injection strategy, MDP formulation, and various hyperparameters. 5.3.1 RL ALGORITHMS Given the significant performance gains from PPO on the LIBERO benchmark, we also investigated the effectiveness of GRPO (Shao et al., 2024) (see Appendix for detailed description), another widely used policy gradient method applied in VLA+RL training. We compare the performance of PPO and GRPO on both the π0 and π0.5 models, with results summarized in Tab. 5. We further visualize training curves of PPO and GRPO in Fig. 5, demonstrating that PPO outperforms GRPO in both final convergence performance and training stability across all four LIBERO task suites. 5.3.2 CRITIC DESIGN Placement. We compare two critic placement strategies, one positioned after the action expert (Vexpert) and the other after the VLM (Vvlm), with π0 model on the LIBERO-Long task suite. As illustrated in Fig. 6, both placements yield comparable performance. However, we observe that Vvlm exhibits slightly superior performance, lower value loss, and higher explained variance, despite not receiving the proprioceptive state as input. This advantage can be attributed to key difference in their input: Vvlm learns direct mapping from observation to value, while Vexpert must contend with optimization challenges arising from coupled state and noisy action inputs. Nevertheless, to align with the design of the value function, we maintain the Vexpert architecture for the π0, ensuring that state information is incorporated to calculate the value."
        },
        {
            "title": "Preprint Version",
            "content": "(a) Eval (b) Value Loss (c) Explained Variance Figure 6: Ablation on the critic structure and placement within Flow-SDE π0 on the LIBERO-Long, indicating that the critic Vvlm attached after the VLM exhibits superior performance. Furthermore, four-layer MLP demonstrates stronger regression capability than one-layer MLP in Vexpert. (a) Train (b) Eval Figure 7: Ablation on the injection strategy within Flow-SDE of π0 on the LIBERO-Long. Structure. We investigate four-layer MLP versus one-layer MLP, which mirrors the actionprojection structure in the action expert. Results in Fig. 6 indicate that the four-layer MLP leads to more accurate value approximation, resulting in enhanced performance and training stability. 5.3.3 STOCHASTICITY INJECTION Flow-Noise and Flow-SDE provide two distinct approaches for injecting stochasticity. Specifically, Flow-Noise employs learnable noise network, while Flow-SDE uses fixed noise level strategy as illustrated in Fig. 3. To isolate the impact of the injection strategy, we evaluate these two strategies on the LIBERO-Long task suite, with the same Flow-SDE MDP formulation. Since the fixed noise approach does not incorporate an entropy coefficient, we set the entropy bonus for learned noise to 0 to ensure fair comparison. We set the fixed noise level to = 0.5, and the lower and upper bounds for the learnable noise logvariance to 0.08 and 0.16, respectively. As depicted in Fig. 7, two noise strategies exhibit similar train and eval performance at step 0, which indicates comparable noise magnitudes. Furthermore, the converged performance affirms the efficiency of both injection methods. 5.3.4 FLOW POLICY MDP Flow-Noise and Flow-SDE also differ in their MDP formulation, as shown in Fig. 2. Built on the standard one-layer MDP, Flow-Noise directly calculates the log-likelihood of the denoised sequence for the policy update. In contrast, Flow-SDE constructs two-layer MDP by integrating the denoising process with the environment, and further employs hybrid ODE-SDE sampling technique for acceleration. With the same Flow-SDE noise injection strategy, we evaluate these different frameworks on the LIBERO-Long task suite, as illustrated in Fig. 8. While the one-layer formulation converges fastest, all three frameworks achieve similar final performance. In terms of computational cost, the hybrid two-layer paradigm reduces training time by half compared to the standard two-layer approach, thanks to shorter effective MDP chain that low-"
        },
        {
            "title": "Preprint Version",
            "content": "(a) Eval (b) Update Time (c) Explained Variance Figure 8: Ablation on the MDP formulation within Flow-SDE of π0 on the LIBERO-Long. Table 6: Ablation study of hyperparameters for Flow-SDE on the LIBERO-Spatial. Performance is reported as task success rate (%). Train refers to policy performance during the stochastic rollout phase, whereas Eval refers to performance during the deterministic evaluation phase. Hyperparameters Models Stage Noise Level Denoise Step Action Chunk SFT RL Train Eval Train Eval 0.2 62.3 65.2 59.5 73. 0.5 56.0 65.2 93.5 94.5 0.8 46.6 65.2 95.3 98. 1 9.4 63.8 73.8 88.5 2 28.3 64.9 90.8 97. 4 56.1 65.2 93.5 94.5 8 62.6 63.2 84.3 86. 5 56.0 65.2 93.5 94.5 10 60.7 70.5 93.3 95. 20 70.3 72.6 87.5 89.2 ers the computational cost per RL update. Moreover, we observe that the one-layer MDP shows no significant speed advantage over the standard two-layer model, as its update stage necessitates re-computing the entire denoising trajectory to calculate the log-likelihood, resulting in comparable computational overhead. 5.3.5 HYPER-PARAMETERS Building on the Flow-SDE with π0, we investigate the influence of the noise level, denoise step, and action chunk on the LIBERO-Spatial benchmark. We denote the train stage as the phase where the policy generates stochastic actions for exploration, whereas the evaluation stage involves generating deterministic actions. The train and eval success rates for the SFT baseline and the RL fine-tuned model after 100 training steps are presented in Tab. 6. Noise Level. The noise level in the Flow-SDE is defined in Eq. (8), which governs the noise injection magnitude during the denoising process. From Tab. 6, we observe that the SFT baselines eval performance is identical across all noise levels as it relies on deterministic ODE sampling. Its training performance, however, degrades as the noise level increases, which is intuitive as higher noise can disrupt the flow path and lead to an inaccurate marginal action distribution. Extending this analysis to the RL fine-tuning stage reveals key trade-off: while lower noise levels mitigate performance degradation induced by policy exploration, the capacity for RL refinement is correspondingly constrained. This trade-off is empirically validated in Fig. 9, which indicates that training with the minimal noise level = 0.2 exhibits instability, manifesting as significantly higher clip fraction. We attribute this instability to the substantially larger gradient magnitudes induced by the low noise level. Denoise Step. The denoise step defines the number of discretization steps for action generation and is critical for controlling the fidelity of the ODE-to-SDE transition in Eq. (8). In Tab. 6, we observe that while all configurations start with similar eval performance, the train success rate plummets at = 1, indicating significant ODE-to-SDE discretization error."
        },
        {
            "title": "Preprint Version",
            "content": "(a) Train (b) Eval (c) Clipped Fraction Figure 9: Ablation on the noise level a, conducted with the Flow-SDE π0 on the LIBERO-Spatial. (a) Train (b) Eval Figure 10: Ablation on the denoise step, conducted with the Flow-SDE π0 on the LIBERO-Spatial. (a) Eval (b) Explained Variance Figure 11: Ablation on the chunk size, conducted with the Flow-SDE π0 on the LIBERO-Spatial. However, as in our noise-level analysis, larger is not necessarily optimal. As shown in Fig. 10, larger introduces clear trade-off: it yields higher rollout performance but complicates the training process due to an increased number of denoising steps. Action chunk. The action chunk refers to the number of consecutive actions the policy executes within single observation. We ablate the action chunk size across 5, 10, and 20, with results presented in Tab. 6 and further visualized in Fig. 11. While larger chunk size yields marginal performance improvement, it also reduces the frequency of policy-environment interactions and hinders accurate reward credit assignment. These factors contribute to less reliable advantage estimation, as reflected in the explained variance metric. Consequently, while large chunk size may provide stronger SFT baseline, it ultimately constrains the potential gains from subsequent RL fine-tuning. In conclusion, our analysis reveals consistent trade-off:"
        },
        {
            "title": "Preprint Version",
            "content": "(a) Eval (b) KL Divergence Figure 12: Ablation study on the learning rate scheduler. The experiment is conducted with Flow-SDE π0.5 on the LIBERO-Long benchmark, demonstrating that the scheduler alleviates overoptimization and stabilizes the training process. Caveat: Hyperparameters optimized for rollout might induce training instability, impeding potential performance gains from RL. Therefore, careful selection of these parameters is essential to achieve suitable balance between train performance and stable training process. 5. INSIGHTS FROM LARGE-SCALE TRAINING In this subsection, we elaborate on some empirical insights we gained during RL training. Hyperparameters. According to the hyperparameters ablation detailed in Sec. 5.3.5, the performance disparity between the train and eval performance of the initial SFT checkpoint warrants close attention. If this disparity is significant, we recommend either reducing the noise magnitude or increasing the number of denoising steps to mitigate the performance degradation caused by the discrepancy between deterministic and stochastic action generation. Furthermore, as previously established, lower noise levels yield larger gradients, requiring smaller learning rate to maintain training stability. We also observed that when train performance improves steadily while eval performance oscillates, increasing the number of denoising steps can help alleviate this, benefiting from reduced divergence in the action distributions between the deterministic and stochastic action generation processes. Regarding the action chunk, we empirically found that long-horizon tasks benefit from larger chunk sizes. For instance, we set the chunk size to 10 for LIBERO-Long and 5 for the other sub-tasks. Training. In our π0.5 experiments on the LIBERO-Long benchmark, we observed that the KullbackLeibler (KL) divergence metric increased steadily throughout training, potentially leading to instability. We mitigated this issue by implementing learning rate scheduler with cosine annealing. As demonstrated in Fig. 12, this scheduler effectively prevents the KL divergence from escalating, thereby stabilizing the training process. Critic. In our ManiSkill experiments, we observe that policy evaluation performance exhibits an initial dip before improving for both π0 and π0.5 models, as shown in Fig. 14. We attribute this transient degradation to the critic providing inaccurate signals during its warm-up phase. The subsequent eval improvement correlates directly with the critics value estimations stabilizing, as evidenced by the rising explained variance. Temporal Efficiency We also study how the rollout of RL in physical simulator helps shape the policy to achieve expert-level temporal efficiency. We analyze the expert motion planning data used for SFT and then tracked the average episodes lengths during the RL training of the π0.5 model using our methods. As shown in Figure Fig. 13, the SFT-initialized policy exhibits significantly"
        },
        {
            "title": "Preprint Version",
            "content": "Figure 13: Episode length: π0.5 RL training in multi-object pick-and-place environment (a) Eval (b) Explained Variance Figure 14: Flow-Noise Training curve in ManiSkill generalization test. longer episodes due to execution errors. In contrast, π0.5 achieves episode lengths that converge to the expert range after RL training, demonstrating substantial improvement in temporal efficiency. We attribute this convergence to two factors: (1) RLs error-correction capability, which helps the policy succeed in more diverse distribution, and (2) our partial reset mechanism with discounted reward, where faster task completion leads to more resets and higher cumulative reward between updates. 5.5 EXTENSION: FINE-TUNE VLM AND ACTION EXPERT SIMULTANEOUSLY In our previous experiments, the VLM is frozen, and the optimization is confined exclusively to the action expert during RL. In this subsection, we aim to investigate the role of the VLM during RL. Specifically, we employ Low-Rank Adaptation (LoRA) (Hu et al., 2022) for the VLM, facilitating its joint optimization with the action expert. We set the LoRA rank to = 32 and the scaling parameter to α = 32, while the action expert remains fully trainable. We conduct experiments with the π0 model with Flow-SDE on the LIBERO-Long benchmark, comparing three distinct configurations: 1) VLM frozen baseline (5 106 learning rate, 4 updates per epoch), 2) VLM LoRA-I (5 106 learning rate, 4 updates per epoch), and 3) VLM LoRA-II with conservative update training config (1 106 learning rate, 2 updates per epoch). As presented in Fig. 15, the VLM LoRA-II configuration achieves learning trajectory comparable to the VLM frozen baseline. This empirical observation yields two critical inferences: First, the benefit of finetuning the VLM on the LIBERO benchmark is not evident; Second, fine-tuning VLM together with the action expert requires more conservative optimization configuration for training stability. We conjecture the limited performance gain attributable to the limited scene variability within LIBERO, for which the pretrained VLM representations are already sufficiently robust."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We introduce πRL, the first framework that enables flow-based VLAs, π0 and π0.5, to be fine-tuned with PPO. We tackle the fundamental challenge of intractable log-likelihoods in flow matching"
        },
        {
            "title": "Preprint Version",
            "content": "(a) Eval (b) KL Divergence Figure 15: Ablation study on VLM Effectiveness during RL. The experiment is conducted with Flow-SDE π0 on the LIBERO-Long benchmark. We compare the performance of frozen VLM baseline (learning rate 5106, 4 updates per epoch) against two LoRA-tuned VLM configurations: LoRA-I (using the same training config) and LoRA-II (a more conservative setting with learning rate 1 106 and 2 updates per epoch). and propose two technical solutions, Flow-Noise and Flow-SDE, which differ in their stochasticity injection strategies and MDP formulations. Our extensive experiments on the challenging LIBERO and ManiSkill benchmarks demonstrated that πRL achieves significant performance improvements over SFT baselines."
        },
        {
            "title": "7 LIMITATIONS AND FUTURE WORK",
            "content": "Noise Injection. Our current noise injection strategy exhibits some train performance drop during the ODE-to-SDE conversion. Flow-CPS (Wang & Yu, 2025) attributes this loss to numerical error and proposes an improved coefficients-preserving sampling method. In our experiments, we attempted this configuration. Consistent with our hyperparameter ablation, our experiments showed that while this configuration mitigated the ODE-SDE precision error, it yielded limited RL improvement. Nevertheless, we argue that improving the noise injection strategy holds significant potential, specifically converting the ODE formulation to an SDE formulation while preserving the action distribution undisturbed. Training Acceleration. Our current implementation of the mixed ODE-SDE rollout is simplistic in Flow-SDE, i.e., it randomly selects one denoising step as an SDE step, while all other steps remain ODE steps. We posit that future investigations into mixed ODE-SDE rollouts, leveraging advances in accelerating flow-based image generation (Li et al., 2025b; He et al., 2025; Liu et al., 2025a; Li et al., 2025c), could further enhance Flow-SDE, leading to faster training and improved performance. Generalization. Our experiments in the Maniskill OOD tests indicate that the semantic generalization capabilities of the SFT and RL models remain limited. We aim to investigate and improve this issue in future studies. Real-world Experiment. Our current experiments are evaluated solely in simulated environments, lacking empirical validation in physical system. We plan to extend this research by applying our RL methodology to real-world tasks in the future."
        },
        {
            "title": "REFERENCES",
            "content": "Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. Johan Bjorck, Fernando Castañeda, Nikita Cherniadev, Xingye Da, Runyu Ding, Linxi Fan, Yu Fang, Dieter Fox, Fengyuan Hu, Spencer Huang, et al. Gr00t n1: An open foundation model for generalist humanoid robots. arXiv preprint arXiv:2503.14734, 2025. Kevin Black, Noah Brown, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, Lachy Groom, Karol Hausman, Brian Ichter, et al. π0: vision-language-action flow model for general robot control. arXiv preprint arXiv:2410.24164, 2024. Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, et al. Rt-1: Robotics transformer for real-world control at scale. arXiv preprint arXiv:2212.06817, 2022. Ricky TQ Chen, Yulia Rubanova, Jesse Bettencourt, and David Duvenaud. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. Cheng Chi, Zhenjia Xu, Siyuan Feng, Eric Cousineau, Yilun Du, Benjamin Burchfiel, Russ Tedrake, and Shuran Song. Diffusion policy: Visuomotor policy learning via action diffusion. The International Journal of Robotics Research, 44(10-11):16841704, 2025. Muhayy Ud Din, Waseem Akram, Lyes Saad Saoud, Jan Rosell, and Irfan Hussain. Vision language action models in robotic manipulation: systematic review. arXiv preprint arXiv:2507.10672, 2025. Senyu Fei, Siyin Wang, Junhao Shi, Zihao Dai, Jikun Cai, Pengfang Qian, Li Ji, Xinzhe He, Shiduo Zhang, Zhaoye Fei, et al. Libero-plus: In-depth robustness analysis of vision-language-action models. arXiv preprint arXiv:2510.13626, 2025. Roya Firoozi, Johnathan Tucker, Stephen Tian, Anirudha Majumdar, Jiankai Sun, Weiyu Liu, Yuke Zhu, Shuran Song, Ashish Kapoor, Karol Hausman, et al. Foundation models in robotics: Applications, challenges, and the future. The International Journal of Robotics Research, 44(5): 701739, 2025. Runlin Guo, Xinsong Lin, Minghua Liu, Jiayuan Gu, and Hao Su. Mplib: lightweight motion planning library. https://github.com/haosulab/MPlib, 2025a. URL https:// motion-planning-lib.readthedocs.io/latest/. Yanjiang Guo, Jianke Zhang, Xiaoyu Chen, Xiang Ji, Yen-Jen Wang, Yucheng Hu, and Jianyu Chen. Improving vision-language-action model with online reinforcement learning. arXiv preprint arXiv:2501.16664, 2025b. Xiaoxuan He, Siming Fu, Yuke Zhao, Wanli Li, Jian Yang, Dacheng Yin, Fengyun Rao, and Bo Zhang. Tempflow-grpo: When timing matters for grpo in flow models. arXiv preprint arXiv:2508.04324, 2025. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Michael Hutchinson. stochastic estimator of the trace of the influence matrix for laplacian smoothing splines. Communications in Statistics-Simulation and Computation, 18(3):10591076, 1989. Physical Intelligence, Kevin Black, Noah Brown, James Darpinian, Karan Dhabalia, Danny Driess, Adnan Esmail, Michael Equi, Chelsea Finn, Niccolo Fusai, et al. π0.5: vision-language-action model with open-world generalization. arXiv preprint arXiv:2504.16054, 2025. Alexander Khazatsky, Karl Pertsch, Suraj Nair, Ashwin Balakrishna, Sudeep Dasari, Siddharth Karamcheti, Soroush Nasiriany, Mohan Kumar Srirama, Lawrence Yunliang Chen, Kirsty ElarXiv preprint lis, et al. Droid: large-scale in-the-wild robot manipulation dataset. arXiv:2403.12945, 2024."
        },
        {
            "title": "Preprint Version",
            "content": "Moo Jin Kim, Karl Pertsch, Siddharth Karamcheti, Ted Xiao, Ashwin Balakrishna, Suraj Nair, Rafael Rafailov, Ethan Foster, Grace Lam, Pannag Sanketi, et al. Openvla: An open-source vision-language-action model. arXiv preprint arXiv:2406.09246, 2024. Moo Jin Kim, Chelsea Finn, and Percy Liang. Fine-tuning vision-language-action models: Optimizing speed and success. arXiv preprint arXiv:2502.19645, 2025. Wouter Kool, Herke Van Hoof, and Max Welling. Attention, learn to solve routing problems! arXiv preprint arXiv:1803.08475, 2018. Haozhan Li, Yuxin Zuo, Jiale Yu, Yuhao Zhang, Zhaohui Yang, Kaiyan Zhang, Xuekai Zhu, Yuchen Zhang, Tianxing Chen, Ganqu Cui, et al. Simplevla-rl: Scaling vla training via reinforcement learning. arXiv preprint arXiv:2509.09674, 2025a. Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, and Zhao Zhong. Mixgrpo: Unlocking flow-based grpo efficiency with mixed ode-sde. arXiv preprint arXiv:2507.21802, 2025b. Xuanlin Li, Kyle Hsu, Jiayuan Gu, Karl Pertsch, Oier Mees, Homer Rich Walke, Chuyuan Fu, Ishikaa Lunawat, Isabel Sieh, Sean Kirmani, et al. Evaluating real-world robot manipulation policies in simulation. arXiv preprint arXiv:2405.05941, 2024. Yuming Li, Yikai Wang, Yuying Zhu, Zhongyu Zhao, Ming Lu, Qi She, and Shanghang Zhang. Branchgrpo: Stable and efficient grpo with structured branching in diffusion models. arXiv preprint arXiv:2509.06040, 2025c. Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Bo Liu, Yifeng Zhu, Chongkai Gao, Yihao Feng, Qiang Liu, Yuke Zhu, and Peter Stone. Libero: Benchmarking knowledge transfer for lifelong robot learning. Advances in Neural Information Processing Systems, 36:4477644791, 2023. Jie Liu, Gongye Liu, Jiajun Liang, Yangguang Li, Jiaheng Liu, Xintao Wang, Pengfei Wan, Di Zhang, and Wanli Ouyang. Flow-grpo: Training flow matching models via online rl. arXiv preprint arXiv:2505.05470, 2025a. Jijia Liu, Feng Gao, Bingwen Wei, Xinlei Chen, Qingmin Liao, Yi Wu, Chao Yu, and Yu Wang. What can rl bring to vla generalization? an empirical study. arXiv preprint arXiv:2505.19789, 2025b. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. Guanxing Lu, Wenkai Guo, Chubin Zhang, Yuheng Zhou, Haonan Jiang, Zifeng Gao, Yansong Tang, and Ziwei Wang. Vla-rl: Towards masterful and general robotic manipulation with scalable reinforcement learning. arXiv preprint arXiv:2505.18719, 2025. Max Sobol Mark, Tian Gao, Georgia Gabriela Sampaio, Mohan Kumar Srirama, Archit Sharma, Chelsea Finn, and Aviral Kumar. Policy agnostic rl: Offline rl and online rl fine-tuning of any class and backbone. arXiv preprint arXiv:2412.06685, 2024. David McAllister, Songwei Ge, Brent Yi, Chung Min Kim, Ethan Weber, Hongsuk Choi, Haiwen Feng, and Angjoo Kanazawa. Flow matching policy gradients. arXiv preprint arXiv:2507.21053, 2025. Reginald McLean, Evangelos Chatzaroulas, Luc McCutcheon, Frank Röder, Tianhe Yu, Zhanpeng He, KR Zentner, Ryan Julian, JK Terry, Isaac Woungang, et al. Meta-world+: An improved, standardized, rl benchmark. arXiv preprint arXiv:2505.11289, 2025. Atharva Mete, Haotian Xue, Albert Wilcox, Yongxin Chen, and Animesh Garg. Quest: Selfsupervised skill abstractions for learning continuous control. Advances in Neural Information Processing Systems, 37:40624089, 2024."
        },
        {
            "title": "Preprint Version",
            "content": "Abby ONeill, Abdul Rehman, Abhiram Maddukuri, Abhishek Gupta, Abhishek Padalkar, Abraham Lee, Acorn Pooley, Agrim Gupta, Ajay Mandlekar, Ajinkya Jain, et al. Open x-embodiment: Robotic learning datasets and rt-x models: Open x-embodiment collaboration 0. In 2024 IEEE International Conference on Robotics and Automation (ICRA), pp. 68926903. IEEE, 2024. Karl Pertsch, Kyle Stachowicz, Brian Ichter, Danny Driess, Suraj Nair, Quan Vuong, Oier Mees, Chelsea Finn, and Sergey Levine. Fast: Efficient action tokenization for vision-language-action models. arXiv preprint arXiv:2501.09747, 2025. Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36:5372853741, 2023. Allen Ren, Justin Lidard, Lars Ankile, Anthony Simeonov, Pulkit Agrawal, Anirudha Majumdar, Benjamin Burchfiel, Hongkai Dai, and Max Simchowitz. Diffusion policy policy optimization. arXiv preprint arXiv:2409.00588, 2024. John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. HigharXiv preprint dimensional continuous control using generalized advantage estimation. arXiv:1506.02438, 2015. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Mustafa Shukor, Dana Aubakirova, Francesco Capuano, Pepijn Kooijmans, Steven Palma, Adil Zouitine, Michel Aractingi, Caroline Pascal, Martino Russi, Andres Marafioti, et al. Smolvla: vision-language-action model for affordable and efficient robotics. arXiv preprint arXiv:2506.01844, 2025. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. Shuhan Tan, Kairan Dou, Yue Zhao, and Philipp Krähenbühl. Interactive post-training for visionlanguage-action models. arXiv preprint arXiv:2505.17016, 2025. Stone Tao, Fanbo Xiang, Arth Shukla, Yuzhe Qin, Xander Hinrichsen, Xiaodi Yuan, Chen Bao, Xinsong Lin, Yulin Liu, Tse-kai Chan, et al. Maniskill3: Gpu parallelized robotics simulation and rendering for generalizable embodied ai. arXiv preprint arXiv:2410.00425, 2024. Octo Model Team, Dibya Ghosh, Homer Walke, Karl Pertsch, Kevin Black, Oier Mees, Sudeep Dasari, Joey Hejna, Tobias Kreiman, Charles Xu, et al. Octo: An open-source generalist robot policy. arXiv preprint arXiv:2405.12213, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. Andrew Wagenmaker, Mitsuhiko Nakamoto, Yunchu Zhang, Seohong Park, Waleed Yagoub, Anusha Nagabandi, Abhishek Gupta, and Sergey Levine. Steering your diffusion policy with latent space reinforcement learning. arXiv preprint arXiv:2506.15799, 2025. Feng Wang and Zihao Yu. Coefficients-preserving sampling for reinforcement learning with flow matching. arXiv preprint arXiv:2509.05952, 2025. Junjie Wen, Yichen Zhu, Jinming Li, Minjie Zhu, Zhibin Tang, Kun Wu, Zhiyuan Xu, Ning Liu, Ran Cheng, Chaomin Shen, et al. Tinyvla: Towards fast, data-efficient vision-language-action models for robotic manipulation. IEEE Robotics and Automation Letters, 2025."
        },
        {
            "title": "Preprint Version",
            "content": "Ronald Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine learning, 8(3):229256, 1992. Chao Yu, Yuanqing Wang, Zhen Guo, Hao Lin, Si Xu, Hongzhi Zang, Quanlu Zhang, Yongji Wu, Chunyang Zhu, Junhao Hu, et al. Rlinf: Flexible and efficient large-scale reinforcement learning via macro-to-micro flow transformation. arXiv preprint arXiv:2509.15965, 2025. Hongzhi Zang, Mingjie Wei, Si Xu, Yongji Wu, Zhen Guo, Yuanqing Wang, Hao Lin, Liangzhi Shi, Yuqing Xie, Zhexuan Xu, et al. Rlinf-vla: unified and efficient framework for vla+ rl training. arXiv preprint arXiv:2510.06710, 2025. Tonghe Zhang, Chao Yu, Sichang Su, and Yu Wang. Reinflow: Fine-tuning flow matching policy with online reinforcement learning. arXiv preprint arXiv:2505.22094, 2025."
        },
        {
            "title": "A ALGORITHM DETAILS",
            "content": "GRPO is critic-free method that estimates the advantage by normalizing rewards within group of rollouts from the same state. In our robotics MDP task, for each initial state, we use the policy πθ to sample group of trajectories, resulting in sparse terminal rewards {R(j)}G j=1 denoting the binary success of the task. The advantage for the i-th trajectory, ˆA(i), is then calculated based on the group-wise reward normalization: ˆA(i) = R(i) mean({R(j)}G std({R(j)}G j=1) j=1) (17) where R(i) is the terminal reward for the i-th trajectory, and the mean and standard deviation are computed over the group of trajectories. Since the reward is only granted at the end of an episode, the advantage estimate remains constant across all timesteps within that trajectory."
        },
        {
            "title": "B EXPERIMENT DETAILS",
            "content": "We record the training hyperparameters used to train both π0 and π0.5 on each LIBERO task, and present them in Tabs. 7 and 8. Table 7: Hyperparameters of Flow-Noise and Flow-SDE with PPO across LIBERO tasks. Parameters Train epochs Batch size Update epochs Actor lr Critic lr Scheduler Reward discount rate γ GAE λ Clip ratio ϵ Interaction steps Parallel environments Rollout epochs Action chunk Denoise steps Noise level σ (Flow-SDE) Max log-var (Flow-Noise) Min log-var (Flow-Noise) Entropy bonus (Flow-Noise) Algorithms and tasks π0 Spatial Object Goal Long π0.5 Spatial Object Goal Long 400 2048 2 1e-5 1e-4 False 0.99 0.95 0. 240 64 8 5 4 0.5 0.16 0.08 0.005 400 2048 2 5e-6 1e-4 False 0.99 0.95 0.2 320 64 8 5 4 0.5 0.16 0.08 0. 400 2048 4 5e-6 1e-4 False 0.99 0.95 0.2 320 64 8 5 4 0.5 0.16 0.08 0.005 400 2048 4 5e-6 1e-4 False 0.99 0.95 0. 480 64 8 10 4 0.5 0.16 0.08 0.005 400 2048 1 5e-6 1e-4 False 0.99 0.95 0.2 240 64 8 5 3 0.5 0.10 0.04 0. 400 2048 1 5e-6 1e-4 False 0.99 0.95 0.2 320 64 8 5 5 0.3 0.10 0.04 0.005 400 2048 3 5e-6 1e-4 False 0.99 0.95 0. 320 64 8 5 5 0.3 0.10 0.04 0.005 400 2048 4 5e-6 1e-4 True 0.99 0.95 0.2 480 64 8 10 5 0.5 0.10 0.04 0. ADDITIONAL RESULTS: RL FOR GR00T N1.5 C.1 SETUP GR00T N1.5. We conduct additional experiments based on the GR00T N1.5 model. GR00T N1.5 is an open-source foundation model 8 tailored for generalist humanoid robot reasoning and manipulation. Designed to enable cross-embodiment adaptability, the model accepts multimodal inputsincluding natural language instructions and visual observationsand integrates robot state information (e.g., joint positions, end-effector poses) to generate continuous motor actions for diverse tasks and environments. Its neural architecture combines vision-language model (Eagle 2.5) 8https://github.com/NVIDIA/Isaac-GR00T"
        },
        {
            "title": "Preprint Version",
            "content": "Table 8: Flow-Noise and Flow-SDE Hyperparameters of in ManiSkill tasks. Parameters SFT train steps RL train steps Global Batch size Update epochs Actor lr Critic lr Scheduler Reward discount rate γ GAE λ Clip ratio ϵ Interaction steps Parallel environments Rollout epochs Action prediction horizon Action replan horizon Denoise steps Noise level σ (Flow-SDE) Max log-var (Flow-Noise) Min log-var (Flow-Noise) Entropy bonus (Flow-Noise) Algorithms and tasks π0 π0.5 Eggplant Carrot Spoon Cube Generalization Eggplant Carrot Spoon Cube Generalization 1000 40 2560 4 5.6e-6 1.1e-4 False 0.99 0.95 0.2 48 256 1 8 5 4 0.5 0.16 0.08 0.005 100040 40 2560 4 5.6e-6 1.1e-4 False 0.99 0.95 0. 48 256 1 8 5 4 0.5 0.16 0.08 0.005 1000 40 2560 4 5.6e-6 1.1e-4 False 0.99 0.95 0.2 48 256 1 8 5 4 0.5 0.16 0.08 0. 1000 130 2560 4 5.6e-6 1.1e-4 False 0.99 0.95 0.2 48 256 1 8 5 4 0.5 0.16 0.08 0.005 1000 150 5120 4 7.91e-6 1.55e-4 False 0.99 0.95 0. 48 320 1 8 5 4 0.5 0.16 0.08 0.005 1000 40 2560 4 5.6e-6 1.1e-4 False 0.99 0.95 0.2 48 256 1 8 5 4 0.5 0.10 0.04 0. 1000 40 2560 4 5.6e-6 1.1e-4 False 0.99 0.95 0.2 48 256 1 8 5 4 0.5 0.10 0.04 0.005 1000 40 2560 4 5.6e-6 1.1e-4 False 0.99 0.95 0. 48 256 1 8 5 4 0.5 0.10 0.04 0.005 1000 70 2560 4 5.6e-6 1.1e-4 False 0.99 0.95 0.2 48 256 1 8 5 4 0.5 0.10 0.04 0. 1000 150 5120 5 7.91e-6 1.55e-4 False 0.99 0.95 0.2 48 320 1 8 5 4 0.5 0.10 0.04 0.005 Table 9: π0, π0.5 Generalization Test Environment Variation-Version-Type π0-SFT π0-RL π0-RL π0.5-SFT Flow-SDE Flow-Noise π0.5-RL Flow-SDE Flow-Noise π0.5-RL In distribution Main-v3-train Visual-language Variations Semantic Reasoning (object/receptacle confounders) Instruct-v1-test VisionImage-v1-test VisionTexture03-v1-test VisionTexture05-v1-test VisionWhole03-v1-test VisionWhole05-v1-test MultiCarrot-v1-test MultiCarrot-v1-train MultiPlate-v1-test MultiPlate-v1-train Action Execution PositionChangeTo-v1-test Position-v1-test 38. 30.10 38.33 35.10 31.04 35.42 28.54 7.81 12.50 5.00 7.29 9.58 16.88 78.83 64.58 68.75 66.04 55.83 62.40 48.96 28.23 36.46 16.35 20. 17.40 45.63 77.76 66.46 71.67 66.77 60.52 68.96 53.85 23.02 31.77 18.33 19.58 10.94 37.50 40. 46.56 46.25 36.67 32.71 40.10 30.73 16.67 28.23 11.77 9.69 13.54 31.15 90.85 76.98 78.75 69.58 58.02 69.58 55.00 36.77 49.48 29.38 22. 36.25 54.48 89.65 85.73 83.13 75.00 62.19 71.56 56.98 38.23 50.10 28.33 25.42 54.69 55.00 optimized for grounding and physical understanding with diffusion transformer (DiT) head for action denoising. It supports multiple robot embodiments via specialized headsincluding humanoid robots with dexterous hands, single-arm robots, and humanoid robots with grippers. For the critic placement, we estimate the value across the entire denoising trajectory, attaching the critic network to the action head. The figure illustration is shown in Fig. 16. Benchmark. We use LIBERO as the evaluation benchmark. Evaluates the model performance across four manipulation task suites: Spatial, Object, Goal, and Long. Implementation Details. Similar to the π0 implementation, we initiate our process with SFT on expert demonstrations. For the SFT stage, we fine-tune the entire model following the official setting. In the subsequent RL stage, we exclusively fine-tune the action expert model while keeping the vision-language model parameters fixed. critical methodological detail during the RL stage is the replacement of the original dropout layers in the expert model with identity layers. Dropout layers are known to destabilize online RL training. Specifically, they introduce an unintended change in the effective policy, transforming the stable probability ratio update from: to the unstable form: ρt(θ) = πθnew(atst) πθold(atst) ρt(θ) = παnew (atst) πθold (atst) , where ρt(θ) is the probability ratio, and αnew signifies the policy after the update and the nondeterministic effect of the dropout mask. This stochasticity, which effectively changes the models"
        },
        {
            "title": "Preprint Version",
            "content": "Figure 16: Illustration for the architecture of GR00T-N1.5. Table 10: Comparison of the PPO and GRPO with Flow-SDE on the LIBERO. Model GR00T LIBERO Spatial Object SFT +PPO 41.4 92.5 58.6 96.2 Goal 48.2 84. Long 61.9 86.6 Avg. 52.5 89.9 Avg. +37. structure in addition to the per-step policy update, significantly deteriorates training stability. For the model configurations, we adhere to the official settings. Our experiments are conducted on 8 NVIDIA H100 80GB GPUs, and detailed training hyperparameters are exactly the same as π0, listed in Tab. 7. The RL training method selected is Flow-SDE. C.2 RESULTS This evaluation adopts all the hyperparameter settings of π0 without further tuning to show the broad applicability and robustness of the proposed RL training method. The evaluation results are shown in Tab. 10 For the few-shot model, the SFT baseline performs poorly, with an average success rate of only 52.5%, indicating that the model struggles with limited demonstration data. Our proposed RL training method substantially boosts performance, reaching 89.9% with Flow-SDE. The results presented above utilize the identical hyperparameter settings as π0. These findings primarily serve to demonstrate the broad applicability and inherent robustness of the proposed RL training methodology. Further optimization through parameter tuning is likely to yield enhanced model performance."
        }
    ],
    "affiliations": [
        "Carnegie Mellon University",
        "Infinigence AI",
        "Institute of Automation, Chinese Academy of Sciences",
        "Peking University",
        "Tsinghua University",
        "Zhongguancun Academy"
    ]
}