{
    "paper_title": "Batch Speculative Decoding Done Right",
    "authors": [
        "Ranran Haoran Zhang",
        "Soumik Dey",
        "Ashirbad Mishra",
        "Hansi Wu",
        "Binbin Li",
        "Rui Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Speculative decoding speeds up LLM inference by using a small draft model to propose multiple tokens that a target model verifies in parallel. Extending this idea to batches is essential for production serving, but it introduces the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, breaking right-alignment and corrupting position IDs, attention masks, and KV-cache state. We show that several existing batch implementations violate output equivalence-the fundamental requirement that speculative decoding must produce identical token sequences to standard autoregressive generation. These violations occur precisely due to improper handling of the ragged tensor problem. In response, we (1) characterize the synchronization requirements that guarantee correctness, (2) present a correctness-first batch speculative decoding EQSPEC that exposes realignment as consuming 40% of overhead, and (3) introduce EXSPEC, which maintains a sliding pool of sequences and dynamically forms same-length groups, to reduce the realignment overhead while preserving per-sequence speculative speedups. On the SpecBench dataset, across Vicuna-7B/68M, Qwen3-8B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our approach achieves up to 3$\\times$ throughput improvement at batch size 8 compared to batch size 1, with efficient scaling through batch size 8, while maintaining 95% output equivalence. Our method requires no custom kernels and integrates cleanly with existing inference stacks. Our code is available at https://github.com/eBay/spec_dec."
        },
        {
            "title": "Start",
            "content": "Ranran Haoran Zhang Binbin Li Rui Zhang The Pennsylvania State University {hzz5361, rmz5227}@psu.edu, eBay Inc Soumik Dey Ashirbad Mishra Hansi Wu {sodey, ashirmishra, hanswu, binbli}@ebay.com 5 2 0 2 6 ] . [ 1 6 7 8 2 2 . 0 1 5 2 : r a"
        },
        {
            "title": "ABSTRACT",
            "content": "Speculative decoding speeds up LLM inference by using small draft model to propose multiple tokens that target model verifies in parallel. Extending this idea to batches is essential for production serving, but it introduces the ragged tensor problem: sequences in the same batch accept different numbers of draft tokens, breaking right-alignment and corrupting position IDs, attention masks, and KVcache state. We show that several existing batch implementations violate output equivalencethe fundamental requirement that speculative decoding must produce identical token sequences to standard autoregressive generation. These violations occur precisely due to improper handling of the ragged tensor problem. In response, we (1) characterize the synchronization requirements that guarantee correctness, (2) present correctness-first batch speculative decoding EQSPEC that exposes realignment as consuming 40% of overhead, and (3) introduce EXSPEC, which maintains sliding pool of sequences and dynamically forms same-length groups, to reduce the realignment overhead while preserving per-sequence speculative speedups. On the SpecBench dataset, across Vicuna-7B/68M, Qwen38B/0.6B, and GLM-4-9B/0.6B target/draft pairs, our approach achieves up to 3 throughput improvement at batch size 8 compared to batch size 1, with efficient scaling through batch size 8, while maintaining 95% output equivalence. Our method requires no custom kernels and integrates cleanly with existing inference stacks. Our code is available at https://github.com/eBay/spec dec."
        },
        {
            "title": "INTRODUCTION",
            "content": "Speculative decoding (Leviathan et al., 2023; Chen et al., 2023) accelerates LLM inference by using small draft model to propose multiple tokens that the target model verifies in parallel, shifting from memory-bound sequential generation to compute-intensive verification and delivering singlesequence speedups (Xia et al., 2024). Batch speculative decoding aims to combine this per-sequence acceleration with standard batching by processing multiple sequences (batch dimension) while verifying multiple draft tokens per sequence (sequence dimension). The core challenge is the raggedtensor effect (Qian et al., 2024): in each verification round, sequences accept single series of the proposed draft tokens, but prefix lengths differ across sequences (e.g., one accepts five tokens while another accepts one), misaligning sequence lengths. This raggedness violates the rectangular-tensor assumption required for GPU-parallel execution. Figure 1 highlights critical, often overlooked issue in batch speculative decoding: methods with impressive throughput can produce corrupted outputs. For lossless acceleration, speculative decoding must yield identical outputs to standard autoregressive generation (Leviathan et al., 2023). As reference point, the widely used HuggingFace implementation (Wolf et al., 2020) preserves this guarantee, but only for batch size 1. By contrast, in our tests of public batch implementationsspecifically, BSP (Su et al., 2023) and DSD (Yan et al., 2025)this requirement is violated at batch sizes > 1, manifesting as repetitive tokens or <unk> symbols under greedy decoding rather than matching standard generation. These failures share single root causebroken synchronization invariants (position tracking, attention, KV-cache) across ragged tensors. We first propose EQSPEC by formalizing these invariants and enforcing them with synchronization-aware scheduling (Section 3.1). Concretely, EQSPEC specifies the minimal synchronization needed for correctness and shows that realignment accounts for 40% 1 YES, High Throughput... BUT, Output Gibberish! Method Generated Output Prompt: The weather today is Ground Truth not as bad as it was yesterday... Spec-1 BSP DSD EQSPEC EXSPEC Matches ground truth not not not not not not not... unkunk The perfect time... Matches ground truth Matches ground truth Figure 1: Batch speculative decoding on Vicuna-7B/68M: Existing methods achieve high throughput but violate the fundamental requirement of output equivalence by producing corrupted outputs. Our approach maintains perfect correctness while still achieving competitive performance. of computationan inherent cost of maintaining invariants across ragged tensors. This fundamental overhead helps explain why even production systems struggle: vLLMs speculative decoding underperforms its non-speculative baseline at higher batch sizes (leading to deprecation in its v1 engine), while SGLang with EAGLE (Li et al., 2024a) consistently exhibits negative speedups. Our analysis indicates that the superlinear growth of synchronization overhead is an inevitable cost of correctness in batch speculative decodinga barrier no existing system has overcome. To address this in practice, our main algorithm EXSPEC (Section 3.2) expands the scheduling scope: it maintains sliding window of active sequences and dynamically groups those with identical lengths, eliminating realignment for homogeneous groups. This strategy preserves the scaling efficiency of standard batchingwhere GPU parallelism drives throughputwhile retaining the per-sequence acceleration benefits of speculation. At batch size 8, we achieve 3 throughput improvement over batch size 1. Nevertheless, beyond batch size 8, throughput degrades as grouping success rates decline, forcing more frequent fallbacks to expensive realignment. Section 4 examines these scaling dynamics and the relationship between sequence diversity, grouping effectiveness, and alignment overhead. Our experiments on SpecBench (Xia et al., 2024) yield two main results. First, Correctness: unlike prior approaches such as BSP and DSD, which suffer severe output corruption, our method preserves approximately 95% output equivalence across Vicuna-7B/68M (Zheng et al., 2023), Qwen38B/0.6B (Yang et al., 2025), and GLM-4-9B/0.6B (GLM et al., 2024) model pairs. Second, Scalability: at batch size 8, EXSPEC achieves up to 3 speedup over batch size 1. Our contributions are summarized as follows: We provide correctness-first analysis of the ragged-tensor problem in batch speculative decoding, identifying precise synchronization requirements for correctness and explaining why existing methods fail (Section 2). We present unified solution that maintains correctness through precise synchronization invariants while avoiding their overhead via cross-batch scheduling of same-length sequence groups (Section 3). We experimentally demonstrate that our approach achieves both >95% output correctness and positive scaling through batch size 8, successfully multiplying batch parallelism with per-sequence speculation gains, whereas production systems (vLLM, SGLang) exhibit negative speedups (Section 4)."
        },
        {
            "title": "2 DESIGN SPACE ANALYSIS",
            "content": "When sequences within batch accept different numbers of draft tokens during verification, tensors become irregular, violating GPUs requirement for rectangular layoutsthis is the ragged-tensor problem illustrated in Figure 2. Despite batchings centrality to production deployments, existing implementations lack principled design that preserves output equivalence with standard decoding while scaling with batch size. We identify three approaches to handle raggedness: Masking, 2 Figure 2: The ragged tensor problem in batch speculative decoding. Differing numbers of accepted draft tokens across sequences in the same batch lead to ragged-shaped input IDs tensors and KV Cache that disrupt subsequent batch operations. Rollback, and Dynamic Padding. Yet, as our systematic analysis shows, current instantiations of these approaches fail to simultaneously maintain correctness and performance at scale. To close this gap, we first analyze the pitfalls of each approach and then introduce correctness-first algorithmic design with explicit synchronization requirements for reliable batch speculative decoding. Masking Approach (non-contiguous position IDs). This approach operates directly on ragged tensors by masking rejected tokens in attention and reassigning position IDs so new tokens align with their content positions. Across verification rounds with varying rejections, sequences accumulate padding in various positions (middle and right), forming non-contiguous position IDs that standard Transformer implementations handle poorly. BSP (Su et al., 2023) attempts this via masking but fails to maintain position-ID consistency across iterations, yielding corrupted outputs (Figure 1). EAGLEs experimental batching code1 (Li et al., 2025) encounters similar framework limitations. Supporting non-contiguous position IDs would require custom CUDA kernels for each base model (Qian et al., 2024)a prohibitive engineering cost that sacrifices portability. Rollback Approach (speculation waste). After each verification step, all sequences are truncated to the batchs minimum accepted length (Wolf et al., 2020; kamilakesbi, 2024). This guarantees alignment but discards correctly verified tokens from faster sequences. As batch size grows and acceptance-rate variance widens, the waste compounds; in the extreme, one persistently rejecting sequence forces single-token progress for the entire batch. In effect, throughput collapses to that of the worst-performing sequence, undermining speculative gains and rendering the approach impractical at larger scales. Dynamic Padding Approach. This approach realigns sequences after each verification by adjusting left padding to maintain right alignment, preserving all accepted tokens. While conceptually simple, correctness requires tight synchronization of position IDs, attention masks, and the KVcache. DSDs experimental code (Yan et al., 2025) follows this idea but merely repads at each It also stepadding varying left padding without ever unpaddingthereby inflating sequences. contains three critical errors: (i) sampling bonus tokens from the draft model rather than the target model; (ii) redundantly regenerating KV-cache entries, causing memory bloat; and (iii) desynchronizing padding, position IDs, and the KV-cache across iterations. Despite the overhead of repeated realignment, correct dynamic-padding implementation fits within standard frameworks and preserves all verified tokens. Among the three approaches, only dynamic padding is viable: position-ID schemes require custom kernels that undermine portability, and rollback discards verified tokens at rates that grow with batch size; dynamic padding maintains correctness within standard frameworks."
        },
        {
            "title": "3 METHOD",
            "content": "We present synchronization-aware approach to batch speculative decoding that co-designs correctness and efficiency. The core tension is that preserving correctness requires synchronizing posi1While EAGLEs main contribution concerns improved draft models rather than batching, its repository includes experimental batch-related code we analyzed for implementation challenges. https://github.com/ SafeAILab/EAGLE/issues/250 3 Algorithm 1 BatchVerify: Single Forward Pass Algorithm 2 EQSPEC Require: Target model Mt, Sequences S, Draft tokens D, KV cache Vectorized first mismatch detection Ensure: Accepted tokens A, Bonus tokens 1: if first iteration then 2: 3: else 4: 5: logits, KVCache Mt(X , KVCache) 6: pred tokens arg max(logits, dim=vocab) 7: 8: matches (pred tokens = D) 9: arg max(matches, dim=seq) 10: 11: for each sequence in batch do 12: 13: 14: 15: 16: return A, B, KV Cache bonus logit logits[i, S[i] + j] B[i] arg max(bonus logit) A[i] D[i][: j] Ragged shape acceptance, no vectorization Get bonus token from first mismatch Require: Draft model Md, Target model Mt, Prompts P, Max tokens , Draft length Batch left padding Ensure: Generated sequences 1: Tokenize(P) 2: KVCache 3: while until max new tokens do Phase 1: Draft Generation 4: Md.Generate(S, K) 5: Phase 2: Batch Verification 6: A, B, KVCache 7: BatchVerify(Mt, S, D, KVCache) See Figure 3 for illustration on index offset. Phase 3: Unpad-Append-Repad for each sequence in batch do S[i] Unpad(S[i]) S[i] S[i] A[i] B[i] S, offset BatchRepad(S) KVCache Realign(KVCache, offset) return 8: 9: 10: 11: 12: 13: 14: 15: tion IDs, attention masks, and the KV-cache across ragged tensorsan overhead that can consume 40% of computation. We introduce two complementary mechanisms: EQSPEC specifies and enforces the minimal synchronization invariants required for valid computation (Section 3.1), while EXSPEC groups same-length sequences to avoid synchronization overhead (Section 3.2). Together, these form unified system in which correctness constraints drive scheduling, enabling both output equivalence and practical performance. 3.1 MINIMAL BATCH REALIGNMENT: EQSPEC Figure 3 illustrates the core challenge and our remedy for maintaining correctness in batch speculative decoding. After each verification round, sequences accept different numbers of draft tokens, producing ragged tensors that GPUs cannot process. To restore valid batch, we apply an unpadappendrepad procedure that converts ragged outputs back to rectangular layout while preserving three invariants: contiguous position IDs, valid attention masks, and aligned KV-cache entries. to zero at Correct implementation requires padding-agnostic position IDs that reset the first content token, attention masks that exclude padding tokens, and precise KV-cache realignment after each verification step (Algorithm 1). After unpadappendrepad, index offsets shift across sequences; consequently, position IDs and attention masks must be recomputed to preserve correct token relationships. Moreover, the bonus token introduces special case: it is sampled from the target models output distribution at the first mismatch position after the verification forward pass has already completed. Consequently, while it encodes the targets authoritative correction, it lacks KV-cache entries in either the draft or target model because KV-cache is computed only for tokens that were present in the input sequence during the forward passand the bonus token, being an output, was not yet part of that input sequence. Therefore, it must be included in the next forward pass to create its KVcache entries, further complicating synchronization. Finally, the realignment is resource-intensive: the KV-cache consists of rank-4 tensors (batch heads sequence dimension), and each padding Figure 3: EQSPEC synchronizes via unpadappendrepad. 4 adjustment triggers allocation and concatenation of high-dimensional zeros. Algorithm 2 details the complete EQSPEC procedure. Speedup Analysis. The speedup of speculative decoding depends on both the token-acceptance rate and computational costs. The original formulation by Leviathan et al. (2023) analyzes single-sequence performance, modeling the expected tokens generated per iteration as (1 αk+1)/(1 α), where α is the token acceptance rate (TAR) and is the number of draft tokens per speculation round. This singlesequence view assumes negligible batch overhead and focuses purely on acceptance dynamics. complexities not introduces Batch speculative decoding, however, additional captured by that modelmost notably alignment overhead, which can dominate and degrade performance. We therefore introduce batch-aware speedup model: = α cdraft + cverify + coverhead(B) (1) where denotes speedup relative to non-speculative decoding (i.e., running the target model alone), cdraft and cverify are the relative costs of draft generation and target verification, and coverhead(B) captures batch-dependent alignment overhead absent from Figure 4: EXSPEC pools ragged sequences by length, avoiding realignment; only unmatched sequences need syncing, turning fixed overhead into optional cost. single-sequence analysis. Two observations follow. First, when sequences within batch share the same length, batch realignment overhead is negligible; likewise, with very small draft models or prompt lookahead decoding (Fu et al., 2024b), cdraft becomes small relative to verification. Second, and critically, coverhead(B) scales superlinearly with batch size due to the ragged-tensor problem (Section 4.3). This overhead decomposes as coverhead(B) = cpad(B) + ckv(B), where cpad(B) accounts for unpadappendrepad operations and ckv(B) for KV-cache realignment on rank-4 tensors. While cdraft and cverify benefit from GPU parallelism and remain relatively stable within the hardwares batch-parallel regime, the alignment overhead grows with both batch size and variance in acceptance rates across sequences. 3.2 CROSS-BATCH SCHEDULING: EXSPEC Profiling EQSPEC shows that alignment overhead consumes 39.4% of computation at batch size 8, rising to 46.7% at batch size 16. Because this cost is inherent to synchronizing ragged tensors, micro-optimizing the primitives yields limInstead of accelerating these operaited gains. tions, EXSPEC avoids them by scheduling. Algorithm 3 EXSPEC: Cross-Batch Scheduling Require: Draft and Target model Md, Mt, Prompts P, Window size , Batch size Tokenize and optionally sort by length Try same-length concatenation Fallback to Unpad-Repad Realignment Phase 1: Lazy Realignment B, mask, KV GetBatch(Window, B) Ensure: Generated sequences 1: Pool InitSequencePool(P) 2: 3: Window RefillWindow(Pool, ) 4: while Pool.hasActive() do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: Phase 2: Draft Generation Md.Generate(B, mask, K) Phase 3: Batch Verification A, B, KV BatchVerify(Mt, B, D, KV) Phase 4: Write-Back and Window Refill for do Pool[i] Pool[i] A[i] B[i] Pool.KV[i] KV[i] if isComplete(Pool[i]) then Pool.deactivate(i) 18: 19: Window RefillWindow(Pool, ) return Pool.sequences EXSPEC differs from EQSPEC in how it manages sequence lifecycles. Rather than maintaining fixed batches that require realignment after every verification, we introduce SequencePool that holds sequences individually in their ragged states. This enables three optimizations: (i) sequences that 5 complete (reach EOS) are immediately removed, avoiding wasted computation on finished items; (ii) lazy realignment defers synchronization until strictly necessary, keeping sequences in their natural ragged form between steps; and (iii) dynamic batch formation over sliding window of > sequences greatly increases the chance of finding same-length groups that can be concatenated without any realignment. Figure 4 illustrates the flow. After verification, ragged sequences return directly to the pool without realignment. The scheduler then scans the active window and attempts to form batches of identical length. Same-length sequences concatenate directlyno padding adjustments, no position-ID recomputation, no KV-cache realignmentbypassing coverhead(B). Only when same-length grouping fails do we fall back to the expensive unpadappendrepad procedure from EQSPEC. Algorithm 3 formalizes this in four phases: dynamic batch formation (prioritizing same-length groups), draft generation, verification, and pool write-back with continuous window refresh. Combined with prompt-length sorting, grouping rates approach unity for similar workloads, turning the worst case (constant realignment) into the rare case."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We evaluate EQSPEC and EXSPEC along two dimensions often conflated in prior work: correctness (whether outputs match non-speculative generation) and throughput (tokens per second). Through systematic evaluation across three model families and comparisons with both research prototypes and production systems, we show that our approach uniquely preserves output equivalence while achieving competitive speedups. 4.1 EXPERIMENTAL SETUP Models. To demonstrate generality, we evaluate three targetdraft pairs: Vicuna-7B/68M (Zheng et al., 2023), Qwen3-8B/0.6B (Yang et al., 2025), and GLM-4-9B/0.6B (GLM et al., 2024). Unless otherwise noted, experiments use NVIDIA A100 80GB GPUs, PyTorch 2.7, HuggingFace Transformers 4.51.3, five draft tokens per speculation round, and greedy decoding for determinism. Evaluation and Datasets. We use SpecBench (Xia et al., 2024), focusing on the first turn of each conversation because our evaluation targets offline batch inference rather than multi-turn interaction. We also use Multi30k (Elliott et al., 2016) for controlled EXSPEC study that contrasts random sampling with an identical-length subset, isolating sequence-length diversity as the driver of grouping rate. For the main evaluation, we measure: (1) Throughput: tokens/s across batch sizes; and (2) Correctness: exact-match rate (full-sequence equivalence with non-speculative decoding) and partial-match rate (fraction of tokens matching until the first divergence). The partial-match metric helps localize failure modesearly divergence typically indicates position-ID or KV-cache misalignment. Batch Speculative Decoding Compared. Following our design-space taxonomy (Section 2), we evaluate: (1) Masking approaches: BSP (Su et al., 2023) attempts masking with adaptive speculation but suffers position-ID inconsistencies (BASS (Qian et al., 2024) also follows this approach but requires custom CUDA kernels, limiting generality); (2) Dynamic-padding approaches: DSD (Yan et al., 2025) explores padding but mishandles the KV-cache, while EQSPEC implements correct synchronization and EXSPEC adds cross-batch scheduling; (3) Reference baselines: Spec-1 (batchsize-1 speculation from Hugging Face Transformers), which does not support batch speculative decoding2. We also compare with production systems vLLM3 (Kwon et al., 2023) (which subsumes TETRIS (Wu et al., 2025) and TurboSpec (Liu et al., 2025) as vLLM forks) and SGLangEAGLE4 (Zheng et al., 2024; Li et al., 2024b; 2025), noting that their continuous batching and memory-management designs complicate direct comparison. We further test SGLang-EAGLEDeterministic (SGLang Team, 2025), which enables deterministic execution to reduce numerical variance. No existing implementation uses rollback due to its inherent wastefulness. 2https://github.com/huggingface/transformers/issues/32165 3We use the vLLM v0 engine because v1 deprecates speculative decoding. 4SGLang is compatible only with the EAGLE family; we compare Vicuna-7B/EAGLE2 and Qwen38B/EAGLE3. There are no available weights for GLM-4. 6 Method Vicuna Qwen3 GLM Batch 1 Batch 4 Batch 1 Batch 4 Batch 1 Batch E P E P Batch-based Methods (vs. batch=1 non-spec) Non-Spec-Batch Spec-1 EQSPEC EXSPEC DSD BSP 53.8 98.2 94.6 97.2 93.3 97.2 97.1 98.4 97.3 98.6 92.1 98.6 94.6 96.9 92.3 95.7 96.7 98.1 96.5 98.3 97.3 98.6 90.8 97.6 94.6 96.9 95.0 97.1 96.7 98.1 95.2 97.7 0.8 8.1 0.0 8.1 1.9 39. 96.0 98.0 92.9 96.5 0.6 0.0 2.1 12.6 2.2 0.0 0.2 31.3 2.2 0.2 3.5 19.9 1.0 0.0 1.0 15. 0.0 0.6 Continuous Batching Systems (vs. own non-spec) vLLM + Spec SGLang + EAGLE SGLang + EAGLE + Det 96.9 / 98.0 69.8 / 79.5 85.0 / 90.0 65.6 / 78.5 47.7 / 65.4 50.6 / 69.5 72.7 / 84.5 Table 1: Correctness check reports exact match (E) and partial match (P) scores. Top: compares with batch=1 non-spec baseline. Bottom: with each systems non-speculative mode (scores reported as / P). Our approach sustains > 95% accuracy, while prior work (DSD, BSP) suffers major drops from KV-cache and position ID errors. Figure 5: Decomposing batch speculative decoding performance. (a) Batch scaling efficiency normalized to BS=1, isolating GPU parallelism from per-sequence speculation; EXSPEC initially exceeds the No-Ragged-Scaling upper bound before degrading due to alignment overhead. (b) Alignment overhead grows super-linearly with batch size, consuming up to 38% of inference time, validating that coverhead(B) dominates at scale. (c) Cross-batch grouping rates on Multi30k for random vs. uniform-length sequences, showing that length homogeneity transforms grouping effectiveness. 4.2 OUTPUT CORRECTNESS VERIFICATION We verify correctness using deterministic greedy decoding to eliminate sampling variance and enable precise bug isolation. This avoids metrics such as ROUGE (Qian et al., 2024), which can mask implementation failures (e.g., repetitive corruption can still score reasonably). Instead, we use exact match (any divergence) and partial match (fraction of tokens before the first mismatch) to diagnose failure modes. Table 1 reveals distinct patterns. Our methods maintain 95% exact match across settings, the remaining 5% divergence stems from numerical non-determinism in floating-point operations (He & Lab, 2025) and tie-breaking in argmax sampling rather than algorithmic errors. By contrast DSD and BSP fail catastrophically with different signatures. DSDs near-zero scores indicate immediate position-ID misalignmentthe model fails from the first token. BSP shows higher partial match (up to 39.7%) but low exact match, indicating gradual degradation: outputs are initially correct before KV-cache drift misdirects attention, triggering repetition. These complementary patternsimmediate failure vs. gradual decayshow how partial-match metrics reveal not just that implementations fail, but when and why. Production systems (vLLM, SGLang) introduce additional complexity. While both are accurate on Vicuna, they degrade markedly on Qwen3. SGLang-EAGLE-Deterministic helps disambiguate the cause: improved accuracy suggests most divergences stem from floating-point non-determinism (He 7 & Lab, 2025) rather than algorithmic bugs. Despite this, we show below that their throughput still falls below non-speculative baselines."
        },
        {
            "title": "4.3 OVERHEAD AND SCALING DYNAMICS",
            "content": "We now validate our theoretical predictions through five studies that decompose batch speculative decoding performance. These experiments isolate alignment overhead growth, quantify its impact on batch scaling, identify sequence diversity as the key bottleneck to grouping effectiveness, compare against production systems, and provide mechanistic insights through overhead profiling. Batch scaling efficiency. Figure 1 shows that EXSPEC outperforms EQSPEC in absolute throughput by combining speculative and batching gains, yet EQSPEC exhibits negative scaling beyond BS=8. To test whether batch speculative decoding can retain GPU-parallelism benefits despite raggedness, Figure 5(a) normalizes each methods throughput to its BS=1 baseline, isolating batchlevel scaling from per-sequence speculation and enabling comparison to No-Ragged-Scaling (standard batched decoding without speculation). notable effect emerges: EXSPEC initially surpasses even this upper bound because speculation converts memory-bound operations into compute-bound verification, improving GPU utilization when managed correctly. However, at larger batch sizes, alignment overhead dominates and the advantage inverts, confirming that coverhead(B) eventually overwhelms parallelism gains. Alignment overhead growth. Figure 5(b) quantifies realignment costs via two metrics: percentage of total time spent on alignment (OH%) and per-round alignment time (OH/Spec). Overhead rises from 13% at BS=1 to nearly 40% at BS=32, with per-round costs increasing even more. This matches our prediction that cpad(B) + ckv(B) grows super-linearly with B. Crucially, this is not merely an implementation inefficiency: the very operations required for correctness (unpadappendrepad and KV-cache realignment) become increasingly expensive as sequence lengths diverge. Figure 6: Speculative decoding lags non-speculative baselines, with larger batches further degrading throughput. Grouping rate sequence-length distribution. Figure 5(c) probes whether cross-batch scheduling limits are algorithmic or circumstantial via Multi30k. Comparing random sampling to an All-Mean subset with identical lengths isolates the bottleneck: sequence diversity, not the method. Random sampling shows grouping rates collapsing with batch size, whereas the All-Mean configuration maintains high grouping effectiveness even at moderate scales, substantially reducing coverhead(B). This contrast suggests that preprocessing strategies (e.g., bucketing, dynamic sorting) can push real workloads toward the ideal, revealing untapped potential when scheduling is paired with workload shaping. Production systems limitation. Figure 6 shows that production frameworks struggle with batch speculative decoding. Although our method is not directly comparable to continuous-batching systems (which vary effective batch size dynamically), the trend is consistent: vLLMs speculative decoding underperforms its baseline at high concurrency (leading to deprecation in v1), and SGLang+EAGLE is slower than non-speculative generation across all batch sizes. Two architectural factors likely contribute: (i) speculation repurposes the prefill stage for parallel verification, breaking the prefilldecode separation these systems optimize; and (ii) continuous batching already induces raggedness from varying request lengths, and speculation compounds this with per-sequence acceptance variance, creating nested raggedness that overwhelms existing schedulers. These outcomes indicate that speculation cannot simply be grafted onto architectures optimized for predictable token-by-token decoding; deeper redesign is required. Overhead Anatomy. Table 2 shows that EXSPEC attains higher throughput via deliberate tradeoff. Cross-batch scheduling cuts total verification calls by one-third and halves alignment overhead by grouping same-length sequences. The trade-off is memory locality: dynamic batching scatters 8 Method TPS Spec Time/Verif. (ms) Verif. % Draft % Overhead % EQSPEC EXSPEC 95.6 156.4 1469 952 24.88 29.13 44.9 55.9 27.4 29. 27.7 14.6 Table 2: Overhead anatomy of batch speculation methods. Despite slower per-verification time, EXSPEC achieves higher throughput by reducing total verification calls and minimizing alignment overhead through intelligent scheduling. KV-cache entries, increasing per-verification latency. Despite slower individual operations, the reduction in operation count yields 64% overall throughput gain. This balance between operation count and efficiency suggests future work on improving KV-cache locality within the cross-batch framework."
        },
        {
            "title": "5 RELATED WORK",
            "content": "Speculative Decoding. Speculative decoding accelerates LLM inference by verifying draft tokens in parallel. Two verification paradigms dominate: sequence verificationas in SpecDec and followups (Xia et al., 2023; Santilli et al., 2023; Yang et al., 2023; Hooper et al., 2023; Zhang et al., 2023; Fu et al., 2024a)which preserves the target models output distribution; and tree verificatione.g., Medusa/EAGLE variants (Miao et al., 2024; Spector & Re, 2023; Sun et al., 2023; He et al., 2023; Cai et al., 2024; Li et al., 2024a;b; 2025)which explores multiple branches to raise acceptance rates. Recent works parallelize multiple draft sequences per request (Stewart et al., 2024; Lee et al., 2024) but still verify each request independently. Approximate schemes (Kim et al., 2023; Zhong et al., 2025) trade exactness for speed; our work assumes lossless verification to detect implementation errors rather than approximation artifacts. Batch Speculative Decoding. Extending speculative decoding to batch settings introduces the ragged tensor problem: when sequences accept different numbers of draft tokens, the resulting variable-length tensors break GPU-friendly rectangular operations (Qian et al., 2024). Existing approaches face fundamental limitations detailed in Section 2. Position ID/masking approaches like BSP (Su et al., 2023) suffer from position ID inconsistencies across iterations. Dynamic padding approaches reveal critical implementation errors: both DSD (Yan et al., 2025) and Metas recent work (Tang et al., 2025) incorrectly sample bonus tokens from the draft models distribution rather than the target models, violating the fundamental correctness guarantee of speculative decoding. This error compounds with improper KV-cache handling, producing corrupted outputsBSP generates repetitive tokens while DSD produces <unk> symbols. BASS (Qian et al., 2024) sidesteps these issues through custom CUDA kernels but sacrifices portability. These failures demonstrate that the ragged tensor problem extends beyond performance to correctness itself, motivating our cross-batch scheduling approach that maintains correctness while achieving batch scaling without custom kernels. Production Systems for Speculative Decoding. Serving stacks integrate speculation atop general batching (vLLM, SGLang/EAGLE, and forks such as TETRIS and TurboSpec (Kwon et al., 2023; Zheng et al., 2024; Li et al., 2025; Wu et al., 2025; Liu et al., 2025)). In practice, variable draft acceptance clashes with continuous batching and paged attention, and our measurements indicate scaling limits (e.g., small single-sequence gains that reverse at higher concurrency); some engines have since reduced or deprecated speculative-decoding support. We intentionally build our reference implementation from scratch, prioritizing correctness checks; compatibility with continuous batching, paged attention, and prefilldecode separation is orthogonal and left as future work."
        },
        {
            "title": "6 CONCLUSION",
            "content": "We presented correctness-first approach to batch speculative decoding that resolves the fundamental tension between maintaining output equivalence and achieving practical speedups. By identifying the precise synchronization invariants required for correctness, we explained why existing methods fail and established the minimal requirements for valid computation. In particular, our EQSPEC implementation enforces these invariants but reveals that realignment overhead consumes up to 40% 9 of computation. By contrast, EXSPEC overcomes this limitation via cross-batch scheduling that dynamically groups same-length sequences to bypass realignment entirely. Moreover, experiments across three model families show that our approach uniquely preserves >95% output equivalence while achieving up to 3 throughput improvement at batch size 8, hereby validating that careful codesign of correctness constraints and scheduling can multiply batch parallelism with per-sequence speculation gains. Our findings on ragged shape tensor synchronization overhead and cross-batch scheduling may inform future designs for production systems, where integrating speculation with continuous batching remains an open challenge."
        },
        {
            "title": "ACKNOWLEDGMENT",
            "content": "This work is supported by an eBay research grant."
        },
        {
            "title": "REFERENCES",
            "content": "Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason D. Lee, Deming Chen, and Tri Dao. Medusa: Simple LLM inference acceleration framework with multiple decoding heads. CoRR, abs/2401.10774, 2024. doi: 10.48550/ARXIV.2401.10774. URL https://doi.org/10.48550/ arXiv.2401.10774. Charlie Chen, Sebastian Borgeaud, Geoffrey Irving, Jean-Baptiste Lespiau, Laurent Sifre, and John Jumper. Accelerating large language model decoding with speculative sampling. arXiv preprint arXiv:2302.01318, 2023. Desmond Elliott, Stella Frank, Khalil Simaan, and Lucia Specia. Multi30k: Multilingual englishgerman image descriptions. In Proceedings of the 5th Workshop on Vision and Language, pp. 7074. Association for Computational Linguistics, 2016. doi: 10.18653/v1/W16-3210. URL http://www.aclweb.org/anthology/W16-3210. Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm inference using lookahead decoding, 2024a. Yichao Fu, Peter Bailis, Ion Stoica, and Hao Zhang. Break the sequential dependency of llm inference using lookahead decoding. arXiv preprint arXiv:2402.02057, 2024b. Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. Horace He and Thinking Machines Lab. ence. https://thinkingmachines.ai/blog/defeating-nondeterminism-in-llm-inference/. Thinking Machines Lab: Connectionism, 2025. doi: Defeating nondeterminism in llm infer10.64434/tml.20250910. Zhenyu He, Zexuan Zhong, Tianle Cai, Jason D. Lee, and Di He. REST: retrieval-based speculative decoding. CoRR, abs/2311.08252, 2023. doi: 10.48550/ARXIV.2311.08252. URL https:// doi.org/10.48550/arXiv.2311.08252. Coleman Hooper, Sehoon Kim, Hiva Mohammadzadeh, Hasan Genc, Kurt Keutzer, Amir Gholami, and Yakun Sophia Shao. SPEED: speculative pipelined execution for efficient decoding. CoRR, abs/2310.12072, 2023. doi: 10.48550/ARXIV.2310.12072. URL https://doi.org/10.48550/ arXiv.2310.12072. kamilakesbi. Enable speculative decoding with batch size 1 (github issue #32165). https:// github.com/huggingface/transformers/issues/32165, 2024. Hugging Face transformers. Opened 2024-07-23. Accessed 2025-09-24. Sehoon Kim, Karttikeya Mangalam, Suhong Moon, Jitendra Malik, Michael W. Mahoney, Amir Gholami, and Kurt Keutzer. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023, 2023. URL http://papers.nips.cc/paper files/paper/2023/hash/ 7b97adeafa1c51cf65263459ca9d0d7c-Abstract-Conference.html. Speculative decoding with big little decoder. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. 11 Minjae Lee, Wonjun Kang, Minghao Yan, Christian Classen, Hyung Il Koo, and Kangwook Lee. In-batch ensemble drafting: Toward fast and robust speculative decoding for multimodal language models, 2024. URL https://openreview.net/forum?id=8o7131Lm83. Yaniv Leviathan, Matan Kalman, and Yossi Matias. Fast inference from transformers via speculative decoding. In International Conference on Machine Learning, pp. 1927419286. PMLR, 2023. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires In International Conference on Machine Learning, pp. 28935 rethinking feature uncertainty. 28948. PMLR, 2024a. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle-2: Faster inference of language models with dynamic draft trees. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 74217432, 2024b. Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. EAGLE-3: Scaling up inference acceleration of large language models via training-time test, 2025. URL https://arxiv.org/ abs/2503.01840. Xiaoxuan Liu, Jongseok Park, Langxiang Hu, Woosuk Kwon, Zhuohan Li, Chen Zhang, Kuntai Du, Xiangxi Mo, Kaichao You, Alvin Cheung, Zhijie Deng, Ion Stoica, and Hao Zhang. Turbospec: Closed-loop speculation control system for optimizing llm serving goodput, 2025. URL https: //arxiv.org/abs/2406.14066. Xupeng Miao, Gabriele Oliaro, Zhihao Zhang, Xinhao Cheng, Zeyu Wang, Zhengxin Zhang, Rae Ying Yee Wong, Alan Zhu, Lijie Yang, Xiaoxiang Shi, Chunan Shi, Zhuoming Chen, Daiyaan Arfeen, Reyna Abhyankar, and Zhihao Jia. Specinfer: Accelerating large language model servIn Proceedings of the 29th ACM ing with tree-based speculative inference and verification. International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 3, ASPLOS 24, pp. 932949, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400703867. doi: 10.1145/3620666.3651335. URL https://doi.org/10.1145/3620666.3651335. Haifeng Qian, Sujan Kumar Gonugondla, Sungsoo Ha, Mingyue Shang, Sanjay Krishna Gouda, Ramesh Nallapati, Sudipta Sengupta, Xiaofei Ma, and Anoop Deoras. Bass: Batched attentionIn Findings of the Association for Computational Linguistics optimized speculative sampling. ACL 2024, pp. 82148224, 2024. Andrea Santilli, Silvio Severino, Emilian Postolache, Valentino Maiorca, Michele Mancusi, Riccardo Marin, and Emanuele Rodol`a. Accelerating transformer inference for translation via In Anna Rogers, Jordan L. Boyd-Graber, and Naoaki Okazaki (eds.), Proparallel decoding. ceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2023, Toronto, Canada, July 9-14, 2023, pp. 1233612355. Association for Computational Linguistics, 2023. doi: 10.18653/v1/2023.acl-long.689. URL https://doi.org/10.18653/v1/2023.acl-long.689. SGLang Team. https://lmsys.org/blog/ 2025-09-22-sglang-deterministic/, September 2025. LMSYS Org Blog, published Sep 22, 2025. Enabling deterministic inference for sglang. Benjamin Spector and Chris Re. Accelerating LLM inference with staged speculative decoding. CoRR, abs/2308.04623, 2023. doi: 10.48550/arXiv.2308.04623. URL https://doi.org/10. 48550/arXiv.2308.04623. Lawrence Stewart, Matthew Trager, Sujan Kumar Gonugondla, and Stefano Soatto. The n-grammys: Accelerating autoregressive inference with learning-free batched speculation. arXiv preprint arXiv:2411.03786, 2024. Qidong Su, Christina Giannoula, and Gennady Pekhimenko. The synergy of speculative decoding and batching in serving large language models. arXiv preprint arXiv:2310.18813, 2023. 12 Spectr: Fast speculative decoding via optimal transport. Ziteng Sun, Ananda Theertha Suresh, Jae Hun Ro, Ahmad Beirami, Himanshu Jain, and Felix X. Yu. In Alice Oh, Tristan Naumann, Amir Globerson, Kate Saenko, Moritz Hardt, and Sergey Levine (eds.), Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December URL http://papers.nips.cc/paper files/paper/2023/hash/ 10 - 16, 2023, 2023. 6034a661584af6c28fd97a6f23e56c0a-Abstract-Conference.html. Bangsheng Tang, Carl Chengyan Fu, Fei Kou, Grigory Sizov, Haoci Zhang, Jason Park, Jiawen Liu, Jie You, Qirui Yang, Sachin Mehta, et al. Efficient speculative decoding for llama at scale: Challenges and solutions. arXiv preprint arXiv:2508.08192, 2025. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pp. 3845, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020.emnlp-demos. 6. Zhaoxuan Wu, Zijian Zhou, Arun Verma, Alok Prakash, Daniela Rus, and Bryan Kian Hsiang Low. Tetris: Optimal draft token selection for batch speculative decoding. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL), 2025. Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In Houda Bouamor, Juan Pino, and Kalika Bali (eds.), Findings of the Association for Computational Linguistics: EMNLP 2023, Singapore, December 6-10, 2023, pp. 39093925. Association for Computational Linguistics, 2023. doi: 10.18653/V1/2023.FINDINGS-EMNLP.257. URL https: //doi.org/10.18653/v1/2023.findings-emnlp.257. Heming Xia, Zhe Yang, Qingxiu Dong, Peiyi Wang, Yongqi Li, Tao Ge, Tianyu Liu, Wenjie Li, and Zhifang Sui. Unlocking efficiency in large language model inference: comprehensive survey of speculative decoding. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar (eds.), Findings of the Association for Computational Linguistics ACL 2024, pp. 76557671, Bangkok, Thailand and virtual meeting, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/ 2024.findings-acl.456. URL https://aclanthology.org/2024.findings-acl.456. Minghao Yan, Saurabh Agarwal, and Shivaram Venkataraman. Decoding speculative decoding. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 64606473, 2025. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Seongjun Yang, Gibbeum Lee, Jaewoong Cho, Dimitris S. Papailiopoulos, and Kangwook Lee. Predictive pipelined decoding: compute-latency trade-off for exact LLM decoding. CoRR, abs/2307.05908, 2023. doi: 10.48550/ARXIV.2307.05908. URL https://doi.org/10.48550/ arXiv.2307.05908. Jun Zhang, Jue Wang, Huan Li, Lidan Shou, Ke Chen, Gang Chen, and Sharad Mehrotra. Draft & verify: Lossless large language model acceleration via self-speculative decoding. CoRR, 13 abs/2309.08168, 2023. doi: 10.48550/arXiv.2309.08168. URL https://doi.org/10.48550/ arXiv.2309.08168. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena, 2023. Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37: 6255762583, 2024. Meiyu Zhong, Noel Teku, and Ravi Tandon. Speeding up speculative decoding via sequential approximate verification. arXiv preprint arXiv:2502.04557, 2025."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "We provide our complete implementation in the Supplementary Material, including all hyperparameters, experimental configurations, and model specifications detailed in Section 4. Our correctness verification framework using exact and partial match metrics enables deterministic validation of both our results and future implementations. All experiments use publicly available models and datasets for reproducibility. Model Configurations. We evaluate three target-draft model pairs: Vicuna-7B (lmsys/vicuna-7bv1.3) with 68M draft model (double7/vicuna-68m), Qwen3-8B (Qwen/Qwen3-8B) with Qwen30.6B (Qwen/Qwen3-0.6B), and GLM-4-9B (zai-org/GLM-4-9B-0414) with 0.6B draft model (jukofyork/GLM-4.5-DRAFT-0.6B-v3.0). All models were loaded in FP16 precision with greedy decoding (temperature=0, top p=1.0) to ensure deterministic outputs. We use five draft tokens per speculation round across all experiments unless otherwise specified. Production Systems Configuration. We evaluated two production inference systems for comparison: vLLM version 0.9.1 and SGLang commit c4e314f (the deterministic decoding had not merged to the stable version at the time of submission, so we compiled it from source). For vLLM, we used the V0 engine with speculative decoding enabled, as the V1 engine does not support draft model speculative decoding5. For SGLang, we used EAGLE-based speculation with model-specific draft models: yuhuili/EAGLE-Vicuna-7B-v1.3 for Vicuna-7B and Tengyunw/qwen3 8b eagle3 for Qwen3-8B. GLM-4 was not evaluated with SGLang due to the unavailability of compatible EAGLE draft models. We tested both standard SGLang inference and SGLang with deterministic mode enabled (SGLang Team, 2025) to isolate floating-point non-determinism from algorithmic correctness issues."
        },
        {
            "title": "DISCLOSE ON LLM USAGE",
            "content": "We used Large Language Models as assistive tools in preparing this manuscript: GPT-5 and Claude Opus 4.1 for polishing writing (grammar correction and sentence restructuring), generating conceptual diagram illustrations, and writing unit tests; NVIDIA/Parakeet-TDT-0.6B-v3 for voice input transcription. LLMs were not used for research ideation, experimental design, data analysis, or scientific conclusions. All core algorithmic implementations and scientific contributions are solely from the authors. We take full responsibility for all content in this paper."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work focuses on improving the efficiency of LLM inference without altering model outputs or introducing biases. Our correctness-preserving approach ensures that acceleration techniques do not 5https://github.com/vllm-project/vllm/issues/21797 14 compromise model safety or reliability. Our open-source release enables reproducible research and transparent evaluation of batch speculative decoding techniques."
        }
    ],
    "affiliations": [
        "The Pennsylvania State University",
        "eBay Inc"
    ]
}