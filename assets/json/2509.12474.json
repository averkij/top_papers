{
    "paper_title": "Image Tokenizer Needs Post-Training",
    "authors": [
        "Kai Qiu",
        "Xiang Li",
        "Hao Chen",
        "Jason Kuen",
        "Xiaohao Xu",
        "Jiuxiang Gu",
        "Yinyi Luo",
        "Bhiksha Raj",
        "Zhe Lin",
        "Marios Savvides"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent image generative models typically capture the image distribution in a pre-constructed latent space, relying on a frozen image tokenizer. However, there exists a significant discrepancy between the reconstruction and generation distribution, where current tokenizers only prioritize the reconstruction task that happens before generative training without considering the generation errors during sampling. In this paper, we comprehensively analyze the reason for this discrepancy in a discrete latent space, and, from which, we propose a novel tokenizer training scheme including both main-training and post-training, focusing on improving latent space construction and decoding respectively. During the main training, a latent perturbation strategy is proposed to simulate sampling noises, \\ie, the unexpected tokens generated in generative inference. Specifically, we propose a plug-and-play tokenizer training scheme, which significantly enhances the robustness of tokenizer, thus boosting the generation quality and convergence speed, and a novel tokenizer evaluation metric, \\ie, pFID, which successfully correlates the tokenizer performance to generation quality. During post-training, we further optimize the tokenizer decoder regarding a well-trained generative model to mitigate the distribution difference between generated and reconstructed tokens. With a $\\sim$400M generator, a discrete tokenizer trained with our proposed main training achieves a notable 1.60 gFID and further obtains 1.36 gFID with the additional post-training. Further experiments are conducted to broadly validate the effectiveness of our post-training strategy on off-the-shelf discrete and continuous tokenizers, coupled with autoregressive and diffusion-based generators."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 4 7 4 2 1 . 9 0 5 2 : r a"
        },
        {
            "title": "Under review",
            "content": "IMAGE TOKENIZER NEEDS POST-TRAINING Kai Qiu1, Xiang Li1, Hao Chen1, Jason Kuen2, Xiaohao Xu3 Jiuxiang Gu2, Yinyi Luo1, Bhiksha Raj1, Zhe Lin2, Marios Savvides1 Carnegie Mellon University1, Adobe Research2, University of Michigan3 Project Page: RobusTok.github.io"
        },
        {
            "title": "ABSTRACT",
            "content": "Recent image generative models typically capture the image distribution in preconstructed latent space, relying on frozen image tokenizer. However, there exists significant discrepancy between the reconstruction and generation distribution, where current tokenizers only prioritize the reconstruction task that happens before generative training without considering the generation errors during sampling. In this paper, we comprehensively analyze the reason for this discrepancy in discrete latent space, and, from which, we propose novel tokenizer training scheme including both main-training and post-training, focusing on improving latent space construction and decoding respectively. During the main training, latent perturbation strategy is proposed to simulate sampling noises, i.e., the unexpected tokens generated in generative inference. Specifically, we propose plug-and-play tokenizer training scheme, which significantly enhances the robustness of tokenizer, thus boosting the generation quality and convergence speed, and novel tokenizer evaluation metric, i.e., pFID, which successfully correlates the tokenizer performance to generation quality. During post-training, we further optimize the tokenizer decoder regarding well-trained generative model to mitigate the distribution difference between generated and reconstructed tokens. With 400M generator, discrete tokenizer trained with our proposed main training achieves notable 1.60 gFID and further obtains 1.36 gFID with the additional post-training. Further experiments are conducted to broadly validate the effectiveness of our post-training strategy on off-the-shelf discrete and continuous tokenizers, coupled with autoregressive and diffusion-based generators."
        },
        {
            "title": "INTRODUCTION",
            "content": "In recent years, image generative modeling has been dominated by two major paradigms: diffusion (Dhariwal & Nichol, 2021; Song et al., 2022), which operates on the continuous latent space (Peebles & Xie, 2023; Rombach et al., 2022; Vahdat et al., 2021; Nichol & Dhariwal, 2021) through denoising, and autoregressive (AR) (Van Den Oord et al., 2016), which relies on the discrete latent space for next token prediction. Image tokenizers have emerged as an essential component to convert the raw pixels into continuous and discrete representations for diffusion and AR model, respectively. Tokenizers aim to provide highly compressed yet structurally meaningful representations for downstream generative models (Song et al., 2022; Van Den Oord et al., 2016), substantially improving both the efficiency and scalability of training large generative models. series of works has further advanced tokenizers design, introducing various improvement directions such as reconstruction quality (Chen et al., 2025b; Kim et al., 2025), compressed ratio (Chen et al., 2024b; Yu et al., 2024c), quantization method (Yu et al., 2023b; Lee et al., 2022a), and latent regularization (Kingma & Welling, 2013; Li et al., 2024c). However, although these improvements have been validated under reconstruction metrics, their effectiveness under generative settings remains more of black box. In practice, we often observe that tokenizers with strong reconstruction ability do not necessarily yield high-quality generations, whereas others with weaker reconstruction performance surprisingly lead to better generation results. This performance discrepancy between reconstruction and generation exposes fundamental Equal contribution."
        },
        {
            "title": "Under review",
            "content": "Figure 1: (a) Discrepancy between reconstruction and generation task imposes latent token distribution difference between them. Specifically, reconstruction always rely on true tokens whereas generation task always sample out-of-distribution (OOD) tokens. To resolve this problem, we propose RobusTok (b) to enhance the robustness of tokenizer during main-trainig by latent perturbation, and (c) align the generated latent space with its target image in post-training stage. gap in how tokenizers are utilized across reconstruction and generation tasks. As illustrated in Figure 1 (a), generative modeling, including both AR and diffusion, inevitably introduces distribution difference for tokenizers between generative training and sampling. Specifically, during training, ground truth representations are provided (Sutton, 1988; Song et al., 2022), and during sampling, future predictions can only rely on previous predicted ones. This issue is further amplified by the typically misaligned objectives of tokenizer training and generator sampling. Tokenizer training prioritizes reconstruction fidelity where the visual decoder takes clean image tokens for accurate image reconstruction. Instead, to decode tokens from well-trained AR model, the sampling error occurs in the predicted tokens which makes the decoder takes noisy and potentially out-of-distribution (OOD) latent patterns. This discrepancy necessitates that the latent space possess sufficient robustness to handle such latent perturbations, where reconstruction quality alone cannot capture. These observations motivate us to introduce robustness as an additional evaluation criterion and to design training strategies that explicitly enhance the tokenizers ability to cope with noisy or perturbed latents. Though robustness, as we illustrated above, should be considered for tokenizer training, it still cannot fully resolve the problem in sampling error of generator since robustness address random perburbation, whereas generator sampling errors are systematic and thus remain unresolved. Therefore, teaching the tokenizer how to interpret and reconstruct from generated tokens, rather than only clean tokens, is essential. However, this remains an open challenge due to the absence of explicit correspondence between each generated latent and its underlying ground truth image, making it more difficult to provide direct supervision for post-training. Building upon these insights, we further introduce novel two-stage tokenizer training scheme to construct robust latent space and decoder respectively. (1) During main-training (Fig. 1 (b)), we propose novel plug-and-play discrete tokenizer training strategy that systematically integrates latent perturbations with an annealing schedule, gradually reducing perturbation intensity to stabilize training and promote robust latent space construction. (2) In post-training (Fig. 1 (c)), we design preservation ratio to control how much information from the target image is retained during generation, and use it to adapt the decoder to the distribution of latents produced by well-trained generator. Extensive experiments conducted on state-of-the-art (SOTA) autoregressive frameworks (Sun et al., 2024; Yu et al., 2024b) across the ImageNet (Deng et al., 2009) generation benchmarks demonstrate the efficacy of our main-training. Moreover, across detailed experiments on off-the-shelf discrete and continuous tokenizers with their corresponding autoregressive and diffusion generative models, our post-training strategy also shows its broad applicability, consistently yielding promising improvements in generative quality. In particular, our method achieves 1.36 gFID with 400M generator, establishing new SOTA under this parameter budget. Our contributions can be summarized as follows: We systematically analyze the discrepancy between reconstruction and generation in tokenizers, and provide the first comprehensive study on how robustness of the latent space impacts generative performance. We introduce RobusTok, tokenizer trained using our novel two-stage training scheme including main-training for robust latent construction and post-training for generative latent alignment. We provide extensive experiments and ablation studies to validate the effectiveness of our latent perturbation and generalization ability of our post-training in autoregressive and diffusion generative models."
        },
        {
            "title": "2 RELATED WORKS",
            "content": "Image tokenizers. Image tokenization has seen significant advancements across various imagerelated tasks. Traditionally, autoencoders (Hinton & Salakhutdinov, 2006; Vincent et al., 2008) have been employed to compress images into latent spaces for downstream applications such as generation and understanding. In generative tasks, VAEs (Van Den Oord et al., 2017; Razavi et al., 2019a) learn to map images to probabilistic distributions; VQGAN (Esser et al., 2021; Razavi et al., 2019b) and its subsequent variants (Lee et al., 2022a; Yu et al., 2023b; Mentzer et al., 2023; Zhu et al., 2024a; Takida et al., 2023; Huang et al., 2023; Zheng et al., 2022; Yu et al., 2023a; Weber et al., 2024; Yu et al., 2024a; Luo et al., 2024; Zhu et al., 2024b; Miwa et al., 2025) introduce discrete latent spaces to enhance compression and facilitate the application of autoregressive models (Vaswani et al., 2023; Dosovitskiy et al., 2021) to image generation tasks by converting images into sequences of discrete tokens. On the other hand, understanding tasks, such as CLIP (Radford et al., 2021), DINO (Oquab et al., 2023; Darcet et al., 2023; Zhu et al., 2024c) and MAE (He et al., 2022), rely heavily on LLM (Vaswani et al., 2023; Dosovitskiy et al., 2021) to tokenize images into semantic representations (Dong et al., 2023; Ning et al.) where shown its promising performance in classification (Dosovitskiy et al., 2021), object detection (Zhu et al., 2010), segmentation (Wang et al., 2021), and multi-modal application (Yang et al., 2024). In this paper, we provide comprehensive analysis of image tokenizer in view of perturbation robustness (Chen et al., 2024a; Li et al., 2024f; Xu et al.; Li et al., 2024g; 2023b;a). Autoregressive visual generation. Autoregressive visual generation has shown remarkable success in generating high-quality images by modeling the distribution of pixels or latent codes in sequential manner. Transformers (Vaswani et al., 2023), has demonstrated their strong capacity for capturing long-range dependencies and fine-grained details in image generation. Inspired by exploded development of language model (Shi et al., 2022; Mizrahi et al., 2024) such as GPT (Achiam et al., 2023), series of works leverage tokenizers to convert images or visual information into discrete latent codes, enabling autoregressive or MLM modeling to generate image in raster-scan (Esser et al., 2021) or parallel (Pang et al., 2024a; Chang et al., 2022; Wang et al., 2024) order. Recently, autoregressive models continued to show their scalability power in larger datasets and multimodal tasks (He et al., 2024); models like LlamaGen (Sun et al., 2024) adapt current advanced LLM architectures for image generation. New directions such as VAR (Tian et al., 2024; Li et al., 2024e; Han et al., 2024; Ren et al., 2024; Qiu et al., 2024) and RAR (Yu et al., 2024b; Pang et al., 2024b) focus on fusing global information into the training of autoregressive model. MAR (Li et al., 2024b; Fan et al., 2024) and GIVIT (Tschannen et al., 2024) have shown the potential for continuous image generation. Through the development of such, various techniques continue to unify the language language model for generation and understanding (Wu et al., 2024; Tong et al., 2024)."
        },
        {
            "title": "3 PRELIMINARY - TOKENIZER ROBUSTNESS",
            "content": "Vector Quantization (VQ). Most AR models are based on discrete tokenizers with quantized latent space. The tokenizer usually consists of an encoder, quantizer, and decoder. Although many quantization techniques for the quantizer were previously proposed (Mentzer et al., 2023; Yu et al., 2023b; Zhao et al., 2024), we focus on the VQ tokenizer (Esser et al., 2021) for its simplicity and natural compatibility with AR models in this paper. Given an RGB image I, the encoder first extracts set of latent representations RHW C, where denotes the spatial resolution of the latent tokens. VQ (Esser et al., 2021) aims to quantize continuous features into set of discrete features with minimum reconstruction error of the original data, ensuring that the quantized representation remains as close as possible to the original continuous ones. Specifically, it maps each continuous feature vector RC to closest quantized codeword RC from learnable codebook = {ek}K k=1 with in total codewords as: = arg min ekC ek2 2. (1) The decoder then reconstructs the original input by taking the quantized as input."
        },
        {
            "title": "Under review",
            "content": "Figure 2: RobusTok overview. We adopt vision transformer as our encoder and decoder D. β of data in one batch will process our Latent Perturbation, which will be randomly replaced by top-δ neighbor from codebook with probability α. frozen DINO encoder is utilized to supervise our latent space. Ideal Scenario D(Z ) = Eval AR/LLM D(Z + ) = ˆI Train Tokenizer D(Z ) = ˆI Table 1: Decoder analysis. I: ground-truth image. ˆI: predicted image. ˆI : predicted image from noisy latent. z: quantized latent feature. : sampling error. D: decoder. Robustness. Our method is motivated by mitigating the discrepancy between tokenizer training and inference schemes. As shown in Table 1, we demonstrate the input/output formulation of visual decoder upon the latent representations . Ideally, the decoder should take clear latent and reconstruct the ground-truth image that aligns with the current tokenizers training target. However, during the inference stage with well-trained generative model, sampling error always happens. This will change the usage of the decoder different from its training target, which significantly challenges the robustness of the visual decoder during inference as we expect D(Z + ) can still reconstruct the ground-truth I. To ease the discrepancy, RobusTok targets predicting ground-truth image from synthetic noisy latent + and real noisy latent from generative model during main-training and post-training respectively. In addition, the robustness of the decoder can be measured by Lipschitz smoothness Lip = ˆI ˆI ˆI . Since the potential choice of is constrained and the discrepancy between ground-truth and reconstructed images ˆI can be better reflected by the Frechet Inception Distance (FID), we introduce perturbed FID (pFID) as new metric to measure the robustness and reconstruction quality of tokenizers in our experiments where pFID = FID( ˆI , I) (detailed in experiment section)."
        },
        {
            "title": "4 ROBUSTOK",
            "content": "RobusTok is transformer-based image tokenizer shown in Fig. 2 with two-stage training recipe. Main-training: constructing latent space with reconstruction target while involving synthetic perturbation to simulate sampling errors during generation. Post-training: generative finetuning tokenizer decoder to align with the well-trained generative model. 4.1 ARCHITECTURE Following prior works (Li et al., 2024c;d; Yu et al., 2024d), RobusTok leverages Vision Transformer (ViT) (Dosovitskiy et al., 2020) as visual encoder and visual decoder. As shown in Fig. 2, we initialize set of learnable tokens and use these tokens as the representation for image reconstruction and subsequent generation. Specifically, the input image is first patchified to tokens, where represents the patch size, and concatenated with learnable tokens to serve as the input of the encoder. We apply vector quantization on the continuous token obtained from the encoder E. After that"
        },
        {
            "title": "Under review",
            "content": "Figure 3: Visualization of (a) traditional tokenizer, (b) semantic tokenizer, and (c) our RobusTok in reconstruction task with Latent Perturbation. Non-semantic tokenizer leads to distorted reconstructions when perturbations are introduced while our method shows promising robustness to those perturbations. latent perturbation approach is applied to guide the latent space construction. Finally, the ViT decoder takes perturbed tokens and new set of learnable tokens to reconstruct the image. Specifically, we incorporate pretrained DINOv2 model (Oquab et al., 2023) to inject semantics, ensuring that the learned tokens retain meaningful visual semantics and structural coherence. 4.2 MAIN-TRAINING In discrete latent space, the latent space is constrained by codebook. Thereby, sampling error happens in form of token mismatch. As shown in Fig. 3, we define set of operations to simulate the sampling error during tokenizer training. Perturbation rate. An important metric to monitor the AR modeling process is the accuracy of predicted tokens. Likewise, we define perturbation rate α to control the proportion of perturbed token within an image. Given the quantized feature RHW C, we define α as: α = , (2) where denotes the perturbed token number. To simulate the sampling error, we can randomly perturb the quantized tokens from the tokenizer encoder. Perturbation proportion. Within batch of images, we apply the perturbation in proportion β of images and keep the remaining images unchanged. With Nc clean images and Np perturbed images, the perturbation proportion is calculated as: β = Nc Nc + Np . (3) Perturbation strength. We define perturbation strength δ to quantify the perturbation level. Specifically, given discrete token = ek with codebook C, we calculate the set of top-δ nearest neighbors: Sδ = arg min en ek2 2, (4) (cid:88) SδC,Sδ=δ enSδ where denotes the counting operation. We randomly replace the original token ek with eδ Sδ to perturb the latent, thereby modifying the latent representation to simulate sampling in AR with the top-k nucleus strategy. Plug-and-play perturbation. During tokenizer training, we apply latent perturbation to enhance its robustness. We apply perturbation after semantic regularization (Li et al., 2024c) to preserve clear semantics in the discrete tokens to maximize the reconstruction capability. Within batch of image, we randomly choose β of them to add perturbation. To apply perturbation to each selected image, we randomly choose α tokens and then calculate the top-δ nearest neighbors to those tokens within the learned codebook. The final perturbation is applied by randomly replacing the original token with its top-δ nearest neighbor."
        },
        {
            "title": "Under review",
            "content": "Figure 4: Generated images under different σ for (left) autoregressive and (right) diffusion model."
        },
        {
            "title": "4.3 POST-TRAINING",
            "content": "Following the training strategy described above, we obtain more robust tokenizer. However, gap still remains between the latents sampled from well-trained generator and those seen in tokenizer training, leading to generation degradation. To mitigate this issue, we introduce lightweight posttraining stage aimed at adapting the decoder to generated latents. Training scheme. In this stage, we freeze the encoder and quantizer, and fine-tune only the decoder. To stabilize adversarial training, the pretrained discriminator is reused directly and optimization continues with the same loss combination. Concretely, generated images are re-encoded and paired with their corresponding real images, allowing the decoder to learn reconstruction from AR generated latents space. Preservation ratio. To provide the decoder with meaningful guidance when learning from generated latents, each latent must be paired with its corresponding image. However, when relying solely on generated latents, the missing of the image pairs (generated latents v.s. ground truth generated image) directly block the decoder training. Inspired by the smooth transition induced by the teacher forcing (Sutton, 1988), we introduce the preservation ratio σ, which interpolates between reconstruction and generation by controlling how much information from the original image is retained in the generated latents. Specifically, in the latent space of the image tokenizer RHW C, an autoregressive model generates an image by sequentially sampling tokens along the grid. During each sampling step, we quantify the preservation ratio σ to determine the ratio of tokens replaced by their ground-truth counterparts, formulated as σ = Nrecon (5) where Nrecon denotes the number of tokens taken directly from the reconstructed latent sequence. As shown in Fig. 4, this formulation allows σ to smoothly control the trade-off between fully reconstructed and fully generated latents and thus make connection between generated and reconstructed image. Moreover, though teacher forcing is only applicable to autoregressive models, similar mechanism can be realized in diffusion-based generation through SDEdit (Meng et al., 2021), detailed explanation can be referenced to the Appendix."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "5.1 EXPERIMENTAL SETTING We experiment on ImageNet (Deng et al., 2009) 256256 benchmark for both reconstruction and generation. We evaluate 11 open-sourced tokenizers across 4 codebook sizes. We follow their official implementation to pre-tokenize images and benchmark their generation performance using LlamaGen generators with default settings (Sun et al., 2024). For our RobusTok, we additionally leverage RAR (Yu et al., 2024b) as an additional generator to validate its wide applicability. In the post-training stage, we collect four representative tokenizers (including both discrete and continuous latent with autoregressive and diffusion model) to validate our methods effectiveness. Perturbed FID. Except for typical Frechet Inception Distance (FID) (Heusel et al., 2017), Inception Score (IS) (Salimans et al., 2016), Precision, and Recall for generator quality, we introduce pFID to assess tokenizer."
        },
        {
            "title": "Under review",
            "content": "(a) rFID vs. gFID with and without CFG. (b) pFID vs. gFID with and without CFG. Figure 5: Comparison of rFID-gFID and pFID-gFID curves of different tokenizers under LlamaGenB training setting. denotes codebook size. Each point represents tokenizer in our benchmarking. Compared to reconstruction FID (rFID) that merely captures the reconstruction quality of the tokenizer, pFID can reflect the robustness and the latent space from tokenizer, and correlates with the sampling error and thus the performance of AR models. To calculate the pFID, we apply perturbation among all images, i.e., β = 1 for all the settings. In addition, we observe that, as shown in Fig. 6, choosing α [0.5, 0.9] and δ [200, 360] has similar distribution with the generated latents. According to this, we define set of perturbation rates α {0.9, 0.8, 0.7, 0.6, 0.5} and set of perturbation strength δ {200, 280, 360} as the basis for computing pFID. For each of the α, δ combinations, we generate perturbed reconstructions and compute the FID against the input images. The final pFID is obtained by averaging over all combinations. To ensure consistency across tokenizers, the perturbation strength δ is linearly scaled to match different codebook sizes. Figure 6: Maximum Mean Discrepency (MMD) between generated and α-perturbed latent. Confirmed by our experiment in Section 5.2, our pFID is more correlated with the tokenizers downstream generation performance compared with rFID. Implementation details. During tokenizer training, we randomly select β = 0.1 of the total data to add perturbation. For these selected samples, we set α = 1.0 and δ = 100, and gradually anneal to half over the training. For the AR generator, we strictly follow the training recipes of LlamaGen (Sun et al., 2024) and RAR (Yu et al., 2024b) except for changing the tokenizer to RobusTok. During post-training, we reduce the learning rate by half and reset the weight decay to zero. 5.2 MAIN EXPERIMENTS ANALYSIS General observations. Before we go through and validate the core focus of this paper, we aim to conclude some generic observations from the benchmarking. The observations are summarized from the benchmarking results of LlamaGen-Base/Large. Codebook size: With similar reconstruction capability, the smaller the codebook size, the better the generation quality. We consider this property primarily results from the simple latent space are easier to capture during the AR modeling. Semantics: Semantic tokenizer typically demonstrates better capability for both reconstruction and generation. Semantic guidance provides structural and clustering latent for better compression capability for reconstruction and robustness property for generation accordingly. Reconstruction: Reconstruction capability measured by traditional rFID does not align with the generation capability. This should be potentially resulted from the discrepancy between tokenizer training and inference, i.e., the latent space lacks robustness. Effectiveness of pFID. To better compare the correlation among metrics, we visualize the rFIDgFID and pFID-gFID curves in Fig. 5 (a detailed value for each method & results for LlamaGenLarge generator are available in the Appendix). (a) When comparing rFID and gFID, we observe that there is no clear correlation between them, regardless of whether classifier-free guidance is used in generation or not. (b) Differently, pFID and gFID demonstrate strong correlation within each codebook size K. We separately compare results within each primarily because we add different"
        },
        {
            "title": "Under review",
            "content": "Type Method Diff. NAR AR ADM (Dhariwal & Nichol, 2021) LDM-4 (Rombach et al., 2022) DiT-L/2 (Peebles & Xie, 2023) MAR-B (Li et al., 2024b) MaskGIT (Chang et al., 2022) RCG (cond.) (Li et al., 2024a) TiTok-S-128(Yu et al., 2024d) MAGVIT-v2 (Yu et al., 2023b) MaskBit (Weber et al., 2024) VQGAN (Esser et al., 2021) RQ-Transformer (Lee et al., 2022b) LlamaGen-L (Sun et al., 2024) VAR (Tian et al., 2024) ImageFolder (Tian et al., 2024) RAR (Yu et al., 2024b) RobusTok (Ours) + Tokenizer Post-Training Tokenizer rFID - - 0.90 1.22 2.28 - 1.52 0.90 1. 7.94 1.83 2.19 0.90 0.80 2.28 1.02 pFID - - - - 5.03 - - - - - - 13.12 17.46 7.23 5.03 2. gFID 10.94 3.60 5.02 2.31 6.18 3.49 1.94 1.78 1.65 18.65 15.72 3.80 3.30 2.60 1.70 1.60 1.36 IS 101.0 247.7 167.2 281.7 182.1 215.5 - 319.4 341.8 80.4 86.8 248.3 274.4 295.0 299.5 305.8 300. Generator Rec 0.63 - 0.57 0.57 0.51 - - - - 0.26 - 0.52 0.51 0.63 0.60 0.65 0.66 Pre 0.69 - 0.75 0.82 0.80 - - - - 0.78 - 0.83 0.84 0.75 0.81 0.78 0. #Para 554M 400M 458M 208M 227M 502M 177M 307M 305M 227M 480M 343M 310M 362M 461M 461M Leng. - - - - 256 256 128 256 256 1024 256 680 286 256 256 Step 1000 250 250 64 8 250 64 64 64 256 64 256 10 10 256 Table 2: System-level performance comparison on class-conditional ImageNet 256x256. and indicate that higher or lower values are better, respectively. Type Method Latent Reconstruction # Tokens rFID Generation w/o P.T. gFID IS Generation w/ P.T. gFID IS Diff. MAETok (Chen et al., 2025a) AR LlamaGen (Sun et al., 2024) GigaTok (Xiong et al., 2025) RobusTok-B RobusTok-L con. disc. disc. disc. disc. 128 256 256 256 0.48 2.19 0.89 1.02 1.02 1.87 3.80 3.84 1.83 1.60 287.4 243.8 207.8 298.3 305. 1.68 3.51 3.68 1.60 1.36 303.1 241.2 214.8 288.0 300.2 Table 3: System-level comparison on class-conditional ImageNet 256 256. con. / disc. denote continuous / discrete latent types. / indicate higher / lower is better. P.T. represents our proposed tokenizer post-training strategy. perturbation strength δ according to K. With the new pFID, we can better access the tokenizers performance without the time-consuming and resource-intensive training of subsequent generators. Systematic comparison. As shown in Table 2, we compare our RobusTok with various state-ofthe-art methods on the ImageNet 256 256 benchmark (Deng et al., 2009). In particular, RobusTok yields notable improvement over prior approaches. Specifically, when applied on top of the RAR generator with the same training recipe, it achieves 0.10 gFID gain. Moreover, with our simple tokenizer post-training strategy, our method attains new state-of-the-art generative performance among generators with fewer than 500M parameters, reaching 1.36 gFID. Broad applicability of post-training. To validate our post-training, we extend our method to four representative tokenizers covering both continuous and discrete tokenizers with their downstream diffusion and autoregressive models, respectively. As illustrated in Table 3, all methods achieve consistent improvements, demonstrating the broad applicability of our post-training strategy. 5.3 MORE ANALYSIS Robust latent space. As shown in Fig. 7, we compare the latent space (i.e., codebook) with and without latent perturbation. We colorize the latent tokens with their frequency of use during inference. When truncating tokens at different usage count thresholds, we observe the space constructed with latent perturbation contains many reusable tokens, which acted as key tokens that can be easily modeled, while the remaining tokens serve as supportive tokens providing Figure 7: T-SNE visualization of latent space of tokenizer trained with and without latent perturbation. Colors and thresholds represent the frequency of tokens being used during inference without perturbation."
        },
        {
            "title": "Under review",
            "content": "Figure 8: Visualization of 256 256 image generation before (top) and after (bottom) post-training. Three improvements are observed: (a) OOD mitigation, (b) color fidelity, and (c) detail refinement. 0.2M 0.5M 1M gFID 1.60 1.60 1.59 Epochs gFID 20 10 50 30 1.79 1.72 1.64 1.60 1.65 0.5 σ 0.9 gFID 1.98 1.83 1.79 1.79 1.80 0.6 0.7 0.8 (a) Training data. (b) Number of epochs. (c) Perservation ratio. Table 4: Design choices for RobusTok post-training. finer detailed information. In contrast, the latent space without latent perturbation distributes usage more uniformly across tokens. 1 2 3 7.91 0.81 rFID pFID Baseline gFID w./o. CFG CFG ID Method + Codebook size 4096 + Latent Perturbation β = 0.5 + Latent Perturbation β = 0.1 Perturbation selection & annealing strategy. As shown in Table 5, we conduct an ablation to determine the optimal selection of perturbation hyperparameters. Our results indicate that using large perturbation parameter, e.g., β = 0.5, degrades the models reconstruction capability and adversely affects generative performance. Furthermore, training without annealing strategy leads to mode collapse and loss of generation diversity, whereas annealing to zero results in an overly deterministic tokenizer, diminishing the flexibility observed in Fig. 7. We find that annealing to half strikes balance between robustness and adaptability, preserving essential latent properties while improving the quality of generated outputs. Table 5: Ablation of RobusTok. gFID with classifierfree guidance (CFG) uses the constant schedule for LlamaGen and the linear schedule for RAR. + Perturbation annealing to 0 + Perturbation annealing to 0.5 6.98 4.52 3. 0.91 3.97 1.58 7.91 9.31 4.60 4.13 5.40 3.93 5.32 4.62 0.97 1.02 4.89 2. 1.97 1.85 14.64 4.48 5 6 Ablation for tokenizer post-training. We conduct systematic ablation study to investigate the impact of different design choices in tokenizer post-training. As shown in Table 4, increasing the amount of training data, or adjusting σ does not necessarily lead to performance improvements. Instead, we find that the stability of training is the key factor for post-training effectiveness. This also explains why our RobusTok achieves better performance improvement after post-training, as its decoder is inherently more robust due to the unique latent perturbation employed during pretraining. More experiment considering other tokenizers can be refered to the Appendix. Qualitative results. We visualize images generated before and after tokenizer post-training in Fig. 8. More generated result can be found in the Appendix."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this paper, we explore the discrepancy between reconstruction and generation in tokenizer. To address this, we introduce novel tokenizer training scheme including plug-and-play latentperturbation main-training to facilitate the construction of latent space, and lightweight posttraining stage to mitigate the degradation cased by distribution difference between generated and reconstructed latent space. Extensive experiments across both autoregressive and diffusion-based generators validate the effectiveness of our approach. We hope our research can shed some light in the direction towards more efficient image representation and generation models."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 CODEBOOK SIZE SELECTION Figure 9: T-SNE visualization of latent space in baseline with varying codebook sizes setting: (a) 1024, (b) 4096, and (c) 8192. Each subfigure presents embeddings derived from (left) 1,000, (middle) 10,000, and (right) 50,000 samples from the ImageNet validation set. Compared to larger codebook sizes, XQGAN-1024 fails to maintain well-structured latent space, leading to increased fragmentation and reduced robustness. 512 22500 1024 1637613 2048 1253384 Cluster Number SSE. As described in ablation, we initialize our tokenizer with XQGAN-8192 Li et al. (2024d) as our baseline. Motivated by insights from Yu et al. (2024b); Weber et al. (2024) and our own benchmarking, we aim to reduce the codebook size for more compact representation while preserving high reconstruction fidelity and generative quality. However, as shown in Fig. 9, the latent space of images in XQGAN-1024 appears highly fragmented, resulting in notable robustness discrepancies compared to tokenizers with larger codebooks, such as XQGAN-8192 and XQGAN-16384. Table 6: K-means clustering analysis of DINO features in ImageNet validation set. SSE. denotes as the Sum of Squared Error. The subscript values represent the difference in SSE. relative to the previous cluster number, indicating the reduction in error as the number of clusters increases. 16384 473138 8192 611317 4096 928325 To better understand this, we analyze DINO features on ImageNet and apply k-means clustering to feature embeddings. As shown in Table 6, the results of the clustering of k-means, evaluated using the elbow method, indicate decreasing improvements in the Sum of Squared Errors (SSE) as the number of clusters increases beyond 4096. The reduction in SSE slows significantly at this point, suggesting that further increasing the number of clusters yields only marginal benefits. Based on this observation, we select = 4096 as the codebook size for our tokenizer."
        },
        {
            "title": "Under review",
            "content": "T-SNE visualization of DINO Pixel features. T-SNE visualization of DINO Class features. Figure 10: Visualization of DINO features in ImageNet Validation Set. (b) Visualization of gFID trends for RAR, XQGAN, and Ours with (left) and without (right) CFG. (a) RAR training loss (left) and accuracy (right) for None, Half, and Zero annealing strategies. A.2 LOSS FUNCTION. Figure 11: RAR training. The RobusTok is trained with composite losses including reconstruction loss Lrecon, vector quantization loss LV Esser et al. (2021), adverserial loss Lad Karras et al. (2019), Perceptual loss LP Ledig et al. (2017), and semantic loss Lclip Li et al. (2024c): = λrecLrec + λV QLV + λadLad + λP LP + λsemLsem. (6) Specifically, the reconstruction loss measures the L2 distance between the reconstructed image and the ground truth; vector quantization loss encourages the encoded features and its aligned codebook vectors; adversarial loss ensures that the generated images are indistinguishable from real ones; perceptual loss compares high-level feature representations to capture structural differences; and semantic loss performs semantic regularization between semantic tokens and the pre-trained DINOv2 Oquab et al. (2023) features. The only detail that requires special attention in our proposed latent perturbation is that the perturbation is added after the VQ/commitment loss, thereby emphasizing decoder robustness. Although we do not directly modify the encoder loss, the perturbation indirectly improves the encoders representation quality by exposing the decoder to perturbed latents and enforcing stable reconstruction under noisy conditions. DINO supervision. As shown in Fig. 10, we visualize the means of DINO pixel features and DINO class features. We observe that DINO class features exhibit more structured representation compared to pixel-level features, which appear to be more scattered. Since the purpose of DINO features in our model is to provide supervision, the structured nature of class features makes them more suitable choice to guide the learning process. A.3 RAR TRAINING We follow the RAR training setting to validate the performance of our RobusTok. Specifically, as shown in Fig. 11, we evaluate RAR, XQGAN (our baseline), and our proposed RobusTok during"
        },
        {
            "title": "Under review",
            "content": "Codebook Size Method Tokenizer Type Tokenizer Generator rFID pFID gFID gFID (CFG) 16384 8192 1024 VQGAN-16384 (Esser et al., 2021) LlamaGen (Sun et al., 2024) IBQ-16384 (Shi et al., 2024) VQGAN-LC (Zhu et al., 2024a) IBQ-8192 (Shi et al., 2024) TiTok (Yu et al., 2024d) XQGAN-8192 (Li et al., 2024d) Non-semantic Non-semantic Non-semantic Semantic Non-semantic Semantic Semantic XQGAN-4096 (Li et al., 2024d) RobusTok (Ours) Semantic Semantic + Robust MaskGIT (Chang et al., 2022) IBQ-1024 (Shi et al., 2024) Non-Semantic Non-Semantic 4.50 2.19 1.41 3.27 1.87 1.03 0.81 0.91 1. 2.28 2.24 18.18 13.12 16.35 16.78 19.62 3.55 7.91 6.98 2.28 4.20 6.37 37.39 26.34 30.19 31. 30.91 25.66 25.43 13.58 9.47 18.02 35.33 14.80 8.61 11.01 11.80 10.85 8.84 10.18 6.91 5. 5.85 11.01 Table 7: Benchmark of tokenizers with the same LlamaGen-B generator. For fair comparison, the gFID with classifier-free guidance utilizes the same classifier value and schedule. All the tokenizers share the same 16 16 latent shape. We discuss the reason of choosing codebook size 4096 to train RobusTok in the ablation. More benchmarking results with larger generators are available in the appendix. denotes semantics captured with linear projection. All metrics, i.e., rFID, pFID and gFID, are the smaller the better. Codebook Size Method Tokenizer Type Tokenizer Generator rFID pFID gFID gFID (CFG) 8192 4096 1024 VQGAN-16384 (Esser et al., 2021) LlamaGen (Sun et al., 2024) IBQ-16384 (Shi et al., 2024) VQGAN-LC (Zhu et al., 2024a) IBQ-8192 (Shi et al., 2024) TiTok (Yu et al., 2024d) XQGAN-8192 (Li et al., 2024d) Non-semantic Non-semantic Non-semantic Semantic Non-semantic Semantic Semantic XQGAN-4096 (Li et al., 2024d) RobusTok (Ours) Semantic Semantic + Robust MaskGIT (Chang et al., 2022) IBQ-1024 (Shi et al., 2024) Non-Semantic Non-Semantic 4.50 2.19 1.41 3. 1.87 1.03 0.81 0.91 1.02 2.28 2.24 18.18 13.12 16.35 16.78 19.62 3.55 7.91 6.98 2. 4.20 6.37 20.89 8.61 21.57 17.55 21.05 14.51 14.64 7.90 6.47 12.37 23.89 6.23 4.40 5.53 5. 5.41 4.47 4.48 4.13 3.51 3.60 5.53 Table 8: Tokenizer benchmarking for LlamaGen-L. All metrics, i.e., rFID, pFID and gFID, are the smaller the better. training. We observe that XQGAN achieves faster convergence speed and better performance without CFG; however, its final performance, with gFID of 2.22 under classifier-free guidance (CFG), remains suboptimal compared to RAR. Our RobusTok, inheriting the structural advantages of the semantic tokenizer while incorporating robust latent space, not only achieves faster convergence but also outperforms both XQGAN and vanilla RAR in final generative quality, demonstrating its effectiveness in preserving semantic consistency and enhancing feature representation. This highlights promising direction for designing more robust training schemes to further improve generative performance. Furthermore, the tokenizer without annealing exhibits strong convergence but compromises diversity, annealing to zero offers limited improvement over the baseline, while our annealing strategy provides balance between generation diversity and quality. A.4 PFID RESULTS IN LLAMAGEN-L As shown in Table 8 and Fig. 12, we further evaluate our perturbed FID (pFID) in the LlamaGen-L setting. Although the results contain some outliers, pFID still shows stronger correlation with gFID than with rFID. A.5 SDEDIT FOR POST-TRAINING For diffusion-based generation model, we employ SDEdit (Meng et al., 2021) to bridge reconstruction and generation. During sampling with pre-trained diffusion model, we start from Gaussian"
        },
        {
            "title": "Under review",
            "content": "(a) rFID vs. gFID with and without CFG. (b) pFID vs. gFID with and without CFG. Figure 12: Comparison of reconstructed FID relation to generative FID with perturbed FID relation to generative FID. All generators follow LlamaGen-L training setting. denotes as codebook size Figure 13: Detailed t-SNE visualization of latent space of tokenizer training with and without our proposed latent perturbation. noise and gradually denoise it to obtain an image sample. In SDEdit, however, the process is initialized not from pure noise but from real image corrupted by controlled noise level. Following the preservation ratio σ in the autoregressive case, we also define σ for diffusion models as the fraction of the original latent space preserved after adding noise, formulated as σ = 1 (7) where denotes the starting timestep and determines the total number of diffusion steps. By varying this noise level, we interpolate between reconstruction (small noise, more structure preserved) and generation (large noise, less structure preserved), making SDEdit natural diffusion counterpart of our proposed method in autoregressive model. A.6 POST-TRAINING RESULTS To find the optimal result for different methods under tokenizer post-training, we systematically design ablation studies for each tokenizer, as shown in Tables 9 and 10. We find that, although our RobusTok is relatively insensitive to the choice of σ, other tokenizers exhibit noticeable performance variation across different σ values, highlighting the importance of proper preservation ratio selection for stable post-training. Epochs baseline σ = 0.7 σ = 0.8 σ = 0.9 σ = 0.95 Epochs baseline σ = 0.6 σ = 0.7 σ = 0.8 σ = 0.9 10 3.80 4.60+0.80 4.50+0.70 4.06+0.26 4.01+0.21 3.770.03 3.750.05 3.510.29 3.610.19 10 3.84 4.15+0.31 4.31+0.47 3.88+0.04 4.00+0.16 3.680.16 3.790.05 3.750.09 3.740.10 (a) LlamaGen (b) GigaTok Table 9: gFID in autoregressive models under different preservation ratio σ and training epochs. For fair comparison, we randomly select 200,000 images from ImageNet training set. Baseline represents the original gFID using original checkpoints from (a) LlamaGen and (b) GigaTok."
        },
        {
            "title": "Under review",
            "content": "Figure 14: qualitative analysis of tokenizers in our latent perturbation. Epoch baseline σ = 0.7 σ = 0.8 σ = 0. Epoch baseline σ = 0.7 σ = 0.8 σ = 0.9 10 1.87 1.820.05 1.730.14 1.760.11 1.790.08 1.680.19 1.90+0.03 10 20 1. 2.00+0.26 2.23+0.49 1.81+0.07 2.12+0.38 1.700.04 1.85+0.11 (a) MAETok wo. finetuning (b) MAETok w. finetuning Table 10: gFID under different SDEdit strength and training epochs. For fair comparison, we randomly select 200,000 images from ImageNet training set. Baseline represents the original gFID using original checkpoints from MAETok (a) without finetuning and (b) with latent noise finetuning."
        },
        {
            "title": "Under review",
            "content": "A.7 LATENT PERTURBATION V.S. OTHER NOISES To avoid potential misunderstanding, we aim to discuss the difference between our proposed latent perturbation and other noises used in generative models. Latent perturbation: Latent perturbation is random noise manually added to the latent space based on the pattern we observed during the real sampling errors. Specifically, it is added in cluster-based manner enlarging the decision boundary and zero-shot generalization during inference. Diffusion noise: Diffusion noise is scheduled noise added to enable the reverse process using diffusion sampler. It follows pre-defined schedule to systematically disrupt the latent space. Gaussian noise in VAE: VAEs reparameterization employs gaussian noise to decompose the mean value and randomness of the distribution to enable the gradient backpropagation. A.8 VISUALIZATION We demonstrate images generated by our approach as shown in Fig. 15. To further illustrate the effect of tokenizer post-training, we also present several failure cases before and after post-training for comparison. As shown in Fig. 16, Our tokenizer post-training is able to recover structural consistency, improve color fidelity, and sharpen local textures from original failed case. Figure 15: Visualization of 256 256 image within ImageNet class."
        },
        {
            "title": "Under review",
            "content": "Figure 16: More visualization for the improvement of tokenizer post-training in failed case."
        },
        {
            "title": "REFERENCES",
            "content": "Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William T. Freeman. Maskgit: Masked generative image transformer, 2022. URL https://arxiv.org/abs/2202.04200. Hao Chen, Yujin Han, Diganta Misra, Xiang Li, Kai Hu, Difan Zou, Masashi Sugiyama, Jindong Wang, and Bhiksha Raj. Slight corruption in pre-training data makes better diffusion models. arXiv preprint arXiv:2405.20494, 2024a. Hao Chen, Yujin Han, Fangyi Chen, Xiang Li, Yidong Wang, Jindong Wang, Ze Wang, Zicheng Liu, Difan Zou, and Bhiksha Raj. Masked autoencoders are effective tokenizers for diffusion models. arXiv preprint arXiv:2502.03444, 2025a. Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024b. Yinbo Chen, Rohit Girdhar, Xiaolong Wang, Sai Saketh Rambhatla, and Ishan Misra. Diffusion autoencoders are scalable image tokenizers. arXiv preprint arXiv:2501.18593, 2025b. Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers, 2023. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Prafulla Dhariwal and Alex Nichol. Diffusion models beat gans on image synthesis, 2021. URL https://arxiv.org/abs/2105.05233. Xiaoyi Dong, Jianmin Bao, Ting Zhang, Dongdong Chen, Weiming Zhang, Lu Yuan, Dong Chen, Fang Wen, Nenghai Yu, and Baining Guo. Peco: Perceptual codebook for bert pre-training of"
        },
        {
            "title": "Under review",
            "content": "vision transformers. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 552560, 2023. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. URL https://arxiv.org/abs/2010.11929. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431, 2024. Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1600016009, 2022. Wanggui He, Siming Fu, Mushui Liu, Xierui Wang, Wenyi Xiao, Fangxun Shu, Yi Wang, Lei Zhang, Zhelun Yu, Haoyuan Li, et al. Mars: Mixture of auto-regressive models for fine-grained text-to-image synthesis. arXiv preprint arXiv:2407.07614, 2024. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017. Geoffrey Hinton and Ruslan Salakhutdinov. Reducing the dimensionality of data with neural networks. science, 313(5786):504507, 2006. Mengqi Huang, Zhendong Mao, Zhuowei Chen, and Yongdong Zhang. Towards accurate image In Procoding: Improved autoregressive image generation with dynamic vector quantization. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 22596 22605, 2023. Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 44014410, 2019. Dongwon Kim, Ju He, Qihang Yu, Chenglin Yang, Xiaohui Shen, Suha Kwak, and Liang-Chieh Chen. Democratizing text-to-image masked generative models with compact text-aware onedimensional tokens. arXiv preprint arXiv:2501.07730, 2025. Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. Christian Ledig, Lucas Theis, Ferenc Huszar, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 46814690, 2017. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1152311532, 2022a."
        },
        {
            "title": "Under review",
            "content": "Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization, 2022b. URL https://arxiv.org/abs/2203. 01941. Tianhong Li, Dina Katabi, and Kaiming He. Return of unconditional generation: self-supervised representation generation method, 2024a. URL https://arxiv.org/abs/2312.03701. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization, 2024b. URL https://arxiv.org/abs/2406. 11838. Xiang Li, Jinglu Wang, Xiaohao Xu, Xiao Li, Bhiksha Raj, and Yan Lu. Robust referring video object segmentation with cyclic structural consensus. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2223622245, 2023a. Xiang Li, Jinglu Wang, Xiaohao Xu, Muqiao Yang, Fan Yang, Yizhou Zhao, Rita Singh, and Bhiksha Raj. Towards noise-tolerant speech-referring video object segmentation: Bridging speech In Proceedings of the 2023 Conference on Empirical Methods in Natural Language and text. Processing, pp. 22832296, 2023b. Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Bhiksha Raj, and Zhe Lin. Imagefolder: Autoregressive image generation with folded tokens. arXiv preprint arXiv:2410.01756, 2024c. Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Jiuxiang Gu, Jindong Wang, Zhe Lin, and Bhiksha Raj. Xq-gan: An open-source image tokenization framework for autoregressive generation. arXiv preprint arXiv:2412.01762, 2024d. Xiang Li, Kai Qiu, Hao Chen, Jason Kuen, Zhe Lin, Rita Singh, and Bhiksha Raj. Controlvar: Exploring controllable visual autoregressive modeling. arXiv preprint arXiv:2406.09750, 2024e. Xiang Li, Kai Qiu, Jinglu Wang, Xiaohao Xu, Rita Singh, Kashu Yamazaki, Hao Chen, Xiaonan Huang, and Bhiksha Raj. 2-bench: Benchmarking the robustness of referring perception models under perturbations. In European Conference on Computer Vision, pp. 211230. Springer, 2024f. Xiang Li, Jinglu Wang, Xiaohao Xu, Xiulian Peng, Rita Singh, Yan Lu, and Bhiksha Raj. Qdformer: towards robust audiovisual segmentation in complex environments with quantization-based seIn Proceedings of the IEEE/CVF Conference on Computer Vision and mantic decomposition. Pattern Recognition, pp. 34023413, 2024g. Zhuoyan Luo, Fengyuan Shi, Yixiao Ge, Yujiu Yang, Limin Wang, and Ying Shan. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. arXiv preprint arXiv:2108.01073, 2021. Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: Vq-vae made simple, 2023. Keita Miwa, Kento Sasaki, Hidehisa Arai, Tsubasa Takahashi, and Yu Yamaguchi. One-d-piece: Image tokenizer meets quality-controllable compression. arXiv e-prints, pp. arXiv2501, 2025. David Mizrahi, Roman Bachmann, Oguzhan Kar, Teresa Yeo, Mingfei Gao, Afshin Dehghan, and Amir Zamir. 4m: Massively multimodal masked modeling. Advances in Neural Information Processing Systems, 36, 2024. Alex Nichol and Prafulla Dhariwal. Improved denoising diffusion probabilistic models, 2021. URL https://arxiv.org/abs/2102.09672. Ning, Li, Zhang, Geng, Dai, He, and Hu. All in tokens: Unifying output space of visual tasks via soft token. arxiv 2023. arXiv preprint arXiv:2301.02229."
        },
        {
            "title": "Under review",
            "content": "Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. Yatian Pang, Peng Jin, Shuo Yang, Bin Lin, Bin Zhu, Zhenyu Tang, Liuhan Chen, Francis EH Tay, Ser-Nam Lim, Harry Yang, et al. Next patch prediction for autoregressive visual generation. arXiv preprint arXiv:2412.15321, 2024a. Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. arXiv preprint arXiv:2412.01827, 2024b. William Peebles and Saining Xie. Scalable diffusion models with transformers, 2023. URL https: //arxiv.org/abs/2212.09748. Kai Qiu, Xiang Li, Hao Chen, Jie Sun, Jinglu Wang, Zhe Lin, Marios Savvides, and Bhiksha Raj. Efficient autoregressive audio modeling via next-scale prediction. arXiv preprint arXiv:2408.09027, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pp. 87488763. PMLR, 2021. Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019a. Ali Razavi, Aaron van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2, 2019b. URL https://arxiv.org/abs/1906.00446. Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Flowar: Scalewise autoregressive image generation meets flow matching. arXiv preprint arXiv:2412.15205, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. Highresolution image synthesis with latent diffusion models, 2022. URL https://arxiv.org/ abs/2112.10752. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in Neural Information Processing Systems, 29, 2016. Fengyuan Shi, Zhuoyan Luo, Yixiao Ge, Yujiu Yang, Ying Shan, and Limin Wang. Taming scalable visual tokenizer for autoregressive image generation. arXiv preprint arXiv:2412.02692, 2024. Jie Shi, Chenfei Wu, Jian Liang, Xiang Liu, and Nan Duan. Divae: Photorealistic images synthesis with denoising diffusion decoder, 2022. URL https://arxiv.org/abs/2206.00386. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models, 2022. URL https://arxiv.org/abs/2010.02502. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Richard Sutton. Learning to predict by the methods of temporal differences. Machine learning, 3:944, 1988. Yuhta Takida, Yukara Ikemiya, Takashi Shibuya, Kazuki Shimada, Woosung Choi, Chieh-Hsin Lai, Naoki Murata, Toshimitsu Uesaka, Kengo Uchida, Wei-Hsiang Liao, et al. Hq-vae: Hierarchical discrete representation learning with variational bayes. arXiv preprint arXiv:2401.00365, 2023."
        },
        {
            "title": "Under review",
            "content": "Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction, 2024. URL https://arxiv.org/abs/ 2404.02905. Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. Michael Tschannen, Cian Eastwood, and Fabian Mentzer. Givt: Generative infinite-vocabulary transformers. In European Conference on Computer Vision, pp. 292309. Springer, 2024. Arash Vahdat, Karsten Kreis, and Jan Kautz. Score-based generative modeling in latent space, 2021. URL https://arxiv.org/abs/2106.05931. Aaron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In International conference on machine learning, pp. 17471756. PMLR, 2016. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need, 2023. URL https://arxiv. org/abs/1706.03762. Pascal Vincent, Hugo Larochelle, Yoshua Bengio, and Pierre-Antoine Manzagol. Extracting and composing robust features with denoising autoencoders. In Proceedings of the 25th international conference on Machine learning, pp. 10961103, 2008. Huiyu Wang, Yukun Zhu, Hartwig Adam, Alan Yuille, and Liang-Chieh Chen. Max-deeplab: Endto-end panoptic segmentation with mask transformers, 2021. URL https://arxiv.org/ abs/2012.00759. Yuqing Wang, Shuhuai Ren, Zhijie Lin, Yujin Han, Haoyuan Guo, Zhenheng Yang, Difan Zou, arXiv preprint Jiashi Feng, and Xihui Liu. Parallelized autoregressive visual generation. arXiv:2412.15119, 2024. Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and LiangarXiv preprint Chieh Chen. Maskbit: Embedding-free image generation via bit tokens. arXiv:2409.16211, 2024. Junfeng Wu, Yi Jiang, Chuofan Ma, Yuliang Liu, Hengshuang Zhao, Zehuan Yuan, Song Bai, and Xiang Bai. Liquid: Language models are scalable multi-modal generators. arXiv preprint arXiv:2412.04332, 2024. Tianwei Xiong, Jun Hao Liew, Zilong Huang, Jiashi Feng, and Xihui Liu. Gigatok: Scaling visual tokenizers to 3 billion parameters for autoregressive image generation. arXiv preprint arXiv:2504.08736, 2025. Xiaohao Xu, Tianyi Zhang, Shibo Zhao, Xiang Li, Sibo Wang, Yongqi Chen, Ye Li, Bhiksha Raj, Matthew Johnson-Roberson, Sebastian Scherer, et al. Scalable benchmarking and robust learning for noise-free ego-motion and 3d reconstruction from noisy video. In The Thirteenth International Conference on Learning Representations. Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1037110381, 2024. Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, Ming-Hsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1045910469, 2023a."
        },
        {
            "title": "Under review",
            "content": "Lijun Yu, Jose Lezama, Nitesh B. Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander G. Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David A. Ross, and Lu Jiang. Language model beats diffusion tokenizer is key to visual generation, 2023b. Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David Ross, Irfan Essa, Yonatan Bisk, Ming-Hsuan Yang, et al. Spae: Semantic pyramid autoencoder for multimodal generation with frozen llms. Advances in Neural Information Processing Systems, 36, 2024a. Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024b. Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation. Advances in Neural Information Processing Systems, 37:128940128966, 2024c. Qihang Yu, Mark Weber, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. An image is worth 32 tokens for reconstruction and generation, 2024d. URL https://arxiv. org/abs/2406.07550. Yue Zhao, Yuanjun Xiong, and Philipp Krahenbuhl. Image and video tokenization with binary spherical quantization. arXiv preprint arXiv:2406.07548, 2024. Chuanxia Zheng, Long Tung Vuong, Jianfei Cai, and Dinh Phung. Movq: Modulating quantized vectors for high-fidelity image generation, 2022. URL https://arxiv.org/abs/2209. 09002. Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of vqgan to 100,000 with utilization rate of 99%. arXiv preprint arXiv:2406.11837, 2024a. Zhu, Su, Lu, Li, Wang, and Dai. Deformable detr: Deformable transformers for end-to-end object detection. arxiv 2020. arXiv preprint arXiv:2010.04159, 2010. Yongxin Zhu, Bocheng Li, Yifei Xin, and Linli Xu. Addressing representation collapse in vector quantized models with one linear layer. arXiv preprint arXiv:2411.02038, 2024b. Yongxin Zhu, Bocheng Li, Hang Zhang, Xin Li, Linli Xu, and Lidong Bing. Stabilize arXiv preprint the latent space for image autoregressive modeling: unified perspective. arXiv:2410.12490, 2024c."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Carnegie Mellon University",
        "University of Michigan"
    ]
}