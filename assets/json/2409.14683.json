{
    "paper_title": "Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling",
    "authors": [
        "Benjamin Clavi√©",
        "Antoine Chaffin",
        "Griffin Adams"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Over the last few years, multi-vector retrieval methods, spearheaded by ColBERT, have become an increasingly popular approach to Neural IR. By storing representations at the token level rather than at the document level, these methods have demonstrated very strong retrieval performance, especially in out-of-domain settings. However, the storage and memory requirements necessary to store the large number of associated vectors remain an important drawback, hindering practical adoption. In this paper, we introduce a simple clustering-based token pooling approach to aggressively reduce the number of vectors that need to be stored. This method can reduce the space & memory footprint of ColBERT indexes by 50% with virtually no retrieval performance degradation. This method also allows for further reductions, reducing the vector count by 66%-to-75% , with degradation remaining below 5% on a vast majority of datasets. Importantly, this approach requires no architectural change nor query-time processing, and can be used as a simple drop-in during indexation with any ColBERT-like model."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 3 2 ] . [ 1 3 8 6 4 1 . 9 0 4 2 : r Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling Benjamin Clavi√© bc@answer.ai Answer.AI Japan Antoine Chaffin antoine.chaffin@lighton.ai LightOn France Griffin Adams ga@answer.ai Answer.AI USA ABSTRACT Over the last few years, multi-vector retrieval methods, spearheaded by ColBERT, have become an increasingly popular approach to Neural IR. By storing representations at the token level rather than at the document level, these methods have demonstrated very strong retrieval performance, especially in out-of-domain settings. However, the storage and memory requirements necessary to store the large number of associated vectors remain an important drawback, hindering practical adoption. In this paper, we introduce simple clustering-based token pooling approach to aggressively reduce the number of vectors that need to be stored. This method can reduce the space & memory footprint of ColBERT indexes by 50% with virtually no retrieval performance degradation. This method also allows for further reductions, reducing the vector count by 66%- to-75% , with degradation remaining below 5% on vast majority of datasets. Importantly, this approach requires no architectural change nor query-time processing, and can be used as simple drop-in during indexation with any ColBERT-like model. CCS CONCEPTS Information systems Retrieval models and ranking. KEYWORDS Neural IR, Late Interaction, ColBERT, Multi-Vector Retrieval, Token Pruning, Pooling, Index Compression ACM Reference Format: Benjamin Clavi√©, Antoine Chaffin, and Griffin Adams. 2024. Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling. In Proceedings of (ArXiv Preprint). ACM, New York, NY, USA, 6 pages. https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "title": "1 INTRODUCTION\nDeep-learning based approaches are becoming increasingly popu-\nlar in retrieval. Effectively, this means using a neural network to\nassess the relevance of documents for a given query. Many different\napproaches to neural IR exist, such as single-vector dense represen-\ntations [32], learned sparse representations [8], or late-interaction",
            "content": "Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. ArXiv Preprint, June 0305, 2018, Woodstock, NY 2024 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 978-x-xxxx-xxxx-x/YY/MM https://doi.org/10.1145/nnnnnnn.nnnnnnn methods, such as ColBERT [11], which employ multi-vector representations by storing vector for each token in document. Using multiple vectors to represent the sequences allows for more fine-grained and expressive representation compared to compressing all the semantics into single vector. As result, this latter approach has frequently been shown to generalize better to out-of-domain settings than dense representations [16, 34]. These properties have led to these methods experiencing considerable amount of interest in the recent months1. However, the enhanced expressiveness of multi-vector representations comes at hefty storage and memory cost. It is relatively trivial to store millions of dense representations using common indexing methods such as HNSW [20]. On the other hand, ColBERT and similar approaches require an order of magnitude more vectors per document. Many techniques have been proposed to alleviate this issue: dimension reduction methods through learned projection layer [11], aggressive quantization using Inverted File [36]- Product Quantization [10] (IVF-PQ) indexing [27] and an optimised indexing and querying mechanism [26]. These methods, while bringing the storage requirement down to the same order of magnitude as single-vector methods, does not fully close the gap with storage and memory requirements of these approaches. This is especially apparent as dense singlevector representations can also be aggressively quantized [33], with relatively little performance loss when used in conjunction with quantization-aware training [24]. Moreover, the indexing methods required to achieve this level of compression with ColBERT add complex processing layers, constraining the index to specific format. This renders them considerably less flexible, notably in terms of document addition and deletion (CRUD) than commonly used dense indexing methods such as HNSW [20]. As result, ColBERT can appear as worse option for evolving corpora, limiting its practical use. Another line of work has focused on attempting to reduce the total number of vectors needed to be stored. However, the existing methods have not reached widespread adoption, either due to requiring modified pipeline and specific training for limited gains [9]; or as result of minimal storage reduction in comparison to the negative retrieval performance impact [14]. Recently, study has also confirmed the intuition that different tokens have vastly varying levels of importance for multi-vector retrieval performance [15]. This is in line with series of studies in the adjacent field of Computer Vision, which have introduced token 1Indeed, ColBERTv2 has gone from 40,000 monthly downloads in late 2023 to being one of the most downloaded models on the HuggingFace model hub, with over 5 million monthly downloads. See https://huggingface.co/colbert-ir/colbertv2.0 ArXiv Preprint, June 0305, 2018, Woodstock, NY Clavi√© et al. merging [2], which consists in averaging the representation of multiple tokens into single vector. These studies have shown that it is possible to merge tokens without requiring specific training, greatly improving the throughput of image processing [2] or image generation [3], with very minimal impact on task-specific performance. Contributions In this work, 1 we introduce method we call Token Pooling, which requires no specific training nor pipeline or model modification. This method consists in reducing the total number of vectors retained at indexing time, by average pooling token representations through simple clustering approaches. We explore different clustering methods and show that hierarchical clustering yields the best results. On variety of commonly used evaluation datasets, this method performs very strongly, allowing for storage cost reductions of up to 50% with no performance degradation on average, and storage cost reductions of 66% with fewer than 3% degradation. 2 We then demonstrate that this approach can be coupled with the usual ColBERT quantization pipeline described above, achieving even greater compression results. 3 Finally, we show that the approach also applies to more than just the ColBERTv2 model and the English language, by showing strong results with Japanese variant of ColBERT on Japanese data [6]."
        },
        {
            "title": "2 TOKEN POOLING\nTo try and mitigate the issues associated with the high number\nof vectors to store when using multi-vector retrieval methods, we\nintroduce a simple Token Pooling approach. This is applied at\nindexing time to reduce the effective number of tokens representing\na document. It can be used with any pre-trained ColBERT model,\nwithout any further training or modifications.",
            "content": "Our core approach is simple and works as two-step system: First, we devise way to group individual vectors together, using one of the three clustering methods detailed below. We then apply mean pooling in order to obtain single vector which contains an average representation of the cluster. The resulting set of pooled vectors serves as our new multi-vector document representation, and the original vectors are discarded. The main intuition behind this approach is the belief that there is considerable redundancy token vectors within as single document, and that tokens therefore have varying importance [15]. Within this assumption, pooling the most similar ones is unlikely to considerably modify the overall document representation. To control the level of compression, we introduce new variable called the pooling factor. This factor is, effectively, compression factor. For example, pooling factor of 2 reduces the total number of vectors stored by factor of 2, i.e. 50% reduction."
        },
        {
            "title": "2.1 Pooling Methods\nWe explore three pooling methods:",
            "content": "Sequential Pooling. This method acts as our baseline does not require any clustering. Tokens are pooled together based on the order in which they appear in the document, from left to right. In this setting, the pooling factor dictates the number of sequential tokens pooled together. We do not use sliding window, which means that each token is only ever pooled once. This baseline is inspired by the common intuition that the individual meaning of words is greatly influenced by its direct neighbours [5, 7]. K-Means based pooling. This method uses k-means clustering [18] based on the cosine distance between vectors. The pooling factor is used in this setting to define the total number of clusters, which is set at initial token count/Pooling Factor + 1. Hierarchical clustering based pooling. This method uses hierarchical clustering [21], again based on the cosine distance between vectors. We use Wards method [31] to produce our clusters, which intuitively would be well-suited for this task, as it would seek to minimize the distance between the original vector and the pooled outputs. Additionally, it has generally been observed to perform well for text data [12, 25]. Using this method, we effectively iteratively merge the vectors that minimize the ward distance. In this setting, the pooling factor is used to define the maximum number of clusters that can be formed, as constraint on the clustering algorithm. This effectively means that this method will result in at most initial token count/Pooling Factor + 1 clusters."
        },
        {
            "title": "3 EXPERIMENTAL SETTING\n3.1 Implementation\nModels We conduct the vast majority of our experiments with\nColBERTv2 [27], trained on English MS-Marco [22]. To assess that\nour method is not specific to English nor to ColBERTv2, we also\nconduct a smaller set of experiments on Japanese using a Japanese\nversion of ColBERT, JaColBERTv2 [6].",
            "content": "Clustering All clustering methods are implemented using existing libraries and widely used implementations. We use SciPy [30] for hierarchical clustering and simple, PyTorch-based [23] implementation for k-means clustering. We evaluate clustering on wide range of pooling factors: 2, 3, 4, 5, 6 and 8. Indexing All experiments are conducted using the official ColBERT implementation to encode both queries and documents. 2-bit quantization and PLAID [26] indexing are also performed with the original codebase. Experiments with non-quantized vectors are conducted using basic HNSW indexing implementation, via the voyager library. We provide further details on indexing parameters in Appendix A."
        },
        {
            "title": "3.2 Evaluation\nData We evaluate our method on a varied mix of datasets, in order\nto capture the impact of the approach in different domains. To do\nso, we select all small to mid-sized datasets from BEIR [29], the\nmost commonly used retrieval evaluation suite, with the exception\nof ArguAna2, and from LoTTe [27], another benchmark commonly\nused for multi-vector approaches. We define as \"small and mid-\nsized\" any dataset containing fewer than 500,000 documents. We\ndecide on this cutoff in order to appropriately explore a variety\nof data sizes while keeping down the cost of experimentation and\nanalysis. In total, this results in 6 datasets from BEIR and 3 from",
            "content": "2Due to sensibly diverging results between our ArguAna baseline reproduction and reported results in the literature, as well as the less representative nature of ArguAna, as it is an argument mining dataset, we have not included it in this study Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling ArXiv Preprint, June 0305, 2018, Woodstock, NY scifact scidocs BEIR nfc fiqa Avg 100.22 99.99 96.93 92.44 95.09 93.13 91.62 89.59 95.37 87.80 97.90 97.00 95.16 87. 99.75 99.65 98.17 94.62 103.01 95.91 102.02 100.71 101.10 96.58 102.33 98.41 94.91 86.17 100.62 99.03 97.03 90.67 97.35 96.16 94.36 91. 96.46 85.78 97.31 94.90 90.24 84.82 95.41 82.61 97.38 95.96 93.60 90.166 97.56 88.03 Hierarc. pool 2 pool 3 pool 4 pool 6 KMeans pool 2 pool 3 pool 4 pool Seq. pool 2 pool 4 Table 1: Relative performance of various token pooling methods, applied to 16-bit unquantized vectors with HNSW Indexing. score of 100 corresponds to the model performance without any pooling, and all results are relative to it. LoTTe. For LoTTe datasets, we use the search query subset. We evaluate token pooling with uncompressed vectors on only datasets with fewer than 100,000 documents, and compressed 2-bit vectors on all our chosen datasets. For the evaluation of JaColBERTv2, we use two commonly used datasets for Japanese retrieval evaluations, with the same evaluation setting as in the original JaColBERT report [6]: JSQuAD [13] and the Japanese split of MIRACL [35]. Metrics We report all results in terms of relative performance, where 100 represents the score obtained by the same model with no token pooling. For all datasets, the metrics used to compute this relative performance are based on the ones previously used in the literature. Respectively, NDCG@10 for all BEIR datasets, Success@5 for LoTTe datasets and Recall@5 for JSQuAD and MIRACL (Japanese split). All metrics are computed using the ranx [1] library."
        },
        {
            "title": "4 RESULTS\n4.1 Unquantized Results\nTable 1 presents the detailed results for various pooling factors\nand pooling methods. In the interest of simplicity and space, we\nonly report the results of sequential pooling for factors 2 and 4,\nas the observed performance degradation varies wildly between\ndatasets and increases is too quickly for this approach to be viable\nin comparison to the other ones. Worth noting however that, de-\nspite its overall noticeably weaker performance, sequential pooling\nperforms remarkably strongly on the scidocs dataset, reaching the\nstrongest performance at a pooling factor of 2, before degrading be-\nhind the other two methods at factor 4. An overview of the relative\nperformance degradation of the best-performing pooling method,\nhierarchical pooling, across pooling factors on all the small-sized\nevaluation datasets can be found in Figure 1 .",
            "content": "We observe that token pooling performs remarkably well in this setting, on all four datasets. In fact, pooling factor of 2, resulting in vector count reduction of 50%, actually slightly increases retrieval performance on average, doing so on 3 out of 4 datasets. pooling factor of 3 achieves an average performance degradation of less than 1%, despite reducing storage requirements by over 66%. We observe starker degradation from pooling factors of 4 onwards, with an average degradation of 3% and degradation of around 5% on two of the evaluated datasets. Figure 1: Relative performance degradation at various pooling factors using 16-bit vectors with HNSW indexing. Further compression continues to degrade performance, with prohibitive average degradation of 10% at pooling factor of 6. However, the very strong results of the lower pooling factors show that it is possible to drastically reduce the number of vectors needing to be stored for late-interaction retrieval, at virtually no cost to retrieval performance. These results are especially promising as the lower pool factors offer comparatively greater gain than subsequent increases."
        },
        {
            "title": "4.2 Quantized Results\nThe next experiment focuses on evaluating whether this method\ncan be combined with standard ColBERTv2 quantization, enabling\nits practical use on larger datasets.",
            "content": "As above, we report the detailed results in Table 2, with the full sequential results truncated, and the overview of relative performance degradation with hierarchical pooling in Figure 2. Immediately, we notice one obvious outlier: Touch√© [4]. Not only does performance never decrease with pooling, but it steadily increases, up to 42.16% increase in retrieval performance at pooling factor of 6. This dataset has frequently been noted as producing unusual results and recent study has shown that due to its nature as an argument-mining dataset repurposed for retrieval, it is an outlier in many ways and contains considerable noise [28]. As result, we choose to leave further exploration of this behavior for future work. There is second, less-pronounced, outlier in place of fiqa. However, unlike Touch√©, it does follow similar trend to other datasets, although the performance degrades noticeably faster as the pool factor increases. While we leave further analysis to future work, fiqa is highly specialized dataset within the financial domain, and its queries tend to focus on very fine-grained details [19], which could partially explain this quicker degradation. ArXiv Preprint, June 0305, 2018, Woodstock, NY scifact scidocs nfc fiqa BEIR trec covid Touch√© Avg w/o outliers Writing Recrea. LOTTE Hierarc. pool 2 pool 3 pool 4 pool 6 KMeans pool 2 pool 3 pool 4 pool 6 Seq. pool 2 pool 4 98.11 94.12 93.28 90.23 94.18 93.71 94.41 87.35 96.55 86.47 98.76 98.74 98.10 94.34 100.05 98.76 97.77 90. 98.87 94.15 93.22 91.23 93.25 89.57 87.89 85.80 98.16 89.80 84.46 74.63 89.18 83.23 80.54 72.28 102.95 101.27 101.05 101.45 103.11 101.75 99.66 96. 112.50 123.29 132.34 136.49 131.23 136.24 138.94 142.13 101.56 100.23 100.41 98.06 101.83 100.54 99.87 95.69 99.34 89.29 94.07 78. 89.75 74.13 94.67 78.48 84.16 91.20 93.09 83.01 99.67 97.07 96.41 94.31 97.65 95.95 94.93 89. 96.16 83.17 99.65 97.78 96.73 92.99 97.31 96.15 94.39 90.42 88.90 80.49 100.61 96.07 93.04 84.72 96.97 94.10 90.92 81. 93.04 80.79 Clavi√© et al. Overall Avg w/o outliers 99.64 97.11 96.02 92.36 97.66 95.77 94.14 88.82 94.62 83. Lifestyle 98.54 97.62 96.70 91.58 98.72 96.34 93.95 90.51 Avg 99.60 97.16 95.49 89.76 97.67 95.53 93.09 87. 95.79 90.66 92.58 83.98 Table 2: Relative performance of various token pooling methods, applied to 2-bit quantized vector with PLAID Indexing. score of 100 corresponds to the model performance without any pooling, and all results are relative to it. Discounting outliers, we observe overall similar results in the quantized settings as with unquantized vectors. Performance degradation appears to be slightly more pronounced but still very contained, with an average degradation of 1.34% at pool factor of 2, and 3.52% at pool factor of 3. While nearly all datasets share the same overall trend, there are noticeable variations between datasets, with some beginning to decrease more rapidly at higher pool factors, while others retain performance, even with aggressive pooling. Interestingly, we note that similarly to the 16-bit setting, KMeans clustering outperforms hierarchical clustering on the scidocs dataset at pool factor 2, and observe similar behaviour. However, the performance of hierarchical clustering still noticeably outperforms it on average at every pool factor, and widens the gap at higher pool factors."
        },
        {
            "title": "4.3 Vector Count & Storage Reduction\nTable 3 provides a comparison in the number of vector stored as well\nas the disk footprint of storing the full index. We use TREC-Covid\nat a truncated length of 256 tokens per document (ColBERTv2‚Äôs\ndefault document length [27]) for these calculations. We report the\nnumber of vectors and index size for 16-bit single-vector dense\nrepresentations in an HNSW index, as well as PLAID-indexed Col-\nBERT at various pooling factors. Note that the index size reduction\nis slightly lower than the vector count reduction, due to the indexing\noverhead introduced by PLAID [26].",
            "content": "We choose to report 16-bit for dense vectors and 2-bit for PLAID, as both methods experience virtually no performance degradation, therefore comparing index without quantization-related performance compromises. Index Size 345 MB 16-bit Dense Vector 2-bit PLAID Index 760 MB 388 MB 260 MB 195 MB 131 MB pool 2 pool 3 pool 4 pool 6 Table 3: An overview of index size (in Megabytes) between single-vector representation and PLAID ColBERT indexes. Figure 2: Relative performance degradation at various pooling factors using PLAID-indexed 2-bit vectors. fiqa at factor 6 is truncated for readability."
        },
        {
            "title": "4.4 Japanese Results\nFinally, Table 4 shows the results of token pooling with hierarchical\nclustering applied to Japanese corpora, using the JaColBERTv2 [6]\nmodel. This evaluation is performed only in the quantized setting,\nwith 2-bit compression on a PLAID [26] index.",
            "content": "While not as thoroughly explored as the English ones, these results show that the pooling approach is neither unique to the ColBERTv2 model nor the English language, as we observe similar pattern. Performance degradation is minimal at lower pooling factors of 2 and 3, despite the impressive compression. Reducing the Footprint of Multi-Vector Retrieval with Minimal Performance Impact via Token Pooling ArXiv Preprint, June 0305, 2018, Woodstock, NY Hierarc. pool 2 pool 3 pool 4 pool 6 JSQuAD MIRACL Avg 99.68 98.62 97.69 96. 100.03 97.75 98.26 93.97 99.855 98.185 97.975 95.095 Table 4: Relative performance of hierarchical clusteringbased pooling on two Japanese datasets, using quantized 2-bit vectors and PLAID indexing."
        },
        {
            "title": "5 CONCLUSION\nIn this paper, we introduced token pooling, a simple approach\nleveraging existing clustering methods and requiring no training\nnor model modifications to very effectively reduce the numbers\nof tokens needed to store for multi-vector retrieval models such\nas ColBERT. Our results show that the number of vectors, can\nbe reduced by 50% with little-to-no performance degradation on\nthe majority of datasets evaluated, thus drastically reducing the\nsize of ColBERT indexes. Reducing the vector count by 66% still\nexhibits minimal degradation, while further compression results in\nincreasing performance degradation. Notably, this method can lead\nto ColBERT being applied to a broader range of uses, as it facilitates\nthe use of addition/deletion (CRUD)-friendly indexing methods such\nas HNSW. Our experiments also show that these results hold true\neven when combined with ColBERT‚Äôs 2-bit quantization process,\nallowing for even greater compression than previously possible.\nWe also reproduce our results using a Japanese ColBERT model,\nshowing that this is not limited to a single model, nor to English\ndocuments. With this paper focusing on introducing this method\nand demonstrating its empirical performance, we hope that our\nfindings will help support future research in better understanding\nthe role of individual tokens in multi-vector retrieval and develop\neven stronger compression methods.",
            "content": "ACKNOWLEDGMENTS We would like to thank Omar Khattab for his valuable and enthusiastic encouragements while working on this paper. REFERENCES [1] Elias Bassani. 2022. ranx: blazing-fast python library for ranking evaluation and comparison. In European Conference on Information Retrieval. Springer, 259264. [2] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. 2022. Token merging: Your vit but faster. arXiv preprint arXiv:2210.09461 (2022). [3] Daniel Bolya and Judy Hoffman. 2023. Token merging for fast stable diffusion. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 45994603. [4] Alexander Bondarenko, Maik Fr√∂be, Meriem Beloucif, Lukas Gienapp, Yamen Ajjour, Alexander Panchenko, Chris Biemann, Benno Stein, Henning Wachsmuth, Martin Potthast, et al. 2020. Overview of Touch√© 2020: argument retrieval. In Experimental IR Meets Multilinguality, Multimodality, and Interaction: 11th International Conference of the CLEF Association, CLEF 2020, Thessaloniki, Greece, September 2225, 2020, Proceedings 11. Springer, 384395. [5] Mikael Brunila and Jack LaViolette. 2022. What company do words keep? Revisiting the distributional semantics of JR Firth & Zellig Harris. arXiv preprint arXiv:2205.07750 (2022). [6] Benjamin Clavi√©. 2023. Jacolbert and hard negatives, towards better japanese-first embeddings for retrieval: Early technical report. arXiv preprint arXiv:2312.16144 (2023). [7] John Firth. 1957. synopsis of linguistic theory, 1930-1955. Studies in linguistic analysis (1957), 1032. [8] Thibault Formal, Benjamin Piwowarski, and St√©phane Clinchant. 2021. SPLADE: Sparse lexical and expansion model for first stage ranking. In Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 22882292. [9] Sebastian Hofst√§tter, Omar Khattab, Sophia Althammer, Mete Sertkan, and Allan Hanbury. 2022. Introducing neural bag of whole-words with colberter: Contextualized late interactions using enhanced reduction. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 737747. [10] Herve Jegou, Matthijs Douze, and Cordelia Schmid. 2010. Product quantization for nearest neighbor search. IEEE transactions on pattern analysis and machine intelligence 33, 1 (2010), 117128. [11] Omar Khattab and Matei Zaharia. 2020. Colbert: Efficient and effective passage search via contextualized late interaction over bert. In Proceedings of the 43rd International ACM SIGIR conference on research and development in Information Retrieval. 3948. [12] Tuomo Korenius, Jorma Laurikkala, Martti Juhola, and Kalervo J√§rvelin. 2006. Hierarchical clustering of Finnish newspaper article collection with graded relevance assessments. Information Retrieval 9 (2006), 3353. [13] Kentaro Kurihara, Daisuke Kawahara, and Tomohide Shibata. 2022. JGLUE: Japanese general language understanding evaluation. In Proceedings of the Thirteenth Language Resources and Evaluation Conference. 29572966. [14] Carlos Lassance, Maroua Maachou, Joohee Park, and St√©phane Clinchant. 2022. Learned token pruning in contextualized late interaction over BERT (colbert). In Proceedings of the 45th International ACM SIGIR Conference on Research and Development in Information Retrieval. 22322236. [15] Jinhyuk Lee, Zhuyun Dai, Sai Meher Karthik Duddu, Tao Lei, Iftekhar Naim, Ming-Wei Chang, and Vincent Zhao. 2024. Rethinking the role of token retrieval in multi-vector retrieval. Advances in Neural Information Processing Systems 36 (2024). [16] Simon Lupart, Thibault Formal, and St√©phane Clinchant. 2023. Ms-shift: An analysis of ms marco distribution shifts on neural retrieval. In European Conference on Information Retrieval. Springer, 636652. [17] Sean MacAvaney and Nicola Tonellotto. 2024. Reproducibility Study of PLAID. arXiv preprint arXiv:2404.14989 (2024). [18] James MacQueen et al. 1967. Some methods for classification and analysis of multivariate observations. In Proceedings of the fifth Berkeley symposium on mathematical statistics and probability, Vol. 1. Oakland, CA, USA, 281297. [19] Macedo Maia, Siegfried Handschuh, Andr√© Freitas, Brian Davis, Ross McDermott, Manel Zarrouk, and Alexandra Balahur. 2018. Www18 open challenge: financial opinion mining and question answering. In Companion proceedings of the the web conference 2018. 19411942. [20] Yu Malkov and Dmitry Yashunin. 2018. Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs. IEEE transactions on pattern analysis and machine intelligence 42, 4 (2018), 824836. [21] Fionn Murtagh and Pedro Contreras. 2012. Algorithms for hierarchical clustering: an overview. Wiley Interdisciplinary Reviews: Data Mining and Knowledge Discovery 2, 1 (2012), 8697. [22] Tri Nguyen, Mir Rosenberg, Xia Song, Jianfeng Gao, Saurabh Tiwary, Rangan Majumder, and Li Deng. 2016. Ms marco: human-generated machine reading comprehension dataset. (2016). [23] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. 2019. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems 32 (2019). [24] Neils Reimers. 2024. Cohere int8 & binary Embeddings - Scale Your Vector Database to Large Datasets. Technical Report. https://cohere.com/blog/int8-binaryembeddings [25] Niyaz Salih and Karwan Jacksi. 2020. Semantic Document Clustering using K-means algorithm and Wards Method. In 2020 International Conference on Advanced Science and Engineering (ICOASE). IEEE, 16. [26] Keshav Santhanam, Omar Khattab, Christopher Potts, and Matei Zaharia. 2022. PLAID: an efficient engine for late interaction retrieval. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 17471756. [27] Keshav Santhanam, Omar Khattab, Jon Saad-Falcon, Christopher Potts, and Matei Zaharia. 2021. Colbertv2: Effective and efficient retrieval via lightweight late interaction. arXiv preprint arXiv:2112.01488 (2021). [28] Nandan Thakur, Luiz Bonifacio, Maik Fr√∂be, Alexander Bondarenko, Ehsan Kamalloo, Martin Potthast, Matthias Hagen, and Jimmy Lin. 2024. Systematic Evaluation of Neural Retrieval Models on the Touch√© 2020 Argument Retrieval Subset of BEIR. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval. [29] Nandan Thakur, Nils Reimers, Andreas R√ºckl√©, Abhishek Srivastava, and Iryna Gurevych. 2021. Beir: heterogenous benchmark for zero-shot evaluation of information retrieval models. arXiv preprint arXiv:2104.08663 (2021). ArXiv Preprint, June 0305, 2018, Woodstock, NY Clavi√© et al. [30] Pauli Virtanen, Ralf Gommers, Travis Oliphant, Matt Haberland, Tyler Reddy, David Cournapeau, Evgeni Burovski, Pearu Peterson, Warren Weckesser, Jonathan Bright, et al. 2020. SciPy 1.0: fundamental algorithms for scientific computing in Python. Nature methods 17, 3 (2020), 261272. [31] Joe Ward Jr. 1963. Hierarchical grouping to optimize an objective function. Journal of the American statistical association 58, 301 (1963), 236244. [32] Andrew Yates, Rodrigo Nogueira, and Jimmy Lin. 2021. Pretrained transformers for text ranking: BERT and beyond. In Proceedings of the 14th ACM International Conference on web search and data mining. 11541156. [33] Jingtao Zhan, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2021. Jointly optimizing query encoder and product quantization to improve retrieval performance. In Proceedings of the 30th ACM International Conference on Information & Knowledge Management. 24872496. [34] Jingtao Zhan, Xiaohui Xie, Jiaxin Mao, Yiqun Liu, Jiafeng Guo, Min Zhang, and Shaoping Ma. 2022. Evaluating interpolation and extrapolation performance of neural retrieval models. In Proceedings of the 31st ACM International Conference on Information & Knowledge Management. 24862496. [35] Xinyu Zhang, Nandan Thakur, Odunayo Ogundepo, Ehsan Kamalloo, David Alfonso Hermelo, Xiaoguang Li, Qun Liu, Mehdi Rezagholizadeh, and Jimmy Lin. 2023. MIRACL: Multilingual Retrieval Dataset Covering 18 Diverse Languages. Transactions of the Association for Computational Linguistics 11 (2023), 11141131. [36] Justin Zobel and Alistair Moffat. 2006. Inverted files for text search engines. ACM computing surveys (CSUR) 38, 2 (2006), 6es. RETRIEVAL SETTING The entire retrieval pipeline uses the same approach as the standard ColBERTv2 [27] + PLAID [26], including the maxSim scoring function [11]. For querying the PLAID index, we use the best-performing query hyperparameters reported in recent reproduction study of PLAID [17]: nprobe=8, ùë°ùëêùë† =0.3 and ndocs=8192. For the HNSW index, we construct it using generous construction hyperparameters ùëÄ = 12 and ùê∏ùêπùëêùëúùëõùë†ùë°ùëüùë¢ùëêùë°ùëñùëúùëõ = 200, designed to optimise retrieval performance. At querying time, we use large values at candidate generation time to ensure the performance is not impacted by the use of approximate search. These hyperparameter choices effectively mean our results are similar to non-approximate searches. Otherwise, we use the default ColBERTv2 and JacolBERTv2 parameters, with respective document length of 256 and 300 tokens. We do not perform any particular optimisation step to maximise absolute retrieval performance, as we are focused on the relative performance of different methods. The exact same settings are used for both the baseline runs, and all token pooling methods compared to them."
        }
    ],
    "affiliations": [
        "Answer.AI Japan",
        "Answer.AI USA",
        "LightOn France"
    ]
}