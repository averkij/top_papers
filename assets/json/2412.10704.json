{
    "paper_title": "VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation",
    "authors": [
        "Manan Suri",
        "Puneet Mathur",
        "Franck Dernoncourt",
        "Kanika Goswami",
        "Ryan A. Rossi",
        "Dinesh Manocha"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Understanding information from a collection of multiple documents, particularly those with visually rich elements, is important for document-grounded question answering. This paper introduces VisDoMBench, the first comprehensive benchmark designed to evaluate QA systems in multi-document settings with rich multimodal content, including tables, charts, and presentation slides. We propose VisDoMRAG, a novel multimodal Retrieval Augmented Generation (RAG) approach that simultaneously utilizes visual and textual RAG, combining robust visual retrieval capabilities with sophisticated linguistic reasoning. VisDoMRAG employs a multi-step reasoning process encompassing evidence curation and chain-of-thought reasoning for concurrent textual and visual RAG pipelines. A key novelty of VisDoMRAG is its consistency-constrained modality fusion mechanism, which aligns the reasoning processes across modalities at inference time to produce a coherent final answer. This leads to enhanced accuracy in scenarios where critical information is distributed across modalities and improved answer verifiability through implicit context attribution. Through extensive experiments involving open-source and proprietary large language models, we benchmark state-of-the-art document QA methods on VisDoMBench. Extensive results show that VisDoMRAG outperforms unimodal and long-context LLM baselines for end-to-end multimodal document QA by 12-20%."
        },
        {
            "title": "Start",
            "content": "VisDoM: Multi-Document QA with Visually Rich Elements Using Multimodal Retrieval-Augmented Generation"
        },
        {
            "title": "Kanika Gowswami",
            "content": ", Puneet Mathur , Franck Dernoncourt , , Dinesh Manocha , Ryan A. Rossi University of Maryland, College Park"
        },
        {
            "title": "IGDTUW",
            "content": "manans@umd.edu, puneetm@adobe.com 4 2 0 2 4 1 ] . [ 1 4 0 7 0 1 . 2 1 4 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Understanding information from collection of multiple documents, particularly those with visually rich elements, is important for documentgrounded question answering. This paper introduces VisDoMBench, the first comprehensive benchmark designed to evaluate QA systems in multi-document settings with rich multimodal content, including tables, charts, and presentation slides. We propose VisDoMRAG, novel multimodal Retrieval Augmented Generation (RAG) approach that simultaneously utilizes visual and textual RAG, thereby combining robust visual retrieval capabilities with sophisticated linguistic reasoning. VisDoMRAG employs multi-step reasoning process encompassing evidence curation and chain-of-thought reasoning for concurrent textual and visual RAG pipelines. key novelty of VisDoMRAG is its consistency-constrained modality fusion mechanism, which aligns the reasoning processes across modalities at inference time to produce coherent final answer. This leads to enhanced accuracy in scenarios where critical information is distributed across modalities and improved answer verifiability through implicit context attribution. Through extensive experiments involving open-source and proprietary large language models, we benchmark state-of-the-art document QA methods on VisDoMBench. Extensive results show that VisDoMRAG outperforms unimodal and longcontext LLM baselines for end-to-end multimodal document QA by 12-20%."
        },
        {
            "title": "Introduction",
            "content": "In todays information-rich landscape, PDF documents play crucial role in storing and disseminating information across various domains, including finance, legal, scientific research, and more. These documents often contain rich blend of textual, visual, and tabular data, making them unique challenge for information retrieval systems. Unlike Figure 1: Multi-document QA systems require inferring relevant context from large volume of unstructured data, inherently making it more challenging task than single-document QA. structured formats like databases, PDFs are inherently unstructured, with diverse layouts combining paragraphs, images, charts, and tables. This complexity demands sophisticated multimodal processing techniques capable of interpreting both the textual and visual content. Effective handling of multimodal content from PDFs is essential for downstream tasks such as question-answering (Ding et al., 2022; Mathew et al., 2021), summarization (Pang et al., 2023), and knowledge extraction (Pal et al., 2023), where accurate and context-aware data extraction can significantly enhance decisionmaking processes. As result, developing advanced methods that can fully leverage the multimodal nature of PDF documents has become critical research challenge. In real-world document QA systems, queries are often directed over collection of source documents rather than single source, requiring the system to identify the document that contains the relevant answer. This reflects common scenarios in domains such as finance, science, and policy analysis, where users interact with large, varied document sets to find specific information. In these cases, the challenge lies in effectively localizing context relevant to the query, from large volume of information distributed across multiple documents (akin to finding \"needle in haystack (Wang et al., 2024b)). Multi-document QA datasets are scarce, with existing multi-document benchmarks (Bai et al., 2023; Wang et al., 2024c), predominantly focused on textual information, often overlooking the diverse content forms found in real-world documents, such as tables, charts, and visual elements. Visually rich elements, such as tables, charts, and slides, provide structured data and visual summaries that are critical for answering certain types of questions. Tables often present dense, organized information that cannot be captured through plain text. At the same time, charts and slides can visually depict trends, relationships, or distributions that require interpretation beyond textual descriptions. The absence of datasets that include these modalities limits the ability of current QA models to address complex, multimodal questions. For instance, answering financial or scientific question may require interpreting both numerical data in tables and trends in graphs alongside the surrounding text. In the context of visually rich content-based documents, existing RAG systems face critical limitation due to their reliance on singular modality (either text or vision) for retrieval. Text-based systems are proficient in linguistic reasoning but often overlook vital visual elements, such as tables and figures, that may contain key information. Conversely, multimodal RAG (Chen et al., 2022) systems that leverage vision-based retrieval can effectively extract visual data but are often constrained in endto-end performance by the LLMs visual reasoning abilities, as text often performs better than visual input when given the same context (Deng et al., 2024), which can be attributed to language bias in visual LLMs (Niu et al., 2021; Wang et al., 2024a), and visual hallucination (Ghosh et al., 2024). Main Results: We introduce VisDoMBench, the first multi-document, multi-modal QA dataset specifically designed to address rich visual content, including tables, charts, and slides. VisDoMBench encompasses diverse range of complex content and question types, along with annotated evidence, allowing for comprehensive evaluation of multimodal QA systems. In this work, we benchmark the performance of various visual and textual retrieval methods on VisDoMBench, providing insights into their effectiveness in handling visually rich, multi-document queries. Further, we propose VisDoMRAG, novel multimodal RAG approach that effectively performs modality fusion over textual and visual RAG pipelines, benefiting from the inherent strengths of both these approaches, unlike contemporary approaches, which perform only-text or only-visionbased retrieval. VisDoMRAG employs parallel RAG pipelines for text and visual elements, each with multi-step reasoning process involving evidence curation, chain-of-thought reasoning, and answer generation. The system then integrates the outputs from both pipelines using modality fusion, which imposes consistency constraint on the reasoning chains, ensuring inference-time alignment across the modalities reasoning processes to produce the final answer. VisDoMRAG offers several significant advantages over traditional unimodal or simpler multimodal systems. Firstly, it ensures comprehensive information utilization by fully leveraging both textual and visual cues, leading to more accurate and complete answers, particularly in scenarios where critical information is distributed across different modalities. Moreover, the evidence curation step provides an additional advantage of answer verifiability, since context attribution is built into our approach. We conduct experiments utilizing various open-source and closed-source LLMs, comparing multiple strategies such as long-context processing, textual RAG, and visual RAG, with our proposed system. We find that our VisDoMRAG improves end-to-end QA performance on our benchmarks, with performance gains in the range of 12%- 20%. Our main contributions are: VisDoMBench, novel multi-document, multimodal QA benchmark designed to address QA tasks across visually rich document content such as tables, charts, and slides, allowing for comprehensive evaluation of multimodal document QA systems. VisDoMRAG, novel multimodal RAG approach that effectively parallelly performs textual and visual RAG via Evidence Curation and Chain-of-Thought reasoning. The output reasoning chains from both the modalities are aligned using consistency analysis and resultant answers are ensembled together via LLM-based modality fusion to enhance visually-rich document QA. VisDoMRAG significantly outperforms strong baselines such as long-context processing, textual RAG, and visual RAG on the VisDoMBench corpus by 12-20% across various open and closed-source LLM settings."
        },
        {
            "title": "2 Related Work",
            "content": "Retrieval Augmented Generation While Large Language Models (LLMs) have achieved significant advancements, they still encounter challenges in integrating external knowledge and adapting to new, unseen data. Retrieval Augmented Generation (RAG) addresses these gaps by incorporating external information, enhancing the precision and reliability of LLM responses (Lewis et al., 2020). RAG is utilized across various downstream unimodal NLP tasks, including machine translation (Gu et al., 2018; He et al., 2021), dialogue generation (Cai et al., 2018), abstractive summarization (Peng et al., 2019), and knowledge-intensive generation (Izacard and Grave, 2020; Lewis et al., 2020). In visual question answering (VQA), (Lin and Byrne, 2022) addresses open-domain challenges by using object detection, image captioning, and optical character recognition (OCR) to transform target images into textual data. Moving beyond text-only contexts, MuRAG retrieves both text and image data, incorporating images as visual tokens (Chen et al., 2022). RAMM enhances performance by retrieving and encoding similar biomedical images and their captions through distinct networks (Yuan et al., 2023). Long Context Document Benchmarks The comparison of long context document question-answer benchmarks  (Table 1)  , highlights the diversity in content types, multi-document capabilities, and domains. Existing benchmarks such as L-Eval (An et al., 2023), Marathon (Zhang et al., 2023), and LooGLE (Li et al., 2023) primarily focus on textbased content from multi-domain sources but do not support multi-document inputs. LongBench (Bai et al., 2023) and Loong (Wang et al., 2024c) extend their evaluations to include multi-document settings, although they remain text-centric. Comparison with existing datasets: Certain benchmarks like MPDocVQA (Tito et al., 2023), UDA (Hui et al., 2024), and MMLONGBENCHDOC (Ma et al., 2024) expand the content spectrum by incorporating tables, charts, and slides, but they are limited to single-document question answering. In contrast, VisDoMBench supports multidocument question answering across various content types, including text, tables, charts, and slides, offering more comprehensive multi-domain evaluation framework."
        },
        {
            "title": "3 Problem Formulation",
            "content": "2, . . . , pi Ni Given query q, we have collection of documents = {d1, d2, . . . , dM }, wherein each document di may consist of set of Ni pages represented by = {pi 1, pi }. We aim to generate text ˆa for each query that accurately answers the user query. The answer generation relies on retrieving relevant evidence context from one or more documents. Each query may require information spread across different pages from one or more of the associated documents in D. We aim to propose framework that can accurately answer questions over collection of multi-page documents where the system first retrieves relevant evidence at the level of individual pages, paragraphs or text chunks, followed by using the retrieved context to generate answer text."
        },
        {
            "title": "VisDoMBench",
            "content": "Every data point in VisDoMBench can be expressed as triple (q, D, ˆa), where question is posed to set of documents D, with groundtruth answer ˆa. We re-purpose five existing document-QA datasets to form our benchmark. Table 2 summarises different data splits present in VisDoMBench, including summary statistics, QA type, and content type."
        },
        {
            "title": "4.1 VisDoMBench",
            "content": "Data Sourcing: In the curation of document question-answering datasets, we adhered to the following criteria: (1) the inclusion of visually rich content, encompassing tables, charts, and presentation slides; (2) the utilization of publicly accessible source documents; and (3) the presence of grounded evidence. These parameters were established to ensure the datasets relevance to multimodal information retrieval and their applicability to real-world question-answering tasks. Our corpus comprises test/eval sets sourced from several established datasets. We incorporated the PaperTab and FeTaTab splits from the UDA Benchmark(Hui et al., 2024), which in turn sourced these datasets from QASPER(Dasigi et al., 2021) and FeTaQA(Nan et al., 2022), respectively. For chartbased question-answering samples, we drew from SciGraphQA (Li and Tajbakhsh, 2023), which is multi-turn QA dataset on charts from scientific papers, and SPIQA(Pramanick et al., 2024), chart and table QA dataset system sourced from (Dasigi et al., 2021). Additionally, we included SlideVQA Benchmark Content Type Multi Document Domain L-Eval (An et al., 2023) LongBench (Bai et al., 2023) Marathon (Zhang et al., 2023) LooGLE (Li et al., 2023) MPDocVQA (Tito et al., 2023) Bench (Zhang et al., 2024) Ruler (Hsieh et al., 2024) Loong (Wang et al., 2024c) UDA (Hui et al., 2024) NarrativeQA (Koˇcisk`y et al., 2018) MMLONGBENCH-DOC (Ma et al., 2024) Text Text Text Text Text, Tables, Charts Text Text Text Text, Tables Text Text, Tables, Charts, Slides VisDoMBench (Ours) Text, Tables, Charts, Slides Multi-domain Wikipedia Multi-domain Multi-domain Multi-domain Multi-domain Wikipedia Multi-domain Multi-domain Movies and Shows Multi-domain Multi-domain Table 1: Comparison of long context document QA benchmarks with VisDoMBench. Dataset PaperTab FetaTab SciGraphQA SPIQA SlideVQA VisDoMBench Domain Wikipedia Scientific Papers Scientific Papers Scientific Papers Presentation Decks Combined Content Type Tables, Text Tables Charts Tables, Charts Slides Tables, Charts, Slides, Text Queries Docs Avg. Question Length Avg. Doc Length (Pages) Avg. Docs per Query Avg. Pages per Query 377 350 407 586 551 2271 297 300 319 117 244 1277 29.44 6.3 12.96 4.1 18.05 1.9 16.06 6.6 22.39 7.8 19.11 5.4 10.55 6.3 15.77 23.9 22.75 29.1 14.03 7.9 20.00 0.0 16.43 14.5 10.82 4.4 7.77 3.1 5.91 2.0 9.51 3.5 6.99 2.0 8.36 3.0 113.10 50.4 124.33 83.0 129.71 81.7 135.58 55.2 139.71 40.6 128.69 62. Table 2: Summary of data splits included in VisDoMBench. (Tanaka et al., 2023), multi-image, multi-hop QA dataset centered on presentation slide decks. Data Sampling: Sourced QA pairs need to be sampled to retain high quality samples. To maintain the integrity and uniqueness of our benchmark, we meticulously removed overlapping samples between PaperTab and SPIQA and implemented rigorous de-duplication of QA pairs across all included datasets. Further, we also perform questionlevel de-duplication to ensure similar questions are not repeated across different document collections. This ensures that QA systems are not rewarded disproportionately for better handling particular question types. For SciGraphQA, we filter out trivial questions related to layout and document metadata. From the remaining questions, we randomly sample 500 questions from the top 50%-ile of questions by length. The rationale for filtering on answer length filter is based on the heuristic that longer questions tend to be more specific, making them better suited for multi-document QA tasks, where specificity is crucial. For SlideVQA, we exclude single-hop questions, as they are generally non-specific and may have more than one correct answer from the document collection. We heuristically observe that multi-hop questions in this dataset are more likely to reference content from specific documents, thus making them better fit for multi-document setups. SciGraphQA and SPIQA contain questions specific to charts or tables extracted from scientific papers. We use the , 200 Pavg ArXiv API1 to extract full document PDFs. Document Augmentation: To simulate realistic multi-document settings, we augment each question across all data splits with varying number of distracting documents, (Di = ). We intend to keep the expected number of total pages per query between 50 to 200 to ensure that there is sufficient distracting content while maintaining the practical feasibility of contemporary long-context models. Hence, based on the average number of pages per document Pavg, we randomly sample the number of distracting documents to lie between the range [ 50 ]. Randomly sampling Pavg ensures that each benchmark instance contains diverse degree of multi-document evidence, allowing for more thorough evaluation of the QA models retrieval and reasoning capabilities. Query Augmentation: To address the challenge of ambiguous questions in datasets such as SciGraphQA, and PaperTab, we implement query augmentation procedure to create one-toone mapping between given question and the document(s) that exclusively answer it. Given an original question and the document containing answer, we utilize GPT-4o to generate more specific variations of the question, ensuring that the generated question can only be answered by the corresponding document. To maintain consistency, we constrain the LLM such that the answer to the generated question must match the provided answer. Once the augmented queries are generated, 1https://info.arxiv.org/help/api/index.html Figure 2: VisDoMRAG: Given set of documents, VisDoMRAG parallelly performs evidence-driven ➊ Visual RAG and ➋ Textual RAG, prompting the LLMs to answer query based on the respective retrieved context via Evidence Curation and Chain-of-Thought reasoning. The reasoning chains, and answers from the text and visual pipeline are ensembled together via ➌ Modality Fusion, where the outputs of both the modalities are aligned using consistency analysis on their reasoning chain to arrive at the final answer. human annotator reviews them using predefined rubric. The rubric guides the annotator to either select one of the five generated questions, retain the original question, or mark all questions (synthetic and actual) as ambiguous, in which case, the data point is discarded. The annotator is tasked with ensuring that the question is sufficiently specific by cross-referencing the localized evidence. Additionally, the annotator performs simple search across the entire document collection to verify that the question cannot be ambiguously answered by any other document. Experimental validation of one-to-one mapping of query with respect to the source document is given in the Appendix."
        },
        {
            "title": "VisDoMRAG",
            "content": "VisDoMRAG (Fig 2) is multimodal RAG approach for visually rich document QA consisting of two steps: (i) parallel evidence-driven unimodal (vision and textual) RAG pipelines, and (ii) Modality Fusion, which imposes consistency constraints to combine unimodal reasoning chains and arrive at final answer."
        },
        {
            "title": "5.1 Evidence-driven Parallel Unimodal RAG",
            "content": "Textual Retrieval Pipeline The textual RAG pipeline commences with the extraction of text from the set of documents utilizing Optical Character Recognition (OCR), followed by the segmentation of the extracted text into smaller, indexable chunks. Metadata indicating the source document and page number is preserved to facilitate traceability. These chunks are then indexed using text embedding model, enabling efficient retrieval. Relevant chunks are subsequently retrieved in relation to the specified query by text retrieval model and provided as contextual input to the LLM along with the query to generate textual answer response. Visual Retrieval Pipeline Simultaneously, the visual RAG pipeline is dedicated to the extraction and analysis of graphical elements, including images, charts, and diagrams. For given set of PDFs, visual embedding model generates an index at the page-level granularity for all documents. Relevant pages are then retrieved by visual retrieval model based on the specified query, and these pages are supplied to multimodal LLMs as visual context. This approach ensures that the model has access to critical visual information, employing its multimodal capability to utilize visual cues from document layout and graphical structures such as charts, diagrams and infographics. Prompting Strategy Both the textual and visual pipelines employ sophisticated three-step prompting strategy. Given set of context artifacts (page images or textual chunks), and query, the LLM is prompted with the following steps: 1. Evidence Curation: As first step, we prompt the LLM to extract relevant evidence from the retrieved context. The LLM must isolate key sections, such as paragraphs, tables, and figure details, that are most likely to address the query and verbalize them in structured form. This curation is crucial in multi-document setup, where nonuniform sources introduce irrelevant, distracting, or adversarial content. Accurately identifying relevant information enhances the models reasoning abilities by filtering out noise and helps mitigate LLM hallucinations. 2. Chain of Thought Reasoning: Extracting reasoning chains from multi-document artefacts can help contextualize curated evidence for final answer generation. We utilize Chain-of-Thought (CoT) (Wei et al., 2022) reasoning to link individual pieces of evidence that form coherent stepby-step narrative, ensuring that the answer is not only accurate but also logically derived from the evidence, leading to more robust and reliable responses. 3. Answer Generation: By leveraging insights from curated, contextually relevant evidence and applying CoT reasoning processes, the answer generation step produces responses that are both precise and well-justified. Additionally, we use targeted prompts to guide the LLM about the appropriate format for answer generation as per the question type. 5.2 Modality Fusion The modality fusion stage is key contribution in VisDoMRAG which differentiates it from simpler multimodal approaches. This stage takes as input the outputs from both the textual and visual pipelines, including the curated evidence, reasoning chains, and generated answers. The fusion process is orchestrated by prompting an LLM to evaluate the consistency between the reasoning chains produced by the textual and visual pipelines. This idea is inspired by self-consistency in CoT (Wang et al., 2023), which leveraged multiple thoughtchains and derives an answer based on the consistency of the individual chains results. Consistency constraint prompting is crucial for identifying and resolving any discrepancies, contradictions and filling in reasoning gaps that may arise from the separate processing of different modalities. When inconsistencies are detected, the LLM is tasked with reconciling the differences, potentially by reevaluating the evidence or adjusting the reasoning steps. This process ensures that the final answer integrates information from both modalities in coherent and logically consistent manner."
        },
        {
            "title": "6 Experiments",
            "content": "In our experiments, we first evaluate different retrieval and indexing models on our benchmark, followed by end-to-end QA evaluation using the identified optimal retrieval models with different LLMs. The experiments, baselines and evaluation are discussed below: 6.1 Retrieval Baselines: We use popular text based retrieval models: BM25 (Robertson et al., 1995) statistical baseline, and , MPNet (Song et al., 2020), MiniLM (Wang et al., 2020), and BGE-1.5 (Xiao et al., 2023), which represent SoTA dense retrieval baselines. Text extraction from PDF documents is performed using PyTesseract. The extracted text is then segmented into 3000-character chunks using the recursive-split method (Sarmah et al., 2023), with 10% overlap to mitigate information loss. For visual retrieval, we utilize recent advances late interaction based multi-vector retrieval models built on top of LLMs (Faysse et al., 2024), namely ColPali and ColQwen2, which have PaliGemma (Beyer et al., 2024) and Qwen2 (Yang et al., 2024) as their base LLMs. Readers are encouraged to refer to the appendix for further details of these models. Evaluation: Evidence extraction is assessed using ANLCS between ground truth evidence and retrieved chunks/pages. Document identification evaluates the retrievers ability to select the correct source document in multi-document setup. We report the rate of instances where the ground truth document is the source of the majority of the retrieved context."
        },
        {
            "title": "6.2 End-to-End QA",
            "content": "We use the best text and visual retrieval models from the retrieval experiments for End-to-End QA evaluation. Baselines: We benchmark our method using LLMs capable of handling multi-image inputs and long context. To this extent, we include two off-the-shelf models Gemini-1.5-Flash (Reid et al., 2024), and ChatGPT-4o (OpenAI, 2024), as well as Qwen2VL-7B-Instruct (Yang et al., 2024), an open-source LLM with visual and long context capabilities. We evaluate these LLMs in four approaches: 1. Long Context: where text content of all documents queries for sample is passed as context, and 2. TextualRAG, 3. VisualRAG, and, 4.VisDoMRAG as described in Section 5. Evaluation: For PaperTab, we borrow the modified implementation of Word Overlap F1 from (Hui et al., 2024), which takes into account different answer types (binary, short text). For all other datasets, we report the Word Overlap F1, which serves as flexible metric to evaluate different answer types. (a) PaperTab (b) FetaTab (c) SciGraphQA (d) SPIQA Figure 3: Comparison of retrieval performance across datasets, for benchmarked retrievers (BM25, MiniLM, MPNet, BGE1.5, ColPali, ColQwen), at different context window lengths, varying [1, 5, 10, 20]. Baseline Long Context Text RAG Visual RAG VisDoMRAG LLM Qwen2-VL Gemini GPT4o Qwen2-VL Gemini ChatGPT4o Qwen2-VL Gemini ChatGPT4o Qwen2-VL Gemini ChatGPT4o PaperTab FetaTab SciGraphQA SPIQA SlideVQA Average 8.23 27.62 28.37 25.33 33.6 37.34 27.37 29.23 42.01 29.89 39.66 44.11 23.1 62.02 60.03 57.56 63.86 60.82 58.57 52.82 61.89 59.24 60.89 63.28 16.74 22.1 24.12 26.75 26.48 29.74 28.13 23.56 31.12 27.98 25.82 31.36 9.93 38.82 36.3 39.77 42.33 42.8 42.81 41.43 43.28 42.8 41.03 44.09 2.46 13.47 15.06 8.82 10.3 15.97 38.42 51.96 66.82 39.77 52.74 67. 12.09 32.81 32.78 31.65 35.31 37.33 39.06 39.80 49.02 39.94 44.03 50.01 Table 3: Performance of our approach, VisDoMRAG, compared to baseline approaches on VisDoMBench. VisDoMRAG outperforms long-context LLM, visual and text-only RAG baselines. Retriever BM25 MiniLM MPNet BGE1.5 ColPali ColQwen2 PaperTab FetaTab SciGraphQA SPIQA SlideVQA Average 65.51 65.51 90.18 96.81 9.81 97.61 84.00 88.85 89.71 94.00 97.71 96. 72.73 91.65 91.40 90.91 95.28 95.58 88.23 61.06 95.84 98.43 11.43 96.85 98.55 0.73 0.73 81.85 97.64 97.82 81.80 61.56 73.57 92.40 54.15 96.94 Table 4: Comparison of performance in source document identification, at = 5."
        },
        {
            "title": "7.1 Retrieval Evaluation on VisDoMBench",
            "content": "Fig. 3 presents the performance of various retrieval models in extracting evidence from documents, evaluated using the Averaged Normalized Longest Common Subsequence (ANLCS) between retrieved evidence and ground truth evidence, for different context window lengths (k = [1, 5, 10, 20]). Based on threshold of ANLCS = 0.7, we use context window of = 5, = 7 for Visual RAG and Textual RAG, respectively, with ColQwen2 and BGE-1.5 as the visual and textual retrievers. ColQwen2 outperforms other retrieval baselines across different datasets due to the presence of strong LLM backbone (Qwen2). Table 4 evaluates the retriever performance in identifying the correct source document, presenting the proportion of queries with accurate document retrieval for = 5. document is considered correctly retrieved if at least k/2 of the retrieved documents correspond to the ground truth source documents. We observe that ColQwen2 is better than the next closest BGE1.5 model by 4.5%. Notably, we observe substantial performance gap in this metric for SlideVQA, with visual models significantly outperforming text-only models. BM25 exhibits better performance than text-only models in this case, as slides typically contain sparse text, often comprising keywords that directly match between the query and context. Conversely, neural models struggle to capture semantic information effectively, as the textual content lacks complete sentences, limiting their ability to exploit contextual meaning."
        },
        {
            "title": "7.2 End-to-End Evaluation",
            "content": "Table 3 presents the comparative performance of VisDoMRAG against Visual RAG, Textual RAG, and Long Context methods across multiple LLMs, including Qwen2VL (7B), Gemini Flash, and GPT4. The results indicate that VisDoMRAG consistently achieves superior performance over the baseline methods across datasets, with performance gains ranging from 2.1-21.6% (PaperTab), 0.6736.14% (FetaTab), 0.24-11.24% (SciGraphQA), 0.81-32.87% (SPIQA), 0.40-52.16% (SlideVQA). Additionally, within each baseline method for most datasets, we observe positive correlation between model size and performance, which aligns with established expectations in LLM scaling behavior (Hestness et al., 2017). (a) Long Context (b) VisDoMRAG Figure 4: Comparative performance between Long Context and VisDoMRAG (averaged across LLMs) evaluated on different ranges of number of pages = (cid:80) dD d, with Low (p 100), Medium (100 < 150), and High (150 p) volumes. Textual vs Visual RAG: In comparing the performance of textual RAG vis-à-vis visual RAG, we observe that visual RAG consistently outperforms textual RAG. This behaviour can be explained on the basis of our dataset composition which predominantly consists of visually-rich content, and visual RAG is able to leverage visual information directly. However, the performance difference is less pronounced in scientific figure datasets such as SciGraphQA and SPIQA due to the text-rich nature of scientific papers, where figures are often accompanied by detailed descriptions within the text and captions, particularly emphasizing key results and structural details. In contrast, we see substantial performance gap between textual and visual RAG for SlideVQA, as slides typically lack extensive textual descriptions of visualizations, forcing the visual modality to be the primary source for answering questions. Additionally, we find that Gemini often performs better in the textual modality compared to the visual modality across most datasets. This disparity could be attributed to factors such as linguistic bias (Niu et al., 2021; Wang et al., 2024a) or visual hallucination (Ghosh et al., 2024), where the models visual perception may be less reliable than its linguistic capabilities. Effect of Long-Context LLMs: We observe that VisDoMRAG has the ability to significantly enhance the performance of smaller models, as seen from Qwen2VL. This improvement can be attributed to its ability to integrate visual and textual reasoning, compensating for the weaker long-context understanding and visual perception. The longcontext LLM baselines prove to be less effective in our setup due to the high token count and the nature of the task, which requires retrieval of specific, localized evidenceessentially needle-inthe-haystack problem. The combination of modalities in VisDoMRAG mitigates these challenges, resulting in more robust answer generation, as reflected in the results. Effect of Increasing Page Count: Figure 4 evaluate the performance of different approaches averaged across LLMs, segmented by the volume of pages associated with each query. As anticipated, long-context models exhibit significant performance drop with increasing number of pages in the collection. Contrastively, our multimodal RAG approach shows consistent QA performance even at high page counts as it is able to constrain the amount of context the LLM needs to process to answer the question effectively. Qualitative Examples: Fig 5 represents qualitative example from the PaperTab dataset, where VisDoMRAG effectively uses reasoning chains and answers from unimodal RAG outputs to synthesize the correct answer. More qualitative results are presented in the Appendix. Figure 5: Qualitative example from the PaperTab dataset, comparing VisDoMRAG with Unimodal RAG strategies."
        },
        {
            "title": "7.3 Ablations",
            "content": "We conducted ablation studies with ChatGPT4o to evaluate the effectiveness of various components in our proposed VisDoMRAG framework, as well as to compare early fusion and late fusion strategies for modality integration. The results are summarized in Table 5. Early Fusion vs. Late Fusion: In our experiments, early fusion, where text extracted from document pages retrieved by the visual retriever is directly appended to the visual RAG context and used as input to the LLM, demonstrated suboptimal performance compared to the late fusion strategy emBaseline Text Vision VisDoMRAG Experiment Ours Prompt Ablation Ours Prompt Ablation Ours Prompt Ablation Early Fusion PaperTab FetaTab SciGraphQA SPIQA SlideVQA Average 37.34 33.29 42.01 34.52 44.11 38.34 37.37 60.82 58.81 61.89 59.85 63.28 62.65 61.29 29.74 30.16 31.12 31.31 31.36 27.85 27.94 42.80 37.81 43.28 32.55 44.09 36.75 33.45 15.97 13.32 66.82 61.44 67.22 64.33 58. 37.33 34.68 49.02 43.93 50.01 45.98 43.63 Table 5: Performance comparison of baseline approaches with ablations on VisDoMBench. ployed in VisDoMRAG. Specifically, early fusion struggled to integrate visual and textual evidence effectively, particularly in cross-modal reasoning, resulting in an average score of 43.63 across datasets. This limitation is likely due to the lack of independent processing for each modality, which led to weaker contextual understanding and reasoning. In contrast, late fusionwhere each modality is processed independently before aggregatingproved more effective. This performance gap highlights the importance of preserving modality-specific representations before combining them, particularly when reasoning requires nuanced cross-modal evidence integration. Prompt Ablation: The ablation of our proposed prompting strategies also revealed the significance of Evidence Curation, Chain-of-Thought (CoT) prompting, and Reasoning Consistency. By replacing these components with simplified prompts that employ basic structure where the model directly generates an answer based on the question and retrieved context, without leveraging evidence curation, chain-of-thought (CoT) prompting, or reasoning consistency mechanisms. For instance, removing these prompting strategies led to an average score drop from 37.33 to 34.68 in the text-only setting and from 49.02 to 43.93 in the vision-only setting, highlighting the importance of structured prompts. For the VisDoMRAG setting, prompt ablation led to an average performance reduction from 50.01 to 45.98, with the most notable declines observed in datasets requiring complex reasoning, such as SPIQA and SlideVQA. The simplified prompts appeared insufficient for handling the intricacies of cross-modal evidence alignment and aggregation, leading to degraded performance in these scenarios."
        },
        {
            "title": "8 Conclusion and Future Work",
            "content": "In this work, we introduced VisDoMBench, the first QA dataset designed to evaluate multi-document systems incorporating visually rich elements such as tables, charts, and slides. By targeting documents that require both textual and visual comprehension, VisDoMBench offers novel benchmark to assess the capability of multimodal retrieval systems. We also presented VisDoMRAG, multimodal Retrieval-Augmented Generation approach that fuses visual and textual pipelines using consistency-constrained modality fusion. This method demonstrated significant improvement over traditional long context, textual, and visual RAG by 12-20%. While the current work focuses on RAG in multimodal multi-doc settings, future work will extend this approach to include reasoning through end-to-end trained models, especially in low-resource settings."
        },
        {
            "title": "9 Ethics Statement",
            "content": "We use publicly available datasets in this research. The identities of human evaluators remain confidential, and no personally identifiable information (PII) is used at any stage of our experiments. Our work is solely intended for document QA applications. For deeper understanding of potential risks and mitigation strategies in LLM safety, we direct users to relevant works by (Kumar et al., 2024; Cui et al., 2024; Luu et al., 2024)."
        },
        {
            "title": "10 Limitations",
            "content": "Despite the advancements presented in this study, several limitations warrant consideration: (1) Text Extraction and Document Parsing: key argument for the efficacy of visual retrieval methods is the elimination of text extraction and document parsing pipelines (Faysse et al., 2024). However, our approach retains this overhead, which may introduce additional complexity and processing time. (2) Multiple LLM calls: Our methodology necessitates multiple LLM calls; specifically, we make three LLM calls per query. While this approach may not be optimal, it is still more cost-effective than utilizing long-context models. (3) Hallucinations: As with all works involving large language models (LLMs), our approach is subject to inherent limitations related to AI safety and the risk of hallucination. These issues can affect the reliability and accuracy of the generated outputs and underscore safety risks, highlighting the need for ongoing research and refinement in the field of AI to mitigate these challenges. Additionally, unlike previous visual QA research, which typically required models to answer questions based solely on visual data, our framework incorporates document context. This inclusion allows for relevant textual information from other sections of the paper to contribute to the query response. However, this reliance on document context represents limitation common to all visually rich document QA datasets, as it challenges the isolation of visual performance testing. Nonetheless, this characteristic may not be entirely detrimental; in fact, it more accurately reflects the complexity of real-world systems where multimodal information is often interdependent."
        },
        {
            "title": "References",
            "content": "Chenxin An, Shansan Gong, Ming Zhong, Xingjian Zhao, Mukai Li, Jun Zhang, Lingpeng Kong, and Xipeng Qiu. 2023. L-eval: Instituting standardized evaluation for long context language models. arXiv preprint arXiv:2307.11088. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. 2023. Longbench: bilingual, multitask benchmark for long context understanding. arXiv preprint arXiv:2308.14508. Lucas Beyer, Andreas Steiner, André Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, Thomas Unterthiner, Daniel Keysers, Skanda Koppula, Fangyu Liu, Adam Grycner, Alexey Gritsenko, Neil Houlsby, Manoj Kumar, Keran Rong, Julian Eisenschlos, Rishabh Kabra, Matthias Bauer, Matko Bošnjak, Xi Chen, Matthias Minderer, Paul Voigtlaender, Ioana Bica, Ivana Balazevic, Joan Puigcerver, Pinelopi Papalampidi, Olivier Henaff, Xi Xiong, Radu Soricut, Jeremiah Harmsen, and Xiaohua Zhai. 2024. Paligemma: versatile 3b vlm for transfer. Preprint, arXiv:2407.07726. Deng Cai, Yan Wang, Victoria Bi, Zhaopeng Tu, Xiaojiang Liu, Wai Lam, and Shuming Shi. 2018. Skeleton-to-response: Dialogue generation guided by retrieval memory. arXiv preprint arXiv:1809.05296. Wenhu Chen, Hexiang Hu, Xi Chen, Pat Verga, and William Cohen. 2022. Murag: Multimodal retrieval-augmented generator for open question answering over images and text. arXiv preprint arXiv:2210.02928. Tianyu Cui, Yanling Wang, Chuanpu Fu, Yong Xiao, Sijia Li, Xinhao Deng, Yunpeng Liu, Qinglin Zhang, Ziyi Qiu, Peiyang Li, Zhixing Tan, Junwu Xiong, Xinyu Kong, Zujie Wen, Ke Xu, and Qi Li. 2024. Risk taxonomy, mitigation, and assessment benchmarks of large language model systems. Preprint, arXiv:2401.05778. Pradeep Dasigi, Kyle Lo, Iz Beltagy, Arman Cohan, Noah Smith, and Matt Gardner. 2021. dataset of information-seeking questions and answers anchored in research papers. arXiv preprint arXiv:2105.03011. Naihao Deng, Zhenjie Sun, Ruiqi He, Aman Sikka, Yulong Chen, Lin Ma, Yue Zhang, and Rada Mihalcea. 2024. Tables as images? exploring the strengths and limitations of llms on multimodal representations of tabular data. arXiv preprint arXiv:2402.12424. Yihao Ding, Zhe Huang, Runlin Wang, YanHang Zhang, Xianru Chen, Yuzhong Ma, Hyunsuk Chung, and Soyeon Caren Han. 2022. V-doc: Visual questions In Proceedings of the answers with documents. IEEE/CVF conference on computer vision and pattern recognition, pages 2149221498. Manuel Faysse, Hugues Sibille, Tony Wu, Gautier Viaud, Céline Hudelot, and Pierre Colombo. 2024. Colpali: Efficient document retrieval with vision language models. arXiv preprint arXiv:2407.01449. Sreyan Ghosh, Chandra Evuru, Sonal Kumar, Utkarsh Tyagi, Oriol Nieto, Zeyu Jin, and Dinesh Manocha. 2024. Vdgd: Mitigating lvlm hallucinations in cognitive prompts by bridging the visual perception gap. Jiatao Gu, Yong Wang, Kyunghyun Cho, and Victor OK Li. 2018. Search engine guided neural machine translation. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32. Qiuxiang He, Guoping Huang, Qu Cui, Li Li, and Lemao Liu. 2021. Fast and accurate neural machine translation with translation memory. In Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), pages 31703180. Joel Hestness, Sharan Narang, Newsha Ardalani, Gregory Frederick Diamos, Heewoo Jun, Hassan Kianinejad, Md. Mostofa Ali Patwary, Yang Yang, and Yanqi Zhou. 2017. Deep learning scaling is predictable, empirically. ArXiv, abs/1712.00409. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. 2024. Ruler: Whats the real context size of your long-context language models? arXiv preprint arXiv:2404.06654. Yulong Hui, Yao Lu, and Huanchen Zhang. 2024. Uda: benchmark suite for retrieval augmented generation in real-world document analysis. arXiv preprint arXiv:2406.15187. Gautier Izacard and Edouard Grave. 2020. Leveraging passage retrieval with generative models for open domain question answering. arXiv preprint arXiv:2007.01282. Tomáš Koˇcisk`y, Jonathan Schwarz, Phil Blunsom, Chris Dyer, Karl Moritz Hermann, Gábor Melis, and Edward Grefenstette. 2018. The narrativeqa reading comprehension challenge. Transactions of the Association for Computational Linguistics, 6:317328. Ashutosh Kumar, Sagarika Singh, Shiv Vignesh Murty, and Swathy Ragupathy. 2024. The ethics of interaction: Mitigating security threats in llms. Preprint, arXiv:2401.12273. Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich Küttler, Mike Lewis, Wen-tau Yih, Tim Rocktäschel, et al. 2020. Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33:94599474. Jiaqi Li, Mengmeng Wang, Zilong Zheng, and Muhan Zhang. 2023. Loogle: Can long-context language models understand long contexts? arXiv preprint arXiv:2311.04939. Shengzhi Li and Nima Tajbakhsh. 2023. Scigraphqa: large-scale synthetic multi-turn question-answering arXiv preprint dataset arXiv:2308.03349. for scientific graphs. Weizhe Lin and Bill Byrne. 2022. Retrieval augmented visual question answering with outside knowledge. arXiv preprint arXiv:2210.03809. Quan Khanh Luu, Xiyu Deng, Anh Van Ho, and Yorie Nakahira. 2024. Context-aware llm-based safe control against latent risks. Preprint, arXiv:2403.11863. Yubo Ma, Yuhang Zang, Liangyu Chen, Meiqi Chen, Yizhu Jiao, Xinze Li, Xinyuan Lu, Ziyu Liu, Yan Ma, Xiaoyi Dong, et al. 2024. Mmlongbench-doc: Benchmarking long-context document understanding with visualizations. arXiv preprint arXiv:2407.01523. Association for Computational Linguistics (Volume 1: Long Papers), pages 63226334, Toronto, Canada. Association for Computational Linguistics. Bo Pang, Erik Nijkamp, Wojciech Kryscinski, Silvio Savarese, Yingbo Zhou, and Caiming Xiong. 2023. Long document summarization with top-down and In Findings of the Associbottom-up inference. ation for Computational Linguistics: EACL 2023, pages 12671284, Dubrovnik, Croatia. Association for Computational Linguistics. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. 2023. Yarn: Efficient context window extension of large language models. Preprint, arXiv:2309.00071. Hao Peng, Ankur Parikh, Manaal Faruqui, Bhuwan Dhingra, and Dipanjan Das. 2019. Text generation with exemplar-based adaptive decoding. arXiv preprint arXiv:1904.04428. Shraman Pramanick, Rama Chellappa, and Subhashini Venugopalan. 2024. Spiqa: dataset for multimodal question answering on scientific papers. arXiv preprint arXiv:2407.09413. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jean-baptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. 2024. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Stephen Robertson, Steve Walker, Susan Jones, Micheline Hancock-Beaulieu, Mike Gatford, et al. 1995. Okapi at trec-3. Nist Special Publication Sp, 109:109. Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. 2021. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209. Bhaskarjit Sarmah, Tianjie Zhu, Dhagash Mehta, and Stefano Pasquali. 2023. Towards reducing hallucination in extracting information from financial reports using large language models. Preprint, arXiv:2310.10760. Linyong Nan, Chiachun Hsieh, Ziming Mao, Xi Victoria Lin, Neha Verma, Rui Zhang, Wojciech Kryscinski, Hailey Schoelkopf, Riley Kong, Xiangru Tang, et al. 2022. Fetaqa: Free-form table question answering. Transactions of the Association for Computational Linguistics, 10:3549. Yulei Niu, Kaihua Tang, Hanwang Zhang, Zhiwu Lu, Xian-Sheng Hua, and Ji-Rong Wen. 2021. Counterfactual vqa: cause-effect look at language bias. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 12700 12710. OpenAI. 2024. Hello, gpt-4o! https://openai.com/ index/hello-gpt-4o/. Vaishali Pal, Andrew Yates, Evangelos Kanoulas, and Maarten de Rijke. 2023. MultiTabQA: Generating tabular answers for multi-table question answering. In Proceedings of the 61st Annual Meeting of the Kaitao Song, Xu Tan, Tao Qin, Jianfeng Lu, and TieYan Liu. 2020. Mpnet: Masked and permuted pre-training for language understanding. CoRR, abs/2004.09297. Ryota Tanaka, Kyosuke Nishida, Kosuke Nishida, Taku Hasegawa, Itsumi Saito, and Kuniko Saito. 2023. Slidevqa: dataset for document visual question answering on multiple images. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 1363613645. Rubèn Tito, Dimosthenis Karatzas, and Ernest Valveny. 2023. Hierarchical multimodal transformers for multipage docvqa. Pattern Recognition, 144:109834. Fei Wang, Wenxuan Zhou, James Huang, Nan Xu, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2024a. mdpo: Conditional preference optimization for multimodal large language models. arXiv preprint arXiv:2406.11839. Hengyi Wang, Haizhou Shi, Shiwei Tan, Weiyi Qin, Wenyuan Wang, Tunyu Zhang, Akshay Nambi, Tanuja Ganu, and Hao Wang. 2024b. Multimodal needle in haystack: Benchmarking long-context capability of multimodal large language models. arXiv preprint arXiv:2406.11230. Xinrong Zhang, Yingfa Chen, Shengding Hu, Zihang Xu, Junhao Chen, Moo Khai Hao, Xu Han, Zhen Leng Thai, Shuo Wang, Zhiyuan Liu, et al. 2024. infty bench: Extending long context evaluation beyond 100k tokens. arXiv preprint arXiv:2402.13718. Minzheng Wang, Longze Chen, Cheng Fu, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, and Yongbin Li. 2024c. Leave no document behind: Benchmarking long-context llms with extended multi-doc qa. Preprint, arXiv:2406.17419. Wenhui Wang, Furu Wei, Li Dong, Hangbo Bao, Nan Yang, and Ming Zhou. 2020. Minilm: Deep self-attention distillation for task-agnostic comPreprint, pression of pre-trained transformers. arXiv:2002.10957. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. 2023. Self-consistency improves chain of thought reasoning in language models. Preprint, arXiv:2203.11171. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. 2022. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837. Shitao Xiao, Zheng Liu, Peitian Zhang, and Niklas Muennighoff. 2023. C-pack: Packaged resources to advance general chinese embedding. Preprint, arXiv:2309.07597. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. 2024. Qwen2 technical report. Preprint, arXiv:2407.10671. Zheng Yuan, Qiao Jin, Chuanqi Tan, Zhengyun Zhao, Hongyi Yuan, Fei Huang, and Songfang Huang. 2023. Ramm: Retrieval-augmented biomedical visual question answering with multi-modal pre-training. In Proceedings of the 31st ACM International Conference on Multimedia, pages 547556. Lei Zhang, Yunshui Li, Ziqiang Liu, Junhao Liu, Min Yang, et al. 2023. Marathon: race through the realm of long context with large language models. arXiv preprint arXiv:2312.09542."
        },
        {
            "title": "A Appendix",
            "content": "A.1 Baselines A.1.1 Retrieval Models BM25 BM25 (Robertson et al., 1995) is widely adopted term-based ranking function based on the probabilistic information retrieval model. It calculates the relevance of document to given query by considering term frequency, inverse document frequency, and document length normalization. BM25 is effective for sparse text retrieval tasks, making it standard baseline in information retrieval evaluations. We use the Python rank_bm25 implementation for our experiments. MiniLM MiniLM (Wang et al., 2020) is lightweight, transformer-based model designed for efficient knowledge distillation. It compresses the knowledge of larger pre-trained models into smaller architecture while maintaining language competitive performance in natural understanding tasks. MiniLM is used in retrieval tasks due to its ability to balance computational efficiency and accuracy. We use the sentence-transformers/all-MiniLM-L6-v2 implementation in our experiments. (Song et al., 2020) MPNet MPNet is transformer-based model that leverages permuted language modeling for pre-training, which helps it capture contextual information more effectively than traditional masked language It excels in variety of natural lanmodels. guage processing tasks, including text retrieval, due to its robust contextual embeddings and representation learning capabilities. We use the sentence-transformers/all-mpnet-base-v2 implementation in our experiments. BGE-1.5 The BGE model family is based on BERT-like architecture and three-stage training process, which collectively enhance its adaptability and generalization capabilities. Pre-training is performed on large-scale plain text corpora using tailored MAE-style approach, effectively encoding polluted text and reconstructing the clean version. The model then undergoes contrastive learning with in-batch negative sampling, leveraging large batch sizes to improve embedding discriminativeness. Finally, task-specific fine-tuning is employed using labeled datasets, applying instruction-based prompts and advanced negative sampling techniques to better accommodate diverse task types. We use the BAAI/bge-base-en-v1.5 model in our experiments, which is their large english model, version 1.5. ColPali, ColQwen2 ColPali (Faysse et al., 2024) performs late interaction retrieval on document embeddings generated directly from document page images using Vision-Language Models (VLMs). By passing the document images through PaliGemma (Beyer et al., 2024), ColPali uses the projected token embeddings to index the document pages, eliminating the need for OCR or document parsing. The multimodal alignment learned by VLMs allows both text queries and document image embeddings to exist in shared semantic vector space, enabling more precise and efficient retrieval. ColQwen2 is similar model with Qwen2 (Yang et al., 2024) as the base VLM. We used the vidore/colpali-v1.2, vidore/colqwen2-v0.1 implementations for our experiments. A.1.2 LLMs used Qwen/Qwen2-VL-7B-Instruct, We chatgpt-4o-latest and gemini-1.5-flash in our experiments. For ChatGPT4o and Gemini, we set the temperature as 0.5, and use the default hyperparameters. For Qwen2-VL, the pixel range is set to [2562828, 6402828]. For Long Context evaluation, we use Qwen/Qwen2-7B-Instruct because of the implementation availability of long context inference using YaRN (Peng et al., 2023). We report results on single run of experiments. A.2 Datasets The datasets use in our benchmark are described below. Fig 6-10 represent the distribution of pages per query in all the data splits. FetaTab FetaTab is derived from UDA (Hui et al., 2024), which sources its data from FetaQA (Nan et al., 2022). Many source datasets provide only segmented and partial content, lacking complete documents. To resolve this, UDA conducted thorough source-document identification process, verifying and collecting the complete original document files based on metadata or content fragments. This was followed by rigorous matching and reorganization to form complete triplet data pairs consisting of document-question-answer. Additionally, UDA categorizes queries based on the source of factual evidence, filters out Q&As without available answers, converts token-based data patterns to natural language, unifies data formats and structures across datasets, and designs specific LLM prompts tailored for each dataset after experimental trials. FetaTab is licensed under the CC-BY-SA-4.0 license. Figure 6: Distribution of pages per query for FetaTab. PaperTab PaperTab is also sourced from UDA (Hui et al., 2024), which obtains its data from the QASPER (Dasigi et al., 2021) dataset. Similar to the process described for FetaTab, UDA emphasizes the necessity of ensuring the integrity of original documents for effective document analysis. This involves comprehensive process of identifying, verifying, and collecting complete original document files, followed by matching and reorganization to create document-question-answer triplets. UDA also categorizes queries, filters out unanswered Q&As, converts data patterns to natural language, unifies data formats, and designs specific LLM prompts for each dataset based on experimental evaluations. PaperTab is released under the CC-BY-SA-4.0 license. SPIQA SPIQA (Pramanick et al., 2024) is large-scale and challenging question-answering dataset that focuses on figures, tables, and text paragraphs extracted from scientific research papers across various computer science domains. The dataset encompasses diverse array of visual elements, including plots, charts, schematic diagrams, and result visualizations. SPIQA consists of 270K questions divided between training, validation, and three different evaluation splits. To ensure the highest quality and reliability, SPIQA employs both auFigure 7: Distribution of pages per query for PaperTab. Figure 9: Distribution of pages per query for SciGraphQA. tomatic and manual curation methods. The dataset is released under the CC-BY-SA-4.0 license, allowing for broad use while ensuring proper attribution. ing the slide content. This dataset requires complex reasoning skills, including single-hop, multi-hop, and numerical reasoning. It also provides annotated arithmetic expressions for numerical answers, enhancing numerical reasoning capabilities. More details about the dataset can be found under the license at this link. Figure 8: Distribution of pages per query for SPIQA. SciGraphQA SciGraphQA (Li and Tajbakhsh, 2023) is synthetic multi-turn question-answer dataset centered on academic graphs, representing significant advancement in the field of visual question answering. At 13 times larger than the previous largest dataset, ChartVQA, it stands as the largest open-sourced chart VQA dataset with nonsynthetic charts. The dataset was constructed from 290,000 Computer Science and Machine Learning papers published on ArXiv between 2010 and 2020, with the help of Palm-2 generating 295,000 samples of open-vocabulary multi-turn questionanswer dialogues about the graphs. Each dialogue is contextualized with the paper title, abstract, relevant paragraphs, and rich contextual data from the graphs, achieving an average of 2.23 questionanswer turns per graph. SciGraphQA is released under the MIT license. SlideVQA SlideVQA (Tanaka et al., 2023) is multi-image document VQA dataset that contains over 2,600 slide decks, comprising more than 52,000 slide images and 14,500 questions regardFigure 10: Distribution of pages per query for SlideVQA. A.2.1 Distracting Documents Distracting documents are introduced as additional, irrelevant documents within the retrieval set to simulate real-world scenarios where the task is to find the most relevant context among multiple documents. These distracting documents are selected randomly from the in-domain documents of given dataset, ensuring that they are contextually similar but not directly relevant to the query. To validate the effectiveness of the one-to-one mapping and evaluate the robustness of the retrieval system in the presence of distracting documents, we conducted an experiment where we removed the oracle document (i.e., the ground truth document) from the retrieval set. In this setup, we provided GPT-4 with the option to refuse to answer the query if it deemed the provided context insufficient for answering the query. The refusal rate was then measured in both the default setting (with the oracle document included) and without the oracle document. The results, shown in Table 6, reveal significant increase in refusal rates when the oracle document is removed. In the default setting, the refusal rate is relatively low across the datasets, with PaperTab and FetaTab having 26% and 4% refusal rates, respectively, indicating that GPT-4 was able to find sufficient context for answering the queries. However, when the oracle document is excluded, the refusal rate jumps dramatically, with all datasets showing refusal rates between 94% and 98%. This increase highlights the importance of having the correct document in the retrieval set, as the model struggles to generate answers without access to the relevant context. This experiment underscores the critical role of the oracle document in ensuring that the retrieval system can effectively answer queries and demonstrates how distracting documents can hinder retrieval performance when they introduce irrelevant or insufficient context. The results validate our approach in testing the one-to-one mapping of queries to documents and emphasize the importance of ensuring that the retrieval system can maintain performance in the presence of distracting documents. Method Default Without Oracle PaperTab FetaTab SciGraphQA SPIQA SlideVQA 18% 94% 4% 98% 26% 97% 15% 97% 40% 98% Table 6: Refusal rate of GPT4o in the default setting and without the oracle document. A.3 Examples A.3.1 Query Augmentation Tables 7 and 8 represent examples of query augmentation during dataset construction for PaperTab and SciGraphQA. A.3.2 End-to-End QA Examples Figures 11-15 illustrate End-to-End QA examples across the five datasets, demonstrating the performance of different LLMs. In Figure 11, we analyze an example from the PaperTab dataset using Qwen2VL. VisualRAG fails in this instance by selecting the incorrect column for computation during reasoning. Conversely, TextualRAG identifies the correct column but overlooks samples from the test and validation sets. VisDoMRAG evaluates both outputs and produces the correct answer, demonstrating its ability to refine responses across modalities. Figure 12 presents an example from the FetaTab dataset, where Gemini is employed as the base LLM. Here, TextualRAG successfully generates the correct answer by accurately verbalizing the OCR-processed table during evidence retrieval. Although VisualRAG underperforms in this case, VisDoMRAG integrates the evidence effectively, providing the overall correct answer. In Figure 13, an example from SciGraphQA shows both Visual and Textual RAG producing correct responses. Consequently, VisDoMRAG corroborates the correct answers, confirming the alignment between both modalities. Figure 14 depicts scenario from the SPIQA dataset where VisDoMRAG fails to provide the correct answer. This error arises from its bias towards the longer response generated by VisualRAG, which itself is incorrect. Lastly, Figure 15 showcases an example from the SlideVQA dataset. In this case, TextualRAG fails to capture the necessary evidence, whereas VisualRAG successfully employs multi-hop reasoning across two slides to derive the correct answer. VisDoMRAG recognizes the precision in VisualRAGs response, favoring its consistency with the questions context. A.4 LLM Prompts Fig. 16 - 18 represent prompt templates used in our experiments for query augmentation, baselines and VisDoMRAG. A.5 Human Review Process We addressed the challenge of trivial or underspecified queries in some datasets by augmenting the queries using ChatGPT4o and relevant context, including the title and abstract of the research paper, the relevant figures caption, and other available metadata. We employ human reviewer to assess the quality of the generated queries and select one of the queries or reject all queries. The reviewer is graduate student who is paid at the hourly rate for Graduate Assistants at the university where they are student. Fig 19 gives brief of the instructions as well as the evaluation rubric given to the reviewer. Figure 11: Qualitative example from the PaperTab dataset, comparing VisDoMRAG with unimodal RAG strategies, with Qwen2VL as the base LLM. Original Query What baselines did they consider? What is the average length of the claims? What was the performance on the self-collected corpus? Do they test their framework performance on commonly used language pairs, such as English-toGerman? Augmented Query What baseline approaches using state-of-the-art PDTB taggers were employed for the evaluation of causality prediction in the automatic causal explanation analysis pipeline? What is the average token count of claims as reported in Table 2 of the PERSPECTRUM dataset? What F1 scores did the CM-Net achieve for intent detection and slot filling on the CAIS dataset as detailed in Table 6? Does the paper report results for English-to-German translation in simulated under-resourced scenario using their proposed multilingual NMT framework? Table 7: Example of query augmentation from PaperTab dataset. Original Query What is the main difference between the two scheduling algorithms compared in this graph? What does the phase diagram indicate about the stability of the different phases? What does the graph show about the impact of the load-changing attack on the frequency of the system? What are some of the implications of the graph for the design of fuzzing tools? Augmented Query In this paper, what scheduling algorithms are analyzed in Figure 8 for WCRT comparison? What does Figure 4.18 reveal about the phase boundaries for different choices of Jt and k? What does the figure show about frequency limits during the 2019 and 2020 load-changing attacks? What relationship does Fig. 3 suggest between performance and resources in fuzzing tools? Table 8: Example of query augmentation from SciGraphQA dataset. A.6 Computational Resources Table 9 describes the Computational Resources used for running this papers experiments. Figure 12: Qualitative example from the FetaTab dataset, comparing VisDoMRAG with unimodal RAG strategies, with Gemini as the base LLM. Metric GPU Hours GPU Specification Number of GPU(S) Max Model Parameters Details 100 RTXA600 1 7B Table 9: Computationa Resources for VisDoM RAG experiments. Figure 13: Qualitative example from the ScigraphQA dataset, comparing VisDoMRAG with unimodal RAG strategies, with Qwen2VL as the base LLM. Figure 14: Qualitative example from the SPIQA dataset, comparing VisDoMRAG with unimodal RAG strategies, with ChatGPT4o as the base LLM. Figure 15: Qualitative example from the SlideVQA dataset, comparing VisDoMRAG with unimodal RAG strategies, with ChatGPT4o as the base LLM. Figure 16: Prompt Template used for Query Augmentation. Figure 17: Prompt Template used for Unimodal RAG and Long Context experiments. Figure 18: Prompt Template used for VisDoMRAG. Figure 19: Brief of Reviewer Instructions, including the Evaluation Rubric."
        }
    ],
    "affiliations": [
        "University of Maryland, College Park"
    ]
}