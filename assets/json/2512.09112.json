{
    "paper_title": "GimbalDiffusion: Gravity-Aware Camera Control for Video Generation",
    "authors": [
        "Frédéric Fortier-Chouinard",
        "Yannick Hold-Geoffroy",
        "Valentin Deschaintre",
        "Matheus Gadelha",
        "Jean-François Lalonde"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GimbalDiffusion, a framework that enables camera control grounded in physical-world coordinates, using gravity as a global reference. Instead of describing motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360-degree videos to construct a wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the model's reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish a benchmark for camera-aware video generation by rebalancing SpatialVID-HQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks."
        },
        {
            "title": "Start",
            "content": "GimbalDiffusion: Gravity-Aware Camera Control for Video Generation Frederic Fortier-Chouinard1,2, Yannick Hold-Geoffroy2, Valentin Deschaintre2, Matheus Gadelha2, Jean-Francois Lalonde1 1Universite Laval, 2Adobe https://lvsn.github.io/GimbalDiffusion 5 2 0 2 9 ] . [ 1 2 1 1 9 0 . 2 1 5 2 : r Figure 1. We propose GIMBALDIFFUSION, framework for absolute camera control in text-to-video generation. Our approach adapts foundational video generation models to accept absolute camera controls, conditioning the entire video on camera parameters expressed in gravity-aligned global coordinate system. This enables the generation of videos with challenging viewpoints, such as low pitch (top) or high roll (bottom), directly from text, something existing methods struggle to achieve. Refer to the supplementary material for the full videos."
        },
        {
            "title": "Abstract",
            "content": "Recent progress in text-to-video generation has achieved remarkable realism, yet fine-grained control over camera motion and orientation remains elusive. Existing approaches typically encode camera trajectories through relative or ambiguous representations, limiting explicit geometric control. We introduce GIMBALDIFFUSION, framework that enables camera control grounded in physical-world coordinates, usInstead of describing ing gravity as global reference. motion relative to previous frames, our method defines camera trajectories in an absolute coordinate system, allowing precise and interpretable control over camera parameters without requiring an initial reference frame. We leverage panoramic 360 videos to construct wide variety of camera trajectories, well beyond the predominantly straight, forward-facing trajectories seen in conventional video data. To further enhance camera guidance, we introduce null-pitch conditioning, an annotation strategy that reduces the models reliance on text content when conflicting with camera specifications (e.g., generating grass while the camera points towards the sky). Finally, we establish benchmark for camera-aware video generation by rebalancing SpatialVIDHQ for comprehensive evaluation under wide camera pitch variation. Together, these contributions advance the controllability and robustness of text-to-video models, enabling precise, gravity-aligned camera manipulation within generative frameworks. serene aerial view captures boat drifting down dark river, framed by lush greenery and shimmering water under peaceful sky. sleek, modern home with high ceilings, natural light, and contemporary furnishings features man in blue suit in its open-plan living area."
        },
        {
            "title": "Ours",
            "content": "AC3D [2] Figure 2. Reproducing real scenes using our representation. From reference frames from driving video (left), GIMBALDIFFUSION can be controlled to replicate its camera viewpoint with respect to gravity (center). In contrast, existing methods (right) do not accurately match the absolute reference pose. 1. Introduction Text-to-video generation is rapidly advancing, offering astonishing photorealism and new creative workflows. Yet, the ability to control generated video contentespecially camera movement and trajectoryremains limited. Early approaches rely solely on textual input which does not provide fine-grained control, such as specifying geometric parameters for camera paths and angles. Recent methods [24, 11, 12, 31] address this limitation by conditioning video models with explicit camera trajectories, frequently encoded as Plucker rays. However, these representations are inherently ambiguous and lack the precision of real camera systems. Typically, camera extrinsics used for Plucker conditioning are defined relative to the first frame, making it impossible to explicitly control camera orientation with an absolute reference within the environment (e.g., with respect to gravity). In this work, we introduce GIMBALDIFFUSION, novel gravity-centric approach to data annotation that overcomes this challenge and enables precise, absolute user control over generated camera trajectories, as illustrated in fig. 1. The name reflects the role of gimbala device that stabilizes orientation in physical spacemirroring our use of gravity as global reference to stabilize and disambiguate camera rotation. To resolve rotational ambiguity, we leverage geometrically calibrated 360 panoramic videos. By sampling camera crops over the sphere, we can remove the camera bias in human-captured datasets, allowing rare camera shots such as Dutch angles (45 roll) or extreme pitches to be sampled. This alignment enables the generation of cinematic shots, such as lowand high-angle perspectives, which are uncommon in regular video datasets. The effectiveness of this approach is demonstrated in fig. 2, where our method produces frames with camera views that more closely match the intended trajectories than previous methods. To design our dataset from panoramic 360 videos, original translation paths are preserved while camera rotations are randomly sampled. This lets our model learn wide variety of camera trajectories, far beyond the predominantly straight, forward-facing paths seen in conventional video data. We further observe that text and camera angles are inherently entangled, and that care must be taken to ensure that appropriate control is achieved. For example, when asked to generate video of green field under blue sky with camera looking down, model must learn to render grass, even if not specified in the prompt. To this end, we propose null-pitch conditioning approach, where captioning is performed on forward-facing crops rather than the actual camera point of view. Finally, we propose new evaluation benchmark for video generation with extreme camera angles. To do so, we use large-scale dataset of high-quality internet videos, SpatialVID-HQ [33], and rebalance the camera pitch to obtain diverse test samples. We will release this test dataset upon publication. In summary, we make the following contributions: pipeline for generating diverse camera trajectories from panoramic 360 video, providing exhaustive coverage of possible camera orientations; strategy named null-pitch conditioning for disentangling text from absolute camera angle; novel benchmark for video generation with extreme camera angles. 2. Related Work Video generation. Generative video modeling has evolved rapidly, from early GAN-based methods that produced short, low-resolution clips [1, 25, 29] to large-scale diffusion-based architectures capable of generating temporally coherent, semantically consistent videos [36]. Recent open-source models such as CogVideoX [41] and WAN [31] push the frontier of video length, resolution, and controllability while proposing both text-to-video and image-to-video variants. Editing generated videos remains challenging: precise modification to content, viewpoint, or lighting is difficult once frames are synthesized. To address this, several control strategies have emerged, introducing user-guided conditioning based on edge maps, depth, sketches, or explicit 3D geometry for structural control [6, 7, 14, 42], and illuminationaware guidance for lighting control [22, 26]. These mechanisms allow precise control over motion, composition, and scene appearance, substantially improving the usability and compositional fidelity of generative video systems. Figure 3. Training data pipeline. (a) We extract the camera poses from dataset of 360 panoramic videos in equirectangular format (see Sec. 3.2). (b) We randomly sample camera rotation trajectory on the sphere, from which we generate field of view masks in equirectangular format for both the projected image and the null-pitch conditioning (see Sec. 3.3). (c) We integrate the two previous steps by combining the sampled rotation with the estimated camera poses. This process creates dataset of perspective images with their camera poses, along with null-pitch reference images. The main advantage of this approach is the ability to sample camera viewing directions across the entire sphere, in contrast to using natural human-captured videos, which are heavily biased towards null roll and pitch. Additionally, we generate text description for both the perspective video and the null-pitch video using VLM. Camera control methods. Beyond content and appearance, viewpoint control has become key in generative video. common representation for such control is Plucker rays, described in Sec. 3.1. This formulation enables differentiable, geometrically consistent camera manipulation. In static image generation, PreciseCam [5] introduces absolute control over camera pose by conditioning on roll, pitch, field of view, and lens distortion alongside textual prompts. For image-to-video generation, methods such as MotionCanvas [40], Diffusion-as-Shader [10], and TrajectoryCrafter [43] employ point cloud conditioning to maintain spatial coherence while navigating scene from an initial reference image. While effective, these approaches depend on the depth and pose estimation quality of the initial reference frame, which can restrict flexibility and generalization. In text-to-video synthesis, AC3D [2] allows user-defined camera trajectories but conditions motion relative to the first frame only, leaving global rotational and translational ambiguities unresolved. Similarly, Li et al. [24] manipulate positional encodings to control viewpoint changes, yet still lack consistent absolute reference such as gravity. These works underscore the need for representations that enable precise, absolute camera control and scalable trajectory generation across space and time. Our approach addresses this gap by aligning camera orientation with gravity and anchoring motion within consistent world frame, advancing controllable text-to-video synthesis. construct camera intrinsics and extrinsics through feature matching, triangulation, and global bundle adjustment. More recent learning-based approaches jointly infer scene geometry and pose from data: VGGT [32] leverages transformerbased global geometry tokens for pose regression, while DUSt3R [35] and MASt3R [23] predict dense correspondence fields to recover metrically scaled 3D structure. MegaSAM [16] extends these ideas to foundation scale, achieving state-of-the-art alignment accuracy. Despite their accuracy, these systems typically operate in arbitrary coordinate frames and lack physical grounding in gravity or global world reference, limiting their absolute interpretability. To mitigate this, several works estimate the gravity direction from single images [18, 30, 39]. However, acquiring datasets with gravity annotations often requires the use of auxiliary sensors, such as accelerometers, or additional semantic information about the scene. Examples include EDINA [8], Horizon Lines in the Wild [37], and the KITTI Horizon dataset [21]. To circumvent such constraints, we leverage geometrically calibrated 360 panoramic videos from WEB360 [15] as backbone for our data generation. By combining the up vector derived from the panoramic video with SfM-estimated intrinsics and extrinsics, we obtain consistent absolute reference for gravity-aware camera control. 3. Method Camera pose estimation. Estimating accurate camera poses from images is long-standing problem in 3D vision. Classical structure-from-motion pipelines such as VisualSFM [38], OpenMVG [27], and COLMAP [28] reWe aim to train camera-conditioned model on precise, gravity-referenced camera viewpoints. We refer to this global gravity reference as an absolute reference, in contrast to existing relative methods that describe camera motion Figure 4. Training data samples from our data augmentation pipeline, capturing highly diverse set of rotation trajectories from 360 videos. Both the trajectory and the prompt are generated on-the-fly in our dataloader. Each color in the graph corresponds to video above. with respect to previous frame. Because our absolute reference is aligned with gravity, pitch and roll are well defined, whereas yaw remains unconstrained. Throughout the paper, absolute rotation therefore denotes pitch and roll expressed in this shared gravity-aligned reference. However, no large-scale video dataset provides the gravity annotations we need to train such model. This section outlines our pipeline for obtaining such data, summarized in fig. 3. In fig. 3 (a) we extract camera pose annotations from dataset of 360 panoramic videos, as detailed in Sec. 3.2. Next, in (b), we sample camera trajectory as described in Sec. 3.2. Finally, in (c), we combine the two previous steps to produce our dataset. We repeat the same procedure for an additional version of each trajectory with null pitch and roll, described in Sec. 3.3. Before detailing the full pipeline, the following section explains the standard representation to encode camera poses in neural-network-compatible buffers. 3.1. Representing cameras Given camera extrinsic matrix Ef = [Rf tf ] and intrinsic matrix Kf , common strategy for conditioning video generation models on viewpoint is to use Plucker rays [2, 3, 11, 12, 19]. For conditioning frame , we can define per-pixel map of Plucker rays as pf,u,v = f,u,v, (tf f,u,v), where tf denotes the camera origin in world coordinates, and df,u,v is the direction vector from the camera center toward the center of pixel (u, v). Each pixel is thus represented as 6D vector describing ray in 3D space. The ray direction can be computed as df,u,v = Rf K1 [u, v, 1]T + tf , with f,u,v denoting the normalized direction vector df,u,v. 3.2. Creating varied camera paths from 360 videos Most video datasets exhibit significant bias towards forward-looking views, where the horizon remains level or close to the center of the image, as shown in fig. 5. However, such normal viewpoints restrict the expressive range that filmmaker might wish to achieve in storyboard. For instance, these datasets often lack cinematic shots, such as top-down, low-angle, or Dutch-angle shots. As result, generative video models trained on such data struggle to produce these types of cinematic shots, since they fall outside the training distribution. To address this limitation, we propose generating synthetic camera trajectories by sampling from 360 videos and rectifying them so that the horizon position is known throughout each sequence. Camera poses from SfM. From 360 video, we compute the relative motion between each frame by extracting six perspective crops (front, back, left, right, top, bottom), and processing the resulting video with structure-from-motion (SfM). This results in set of camera poses ESfM,f at each frame . Taking the first frame of the sequence = 0 as reference, we can then compute the relative motion Erel,f = E1 SfM,0ESfM,f . (1) Sampling random camera paths. We can generate several perspective videos from 360 sequence by sampling random camera rotation paths. We outline our approach in Algorithm 1. We first sample random pitch, roll, and yaw angles uniformly to obtain an initial viewpoint. Next, 1 to 4 additional random rotations are sampled and distributed across the sequence length. Those rotations are applied sequentially to produce smooth and varied motion paths. We further introduce variations in the camera field of view (FoV). For each sequence, we sample either 1, 2, or 3 keyframes at random timestamps, with an FoV ranging from 35 to 100. The FoV values are then interpolated using cubic splines with randomized derivative boundary conditions. Examples from our sampling are shown in fig. 4. This approach provides broad coverage of the possible viewpoints, as shown in fig. 5. Notably, this sampling slightly biases viewpoints towards the poles, which we found to better support model stability and representation of more extreme viewpoints. Gravity annotation. To anchor the videos within shared global reference frame, we align their rotation with gravity. We are able to compute the absolute, gravity-aligned camera pose for each frame as Eabs,f = φ(Rpano,0)Rpano,f Erel,f , (2) where φ() is function that preserves only the roll and pitch angles (removes the yaw) of the first sampled camera pose. After this adjustment, the transformed pose Eabs,f has no rotation about the global up-vector axis. This function φ() is obtained by computing the gravity direction in camera coordinates and inverting it to recover the corresponding up-vector, see the supplementary for the complete definition. This process establishes gravity-aligned coordinate system that is consistent across all videos, allowing rotations to be defined within common global frame. Consequently, each sequence is initialized at null yaw, reducing ambiguity and easing training on this representation. 3.3. Null-pitch conditioning We observed that captioning videos using their actual rotated crops causes the model to overemphasize the textual prompt when determining the camera angle, rather than Number of rotation axis Algorithm 1 Random camera path sampling 1: pitch0 Uniform(90, 90) 2: roll0 Uniform(90, 90) 3: yaw0 Uniform(0, 360) 4: Ri RYXZ(yaw0, pitch0, roll0) 5: {1, 2, 3, 4} 6: for = 1 to do 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: end for 18: for = 0 to 1 do 19: 20: end for 21: return Rpano θf clamp(s(f /F ), 0, 1) θmax Ri end for dt Uniform(0, 1) ts Uniform(0, 1 dt) te ts + dt S2 θmax Beta(1.0, 5.0) 720 dt Construct cubic spline s(t) on [ts, te] with random boundary derivatives for = 0 to 1 do Random rotation duration Random start time End time Random unit axis of rotation Random angular displacement Loop over the frames Compute the rotation angle Compute the rotation matrix Combine the rotations together exp([θf a]) Rpano,f ΠN i=1Ri strictly adhering to the Plucker conditioning. For instance, if prompt includes ground-level features like grass or asphalt but the camera condition is oriented toward the sky, the model tends to ignore the intended camera control and produce downward-looking viewpoint. To mitigate this entanglement, we caption each video using an upright, forwardlooking image with 90 field of view, which strengthens the models adherence to the camera conditioning (see fig. 6). We refer to this strategy as null-pitch conditioning. Specifically, we generate two sets of crops: one using the original randomly sampled camera trajectory, and another with pitch and roll set to 0 and yaw unchanged. As illustrated in fig. 3, trajectory that consistently points towards the ground would otherwise yield captions describing only ground features. model trained on such captions would then expect these elements to appear in its outputs, even when the camera conditioning specifies view of the sky, leading to semantically incorrect generations. Training with null-pitch captions instead provides descriptions of the full scene, making it easier for the model to handle extreme camera angles. During training, the randomly sampled perspective crops are used for the diffusion loss, while the null-pitch crops are used for captioning. We found that applying this scheme to 50% of the samples provides good balance between camera controllability and prompt adherence, as shown in fig. 6. 4. Experiments This section presents the training data, provides details on model training and losses, introduces our novel evaluation benchmark, presents baselines, and shows quantitative and qualitative experimental results on our benchmark. Figure 5. Comparison of parameter distribution between our sampling and typical video dataset for camera control (RealEstate10K [9]). Our sampling produces videos with more rotational motion, as measured by total angular distance (the sum of absolute angular differences between consecutive camera poses), and greater diversity in Euler angles (pitch, roll, yaw). All values on the x-axis are expressed in degrees. Up (+90) Forward (0) Down (-90) 4.2. Model and losses e e c p u / u Prompt: prairie with grass and flowers, and beautiful blue sky with few clouds. Figure 6. Entanglement between prompt and camera pitch. Without careful captioning strategy, diffusion models may ignore the camera conditioning when the prompt semantics conflict with it. Our null-pitch conditioning mitigates this issue and preserves accurate camera control. 4.1. Training data We use the WEB360 dataset [34], which contains geometrically calibrated panoramic videos with leveled horizons. From each 360 frame, we extract six perspective crops and apply ViPE [15] to obtain camera pose annotations, as detailed in Sec. 3.2. The estimated camera poses are then combined with the random crop sampling to obtain the final camera poses, as described in Sec. 3.2. During training, we sample camera trajectory using Algorithm 1. We use InternVL-3-2B [44] for captioning, providing it with 6 evenly spaced frames across the video timeline. This is also done during training since new videos from different trajectories are sampled each iteration. We adopt the same lightweight ControlNet architecture as AC3D [2], using CogVideoX-2B [41] as the backbone diffusion model. Following their method, we train the model only on the first 40% of diffusion timesteps and use 8 layers of the original network for the ControlNet training, leaving the remaining layers frozen. We use the same training loss with normalization [20], and change the relative Pluckerray representation to our absolute Plucker representation for camera control. We train on four NVIDIA A100 GPUs with 80GB VRAM until convergence ( 8, 000 iterations). See the supplementary material for details and results with our method trained on the WAN [31] backbone. 4.3. Evaluation benchmark There exists no standard evaluation dataset containing paired text prompts, camera poses, and reference videos that provide extreme camera angles. Most methods [2, 12] use test splits from video datasets like RealEstate10K [9], which have very limited diversity in terms of pitch and roll angles, as shown in fig. 5 and the supplementary material. We propose new evaluation benchmark, featuring diverse, high-quality videos, distributed with near-uniform pitch and roll coverage. We start with 371K video clips from the HQ subset of the SpatialVID [33] dataset, which still exhibit bias toward forward-looking orientations. We estimate the average pitch from the videos using Perspective Fields [17] and organize them into 10-degree bins spanning -85 to +85 degrees. We manually filter out videos where the pitch estimation clearly failed until we obtain subset of 140 videos with uniform pitch distribution. For the roll, we found that even among the 371K videos, nearly all have near-zero roll. We thus artificially add roll angle by randomly sampling roll trajectory between -40 and 40 and applying it by warping the video. We name the resulting dataset SpatialVID-extreme. By having diverse set of Prompt AC3D [2] + cam. text. PreciseCam [5] + WAN [31] Ours Prompt AC3D [2] + cam. text. PreciseCam [5] + WAN [31] Ours luxurious cliffside resort overlooks lush landscape, with vibrant greenery and distant town visible beneath bright, clear sky, evoking sense of serene exclusivity. quiet stone church with red pews, arched windows, and historical architecture exudes reverence and solemnity through its timeless design and natural lighting. poised wingsuit flyer stands on grassy mountain peak, overlooking forested valley and distant town beneath an overcast sky, evoking tension and grandeur. lively pedestrian street lined with traditional shops and trees, bathed in soft afternoon light, exudes relaxed, vibrant energy amid crowd of people. Figure 7. Qualitative results on the SpatialVID-extreme benchmark. The input absolute camera angle is shown as dark overlay on the image. We observe that our method aligns well with the overlay, showing the sky when the zenith (UP) is in the view and showing the horizon aligned with the equator line (going through FRONT), for example. Please refer to the supp. mat. for additional results. Table 1. Quantitative results on our SpatialVID-extreme evaluation dataset. Our method outperforms all baselines on both relative and absolute camera rotation metrics while achieving competitive performance on content alignment (CLIP) and aesthetics (FID, FVD). Method PitchErr. (abs.) GravityErr. (abs.) RotErr (rel.) TransErr CLIP FID FVD AC3D + cam. text. AC3D + cam. text. + abs. Plucker PreciseCam + WAN-I2V-CC Ours 41.65 39.31 30.88 23. 46.30 43.74 34.83 27.06 27.81 33.02 24.56 14.25 0.75 0.79 0.78 0.75 23.55 22.98 22.27 21.35 106.82 108.52 108.21 110.71 942.07 995.09 798.87 896. pitches and rolls, we ensure diverse set of text prompts and reference videos for FID and FVD computations. Please refer to the supplementary material for an evaluation of the diversity of our benchmark dataset. For the camera trajectories fed to the models, we reuse the original camera pose translations, but discard the rotation. We instead generate random rotation by sampling roll, pitch, and yaw trajectories (see the supp. for details). This strategy allows for broader range of rotations and also better tests the methods adherence to the camera trajectory. Indeed, we receive normal environment description as input prompt, but for extreme camera angles, such as looking straight up, adhering to the camera conditioning likely requires generating views of the sky. 4.4. Baselines and metrics PreciseCam + WAN. straightforward approach to camera-controlled video generation is to first use cameraconditioned image generator to synthesize the initial frame, and then apply an image-to-video model to produce the full sequence. This baseline follows that strategy: we use PreciseCam [5] to generate the first frame conditioned on the absolute pitch, roll, and field of view, and then use relative camera control model, WAN-2.1-Fun-I2V Camera Control [31], to extend it into video. AC3D + cam. text. We employ AC3D [2], text-to-video model with camera control, as baseline. We use their pretrained checkpoint that uses the same CogVideoX-2B as our model. To provide it with the required initial and final camera poses, we append text description of the absolute Table 2. Ablation study on null pitch conditioning. Incorporating null-pitch conditioning improves performance across all metrics and enhances the models robustness. Method PitchErr. (abs.) GravityErr. (abs.) RotErr (rel.) TransErr CLIP FID FVD w/o null pitch conditioning Ours 28.75 23.11 31.70 26.20 21.55 15. 0.76 0.74 21.29 21.45 113.90 110.75 983.50 937.62 poses, generated from fixed template, to the original content prompt (refer to the supp. for details). AC3D + cam. text. + abs. Pl ucker This baseline extends the previous one by replacing the relative camera poses with our absolute camera poses. We keep the same model and textual camera descriptions. Metrics. To evaluate camera rotation controls, we first measure absolute orientation accuracy using two metrics: angular pitch error, and gravity error, defined as the angle between the ground-truth up vector and the up vector of the generated video, in camera space. For each frame, we extract the pitch and up vector using Perspective Fields [17]. We do not directly evaluate roll error, as estimating it near the poles (looking straight up or down) is unreliable; however, roll deviations are implicitly reflected in the gravity error. To evaluate the relative rotation error (RotErr), we run VGGT on the generated frames and compute the average relative angular error. We reuse the same definition of rotation error as [11]. We evaluate content adherence using the CLIP score [13] to measure the alignment between the generated video frames and the reference text description. We employ the Frechet Inception Distance (FID) and Frechet Video Distance (FVD) to assess the visual quality and realism of the generated frames and sequences, respectively. To evaluate camera motion accuracy, we compute translation error by comparing the estimated and ground-truth trajectories; both trajectories are normalized prior to computing the error to account for scale differences. 4.5. Results We present the result of our evaluation in Table 1, which shows that our method clearly outperforms existing methods. Without any change to the architecture, our method nearly halves the pitch and gravity errors and significantly reduces the relative rotation error. We attribute the slight decrease in performance on CLIP, FID and FVD scores to differences in the training data: AC3D is trained on all 65k videos and prompts from RealEstate10K, whereas we only train on 2K 360 videos from WEB360. We showcase qualitative results in Fig. 7. We observe that while AC3D follows the motion correctly (e.g., the camera progressively looks down throughout the video), it fails to capture the initial target view direction. Feeding it instead the absolute camera poses, which is out of its training distribution, results in negligible improvements in absolute metrics but generates noticeable artefacts (see supp.). PreciseCam [5] coupled with WAN-I2V [31] exhibits several failure cases, in particular when used with extreme or unusual camera angles. This is because the input prompt describes the overall environment, which may not be what the camera is pointed at. For example, the prompt in fig. 7 (top left) describes ground elements while the camera aims at the sky. We observe that PreciseCam focuses on text content and often fails to produce the correct viewpoint when it cant place the content in semantically meaningful way within the image. In contrast, we use our null-pitch conditioning to ensure that generic descriptions of environments do not prevent the generation of accurate camera viewpoints, as shown in fig. 7 (right). Ablations. We ablate our proposed null-pitch conditioning in Tab. 2, which shows that removing it leads to 20% and 17% drop in absolute camera pitch and gravity accuracy, respectively. This is corroborated qualitatively in fig. 6: in the top and middle rows, where null-pitch conditioning is omitted, the model incorrectly introduces elements such as grass even when the camera is oriented fully upward. 5. Discussion While our method introduces novel capability for text-tovideo models, it has few limitations. First, our current on-the-fly camera sampling supports only rotational motion. Extending to translationspotentially by leveraging recent successes in real-time novel view synthesis methods such as Gaussian Splatsremains an interesting direction for future work. Second, the generated videos can exhibit visual artifacts, as visible in the generated crowd in fig. 7. We expect that continued progress in video generation models will mitigate these issues while remaining compatible with our framework. 6. Conclusion We presented GIMBALDIFFUSION, the first method to enable absolute camera control in text-to-video generation by using gravity as global reference. Our data pipeline combines 360 panoramic videos with camera pose estimation to produce gravity-aligned camera pose annotations. Leveraging panoramic videos allows us to sample the full sphere of possible viewpoints, reducing the domain gap for rare but expressive camera angles. In addition, we proposed null-pitch conditioning, data generation step that enhances camera conditioning fidelity when the model is given prompts with conflicting cues. We believe our work lays the foundation for physically grounded camera control in video generation, empowering creators to produce dynamic storyboards and convey their visual ideas with greater precision and speed. Acknowledgements. This research was supported by Adobe and Natural Sciences and Engineering Research Council of Canada (NSERC) scholarship, reference number 600578. Computing resources were provided by Adobe and the Digital Research Alliance of Canada. The authors thank Yohan Poirier-Ginter, Qitao Zhao, Rahul Sajnani, Jack Hilliard and Jonathan Roussel for discussions and proofreading help."
        },
        {
            "title": "References",
            "content": "[1] Dinesh Acharya, Zhiwu Huang, Danda Pani Paudel, and Luc Van Gool. Towards high resolution video generation with progressive growing of sliced wasserstein gans. In CoRR, 2018-01-01. 2 [2] Sherwin Bahmani, Ivan Skorokhodov, Guocheng Qian, Aliaksandr Siarohin, Willi Menapace, Andrea Tagliasacchi, David Lindell, and Sergey Tulyakov. Ac3d: Analyzing and improving 3d camera control in video diffusion transformers. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2025. 2, 3, 4, 6, 7 [3] Sherwin Bahmani, Ivan Skorokhodov, Aliaksandr Siarohin, Willi Menapace, Guocheng Qian, Michael Vasilkovsky, HsinYing Lee, Chaoyang Wang, Jiaxu Zou, Andrea Tagliasacchi, David B. Lindell, and Sergey Tulyakov. VD3D: Taming large video diffusion transformers for 3d camera control. In Int. Conf. Learn. Represent., 2025. 4 [4] Jianhong Bai, Menghan Xia, Xiao Fu, Xintao Wang, Lianrui Mu, Jinwen Cao, Zuozhu Liu, Haoji Hu, Xiang Bai, Pengfei Wan, et al. Recammaster: Camera-controlled generative rendering from single video. In IEEE/CVF Int. Conf. Comput. Vis., 2025. 2 [5] Edurne Bernal-Berdun, Ana Serrano, Belen Masia, Matheus Gadelha, Yannick Hold-Geoffroy, Xin Sun, and Diego Gutierrez. PreciseCam: Precise camera control for text-to-image generation. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2025. 3, 7, 8 [6] Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing Yan, Yizhe Zhu, Xiao Yang, and Mohammad Soleymani. MagicPose: Realistic Human Poses and Facial Expressions Retargeting with Identity-aware Diffusion. In Int. Conf. Mach. Learn., 2024. [7] Weifeng Chen, Jie Wu, Pan Xie, Hefeng Wu, Jiashi Li, Xin Xia, Xuefeng Xiao, and Liang Lin. Control-a-video: Controllable text-to-video generation with diffusion models. In CoRR, 2023. 2 [8] Tien Do, Khiem Vuong, and Hyun Soo Park. Egocentric scene understanding via multimodal spatial rectifier. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2022. 3 [9] Google Research. RealEstate10K: large-scale dataset of camera poses. https://google.github.io/ realestate10k/, 2018. Camera trajectories from approximately 80,000 video clips (from 10,000 YouTube videos), totaling about 10 million frames; poses generated via SLAM and bundle-adjustment. 6 [10] Zekai Gu, Rui Yan, Jiahao Lu, Peng Li, Zhiyang Dou, Chenyang Si, Zhen Dong, Qifeng Liu, Cheng Lin, Ziwei Liu, Wenping Wang, and Yuan Liu. Diffusion as shader: 3daware video diffusion for versatile video generation control. In ACM SIGGRAPH Conf., 2025. 3 [11] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. CameraCtrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. 2, 4, 8 [12] Hao He, Ceyuan Yang, Shanchuan Lin, Yinghao Xu, Meng Wei, Liangke Gui, Qi Zhao, Gordon Wetzstein, Lu Jiang, and Hongsheng Li. CameraCtrl ii: Dynamic scene exploration via camera-controlled video diffusion models. arXiv preprint arXiv:2503.10592, 2025. 2, 4, [13] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Conf. Emp. Metho. Nat. Lang. Proc., 2021. 8 [14] Li Hu, Xin Gao, Peng Zhang, Ke Sun, Bang Zhang, and Liefeng Bo. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. arXiv preprint arXiv:2311.17117, 2023. 2 [15] Jiahui Huang, Qunjie Zhou, Hesam Rabeti, Aleksandr Korovko, Huan Ling, Xuanchi Ren, Tianchang Shen, Jun Gao, Dmitry Slepichev, Chen-Hsuan Lin, Jiawei Ren, Kevin Xie, Joydeep Biswas, Laura Leal-Taixe, and Sanja Fidler. Vipe: Video pose engine for 3d geometric perception. In NVIDIA Research Whitepapers, 2025. 3, 6 [16] Fang Jiang et al. Megasam: Scaling up camera pose estimation with foundation model for structure-from-motion. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2025. 3 [17] Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver Wang, Kevin Blackburn-Matzen, Matthew Sticha, and David Fouhey. Perspective fields for single image camIn IEEE/CVF Conf. Comput. Vis. Pattern era calibration. Recog., 2023. 6, 8 [18] Linyi Jin, Jianming Zhang, Yannick Hold-Geoffroy, Oliver Wang, Kevin Matzen, Matthew Sticha, and David F. Fouhey. Perspective fields for single image camera calibration. IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2023. 3 [19] Yash Kant, Aliaksandr Siarohin, Ziyi Wu, Michael Vasilkovsky, Guocheng Qian, Jian Ren, Riza Alp Guler, Bernard Ghanem, Sergey Tulyakov, and Igor Gilitschenski. Spad: Spatially aware multi-view diffusers. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2024. 4 [20] Tero Karras, Miika Aittala, Jaakko Lehtinen, Janne Hellsten, Timo Aila, and Samuli Laine. Analyzing and improving the training dynamics of diffusion models. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2024. [34] Qian Wang, Weiqi Li, Chong Mou, Xinhua Cheng, and Jian Zhang. 360dvd: Controllable panorama video generation with 360-degree video diffusion model. arXiv preprint arXiv:2401.06578, 2024. 6 [35] Shuzhe Wang, Vincent Leroy, Yohann Cabon, Boris Chidlovskii, and Jerome Revaud. Dust3r: Geometric 3d vision made easy. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2024. 3 [36] Lilian Weng. Diffusion models for video generation. LilLog (blog), 2024. https://lilianweng.github.io/posts/2024-04-12diffusion-video/. 2 [37] Scott Workman, Menghua Zhai, and Nathan Jacobs. Horizon lines in the wild. In Brit. Mach. Vis. Conf., 2016. 3 [38] Changchang Wu et al. Visualsfm: visual structure from motion system, 2011. 3 [39] Wenqi Xian, Zhengqi Li, Matthew Fisher, Jonathan Eisenmann, Eli Shechtman, and Noah Snavely. Uprightnet: geometry-aware camera orientation estimation from single images. In IEEE/CVF Int. Conf. Comput. Vis., 2019. 3 [40] Jinbo Xing, Long Mai, Cusuh Ham, Jiahui Huang, Aniruddha Mahapatra, Chi-Wing Fu, Tien-Tsin Wong, and Feng Liu. Motioncanvas: Cinematic shot design with controllable image-to-video generation. In Proceedings of the Special Interest Group on Computer Graphics and Interactive Techniques Conference Conference Papers, pages 111, 2025. 3 [41] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 2, 6 [42] Jiraphon Yenphraphai, Xichen Pan, Sainan Liu, Daniele Image sculpting: Precise obPanozzo, and Saining Xie. ject editing with 3D geometry control. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2024. 2 [43] Mark YU, Wenbo Hu, Jinbo Xing, and Ying Shan. TrajectoryCrafter: Redirecting camera trajectory for monocular videos via diffusion models. In IEEE/CVF Int. Conf. Comput. Vis., 2025. 3 [44] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 6 [21] Florian Kluger, Hanno Ackermann, Michael Ying Yang, and Bodo Rosenhahn. Temporally consistent horizon lines. In Int. Conf. Robot. Autom., 2020. [22] Peter Kocsis, Julien Philip, Kalyan Sunkavalli, Matthias Nießner, and Yannick Hold-Geoffroy. LightIt: Illumination modeling and control for diffusion models. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2024. 2 [23] Vincent Leroy, Yohann Cabon, and Jerˆome Revaud. Grounding image matching in 3d with Mast3r. In Eur. Conf. Comput. Vis., 2024. 3 [24] Ruilong Li, Brent Yi, Junchen Liu, Hang Gao, Yi Ma, and Angjoo Kanazawa. Cameras as relative positional encoding. In Adv. Neural Inform. Process. Syst., 2025. 3 [25] Yitong Li, Martin Min, Dinghan Shen, David Carlson, and Lawrence Carin. Video generation from text. In Assoc. Adv. of Art. Int., 2018. 2 [26] Nadav Magar, Amir Hertz, Eric Tabellion, Yael Pritch, Alex Rav-Acha, Ariel Shamir, and Yedid Hoshen. LightLab: Controlling light sources in images with diffusion models. In ACM SIGGRAPH Conf., 2025. 2 [27] Pierre Moulon, Pascal Monasse, Romuald Perrot, and Renaud Marlet. Openmvg: Open multiple view geometry. In Int. Work. Reproduc. Res. Patt. Recog., 2016. [28] Johannes L. Schonberger and Jan-Michael Frahm. Structurefrom-motion revisited. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2016. 3 [29] Sergey Tulyakov, Ming-Yu Liu, Xiaodong Yang, and Jan Kautz. Mocogan: Decomposing motion and content for video generation. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2018. 2 [30] Alexander Veicht, Paul-Edouard Sarlin, Philipp Lindenberger, and Marc Pollefeys. GeoCalib: Single-image calibration with geometric optimization. In Eur. Conf. Comput. Vis., 2024. 3 [31] Team Wan, Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, Jiayu Wang, Jingfeng Zhang, Jingren Zhou, Jinkai Wang, Jixuan Chen, Kai Zhu, Kang Zhao, Keyu Yan, Lianghua Huang, Mengyang Feng, Ningyi Zhang, Pandeng Li, Pingyu Wu, Ruihang Chu, Ruili Feng, Shiwei Zhang, Siyang Sun, Tao Fang, Tianxing Wang, Tianyi Gui, Tingyu Weng, Tong Shen, Wei Lin, Wei Wang, Wei Wang, Wenmeng Zhou, Wente Wang, Wenting Shen, Wenyuan Yu, Xianzhong Shi, Xiaoming Huang, Xin Xu, Yan Kou, Yangyu Lv, Yifei Li, Yijing Liu, Yiming Wang, Yingya Zhang, Yitong Huang, Yong Li, You Wu, Yu Liu, Yulin Pan, Yun Zheng, Yuntao Hong, Yupeng Shi, Yutong Feng, Zeyinzi Jiang, Zhen Han, Zhi-Fan Wu, and Ziyu Liu. WAN: Open and Advanced Large-Scale Video Generative Models. arXiv preprint arXiv:2503.20314, 2025. 2, 6, 7, 8 [32] Jianyuan Wang, Minghao Chen, Nikita Karaev, Andrea Vedaldi, Christian Rupprecht, and David Novotny. Vggt: Visual geometry grounded transformer. In IEEE/CVF Conf. Comput. Vis. Pattern Recog., 2025. 3 [33] Jiahao Wang, Yufeng Yuan, Rujie Zheng, Youtian Lin, Jian Gao, Lin-Zhuo Chen, Yajie Bao, Yi Zhang, Chang Zeng, Yanxi Zhou, Xiaoxiao Long, Hao Zhu, Zhaoxiang Zhang, Xun Cao, and Yao Yao. SpatialVID: large-scale video dataset with spatial annotations, 2025. 2,"
        }
    ],
    "affiliations": [
        "Adobe",
        "Universite Laval"
    ]
}