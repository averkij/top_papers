{
    "paper_title": "GAS: Improving Discretization of Diffusion ODEs via Generalized Adversarial Solver",
    "authors": [
        "Aleksandr Oganov",
        "Ilya Bykov",
        "Eva Neudachina",
        "Mishan Aliev",
        "Alexander Tolmachev",
        "Alexander Sidorov",
        "Aleksandr Zuev",
        "Andrey Okhotin",
        "Denis Rakitin",
        "Aibek Alanov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While diffusion models achieve state-of-the-art generation quality, they still suffer from computationally expensive sampling. Recent works address this issue with gradient-based optimization methods that distill a few-step ODE diffusion solver from the full sampling process, reducing the number of function evaluations from dozens to just a few. However, these approaches often rely on intricate training techniques and do not explicitly focus on preserving fine-grained details. In this paper, we introduce the Generalized Solver: a simple parameterization of the ODE sampler that does not require additional training tricks and improves quality over existing approaches. We further combine the original distillation loss with adversarial training, which mitigates artifacts and enhances detail fidelity. We call the resulting method the Generalized Adversarial Solver and demonstrate its superior performance compared to existing solver training methods under similar resource constraints. Code is available at https://github.com/3145tttt/GAS."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 2 ] . [ 1 9 9 6 7 1 . 0 1 5 2 : r Preprint. GAS: IMPROVING DISCRETIZATION OF DIFFUSION ODES VIA GENERALIZED ADVERSARIAL SOLVER Aleksandr Oganov,1,2,, Ilya Bykov,1, Eva Neudachina,1, Mishan Aliev1, Alexander Tolmachev, Alexander Sidorov, Aleksandr Zuev, Andrey Okhotin1, Denis Rakitin1, Aibek Alanov1 1HSE University, Russia 2Lomonosov Moscow State University, Russia"
        },
        {
            "title": "ABSTRACT",
            "content": "While diffusion models achieve state-of-the-art generation quality, they still suffer from computationally expensive sampling. Recent works address this issue with gradient-based optimization methods that distill few-step ODE diffusion solver from the full sampling process, reducing the number of function evaluations from dozens to just few. However, these approaches often rely on intricate training techniques and do not explicitly focus on preserving fine-grained details. In this paper, we introduce the Generalized Solver: simple parameterization of the ODE sampler that does not require additional training tricks and improves quality over existing approaches. We further combine the original distillation loss with adversarial training, which mitigates artifacts and enhances detail fidelity. We call the resulting method the Generalized Adversarial Solver and demonstrate its superior performance compared to existing solver training methods under similar resource constraints. Code is available at https://github.com/3145tttt/GAS."
        },
        {
            "title": "INTRODUCTION",
            "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020b) offer state-of-the-art generation quality in diverse vision problems, including unconditional and conditional (Dhariwal & Nichol, 2021; Ho & Salimans, 2022) generation, text-to-image (Nichol et al., 2021; Ramesh et al., 2022; Saharia et al., 2022; Rombach et al., 2022; Esser et al., 2024), text-to-video (Blattmann et al., 2023; Brooks et al., 2024; Zheng et al., 2024; Chen et al., 2024b) and even text-to-3D (Poole et al., 2022; Wang et al., 2023) generation. One of the reasons for their success consists in satisfying both high sample quality (Dhariwal & Nichol, 2021; Karras et al., 2022) and mode coverage from the generative trilemma (Xiao et al., 2021). In theory, this allows diffusion models to produce desirable samples from the target distribution given unlimited computation time. Besides, many improvements were made to satisfy the third requirement on generation speed. One way to tackle high inference time is to train new model that utilizes the pre-trained diffusion and requires fewer inference steps. This may be achieved by straightening the generation trajectories (Liu et al., 2022b; 2023; Wang et al., 2024) or by directly performing diffusion distillation (Salimans & Ho, 2022; Song et al., 2023; Sauer et al., 2023; Yin et al., 2023) into few-step student. These training-based methods are capable of fast generation with superior quality on large-scale scenarios. Their training procedures, however, are computation and memory-heavy and may be infeasible for users with resource constraints on cutting-edge problems, such as video generation. Due to the mentioned resource requirements, the lightweight approach of directly accelerating generation is preferable most of the time. Such inference-time methods as designing specific solvers (Song et al., 2020a; Lu et al., 2022a; Zhang & Chen, 2022), caching intermediate steps (Ma et al., 2024; Wimbauer et al., 2024), or performing quantization (Gu et al., 2022; Badri & Shaji, 2023), push the boundaries of the pre-trained model by utilizing its knowledge as much as possible given fixed computational budget. Among them, specifically designed solvers are mostly theoretically sound and are capable of producing high-quality samples similar to the full-inference model. However, Equal contribution. Correspondence: 3145tttt@gmail.com 1 Preprint. Figure 1: Illustration of the Generalized Adversarial Solver image generation in comparison with the training-free UniPC (Zhao et al., 2024) solver with equal number of function evaluations (NFEs). Our method shows superior results that are almost identical to teacher images in terms of generation quality. they require significant hyperparameter search (Zhou et al., 2024b; Zhao et al., 2024) for each model and may be suboptimal depending on the particular setting. natural improvement of the idea consists of training (hyper-)parameters of the inference-time \"student\" sampler to match the full-inference \"teacher\" model. The approach is free-form and allows for optimizing timestep schedule (Sabour et al., 2024; Tong et al., 2024) as well as the sampler coefficients (Kim et al., 2024; Frankel et al., 2025) for each prediction step. Currently existing methods for training the sampler succeed in improving test-time efficiency of the model compared to the standard solvers. At the same time, they do not realize the full potential of the paradigm and tend to have inefficiencies that lead to nuanced and complicated training schemes. Among these are the unstable loss scale (Sabour et al., 2024), limited parameter space (Tong et al., 2024) and disentanglement of the parameter subsets (Frankel et al., 2025) which we find to be harmful for training. Besides, straightforward sampler distillation into student with limited parameters may be ineffective for preserving the fine-grained details and may interfere with the generation quality. In this paper, we aim to tackle the aforementioned issues by introducing simple yet effective sampler parameterization and modifying the distillation loss. Specifically, we construct sampler that performs each sampling step by calculating weighted sum of the current velocity direction with all of the points and directions from previous steps. We propose to utilize pre-defined solver as time-dependent guidance and learn correction to its theoretically derived weights to facilitate and accelerate training. On top of that, we endow the sampler distillation with the adversarial loss (Goodfellow et al., 2014) to further boost the sampler quality. Most importantly, we 1. Introduce novel sampler parameterization that we call the Generalized Solver and demonstrate its significant impact on training acceleration; 2. Combine it with the adversarial training and validate its positive impact on the fine-grained generation details; 3. Show that the resulting Generalized Adversarial Solver achieves superior results compared to the existing methods of solver/timestep training on several pixel-space and latent-space data sets."
        },
        {
            "title": "2 BACKGROUND",
            "content": "2.1 DIFFUSION MODELS Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020; Song et al., 2020b) simulate the data distribution by defining the forward process of gradual data noising and constructing its time reversal. The forward process is commonly defined by sequence {pt0}t[0,T ] of transition probabilities I(cid:1). It perturbs the initial data distribution pdata(x0) = p0(x0) by pt0(xtx0) = (cid:0)xt αtx0, σ2 destroying part of its signal and replacing it with the independent Gaussian noise. Here, αt and σ2 are positive differentiable functions that define the corresponding noise schedule. Typically, their choice ensures that the sequence of the corresponding marginal distributions pt(xt) converges to simple and tractable prior distribution pT (xT ) (e.g. standard normal). For each noise schedule one 2 Preprint. can construct the equivalent Probability Flow ODE (PF-ODE) (Song et al., 2020b) where setting (cid:20) (t)xt dxt = 1 2 g2(t)xt log pt(xt) (cid:21) dt, (t) = log αt dt , g2(t) = dσ2 2 log αt dt σ2 (1) (2) and sampling the endpoint xT from the prior distribution pT ensures (Lu et al., 2022a) that xt pt for all timesteps. Essentially, ODE formulation allows one to obtain backward process of data generation by reversing the velocity of the particle given access to the score function xt log pt(xt) of the perturbed data distribution. In practice, diffusion models approximate the score function by optimizing the Denoising Score Matching (Vincent, 2011) objective (cid:90) min θ Ep0,t(x0,xt)sθ(xt, t) xt log pt0(xtx0)2dt, (3) where the score functions xt log pt0(xtx0) of the conditional Gaussian distributions are tractable and equal to (xt αtx0)/σ2 . Besides the score networks, one can directly approximate the ODE velocity function by setting vθ(xt, t) = (t)xt (1/2)g2(t)sθ(xt, t). 2.2 ODE SOLVERS Sampling from diffusion model amounts to numerically approximating the solution of the corresponding PF-ODE (Eq. 1). Standard numerical methods for solving general-form ODE dxt = v(xt, t)dt are mainly based on approximating the direction xt+h xt via Taylor expansion. The first-order Euler scheme makes step v(xt, t), which is simple, yet has large discretization error. Its higher-order modifications generally approximate the derivatives with finite differences. This correction allows Runge-Kutta methods to produce high-quality results (Lu et al., 2022a; Zhang & Chen, 2022; Karras et al., 2022). However, these methods require mid-point evaluations, which harms performance in low-NFE regimes (see e.g. (Zhang & Chen, 2022, Table 2)). In contrast, Linear Multistep solvers (Liu et al., 2022a; Zhang & Chen, 2022) use only previously calculated points and directions for the same approximation, thus remain useful in this setting. Recently designed solvers such as DDIM (Song et al., 2020a), DPM-Solver(++) (Lu et al., 2022a;b), DEIS (Zhang & Chen, 2022), and UniPC (Zhao et al., 2024), exploit the semi-linear nature of the PF-ODE (Hochbruck & Ostermann, 2010). They approximate the integral in the \"variation of constants\" formula αt ατ allowing more accurate steps thanks to the non-unit coefficient of xu, and enabling computationally efficient multistep solvers. s(xτ , τ )dτ, αt αu xu xt = g2(τ ) 2 (cid:90) (4) Several previous works highlight the importance of choosing the timestep schedule (the set of time points at which function evaluations are performed), which has significant impact on the image generation quality (see (Karras et al., 2022, Appendix D.1) and (Frankel et al., 2025, Appendix H.3)). 2.3 SOLVER AND SCHEDULE DISTILLATION Several recently introduced acceleration methods outsource the choice of solver coefficients and the timestep schedule to the gradient-based optimization. Specifically, LD3 (Tong et al., 2024) and S4S (Frankel et al., 2025) formulate this as an instance of knowledge distillation (Hinton et al., 2015). Given the pre-trained diffusion model and the corresponding ODE dxt = v(xt, t)dt, one can define the complete \"teacher\" sampler to be the output of multi-step high-quality approximation of the PF-ODE, which we denote by ΦT (xT ) = ODESolve (xT , v(, ), 0 Schedule, Solver; Params) . (5) Here, xT is the initial value, v(, ) is the corresponding velocity field and 0 shows the interval, where we solve the ODE. \"Solver\" and \"Schedule\" define the sampling scheme and \"Params\" account 3 Preprint. for the additional parameters of the scheme. Then, one could take any parameterization of the lightweight \"student\" ΦS (xT θ, ϕ, ξ) = ODESolve (xT , v(, ), 0 Schedule(θ), Solver(ϕ); Params(ξ)) (6) with bounded computational requirements and optimize its parameters by minimizing distance between the corresponding outputs min θ,ϕ,ξ Ldistill(θ, ϕ, ξ) = min θ,ϕ,ξ EpT (xT )d (cid:0)ΦS (xT θ, ϕ, ξ); ΦT (xT )(cid:1) . (7) In addition, LD3 and S4S account for the limited parameterization of the student and simplify its objective by allowing to slightly adapt the input and facilitate replication of the teacher output min θ,ϕ,ξ Lsoft(θ, ϕ, ξ) = min θ,ϕ,ξ EpT (xT ) min B(xT ,rσT ) (cid:0)ΦS (x θ, ϕ, ξ); ΦT (xT )(cid:1) , (8) where B(xT , rσT ) = {x : xxT 2 rσT } is the ball centered in xT with radius rσT controlled by the additional hyperparameter r. We thoroughly discuss parameterizations of the methods and compare them with our Generalized Solver in Section 3.1. 2.4 ADVERSARIAL TRAINING Adversarial training (Goodfellow et al., 2014) is powerful way to guide free-form generator Gθ(z) towards realistic outputs via optimizing the minimax objective (Nowozin et al., 2016) min θ max ψ Ep(z)f (Dψ(Gθ(z))) + Epdata(x)f (Dψ(x)) . (9) Here, (t) is commonly equal to log(1 + et), the discriminator Dψ is trained to distinguish real samples from the fake ones, while the generator aims to trick him. Family of the GAN losses with the form of Equation 9 (Nowozin et al., 2016; Mao et al., 2017; Lim & Ye, 2017) suffers from mode collapse (Arjovsky et al., 2017; Gulrajani et al., 2017). One of the alternatives is the relativistic GAN loss (Jolicoeur-Martineau, 2018) min θ max ψ Ep(z)pdata(x)f (cid:16) Dψ(Gθ(z)) Dψ(x) (cid:17) (10) that is specifically designed to discourage mode dropping (Sun et al., 2020). Together with the gradient penalty Lgrad(θ, ψ) = λ1Epdata(x)xDψ(x)2 + λ2Ep(z)xDψ(Gθ(z))2 on discriminator outputs and architecture improvements, relativistic loss allows Huang et al. (2024) to build novel high-quality GAN baseline R3GAN which we use throughout the paper. (11)"
        },
        {
            "title": "3 METHOD",
            "content": "In this section, we construct Generalized Adversarial Solver (GAS): an automatic sampler learning method that combines simple yet effective parameterization with distillation and adversarial training. 3.1 GENERALIZED SOLVER (GS) In Section 2.2 we have discussed that linear multi-step solvers and their specifically designed diffusion counterparts are the preferable families under strict requirements on computations. Given timestep schedule = t0 > t1 > . . . > tN = δ > 0 and order they all have the same signature xn+1 = anxn + (cid:88) j=max(nK+1,0) cj,nv(xj, tj), (12) where the coefficients an := an(tn, tn+1) and cj,n := cj,n(tj, tj+1) typically depend on the current and the next timesteps. We propose several modifications to this basic signature. First, we stress that the less restriction on NFE is, the less parameters the method has. Second, one can see that depending on the parameterization of the diffusion model the formula may also contain the weighted 4 Preprint. Figure 2: Illustration of the Generalized Adversarial Solver. Our student makes each sampling step by calculating the weighted average of all previous points and velocity directions. We train the corresponding weights and timestep schedule via distillation and adversarial loss. sum of previous points (if e.g. one substitutes v(xj, tj) = (tj)xj (1/2)g2(tj)s(xj, tj)) along with the network predictions. We thus propose to increase the capacity of the signature by adding the weighted sum of all previous points 1 and remove the restriction on the order of the solver: xn+1 = (cid:88) j=0 aj,nxj + (cid:88) j=0 cj,nv(xj, tj). (13) Given this signature, we next define our parameterization that has three sets of parameters: (θ, ϕ, ξ). The first set θ of parameters defines the timestep schedule via the cumprod transformation: the logits θn are transformed into \"stick breaking\" portions σ(θn) [0, 1]. The timesteps are then defined as tθ = (T δ) (cid:89) j=1 σ(θj) + δ. (14) The second set ϕ defines the solver coefficients. However, we do not straightforwardly set aj,n := aj,n(ϕ) and cj,n := cj,n(ϕ). Instead, we use powerful base multi-step solver (e.g. DPM-Solver++(3M) (Lu et al., 2022b)) as source of theoretical guidance for the trained coefficients. This base solver offers time-dependent theoretical coefficients an,n(tθ j:n+1), which we can use as strong backbone for our solver. We then train additive corrections to these coefficients in the following way. We set n:n+1) and cj,n(tθ aj,n(θ, ϕ) := (cid:40) an,n(tθ ˆaj,n(ϕ), else, n:n+1) + ˆan,n(ϕ), = n; (15) thus adding trainable scalar ˆan,n(ϕ) to the current point coefficient an,n(tθ scalars ˆaj,n(ϕ) for all the previous point coefficients. n:n+1) and training Next, since the \"old\" velocities (computed more than steps before) do not have theoretical coefficients, we train one scalar ˆcj,n(ϕ) per timestep and set cj,n(θ, ϕ) = ˆcj,n(ϕ). (16) Finally, we define the coefficients before the \"recent\" velocities (computed less than steps before). Here, theoretical base coefficients are typically constructed via weighted sum of the approximations ˆv(j) of the higher-order derivatives v(j)(xn, tθ n) via finite differences (which are themselves weighted sums of previously computed velocities). This leads to the sum of the form (cid:80)K1 j:n+1) ˆv(j) . j=0 cj,n(tθ 1Theoretically, one could represent previous points as linear combination of the previous velocity vectors. However, this \"over-parameterization\" may simplify training. 5 Preprint. Table 1: Comparison of solver parameterizations between our GS, LD3 (Tong et al., 2024) and S4S (Frankel et al., 2025). We propose to add additive guidance to several velocity coefficients with theoretical term from pre-defined solver instead of just two multiplicative terms an and bn. The guidance is marked by the dependence of coefficients on θ. We add weighted sum of the previous points to the prediction."
        },
        {
            "title": "Method",
            "content": "LD3 S4S GS xn+1 = an(tθ n, tθ n+1) xn +"
        },
        {
            "title": "Parameterization",
            "content": "n (cid:80) j=max(nK+1,0) cj,n(tθ , . . . , tθ n+1) v(xj, tθ + ξj) xn+1 = an(tθ n, tθ n+1) xn + bn(tθ n, tθ n+1) (cid:80) j=max(nK+1,0) cj,n(ϕ) v(xj, tθ + ξj) xn+1 = an,n(θ, ϕ) xn + n1 (cid:80) j=0 aj,n(ϕ) xj + (cid:80) j=0 cj,n(θ, ϕ) v(xj, tθ + ξj) Combined with the finite-difference approximation of the derivatives ˆv(j) we obtain = (cid:80)n i=nj ωi,n v(xi, tθ ), K1 (cid:88) (cid:88) cj,n(tθ j:n+1) ωi,n v(xi, tθ ). (17) j=0 i=nj Here, we train additive corrections ˆcj,n(ϕ) for the coefficients cj,n(tθ derivatives approximation. We thus obtain sum j:n+1) corresponding to the K1 (cid:88) j=0 (cid:104) cj,n(tθ j:n+1) + ˆcj,n(ϕ) (cid:88) (cid:105) i=nj ωi,n v(xi, tθ ), which produces recent velocity coefficients ci,n(θ, ϕ) = ωi,n K1 (cid:88) j=ni (cid:2)cj,n(tθ j:n+1) + ˆcj,n(ϕ)(cid:3) . (18) (19) We initialize the corrections with zeros to obtain an efficient initialization. By doing this, we ensure that even sudden change of the timesteps does not completely ruin the solver performance due to the meaningful dependence of its coefficients on time. We show the positive impact of the theoretical guidance in Section 4.2. The last set ξ of parameters acts as correction to the timesteps that we evaluate the pre-trained model on. Analogous to Tong et al. (2024) and Frankel et al. (2025) we define the decoupled timesteps tθ + ξj and use them for making predictions with the diffusion model. Combining the signature from Equation 13 with the introduced parameterization, we obtain the Generalized Solver (GS) xn+1 = an,n(θ, ϕ) xn + n1 (cid:88) j=0 aj,n(ϕ) xj + (cid:88) j=0 cj,n(θ, ϕ) v(xj, tθ + ξj). (20) and extensively compare it with the parameterizations of LD3 and S4S in Table 1. 3.2 GENERALIZED ADVERSARIAL SOLVER (GAS) We train the Generalized Solver on the previously established distillation loss from Equation 7. Specifically, we take from the distillation loss (Equation 7) to be LPIPS in pixel-space and L1 in latent-space experiments. We do not use the soft version from Equation 8. It is important to examine the \"solver distillation\" problem from another perspective. Essentially, it is an instance of the paired translation problem/learning mapping from its input/output samples. Several works (Isola et al., 2017; Ledig et al., 2017) have shown that the standard regression loss could greatly benefit from adding the adversarial loss on the outputs. Recently, adversarial loss has been established as 6 Preprint. powerful tool to boost performance of the diffusion distillation (Kim et al., 2023; Sauer et al., 2023; 2024; Yin et al., 2024) methods. Given this, we augment distillation-based training of the GS via distillation loss and obtain the Generalized Adversarial Solver (GAS). We denote our solvers output as ΦS (xT θ, ϕ, ξ) = ODESolve(xT , v(, ), 0 GS(θ, ϕ, ξ)), (21) where GS(θ, ϕ, ξ) defines the Generalized Solver signature and parameterization, defined in Section 3.1 and Equation 20 specifically. We denote the discriminator by Dψ and train GAS on the sum of distillation and adversarial losses (cid:40)min θ,ϕ,ξ max ψ LGAS(θ, ϕ, ξ, ψ) = min max θ,ϕ,ξ ψ Ladv(θ, ϕ, ξ, ψ) = EpT (xT )pT (yT )f (cid:0)Dψ Ldistill(θ, ϕ, ξ) + Ladv(θ, ϕ, ξ, ψ); (cid:0)ΦS (xT θ, ϕ, ξ)(cid:1) Dψ (cid:0)ΦT (yT )(cid:1)(cid:1) . (22) We exploit R3GAN (Huang et al., 2024) relativistic loss with (t) = log(1 + et) and add the discriminator gradient penalties from Equation 11 to facilitate its training dynamics. The incorporation of the adversarial loss is also effective in terms of removing generation artifacts in low NFE regimes, where regression task becomes harder. We will further demonstrate this in Section 4."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "We demonstrate efficiency of the proposed method by conducting experiments on several pixel and latent space experiments. We perform evaluation on pixel-space CIFAR10 (3232) (Krizhevsky & Hinton, 2009), FFHQ (6464) (Karras et al., 2019), and AFHQv2 (6464) (Choi et al., 2020). Among latent diffusion models (Rombach et al., 2022) we cover LSUN Bedroom (256256) (Yu et al., 2015) and the class-conditional ImageNet (256256) (Russakovsky et al., 2015). Additionally, we assess the Stable Diffusion (Rombach et al., 2022) model on the MSCOCO (512512) (Lin et al., 2015) text-to-image dataset. We use Karras et al. (2022) and Rombach et al. (2022) pretrained models for pixel and latent space experiments respectively. We choose distance (Equation 7) in distillation loss to be LPIPS (Zhang et al., 2018) in pixel-space and L1 in latent-space experiments. We initialize timesteps using time-uniform schedule and utilize the DPM-Solver++(3M) (Lu et al., 2022b) coefficients as the guiding theoretical parameters. For pixel-space models we use pretrained R3GAN discriminator. For latent experiments we adapt the same discriminator architecture, but train it from scratch. We calculate FID (Heusel et al., 2017) using 50000 samples, unless stated otherwise. The additional training details can be found in Appendix D. 4.1 MAIN RESULTS In Table 2 we illustrate that the proposed methods, GS and GAS, systematically enhance image sampling quality across different solvers, especially in low NFE setups. As an example, the S4S Alt (Frankel et al., 2025) algorithm reports FID score of 10.63 with NFE=4 on the FFHQ dataset, whereas GAS achieves significantly better FID score of 7.86 under the same conditions. Our approach outperforms all previously proposed methods across all evaluated datasets. Specifically, GAS achieves FID score of 4.48 with NFE=4 on the AFHQv2 dataset and 3.79 on the FFHQ dataset using NFE=6. Additionally, we achieve the FID score of 5.38 on the conditional ImageNet dataset with NFE=4, 4.60 on the LSUN Bedrooms dataset with NFE=5, and 14.71 on the MS-COCO dataset with NFE = 4. 4.2 ABLATION STUDY Coefficients parametrization First, we demonstrate significant impact of solver parameterization on training efficiency. Specifically, we show the difference between our parameterization, that represents coefficients as sum of fixed theoretical guidance and explicitly trained additive corrections, and the parameterization from another high-quality method S4S (Frankel et al., 2025). For fair comparison, we implemented LMS + PC S4S solver type removing constraint on the solver order. This guarantees Generalized Solver and S4S to have the same number of trainable parameters. 7 Preprint. Table 2: We evaluate FID score comparison of the proposed GS and GAS methods against existing solvers like UniPC and iPNDM, and alongside training-based approaches such as GITS, DMN, LD3, and S4S. We report the FIDs of the teacher models as those utilized during our training process. The baseline scores were taken from the corresponding papers, unless otherwise noted. report utilizing teacher model having significantly different FID score, thus it cannot be fairly compared to other methods. (a) Pixel-space datasets include CIFAR10 (3232), AFHQv2 (6464), and FFHQ (6464) (b) Latent diffusion models are tested on the LSUNBedroom and ImageNet datasets (256256). NFE=4 NFE=6 NFE=8 NFE= Method NFE=4 NFE=5 NFE=6 NFE=7 Method CIFAR10 Solvers DPM++ (3M) UniPC (3M) iPNDM (3M) 46.59 43.92 35.04 12.16 13.12 11.80 Solver optimization methods UniPC [GITS] UniPC [DMN] iPNDM [GITS] iPNDM [DMN] Best LD3 S4S Alt GS (Ours) GAS (Ours) 25.32 26.35 15.63 28.09 9.31 6.35 4.41 4.05 11.19 8.09 6.82 9.24 3.35 2.67 2.55 2. Teacher FFHQ Solvers 4.62 4.41 5.67 5.67 5.90 4.29 7.68 2.81 2.39 2.25 2.24 2. DPM++ (3M) UniPC (3M) iPNDM (3M) 46.14 53.25 36.54 14.01 11.24 16.44 6.18 5.59 8.11 Solver optimization methods UniPC [GITS] UniPC [DMN] iPNDM [GITS] iPNDM [DMN] Best LD3 S4S Alt GS (Ours) GAS (Ours) 21.38 25.82 18.05 31.30 17.96 10.63 10.70 7.86 Teacher AFHQv2 Solvers 12.21 9.47 9.38 12.12 5.97 4.62 4.49 3.79 7.84 6.85 5.72 11.00 3.50 3.15 2.96 2. 2.60 DPM++ (3M) UniPC (3M) iPNDM (3M) 27.82 33.78 23.20 10.72 8.27 9.55 4.28 4.60 4.49 Solver optimization methods UniPC [GITS] UniPC [DMN] iPNDM [GITS] iPNDM [DMN] Best LD3 S4S Alt GS (Ours) GAS (Ours) Teacher 12.20 30.32 12.89 33.15 9.96 6.52 5.92 4.48 7.26 14.46 6.10 16.01 3.63 2.70 2.87 2.66 3.86 6.85 4.03 10.12 2.63 2.29 2.33 2.29 2. 3.08 3.16 3.69 3.70 2.45 2.78 3.31 2.38 2.18 2.18 2.17 4.18 3.90 5.39 4.46 3.54 3.96 5.24 3.25 2.91 2.67 2.66 3.19 3.81 3.19 2.88 2.94 3.26 3.22 2.27 2.18 2.25 2. LSUN-Bedroom-256 (latent space) Solvers DPM++ (3M) UniPC (3M) iPNDM (3M) 48.82 39.78 11.93 18.64 13.88 6.38 8.50 6.57 5. Solver optimization methods UniPC [GITS] UniPC [DMN] iPNDM [GITS] iPNDM [DMN] Best LD3 S4S Alt GS (Ours) GAS (Ours) Teacher 70.93 29.22 76.86 11.82 8.48 20.89 9.83 6.68 47.37 8.21 59.17 6.15 5.93 13.03 5.32 4.60 22.33 4.40 28.09 4.71 4.52 10.49 3.77 3. 3.06 Imagenet-256 (latent space) Solvers DPM++ (3M) UniPC (3M) iPNDM (3M) 26.07 20.01 13.86 11.91 8.51 7. 7.51 5.92 6.03 Solver optimization methods UniPC [GITS] UniPC [DMN] iPNDM [GITS] iPNDM [DMN] Best LD3 S4S Alt GS (Ours) GAS (Ours) Teacher 54.88 16.72 56.00 10.15 9.19 5.13 7.87 5.38 34.91 7.96 43.56 7.33 5.03 4.30 4.93 4. 14.62 7.54 19.33 7.25 4.46 4.09 4.30 4.32 4.10 5.16 4.56 4.39 17.27 4.55 19.54 5.16 4.16 10.03 3.34 3.36 5.95 5.20 5.35 9.04 7.81 10.33 7.40 4.32 4.06 4.17 4. (c) Training dataset for SD consists of 1000 MSCOCO samples, while FID is computed across 30,000 prompts to generate images with spatial resolution of 512512. Method NFE=4 NFE=5 NFE=6 NFE=7 MS-COCO (Stable Diffusion v1.5) iPNDM (2M) iPNDM [GITS] Best LD3 S4S GS (Ours) GAS (Ours) Teacher 17.76 18.05 17.32 16.05 14.94 14.71 14.10 14.41 14.11 13.07 13.26 11.97 11.91 12.08 13.86 12.10 12.40 11.17 11.71 11.73 11. 13.76 11.80 11.83 10.83 11.32 11.36 11.48 In Table 3 we demonstrate our parameterizations superior performance on different datasets and NFEs. Our results are consistent with the training issue reported in (Frankel et al., 2025). Figure 3 represents the dynamics of LPIPS loss on the evaluation dataset in different training iterations of the experiment. Our parametrization shows more efficient training process, faster convergence and more stable training behavior. 8 Preprint. Table 3: We compare our parametrization with S4S variant on CIFAR and FFHQ datasets in terms of FID and LPIPS scores. Both setups use batch size of 24, while training dataset consists of 49k samples. Teacher dataset has FID score of 2.03 and 2.60 for CIFAR10 and FFHQ datasets respectively. NFE=4 NFE=6 NFE=8 NFE=10 FID LPIPS FID LPIPS FID LPIPS FID LPIPS CIFAR10 S4S Our 31.44 4.39 0.273 0.116 2.93 2.51 0.073 0.046 2.87 2.21 0.072 0. 2.26 2.15 0.027 0.010 FFHQ S4S Our 24.24 10.79 0.175 0. 11.08 4.40 0.117 0.046 7.76 2.97 0.098 0.016 3.97 2.70 0.045 0. Figure 3: LPIPS evaluation loss for training iterations comparing S4S and our parametrization. Our method results in more stable training process. Adversarial training Addition of the adversarial training is crucial part of our contribution, because it significantly improves the image generation quality as seen in Tables 2a, 2b. It is crucial for low NFE setups because teacher image can be too difficult for the student to replicate, therefore smaller values of the regression loss (LPIPS or L1 for pixel and latent models respectively) do not always correlate with smaller FID scores (as can be seen in Table 4) and occasionally result in visible artifacts. Examples of such behavior are presented in Figure 4. Adding adversarial loss makes the students generation closer to teachers distribution and thus removes appearing artifacts and makes generation more realistic, in spite of occasionally resulting in bigger LPIPS or L1 losses. Table 4: Results of 10k training iterations calculated on 1000 validation samples. FID Ldistill FFHQ GS GAS 10.70 7.86 0.116 0.127 LSUN GS GAS 9.64 7. 0.172 0.174 Figure 4: Incorporating an adversarial loss into the training process enhances generation quality reducing occurring image artifacts in low NFEs regimes. In this setup, the teacher model uses UniPC (3M) solver with NFE=10, while the student models operate with reduced NFE=4. 4.3 METHOD EFFICIENCY Dataset size We next show that GAS is efficient in terms of dataset size and training time. To this end, we measure methods performance on the \"full\" dataset scenario with 49000 samples and find the smaller dataset size that demonstrates equivalent results. First, we observe that the dataset size of 1400 is enough for training GS without adversarial loss. However, the solvers optimization problem becomes more challenging in low-NFE scenarios with adversarial loss. Here, we expand the dataset from 1400 samples to 5000 and obtain results indistinguishable from the full-dataset scenario in all datasets and settings. Additional information provided in Appendix C.1. Performance Without adversarial training, GS converges within 1-2.5 hours depending on the dataset, which is comparable to the most relevant baselines LD3 and S4S. In case of GAS, training time increases to 2-9 hours, which is larger, but still requires similar order. We refer the reader to the Appendix C.2 for the exact comparison of metrics depending on training time and Appendix C.3 for peak-memory usage in the backward pass. Preprint."
        },
        {
            "title": "5 DISCUSSION",
            "content": "In this paper, we propose Generalized Adversarial Solver, the novel parameterization and training algorithm for automatic gradient-based solver optimization. The main novelty is additive theoretical guidance of solver coefficients and combination of distillation loss with adversarial training. We establish that the introduced Generalized Solver parameterization significantly accelerates training compared to the existing parameterizations. We show that adding the adversarial loss significantly boosts methods performance and allows to tackle the image artifacts present in simple solver distillation. We extensively compare our method with other solver/timestep training approaches and demonstrate its superior performance on 6 datasets, ranging from 32 32 pixel-space CIFAR10 to 256 256 latent-space ImageNet and 512 512 MS-COCO with Stable Diffusion. Limitations Our method relies on performing backpropagation through the whole solver inference, which may face scalability issues when applied to larger image sizes and bigger models. We explore the generalizability of our method between different datasets in Section B.2. However, potential concern remains as to whether GS/GAS requires separate training for each preferred inference NFEs. We leave the development of lightweight modifications to our method for future work."
        },
        {
            "title": "REPRODUCIBILITY STATEMENT",
            "content": "To ensure the clarity and reproducibility of our work, we provide excessive description of all parts of our method. Appendix provides the pseudocode of our algorithm, exactly matching the way it appears in our implementation; configurations and hyperparameters of all \"teacher\" generations and \"student\" training processes, including batch sizes, optimizer choice and other fine-grained details; and expressions for commonly used timestep schedules mentioned in the paper. Furthermore, our experiments are built upon publicly available datasets (e.g., CIFAR10, FFHQ) and pre-trained model checkpoints to ensure our experimental setups are accessible and verifiable."
        },
        {
            "title": "REFERENCES",
            "content": "Martin Arjovsky, Soumith Chintala, and Léon Bottou. Wasserstein generative adversarial networks. In International conference on machine learning, pp. 214223. PMLR, 2017. Hicham Badri and Appu Shaji. Half-quadratic quantization of large machine learning models, November 2023. URL https://mobiusml.github.io/hqq_blog/. Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. 2024. URL https://openai. com/research/video-generation-models-as-world-simulators, 3:1, 2024. Thibault Castells, Hyoung-Kyu Song, Bo-Kyeong Kim, and Shinkook Choi. Ld-pruner: Efficient pruning of latent diffusion models using task-agnostic insights, 2024. URL https://arxiv. org/abs/2404.11936. Defang Chen, Zhenyu Zhou, Can Wang, Chunhua Shen, and Siwei Lyu. On the trajectory regularity of ode-based diffusion sampling. arXiv preprint arXiv:2405.11326, 2024a. Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 73107320, 2024b. Yunjey Choi, Youngjung Uh, Jaejun Yoo, and Jung-Woo Ha. Stargan v2: Diverse image synthesis for multiple domains. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 81888197, 2020. Preprint. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. arXiv preprint arXiv:2403.03206, 2024. Gongfan Fang, Xinyin Ma, and Xinchao Wang. Structural pruning for diffusion models, 2023. URL https://arxiv.org/abs/2305.10924. Eric Frankel, Sitan Chen, Jerry Li, Pang Wei Koh, Lillian Ratliff, and Sewoong Oh. S4s: Solving for diffusion model solver. arXiv preprint arXiv:2502.17423, 2025. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. Jiatao Gu, Shuangfei Zhai, Yizhe Zhang, Lingjie Liu, and Joshua Susskind. Boot: Data-free distillation of denoising diffusion models with bootstrapping. In ICML 2023 Workshop on Structured Probabilistic Inference {&} Generative Modeling, 2023. Shuyang Gu, Dong Chen, Jianmin Bao, Fang Wen, Bo Zhang, Dongdong Chen, Lu Yuan, and Baining Guo. Vector quantized diffusion model for text-to-image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1069610706, 2022. Ishaan Gulrajani, Faruk Ahmed, Martin Arjovsky, Vincent Dumoulin, and Aaron Courville. Improved training of wasserstein gans. Advances in neural information processing systems, 30, 2017. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Marlis Hochbruck and Alexander Ostermann. Exponential integrators. Acta Numerica, 19:209286, 2010. Nick Huang, Aaron Gokaslan, Volodymyr Kuleshov, and James Tompkin. The gan is dead; long live the gan! modern gan baseline. Advances in Neural Information Processing Systems, 37: 4417744215, 2024. Phillip Isola, Jun-Yan Zhu, Tinghui Zhou, and Alexei Efros. Image-to-image translation with conditional adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 11251134, 2017. Alexia Jolicoeur-Martineau. The relativistic discriminator: key element missing from standard gan. arXiv preprint arXiv:1807.00734, 2018. Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 44014410, 2019. Tero Karras, Miika Aittala, Timo Aila, and Samuli Laine. Elucidating the design space of diffusionbased generative models. Advances in Neural Information Processing Systems, 35:2656526577, 2022. 11 Preprint. Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. Sanghwan Kim, Hao Tang, and Fisher Yu. Distilling ode solvers of diffusion models into smaller steps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 94109419, 2024. Alex Krizhevsky and Geoffrey Hinton. Learning multiple layers of features from tiny images. Technical report, University of Toronto, 2009. Christian Ledig, Lucas Theis, Ferenc Huszár, Jose Caballero, Andrew Cunningham, Alejandro Acosta, Andrew Aitken, Alykhan Tejani, Johannes Totz, Zehan Wang, et al. Photo-realistic single image super-resolution using generative adversarial network. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 46814690, 2017. Jae Hyun Lim and Jong Chul Ye. Geometric gan. arXiv preprint arXiv:1705.02894, 2017. Tsung-Yi Lin, Michael Maire, Serge Belongie, Lubomir Bourdev, Ross Girshick, James Hays, Pietro Perona, Deva Ramanan, C. Lawrence Zitnick, and Piotr Dollár. Microsoft coco: Common objects in context, 2015. URL https://arxiv.org/abs/1405.0312. Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao. Pseudo numerical methods for diffusion models on manifolds. In International Conference on Learning Representations, 2022a. URL https: //openreview.net/forum?id=PlKWVd2yBkY. Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022b. Xingchao Liu, Xiwen Zhang, Jianzhu Ma, Jian Peng, et al. Instaflow: One step is enough for high-quality diffusion-based text-to-image generation. In The Twelfth International Conference on Learning Representations, 2023. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. Advances in Neural Information Processing Systems, 35:57755787, 2022a. Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022b. Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diffinstruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36, 2024. Xinyin Ma, Gongfan Fang, and Xinchao Wang. Deepcache: Accelerating diffusion models for free. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1576215772, 2024. Xudong Mao, Qing Li, Haoran Xie, Raymond YK Lau, Zhen Wang, and Stephen Paul Smolley. Least squares generative adversarial networks. In Proceedings of the IEEE international conference on computer vision, pp. 27942802, 2017. Thuan Hoang Nguyen and Anh Tran. Swiftbrush: One-step text-to-image diffusion model with variational score distillation. arXiv preprint arXiv:2312.05239, 2023. Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and Mark Chen. Glide: Towards photorealistic image generation and editing with text-guided diffusion models. arXiv preprint arXiv:2112.10741, 2021. Sebastian Nowozin, Botond Cseke, and Ryota Tomioka. f-gan: Training generative neural samplers using variational divergence minimization. Advances in neural information processing systems, 29, 2016. 12 Preprint. Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. Amirmojtaba Sabour, Sanja Fidler, and Karsten Kreis. Align your steps: Optimizing sampling schedules in diffusion models. arXiv preprint arXiv:2404.14507, 2024. Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. Advances in neural information processing systems, 35:3647936494, 2022. Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. Tim Salimans, Thomas Mensink, Jonathan Heek, and Emiel Hoogeboom. Multistep distillation of diffusion models via moment matching. arXiv preprint arXiv:2406.04103, 2024. Axel Sauer, Dominik Lorenz, Andreas Blattmann, and Robin Rombach. Adversarial diffusion distillation. arXiv preprint arXiv:2311.17042, 2023. Axel Sauer, Frederic Boesel, Tim Dockhorn, Andreas Blattmann, Patrick Esser, and Robin Rombach. Fast high-resolution image synthesis with latent adversarial diffusion distillation. In SIGGRAPH Asia 2024 Conference Papers, pp. 111, 2024. Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pp. 22562265. pmlr, 2015. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020a. Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020b. Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. Ruoyu Sun, Tiantian Fang, and Alexander Schwing. Towards better global loss landscape of gans. Advances in Neural Information Processing Systems, 33:1018610198, 2020. Vinh Tong, Trung-Dung Hoang, Anji Liu, Guy Van den Broeck, and Mathias Niepert. Learning to discretize denoising diffusion odes. arXiv preprint arXiv:2405.15506, 2024. Pascal Vincent. connection between score matching and denoising autoencoders. Neural Computation, 23:16611674, 2011. URL https://api.semanticscholar.org/CorpusID: 5560643. Fu-Yun Wang, Ling Yang, Zhaoyang Huang, Mengdi Wang, and Hongsheng Li. Rectified diffusion: Straightness is not your need in rectified flow. arXiv preprint arXiv:2410.07303, 2024. Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36:84068441, 2023. 13 Preprint. Daniel Watson, William Chan, Jonathan Ho, and Mohammad Norouzi. Learning fast samplers for diffusion models by differentiating through sample quality. In International Conference on Learning Representations, 2021. Felix Wimbauer, Bichen Wu, Edgar Schoenfeld, Xiaoliang Dai, Ji Hou, Zijian He, Artsiom Sanakoyeu, Peizhao Zhang, Sam Tsai, Jonas Kohler, et al. Cache me if you can: Accelerating diffusion models through block caching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 62116220, 2024. Zhisheng Xiao, Karsten Kreis, and Arash Vahdat. Tackling the generative learning trilemma with denoising diffusion gans. arXiv preprint arXiv:2112.07804, 2021. Shuchen Xue, Zhaoqiang Liu, Fei Chen, Shifeng Zhang, Tianyang Hu, Enze Xie, and Zhenguo Li. Accelerating diffusion sampling with optimized time steps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82928301, 2024. Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. arXiv preprint arXiv:2311.18828, 2023. Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and William Freeman. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024. Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao. Lsun: Construction of large-scale image dataset using deep learning with humans in the loop. arXiv preprint arXiv:1506.03365, 2015. Qinsheng Zhang and Yongxin Chen. Fast sampling of diffusion models with exponential integrator. arXiv preprint arXiv:2204.13902, 2022. Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 586595, 2018. Wenliang Zhao, Lujia Bai, Yongming Rao, Jie Zhou, and Jiwen Lu. Unipc: unified predictorcorrector framework for fast sampling of diffusion models. Advances in Neural Information Processing Systems, 36, 2024. Kaiwen Zheng, Cheng Lu, Jianfei Chen, and Jun Zhu. Dpm-solver-v3: Improved diffusion ode solver with empirical model statistics. Advances in Neural Information Processing Systems, 36: 5550255542, 2023. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, march 2024. URL https://github. com/hpcaitech/Open-Sora, 1(3):4, 2024. Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024a. Zhenyu Zhou, Defang Chen, Can Wang, and Chun Chen. Fast ode-based sampling for diffusion models in around 5 steps. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 77777786, 2024b. 14 Preprint."
        },
        {
            "title": "A RELATED WORK",
            "content": "Among many inference-time acceleration algorithms, solver-based methods treat diffusion models as ODEs with (partially) black-box velocity function. Specifically, PNDM (Liu et al., 2022a) and iPNDM (Zhang & Chen, 2022) apply the linear multistep method to the corresponding PFODE. DPM-Solver (Lu et al., 2022a), DEIS (Zhang & Chen, 2022) use the variation of constants (Equation 4) and approximate the underlying integral. DPM-Solver++ (Lu et al., 2022b) extends this idea to the multi-step version, and UniPC (Zhao et al., 2024) modifies it with the predictor-corrector framework. Besides the solver distillation loss, introduced for optimizing the timesteps in LD3 (Tong et al., 2024) and used for optimizing both timesteps and solver coefficients in S4S (Frankel et al., 2025), many automatic solver selection methods were proposed. DDSS (Watson et al., 2021) directly optimizes generation quality of the solver. AYS (Sabour et al., 2024) optimizes timesteps to minimize the KL divergence between the backward SDE and the discretization. GITS (Chen et al., 2024a) choose the timesteps by utilizing trajectory structure of the PF-ODE and DMN (Xue et al., 2024) allows for the fast model-free choice of parameters via optimizing an upper-bound on the solution error. Some approaches manipulate diffusion-specific properties and utilize redundancies in their computations. Namely, DeepCache (Ma et al., 2024) and CacheMe (Wimbauer et al., 2024) propose to perform block or layer caching and reuse activations from the previous timesteps. The other directions of acceleration include quantization (Gu et al., 2022; Badri & Shaji, 2023) and pruning (Fang et al., 2023; Castells et al., 2024). In contrast, diffusion distillation techniques aim at compressing pre-defined diffusion model by training few-step student. Several methods learn to mimic solution of the PF-ODE. This includes optimizing the regression loss between the outputs (Salimans & Ho, 2022) or learning the integrator between arbitrary timesteps (Gu et al., 2023; Song et al., 2023; Kim et al., 2023).The other use diffusion models as training signal that assesses likelihood of the generated images. It is commonly formalized as optimizing the Integrated KL divergence (Luo et al., 2024; Yin et al., 2023; 2024; Nguyen & Tran, 2023) by training an additional \"fake\" diffusion model on the generators output distribution. Other methods consider matching scores (Zhou et al., 2024a) or moments (Salimans et al., 2024) of the corresponding distributions. Many distillation methods enhance student generation quality by adding the adversarial training (Kim et al., 2023; Yin et al., 2024), including discriminator loss on detector (Sauer et al., 2023) or teacher features (Sauer et al., 2024)."
        },
        {
            "title": "B ADDITIONAL EXPERIMENTS",
            "content": "B.1 FID PROGRESSION DURING TRAINING To better understand the training process, we visualize the dynamics of the FID score during the training process. When comparing the GS and GAS FID scores for FFHQ, as visualized in Figure 5a, we observe that incorporating the adversarial objective requires more training iterations for our method to converge. However, it is more important that, as previously reported in Table 2a, it achieves significantly lower FID score, allowing for better trade-off between generation quality and slight increase in training time. Figure 5b demonstrates that although GAS achieves excellent FID scores after 30k iterations, it could potentially yield even better results with further training. This is suggested by the continuing decrease in the FID score for NFEs of 4 and 5 with each additional training iteration. Scenarios involving larger number of NFEs for model inference do not display this pattern, since they comprise bigger students capacity and lead to easier optimization task and earlier convergence. B.2 GENERALIZATION ACROSS DATASETS Regarding generalization across datasets with significantly different dimensionalities (e.g., CIFAR vs. COCO), the optimal schedule for smaller resolution may not be optimal for higher resolutions due to simpler denoising tasks at equivalent noise levels (larger images have greater correlation among nearby pixels). To further demonstrate the methods generalization results, we tested solver 15 Preprint. (a) FID values during training for the FFHQ dataset, using 4 NFE with the GS and GAS. We evaluate FID every 500 training iterations, computing it based on 5000 generated samples. (b) GAS FID training dynamics for latent space datasets for several NFE scenarios. We generate 50k images for each of 5000, 10k, 20k and 30k iterations checkpoints, and evaluate FID scores based on those datasets. Figure 5: FID score for several checkpoints during training of GS and GAS for both pixel (FFHQ) and latent space (LSUN, ImageNet) datasets. transfer between closely related diffusion models (FFHQ and AFHQv2), demonstrating practical generalizability. We thus illustrate its generalization in Table 5. Table 5: We evaluate FID score comparison of GS and GAS trained on dataset and applied on another against DPM-Solver++, LD3 and S4S. We use the GS and GAS checkpoints as in Table 2a. (a) Solvers GS, GAS, trained on FFHQ and applied on AFHQv2 (denoted as GS and GAS) consistently outperform baseline methods. (b) Solvers GS, GAS, trained on AFHQv2 and applied on FFHQ (denoted as GS and GAS) consistently outperform baseline methods. Method NFE=4 NFE=6 NFE=8 NFE=10 Method NFE=4 NFE=6 NFE=8 NFE=10 DPM-Solver++ Best LD3 S4S Alt GS (Ours) GAS (Ours) GS (Ours) GAS (Ours) 27.82 9.96 6.52 5.92 4.48 6.54 5.15 10.72 3.63 2. 2.87 2.66 3.01 2.81 4.28 2.63 2.29 2.33 2.29 2.41 2.44 3.19 2.27 2.18 2.25 2.31 2.29 2.32 DPM-Solver++ Best LD3 S4S Alt GS (Ours) GAS (Ours) GS (Ours) GAS (Ours) 46.14 17.96 10.63 10.70 7.86 16.01 9.39 14.01 5.97 4.62 4.49 3.79 5.91 4.21 6.18 3.50 3. 2.96 2.87 3.27 2.92 4.18 3.25 2.91 2.67 2.66 2.70 2.72 B.3 ADVERSARIAL LOSS WEIGHT One of the few hyperparameters of GAS is the GAN-weight. Starting from the resolution of 64 64, the weight of the adversarial loss was fixed to 1.0 for all datasets. Figure 6 demonstrates that GAS is insensitive to the GAN-weight selection and achieves similar FID with different weights. This shows that our method achieves strong results without the need for hyperparameter tuning."
        },
        {
            "title": "C EFFICIENCY OF THE METHOD",
            "content": "C.1 TRAINING DATASET SIZE We conduct experiments to assess the efficiency of the proposed methods with respect to the size of the training dataset. We examine several variations of sizes: 49000 as baseline, 5000 and 1400 as the more lightweight alternatives. For GS, we observe that taking 1400 images and performing 10000 training iterations is sufficient for our method to converge, regardless of NFE. We note that it reaches equivalent or better FID scores compared to bigger training dataset (see Table 6a). The same pattern occurs with GAS on CIFAR. The dataset of 1400 images is optimal for its training. However, starting from the higher-dimensional FFHQ dataset, we observe the typical challenges of adversarial training. As the discriminator used in GAS is trained simultaneously with the other parameters of the solver, it tends to overfit and demands larger dataset size to alleviate this problem. 16 Preprint. Figure 6: FID values for FFHQ dataset with 4 NFEs for different adversarial loss weights. The metric remains stable even at large weight values. Setting Ladv = 0 for GAS results in absence of adversarial training, thus is GS setting. Adversarial training has demonstrated its efficiency, especially in scenarios with smaller inference steps. We thus illustrate its performance in Table 6b on NFE = 4 and NFE = 6. It shows that the training dataset size of 5000 is sufficient for matching performance of the model trained on 49000. Table 6: Comparison of different dataset sizes with several NFEs, where indicates the number of samples in the training dataset. In Table 6a the FID score is calculated after 10k and 20k training iterations to show the early convergence of the GS method. Table 6b presents the results of GAS evaluation after 10k iterations of training. (a) Generalized solver (b) Generalized Adversarial solver NFE=4 NFE= NFE=4 NFE=6 CIFAR10 FFHQ 1400 1400 49000 10k 4.35 4.39 20k 4.35 4.39 10.70 10. 10.72 10.82 10k 2.14 2.15 2.71 2.70 20k 2.15 2. 2.71 2.71 CIFAR10 FFHQ 1400 49000 1400 5000 49000 3.98 3. 9.44 7.83 7.93 2.44 2.48 4.48 3.79 3.76 C.2 TRAINING TIME We further investigate GS/GAS training dynamics by estimating their convergence time and comparing their computational efficiency with other methods. In Table 7a we demonstrate the training time of Progressive Distillation (PD, (Salimans & Ho, 2022)) and Consistency Distillation (CD, (Song et al., 2023)). Those methods focus on training new generator model that can sample images in few-NFE manner. Both require days of training time and are computationally demanding. We also compare our methods with several approaches that involve training certain parameters of solvers. In pixel space GS requires less than an hour of training time on CIFAR10, which is comparable to LD3, S4S and S4S-Alt. Notably, it achieves FID of 2.44 with NFE = 6, while S4S-Alt results in FID score of 2.52 with NFE = 7 and equivalent training time. Adversarial loss extends the training time to up to 2 hours, however, as we report in Table 2a, it achieves superior results in terms of FID score. In the latent diffusion setting, we compare our method with LD3, which reports convergence within an hour of training time. We observe that GS and GAS require up to 3 hours; however, this is still within the same order (for more details, see Table 7b). In Table 8 we also provide more details about training time of our methods for both pixel and latent space models. 17 Preprint. Table 7: Comparison of different training-based methods in terms of computational effectiveness across both pixel and latent space selected dataset. (a) CIFAR10 (b) Imagenet-256 Time GPU Type Method NFEs Method NFE CD PD S4S-Alt S4S LD3 GS GAS 2 8 7 10 6 10 4 FID 2.93 2.47 2.52 2.18 2.32 2.44 2. 8 days 8 days < 1 hour < 1 hour < 1 hour < 1 hour < 1 hour 3.98 < 2 hours A100 TPU A100 A100 A100 H100 H100 H100 LD3 GS GAS 4 5 6 7 4 5 6 7 4 FID 9.19 5.03 4.46 4.32 7.97 4.94 4.29 4. 6.06 Time GPU Type < 1 hour A100 < 1.5 hours < 2 hours < 2 hours < 2.5 hours H100 < 3 hours H100 Table 8: Approximate training time (in minutes) for 10k iterations scenarios for GS and GAS in both pixel and latent space. For MS-COCO we use 1k iterations scenarios. All the numbers reported are computed using one H100 GPU. (a) Pixel space models (b) Latent space models NFE=4 NFE=6 NFE=8 NFE=10 NFE=4 NFE=5 NFE=6 NFE=7 GS GAS CIFAR FFHQ AFHQv2 CIFAR FFHQ AFHQv 30m 40m 40m 85m 160m 160m 40m 60m 60m 100m 185m 185m 50m 80m 80m 115m 210m 210m 60m 95m 95m 130m 240m 240m GS GAS 35m LSUN ImageNet 75m MS-COCO 50m LSUN ImageNet MS-COCO 60m 125m 185m 45m 95m 60m 140m 210m 75m 50m 115m 70m 150m 245m 90m 60m 135m 80m 165m 270m 105m C.3 MEMORY USAGE We are investigating the peak-memory GS/GAS required for training iteration depending on NFE. In Table 9 we demonstrate the peak-memory usage for GS/GAS compared to LD3. When measuring the memory, we used the config we further report in Appendix D. GS requires the same amount of peak-memory allocated as LD3. Incorporation of the discriminator loss to the training process of GAS only requires additional less than 4 gigabyte of memory usage, which is minor overhead, especially considering its efficiency in terms of the final generation quality. This overhead is limited to training at inference time, GAS and GS sample at the same speed. Additionally, storing prior states does not incur additional overhead for peak-memory usage. Table 9: Peak-memory usage (in gigabyte) for training iteration for GS and GAS in CIFAR10 and Imagenet-256. We use LD3 in our implementation. The official implementation uses LPIPS, rather than L1 distance in latent space as we do, which leads to the use of VAE decoder at each step and incurs additional memory usage. (a) CIFAR10 (b) Imagenet-256 NFE=4 NFE=6 NFE=8 NFE=10 NFE=4 NFE=5 NFE=6 NFE=7 GS GAS LD3 17GB 19GB 17GB 23GB 25GB 23GB 28GB 30GB 28GB 34GB 35GB 34GB GS GAS LD3 37GB 41GB 37GB 45GB 49GB 45GB 54GB 57GB 54GB 62GB 66GB 62GB C.4 INFERENCE TIME Inference process of our method requires additional operations performed with all prior states. However, they are incomparably computationally simpler than one step of diffusion model (function Preprint. evaluation). Thus, the wall-clock time of inference for GS is comparable to the solvers baselines, which we show in Table 10. Table 10: Inference time in minutes for ImageNet dataset. We obtain the comparison by generating 1,024 images with batch 64 utilizing single H100 GPU. GAS differs from GS only in the training process; their inference times are identical."
        },
        {
            "title": "Method",
            "content": "NFE=4 NFE=5 NFE=6 NFE=7 UniPC(3M) GS (Ours) 0.36m 0.36m 0.46m 0.45m 0.55m 0.55m 0.64m 0.64m This pattern does not depend on the model and dataset choice; therefore, our method does not introduce any inference time overhead on both pixel, latent or text-to-image diffusion models."
        },
        {
            "title": "D EXPERIMENTAL DETAILS",
            "content": "D.1 BASELINE DISCRETIZATION HEURISTICS In this section, we provide the reader with the common timestep schedules, used in the paper. Polynomial discretization (time-quadratic, time-uniform) defines the timestep schedule via polynomial function of the uniform sequence. Specifically, it defines ti = (cid:19)ρ (cid:18) (T teps) + teps, = 0, 1, . . . , N. (23) Here ρ is often set to 1 or 2 (Song et al., 2020b; Ho et al., 2020; Song et al., 2020a) which corresponds to time quadratic and time uniform discretization. Time logSNR schedule builds on top of the signal-to-noise ratio α2 the transformation λt = log(σt/αt) and defines . Specifically, log-SNR uses /σ λ(ti) = (λT λeps) + λeps, = 0, 1, . . . , N. (24) This schedule offers high generation quality with different versions of the DPM-Solver (Lu et al., 2022a;b; Zheng et al., 2023). GITS schedule provides an optimized sequence of noise levels for diffusion models, targeting very low NFEs. Originally proposed in Chen et al. (2024a) for ODE-based diffusion processes with trajectory regularity constraints. We use optimized timesteps in Stable Diffusion experiments from Tong et al. (2024). Concretely, the timestep schedules are: NFE = 4 : NFE = 5 : NFE = 6 : NFE = 7 : NFE = 8 : (cid:2)1, 0.6837, 0.3673, 0.1176, 0.001(cid:3); (cid:2)1, 0.7669, 0.4839, 0.2341, 0.0676, 0.001(cid:3); (cid:2)1, 0.7836, 0.5504, 0.3340, 0.1508, 0.0343, 0.001(cid:3); (cid:2)1, 0.8502, 0.6004, 0.4006, 0.2175, 0.0843, 0.0176, 0.001(cid:3); (cid:2)1, 0.8502, 0.6504, 0.4672, 0.3007, 0.1675, 0.0676, 0.0176, 0.001(cid:3). D.2 TEACHER SOLVER Data generation For fair comparison, we follow Tong et al. (2024) to generate the teacher dataset. We choose UniPC with the parameters used in LD3. We utilize class condition of the ImageNet-256 teacher and generate the corresponding dataset with the classifier-free guidance scale of 2.0 and generate 50 images per each of the 1000 classes. We report details in Table 11. Stable Diffusion details Regarding text-to-image generation with Stable Diffusion, we observe that output image distributions of low-NFE students (NFE = 3-5) differ significantly from those of 19 Preprint. Table 11: Detailed description of the UniPC solver parameters used for teacher dataset generation consisting of 50000 images for both pixel and latent space scenarios. CIFAR"
        },
        {
            "title": "FFHQ",
            "content": "AFHQv2 LSUN-Bedroom-256 Imagenet-256 Order NFE Time schedule B(h) teps FID 3 20 logSNR bh1 1e-4 3 20 3 20 logSNR logSNR bh1 1ebh1 1e-4 3 20 time-uniform bh2 1e-3 2.03 2.60 2.16 3. 3 10 time-quadratic bh2 1e-3 4.10 high-NFE teacher (e.g., NFE = 10). Since such students have very few trainable parameters, direct distillation can be inefficient. The same pattern was found in Tong et al. (2024). For such reason and fair comparison, we follow identical to the LD3 approach teacher generation protocol. We train student at NFE = with the teacher at NFE = + 1. This \"one-plus\" teacher minimizes the gap in noise dynamics and yields smoother, more reliable convergence. Moreover, in our experiments, we find that FID loses its correlation with perceived fidelity at high NFEs, so we treat improvements in that regime with particular caution. Recognizing this unreliability beyond NFE 8 reinforces our choice of simpler teachers as the most robust path to high-quality samples. Further details on teacher parameters are provided in Table 12. Table 12: Detailed solver parameter settings for teacher-generated dataset using 30000 MS-COCO prompts. Students NFE NFE= NFE=5 NFE=6 NFE=7 Teachers NFE 5 Solver Time schedule GITS IPNDM(2M) 6 IPNDM(2M) GITS 7 IPNDM(2M) GITS 8 IPNDM(2M) GITS FID 14.10 12.08 11. 11.48 D.3 SOLVER COEFFICIENTS PARAMETERIZATION The detailed description of the Generalized Solver step is provided in Algorithm 1. Specifically, when all parameters ϕ are set to zero, the GS reduces exactly to DPM-Solver++(3M) (Lu et al., 2022b). D.4 PRACTICAL IMPLEMENTATION DETAILS We define , H, and as the width, height, and number of channels of an image, respectively. Similarly, , , and represent the corresponding dimensions in the latent space for the Latent Diffusion model (Rombach et al., 2022). Optimizer and trainable parameters We update three primary parameter sets during training: θ defines the timestep schedule, ϕ defines the solver coefficients and ξ acts as correction to the timesteps that we evaluate the pre-trained model on. We use one optimizer for all parameter groups. We use time-uniform schedule for the initialization of parameters θ. We initialize ξ and ˆcj,n(ϕ), aj,n(ϕ) with zeros. We use the EMA version of the model parameters for evaluation and update the EMA weights after each training iteration. Evaluation We evaluate our models (Table 2a, 2b) using the FID score with 50 000 randomly generated samples. For ImageNet, we generate an equal number of samples for each class to ensure balanced FID evaluation. We use EMA weights for evaluations. We calculate FID using reference statistics and code from Karras et al. (2022). For MS-COCO (Table 2c) we obtain the FID score on 20 Preprint. Algorithm 1 Generalized solver (GS) with theoretical guidance from DPM-Solver++(3M). Denote hi = λtθ for = 1, . . . , . λtθ i1 n:n+1) + ˆan,n(ϕ)(cid:3) xn (cid:2)αtn+1 ϕ1 + ˆcn,n(ϕ)(cid:3) v(xn, tθ + ξn) 1: ψ1 ehn 1 2: xn+1 (cid:2)an,n(tθ 3: if = 1 then r0 hn1 4: hn D10 1 r0 xn+1 xn+1 (cid:2) αtn+1 ϕ1 (cid:2)v(xn, tθ 5: + ξn) v(xn1, tθ n1 + ξn1)(cid:3) + ˆcn1,n(ϕ)(cid:3) D10 2 9: , hn2 hn 6: 7: else if 2 then r0, r1 hn1 8: hn ψ2 ψ1 + 1 ψ3 ψ2 1 D10 1 r0 D11 1 r1 D1 D10 + r0 D2 1 xn+1 xn+1 + (cid:2)αtn+1 ϕ2 + ˆcn1,n(ϕ)(cid:3) D1 (cid:2)αtn+1 ϕ3 + ˆcn2,n(ϕ)(cid:3) D2 n1 + ξn1) v(xn2, tθ (cid:2)D10 D11 (cid:3) (cid:3) (cid:2)v(xn, tθ (cid:2)v(xn1, tθ + ξn) v(xn1, tθ n1 + ξn1)(cid:3) n2 + ξn2)(cid:3) (cid:2)D10 D11 r0+r1 r0+r1 2 10: 11: 12: 13: 14: 15: 16: end if 17: xn+1 xn+1 + max(n1,0) (cid:80) j= aj,n(ϕ) xj + max(n3,0) (cid:80) j=0 cj,n(ϕ) v(xj, tθ + ξj) 30 000 images using the same validation captions and FID reference statistics as in LD3 (Tong et al., 2024). D.4.1 PIXEL SPACE DIFFUSION ON CIFAR10, FFHQ, AND AFHQV Pre-trained diffusion model: EDM (Karras et al., 2022); Teacher: UniPC solver, NFE = 20, logSNR schedule; Discriminator R3GAN (Huang et al., 2024): Pre-trained CIFAR10 checkpoint for CIFAR10; Pre-trained FFHQ-64 checkpoint for both FFHQ and AFHQv2; Training in pixel space; Image resolution: = = 32, = 3 for CIFAR10; = = 64, = 3 for FFHQ and AFHQv2; Training/validation dataset size: CIFAR10: 1400/1000 for GS and GAS; FFHQ and AFHQv2: 1400/1000 for GS; 5000/1000 for GAS; Solver training: Ldistill is LPIPS; Ladv with weight = 0.1 for CIFAR10 and weight = 1.0 for FFHQ and AFHQv2; EMA decay = 0.999; Batch size = 24; Adam optimizer, lr = 0.001, betas = (0.9, 0.999), weight decay = 0.0; Gradients are clipped by the norm of 1.0; Discriminator training: 21 Preprint. Batch size = 24; Adam optimizer, lr = 0.00001, betas = (0.9, 0.999), weight decay = 0.0; λ1 and λ2 in Equation 11 are equal to 0.1; Training duration: 10k iterations for GS/GAS; D.4.2 LATENT SPACE DIFFUSION ON LSUN-BEDROOM AND IMAGENET Pre-trained diffusion model: LDM (Rombach et al., 2022); Teacher: UniPC solver for both LSUN-Bedrooms and ImageNet; NFE = 20 and time-uniform schedule for LSUN; NFE = 10 and time-quadratic schedule for ImageNet; Discriminator R3GAN (Huang et al., 2024): FFHQ-64 architecture with random initialization; Training in latent space; Image resolution: = = 256, = 3; = = 64, = 3; Guidance scale: 2.0 (for ImageNet); Training/validation dataset size: 1400/1000 for GS; 5000/1000 for GAS; Solver training: Ldistill is L1 in latent space; Ladv with weight = 1.0; EMA decay = 0.999; Batch size = 8; Adam optimizer, lr = 0.001, betas = (0.9, 0.999), weight decay = 0.0; Gradients are clipped by the norm of 1.0; Discriminator training: Batch size = 8; Adam optimizer, lr = 0.00001, betas = (0.9, 0.999), weight decay = 0.0; λ1 and λ2 in Equation 11 are equal to 0.1; Training duration: 30k iterations for GS/GAS; D.4.3 TEXT-TO-IMAGE GENERATION WITH STABLE DIFFUSION Pre-trained diffusion model: Stable Diffusion v1.5 (Rombach et al., 2022); Gradient checkpointing at every UNet inference; Teacher: NFE = + 1, where = student NFE; IPNDM(2M) solver with GITS; Discriminator R3GAN (Huang et al., 2024): FFHQ-64 architecture with random initialization; 22 Preprint. First convolution layer modified to accept 4-channel latent inputs; Training in latent space; Image resolution: = 512 512, = 3 = 64 64, = 4 Guidance scale: 7.5; Training/validation dataset size: 1400/128 for GS; 5000/128 for GAS; Solver training: Ldistill is L1 in latent space; Ladv with weight = 1.0; EMA decay = 0.999; Batch size = 4; Adam optimizer, lr = 0.001, betas = (0.9, 0.999), weight decay = 0.0; Gradients are clipped by the norm of 1.0; Discriminator training: Batch size = 4; Adam optimizer, lr = 0.00001, betas = (0.9, 0.999), weight decay = 0.0; λ1 and λ2 in Equation 11 are equal to 0.1; Training duration: 1k iterations for GS; 2k iterations for GAS;"
        },
        {
            "title": "E ADDITIONAL SAMPLES",
            "content": "To further demonstrate the methods competitive results, we provide the reader with the additional samples of GS and GAS, compared to the teacher and the baseline UniPC with the same NFE. For all models/datasets except Stable Diffusion, we choose samples corresponding to 6 random seeds (marked as \"random\") and 6 samples that are the most distinguishable between GS and GAS in terms of pixel-space L1 distance (marked as \"selected\"). We choose the selected sample seeds at NFE = 4 and report the corresponding samples for all NFEs. We report the samples for FFHQ (Figures 7, 8, 9, 10), AFHQv2 (Figures 11, 12, 13, 14), LSUN Bedroom (Figures 15, 16, 17, 18) and ImageNet (Figures 19, 20, 21, 22). Most random samples show only minor fine-grained differences between GS and GAS (which is still important and has positive effect on FID, as indicated in Table 2). At the same time, the selected samples fully demonstrate the potential effect of the adversarial loss on the image quality. Most GAS samples at NFE = 4 demonstrate superior image quality compare to GS, while being farther from teacher. This further complements the results demonstrated in Figure 4. At the same time, one could tell that the pictures enhanced by adversarial loss, differ depending on NFE: pictures from the same random seeds become significantly closer to the teacher starting from NFE = 6. This also indicates that the effect of the adversarial loss is the most prominent at low NFEs, where it is harder for the student to replicate teachers performance. Mode collapse It is also worth noting that incorporation of the adversarial loss to the training process does not lead to mode collapse common concern in such cases as we explicitly address this issue using the relativistic GAN loss from Huang et al. (2024). The random samples reported in Figures 724 show generation diversity, while low resulting FID values indicate both high quality of our images and the absence of mode collapse. 23 Preprint. Stable Diffusion For the Stable Diffusion experiments, we generate images from the 250 MSCOCO-val prompts with both the official LD3 implementation and our GAS method, initializing both with identical random latent noise. From these outputs, we select six images at random (marked \"random\") and six that best highlight the visual differences between GAS and LD3 (marked \"selected\"). Random prompts: woman sitting on bench and woman standing waiting for the bus. jumbo jet sits on the tarmac while another takes off An old green car parked on the side of the street. gas stove next to stainless steel kitchen sink and countertop. person walking through the rain with an umbrella. Selected prompts: man in wheelchair and another sitting on bench that is overlooking the water. fireplace with fire built in it. half eaten dessert cake sitting on cake plate. an airport with one plane flying away and the other sitting on the runway dirt bike rider doing stunt jump in the air The resulting comparisons are shown in Figures 23, 24. 24 Preprint. Figure 7: Comparison of GS and GAS with the teacher and UniPC on FFHQ with NFE = 4. Figure 8: Comparison of GS and GAS with the teacher and UniPC on FFHQ with NFE = 6. 25 Preprint. Figure 9: Comparison of GS and GAS with the teacher and UniPC on FFHQ with NFE = 8. Figure 10: Comparison of GS and GAS with the teacher and UniPC on FFHQ with NFE = 10. 26 Preprint. Figure 11: Comparison of GS and GAS with the teacher and UniPC on AFHQv2 with NFE = 4. Figure 12: Comparison of GS and GAS with the teacher and UniPC on AFHQv2 with NFE = 6. 27 Preprint. Figure 13: Comparison of GS and GAS with the teacher and UniPC on AFHQv2 with NFE = 8. Figure 14: Comparison of GS and GAS with the teacher and UniPC on AFHQv2 with NFE = 10. 28 Preprint. Figure 15: Comparison of GS and GAS with the teacher and UniPC on LSUN-Bedroom with NFE = 4. Figure 16: Comparison of GS and GAS with the teacher and UniPC on LSUN-Bedroom with NFE = 5. 29 Preprint. Figure 17: Comparison of GS and GAS with the teacher and UniPC on LSUN-Bedroom with NFE = 6. Figure 18: Comparison of GS and GAS with the teacher and UniPC on LSUN-Bedroom with NFE = 7. 30 Preprint. Figure 19: Comparison of GS and GAS with the teacher and UniPC on ImageNet with NFE = 4. Figure 20: Comparison of GS and GAS with the teacher and UniPC on ImageNet with NFE = 5. 31 Preprint. Figure 21: Comparison of GS and GAS with the teacher and UniPC on ImageNet with NFE = 6. Figure 22: Comparison of GS and GAS with the teacher and UniPC on ImageNet with NFE = 7. 32 Preprint. Figure 23: Comparison of GAS with LD3 and GITS on MS-COCO with NFE = 5. Figure 24: Comparison of GAS with LD3 and GITS on MS-COCO with NFE = 6."
        }
    ],
    "affiliations": [
        "HSE University, Russia",
        "Lomonosov Moscow State University, Russia"
    ]
}