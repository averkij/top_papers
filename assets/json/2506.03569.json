{
    "paper_title": "MiMo-VL Technical Report",
    "authors": [
        "Xiaomi LLM-Core Team",
        ":",
        "Zihao Yue",
        "Zhenru Lin",
        "Yifan Song",
        "Weikun Wang",
        "Shuhuai Ren",
        "Shuhao Gu",
        "Shicheng Li",
        "Peidian Li",
        "Liang Zhao",
        "Lei Li",
        "Kainan Bao",
        "Hao Tian",
        "Hailin Zhang",
        "Gang Wang",
        "Dawei Zhu",
        "Cici",
        "Chenhong He",
        "Bowen Ye",
        "Bowen Shen",
        "Zihan Zhang",
        "Zihan Jiang",
        "Zhixian Zheng",
        "Zhichao Song",
        "Zhenbo Luo",
        "Yue Yu",
        "Yudong Wang",
        "Yuanyuan Tian",
        "Yu Tu",
        "Yihan Yan",
        "Yi Huang",
        "Xu Wang",
        "Xinzhe Xu",
        "Xingchen Song",
        "Xing Zhang",
        "Xing Yong",
        "Xin Zhang",
        "Xiangwei Deng",
        "Wenyu Yang",
        "Wenhan Ma",
        "Weiwei Lv",
        "Weiji Zhuang",
        "Wei Liu",
        "Sirui Deng",
        "Shuo Liu",
        "Shimao Chen",
        "Shihua Yu",
        "Shaohui Liu",
        "Shande Wang",
        "Rui Ma",
        "Qiantong Wang",
        "Peng Wang",
        "Nuo Chen",
        "Menghang Zhu",
        "Kangyang Zhou",
        "Kang Zhou",
        "Kai Fang",
        "Jun Shi",
        "Jinhao Dong",
        "Jiebao Xiao",
        "Jiaming Xu",
        "Huaqiu Liu",
        "Hongshen Xu",
        "Heng Qu",
        "Haochen Zhao",
        "Hanglong Lv",
        "Guoan Wang",
        "Duo Zhang",
        "Dong Zhang",
        "Di Zhang",
        "Chong Ma",
        "Chang Liu",
        "Can Cai",
        "Bingquan Xia"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL."
        },
        {
            "title": "Start",
            "content": "MiMo-VL Technical Report LLM-Core Xiaomi"
        },
        {
            "title": "Abstract",
            "content": "We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL. 5 2 0 2 4 ] . [ 1 9 6 5 3 0 . 6 0 5 2 : r Figure 1 Benchmark performance of MiMo-VL-7B. 4 5 5 6 6 7 7 8 8 8"
        },
        {
            "title": "Contents",
            "content": ""
        },
        {
            "title": "2.2 Pre-training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "2.2.1 Image Caption Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "2.2.6 Synthetic Reasoning Data . . . . . . . . . . . . . . . . . . . . . . . . . .",
            "content": "2.3 Pre-training Stages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Post-Training 3.1 Reinforcement Learning with Verifiable Rewards . . . . . . . . . . . . . . . . . 3.2 Reinforcement Learning from Human Feedback . . . . . . . . . . . . . . . . . 10 3.3 Mixed On-Policy Reinforcement Learning . . . . . . . . . . . . . . . . . . . . . 11 4 Evaluation 12 4.1 Evaluation Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 4.2 General Capabilities . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 4.3 Reasoning Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12 4.4 GUI Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 4.5 Elo Rating . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16 5 Discussion 16 5.1 Boosting Reasoning Capability in Pre-training . . . . . . . . . . . . . . . . . . 16 5.2 On-Policy RL v.s. Vanilla GRPO . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 5.3 Interference Between RL Tasks . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 6 Case Study 7 Conclusions Contributions and Acknowledgments Model Configuration of MiMo-VL-7B 17 20"
        },
        {
            "title": "E More Qualitative Examples",
            "content": "27 28 28"
        },
        {
            "title": "Introduction",
            "content": "Vision-language models (VLMs) have emerged as the foundational backbone for multimodal AI systems, enabling autonomous agents to perceive the visual world, reason over multimodal content (Yue et al., 2024b), and interact with both digital (Xie et al., 2024; OpenAI, 2025) and physical environments (Zitkovich et al., 2023; Black et al., 2024). The significance of these capabilities has motivated extensive exploration across multiple dimensions, including novel architectural designs (Alayrac et al., 2022; Team, 2024; Ye et al., 2025) and innovative training methodologies with optimized data recipes (Karamcheti et al., 2024; Dai et al., 2024), leading to rapid progress in the field (Liu et al., 2023; Tong et al., 2024; Bai et al., 2025a). In this report, we share our efforts to build compact yet powerful VLM, MiMo-VL-7B. MiMoVL-7B comprises three components: (1) native-resolution Vision Transformer (ViT) encoder that preserves fine-grained visual details, (2) Multi-Layer Perceptron (MLP) projector for efficient cross-modal alignment, and (3) our MiMo-7B (Xiaomi, 2025) language model, specifically optimized for complex reasoning tasks. The development of MiMo-VL-7B involves two sequential training processes: (1) four-stage pre-training phase, which includes projector warmup, vision-language alignment, general multimodal pre-training, and long-context Supervised Fine-Tuning (SFT). Throughout these stages, we curate high-quality datasets by strategically combining open-source collections with synthetic data generation techniques, consuming 2.4 trillion tokens, adjusting the dataset distribution in different stages to facilitate training. This phase yields the MiMo-VL-7B-SFT model. (2) subsequent post-training phase, where we introduce Mixed On-policy Reinforcement Learning (MORL), novel framework that seamlessly integrates diverse reward signals spanning perception accuracy, visual grounding precision, logical reasoning capabilities, and human preferences. We adopt the idea of GRPO (Shao et al., 2024) and enhance training stability by exclusively performing on-policy gradient updates during this stage. This phase yields the MiMo-VL-7B-RL model. During this journey, we find: (1) Incorporating high-quality, broad-coverage reasoning data from the pre-training stage In the current era of thinking models, vast is crucial for enhancing model performance. quantities of multimodal pre-training data are undergoing significant re-evaluation. Traditional question-answering (QA) data, with its direct, short answers, often restricts models to superficial pattern matching and leads to overfitting. In contrast, synthesized reasoning data with long Chainof-Thought (CoT) enables learning of complex logical relationships and generalizable reasoning patterns, providing richer supervision signals that markedly improve both performance and training efficiency. To leverage this advantage, we curate high-quality reasoning data by identifying diverse queries, employing large reasoning models to regenerate responses with long CoT, and applying rejection sampling to ensure quality. Furthermore, rather than treating this as supplementary fine-tuning data, we incorporate substantial volumes of this synthetic reasoning data directly into the later pre-training stages, where extended training yields continued performance improvements without saturation. (2) Mixed On-policy Reinforcement Learning further enhances model performance, while achieving stable simultaneous improvements remains challenging. We apply RL across diverse capabilities, including reasoning, perception, grounding, and human preference alignment, spanning modalities including text, images, and videos. While this hybrid training approach further unlocks the models potential, interference across data domains remains challenging. Disparities in the growth trends of response length and task difficulty levels hinder stable, simultaneous improvements across all capabilities. 4 MiMo-VL-7B-RL delivers exceptional results across the full spectrum of multimodal capabilities. (1) On fundamental visual perception tasks, it achieves state-of-the-art performance among open-source VLMs of comparable scale, scoring 66.7 on MMMU (Yue et al., 2024b) and outperforming Qwen2.5-VL-7B (Bai et al., 2025a) on 35 out of 40 evaluated tasks. (2) For complex multimodal reasoning, MiMo-VL-7B-RL exhibits outstanding performance, scoring 59.4 on OlympiadBench (He et al., 2024) and surpassing models up to 72B parameters. (3) In GUI grounding for agentic applications, our model sets new standard by achieving score of 54.7 on OSWorld-G (Xie et al., 2025), even exceeding specialized models like UI-TARS (Qin et al., 2025b). (4) In terms of user experience and preference, MiMo-VL-7B-RL achieves the highest Elo rating among all open-source VLMs on our in-house user preference evaluation, performing competitively with proprietary models such as Claude 3.7 Sonnet. These results validate our approach: by combining strong perception, sophisticated reasoning, and precise grounding capabilities through our MORL framework, MiMo-VL-7B-SFT and MiMo-VL7B-RL establish new standards for open-source vision-language models. To promote transparency and reproducibility, we also contribute comprehensive evaluation suite covering 50+ tasks with complete prompts and protocols, enabling the community to build upon our work."
        },
        {
            "title": "2 Pre-Training",
            "content": "In this section, we introduce the architecture design of our MiMo-VL-7B, followed by the data curation process and training recipes of pre-training stages."
        },
        {
            "title": "2.1 Architecture",
            "content": "MiMo-VL-7B consists of three components: (1) Vision Transformer (ViT) for encoding visual inputs such as images and videos; (2) projector that maps the visual encodings into latent space aligned with the Large Language Model (LLM); and (3) the LLM itself, which performs textual understanding and reasoning. To support native resolution inputs, we adopt Qwen2.5-ViT (Bai et al., 2025a) as our visual encoder. We initialize the LLM backbone with MiMo-7B-Base (Xiaomi, 2025) to leverage its strong reasoning capability, and utilize randomly initialized Multi-Layer Perceptron (MLP) as the projector. The overall architecture is illustrated in Figure 2, and the model configuration can be found in Appendix B. Stages Purpose Dataset Stage 1 Projector Warmup Stage 2 Vision-Language Alignment Stage 3 Multimodal Pre-training Stage 4 Long-context SFT Image-Caption Pairs + Interleaved Data + Pure Text, OCR, Grounding, QA, Video, GUI, Instruction Data, Reasoning Data + Long Pure Text, Long Documents, High-resolution Images, Extended Videos, Long Reasoning Data Learning Rate Training Tokens Sequence Length Trainable Components 1e-3 300B 8K Projector 1e-4, 1e-5 167B 8K ViT, Projector 1e-5 1.4T 8K All 2.5e-5 550B 32K All Table 1 Overview of MiMo-VL-7B training stages. 5 Figure 2 Model architecture of MiMo-VL-7B."
        },
        {
            "title": "2.2 Pre-training Data",
            "content": "The MiMo-VL-7B pre-training dataset comprises 2.4 trillion tokens of high-quality, diverse multimodal data spanning images, videos, and text. This comprehensive dataset includes general image captions, interleaved data, Optical Character Recognition (OCR) data, grounding data, video content, GUI interactions, reasoning examples, and text-only sequences. To ensure the quality of each data modality, we implement dedicated data curation pipelines tailored to the characteristics of each data type. Throughout the training process, we systematically adjust the proportions of different data modalities across various training stages to optimize both training efficiency and model stability. Additionally, we employ phash-based image deduplication to eliminate potential overlaps between our training datasets and evaluation benchmarks, thereby minimizing data contamination. We detail the specific processing procedures for each data type in the following sections. 2.2.1 Image Caption Data The construction of our image caption dataset involves multi-stage process designed to ensure high quality and distributional balance. We begin by aggregating substantial volume of publicly available caption data from web sources. This initial corpus then undergoes rigorous deduplication phase, employing image perceptual hashing (phash) in conjunction with text filtering, to yield refined set of unique raw captions. Subsequently, leveraging both the images and their original textual descriptions as priors, we utilize specialized captioning model to re-caption the entire raw caption dataset. Following re6 captioning, we apply filtering mechanisms to the generated captions based on linguistic consistency and repetition patterns to ensure the quality of the re-captioned text. To address inherent data imbalances, we adopt the methodology of MetaCLIP (Xu et al., 2023), which involves constructing novel bilingual (Chinese and English) metadata. This critical step serves to refine the caption distribution, thereby mitigating the over-representation of high-frequency entries and reducing dataset noise. The culmination of this meticulous process is balanced, high-quality, and diverse caption dataset. Crucially, we observe that this rich dataset significantly enhances model generalization and qualitative performance, offering advantages not always fully reflected in evaluations on existing, specialized benchmarks. 2.2.2 Interleaved Data We compile an extensive corpus of interleaved image-text data from diverse sources, including webpages, books, and academic papers. For content originating from books and papers, we employ advanced PDF parsing toolkits for content extraction and cleansing. The filtering process prioritizes and retains data types rich in world knowledge, such as textbooks, encyclopedias, manuals, guides, patents, and biographies. Within this interleaved dataset, textual segments are evaluated based on metrics including knowledge density and readability. For the visual components, we implement filters to exclude images of diminutive size, anomalous aspect ratios, unsafe content, and those with minimal visual information (e.g., decorative chapter headings and illustrations). Finally, image-text pairs are scored on relevance, complementarity, and the balance of information density, ensuring the retention of high-quality data. This curated dataset significantly augments the models knowledge repository, thereby establishing robust foundation for its subsequent reasoning capabilities. 2.2.3 OCR and Grounding Data To enhance the models capabilities in OCR and object grounding, we compile an extensive corpus of OCR and grounding data from open-source datasets for pre-training. For the OCR data, the images encompass diverse textual content from documents, tables, general scenes, product packaging, and mathematical formulas. To increase learning difficulty, in addition to standard printed text, we specifically incorporate images containing handwritten script, typographically deformed text, and blurry/occluded text, thereby improving the models recognition performance and robustness. For portion of this data, textual regions are annotated with bounding boxes, enabling the model to simultaneously predict these locations. For the grounding data, our images feature scenarios with both single and multiple objects. We further improve the models capacity to comprehend complex referential intentions by employing intricate object expressions within localization prompts. In all scenarios involving localization, we use absolute coordinates for representation. 2.2.4 Video Data Our video dataset primarily draws from publicly available online videos, spanning wide array of domains, genres, and durations. Based on the videos, we design video re-captioning pipeline that produces dense, fine-grained event-level descriptions. Each caption is temporally grounded with precise start and end timestamps, enabling the model to develop general video perception with time-awareness. From the caption dataset, we further collect subset with balanced distribution of event durations for temporal grounding pretraining. We also curate video analysis data that 7 summarize the global semantics of videos, such as narrative structures, stylistic elements, and implicit intentions. These analytical paragraphs enable the model to grasp in-depth comprehension of videos and extended world knowledge. To enhance the models conversational coherence, we collect diverse and challenging questions about videos and synthesize corresponding responses. We also incorporate open-source video captioning and conversation datasets to further enrich our video pretraining data. 2.2.5 Graphical User Interface Data To enhance the models capabilities in navigating Graphical User Interfaces (GUIs), we collect open-source pre-training data covering all sorts of platforms such as mobile, web, and desktop. synthetic data engine is also devised to compensate for the limitations of open-source data and to strengthen specific aspects of the models capabilities. For example, we have constructed vast amount of Chinese GUI data to enable the model to better handle Chinese GUI scenarios. For GUI Grounding, we gather data for both element grounding and instruction grounding. Element grounding trains the model to precisely locate interface elements based on textual descriptions, establishing robust perception of static user interfaces. Instruction grounding requires the model to identify target objects on screenshots according to user instructions, enhancing comprehension of GUI interaction logic. For this part, we additionally introduce pre-training task that involves predicting intermediate actions based on before-and-after screenshots. Empirical evidence demonstrates that this approach significantly improves the models dynamic perception of GUI interfaces. For GUI Action, we collect large scale of long GUI action trajectories. To ensure consistency across different platforms, we unified actions from mobile, web, and desktop environments into standardized action space. detailed specification of this action space is provided in Appendix D. This harmonization prevents action conflicts while maintaining platform-specific functionality. 2.2.6 Synthetic Reasoning Data Our approach to generating synthetic reasoning data begins with the comprehensive curation of open-source questions. This diverse collection spans perceptual question answering, document question answering, video question answering, and visual reasoning tasks, supplemented by question-answer pairs derived from web content and literary works. Following initial filtering of these source questions, we leverage large reasoning model to produce answers that integrate explicit reasoning. cornerstone of our methodology is rigorous, multi-stage quality control. Beyond verifying the factual correctness of answers, we apply strict filtering criteria to the reasoning processes themselves, evaluating clarity of thought, eliminating redundancy, and ensuring consistent formatting. The resulting high-fidelity dataset plays critical role in empowering our model. It facilitates the effective inheritance of strong reasoning abilities inherent in MiMo-7B-Base (Xiaomi, 2025), enabling their seamless transfer and adaptation to multimodal contexts. Consequently, this allows our model to exhibit powerful and versatile multimodal reasoning capabilities across broad array of domains."
        },
        {
            "title": "2.3 Pre-training Stages",
            "content": "Our model undergoes four pre-training stages as illustrated in Table 1: Stage 1: We freeze the ViT and LLM components, and warm up the randomly initialized projector 8 using image-caption pairs. This ensures the projector learns to map visual concepts to the language models representation space effectively, providing informative gradient signals for subsequent training stages rather than noisy updates from poorly aligned projector. Stage 2: The ViT is then unfrozen, and interleaved data is introduced to further strengthen vision-language alignment. The inclusion of complex and diverse images within the interleaved data enhances the ViTs performance and robustness. Stage 3: In this stage, all parameters are trainable. We introduce more diverse array of data and tasks, including OCR, grounding, video, and GUI dataaccumulating to 1.4 trillion tokensto bolster the models general multimodal capabilities. To ensure stable mid-stage evaluation monitoring, small quantities of QA, instruction-following, and reasoning data are incorporated. Furthermore, limited amount of text-only data is utilized to preserve MiMo-7BBases textual capability. Stage 4: This stage is dedicated to enhancing the models adaptability to long-context inputs. The training sequence length is extended from 8K to 32K tokens. We introduce additional data types such as long pure text, high-resolution images, long documents, extended videos, and long reasoning data to augment its long-context processing capabilities. As long-context packing significantly increases the effective batch size, the learning rate is adjusted from 1e-5 to 2.5e5. Relative to Stage 3, this stage features markedly increased proportion of reasoning data, alongside the introduction of long-form reasoning patterns. These four stages create powerful model, MiMo-VL-7B-SFT. With particular emphasis on Stage 4, the models reasoning capabilities are fully realized, enabling it to address highly intricate STEM problems. This advanced reasoning aptitude also generalizes effectively to common perception tasks. Consequently, our model demonstrates exceptionally high performance across various downstream benchmarks."
        },
        {
            "title": "3 Post-Training",
            "content": "Building upon the visual perception capabilities and multimodal reasoning established during pre-training, we conduct post-training to further enhance MiMo-VL-7B. Our approach employs novel Mixed On-policy Reinforcement Learning (MORL) framework that seamlessly integrates Reinforcement Learning with Verifiable Rewards (RLVR) (Shao et al., 2024; Lambert et al., 2025) with Reinforcement Learning from Human Feedback (RLHF) (Ouyang et al., 2022) to improve MiMo-VL-7B on challenging reasoning tasks and alignment with human preferences."
        },
        {
            "title": "3.1 Reinforcement Learning with Verifiable Rewards",
            "content": "RLVR relies exclusively on rule-based reward functions, enabling continuous model self-improvement. In the post-training of MiMo-VL-7B, we design multiple verifiable reasoning and perception tasks where final solutions can be precisely validated using predefined rules. Visual Reasoning Visual reasoning capabilities are fundamental for multimodal models to understand and solve complex problems that require both visual perception and logical thinking. To facilitate this capability, we compile diverse verifiable STEM questions from open-source communities and proprietary K-12 collections. An LLM is prompted to filter proof-based problems and rewrite multiple-choice questions with numerical or symbolic answers into free-answer formats, alleviating potential reward hacking. We further refine question quality through comprehensive model-based difficulty assessment, excluding questions that either cannot be solved by advanced 9 VLMs or are too easy, with MiMo-VL-7B rollout pass rate exceeding 90%. Additionally, we remove questions solvable even without image inputs. After data cleaning and category balancing, we curate visual reasoning dataset of 80K problems. For evaluation, we use the rule-based Math-Verify library to determine response correctness.1 Text Reasoning Since most visual reasoning data is limited to K-12 level questions, the reasoning performance of RL-trained models could be constrained. In contrast, text-only reasoning datasets include more challenging queries requiring college or competition-level intelligence. To unleash the full reasoning potential of our model, we incorporate mathematical reasoning data from Xiaomi (2025). Rewards are computed using the same rule-based Math-Verify library to ensure consistent evaluation across both visual and textual reasoning tasks. Image Grounding Accurate spatial localization is essential for models to understand object relationships and spatial reasoning in images. We include both general and GUI grounding tasks in our RLVR framework to enhance MiMo-VL-7Bs grounding capability. For bounding box predictions, rewards are calculated using the Generalized Intersection over Union (GIoU) (Rezatofighi et al., 2019) between predicted and ground-truth boxes. For point-style outputs, rewards are determined by whether the predicted point falls within the ground-truth bounding box. Visual Counting Precise counting abilities are essential for quantitative visual understanding and mathematical reasoning in visual contexts (Chen et al., 2025a). We enhance visual counting capabilities through RL training, where rewards are defined by the accuracy of the models counting predictions compared to ground-truth counts. Temporal Video Grounding Beyond static image understanding and reasoning, we extend our RLVR framework to dynamic video content to capture temporal dependencies. We incorporate temporal video grounding tasks that require the model to localize video segments corresponding to natural language queries (Wang et al., 2025). The model outputs timestamps in [mm:ss,mm:ss] format to indicate the start and end times of the target video segments. Rewards are computed as the Intersection over Union (IoU) between predicted and ground-truth temporal segments."
        },
        {
            "title": "3.2 Reinforcement Learning from Human Feedback",
            "content": "To align model outputs with human preferences and mitigate undesirable behaviors, we employ Reinforcement Learning from Human Feedback (RLHF) as complementary approach to our verifiable reward framework. Query Collection Query diversity is paramount to the success of RLHF. Our methodology commences with gathering multimodal and text-only queries from open-source instruction tuning datasets and in-house human-written sources. All collected queries, both text and multimodal, then undergo dedicated screening process. To further enhance diversity, we employ techniques such as clustering queries based on their embeddings and analyzing the resultant patterns. Crucially, we balance the proportions of Chinese and English queries, as well as those targeting helpfulness and harmlessness, before curating the final query set. For each selected query, MiMo-VL-7B and multiple other top-performing VLMs are prompted to generate responses. These responses are subsequently pairwise ranked by an advanced VLM to construct the definitive dataset for reward 1https://github.com/huggingface/Math-Verify 10 Figure 3 Mixed On-policy Reinforcement Learning in post-training phase. model training. Notably, to mitigate potential reward hacking, this same query set is utilized for both reward model training and the RLHF process. Reward Model We develop two specialized reward models tailored to different input modalities, training them using the Bradley-Terry reward modeling objective (Ouyang et al., 2022). The textonly reward model is initialized from MiMo-7B (Xiaomi, 2025) to leverage its strong language understanding capabilities, while the multimodal reward model builds upon MiMo-VL-7B to effectively process queries containing visual inputs. This dual-model approach ensures optimal performance across both textual and multimodal evaluation scenarios."
        },
        {
            "title": "3.3 Mixed On-Policy Reinforcement Learning",
            "content": "In the post-training phase of MiMo-VL-7B, we implement Mixed On-policy Reinforcement Learning (MORL) to simultaneously optimize RLVR and RLHF objectives. As illustrated in Figure 3, we integrate rule-based and model-based rewards as unified services within the verl framework (Sheng et al., 2024), enhanced by the Seamless Rollout Engine (Xiaomi, 2025). On-Policy RL Recipe We adopt fully on-policy variant of GRPO (Shao et al., 2024) as the RL algorithm, which demonstrates robust training stability and effective exploration capabilities (Chen et al., 2025b). For each problem ùëû, the algorithm samples group of responses {ùëú1, ùëú2, ..., ùëúùê∫} from the policy ùúãùúÉ, and updates the policy by maximizing the following objective: JGRPO (ùúÉ) = Eùëûùê∑,{ùëúùëñ }ùê∫ ùëñ=1ùúãùúÉ ( ùëû) , (1) 1 ùëñ=1 ùëúùëñ (cid:205)ùê∫ ùê∫ ùëúùëñ ùëñ= ùëó=1 ùê¥ùëñ, ùëó where ùê¥ùëñ, ùëó is the advantage, which is computed by the rewards {ùëü1, ùëü2, ..., ùëüùê∫} of responses in the same group: ùê¥ùëñ, ùëó = ùëüùëñ mean({ùëüùëñ}ùê∫ std({ùëüùëñ}ùê∫ ùëñ=1) ùëñ=1) . (2) Compared to vanilla GRPO, this on-policy variant performs single-step policy updates following response rollout, eliminating the need for clipped surrogate training objective. Following Xiaomi (2025), we integrate several advancements, including removal of the KL loss, dynamic sampling, easy data filter, and re-sampling strategies, into our RL training recipe. Reward-as-a-Service The MORL process integrates tasks across reasoning, perception, grounding, multimodal RLHF, and text-only RLHF, each requiring distinct reward functions or dedicated reward models. To provide unified interface and near-zero latency reward computation, we introduce Reward-as-a-Service (RaaS). reward router dynamically selects the appropriate reward function based on the querys task type. To minimize latency, reward models are deployed as standalone services, ensuring scalable reward computation accessible via HTTP. All rewards are normalized to the range [0, 1]. No additional reward, such as format rewards, is incorporated in our training process."
        },
        {
            "title": "4 Evaluation",
            "content": "We evaluate MiMo-VL-7B across 50 tasks to comprehensively assess its capabilities. Appendix lists all the benchmarks adopted in our evaluation. We also assess model performance with an in-house evaluation set."
        },
        {
            "title": "4.1 Evaluation Setting",
            "content": "For image understanding benchmarks, we set the max pixels for the input image to 4096 28 28 and the maximum generation tokens to 32,768, employing greedy search for decoding. For video benchmarks, videos are sampled at 2 FPS, with maximum of 256 frames and total token limit of 16,384. For text evaluation benchmarks, we set the max new tokens for generation as 32,768, temperature as 0.6 and top-p as 0.95. We adapt the existing framework based on LMMs-Eval (Zhang et al., 2024a) to better accommodate long-CoT reasoning models. We further optimize the evaluation logic for specific tasks to ensure better evaluation consistency. To facilitate open research, we open-source our evaluation framework with all prompts used."
        },
        {
            "title": "4.2 General Capabilities",
            "content": "Table 2 presents benchmark results assessing the general capabilities of VLMs. Our MiMo-VL-7B models demonstrate consistently leading performance across diverse range of vision-language and text benchmarks, establishing new state-of-the-art results among open-source models and even surpassing proprietary counterparts. Specifically, our MiMo-VL-7B models achieve state-of-the-art results among open-source models. MiMo-VL-7B-SFT and (i) On general vision-language tasks, our models achieve exceptional performance that leads the open-source field. MiMo-VL-7B-SFT and MiMo-VL-7B-RL obtain 64.6% and 66.7% on MMMUval respectively, outperforming much larger models such as Gemma 3 27B. For document and chart understanding, MiMo-VL-7B-RL excels with top open-source score of 56.5% on CharXivRQ, significantly exceeding Qwen2.5-VL (42.5%) by 14.0 points and InternVL3 (37.6%) by 18.9 points. (ii) Our models demonstrate superior video understanding capabilities while maintaining strong textual performance. MiMo-VL-SFT achieves leading 53.1% on Video-MMMU, and MiMo-VL-RL obtains an impressive 50.0% mIoU on Charades-STA. (iii) Compared with MiMo-7B, our models maintain decent performance on text-only benchmarks. (iv) Remarkably, our MoRL yields comprehensive improvements, with the most impressive gains observed on challenging benchmarks such as VibeEval and CountBench."
        },
        {
            "title": "4.3 Reasoning Tasks",
            "content": "Table 3 presents evaluation results for multimodal and text reasoning benchmarks. In multimodal reasoning, both the SFT and RL models significantly outperform all compared open-source 2https://github.com/XiaomiMiMo/lmms-eval 12 Benchmark Metrics MiMo-VL MiMo-VL Qwen2.5-VL 7B-SFT 7B-RL 7B General MMMU val MMMU-Prostandard MMMU-Provision MMBench-entest MMBench-cntest Mantis MME-RealWorlden MME-RealWorldcn AI2D BLINKval CV-Bench VibeEval VL-RewardBench V* VLMs are Blind PixmoCount CountBench RefCOCOavg val RQ Doc & OCR ChartQA CharXiv CharXiv DQ DocVQA InfoVQA val SEED-Bench-2-Plus OCRBench val Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. Acc. GPT-Score Macro Acc. Acc. Acc. Acc. Acc. Acc.@0.5 Acc. Acc. Acc. Acc. Acc. Acc. Acc. GUI VisualWebBenchavg WebSrcval ScreenSpot ScreenSpot-v2 ScreenSpot-Proavg OSWorld-Gno_refusal Video Video-MMEw/o sub. Video-MMMU EgoSchemaval Charades-STA Text GPQA Diamond SuperGPQA DROP MMLU-Pro IF-Eval Acc. SQuAD F1 Center Acc. Center Acc. Center Acc. Center Acc. Acc. Acc. Acc. mIoU Pass@1 Pass@1 3-shot F1 EM Prompt Strict 64.6 45.2 39.4 84.5 81.9 78.8 57.4 55.0 83.2 62.5 81.8 47.2 61.9 80.6 78.0 79.4 87.0 85.7 92.9 54.4 87.0 95.2 87.2 71.9 87.6 80.2 96.5 87.3 89.5 39.9 54. 66.9 53.1 60.4 38.5 56.3 42.6 82.7 59.8 75.3 66.7 46.2 40.3 84.4 82.0 78.3 59.1 55.5 83.5 62.4 82.3 54.7 62.7 81.7 79.4 79.4 90.4 89.6 91.7 56.5 86.8 95.7 88.0 72.4 86. 80.2 95.3 87.2 90.5 41.9 56.1 67.4 43.3 59.6 50.0 58.3 44.3 85.1 64.8 75.9 58.6 34.7* 29.4* 83.5 83.4 74.7* 57.4 51.2* 83.9 56.4 75.4* 47.7* 47.3* 73.8* 37.4* 60.7* 74.1* 87.1 90.2* 42.5 73.9 95.5* 81.4* 70.7 89.7* 72.9* 94.6* 84.7 88.0 29.0 37.5* 65.1 47.4 62.4* 43.6 30.3* 25.4* 67.6* 48.7* 75.1* InternVL3 Gemma-3 8B 27B-IT GPT-4o Claude 3.7 Sonnet 62.7 45.6* 37.8* 83.4 82.2 72.8* 56.1* 58.5* 85.2 55.5 81.0* 43.6* 49.7* 72.8* 36.8* 62.0 80.0* 90.1 89.6* 37.6 73.6 89.4* 70.7* 69.7 88.0 62.7 91.1 79.5 81.4 - - 66.3 48.9* 68.2* 25.4 33.5* 29.4* 77.5* 58.3* 87.2* 64.9 37.8* 24.9* 81.6* 82.4* 70.0* 51.9* 47.9* 84.5 53.3* 70.4* 44.0* 51.9* 50.8* 18.6* 48.6* 77.2* - 78.0 29.2* 63.8* 86.6 70.6 66.3* 77.6* 49.7* 89.0* - - - - - - - - 40.9 29.3* 72.2 45.3 88.9 70.7 42.5* 36.1* 84.6* 84.5* 75.6* 57.5* 58.5* 82.6* 60.0 76.0* 64.7 62.4 73.9 49.8* 54.4* 85.7* - 86.7 52.0 86.5 93.0* 82.1* 71.1* 84.3* 80.2 89.1* 18.3 18.5 - - - - - - 50.6* 31.6* 81.5 69.1 82.5 69.8* 56.5* 45.8* 84.8* 83.7* 75.1* 50.8* 40.6* 81.4* 62.3* 75.4* 39.0* 67.4* - 72.1* 53.5* 90.2* - 92.2* 63.0* 89.5* 94.1* 65.5* 72.9* 80.6* 79.3* 91.1* - - - - - - - - 66.0* 54.4* 87.2* 81.0 88.7* Table 2 Comparison of MiMo-VL-7B models with other models on diverse visual-language and text benchmarks. Results marked with * are obtained using our evaluation framework. indicates that evaluation is performed using GPT-4o. The best results among open-source models are bolded. 13 Intern-VL3 Qwen2. GPT-4o Gemini-2.5 Pro Reasoning Benchmark Metrics MiMo-VL MiMo-VL QVQ-72B Qwen2.5-VL 7B-SFT Preview 7B-RL 72B Multi-modal OlympiadBench MathVision MathVerse DynaMath WeMath LogicVista MathVista mini vision_only Text MATH500 AIME24 AIME Acc. Acc. Acc. Worst-case Acc. Strict Score Acc. Acc. Pass@1 Pass@1 Pass@1 59.4 57.9 67.1 46.9 65.1 61.2 81.8 95.0 66.4 50. 59.4 60.4 71.5 45.9 66.3 61.4 81.5 95.4 67.5 52.5 20.4 35.9 45.1* 30.7 37.7* 53.8* 71.4 83.8* 25.2* 18.1* 37.2* 38.1 57.6 38.1* 50.6* 57.1* 74.8 83.0 16.7* 10.8* 78B 12.3 43.2 51.0 35.1 46.1 55.9 72.2 68.8* 12.2* 11.7* 72B - - - - - - - 25.9 31.2 49.9 48.5 50.6 64.4 63. 82.8* 19.4* 13.3* 78.2* 10.9* 8.7* 69.8 69.1 76.7 56.3 78.0 73.8 80.9 95.2 92.0 86.7 Table 3 Comparison of MiMo-VL-7B with other models on reasoning benchmarks. Results marked with * are obtained using our evaluation framework. indicates that evaluation is performed using GPT-4o. The best results among open-source models are bolded. Figure 4 GUI understanding and grounding results. MiMo-VL-7B-RL achieves comparable results with GUI specialized models. baselines across these benchmarks. Notably, MiMo-VL-7B-SFT surpasses much larger models, including Qwen2.5-VL-72B and QVQ-72B-Preview. The RL model further improves performance on most reasoning benchmarks. For example, MiMo-VL-7B-RL boosts accuracy on MathVision from 57.9% to 60.4%. MiMo-VL-7B models also exhibit impressive reasoning capabilities on pure text benchmarks, even outperforming Qwen2.5-72B. These results demonstrate that our multimodal pre-training and post-training recipes effectively endow the model with exceptional visual capabilities and strong text intelligence."
        },
        {
            "title": "4.4 GUI Tasks",
            "content": "In addition, we demonstrate that MiMo-VL-7B models possess exceptional GUI understanding and grounding capabilities. In Table 2, MiMo-VL-7B-RL outperforms all other general VLMs compared. In Figure 4, we further compare MiMo-VL-7B-RL with GUI-specialized models (UITARS-1.0 (Qin et al., 2025a), Aguvis (Xu et al., 2024), OS-Atlas (Wu et al., 2024)) of similar size on GUI Understanding (WebSrc, VisualWebBench) and Grounding (Screenspot, Screenspotv2, Screenspot-Pro, OSWorld-G) benchmarks. As general-purpose VLM, MiMo-VL achieves comparable or even superior performance to GUI-specialized models, particularly on the more challenging Screenspot-Pro and OSWorld-G benchmarks. 14 Figure 5 Elo ratings comparison across VLMs. MiMo-VL-7B-RL achieves the highest rating among open-source models, approaching the performance of proprietary alternatives such as Claude 3.7 Sonnet. Figure 6 MiMo-VL-7B-SFT training curves in Stage 4. 15 Figure 7 On-policy RL and vanilla GRPO shows contrasting scaling behavior: on-policy RL performance continuously improves with more data, while vanilla GRPO reaches plateau around 20,000 samples."
        },
        {
            "title": "4.5 Elo Rating",
            "content": "Inspired by ChatbotArena (Zheng et al., 2023), we construct balanced bilingual (Chinese and English) in-house evaluation dataset comprising real user prompts. This approach assesses user preference beyond traditional benchmark scores, providing insights into practical model performance in real-world scenarios. Following the methodology of (Chou et al., 2024), we conduct pairwise comparisons between MiMo-VL-7B and competing models, including leading proprietary models and open-source VLMs ranging from 7B to 72B parameters. We compute Elo ratings based on GPT-4o judgments with style-controlled evaluation protocols. Our evaluation covers diverse range of visual-linguistic tasks, including multimodal reasoning, image understanding, and GUI interaction scenarios, therefore serving as good proxy of user preference. As illustrated in Figure 5, MiMo-VL-7B-RL achieves the highest Elo rating among all evaluated open-source VLMs, ranking first across models spanning from 7B to 72B parameters. This demonstrates superior user experience across the evaluation set, with our models performance closely approaching that of proprietary models such as Claude 3.7 Sonnet. Moreover, MORL brings boost of 22+ points for the MiMo-VL-7B-SFT. These results highlight the competitive capability of our models and validate the effectiveness of our training methodology."
        },
        {
            "title": "5.1 Boosting Reasoning Capability in Pre-training",
            "content": "Figure 6 shows the performance of MiMo-VL-7B-SFT during Stage 4, its final pre-training phase. In this stage, substantial volumes of synthetic long-form reasoning data are incorporated, and model performance increases sharply, e.g., +9 on MMMU, +14 on OSWorld-G, and +16 on OlympiadBench. Notably, model performance continuously improves without saturation. These 16 improvements are attributed to an increased depth in the models reasoning. For instance, on MMMU, the models average number of response tokens grows from 680 to 2.5K per question after Stage 4, indicating more detailed and profound level of reasoning when tackling problems."
        },
        {
            "title": "5.2 On-Policy RL v.s. Vanilla GRPO",
            "content": "We explore the benefits of on-policy RL v.s. vanilla GRPO with text-only reasoning tasks. As illustrated in Figure 7, the on-policy algorithm demonstrates consistent positive correlation between training data volume and performance score. Its learning curve shows no signs of saturation within the observed training window, suggesting potential for further enhancement with additional computational resources and data. Conversely, the vanilla GRPO algorithm initially exhibits higher sample efficiency, achieving robust performance early in training. This advantage, however, is transient. The algorithms performance generally saturates around 20,000 training samples, beyond which further training yields negligible improvements."
        },
        {
            "title": "5.3 Interference Between RL Tasks",
            "content": "While MORL training enhances performance on nearly all evaluated tasks, achieving stable and simultaneous improvements across diverse task domains remains significant challenge. During training, we observe that reasoning tasks exhibit disparities with visual perception and grounding tasks, such as visual grounding and counting, making it difficult to match the performance of standalone RL on individual tasks. The potential cause lies in the opposing growth trends of response length: reasoning tasks encourage longer CoT during the RL process, whereas grounding and counting tasks lead to shrinking ones. Disparities in task difficulty and the risk of reward hacking may also contribute to this interference. We are actively investigating the underlying causes of this phenomenon and seeking solutions to achieve consistent and persistent growth across all tasks."
        },
        {
            "title": "6 Case Study",
            "content": "We present qualitative results in Figure 8 and Appendix E. As depicted in the top example in Figure 8, our model showcases strong plot understanding capabilities, successfully converting an intricate plot into well-structured markdown table. Additionally, we highlight the models superior reasoning capabilities in STEM tasks. In the examples shown in Figure 10 and Figure 13, the model effectively addresses multiple STEM questions within single response. Furthermore, our model exhibits strong agentic capabilities. As illustrated in Figure 9, MiMo-VL-7B successfully navigates website to add the Xiaomi SU7 to the wishlist, with customized paint and interior options. 17 Instruction: Convert it to table. MiMo-VL-7B-RL: Instruction: Answer the question in image. MiMo-VL-7B-RL: <think>...</think> To solve for the tension in the wire, we use the equilibrium of torques about the hinge point ùëÉ. For beam in equilibrium, the sum of clockwise torques equals the sum of counterclockwise torques. Step 1: Identify Forces and Torques - Beams weight (425 N): Acts downward at the beams midpoint (since it is uniform). The beams length is 8.4 m, so the midpoint is 8. 2 = 4.2 from ùëÉ. - Blocks weight (720 N): Acts downward at 8.4 from ùëÉ. - Tension in the wire (ùëá): Acts at 3.4 from ùëÉ at an angle of 23 above the horizontal. Only the vertical component of tension (ùëá sin 23) contributes to torque (the horizontal component acts along the beam, so its torque about ùëÉ is zero). Step 2: Calculate Torques Torque is defined as force perpendicular distance from pivot. - Clockwise torques (from the beam and block weights): - Beams weight: 425 4.2 = 1785 Nm - Blocks weight: 720 8.4 = 6048 Nm - Total clockwise torque: 1785 + 6048 = 7833 Nm - Counterclockwise torque (from tensions vertical component): - Tensions vertical component: ùëá sin 23 - Perpendicular distance from ùëÉ to the wire: 3.4 - Torque from tension: ùëá sin 23 3.4 Step 3: Solve for Tension In equilibrium, clockwise torque = counterclockwise torque: ùëá sin 23 3.4 = 7833 Solve for ùëá: ùëá = Using sin 23 0.3907: ùëá = Final Answer The tension in the wire is approximately 5900 N. 3.40.3907 7833 1.328 5900 7833 3.4sin 23 7833 Figure 8 Examples of MiMo-VL-7B solving complicated problems. 18 Figure 9 case demonstrating the agentic capabilities of our model. MiMo-VL-7B successfully navigates website to add the Xiaomi SU7 to the wishlist, customizing both paint and interior options. All screenshots are of size 1886*1544 (width*height)."
        },
        {
            "title": "7 Conclusions",
            "content": "In this report, we present our efforts in building MiMo-VL-7B models. Leveraging curated highquality pre-training datasets and our MORL framework, MiMo-VL-7B-SFT and MiMo-VL-7B-RL demonstrate state-of-the-art performance across evaluated benchmarks. We share key observations from our development process: the consistent performance gains from incorporating reasoning data in later pre-training stages, the advantages of on-policy RL over vanilla GRPO, and the challenges of task interference when applying MORL across diverse capabilities. Alongside the released model checkpoints, we open-source our comprehensive evaluation suite to promote transparency and reproducibility in multimodal research. We hope our work advances the development of capable open-source vision-language models and provides valuable insights for the community."
        },
        {
            "title": "References",
            "content": "J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, R. Ring, E. Rutherford, S. Cabi, T. Han, Z. Gong, S. Samangooei, M. Monteiro, J. L. Menick, S. Borgeaud, A. Brock, A. Nematzadeh, S. Sharifzadeh, M. Binkowski, R. Barreira, O. Vinyals, A. Zisserman, and K. Simonyan. Flamingo: visual language model for few-shot learning. In NeurIPS, 2022. S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, H. Zhong, Y. Zhu, M. Yang, Z. Li, J. Wan, P. Wang, W. Ding, Z. Fu, Y. Xu, J. Ye, X. Zhang, T. Xie, Z. Cheng, H. Zhang, Z. Yang, H. Xu, and J. Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025a. S. Bai, K. Chen, X. Liu, J. Wang, W. Ge, S. Song, K. Dang, P. Wang, S. Wang, J. Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025b. K. Black, N. Brown, D. Driess, A. Esmail, M. Equi, C. Finn, N. Fusai, L. Groom, K. Hausman, B. Ichter, S. Jakubczak, T. Jones, L. Ke, S. Levine, A. Li-Bell, M. Mothukuri, S. Nair, K. Pertsch, L. X. Shi, J. Tanner, Q. Vuong, A. Walling, H. Wang, and U. Zhilinsky. ùúã0: vision-languageaction flow model for general robot control. ArXiv, abs/2410.24164, 2024. URL https: //api.semanticscholar.org/CorpusID:273811174. L. Chen, L. Li, H. Zhao, Y. Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V, 2025a. Accessed: 2025-02-02. X. Chen, Z. Zhao, L. Chen, D. Zhang, J. Ji, A. Luo, Y. Xiong, and K. Yu. Websrc: dataset for web-based structural reading comprehension. arXiv preprint arXiv:2101.09465, 2021. Y. Chen, Z. Yang, Z. Liu, C. Lee, P. Xu, M. Shoeybi, B. Catanzaro, and W. Ping. Acereasonnemotron: Advancing math and code reasoning through reinforcement learning. arXiv preprint arXiv:2505.16400, 2025b. K. Cheng, Q. Sun, Y. Chu, F. Xu, Y. Li, J. Zhang, and Z. Wu. Seeclick: Harnessing gui grounding for advanced visual gui agents. arXiv preprint arXiv:2401.10935, 2024. C. Chou, L. Dunlap, K. Mashita, K. Mandal, T. Darrell, I. Stoica, J. Gonzalez, and W.-L. Chiang. Visionarena: 230k real world user-vlm conversations with preference labels. ArXiv, abs/2412.08687, 2024. URL https://api.semanticscholar.org/CorpusID: 274655992. 20 W. Dai, N. Lee, B. Wang, Z. Yang, Z. Liu, J. Barker, T. Rintamaki, M. Shoeybi, B. Catanzaro, and W. Ping. Nvlm: Open frontier-class multimodal llms. arXiv preprint, 2024. M. Deitke, C. Clark, S. Lee, R. Tripathi, Y. Yang, J. S. Park, M. Salehi, N. Muennighoff, K. Lo, L. Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art multimodal models. ArXiv preprint, abs/2409.17146, 2024. X. Du, Y. Yao, K. Ma, B. Wang, T. Zheng, K. Zhu, M. Liu, Y. Liang, X. Jin, Z. Wei, et al. Supergpqa: Scaling llm evaluation across 285 graduate disciplines. ArXiv preprint, abs/2502.14739, 2025. URL https://arxiv.org/abs/2502.14739. D. Dua, Y. Wang, P. Dasigi, G. Stanovsky, S. Singh, and M. Gardner. DROP: reading comprehension benchmark requiring discrete reasoning over paragraphs. In J. Burstein, C. Doran, and T. Solorio, editors, Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 23682378, Minneapolis, Minnesota, 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1246. URL https://aclanthology.org/N19-1246. C. Fu, Y. Dai, Y. Luo, L. Li, S. Ren, R. Zhang, Z. Wang, C. Zhou, Y. Shen, M. Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024a. X. Fu, Y. Hu, B. Li, Y. Feng, H. Wang, X. Lin, D. Roth, N. A. Smith, W.-C. Ma, and R. Krishna. Blink: Multimodal large language models can see but not perceive. In European Conference on Computer Vision, pages 148166. Springer, 2024b. J. Gao, C. Sun, Z. Yang, and R. Nevatia. Tall: Temporal activity localization via language query. In Proceedings of the IEEE international conference on computer vision, pages 52675275, 2017. C. He, R. Luo, Y. Bai, S. Hu, Z. L. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. arXiv preprint arXiv:2402.14008, 2024. D. Hendrycks, C. Burns, S. Kadavath, A. Arora, S. Basart, E. Tang, D. Song, and J. Steinhardt. Measuring mathematical problem solving with the math dataset. ArXiv preprint, abs/2103.03874, 2021. URL https://arxiv.org/abs/2103.03874. K. Hu, P. Wu, F. Pu, W. Xiao, Y. Zhang, X. Yue, B. Li, and Z. Liu. Video-mmmu: Evaluating knowledge acquisition from multi-discipline professional videos. arXiv preprint arXiv:2501.13826, 2025. D. Jiang, X. He, H. Zeng, C. Wei, M. Ku, Q. Liu, and W. Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. S. Karamcheti, S. Nair, A. Balakrishna, P. Liang, T. Kollar, and D. Sadigh. Prismatic vlms: Investigating the design space of visually-conditioned language models. In ICML, 2024. S. Kazemzadeh, V. Ordonez, M. Matten, and T. Berg. Referitgame: Referring to objects in photographs of natural scenes. In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP), pages 787798, 2014. A. Kembhavi, M. Salvato, E. Kolve, M. Seo, H. Hajishirzi, and A. Farhadi. diagram is worth dozen images. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 235251. Springer, 2016. 21 N. Lambert, J. Morrison, V. Pyatkin, S. Huang, H. Ivison, F. Brahman, L. J. V. Miranda, A. Liu, N. Dziri, S. Lyu, Y. Gu, S. Malik, V. Graf, J. D. Hwang, J. Yang, R. L. Bras, O. Tafjord, C. Wilhelm, L. Soldaini, N. A. Smith, Y. Wang, P. Dasigi, and H. Hajishirzi. Tulu 3: Pushing frontiers in open language model post-training, 2025. URL https://arxiv.org/abs/2411.15124. B. Li, Y. Ge, Y. Chen, Y. Ge, R. Zhang, and Y. Shan. Seed-bench-2-plus: Benchmarking multimodal large language models with text-rich visual comprehension. arXiv preprint arXiv:2404.16790, 2024a. K. Li, Z. Meng, H. Lin, Z. Luo, Y. Tian, J. Ma, Z. Huang, and T.-S. Chua. Screenspot-pro: Gui grounding for professional high-resolution computer use. arXiv preprint arXiv:2504.07981, 2025. L. Li, Y. Wei, Z. Xie, X. Yang, Y. Song, P. Wang, C. An, T. Liu, S. Li, B. Y. Lin, L. Kong, and Q. Liu. Vlrewardbench: challenging benchmark for vision-language generative reward models. ArXiv, abs/2411.17451, 2024b. URL https://api.semanticscholar.org/CorpusID: 274281459. H. Liu, C. Li, Q. Wu, and Y. J. Lee. Visual instruction tuning. In NeurIPS, 2023. J. Liu, Y. Song, B. Y. Lin, W. Lam, G. Neubig, Y. Li, and X. Yue. Visualwebbench: How far have multimodal llms evolved in web page understanding and grounding? arXiv preprint arXiv:2404.05955, 2024a. Y. Liu, H. Duan, Y. Zhang, B. Li, S. Zhang, W. Zhao, Y. Yuan, J. Wang, C. He, Z. Liu, et al. Mmbench: Is your multi-modal model an all-around player? In European conference on computer vision, pages 216233. Springer, 2024b. Y. Liu, Z. Li, M. Huang, B. Yang, W. Yu, C. Li, X.-C. Yin, C.-L. Liu, L. Jin, and X. Bai. Ocrbench: on the hidden mystery of ocr in large multimodal models. Science China Information Sciences, 67 (12):220102, 2024c. P. Lu, H. Bansal, T. Xia, J. Liu, C. Li, H. Hajishirzi, H. Cheng, K.-W. Chang, M. Galley, and J. Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. arXiv preprint arXiv:2310.02255, 2023. MAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME, 2024. URL https://maa.org/math-competition s/american-invitational-mathematics-examination-aime. MAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME, 2025. URL https://maa.org/math-competition s/american-invitational-mathematics-examination-aime. K. Mangalam, R. Akshulakov, and J. Malik. Egoschema: diagnostic benchmark for very longform video language understanding. Advances in Neural Information Processing Systems, 36: 4621246244, 2023. A. Masry, D. X. Long, J. Q. Tan, S. Joty, and E. Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning. arXiv preprint arXiv:2203.10244, 2022. M. Mathew, D. Karatzas, and C. Jawahar. Docvqa: dataset for vqa on document images. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22002209, 2021. M. Mathew, V. Bagal, R. Tito, D. Karatzas, E. Valveny, and C. Jawahar. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 16971706, 2022. Infographicvqa. V. K. Nagaraja, V. I. Morariu, and L. S. Davis. Modeling context between objects for referring expression understanding. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part IV 14, pages 792807. Springer, 2016. OpenAI. Computer-using agent: Introducing universal interface for ai to interact with the digital world. 2025. URL https://openai.com/index/computer-using-agent. L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. L. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. F. Christiano, J. Leike, and R. Lowe. Training language models to follow instructions with human feedback. In NeurIPS, 2022. P. Padlewski, M. Bain, M. Henderson, Z. Zhu, N. Relan, H. Pham, D. Ong, K. Aleksiev, A. Ormazabal, S. Phua, et al. Vibe-eval: hard evaluation suite for measuring progress of multimodal language models. ArXiv preprint, abs/2405.02287, 2024. R. Paiss, A. Ephrat, O. Tov, S. Zada, I. Mosseri, M. Irani, and T. Dekel. Teaching clip to count to ten. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 31703180, 2023. R. Qiao, Q. Tan, G. Dong, M. Wu, C. Sun, X. Song, Z. GongQue, S. Lei, Z. Wei, M. Zhang, et al. We-math: Does your large multimodal model achieve human-like mathematical reasoning? arXiv preprint arXiv:2407.01284, 2024. Y. Qin, Y. Ye, J. Fang, H. Wang, S. Liang, S. Tian, J. Zhang, J. Li, Y. Li, S. Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025a. Y. Qin, Y. Ye, J. Fang, H. Wang, S. Liang, S. Tian, J. Zhang, J. Li, Y. Li, S. Huang, et al. Ui-tars: Pioneering automated gui interaction with native agents. arXiv preprint arXiv:2501.12326, 2025b. P. Rahmanzadehgervi, L. Bolton, M. R. Taesiri, and A. T. Nguyen. Vision language models are blind: Failing to translate detailed visual features into words, 2025. URL https://arxiv.or g/abs/2407.06581. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. H. Rezatofighi, N. Tsoi, J. Gwak, A. Sadeghian, I. Reid, and S. Savarese. Generalized intersection over union: metric and loss for bounding box regression. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 658666, 2019. Z. Shao, P. Wang, Q. Zhu, R. Xu, J. Song, X. Bi, H. Zhang, M. Zhang, Y. Li, Y. Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 23 G. Sheng, C. Zhang, Z. Ye, X. Wu, W. Zhang, R. Zhang, Y. Peng, H. Lin, and C. Wu. Hybridflow: flexible and efficient rlhf framework. ArXiv preprint, abs/2409.19256, 2024. URL https: //arxiv.org/abs/2409.19256. C. Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. P. Tong, E. Brown, P. Wu, S. Woo, A. J. V. IYER, S. C. Akula, S. Yang, J. Yang, M. Middepogu, Z. Wang, et al. Cambrian-1: fully open, vision-centric exploration of multimodal llms. Advances in Neural Information Processing Systems, 37:8731087356, 2024. K. Wang, J. Pan, W. Shi, Z. Lu, H. Ren, A. Zhou, M. Zhan, and H. Li. Measuring multimodal mathematical reasoning with math-vision dataset. Advances in Neural Information Processing Systems, 37:9509595169, 2024a. Y. Wang, X. Ma, G. Zhang, Y. Ni, A. Chandra, S. Guo, W. Ren, A. Arulraj, X. He, Z. Jiang, T. Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In A. Globersons, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. M. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024b. URL http://pape rs.nips.cc/paper_files/paper/2024/hash/ad236edc564f3e3156e1b2feafb99a2 4-Abstract-Datasets_and_Benchmarks_Track.html. Y. Wang, B. Xu, Z. Yue, Z. Xiao, Z. Wang, L. Zhang, D. Yang, W. Wang, and Q. Jin. Timezero: Temporal video grounding with reasoning-guided lvlm. arXiv preprint arXiv:2503.13377, 2025. Z. Wang, M. Xia, L. He, H. Chen, Y. Liu, R. Zhu, K. Liang, X. Wu, H. Liu, S. Malladi, et al. Charxiv: Charting gaps in realistic chart understanding in multimodal llms. Advances in Neural Information Processing Systems, 37:113569113697, 2024c. P. Wu and S. Xie. V*: Guided visual search as core mechanism in multimodal llms, 2023. URL https://arxiv.org/abs/2312.14135. Z. Wu, Z. Wu, F. Xu, Y. Wang, Q. Sun, C. Jia, K. Cheng, Z. Ding, L. Chen, P. P. Liang, et al. Os-atlas: foundation action model for generalist gui agents. arXiv preprint arXiv:2410.23218, 2024. Y. Xiao, E. Sun, T. Liu, and W. Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. L.-C.-T. Xiaomi. Mimo: Unlocking the reasoning potential of language modelfrom pretraining to posttraining. arXiv preprint arXiv:2505.07608, 2025. T. Xie, D. Zhang, J. Chen, X. Li, S. Zhao, R. Cao, T. J. Hua, Z. Cheng, D. Shin, F. Lei, et al. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094, 2024. T. Xie, J. Deng, X. Li, J. Yang, H. Wu, J. Chen, W. Hu, X. Wang, Y. Xu, Z. Wang, Y. Xu, J. Wang, D. Sahoo, T. Yu, and C. Xiong. Scaling computer-use grounding via user interface decomposition and synthesis, 2025. URL https://arxiv.org/abs/2505.13227. H. Xu, S. Xie, X. E. Tan, P.-Y. Huang, R. Howes, V. Sharma, S.-W. Li, G. Ghosh, L. Zettlemoyer, and C. Feichtenhofer. Demystifying clip data. arXiv preprint arXiv:2309.16671, 2023. 24 Y. Xu, Z. Wang, J. Wang, D. Lu, T. Xie, A. Saha, D. Sahoo, T. Yu, and C. Xiong. Aguvis: Unified pure vision agents for autonomous gui interaction. arXiv preprint arXiv:2412.04454, 2024. J. Ye, Z. Xie, L. Zheng, J. Gao, Z. Wu, X. Jiang, Z. Li, and L. Kong. Dream 7b, 2025. URL https://hkunlp.github.io/blog/2025/dream. L. Yu, P. Poirson, S. Yang, A. C. Berg, and T. L. Berg. Modeling context in referring expressions. In Computer VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part II 14, pages 6985. Springer, 2016. X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 95569567, 2024a. X. Yue, Y. Ni, T. Zheng, K. Zhang, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun, C. Wei, B. Yu, R. Yuan, R. Sun, M. Yin, B. Zheng, Z. Yang, Y. Liu, W. Huang, H. Sun, Y. Su, and W. Chen. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for expert AGI. In CVPR, pages 95569567, 2024b. X. Yue, T. Zheng, Y. Ni, Y. Wang, K. Zhang, S. Tong, Y. Sun, B. Yu, G. Zhang, H. Sun, et al. Mmmupro: more robust multi-discipline multimodal understanding benchmark. arXiv preprint arXiv:2409.02813, 2024c. K. Zhang, B. Li, P. Zhang, F. Pu, J. A. Cahyono, K. Hu, S. Liu, Y. Zhang, J. Yang, C. Li, and Z. Liu. Lmms-eval: Reality check on the evaluation of large multimodal models, 2024a. URL https://arxiv.org/abs/2407.12772. R. Zhang, D. Jiang, Y. Zhang, H. Lin, Z. Guo, P. Qiu, A. Zhou, P. Lu, K.-W. Chang, P. Gao, et al. Mathverse: Does your multi-modal llm truly see the diagrams in visual math problems? ArXiv preprint, abs/2403.14624, 2024b. Y.-F. Zhang, H. Zhang, H. Tian, C. Fu, S. Zhang, J. Wu, F. Li, K. Wang, Q. Wen, Z. Zhang, et al. Mme-realworld: Could your multimodal llm challenge high-resolution real-world scenarios that are difficult for humans? arXiv preprint arXiv:2408.13257, 2024c. L. Zheng, W. Chiang, Y. Sheng, S. Zhuang, Z. Wu, Y. Zhuang, Z. Lin, Z. Li, D. Li, E. P. Xing, H. Zhang, J. E. Gonzalez, and I. Stoica. Judging llm-as-a-judge with mt-bench and chatbot arena. In NeurIPS, 2023. J. Zhou, T. Lu, S. Mishra, S. Brahma, S. Basu, Y. Luan, D. Zhou, and L. Hou. Instruction-following evaluation for large language models, 2023. URL https://arxiv.org/abs/2311.07911. B. Zitkovich, T. Yu, S. Xu, P. Xu, T. Xiao, F. Xia, J. Wu, P. Wohlhart, S. Welker, A. Wahid, et al. Rt-2: Vision-language-action models transfer web knowledge to robotic control. In Conference on Robot Learning, pages 21652183. PMLR, 2023. C. Zou, X. Guo, R. Yang, J. Zhang, B. Hu, and H. Zhang. Dynamath: dynamic visual benchmark for evaluating mathematical reasoning robustness of vision language models. arXiv preprint arXiv:2411.00836, 2024."
        },
        {
            "title": "A Contributions and Acknowledgments",
            "content": "We would like to express our sincere gratitude to all contributors for their invaluable support and efforts, including the Xiaomi LLM-Plus, Mify, MiChat and CloudML teams, as well as those not explicitly listed in this paper. Authors within each role are listed in reverse order by their first names. Core Contributors Zihao Yue Zhenru Lin Yifan Song Weikun Wang Shuhuai Ren Shuhao Gu Shicheng Li Peidian Li Liang Zhao Lei Li Kainan Bao Hao Tian Hailin Zhang Gang Wang Dawei Zhu Cici Chenhong He Bowen Ye Bowen Shen Contributors Zihan Zhang Zihan Jiang Zhixian Zheng Zhichao Song Zhenbo Luo Yue Yu Yudong Wang Yuanyuan Tian Yu Tu Yihan Yan Yi Huang Xu Wang Xinzhe Xu Xingchen Song Xing Zhang Xing Yong Xin Zhang Xiangwei Deng Wenyu Yang Wenhan Ma Weiwei Lv Weiji Zhuang Wei Liu Sirui Deng Shuo Liu Shimao Chen Shihua Yu Shaohui Liu Shande Wang Rui Ma Qiantong Wang Peng Wang Nuo Chen Menghang Zhu Kangyang Zhou Kang Zhou Kai Fang Jun Shi Jinhao Dong Jiebao Xiao Jiaming Xu Huaqiu Liu Hongshen Xu Heng Qu Haochen Zhao Hanglong Lv Guoan Wang Duo Zhang Dong Zhang Di Zhang Chong Ma Chang Liu Can Cai Bingquan Xia 26 Model Configuration of MiMo-VL-7B In Table 4, the architecture and configuration of MiMo-VL-7B are detailed. Vision Encoder Language Model # Layers # Heads Hidden Size Intermediate Size Position Embedding Patch Size 32 16 1280 3456 2D RoPE 14 36 32 4096 11008 MRoPE (Bai et al., 2025b) - Table 4 Configuration of MiMo-VL-7B. We adopt Qwen2.5-ViT (Bai et al., 2025a) as our visual encoder to support native resolution inputs, and MiMo-7B-Base (Xiaomi, 2025) as our LLM backbone to leverage its strong reasoning capability. Compared to the LLM backbone of Qwen2.5VL-7B (Bai et al., 2025a), our LLM differs in its number of layers (36 vs. 28), hidden size (4096 vs. 3584), and intermediate size (11008 vs. 18944)."
        },
        {
            "title": "C Evaluation Benchmarks",
            "content": "We evaluate our models across 50 diverse tasks, including: General Visual Understanding AI2D (Kembhavi et al., 2016), BLINK (Fu et al., 2024b). CVBench (Tong et al., 2024), MMMU (Yue et al., 2024a), MMMU-Pro (Standard and Vision) (Yue et al., 2024c), Mantis (Jiang et al., 2024), MME-RealWorld (English and Chinese) (Zhang et al., 2024c), MMBench (English and Chinese) (Liu et al., 2024b), VibeEval (Padlewski et al., 2024), VLRewardBench (Li et al., 2024b), V* (Wu and Xie, 2023), and VLMs are Blind (Rahmanzadehgervi et al., 2025). General Grounding and Counting RefCOCO (Yu et al., 2016; Nagaraja et al., 2016; Kazemzadeh et al., 2014), CountBench (Paiss et al., 2023), and PixmoCount (Deitke et al., 2024) Document and Chart Understanding ChartQA (Masry et al., 2022), InfographicVQA (Mathew et al., 2022), DocVQA (Mathew et al., 2021), OCRBench (Liu et al., 2024c), SEED-Bench-2-Plus (Li et al., 2024a), and CharXiv (RQ/DQ) (Wang et al., 2024c). Video Understanding and Localization Video-MME (Fu et al., 2024a), Video-MMMU (Hu et al., 2025), EgoSchema (Mangalam et al., 2023), and Charades-STA Gao et al. (2017). GUI Understanding and Grounding WebSrc (Chen et al., 2021), VisualWebBench (Liu et al., 2024a), ScreenSpot (Cheng et al., 2024), ScreenSpot-V2 (Wu et al., 2024), ScreenSpot-Pro (Li et al., 2025), and OSWorld-G (Xie et al., 2025). Text-only Benchmarks GPQA (Rein et al., 2024), SuperGPQA (Du et al., 2025), DROP (Dua et al., 2019), MMLU-Pro (Wang et al., 2024b), and IFEval (Zhou et al., 2023). 27 Multimodal Reasoning OlympiadBench (He et al., 2024), MathVision (Wang et al., 2024a), MathVerse (Vision Only) (Zhang et al., 2024b), DynaMath (Zou et al., 2024), WeMath (Qiao et al., 2024), LogicVista (Xiao et al., 2024), and MathVista (Lu et al., 2023). Text Reasoning MATH500 (Hendrycks et al., 2021), AIME 2024 (MAA, 2024), and AIME 2025 (MAA, 2025)."
        },
        {
            "title": "D GUI Action Space",
            "content": "The syntax and definition for each action in the GUI action space are summarized in Table 5. click scroll input drag open press finished longpress hover select wait Syntax: {\"action\":\"click\",\"start_point\":[x,y],\"text\"(opt.):text} Definition: Click at (x, y) on an element with the given text. Syntax: {\"action\":\"scroll\",\"direction\":dir,\"scroll_distance\"(opt.):dist} Definition: Scroll in the given direction by specified distance. Notes: Scroll up/down to see more below/above. The same logic applies horizontally. Syntax: {\"action\":\"input\",\"text\":text,\"start_point\"(opt.):[x,y]} Definition: Type the specified text at coordinates (x, y). Syntax: {\"action\":\"drag\",\"start_point\":[x1,y1],\"end_point\":[x2,y2]} Definition: Drag from the start_point to the end_point. Syntax: {\"action\":\"open\",\"app\":app_name} Definition: Open the specified application app_name. Syntax: {\"action\":\"press\",\"keys\":[key1,key2,...]} Definition: Press the specified hotkeys ([key1, key2,...]). Syntax: {\"action\":\"finished\",\"status\":status} Definition: Mark the task as complete with given status. Syntax: {\"action\":\"longpress\",\"start_point\":[x,y]} Definition: Long press at coordinates (x,y). Syntax: {\"action\":\"hover\"} Definition: Hover the mouse over location. Syntax: {\"action\":\"select\",\"text\":text} Definition: Select the specified text. Syntax: {\"action\":\"wait\"} Definition: Pause for brief moment. appswitch Syntax: {\"action\":\"appswitch\",\"app\":app_name} Definition: Switch to the specified application app_name. Table 5 Action Space Details. opt. denotes optional."
        },
        {
            "title": "E More Qualitative Examples",
            "content": "28 Input image: Instruction: ÂÆåÊàêËØ•ËØïÂç∑ = 450 5 = 90 MiMo-VL-7B: <think>...</think> Á¨¨1È¢òÈõÜÂêàÁöÑ‰∫§ÈõÜ ÈõÜÂêàùê¥ = {0, 1, 2, 8}ùêµ = {1, 1, 6, 8}‰∫§ÈõÜ‰∏∫ÂêåÊó∂Â±û‰∫é‰∏§‰∏™ÈõÜÂêàÁöÑÂÖÉÁ¥†Âç≥ùê¥ ùêµ = {1, 8} Á¨¨2È¢òÂ§çÊï∞ÁöÑÂÆûÈÉ® ËÆæùëß = ùë• + ùë¶ùëñùë•, ùë¶ ‰∏∫ÂÆûÊï∞Áî±ùëñ ùëß = 1 + 2ùëñÂç≥ùëñ(ùë• + ùë¶ùëñ) = ùë¶ + ùë•ùëñ = 1 + 2ùëñÂàÜÁ¶ªÂÆûÈÉ®ÂíåËôö ÈÉ®ùë¶ = 1ÂÆûÈÉ®ùë• = 2ËôöÈÉ®ÊïÖùëß ÁöÑÂÆûÈÉ®‰∏∫2 Á¨¨3È¢òËåéÂè∂ÂõæÁöÑÂπ≥ÂùáÊï∞ ËåéÂè∂Âõæ‰∏≠Ëåé‰∏∫ÂçÅ‰ΩçÂè∂‰∏∫‰∏™‰ΩçÂàÜÊï∞‰∏∫89, 89, 90, 91, 91ÂÖ±5‰∏™Âπ≥ÂùáÊï∞ ‰∏∫ 89+89+90+91+91 5 Á¨¨4È¢ò‰º™‰ª£Á†ÅÁöÑÂæ™ÁéØÁªìÊûú ÂàùÂßãÂåñùêº = 1ùëÜ = 1Âæ™ÁéØÊù°‰ª∂ùêº < 6Á¨¨1Ê¨°Âæ™ÁéØùêº = 1 + 2 = 3ùëÜ = 2 1 = 2Á¨¨2Ê¨°Âæ™ÁéØùêº = 3 + 2 = 5ùëÜ = 2 2 = 4Á¨¨3Ê¨°Âæ™ÁéØùêº = 5 + 2 = 7‰∏çÊª°Ë∂≥ùêº < 6ÈÄÄÂá∫Âæ™ÁéØÊúÄÁªàùëÜ = 8 Á¨¨5È¢òÂáΩÊï∞ÁöÑÂÆö‰πâÂüü ÂáΩÊï∞ ùëì (ùë•) = log2 ùë• 1 ÈúÄÊª°Ë∂≥1. ‰∫åÊ¨°Ê†πÂºèÂÜÖÈùûË¥ülog2 ùë• 1 02. ÂØπÊï∞ÊúâÂÆö ‰πâùë• > 0Áî±log2 ùë• 1 Âæóùë• 21 = 2ÊïÖÂÆö‰πâÂüü‰∏∫[2, +) Á¨¨6È¢òÁªÑÂêàÁöÑÊ¶ÇÁéá 2Áî∑3Â•≥‰∏≠ÈÄâ2‰∫∫ÊÄªÈÄâÊ≥ïùê∂2 Á¨¨7È¢òÊ≠£Âº¶ÂáΩÊï∞ÁöÑÂØπÁß∞ËΩ¥ ÂáΩÊï∞ùë¶ = sin(2ùë• + ùúë) ÂÖ≥‰∫éùë• = ùúã Âæóùúë = ùúã 2 6 ùë¶2 Á¨¨8È¢òÂèåÊõ≤Á∫øÁöÑÁ¶ªÂøÉÁéá ÂèåÊõ≤Á∫ø ùë•2 ùëè2 = 1Âè≥ÁÑ¶ÁÇπùêπ(ùëê, 0)ùëê = ËøëÁ∫øÁöÑË∑ùÁ¶ª‰∏∫ ùëèùëê 3 2 ùëêÁªìÂêàùëê2 = ùëé2 + ùëè2Âæóùëé = ùëê Á¨¨9È¢òÂë®ÊúüÂáΩÊï∞‰∏éÂ§çÂêàÂáΩÊï∞ ùëì (ùë• + 4) = ùëì (ùë•) ËØ¥ÊòéÂë®Êúü‰∏∫4ùëì (15) = ùëì (15 4 3) = ùëì (3) = ùëì (1)3 = 1 + 4ùë• (2, 0] Êó∂ ùëì (1) = 1 + 1 2 - ùëì ( 1 Á¨¨10È¢òÂ§öÈù¢‰ΩìÁöÑ‰ΩìÁßØ Ê≠£Êñπ‰ΩìÊ£±Èïø2Èù¢‰∏≠ÂøÉÂùêÊ†á‰∏∫(1, 0, 0), (0, 1, 0), (0, 0, 1)ÊûÑÊàêÊ≠£ÂÖ´Èù¢‰ΩìÊ≠£ÂÖ´Èù¢ 2È´ò1‰ΩìÁßØ‰∏∫2 1 ‰Ωì‰ΩìÁßØ‰∏∫‰∏§‰∏™ÂõõÊ£±Èî•‰πãÂíåÂ∫ïÈù¢Ê≠£ÊñπÂΩ¢ËæπÈïø Á¨¨11È¢òÂáΩÊï∞ÁöÑÊûÅÂÄº‰∏éÈõ∂ÁÇπ ÂáΩÊï∞ ùëì (ùë•) = 2ùë•3 ùëéùë•2 +1 Âú®(0, +) ÂÜÖÊúâ‰∏îÂè™Êúâ‰∏Ä‰∏™Èõ∂ÁÇπÂØºÊï∞ ùëì (ùë•) = 6ùë•2 2ùëéùë• ‰∏¥ÁïåÁÇπùë• = 0 Âíåùë• = ùëé 3 ) = 0 Êó∂ÊûÅÂ∞èÂÄº‰∏∫0‰ªÖ1‰∏™Èõ∂ÁÇπËß£Âæóùëé = 3 ùëì (ùë•) Âú®[1, 1] ‰∏ä ùëì (0) = 1ÊúÄÂ§ßÂÄº ùëì (1) = 4ÊúÄÂ∞èÂÄºÂíå‰∏∫1 + (4) = 3 Á¨¨12È¢òÂêëÈáèÂûÇÁõ¥ÁöÑÊù°‰ª∂ ËÆæùê¥(ùë•, 2ùë•)ùê¥ùêµ = (5 ùë•, 2ùë•)ÂúÜÂøÉùê∂ ùëé2 + ùëè2Ê∏êËøëÁ∫øùëèùë• ùëéùë¶ = 0ÁÑ¶ÁÇπÂà∞Ê∏ê 2 Á¶ªÂøÉÁéáùëí = ùëê 5 = 10ÈÄâ‰∏≠2Â•≥ÁîüÁöÑÈÄâÊ≥ïùê∂2 3 ÂØπÁß∞Âç≥2 ùúã ùëé2 ùëé2+ùëè2 = ùëèÁî±È¢òËÆæùëè = (cid:17)Âè¶‰∏Ä‰∫§ÁÇπùê∑(1, 2)Áî±ÂúÜ‰∏éÁõ¥Á∫ø‰∫§ 3 = 3Ê¶ÇÁéá‰∏∫ 3 3 + ùúë = ùëòùúã + ùúã 2 ùëò ZÂèñùëò = 0 2 ) = cos (cid:16) ùúã 3 2 1 = 4 2 = 1 2 (0, 2] 2 < ùúë < ùúã 3 ÂΩì ùëì ( ùëé 2 2 1 Êª°Ë∂≥ ùúã ùëé = 2 = cos ùúã 2 1 2 10 (cid:16) ùë•+5 3 4 = (cid:17) 2 , ùë• ùê∂ ùê∑ = (cid:16) ùë•3 2 ÁÇπÂîØ‰∏ÄÊÄßÂæóÂêëÈáè Á≠îÊ°à 1. {1, 8} 2. 2 3. 90 4. 8 5. [2, +) 6. 3 10 , 2 ùë• (cid:17)Áî± ùê¥ùêµ ùê∂ ùê∑ = 0Ëß£Âæóùë• = 3 7. ùùÖ 6 8. 2 9. 2 10. 4 3 11. 3 12. 3 Figure 10 Example of MiMo-VL-7B-RL answering all questions within whole test paper. 29 Instruction: ËøôÁì∂ÂèØ‰πêÂñù10ÁΩê‰ºöÊëÑÂÖ•Â§öÂ∞ëÂÖãÈí† MiMo-VL-7B-RL: <think>...</think>Ë¶Å ËÆ° ÁÆóÂñù10ÁΩêÂèØÂè£ÂèØ‰πêÈõ∂Â∫¶Êó∂ÊëÑÂÖ•ÁöÑÈí†ÈáèÈúÄÊåâ‰ª• ‰∏ãÊ≠•È™§ÂàÜÊûê 1. Ê≠•È™§1ÊòéÁ°ÆËê•ÂÖªÊàêÂàÜË°®‰ø°ÊÅØ ‰ªéÂõæÁâá‰∏≠Ëê• ÂÖªÊàêÂàÜË°®ÂèØÁü•ÊØè100ÊØ´ÂçáËØ•È•ÆÊñô‰∏≠Èí† ÁöÑÂê´Èáè‰∏∫18ÊØ´ÂÖã 2. Ê≠• È™§2 Á°Æ ÂÆö Âçï ÁΩê È•Æ Êñô ÂÆπ Èáè Ë°å ‰∏ö Â∏∏ ËßÅ ËßÑ Ê†º ÂèØ Âè£ ÂèØ ‰πê Èõ∂ Â∫¶ ÁöÑ ÁΩê Ë£Ö ËßÑ Ê†º ÈÄö Â∏∏ ‰∏∫330ÊØ´Âçá/ÁΩêËøôÊòØÈ•ÆÊñôË°å‰∏öÁöÑÂ∏∏ËßÅÁΩê Ë£ÖÂÆπÈáèËã•ÂÆûÈôÖÂÆπÈáè‰∏çÂêåÈúÄ‰ª•‰∫ßÂìÅÊ†áÊ≥®‰∏∫ ÂáÜÊ≠§Â§ÑÈªòËÆ§330ml 3. Ê≠•È™§3ËÆ°ÁÆóÂçïÁΩêÈí†Âê´Èáè ÂçïÁΩê330ml ‰∏≠Èí†ÁöÑÂê´Èáè= ÊØè100mlÈí†Âê´Èáè ÂçïÁΩêÂÆπÈáè 100 Âç≥18 ÊØ´ÂÖã/100ml 330 ml 100 = 59.4 ÊØ´ÂÖã 4. Ê≠• È™§4 ËÆ° ÁÆó10ÁΩê Èí† ÊÄª Èáè 10ÁΩê Èí† ÊÄª Èáè= Âçï ÁΩê Èí† Âê´ Èáè 10 Âç≥ 59.4 ÊØ´ÂÖã 10 = 594 ÊØ´ÂÖã Âõ† Ê≠§ Âñù10ÁΩê ËØ• Ê¨æ ÂèØ Âè£ ÂèØ ‰πê Èõ∂ Â∫¶ Á∫¶ ÊëÑ ÂÖ•594ÊØ´ÂÖã Èí† Instruction: ËØ•ÂèëÁ•®Áº∫Â§±ÁöÑÁ®éÈ¢ùÂ∫îËØ•ÊòØÂ§öÂ∞ë MiMo-VL-7B-RL: <think>...</think>Ë¶Å ËÆ° ÁÆóËØ•ÂèëÁ•®ÁöÑÁ®éÈ¢ùÈúÄÁªìÂêàÁ®éÁéáÂíå‰ª∑Á®éÂêàËÆ°ÈáëÈ¢ù Êù•Êé®ÂØº 1. Ê≠• È™§1 Êòé Á°Æ ÂÖ≥ ÈîÆ ‰ø° ÊÅØ - Âèë Á•® Á®é Áéá13%ÂèëÁ•®‰∏≠Á®éÁéáÊ†èÊòæÁ§∫‰ª∑Á®éÂêà ËÆ°ÈáëÈ¢ùÂ§ßÂÜôÂ£πÊãæÂ£π‰∏áË¥∞‰ªü‰ºç‰Ω∞ÊçåÊãæ ÁéñÂúÜ‰ºçËßíË¥∞ÂàÜÂØπÂ∫îÂ∞èÂÜô‰∏∫112589.52ÂÖÉ 2. Ê≠•È™§2Á®éÈ¢ùÁöÑËÆ°ÁÆóÈÄªËæë ÂèëÁ•®Á®éÈ¢ù= ‰ª∑Á®é ÂêàËÆ°ÈáëÈ¢ù 1 - Á®éÁéá Á®éÁéáÂç≥ÊÄª‰ª∑ ‰ª∑Á®éÂêàËÆ°= ‰∏çÂê´Á®éÈáëÈ¢ù+ Á®éÈ¢ù Á®éÈ¢ù= ‰ª∑ Á®éÂêàËÆ° Á®éÁéá (1 + Á®éÁéá) ËÆæ ‰∏ç Âê´ Á®é Èáë È¢ù ‰∏∫ ùëã Âàô ùëã + ùëã 13% = 112589.52 ùëã 1.13 = 112589.52 ùëã = 112589.52 1.13 = 99636.74 ÂÖÉ‰∏çÂê´Á®éÈáë È¢ù Á®éÈ¢ù= ‰∏çÂê´Á®éÈáëÈ¢ù Á®éÁéá= 99636.74 13% = 12952.78 ÂÖÉ Figure 11 Example of MiMo-VL-7B-RL solving complex OCR problems with reasoning. 30 Instruction: How many muffin in the image? MiMo-VL-7B-RL: <think>...</think> To determine the number of muffins in the image, we analyze the 34 grid (12 cells total) and count muffins row by row: - Top row (row 1): Columns 1 and 3 are muffins 2 muffins. - Middle row (row 2): Columns - Bottom 2 and 4 are muffins 2 muffins. row (row 3): Columns 1 and 3 are muffins 2 muffins. Adding these, 2 + 2 + 2 = 6. There are 6 muffins in the image. Instruction: How many cats in the image? MiMo-VL-7B-RL: <think>...</think> To determine the number of cats in the image, we can observe and count: - There is black cat lying on the floor. - There is white and brown cat sitting behind the black cat. Thus, the number of cats in the image is 2. Instruction: How many dogs in the image? MiMo-VL-7B-RL: <think>...</think> To determine the number of dogs in the image, we analyze both the printed dogs on the blanket and the real dog in the foreground: 1. Printed dogs on the blanket: The blanket features grid of dog faces. Counting these, we see 3 rows with 4 dogs each, so 3 4 = 12 printed dogs. 2. Real dog in the foreground: There is 1 real dog standing in front of the blanket. Adding these together, 12 + 1 = 13. Thus, the total number of dogs in the image is 13. Figure 12 Example of MiMo-VL-7B-RL solving detailed perception problem. Instruction: answer all the questions. MiMo-VL-7B-RL: <think>...</think> 1. Problem 7 To find ùë• and ùë¶, we analyze the two right triangles formed by the height: - **Right triangle (30 angle)**: Hypotenuse = 44. The height ‚Ñé (opposite 30) is 44 sin(30) = 44 0.5 = 22. The base segment ùë¶ (adjacent to 30) is 44 cos(30) = 44 - **Right triangle (45 angle)**: This is 45-45-90 triangle, so legs are equal. The height ‚Ñé = 22, so the other leg (left base segment) is also 22. Hypotenuse ùë• (45-45-90 ratio: leg 3 2 = 22 2) is 2. 3. 2. Problem 8 For square with diagonal ùëë = 22, the side length ùë† relates 2 2 = 2. Solving for ùë†: ùë† = ùëë to the diagonal by ùëë = ùë† (rationalizing the denominator). 2 = 22 3. Problem 9 pentagon circumscribed about circle (tangential pentagon) has the property that the sum of the lengths of all sides equals 2 the sum of the tangent segments from each vertex. However, since the problem provides the side lengths ùëÑùëç = 10, ùëå ùëã = 9, ùëãùëä = 13, ùëàùëä = 16, ùëÜùëà = 17, and the pentagon is tangential, the perimeter is the sum of these sides (as they represent the sides of the pentagon). Perimeter = 10 + 9 + 13 + 16 + 17 = 65. 4. Problem 10 ùëÜùëÖ is tangent to ùëÑ at ùëÖ, so ùëÑùëÖ ùëÜùëÖ (tangent is perpendicular to radius). Thus, ùëÑùëÖùëÜ is right triangle with ùëÑùëÖ = 7 (radius) and ùëÑùëÜ = 25 (hypotenuse). By the Pythagorean theorem: ùëÜùëÖ = ùëÑùëÜ2 ùëÑùëÖ2 = 625 49 = 252 72 = 576 = 24 5. Problem 11 perpendicular from the circles center to chord bisects the chord. The chord length is 16, so half the chord is 16 2 = 8. The distance from the center to the chord is 6, forming right triangle with legs 6 and 8, and hypotenuse ùë• (radius). By the 62 + 82 = Pythagorean theorem: ùë• = 36 + 64 = 100 = 10 6. Problem 12 Since ùëÖ, ùëÜ, ùëá are collinear and ùëÜ is between ùëÖ and ùëá, ùëÖùëÜ + ùëÜùëá = ùëÖùëá. Substitute ùëÖùëÜ = 2ùë§ + 1, ùëÜùëá = ùë§ 1, and ùëÖùëá = 18: (2ùë§ + 1) + (ùë§ 1) = 18 Simplify: 3ùë§ = 18 = ùë§ = 6. Substitute ùë§ = 6 into ùëÖùëÜ: ùëÖùëÜ = 2(6) + 1 = 13 Final Answers 7. ùë• = 22 ùë§ = 6, ùëÖùëÜ = 13 2 9. 65 10. 24 11. 10 12. 2, ùë¶ = 3 8. 11 Figure 13 Example of MiMo-VL-7B-RL solving multiple geometry problems."
        }
    ],
    "affiliations": [
        "Xiaomi"
    ]
}