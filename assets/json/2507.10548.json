{
    "paper_title": "EmbRACE-3K: Embodied Reasoning and Action in Complex Environments",
    "authors": [
        "Mingxian Lin",
        "Wei Huang",
        "Yitang Li",
        "Chengjie Jiang",
        "Kui Wu",
        "Fangwei Zhong",
        "Shengju Qian",
        "Xin Wang",
        "Xiaojuan Qi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advanced vision-language models(VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings, which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from a first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. To address this gap, we introduce EmRACE-3K, a dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass a wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as a multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agent's intent at every step. Using EmRACE-3K, we establish a benchmark to evaluate the embodied reasoning capabilities of VLMs across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the dataset's effectiveness in enabling the development of embodied reasoning capabilities."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 8 4 5 0 1 . 7 0 5 2 : r EmbRACE-3K: Embodied Reasoning and Action in Complex Environments Mingxian Lin1* Wei Huang1* Yitang Li2 Chengjie Jiang3 Kui Wu4 Fangwei Zhong4 Shengju Qian3 Xin Wang3 Xiaojuan Qi1 1The University of Hong Kong 2Tsinghua University 3LIGHTSPEED 4Beijing Normal University https://mxllc.github.io/EmbRACE-3K/"
        },
        {
            "title": "Abstract",
            "content": "Recent advanced vision-language models (VLMs) have demonstrated strong performance on passive, offline image and video understanding tasks. However, their effectiveness in embodied settings which require online interaction and active scene understanding remains limited. In such scenarios, an agent perceives the environment from first-person perspective, with each action dynamically shaping subsequent observations. Even state-of-the-art models such as GPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro struggle in open-environment interactions, exhibiting clear limitations in spatial reasoning and long-horizon planning. These limitations are further emphasized by our empirical analysis of modern VLMs, which reveals consistent failure modes when applied to embodied tasks. To address this gap, we introduce EmbRACE-3K, dataset of over 3,000 language-guided tasks situated in diverse, photorealistic environments constructed using Unreal Engine and the UnrealCV-Zoo framework. The tasks encompass wide range of embodied challenges, including navigation, object manipulation, and multi-stage goal execution. Each task unfolds as multi-step trajectory, pairing first-person visual observations with high-level instructions, grounded actions, and natural language rationales that express the agents intent at every step. This design results in fine-grained, temporally grounded annotations that closely align perception with decision-making. In total, the dataset contains approximately 26,000 decision steps, each annotated with multimodal context and step-wise reasoning. Using EmbRACE-3K, we establish benchmark to evaluate the embodied reasoning capabilities of VLMs such as GPT-4o, Gemini 2.5 Pro, and Qwen2.5-VL-7B, across three key dimensions: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot settings, all models achieve success rates below 20%, underscoring the challenge posed by our benchmark and the current limitations of VLMs in interactive environments. To demonstrate the utility of EmbRACE-3K, we further fine-tune Qwen2.5-VL-7B using supervised learning followed by reinforcement learning. This approach yields substantial improvements across all three challenge categories, highlighting the datasets effectiveness in enabling the development of embodied reasoning capabilities."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in vision-language models (VLMs) have led to strong performance across wide range of offline, passive understanding tasks, including image captioning, video summarization, * Equal Contribution, Project Lead, Corresponding Author. Preprint. Under review. Figure 1: EmbRACE-3K dataset. The dataset contains over 3k language-guided tasks and 26k decision steps set in diverse, photorealistic environments. Each task involves high-level natural language instructions, grounded actions (e.g., move, turn, look, interact), egocentric visual observations, and step-wise reasoning. Agents interpret visual inputs and follow instructions to execute multi-step decision trajectories, with each step annotated with natural language rationalesforming coherent and interpretable decision process over long horizon that involves spatial reasoning. and visual question answering. State-of-the-art models such as GPT-4o [17], Gemini 2.5 Pro [6, 7], Claude-3.5-Sonnet [2], and Qwen2.5-VL [3] demonstrate impressive capabilities in aligning visual and linguistic information, particularly when operating on pre-recorded image or vision sequences in static, non-interactive settings. However, these models often fall short when applied to embodied tasks, where an agent must actively perceive, reason, and act within an interactive environment [5, 12]. Unlike passive vision benchmarks, embodied scenarios involve closed-loop perception-action cycle: what the agent sees next is determined by the actions it takes now. single turn or misstep can dramatically alter subsequent observations. In this dynamic, egocentric setting, agents must follow high-level instructions, adapt to constantly shifting visual inputs, and make temporally coherent decisions under partial observability. This tight coupling between perception and action introduces challenges that go far beyond object recognition or static scene understanding, requiring reasoning over how decisions shape future inputs posing fundamentally different learning problem. Despite this fundamental shift, existing VLMs are often deployed in embodied settings without structural adaptation. In practice, they process short video clips or image sequences as static input, ignoring the dynamic nature of egocentric interaction. This results in training-deployment mismatch. In our preliminary experiments with Qwen2.5-VL and GPT-4o in simulated embodied environments, we observed consistent failure patterns: the models tend to overfit to immediate visual cues, fail to adjust spatial reasoning as the viewpoint changes, and struggle to maintain attention on objects that briefly exit the field of view. These issues underscore the limitations of passive pretraining when applied to sequential, interactive decision-making tasks. To address this gap, we introduce EmbRACE-3K: Embodied Reasoning and Action in Complex Environments (shown in Figure 1). This dataset comprises over 3,000 language-guided tasks collected in diverse, photorealistic Unreal Engine environments and controlled via the UnrealCV-Zoo framework. Each task unfolds as multi-step trajectory in which the agent receives high-level instruction and interacts with the environment through vision and action. In total, EmbRACE-3K includes approximately 26,000 decision steps, each annotated with egocentric visual observations, the selected action, and natural language thinking rationale that explains the agents intent. 2 Unlike prior datasets, EmbRACE-3K is built to capture the causal structure of embodied interaction by explicitly modeling how decisions affect perception, and how perception guides subsequent reasoning. It provides fine-grained, step-level annotations that align not only with what the agent observes and does, but also with why it acts. These annotations include intermediate reasoning steps, enabling models to learn perception-conditioned decision making rather than relying solely on end-to-end action prediction. Crucially, EmbRACE-3K supports online, closed-loop interaction, where each action taken by the agent dynamically changes the environment and influences future observations. This setup allows for realistic, temporally extended evaluation under partial observability, supporting spatial-semantic consistency and long-horizon goal pursuit. Compared to prior work such as Octopus [28], which formulates reasoning at the level of code generation without step-wise visual grounding, or datasets like ALFRED [23] that rely on pre-defined trajectories, EmbRACE-3K enables fine-grained, multimodal alignment between perception, language, reasoning, and action. This makes it strong foundation for evaluating and training embodied agents that not only act, but also interpret, reason, and adapt over time in photo-realistic simulated environments. By shifting the focus from passive visual comprehension to instruction-guided, step-wise reasoning, EmbRACE-3K enables the training and evaluation of vision-language agents in goal-oriented embodied scenarios. First, we use EmbRACE-3K to establish benchmark targeting three core embodied capabilities: Exploration, Dynamic Spatial-Semantic Reasoning, and Multi-stage Goal Execution. In zero-shot evaluations, state-of-the-art models GPT-4o, Gemini 2.5 Pro, and Qwen2.5-VL-7Bachieve success rates below 20% on all three tasks, revealing significant gap between current VLM capabilities and the demands of embodied reasoning. Next, we fine-tune Qwen2.5-VL-7B on EmbRACE-3K using two-stage approach: supervised fine-tuning (SFT) followed by reinforcement learning (RL). This significantly boosts the models performance across all tasks, resulting in higher success rates and lower goal distance error (GDE) compared to GPT-4o and Gemini 2.5 Pro. These results validate the effectiveness of EmbRACE-3K in enabling VLMs to acquire embodied reasoning capabilities. Moreover, we observe that models trained with SFT alone perform well on in-domain tasks, but suffer marked performance degradation on out-of-domain scenarios. This limited generalization highlights the importance of reinforcement-based adaptation for improving robustness in unfamiliar environments. It also underscores the pressing need for highquality, interaction-centric datasets to support training in such settings an area where EmbRACE-3K is particularly well-positioned to contribute."
        },
        {
            "title": "2 Related Work",
            "content": "Vision-Language Models Recent advancements in vision-language models (VLMs) have demonstrated remarkable progress in both architectural innovations and performance. For example, GPT4o [18] has achieved significant improvements in visual understanding by leveraging enhanced reasoning capabilities. Similarly, Gemini Pro 1.5 [24] has extended the context length to an unprecedented 1 million tokens, positioning itself as leader on long video benchmarks [9]. The open-source VLM ecosystem has also seen substantial growth, driven by improvements in model architecture and training methodologies. Notable contributions include state-of-the-art models such as InternVL3 [34], Qwen2.5-VL [26], LLaVA-OneVision [13], and Llama3.2-Vision [16]. These advancements have significantly narrowed the performance gap between open-source and proprietary VLMs, with many open-source models now achieving competitive results. Embodied Tasks for VLM Recent efforts in embodied vision-language modeling have targeted diverse forms of perception, control, and supervision. Octopus [28] leverages code synthesis to bridge language and action planning, incorporating environmental feedback in simulation. SayCan [1] grounds language in executable actions using affordance-based filtering. Ego4D [10] focuses on passive egocentric video understanding without action execution, while TEACh [19] centers on dialog-driven task planning in multi-agent settings. Octopi [31] extends grounded reasoning to the tactile modality. Despite these advances, many benchmarks lack fine-grained supervision, online interaction, or visual realism. EmbRACE-3K addresses these limitations by providing fully stepwise, spatio-temporally grounded, and closed-loop evaluation framework built in photo-realistic Unreal Engine environments. Each decision step is paired not only with egocentric observations and grounded actions, but also with explicit reasoning annotations that capture the agents intent and intermediate thinking process. This enables more interpretable and diagnostic evaluation of vision-language models in long-horizon embodied tasks. 3 Table 1: Comparison of Embodied Benchmarks for VLM Evaluation across Multiple Dimensions Spatio-Temporal Aware Online Closed-loop Benchmark"
        },
        {
            "title": "Scene",
            "content": "ALFRED [23] Octopus [28] HabitatNav [25] MindCube [30] V-IRL [27] VSI [29] MCU [14] Language&Vision Language&Vision Vision Vision Language&Vision Vision Language&Vision Indoor Indoor&Outdoor Indoor Indoor Outdoor Indoor Indoor&Outdoor Photo-Realistic Game-based Real-world Real-world Real-world Real-world Game-based EmbRACE-3K Language&Vision Indoor&Outdoor Photo-Realistic"
        },
        {
            "title": "3 Pilot Study",
            "content": "Step-wise We conducted set of preliminary evaluations using GPT-4o and Qwen2.5-VL in simulated embodied environments. Despite architectural differences, both models exhibited similar failure patterns when tasked with step-wise, instruction-driven tasks. These observations highlight fundamental limitations of current video-trained VLMs in embodied settings. (i) Short-sighted exploration. The models tend to fixate on immediate visual cues, lacking the ability to plan toward long-term goals. In the task \"Locate and approach the red car\", for instance, agents briefly glance left, find no immediate evidence of the target, then glance right with the same result, and promptly proceed forward without conducting broader search. This behavior suggests that each movement is selected based on local visual feedback rather than an integrated exploration strategy. In \"Find the plant near the shelf\", agents abandon shelf traversal after only few head turns, missing nearby objects outside their immediate field of view. This limitation can be traced to how VLMs are typically trained. In conventional video datasets, the model passively receives visual input and learns to summarize or answer questions based on full-sequence observations. There is no need to actively decide where to look or how to explore, so these models never acquire the capacity for information-seeking behavior. (ii) Dynamic spatial-semantic drift. The interpretation of spatial relations becomes unstable as the agent moves, due to lack of egocentric pose awareness. In tasks like \"Approach the second trash can\" or \"Go to the white house in front\", agents initially respond correctly to spatial cues but fail to adapt as their viewpoint changes. Ordinal and directional terms such as \"second\" and \"front\" become detached from the agents current orientation, leading to consistent semantic misalignment. This issue arises because most VLMs are trained on static or loosely time-linked image-text data. Even in video-based pretraining, spatial reasoning is often limited to temporal QA, captioning, or event ordering, where egocentric position and spatial frame shifts are rarely encoded. As result, these models rely on static geometric assumptions that do not update as the agent moves, causing gradual drift in language grounding. (iii) Target forgetting. Models often fail to retain intent beyond the current frame, leading to target loss when objects briefly leave the field of view. For instance, in \"Go near the red car\", temporary disappearance of the car results in its permanent omission. Similarly, in multi-stage instructions like \"First reach the trash can, then go to the red car\", the agent frequently completes the first task but fails to recall the second goal after unrelated actions. This issue stems from the pretraining objectives of most video-language datasets, which emphasize frame-level recognition, counting, or sequence-level QA, rather than persistent target awareness over time. As result, sudden appearances or disappearances of objects are not treated as meaningful, leaving the model unable to track unseen but relevant entities. These behaviors expose critical gaps in current VLM capabilities and motivate the design of EmbRACE-3K to support better spatial reasoning, goal continuity, and instruction-grounded action planning."
        },
        {
            "title": "4 Data Collection and Benchmark Construction",
            "content": "To support systematic investigation of embodied vision-language reasoning, we construct EmbRACE-3K: Embodied Reasoning and Action in Complex Environments. This benchmark is designed to expose the structural mismatch between passively trained VLMs and the demands of step-wise, instruction-driven embodied tasks. Informed by our pilot study, which highlighted issues 4 Figure 2: Multi-stage Embodied Task Data Collection Pipeline. The EmbRACE-3K dataset is built in four stages: (1) sampling diverse 6-DoF agent poses with ego views in virtual environments, (2) generating grounded task instructions using Gemini, (3) collecting human demonstrations, and (4) annotating each action with step-wise natural language reasoning to explain agent decisions and enhance interpretability. such as short-sighted action selection, spatial-semantic misalignment, and goal forgetting, we develop multi-stage data pipeline (shown in Figure. 2) that provides fine-grained supervision aligned with closed-loop reasoning."
        },
        {
            "title": "4.1 Simulation Platform and Environmental Diversity",
            "content": "All data in EmbRACE-3K are collected within the UnrealCV-Zoo framework [33], which extends Unreal Engine with first-person control and low-level API access. From 100 available photorealistic environments, we select 24 diverse maps that span indoor and outdoor settings, varying in object density, spatial topology, lighting, and navigational complexity. This diversity enables robust evaluation of generalization across scene types and task variations."
        },
        {
            "title": "4.2 Multi-stage Embodied Task Data Collection",
            "content": "Our data collection proceeds in four structured stages, designed to capture the full perceptionreasoning-action loop required for interactive embodied tasks. This process emphasizes both scene diversity and reasoning complexity to facilitate robust agent training and evaluation. Stage 1: Environment Sampling and Pose Selection We begin by sampling diverse agent poses across selected simulation maps using hybrid strategy that combines automated scripts and manual inspection. The automated component leverages the Navigation Movement API provided by Unreal Engine to uniformly explore traversable regions. To ensure data quality, sampled positions undergo manual validation to filter out visually trivial locations (e.g., texture-less walls) or physically unreachable areas (e.g., positions blocked by obstacles or un-navigable terrain). Each selected pose is recorded with full 6-DoF (degrees of freedom) coordinatesincluding position and orientation as well as the corresponding egocentric RGB image captured from the agents first-person perspective. Stage 2: Task Instruction Generation For each selected agent pose, we retrieve object-level metadata within 1000 meter radius, including the semantic names and spatial positions of nearby objects. This contextual information, along with the egocentric RGB view captured at the pose, is provided to the Gemini 2.5 pro model to generate natural language task instructions. The model is explicitly conditioned on the spatial layout and visual context to ensure semantic grounding, producing instructions that are both plausible and solvable within the local environment. We also inform the model of the desired task type prior to generation, guiding it to produce tasks aligned with one of five categories identified in our pilot analysis of embodied reasoning challenges: 5 Figure 3: The Token Number and Word-Cloud Distribution of EmbRACE-3K. (a)Token number distribution of reasoning and the length distribution of action trajectories. (b)The word clouds of task instructions and agent thinking processes in EmbRACE-3K. 0. Basic: Target is clearly visible and immediately reachable, requiring minimal reasoning. 1. Exploration: Target is initially out of view, prompting the agent to perform an active search. 2. Dynamic Spatial-Semantic: Target is described using relative or ordinal spatial references. 3. Multi-stage: Task requires completing series of subgoals in specific order. 4. Interaction: Task requires direct manipulation (e.g., open door, pick or drop an object). To ensure quality and diversity, all generated instructions undergo post-processing stage that includes manual verification and targeted manual authoring. Annotators inspect generated instructions to verify consistency with visual and spatial context, correct ambiguous phrasing, and supplement the dataset with novel, human-authored tasks for underrepresented cases. This hybrid generation-and-curation pipeline ensures both scalability and high-quality alignment with embodied agent capabilities. Stage 3: Human Demonstration and Trajectory Capture Each generated instruction is performed by human player controlling the agent in real time. We record all egocentric frames, executed actions, and precise pose trajectories. These demonstrations provide high-quality behavioral examples that encode closed-loop dependencies between perception, action context, and intent. The resulting action sequences are typically sparse and efficient, reflecting realistic strategies for exploration and goal completion. Stage 4: Step-wise Reasoning Annotation To improve interpretability and facilitate cognitive supervision, we annotate each step in the demonstration trajectory with natural language explanations of the chosen actions. Unlike traditional chain-of-thought (CoT) [21] methods that focus on isolated frames, our annotations are grounded in the agents egocentric perspective and the full task context. Gemini receives the task instructions, complete egocentric views, and the entire action trajectory, enabling holistic reasoning about how each action contributes to the final goal and influences future observations. These explanations capture not only the action taken but also its relevance to the spatial structure, task dynamics, and overall intent. This approach ensures that CoT traces provide decision-level supervision tightly aligned with the perception-action cycle."
        },
        {
            "title": "4.3 Data Curation",
            "content": "To ensure high-quality and interpretable data, we apply series of post-processing and analysis steps to refine the raw collection. First, we filter out trajectories with more than 32 steps to simplify training and evaluation, ensuring consistent sequence length across tasks. Second, we categorize all instructions into five high-level task types based on reasoning demands: Basic, Exploration, Spatial-Relational, Multi-stage, and Interaction. The Interaction class is further subdivided into two subtypes, Open the Door and Pick and Drop the Object, based on UnrealZoos interaction primitives. The resulting type distribution is shown in Figure 4, with Basic tasks accounting for approximately half of the dataset. In addition to type-based categorization, we conduct series of corpus-level analyses to characterize the dataset. Figure 3(a) illustrates the distributions of action trajectory lengths and reasoning token counts, showing that most tasks involve under 15 steps and 80 thinking tokens. Figure 3(b) presents word clouds derived from both task instructions and CoT annotations, revealing distinct vocabularies for goal specification and intermediate reasoning. Finally, we standardize all trajectories into unified format, including ordered egocentric frames, discrete action sequences, 6-DoF poses, and aligned language fields such as instruction, thinking trace, and step-level justification. Visual content is normalized for resolution and field of view to 6 Figure 5: Two-stage Training framework for Embodied Agent on EmbRACE-3K. (a) SFT training pipeline for agent in open-environment; (b) GRPO training pipeline for agent in open-environment ensure consistency across samples. These steps ensure that EmbRACE-3K offers not only scale and diversity, but also the structural coherence required for training and evaluating embodied visionlanguage models."
        },
        {
            "title": "5 Reasoning Training Pipeline",
            "content": "As illustrated in Figure 5, we designed two distinct training frameworks within the EmbRACE-3K dataset to enhance the spatial understanding and action planning capabilities of embodied agents. Specifically, we proposed supervised fine-tuning (SFT) pipeline based on spatial reasoning, which leverages memory learning to strengthen the agents reasoning abilities for scenes and actions. Additionally, we developed an exploratory reasoning framework grounded in reinforcement learning, where rule-based reward function enables the agent to autonomously learn reasoning skills. Figure 4: Distribution of task types."
        },
        {
            "title": "5.1 Supervised Fine-tuning for Reasoning Memory",
            "content": "We utilized Qwen2.5-VL-7B as the foundational models and trained with 2,344 high-quality reasoning trajectories from EmbRACE-3K, encompassing total of 10k trainable actions. This training phase was designed to endow the model with enhanced capabilities for understanding new visual scenes and reasoning for action decision-making. As shown in Figure 5(a), we directly deployed Qwen2.5-VL7B into the Llama-Factory [32] framework for instruction-based SFT through multi-turn dialogues. The supervised training outputs consisted of two key components: the reasoning process enclosed in the <think></think> tag and the final action decision enclosed in the <action></action> tag. The SFT process was conducted on 8 GPUs."
        },
        {
            "title": "5.2 Reinforcement Learning for Reasoning Exploring",
            "content": "Recent advancements [22, 15] suggest that reinforcement learning fine-tuning can lead to breakthroughs in reasoning capabilities across domains such as mathematics and coding. Additionally, reinforcement learning has achieved promising results in multimodal understanding tasks involving images and videos. Building on the observed advancements of the group relative policy optimization (GRPO) algorithm in DeepSeek-R1 [11] and prior explorations of multimodal reasoning training [8, 20], we further adopt the standard GRPO framework, as illustrated in Figure. 5(b), to investigate the impact of reinforcement learning on the reasoning abilities of embodied agents using the EmbRACE-3K dataset. For given question q, the policy model generates group of candidate responses {o1, o2, ..., oG} using the previous policy πθold . Each candidate response is associated with corresponding reward {r1, r2, ..., rG}, which is computed based on rule-based reward functions, such as those evaluating format and accuracy. The updated model πθ is subsequently trained by maximizing the following objective function: 7 Table 2: Performance comparison on Type Basic, Exploration and Dynamic Spatial-Semantic tasks. Method SR GDE Basic SSPL Steps TR SR GDE Exploration SSPL Steps TR SR Dynamic Spatial-Semantic SSPL GDE Steps TR GPT-4o Gemini 2.5 Pro Qwen2.5-VL-origin Qwen2.5-VL-no-thinking Qwen2.5-VL-sft-only Qwen2.5-VL-sft-rl GPT-4o Gemini 2.5 Pro Qwen2.5-VL-origin Qwen2.5-VL-no-thinking Qwen2.5-VL-sft-only Qwen2.5-VL-sft-rl 53.6% 76.4% 26.4% 79.3% 72.9% 81.4% 20.8% 38.0% 10.6% 45.8% 49.1% 53.2% 484.7 232.2 531.6 232.4 237.9 215.7 1278.6 643.8 4276.2 595.4 594.2 520. 0.396 0.649 0.176 0.775 0.647 0.766 0.163 0.336 0.083 0.446 0.424 0.513 17.9 10.1 23.4 4.3 7.9 6.2 20.9 12.4 25.3 6.9 10.7 7.9 In-Domain 14.3 % 37.1% 39.3 % 13.6% 65.7% 0.0 % 0.7% 28.6 % 71.4 % 4.3% 60.7 % 3.6% 1178.3 1068.9 991.8 652.1 279.3 391.8 Out-of-Domain 3.6 % 45.4% 4017.8 9.1 % 16.2% 2166.5 73.1% 0.0 % 9978.3 0.9% 10.9 % 1340.4 1239.7 22.8 % 5.6% 30.9 % 1162.8 2.8% 0.086 0.264 0.000 0.268 0.594 0.578 0.011 0.077 0.000 0.105 0.224 0.288 28.5 24.3 28.7 8.3 15.1 11. 30.9 25.3 31.2 8.2 19.3 13.4 62.9% 75.0% 71.4% 57.1% 85.7% 14.3% 3.6% 68.6% 68.6% 7.1% 68.6% 7.1% 10.2% 90.9% 20.3% 60.0% 94.5% 8.5% 0.0% 27.1% 35.6% 18.2% 42.4% 7.3% 374.1 238.1 527.9 298.5 245.0 238.4 2144.2 971.9 7844.0 907.5 839.7 824.6 0.521 0.589 0.079 0.580 0.557 0. 0.078 0.176 0.079 0.268 0.333 0.405 12.5 13.1 28.0 6.0 10.1 7.3 26.2 19.2 26.0 7.6 12.1 9.8 25.7% 25.7% 80.0% 0.0% 2.9% 2.9% 67.8% 37.3% 74.6% 3.4% 5.1% 5.1% Table 3: Performance comparison on Type Multi-stage, Open Door and Pick&Drop tasks. Method SR GDE Multi-stage SSPL Steps TR SR Interaction - Open Door SSPL GDE Steps TR SR Interaction - Pick and Drop Steps SSPL GDE TR GPT-4o Gemini 2.5 Pro Qwen2.5-VL-origin Qwen2.5-VL-no-thinking Qwen2.5-VL-sft-only Qwen2.5-VL-sft-rl 27.3% 40.9% 0.0% 72.7% 81.8% 81.8% 478.9 362.5 643.7 278.8 205.2 283.6 GPT-4o Gemini 2.5 Pro Qwen2.5-VL-origin Qwen2.5-VL-no-thinking Qwen2.5-VL-sft-only Qwen2.5-VL-sft-rl 1312.9 2.7% 1207.4 16.2% 8788.9 0.0% 1059.9 10.8% 1122.5 18.9% 27.0% 1265.7 0.194 0.383 0.000 0.708 0.707 0.771 0.027 0.156 0.000 0.108 0.170 0.253 23.0 19.8 28.3 9.7 13.9 10.6 25.8 17.6 30.7 11.3 18.7 16.5 In-Domain 28.6 % 40.9% 53.8 % 40.9% 9.9 % 86.4% 68.1 % 4.5% 80.2 % 4.5% 0.0% 73.6 % 275.2 179.8 354.34 113.2 99.9 107.9 Out-of-Domain 16.7 % 64.9% 33.3 % 32.4% 94.6% 7.6 % 2.7% 30.3 % 42.4 % 21.6% 45.5 % 16.2% 1804.6 430.3 9761.5 316.1 283.2 268.9 0.160 0.474 0.043 0.648 0.723 0. 0.069 0.318 0.037 0.290 0.352 0.381 25.1 10.5 29.7 7.1 8.4 7.6 29.1 17.8 28.8 8.3 12.9 11.8 2124.6 23.5% 62.6% 1615.3 39.8% 12.1% 84.6% 8882.8 0.0% 1.1% 64.7% 3069.3 2645.5 3.3% 44.1% 1.1% 50.0% 2665.5 5787.5 29.2% 83.3% 6746.2 25.0% 30.3% 9999.0 83.3% 0.0% 0.0% 33.3% 8431.9 32.8% 9.1% 5147.6 37.5% 4527.3 6.1% 0.213 0.255 0.000 0.587 0.388 0. 0.270 0.192 0.000 0.457 0.319 0.341 22.8 26.2 32.0 13.3 15.2 13.4 25.8 23.1 32.0 11.8 19.6 14.5 55.9% 73.5% 100.0% 0.0% 0.0% 0.0% 70.3% 54.2% 100.0% 0.0% 8.3% 4.2% JGRP O(θ) = Eq,{oi} (cid:104) 1 (cid:80)G i=1 (cid:16) min (cid:16) πθ(oiq) πθold (oiq) Ai, clip (cid:16) πθ(oiq) πθold (oiq) , 1 ϵ, 1 + ϵ (cid:17) (cid:17) Ai βDKL(πθπref ) (cid:17)(cid:105) where, ϵ and β are hyperparameters. Given that EmbRACE-3K contains action trajectories of up to 32 steps in length and corresponding input vision, we set = 6 to normalize the sampled rewards, thereby computing the advantage, Ai = rimean({r1,r2,...,rG}) , for updating the model. This approach aims to guide the embodied agent in freely exploring reasoning strategies within open environments. The rule-based supervision incorporates reward format in the form of <think></think> and <action></action>, directly evaluating the content of the actions. We conducted GRPO training on the R1V [4] framework using 8 GPUs. std({r1,r2,...,rG})"
        },
        {
            "title": "6.1 Experiment Setup",
            "content": "To evaluate the effectiveness of EmbRACE-3K and its contribution to embodied reasoning, we conduct experiments on both in-domain and out-of-domain tasks sampled from UnrealZoo environments. Test scenarios cover six task types defined in our benchmark: Basic, Exploration, Dynamic Spatial-Semantic, Multi-stage, Interaction - Open Door, and Interaction - Pick and Drop. This diverse coverage allows us to assess model behavior under different spatial, semantic, and sequential reasoning requirements. Each test prompt includes structured input consisting of the task instruction, brief description of the current scene, and history of previously executed actions. For the visual input, we provide the first-person egocentric views at the current time step, as well as the five most recent frames and the initial frame. This limited-frame strategy strikes balance between temporal context and computational tractability. Including the full trajectory often leads to excessive latency and model timeout. Then, we evaluate range of models as baselines: - GPT-4o, Gemini 2.5 Pro, and the original Qwen2.5-VL-origin serve as zero-shot or few-shot baselines without task-specific tuning. - Qwen2.5-VL-sft-rl: Our fully fine-tuned variant, which begins with SFT on EmbRACE-3K and is further trained using reinforcement learning with trajectory-level reward shaping. - Qwen2.5VL-sft-only: model trained only with SFT on our dataset, without additional RL optimization. - 8 Qwen2.5-VL-no-thinking: An ablated variant trained via SFT, where all chain-of-thought (<think>) reasoning annotations are removed from the input. This model isolates the contribution of explicit reasoning supervision to decision-making performance. All Qwen variants in our experiments are based on the 7B architecture. All models are tested under consistent inference conditions, using the same evaluation protocol and task sets to ensure fair comparison."
        },
        {
            "title": "6.2 Evaluation Metrics",
            "content": "We evaluate agent performance mainly using the following five metrics: Success Rate (SR): This metric measures the proportion of tasks that the agent completes successfully. task is considered successful if the agent reaches the goal under task-specific spatial and behavioral constraints, such as reaching within 300 meters of the target and issuing Finish action. Goal Distance Error (GDE): GDE quantifies the Euclidean distance (in centimeters) between the agents final position and the assigned target. For multi-stage tasks, GDE is computed as the sum of distances to each subgoal, with special handling to account for missing or inaccurate midway targets. Step-based Success weighted by Path Length (SSPL): SSPL evaluates the efficiency of successful episodes. It is defined as the ratio of the optimal number of steps to the actual steps taken, weighted by success. Specifically, for each task i, SSPLi = Si sopt max(si, sopt ) where Si indicates task success, sopt is the number of actions executed by the agent. is the length of the shortest ground-truth action sequence, and si Steps: This metric reports the average number of discrete actions (e.g., MoveForward, TurnLeft) taken by the agent per episode, regardless of success or failure, reflecting the behavioral cost. Timeout Rate (TR): Timeout Rate measures the proportion of episodes in which the agent exceeds maximum step threshold (e.g., 32 steps) without completing the task. high TR indicates frequent inefficiencies or failures to terminate appropriately."
        },
        {
            "title": "6.3 Experiment Analysis",
            "content": "We analyze the results presented in Table 2 and Table 3 to assess how different models perform across task types in both in-domain and out-of-domain settings. Challenge difficulty. Across all models without fine-tuning, performance remains low on exploration, spatial-relational, and multi-stage tasks. For example, the success rate (SR) of GPT-4o is only 3.6% (Exploration), 10.2% (Dynamic Spatial-Semantic), and 2.7% (Multi-stage) on out-of-domain tasks. Similar patterns are observed for Gemini 2.5 Pro and Qwen2.5-VL-origin. These results confirm that EmbRACE-3K poses substantial challenges for zero-shot models, particularly in tasks requiring long-horizon planning and egocentric spatial reasoning. Supervised and RL fine-tuning improves performance. Fine-tuning Qwen2.5-VL with our dataset leads to strong gains across all task types. In Table 2, the sft-rl variant achieves 30.9% SR on Exploration tasks and 42.4% SR on Spatial-Semantic tasks out-of-domain, both higher than GPT-4o and Gemini 2.5 Pro. Notably, its GDE drops from over 9978.3 to 1162.8 on Exploration, and from 7844.0 to 824.6 on Spatial-Semantic. Similar improvements are observed in Table 3: on Multi-stage tasks, SR improves from 0.0% (Qwen2.5-VL-origin) to 27.0%, and GDE reduces from 8788.9 to 1265.7. These gains show that environment-aligned supervision paired with RL reward shaping substantially improves success rates and path efficiency. Reasoning annotation improves decision quality. Comparing sft-only and no-thinking models isolates the contribution of chain-of-thought reasoning. In Dynamic Spatial-Semantic (in-domain), SR improves from 27.1% (no-thinking) to 42.4%, and SSPL increases from 0.268 to 0.405. Similarly, in Multi-stage tasks, the SR gap between no-thinking (10.8%) and sft-only (18.9%) indicates more stable sequential behavior when decision steps are paired with language rationale. These results suggest that step-wise reasoning supervision helps maintain spatial grounding and task context. 9 Generalization remains key challenge. The sft-only model performs well in-domain but shows large performance drops out-of-domain. For instance, its SR on Exploration drops from 71.4% to 22.8%, and on Multi-stage from 68.6% to 35.6%. In contrast, the RL-augmented model generalizes better: Exploration SR is 30.9%, and Multi-stage is 42.4%. This supports our hypothesis that trajectory-level reinforcement signals promote policy robustness in unseen scenes, where spatial layout and object configuration differ from training environments."
        },
        {
            "title": "7 Conclusion",
            "content": "This work introduces EmbRACE-3K, novel dataset and benchmark designed to address the limitations of current VLMs in embodied, interactive scenarios. Featuring diverse environments and multi-actions, EmbRACE-3K fosters research in dynamic, goal-oriented contexts within open environments. High-quality CoT annotations enhance agent actions by incorporating reasoning into spatial planning. This approach bridges the gap between instructional tasks and visual inputs, enabling more robust and logical decision-making. Benchmarking experiments reveal significant challenges in spatial reasoning, long-term planning, and causal understanding, underscoring the datasets value in advancing embodied reasoning. Notably, fine-tuning VLMs like Qwen2.5-VL-7B with EmbRACE-3K achieves superior performance compared to GPT-4o and Gemini 2.5 Pro. By enabling temporal generalization and integrating perception with language-guided behavior, EmbRACE-3K establishes foundation for developing intelligent agents capable of real-world applications."
        },
        {
            "title": "References",
            "content": "[1] Michael Ahn, Anthony Brohan, Noah Brown, Yevgen Chebotar, Omar Cortes, Byron David, Chelsea Finn, Chuyuan Fu, Keerthana Gopalakrishnan, Karol Hausman, et al. Do as can, not as say: Grounding language in robotic affordances. arXiv preprint arXiv:2204.01691, 2022. claude https://www.anthropic.com/news/ [2] Anthropic."
        },
        {
            "title": "Introducing",
            "content": "sonnet. 3.5 introducing-claude-3-5-sonnet, 2024. Accessed: 2025-05-16. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [4] Liang Chen, Lei Li, Haozhe Zhao, Yifan Song, and Vinci. R1-v: Reinforcing super generalization ability in vision-language models with less than $3. https://github.com/Deep-Agent/R1-V, 2025. Accessed: 2025-02-02. [5] An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Zaitian Gongye, Xueyan Zou, Jan Kautz, Erdem Bıyık, Hongxu Yin, Sifei Liu, and Xiaolong Wang. Navila: Legged robot vision-language-action model for navigation. arXiv preprint arXiv:2412.04453, 2024. [6] Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. [7] DeepMind. Gemini 1.5: Unlocking multimodal understanding across millions of tokens. arXiv preprint arXiv:2403.05530, 2024. [8] Kaituo Feng, Kaixiong Gong, Bohao Li, Zonghao Guo, Yibing Wang, Tianshuo Peng, Benyou Wang, and Xiangyu Yue. Video-r1: Reinforcing video reasoning in mllms. arXiv preprint arXiv:2503.21776, 2025. [9] Chaoyou Fu, Yuhan Dai, Yongdong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. arXiv preprint arXiv:2405.21075, 2024. [10] Kristen Grauman, Andrew Westbury, Eugene Byrne, Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, et al. Ego4d: Around the world in 3,000 hours of egocentric video. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1899519012, 2022. [11] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [12] Runyu Jiao, Alice Fasoli, Francesco Giuliari, Matteo Bortolon, Sergio Povoli, Guofeng Mei, Yiming Wang, and Fabio Poiesi. Free-form language-based robotic reasoning and grasping. arXiv preprint arXiv:2503.13082, 2025. [13] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, et al. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [14] Haowei Lin, Zihao Wang, Jianzhu Ma, and Yitao Liang. Mcu: task-centric framework for open-ended agent evaluation in minecraft. arXiv preprint arXiv:2310.08367, 2023. [15] Changshu Liu, Shizhuo Dylan Zhang, Ali Reza Ibrahimzada, and Reyhaneh Jabbarvand. Codemind: framework to challenge large language models for code reasoning. arXiv preprint arXiv:2402.09664, 2024. [16] Meta. Llama 3. 2024. [17] OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [18] OpenAI. Gpt-4o. 2025. [19] Aishwarya Padmakumar, Jesse Thomason, Ayush Shrivastava, Patrick Lange, Anjali Narayan-Chen, Spandana Gella, Robinson Piramuthu, Gokhan Tur, and Dilek Hakkani-Tur. Teach: Task-driven embodied agents that chat. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 36, pages 20172025, 2022. [20] Yingzhe Peng, Gongrui Zhang, Miaosen Zhang, Zhiyuan You, Jie Liu, Qipeng Zhu, Kai Yang, Xingzhong Xu, Xin Geng, and Xu Yang. Lmm-r1: Empowering 3b lmms with strong reasoning abilities through two-stage rule-based rl. arXiv preprint arXiv:2503.07536, 2025. [21] Hao Shao, Shengju Qian, Han Xiao, Guanglu Song, Zhuofan Zong, Letian Wang, Yu Liu, and Hongsheng Li. Visual cot: Advancing multi-modal language models with comprehensive dataset and benchmark for chain-of-thought reasoning. Advances in Neural Information Processing Systems, 37:86128642, 2024. [22] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [23] Mohit Shridhar, Jesse Thomason, Daniel Gordon, Yonatan Bisk, Winson Han, Roozbeh Mottaghi, Luke Zettlemoyer, and Dieter Fox. Alfred: benchmark for interpreting grounded instructions for everyday tasks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1074010749, 2020. 11 [24] Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. [25] Karmesh Yadav, Jacob Krantz, Ram Ramrakhya, Santhosh Kumar Ramakrishnan, Jimmy Yang, Austin Wang, John Turner, Aaron Gokaslan, Vincent-Pierre Berges, Roozbeh Mootaghi, Oleksandr Maksymets, Angel Chang, Manolis Savva, Alexander Clegg, Devendra Singh Chaplot, and Dhruv Batra. Habitat challenge 2023. https://aihabitat.org/challenge/2023/, 2023. [26] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [27] Jihan Yang, Runyu Ding, Ellis Brown, Xiaojuan Qi, and Saining Xie. V-irl: Grounding virtual intelligence in real life. In European conference on computer vision, pages 3655. Springer, 2024. [28] Jingkang Yang, Yuhao Dong, Shuai Liu, Bo Li, Ziyue Wang, Haoran Tan, Chencheng Jiang, Jiamu Kang, Yuanhan Zhang, Kaiyang Zhou, et al. Octopus: Embodied vision-language programmer from environmental feedback. In European Conference on Computer Vision, pages 2038. Springer, 2024. [29] Jihan Yang, Shusheng Yang, Anjali Gupta, Rilyn Han, Li Fei-Fei, and Saining Xie. Thinking in space: How multimodal large language models see, remember, and recall spaces. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1063210643, 2025. [30] Baiqiao Yin, Qineng Wang, Pingyue Zhang, Jianshu Zhang, Kangrui Wang, Zihan Wang, Jieyu Zhang, Keshigeyan Chandrasegaran, Han Liu, Ranjay Krishna, et al. Spatial mental modeling from limited views. arXiv preprint arXiv:2506.21458, 2025. [31] Samson Yu, Kelvin Lin, Anxing Xiao, Jiafei Duan, and Harold Soh. Octopi: Object property reasoning with large tactile-language models. arXiv preprint arXiv:2405.02794, 2024. [32] Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. Llamafactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand, 2024. Association for Computational Linguistics. [33] Fangwei Zhong, Kui Wu, Churan Wang, Hao Chen, Hai Ci, Zhoujun Li, and Yizhou Wang. Unrealzoo: Enriching photo-realistic virtual worlds for embodied ai. arXiv preprint arXiv:2412.20977, 2024. [34] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025."
        }
    ],
    "affiliations": [
        "Beijing Normal University",
        "LIGHTSPEED",
        "The University of Hong Kong",
        "Tsinghua University"
    ]
}