{
    "paper_title": "Novel Deep Learning Architectures for Classification and Segmentation of Brain Tumors from MRI Images",
    "authors": [
        "Sayan Das",
        "Arghadip Biswas"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Brain tumors pose a significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in a substantial volume of data, as a result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%."
        },
        {
            "title": "Start",
            "content": "Sayan Dasa,, Arghadip Biswasb, aIIIT Delhi, Delhi, India bJadavpur University, Kolkata, India"
        },
        {
            "title": "Abstract",
            "content": "5 2 0 2 6 ] . [ 1 1 3 5 6 0 . 2 1 5 2 : r Brain tumors pose significant threat to human life, therefore it is very much necessary to detect them accurately in the early stages for better diagnosis and treatment. Brain tumors can be detected by the radiologist manually from the MRI scan images of the patients. However, the incidence of brain tumors has risen amongst children and adolescents in recent years, resulting in substantial volume of data, as result, it is time-consuming and difficult to detect manually. With the emergence of Artificial intelligence in the modern world and its vast application in the medical field, we can make an approach to the CAD (Computer Aided Diagnosis) system for the early detection of Brain tumors automatically. All the existing models for this task are not completely generalized and perform poorly on the validation data. So, we have proposed two novel Deep Learning Architectures - (a) SAETCN (Self-Attention Enhancement Tumor Classification Network) for the classification of different kinds of brain tumors. We have achieved an accuracy of 99.38% on the validation dataset making it one of the few Novel Deep learning-based architecture that is capable of detecting brain tumors accurately. We have trained the model on the dataset, which contains images of 3 types of tumors (glioma, meningioma, and pituitary tumors) and non-tumor cases. and (b) SAS-Net (Self-Attentive Segmentation Network) for the accurate segmentation of brain tumors. We have achieved an overall pixel accuracy of 99.23%. Keywords: Brain Tumor, MRI Images, Deep Learning, Machine Learning, CNN, Classification 1. Introduction Brain Tumors are huge concern in the field of medicine because of their high mortality rate. Brain tumor forms when there is an uncontrollable abnormal growth of the cells within the Brain. The abnormal growth may occur in the brain itself which is called primary tumor or it may spread to the brain from the other parts of the body which are called secondary or metastatic tumors [8]. The proper reason and causes of brain tumors are not yet understood but according to researchers, they occur due to genetic mutations that affect cell growth and division [6]. This mutation can cause the cell to multiply causing the tumor. There are 120 types of tumors out of which some tumors are benign (non-cancerous) which grows slowly and some are malignant (cancerous) which grow rapidly [22]. These tumors pose significant global health challenge due to their high mortality rates, and complex procedure of treatment. Some common types of Brain tumors that we are going to detect and classify from the MRI Images by our proposed novel deep learning architectures - SAETCN and SAS-Net are - Gliomas, meningiomas, and Pituitary. All the brain tumor types and an MRI image with no tumor are shown in Figure 1. These authors contributed equally to this work. Note: This is preprint made available via arXiv for open access and academic dissemination. It has not been submitted for peer-reviewed journal publication. Gliomas are the most common type of Brian tumors. Around 30% of all brain and CNS tumors and 80% of all malignant tumors are of this kind. This type of tumor can occur at any age, but they are most common in the age range between 45 and 65 years of age. The incidence of gliomas has been stable in the last few years but due to the advancement of treatment, the mortality rate slightly increased [10]. Meningiomas are also one of the common primary types of brain tumors. Around 37% of all brain tumors are of this kind. They are mostly common amongst older individuals with ages greater than 65. The incidence of occurrence of this tumor is increasing and they are benign but their location can make them difficult to treat[10]. Around 15% of all brain tumors are of type Pituitary. This can occur at any age group but the most common age range is between 30 to 50 years. [19] Most of them are benign in nature and are slow-growing. The detection of pituitary tumors has increased due to the MRI images. The overall rate of occurrence of brain tumors is 6.2 per 1,00,000 people per year and the death rate is 4.4 per 1,00,000 people per year [10]. Brain tumors are the leading cause of cancer-related deaths in males aged birth to 39 years and females aged birth to 19 years [13]. There are almost 2,08,620 cases of brain tumors in adolescents and adults in the age group of 19 to 39, who are living with primary brain or spinal cord tumor in the United States. The pituitary tumors are most comThe proposition of our Novel Architecture for Classification: SAETCN or Self-Attention Enhanced Tumor Classification Network is completely novel architecture that has the power to beat many state-of-the-art deep learning models for the classification of the brain tumor into three classes - glioma, meningioma, and pituitary from the MRI images. The proposition of our Novel Architecture for Segmentation: SAS-Net or Self Attentive Segmentation Network is completely novel deep learning architecture that can segment the tumour region accurately with an overall pixel accuracy of 99.23%. In this paper, we have discussed the following sections: Section 2 discusses the related works that have been done in this field and the recent state-of-the-art methods and their outcomes. Section 3 explains our proposed methodology for the classification and segmentation of brain tumors. Section ?? provides the experimental setup, details about the dataset utilized in this study, and the experimental results obtained through the methodology. Section ?? discusses the future scope of this research and our plan to integrate this model into mobile application. Section 6 discusses the conclusions drawn from the study. 2. Related Works Over the last few years, many researches have been done and many methodologies have been proposed for the detection, classification, and segmentation of brain tumors from MRI images. 2.1. Review of previous works on classification Muhammad Aamir [1] used an optimized convolutional neural network on the Brain MRI dataset present in Kaggle and got an accuracy of 97%. However the limitation of this model is that it is not completely generalized and as result, the model gets overfitted to the training data and may work poorer in the unseen data despite having good performance metrics. Saeedi et al. proposed 2D CNN and an auto-encoder for the classification of brain images [21]. They have used dataset comprised of 3264 T1-weighted contrast-enhanced MRI images. The training accuracy of the proposed 2D CNN and that of the proposed auto-encoder network were found to be 96.47% and 95.63%, respectively and the test accuracies are 93.44% and 90.92%. The drawback of this paper is that the test accuracy is quite low and thus it may hinder the accurate prediction and classification of the brain tumor types in real-life scenarios thus affecting the treatment and diagnosis procedure. Takowa Rahman [18] used PDCNN over three datasets - The binary tumor identification dataset-I, Figshare dataset-II, and Multiclass Kaggle dataset-III provided accuracy of 97.33%, 97.60%, and 98.12%, respectively on the augmented dataset. But in the original dataset, they got result of 96%, 96.10%, and 95.60% respectively. It is clear that the model gave better results on the augmented data than that of the original data. The Figure 1: MRI Images of Types of Brain Tumor and No Tumor mon among those incidents in that age group [19]. The 5-year relative survival rate for brain and other nervous system cancers is about 33.4% [10]. The brain is the most important organ of our body, Proper functioning of the brain is very crucial for overall body performance. So, Both benign and malignant tumors present in the brain pose significant challenge due to their complex nature and the critical functions of the brain. The accurate classification and segmentation of different types of brain tumors, such as gliomas, meningiomas, and pituitary tumors, in early stages, is essential for effective treatment of the patient. The advancement of Artificial Intelligence in the modern world and its vast application in the field of medicine help us to classify and segment different types of tumors and their exact position. Over the last few years, several researches have taken place to support that deep learning approaches are one of the best for the classification of brain tumors automatically and efficiently with good accuracy rate and fewer errors [2; 4; 18; 21]. This paper consists of detailed explanation of our novel proposed deep learning architecture with four modules that are made with 16 layers of custom-made SAE blocks with attention and skip connection mechanism inspired from the residual and inception operations divided into four modules, giving good accuracy that beats any state of the art deep learning ImageNet architectures. The study focuses on enhancing the accuracy and efficiency of brain tumor classification and early detection along with proper segmentation, which is crucial for effective diagnosis and treatment planning. The research contribution of this study is as follows: Data Preprocessing: Effective data preprocessing has been done from the figshare data which was on mat files, they are converted into image files. Basic preprocessing like contrast enhancement and normalization has been done for the other two datasets. Both three and four-class classifications have been performed based on our dataset. 2 Figure 2: Processflow Diagram of our Experiment major drawback of using augmented data is that it may introduce some artifacts that do not present in the real MRI Images. Also, the model may get overfitted with the augmented data, This can lead to models that perform well on augmented data but poorly on real, unseen data. Badža and Barjaktarovic conducted study utilizing convolutional neural network (CNN) to classify glioma, meningioma, and pituitary tumors [4]. The architecture of the network included an input layer, two blocks, two blocks, classification block, and an output layer, totaling 22 layers. The performance of the network was assessed using the k-fold cross-validation method, achieving peak accuracy of 96.56% with tenfold cross-validation. The dataset comprised 3064 T1weighted contrast-enhanced MRI images sourced from Nanfang Hospital, General Hospital, and Tianjin Medical University in China. Nyoman Abiwinanda [2] used CNN to classify the three most common types of brain tumors: Gliomas, Meningiomas, and Pituitary. In the study, the CNN was trained in the 2017 figshare dataset which contains 3064 T-1 weighted CE-MRI from brain tumor images provided by Cheng. He used an \"Adam\" optimizer and 32 filters were applied in all the convolutional layers with ReLU activation function and max pool with size 2 2, and fully connected layer of 64 neurons with softmax activation function. The best-reported accuracy rates for training and validation were 98.51% and 84.19%, respectively. The low test accuracy is major limitation of this model that doubts the real-life classification of brain tumors. Sai Samarth R. Phaye [17] developed capsule algorithm networks. (DCNet) and diverse capsule networks (DCNet++). deeper convolutional network was added in DCNet to learn distinctive feature maps. The DCNet is more efficient for learning complex data. They used dataset comprising 3064 MRI images of 233 brain tumor patients for classification. The accuracy of the DCNet algorithm test was 93.04%, and the accuracy of the DCNet++ algorithm was 95.03%. Pashaei et al. offered various methods for identifying three types of tumors: meningiomas, gliomas, and pituitary malignancies [15] . To extract information from photographs, the technique employed convolutional neural network (CNN). The architecture had four convolutional layers, four pooling layers, one fully connected layer, and four batch normalization layers. The model was trained in 10 epochs, each with 16 iterations and learning rate of 0.01. Cheng contributed to the data utilized in this study. The models performance was evaluated using tenfold cross-validation technique, with 70% of the data utilized for training and 30% for testing. The suggested algorithm outperformed MLP, Stacking, XGBoost, SVM, and RBF in terms of accuracy, with 93.68% success rate. Paul et al. [16] used deep learning algorithms to categorize brain scans of meningiomas, gliomas, and pituitary cancers. They analyzed 3064 T1-weighted contrast-enhanced MRI images from 233 individuals. The study developed two types of neural networks: fully connected networks and CNNs. The fivefold cross-validation procedure demonstrated that generic techniques performed better than particular methods that needed picture dilation, with an accuracy of 91.43%. In all those previous studies we have found some major drawbacks. In [1] the model is not generalized enough. In [21; 2] the model performs poorly on the test dataset and thus becomes risky to implement it in real world. In [18] the model performs good in augmented data but not on the original data. 2.2. Review of previous works on segmentation Brain tumor segmentation has been focal point of research in medical imaging, with deep learning methods showing significant promise. Liu et al.[14] conducted comprehensive survey on deep learning-based brain tumor segmentation, highlighting various network architectures, segmentation under imbalanced conditions, and multi-modality processes. Their work underscores the importance of robust models capable of handling diverse imaging conditions. Aggarwal et al.[3] proposed 3 Figure 3: Self-Attention Enhanced Tumor Classification Network (SAETCN) an improved residual network (ResNet) for brain tumor segmentation, addressing gradient diffusion issues and achieving higher precision compared to traditional methods. Their approach demonstrates the potential of enhanced deep learning models in improving segmentation accuracy. Bouhafra and El Bahi[5] reviewed deep learning approaches for brain tumor detection and classification, emphasizing the role of transfer learning, autoencoders, and attention mechanisms. Their analysis provides valuable insights into the advancements and limitations of current methodologies. Rao and Karunakara[20] presented comprehensive review on MRI-based brain tumor segmentation, discussing semi-automatic techniques and the latest trends in deep learning methods. Their work highlights the ongoing evolution of segmentation techniques and the need for continuous improvement. Verma et al.[23] conducted comparative study of brain tumor segmentation methods, categorizing them into conventional, machine learning-based, and deep learning-based approaches. Their findings indicate that deep learning models, particularly variants of the U-Net, outperform other methods in terms of accuracy and reliability. These studies collectively contribute to the understanding and advancement of brain tumor segmentation, providing foundation for the development of novel deep-learning architectures. 3. Proposed Methodology 3.1. Overall Framework Classification Network (SAETCN), designed to improve the accuracy of distinguishing between different types of brain tumors. SAETCN utilizes self-attention mechanisms to focus on relevant features within the medical imaging data, effectively capturing intricate patterns that differentiate tumor types. On the segmentation front, we introduce the Self-Attentive Segmentation Network (SAS-Net), robust model aimed at precisely delineating tumor boundaries within the brain. SAS-Net employs similar self-attentive techniques to SAETCN, ensuring consistent and high-quality extraction of spatial features necessary for accurate tumor segmentation. Together, these models form unified framework that not only enhances the classification and segmentation tasks individually but also paves the way for integrated approaches that can significantly improve the overall accuracy and reliability of brain tumor diagnosis. 3.2. Classification Architecture We have proposed novel deep-learning architecture - SelfAttention Enhanced Tumor Classification Network(SAETCN) for the classification of Brain tumors. Our model has beaten many state-of-the-art deep learning architectures by achieving an accuracy of 99.31% and 98.20% in the Brain MRI dataset from Kaggel and Figshare Dataset by Jun Cheng respectively. multiclass classification of four types of classes is performed by our proposed architecture - SAETCN which is inspired by the Residual and Inception theory. The detailed architecture and mechanism of our proposed model are described in the following subsections. In this study, we present comprehensive framework that addresses both the classification and segmentation of brain tumors, leveraging advanced neural network architectures to enhance performance across these critical tasks. For tumor classification, we propose the Self Attention Enhancement Tumor 3.2.1. SAETCN Architecture Our proposed Deep Learning CNN architecture - SAETCN stands for Self-Attention Enhanced Tumor Classification Network, which can accurately classify 3 classes of tumor and nontumor patients from the MRI images given as input into the 4 Figure 4: Self Attention Enhancement Block Model. As shown in Figure 3 SAETCN consists of five main parts: \"Normalised Convolutional Activation Block\", \"Initial TriSAE Module\", \"QuadSAE Module\", \"HexaSAE Module\" and \"Final SAE-Fusion Module\". Each Module Consists of several Self Attention Enhancement Blocks. The SAETCN Architecture has 16 custom-made Self Attention Enhancement Blocks arranged serially one after another. After the Complete Feature Extraction, the feature maps go through the Adaptive Averagepooling Layer and finally into the Dense Layer for nonlinear classification. 3.2.2. Normalised Convolutional Activation Block This is the initial Block of this novel Architecture. The main operation of the Normalised Convolutional Activation Block (NCAB) is the primary extraction of underlying spatial features from the input images, detecting the important textures and edges. The input images are converted into tensors and then directly sent into the NCAB as input to the Network. The NCAB consists of Convolutional layer with the size 77, with ReLu activation followed by Batchnormalisation and Max Pooling Layer for dimensionality reduction. Let the output result of the NCAB be denoted as FNCAB. The process at this stage can be expressed as: FNCAB = KNCAB(X) (1) Where, FNCAB is the output feature map of the Neural Convolutional Attention Block (NCAB). KNCAB is the function representing the operations within the NCAB, which includes the convolutional layer, ReLU activation, batch normalization, and max pooling. is the input feature map to the NCAB. The internal operations of the KNCAB is represented further in the equation 2. KNCAB(X) = MaxPool(BatchNorm(ReLU (Conv2D(Xinput, kernel size = 7 7)))) (2) Figure 5: Normalised Convolutional Activation Block 3.2.3. Self Attention Enhancement Block The Self Attention Enhancement Block (SAEB) is the basic Fundamental Block that is used in our proposed architecture. This architecture is inspired by the operation of residual and inception blocks. The basic operations of both of these blocks are described below: Residual Blocks use the concept of skip connections that allow the gradient to flow directly and help in training deep 5 networks. The residual block can be mathematically represented as: = F(x, {Wi}) + Where, is the Input to the block, is the Residual function (e.g., convolution, batch normalization, ReLU), and {Wi} are the Weights of the layers within the block. Spatial Features are captured at multiple scales by the inception operation. It does this by applying convolutions of different kernel sizes in parallel and concatenating their outputs. The Inception operation can be mathematically represented as: Mathematically, the output of an Inception module can be expressed as: = O1 O3 O5 Opool Where, O1 is the Output from 1 1 convolution branch, O3 is the Output from 3 3 convolution branch, O5 is the Output from 5 5 convolution branch, and Opool is the Output from the pooling branch. Here, denotes the concatenation operator. Using the concept of both of these operations in our Self Attention Enhancement Block (SAEB) has benefitted lot. The skip connection helped to pass the gradient directly while skipping some layers making the deep network train faster and the inception operation helped to capture multiple spatial features from the image. As shown in Figure 4, the input of the SAEB block is divided into 4 Branches and skip connection line. The first branch goes through single Convolutional Layer. The second Branch goes through the convolutional layer with batch normalization and ReLu activation function followed by another convolutional layer with size 3 3. The third branch consists of the same layers as that of the second branch but the size of the convolutional layer is 5 5. The fourth branch goes through max pooling with size 3 3 followed by convolutional layer. The skip connection line from the input goes through convolution layer with batch normalization. The process can be expressed as follows: The output of each branch is denoted as Oi (where = 1, 2, 3, 4), and the output of the skip connection line is denoted as Oskip. O1 = fconv(X) O2 = fconv3( fBN( fReLU( fconv(X)))) O3 = fconv5( fBN( fReLU( fconv(X)))) O4 = fconv( fpool(X)) Oskip = fBN( fconv(X)) (3) (4) (5) (6) (7) Where, is the Input feature map to the SAEB, Oi is the Output of the i-th branch, Oskip is the Output of the skip connection line, fconv is Convolutional layer, fconv3 is Convolutional layer with kernel size 3 3, fconv5 is the Convolutional layer with kernel size 5 5, fBN is the Batch normalization, fReLU is the ReLU activation function, fpool is the Max pooling layer with size 3 3. All the outputs of the first four branches are concatenated and batch normalization operation is done on the resulting output. This process can be expressed as: Oconcat = BN(O1 O2 O3 O4) (8) where denotes the concatenation operator. Finally, the output is added with the skip connection layer and an activation function ReLu is activated to the ultimate output. Ofinal = ReLU(Oconcat + Oskip) (9) 3.2.4. Initial TriSAE Module The Initial TriSAE Module is the Primary Module of the Architecture that contains three Self-Attention Enhancement Blocks (SAEB). It has 64 input channels and 256 output channels. The result FNCAB of the Normalised Convolutional Activation Block (NCAB) is taken as input and passes through three Self-Attention Enhancement Blocks serially, one after another. The process is described as follows: FSAEB1 = SAEB1(FNCAB) FSAEB = SAEB2(FSAEB1 ) FSAEB3 = SAEB3(FSAEB2 ) (10) (11) (12) Where, FSAEB3 is the output of the Initial TriSAE Module, with 256 output channels. 3.2.5. QuadSAE Module The QuadSAE Module, serving as the secondary module in the architecture, comprises four Self-Attention Enhancement Blocks (SAEB). The output FSAEB3 from the Initial TriSAE Module is used as the input for the QuadSAE Module. This input sequentially passes through the four Self-Attention Enhancement Blocks. This Module has 256 input channels and 512 output channels. The process is mathematically represented as follows: FSAEB4 = SAEB4(FSAEB3 ) FSAEB5 = SAEB5(FSAEB4 ) FSAEB6 = SAEB6(FSAEB5 ) FSAEB7 = SAEB7(FSAEB6 ) (13) (14) (15) (16) Where, FSAEB7 is the output of the QuadSAE Module, with 512 output channels. 6 Figure 6: Comparison of all metrics on different models on dataset 1 3.2.6. HexaSAE Module the HexaSAE Module. The Operations of this HexaSAE Module is similar to the Initial TriSAE Module and QuadSAE Module. But the difference is it has six Self Attention Enhancement Modules and the input has 512 channels and 1024 output channels. The output FSAEB7 of the QuadSAE module is taken as input and computed by the HexaSAE function to present the ultimate Output of this module. The process of this module is represented in equation 17. Fout = KHexaSAE(FSAEB7 ) (17) Where, Fout is the output of the HexaSAE Module, with 1024 output channels and KHexaSAE is the six SAE Block Operations performed in HexaSAE Module. 3.2.7. Final SAE Fusion Module The Final SAE Fusion Module is the Final Module of our Deep Learning Architecture that takes 1024 channels as input and produces an output of 2048 channels. It consisted of three Self Attention Enhancement blocks like the TriSAE Module but has difference in the input-output channels. The output of the HexaSAE Module is taken as input and passes through three SAE blocks to compute the Output. The process is represented as follows: FSAEFMout = SAEB16(SAEB15(SAEB14(Fout))) (18) Where, FSAEFMout is the output of the Final SAE Fusion Module, with 2048 output channels and Fout is the output of After passing the Final SAE Fusion Module the output goes through an Average Pooling Layer which calculates the average value of each patch of the filter-covered feature map. This operation is beneficial for smoothing and minimizing the effects of outliers [25; 12]. flattened layer is used to transform the multi-dimensional output of the proposed CNN network into one-dimensional vector. This transformation is necessary because fully connected (dense) layers, which typically follow convolutional layers, require one-dimensional input. In dense layer 2048 neorons are assigned for non-linear classification with an activation function ReLU. For positive input, the ReLU function outputs the value directly; if not, it outputs zero [11; 9]. In the Final Dense Layer, 4 neurons are placed to classify 3 types of tumor and Non-Tumor classes. softmax activation is used in the final layer which converts logits into probabilities[7]. 3.2.8. Model Complexity Analysis In Saeedi et al. [21] 2D CNN has been proposed whose mathematical equation is represented in the equation 19. They have used sequential layers of two convolutions followed by maxpooling and this has been repeated four times sequentially with different filter sizes (64,32,16 and 8 respectively). Xout = Dropout(MaxPool(Conv(Conv( . . . Dropout(MaxPool(Conv(Conv(X))))))))) (19) Figure 7: Self-Attentive Segmentation Network As compared to our model they have not used any residual or inception operation which can help in multiple feature extraction and faster convergence. The inception operation used in our model is one of the big reasons for achieving perfect classification accuracy and beating others works. The main foundation of our proposed architecture is our Self Attention Enhancement Block. We have used four parallel connections and skip connection. The First 1x1 convolution acts as linear transformation, reducing dimensionality and capturing spatial relationships without changing the feature map size. It retains essential information while making the model computationally efficient. This connection helps in identifying fine-grained, low-level features. In the second Connection, The initial 1x1 convolution helps in reducing dimensionality, followed by batch normalization, which normalizes activations, and ReLU adds non-linearity. The second 1x1 convolution further refines the extracted features. This connection is capable of learning more complex features due to the added non-linearity and normalization, ensuring stable and faster convergence. The third connection works quite bit the same as the second connection, but its parallel nature allows learning slightly different set of features due to different initial weights and gradient flows. It provides redundancy and robustness, capturing diverse patterns that might be missed by single path. In the fourth connection, the max pooling reduces the spatial dimensions, capturing dominant features and making the subsequent convolution focus on these. The 1x1 convolution then processes this reduced information efficiently. This connection highlights the most prominent features, ensuring that the essential information is retained and further refined. the fifth connection acts as skip connection ensuring that the network can learn residuals, thus combating the vanishing gradient problem. This helps in stable and faster training. It Helps in capturing both the initial low-level features and the refined features from the other paths, improving the overall feature richness. In our model, we have used 4 different modules for extracting different features. The first Module extract the primary edges of the brain along with the edges of the tumor and other critical parts of it. It just extracts the features of the outline, while the second module goes deep into the outline and extracts features on the deep pixel levels. In this module, clear predictions can be made whether the tumor is present or not. The third module is given to understand the pixeled images to classify different types of tumors present in the tumor. This layer analyses the different shapes and structures of different classes of tumors. The fourth module is given to improve performance results based on our experiments. These four are the main building blocks of our proposed model. The result of our Model is Completely based on repetitive experiments and training step by step and module by module. The only reason to use 16 Self Attention Enhancement Blocks serially one after another, is because this is the optimum value where we are getting the best classification result. The number of SAE Blocks that we add one by one is directly proportional to the best positive results but after certain number of block( that is 16 in our experiment) the classification ability gets satu8 Figure 8: SFD Block rated and thus we select that optimum 16 SAE blocks. Those 16 SAE blocks are divided into 4 modules because of their different feature extraction capabilities. This architecture is powerful for capturing the varied and intricate features necessary for accurate brain tumor classification, offering both computational efficiency and improved performance. 3.3. Segmentation Architecture Brain Tumor Segmentation is very crucial for diagnosis planning. Doctors can plan the treatment process by properly localizing the tumor. Thus we have proposed novel architecture - Self Attentive Segmentation Network, that can accurately segment the tumor region with pixel accuracy of 99.23%. 3.3.1. Self-Attentive Segmentation Network The Self Attentive Segmentation Network consists of five Self Attention Enhancement Blocks connected sequentially with max pooling layer between them and an Enhanced Segmentation Integration Module as decoder which has four Segmental Feature Decoding Block in it. Each SAE Block has two outputs. One is connected as an input to the consecutive SAE Block and another is passed as an input to the (k-N)th SFD Block, where is the total number of SAE Block used in the architecture (here k=5) and is the current number of the SAE Block. The output from the Enhanced Segmentation Integration Module is subsequently processed through Convolutional Layer, culminating in the precise segmentation of the tumor. The complete architecture of the Self Attentive Segmentation Network is shown in the figure 7. 3.3.2. Segmental Feature Decoding Block The Segmental Feature Decoder Block (SFD Block) is an integral part of the segmentation model, meticulously designed to enhance the resolution and accuracy of feature maps through combination of upsampling and feature integration from the encoder. This block begins with ConvTranspose2d layer, which upsamples the input feature maps, thereby increasing their spatial resolution. Following this, the upsampled features are concatenated with corresponding feature maps from the (k-n)th SAE Blocks(where is the total number of SAE Block used in the architecture, here k=5 and is the current number of the SFD Block), ensuring the preservation of critical spatial information. The concatenated feature map then undergoes further processing through complex Residual Inception module. This module consists of multiple convolutional paths: 1x1 convolution, sequence of 1x1 followed by 3x3 convolutions, and 1x1 followed by 5x5 convolutions, each capturing features at different scales. Additionally, path involving max pooling followed by 1x1 convolution is included to capture spatial information from pooled features. The outputs from these paths are concatenated and batch-normalized. Also, there is another skip layer(L-skip) having 1x1 convolution with batch normalization that gets added to the output of the four parallel layers. And then the added output is activated using ReLU, creating rich, multi-scale feature representation. crucial aspect of this block is the residual connection that adds the original input back to the processed output, ensuring efficient gradient flow and mitigating the risk of vanishing gradients. This comprehensive approach results in refined, high-resolution feature 9 maps that significantly enhance the precision of the segmentation, particularly for complex tasks like brain tumor identification. The complete architecture of this block is shown in the figure 8. The mathematics is described in detail in the following equations. Lup = fconvT (X) Lconcat = Lup L1 = fconv1x1(Lconcat) (1) (2) (3) L2 = fconv3x3( fBN( fReLU( fconv1x1(Lconcat)))) (4) L3 = fconv5x5( fBN( fReLU( fconv1x1(Lconcat)))) (5) 3.3.3. Enhanced Segmentation Integration Module Figure 9: Segmentating the Tumor region accurately L4 = fconv1x1( fpool(Lconcat)) Lskip = fBN( fconv1x1(Lconcat)) The Enhanced Segmentation Integration Module acts as decoder in this novel Architecture. This module consists of four sequential Segmental Feature Decoding Block. The mathematical Operation of this Module is described in the following equation. IN(S FDn) = AEB5 AEBkn FDn1 AEBkn if = 1 if > 1 (20) (6) (7) Loutput = fReLU( fBN(L1 L2 L3 L4) + Lskip) (8) Where, Lup is the upsampled feature map obtained using the fconvT is transposed convolution opertransposed convolution. ation, which upsamples the input feature map X. is the input feature map to the UpBlock. Lconcat: The concatenated feature map from upsampling and the skip connection. is concatenation operation. is the skip connection from the corresponding encoder layer. L1 is output of the 1x1 convolution branch. fconv1x1 is 1x1 convolution operation. L2 is the Output of the fBN 1x1 convolution followed by 3x3 convolution branch. fReLU is ReLU activation is the Batch normalization function. fconv3x3 is the 3x3 convolution operation. L3is the function. Output of the 1x1 convolution followed by 5x5 convolution fconv5x5 is 5x5 convolution operation. L4 is the Output branch. of the max pooling followed by 1x1 convolution branch. fpool is Max pooling operation. Lskipis the Output of the shortcut connection. Loutput is the Final output of the Residual Inception Block after combining all branches and adding the residual connection. 10 We define FDn as the nth SFD block, AEBk as the k-th SAEB block, and IN(S FDn) as the input to the n-th SFD block. Here, represents the concatenation operation. The equation 20 captures the input to each SFD block. For = 1, The first SFD block takes inputs from AEB5 and AEB4. For > 1, Each subsequent SFD block takes input from the previous SFD block and the corresponding SAEB block (k n). Here, k=5, because we have used five SAEB blocks in our architecture. 4. Experiment and Result In this section we have discussed about the dataset and the preprocessing steps, the total experimental setup, and the evaluation metrics along with the results. Also, we have compared our model with previous works and other state-of-the-art existing models. 4.1. Dataset and Preprocessing We have used three publicly available MRI Image datasets for the classification of the Brain tumors in this study. The datasets are as follows: Brain MRI Dataset (Dataset 1) from Kaggle. This dataset contains 7023 images of human brain MRI images which Dataset 1 Dataset Dataset 3 Class Images Train Test Class Images Train Test Class Images Train Test Gliomas 1621 Meningiomas 1645 Pituitary No Tumor Total 1757 2000 7023 1339 1457 1595 5712 Gliomas 1153 Meningiomas Pituitary 708 930 732 273 142 198 300 300 405 Gliomas 1426 1129 Meningiomas Pituitary No Tumor 708 930 1485 572 749 1189 3639 297 181 296 910 1311 Total 2451 613 Total Table 1: Dataset Description are classified into four classes - Gliomas(1621), meningiomas(1645), pituitary(1757), and No Tumor(2000). This dataset is combination of Figshare, SARTAJ, and Br35H datasets. (Reference dataset word 1). The dataset is split into training and test sets in the ratio 80:20. Out of 7023 images in the dataset, 5712 images are taken in the training set and the rest are taken for the test set. Figshare dataset (Dataset 2) given by Jun Cheng, School of Biomedical Engineering, Southern Medical University, Guangzhou, China. This brain tumor dataset contains 3064 T1-weighted contrast-enhanced images from 233 patients with three kinds of brain tumor: meningioma (708 slices), glioma (1426 slices), and pituitary tumor (930 slices). This data is organized in Matlab data format (.mat file). Each file stores struct containing the following labels for an image - 1 for meningioma, 2 for glioma, and 3 for pituitary tumor. This is legitimate dataset used in many researches across the globe. Out of 3064 images in the dataset, 2451 images are chosen to train the network, and the rest 613 images are taken to evaluate the performance of the model. Thus splitting the dataset in the ratio 80:20. (Reference dataset word 2) The third dataset (Dataset 3) is custom-made cross-mixed dataset of Dataset 1 and Dataset 2. In this, we have made custom dataset containing four classes since this is mixed dataset so good accurate result in this dataset proves the generalization capability of our proposed architecture. This dataset contains total of 4549 images out of which 3639 images are taken to train the model and the rest are used to evaluate the performance of the model. All the details of the datasets and their divisions (i.e., training and testing) are listed in Table 1 and Visualisation is provided in Figure ??, ?? and ??. Dataset 1 and Dataset 3 contain images of different sizes so they are resized to images of similar size. All the images are resized to (224x224) pixels size and then basic preprocessing steps like contrast enhancement and normalization are performed. On the other hand, the Figshare dataset (Dataset 2) contains images in MAT files in raw form. So preprocessing steps are performed to convert the images from .mat files to .jpg files to use for training and test set to train and evaluate our network respectively. After conversion to image files, the Dataset is split into training and test sets in ratio of 80:10. Other preprocessing steps that are performed are zscore normalization and rescaling pixel values to the specified range(0,1). We have used the BratS2020 dataset for Segmentation. The BRATS2020 dataset is comprehensive resource for brain tumor segmentation, specifically focusing on gliomas. It includes multi-institutional, pre-operative MRI scans from 19 different institutions, encompassing T1, T1 post-contrast, T2, and T2FLAIR modalities2. The dataset provides ground truth labels for the enhancing tumor, peritumoral edema, and necrotic/nonenhancing tumor core, manually segmented by expert neuroradiologists. This rich, multimodal dataset is designed to advance the development and evaluation of segmentation algorithms, with the ultimate goal of improving brain tumor diagnosis and treatment planning. 4.2. Experimental Setup Many images are of different shapes so we have to resize those images in patches of 224x224 and then they are preprocessed. All the datasets are split into training and test sets in the ratio of 80:20 before passing the training set to train the neural network for classification. We have used learning rate of 0.0001, and the \"Adam\" Optimizer with crossentropy Loss function for classification and Binary Cross Entropy with Logits Loss for Segmentation purposes, to train our model. The Loss functions are represented in the equation 21 and 22. The accuracy and loss graph plot is shown in Figure 10 and 11, and the confusion matrix of the test dataset of different datasets are shown in Figure 13, 14 and 15. Our architecture mainly includes an NCA block and 16 SAE blocks divided into four Modules - Initial TriSAE, QuadSAE, HexaSAE, and Final TriSAE Fusion Module as shown in the figure 3. The proposed model is implemented in pytorch. The hardware specifications and Software environment used by us to develop this proposed model and to train it are represented in Table 2. The process flow diagram of the experimental process is shown in figure 2. = N(cid:88) C(cid:88) i=1 c=1 yi,c log(pi,c) (21) Where, is the number of samples, is the number of classes, yi,c is binary indicator (0 or 1) if class label is the correct classification for sample i, pi,c is the predicted probability that sample is of class c. BCE ="
        },
        {
            "title": "1\nN",
            "content": "N(cid:88) i=1 11 (cid:2)yi log(pi) + (1 yi) log(1 pi)(cid:3) (22) Where, is the number of samples. yi is the true label for the i-th sample (either 0 or 1). pi is the predicted probability for the i-th sample. log is the natural logarithm function. 1 yi is the complement of the true label for the i-th sample. 1 pi is the complement of the predicted probability for the i-th sample. (cid:80) denotes summation over all samples. 1 normalizes the loss by dividing the total loss by the number of samples. (a) (b) Figure 10: Accuracy and Loss Metrics for Dataset 2 (Figshare). (a) Accuracy and (b) Loss (a) (b) Figure 12: ROC-AUC Curve of our proposed model on the dataset"
        },
        {
            "title": "RAM",
            "content": "Ryzen 7 7700x Rtx 4060ti 8GB"
        },
        {
            "title": "Operating System",
            "content": "Windows"
        },
        {
            "title": "Libraries",
            "content": "Pytorch, Scikit Learn Table 2: Hardware Specifications and Software Environment used in our Study 4.3. Evaluation Metrics for Classification It is very much essential to evaluate the performances of proposed architecture for image classification purposes to investigate the proper functioning of the deep learning model In this study, we have used different in real-life scenarios. standard parametric methods for evaluation of our model like precision, F1 Score, R2 Score, Recall, confusion Matrix, and ROC-AUC curve. For multiclass classification there are two kinds of Metrics evaluation, one is Macro Averaged Metrics and the other is Micro Averaged Metrics. In Macro Averaged Metrics, it calculates the metric independently for each class and then takes the average (treating all classes equally). On the other hand, Micro-averaging aggregates the contributions of all classes to compute the average metric (treating all instances equally). In this Study we have evaluated the Micro Averaged Metrics. These are some standard evaluation metrics which are discussed one by one along with their mathematical representations: Figure 11: Accuracy and Loss Metrics for Dataset 3 (Custom Dataset). (a) Accuracy and (b) Loss Precision: Precision measures the accuracy of the positive 12 predictions. 4.4. Evaluation Metrics for Segmentation Micro Precision = (cid:80)N i=1 Pi i=1(T Pi + FPi) (cid:80)N (23) Recall: Recall measures the ability of the model to find all the relevant cases. Micro Recall = (cid:80)N i=1 Pi i=1(T Pi + FNi) (cid:80)N (24) F1 Score: The F1 Score is the harmonic mean of precision and recall. Micro F1 Score = 2 Micro Precision Micro Recall Micro Precision + Micro Recall (25) R2 Score (Coefficient of Determination): The R2 Score indicates how well the models predictions approximate the real data points. When evaluating segmentation models, several key metrics are essential to gauge their performance accurately. Intersection Over Union (IoU) measures the overlap between the predicted segmentation and the ground truth, providing ratio of their intersection over their union. The Matthews Correlation Coefficient (MCC) offers balanced measure of binary classification, considering true and false positives and negatives, making it particularly useful even when class sizes differ. The Dice Similarity Coefficient (DSC), assesses the similarity between the predicted and actual segments by comparing their overlap relative to their sizes. Specificity evaluates how well the model identifies true negatives, which is crucial for understanding the models ability to recognize non-tumorous regions. Finally, the Boundary F1 Score (BF1) focuses on the accuracy of the predicted boundaries of the segmentation, ensuring that the model can precisely delineate the edges of the segmented objects. Together, these metrics provide comprehensive understanding of the segmentation models accuracy, robustness, and reliability. All of these are described below on eby one with their mathematical equations. R2 = 1 (cid:80)N i=1(yi ˆyi)2 i=1(yi y)2 (cid:80)N (26) Intersection Over Union (IoU) ROC-AUC Curve (Receiver Operating Characteristic - Area Under Curve): The ROC curve is graphical representation of the true positive rate (TPR) against the false positive rate (FPR) at various threshold settings. The AUC represents the degree or measure of separability. The ROC-AUC curve of our study is shown in figure 12 Here we have used One vs One (OvO) Approach (refer to equation 27) and Micro Averaged Approach (refer to equation 28) to calculate the ROC-AUC. The expressions are showed as follows: AUCOvO ="
        },
        {
            "title": "2\nN(N − 1)",
            "content": "N1(cid:88) N(cid:88) i=1 j=i+"
        },
        {
            "title": "AUCi j",
            "content": "(27) AUCmicro = (cid:80)N i=1 (cid:80)N j=1 AUCi (28) In this section, the notation used for evaluation metrics includes the following: is the total number of classes. Pi represents the True Positives for class i, which are the instances correctly predicted as class i. FPi denotes the False Positives for class i, which are the instances incorrectly predicted as class i. FNi stands for the False Negatives for class i, which are the instances that belong to class but were incorrectly predicted as another class. Ni indicates the True Negatives for class i, which are the instances correctly predicted as not belonging to class i. yi is the actual value for instance i, while ˆyi is the predicted value for instance i. represents the mean of the actual values. Additionally, is the total number of classes, and AUCi is the AUC score for the binary classification problem between class and class j, all of which are used in ROC-AUC calculations. 13 IoUc = Pc Pc + FPc + FNc (29) IoU measures the overlap between the predicted segmentation and the ground truth. It is calculated as the area of overlap divided by the area of union between the predicted and ground truth masks. Matthews Correlation Coefficient (MCC) MCC is balanced measure that takes into account true and false positives and negatives and is generally regarded as balanced metric for binary classification, even if the classes are of very different sizes. Dice Similarity Coefficient (DSC) DS Cc = 2 Pc 2 Pc + FPc + FNc (30) The DSC, also known as the F1 Score, measures the similarity between two sets. It is calculated as twice the area of overlap divided by the total number of pixels in both the predicted and ground truth masks. Specificity peci icityc = Nc Nc + FPc (31) Specificity measures the proportion of true negatives that are correctly identified. It is calculated as the number of true negatives divided by the sum of true negatives and false positives. Boundary F1 Score (BF1) BF1c = 2 Pc Gc Pc + Gc (32) The Boundary F1 Score evaluates the accuracy of boundary prediction. It is similar to the DSC but focuses on the boundaries of the segmented objects. Accuracy Precision Recall F1 Score ROC AUC Class 0 Class Class 2 Class 3 99.79 59.01 82.27 84. 99.89 69.10 84.71 91.32 99.89 74. 83.24 91.32 99.89 71.76 83.97 89. 99.98 99.41 99.94 99.98 Table 3: Class wise Evaluation metrics of Segmentation IoU Specificity MCC Boundary F1 Score DSC Class 0 Class 1 Class Class 3 99.58 45.85 69.72 71.87 93. 99.86 99.90 99.95 93.41 71.47 83. 89.67 89.53 92.45 97.90 99.79 62. 82.16 83.64 Table 4: Class wise Evaluation Figure 13: Confusion Matrix of Dataset 1 4.5. Performance of our Proposed Classification Model on individual datasets Figure 14: Confusion Matrix of Dataset 2 We have trained our proposed network on three datasets and their results are compared in the table ?? and the confusion matrix of the test data of those three datasets are shown in the figure 13, 14, 15. Name of the Dataset Accuracy Precision F1 Score Recall R2 Score ROC-AUC Dataset 1 (Brain MRI ) 99.38 Dataset 2 (Figshare) 98.69 99.39 98.69 99.39 99.38 99. 98.68 98.69 98.25 99.99 99.90 Dataset 3 (Custom Cross-Mixed) 98.57 98.59 98.55 98.57 97. 99.90 Table 6: Comparison of our model on Different Datasets Figure 15: Confusion Matrix of Dataset 3 14 Contribution Year of Publish Types of Classifier Dataset Test Accuracy Muhammad Aamir et al. [1] Saeedi et al. [21] 2023 Hyperparametric CNN Brain MRI (Kaggle) 2D CNN Brain Tumor Classification Auto-Encoder (MRI): Four Classes Takowa Rahman et al. [18] 2023 PDCNN Badza et al. [4] Abiwinanda et al. [2] Sai Samarth R. Phaye et al. [17] Pashaei et al. [15] Paul et al. [16] 2020 2018 2018 2017 22 layers CNN CNN DCNet DCNet++ CNN CNN Our Proposed Architecture 2024 Enhancement Tumor Classification Network SAETCN - Self-Attention Figshare and Multiclass Kaggle Dataset (Augmented) Figshare and Multiclass Kaggle Dataset (Original) Figshare Dataset Figshare Dataset Figshare Dataset Figshare Dataset Figshare Dataset Brain MRI (Kaggle) Figshare Dataset Cross-Mixed Custom Dataset 97% 93.44% 90.92% 98.12% 95.60% 96.56% 84.19% 93.04% 95.03% 93.68% 91.43% 99.38% 98.69% 98.57% Table 5: Comparison of Our Work (Classification Architecture - SAETCN) with State of the Art Works 4.6. Comparison of our Classification Model with Previous Dataset"
        },
        {
            "title": "Works",
            "content": "We have compared our model with various state-of-the-art models like EfficientNetB4, ResNet18, InceptionNetV3, Swin Transformer, and ViT (Vission Transformer) but ultimately we have beat all those models by achieving an accuracy of 99.38%, 98.69% and 98.57% in dataset 1 dataset 2 and dataset 3 respectively (refer to table ??). All the evaluation metrics of all the models compared with our model on different datasets are also shown in detail in table??, table??, table ??. In table 5 we have compared our proposed model with other previous works that was performed in the last recent years. Model Precision Recall F1 Score R2 Score ROC-AUC EfficientNetB4 91.73 91.68 91.70 ResNet 96.22 96.24 96.22 InceptionNetV3 97.87 97. 97.85 Swin Transformer 87.85 87.92 87.87 ViT 79.00 79.77 79.26 Proposed 98.69 98. 98.68 84.47 94.29 97.19 72.75 59. 98.25 97.66 99.49 99.85 96.03 90. 99.90 Table 8: Comparison of our Proposed model with Other state of the art existing Models in terms of Evaluation Metrics on Figshare Dataset (Dataset 2) Model Model Dataset 1 Dataset Precision Recall F1 Score R2 Score ROC-AUC Precision Recall F1 Score R2 Score ROC-AUC EfficientNetB4 95. 95.31 95.21 ResNet18 98.22 98.22 98. InceptionNetV3 98.95 98.90 98.94 Swin Transformer 94. 94.43 94.41 ViT 87.57 87.56 87. Proposed 99.39 99.38 99.39 92.82 96. 98.25 84.86 67.95 99.08 99.46 99. 99.97 99.27 96.85 99.99 EfficientNetB4 96. 96.15 96.14 ResNet18 97.12 97.14 97. InceptionNetV3 98.05 98.02 98.03 Swin Transformer 92. 92.96 92.86 ViT 85.38 85.60 85. Proposed 98.59 98.57 98.55 90.53 96. 95.42 89.69 69.55 97.69 99.51 99. 99.89 98.81 96.52 99.90 Table 7: Comparison of our Proposed model with Other state of the art existing Models in terms of Evaluation Metrics on Brain MRI Dataset (Dataset 1) Table 9: Comparison of our Proposed model with Other State of the art existing Models in Terms of Evaluation Metrics on Cross-Mixed Dataset (Dataset 3) 15 NCA Initial TriSAE QuadSAE HexaSAE Final SAE Fusion Accuracy F1 R2 Recall Precision AUC 59.42 83. 98.09 96.26 99.38 59.94 -0.31 59. 83.82 39.57 83.67 98.09 98.02 98. 96.28 95.28 96.26 99.39 99.08 99. 63.28 87.27 98.13 96.40 99.39 85. 98.80 99.94 99.88 99.99 Table 10: Module Wise Ablation Study of the Proposed Model components Model Dataset 1 Dataset 2 Dataset 3 Train Test Train Test Train Test EfficientNetB4 99.57 95. 98.98 91.68 99.98 96.15 ResNet18 99. 98.22 99.85 96.24 99.56 97.14 InceptionNetV 100 98.90 99.20 97.87 100 98. Swin Transformer 97.25 94.43 98.29 87.92 99. 92.96 ViT 96.56 87.56 96.24 79. 97.89 85.60 Proposed 1.00 99.38 99. 98.69 99.46 98.57 Table 11: Comparison of Our Proposed Model with State of the art models between Dataset 1 (Brain MRI), Dataset 2 (Figshare) and Dataset 3 (Custom Cross Mixed Dataset) with respect to Train and Test accuracy 4.7. Comparison of our segmentation model with previous works Table 12 presents comparative analysis of the proposed SAS-Net architecture against previous works by Agarwal et al. and Wentau Wu et al. The SAS-Net outperforms the other models across most metrics, achieving Dice Similarity Coefficient (DSC) of 99.79%, specificity of 93.55%, and sensitivity of 99.89%. Notably, while Wentau Wu et al.s model exhibits high specificity at 99.82%, it falls behind in DSC and sensitivity compared to SAS-Net. Meanwhile, Agarwal et al.s model shows balanced performance but with lower sensitivity at 79.89%. Overall, the SAS-Net demonstrates superior performance, particularly in terms of DSC and sensitivity, indicating its effectiveness in accurately segmenting brain tumors. DSC Specificity Sensitivity Accuracy Agarwal et al. [3] 94.5 92.6 Wentau Wu et al.[24] 89.58 99.82 SAS-Net (Proposed) 99.79 93.55 79.89 91.10 99.89 91. - 99.23 Table 12: Comparison of our proposed segmentation architecture with previous works 5. Abalation Study Module wise ablation study is performed and evaluated. The result is shown in the table 10. In figure 16, the improvement of the result is represented that describes the view of choosing 16 SAEB blocks as this gives the better classification performance amongst every combinations. detailed Gradientweighted Class Activation Mapping of all model components is shown in figure 6. Figure 16: Representation of the evaluation metrics on different model components 6. Conclusions In this paper, we proposed SAETCN - Self-Attention Enhancement Tumor Classification Network for detecting and classification of three kinds of Brain tumors. This study explored methods for improving the accuracy of detection and classification of different kinds of Brain Tumor - Gliomas, Meningiomas, and Pituitary. Our model can also predict whether the brain has any tumor or not. Since Brain Tumor is huge problem and detecting and classifying those require lot of time for radiologists and is hectic and thus hinders the proper treatment plan and diagnosis. Therefore this type of advanced CAD system is very much required in the Medical field. Manual Classification also leads to human error but with an accuracy of 99.31% the Artificial Intelligence Model produces less error than manual classification. Also, it can be integrated with Mobile or web applications which increases the ease of use and thus people in remote areas who are not getting proper healthcare also use our web-based model to detect whether they are suffering from brain tumor or not. We have compared our model with other state-of-the-art models and with other researchers previous works, we found that our model has beaten most of the model by achieving an accuracy of 99.31%, 99.20%, and in dataset 1, dataset 2, and dataset 3 respectively. Although our proposed model has achieved great accuracy, still there is probability that the model may get overfitted on the three datasets we have used in this study, thus it should be trained in larger datasets before using it for real-world classification in Hospitals. More research and development can be done in this work in the future with more accurate and generalized results and thus in this way, artificial intelligence will help mankind to be better in the upcoming days. Although the accuracy of our proposed model is quite high still there is lot of future scope for this study, they include making the model more generalized for working more accurately in real-time classification. This can be achieved by training the network in Big data with machine with high computational resources to train it effectively. Also, in the future, we have planned to integrate this model in Mobile application, where doctors and patients can be the users and detect the brain tumor and classify it with their proper location thus helping in If they want they can also use their own data the treatment. to train the model and fine-tune it based on their specific MRI machine to get better results of classification. 7. Data Availablity For maximum transparency and to foster open science, all computational codes and saved model architectures related to this study have been made publicly available in the full GitHub repository at https://github.com/arghadip2002/ SAETCN-and-SASNET-Architectures. repository The datasets utilized, which include two publicly available datasets and custom-made dataset, are linked within (https://github.com/arghadip2002/ the SAETCN-and-SASNET-Architectures/blob/main/ dataLinks), with the Custom dataset accessible upon reasonable request to the author. Furthermore, the practical utility of the novel SAETCN and SASNET architectures is demonstrated by their integration into the backend of the NeuroGuard Web Application, which performs classification and detection of various brain tumors from MRI images, thereby showcasing the direct application of Artificial Intelligence in the medical domain for building futureready applications."
        },
        {
            "title": "References",
            "content": "[1] Muhammad Aamir, Abdallah Namoun, Sehrish Munir, Nasser Aljohani, Meshari Huwaytim Alanazi, Yaser Alsahafi, and Faris Alotibi. Brain tumor detection and classification using an optimized convolutional neural network. Diagnostics, 14(16):1714, 2024. [2] Nyoman Abiwinanda, Muhammad Hanif, Tafwida Hesaputra, Astri Handayani, and Tati Rajab Mengko. Brain tumor classification using convolutional neural netIn World Congress on Medical Physics and work. Biomedical Engineering 2018: June 3-8, 2018, Prague, Czech Republic (Vol. 1), pages 183189. Springer, 2019. [3] Mukul Aggarwal, Amod Kumar Tiwari, Partha Sarathi, and Anchit Bijalwan. An early detection and segmentation of brain tumor using deep neural network. BMC Medical Informatics and Decision Making, 23(1):78, 2023. [4] Milica Badža and Marko ˇC Barjaktarovic. Classification of brain tumors from mri images using convolutional neural network. Applied Sciences, 10(6):1999, 2020. [5] Sara Bouhafra and Hassan El Bahi. Deep learning approaches for brain tumor detection and classification using mri images (2020 to 2024): systematic review. Journal of Imaging Informatics in Medicine, pages 131, 2024. [6] Cleveland Clinic."
        },
        {
            "title": "Brain cancer",
            "content": "(brain tumor). https://my.clevelandclinic.org/health/ diseases/6149-brain-cancer-brain-tumor, 2022. [7] Michael Franke and Judith Degen. The softmax function: Properties, motivation, and interpretation. 2023. [8] Anna Giorgi. What is brain tumor? https://www. verywellhealth.com/brain-tumor-7253734, April 06, 2023. You can access the data online on the provided link. [9] Richard HR Hahnloser, Rahul Sarpeshkar, Misha Mahowald, Rodney Douglas, and Sebastian Seung. Correction: Digital selection and analogue amplification coexist in cortex-inspired silicon circuit. Nature, 408(6815):10121012, 2000. [10] National Cancer Institute. Cancer stat facts: Brain and other nervous system cancer. https://seer.cancer. gov/statfacts/html/brain.html, 2014-2020. [11] Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton. Imagenet classification with deep convolutional neural networks. Advances in neural information processing systems, 25, 2012. [12] Yann LeCun, Léon Bottou, Yoshua Bengio, and Patrick Haffner. Gradient-based learning applied to document recognition. Proceedings of the IEEE, 86(11):22782324, 1998. [13] Dongdong Lin, Ming Wang, Yan Chen, Jie Gong, Liang Chen, Xiaoyong Shi, Fujun Lan, Zhongliang Chen, Tao Xiong, Hu Sun, et al. Trends in intracranial glioma incidence and mortality in the united states, 1975-2018. Frontiers in oncology, 11:748061, 2021. [14] Zhihua Liu, Lei Tong, Long Chen, Zheheng Jiang, Feixiang Zhou, Qianni Zhang, Xiangrong Zhang, Yaochu Jin, and Huiyu Zhou. Deep learning based brain tumor segmentation: survey. Complex & intelligent systems, 9(1):10011026, 2023. [15] Ali Pashaei, Hedieh Sajedi, and Niloofar Jazayeri. Brain tumor classification via convolutional neural network and In 2018 8th International extreme learning machines. conference on computer and knowledge engineering (ICCKE), pages 314319. IEEE, 2018. [25] Matthew Zeiler and Rob Fergus. Visualizing and understanding convolutional networks. In Computer Vision ECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pages 818833. Springer, 2014. [16] Justin Paul, Andrew Plassard, Bennett Landman, and Daniel Fabbri. Deep learning for brain tumor classification. In Medical Imaging 2017: Biomedical Applications in Molecular, Structural, and Functional Imaging, volume 10137, pages 253268. SPIE, 2017. [17] Sai Samarth Phaye, Apoorva Sikka, Abhinav Dhall, and Deepti Bathula. Dense and diverse capsule networks: Making the capsules learn better. arXiv preprint arXiv:1805.04001, 2018. [18] Takowa Rahman and Md Saiful Islam. Mri brain tumor detection and classification using parallel deep convolutional neural networks. Measurement: Sensors, 26:100694, 2023. [19] Neuro-Oncology Branch Scientific Communications Raleigh McElvery. Statistical report highlights key trends in adolescents and young adults with brain tumors. https://shorturl.at/MhBCT, 2024. National Cancer Institute. [20] Champakamala Sundar Rao and Karunakara. comprehensive review on brain tumor segmentation and classification of mri images. Multimedia Tools and Applications, 80(12):1761117643, 2021. [21] Soheila Saeedi, Sorayya Rezayi, Hamidreza Keshavarz, and Sharareh R. Niakan Kalhori. Mri-based brain tumor detection using convolutional deep learning methods and chosen machine learning techniques. BMC Medical Informatics and Decision Making, 23(1):16, 2023. [22] The Johns Hopkins University, The Johns Hopkins Hospital, and Johns Hopkins Health System. Brain tumors and brain cancer. https://www.hopkinsmedicine.org/ health/conditions-and-diseases/brain-tumor, 2024. [23] Amit Verma, Shiv Naresh Shivhare, Shailendra Singh, Naween Kumar, and Anand Nayyar. Comprehensive review on mri-based brain tumor segmentation: comparative study from 2017 onwards. Archives of Computational Methods in Engineering, pages 147, 2024. [24] Wentao Wu, Daning Li, Jiaoyang Du, Xiangyu Gao, Wen Gu, Fanfan Zhao, Xiaojie Feng, and Hong Yan. An intelligent diagnosis method of brain mri tumor segmentation using deep convolutional neural network and svm algorithm. Computational and mathematical methods in medicine, 2020(1):6789306, 2020."
        }
    ],
    "affiliations": [
        "IIIT Delhi",
        "Jadavpur University"
    ]
}