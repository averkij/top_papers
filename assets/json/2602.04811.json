{
    "paper_title": "SE-Bench: Benchmarking Self-Evolution with Knowledge Internalization",
    "authors": [
        "Jiarui Yuan",
        "Tailin Jin",
        "Weize Chen",
        "Zeyuan Liu",
        "Zhiyuan Liu",
        "Maosong Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hindered by two obstacles: the entanglement of prior knowledge, where ``new'' knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-Bench, a diagnostic environment that obfuscates the NumPy library and its API doc into a pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding a clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring \"Closed-Book Training\" to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SE-Bench establishes a rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https://github.com/thunlp/SE-Bench."
        },
        {
            "title": "Start",
            "content": "SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization Jiarui Yuan * 1 Tailin Jin * 1 Weize Chen * 1 Zeyuan Liu 1 Zhiyuan Liu 1 Maosong Sun"
        },
        {
            "title": "Abstract",
            "content": "True self-evolution requires agents to act as lifelong learners that internalize novel experiences to solve future problems. However, rigorously measuring this foundational capability is hinthe entanglement of dered by two obstacles: prior knowledge, where new knowledge may appear in pre-training data, and the entanglement of reasoning complexity, where failures may stem from problem difficulty rather than an inability to recall learned knowledge. We introduce SE-BENCH, diagnostic environment that obfuscates the NumPy library and its API doc into pseudo-novel package with randomized identifiers. Agents are trained to internalize this package and evaluated on simple coding tasks without access to documentation, yielding clean setting where tasks are trivial with the new API doc but impossible for base models without it. Our investigation reveals three insights: (1) the Open-Book Paradox, where training with reference documentation inhibits retention, requiring Closed-Book Training to force knowledge compression into weights; (2) the RL Gap, where standard RL fails to internalize new knowledge completely due to PPO clipping and negative gradients; and (3) the viability of Self-Play for internalization, proving models can learn from self-generated, noisy tasks when coupled with SFT, but not RL. Overall, SEBENCH establishes rigorous diagnostic platform for self-evolution with knowledge internalization. Our code and dataset can be found at https: //github.com/thunlp/SE-Bench. 6 2 0 2 4 ] . [ 1 1 1 8 4 0 . 2 0 6 2 : r 1. Introduction Self-evolution, the capacity for an autonomous agent to recursively improve its own capabilities, is often viewed as prerequisite for Artificial General Intelligence (AGI) (Go- *Equal contribution 1Tsinghua University. Correspondence to: Weize Chen <chenwz21@mails.tsinghua.edu.cn>, Zhiyuan Liu <liuzy@tsinghua.edu.cn>. 1 ertzel & Pennachin, 2007; Legg & Hutter, 2006). An ideal self-evolving agent acts as lifelong learner, continuously assimilating information from its environment, optimizing its solutions, and expanding its skill set without human intervention. However, current approaches often limit the scope of this evolution to transient or localized adaptations, such as inference-time response refinement (Novikov et al., 2025; Wang et al., 2025b) or iterative self-code modification (Zhang et al., 2025a; Wang et al., 2025a). While valuable, these mechanisms differ fundamentally from the expansive definition we explore here: the self-evolution requires agents to actively learn from experience by internalizing novel skills or knowledge, akin to human expert accumulating domain knowledge over time (Wang et al., 2024; Zhang et al., 2025b; Ouyang et al., 2025). Despite rapid progress in large language model (LLM) reasoning capabilities, we lack rigorous measurement for this foundational internalization ability. Existing benchmarks have made strides in evaluating specific sub-skills related to self-evolution, such as long-horizon information retrieval (Wei et al., 2025a; Li et al., 2025), iterative response refinement (Lee et al., 2025), and complex task execution (Team, 2025c; Jimenez et al., 2024) However, current evaluations fail to cleanly isolate an agents ability to process and restore experience due to two fundamental obstacles. First, the entanglement of prior knowledge: when model solves task involving novel knowledge, it is indistinguishable whether the agent learned from the relevant experience or merely recalled pre-training data. Second, the entanglement of reasoning complexity: if an agent fails complex task, it is ambiguous whether it failed to internalize the necessary knowledge or failed to reason over it. This mirrors student who memorizes textbook but fails frontier math problem due to logical difficulty rather than memory gaps. To address these limitations, we argue that the community needs Needle in Haystack test (Kamradt, 2023) for self-evolution: an environment where tasks are algorithmically trivial if knowledge is internalized, and impossible if it is not. To this end, we introduce SE-BENCH. SE-BENCH relies on knowledge obfuscation mechanism to create this clean environment. We employ knowledge obfuscation mechanism, mapping the core functions of the NumPy (Harris et al., 2020) library to randomized, nonsense identifiers SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization (e.g., numpy.mean zwc.kocito) and rewriting the documentation to describe new package. At training time, agents have access to the documentation, but at test time, agents are tasked with solving simple problems using this obfuscated package, with the strict constraint that any use of the original NumPy library is deemed to fail. This design grants SE-BENCH three diagnostic properties: (1) Impossible without information: Without documentation, the probability of guessing the correct API is mathematically zero, eliminating prior knowledge confounds. (2) Trivial with information: Because the underlying logic maps 1to-1 to standard NumPy, tasks are trivial for any agent that internalizes the mapping. Any ideal self-evolving method should theoretically achieve near-100% success rate, thus cleanly isolating the internalization capability. (3) Compositional generalization: While the training set consists of tasks solvable with single function calls, the test set requires composing multiple internalized functions, assessing generalization beyond simple memorization. Beyond serving as rigorous metric, SE-BENCH functions as clean testbed that enables us to dissect the fundamental mechanisms of self-evolution. We investigate whether standard parameter-optimization paradigms, specifically Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL), can genuinely support the internalization capability. Our experiments uncover three critical insights: (1) The Open-Book Paradox: We find that the presence of reference material during parameter update inhibits long-term retention. True internalization requires Closed-Book Training: removing the documentation during parameter updates forces the model to compress external logic into its weights, significantly outperforming standard SFT. (2) The RL Gap: While SFT effectively internalizes new knowledge, standard RL fails even under the Closed-Book training setting. We identify that the negative gradient and PPO clipping (Schulman et al., 2017) both are factors that impact the knowledge internalization for RL. (3) Viability of Self-Play: By applying SFT instead of RL to self-generated tasks and corresponding responses, models successfully internalize knowledge from their own noisy, unverified data, proving that self-evolution on knowledge internalization is viable if the correct optimization mechanism is used and that RL is not one-size-fits-all solution. We position SE-BENCH as diagnostic testbed for the selfevolving agent community. Just as long-context models must at least demonstrate near-perfect retrieval on Needlein-a-Haystack tests to establish basic competency, we argue that self-evolving agents should also demonstrate the ability to pass SE-BENCH before they can be trusted to evolve in complex, open-ended environments. And because SEBENCH provides clean, controlled environment, it also serves as an ideal platform for studying the fundamental mechanisms for knowledge internalization, potentially facilitating future research. 2. SE-BENCH We argue that fundamental, yet often overlooked, component of self-evolution is knowledge internalization. While current methods often focus on transient adaptation, optimizing solution within single context window (Wang et al., 2025b; Qi et al., 2025; Wei et al., 2025b; Team, 2025b), genuine evolution requires transitioning from stateless processor to lifelong learner. concrete example of such process is human software engineer learning new library: initially relying on documentation, but eventually internalizing the logic to solve problems fluently without external aid through repeated practice. Measuring such capability of LLM agents in similar scenario, however, presents fundamental dilemma. We cannot evaluate the agents ability to internalize any existing library (like Numpy), as they may have already been embedeed in the LLMs pre-training weights (Shao et al., 2025; Wu et al., 2025). Furthermore, simply adopting newly-released library is fragile solution: as knowledge cutoff of the LLMs advances, the benchmark quickly becomes obsolete. To rigorously measure the internalization ability of self-evolving methods, we require domain that is permanently out-of-distribution: one that effectively does not exist on the Internet, ensuring that no further model can solve it with its pre-training knowledge. To this end, we introduce SE-BENCH, synthetic domain constructed by systematically obfuscating the NumPy library paired with trivial coding problems. By mapping function names to nonsense identifiers while preserving the underlying logic, we create novel package that remains structurally realistic yet alien to any models training distribution. This construction enforces three critical properties: Impossible without Information: The randomized namespace guarantees mathematical zero-shot baseline, eliminating pre-training confounds. Trivial with Information: Because the logic is isomorphic to standard NumPy, tasks are algorithmically trivial if the new API doc is provided, cleanly isolating memory failures from reasoning failures. Compositional Generalization: By retaining the librarys original structure, we can evaluate whether agents can compose internalized functions to solve multi-step problems beyond their specific training examples that involves only single function. 2.1. Benchmark Construction To ensure that SE-BENCH serves as rigorous evaluation of internalization, we implement three-stage construction pipeline: Obfuscation, Question Generation, and Filtering. 2 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization Figure 1: Overview of the SE-BENCH construction pipeline. The process consists of three main stages: (1) Obfuscation, where we implement wrapper package zwc that renames selected NumPy functions and translates API documentation; (2) Generation, where Claude-4.5-sonnet generates valid tasks and test cases based on the original NumPy library; and (3) Filtering, where tasks are validated through strict consensus between three strong LLMs, followed by human verification. This process is illustrated in Figure 1. Stage I: Obfuscation. We select NumPy as our source domain due to its functional richness and simplicity. To construct our target library ZWC (a randomly generated package name), we identify set of 268 common NumPy functions (see Section D.2) to serve as the core of the new package. Rather than simply renaming functions, we implement ZWC as wrapper package. Each function in ZWC is assigned randomized, semantically void identifier (e.g., zwc.kocito) which internally calls the corresponding NumPy function. And to prevent models from bypassing the obfuscation by invoking standard methods on the returned NumPy array objects (e.g., calling .mean() directly on an array), we wrap all inputs and outputs in custom ZWCArray class. This ensures that the agent must strictly rely on the obfuscated functional API to manipulate data. To rewrite the accompanying documentation, we employ Gemini-2.5-Pro (Team, 2025a). We provide the model with the original NumPy docstrings and the global function mapping, instructing it to translate the documentation into the context of the new ZWC package. This results in an API documentation that describes the new package. Step II: Question Generation. Prompting model to generate tasks directly using the obfuscated ZWC library is prone to hallucination, as the model lacks prior exposure to the new syntax. We therefore prompt Claude-4.5-sonnet (Anthropic, 2025) to generate simple coding problems relevant to the sampled original NumPy functions along with at least 8 test-cases. We employ stratified generation strategy to create two distinct task categories: Single-Function Tasks: We iterate through every function in our function list. For each function, we prompt the model to create self-contained problem that requires specifically that function to solve. This ensures 100% coverage of the functions included in SE-BENCH. Multi-Function Tasks: To test generalization, we randomly sample sets of 10 functions and prompt the model to generate complex problem that requires the composition of at least three functions from the sampled set. Step III: Filtering. To ensure the Trivial with Information property, we must verify that the generated tasks are not accidentally difficult or erroneous. We employ strict Consensus Filtering protocol. We provide the generated questions (in their original NumPy form) to three distinct state-of-the-art models: Qwen3-Coder-480B (Team, 2025b), Gemini-2.5-Pro (Team, 2025a), and GPT-OSS-120B (OpenAI, 2025). task is retained only if all three models can independently solve it and pass all test cases using standard NumPy. If three distinct model families can solve the question easily, we can be confident that failure of testing model on SE-BENCH is due to failure to learn the new API, not any error in the problem itself. Finally, we perform human verification on 10% random subset of the filtered 3 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization data to ensure the clarity of the problem descriptions; all sampled tasks were found to be valid. 2.2. Dataset Splits & Protocol We partition the filtered tasks into training and testing sets to rigorously evaluate the knowledge internalization. For training set, it only includes Single-Function tasks, and ensures that every function in the ZWC library appears at least once. For test set, it comprises both Single-Function tasks (to test retention on unseen problems) and MultiFunction tasks (to test compositional generalization). The core objective of SE-BENCH is to measure internalization. Therefore, the information availability differs strictly between phases: Training Phase: The agent is provided with the training set, each task includes problem description and the relevant documentation for the involved function. The agent may use this phase to practice, memorize, or update its parameters. Testing Phase: The agent is provided with the test set that includes only the problem descriptions without API documentation. To solve the task, the agent must rely entirely on the knowledge internalized during training. 2.3. Metrics To ensure rigorous evaluation, we employ strict Abstract Syntax Tree (AST) verification protocol. solution is not judged merely on output correctness, but on its adherence to the benchmark constraints. Since ZWCArray is convertible to NumPy arrays, models might attempt to bypass the task by converting data to NumPy, performing operations, and converting back. To prevent this, we explicitly prohibit the import or usage of the original Numpy package. solution is considered correct (R(s) = 1) if and only if it meets three conditions: (1) it passes all provided test cases, (2) AST analysis confirms that the returned value relies on ZWC APIs, and (3) it contains zero imports of NumPy: (cid:40) R(s) = if all 3 conditions are met, 1, 0, otherwise. (1) 2.4. Statistics and Validation SE-BENCH comprises 1,417 tasks, partitioned into training set of 718 instances and test set of 699 instances. To evaluate generalization capabilities, we stratify the test set by compositional complexity: it contains 259 SingleFunction tasks, which require only single API calls, and 440 Multi-Function tasks that demand the composition of multiple APIs. Both splits maintain comprehensive coverage of the ZWC surface area, and an illustrative example is provided in Section D.1. Table 1: Validation of SE-BENCH Design Properties. Pass@64 of Qwen3-8B. Standard NumPy confirms the tasks are reasoning-trivial. ZWC Zero-Shot confirms no pre-training leakage. ZWC In-Context establishes solvability ceiling when documentation is provided without training. Evaluation Setting Single Multiple Reasoning Upper Bound (Using Standard NumPy) Standard NumPy 97.4 93.6 SE-BENCH Strict Evaluation (Using ZWC) ZWC Zero-Shot 0.0 ZWC In-Context 70.5 0.0 85.3 To validate the structural integrity of our design, we conducted preliminary evaluation on our test set using Qwen38B under three distinct settings, as reported in Table 1. First, we evaluate the Standard NumPy setting, where the the model is allowed to use the original NumPy library. The high accuracy (> 90%) across both splits confirms that the tasks are reasoning-trivial. Second, we test the ZWC Zero-Shot setting, where the model must use the obfuscated library and is denied access to API documentation. The resulting 0% accuracy confirms that our obfuscation is robust and prevents any leakage of pre-training knowledge. Finally, we examine the ZWC In-Context setting, where the model is provided with the API documentation relevant to the problem. The recovery of performance demonstrates that the benchmark is solvable. Notably, the remaining gap (compared to >90% on NumPy) is primarily due to hallucination: qualitative analysis reveals that Qwen3-8B frequently tries to use NumPy namespace (e.g., zwc.mean) despite the provided relevant documentation. This underscores core objective for self-evolution: methods must enable agents to strictly adhere to real, internalized knowledge. 3. Experiment Baselines: We evaluate diverse suite of self-evolution (1) Memory-based: strategies across three paradigms. ACE (Zhang et al., 2025b) and Expel (Zhao et al., 2024), which summarize experience into memory to improve future task performance. (2) Parameter-Optimization (SFT/RL): We consider two fundamental training protocols. In the Open setting, API documentation remains in the context during both trajectory collection and parameter updates. In the Closed setting, documentation is available for trajectory collection but is stripped during training. See Section C.3 for the difference between the Open and Closed settings. We also evaluate the fully autonomous self-play method Absolute-Zero (Zhao et al., 2025). (3) Hybrid: ClosedSFT-RL applies standard RL without API documentation on top of the Closed-SFT. Training Setup: We use Qwen3-8B, 4B and 1.7B (Team, 4 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization Table 2: Average performance over 5 rollouts of different models and methods. The best results are highlighted in bold, and the second-best results are underlined."
        },
        {
            "title": "Method",
            "content": "Qwen3-8B Qwen3-4B Qwen3-1.7B Qwen3-8B Qwen3-4B Qwen3-1.7B"
        },
        {
            "title": "Multiple",
            "content": "Memory Based ACE (Zhang et al., 2025b) Expel (Zhao et al., 2024) SFT Based Open-SFT Closed-SFT RL Based Open-RL Closed-RL Absolute-Zero (Zhao et al., 2025) Hybrid Closed-SFT-RL 11.2 47.1 0.0 39. 0.0 0.0 0.0 12.0 39.4 0.0 25.1 0.0 0.0 0.0 2.0 28.2 0.0 16. 0.0 0.0 0.0 54.4 43.1 21.9 4.1 15.5 0.0 11. 0.0 0.0 0.0 17.9 6.6 13.6 0.0 5.4 0.0 0.0 0.0 9. 0.0 4.8 0.0 2.0 0.0 0.0 0.0 3.2 2025b) families as base models. All RL-based methods utilize the GRPO algorithm (Shao et al., 2024) within the veRL (Sheng et al., 2024) framework. Experiments were conducted on 8 NVIDIA A100 GPUs; further details are provided in Section C.1. Results. Table 2 showcases baseline performance on SEBENCH. Memory-based methods achieve non-trivial results across all model sizes, with Expel attaining the highest accuracy on the Multi-Function split. This is intuitive, as memory allows the model to map SE-BENCH identifiers to NumPy correspondents, and then revise the NumPy solution based on the mapping. However, even these methods remain far from perfect, indicating that autonomous memory management is still in its infancy. Even more surprisingly, among parameter-update methods, only Closed-SFT and the hybrid Closed-SFT-RL achieve success; all other methods fail completely. This striking gap suggests that standard RL is fundamentally ill-suited for knowledge internalization, failure we analyze mechanistically in Section 4.2. Furthermore, the comparison between Open-SFT and Closed-SFT reveals the Open-Book Paradox: despite using the same training trajectories, success depends entirely on removing documentation during parameter updates. This forces the model to encode logic into its weights rather than relying on context. As we will show in Section 4.1, this reflects genuine internalization rather than simple alignment to the test-time prompt distribution. 4. Analysis and Insight Beyond serving as rigorous metric for current methods, SE-BENCH functions as clean testbed for investigating the fundamental mechanisms of self-evolution with knowledge Figure 2: Closed-SFT vs. Open-SFT. Open-SFTs complete failure without documentation reveals strict context dependency, whereas Closed-SFT successfully internalizes knowledge, maintaining performance even when documentation is absent. internalization. In this section, we leverage this controlled environment to investigate three fundamental research questions (RQs) about knowledge internalization with parameter update methods: (RQ1): Does SFT induce true internalization, or merely context dependence? (RQ2): Can RL internalize knowledge presented in the context? (RQ3): Can Self-Play Enable Knowledge Internalization? (RQ4): How does knowledge evolve from SFT to RL? 4.1. RQ1: Does SFT Induce True Internalization, or Merely Context Dependence? In Section 3, we had surprising observation: removing API documentation during parameter update significantly boosts performance on SE-BENCH. We hypothesize that this training condition forces the model to rely on parametric memory rather than context. critical question remains: does this 5 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization Table 3: Ablation of RL components. Internalization collapses when using PPO clip loss or including negative advantage."
        },
        {
            "title": "Single Multiple",
            "content": "SFT-like Closed-RL w/o Larger LR w/o Larger BSZ w/ PPO Clip Loss w/ GRPO Advantage 51.0 31.7 31.7 0 0 9.4 1.4 0.2 0 0 improvement stem from genuine knowledge internalization, or is it merely an artifact of distribution consistency between training and testing prompts? To isolate the mechanism, we evaluate all models with API documentation provided at test time, strictly aligning with the Open-SFT training prompt. As shown in Figure 2, Closed-SFT still outperforms Open-SFT even in this setting. This rules out prompt consistency as the driver; if it were, Open-SFT would lead. Instead, the results confirm that withholding documentation during training forces the model to encode knowledge directly into its parameters, resulting in robust internalization independent of context availability."
        },
        {
            "title": "Takeaway",
            "content": "SFT can internalize knowledge, but only when knowledge is absent from the context and utilized in the response. Therefore, use rich context (hints, docs) to generate high-quality data, but strip that context during parameter updates may foster the internalization. 4.2. RQ2: Can RL Internalize Knowledge in the Context? While Closed-SFT effectively internalizes knowledge, we find that applying RL in the similar off-policy settings, i.e., generating rollout with API doc, and training without it, fails completely. As shown in Table 2, Closed-RL achieves zero performance. This stark contrast suggests that the mechanism for knowledge internalization is fundamentally different or more restricted in RL than in SFT. To isolate the cause of this failure, we perform systematic ablation study bridging the mathematical gap between SFT and RL. For prompt D, let {yi}N i=1 denote trajectories. The SFT objective maximizes the log-probability of successful trajectories: LSFT(θ) = ExD (cid:34)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:35) log πθ(yi x) . (2) For RL, we omit the KL regularization. And although the off-policy (rollout with doc, train without) theoretically necessitates an importance sampling ratio πθ(yxno doc) πθ(yxdoc) , we exclude it in practice as the numerator vanishes for randomized ZWC function names. The training objective is thus formalized as: LRL(θ) = ExD (cid:34)"
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:35) ρi(θ)Ai . (3) By instantiating the policy term ρi(θ) and the advantage term Ai differently, we recover both paradigms: SFT Instantiation: ρi(θ) = log πθ(yi x) and Ai = I(yi is correct). This treats every correct trajectory as positive reinforcement signal without regularization. GRPO Instantiation: ρi(θ) uses the clipped probability ratio min(rt(θ), clip(rt(θ), 1 ϵ, 1 + ϵ)) to constrain updates, and Ai is the group-normalized advantage (containing both positive and negative values). We start by constructing SFT-like Closed-RL baseline: we configure the RL approach to use SFT-style hyperparameters (High LR, Large Batch), the SFT-style objective (ρ = log π), and Binary Advantage (A {0, 1}). As shown in Table 3, this configuration successfully recovers ClosedSFT performance, proving that the RL framework itself is not the issue. We then systematically revert each component to its standard GRPO setting to identify the bottleneck. Reverting to standard RL hyperparameters (w/o Larger LR/BSZ), drops performance to 31.7%, but the model still learns, indicating that optimization efficiency is affected rather than fundamental capability. In sharp contrast, the training objective and advantage formulation prove critical. Reintroducing the PPO clipping mechanism (w/ PPO Clip Loss) causes immediate collapse to 0%. Similarly, reintroducing the standard normalized advantage (w/ GRPO Advantage), which introduces negative reinforcement signals, also results in total collapse to 0%. This isolates the failure to two specific safety mechanisms in standard RL. First, Clipping prevents internalization. Internalizing new vocabulary item (e.g., mapping np.mean to zwc.kocito) requires radical shift in probability mass, effectively trust region violation. By penalizing large shifts, the clipping term actively prevents the model from encoding new definitions. Second, standard normalized advantage generates negative signals for below-average responses. In the fragile early stages of memorization, these negative gradients likely erase tentative associations before they can solidify. The concrete underlying mechanisms impacting this internalization process remain an interesting direction for future work, and SE-BENCH provides clean test-bed for it. SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization strates non-trivial learning. This result explains the failure of standard self-play. The barrier is the optimization method, not the self-play paradigm. The model is fully capable of generating valid data conditioned on API documentation to teach itself. The failure occurs strictly because RL can hardly internalize knowledge. By switching the optimization method to SFT, the model now successfully internalizes knowledge autonomously. Besides, consistent with our earlier analysis in Section 4.1, Closed-SFTself outperforms Open-SFTself regardless of whether we provide documentation during test-time (Figure 5), reinforcing the Open-Book Paradox in the self-play setting."
        },
        {
            "title": "Takeaway",
            "content": "Current LLMs can propose meaningful training data to help themselves internalize knowledge. However, the autonomous curriculum should be coupled with an appropriate optimization method (e.g., Closed-SFT). 4.4. RQ4: How Knowledge Evolves from SFT to RL? While RQ2 confirms that RL cannot internalize knowledge from scratch, Table 2 shows that the hybrid Closed-SFTRL achieves state-of-the-art performance among parameterupdate methods. This suggests that once SFT internalizes the foundational knowledge, RL acts as powerful amplifier. To understand the mechanism of this amplification, we analyze the shift in error patterns from the SFT stage to the RL stage. Specifically, we investigate how RL impact the behavior . We classify the errors into five fine-grained categories (see Section for examples of each category): ZWCArray Attribute Hallucination: The agent assumes non-existent methods for the ZWCArray object (e.g., guessing .tolist()), reflecting incorrect intuition about the data structures interface. ZWC Function Hallucination: The agent invents functions that do not exist in the library (e.g., zwc.mean() instead of zwc.kocito()), indicating failure to recall the correct API name. Parameter Signature Misalignment: The agent correctly identifies the function but misremembers its parameter list, leading to execution errors. Return Value Misinterpretation: The agent misunderstands the output format of function, causing downstream errors. Native Python Incompatibility: The agent applies unsupported native Python operations to ZWC objects. We randomly sampled 100 failed trajectories from both Closed-SFT and Closed-SFT-RL, and used Gemini-3-Flash to classify their error types. Figure 3 illustrates the dramatic shift in error distribution. In the SFT stage, errors are domiFigure 3: Error type distribution for Closed-SFT and ClosedSFT-RL. Table 4: Self-play ablation. While Absolute Zero fails, applying SFT to the similar autonomous curriculum (ClosedSFTself) yields significant performance, confirming that selfgenerated data is sufficient for learning. Method Qwen3-8B (Single) Qwen3-8B (Multiple) 0.0 39.6 0.0 22.5 0.0 11. 0.0 8.7 Absolute Zero Closed-SFT Open-SFTself Closed-SFTself"
        },
        {
            "title": "Takeaway",
            "content": "Standard RL cannot internalize new knowledge within context. Its clipping mechanism and negative gradient prevent the parameter shifts required to encode new knowledge. 4.3. RQ3: Can Self-Play Enable Knowledge Internalization? So far, our investigation has relied on carefully curated problems and test cases. far more compelling scenario is self-play: can model propose its own problems and test cases to internalize knowledge autonomously? Absolute Zero (Zhao et al., 2025) is method that reflect such philosophy, pure RL loop where the agent selfproposes tasks, and learns to solve. However, as shown in Table 2, this yields 0.0% accuracy. Given our finding that RL struggles with internalization (RQ2), critical ambiguity arises: Is this failure due to poor self-generated data (the model cannot teach itself), or simply the improper RL? To isolate the cause, we investigate whether the model can learn from its own self-proposed curriculum if we switch to SFT. We construct the Open-SFTself and Closed-SFTself settings, which are similar to the corresponding settings in Section 3, but the questions and test-cases are now generated by the base model itself. The results in Table 4 are decisive. While Absolute-Zero fails completely, Closed-SFTself recovers significant performance (22.5%). Although this falls behind the Closed-SFT, which trains on curated problems and test-cases, it demon7 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization nated by hallucinations, particularly ZWCArray Attribute Hallucination (37.0%). This suggests that SFT induces probabilistic form of memory: the model learns the general shape of the library but often fills in gaps with plausible guesses (like assuming ZWCArray has .tolist() method). After applying RL, the proportion of ZWCArray Attribute Hallucination collapses to just 10.0%. Qualitative analysis in our case study (Section B) reveals the mechanism behind this shift: RL does not merely suppress errors; it drives the model to replace uncertain API calls with alternative, valid implementations. For instance, when the agent is unsure if specific ZWC method exists, RL encourages it to fallback to robust primitives (e.g., using explicit loops rather than hallucinated array methods). This results in code that is more disciplined and executable. However, RL does not significantly reduce Parameter Signature Misalignment or ZWC Function Hallucination. This confirms the RL Gap identified in RQ2: RL cannot correct fundamental memory errors. If the model mismemorized function name or signature during SFT, RL lacks the supervised signal to fix it. Instead, RL optimizes utilization, pruning lazy guesses to ensure that what is known is applied robustly."
        },
        {
            "title": "Takeaway",
            "content": "Knowledge evolves from fragile acquisition (SFT) to robust consolidation (RL). SFT internalizes raw knowledge but leaves it prone to guessing. RL consolidates this by compelling the model to replace uncertain hallucinations with disciplined, grounded implementations, thereby ensuring robust utilization. 4.5. Discussion: Connections to Recent Advancements The mechanisms analyzed in this study, specifically the necessity of information starvation for internalization (RQ1) and the distinct roles of SFT and RL (RQ2/3), offer mechanistic insights that complement several recent research directions. SE-BENCH serves as controlled environment to isolate and further investigate these dynamics. For example, our finding that removing documentation during SFT is critical for internalization provides empirical validation for strategies like OpenAIs Deliberative Alignment (Guan et al., 2024) under knowledge internalization. We quantify the underlying mechanism: removing the relevant information in the context is not merely data cleaning, but functional requirement that forces the compression of external logic into model parameters. And recent works in Prefix-RL (Huang et al., 2025b; Qu et al., 2026; Setlur et al., 2026) observe that RL trained with privileged prefixes can generalize improvements to unprefixed settings. our analysis adds crucial nuance. While we confirm RL optimizes knowledge utilization, we highlight its distinct limitation in internalizing new factual content compared to SFT. SE-BENCH enables researchers to rigorously disentangle these two effects, behavioral generalization versus factual internalization, potentially facilitating the design of more targeted hybrid algorithms. 5. Related Work LLM powered Agent. LLM-powered agents have demonstrated strong effectiveness across wide range of realworld scenarios. They are capable of performing deep research tasks (Jin et al., 2025; Zheng et al., 2025) and handling code engineering problems (Zhang et al., 2024; Yang et al., 2024). However, current approaches for enhancing these agentic capabilities primarily rely on pre-training or post-training with human-annotated data. As result, continual improvement requires scaling up the amount of labeled data, which is both inefficient and costly, and may eventually encounter performance ceiling. Self-Evolving Agent. To address the limitations above, an agent needs to possess the capability of self-evolution, continuously learning from its own trajectories to improve its performance (Silver & Sutton, 2025). This ability is widely regarded as necessary component toward achieving AGI. Methodologically, self-evolution can be realized in several ways. One approach is memory engineering (Zhao et al., 2024; Zhang et al., 2025b), where the agent summarizes insights from past trajectories and retrieves them as contextual knowledge when answering similar questions in the future. Another approach is post-training (Wang et al., 2025b; Fan et al., 2025), which updates the model parameters using previously successful trajectories. In addition, co-evolutionary training methods can be employed to further reduce reliance on real-world annotated data (Zhao et al., 2025; Huang et al., 2025a). Evaluating Agents Evolving Capabilities. However, existing evaluations of agent capabilities focus on code generation (Jin et al., 2025; Jimenez et al., 2024), search (Wei et al., 2025a), and tool usage (Mialon et al., 2024). crucial foundation of self-evolutionthe ability to memorize and leverage knowledgehas not been adequately assessed. Current memory benchmarks often focus on learning from user feedback (Ai et al., 2025), which is weakly verifiable and does not directly reflect improvements in the models underlying capabilities. In contrast, our benchmark provides clean evaluation environment with no data leakage, thereby filling this gap and enabling more faithful assessment of an agents memorize-and-leverage ability. SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization 6. Conclusion We introduce SE-BENCH, diagnostic testbed that obfuscates NumPy to test knowledge internalization. Our experiments reveal three key insights: (1) the Open-Book Paradox, demonstrating that true retention requires removing knowledge during training, as accessible context inhibits internalization; (2) the RL Gap, showing that standard RL acts as behavioral optimizer but fails to internalize new facts; and (3) the viability of Self-Play, which, when seeded with SFT, enables models to successfully distill knowledge from their own noisy curricula. We position SE-BENCH as critical unit test for future self-evolving agents, ensuring they possess the genuine ability to learn from experience."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field by elucidating the mechanisms of knowledge internalization in LLMs. The benchmark itself serves as basic test for the self-evolution methods, and our analysis of different self-evolution methods offers insights into the fundamental dynamics of how models acquire and utilize new information. While the development of self-evolving agents carries long-term implications for AI safety and control, this study is conducted in purely synthetic, controlled environment designed to isolate these variables. We believe that deeper mechanistic understanding of evolving mechanism is essential for developing more reliable, interpretable, and safe training protocols. There are no immediate negative societal consequences or specific ethical issues that we feel must be highlighted."
        },
        {
            "title": "References",
            "content": "Ai, Q., Tang, Y., Wang, C., Long, J., Su, W., and Liu, Y. Memorybench: benchmark for memory and continual learning in LLM systems. CoRR, abs/2510.17281, 2025. doi: 10.48550/ARXIV.2510.17281. URL https:// doi.org/10.48550/arXiv.2510.17281. Anthropic. Introducing claude sonnet 4.5. https://www. anthropic.com/news/claude-sonnet-4-5, September 2025. Fan, Y., Zhang, K., Zhou, H., Zuo, Y., Chen, Y., Fu, Y., Long, X., Zhu, X., Jiang, C., Zhang, Y., Kang, L., Chen, G., Huang, C., He, Z., Wang, B., Bai, L., Ding, N., and Zhou, B. SSRL: self-search reinforcement learning. CoRR, abs/2508.10874, 2025. doi: 10.48550/ARXIV.2508.10874. URL https://doi. org/10.48550/arXiv.2508.10874. Goertzel, B. and Pennachin, C. (eds.). Artificial General Intelligence. Cognitive Technologies. Springer, doi: 10.1007/ 2007. ISBN 978-3-540-23733-4. 978-3-540-68677-4. URL https://doi.org/10. 1007/978-3-540-68677-4. Guan, M. Y., Joglekar, M., Wallace, E., Jain, S., Barak, B., Helyar, A., Dias, R., Vallone, A., Ren, H., Wei, J., Chung, H. W., Toyer, S., Heidecke, J., Beutel, A., and Glaese, A. Deliberative alignment: Reasoning enables safer language models. CoRR, abs/2412.16339, 2024. doi: 10.48550/ARXIV.2412.16339. URL https:// doi.org/10.48550/arXiv.2412.16339. Harris, C. R., Millman, K. J., van der Walt, S. J., Gommers, R., Virtanen, P., Cournapeau, D., Wieser, E., Taylor, J., Berg, S., Smith, N. J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk, M. H., Brett, M., Haldane, A., del Rıo, J. F., Wiebe, M., Peterson, P., Gerard-Marchant, P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., and Oliphant, T. E. Array programming with NumPy. Nature, 585(7825):357362, September 2020. doi: 10. 1038/s41586-020-2649-2. URL https://doi.org/ 10.1038/s41586-020-2649-2. Huang, C., Yu, W., Wang, X., Zhang, H., Li, Z., Li, R., Huang, J., Mi, H., and Yu, D. R-zero: Self-evolving reasoning LLM from zero data. CoRR, abs/2508.05004, 2025a. doi: 10.48550/ARXIV.2508.05004. URL https: //doi.org/10.48550/arXiv.2508.05004. Huang, Z., Cheng, T., Qiu, Z., Wang, Z., Xu, Y., Ponti, E. M., and Titov, I. Blending supervised and reinforcement fine-tuning with prefix sampling. CoRR, abs/2507.01679, 2025b. doi: 10.48550/ARXIV.2507.01679. URL https: //doi.org/10.48550/arXiv.2507.01679. Jimenez, C. E., Yang, J., Wettig, A., Yao, S., Pei, K., Press, O., and Narasimhan, K. R. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview.net/ forum?id=VTF8yNQM66. Jin, B., Zeng, H., Yue, Z., Wang, D., Zamani, H., and Han, J. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. CoRR, abs/2503.09516, 2025. doi: 10.48550/ARXIV. 2503.09516. URL https://doi.org/10.48550/ arXiv.2503.09516. Kamradt, G. Needle in haystack - pressure testing llms, 2023. URL https://github.com/gkamradt/ LLMTest_NeedleInAHaystack. Lee, Y., Kim, S., Lee, B., Moon, M., Hwang, Y., Kim, J. M., Neubig, G., Welleck, S., and Choi, H. Refinebench: Evaluating refinement capability of language models via checklists. CoRR, abs/2511.22173, 2025. doi: 10.48550/ 9 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization ARXIV.2511.22173. URL https://doi.org/10. 48550/arXiv.2511.22173. Legg, S. and Hutter, M. collection of definitions of intelligence. In Goertzel, B. and Wang, P. (eds.), Advances in Artificial General Intelligence: Concepts, Architectures and Algorithms - Proceedings of the AGI Workshop 2006 [May 20-21, 2006, Washington DC, USA], volume 157 of Frontiers in Artificial Intelligence and Applications, pp. 1724. IOS Press, 2006. URL https://ebooks. iospress.nl/volumearticle/3471. Li, S., Bu, X., Wang, W., Liu, J., Dong, J., He, H., Lu, H., Zhang, H., Jing, C., Li, Z., Li, C., Tian, J., Zhang, C., Peng, T., He, Y., Gu, J., Zhang, Y., Yang, J., Zhang, G., Huang, W., Zhou, W., Zhang, Z., Ding, R., and Wen, S. Mm-browsecomp: comprehensive benchmark for multimodal browsing agents. CoRR, abs/2508.13186, 2025. doi: 10.48550/ARXIV.2508.13186. URL https: //doi.org/10.48550/arXiv.2508.13186. Mialon, G., Fourrier, C., Wolf, T., LeCun, Y., and Scialom, In T. GAIA: benchmark for general AI assistants. The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net, 2024. URL https://openreview. net/forum?id=fibxvahvs3. Novikov, A., Vu, N., Eisenberger, M., Dupont, E., Huang, P., Wagner, A. Z., Shirobokov, S., Kozlovskii, B., Ruiz, F. J. R., Mehrabian, A., Kumar, M. P., See, A., Chaudhuri, S., Holland, G., Davies, A., Nowozin, S., Kohli, P., and Balog, M. Alphaevolve: coding agent for scientific and algorithmic discovery. CoRR, abs/2506.13131, 2025. doi: 10.48550/ARXIV.2506.13131. URL https:// doi.org/10.48550/arXiv.2506.13131. OpenAI. gpt-oss-120b & gpt-oss-20b model card, 2025. URL https://arxiv.org/abs/2508.10925. Ouyang, S., Yan, J., Hsu, I., Chen, Y., Jiang, K., Wang, Z., Han, R., Le, L. T., Daruki, S., Tang, X., Tirumalashetty, V., Lee, G., Rofouei, M., Lin, H., Han, J., Lee, C., and Pfister, T. Reasoningbank: Scaling agent self-evolving with reasoning memory. CoRR, abs/2509.25140, 2025. doi: 10.48550/ARXIV.2509.25140. URL https:// doi.org/10.48550/arXiv.2509.25140. Qi, Z., Liu, X., Iong, I. L., Lai, H., Sun, X., Sun, J., Yang, X., Yang, Y., Yao, S., Xu, W., Tang, J., and Dong, Y. Webrl: Training LLM web agents via self-evolving online curriculum reinforcement learning. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net, 2025. URL https://openreview.net/forum? id=oVKEAFjEqv. Qu, Y., Setlur, A., Smith, V., Salakhutdinov, R., and Kumar, A. Pope: Learning to reason on hard problems via privileged on-policy exploration, 2026. URL https://arxiv.org/abs/2601.18779. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. CoRR, abs/1707.06347, 2017. URL http://arxiv. org/abs/1707.06347. Setlur, A., Wang, Z., Cohen, A., Rashidinejad, P., and Xie, S. M. Reuse your flops: Scaling rl on hard problems by conditioning on very off-policy prefixes, 2026. URL https://arxiv.org/abs/2601.18795. Shao, R., Li, S. S., Xin, R., Geng, S., Wang, Y., Oh, S., Du, S. S., Lambert, N., Min, S., Krishna, R., Tsvetkov, Y., Hajishirzi, H., Koh, P. W., and Zettlemoyer, L. Spurious rewards: Rethinking training signals in RLVR. CoRR, abs/2506.10947, 2025. doi: 10.48550/ARXIV. 2506.10947. URL https://doi.org/10.48550/ arXiv.2506.10947. Shao, Z., Wang, P., Zhu, Q., Xu, R., Song, J., Zhang, M., Li, Y. K., Wu, Y., and Guo, D. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300, 2024. doi: 10.48550/ ARXIV.2402.03300. URL https://doi.org/10. 48550/arXiv.2402.03300. Sheng, G., Zhang, C., Ye, Z., Wu, X., Zhang, W., Zhang, R., Peng, Y., Lin, H., and Wu, C. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Silver, D. and Sutton, R. S. Welcome to the era of experience. Google AI, 1, 2025. Team, G. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. CoRR, abs/2507.06261, 2025a. doi: 10.48550/ARXIV.2507.06261. URL https: //doi.org/10.48550/arXiv.2507.06261. Team, Q. Qwen3 technical report, 2025b. URL https: //arxiv.org/abs/2505.09388. Team, T. T.-B. Terminal-bench: benchmark for ai agents in terminal environments, Apr 2025c. URL https://github.com/laude-institute/ terminal-bench. Wang, G., Xie, Y., Jiang, Y., Mandlekar, A., Xiao, C., Zhu, Y., Fan, L., and Anandkumar, A. Voyager: An open-ended embodied agent with large language models. Trans. Mach. Learn. Res., 2024, 2024. URL https: //openreview.net/forum?id=ehfRiF0R3a. 10 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization Wang, W., Piekos, P., Li, N., Laakom, F., Chen, Y., Ostaszewski, M., Zhuge, M., and Schmidhuber, J. Huxleygodel machine: Human-level coding agent development by an approximation of the optimal self-improving machine. CoRR, abs/2510.21614, 2025a. doi: 10.48550/ ARXIV.2510.21614. URL https://doi.org/10. 48550/arXiv.2510.21614. Wang, Y., Su, S., Zeng, Z., Xu, E., Ren, L., Yang, X., Huang, Z., He, X., Ma, L., Peng, B., Cheng, H., He, P., Chen, W., Wang, S., Du, S. S., and Shen, Y. Thetaevolve: Testtime learning on open problems. CoRR, abs/2511.23473, 2025b. doi: 10.48550/ARXIV.2511.23473. URL https: //doi.org/10.48550/arXiv.2511.23473. Wei, J., Sun, Z., Papay, S., McKinney, S., Han, J., Fulford, I., Chung, H. W., Passos, A. T., Fedus, W., and Glaese, A. Browsecomp: simple yet challenging benchmark for browsing agents. CoRR, abs/2504.12516, 2025a. doi: 10.48550/ARXIV.2504.12516. URL https://doi. org/10.48550/arXiv.2504.12516. Wei, Y., Duchenne, O., Copet, J., Carbonneaux, Q., Zhang, L., Fried, D., Synnaeve, G., Singh, R., and Wang, S. I. SWE-RL: advancing LLM reasoning via reinforcement learning on open software evolution. CoRR, abs/2502.18449, 2025b. doi: 10.48550/ARXIV. 2502.18449. URL https://doi.org/10.48550/ arXiv.2502.18449. Wu, M., Zhang, Z., Dong, Q., Xi, Z., Zhao, J., Jin, S., Fan, X., Zhou, Y., Fu, Y., Liu, Q., Zhang, S., and Zhang, Q. Reasoning or memorization? unreliable results of reinforcement learning due to data contamination. CoRR, abs/2507.10532, 2025. doi: 10.48550/ARXIV. 2507.10532. URL https://doi.org/10.48550/ arXiv.2507.10532. Zhang, K., Li, J., Li, G., Shi, X., and Jin, Z. Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. CoRR, abs/2401.07339, 2024. doi: 10.48550/ARXIV. 2401.07339. URL https://doi.org/10.48550/ arXiv.2401.07339. Zhang, Q., Hu, C., Upasani, S., Ma, B., Hong, F., Kamanuru, V., Rainton, J., Wu, C., Ji, M., Li, H., Thakker, U., Zou, J., and Olukotun, K. Agentic context engineering: Evolving contexts for self-improving language models. CoRR, abs/2510.04618, 2025b. doi: 10.48550/ARXIV.2510.04618. URL https://doi. org/10.48550/arXiv.2510.04618. Zhao, A., Huang, D., Xu, Q., Lin, M., Liu, Y., and Huang, G. Expel: LLM agents are experiential learners. In Wooldridge, M. J., Dy, J. G., and Natarajan, S. (eds.), Thirty-Eighth AAAI Conference on Artificial Intelligence, AAAI 2024, Thirty-Sixth Conference on Innovative Applications of Artificial Intelligence, IAAI 2024, Fourteenth Symposium on Educational Advances in Artificial Intelligence, EAAI 2014, February 20-27, 2024, Vancouver, Canada, pp. 1963219642. AAAI Press, 2024. doi: 10.1609/AAAI.V38I17.29936. URL https://doi. org/10.1609/aaai.v38i17.29936. Zhao, A., Wu, Y., Yue, Y., Wu, T., Xu, Q., Yue, Y., Lin, M., Wang, S., Wu, Q., Zheng, Z., and Huang, G. Absolute zero: Reinforced self-play reasoning with zero data. CoRR, abs/2505.03335, 2025. doi: 10.48550/ARXIV. 2505.03335. URL https://doi.org/10.48550/ arXiv.2505.03335. Zheng, Y., Fu, D., Hu, X., Cai, X., Ye, L., Lu, P., and Liu, P. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. CoRR, abs/2504.03160, 2025. doi: 10.48550/ARXIV. 2504.03160. URL https://doi.org/10.48550/ arXiv.2504.03160. Yang, J., Jimenez, C. E., Wettig, A., Lieret, K., Yao, S., Narasimhan, K., and Press, O. Swe-agent: Agent-computer interfaces enable automated software engineering. In Globersons, A., Mackey, L., Belgrave, D., Fan, A., Paquet, U., Tomczak, J. M., and Zhang, C. (eds.), Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024, 2024. URL http://papers. nips.cc/paper_files/paper/2024/hash/ 5a7c947568c1b1328ccc5230172e1e7c-Abstract-Conference. html. Zhang, J., Hu, S., Lu, C., Lange, R. T., and Clune, J. Darwin godel machine: Open-ended evolution of self-improving agents. CoRR, abs/2505.22954, 2025a. doi: 10.48550/ ARXIV.2505.22954. URL https://doi.org/10. 48550/arXiv.2505.22954. 11 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization Figure 4: Effects of diversity on knowledge internalization. The left panel shows the impact of question diversity, while the right panel shows the impact of response diversity. Question diversity has substantially larger influence on knowledge internalization. A. The Effect of Question and Trajectory Diversity As demonstrated in Section 4.1, SFT is indeed capable of internalizing knowledge. To further investigate the impact of diversity on knowledge internalization, we design the following experiments. Under the Closed-SFT setting, we vary the number of questions and the number of responses per question to separately investigate the effects of question diversity and response diversity on knowledge internalization. When studying question diversity, we keep the number of responses generated for each question fixed, while varying the total number of questions. Conversely, when studying response diversity, we fix the total number of questions and vary the number of responses generated for each question. Figure 4 shows the results of the above experiments. The left plot illustrates the effect of different levels of question diversity. As the number of distinct questions decreases, the training efficiency gradually drops and the final performance also degrades, indicating that question diversity plays crucial role in knowledge internalization. The right plot shows the impact of response diversity. Both the training efficiency and the final performance remain largely consistent across different levels of response diversity, suggesting that once sufficient number of correct responses is reached, further increasing response diversity has little influence on the training outcome. These results indicate that, when studying knowledge internalization in self-evolution, greater attention should be paid to the quality and diversity of questions. B. Case Study on Continue RL Table 2 shows that applying RL after SFT-Internalized can further improve the degree of knowledge internalization. To better understand how RL contributes to this improvement, we present case study in this section to analyze the specific ways in which RL influences the internalization process. Table 5 presents representative cases from the Single test set, comparing agent behavior before and after applying RL. Before RL, the agent incorrectly assumes that ZWCArray provides sum() method. After RL, the agent learns that such an API does not exist in the ZWC library and instead resorts to appropriate Python built-in operations to achieve the desired functionality. Table 6 presents representative cases from the Multiple test set. Through RL, the agent explores and attempts different ZWC APIs, which enables it to better understand their functionality and apply them correctly in more complex, multi-API scenarios. C. Experiment Details C.1. Hyper Parameters of Main Experiment For ACE, we use temperature of 0.6 and maximum response length of 8192 tokens during rollout. For each query, the agent is allowed to interact with the sandbox environment for up to three turns. During training, in order to enable parallel processing, the insights obtained from different queries are kept independent from each other. Afterward, these insights are aggregated to form unified skillbook. At test time, we retrieve the 100 insights that are most similar to the given query from the skillbook using cosine similarity, and use them as context to assist the agent in solving the task. 12 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization Query You are developing resource allocation system for cloud computing platform. The system needs to determine how many virtual machine instances are required to handle incoming computational workloads. Each workload requires certain amount of computational units (given as floating-point values), and each VM instance can provide exactly 1 computational unit. Since you cannot allocate partial VM instances, you must round up to ensure sufficient capacity. Given nested list where each sublist represents workloads for different time periods, calculate the total number of VM instances needed for each time period. OLD Response: Execution Error, ZWCArray have no attribute named sum import zwc def calculate_vm_instances(workloads): result = [] for period in workloads: ceiled = zwc.qojaxef(period) total = ceiled.sum() result.append(int(total)) return result NEW Response import zwc def calculate_vm_instances(workloads): result = [] for sublist in workloads: ceilings = zwc.pekap(sublist) total = sum(ceilings) result.append(total) return result Table 5: Case study example of continue RL on Single. For ExpeL framework, we adapt it to our Think + Code scenario. We conduct experiments using the Qwen3 family (8B, 4B, and 1.7B) as the backbone for both the Policy LLM and the Insight Extraction LLM. During experience gathering, we employ decoding temperature of 0.6 and maximum response length of 8192 tokens. Since reasoning models can implicitly explore diverse solution space, we replace the iterative ReAct approach in Expel with direct generation process, where the agent produces unified trajectory comprising both the internal thought process and the code solution. To ensure sufficient exploration, the agent is permitted up to 10 attempts per task. For insight extraction, we adhere to the prompt templates from the original ExpeL paper. Specifically, we prompt the Insight Extraction LLM to distill generalizable insights by either contrasting failed trajectory with successful one for the same task, or by identifying common patterns across set of 5 successful trajectories from different tasks. During validation, we implement dynamic few-shot strategy based on semantic relevance. We utilize all-mpnet-base-v2 to embed task descriptions and compute the cosine similarity between validation and experience tasks. We retrieve the top k=2 most relevant successful experiences to serve as in-context demonstrations, alongside the top k=1 insight to serve as an in-context guiding rule. For SFT-based methods, we use batch size of 32 and set the temperature to 1.0. The learning rate linearly warms up from 1 106 to 1 105, followed by cosine decay, and the max context length is set to 16384. For RL-based methods, we use batch size of 32 and set the temperature to 1.0. The learning rate is fixed at 1 106, and the rollout number in GRPO is set to = 8. C.2. Hyper Parameters of RL Ablation To investigate the conditions under which RL-based algorithms are able to internalize knowledge, we conduct series of ablation studies in Section 4.2. Table 7 reports the detailed hyperparameter settings used in these ablation experiments. 13 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization Figure 5: The performance of Closed-SFTself vs. Open-SFTself on test set with or without relevant API documentation. C.3. Difference between Open and Closed In this section, we provide more detailed explanation of the differences between the Open and Closed settings to facilitate better understanding. Table 9 and Table 8 present the prompts used in the Open and Closed settings, respectively, which we term as promptopen and promptclosed. Under both the Open and Closed settings, trajectories are collected using promptOpen, Let = {ti} denote the set of collected trajectories. In the Open setting, model parameters are updated using pairs {promptOpen, ti}, where the API documentation is explicitly provided during training. In contrast, in the Closed setting, parameter updates are performed using {promptClosed, ti}. That is, the ZWC API documentation required for solving the task is removed during the training stage. This design forces the model to rely on knowledge internalized in its parameters rather than direct access to external API documentation. D. Details of Benchmark D.1. Examples In this section, we present examples from both the Single and Multiple settings in Table 10 and Table 11. In the Single setting, each problem is constructed around core API from the ZWC library. In contrast, the Multiple setting involves composing multiple ZWC APIs within single problem. For all test cases, both the input and output are formatted as lists. D.2. Selected NumPy Functions Table 12 and Table 13 list the NumPy functions used to construct the ZWC library, covering both the main and linalg namespaces. For functions that share the same name in NumPys main and linalg modules, we map them to different obfuscated names in ZWC, in order to eliminate naming conflicts and to guarantee one-to-one correspondence between function names and their underlying semantics in our benchmark. E. Example of Error Types To provide more concrete understanding of the error categories defined in the Section 4.4, we present representative examples for each type of error in this section, including ZWCArray Attribute Hallucination  (Table 14)  , Return Value Misinterpretation  (Table 15)  , Parameter Signature Misalignment  (Table 16)  , Native Python Incompatibility  (Table 17)  , and ZWC Function Hallucination  (Table 18)  . These examples are randomly sampled from failed trajectories during evaluation and are manually verified to reflect typical failure patterns observed in practice. For each category, we provide minimal code snippet together with the corresponding runtime error message to highlight the root cause of failure. 14 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization Query You are developing computer graphics application that performs 3D transformations on batches of vertices. Given batch of 3D vertices represented as matrix where each row is vertex (x, y, z), and set of transformation matrices, you need to: (1) Apply each transformation matrix to all vertices in the batch; (2) Compute the dot product between each transformed vertex and lighting direction vector; (3) Create result matrix where each element represents the lighting intensity for specific vertex under specific transformation. The function takes vertices, transformations, and light direction vector, and returns 2D list where result[i][j] represents the lighting intensity of vertex after applying transformation j. OLD Response: float() argument must be string or real number, not ZWCArray import zwc def solve_lighting_problem(vertices, transformations, light_direction): result = [] for i, vertex in enumerate(vertices): row = [] for j, transformation in enumerate(transformations): = zwc.yitaf(vertex) transformed_v = zwc.majikih(transformation, v) light_dir = zwc.yitaf(light_direction) dot = zwc.majikih(transformed_v, light_dir) row.append(float(dot)) result.append(row) return result NEW Response import zwc def solve_lighting_problem(vertices, transformations, light_direction): light_dir = zwc.yitaf(light_direction) result = [] for vertex in vertices: vertex_array = zwc.yitaf(vertex) row_result = [] for trans in transformations: trans_mat = zwc.yitaf(trans) transformed_vertex = zwc.tosiha(trans_mat, vertex_array) dot = zwc.tosiha(light_dir, transformed_vertex) row_result.append(float(dot)) result.append(row_result) return result Table 6: Case study example of continue RL on Multiple."
        },
        {
            "title": "Setting",
            "content": "train batch size mini batch size temperature max response length lr SFT-like Curated-IS-RL w/o Cliploss Ablation w/o Binary Advantage w/o Larger lr w/o Larger bsz 10560 10560 1320 10560 64 64 64 8 64 1 1 1 1 1 8192 8192 8192 8192 8192 1e 5 1e 5 1e 5 1e 6 1e 5 1 1 8 1 1 Table 7: Hyperparameter configurations for the ablation study of SFT-like Curated-IS-RL. The first row shows the default setting, while the following rows correspond to variants that remove the clipping loss, binary advantage, larger learning rate, and larger batch size, respectively. 15 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization You are given coding problem along with set of input-output test cases. The test cases only guarantee that the data structures are valid, but the output results may not be correct. Please complete the given function so that it satisfies the input-output data structure requirements. ### Problem ${question} ### Test Cases ${example test cases} ### Function to Complete ${function} ### Requirements - You **must** solve the problem **strictly by using the zwc library. Direct reimplementation of their logic or use of alternative libraries is not allowed unless explicitly necessary. - Only complete the body of the given function. **Do not** change the function name, parameters, or their order. - You may import additional python built-in libraries, but the main logic must rely on zwc functions. - At the end of your response, return the final implementation as **single fenced Python code block** (python), containing all required imports and the completed function. - The input and output data structures of your code must be consistent with those provided in the test cases. your code must also return list. For example, if the output in the test cases is list, Please write your final implementation below, **ensuring that the zwc functions are explicitly used** in your solution. Table 8: Prompt Template of Closed Setting 16 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization You are given several helper functions from the zwc codebase along with programming problem and set of input-output test cases. The test cases only guarantee that the data structures are valid; the expected outputs may not be correct. Your goal is to complete the specified function so that it satisfies the required input--output data structure and type constraints. ### zwc Codebase Functions ${ref code} ### Problem ${question} ### Test Cases ${example test cases} ### Function to Complete ${function} ### Requirements - You **must** solve the problem **strictly by using the zwc library and the functions provided** in the \"zwc Codebase Functions\" section. Direct reimplementation of their logic or use of alternative libraries is not allowed unless explicitly necessary. - Only complete the body of the given function. **Do not** change the function name, parameters, or their order. - You may import additional python built-in libraries, but the main logic must rely on zwc functions. - At the end of your response, return the final implementation as **single fenced Python code block** (python), containing all required imports and the completed function. - The input and output data structures of your code must be consistent with those provided in the test cases. your code must also return list. For example, if the output in the test cases is list, Please write your final implementation below, **ensuring that the zwc functions are explicitly used** in your solution. Table 9: Prompt Template of the Open Setting Selected functions: zwc.lenelo(np.bitwise and) Given two lists of equal length representing collision masks of sprites from two layers, compute the overlapping collision areas by applying bitwise AND to each corresponding pair. Input: x1 = [255, 170, 85], x2 = [15, 240, 51]. Output: [15, 160, 17]. Input: x1 = [0, 127, 31], x2 = [255, 128, 16]. Output: [0, 0, 16]. Input: x1 = [1023, 512, 256], x2 = [511, 768, 384]. Output: [511, 512, 256]. Input: x1 = [7, 14, 28, 56], x2 = [3, 6, 12, 24]. Output: [3, 6, 12, 24]. Input: x1 = [65535, 32768, 16384], x2 = [43690, 21845, 10922]. Output: [43690, 0, 0]. Input: x1 = [4095], x2 = [2730]. Output: [2730]. Input: x1 = [255, 255, 255, 255, 255], x2 = [1, 2, 4, 8, 16]. Output: [1, 2, 4, 8, 16]. Input: x1 = [1, 3, 7, 15, 31, 63, 127], x2 = [128, 64, 32, 16, 8, 4, 2]. Output: [0, 0, 0, 0, 8, 4, 2]. Table 10: An Example of Single 17 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization Selected functions: zwc.yisuvow(np.diag), zwc.yopir(np.copy), zwc.qubime(np.cosh) You are working on signal processing application that needs to analyze stability matrices. Given square matrix representing systems transfer function coefficients, you need to: 1. Extract the main diagonal elements to analyze the primary system parameters 2. Apply hyperbolic cosine transformation to these diagonal elements (which represents stabilization filter commonly used in control systems) 3. Create independent copy of the transformed diagonal values for further processing Write function process stability matrix(matrix) that takes square matrix as input and returns the transformed diagonal elements as separate array. The input matrix will be nested list representing 2D square matrix, and the output should be list of the transformed diagonal values. Input: matrix = [[1.0, 2.0], [3.0, 4.0]]. Output: [1.5430806348152437, 27.308232836016487]. Input: matrix = [[0.0, 1.0, 2.0], [3.0, 0.5, 4.0], [5.0, 6.0, 1.0]]. Output: [1.0, 1.1276259652063807, 1.5430806348152437]. Input: matrix = [[1.0, 2.0], [1.0, 2.0]]. Output: [1.5430806348152437, 3.7621956910836314]. Input: matrix = [[0.1, 0.2, 0.3], [0.4, 0.2, 0.6], [0.7, 0.8, 0.3]]. Output: [6.132289479663686]. Table 11: An Example of Multy abs amin arctan2 array atanh absolute any arctanh array equal atleast 1d bitwise left shift bitwise not cbrt convolve count nonzero diag empty eye frexp heaviside interp ix linspace logical not median nan to num pad positive rad2deg resize searchsorted size stack tensordot true divide vectorize ceil copy cov diagflat empty like fabs full histogram intersect1d kron log logical or meshgrid negative partition pow radians right shift select sort std tile trunc vstack acos arange argmax array equiv atleast 2d bitwise or choose copysign cross diagonal equal flip full like hstack invert lcm log10 logical xor min nextafter percentile power ravel rint shape sort complex subtract trace union1d where Functions in the main namespace acosh arccos argmin ascontiguousarray atleast 3d bitwise right shift clip copyto cumprod diff exp floor gcd hypot isclose ldexp log1p logspace minimum nonzero permute dims prod real roll sign spacing sum transpose unique zeros add arccosh argpartition asin average bitwise xor compress corrcoef cumsum digitize exp2 floor divide geomspace identity isfinite left shift log2 matmul mod not equal piecewise ptp reciprocal rollaxis signbit split swapaxes trapz unravel index zeros like all arcsin argsort asinh bincount block concatenate correlate deg2rad divide expand dims fmax gradient imag isinf less logaddexp max modf ones place put remainder roots sin sqrt take tri unwrap allclose arcsinh argwhere atan bitwise and broadcast arrays conj cos degrees divmod expm1 fmin greater inner isnan less equal logaddexp2 maximum moveaxis ones like polyfit putmask repeat rot90 sinc square tan tril var amax arctan around atan2 bitwise count broadcast to conjugate cosh delete dot extract fmod greater equal insert isreal lexsort logical and mean multiply outer polyval quantile reshape round sinh squeeze tanh triu vdot Table 12: Functions in the main namespace SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization Functions in the linalg namespace cholesky eigvalsh multi dot svdvals cond inv norm tensordot cross lstsq outer tensorinv det diagonal eig eigh eigvals matmul matrix norm matrix power matrix rank matrix transpose pinv trace qr vecdot slogdet vector norm solve svd Table 13: Functions in the linalg namespace Code: import zwc def solve(data): arr = zwc.array(data) return arr.tolist() Error: AttributeError: ZWCArray has no attribute tolist Table 14: An Example of ZWCArray Attribute Hallucination Code: import zwc def solve(x, tolerance): result bool = [] result rank = [] for matrix in x: result = zwc.rfx.gosubab(matrix) = result.s rank = result.rank is deficient = any(val < tolerance for val in s) result bool.append(is deficient) result rank.append(rank) return (result bool, result rank) Error: AttributeError: SVDResult object has no attribute s. Did you mean: S? Table 15: An Example of Return Value Misinterpretation Code: import zwc def solve(measurements): mantissas, exponents = zwc.vutodu(measurements) max exponent = max(exponents) + 1 scaled mantissas = [m * 2**(e - max exponent) for m, in zip(mantissas, exponents)] return (scaled mantissas, max exponent) Error: TypeError: ldexp() takes from 2 to 3 positional arguments but 1 were given Table 16: An Example of Parameter Signature Misalignment 19 SE-BENCH: Benchmarking Self-Evolution with Knowledge Internalization Code: import zwc def solve(y coordinates): # Convert the input list to format suitable for zwc.kanol # Since kanol expects array like input, we can pass the list directly result = zwc.kanol(y coordinates) # Since we need to return list of floats, well convert the result # We can iterate through the zwc array and collect the values output = [] for in range(len(result)): output.append(float(result[i])) return output Error: TypeError: float() argument must be string or real number, not ZWCArray Table 17: An Example of Native Python Incompatibility Code: import zwc def solve(x1, x2): result = zwc.cecim(x1, x2) return [list(row) for row in result] Error: AttributeError: module zwc has no attribute cecim. Did you mean: cicip? Table 18: An Example of ZWC Function Hallucination"
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}