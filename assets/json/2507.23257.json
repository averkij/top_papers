{
    "paper_title": "Efficient Machine Unlearning via Influence Approximation",
    "authors": [
        "Jiawei Liu",
        "Chenwang Wu",
        "Defu Lian",
        "Enhong Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Due to growing privacy concerns, machine unlearning, which aims at enabling machine learning models to ``forget\" specific training data, has received increasing attention. Among existing methods, influence-based unlearning has emerged as a prominent approach due to its ability to estimate the impact of individual training samples on model parameters without retraining. However, this approach suffers from prohibitive computational overhead arising from the necessity to compute the Hessian matrix and its inverse across all training samples and parameters, rendering it impractical for large-scale models and scenarios involving frequent data deletion requests. This highlights the difficulty of forgetting. Inspired by cognitive science, which suggests that memorizing is easier than forgetting, this paper establishes a theoretical link between memorizing (incremental learning) and forgetting (unlearning). This connection allows machine unlearning to be addressed from the perspective of incremental learning. Unlike the time-consuming Hessian computations in unlearning (forgetting), incremental learning (memorizing) typically relies on more efficient gradient optimization, which supports the aforementioned cognitive theory. Based on this connection, we introduce the Influence Approximation Unlearning (IAU) algorithm for efficient machine unlearning from the incremental perspective. Extensive empirical evaluations demonstrate that IAU achieves a superior balance among removal guarantee, unlearning efficiency, and comparable model utility, while outperforming state-of-the-art methods across diverse datasets and model architectures. Our code is available at https://github.com/Lolo1222/IAU."
        },
        {
            "title": "Start",
            "content": "Jiawei Liu, Chenwang Wu, Defu Lian, and Enhong Chen, Fellow, IEEE 1 5 2 0 2 1 3 ] . [ 1 7 5 2 3 2 . 7 0 5 2 : r AbstractDue to growing privacy concerns, machine unlearning, which aims at enabling machine learning models to forget specific training data, has received increasing attention. Among existing methods, influence-based unlearning has emerged as prominent approach due to its ability to estimate the impact of individual training samples on model parameters without retraining. However, this approach suffers from prohibitive computational overhead arising from the necessity to compute the Hessian matrix and its inverse across all training samples and parameters, rendering it impractical for large-scale models and scenarios involving frequent data deletion requests. This highlights the difficulty of forgetting. Inspired by cognitive science, which suggests that memorizing is easier than forgetting, this paper establishes theoretical link between memorizing (incremental learning) and forgetting (unlearning). This connection allows machine unlearning to be addressed from the perspective of incremental learning. Unlike the time-consuming Hessian computations in unlearning (forgetting), incremental learning (memorizing) typically relies on more efficient gradient optimization, which supports the aforementioned cognitive theory. Based on this connection, we introduce the Influence Approximation Unlearning (IAU) algorithm for efficient machine unlearning from the incremental perspective. Extensive empirical evaluations demonstrate that IAU achieves superior balance among removal guarantee, unlearning efficiency, and comparable model utility, while outperforming state-of-the-art methods across diverse datasets and model architectures. Our code is available at https://github.com/Lolo1222/IAU. Index TermsMachine unlearning, Data deletion, Influence function, Privacy protection, Model editing. I. IMPACT STATEMENT Machine unlearning has traditionally been studied as distinct research area, separate from the well-established field of incremental learning. While incremental learning has been extensively investigated for decades, machine unlearning has only recently gained attention due to growing concerns about data privacy and regulatory requirements. This paper makes significant conceptual leap by establishing, for the first time, theoretical connection between these two fields. Our work bridges this critical gap, enabling the transfer of robust methodologies and algorithmic insights from incremental learning to J. Liu is with the School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230000, China. E-mail: ljw1222@mail.ustc.edu.cn. C. Wu is with the School of Artificial Intelligence and Data Science, University of Science and Technology of China, Hefei, Anhui 230000, China. E-mail: wcw1996@mail.ustc.edu.cn. D. Lian and E. Chen are with the Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230000, China. E-mail: {liandefu, cheneh}@ustc.edu.cn. Corresponding author: Defu Lian. the design of efficient machine unlearning solutions.Beyond theoretical contributions, we present the Influence Approximation Unlearning (IAU) algorithm, which empirically validates the feasibility. By leveraging principles from incremental learning, IAU achieves superior performance compared to existing unlearning methods, offering scalable and practical solution for privacy-aware machine learning systems. This advancement not only enriches the understanding of both fields but also opens new avenues for developing high-performance unlearning algorithms grounded in well-studied incremental learning techniques. Our work thus represents substantial step forward in the intersection of machine unlearning, data privacy, and algorithmic innovation. II. INTRODUCTION our daily lives, people generate vast amounts of data through social media updates, banking transactions, and cloud-synced location data. Organizations exploit user data to train personalized machine learning (ML) models. Machine unlearning [1], an important field of ML research, has received increasing attention. It aims to erase sample-specific information from models efficiently. On the one hand, growing privacy concerns drive users to selectively delete sensitive information, aligning with privacy regulations such as the European Unions GDPR [2], which grants individuals the right to be forgotten, i.e., the right of an individuals data to be deleted from database (and derived products) and requires companies to delete personal data upon request. On the other hand, efficient machine unlearning is crucial in various scenarios, including removing contaminated data points due to data poisoning attacks [3], [4], eliminating outdated information [5], and handling misleading or ambiguous data [6]. These diverse needs underscore the importance of developing algorithms that enable models to quickly forget specific training points without significant utility loss. An intuitive approach to implementing unlearning is to retrain the model from scratch based on the remaining data upon receiving forgetting request. Although this can provide precise removal guarantees and maintain model utility, it is time-consuming and computationally expensive, especially when dealing with large-scale datasets and frequent forgetting requests. Therefore, it is crucial to design an efficient unlearning mechanism that balances the trade-offs among removal guarantee, unlearning efficiency, and comparable model utility. Current unlearning approaches can be broadly categorized into two main classes: (1) Exact unlearning [7], which typically involves partitioning the training dataset into distinct 2 shards, with each shard used to train an isolated sub-model. During inference, the results from all sub-models are aggregated to produce the final output. When forgetting request arrives, only the shard containing the data points to be forgotten is retrained, while other sub-models remain unchanged [1], [7] [9]. This approach ensures precise removal of the targeted data but disrupts the inherent relationships between data points, leading to significant performance degradation. Additionally, retraining the affected sub-model, even for single shard, remains computationally expensive and time-consuming process. (2) Approximate unlearning [10], which adjusts model parameters to scrub the contribution of unlearning data, ensuring approximate indistinguishability between the unlearned model and retrained model [11][19]. The influence function [20] shows great potential by using first-order Taylor expansion of the loss function to estimate the effect of removing single sample on models parameters. While this aligns well with the objectives of machine unlearning and achieves notable enhancement in model performance, the computation of influence function necessitates the calculation of the Hessian matrix across all model parameters and the entire dataset, followed by the subsequent inversion of this high-dimensional matrix. This procedure is inherently computationally intensive, as it requires significant computational resources and storage capacity to handle the large-scale matrix operations involved. The above discussion of existing methods highlights the challenges of unlearning, which often entails substantial computational demands and memory requirements to achieve data removal. Inspired by cognitive science, which suggests that memorizing is easier than forgetting [21], this paper attempts to address machine unlearning from the perspective of memory (incremental learning [22]). Specifically, we first establish theoretical bridge between incremental learning and machine unlearning. This theoretical breakthrough connects two previously distinct research domains and reveals new perspective for implementing unlearning through incremental learning. Incremental learning typically relies on gradientbased optimization, which is more efficient than the timeconsuming computation and inversion of Hessian matrices required for unlearning, thereby supporting the notion that memorizing is easier than forgetting. Furthermore, we propose novel unlearning algorithm called Influence Approximate Unlearning (IAU) that synergistically integrates incremental learning algorithms. IAU consists of three core modules: incremental approximation, gradient correction, and gradient restriction. Incremental approximation achieves the forgetting effect by incrementally learning negative samples of forgotten points, avoiding the need for costly Hessian matrix calculations and inversions. However, the gradient-based update strategy in incremental approximation may result in over-forget and be affected by abnormal gradients. To address this, gradient correction in the unlearning phase adjusts gradient information for the remaining data, while gradient restriction during model training limits gradient size to mitigate the impact of abnormal gradients on unlearning updates. Extensive experimental results prove that our IAU algorithm effectively balances multiple unlearning properties and delivers superior performance in comparison with state-of-the-art methods. Our main contributions are as follows: Inspired by cognitive science that memorizing is easier than forgetting, we establish bridge between incremental learning and machine unlearning through theoretical analysis and innovatively transform unlearning (forgetting) into incremental learning (memorizing). We propose IAU, novel efficient unlearning framework developed under the perspective of incremental learning. This approach not only significantly reduces computational overhead and memory consumption but also demonstrates the potential of leveraging incremental learning methodologies to design unlearning mechanisms, thereby establishing novel paradigm for advancing future algorithmic advancements in unlearning research. We conducted comprehensive experiments to assess the efficacy of the proposed algorithm. The empirical results consistently demonstrate that the proposed IAU framework achieves superior trade-off among removal guarantee, comparable model utility, and unlearning efficiency, outperforming existing state-of-the-art methods. III. RELATED WORK Machine unlearning refers to the process of removing the influence of specific training data subsets from trained model without necessitating full retraining. Driven by growing regulatory mandates and ethical imperatives surrounding data privacysuch as the General Data Protection Regulation (GDPR) [2], the California Consumer Privacy Act of 2018 (CCPA) [23], and the UK Information Commissioners Office (ICO) guidelines [24]this domain has emerged as critical research area in machine learning. While retraining the model from scratch constitutes naive solution that ensures complete data exclusion, this approach incurs prohibitive computational and temporal costs, rendering it impractical for large-scale systems [1], [7]. To address this issue, two alternative forgetting routes have been proposed: exact unlearning and approximate unlearning. Exact unlearning seeks to construct new model that exactly performs the behavior of model retrained on the remaining training data after specified samples are excluded. For foundational machine learning models, prior studies have explored unlearning techniques for specific algorithms: Ginart et al. [8] proposed k-means clustering-based approaches, while SVM-based unlearning methods were developed by Romero et al. [25] and Karasuyama et al. [26]. Naive Bayes classifiers were also addressed in [1]. In the context of deep learning, the SISA framework [7] employs divide and conquer strategy by partitioning the training data into disjoint shards. Each subset is used to train isolated sub-models, which are subsequently aggregated into consolidated final model. When handling unlearning requests, only the sub-models associated with the affected data shards are retrained. However, this partitioning approach introduces critical limitations: disjoint data shards disrupt inherent sample correlations within the dataset, leading to model performance degradation. Additionally, even when retraining single data shard, the computational burden remains substantial, particularly under large-scale datasets and frequent TABLE I: Summary of key notations Notation Definition Df Dr zi z+ h(D) hu(h(D), Df ) l(z, θ) Training dataset The data that the model should forget The data that the model should remember The i-th sample pair in D, which is associated with data xi and label yi sample that needs to be removed sample added to the dataset D, which can achieve the effect of unlearning The model is trained on the dataset The sanitized model hu who approximates to the model trained on Df Loss on for the model parameterized as θ unlearning operations. In such scenarios, the iterative retraining of sub-models incurs prohibitive time and resource costs, undermining the feasibility of SISAs divide and conquer strategy in real-world applications where privacy compliance demands quick and efficient data exclusion. Approximate unlearning aims to ensure that the unlearned model remain nearly indistinguishable from those of model retrained on the remaining dataset [11], [27]. Among existing approaches, influence-based unlearning stands out as promising model-agnostic method due to its minimal impact on model utility. The Certified Removal [10] pioneered this direction by unlearning linear models through influence function, which compute the parameter adjustments required to remove specific training instances influence. However, when scaling this approach to large neural networks, the need to compute the Hessian matrix and inverse-Hessian vector products introduces prohibitively expensive computational demands. To address this, LCODEC [17] updates only subset of parameters to reduce computational overhead. Nevertheless, the method exhibits significant performance gap compared to retraining: for instance, unlearning 0.5% of the training data in an MNIST logistic regressor results in over 10% accuracy drop. This trade-off underscores the urgent need for novel approximate unlearning techniques that simultaneously achieve computational efficiency and preserve model utility. Our proposed method shares the same objective as approximate unlearning in preserving model utility while removing training data influence, but it is fundamentally distinguished by its novel conceptual framework rooted in incremental learning paradigms. Our approach innovatively redefines the unlearning process as memorizing task, effectively transforming the conventional notion of forgetting into remembering. This paradigm shift offers theoretically distinct solution in the unlearning problem domain. IV. PRELIMINARY This section will give necessary preliminaries, including the definition of machine unlearning and the influence function. Table shows the key notations used in the paper. A. Machine Unlearning Machine unlearning is technique that enables trained models to forget previously learned data. This technique 3 involves training dataset of samples = {zi : (xi, yi)}N i=1, where each sample pair zi is associated with data xi Rd and label yi = {1, 2, . . . , }, where is the number of classes. classification model h(D) is trained on the complete training dataset D. Users can submit data removal request at any time, which partitions the dataset into two subsets: Df D, which represents the data that the model should forget, and Dr D, which represents the data that the model should remember. The goal of machine unlearning is to eliminate the influence of Df from h(D). One solution is to use Dr as the training data to retrain new classification model h(Dr) from scratch. However, this method can be time-consuming for large-scale datasets. more efficient method is to use the unlearning mechanism hu to generate sanitized model hu(h(D), Df ) directly from the deployed model h(D), and we expect the unlearned model hu(h(D), Df ) is as similar to the retrained model h(Dr) as possible, i.e., hu(h(D), Df ) h(Dr). The measure of similarity is too broad. For this reason, there are multiple ideal properties that good machine unlearning method must satisfy [28]: Removal Guarantee. The unlearning mechanism must completely remove the information of deleted data from the trained model, including the deleted data itself and its influences on other samples. Unlearning Efficiency. The unlearning mechanism should be time-efficient compared to model retraining. Comparable Model Utility. The unlearning mechanism in only small utility gap compared to should result retraining from scratch in order to be practical. B. Influence Function Consider to be function parameterized by θ, which maps from an input feature space to an output space denoted by Y. The training samples are represented by the set = {zi : (xi, yi)}n i=1, while for particular training sample z, the loss function is denoted as ℓ(z, θ, h) (abbreviated as ℓ(z, θ)). The standard empirical risk minimization aims to solve the following optimization problem: θ = arg min θ 1 n (cid:88) i=1 ℓ(zi, θ). If training example is up-weighted by an infinitesimal amount ϵ, it results in modified set of model parameters denoted as θϵ z. This modification is obtained by solving: θϵ {z} = arg min θ 1 (cid:88) ℓ(zi, θ) + ϵℓ(z, θ). i=1 An intuitive approach is to retrain the entire model to obtain accurate θϵ {z}, but as emphasized above, the time cost is intolerable. To this end, [20] suggested approximating θϵ {z} using the first-order Taylor series expansion around the optimal model parameters represented by θ. This approximation yields: {z} θ ϵH 1 θϵ θ θℓ(z, θ). Here, Hθ represents the Hessian matrix with respect to the model parameters θ, that is, (cid:88) Hθ = 1 2 θℓ(zi, θ). i=1 As removing point is equivalent to upweighting it by ϵ = 1 , we can approximate the changes in model parameters without having to retrain the model: {z} θ θ 1 1 θ θℓ(z, θ). (1) Here θ {z} denotes the new empirical risk minimizer on dataset Dr = {z}. In this work, we refer to it as influence unlearning. V. METHODOLOGY A. Incremental Approximation According to the major theory in cognitive science, it is often easier to memorize than to forget [21]. Therefore, our studys starting point lies in counterfactual: Can we approximately counteract the influence of unlearning points by adding point? That is, approximating the forgetting effect through incremental learning of the original model. Before answering this question, we first introduce theorem about how much incremental learning of point will cause the models predictions to change. Theorem V.1 (Influence of adding point). For point and parameters θ, let ℓ(z, θ) be the loss, and the empirical risk minimizer is given by θ def i=1 ℓ(zi, θ). We add point z+ to the training dataset. The new empirical risk def minimizer is given by θ = argminθ i=1 ℓ(zi, θ) + ℓ(z+, θ)). We have n+1 ((cid:80)n = argminθ (cid:80)n {z+} 1 {z+} θ θ 1 1 θ θℓ(z+, θ), The proof of Theorem V.1 can be found in Appendix A. Summarizing Eq. 1 and Theorem V.1, we can get the model parameters after adding sample z+ and deleting sample respectively: {z+} θ θ {z} θ + θ 1 1 1 1 θ θℓ(z+, θ) θ θℓ(z, θ). (2) If we counteract the influence of deleting by adding z+, then the model parameters after adding sample z+ should be the same as the ones after deleting z, that is θ {z}, put it into Eq. 2, we can get: {z+} = θ 1 1 θ θℓ(z, θ) 1 H 1 θ θℓ(z+, θ). (3) To avoid the operation of calculating 1 conditions that satisfy Eq.3, and we have: θ , we seek sufficient θℓ(z, θ) θℓ(z+, θ). (4) Eq. 4 means that we can add point with an opposite gradient to the deleted point to counteract its influence. Figuratively speaking, if we want to remove point from linear regression, 4 we can add another point on the opposite side to balance out the impact of the point to be deleted. Although Gradient Inversion Attacks [29], [30] can provide us with the value of z+ using gradient information, it is time-consuming process. To this end, considering that Eq. 4 provides gradient relationship between the forgotten sample and its opposite sample, it is natural to choose gradient descent for incremental learning, which allows us to cleverly avoid the challenge of generating opposite samples, making incremental learning easier. Therefore, when we use gradient descent to incrementally learn the new sample z+, then, according to Eq. 4, it is equivalent to performing gradient ascent on z, that is, unlearn = θ + η θℓ(z, θ), θ where η is the learning rate. This incremental approximate learning method avoids retraining on D{z} and constructing z+, saving time and effort. More importantly, this method does not require the calculation of the Hessian matrix and its inverse. The gradient descent is only means to achieve incremental learning, which allows us to cleverly avoid the challenge of generating opposite samples, making incremental learning easier. Undeniably, using more advanced incremental learning strategies is intriguing and meaningful, and it will be considered part of our future research endeavors. Notably, this strategy can be easily generalized to batch deletions. If we want to unlearn subset Df from the training set, the parameters change can be described by unlearn = θ + η θ (cid:88) ziDf θℓ(zi, θ). (5) B. Gradient Correction Only letting the parameters rise by the gradient of the unlearning point without Hessian matrix as weight constraint may cause the model to over-forget the unlearning point and ignore the gradients at the remaining points. Therefore, it is necessary to correct the update of the model on the gradient of the forgotten point to prevent this situation. Here, based on the ideal properties of machine unlearning introduced in Section IV-A, we suggest correcting the gradient in two directions: (1) according to the requirements of the comparable model utility, it should not damage the model performance at the remaining points, and (2) based on the the model should keep forgetting the removal guarantee, forgotten points. This can be achieved by the idea of model catastrophic forgetting [31] subtly. Specifically, when neural network trains on new dataset, it will be more inclined to fit the new dataset, forget what it has previously learned, and cause the model to lose its previous capabilities. This inspires us to let the model strengthen the learning of the remaining training data, which can not only maintain the effect of the model on the remaining data but also consolidate the models forgetting at the unlearned point z. Based on the above considerations, we get the corrected gradient as follows: add = θ η θ (cid:88) ziDr θℓ(zi, θ). (6) C. Gradient Restriction Unlike traditional work that only focuses on the unlearning stage, we desire to improve the training quality of the model so that it has the potential to perform unlearning better and faster. This idea is also used in Unrolling SGD (USGD) [14]. Outliers and abnormal points usually have large gradients on the model. If unlearn requests contain these points, simply letting the parameters rise by the gradient may destroy most of the information in the model. Since the forgetting point can be any data, we cannot ignore this phenomenon. So, we need to restrict the gradient of all points in the training dataset. Existing gradient-restricting techniques, such as Gradient Clip [32] and SignSGD [33], mainly focus on correcting the gradient to prevent gradient explosion. However, they cannot guarantee that the real gradients of the sample are small enough. This leads to the fact that in the final trained model, the gradients of the unlearning samples may still be very large. We will further verify this in our experiments. To this end, we propose gradient restricted (GR) loss in model training, which limits the gradients of the model to training samples not to be large, as shown below: ℓGR(z, θ) = ℓ(z, θ) + α θℓ(z, θ)2. (7) the def= argminθ The new item can be seen as regularization term, where α serves corresponding regularization as risk minimizer coefficient. By utilizing the empirical θ i=1 ℓ(zi, θ), we can conclude that θ (cid:80)n i=1 ℓ(z, θ) = 0 [10], [20]. Therefore, when we regulate the first-order gradient θℓ(z, θ), it results in θ i=1 ℓ(z, θ) tending towards 0, essentially leading to an optimal model with more accurate direction. (cid:80)n (cid:80)n 1 The regularization term acts as significant penalty on the gradients of parameters with high values on data points, ultimately favoring smaller, more uniform gradients. This property is highly beneficial as it encourages the network to use all points rather than just accommodating outliers and abnormal points. Therefore the model ends up with small gradients at all points, rather than large gradients at few points. Notably, although it is necessary to calculate the gradient and update it backward, the model is able to converge more quickly without significantly increasing the cost of model training. Our empirical results, as shown in Section VII-E, confirm these conclusions. D. Overall Framework Through the above three modules, we achieve an approximation of the influence function and alleviate the time delay in calculating the Hessian matrix. Model Training Phase. Based on Section V-C, we minimize the objective loss of Eq 7. As we emphasized, gradient restriction helps the convergence of the model, and for this purpose, we use an early stopping mechanism. This makes the training delay of the overall model tolerable even though the computational complexity of single round of training is higher than that of traditional training. 5 Model Unlearning Phase. Combining incremental approximation (Eq. 5) and gradient correction (Eq. 6), we can get the unlearning strategy for model parameters updated as unlearn = θ η ( θ (cid:88) θℓ(zi, θ) (cid:88) θℓ(zj, θ)). ziDr zj Df (8) Specifically, when receiving an unlearning request, we let the model parameters increase on the gradient of Df and decrease on the one of Dr according to the Eq. 8. Note that we only update model parameters once, i.e., we compute gradient on θ for the residual and forgotten sets, then update θ by these two gradients. Besides, our method is independent of the number of deletion points, which is very useful when receiving large batches of deletion requests. Differences from gradient ascent unlearning methods. Although the existing strategies based on gradient ascent (descent) are common and intuitive, it is unclear how closely this heuristic strategy based on model learning relates to machine unlearning. Due to the wide application of unlearning in privacy protection, data security and other fields, this nonrigorous heuristic strategy may reduce users trust in the unlearning model. On the contrary, inspired by the main theory of cognitive science that memorizing is often easier than forgetting [21] and from the perspective of incremental learning, our work innovatively transforms forgetting into memorizing and establishes bridge between incremental learning and machine unlearning through theoretical analysis (Section V-A). E. Complexity Analysis Without specifying the model structure, assume that t1 is the time for one forward propagation of the model, k1 and k2 are the maximum cost of computing an individual element of gradient and hessian matrix respectively, and is the number of model parameters. Upon receiving one unlearning request, the time complexity of the proposed strategy is O(nt1 + nk1p). For the Hessian-based method, the time complexity of calculating the Hessian matrix is O(nt1 + nk2p2). The time complexity of directly calculating the inverse of the Hessian matrix is O(p3), so the total time complexity of the Hessianbased method is O(nt1 + nk2p2 + p3), which is much larger than the proposed algorithm. Even if the inverse of the hessian matrix can be approximated through numerical optimization algorithms such as L-BFGS or Lissa and the complexity can be reduced to O(nt1 + nk2p2 + tp), where is the number of optimizations, it is still much larger than the proposed IAU. VI. EXPERIMENT A. Experimental Setup 1) Datasets: We conducted experiments on CIFAR10 [34] and SVHN [35], widely used datasets for evaluating the performance of deep neural networks [7], [14], [36], [37]. Additionally, we also explored our IAU algorithm on tabular dataset Purchase100 and more complex dataset CIFAR100 [34], which allowed us to assess the versatility of our approach across diverse data formats and levels of complexity. 2) Models: We conduct comparisons of unlearning methods using two widely-used models [14], [15], [37]: LeNet5 [38] and ResNet18 [39]. LeNet5 consists of two convolutional layers, followed by max-pooling and then two fully connected layers. ResNet-18 consists of 18 weight layers, including 7x7 convolutional layer, four residual blocks, and fully connected layers. Additionally, we conducted experiments on MLP [40] in Section VII-A and VGG19 [41] in Section VII-B to further assess the versatility of our unlearning approaches across diverse model architectures. 3) Evaluation Metrics: The primary goal of approximate unlearning is to ensure that the distribution of final activation results from the unlearned model closely resembles that of the retrained model, making them nearly indistinguishable. Consequently, we regard the retrained model as the gold standard for evaluating unlearning methods based on the following three criteria, which collectively offer comprehensive evaluation of the unlearning models similarity to the retrained model and the efficiency of the unlearning method. The following three metrics, namely Model Utility (MU), Unlearning Time (Time), and Unlearning Efficacy (UE), are specifically tailored to be consistent with the three ideal unlearning properties (Removal Guarantee, Unlearning Efficiency, and Comparable Model Utility discussed in Section IV-A) respectively. In addition, their combined consideration (i.e., Avg Rank) will provide comprehensive assessment framework for evaluating the effectiveness of an unlearning algorithm. Model Utility (MU). For an effective unlearning algorithm, it is imperative that the MU closely approximates that of the retrained model. This alignment can be quantitatively assessed by measuring the gap between the accuracy of the test dataset achieved by the unlearning model and that of the golden model. low MU value indicates minimal deviation from the retrained models performance. Unlearning Time (Time). The unlearning mechanism should be time-efficient compared to retraining. We record the time consumed by the unlearning algorithm, serving for the quantitative evaluation of Unlearning Efficiency. Greater efficiency is achieved with shorter Unlearning Time. Unlearning Efficacy (UE). From the attackers perspective, the unlearning model should closely resemble the retrained model. UE is to quantify the degree of proximity between the unlearning model and the golden model, as perceived by potential attacker. It is quantified by calculating the gap in the attack success rate concerning the erased dataset between the unlearning model and the golden model, typically through the use of Membership Inference Attack (MIA) [42]. lower UE value signifies heightened degree of resemblance between the unlearning model and the golden model, as perceived by the potential attacker. Average Rank(Avg Rank). When evaluating the unlearning algorithm, three essential criteria are considered: MU, Time, and UE. Each unlearning algorithm aims to achieve balance or trade-off between these dimensions. To thoroughly evaluate the algorithms performance, we use Avg Rank as composite metric that reflects the average ranking across all three dimensions. lower rank indicates better trade-off achieved by the unlearning algorithm, and an ideal unlearning algorithm would have rank of 0. 4) Baselines: We implement the following baseline unlearning methods for comparisons: Retrain. We train the model from scratch with the remaining data as the retrained model. Thus, the retrained model is the optimal unlearned model and is seen as the gold model. Unrolling SGD (USGD) [14]. USGD uses the standard deviation (SD) loss in the training framework for pre-training epochs, then trains additional epochs on the subset of the training set and records gradient. In the unlearning phase, it resumes the gradient decreased by unlearning points. Amnesiac Unlearning [15]. Amnesiac Unlearning removes unlearning examples and inserts small number of copies of them with randomly selected incorrect labels. Then, it fine-tunes the model with those random labels on forgotten samples. Bad Teaching [36]. Bad teaching explores the utility of competent and incompetent teachers in student-teacher framework to induce forgetfulness. The knowledge from the competent and incompetent teachers is selectively transferred to the student to obtain model that does not contain any information about the forgotten data. Fisher [16]. Fisher locates the influence of unlearning points by using the Fisher Information Matrix as Hessian approximation. Then, it scrubs the influence of the unlearning points on model parameters. 5) Membership Inference Attack Details: We adopt the Membership Inference Attack(MIA) proposed in [43]. Specifically, In our scenario, the adversary only has black-box access to the target model, meaning that the adversary can submit data point to the target model and subsequently obtain the probabilistic output. Furthermore, the attacker is privy to the architecture of the victim model and has access to data distributions identical to those used during the victim models training process. Consequently, the attacker leverages this knowledge to construct multiple shadow models that mirror the behavior of the victim model. In particular, we have trained three shadow models, each of which shares the same structure as the target model. The dataset employed for training the shadow models is drawn from the same distribution as the training data used for the victim model. The attack model is designed as fully connected network with two hidden layers featuring widths of 256 and 128, respectively. ReLU activation functions, dropout layers with rate of 0.5, and sigmoid output layer are incorporated into this architecture. The attack model is applier to the model on the forget dataset Df to calculating UE. 6) Implementation Details: We conduct LeNet experiments on single Nvidia RTX 3090 GPU server with Intel Xeon CPUs ResNet18 and VGG19 experiments on two Nvidia A100 GPU servers with Intel Xeon CPUs and MLP experiments on one Nvidia RTX 2080 Ti GPU servers with Intel Xeon CPUs. We implemented and conducted training using the PyTorch deep learning framework, version 2.0.1. All experiments have been conducted 10 times, and the reported results represent the average values across these repetitions. To mitigate overfitting, we employ an early stopping mechanism. Specifically, if there 7 TABLE II: Comparison of Model Utility(MU), unlearning time, and Unlearning Efficacy(UE) with baselines for 5% points unlearned randomly from the original training points in two datasets and two ML models. The optimal approach for these three indicators is to minimize their values. The optimal outcomes for each backbone on each dataset are represented in bold typeface, while the second-best outcomes are indicated with an underline. Backbone Strategy LeNet5 ResNet18 Retrain USGD Bad Teaching Amnesiac Unlearning Fisher IAU(Ours) Retrain USGD Bad Teaching Amnesiac Unlearning Fisher IAU(Ours) CIFAR10 SVHN MU 0 0.80 1.38 0.51 0.61 1.31 0 1.52 0.98 5.13 1.51 0. Time(second) 414 33 23 33 1294 13 424 27 20 39 3078 19 UE 0 2.27 6.11 3.21 8.08 5.21 0 13.98 64.93 64.80 24.74 20.10 Avg rank MU 1.7 2.7 1 3 1.7 1.7 2 3.3 2.7 0.3 0 5.16 3.10 0.04 4.53 0.09 0 0.07 0.01 1.64 0.02 0. Time(second) 822 20 14 26 1926 10 575 43 20 50 4503 12 UE 0 6.17 2.17 1.67 4.75 2.46 0 4.99 4.26 20.95 5.04 3.10 Avg rank 3.3 1.3 1 3.3 1 2 0.7 3.7 2.7 1 is no improvement in validation set accuracy for 10 epochs, we terminate the models training. B. Comparison with Baselines TABLE II shows the results of comparison with baselines with 5% of training points randomly unlearned. From the table, we have the following important findings. IAU consistently outperforms the four state-of-the-art unlearning baselines in terms of removal guarantee, unlearning efficiency, and comparable model utility. The results clearly demonstrate that our method outperformed the baselines in the two experiments LeNet5 on CIFAR10 and ResNet18 on SVHN, achieving the top ranking. In the remaining two experiments, our method secured the second position, still showing strong performance that surpassed the other baselines. This consistent pattern of success across the experiments underlines the superior performance of our method, highlighting its effectiveness and robustness in outperforming existing approaches for different datasets and models. IAU demonstrates exceptional performance in terms of time efficiency, outperforming all baseline methods in every experiment, and it can strike superior balance between unlearning efficiency and model utility. For the performance of unlearning efficiency, IAU efficiently minimizes unlearning time. For example, on the LeNet5 model, IAU outperforms the second fastest method by around 28.6%43.5%. This accomplishment is particularly noteworthy in situations where deletion requests are frequent. In terms of model utility, the IAU algorithm yields results within margin of 2 units from the retraining process, feat unattained by any of the other baseline methods. These findings unequivocally establish IAUs ability to balance performance and efficiency, consistently surpassing established baseline techniques. IAU consistently achieves comparable performance to Fisher in terms of unlearning effectiveness while significantly reducing the unlearning time by several orders of magnitude. Both approaches are dedicated to alleviating the influence of unlearning data points on model parameters through the utilization of influence functions, thus presenting commonality in their efficacy for unlearning. Nonetheless, the pivotal divergence emerges with Fishers approach, which entails the approximation of the Hessian matrix by employing the Fisher Information Matrix and subsequently inverting it. In contrast, our proposed IAU methodology adeptly circumvents this computational step. Hence, IAU exhibits superior time efficiency while retaining good forgetting effect compared to Fisher. C. Unlearning Efficacy We conducted an evaluation of ML unlearning methods on deep neural networks that were trained for image classification. Our experiments were performed on the CIFAR10 dataset in the ResNet18 model To observe the unlearning performance, we randomly selected one training image and applied unlearning to it. The results are as shown in Fig.1. It is observable that the activation map of the unlearned image, as obtained through retraining, exhibits slight downward shift rather than significant alteration. This outcome is in line with expectations since the unlearning point is not orthogonal to the remaining data points. The activation maps for the unlearned image and the retained image following the application of USGD present significant shift in the models attention. This result underscores that USGD erases substantial amount of information linked to the unlearned image, including information that is also associated with other images in the same class. Consequently, this makes it challenging for an attack model to discern accurately which images were utilized during model training, and yet, there is drop in model utility. This observation is consistent with the results presented in TABLE II, where USGD outperforms our method in Unlearning Efficiency while trailing behind in Model Utility. As for Bad Teaching and Amnesiac unlearning methods, the results on the unlearned image differ significantly from the"
        },
        {
            "title": "Fisher",
            "content": "IAU(Ours)"
        },
        {
            "title": "Fisher",
            "content": "IAU(Ours) Fig. 1: These are activation maps that demonstrate the impact of scrubbing on model. The top row displays the scrubbed image, while the bottom row displays non-scrubbed image. Each row shows the original image, the activation map before unlearning, and the activation map after the unlearning method. The method names are noted below the activation map obtained using each method. retrained model, indicating substantial departure in the performance of the unlearned image from the desired gold model. Such discrepancies can potentially give rise to Streisand effect, leading to more pronounced inadvertent disclosure of information about the unlearned image. This observation is also consistent with the high Unlearning Efficiency values associated with these two methods, as detailed in TABLE II. Both Fisher and IAU yield results that resemble subset of the retrained model results, suggesting their ability to approximate the true change in model parameters. The result from IAU follows similar trend to the retrained model, displaying slight downward shift. Notably, it is intriguing to observe that the activation map on the retained image, as produced by the IAU method, closely mirrors the shape of the ones by the retrained model, in contrast to other methods where the activation maps remain unchanged or undergo shrinkage. This observation implies that the IAU method possesses the capability not only to eliminate the influence of unlearned images but also to effectively adjust model parameters, enabling it to align with the gold model characteristics on the retained data samples. VII. COMPLEMENTARY EXPERIMENTS In this section, we show more experiments to show the effectiveness of our IAU algorithm in various scenarios. Since the time overhead of the Fisher method even exceeds retraining, we no longer report its results. A. Tabular Data Experiments Machine unlearning is particularly beneficial in situations pertaining to tabular data, such as medical or purchase records. In this regard, we present the findings of our experiments on the Purchase100 tabular dataset on the three-layer MLP model. As illustrated in TABLE III, our proposed approach IAU demonstrates noteworthy advantages in unlearning when compared to all baselines, thereby highlighting the practicality of our method. TABLE III: Unlearning performance comparison with baselines for 5% points unlearned randomly from the original training points on tabular dataset Purchase100 and model MLP. The optimal outcome is represented in bold typeface. Backbone Strategy MLP Retrain USGD Bad Teaching Amnesiac Unlearning Ours(IAU) Purchase100 time 138 3 8 44 2 UE 0 7.94 24.30 13.19 8.19 Avg rank - 0.7 2.7 2.3 0.3 MU 0 0.50 7.02 2.49 0.21 TABLE IV: Unlearning performance comparison with baselines for 5% points unlearned randomly from the original training points on the CIFAR-100 dataset and model VGG19. The optimal outcome is represented in bold typeface. Backbone Strategy VGG19 Retrain USGD Bad Teaching Amnesiac Unlearning Ours(IAU) CIFAR100 UE 0 68.48 31.98 26.54 23.62 time 747 45 25 106 MU 0 0.18 1.29 0.03 2.88 Avg rank - 2 1.7 1.3 1 B. More Difficult Task To better demonstrate the effectiveness of the proposed method IAU, we extend the comparison of the model utility, required time, and unlearning efficacy achieved by the proposed method and the several baselines on more difficult computer vision task. we supplement the performance evaluation on the CIFAR-100 dataset and model VGG19, and the results are shown in TABLE IV below. It is evident that our method still maintains comparable competitiveness, which again emphasizes the effectiveness of IAU. C. Outlier Removal Machine unlearning becomes particularly challenging when faced with outliers. To better assess the proposed method IAU, 9 (a) Model Utility (b) Unlearning Efficacy Fig. 2: Ablation study of Incremental Approximation (IA), Gradient Correction (GC), and Gradient Restriction (GR) modules. TABLE V: Unlearning performance comparison with baselines for removing outliers task on SVHN dataset and model LeNet5. The optimal outcome is represented in bold typeface. TABLE VI: Number of training epochs required under early stopping for both the original loss and the Gradient Restriction (GR) loss. Backbone Strategy LeNet5 Retrain USGD Bad Teaching Amnesiac Unlearning Ours(IAU) SVHN UE 0 7.33 3.71 0.24 0.58 time 538 31 17 31 MU 0 1.20 1.65 1.75 1.41 Avg rank - 1.7 1.7 2 0.7 LENET5 ResNet18 CIFAR10 Original Loss GR Loss Original Loss GR Loss 13 12 28 SVHN Original Loss GR Loss Original Loss GR Loss 20 18 12 9 we conducted an experiment on removing outliers. We use the isolation forest [44] to find outliers in the SVHN dataset. It reports 587 outliers out of 58606 total training data. We have supplemented the performance evaluation of removing those outliers of LeNet5, and the results are shown in TABLE below. Compared with random forgetting (upper right part of TABLE II), the performance improvement in deleting outliers is more significant, which confirms that the proposed method is particularly effective in dealing with outliers by the gradientrestriction method. D. Ablation Study In this section, we conduct an ablation study to investigate the role and interplay of the Incremental Approximation (IA), Gradient Correction (GC) and Gradient Restriction (GR) modules in our method. The study has been structured to elucidate the distinct and collective influences of these components on the methods performance. The outcomes are presented in Fig 2. Model Utility (MU) and Unlearning Efficacy (UE) exhibit opposing trends, as discussed in Section VII-F. As result, we can observe that the IA method leads to bad in Model Utility with excellent Unlearning Efficacy, and IA+GC yields significant enhancement in Model Utility but decline in Unlearning Efficacy. However, when the IA+GC components are combined with GR (IA+GC+GR), there is an improvement in Unlearning Efficacy, while the Model Utility remains comparable to that of IA+GC. This implies that the IA+GC+GR combination achieves more favorable trade-off between Model Utility and Unlearning Efficacy than other configurations. (a) CIFAR (b) SVHN Fig. 3: The L2-norm of the gradients for ResNet18 is shown in four different scenarios after the model ceases training. The blue dots depict the original loss, the orange dots represent our Gradient Restriction loss, the green dots show gradient clipping, and the red dots are for SignSGD loss. These experiments were conducted on both CIFAR10 and SVHN datasets. E. Effective of GR Loss In this section, we present the empirical evidence that supports the effectiveness of the proposed Gradient Restriction (GR). Our findings are based on series of experiments designed to assess the impact of the proposed loss function (cf., Eq. 7) on decreasing the required training epochs and minimizing the absolute value of the model gradient. TABLE VI shows the number of epochs required for model convergence before and after using GR loss. It can be seen that in all experimental settings, using GR loss requires fewer epochs. This verifies our discussion in Section V-C that penalizing high-valued gradients helps the model converge faster. In addition, to verify the reasonableness of the proposed GR loss, we compare it with existing gradient restriction methods, including Gradient Clip [32] and SignSGD [33]. 10 in turn, may adversely affect MU. Various values of α manifest different trade-offs between Model Utility (MU) and Unlearning Efficacy (UE). On average, the IAU method exhibits limited sensitivity to changes in α, as indicated by the relatively stable behavior of the y-axis. As ρ increases, both Model Utility (MU) and Unlearning Efficacy (UE) deteriorate. This trend can be attributed to the inherently approximate nature of the unlearning process. With higher ratio of unlearned data points, the results tend to become less precise, leading to increased vagueness in the outcomes. VIII. CONCLUSION This paper addressed the challenge of machine unlearning with minimal time overhead. We identified limitations in existing methods, particularly influence-based methods when dealing with large datasets and frequent unlearning demands. Drawing insights from cognitive science, we proposed an efficient unlearning method that approximates influence functions with high efficiency while preserving model utility. Our use of incremental learning in machine unlearning offers novel perspective and has the potential to inspire future research. The empirical analysis demonstrated our methods efficiency in erasing learned information while maintaining model efficacy, surpassing current state-of-the-art methods in removal guarantee, unlearning efficiency, and comparable model utility. APPENDIX PROOF OF THEOREM V. We define R(θ) as the empirical risk of model h: R(θ) def= 1 (cid:88) i=1 ℓ(zi, θ). Empirical risk minimization(ERM) is the method of finding the minimizer of R(θ), which we call θ def= arg min θ 1 (cid:88) i=1 ℓ(zi, θ). We assume that is strictly twice-differentiable and convex; thus we know that Hθ def= 2 θR(θ) = 1 (cid:88) i=1 2 θℓ(zi, θ) exists and is positive definite. After adding point z+ and up-weighting it by an infinitesimal amount µ on original model θ, the new model θµ {z+} is defined as (cid:32) (cid:33) θµ {z+} def= arg min θ ℓ(zi, θ) + µℓ(z+, θ) 1 (cid:88) i= + 1 + 1 = arg min θ R(θ) + µℓ(z+, θ). We define the new empirical risk minimizer as θ {z+} def= argminθ 1 + 1 (cid:32) (cid:88) i=1 (cid:33) ℓ(zi, θ) + ℓ(z+, θ) . (a) MU over α (b) UE over α (c) MU over ρ (d) UE over ρ Fig. 4: Impact of parameter α in the GR loss function and unlearning ratio ρ on LENET5 model with CIFAR10 dataset. Both Model Utility (MU) and Unlearning Efficacy (UE) are ultra-small indicators. Fig. 3 visually depicts the L2-norm distribution of model gradients for ResNet18 after convergence. We can clearly find that Gradient Clip and SignSGD cannot limit the real gradient size, and even the gradient is larger than the origin (cf., OriGrad). Instead, Our method can limit the amplitude of the gradient. This underlines the rationale for the proposed GR loss. Those experimental results provide robust evidence of the effectiveness of GR loss. This loss function not only enhances directional accuracy during training but also promotes smoother convergence by moderating gradients. These findings underscore the valuable impact of GR loss function in the context of efficient machine unlearning. F. Hyperparameter Study Subsequently, we investigate hyperparameters, specifically examining unlearning tasks with parameter α in the GR loss function and different unlearning ratios denoted as ρ. These experiments are conducted on the CIFAR10 dataset using the LeNet5 model as our foundation. Fig.4 shows the experimental result with parameter α ranging from 0 to 0.10 and ratio ρ range from 0.01 to 0.10. We can observe that there exists an inverse relationship between Model Utility (MU) and Unlearning Efficacy (UE); high MU is associated with low UE, and vice versa. This observation aligns with our expectations since the unlearning process, based on the influence function, aims to approximate the change direction in model parameters rather than achieving the exact direction. Therefore, in order to enhance UE, the unlearning algorithm must induce more substantial changes in model parameters to effectively eradicate the influence of unlearned points, which, θµ {z+} is the minimizer of n+1 R(θ) + µℓ(z+, θ), then we have + 1 Next, since θ expansion: θR(θµ {z+}) + µθℓ(z+, θµ {z+}) = 0. {z+} θ as µ 0, we can perform Taylor 0 (cid:104) θR(θ) + 2 + 1 µθℓ(hθ (z+)) + µ2 θR(θ)(θµ θℓ(z+, θ)(θµ {z+} θ) (cid:105) + {z+} θ) where we have dropped o(µ) terms. Defining the parameter change µ = θµ 0 {z+} θ, we have: (cid:2)θR(θ) + 2 + 1 µθℓ(z+, θ) + µ2 (cid:3) + θR(θ)µ θℓ(z+, θ)µ. Arrange the above formula to get + 1 (cid:20) + 1 0 2 θR(θ) + µθℓ(z+, θ)+ (cid:21) θR(θ) + µ θℓ(z+, θ) µ Solving for µ, we get: µ (cid:20) + 1 θR(θ) + µ2 (cid:21)1 θℓ(z+, θ) (cid:21) θR(θ) + µθℓ(z+, θ) (cid:20) + 1 Since θ is the minimizer of R, we have θR(θ) = 0. Only keeping O(µ) terms, we get µ (cid:20) + 1 Thus we can have (cid:21)1 2 θR(θ) µθℓ(z+, θ). dθµ {z+} dµ (cid:12) (cid:12) (cid:12) (cid:12) (cid:12)µ=0 = dµ dµ (cid:12) (cid:12) (cid:12) (cid:12)µ= = + 1 1 θ θℓ(z+, θ) This yields θµ {z+} θ µ + 1 As adding point z+ is equal to up-weighting it by µ = 1 we can get approximation of parameter change θ θℓ(z+, θ). 1 n+1 , {z+} θ θ 1 H 1 θ θℓ(z+, θ). REFERENCES [1] Y. Cao and J. Yang, Towards making systems forget with machine IEEE, unlearning, in 2015 IEEE Symposium on Security and Privacy. 2015, pp. 463480. [2] P. Voigt and A. Von dem Bussche, The eu general data protection regulation (gdpr), Practical Guide, 1st Ed., Cham: Springer International Publishing, vol. 10, no. 3152676, pp. 105555, 2017. [3] B. I. Rubinstein, B. Nelson, L. Huang, A. D. Joseph, S.-h. Lau, S. Rao, N. Taft, and J. D. Tygar, Antidote: understanding and defending against poisoning of anomaly detectors, in Proceedings of the 9th ACM SIGCOMM Conference on Internet Measurement, 2009, pp. 114. [4] J. Zhang, C. Dongdong, Q. Huang, J. Liao, W. Zhang, H. Feng, G. Hua, and N. Yu, Poison ink: Robust and invisible backdoor attack, IEEE Transactions on Image Processing, vol. 31, pp. 56915705, 2022. 11 [5] W. Wang, X. Lin, F. Feng, X. He, M. Lin, and T.-S. Chua, Causal representation learning for out-of-distribution recommendation, in Proceedings of the ACM Web Conference 2022, 2022, pp. 35623571. [6] T. Pang, H. Zheng, Y. Quan, and H. Ji, Recorrupted-to-recorrupted: Unsupervised deep learning for image denoising, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 20432052. [7] L. Bourtoule, V. Chandrasekaran, C. A. Choquette-Choo, H. Jia, A. Travers, B. Zhang, D. Lie, and N. Papernot, Machine unlearning, in 2021 IEEE Symposium on Security and Privacy (SP). IEEE, 2021, pp. 141159. [8] A. Ginart, M. Guan, G. Valiant, and J. Y. Zou, Making ai forget you: Data deletion in machine learning, Advances in Neural Information Processing Systems, vol. 32, 2019. [9] Y. Liu, L. Xu, X. Yuan, C. Wang, and B. Li, The right to be forgotten in federated learning: An efficient realization with rapid retraining, in IEEE INFOCOM 2022-IEEE Conference on Computer Communications. IEEE, 2022, pp. 17491758. [10] C. Guo, T. Goldstein, A. Hannun, and L. Van Der Maaten, Certified data removal from machine learning models, in International Conference on Machine Learning. PMLR, 2020, pp. 38323842. [11] A. Sekhari, J. Acharya, G. Kamath, and A. T. Suresh, Remember what you want to forget: Algorithms for machine unlearning, Advances in Neural Information Processing Systems, vol. 34, pp. 18 07518 086, 2021. [12] G. Wu, M. Hashemi, and C. Srinivasa, Puma: Performance unchanged model augmentation for training data removal, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 36, no. 8, 2022, pp. 86758682. [13] E. Ullah, T. Mai, A. Rao, R. A. Rossi, and R. Arora, Machine unlearning via algorithmic stability, in Conference on Learning Theory. PMLR, 2021, pp. 41264142. [14] A. Thudi, G. Deza, V. Chandrasekaran, and N. Papernot, Unrolling sgd: Understanding factors influencing machine unlearning, in 2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P). IEEE, 2022, pp. 303319. [15] L. Graves, V. Nagisetty, and V. Ganesh, Amnesiac machine learning, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 35, no. 13, 2021, pp. 11 51611 524. [16] A. Golatkar, A. Achille, and S. Soatto, Eternal sunshine of the spotless net: Selective forgetting in deep networks, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2020, pp. 93049312. [17] R. Mehta, S. Pal, V. Singh, and S. N. Ravi, Deep unlearning via randomized conditionally independent hessians, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2022, pp. 10 42210 431. [18] S. Neel, A. Roth, and S. Sharifi-Malvajerdi, Descent-to-delete: Gradientbased methods for machine unlearning, in Algorithmic Learning Theory. PMLR, 2021, pp. 931962. [19] Z. Ma, Y. Liu, X. Liu, J. Liu, J. Ma, and K. Ren, Learn to forget: Machine unlearning via neuron masking, IEEE Transactions on Dependable and Secure Computing, 2022. [20] P. W. Koh and P. Liang, Understanding black-box predictions via influence functions, in International Conference on Machine Learning. PMLR, 2017, pp. 18851894. [21] T. H. Wang, K. Placek, and J. A. Lewis-Peacock, More is less: increased processing of unwanted memories facilitates forgetting, Journal of Neuroscience, vol. 39, no. 18, pp. 35513560, 2019. [22] R. Polikar, L. Upda, S. S. Upda, and V. Honavar, Learn++: An incremental learning algorithm for supervised neural networks, IEEE transactions on systems, man, and cybernetics, part (applications and reviews), vol. 31, no. 4, pp. 497508, 2001. [23] L. de la Torre, guide to the california consumer privacy act of 2018, Available at SSRN 3275571, 2018. [24] U. ICO, Guidance on the ai auditing framework: Draft guidance for consultation, 2020. [25] E. Romero, I. Barrio, and L. Belanche, Incremental and decremental learning for linear support vector machines, in International Conference on Artificial Neural Networks. Springer, 2007, pp. 209218. [26] M. Karasuyama and I. Takeuchi, Multiple incremental decremental learning of support vector machines, Advances in Neural Information Processing Systems, vol. 22, 2009. [27] A. Golatkar, A. Achille, and S. Soatto, Forgetting outside the box: Scrubbing deep networks of information accessible from input-output observations, in Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XXIX 16. Springer, 2020, pp. 383398. 12 [28] J. Wu, Y. Yang, Y. Qian, Y. Sui, X. Wang, and X. He, Gif: general graph unlearning strategy via influence function, in Proceedings of the ACM Web Conference 2023, 2023, pp. 651661. [29] L. Zhu, Z. Liu, and S. Han, Deep leakage from gradients, Advances in Neural Information Processing Systems, vol. 32, 2019. [30] J. Geiping, H. Bauermeister, H. Droge, and M. Moeller, Inverting gradients-how easy is it to break privacy in federated learning? Advances in Neural Information Processing Systems, vol. 33, pp. 16 93716 947, 2020. [31] O.-M. Moe-Helgesen and H. Stranden, Catastophic forgetting in neural networks, Dept. Comput. & Information Sci., Norwegian Univ. Science & Technology (NTNU), Trondheim, Norway, Tech. Rep, vol. 1, p. 22, 2005. [32] R. Pascanu, T. Mikolov, and Y. Bengio, On the difficulty of training recurrent neural networks, in International Conference on Machine Learning. Pmlr, 2013, pp. 13101318. [33] J. Bernstein, Y.-X. Wang, K. Azizzadenesheli, and A. Anandkumar, signsgd: Compressed optimisation for non-convex problems, in International Conference on Machine Learning. PMLR, 2018, pp. 560569. [34] A. Krizhevsky, G. Hinton et al., Learning multiple layers of features from tiny images, 2009. [35] Y. Netzer, T. Wang, A. Coates, A. Bissacco, B. Wu, and A. Y. Ng, Reading digits in natural images with unsupervised feature learning, 2011. [36] V. S. Chundawat, A. K. Tarun, M. Mandal, and M. Kankanhalli, Can bad teaching induce forgetting? unlearning in deep networks using an incompetent teacher, in Proceedings of the AAAI Conference on Artificial Intelligence, vol. 37, no. 6, 2023, pp. 72107217. [37] Z. Zhang, Y. Zhou, X. Zhao, T. Che, and L. Lyu, Prompt certified machine unlearning with randomized gradient smoothing and quantization, Advances in Neural Information Processing Systems, vol. 35, pp. 13 43313 455, 2022. [38] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, Gradient-based learning applied to document recognition, Proceedings of the IEEE, vol. 86, no. 11, pp. 22782324, 1998. [39] K. He, X. Zhang, S. Ren, and J. Sun, Deep residual learning for image recognition, in Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2016, pp. 770778. [40] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Learning representations by back-propagating errors, nature, vol. 323, no. 6088, pp. 533536, 1986. [41] K. Simonyan and A. Zisserman, Very deep convolutional networks for large-scale image recognition, in 3rd International Conference on Learning Representations (ICLR 2015). Computational and Biological Learning Society, 2015. [42] S. Yeom, I. Giacomelli, M. Fredrikson, and S. Jha, Privacy risk in machine learning: Analyzing the connection to overfitting, in 2018 IEEE 31st computer security foundations symposium (CSF). IEEE, 2018, pp. 268282. [43] R. Shokri, M. Stronati, C. Song, and V. Shmatikov, Membership inference attacks against machine learning models, in 2017 IEEE Symposium on Security and Privacy (SP). IEEE, 2017, pp. 318. [44] F. T. Liu, K. M. Ting, and Z.-H. Zhou, Isolation forest, in 2008 eighth IEEE, 2008, pp. 413422. ieee international conference on data mining."
        }
    ],
    "affiliations": [
        "Anhui Province Key Laboratory of Big Data Analysis and Application, School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230000, China",
        "School of Artificial Intelligence and Data Science, University of Science and Technology of China, Hefei, Anhui 230000, China",
        "School of Computer Science and Technology, University of Science and Technology of China, Hefei, Anhui 230000, China"
    ]
}