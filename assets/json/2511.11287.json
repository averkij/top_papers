{
    "paper_title": "Building the Web for Agents: A Declarative Framework for Agent-Web Interaction",
    "authors": [
        "Sven Schultze",
        "Meike Verena Kietzmann",
        "Nils-Lucas Schönfeld",
        "Ruth Stock-Homburg"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The increasing deployment of autonomous AI agents on the web is hampered by a fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, a web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces <tool> and <context> tags, allowing developers to explicitly define available actions and relevant state, thereby creating a clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the framework's practicality, learnability, and expressiveness in a three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides a foundational mechanism for realizing the Agentic Web, enabling a future of seamless and secure human-AI collaboration on the web."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 7 8 2 1 1 . 1 1 5 2 : r Building the Web for Agents: Declarative Framework for AgentWeb Interaction. Sven Schultze sven.schultze@tu-darmstadt.de Technical University of Darmstadt Darmstadt, Hessen, Germany Nils Lucas Schoenfeld nilslucas.schoenfeld@tu-darmstadt.de Technical University of Darmstadt Darmstadt, Hessen, Germany Meike Kietzmann meike.kietzmann@tu-darmstadt.de Technical University of Darmstadt Darmstadt, Hessen, Germany Ruth Stock-Homburg rsh@bwl.tu-darmstadt.de Technical University of Darmstadt Darmstadt, Hessen, Germany Abstract The increasing deployment of autonomous AI agents on the web is hampered by fundamental misalignment: agents must infer affordances from human-oriented user interfaces, leading to brittle, inefficient, and insecure interactions. To address this, we introduce VOIX, web-native framework that enables websites to expose reliable, auditable, and privacy-preserving capabilities for AI agents through simple, declarative HTML elements. VOIX introduces <tool> and <context> tags, allowing developers to explicitly define available actions and relevant state, thereby creating clear, machine-readable contract for agent behavior. This approach shifts control to the website developer while preserving user privacy by disconnecting the conversational interactions from the website. We evaluated the frameworks practicality, learnability, and expressiveness in three-day hackathon study with 16 developers. The results demonstrate that participants, regardless of prior experience, were able to rapidly build diverse and functional agent-enabled web applications. Ultimately, this work provides foundational mechanism for realizing the Agentic Web, enabling future of seamless and secure human-AI collaboration on the web. CCS Concepts Human-centered computing Web-based interaction; User studies; Computing methodologies Intelligent agents; Information systems Web interfaces; Security and privacy Privacy protections. Keywords Agentic Web, AI Agents, Web Agents, Large Language Models, Machine-Readable Web, Multimodal Interaction, Developer Experience, Web Standards, Privacy, Decentralization"
        },
        {
            "title": "1 Introduction\nThe past years have seen rapid progress in large language mod-\nels (LLMs) and their integration into interactive systems. Increas-\ningly, these models are being deployed as autonomous or semi-\nautonomous agents capable of acting on behalf of users in complex\nenvironments such as the web. Research like WebArena [25] has\nbenchmarked such agents in realistic, long-horizon tasks, while\ncommercial systems (e.g., Claude for Chrome, Perplexity Comet,",
            "content": "Authors contributed equally to this research. Gemini in Chrome) demonstrate growing interest in agent-mediated browsing. Yet, the integration of conversational agents into the existing web ecosystem remains fundamentally misaligned with the current architecture of the web. Todays web is designed primarily for human consumption. Agents must infer available actions by scraping HTML, heuristically parsing Document Object Models (DOMs) or even analyzing rendered screenshots. With these ad hoc practices, even minor state changes can disrupt agents workflow. Agents are inefficient, as they have to repeatedly rediscover affordances. In addition, they are insecure, since unintended operations or unauthorized data access cannot be ruled out: Sensitive, personal, or proprietary information embedded in the web page, such as private messages, financial data, or user details, could be shared without the users explicit consent. Empirical evaluations show that browsing-only agents underperform human users on realistic tasks, and that augmenting browsing with machine-native APIs or hybrid browsing+API access yields substantial gains in completion rates and efficiency [6, 25]. This highlights deeper structural challenge: While the web has evolved rich standards for human interaction, it lacks equivalent, machinenative affordances for agents. Without explicit, declarative mechanisms to declare actions and state, AI developers are forced to retrofit agent capabilities onto interfaces built for humans, leading to fragile integrations, long trajectories, and limited scalability. Finally, the current paradigm strips website developers of control over the user experience on their own pages. When an external agent scrapes site, it bypasses the carefully crafted workflows and interaction patterns designed by the developer. The agent provider, not the site owner, unilaterally decides how to interpret and interact with the pages functionality. This lack of an explicit, machinereadable contract leaves developers unable to communicate their sites capabilities, define safe actions, or protect sensitive data, creating an unpredictable and unstable environment for both agents and the websites they navigate. Recent position papers [7, 21] converge on the need for an Agentic Web. In this web, machine-readable, standardized affordances are first-class citizens and agents can operate without reverse engineering user interfaces built for humans. To this end, we introduce VOIX, web-native framework that embodies this principle through simple, declarative substrate for robust, privacy-preserving agentweb interaction. VOIX enables websites to explicitly declare actions and relevant state in way that is equally accessible to autonomous agents, reducing the need for brittle inference from human-oriented user interfaces (UIs) and lowering the development barrier for rich, multimodal experiences. This shifts agentweb interaction from model where external providers unilaterally interpret sites DOM, to one where the site developer defines an explicit, auditable contract for agent behavior. To validate the practicality and accessibility of VOIX, we conducted three-day hackathon study with 16 developers. The results demonstrate that VOIX can be rapidly adopted to build diverse agent-enabled web applications, providing initial empirical evidence of its learnability and expressiveness. Our work advances the research discourse on agent-web interaction by operationalizing the Agentic Web paradigm, which has been described in the literature as vision but has not yet been realized in practice [7, 21]. While prior work has identified machine-readable affordances as necessary foundation for agentic web interaction, VOIX provides the first concrete implementation, translating abstract calls for Agentic Web Interfaces into an empirically validated mechanism. Next, we offer normative model for trust and governance in agentic systems. In landscape where current architectures either centralize control with inference providers or with site operators, VOIX formalizes an alternative model that distributes responsibility through explicit, auditable contracts of interaction among web developers, inference providers, and end-users. In doing so, VOIX contributes to broader theoretical debates on safety, privacy, and accountability in human-AI collaboration."
        },
        {
            "title": "2.1 Web Agents and Environments\nRecent position and survey papers argue that the web is entering\nan “Agentic Web” era, in which autonomous agents act on users’\nbehalf through goal-directed interaction, demanding new protocols\nand semantics beyond human-only UIs [21]. Complementing this\nvision, Lù et al. [7] contend that forcing agents to adapt to human-\nfacing DOM and screenshots is misaligned with agent capabilities\nand safety. They advocate Agentic Web Interfaces: standardized,\nmachine-native affordances that are safe, transparent, and efficient\nfor agents to consume.",
            "content": "Realistic, reproducible environments have clarified the limits of current agents and the opportunity space for machine-native interfaces. WebArena assembles fully functional websites spanning common domains and shows that even strong models underperform humans on long-horizon tasks [25]. WebVoyager demonstrates an LMM-driven, end-to-end agent for real sites and introduces an evaluation protocol, substantially outperforming text-only baselines [6]. Crucially, exposing machine-native endpoints materially improves agent success. Song et al. [15] compare browsing-only, APIonly, and hybrid agents on WebArena, finding Hybrid (browsing + APIs) achieves improvements over browsing alone, and that API Schultze et al. quality strongly moderates outcomes. Reddy et al. [13] further demonstrate, for information aggregation across multiple sites, modular agent with NavigatorExtractorAggregator components, showing gains under both direct API-driven access and interactive visual access. recent survey synthesizes components, data, and evaluation practices for LLM-based graphical user interface (GUI) agents, mapping systems across perception, grounding, planning, and action spaces [22]. In mobile, Zhang et al. [23] present AppAgent, which interacts with smartphone apps via simplified action space (tap, long-press, swipe, text, back, exit) and two-phase learning process (exploration or observing demonstrations to build reference documentation). Empirically, AppAgents simplified, machine-readable affordances and documentation substantially improve success over raw coordinates and no-doc baselines."
        },
        {
            "title": "2.2 Multimodal Interaction\nDecades of human–computer interaction research show that inter-\naction is more powerful when combining modalities such as natural\nlanguage and direct manipulation. Oviatt’s foundational work on\nmultimodal systems [10] demonstrates that pairing speech or text\nwith pointing or selection reduces ambiguity, increases robustness,\nand is strongly preferred by users for spatial or object-specific tasks.\nRecent LLM-powered systems confirm this from two complemen-\ntary directions: DirectGPT [8] enriches chat-based interaction with\ngraphical cues such as selection and pointing, while ReactGenie\n[20] enriches GUIs with voice input interpreted by an LLM. Build-\ning on this, GenieWizard simulates user personas and dialogues to\nelicit multimodal commands, uses a zero-shot parser plus abstract\ninterpretation to identify missing APIs, and helps developers close\ncoverage gaps [19]. Earlier, Geno showed that developer-side aug-\nmentation can retrofit multimodality onto existing web apps [14].\nWen et al. [16] show that adding visual prompts and direct manip-\nulation to text improves disambiguation in visualization without\nsacrificing efficiency. On mobile text correction, Tap&Say integrates\ntouch location into an LLM’s context, outperforming state-of-the-\nart text correction baselines and commercial voice access while\nreducing keystrokes and effort [24].",
            "content": "Together, these results show that restricting agent interaction to chat-only paradigm omits proven opportunities for richer, more efficient interaction. Yet, multimodal systems have historically been more difficult to implement than single-modality interfaces. To avoid user confusion, the feature sets across modalities must remain aligned, and maintaining this parity significantly increases development complexity. Non-visual cues are inherently harder for users to discover because they lack persistent visibility. On the web, these challenges intersect with the existing limitations faced by agents: without standard interface for exposing capabilities and state, developers must retrofit multimodal support onto humanoriented UIs, compounding both the engineering burden and the brittleness of agent behavior."
        },
        {
            "title": "2.3 Human-in-the-Loop Agentic Systems\nThe growth of agent capabilities raises concerns about misalign-\nment, deception, unsafe exploration, security, and broader soci-\netal impacts [2]. Systems increasingly add scaffolds for oversight",
            "content": "Building the Web for Agents: Declarative Framework for AgentWeb Interaction. and control. Magentic-UI proposes an extensible, multi-agent interface with co-planning and co-tasking, action approval, answer verification, memory, and multi-tasking [9]. Results across agentic benchmarks and user studies suggest that lightweight human participation improves safety and outcomes. practical friction is that approval processes are often highly individual to the use cases and application domains: An online banking application will likely require different approval processes than static news article. This emphasizes central argument that the transition into the Agentic Web should involve website maintainers as central stakeholders of its architecture, instead of agent providers attempting to infer these complex, domain-specific safety requirements from the outside."
        },
        {
            "title": "2.4 Positioning VOIX\nThis prior work reveals the need for a standardized, machine-native\nprotocol for the web that leverages powerful multimodal patterns,\nyet it also highlights the critical challenges of agent brittleness,\nimplementation complexity, and the need for developer-defined\nsafety and control. To address these challenges, we propose VOIX, a\nweb-native framework for agentic interaction. The design of VOIX\nis guided by a set of core principles derived from architectural prin-\nciples for building the Agentic Web [7], and foundational research\non effective multimodal interaction [10, 11].",
            "content": "R1 Privacy and Safety: Control over privacy and safety must be placed in the hands of developers and users. Developers must be able to define what data is exposed and which tools are safety-critical, while users must retain control over their conversational data and choice of LLM provider. This counters the significant privacy risks of sending full page content to third-party services. R2 Optimal Representation: The framework must provide an efficient, machine-readable representation of websites affordances. This representation must be dynamically scoped, containing only the necessary information for the agent to optimally solve its tasks, excluding all other data. R3 Efficient to Host: The architecture must not place the computational and financial burden of LLM inference on the website owner. By operating on the client-side, the framework respects the decentralized nature of the web and removes major barrier of widespread adoption. R4 Standardized and Developer-Friendly: The framework must be built on universal standard using familiar web patterns. This addresses the need for easy developer adoption and aligns with the call for standardized Agentic Web Interfaces that is compatible across various agents and websites. R5 Expressive: The framework must also be expressive enough to model the key patterns of effective multimodal interaction as identified by Oviatt [10, 11]. This is critical requirement for moving beyond simple feature substitution towards truly synergistic and flexible interfaces. Specifically, the framework should handle both simultaneous and sequential integration of inputs, abstract and high-level user inputs, and the ability to implement both complementary and redundant actions."
        },
        {
            "title": "3 VOIX: A Web-Native Interface for Agents\nVOIX contributes a concrete, web-native mechanism that makes site\ncapabilities and state discoverable and invokable by agents through\ndeclarative, typed semantics. VOIX introduces two new HTML ele-\nments: <tool> exposes actions with names, parameter types, and\nnatural language descriptions; <context> exposes task-relevant\nstate. VOIX aims to: (1) improve agent reliability and efficiency\nby eliminating affordance inference; (2) preserve the web’s decen-\ntralization and backward compatibility; (3) provide human control,\nprivacy, and transparency by design; and (4) remain model- and\nprovider-agnostic so multiple agent stacks can interoperate.",
            "content": "To this end, VOIX defines clear architectural model that decouples the websites functional capabilities from the agents reasoning and execution, creating standardized interface that prioritizes security, privacy, and decentralization. The VOIX architecture distributes responsibilities among three distinct stakeholders: The Website. The web page acts as the authoritative source of its own capabilities. Its responsibility is to declare set of invokable tools and expose relevant application context using the HTML markup. It is also responsible for implementing the business logic that executes tool call and, if necessary, returns result. Thereby, the websites declares the contract in which an LLM is allowed to interact with it. This happens alongside the development of the user interface, allowing developers integrate their existing application logic seamlessly and to benefit from the entire ecosystem of modern frameworks like React and Vue or server-side like Laravel without the need to install new packages. The Browser Agent. The browser serves as the intermediary, decoupling the website from the Inference Provider. Its primary functions are to: (a) scan the website to discover and catalog all declared <tool> and <context> elements; (b) present this catalog to the agent for reasoning; and (c) dispatch events to trigger tool execution on the web page when instructed by the agent. This architectural role is designed to be implementation-agnostic and can be realized through various means, including open-source extensions that support multiple inference providers, enterprise-specific modules with enhanced security and integration, or native browser integrations. Our reference implementation, for instance, is Chrome extension that demonstrates this open, provider-agnostic model. The Inference Provider. The LLM, which can be cloud-based or local model hosted by the user, is the decision-making component. It receives the catalog of tools and context from the browser agent and, based on the users natural language objective, selects the appropriate tool and parameters for execution. Its interaction is with the structured declarations, not the visual UI. This decoupled architecture enables diverse ecosystem of interaction models, each with distinct trust and data-flow characteristics. The frameworks flexibility supports wide range of scenarios. For instance, user could opt for fully sovereign, private execution by combining an open-source browser extension with locally hosted LLM, ensuring no data leaves their personal machine. Alternatively, user could connect to powerful cloud-based model like OpenAIs GPT or Anthropics Claude for maximum capability. This flexibility also extends to more controlled environments. In corporate Schultze et al. (a) Example Web App: Todo list (b) Embedded VOIX Elements in the Web App (c) VOIX Reference Chrome Extension Sidepanel Figure 1: Integration of VOIX into task management web application. (a) The human-facing interface allows users to add, complete, and clear tasks. (b) The application embeds VOIX-HTML elements in its markup, declaratively exposing state and invokable actions to match or complement these features. (c) The VOIX reference Chrome extension automatically discovers the declarations and surfaces them in side panel, where an LLM-powered agent can reason over available tools and execute actions based on voice or chat user input. setting, an enterprise could deploy proprietary extension with Single Sign-On to securely interface with an internal LLM. It also accommodates native integration, where browser might embed support directly into its product and connect to its own inference provider to offer seamless, vertically integrated user experience."
        },
        {
            "title": "3.1 Trust Boundaries\nThe architecture of VOIX creates clear trust boundaries by keeping\nthe user’s conversation private while giving the website and the\nuser strict control over data sharing. When a user gives a command,\ntheir words are sent directly from the Browser Agent to their chosen\nInference Provider, ensuring the website never sees the original\nrequest. Furthermore, the agent does not see the entire state of the\nwebpage; it only has access to the specific information and actions\nthe website explicitly shares. This allows the website to protect\npotentially sensitive user data that has been entered on the page\nbut is not meant to be shared with a third-party Inference Provider.\nAs a final layer of control, the user can configure the Browser Agent\nto disable specific contexts they do not want the agent to see. This",
            "content": "multi-level system of permissions ensures that interactions are both powerful and privacy-preserving. This approach stands in contrast with two prevailing models of web-agent integration, each of which centralizes control at the expense of key stakeholder. In one model, the website implements its own bespoke LLM support, forcing the user into position where they must trust the site operator with their conversational data, which may be used in ways that are not transparent or aligned with the users interests. In the opposing model (e.g., Claude for Chrome1, Perplexity Comet2), an inference provider deploys universal agent that attempts to infer actions and state from websites raw HTML and screenshots. This disempowers the website developer, who loses control over both the user experience and data privacy, as the agent may perform unintended actions or access data not meant for exposure. VOIX, by design, avoids this by creating standardized, explicit contract that balances the needs of both the user and the website developer. 1https://www.anthropic.com/news/claude-for-chrome, accessed November 17, 2025 2https://www.perplexity.ai/comet/, accessed November 17, 2025 Building the Web for Agents: Declarative Framework for AgentWeb Interaction."
        },
        {
            "title": "3.2 Reference Implementation\nWe contribute a reference implementation of VOIX with a Chrome\nextension that enables chat and voice interaction with websites.\nIts operation begins with a script, injected into the website, which\ndiscovers all declared <tool> and <context> elements. This script\nalso actively monitors the DOM for any changes, allowing it to\nmaintain a dynamic and up-to-date model of the available tool and\ncontext space. Figures 1a and 1b showcase an example task list\napplication that embeds these VOIX elements alongside its normal\nHTML code.",
            "content": "The primary user interface is side panel that supports multiple modes of interaction, see Figure 1c. Users can communicate via standard text chat or through advanced voice inputs. This includes voice mode that uses an on-device voice activity detection model to enable continuous conversation, as well as transcription voice input that allows the user to see and edit their spoken words before sending them to the agent. To enhance discoverability, the side panel visualizes the available tools and contexts from the website and generates example prompts derived from them. It also allows the user to toggle model-specific features, such as thinking mode. Finally, in an options page of the chrome extension, the user can configure their preferred Inference Provider for both the language model and the transcription service, with support for any OpenAIcompatible API endpoint. This ensures that users retain full control over their data, their choice of models, and their costs. The agentic LLM interaction in the extension maintains conversation history. When the user makes request, the contexts are prepended to the message, and request is sent to the configured Inference Provider, including the currently available tools. When tool call is generated, an event is triggered through the injected script to invoke the websites tool handler. If the tool returns something, this is reported back to the LLM Agent, which finally writes message explaining what happened."
        },
        {
            "title": "3.3 Developing VOIX-supported Websites\nIntegrating VOIX into a web application involves two primary steps:\nfirst, declaratively exposing the application’s state and capabilities\nthrough HTML, and second, connecting these declarations to the\napplication’s existing logic. Using a task management application\nas a running example (Figure 1a), this section explains how to\nimplement VOIX-supported web apps.",
            "content": "An agent must first understand the current state of the application to act effectively. The <context> element provides this information as simple plain text. In task management application, the list of current tasks is exposed so the agent knows what can be acted upon. This context can be dynamically populated when the applications state changes. Next, the application declares what actions an agent can perform. The <tool> element defines an action, its purpose, and its parameters. The call event handler is used to link the tool invocation to application logic. When the agent invokes this tool, the associated JavaScript function is executed. The function receives the parameters gathered by the agent in the events detail property. Beyond simply invoking actions, VOIX also implements way to report back tool calls outcome. This is essential for confirming success, handling errors gracefully, and enabling tools that fetch data. VOIX facilitates this as an asynchronous event-driven path: developer can signal that tool will provide feedback by adding return attribute to its definition. When this attribute is present, the Agent waits and listens for return event after the tool is called, before sending its next response to the user. As illustrated in Figure 1b, this events detail payload contains structured object indicating the result."
        },
        {
            "title": "4 Hackathon\nTo study the learnability and expressiveness of VOIX, we conducted\nan IRB-approved hackathon study spanning three days. In teams\nof two or three, 16 participants with varying degrees of web devel-\nopment experience implemented six VOIX-supported web apps for\ndifferent use cases. This section explains the study setup, analyzes\nthe resulting applications, and discusses the outcome to answer the\nfollowing research questions:",
            "content": "Q1 Can developers learn to use VOIX tools within short time periods to create meaningful multimodal interactions in their web apps? Q2 Is VOIX expressive enough to implement the interactions developers come up with in their use cases?"
        },
        {
            "title": "4.1 Study Setup\nTo address our research questions, we conducted a three-day, in-\nperson hackathon study. We recruited 16 participants (4 teams of\nthree, 2 teams of two) who completed the study and submitted\na final project. The event was held on-site, with complimentary\ncatering and a required minimum of four hours of presence to\nincentivize participation and collaboration.",
            "content": "Participants came from diverse technical backgrounds, ensuring representative sample of web developers. pre-hackathon survey revealed balanced distribution of self-reported web development experience: 18% as advanced, 41% identified as solid, 29% as having basics, and 12% as having none. Four of the six teams registered together, while the remaining two teams were formed during the kickoff session. The hackathon began with an onboarding session on Day 1, which included study briefing, hands-on VOIX workshop, and the announcement of the theme. The second day was dedicated entirely to project development, with facilitators providing support through observational mentorship. The event culminated on Day 3 with final coding session, followed by live demos, and an awards ceremony in the afternoon. The central theme for the hackathon was content creation. This theme was intentionally chosen to allow teams the freedom to explore diverse and creative use cases for VOIX while requiring complex workflow. To support their work, we provided each team with comprehensive documentation package for VOIX, including code examples, and set of API keys to give their VOIX Chrome extension access to state-of-the-art LLM (Qwen3-235B-A22B [18]) and transcription endpoint."
        },
        {
            "title": "4.2 Data Collection\nWe employed a mixed-methods approach to gather comprehensive\ndata on VOIX’s learnability, expressiveness, and the challenges of\nits implementation.",
            "content": "Schultze et al. Figure 2: The graphic design application demonstrates synergistic multimodal interaction using VOIX: dynamic context elements contain information about the objects on the canvas and their state, allowing the Agent to understand which objects to change how in order to implement the users instructions. Then, broad set of tools enables the LLM to create, edit, rearrange and delete objects. Survey. Participants completed post-hackathon survey to observe their understanding and perception of VOIX. Specifically, we collected the System Usability Scale [1] and the Trust of Automated Systems Test (TOAST) [17]. Observational Mentorship. Throughout Facilitators engaged in casual conversations with participants and observed their workflows during the hackathon, capturing spontaneous reactions, collaboration patterns, and contextual challenges. Interviews. Each team participated in scheduled one-hour interview on one of the three days, allowing for in-depth discussion of their design process, implementation hurdles, and experience with the framework. Project Artifacts. We collected the final source code of all six projects for detailed analysis of the implemented VOIX integrations. To motivate participants, we offered tiered prize pool. Members of the winning team each received EUR 300, with prizes of EUR 200, EUR 100, and EUR 50 per member for the second, third, and remaining teams, respectively."
        },
        {
            "title": "4.3 Expressiveness of the Applications\nDevelopment teams built a diverse set of VOIX-supported web appli-\ncations. The projects included a Creative Studio for graphic design,\na Fitness App for generating workout plans, a Soundscape Creator\nfor designing ambient audio environments, a Kanban-style Project\nManagement tool, an Anki Creator for building flashcard decks,\nand a form-based fantasy Character Creation website. Detailed de-\nscriptions and screenshots of these applications can be found in\nAppendix A. An analysis of these applications demonstrates that\nour framework successfully meets two of its primary design goals:\nit is sufficiently expressive to support sophisticated multimodal\ninteractions (R5), and it enables practical, effective scoping using\nstandard web development patterns (R2).",
            "content": "The applications confirmed the frameworks expressiveness by demonstrating its capacity to handle range of interaction patterns. The frameworks ability to process abstract, high-level user inputs was evident in the Fitness App, where command such as create full high-intensity training plan for my back and shoulders was successfully interpreted and executed, selecting the appropriate exercises, set counts, and repetition schemes from its knowledge base Building the Web for Agents: Declarative Framework for AgentWeb Interaction. to assemble and display the complete routine. Similarly, the Soundscape Creator allowed users to make broad, conceptual requests like make it sound like rainforest. Furthermore, participants implemented both complementary and redundant multimodal actions. The Creative Studio offered clear example of complementary input, supporting deictic interactions where user could click on canvas element while issuing voice command like rotate this by 45 degrees (see Figure 2). This showcased the frameworks ability to fuse inputs from different modalities to resolve single intention. In contrast, applications like the Project Management tool provided redundant modalities, allowing users to perform the same action, such as creating task, through either the graphical interface or an equivalent voice command. Requirement R2 for the framework was to allow developers to scope the available tools and context to prevent ambiguity and enable dynamic, stateful interactions. The hackathon participants demonstrated that this can be achieved idiomatically using the component-based architectures of modern JavaScript frameworks. Developers naturally scoped multimodal functionality by defining the available tools and context within their React or Vue components, thus restricting these capabilities to the specific UI region where component was active. This pattern of context-sensitive availability was common. For instance, the Anki Creator only exposed the tool for image generation when the user was interacting with specific flashcard category component, while the Project Management tool only made the create task tool available when the main Kanban board was rendered. This scoping was often dynamic, changing in response to the applications state. The Creative Studio and Fitness App, for example, used conditional rendering logic to expose editing tools only after user had selected specific element. These implementations confirm that developers can use familiar, state-driven patterns from component-based frameworks to effectively manage the scope of multimodal interactions, making the integration of VOIX both powerful and practical."
        },
        {
            "title": "4.4 Learnability of VOIX\nTo evaluate how easily developers could learn and apply the VOIX\nframework (Q1), we gathered quantitative data using standardized\nusability questionnaires and analyzed the practical outcomes of\nthe hackathon. Our primary measure was the System Usability\nScale (SUS) [1], a widely adopted and reliable tool for assessing\nperceived usability. The results from the post-hackathon survey\n(N=16) yielded a mean SUS score of 72.34 (SD=14.82). A score above\nthe industry average of 68 is considered “good”. This score indicates\nthat, on the whole, developers found the VOIX framework to be\nusable and easy to learn.",
            "content": "In addition to usability, we assessed participants trust in VOIX using the Trust of Automated Systems Test (TOAST) [17]. Following the validated two-factor structure of the scale, we report results separately for System Understanding and System Performance. The mean score for System Understanding was 5.81 (SD = 0.85), while the mean score for System Performance was 5.14 (SD = 0.87), both on 17 scale. These results suggest that participants not only developed solid conceptual grasp of VOIXs functionality, but also perceived it as performing reliably. Together with the SUS results, these findings provide evidence that VOIX is both learnable and capable of inspiring confidence in its expressiveness and reliability. The interviews and observations reinforced that VOIX was easy to learn and apply within the limited timeframe. Participants emphasized that the frameworks reliance on only two additional HTML tags (<content> and <tool>) allowed them to integrate functionality as naturally as defining <button> element. Some teams reported that this simplicity fit well with familiar patterns in frameworks like React and Vue, where tools and context could be scoped through standard component logic and conditional rendering. Several teams noted that this alignment simplifies the process and made rapid prototyping feasible. At the same time, participants highlighted conceptual challenge: deciding which tools were meaningful to implement. While technically straightforward, teams debated whether specific actions provided real benefit compared to existing UI interactions, sometimes defaulting to simple CRUD-style tools. Thus, while the mechanics of VOIX were quickly mastered, identifying valuable use cases required more reflection."
        },
        {
            "title": "5 Discussion\nThe architectural design of VOIX and the empirical findings from\nour hackathon study confirm that the framework successfully meets\nits core design requirements. The results demonstrate that by em-\nbedding machine-readable affordances directly into the DOM, VOIX\noffers a practical path toward a more robust, decentralized, and\nprivacy-preserving Agentic Web.",
            "content": "The frameworks architecture directly satisfies the requirement for Privacy and Safety (R1). By design, VOIX creates clear trust boundary that separates the users conversational data from the websites domain. The Browser Agent sends user prompts directly to the users chosen LLM provider, ensuring the website operator never sees the content of the conversation. Conversely, the website only exposes explicitly declared information via the context tag, protecting sensitive or proprietary user data from being indiscriminately scraped by third-party agent. The study also verified the need for an Optimal Representation (R2) of sites capabilities. The tool and context primitives provide concise, structured summary of affordances, eliminating the inefficiency and brittleness of parsing entire DOMs. The hackathon results provided strong empirical validation for this principle, as development teams naturally leveraged component-based frameworks like React and Vue to dynamically scope these affordances. This common pattern ensured that tools were only discoverable when relevant, inherently linking the agents capabilities to the real-time state of the user interface. Furthermore, VOIX is architecturally designed to be Efficient to Host (R3). The framework is fundamentally client-side, placing the computational and financial burden of LLM inference on the users agent and chosen provider, not the website owner. This decentralization removes significant barrier to entry and aligns with the webs ethos, making the widespread adoption of the standard feasible for developers and organizations of all sizes. The frameworks success in the hackathon underscored its adherence to being Standardized and Developer-Friendly (R4). VOIX is built upon familiar web patterns: simple, declarative HTML tags and standard JavaScript event listeners. The study provided direct evidence of its learnability, as all teams were able to create functional, agent-enabled web applications within the short three-day timeframe. Also, the participants reported that the framework was easy to grasp, with participants emphasizing that its two additional tags could be used as naturally as standard HTML elements. Finally, the applications developed during the hackathon confirmed that the framework is sufficiently Expressive (R5) to support the key patterns of effective multimodal interaction. The projects demonstrated range of sophisticated patterns, including abstract, high-level commands, complementary deictic interactions where click was paired with voice command, and redundant modalities that allowed users to perform the same action through either the GUI or voice. Table 1: Latency Benchmark comparing VOIX with Perplexity Comet3 and BrowserGym [3] with GPT-5-mini. All latency values are in seconds. Details in Appendix B. Task VOIX Comet BrowserGym Creative Studio Add blue triangle Rotate the green triangle 90 Delete selected object Export as an image Fitness App Create full week HIIT plan Start Day 1 of plan Export Day 2 & 5 as PDF Show workout statistics Project Management Tool Create task Report on tasks in progress Copy task to another project 2.32 1.11 0.96 1.30 14.38 1.07 1.87 0.91 1.62 1.30 2.67 27.21 89.12 16.29 10.12 229.52 Failed 17.37 10.42 26.14 4.43 61. 25.29 Failed 5.69 4.25 1271.00 26.27 13.82 5.79 33.01 6.90 Failed"
        },
        {
            "title": "5.1 Latency Comparison\nA latency comparison (Table 1) demonstrates a fundamental ar-\nchitectural failure in the industry standard affordance inference\nbased paradigm. As established by foundational research, the clas-\nsic threshold for an action to be perceived as “instantaneous” is\n100-200ms [5]. However, for the direct manipulation tasks often\ncombined with speech in multimodal commands, performance is\nmeasurably degraded when latency exceeds as little as 25ms [4].\nOur tests show that vision-based agents exhibit latencies ranging\nfrom 4.25 seconds to over 21 minutes for complex tasks with long-\nhorizon multi-step action chains. These response times are not\nmerely slow; they are orders of magnitude beyond these critical\nperceptual and performance thresholds. This inherent architectural\nlatency makes it impossible to create the tight temporal binding\nbetween modalities, which is the primary cue used to fuse speech\nand gesture into a single, coherent perceptual event [12]. In com-\nparison, VOIX enables efficient, single-step solutions to these tasks",
            "content": "3https://www.perplexity.ai/comet, accessed November 17, 2025 Schultze et al. since feedback is immediate and long loops of inferring affordances, executing actions and checking success can be avoided."
        },
        {
            "title": "5.2 Limitations and Future Work\nWhile the hackathon study demonstrated VOIX’s promise, its con-\ntrolled environment does not fully capture the challenges of real-\nworld adoption. A primary limitation is the long-term developer\nburden, which manifests in two key areas: maintenance and con-\nceptual design. First, integrating VOIX into large, legacy codebases\nintroduces a risk where the declarative tools fall out of sync with\nthe evolving graphical user interface. If a new GUI feature is added\nwithout a corresponding VOIX implementation, the multimodal\nexperience becomes inconsistent and frustrating, undermining the\nframework’s goal of seamless interaction. Preventing this requires\nsignificant development discipline and new testing methodologies\nnot explored in our short-term study.",
            "content": "Second, VOIX demands significant conceptual shift for developers, moving from familiar visual paradigm to an affordance-centric one. This creates difficult abstraction dilemma when designing tools. On one hand, low-level tools that mirror atomic GUI actions are simple to maintain but offer little performance benefit over traditional agents. On the other hand, high-level, intent-aligned tools provide high performance but are difficult to design comprehensively and become brittle when users goal falls outside the developers preconceived notions. Finding balanced and effective middle ground imposes substantial design burden on development teams, representing key hurdle for practical adoption."
        },
        {
            "title": "6 Conclusion\nThis work introduced VOIX, a web-native framework designed\nto resolve the fundamental misalignment between AI agents and\nthe human-first web. By enabling websites to expose reliable, au-\nditable, and privacy-preserving affordances through simple declar-\native HTML, VOIX offers a concrete solution to the brittleness,\ninefficiency, and security risks inherent in current screen-scraping\nand DOM-parsing approaches.",
            "content": "Our empirical evaluation through multi-team hackathon demonstrated that VOIX is practical and accessible tool for developers. The study confirmed that developers, regardless of prior experience, could rapidly build functional, agent-enabled web applications. Furthermore, the analysis of the resulting applications revealed the emergence of sophisticated, synergistic multimodal interaction patterns, validating that VOIX is expressive enough to realize the benefits of combining direct manipulation and natural language as envisioned by foundational human-computer interaction research. By shifting the responsibility of defining agent capabilities from the agent provider to the website developer, VOIX creates balanced, decentralized architecture that respects user privacy and developer control. It represents critical and deployable step toward an Agentic Web built for seamless and secure collaboration between humans and AI. [19] [20] Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. 2025. Qwen3 Technical Report. doi:10.48550/arXiv.2505.09388 arXiv:2505.09388. Jackie (Junrui) Yang, Yingtian Shi, Chris Gu, Zhang Zheng, Anisha Jain, Tianshi Li, Monica S. Lam, and James A. Landay. 2025. GenieWizard: Multimodal App Feature Discovery with Large Language Models. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. ACM, Yokohama Japan, 117. doi:10.1145/3706598.3714327 Jackie (Junrui) Yang, Yingtian Shi, Yuhan Zhang, Karina Li, Daniel Wan Rosli, Anisha Jain, Shuning Zhang, Tianshi Li, James A. Landay, and Monica S. Lam. 2024. ReactGenie: Development Framework for Complex Multimodal Interactions Using Large Language Models. In Proceedings of the CHI Conference on Human Factors in Computing Systems. ACM, Honolulu HI USA, 123. doi:10.1145/3613904.3642517 [21] Yingxuan Yang, Mulei Ma, Yuxuan Huang, Huacan Chai, Chenyu Gong, Haoran Geng, Yuanjian Zhou, Ying Wen, Meng Fang, Muhao Chen, Shangding Gu, Ming Jin, Costas Spanos, Yang Yang, Pieter Abbeel, Dawn Song, Weinan Zhang, and Jun Wang. 2025. Agentic Web: Weaving the Next Web with AI Agents. doi:10.48550/arXiv.2507.21206 arXiv:2507.21206. [22] Chaoyun Zhang, Shilin He, Jiaxu Qian, Bowen Li, Liqun Li, Si Qin, Yu Kang, Minghua Ma, Guyue Liu, Qingwei Lin, Saravan Rajmohan, Dongmei Zhang, and Qi Zhang. 2025. Large Language Model-Brained GUI Agents: Survey. doi:10.48550/arXiv.2411.18279 arXiv:2411.18279. [23] Chi Zhang, Zhao Yang, Jiaxuan Liu, Yanda Li, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2025. AppAgent: Multimodal Agents as Smartphone Users. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. ACM, Yokohama Japan, 120. doi:10.1145/3706598.3713600 [24] Maozheng Zhao, Michael Xuelin Huang, Nathan Huang, Shanqing Cai, Henry Huang, Michael Huang, Shumin Zhai, Iv Ramakrishnan, and Xiaojun Bi. 2025. Tap&Say: Touch Location-Informed Large Language Model for Multimodal Text Correction on Smartphones. In Proceedings of the 2025 CHI Conference on Human Factors in Computing Systems. ACM, Yokohama Japan, 117. doi:10.1145/3706598. 3713376 [25] Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Tianyue Ou, Yonatan Bisk, Daniel Fried, Uri Alon, and Graham Neubig. 2024. WebArena: Realistic Web Environment for Building Autonomous Agents. In The Twelfth International Conference on Learning Representations. https://openreview.net/forum?id=oKn9c6ytLx Building the Web for Agents: Declarative Framework for AgentWeb Interaction. References [1] john Brooke. 1996. SUS: Quick and Dirty Usability Scale. Evaluation In Industry. CRC Press. In Usability [2] Alan Chan, Rebecca Salganik, Alva Markelius, Chris Pang, Nitarshan Rajkumar, Dmitrii Krasheninnikov, Lauro Langosco, Zhonghao He, Yawen Duan, Micah Carroll, Michelle Lin, Alex Mayhew, Katherine Collins, Maryam Molamohammadi, John Burden, Wanru Zhao, Shalaleh Rismani, Konstantinos Voudouris, Umang Bhatt, Adrian Weller, David Krueger, and Tegan Maharaj. 2023. Harms from Increasingly Agentic Algorithmic Systems. In 2023 ACM Conference on Fairness Accountability and Transparency. ACM, Chicago IL USA, 651666. doi:10.1145/3593013.3594033 [3] Thibault Le Sellier de Chezelles, Maxime Gasse, Alexandre Lacoste, Massimo Caccia, Alexandre Drouin, Léo Boisvert, Megh Thakkar, Tom Marty, Rim Assouel, Sahar Omidi Shayegan, Lawrence Keunho Jang, Xing Han Lù, Ori Yoran, Dehan Kong, Frank F. Xu, Siva Reddy, Graham Neubig, Quentin Cappart, Russ Salakhutdinov, and Nicolas Chapados. 2025. The BrowserGym Ecosystem for Web Agent Research. Transactions on Machine Learning Research (2025). https://openreview.net/forum?id=5298fKGmv3 Jonathan Deber, Ricardo Jota, Clifton Forlines, and Daniel Wigdor. 2015. How Much Faster is Fast Enough?: User Perception of Latency & Latency Improvements in Direct and Indirect Touch. In Proceedings of the 33rd Annual ACM Conference on Human Factors in Computing Systems. ACM, Seoul Republic of Korea, 18271836. doi:10.1145/2702123.2702300 [4] [5] Valentin Forch, Thomas Franke, Nadine Rauh, and Josef F. Krems. 2017. Are 100 ms Fast Enough? Characterizing Latency Perception Thresholds in MouseBased Interaction. In Engineering Psychology and Cognitive Ergonomics: Cognition and Design, Don Harris (Ed.). Springer International Publishing, Cham, 4556. doi:10.1007/978-3-319-58475-1_4 [6] Hongliang He, Wenlin Yao, Kaixin Ma, Wenhao Yu, Yong Dai, Hongming Zhang, Zhenzhong Lan, and Dong Yu. 2024. WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models. doi:10.48550/arXiv.2401.13919 arXiv:2401.13919. [7] Xing Han Lù, Gaurav Kamath, Marius Mosbach, and Siva Reddy. 2025. Build the web for agents, not agents for the web. doi:10.48550/arXiv.2506.10953 arXiv:2506.10953. [8] Damien Masson, Sylvain Malacria, Géry Casiez, and Daniel Vogel. 2024. DirectGPT: Direct Manipulation Interface to Interact with Large Language Models. In Proceedings of the CHI Conference on Human Factors in Computing Systems. ACM, Honolulu HI USA, 116. doi:10.1145/3613904.3642462 [9] Hussein Mozannar, Gagan Bansal, Cheng Tan, Adam Fourney, Victor Dibia, Jingya Chen, Jack Gerrits, Tyler Payne, Matheus Kunzler Maldaner, Madeleine Grunde-McLaughlin, Eric Zhu, Griffin Bassman, Jacob Alber, Peter Chang, Ricky Loynd, Friederike Niedtner, Ece Kamar, Maya Murad, Rafah Hosn, and Saleema Amershi. 2025. Magentic-UI: Towards Human-in-the-loop Agentic Systems. doi:10.48550/arXiv.2507.22358 arXiv:2507.22358. [10] Sharon Oviatt. 1999. Ten myths of multimodal interaction. Commun. ACM 42, 11 (Nov. 1999), 7481. doi:10.1145/319382.319398 [11] Sharon Oviatt. 2015. The Paradigm Shift to Multimodality in Contemporary Computer Interfaces. Morgan & Claypool Publishers, San Rafael. [12] Sharon Oviatt and Philip Cohen. 2000. Perceptual user interfaces: multimodal interfaces that process what comes naturally. Commun. ACM 43, 3 (March 2000), 4553. doi:10.1145/330534.330538 [13] Revanth Gangi Reddy, Sagnik Mukherjee, Jeonghwan Kim, Zhenhailong Wang, Dilek Hakkani-Tur, and Heng Ji. 2024. Infogent: An Agent-Based Framework for Web Information Aggregation. doi:10.48550/arXiv.2410.19054 arXiv:2410.19054. [14] Ritam Jyoti Sarmah, Yunpeng Ding, Di Wang, Cheuk Yin Phipson Lee, Toby JiaJun Li, and Xiang Anthony Chen. 2020. Geno: Developer Tool for Authoring Multimodal Interaction on Existing Web Applications. In Proceedings of the 33rd Annual ACM Symposium on User Interface Software and Technology. ACM, Virtual Event USA, 11691181. doi:10.1145/3379337.3415848 [15] Yueqi Song, Frank Xu, Shuyan Zhou, and Graham Neubig. 2025. Beyond Browsing: API-Based Web Agents. doi:10.48550/arXiv.2410.16464 arXiv:2410.16464. [16] Zhen Wen, Luoxuan Weng, Yinghao Tang, Runjin Zhang, Yuxin Liu, Bo Pan, Minfeng Zhu, and Wei Chen. 2025. Exploring Multimodal Prompt for Visualization Authoring with Large Language Models. doi:10.48550/arXiv.2504.13700 arXiv:2504.13700. [17] Heather M. Wojton, Daniel Porter, Stephanie T. Lane, Chad Bieber, and Poornima Madhavan. 2020. Initial validation of the trust of automated systems test (TOAST). The Journal of Social Psychology 160, 6 (Nov. 2020), 735750. doi:10.1080/00224545. 2020.1749020 [18] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin,"
        },
        {
            "title": "A Hackathon Applications",
            "content": "Schultze et al. Soundscape Creator. This vanilla HTML application allows users to create soundscapes by mixing samples on set of sliders. Meanwhile, the application generates images that match the current soundscape using diffusion model. The application implements VOIX tools to change the soundscape based on user instructions, potentially changing multiples sliders at once. Creative Studio. With this React app, users can create visual designs on canvas using basic shapes and text. The interface provides tools to modify object properties like color, size, rotation, and position. The application exposes its core functionalities as VOIX tools, allowing users to perform multimodal interactions combining selection and natural language instructions to perform tasks. Fitness App. This React app is fitness application for creating multi-week training plans with customizable days and exercises. It includes an interactive workout interface with timers, set tracking, and real-time exercise management. Performance is tracked through data dashboards with an option to export reports as PDFs. The app integrates VOIX tools to operate core functions hands-free and enable LLM-based exercise planning and support. Project Management Tool. This is Vue.js-based project management tool that uses Kanban board to track tasks through different stages, such as To Do, In Progress, and Done. Through the VOIX interface, users can interact with the board using natural language to create new tasks. Character Creator. This vanilla HTML website helps users create fantasy characters using quiz and facilitates LLM-guided role playing with those characters. It primarily uses VOIX tools to support filling out the quiz using instructions instead of tediously selecting each question one-by-one. Anki Creator. This Vue.js-based application is tool for generating image-based flashcard decks compatible with the Anki spaced repetition system. Users can define categories and then use AIpowered tools to generate images for those categories. The entire workflow from creating categories and generating images to exporting the final deck is controlled via natural language commands in the VOIX interface. Fitness App. Create full week HIIT plan: The agent was given the highlevel prompt: create full week high-intensity training plan for my back and shoulders. Start Day 1 of plan: The agent was instructed with the prompt: start day one of my high intensity training plan. Export Day 2 & 5 as PDF: The agent was instructed with the prompt: export day 2 and 5 from my training plan as pdf. Show workout statistics: The agent was instructed with the prompt: show me statistics on my workout routine. Project Management Tool. Create task: The agent was instructed with the prompt: Create task to finish the database optimization by wednesday. Report on tasks in progress: The agent was instructed with the prompt: Give me report of how many tasks are currently in progress on the ecommerce platform project. Copy task to another project: The agent was instructed with the prompt: copy the most recently added task from the Website Redesign project over to this one. Received November 17, 2025 Building the Web for Agents: Declarative Framework for AgentWeb Interaction. Latency Benchmark This appendix details the methodology used for the latency benchmark comparison presented in Table 1 and Section 5.1 of the main paper. B.1 Benchmark Objective The primary objective of this benchmark was to quantitatively compare the end-to-end task completion latency of our proposed VOIX framework against two representative affordance inferencebased agentic systems: Perplexity Comet and BrowserGym. The goal was to measure the performance difference between declarative, machine-readable approach (VOIX) and approaches that rely on interpreting visual UIs and raw DOM structures. B.2 Systems Under Test Three distinct systems were evaluated: VOIX: The web applications built by hackathon participants using the VOIX framework, controlled via our reference Chrome extension connected to the Qwen3-235B-A22B large language model. Perplexity Comet: commercial, vision-based web agent that infers actions from screenshots and HTML. Tests were conducted using the publicly available version as of September 4, 2025. BrowserGym: An open-source research framework for web agents [3]. We used it with GPT-5-mini model to execute tasks on the same web applications used for the VOIX tests. The BrowserGym agent was provided with the same high-level objective for each task. B.3 Measurement Protocol Latency Measurement: For each task, latency was measured in seconds from the moment the natural language prompt was submitted by the user to the moment the requested change was fully rendered and visually confirmed on the screen. Trials: Each task was executed once per platform to measure direct, single-shot completion time. If task did not complete successfully, it was retried up to maximum of three total attempts. The VOIX framework never required retry for any task. Failures: task was marked as Failed if the agent could not complete it within the three-attempt limit or exceeded timeout of 25 minutes. B.4 Task Descriptions The following lists the exact natural language prompts used for each task. Creative Studio. Add blue triangle: The agent was instructed with the prompt: add blue triangle to the canvas. Rotate the green triangle 90: The agent was instructed with the prompt: rotate the green triangle by 90 degrees to the left. Delete selected object: With an object already selected, the agent was instructed with the prompt: delete this object. Export as an image: The agent was instructed with the prompt: export this as as an image."
        }
    ],
    "affiliations": [
        "Technical University of Darmstadt"
    ]
}