{
    "paper_title": "R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing",
    "authors": [
        "Tianyu Fu",
        "Yi Ge",
        "Yichen You",
        "Enshu Liu",
        "Zhihang Yuan",
        "Guohao Dai",
        "Shengen Yan",
        "Huazhong Yang",
        "Yu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs' reasoning paths. Luckily, we reveal that only a small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce **Roads to Rome (R2R)**, a neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6x, outperforming even the R1-14B model. Compared to R1-32B, it delivers a 2.8x wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 2 ] . [ 1 0 0 6 1 2 . 5 0 5 2 : r R2R: Efficiently Navigating Divergent Reasoning Paths with Small-Large Model Token Routing Tianyu Fu 1,2, Yi Ge1, Yichen You1, Enshu Liu1, Zhihang Yuan2, Guohao Dai3,2, Shengen Yan2, Huazhong Yang1, Yu Wang 1Tsinghua University 2Infinigence AI 3Shanghai Jiao Tong University"
        },
        {
            "title": "Abstract",
            "content": "Large Language Models (LLMs) achieve impressive reasoning capabilities at the cost of substantial inference overhead, posing substantial deployment challenges. Although distilled Small Language Models (SLMs) significantly enhance efficiency, their performance suffers as they fail to follow LLMs reasoning paths. Luckily, we reveal that only small fraction of tokens genuinely diverge reasoning paths between LLMs and SLMs. Most generated tokens are either identical or exhibit neutral differences, such as minor variations in abbreviations or expressions. Leveraging this insight, we introduce Roads to Rome (R2R), neural token routing method that selectively utilizes LLMs only for these critical, path-divergent tokens, while leaving the majority of token generation to the SLM. We also develop an automatic data generation pipeline that identifies divergent tokens and generates token-level routing labels to train the lightweight router. We apply R2R to combine R1-1.5B and R1-32B models from the DeepSeek family, and evaluate on challenging math, coding, and QA benchmarks. With an average activated parameter size of 5.6B, R2R surpasses the average accuracy of R1-7B by 1.6, outperforming even the R1-14B model. Compared to R1-32B, it delivers 2.8 wall-clock speedup with comparable performance, advancing the Pareto frontier of test-time scaling efficiency. Our code is available at https://github.com/thu-nics/R2R."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) demonstrate strong capabilities across wide range of tasks [13]. Building upon the largest and strongest LLMs, test-time scaling has become prominent way to further boost their abilities on challenging tasks [47]. It is typically done by generating extensive Chain-of-Thought (CoT) reasoning before producing the final answer. However, this approach requires massive LLMs with hundreds of parameters to generate thousands of tokens per query [8], resulting in significant inference overhead [9]. Distilled Small Language Models (SLMs), containing only few billion parameters, offer much higher generation efficiency. Through supervised finetuning on LLM responses, SLMs can mimic LLM reasoning behaviors, making them popular alternative. However, SLMs may still produce different reasoning paths from their LLM counterparts during inference, causing severe performance degradation. For example, compared to the R1-32B LLM, the R1-1.5B SLM provides different final answers on 45% questions in the AIME benchmark [10], suffering 4.8 reduction in accuracy as shown in Table 2. Equal contribution. Corresponding author: Yu Wang (yu-wang@tsinghua.edu.cn). Preprint. Under review. Figure 1: (a) Examples of R2R routing objective. Given partial response as context, if SLM next-token prediction is not identical with LLMs, it is further categorized as neutral or divergent based on their affects on the reasoning path. (b) Distribution of identical, neural and divergent labels in the R2R training set with 7.6M token labels. Fortunately, we find that SLMs and LLMs often agree on next-token predictions given the same context. Instead, large performance gaps between them primarily arise from cumulative errors: their reasoning paths increasingly drift apart after some crucial differences in partial responses. To investigate this, we treat each steps LLM partial response as prefix context and assess whether the next-token prediction from SLM is identical to LLM (Figure 1(a)). Across 2,094 queries totaling 7.6M tokens generated by the 32B LLM, the 1.5B SLM differs on only 11% of tokensfar less frequent than differences observed in final answers. Moreover, some of these differences are merely neutral variations, such as abbreviations or alternative expressions (e.g., lets vs. let us), which do not affect reasoning outcomes. The key drifts start from subset of different tokens, which we call divergent tokens. These tokens genuinely alter the meaning, logic, or conclusion of the current sentence, thus diverging the subsequent reasoning path. This observation motivates us to selectively use SLM and LLM for different generation steps. It naturally leads to critical research question: Can SLMs follow LLM reasoning paths by replacing only divergent tokens? If addressed, we could unlock substantial efficiency advantage of SLMs for most generation steps, yet preserving the high-quality reasoning typical of LLMs. This can enable better test-time scaling by advancing the efficieny-performance Pareto frontier. The main challenge of SLM-LLM mix inference involves two interconnected parts: labeling the preferred model under certain objective, and designing the routing scheme to enforce it during inference. Previous methods typically route at the query level, selecting either SLM or LLM for entire response to maximize human preference win-rate within cost budget [11, 12]. However, these approaches rely on human annotations and complex router designs, whose data labeling and routing scheme are both too expensive for fine-grained, token-level routing. Alternatively, speculative decoding methods aim for identical outputs between SLM and LLM at the token level [1316]. They draft outputs with SLMs (or draft models) and periodically verify them with LLMs. While accurate, this strict verification leads to low acceptance rates. Additionally, mid-draft differences invalidate all subsequent tokens, severely restricting the accepted lengths as shown in Figure 2(b). To address these challenges, we propose Roads to Rome (R2R), token-level routing method that selectively utilizes LLMs only for path-divergent tokens during SLM generation. We begin by automating token-level model preference labeling under path-following objective. Starting from the LLMs reasoning paths, we identify different predictions for SLM and LLM, briefly continue generation from the point of difference, then use another LLM as verifier to determine whether the difference is truly divergent or merely neutral variation. This labeling approach minimizes the lower bound of LLM usage by allowing neutral SLM-LLM differences. Using the resulting labeled dataset, we train lightweight neural router to predict and immediately route divergent SLM tokens to the LLM for correction. We further improve routing accuracy by identifying predictive indicators of divergence such as SLM uncertainty and token rarity, available directly during SLM inference. Our contributions are summarized as follows. Data Labeling Pipeline. We develop an automatic pipeline to label divergent tokens. We formalize the global token-routing optimization problem, then propose path-following strategy to generate routing labels with highly parallel, local decisions. We validate that SLM can effectively match LLM reasoning quality by following these routing labels. Token-Router Design. We introduce token-level routing scheme using lightweight neural router. We investigate SLM outputs that aid accurate token routing and incorporate them into the router, enabling immediate and more accurate routing of divergent tokens. 2 Figure 2: (a) R2R uses neural router to inspect SLM outputs at each step, immediately corrects divergent tokens with LLM, then continues generation from the corrected outputs. (b) Speculative decoding uses LLM to periodically verify if SLM outputs are identical to LLM predictions, invalidating all tokens after the first correction within the period. Performance-Efficiency Pareto Frontier. R2R enables more efficient performance scaling at test time. Compared to query-level routing and distilled R1-14B, it delivers 1.11.5 higher AIME accuracy with 1.51.6 lower latency. R2R also provides 2.8 wall-clock speedup over R1-32B LLM at similar accuracy, while raising R1-1.5B SLM accuracy by 4.6 with only 12.9% LLM usage."
        },
        {
            "title": "2 Related Work",
            "content": "Test-time scaling improves LLM performance at the higher cost of inference, often through the explicit generation of the CoT reasoning paths [4, 6]. For more effective scaling, previous works optimize the length and width of reasoning paths, or reduce the generation overhead of each path [17, 18]. Controlling Reasoning Paths. Some approaches reduce LLM output lengths. They employ prompting [19], post-training [20], or heuristics [21] to generate concise CoT with fewer decoding steps [19, 21]. Others explore the width of paths. They let LLMs generate multiple reasoning paths in parallel, then select the best outcome with methods like best-of-N voting [22] or external verifiers [23]. Both strategies modify the structure of reasoning paths, which are perpendicular to R2Rs focus on reducing the overhead of each path. Model Routing. Model routing reduces generation cost by selecting the most suitable model for each query based on difficulty and budget. Current works explore selection criteria of learned human preferences [11], reward signals [24], query tags [25], and model profiles [12]. Despite simplicity, they enforce the same LLM for each response, yielding suboptimal performance for the common mixed-difficulty generations. In contrast, R2R routes at token level to further improve efficiency. Speculative Decoding. Speculative decoding accelerates generation by fixing the low parallelism in LLM decoding [1316]. It drafts outputs through SLM sequential decoding, then periodically verifies them with high-parallel LLM prefilling. However, speculative decoding pursuits identical output token (distribution) between SLM and LLM, causing low acceptance rate. In addition, it is often that not all tokens generated by SLM within one draft-verify cycle can pass LLM verification. The mid-draft rejection invalidates all subsequent drafts and LLM verifications as shown in Figure 2(b), leading to frequent rollbacks. Expanding the single draft-chain to draft-tree alleviative the problem, but also incurs higher overheads that harm batch serving efficiency [16]. Considering the internality of CoT process, R2R accepts neutrally different output tokens, and immediately corrects all divergent tokens to avoid any rollback."
        },
        {
            "title": "3 Model Preference Labeling",
            "content": "In Section 3.1, we formalize the token-level routing problem, aiming to minimize generation cost without sacrificing response quality. In Section 3.2, we introduce the path-following strategy for this problem, which assigns model preference labels to each output token, and empirically validate its effectiveness. 3 3.1 Token-level Routing Formulation For autoregressive language models, reasoning can be represented as sequence of next-token predictions. Throughout this paper, we focus on greedy sampling for simplicity: yi = arg max Pmi(yx0, . . . , xn1, y0, . . . , yi1) = arg max Pmi (yS<i). (1) Here, xi and yi denote the input and output tokens, respectively. For notational simplicity, we define the token sequence at step as S<i = [x0, . . . , xn1, y0, . . . , yi1], where S<0 is the input tokens. The next-token probability Pmi is predicted by model mi {θs, θl} at step i, where θs and θl denote the SLM and LLM, respectively. The essence of the routing strategy is to define routing function that selects the model for each decoding step: mi = R(S<i, θs, θl) (2) Our objective is to minimize the total generation cost while ensuring that the output sequence matches the quality of LLM-only outputs. We define the cost as the sum of activated model parameters per token over the entire generation process. The quality of response is evaluated by task-specific criteria, such as correctness for math problems, pass rates for coding tasks, or LLM-based grading for writing tasks. We define as the verifier function, which returns 1 if and only if two sequences are of equivalent quality. 3.2 Path-following Routing Strategy Optimally solving the token-level routing problem is computationally prohibitive, especially for large-scale data generation. While better routing sequencespotentially diverging from the LLMs reasoning pathmay exist, finding them requires exhaustively searching vast O(2n) space and generating thousands of output tokens for each search. To overcome this practical limitation, we propose greedy, sentence-level path-following routing strategy that reduces the search complexity to O(n). Rather than exploring all possible model choices, our approach incrementally aligns mixed-model generation with the reasoning path established by the LLM. At each generation step, the strategy prefers the efficient SLM unless this would cause meaningful divergence from the LLMs intended reasoning path, as determined by continuationand-verification mechanism. Specifically, at each step, we first compare the next-token predictions from the SLM and LLM. If the predictions are identical, we confidently select the SLM, as this does not affect the output sequence. When predictions differ, we must determine whether the difference is neutral or divergent. To do so, we construct two candidate sequences by appending predictions from SLM and LLM to the previous token sequence, respectively. Both sequences are then continued using the LLM until stopping criterion is met (e.g., EOS token is generated). These continuations reveal how the initial token difference affects subsequent reasoning, measured under optimal yet achievable conditions (i.e., LLM-only continuation). If the first continuation still matches the quality of the second under the verifier function V, the difference is considered neutral; otherwise, it is divergent and the token is routed to the LLM. mi = θs, yi(θsS<i) = yi(θlS<i) (cid:123)(cid:122) (cid:125) (cid:124) identical θl, V(Ss, Sl) = 0 (cid:125) (cid:124) (cid:123)(cid:122) divergent or V(Ss, Sl) = 1 (cid:125) (cid:124) (cid:123)(cid:122) neutral Ss = S<i [yi(θsS<i)] (cid:125) (cid:124) (cid:123)(cid:122) SLM token [yi+1(θlS<i [yi(θsS<i)]), . . . , EOS] (cid:125) (cid:123)(cid:122) LLM continuation (cid:124) Sl = S<i [yi(θlS<i)] (cid:125) (cid:124) (cid:123)(cid:122) LLM token [yi+1(θlS<i [yi(θlS<i)]), . . . , EOS] (cid:125) (cid:123)(cid:122) LLM continuation (cid:124) (3) (4) (5) Equations 35 formalize the routing strategy. Here, yi(miS<i) indicates that this output token is generated by model mi given the previous sequence S<i, as simplified expression of Equation 1. The continuation sequences, respectively generated after SLM and LLM token, are denoted by Ss and Sl. The operator indicates concatenation of token sequences. 4 Table 1: Statistics of tokens difference and divergence across query types in the training dataset. Type Math Code QA #Query #Token #Different Diff. Rate #Divergent Div. Rate 587 698 735 2.9M 3.2M 1.4M 7.6M 195.1K 329.0K 290.8K 814.9K 6.8% 10.3% 20.2% 10.8% 81.8K 151.9K 139.4K 373.1K 2.8% 4.7% 9.7% 4.9% Summary 2094 When continuation produces the full response by stopping only at the regular EOS token, we call this full path-following routing. By using the quality verifier from Section 3.1, the mixed-generated token sequence is guaranteed to achieve the same quality as its LLM-only counterpart, as it always remains on path that could achieve LLM-only quality. The formal proof of this quality guarantee is provided in Appendix E. While the resulting model choice sequence M<i = [m0 . . . mi1] can be used as labels for router training, full continuation is computationally expensive for large-scale data generation. In addition, the effect of current difference to the final output quality thousands of tokens away is too hard to learn for the neural router to be trained. In practice, we use sentence-level path-following routing, where the continuation ends at the current sentence, as shown in Figure 3 (step 2). We monitor sentence-ending symbols, like period, during continuation and use existing semantical sentence seperators [26, 27] to conclude generation if the sentence truly ends. To verify this local continuation, capable LLM serves as sentence-level verifier , as shown in Figure 3 (step 3). It is prompted to compare the continuations and determine whether the initial token difference introduces meaningful divergence from the LLMs intended reasoning path, or merely neutral abbreviation. Instead of verifying the entire generation, this approach checks the reasoning path at the sentence level, greatly improving data labeling efficiency. We empirically validate the effectiveness of sentence-level path-following routing using Qwen2.572B [28] as the verifier model, with prompts detailed in Appendix F.1. Among 17 AIME-24 questions correctly solved by R1-32B within an 8K-token limit, our path-following strategy achieves comparable accuracy (16 questions correctly answered) while relying on the smaller R1-1.5B model for only 3% of generated tokens. By locally evaluating token choices through sentence-level path-following routing, we closely align mixed inference with the LLMs high-quality reasoning path, eliminating the prohibitive overhead of global evaluations. However, direct use of this strategy for real-time inference is impractical, as it relies on costly LLM continuation and verification. Instead, the local nature of our strategy simplifies routing decisions, creating an easier learning task for neural router compared to global routing. We therefore design and train lightweight neural router that efficiently approximates this strategy, relying solely on SLM outputs to determine when to use the LLM during inference."
        },
        {
            "title": "4 Token-Level Neural Router",
            "content": "This section describes our methodology for constructing the neural router. Specifically, we detail how routing labels are generated for training using the sentence-level path-following strategy (Section 4.1), identify predictive SLM indicators for the router (Section 4.2), and outline the neural routers architecture along with its routing scheme for inference deployment (Section 4.3). 4.1 Training Data Generation We use sentence-level path-following routing to generate training labels for the neural router, incorporating several optimizations to control data labeling overhead. Figure 3 shows our data generation pipeline. Given queries from existing datasets, we first obtain the complete LLM response, either directly from the dataset or via batched LLM inference. Next, we use highly parallel SLM prefilling to efficiently identify tokens where the SLM prediction is identical to the LLM, allowing us to exclude about 90% of tokens from further processing. For the remaining 10% of differing tokens, we perform batched LLM continuations from each SLM prediction. To further improve efficiency, we apply prefix caching in current frameworks [29, 30] to reuse KV-Cache 5 Figure 3: R2R data labeling pipeline. Given query question, the LLM first generates response to establish the desired reasoning path. The SLM then prefills this path to identify identical and different next-token predictions. For each different SLM token, the LLM continues generation from that point. Finally, verifier model determines whether each difference leads to neutral or divergent outcome, labeling the model preference as SLM or LLM, respectively. Figure 4: Oracle insights for router design. (a) SLM entropy distribution, clipped at 99th percentile for visualization clarity (b) Divergence rate and frequency of different tokens. computations for shared context prefixes across multiple continuations (e.g., everything preceding Let in Figure 3). Continuations for the corresponding LLM tokens, S(θl), are directly extracted from the pre-generated LLM response, eliminating redundant computation. Finally, the verifier model compares both continuations and label routing preference. Using this pipeline, we efficiently generate 7.6 million routing labels in approximately 2.3 days on 8 A800 GPUs, covering topics of math, coding, and QA with queries from the Bespoke-Stratos [31] dataset. Table 1 summarizes the statistics of the generated training dataset. 4.2 Predictive Indicators of Divergence We explore predictive indicators that can help identify divergent tokens. To enable immediate routing, we focus on indicators that can be acquired solely during the SLMs next-token predictions. The following analysis is based on 7.6 million tokens in our training set. SLM Logits. As shown in Figure 4(a), divergent tokens exhibit substantially higher entropy in the SLMs output logits, with 3.8 mean value over that of non-divergent tokens. We observe similar trends with other uncertainty measures, such as aleatoric uncertainty [32]. These empirical results indicate that increased uncertainty in SLM predictions is strongly correlated with token divergence. Motivated by this, our router takes top-100 SLM logit values as one of its input features. Token Frequency. Figure 4(b) shows that low-frequency tokens in the dataset are more likely to be divergent. This likely arises from the long-tail token distribution in the training data, making rare tokens harder for SLMs to model effectively due to the limited capacity [33]. Given this insight, our router explicitly incorporates token-frequency biases by using the token embedding of as router inputs. 6 4.3 Router Design and Routing Scheme Model Architecture. Guided by insights in Section 4.2, we design the neural router as lightweight, five-layer feed-forward network (FFN) with 56M parameters. It takes the SLMs output logits and tokenized embedding, along with its last-layer hidden states for additional semantic context. All inputs are linearly projected, concatenated, and fed into the FFN backbone. The router outputs binary classification probability, indicating whether the current token diverges from the LLMs reasoning path. Full network architecture detail descriptions are in Appendix A.1. Training Scheme. We train the router with cross-entropy loss using the labeled data described in Section 4.1. To address class imbalance caused by the low divergence rate, we re-weight the loss inversely to class frequency. After training, we use the validation set to select the routing threshold that meets the user-defined LLM usage rate. The full training details are provided in Appendices A.3 and A.2. Routing Scheme. Unlike speculative decoding methods that periodically verify SLM outputs, our routing scheme aims to immediately decide whether to accept each SLM token, eliminating the need for rollbacks. As shown in Figure 2, this approach reduces unnecessary draft and verification computations, which is especially beneficial in computation-intensive batch-serving scenarios. Specifically, the neural router estimates divergence probability at each generation step using SLM outputs. When this probability exceeds predefined threshold pth, the LLM is invoked to correct the current output token. Following speculative decoding methods [13, 15], we utilize highly parallel prefilling for efficient LLM KV-Cache updates, whose overhead can be further reduced by overlapping them with SLM decoding [34]."
        },
        {
            "title": "5 Experiment",
            "content": "5.1 Setup Baselines. We use DeepSeek-R1-Distill-Qwen models as baselines, denoted by R1-M B, where indicates the model size in billions. We designate R1-1.5B and R1-32B as SLM and LLM, respectively, while intermediate sizes (7B, 14B) capture distillation scaling behavior. We compare various query-level routing (QR) methods from the RouteLLM framework [11], including similarityweighted ranking (QR-SW), matrix factorization (QR-MF), BERT-based classifier (QR-BERT), and Llama3-8B-based classifier (QR-LLM). For speculative decoding, we adopt EAGLE2 [14] and HASS [15] with R1-32B LLM. We use the official HASS draft model, and train the EAGLE2 draft model using its official script, as no pre-trained EAGLE2 draft is provided for R1-32B. R2R setup. R2R routes between R1-1.5B and R1-32B using lightweight 56M-parameter FFN router, trained on 7.6M token-level routing labels described in Section 4.1. More details on router architecture, training data, hyperparameters, and implementation are presented in Appendix A. Note that the router weights are fixed for all evaluations. The routing threshold pth is selected for 6B average parameter usage on the validation set. Performance-efficiency trade-offs are controlled solely by adjusting pth, without retraining the router. Benchmark. We evaluate methods across challenging reasoning benchmarks, including mathematics (AIME 20242025 [10]; denoted as AIME), graduate-level question-answering (GPQADiamond [35]; denoted as GPQA), and coding tasks (LiveCodeBench 2024-082025-01; denoted as LiveCodeBench [36]). All experiments use maximum output length of 32K tokens and zero generation temperature to ensure reproducibility. Efficiency metric. Following previous works [2, 37], we use the average activated parameters per token as hardware-agnostic efficiency metric, referred to as the average parameter ( ) for brevity. For query-level routing, is computed as the weighted average of SLM and LLM parameters based on their activation ratios across all outputs. For R2R, includes the SLM and router parameters, along with the LLM parameters weighted by the LLM activation ratio. We also report the total Cost (C), defined as average activated parameters multiplied by the average output tokens per query. The average parameter and total cost reflect the average decoding speed and total latency, respectively. In addition, we report hardware-specific decoding speed on NVIDIA A800-80GB GPUs using SGLang [29] framework. 7 Figure 5: Scaling of accuracy versus average activated parameters per token, evaluated across AIME, GPQA, and LiveCodeBench. R2R advances the Pareto frontier beyond distillation and query-level routing methods. Table 2: Performance and efficiency comparison across benchmarks and methods. Param. denotes the average activated parameters per token in billions; Cost is the average output tokens (thousands) per query multiplied by average parameters (billions) . AIME LiveCodeBench GPQA Average Type Method Acc. Param. Cost Acc. Param. Cost Acc. Param. Cost Acc. Param. Cost SLM R1-1.5B LLM R1-32B 12% 1.5 57% 32.0 7B 14B 32% R1-7B QR-SW 27% QR-LLM 27% QR-BERT 28% 27% QR-MF 7.0 7.2 7.1 7.1 7.5 R1-14B 48% 14.0 QR-SW 37% 14.5 QR-LLM 45% 14.8 QR-BERT 37% 14.0 42% 14.7 QR-MF 42 487 148 168 170 160 168 239 295 277 280 284 9% 1.5 45% 32. 24% 16% 13% 15% 16% 7.0 7.1 7.1 7.0 7.1 38% 14.0 27% 14.0 21% 14.1 25% 14.0 25% 14.0 43 606 168 188 195 189 190 267 333 356 342 8% 1.5 46% 32.0 29% 16% 19% 21% 16% 7.0 7.1 7.0 7.0 7.1 44% 14.0 25% 14.1 28% 14.1 29% 14.1 20% 14.2 42 519 147 179 172 169 197 318 299 297 338 10% 50% 28% 20% 20% 21% 20% 1.5 32 7.0 7.1 7.1 7.0 7.2 43% 14.0 30% 14.2 31% 14.3 30% 14.0 29% 14. R2R Ours 55% 5.5 101 39% 5.1 106 44% 6.3 101 46% 5.6 42 154 178 179 173 180 234 315 311 306 319 103 5.2 Performance Scaling Behavior. Figure 5 shows accuracy scaling with average activated parameters per token. Query-level routing (QR) methods exhibit near-linear accuracy scaling from 1.5B to 32B parameters. Distilled models (R1-7B, R1-14B) achieve superlinear gains with extensive training, reaching 88% of R1-32Bs accuracy with just 50% of the parameter size at 14B. By routing only divergent tokens to the LLM, R2R achieves 92% average accuracy with only 17% average parameters, delivers even better scaling at new Pareto frontier. Moreover, due to reduced output lengths, R2R offers an even better trade-off in terms of accuracy versus total test-time cost (see Appendix B.3). The routing threshold in R2R also enables flexible, post-training control of this trade-off. Numerical Comparison. Table 2 shows numerical details of model performance around average parameter sizes of 7B and 14B. With an average parameter size of 5.6B, R2R outperforms the best query-level routing methods (in both 7B and 14B) by 1.42.4 and 1.21.4, respectively. Compared to distilled models, R2R improves accuracy by 1.41.7 over R1-7B and even surpasses R1-14B in average accuracy by 1.1. Relative to the extremes, R2R achieves 4.6 higher accuracy than R1-1.5B and retains 92% of R1-32Bs accuracy, while using the LLM for only 1115% of tokens. 5.3 Wall-clock Speed Table 3 reports the wall-clock latency and speed for all methods on the AIME benchmark. All baselines use the official, highly efficient SGLang [29] framework and are evaluated with tensor parallelism on two NVIDIA A800-80GB GPUs. R2R uses the same thresholds as in Table 2; query8 Method #Token(K) Latency(s) Speed(tok/s) R1-1.5B 28.2 10.5 199 81 141.6 R1-14B R1-32B 17.1 12.3 15.2 12.4 QR-SW 20.3 13.5 QR-LLM 18.7 13.5 QR-BERT 19.9 13.2 19.3 13.3 QR-MF Eagle2 HASS Ours 17.4 13.1 18.8 12.9 328 272 498 456 336 379 332 334 350 367 347 359 244 194 256 18.4 13.5 218 161 52.1 30.5 55.5 56.1 57.0 55.6 71.4 73.3 84. Table 3: Comparison of latency, output token length, and average speed across methods. Subscripts note the standard deviations across AIME. Figure 6: LLM usage rate at different positions, normalized by (a) thinking and reply process, (b) each thought. Table 4: Ablation study on routing objectives and router inputs. HS denotes last-layer hidden states; Token denotes token embedding. Italicized words indicate ablation focuses. Objective Router Input Acc. #Token(K) LLM Rate(%) Param.(B) Cost(KB) Latency(s) Divergent HS+Token+Logits 55% Different HS+Token+Logits 40% Divergent HS+Token HS 47% 42% 18.4 19. 18.8 18.4 12.4 14.4 13.3 14.1 5.5 6. 5.8 6.0 101 120 109 110 218 353 345 level routing methods use the 14B version for comparable performance. R2R achieves 1.62 and 2.76 generation speed over R1-14B and R1-32B, respectively. Compared to query-level routing, R2R delivers 1.481.52 speedup. It also outperforms highly optimized speculative decoding methods with tree-like drafts, which speedup mostly at the current single-batch setup [16]. Further system-level optimization can be done to yield even greater gains for R2R. Note that, in theory, speculative decoding should exactly match the R1-32B LLMs output. However, we occasionally observe inconsistencies for very long outputs, likely due to cumulative numerical errors. We faithfully report this observation without questioning the equivalence guarantee of speculative decoding. 5.4 Ablation Study Starting from R2R in the first row of Table 4, we evaluate the effectiveness of our design by retraining the router with alternative objectives or reduced inputs, keeping all other settings fixed. All experiments are conducted on the AIME benchmark. Routing objective. As discussed in Section 3, we categorize different next-token predictions as either neutral or divergent. R2R improves efficiency by tolerating neutral differences and only routing truly divergent tokens to the LLM. When the router is trained to use the LLM for all different tokens, it fails to reach the original accuracy within the same amount of LLM usage, facing 1.4 accuracy degradation, as shown in the second row of Table 4. This confirms that restricting LLM usage to only divergent tokens is crucial for reducing cost while maintaining high accuracy. Router Input. As discussed in Section 4, both SLM logits and token embeddings are strong indicators of divergence and are used as router inputs. When these features are gradually removed, routing accuracy drops by up to 1.3, underscoring their importance. Note that while SLM logits can, in principle, be computed from last-layer hidden states within the router. However, doing so requires the capacity of the 234M-parameter embedding layer, which exceeds the capacity of the 56M-parameter neural router. 9 5.5 Routing Result Observation We analyze the routing behavior of R2R on the AIME benchmark, considering finished responses within the 32K token limit. Figure 6(a) shows the LLM usage rate across response positions. Each response is divided into the thinking process and the subsequent reply, with positions normalized to [0, 1]. The subplot widths reflect their respective average lengths. The results show that R2R routes noticeably fewer tokens to the LLM during the reply phase. It reflects the intuition that after internal thinking, the reply itself is straightforward and less demanding. Following prior work [21], we further segment the thinking process into sequential thoughts based on tokens such as Wait and Alternatively. Figure 6(b) examines the LLM usage ratio within each thought. It shows that R2R relies more on the LLM at the beginning and end of each thought. This aligns with the expectation that the initial tokens set the direction for the thought, while the concluding tokens determine whether to end the thought, branch into alternatives, or continue deeper reasoning. Notably, these routing patterns are not hand-crafted but naturally emerge from router training. It helps R2R to effectively allocate LLMs for more better test-time scaling."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduce R2R, token-level routing method that enables SLMs to follow LLM reasoning paths by correcting only divergent tokens. We design path-following routing strategy for effective data labeling and investigate predictive indicators for neural router to achieve accurate token selection. Experiments on challenging benchmarks demonstrate that R2R outperforms the R1-14B model with less than 7B average parameters, boosts SLM performance by 4.6 with under 15% LLM usage, and achieves 2.8 wall-clock speedup over the LLM at comparable accuracy. Limitations. Our current routing strategy assumes greedy sampling. Exploring alternative sampling strategies may further enhance versatility. Additionally, more dedicated system-level optimizations are required to fully realize the theoretical cost benefits of R2R."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "This work was supported by National Natural Science Foundation of China (No. 62325405, 62104128, U19B2019, U21B2031, 61832007, 62204164, 92364201), Tsinghua EE Xilinx AI Research Fund, and Beijing National Research Center for Information Science and Technology (BNRist). We thank Zinan Lin, Xuefei Ning, and Donglin Yang for their valuable discussions and suggestions. We thank Chao Xiong for his support with the SGLang interface. We also thank all the support from Infinigence-AI."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [3] Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [4] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. [5] OpenAI. Openai o3 and o4-mini system card, April 2025. URL https://cdn.openai.com/ pdf/2221c875-02dc-4789-800b-e7758f3722c1/o3-and-o4-mini-system-card. pdf. [6] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [7] Qwen Team. Qwen3, April 2025. URL https://qwenlm.github.io/blog/qwen3/. [8] Hugging Face. Open r1: fully open reproduction of deepseek-r1, January 2025. URL https://github.com/huggingface/open-r1. [9] DeepSeek AI. Day 6: One more thing deepseek-v3/r1 inference system overview. https:// github.com/deepseek-ai/open-infra-index/blob/main/202502OpenSourceWeek/ 2025. day_6_one_more_thing_deepseekV3R1_inference_system_overview.md, Accessed: 2025-05-03. [10] Hemish Veeraboina. Aime problem set 1983-2024, 2023. URL https://www.kaggle.com/ datasets/hemishveeraboina/aime-problem-set-1983-2024. [11] Isaac Ong, Amjad Almahairi, Vincent Wu, Wei-Lin Chiang, Tianhao Wu, Joseph Gonzalez, Waleed Kadous, and Ion Stoica. Routellm: Learning to route llms from preference data. In The Thirteenth International Conference on Learning Representations, 2024. [12] Tao Feng, Yanzhen Shen, and Jiaxuan You. Graphrouter: graph-based router for llm selections. In Proceedings of the International Conference on Learning Representations (ICLR), 2025. URL https://openreview.net/forum?id=eU39PDsZtT. Poster presentation. [13] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024. [14] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle-2: Faster inference of language models with dynamic draft trees. arXiv preprint arXiv:2406.16858, 2024. 11 [15] Lefan Zhang, Xiaodan Wang, Yanhua Huang, and Ruiwen Xu. Learning harmonized representations for speculative sampling. arXiv preprint arXiv:2408.15766, 2024. [16] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle-3: Scaling up inference acceleration of large language models via training-time test. arXiv preprint arXiv:2503.01840, 2025. [17] Xiaoye Qu, Yafu Li, Zhaochen Su, Weigao Sun, Jianhao Yan, Dongrui Liu, Ganqu Cui, Daizong Liu, Shuxian Liang, Junxian He, et al. survey of efficient reasoning for large reasoning models: Language, multimodality, and beyond. arXiv preprint arXiv:2503.21614, 2025. [18] Yang Sui, Yu-Neng Chuang, Guanchu Wang, Jiamu Zhang, Tianyi Zhang, Jiayi Yuan, Hongyi Liu, Andrew Wen, Hanjie Chen, Xia Hu, et al. Stop overthinking: survey on efficient reasoning for large language models. arXiv preprint arXiv:2503.16419, 2025. [19] Silei Xu, Wenhao Xie, Lingxiao Zhao, and Pengcheng He. Chain of draft: Thinking faster by writing less. arXiv preprint arXiv:2502.18600, 2025. [20] Tingxu Han, Zhenting Wang, Chunrong Fang, Shiyu Zhao, Shiqing Ma, and Zhenyu Chen. Token-budget-aware llm reasoning. arXiv preprint arXiv:2412.18547, 2024. [21] Yue Wang, Qiuzhi Liu, Jiahao Xu, Tian Liang, Xingyu Chen, Zhiwei He, Linfeng Song, Dian Yu, Juntao Li, Zhuosheng Zhang, et al. Thoughts are all over the place: On the underthinking of o1-like llms. arXiv preprint arXiv:2501.18585, 2025. [22] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [23] Huajian Xin, Daya Guo, Zhihong Shao, Zhizhou Ren, Qihao Zhu, Bo Liu, Chong Ruan, Wenda Li, and Xiaodan Liang. Deepseek-prover: Advancing theorem proving in llms through large-scale synthetic data. arXiv preprint arXiv:2405.14333, 2024. [24] Keming Lu, Hongyi Yuan, Runji Lin, Junyang Lin, Zheng Yuan, Chang Zhou, and Jingren Zhou. Routing to the expert: Efficient reward-guided ensemble of large language models. arXiv preprint arXiv:2311.08692, 2023. [25] Xinyuan Wang, Yanchi Liu, Wei Cheng, Xujiang Zhao, Zhengzhang Chen, Wenchao Yu, Yanjie Fu, and Haifeng Chen. Mixllm: Dynamic routing in mixed large language models. arXiv preprint arXiv:2502.18482, 2025. [26] Edward Loper and Steven Bird. Nltk: The natural language toolkit. arXiv preprint cs/0205028, 2002. [27] Fengxiang Sun. jieba: Chinese text segmentation. https://github.com/fxsjy/jieba, 2013. Accessed: 2025-04-01. [28] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [29] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph E. Gonzalez, Clark Barrett, and Ying Sheng. Sglang: Efficient execution of structured language model programs, 2023. [30] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. [31] Bespoke Labs. Bespoke-stratos: The unreasonable effectiveness of reasoning distillation. https://www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-ofreasoning-distillation, 2025. Accessed: 2025-01-22. 12 [32] Huan Ma, Jingdong Chen, Guangyu Wang, and Changqing Zhang. Estimating llm uncertainty with logits. arXiv preprint arXiv:2502.00290, 2025. [33] Rodolfo Zevallos, Mireia Farrús, and Núria Bel. Frequency balanced datasets lead to better language models. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 78597872, 2023. [34] Yash Akhauri, Anthony Fei, Chi-Chih Chang, Ahmed AbouElhamayed, Yueying Li, and arXiv preprint Mohamed Abdelfattah. Splitreason: Learning to offload reasoning. arXiv:2504.16379, 2025. [35] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. Gpqa: graduate-level google-proof q&a benchmark, 2023. [36] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code, 2024. [37] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [38] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. [39] Zixuan Zhou, Xuefei Ning, Ke Hong, Tianyu Fu, Jiaming Xu, Shiyao Li, Yuming Lou, Luning Wang, Zhihang Yuan, Xiuhong Li, Shengen Yan, Guohao Dai, Xiao-Ping Zhang, Yuhan Dong, and Yu Wang. survey on efficient inference for large language models. ArXiv, abs/2404.14294, 2024. [40] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [41] Tianyu Fu, Haofeng Huang, Xuefei Ning, Genghan Zhang, Boju Chen, Tianqi Wu, Hongyi Wang, Zixiao Huang, Shiyao Li, Shengen Yan, et al. Moa: Mixture of sparse attention for automatic large language model compression. arXiv preprint arXiv:2406.14909, 2024. [42] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. In International Conference on Machine Learning (ICML), 2025. [43] Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. [44] Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025. [45] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. Awq: Activation-aware weight quantization for on-device llm compression and acceleration. Proceedings of Machine Learning and Systems, 6:87100, 2024. [46] Shiyao Li, Xuefei Ning, Luning Wang, Tengxuan Liu, Xiangsheng Shi, Shengen Yan, Guohao Dai, Huazhong Yang, and Yu Wang. Evaluating quantized large language models. In Proceedings of the 41st International Conference on Machine Learning, pages 2848028524, 2024. [47] Jintao Zhang, Jia Wei, Pengle Zhang, Jun Zhu, and Jianfei Chen. Sageattention: Accurate 8-bit attention for plug-and-play inference acceleration. In International Conference on Learning Representations (ICLR), 2025. 13 [48] Jintao Zhang, Haofeng Huang, Pengle Zhang, Jia Wei, Jun Zhu, and Jianfei Chen. Sageattention2: Efficient attention with thorough outlier smoothing and per-thread int4 quantization. In International Conference on Machine Learning (ICML), 2025. [49] Jintao Zhang, Jia Wei, Pengle Zhang, Xiaoming Xu, Haofeng Huang, Haoxu Wang, Kai Jiang, Jun Zhu, and Jianfei Chen. Sageattention3: Microscaling fp4 attention for inference and an exploration of 8-bit training. arXiv preprint arXiv:2505.11594, 2025. [50] Tengxuan Liu, Shiyao Li, Jiayi Yang, Tianchen Zhao, Feng Zhou, Xiaohui Song, Guohao Dai, Shengen Yan, Huazhong Yang, and Yu Wang. Pm-kvq: Progressive mixed-precision kv cache quantization for long-cot llms. arXiv preprint arXiv:2505.18610, 2025. [51] Wang Yang, Xiang Yue, Vipin Chaudhary, and Xiaotian Han. Speculative thinking: Enhancing small-model reasoning with large model guidance at inference time. arXiv preprint arXiv:2504.12329, 2025. [52] Rui Pan, Yinwei Dai, Zhihao Zhang, Gabriele Oliaro, Zhihao Jia, and Ravi Netravali. Specreason: Fast and accurate inference-time compute via speculative reasoning. arXiv preprint arXiv:2504.07891, 2025. [53] Wenhao Zheng, Yixiao Chen, Weitong Zhang, Souvik Kundu, Yun Li, Zhengzhong Liu, Eric Xing, Hongyi Wang, and Huaxiu Yao. Citer: Collaborative inference for efficient large language model decoding with token-level routing. arXiv preprint arXiv:2502.01976, 2025."
        },
        {
            "title": "A Additional Experiment Setups",
            "content": "A.1 Router achitecture Inputs projection. At each decoding step, we use the hidden states of the last layer, the top 100 logits with the highest values and the embeddings of the predicted token from the SLM to generate the routing result. We first apply linear projections to align their dimensions with the hidden states and then concatenate the features from the logits, hidden states, and token embeddings. Finally, we use another linear layer to project the concatenated features to match the input feature dimension of the model backbone. Neural network backbone. For the router architecture, we adopt six-layer feed-forward network (FFN) with residual connections between blocks as the backbone, using hidden size of 1024. The architecture of each FFN follows the common design used in previous LLMs [38]. Each block begins with LayerNorm for input normalization, followed by pair of linear projections forming an expand-then-contract structure with an expansion factor of 4. Dropout is applied to each linear layer, with GELU activation function between them. These blocks are connected using residual connections. At the end of the last block, we apply an additional layer normalization and linear layer to convert the output to single value, followed by sigmoid function to produce the normalized prediction of the router. predefined threshold pth between 0 and 1 is used for generating binary results from the router output. Predictions above the pth are considered that current tokens diverge from the LLMs reasoning path. A.2 Routing Data Training dataset. Our training data for the router are sourced from tasks across three distinct scenarios: mathematics, code, and question answering (QA). The mathematics problems are drawn from the American Invitational Mathematics Examination (AIME) [10], covering the years 1983 to 2022. Code and QA problems are sampled from Bespoke-Stratos-17k dataset [31]. We use only the formatted questions from these datasets as prompts and generate responses using DeepSeek-R1Distill-Qwen-32B, with the temperature set to 0 and maximum generation length of 32,768 tokens. Only responses that contain an end-of-sequence (EOS) token within the specified length are retained as effective samples, which will be used for subsequent stages of our data generation pipeline, as discussed in Section 4.1. Validation dataset. Our validation dataset are constructed in the exact same way as the training data, but with different queries. The validation dataset comprises all 30 problems from AIME 2023, 69 coding problems from the Bespoke-Stratos-17k dataset that are excluded from the training set, and 60 QA problems selected from the GPQA-Extended [35] dataset. A.3 Training Scheme Loss function. Due to the significant class imbalance in the training data, we adopt the weighed BCEWithLogitsLoss as our loss function. The weight of each class is calculated inversely proportional to its frequency in the class, which encourages the model to pay more attention to underrepresented classes. Training hyperparameters. During training, we employ the AdamW optimizer with hyperparameters β1 = 0.9 and β2 = 0.999. The learning rate is set to 5105, with dropout rate of 0.1 and weight decay of 5 104. We train the neural network with float32 precision. The router is trained for up to 50 epochs using batch size of 1024, with early stopping applied based on patience of 10 epochs. Validation is performed at every epoch. We adopt the checkpoint corresponding to the best-performing epoch on the validation set as the final router used. Threshold selection. After training, we use the validation dataset to select preferred threshold. We pass the pre-collected neural router inputs from the validation dataset through the neural router and record the predicted divergence probabilities. By sweeping pth from 0 to 1, we analyze how different thresholds affect the LLM usage rate and average parameter size., as shown in Figure 8. This process is efficient, as all router inputs (SLM logits, token embeddings, and last-layer hidden states) are pre-collected and evaluated in single pass. During inference, given any user-defined average parameter budget, we set the threshold to meet the target budget accordingly. 15 A.4 Routing system implementation Model Initialization. The routing system consists of three components: SLM (R1-1.5B), LLM (R1-32B), and router model. The SLM is loaded onto single GPU (GPU 1) using the SGLang scheduler, with the mem_fraction_static set to 0.15. The LLM employs tenser-parallel inference distributed across two GPUs (GPU 0 and GPU 1) via SGLang schedulers managed by PyTorchs distributed multiprocessing framework with the NCCL backend, with the mem_fraction_static set to 0.80. The router model is directly loaded onto GPU 0 using standard PyTorch, independent of the SGLang interface. Prior to inference, each of SLM and LLM is individually warmed up using simple inputs to stabilize GPU kernels and caches, ensuring consistent inference latency. Inference workflow. During each inference step, the workflow begins with the SLM decoding single token, returning the corresponding last-layer hidden states and output logits. The router generates divergence probability based on these outputs of SLM. If the probability surpasses the predefined threshold, the LLM is activated to extend the sequence by one token. Specifically, new request, constructed from the input token IDs, is placed into the LLMs input queue. Subsequently, new schedule batch is initialized for the LLM, explicitly setting the forward mode to EXTEND and allocating appropriate memory for handling input sequences. The system maintains prefix indices to track processed tokens, enabling efficient token management between models. When the LLM extends token, it is communicated back through an output queue to replace the SLMs predicted token. token manager actively tracks the sequence states during the generation process, managing active sequences and handling termination conditions effectively. At each token position, the dynamic routing mechanism assesses model outputs, determines the appropriate routing decision, and updates sequence states accordingly. This iterative process continues until sequence is completed or reaches the predefined maximum token limit."
        },
        {
            "title": "B Additional Experiment Results",
            "content": "B.1 Speculative Decoding Table 5: Comparison of latency and average speed across methods and batch sizes. Subscripts note the standard deviations across AIME. To ensure measurement accuracy, each query was replicated according to its respective batch size. The Speed(tok/s) denotes tokens processed per second for single batch operation. Speed(tok/s) Latency(s) Method batch = 1 batch = 2 batch = 4 batch = 1 batch = 2 batch = 4 Eagle2 HASS 71.4 73.3 56.5 60.4 38.3 37.7 244 194 256197 300 257 255 246 459 388 474 Experiment hyperparameters. For both Eagle2 and HASS, we follow their official implementations to set the total number of draft tokens to 60, with draft tree depth of 6, and select 10 nodes during the expansion phase. Limitations of speculative decoding. Speculative decoding accelerates inference by fixing the low parallelism of LLM decoding. It drafts outputs through SLM sequential decoding, then periodically verifies them with high-parallel LLM prefilling. However, in the official implementation, the prefill length (60) of LLM verification substantially exceeds the depth of the draft tree (6), significantly increasing computational demands. At batch size 1, the overall process remains memory bandwidth bound, meaning that the excessive computation required for prefilling does not significantly impact inference latency. As batch size increases, the process transitions to being computation bound, resulting in noticeable latency increases as shown in Table 5. B.2 Data Generation Overhead As Table 6 shows, our four-stage pipeline completes in 56 (448 GPU hours). The LLM response stage dominates runtime (35 h, 280 GPU hours), but it can be mitigated by directly utilizing the responses of LLMs from open-source SFT datasets, provided they were generated by the same LLM 16 Table 6: Latency and GPU usage across different stages of data labeling. Latency (hours) #GPU GPU Hour Stage LLM response SLM Prefill LLM Continuation Verify Total 35 0.1 7 14 8 8 8 8 8 280 0.5 56 112 448 used for routing. The SLM prefill step is highly efficient, requiring only 0.1 of wall clock time. The subsequent LLM continuation and verification stages take 7 (56 GPU hours) and 14 (112 GPU hours), respectively. Compared to downstream tasks, the overall data generation pipeline remains relatively lightweight and efficient. B.3 Performance-Cost Trade-off Figure 7: Scaling of accuracy versus total cost, evaluated across AIME, GPQA, and LiveCodeBench. R2R advances the Pareto frontier beyond distillation and query-level routing methods. Figure 7 illustrates the trade-off between accuracy and the total cost of generation. As defined in Section 5.1, the cost metric is calculated as the average number of output tokens multiplied by the average parameter size, serving as hardware-agnostic indicator of latency across methods. R2R consistently outperforms both query-level routing methods and distilled LLMs, establishing new Pareto frontier in performance-cost trade-offs. B.4 Influence of Routing Threshold Figure 8: Relationship between the routing threshold, recall for divergent tokens, and average parameter size. The average parameter size is computed based on the positive prediction rate of the router at each threshold. Figure 8 visualizes how the routing threshold pth affects the average parameter size on the validation dataset. In our experiments, we select thresholds based on the user-specified average parameter budget (e.g., 6B) measured on the validation set. However, the threshold does not strictly guarantee the same average parameter size during inference, particularly when query difficulty varies significantly from the validation set. Empirically, we observe minimal variance between the target and actual parameter sizes, as the difficulty of our validation and evaluation datasets is generally well aligned. For tasks that are much easier or harder than those considered in this paper, users can construct tailored validation set using our data labeling pipeline to determine an appropriate threshold. Beyond analyzing the relationship between threshold and average parameter size, we also examine its effect on divergent token recall. This links the average parameter size to the ability to recall divergent tokens. As shown in Figure 8, recall rises rapidly with increasing average parameter size, demonstrating the strong predictive performance of our router model."
        },
        {
            "title": "C Additional Related Work",
            "content": "C.1 Model-level Compression Extensive studies have been proposed to accelerate the costly decoding processes of LLM by compressing the models themselves [39]. Prominent techniques include sparse attention mechanisms [4044] and model quantization [4550]. In contrast, our R2R method focuses on optimizing inference above the model level, complementing these model compression techniques. Therefore, it can be effectively combined with them to further enhance inference efficiency. C.2 Concurrent Mix Inference Methods Given recent rapid advancements in reasoning LLMs, several concurrent studies also explore mix inference strategies that integrate small and large language models. These methods differ primarily in their routing granularity, objectives, and specific routing schemes. Step-level methods: Speculative Thinking [51], SplitReason [34], and SpecReason [52] operate at the reasoning step granularity. Speculative Thinking observes that LLMs excel at affirmation and reflection compared to SLMs. Thus, it employs heuristic triggerssuch as affirmations (\"yeah\"), reflections (\"wait\"), or verification signals (\"check\")to invoke the LLM selectively after detecting delimiter tokens (e.g., \"nn\"), enhancing subsequent SLM-generated segments. SplitReason aims to offload difficult reasoning steps to the LLM. It first uses strong LLM to identify challenging reasoning steps, then trains the SLM to generate special token (i.e., <bigmodel>) signaling the LLM to take over these difficult steps. SpecReason adapts speculative decoding to reasoning step-level. It utilizes the LLM to evaluate steps generated by the SLM, reverting to the LLM only when the score of SLM-generated steps falls below certain threshold. Token-level methods: Unlike step-level methods, CITER [53] adopts token-level routing strategy. It formulates the routing problem as long-horizon reinforcement learning task, optimizing for final answer quality and inference efficiency. Because CITER targets general decoding tasks (e.g., short-form QA), repeatedly generating the complete response to determine token-level preferences remains computationally manageable. Distinctiveness of R2R: R2R distinguishes itself from concurrent works by specifically targeting immediate divergence correction at token granularity. Unlike methods focused on offloading complex reasoning steps, R2R addresses the subtle scenario where the SLM and LLM may agree on challenging steps, yet diverge unexpectedly on seemingly straightforward tokens (under human or LLM judgment). Such divergences can significantly alter the subsequent reasoning path, thus requiring immediate correction. Moreover, R2R differs from the speculative decoding scheme, as it does not rely on periodic LLM verification steps to inform routing decisions. Instead, R2R immediately routes divergent tokens to the LLM, effectively preventing divergence without incurring rollback overhead. Given these distinct objectives and design choices, integrating R2R with these concurrent methods represents promising direction for future research, enabling even more effective mix inference frameworks."
        },
        {
            "title": "D Routing Algorithm",
            "content": "As illustrated in Algorithm 1, the objective of our routing algorithm is to identify and correct pathdivergent tokens during inference by using predictions from large language model (LLM). Both SLM and LLM perform greedy decoding, and token is considered identical if both models produce the same prediction. 18 Algorithm 1 Path-Following Routing identical mi θs Input: Partial sequence S<i, models {θs, θl} Output: Selected model mi 1: ys arg maxy Pθs(y S<i) 2: yl arg maxy Pθl (y S<i) 3: if ys = yl then 4: 5: else 6: 7: 8: 9: 10: 11: 12: 13: end if 14: return mi Ss CONTINUATION(S<i, ys) Sl CONTINUATION(S<i, yl) if Je(Ss, Sl) = 0 then mi θs mi θl end if else neutral divergent Algorithm 2 Continuation (S, y) Input: Prefix sequence S, initial token Output: Completed sequence 1: + 2: while / SEP do 3: 4: 5: end while 6: return arg maxy Pθl (y S) + Algorithm 3 LLM Verifier V(Ss, Sl) Input: Sequences Ss, Sl Output: (0: neutral, 1: divergent) 1: LLM verifies if Ss and Sl diverges 2: return When the SLM and LLM outputs differ, the algorithm triggers continuation process for each model: the SLM and LLM, respectively, continue generating tokens, starting from their initial divergent prediction, until special separator (SEP) token is produced. These continuations yield two complete sequences that differ only at the initial divergence point and subsequent tokens. To assess whether this divergence impacts reasoning, separate LLM-based verifier is employed. This verifier receives the two generated sequences and outputs binary decision: 0 if the sequences are semantically neutral, and 1 if they diverge significantly in meaning or logic. If the verifier outputs 0 (neutral), the router accepts the SLMs prediction. However, if the verifier outputs 1 (divergent), the algorithm corrects the current token by adopting the LLMs prediction, thus preventing further drift from the intended reasoning path. This approach ensures that the system maintains high alignment with LLM reasoning, while minimizing unnecessary reliance on the LLM by routing to the more efficient SLM whenever possible. Proof of Quality Guarantee for Full Path-Following Routing E.1 Notations We summarize the notations used throughout this proof: θl, θs: the large and small language models (LLM, SLM), respectively. S<i = [x0, . . . , xn1, y0, . . . , yi1]: the prefix sequence up to, but not including, position i, where S<0 contains only the input tokens. yi(mS<i): the token generated at position by model {θs, θl} given prefix S<i. V(, ) {0, 1}: the quality verifier function, returning 1 iff the first sequence achieves the same quality as the second. S(L) <i : sequence up to generated by the LLM only. S(M ) <i : sequence up to generated by the mixed (routed) strategy. Ss, Sl: continuation sequences as defined in Equations 8 and 9. T<i: the sequence formed by S(M ) <i concatenated with the LLMs continuation tokens: T<i = S(M ) <i [yi(θlS(M ) <i ), yi+1(θlS(M ) <i yi(θlS(M ) <i )), . . . , EOS] (6) The routing decision at each step is given by: mi = θs, yi(θsS<i) = yi(θlS<i) (cid:123)(cid:122) (cid:125) (cid:124) identical θl, V(Ss, Sl) = 0 (cid:125) (cid:124) (cid:123)(cid:122) divergent or V(Ss, Sl) = 1 (cid:125) (cid:124) (cid:123)(cid:122) neutral The continuation sequences after the i-th token are: Ss = S<i [yi(θsS<i)] [yi+1(θlS<i yi(θsS<i)), . . . , EOS] Sl = S<i [yi(θlS<i)] [yi+1(θlS<i yi(θlS<i)), . . . , EOS] where denotes sequence concatenation. E.2 Theorem (7) (8) (9) The sequence S(M ), generated by the path-following routing strategy, is guaranteed to achieve the same quality as its LLM-only counterpart S(L) under V. E.3 Proof Base Case. At = 0, S(M ) Thus, V(T<0, S(L)) = 1. <0 = S(L) <0 = [x0, . . . , xn1], so T<0 is simply the LLMs full sequence. Inductive Hypothesis. Suppose that for some with 0 < n, we have V(T<k, S(L)) = 1; i.e., continuing from S(M ) <k with the LLM produces sequence of equal quality to S(L). Inductive Step. Consider the (k + 1)-th token y(M ) determined by Eq. 7. There are three cases: (a) Identical: yk(θsS(M ) <k ). Then y(M ) sequence remains identical and V(T<k+1, S(L)) = 1. <k ) = yk(θlS(M ) matches the LLMs output, so the (b) Neutral: yk(θsS(M ) <k ) = yk(θlS(M ) token, so T<k+1 = Ss. By definition, V(T<k+1, S(L)) = 1. <k ) but V(Ss, Sl) = 1. The router selects the SLM (c) Divergent: V(Ss, Sl) = 0. The router selects the LLM token, so the mixed sequence again matches the LLMs output, and thus V(T<k+1, S(L)) = 1. Conclusion. By mathematical induction, for all [0, n], the continuation T<i maintains the same quality as S(L) under V. At generation completion (i = n), S(M ) = S(M ) <n+1, so V(S(M ), S(L)) = 1."
        },
        {
            "title": "F Prompts and Examples",
            "content": "F.1 Prompt for Verifier Model As discussed in Section 4.1, we design structured prompt for the verifier model (Qwen2.5-72BInstruct) to assess whether divergences between two sentences affect their meaning, reasoning, logic, or conclusions. Please refer to Text 2. for the exact prompt. The prompt highlights the divergence point and provides explicit criteria for labeling. It instructs the model to justify its judgment with brief explanation and includes illustrative examples to guide the models understanding of both scenarios. F.2 Response Example We use an example question from AIME and responses from R1-1.5B, R1-32B and R2R to provide an intuitive understanding of our method. 20 Text 1. Question Find the largest possible real part of (75 + 117i)z + 96 + 144i where is complex number with = 4. Text 3-5 shows the example responses. The R1-1.5B and R1-32B models produce distinct final answers for the maximum real part, reflecting divergence in their reasoning paths. By contrast, R2R identifies and corrects this divergence, navigating the correct reasoning path to get the final answer matches that of the stronger 32B model. At the same time, R2R tolerates neutral differencessuch as minor phrasing or presentationbetween models when these do not affect the core reasoning or conclusions. This selective routing mechanism enables R2R to deliver both high efficiency and accuracy by only invoking the large model for tokens that would otherwise lead to substantive differences in meaning or logic. Text 2. Prompt For Verifier Model Task: Determine if the divergence between Sentence 1 and Sentence 2 affects the meaning, reasoning, logic, or conclusions derived from them. Instructions: The marker indicates where the sentences diverge. It is not part of the original text. Assess whether this divergence changes the meaning, reasoning, logic, or conclusions, or if it introduces new information or contradictions. Output 1 if: The divergence causes change in meaning, reasoning, logic, or conclusions. It introduces new information, shifts focus, or contradicts prior facts. The sentences follow different reasoning paths or focus on different aspects. Output 0 if: The divergence is superficial and does not affect meaning, reasoning, logic, or conclusions. Both sentences follow the same reasoning path or lead to the same conclusion. Reasoning: Provide brief explanation of how the divergence impacts (or does not impact) meaning, reasoning, logic, or conclusions. Example 1 (Same - 0): Sentence 1: \"The ratio of adults to total people is now 11/25.\" Sentence 2: \"The ratio of adults to total people changed from 5/12 to 11/25 after adding 50 people.\" Output: 0 Reasoning: The change from \"is\" to \"changed\" does not affect the overall meaning, reasoning, logic, or conclusions. Example 2 (Different - 1): Sentence 1: \"Lets solve this using integration by parts.\" Sentence 2: \"Lets solve this using u-substitution.\" Output: 1 Reasoning: The change in method (from integration by parts to substitution) alters the reasoning and approach to solving the problem. Now complete the task: Common Context: {CONTEXT} Sentence 1: {SENTENCE1} Sentence 2: {SENTENCE2} Answer (Output: <0 or 1>) Reasoning: 22 Text 5. R2R Response Okay, so have this problem: need to find the largest possible real part of the expression (75+117i)z+ 96 + 144i , where is complex number with = 4. Hmm, that sounds bit tricky, but let me try to break it down step by step. First, know that is complex number with magnitude 4. So, can represent in polar form as = 4eiθ, where θ is the argument of z. That might make things easier because working with exponentials can sometimes simplify multiplication and division. So, let me rewrite the expression using this polar form. The expression is: (75+117i)z+ 96 + 144i Substituting = 4eiθ, we get: (75 + 117i)(4eiθ) + 96+144i 4eiθ Simplify each term: First term: (75 + 117i) 4eiθ =4(75 + 117i)eiθ Which is: 300 cos θ 468 sin θ + i(300 sin θ + 468 cos θ) Thus, the largest possible real part is: 540 Text 3. R1-1.5B Response Text 4. R1-32B Response Okay, so have this problem: need to find the largest possible real part of the expression (75+117i)z+ 96 + 144i , where is complex number with = 4. Hmm, complex numbers can sometimes be tricky, but let me try to break this down step by step. First, know that any complex number can be written in polar form as = 4eiθ since its magnitude is 4. That might make things easier because then can express in terms of its angle θ. So, let me write that down: = 4eiθ would be 1 Then, 1 4 eiθ because the reciprocal of eiθ is eiθ. So, substituting back into the expression, we have: (75 + 117i) 4eiθ + 96+144i 4eiθ Let me compute each term separately. First, lets compute (75 + 117i) 4eiθ. Multiplying 75 + 117i by 4 gives: 4 75 = 300 and 4 117i = 468i, so the first term is 300 + 468i. The final answer is: 300 + 12 Okay, so have this problem here: need to find the largest possible real part of the expression (75+117i)z+ 96 + 144i So, where is complex number with = 4. Hmm, that sounds bit tricky, but let me try to break it down step by step. First, know that is complex number with magnitude 4. can represent in polar form. That might make things easier because working with polar forms can simplify multiplication and division. So, let me write = 4eiθ, where θ is the argument of z. That way, = 4 is automatically satisfied. Now, substitute = 4eiθ into the expression. So, the expression becomes: (75 + 117i) 4eiθ + 96+144i 4eiθ lets Simplify each term separately. Lets start with the first term: (75 + 117i) 4eiθ =4(75 + 117i)eiθ 475 cos θ = 300 cos θ 4 (117 sin θ) = 468 sin θ Thus, the largest possible real part is:"
        }
    ],
    "affiliations": [
        "Infinigence AI",
        "Shanghai Jiao Tong University",
        "Tsinghua University"
    ]
}