{
    "paper_title": "Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization",
    "authors": [
        "Jialu Li",
        "Shoubin Yu",
        "Han Lin",
        "Jaemin Cho",
        "Jaehong Yoon",
        "Mohit Bansal"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in text-to-video (T2V) diffusion models have significantly enhanced the visual quality of the generated videos. However, even recent T2V models find it challenging to follow text descriptions accurately, especially when the prompt requires accurate control of spatial layouts or object trajectories. A recent line of research uses layout guidance for T2V models that require fine-tuning or iterative manipulation of the attention map during inference time. This significantly increases the memory requirement, making it difficult to adopt a large T2V model as a backbone. To address this, we introduce Video-MSG, a training-free Guidance method for T2V generation based on Multimodal planning and Structured noise initialization. Video-MSG consists of three steps, where in the first two steps, Video-MSG creates Video Sketch, a fine-grained spatio-temporal plan for the final video, specifying background, foreground, and object trajectories, in the form of draft video frames. In the last step, Video-MSG guides a downstream T2V diffusion model with Video Sketch through noise inversion and denoising. Notably, Video-MSG does not need fine-tuning or attention manipulation with additional memory during inference time, making it easier to adopt large T2V models. Video-MSG demonstrates its effectiveness in enhancing text alignment with multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V generation benchmarks (T2VCompBench and VBench). We provide comprehensive ablation studies about noise inversion ratio, different background generators, background object detection, and foreground object segmentation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 1 ] . [ 1 1 4 6 8 0 . 4 0 5 2 : r Training-free Guidance in Text-to-Video Generation via Multimodal Planning and Structured Noise Initialization Jialu Li* Shoubin Yu* Han Lin*"
        },
        {
            "title": "Jaehong Yoon Mohit Bansal",
            "content": "UNC Chapel Hill {jialuli, shoubin, hanlincs, jmincho, jhyoon, mbansal}@cs.unc.edu Video-MSG.github.io"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in text-to-video (T2V) diffusion models have significantly enhanced the visual quality of the generated videos. However, even recent T2V models find it challenging to follow text descriptions accurately, especially when the prompt requires accurate control of spatial layouts or object trajectories. recent line of research uses layout guidance for T2V models that require fine-tuning or iterative manipulation of the attention map during inference time. This significantly increases the memory requirement, making it difficult to adopt large T2V model as backbone. To address this, we introduce VIDEO-MSG, training-free Guidance method for T2V generation based on Multimodal planning and Structured noise initialization. VIDEO-MSG consists of three steps, where in the first two steps, VIDEO-MSG creates VIDEO SKETCH, fine-grained spatio-temporal plan for the final video, specifying background, foreground, and object trajectories, in the form of draft video frames. In the last step, VIDEO-MSG guides downstream T2V diffusion model with VIDEO SKETCH through noise inversion and denoising. Notably, VIDEO-MSG does not need fine-tuning or attention manipulation with additional memory during inference time, making it easier to adopt large T2V models. VIDEO-MSG demonstrates its effectiveness in enhancing text alignment with multiple T2V backbones (VideoCrafter2 and CogVideoX-5B) on popular T2V generation benchmarks (T2VCompBench and VBench). We provide comprehensive ablation studies about noise inversion ratio, different background generators, background object detection, and foreground object segmentation. 1. Introduction Recent advances in text-to-video (T2V) diffusion models [1, 2, 12, 17, 32, 34, 37, 38, 45, 46, 57] have dramatTo address ically improved the quality of generated videos in diverse domains. However, even recent T2V generation models still often struggle to follow text descriptions accurately, especially when the prompt requires accurate control of spatial layouts or object trajectories. As illustrated in Fig. 1 (b), recent work has studied improving text alignment by providing detailed layout guidance as an additional input to T2V models, such as bounding boxes [21, 24, 26], optical flow [23], and object trajectories [53, 62], which are often created from large language model (LLM). However, since the original T2V models do not understand the layout guidance, these approaches fine-tune the T2V models with layout annotations [24, 26] or iteratively manipulating the attention map of T2V models during inference time [21]. While effective, these techniques substantially increase memory consumption at inference time or require retraining for different T2V backbones, limiting their scalability to large T2V models. this, we introduce VIDEO-MSG, Multimodal Sketch Guidance for video generation, training-free guidance method for T2V generation based on multimodal planning and structured noise initialization, as illustrated in Fig. 1 (c). As illustrated in Fig. 2, (1) background VIDEO-MSG consists of three steps: planning (Sec. 3.1), (2) foreground object layout and trajectory planning (Sec. 3.2), and (3) video generation with structured noise inversion (Sec. 3.3). From the first two steps, VIDEO-MSG creates VIDEO SKETCH, fine-grained spatial and temporal plan with set of multimodal models, including multimodal LLM (MLLM), object detection, and instance segmentation models. Then in the last step, VIDEO-MSG guides downstream T2V diffusion model with VIDEO SKETCH through structured noise inversion and denoising. Notably, VIDEO-MSG does not need fine-tuning or additional memory during inference time, making it easier to adopt large T2V models, compared to existing methods based on fine-tuning or iterative attention manipulation. *equal contribution VIDEO-MSG demonstrates their effectiveness in en1 Figure 1. Comparison of different text-to-video generation methods: (a) single model for video generation, (b) video generation with (attention-based) layout guidance, and our (c) VIDEO-MSG, training-free guidance method for T2V generation based on multimodal planning and structured noise initialization. Since VIDEO-MSG does not need fine-tuning or additional memory during inference time, it is easier to adopt large T2V models than previous video layout guidance methods based on fine-tuning or iterative attention manipulation. hancing text alignment with multiple T2V backbones (VideoCrafter2 [4] and CogVideoX-5B [57]) on popular T2V generation benchmarks (T2VCompBench [43] and VBench [11]). For example, VIDEO-MSG improves motion binding with relative gain of 52.46%, numeracy with relative gain of 40.11%, and spatial relationship with relative gain of 11.15% with CogVideoX-5B as T2V generation backbone. We provide comprehensive quantitative and qualitative ablation studies about noise inversion ratio, different background generators, background object detection, and foreground object segmentation. We hope our method can inspire future work on effectively and efficiently integrating LLMs planning ability into video generation. 2. Related Work 2.1. MLLM Planning for Video Generation There are recent research works [8, 22, 24, 51, 64] that leverage the reasoning capabilities and world knowledge of LLMs or multimodal LLMs for the task of video generation. For example, one line of work [22, 24, 51] applies GPT-4 / GPT-4o to expand single text prompt into video plan in the format of bounding boxes or detailed prompt description [55], which is then given as input to downstream video diffusion model for layout-guided video generation. The other line of work [16, 28, 29, 48, 50, 54] performs token-level planning utilizing multimodal LLMs. For example, [16, 50] tokenize videos and text into the same space and generate video tokens using the same strategy as text (e.g., next-token prediction). However, both directions either rely on high-quality prompts and the bounding box planning or require extensive training and do not fully leverage the power of existing visual tools for fine-grained video generation. In contrast, our work leverages the power of both multimodal LLMs and image/video diffusion models to generate VIDEO SKETCH for final fine-grained motion control, and is fully training-free. 2.2. Motion Direction Control in Video Generation Controllability in video generation is gaining increasing attention in the field of generative AI, as it enables models to generate videos aligned with user intent. One line of research focuses on training models with the capability of trajectory control, camera control, or motion control by generating intermediate representations. For trajectory control, recent works such as DragNUMA [58], IVA-0 [59], DragAnything [53], and TrackGo [63] encode object movement trajectories into dense features, which are then fused into the diffusion model to enable object movement control. On the other hand, CameraCtrl [7], MotionCtrl [52], and Image Conductor [20] encode camera extrinsics as features to control camera motion in the generated videos. common drawback of both of these categories is their reliance on accurate object trajectory or camera movement information, which are difficult for users to manipulate directly. Additionally, video datasets with accurate trajectory annotations are limited, which constrains the performance of these models. The third category, including VideoJAM [3] and MotionI2V [40], produces motion and video representations jointly, or sequentially by first generating intermediate representations, which then serve as guidance for generating video outputs. However, such methods require extensive 2 Figure 2. Three stages of VIDEO-MSG. In the first stage, the MLLM plans specific global and local contexts that fit the provided text-tovideo prompt. The text-to-image (T2I) model uses the MLLM planned context to render the necessary components of the video. In the third stage, we generate video with VIDEO SKETCH via noise inversion. training due to extra generation objectives. In contrast, our method uses an image-to-video model, allowing us to transform existing, real-world images into controllable videos under LLM planning. 3. Method We introduce VIDEO-MSG, Multimodal Sketch Guidance for video generation, training-free guidance method for T2V generation based on multimodal planning and structured noise initialization. VIDEO-MSG consists of three stages (illustrated in Fig. 2): Background planning (Sec. 3.1), where we adopt T2I and I2V models to generate background image priors with natural animation. Foreground Object Layout and Trajectory Planning (Sec. 3.2), where we apply MLLM and object detectors to plan and place foreground objects into the background harmoniously. Video Generation with Structured Noise Initialization (Sec. 3.3), where the synthesized images derived from the above stages are used as VIDEO SKETCH for final video generation via inversion techniques. 3.1. Background Planning Given prompt for video generation, we first ask an MLLM (GPT-4o [33]) to generate detailed background description (see Stage 1 in Fig. 2). Here, we explicitly instruct the MLLM to generate only the background and avoid including any moving or key objects mentioned in the original prompt, thereby enforcing proper decoupling. We find that this strategy helps address issues in conditional T2I generation based on bounding boxes, where the T2I model may fail to generate the foreground object at the specified box location in the image. In addition, we explore two approaches for background generation: (1) Using T2I model to generate an initial background, followed by an I2V model to animate it. In this way, we can adopt strong T2V model to potentially achieve improved video aesthetic quality. (2) Directly using T2V model to generate the background with animation, which avoids the potential distribution gap between the two models in (1). In both cases, we adopt video generation model. We aim to introduce natural background animation rather than keeping it static while only animating foreground objects. This ensures that elements such as flowing water, moving clouds, or swaying trees are naturally incorporated, making the generated videos more realistic and visually coherent. Moreover, by comparing approaches (1) with (2), we notice that the advantage of adopting strong T2I model in (1) outweighs the domain gap between the T2I and I2V models in (2) as discussed in Sec. 4.3. Therefore, we apply approach (1) as our default experiment setting. 3.2. Foreground Object Layout and Trajectory"
        },
        {
            "title": "Planning",
            "content": "This stage aims to place the property of the foreground object in the background in spatially coherent manner. We first implement this stage by providing the background images generated in stage 1, along with prompt describing movement dynamics to GPT-4o [33], then ask it to generate sequence of bounding boxes to represent the foreground 3 objects movement. For instance, given the text prompt: cat sinking to the left in the living room, GPT-4o can correctly infer the cats movement direction (i.e., moving left). However, when provided with background image of living room, GPT-4o often fails to position the cats bounding box appropriately on the floor (e.g., with the bounding box floating in mid-air or overlapping with unrelated objects), as illustrated in Figure 4. This suggests that while GPT-4o demonstrates strong motion reasoning capabilities, it lacks direct grounding capability for visual elements and struggles to align foreground objects with the background scene in spatially consistent manner. [61] then extract To overcome this limitation, we first detect all objects in the background image with Recognize-Anything (RAM) their bounding boxes with Grounding-DINO [25]. These bounding boxes are fed into GPT-4o to provide explicit spatial context, which helps it accurately position and animate foreground objects, enhancing spatial coherence in generated videos and reducing placement errors. Qualitative examples of the effectiveness of object detection with Grounding-DINO and RAM are presented in Figure 4. With the above inputs (i.e., video text prompt, background image, and the bounding boxes of objects in the background), GPT-4o generates sequence of bounding boxes for the foreground objects in the format [object name, bounding box coordinates] (see stage 2 in Fig. 2). Additionally, it provides textual description for each frame and reasoning process explaining the planned object motions after the sequence of frames. This reasoning step enhances the coherence and accuracy of motion planning. Once the sequence of bounding boxes is obtained, we utilize T2I model to generate the appearance of the foreground object using the prompt: An image of {object name}. However, directly merging the generated object image with the background presents challengethe background in the generated object image can significantly affect the overall visual coherence, as illustrated in Fig. 5. To address this, we apply SAM [14] to extract the object from the generated image, removing any unintended background. Based on the planned bounding boxes, the extracted object is then resized and placed onto the background image at the corresponding location. This process ensures more seamless integration of the foreground object into the background, improving the visual consistency of the generated video. 3.3. Video Generation with Structured Noise Initialization In this stage, we generate final video by guiding the T2V diffusion model with the VIDEO SKETCH created from the previous stage (Sec. 3.2). Inversion methods [41], which are often used in image and video editing tasks [31, 39], 4 can be effectively utilized here to create structured noise to fuse the information from VIDEO SKETCH. While the normal denoising process starts from the terminal timestep (a random noise) to the initial timestep 0 (a clean video), we create per-frame initial noises from VIDEO SKETCH via noise inversion [31] and start denoising from timestep tinv. Specifically, we first encode the sequence of VIDEO SKETCH frames into the latent space using 3D VAE [13, 57]. Next, we obtain the initial noise ztinv αtz0 ` via the forward diffusion process [9]: ztinv ś ? s1p1 βsq ϵ p0, Iq, where αt is the cumulative noise schedule, and ϵ represents Gaussian noise. We parameterize tinv α ˆ , where α p0.0, 1.0q. Inspired by VideoDirectorGPT [24], which uses an LLM to estimate confidence score along with bounding box layouts as layout guidance strength, we employ an LLM to infer an appropriate noise inversion ratio α value given text (see Sec. 4.3 for detailed experiments). We description. explain more details about the noise inversion in Appendix. 1 αtinvϵ, ? 4. Experiments 4.1. Experiment Setups Datasets. We evaluate VIDEO-MSG on popular text-tovideo generation benchmarks, T2V-CompBench [43] and VBench [11]. T2V-CompBench and VBench measure diverse aspects of text-to-video generation tasks with seven (e.g., consistent attribute binding, motion binding, spatial relationships) and sixteen categories (e.g., overall consistency, color, temporal flickering, motion smoothness), respectively. In this work, we primarily use T2V-CompBench to evaluate video diffusion models capability in compositional text-to-video generation, and use VBench to measure the motion smoothness of the generated video. Implementation details. We implement VIDEO-MSG on two recent text-to-video generation diffusion models: VideoCrafter2 [4] and CogVideoX-5B [57]. To generate the VIDEO SKETCH, we employ FLUX.1-dev [19] and SDXL [36] as the background generator, and CogVideoX5B as the image-to-video generator. We utilize RecognizeAnything [61] and Gounded-Segment-Anything [14] for foreground object segmentation. We utilize GPT4o as the multi-modal LLM for background description generation, foreground object layout and trajectory planning, and determining the noise inversion ratio α dynamically based on the prompt. For noise inversion ratio α (Sec. 3.3), we find the range [0.7, 0.9] works well for CogVideoX-5B, and the range [0.5, 0.8] works well for VideoCrafter2 (see Sec. 4.3 for ablation study). All experiments are conducted on A100 and A6000 GPUs, with batch size 1 and an approximate memory usage of 16 GB. We provide additional details, such as prompts used for GPT-4o, in the Appendix. Model (Closed-source models) Pika [44] Gen-3 [38] Dreamina [5] PixVerse [35] Kling [15] (Open-source models) ModelScope [49] ZeroScope [42] AnimateDiff [6] Latte [30] Show-1 [60] Open-Sora 1.2 [10] Open-Sora-Plan v1.1.0 [18] VideoTetris [47] Vico [56] VideoCrafter2 [4] VideoCrafter2 + LVD [21] VideoCrafter2 + VIDEO-MSG (Ours) CogVideoX-5B [57] CogVideoX-5B + VIDEO-MSG (Ours) Consist-attr Dynamic-attr Spatial Motion Action Interaction Numeracy 0.6513 0.7045 0.8220 0.7370 0.8045 0.5483 0.4495 0.4883 0.5325 0.6388 0.6600 0.7413 0.7125 0.7025 0.6750 0.6663 (-0.0087) 0.7536 (+0.0786) 0.7220 0.7109 (-0.0111) 0.1744 0.2078 0.2114 0.1738 0.2256 0.1654 0.1086 0.1764 0.1598 0.1828 0.1714 0.1770 0.2066 0.2376 0.1850 0.2308 (+0.0458) 0.2110 (+0.0260) 0.2334 0.2102 (-0.0232) 0.5043 0.5533 0.6083 0.5874 0.6150 0.2221 0.3111 0.2391 0.2178 0.2448 0.5380 0.6280 0.6660 0.6960 0.6460 0.4220 0.4073 0.3883 0.4476 0.4649 0.5406 0.5587 0.5148 0.4952 0.4891 0.5106 (+0.0215) 0.5866 (+0.0975) 0.5461 0.6070 (+0.0609) 0.2552 0.2319 0.2236 0.2187 0.2316 0.2388 0.2187 0.2204 0.2225 0.2233 0.2178 (-0.0055) 0.3732 (+0.1499) 0.2943 0.4487 (+0.1544) 0.4880 0.4620 0.4140 0.5200 0.4940 0.5717 0.6780 0.5280 0.5480 0.5800 0.5640 (-0.0160) 0.5737 (-0.0063) 0.5960 0.5960 (+0.0000) 0.6625 0.7900 0.8175 0.8275 0.8475 0.7075 0.5550 0.6550 0.6625 0.7700 0.7400 0.7275 0.7600 0.7775 0.7600 0.8125 (+0.0525) 0.8220 (+0.0620) 0.7950 0.7800 (-0.0150) 0.2613 0.2169 0.4006 0.3281 0.3044 0.2066 0.2378 0.0884 0.2187 0.1644 0.2556 0.2928 0.2609 0.2116 0.2041 0.2869 (+0.0828) 0.3138 (+0.1097) 0.2603 0.3647 (+0.1044) Table 1. T2V-CompBench evaluation results. We highlight the best/second-best scores for open-sourced models with bold/underline. 4.2. Quantitative Evaluation Improved control on spatial layout and object trajectory. Table 1 shows that VIDEO-MSG significantly improves both T2V backbone models (VideoCrafter2 and CogVideoX-5B) in many skills, especially in motion binding (Motion), with an increase of 0.1499 on VideoCrafter2 and 0.1544 on CogVideoX-5B. VIDEO-MSG also provides large improvements in spatial relationships (Spatial), and numeracy (Numeracy) in both backbone models. These results show that the planning and structured noise initialization of VIDEO-MSG effectively improve the control of spatial layouts and object trajectories in video generation. It is also noteworthy that VIDEO-MSG, implemented with open-source T2V backbone models, archives higher motion binding scores than closed-source models such as Gen3 [38]. The VIDEO-MSG did not improve the scores in dynamic attribute binding (Dynamic-attr) and object action and interaction (Action and Interaction) categories. This is likely because dynamic changes in object or environment states and interactions and actions between objects are difficult to guide solely with bounding boxes. Comparison to planning-based baseline. We also compare VIDEO-MSG with LVD [21], recent T2V layout guidance method, where it adds gradient-based energy function optimization step before each denoising step of the T2V diffusion backbone. The energy function adjusts the cross-attention map of diffusion models to concentrate within set of object bounding boxes generated by an LLM. On the VideoCrafter2 backbone, we find that VIDEO-MSG outperforms LVD in all categories except for dynamic attribute binding, with the largest improvement observed in motion binding (Motion), where VIDEO-MSG surpasses LVD by 0.1554. This demonstrates the effectiveness of our approach. Note that VIDEO-MSG is also more memoryefficient than LVD, as the layout guidance in LVD requires backpropagation through the T2V diffusion backbone, making it hard to adapt to large diffusion models; we could implement VIDEO-MSG with CogVideoX-5B backbone to run on an A6000 GPU (48GB), but we could not fit LVD even on an A100 (80GB). 4.3. Ablation Studies Noise inversion ratio α. As described in Sec. 3.3, we guide the T2V generation backbone by denoising from an intermediate timestep tinv α ˆ to the initial timestep 0. Here, we experiment with different noise inversion ratios α (i.e., varying the noise injected into the VIDEO SKETCH). Table 2 shows that lower α achieves better performance in motion binding (e.g., moving left/right), numeracy, and spatial relationships but hurts the smoothness of motions. This aligns with the intuition that increasing the number of refinement steps based on VIDEO SKETCH enhances the final motion quality. We observe that automatically inferring proper α given text description with LLM achieves good trade-off and use this approach by default. 5 Figure 3. Videos generated with CogVideoX-5B and VIDEO-MSG with CogVideoX-5B backbone. The videos generated with VIDEOMSG are more accurate regarding object motions, numeracy, and spatial relationships. 6 No. Noise inversion ratio α T2V-CompBench"
        },
        {
            "title": "Spatial Motion Smoothness",
            "content": "1. 2. 3. 4. 5. 6. Direct T2V (no inversion) 0.8 0.7 0.6 0.5 LLM-controlled 0.2233 0.2793 0.3197 0.3352 0.3980 0.3732 0.2041 0.4891 0.2081 0.2653 0.3059 0. 0.3138 0.5502 0.5678 0.6057 0.6447 0.5866 97.73 98.69 98.62 98.63 98.58 99. Table 2. Comparison of different noise inversion ratio α, where we compare static values and LLM-based dynamic values. Backbone T2V: VideoCrafter2. Background generator: Flux + CogVideoX-5B. No. Background Generator Motion Numeracy 1. 2. 3. 4. Direct T2V (no background) SDXL (T2I) + CogVideoX-5B (I2V) FLUX (T2I) + CogVideoX-5B (I2V) CogVideoX-5B (T2V) 0.2897 0.4487 0. 0.4565 0.2750 0.3559 0.3647 0.3028 Table 3. Ablation studies on different background generators. Different background generator. In Table 3, we compare different background generation methods (Sec. 3.1): (1) generating background with text-to-image (T2I) model, followed by an image-to-video (I2V) model for animation, and (2) directly using text-to-video (T2V) model to generate an animated background. While both approaches improve motion binding and numeracy compared to using single T2V model for video generation, the T2I + I2V pipeline scores higher in numeracy, and the T2V approach scores higher in motion binding. We attribute this to the video generation models ability to better refine object motion when the background follows static camera, making foreground changes more salient for the video diffusion model. The I2V pipeline better adheres to the Static Camera prompt, producing natural background animations In contrast, T2V models of- (e.g., wind, light changes). ten disregard the Static Camera requirement, introducing excessive camera motion and scene changes in the video. These inconsistencies make it harder for the video diffusion model to refine foreground objects, leading to performance degradation (e.g., 0.3028 with CogVideoX-5B vs. 0.3647 with FLUX on numeracy). Additionally, we find that stronger T2I model (e.g., FLUX [19]) yields better results than weaker one (e.g., SDXL [36]), highlighting the potential of leveraging high-quality T2I models for layoutcontrolled text-to-video generation. 4.4. Qualitative Analysis VIDEO SKETCH improves control of spatial layout and object trajectory. Fig. 3 compares videos generated from CogVideoX-5B, and VIDEO-MSG (with CogVideoX-5B backbone). We observe that CogVideoX-5B struggles with motion direction (e.g., an egg moves to the right instead of to the left, helicopter ascends instead of descending to the land), numeracy (e.g., generated four bears instead of three bears, four penguins instead of six penguins), and spatial relationships (e.g., vending machine should be located to the right of gorilla, but it is missing; the umbrella should be located on the left of the children). In contrast, VIDEO-MSG successfully guides the T2V backbone to generate videos with correct semantics in all cases. Note that the T2V model can understand the coarse guidance in VIDEO SKETCH and place objects that harmonize well with the background through noise inversion. For example, in the middle example (three bears in river surrounded by mountains), even when the VIDEO SKETCH includes three bears only with other heads facing forward, the T2V model could place the three bears in the river naturally. Effect of different noise inversion ratios α. Fig. 3 shows the video generation results from VIDEO SKETCH (with CogVideoX-5B backbone), with different noise inversion ratios α. Interestingly, the model can automatically refine objects to better align with the prompt and surrounding environment based on different α. We find that lower α (i.e., less noise) generally provides stronger layout control. For example, in the left top example, the egg in the videos with α 0.7 and α 0.8 closely follow the trajectory in VIDEO SKETCH, while in the video α 0.9, the egg movement is small and does not follow the trajectory. However, lower α can lead to less natural generations; e.g., in the bottom-middle example, the boy motion appears less natural at α 0.7 compared to α 0.9. This highlights the importance of selecting an appropriate α to balance motion smoothness with faithful adherence to VIDEO SKETCH. Background object detection helps foreground object placement. We find that deep understanding of the background images through object detection is crucial in foreground planning (Sec. 3.2). As shown in Fig. 4, withFigure 4. Example video showing the importance of background object detection in foreground object placement. Figure 5. Example video showing the importance of foreground object segmentation. out access to background bounding box information, the MLLM fails to place the golden retriever on the grass when relying solely on the background image input. In contrast, when provided with bounding box information from the background (e.g., {\"label\": \"path\", \"box\": [0.44, 0.57, 0.99, 0.99]}), the MLLM successfully positions the golden retriever at the correct location on the grass. Moreover, we find that this planning step directly impacts the final video quality. Conditioning the generation on inaccurate bounding box plans can conflict with the video diffusion models prior knowledge. For instance, in the first frame, the model may generate two golden retrieversone on the ground based on its prior knowledge and another floating in the air according to the VIDEO SKETCHresulting in unrealistic outputs, such as golden retriever running mid-air across the garden. In contrast, our approach, which conditions planning on background bounding boxes, enables the generation of more natural and commonsense-aligned videos. Segmentation of foreground objects improves harmonization. As demonstrated in Fig. 5, without object segmentation, the foreground object (a balloon) does not align well with the background, and the quantity is not well controlled (multiple balloons). This occurs because the video diffusion model does not inherently distinguish between the appearance of the background and that of the balloon, causing them to blend together. In contrast, when we first segment the balloon from the generated foreground object image and then place it onto the background to create VIDEO SKETCH, the balloon in the generated video harmonizes well with the background. 5. Conclusion In this work, we introduce VIDEO-MSG, training-free guidance method designed to enhance text-to-video (T2V) generation through multimodal planning and structured noise initialization. VIDEO-MSG consists of three steps, wherein the first two steps, VIDEO-MSG creates VIDEO SKETCH, detailed spatial and temporal plan utilizing set of multimodal models, including multimodal LLM, object detection, and instance segmentation models. In the final step, VIDEO-MSG guides downstream T2V diffusion model with VIDEO SKETCH through noise inversion and denoising. Notably, VIDEO-MSG does not require fine-tuning or additional memory during inference, making it easier to adopt large T2V models than existing methods that rely on fine-tuning or iterative attention manipulation. VIDEO-MSG demonstrates its effectiveness in enhancing text alignment with multiple T2V backbones on popular T2V generation benchmarks. We also provide comprehensive ablation studies and qualitative examples that support the design choices of VIDEO-MSG. We hope our method can inspire future work on effectively and efficiently integrating LLMs planning capabilities into video generation."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by ARO W911NF2110220, DARPA MCS N66001-19-2-4031, DARPA KAIROS Grant FA8750-19-2-1004, DARPA ECOLE Program No. HR00112390060, NSF-AI Engage Institute DRL211263, ONR N00014-23-1-2356, Microsoft Accelerate Foundation Models Research (AFMR) grant program, and Bloomberg Data Science PhD Fellowship. The views, opinions, and/or findings contained in this article are those of the authors and not of the funding agency."
        },
        {
            "title": "References",
            "content": "[1] Niket Agarwal, Arslan Ali, Maciej Bala, Yogesh Balaji, Erik Barker, Tiffany Cai, Prithvijit Chattopadhyay, Yongxin Chen, Yin Cui, Yifan Ding, et al. Cosmos world founarXiv preprint dation model platform for physical ai. arXiv:2501.03575, 2025. 1 [2] Jake Bruce, Michael Dennis, Ashley Edwards, Jack Parker-Holder, Yuge Shi, Edward Hughes, Matthew Lai, Aditi Mavalankar, Richie Steigerwald, Chris Apps, et al. Genie: Generative interactive environments. In Forty-first International Conference on Machine Learning, 2024. 1 [3] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. Videojam: Joint appearance-motion representations for enhanced motion generation in video models. arXiv preprint arXiv:2502.02492, 2025. 2 [4] Haoxin Chen, Yong Zhang, Xiaodong Cun, Menghan Xia, Xintao Wang, Chao Weng, and Ying Shan. Videocrafter2: Overcoming data limitations for high-quality video diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 7310 7320, 2024. 2, 4, 5 [5] Dreamina AI. Dreamina. capcut.com/ai-tool/platform, 2024. https : / / dreamina . [6] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 5 [7] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. CoRR, 2024. 2 [8] Yingqing He, Menghan Xia, Haoxin Chen, Xiaodong Cun, Yuan Gong, Jinbo Xing, Yong Zhang, Xintao Wang, Chao Weng, Ying Shan, et al. Animate-a-story: Storytelling with retrieval-augmented video generation. arXiv preprint arXiv:2307.06940, 2023. 2 [9] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising Diffusion Probabilistic Models. In NeurIPS, pages 125, 2020. 4 [10] hpcaitech. Open-sora: Democratizing efficient video production for all, 2024. [11] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, 9 Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. VBench: Comprehensive benchmark suite for video generative models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024. 2, 4 [12] Hedra Inc. Hedra. https://www.hedra.com/, 2025. 1 [13] Diederik Kingma and Max Welling. Auto-encoding variational bayes. In ICLR, 2014. 4 [14] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 4 [15] Kling AI. Kling. https://klingai.com/, 2024. 5 [16] Dan Kondratyuk, Lijun Yu, Xiuye Gu, Jose Lezama, Jonathan Huang, Grant Schindler, Rachel Hornung, Vighnesh Birodkar, Jimmy Yan, Ming-Chang Chiu, Krishna Somandepalli, Hassan Akbari, Yair Alon, Yong Cheng, Joshua V. Dillon, Agrim Gupta, Meera Hahn, Anja Hauth, David Hendon, Alonso Martinez, David Minnen, Mikhail Sirotenko, Kihyuk Sohn, Xuan Yang, Hartwig Adam, MingHsuan Yang, Irfan Essa, Huisheng Wang, David Ross, Bryan Seybold, and Lu Jiang. Videopoet: large language model for zero-shot video generation. In Forty-first International Conference on Machine Learning, 2024. 2 [17] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. 1 [18] PKU-Yuan Lab and Tuzhan AI etc. Open-sora-plan, 2024. 5 [19] Black Forest Labs. Flux. https://github.com/ black-forest-labs/flux, 2024. 4, 7 [20] Yaowei Li, Xintao Wang, Zhaoyang Zhang, Zhouxia Wang, Ziyang Yuan, Liangbin Xie, Yuexian Zou, and Ying Shan. Image conductor: Precision control for interactive video synthesis. CoRR, 2024. [21] Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and arXiv Boyi Li. Llm-grounded video diffusion models. preprint arXiv:2309.17444, 2023. 1, 5 [22] Long Lian, Baifeng Shi, Adam Yala, Trevor Darrell, and In The Boyi Li. Llm-grounded video diffusion models. Twelfth International Conference on Learning Representations, 2024. 2 [23] Xinyao Liao, Xianfang Zeng, Liao Wang, Gang Yu, Guosheng Lin, and Chi Zhang. Motionagent: Fine-grained controllable video generation via motion field agent. arXiv preprint arXiv:2502.03207, 2025. 1 [24] Han Lin, Abhay Zala, Jaemin Cho, and Mohit Bansal. Videodirectorgpt: Consistent multi-scene video generation via llm-guided planning. In First Conference on Language Modeling, 2024. 1, 2, 4 [25] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Chunyuan Li, Jianwei Yang, Hang Su, Jun Zhu, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. arXiv preprint arXiv:2303.05499, 2023. [26] Fuchen Long, Zhaofan Qiu, Ting Yao, and Tao Mei. Videodrafter: Content-consistent multi-scene video generation with llm. CoRR, 2024. 1 [27] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 13 [28] Jiasen Lu, Christopher Clark, Rowan Zellers, Roozbeh Mottaghi, and Aniruddha Kembhavi. Unified-io: unified model for vision, language, and multi-modal tasks. In The Eleventh International Conference on Learning Representations, 2023. 2 [29] Jiasen Lu, Christopher Clark, Sangho Lee, Zichen Zhang, Savya Khosla, Ryan Marten, Derek Hoiem, and Aniruddha Kembhavi. Unified-io 2: Scaling autoregressive multimodal models with vision, language, audio, and action. In CVPR, 2024. 2 [30] Xin Ma, Yaohui Wang, Gengyun Jia, Xinyuan Chen, Ziwei Liu, Yuan-Fang Li, Cunjian Chen, and Yu Qiao. Latte: Latent diffusion transformer for video generation. arXiv preprint arXiv:2401.03048, 2024. 5 [31] Chenlin Meng, Yutong He, Yang Song, Jiaming Song, Jiajun Wu, Jun-Yan Zhu, and Stefano Ermon. Sdedit: Guided image synthesis and editing with stochastic differential equations. In International Conference on Learning Representations, 2022. [32] OpenAI. Sora. https://openai.com/sora/, 2024. 1 [33] OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, Alex Nichol, Alex Paino, Alex Renzin, Alex Tachard Passos, Alexander Kirillov, Alexi Christakis, Alexis Conneau, Ali Kamali, Allan Jabri, Allison Moyer, Allison Tam, Amadou Crookes, Amin Tootoochian, Amin Tootoonchian, Ananya Kumar, Andrea Vallone, Andrej Karpathy, Andrew Braunstein, Andrew Cann, Andrew Codispoti, Andrew Galu, Andrew Kondrich, Andrew Tulloch, Andrey Mishchenko, Angela Baek, Angela Jiang, Antoine Pelisse, Antonia Woodford, Anuj Gosalia, Arka Dhar, Ashley Pantuliano, Avi Nayak, Avital Oliver, Barret Zoph, Behrooz Ghorbani, Ben Leimberger, Ben Rossen, Ben Sokolowsky, Ben Wang, Benjamin Zweig, Beth Hoover, Blake Samic, Bob McGrew, Bobby Spero, Bogo Giertler, Bowen Cheng, Brad Lightcap, Brandon Walkin, Brendan Quinn, Brian Guarraci, Brian Hsu, Bright Kellogg, Brydon Eastman, Camillo Lugaresi, Carroll Wainwright, Cary Bassin, Cary Hudson, Casey Chu, Chad Nelson, Chak Li, Chan Jun Shern, Channing Conger, Charlotte Barette, Chelsea Voss, Chen Ding, Cheng Lu, Chong Zhang, Chris Beaumont, Chris Hallacy, Chris Koch, Christian Gibson, Christina Kim, Christine Choi, Christine McLeavey, Christopher Hesse, Claudia Fischer, Clemens Winter, Coley Czarnecki, Colin Jarvis, Colin Wei, Constantin Koumouzelis, Dane Sherburn, Daniel Kappler, Daniel Levin, Daniel Levy, David Carr, David Farhi, David Mely, David Robinson, David Sasaki, Denny Jin, Dev Valladares, Dimitris Tsipras, Doug Li, Duc Phong Nguyen, 10 Duncan Findlay, Edede Oiwoh, Edmund Wong, Ehsan Asdar, Elizabeth Proehl, Elizabeth Yang, Eric Antonow, Eric Kramer, Eric Peterson, Eric Sigler, Eric Wallace, Eugene Brevdo, Evan Mays, Farzad Khorasani, Felipe Petroski Such, Filippo Raso, Francis Zhang, Fred von Lohmann, Freddie Sulit, Gabriel Goh, Gene Oden, Geoff Salmon, Giulio Starace, Greg Brockman, Hadi Salman, Haiming Bao, Haitang Hu, Hannah Wong, Haoyu Wang, Heather Schmidt, Heather Whitney, Heewoo Jun, Hendrik Kirchner, Henrique Ponde de Oliveira Pinto, Hongyu Ren, Huiwen Chang, Hyung Won Chung, Ian Kivlichan, Ian OConnell, Ian OConnell, Ian Osband, Ian Silber, Ian Sohl, Ibrahim Okuyucu, Ikai Lan, Ilya Kostrikov, Ilya Sutskever, Ingmar Kanitscheider, Ishaan Gulrajani, Jacob Coxon, Jacob Menick, Jakub Pachocki, James Aung, James Betker, James Crooks, James Lennon, Jamie Kiros, Jan Leike, Jane Park, Jason Kwon, Jason Phang, Jason Teplitz, Jason Wei, Jason Wolfe, Jay Chen, Jeff Harris, Jenia Varavva, Jessica Gan Lee, Jessica Shieh, Ji Lin, Jiahui Yu, Jiayi Weng, Jie Tang, Jieqi Yu, Joanne Jang, Joaquin Quinonero Candela, Joe Beutler, Joe Landers, Joel Parish, Johannes Heidecke, John Schulman, Jonathan Lachman, Jonathan McKay, Jonathan Uesato, Jonathan Ward, Jong Wook Kim, Joost Huizinga, Jordan Sitkin, Jos Kraaijeveld, Josh Gross, Josh Kaplan, Josh Snyder, Joshua Achiam, Joy Jiao, Joyce Lee, Juntang Zhuang, Justyn Harriman, Kai Fricke, Kai Hayashi, Karan Singhal, Katy Shi, Kavin Karthik, Kayla Wood, Kendra Rimbach, Kenny Hsu, Kenny Nguyen, Keren Gu-Lemberg, Kevin Button, Kevin Liu, Kiel Howe, Krithika Muthukumar, Kyle Luther, Lama Ahmad, Larry Kai, Lauren Itow, Lauren Workman, Leher Pathak, Leo Chen, Li Jing, Lia Guy, Liam Fedus, Liang Zhou, Lien Mamitsuka, Lilian Weng, Lindsay McCallum, Lindsey Held, Long Ouyang, Louis Feuvrier, Lu Zhang, Lukas Kondraciuk, Lukasz Kaiser, Luke Hewitt, Luke Metz, Lyric Doshi, Mada Aflak, Maddie Simens, Madelaine Boyd, Madeleine Thompson, Marat Dukhan, Mark Chen, Mark Gray, Mark Hudnall, Marvin Zhang, Marwan Aljubeh, Mateusz Litwin, Matthew Zeng, Max Johnson, Maya Shetty, Mayank Gupta, Meghan Shah, Mehmet Yatbaz, Meng Jia Yang, Mengchao Zhong, Mia Glaese, Mianna Chen, Michael Janner, Michael Lampe, Michael Petrov, Michael Wu, Michele Wang, Michelle Fradin, Michelle Pokrass, Miguel Castro, Miguel Oom Temudo de Castro, Mikhail Pavlov, Miles Brundage, Miles Wang, Minal Khan, Mira Murati, Mo Bavarian, Molly Lin, Murat Yesildal, Nacho Soto, Natalia Gimelshein, Natalie Cone, Natalie Staudacher, Natalie Summers, Natan LaFontaine, Neil Chowdhury, Nick Ryder, Nick Stathas, Nick Turley, Nik Tezak, Niko Felix, Nithanth Kudige, Nitish Keskar, Noah Deutsch, Noel Bundick, Nora Puckett, Ofir Nachum, Ola Okelola, Oleg Boiko, Oleg Murk, Oliver Jaffe, Olivia Watkins, Olivier Godement, Owen Campbell-Moore, Patrick Chao, Paul McMillan, Pavel Belov, Peng Su, Peter Bak, Peter Bakkum, Peter Deng, Peter Dolan, Peter Hoeschele, Peter Welinder, Phil Tillet, Philip Pronin, Philippe Tillet, Prafulla Dhariwal, Qiming Yuan, Rachel Dias, Rachel Lim, Rahul Arora, Rajan Troll, Randall Lin, Rapha Gontijo Lopes, Raul Puri, Reah Miyara, Reimar Leike, Renaud Gaubert, Reza Zamani, Ricky Wang, Rob Donnelly, Rob Honsby, Rocky Smith, Rohan Sahai, Rohit Ramchandani, Romain Huet, Rory Carmichael, Rowan Zellers, Roy Chen, Ruby Chen, Ruslan Nigmatullin, Ryan Cheu, Saachi Jain, Sam Altman, Sam Schoenholz, Sam Toizer, Samuel Miserendino, Sandhini Agarwal, Sara Culver, Scott Ethersmith, Scott Gray, Sean Grove, Sean Metzger, Shamez Hermani, Shantanu Jain, Shengjia Zhao, Sherwin Wu, Shino Jomoto, Shirong Wu, Shuaiqi, Xia, Sonia Phene, Spencer Papay, Srinivas Narayanan, Steve Coffey, Steve Lee, Stewart Hall, Suchir Balaji, Tal Broda, Tal Stramer, Tao Xu, Tarun Gogineni, Taya Christianson, Ted Sanders, Tejal Patwardhan, Thomas Cunninghman, Thomas Degry, Thomas Dimson, Thomas Raoux, Thomas Shadwell, Tianhao Zheng, Todd Underwood, Todor Markov, Toki Sherbakov, Tom Rubin, Tom Stasi, Tomer Kaftan, Tristan Heywood, Troy Peterson, Tyce Walters, Tyna Eloundou, Valerie Qi, Veit Moeller, Vinnie Monaco, Vishal Kuo, Vlad Fomenko, Wayne Chang, Weiyi Zheng, Wenda Zhou, Wesam Manassra, Will Sheu, Wojciech Zaremba, Yash Patil, Yilei Qian, Yongjik Kim, Youlong Cheng, Yu Zhang, Yuchen He, Yuchen Zhang, Yujia Jin, Yunxing Dai, and Yury Malkov. Gpt-4o system card, 2024. 3 [34] Jack Parker-Holder, Philip Ball, Jake Bruce, Vibhavari Dasagi, Kristian Holsheimer, Christos Kaplanis, Alexandre Moufarek, Guy Scully, Jeremy Shar, Jimmy Shi, Stephen Spencer, Jessica Yung, Michael Dennis, Sultan Kenjeyev, Shangbang Long, Vlad Mnih, Harris Chan, Maxime Gazeau, Bonnie Li, Fabio Pardo, Luyu Wang, Lei Zhang, Frederic Besse, Tim Harley, Anna Mitenkova, Jane Wang, Jeff Clune, Demis Hassabis, Raia Hadsell, Adrian Bolton, Satinder Singh, and Tim Rocktäschel. Genie 2: large-scale foundation world model, 2024. 1 [35] PixVerse. Pixverse. https://app.pixverse.ai, 2024. 5 [36] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models In The Twelfth Interfor high-resolution image synthesis. national Conference on Learning Representations, 2024. 4, 7 [37] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, ChihYao Ma, Ching-Yao Chuang, David Yan, Dhruv Choudhary, Dingkang Wang, Geet Sethi, Guan Pang, Haoyu Ma, Ishan Misra, Ji Hou, Jialiang Wang, Kiran Jagadeesh, Kunpeng Li, Luxin Zhang, Mannat Singh, Mary Williamson, Matt Le, Matthew Yu, Mitesh Kumar Singh, Peizhao Zhang, Peter Vajda, Quentin Duval, Rohit Girdhar, Roshan Sumbaly, Sai Saketh Rambhatla, Sam Tsai, Samaneh Azadi, Samyak Datta, Sanyuan Chen, Sean Bell, Sharadh Ramaswamy, Shelly Sheynin, Siddharth Bhattacharya, Simran Motwani, Tao Xu, Tianhe Li, Tingbo Hou, Wei-Ning Hsu, Xi Yin, Xiaoliang Dai, Yaniv Taigman, Yaqiao Luo, Yen-Cheng Liu, Yi-Chiao Wu, Yue Zhao, Yuval Kirstain, Zecheng He, Zijian He, Albert Pumarola, Ali Thabet, Artsiom Sanakoyeu, Arun Mallya, Baishan Guo, Boris Araya, Breena Kerr, Carleigh Wood, Ce Liu, Cen Peng, Dimitry Vengertsev, Edgar Schonfeld, Elliot Blanchard, Felix Juefei-Xu, Fraylie Nord, Jeff Liang, John Hoffman, Jonas Kohler, Kaolin Fire, Karthik Sivakumar, Lawrence Chen, Licheng Yu, Luya Gao, Markos Georgopoulos, Rashel Moritz, Sara K. Sampson, Shikai Li, Simone Parmeggiani, Steve Fine, Tara Fowler, Vladan Petrovic, and Yuming Du. Movie gen: cast of media foundation models. https://ai.meta.com/blog/moviegenmediafoundationmodels- generativeai-video/, 2025. 1 [38] Runway. Gen-3. https://runwayml.com/blog/ introducing-gen-3-alpha/, 2024. 1, [39] Shelly Sheynin, Adam Polyak, Uriel Singer, Yuval Kirstain, Amit Zohar, Oron Ashual, Devi Parikh, and Yaniv Taigman. Emu edit: Precise image editing via recognition and generation tasks. In CVPR, 2023. 4 [40] Xiaoyu Shi, Zhaoyang Huang, Fu-Yun Wang, Weikang Bian, Dasong Li, Yi Zhang, Manyuan Zhang, Ka Chun Cheung, Simon See, Hongwei Qin, et al. Motion-i2v: Consistent and controllable image-to-video generation with explicit motion modeling. In SIGGRAPH (Conference Paper Track), 2024. 2 [41] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 4 and Stefano Ermon. arXiv preprint [42] Spencer Sterling. https : / / huggingface . co / cerspense / zeroscope _ v2 _ 576w, 2024. 5 Zeroscope v2 576w. [43] Kaiyue Sun, Kaiyi Huang, Xian Liu, Yue Wu, Zihan Xu, Zhenguo Li, and Xihui Liu. T2v-compbench: comprehensive benchmark for compositional text-to-video generation. CoRR, abs/2407.14505, 2024. 2, 4 [44] Pika Team. Pika art. https://pika.art/, 2024. 5 [45] WorldLab Team. https : / / www . Worldlab. worldlabs.ai/blog, 2024. [46] Wan Team. Wan: Open and advanced large-scale video generative models, 2025. 1 [47] Ye Tian, Ling Yang, Haotian Yang, Yuan Gao, Yufan Deng, Jingmin Chen, Xintao Wang, Zhaochen Yu, Xin Tao, Pengfei Wan, Di Zhang, and Bin Cui. Videotetris: Towards compositional text-to-video generation, 2024. 5 [48] Shengbang Tong, David Fan, Jiachen Zhu, Yunyang Xiong, Xinlei Chen, Koustuv Sinha, Michael Rabbat, Yann LeCun, Saining Xie, and Zhuang Liu. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164, 2024. 2 [49] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 5 [50] Xinlong Wang, Xiaosong Zhang, Zhengxiong Luo, Quan Sun, Yufeng Cui, Jinsheng Wang, Fan Zhang, Yueze Wang, Zhen Li, Qiying Yu, et al. Emu3: Next-token prediction is all you need. CoRR, 2024. 2 [51] Zun Wang, Jialu Li, Han Lin, Jaehong Yoon, and Mohit Bansal. Dreamrunner: Fine-grained storytelling video generation with retrieval-augmented motion adaptation. arXiv preprint arXiv:2411.16657, 2024. 11 In attention for long-range image and video generation. The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2 [52] Zhouxia Wang, Ziyang Yuan, Xintao Wang, Yaowei Li, Tianshui Chen, Menghan Xia, Ping Luo, and Ying Shan. Motionctrl: unified and flexible motion controller for video generation. In SIGGRAPH (Conference Paper Track), 2024. 2 [53] Weijia Wu, Zhuang Li, Yuchao Gu, Rui Zhao, Yefei He, David Junhao Zhang, Mike Zheng Shou, Yan Li, Tingting Gao, and Di Zhang. Draganything: Motion control for anyIn European Conference thing using entity representation. on Computer Vision, pages 331348. Springer, 2024. 1, 2 [54] Yecheng Wu, Zhuoyang Zhang, Junyu Chen, Haotian Tang, Dacheng Li, Yunhao Fang, Ligeng Zhu, Enze Xie, Hongxu Yin, Li Yi, et al. Vila-u: unified foundation model integrating visual understanding and generation. arXiv preprint arXiv:2409.04429, 2024. 2 [55] Ling Yang, Zhaochen Yu, Chenlin Meng, Minkai Xu, Stefano Ermon, and CUI Bin. Mastering text-to-image diffusion: Recaptioning, planning, and generating with multimodal llms. In Forty-first International Conference on Machine Learning, 2024. [56] Xingyi Yang and Xinchao Wang. Compositional video generation as flow equalization, 2024. 5 [57] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 1, 2, 4, 5 [58] Shengming Yin, Chenfei Wu, Jian Liang, Jie Shi, Houqiang Li, Gong Ming, and Nan Duan. Dragnuwa: Fine-grained control in video generation by integrating text, image, and trajectory. arXiv preprint arXiv:2308.08089, 2023. 2 [59] Shoubin Yu, Jacob Zhiyuan Fang, Jian Zheng, Gunnar Sigurdsson, Vicente Ordonez, Robinson Piramuthu, and Mohit Bansal. Zero-shot controllable image-to-video animation via In Proceedings of the 32nd ACM motion decomposition. International Conference on Multimedia, pages 33323341, 2024. 2 [60] David Junhao Zhang, Jay Zhangjie Wu, Jia-Wei Liu, Rui Zhao, Lingmin Ran, Yuchao Gu, Difei Gao, and Mike Zheng Shou. Show-1: Marrying pixel and latent diffusion models for text-to-video generation. arXiv preprint arXiv:2309.15818, 2023. 5 [61] Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: strong image tagging model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17241732, 2024. [62] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. arXiv preprint arXiv:2407.21705, 2024. 1 [63] Haitao Zhou, Chuang Wang, Rui Nie, Jinxiao Lin, Dongdong Yu, Qian Yu, and Changhu Wang. Trackgo: flexible and efficient method for controllable video generation. CoRR, 2024. 2 [64] Yupeng Zhou, Daquan Zhou, Ming-Ming Cheng, Jiashi Storydiffusion: Consistent selfFeng, and Qibin Hou."
        },
        {
            "title": "Appendix",
            "content": "In this appendix, we present the following: Noise inversion details using DPM-Solver++ [27] in Sec. A. MLLM prompts we use to collect background description, foreground object layout and trajectory, and α used to determine how much noise to inject during inversion in Sec. B. A. Noise Inversion Details Here, we describe in detail how we adopt the inversion technique for final video generation with the motion priors prepared in Stage 3. Specifically, we first encode the sequence of images with the planned layout, collected in the previous stage (Sec. 3.2), into the latent space using 3D Variational Autoencoder (3D VAE). Then, we perform the forward diffusion process where Gaussian noise is gradually added to the latent. Following the DPM-Solver++ [27] scheduler in CogVideoX, the noised latent at diffusion step is: separate the generation of foreground and background, ensuring that key foreground objects are not mistakenly included in the background. In Fig. 7, we first prompt the MLLM to generate bounding boxes for foreground objects, followed by reasoning for their placement. Additionally, we emphasize that the placement of foreground objects should be informed by background bounding box annotations to improve spatial coherence. Finally, Fig. 8 illustrates the prompt template used to determine the appropriate level of noise injection during inversion. We explicitly incorporate prior knowledge into the template, instructing the MLLM to apply less noise for tasks requiring precise trajectory or layout control and more noise for tasks involving dynamic changes or object actions that cannot be effectively modeled with bounding box plans. ? zt ? 1 αtϵ, αtz0 ` ś s1p1βsq is the cumulative noise schedϵ p0, Iq. (1) Here, αt ule, and ϵrepresents Gaussian noise. Then, given noisy latent zt, we attempt to recover the clean latent as general reverse denoising starting from step t. The model then denoises to zt1 using the DPMSolver++ method, which provides high-order approximation of the reverse diffusion process. Specifically, the update equation for zt1 in second-order solver is: zt1 zt ` λ1 ˆF pzt, tq ` λ2 ˆF pzt ` λ3 ˆF pzt, tq, tmq, (2) where ˆF pz, tq 1 2 βtz g2ptqϵθpz, tq is the estimated drift term, λ1, λ2, λ3 are step-size coefficients computed adaptively, and tm is an intermediate timestep between and 1. We observe that selecting within specific range enables video diffusion models to inject smooth object motions naturally. This process effectively transforms sequence of static images into coherent video with realistic motion dynamics. B. Prompt for MLLM Planning In this section, we present the prompt templates used to collect background descriptions, foreground object layouts and trajectories, and the parameter α, which determines the amount of noise injected during inversion. As shown in Fig. 6, we explicitly instruct the multi-modal LLM to Figure 6. Prompt template used to query background description. Figure 7. Prompt template used to query foreground object layout and trajectory plan. 14 Figure 8. Prompt template used to determine how much noise to inject during inversion."
        }
    ],
    "affiliations": [
        "UNC Chapel Hill"
    ]
}