{
    "paper_title": "MirrorBench: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness",
    "authors": [
        "Ashutosh Hathidara",
        "Julien Yu",
        "Vaishali Senthil",
        "Sebastian Schreiber",
        "Anil Babu Ankisettipalli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are increasingly used as human simulators, both for evaluating conversational systems and for generating fine-tuning data. However, naive \"act-as-a-user\" prompting often yields verbose, unrealistic utterances, underscoring the need for principled evaluation of so-called user proxy agents. We present MIRRORBENCH, a reproducible, extensible benchmarking framework that evaluates user proxies solely on their ability to produce human-like user utterances across diverse conversational tasks, explicitly decoupled from downstream task success. MIRRORBENCH features a modular execution engine with typed interfaces, metadata-driven registries, multi-backend support, caching, and robust observability. The system supports pluggable user proxies, datasets, tasks, and metrics, enabling researchers to evaluate arbitrary simulators under a uniform, variance-aware harness. We include three lexical-diversity metrics (MATTR, YULE'S K, and HD-D) and three LLM-judge-based metrics (GTEval, Pairwise Indistinguishability, and Rubric-and-Reason). Across four open datasets, MIRRORBENCH yields variance-aware results and reveals systematic gaps between user proxies and real human users. The framework is open source and includes a simple command-line interface for running experiments, managing configurations and caching, and generating reports. The framework can be accessed at https://github.com/SAP/mirrorbench."
        },
        {
            "title": "Start",
            "content": "MIRRORBENCH: AN EXTENSIBLE FRAMEWORK TO EVALUATE USER-PROXY AGENTS FOR HUMAN-LIKENESS 6 2 0 2 3 1 ] . [ 1 8 1 1 8 0 . 1 0 6 2 : r Ashutosh Hathidara 1 Julien Yu 1 Vaishali Senthil 1 Sebastian Schreiber 1 Anil Babu Ankisettipalli 1 ABSTRACT Large language models (LLMs) are increasingly used as human simulators, both for evaluating conversational systems and for generating fine-tuning data. However, naive act-as-a-user\" prompting often yields verbose, unrealistic utterances, underscoring the need for principled evaluation of so-called user proxy agents. We present MIRRORBENCH, reproducible, extensible benchmarking framework that evaluates user proxies solely on their ability to produce human-like user utterances across diverse conversational tasks, explicitly decoupled from downstream task success. MIRRORBENCH features modular execution engine with typed interfaces, metadatadriven registries, multi-backend support, caching, and robust observability. The system supports pluggable user proxies, datasets, tasks, and metrics, enabling researchers to evaluate arbitrary simulators under uniform, variance-aware harness. We include three lexical-diversity metrics (MATTR, YULES K, and HD-D) and three LLM-judgebased metrics (GTEVAL, PAIRWISE INDISTINGUISHABILITY, and RUBRIC-AND-REASON). Across four open datasets, MIRRORBENCH yields variance-aware results and reveals systematic gaps between user proxies and real human users. The framework is open source and includes simple command-line interface for running experiments, managing configurations and caching, and generating reports. The framework can be accessed at https://github.com/SAP/mirrorbench 1 INTRODUCTION & RELATED WORK The idea of simulating users to train and evaluate dialogue agents has long history in NLP (Balog & Zhai, 2023). Early user simulators were typically goal-driven or rulebased, following predefined agendas of dialogue acts to mimic human behavior (Schatzmann & Young, 2009). Recent work has moved towards large language model (LLM) based user proxies that generate more realistic, open-ended interactions. For example, (Wang et al., 2025) introduced User Simulator with Implicit Profiles (USP) that infers latent user traits to produce personalized, authentic dialogues, addressing issues of role confusion and limited diversity in naive LLM role-play. Likewise, DialogueForge (Zhu et al., 2025) bootstraps AI-to-AI conversations from real chat logs as seed prompts, leveraging powerful models (e.g. GPT-4) as the user to yield multi-turn dialogues. It demonstrates that while larger LLMs produce the most human-like exchanges, fine-tuned smaller models can approximate realism with customization, though maintaining long-range coherence remains challenge. Collectively, these simulators underscore the potential of LLM-driven user agents for producing high-fidelity dialogues. 1SAP Labs. Correspondence to: Ashutosh Hathidara <ashutosh.hathidara@sap.com>. Parallel to improvements in user simulation, researchers have begun using LLMs as judges to evaluate dialogue quality. These LLM-as-a-judge\" evaluators prompt strong model to rate or compare chatbot responses, bypassing the need for reference answers or extensive human annotation (Liu et al., 2023a), (Li et al., 2023). With careful prompt design (e.g. asking the model to explain its criteria via chainof-thought reasoning), such LLM-based evaluation metrics can flexibly assess custom quality criteria (Shankar et al., 2024). Notably, LLM evaluators have enabled large-scale dialogue assessments by providing fast, consistent scoring of responses. However, existing LLM-as-judge systems mainly focus on rating the assistants outputs rather than evaluating the simulators themselves. Deploying and improving conversational AI hinges on evaluation under realistic user interactions. Expert human annotation, while authoritative, is costly and difficult to scale for systematic evaluation. At the same time, the instructiontuning of models for production systems demands large volumes of authentic userassistant exchanges. To meet both needs, practitioners increasingly rely on user proxies, namely LLMs prompted to emulate specified human personas, to synthesize user utterances for automated testing and post-training at scale. For example, recent work shows that LLM simulator can be used to evaluate an AI assistant over hundreds of interactive conversations (Lu et al., 2025), (Ahmad et al., 2025), (Hathidara et al., 2025). MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness Yet the naive prompting of single LLM to act as user simulator often yields verbose, overly cooperative behavior that diverges from real user speech patterns (Zhou et al., 2024). Recent studies further note that assistant LLMs can lose coherence in extended dialogues, in part because they fail to capture or elicit the subtleties of human behavior (Laban et al., 2025). Scalable, practical evaluation of conversational AI therefore depends on realistic user simulators, which must themselves be rigorously evaluated for how closely they resemble human users across diverse scenarios. This leads to the central question: How can we rigorously measure the human-likeness or user-likeness of user proxy, independently of downstream task success? Here, we define user as human interlocutor participating in specific conversational or task-oriented dialogue. In this work, we introduce MIRRORBENCH, reproducible and extensible framework for benchmarking user-proxy agents on human-likeness metrics using multiple human reference datasets across diverse conversational settings. MIRRORBENCH evaluates proxies solely on human-likeness and deliberately separates this from task completion. The framework comprises strongly typed domain models and metadata-rich registries, compatibility-aware planner that validates dataset and metric compatibility while producing replayable manifests for experiment reproducibility, and flexible execution engine supporting multi-turn interactions with synchronous, asynchronous, and distributed backends such as Ray (Moritz et al., 2018). It also includes caching layer for LLM outputs and dataset statistics, and commandline interface (CLI) for validating, dry-running, executing, and reporting evaluation experiments. Together, these components form modular harness in which proxies, datasets, tasks, and metrics are pluggable yet rigorously validated. MIRRORBENCH is designed for extensibility and controlled variation. Researchers can register custom LLM clients, user-proxy adapters, datasets, evaluation metrics, task drivers, and execution back-ends through unified API. Component capabilities and compatibility constraints are declared in metadata (while registering) and validated by the planner, allowing modules to be swapped or extended without modifying the core. This separation of concerns enables fair, apples-to-apples comparisons across proxies and judges under consistent orchestration and logging, and ensures seamless integration into the evaluation pipeline. To evaluate the realism of userproxy agents against realworld dialogues, MIRRORBENCH packages four opensource corpora preprocessed for user-proxy evaluation: QULAC (Aliannejadi et al., 2019), ClariQ (Aliannejadi et al., 2021), OASST1 (Köpf et al., 2023), and ChatbotArena (Zheng et al., 2023). To quantify alignment in lexical diversity with human users, we implement Moving-Average TypeToken Ratio (MATTR) (Covington & McFall, 2010), YULES (Tanaka-Ishii & Aihara, 2015), and Hypergeometric Distribution Diversity (HD-D) (Carmona et al., 2019). To assess behavioral indistinguishability, we employ family of LLM-judge metrics, including GTEval (Zhu et al., 2025), Pairwise Indistinguishability (PI) (Zheng et al., 2023), and Rubric-and-Reason (RNR) (Liu et al., 2023b). Taken together, these metrics quantify the realism of the user side of the dialogue, explicitly decoupled from assistant capabilities and task success. MIRRORBENCH is not merely metric suite; it is fullfledged evaluation harness. It validates inputs against strongly typed schemas, persists every turn, score, and judge invocation to run database, and caches LLM outputs for reproducibility and cost control. The engine logs fine-grained telemetry (latency, token counts, cost) at each turn and supports synchronous, asynchronous, and distributed evaluation. compatibility-aware planner expands grids over proxies, datasets, metrics, and random seeds, and emits replayable manifest that precisely reproduces an evaluation. The command-line interface supports quick dry-runs for validation, long-running jobs for large-scale experiments, and artifact inspection for post-hoc analysis. Structured logging via OpenTelemetry (OpenTelemetry, 2025) captures detailed traces for debugging and performance profiling. Together, these capabilities make MIRRORBENCH both rigorous benchmark of human-likeness and scalable harness for stress-testing user proxies in conversational setting. In the next sections, we formalize the evaluation problem (2), introduce the MIRRORBENCH architecture and execution flow (3), define metrics (4) and datasets (5), and detail the experimental setup and results (6)."
        },
        {
            "title": "2 PROBLEM DEFINITION",
            "content": "We formalize evaluation of userproxy agent θu on domain-agnostic dialogue dataset Dref with respect to set of human-likeness metrics . Let Dref = { dref }n j=1 denote corpus of reference, human-grounded dialogues (where is the set of finite dialogues). Each dialogue is sequence of user/assistant turns, = (cid:2)(uref dref j,1), . . . , (uref j,Lj j,1, aref , aref j,Lj )(cid:3), with variable length Lj 1. Here uref j,t is human user utterance at turn t, and aref j,t is the corresponding assistant reply (curated by human expert or generated by an LLM in the source dataset). Goal specification. In some datasets, each reference dialogue dref includes an explicit user-goal annotation gj G. When no goal is provided, we derive one using an auxiliary MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness goal generator θgoal : applied to dref : gj = ( given goal attached to dref θgoal (cid:0)dref (cid:1), , if available, otherwise. The goal specification is used to condition θu when synthesizing user utterances; if Dref already provides such annotations, the goal-inference step is omitted. Synthetic rollouts. To compare userproxy agents behavior against human references, we synthesize proxyassistant dialogues by rolling out the userproxy θu against an assistant model θa. For each reference dialogue dref with user goal gj, we condition θu on gj and the accumulated dialogue history to produce user turns ˆuj,t, while θa produces contextual assistant replies ˆaj,t. This yields synthetic transcript ˆdj = (cid:2)(ˆuj,1, ˆaj,1), . . . , (ˆuj,Lj , ˆaj,Lj )(cid:3), which we compare with the corresponding reference dialogue dref to score human-likeness of the user-proxy. Importantly, our evaluation targets the user side {ˆuj,t}Lj t=1 only: we consider the behavior, tone, and style of the proxys utterances, and do not score assistant response content. Task formalization. Let denote the fully specified, realism-evaluation task for the human-likeness of userproxy agent θu over the reference corpus Dref . The task encapsulates all configuration required for execution and metric aggregation, ensuring that the evaluation process is reproducible and self-contained. Inputs. The inputs to include reference dataset Dref , an optional goal generator θgoal (used when no explicit goal is available), the user-proxy agent under test θu, an assistant model θa used for synthetic rollouts, and set of humanlikeness metrics . Outputs. The evaluation can be expressed as composite executable function Rθu = ΨT (cid:0)θu, Dref , ; θa, θgoal (cid:1), where ΨT denotes the instantiated evaluation pipeline. This pipeline performs rollout generation by simulating synthetic dialogues ˆD = { ˆdj}n j=1 between θu and θa under the same dialogue structures and goals as Dref . It then applies the metric suite to each dialogue pair ( ˆdj, dref ), and aggregates the resulting scores across dialogues. Given userproxy agent θu, the output Rθu is structured evaluation record containing per-sample metric values, aggregated results with confidence intervals, and auxiliary metadata such as random seeds, model identifiers, and telemetry reports. We also report metric-level calibration and reliability diagnostics that quantify metric stability and judge variability. Figure 1. MIRRORBENCH Architecture: Six-layer stack from low-level execution backends & persistence up through the core engine, plugin components, CLI & reporting, and task drivers. Top layers are user-facing; bottom layers are low-level infrastructure abstractions."
        },
        {
            "title": "3 SYSTEM ARCHITECTURE",
            "content": "We present MIRRORBENCH, domain-agnostic framework for evaluating the human-likeness of user-proxy agents. Its architecture comprises six stacked layers. This section details the system design and end-to-end execution flow. 3.1 Overview MIRRORBENCH is six-layer stack that cleanly separates infrastructure from evaluation logic  (Fig. 1)  . Layers build upward, lower layers provide execution and data services; upper layers host pluggable components, task logic, and user interfaces. Full component details appear in the appendix A; here, we summarize each layer. Execution Backends & Persistence (Layer 1): This base layer runs evaluation units and manages durable state. The execution substrate supports synchronous and asynchronous runners (with an extension point for Ray (Moritz et al., 2018)) and persistence plane (SQLite + files) that stores runs, episodes, metrics, telemetry, and caches. Core Engine (Layer 2): This layer defines typed data models for all execution artifacts (e.g., messages, metric and dataset parameters), parses run configuration, and emits run manifests to ensure reproducibility. metadata-aware registry and validators enforce compatibility constraints prior to execution, preventing silent integration errors. Orchestration (Layer 3): The planner expands job configuration into compatible (proxy, dataset, metric, seed) evaluation units; the run controller schedules them on the chosen backend (sync/async/ray), handles retries/timeouts, and records lineage. cache layer deduplicates judge/- model calls, while observability provides structured logging and optional tracing/metrics. Plugin Components (Layer 4): All domain logic is modular: model clients (provider SDK wrappers with unified MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness I/O and telemetry), user-proxy adapters (LLMs or agents behind interface), datasets (normalized episode loaders with manifests), and metrics (lexical and LLM-judge families with declared dependencies). Task Drivers (Layer 5): Drivers encode the interaction protocol between user-proxy and assistant for given task. We ship single-turn and multi-turn mirror conversation drivers that emit standardized artifacts consumable by metrics. API & CLI (Layer 6): thin programmatic API facade exposes plan/run/report operations; the CLI mirrors these (plan, dryrun, run, report, cache). Outputs include JSON summaries with aggregated metric results and confidence intervals, suitable for post-hoc analysis. 3.2 Execution Flow This section details the end-to-end execution that instantiates the pipeline ΨT (as described in 2). Figure 2 shows the path from CLI invocation through planning, orchestration, and nested execution to persisted results, compiling highlevel specifications into reproducible evaluation artifacts. CLI Invocation and Planning. An evaluation job is declared via configuration file (appendix E.2) that specifies user proxies = {θu1 , . . . , θuq }, datasets = {D1, . . . , Dd}, the metric set = {m1, . . . , mk}, fixed assistant model θa, and run parameters (backend, observability, maximum concurrency, and seed set = {s1, . . . , sr}). The seed set enables repeated trials of the same combination under distinct random initializations, supporting variance estimation and confidence intervals. The Planner validates the configuration against component registries and enumerates all mutually compatible tuples to produce execution units = {U1, . . . , Up}. = (θux, Dy, mz, sw) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) (cid:12) θux P, Dy D, mz M, sw S, Compat(θux , Dy, mz) Here Compat() returns true iff the components can be jointly executed under their declared capabilities and requirements (e.g. metric prerequisites, model/back-end constraints). Each unit Ui = (θux , Dy, mz, sw) is executed and evaluated independently, with models, prompts, and driver assignments fixed at plan time. The planner creates & persists replayable manifest file (appendix E.2) capturing all requisite metadata, and exposes dryrun checkpoint for inspection prior to committing computational resources. Orchestration and Backend Selection. The Runner instantiates Run Controller that manages execution state, telemetry, and result persistence. The controller initializes run-specific SQLite database to store task driver outputs and metric values. Based on the run configuration, the system Figure 2. MIRRORBENCH execution flow. The framework decomposes an evaluation job into units {U1, . . . , Up}; each unit Ui iterates over episodes {e1, . . . , en} produced by the dataset adapter and executed via task drivers. Metrics are computed per episode and aggregated within each unit with confidence intervals. selects an available backend (sync, async, or distributed) and dispatches the list of units for execution under the chosen strategy. For each unit Ui , the backend invokes unit executor that loads the dataset, instantiates the proxy, initializes the metrics, and configures the task driver. Unit Execution and Episode Extraction. For each unit Ui = (θux , Dy, mz, sw), the unit executor loads dataset Dy and enumerates its constituent episodes. Each episode ej is an immutable specification of single reference dialogue (2), with dataset layout and fields given by Dy = { e1, e2, . . . , en }, ej = (cid:0)href (cid:1), = (cid:2)(uref href , gj, metaj j,1, aref j,1), . . . , (uref j,Lj , aref j,Lj )(cid:3), MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness where gj denotes the user goal (explicit or inferred), and metaj is an optional field containing evaluation-specific annotations (e.g., task tags or reference statistics). The unit executor iterates over the episode set {ej}, delegating each episode to the appropriate episode executor for execution. Episode executor first synthesizes proxy-assistant conversation using task driver and later compute evaluation results using metric implementation mz. Conversation Synthesis via Task Drivers. For each episode ej, the task driver orchestrates the synthesis of proxy-assistant interaction by alternating invocations of the user proxy θux (abbrev. θu) and the assistant model θa, . Let ˆhj,t matching the reference dialogue length Lj = href denote the synthetic history after turns, with ˆhj,0 = . The synthetic rollout for episode ej is: Initialize: ˆhj,0 For = 1, . . . , Lj: ˆuj,t θu (cid:1), (cid:0)ˆhj,t1, gj (cid:0)ˆhj,t1 (ˆuj,t), href (cid:1), ˆaj,t θa ˆhj,t ˆhj,t1 (ˆuj,t, ˆaj,t). The driver records per-turn telemetry (latency and token usage) and produces the episode artifact mean, standard deviation, and 95% confidence interval: vUi,mz = vj,mz , 1 j=1 sUi,mz = CI95(Ui, mz) = u 1 (vj,mz vUi,mz )2, j=1 vUi,mz 1.96 vUi,mz + 1.96 . sUi,mz sUi,mz The run database stores these aggregates, telemetry totals, and per-episode status indicators (success/failure). Iteration and Reporting. The backend executes each unit Ui = (θux, Dy, mz, sw) in = {U1, . . . , Up}. Upon completion, the controller updates the last-run pointer for convenient access and consolidates unified RunSummary that collates, for every mz and unit Ui, the tuple (cid:0)vUi,mz , sUi,mz , CI95(Ui, mz)(cid:1), along with cumulative telemetry and execution metadata. This summary serves as the canonical artifact for subsequent JSON/HTML report generation (appendix E.2) and cross-proxy comparison. The caching layer eliminates repeated LLM calls across units/runs, and the plan manifest enables exact replays of component instantiation and invocation for reproducible results. rj = (cid:0) ˆhj,Lj , ej, telemetryj (cid:1),"
        },
        {
            "title": "4 METRICS & EVALUATION",
            "content": "which binds the synthetic transcript to its reference. The synthetic history is persisted in cache, ensuring that other metric computations can reuse it. Metric Computation. After producing an episode artifact rj, metric mz evaluates rj to yield scalar value vj,mz . Depending on the metric type (e.g., lexical, or LLM-judge), the computation may involve reference comparison, pairwise scoring, or external model inference. Formally, MIRRORBENCH quantifies the human-likeness of user proxies with two metric families: (i) lexical-diversity measures that characterize vocabulary richness and repetition patterns (Section 4.1), and (ii) judge-based measures that assess behavioral realism (Section 4.2). All metrics are humananchored: proxy scores are compared against the empirical distribution of real-user utterances from the same dataset and tokenization, yielding interpretable, calibrated summaries (implementation details in Appendix D). vj,mz = mz (cid:0)rj (cid:1), 4.1 Lexical Diversity Metrics where vj,mz represents the realism score for metric mz on episode ej. Each evaluation may also produce auxiliary data such as judge verdicts, raw reasoning traces, or calibration diagnostics. The full episode artifact, together with all metric outputs, is persisted to disk for aggregation, post-hoc inspection and reproducibility. Lexical-diversity metrics capture how varied user proxys word choices are and how often they repeat tokens. Because raw values are sensitive to sequence length, domain, and tokenization, we normalize (z-score) proxy scores with respect to the human distribution on the same dataset. 4.1.1 Moving Average Type-Token Ratio (MATTR) Result Aggregation. After processing all episodes {e1, . . . , en} within unit Ui, the executor aggregates the metric outputs into summary statistics. For metric mz, let VUi,mz = {v1,mz , v2,mz , . . . , vn,mz } denote the set of episode-level scores. The unit-level aggregate includes the The classical typetoken ratio (TTR) (Chotlos, 1944; Heaps, 1978) decreases as sequence length grows. The movingaverage TTR (MATTR) mitigates this length bias by averaging TTR over sliding window of width (Covington & McFall, 2010). Let types() return the set of distinct token MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness types in its argument. For token sequence = (t1, . . . , tN ) and window size , MATTRw(t) = w+1 i=1 (cid:12)types(ti, . . . , ti+w1)(cid:12) (cid:12) (cid:12) w(N + 1) , (1) where types() returns the set of distinct token types in the window. We use = 50 by default unless stated otherwise."
        },
        {
            "title": "4.1.2 Yule’s K",
            "content": "Yules characteristic constant (Tanaka-Ishii & Aihara, 2015) summarizes repetitiveness from the token-frequency spectrum. Let Vi be the number of types occurring exactly times, and = i1 iVi the token count. Then K(t) = 104 i1 i2Vi 2 . (2) Lower indicates richer, less repetitive text; higher indicates greater repetition. The factor 104 provides convenient numeric range. 4.1.3 Hypergeometric Distribution Diversity (HD-D) HD-D (Carmona et al., 2019) estimates vocabulary diversity via hypergeometric sampling, offering robustness to length variation. It approximates the expected number of distinct types observed in sample of size drawn without replacement from token sequence = (t1, . . . , tN ). For type with frequency fi in t, the probability that the sample contains at least one instance of that type is (type in sample) = 1 (cid:1) (cid:1) , (cid:0)N fi (cid:0)N 1 N, and the HD-D score is their normalized sum: HD-Ds(t) = 1 X i=1 1 ! (cid:1) , (cid:0)N fi (cid:0)N (cid:1) (3) where = (cid:12) we use = 42 by default (Mccarthy & Jarvis, 2007). (cid:12). Following the VOCD-D convention, (cid:12)types(t)(cid:12) 4.1.4 Human-Anchored Z-Score Normalization Raw lexical-diversity scores are dataset-dependent; MIRRORBENCH therefore reports human-anchored z-scores computed on the same split and tokenizer. Let = {tref } be human user-side token sequences, where each tref concatenates the human user For any turns of one reference dialogue. {MATTRw, K, HD-Ds}, let µhum tH m(t) and 1 , . . . , tref 1 σhum = . During evaluation, each rollout episode produces synthetic proxyassistant dialogue, which we denote by ˆdi (Section 2). From H1 tH = 1 (cid:1)2 (cid:0)m(t) µhum ˆdi, we extract only the proxy user turns (i.e., the outputs of the userproxy agent θu) and concatenate them in temporal order to obtain proxy user token sequence ˆti. This ˆti is the proxy-side analogue of the human sequence tref in H. We then assign that rollout episode human-anchored standardized lexical score: zm(ˆti) = m(ˆti) µhum σhum . By design, zm 0 indicates that the proxys lexical behavior matches the human mean for metric m. The sign of zm is metric-dependent: for MATTRw and HD-Ds, zm > 0 corresponds to greater lexical diversity than the human average; for Yules K, zm > 0 corresponds to more repetition (i.e., lower diversity). For reporting at the unit level (Section 3.2), we aggregate across all rollout episodes {e1, . . . , en} executed under fixed configuration. Let {zm(ˆt1), . . . , zm(ˆtn)} be the per-episode standardized scores for metric m. We report their sample mean zm together with two-sided 95% confidence interval derived from the Student-t distribution: CI0.95(m) = zm t0.975, n1 sm , (4) where sm is the unbiased sample standard deviation, and t0.975, n1 is the 97.5th percentile of the t-distribution with 1 degrees of freedom. These aggregated statistics, namely zm, sm, and CI0.95(m), are the quantities that MIRRORBENCH reports for downstream comparison across user proxies, datasets, and seeds. 4.2 Judge-Based Realism Metrics Lexical diversity alone cannot capture whether simulated user feels human. Human-likeness also depends on discourse phenomena such as tone, politeness, hesitations, or style. MIRRORBENCH therefore includes judge-based realism metrics: LLM evaluators that score proxy behavior against human references along higher-level dimensions. All judge prompts (appendix E.1) are adapted from prior evaluation work (e.g., MT-Bench and ChatbotArena (Zheng et al., 2023), G-Eval (Liu et al., 2023a), and Prometheus (Kim et al., 2024)) without task-specific fine-tuning, in order to preserve domain generality. Each judge call uses chain-of-thought style prompting strategy (Wei et al., 2022): the judge is instructed to reason privately and then produce final structured verdict. Judge outputs are cached for reproducibility and cost control. To improve robustness, each judge can be run with self-consistency parameter 1, which repeats the judgment times under the same prompt and aggregates the resulting scores. Let ˆdj denote the proxygenerated rollout for episode ej, and let dref denote the corresponding reference (human-grounded) dialogue. We evaluate judge-based realism metrics per episode on each pair ( ˆdj, dref ), and then aggregate these scores across episodes. MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness"
        },
        {
            "title": "4.2.1 GTEval: Relative Realism Scoring",
            "content": "GTEval (Zhu et al., 2025) compares proxy-generated user transcript against human reference and returns similarity score in [0, 1], where higher values indicate that the proxy is judged to behave more like the human reference. The judge model is shown the pair ( ˆdj, dref ) and asked to assess stylistic and behavioral similarity (e.g., tone, naturalness). To improve robustness, we allow 1 repeated judgments under fixed evaluation prompt (default = 1). Let J(r)() [0, 1] denote the scalar verdict returned by the judge on repetition r. The GTEval score for episode is s(GTEval) = 1 X r=1 J(r)( ˆdj, dref ), (5) where each J(r) corresponds to an independent call to the judge with different random seed. 4.2.2 Pairwise Indistinguishability (PI) Inspired by MT-Bench and ChatbotArena (Zheng et al., 2023), PI asks judge LLM to choose which of two anonymized user conversations sounds more like human. For episode j, we present one proxy conversation ˆdj and one human reference dref in random order, unlabeled except as or B. The judge returns verdict vj,r {A, B, Tie} for judgment round r. Let pj,r {A, B} be the position where the proxy was shown for that comparison (randomized per judgment). With repeated judgments (default = 3), we define the per-episode win rate w(PI) = 1 X r=1 (1[vj,r = pj,r] + 0.5 1[vj,r = Tie]). (6) We repeat each judgment 1 times with different random seeds (default = 2). The aggregate RNR score for episode is sRNR( ˆdj) = 1 X r=1 ˆs(r) RNR( ˆdj). (7) Unlike GTEval and PI, which are explicitly comparative, RNR is reference-free and produces an absolute realism score for the proxy alone. Because this absolute score depends on the judge model and the elaborated rubric, it is interpreted relatively: higher sRNR( ˆdj) means the proxy is more consistently judged to behave like real user under the fixed judge and rubric set. Optionally, we also evaluate the human reference dref under the same procedure to obtain sRNR(dref ), which serves as an empirical upper anchor."
        },
        {
            "title": "4.2.4 Calibration via HH and PP Controls",
            "content": "Judge models can exhibit systematic bias (e.g., favoring verbosity, excessive politeness, or particular register). To expose this bias and to place GTEval and PI scores on an interpretable scale, MIRRORBENCH optionally computes two control conditions: Human-Human (HH): Compare human conversation to itself, sHH,j = Judge(dref ). This approxij mates the judges effective ceiling for genuine human behavior on episode j. , dref Proxy-Proxy (PP): Compare proxy conversation to itself, sPP,j = Judge( ˆdj, ˆdj). This estimates how the judge scores the proxy in isolation. Pn j=1 w(PI) Aggregated win rate is summarized by the mean w(PI) = 1 and its centered form w(PI) = w(PI) 0.5. Here, > 0 indicates the judge tends to prefer the proxy user over the human baseline, < 0 favors real users, and 0 indicates that the proxy is effectively indistinguishable from the human baseline. For GTEval, both sHH,j and sPP,j are typically close to 1. For PI, self comparison should yield an effective win rate near 0.5. We calculate their empirical means across all evaluated episodes: µHH = 1 j=1 sHH,j, µPP = 1 j=1 sPP,j. For PI Metric, we also calculate calibrated score scal. Let sraw denote the uncalibrated proxy score of interest (e.g. w(PI)). We define calibrated [0, 1] score via an affine rescaling between the PP and HH anchors: Pn Pn 4.2.3 Rubric-and-Reason (RNR) RNR adapts rubric-style LLM evaluation (e.g., G-Eval, Prometheus) to judge human-likeness without requiring paired human reference. The judge sees only the proxyside user transcript ˆdj and rubric that defines realism along dimensions such as conciseness, behavior, and tone. For given rubric, the judge returns binary verdict v(r) {YES, NO} on repetition r, which we map to ˆs(r) RNR( ˆdj) = ( 1.0, 0.0, if v(r) = YES, if v(r) = NO. scal = clip sraw µPP max(cid:0)ϵ, µHH µPP ! (cid:1) , 0, 1 , (8) where ϵ = 106 prevents division by zero and clip(, 0, 1) bounds the result. Under this normalization, scal 0 means the proxy is no better than its own PP baseline, while scal 1 means the proxy is judged as indistinguishable from human under the same judge. MIRRORBENCH logs both raw and calibrated scores, along with HH/PP control distributions, for transparency. MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness Figure 3. Human-likeness of five user-proxy LLMs across four datasets. Higher is better for judge-based metrics (GTEval, PI, RNR). Lexical-diversity metrics are z-scored to human baselines (0 is best). We fix the judge to Claude-4-Sonnet and the assistant to GPT-4o."
        },
        {
            "title": "5 DATASETS & TASKS",
            "content": "We evaluate user proxies across four open-source conversational datasets spanning diverse domains and interaction patterns, collectively providing 795 conversations with real human user turns, enabling direct comparison between proxygenerated and real user behavior. More details about the datasets are appended to Appendix and Table 2. All datasets are preprocessed into unified JSONL format. For each conversation, we generate user goal description using an auxiliary LLM (θg defined in Section 2) that summarizes the users intent, behavior, tone, and persona based on the real conversation. This description serves as the initialization prompt for user proxies during evaluation. For the above datasets, we use the Mirror Conversation Driver (implemented task driver for these datasets) to orchestrate multi-turn dialogues where the user proxy receives the goal description and generates responses turn-by-turn, with the assistant LLM (θa) producing intermediate replies to maintain conversational flow. The assistant is provided the reference conversation to follow similar trajectory, ensuring evaluation focuses solely on user-proxy realism rather than assistant behavior. This enables conversation-level evaluation where proxy-generated dialogues are compared against real user conversations using both lexical diversity and judge-based realism metrics."
        },
        {
            "title": "6 EXPERIMENTS & RESULTS",
            "content": "We evaluate multiple user-proxy LLMs across four diverse conversational datasets, ChatbotArena, ClariQ, OASST1, and QULAC, to quantify human-likeness using both lexicaldiversity and LLM-judge realism metrics. Concretely, we report MATTR, YULES K, and HD-D (human-anchored z-scores), together with three judge-metrics GTEVAL, PI, and RNR. Our experiments (i) measure how closely different proxy architectures reproduce human user behavior, (ii) assess the reliability and calibration of LLM-as-judge metrics, and (iii) characterize computational cost, latency, and throughput under large-scale runs. We compare five LLMs as user proxies: GPT-4o (Hurst et al., 2024), GPT-5 (OpenAI, 2025), GPT-OSS-120B (OpenAI, 2025), Claude-4-Sonnet (Anthropic, 2025), and Gemini-2.5Pro (Google, 2025). Unless otherwise noted, the assistant is fixed to GPT-4o and the judge to Claude-4-Sonnet for all primary results, enabling apples-to-apples comparisons across proxies. All the evaluations are performed with single seed, and we report aggregated scores with 95% confidence intervals. Judge scores are calibrated, and we include human correlation check to validate judge trends. We also analyze fine-grained telemetry to make cost/performance trade-offs explicit. 6.1 Comparing User-Proxy LLMs on Human-Likeness Figure 3 compares five user-proxy LLMs across four datasets with fixed judge (Claude-4-Sonnet) and assistant (GPT-4o). The left three columns of subplots report judge-based realism (GTEVAL, PI w, RNR; higher is better); the right three columns report human-anchored lexical diversity (MATTR, HD-D, YULES K; best at the human z-score = 0). Dashed red (HH) and green (PP) lines provide calibration controls; error bars denote 95% CIs. Judge realism is consistent across datasets. Across MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness Figure 4. Judge sensitivity of judge-realism metrics on ChatbotArena with assistant & user-proxy both set to GPT-4o; bars vary the judge model. Error bars are 95% CIs. Figure 5. Judgehuman correlation on ChatbotArena: Correlation of Claude-4-Sonnet judge scores & human scores for GTEval and PI, evaluated on Gemini-2.5-Pro user-proxy outputs (N=100 per metric). Solid line: linear fit; dashed: identity. Point size encodes local sample density. All correlations < 0.001. GTEval, PI w, and RNR, Gemini-2.5-Pro and Claude4-Sonnet are the most human-like on every dataset, with GPT-4o competitive but generally behind, and GPT-OSS120B and GPT-5 trailing. On ClariQ and QULAC, Claude4-Sonnet/Gemini-2.5-Pro approach the HH ceiling under RNR, while PI shows clear positive win margins for these models and negative/near-zero deltas for the rest. The agreement among the three judge metrics indicates stable ordering not driven by any single rubric. Diversity shows strong dataset effects and realismdiversity tension. Lexical diversity diverges from the realism ranking. On ClariQ, most notably Claude-4-Sonnet and GPT-5 exceed the human anchor on MATTR/HD-D and exhibit lower YULES K, indicating more diverse vocabulary than human questioners in this informationseeking regime. In contrast, QULAC exhibits uniform diversity deficit: all proxies fall below the human baseline on MATTR and HD-D (with positive shifts in YULES K), suggesting more templated clarifications than humans. ChatbotArena and OASST1 sit between these extremes with smaller deviations around the human baseline. Overall, Gemini-2.5-Pro yields the strongest diversity alignment with humans across datasets; Claude-4-Sonnet generally tends to overshoot the human baseline, whereas GPT-4o is better calibrated to stay closer to the human anchor with fewer extremes. GPT-5 and GPT-OSS-120B have generally larger diversity difference compared to human baselines. Judge realism and diversity are partially decoupled. High judge realism does not guarantee human-level diversity. Claude-4-Sonnet and Gemini-2.5-Pro lead on GTEVAL/PI/RNR but under-shoot diversity on QULAC, indicating judges favor intent/style over surface variety. Conversely, GPT-4o shows more stable diversity with moderate judge-realism gains, underscoring that diversity alone is insufficient to achieve judge indistinguishability. Uncertainty and robustness. Confidence intervals are generally narrow. When overlaps occur (e.g., GTEVAL on ChatbotArena), rank differences are small and concur with PI/RNR trends. For lexical-diversity panels, confidence intervals occasionally widen, notably for YULES K, as length sensitivity inflates variance. Overall, Figure 3 supports three takeaways: (i) Gemini-2.5-Pro and Claude-4-Sonnet are reliably the most human-like by judge criteria; (ii) diversity depends strongly on dataset regime and proxies often lag on clarification-centric tasks like QULAC; and (iii) realism and diversity capture complementary facets of user-proxy quality, motivating the use of both families of metrics. 6.2 Judge Sensitivity Analysis With the assistant and user-proxy fixed to GPT-4o, Figure 4 shows substantial judge sensitivity on realism scores. On GTEval, scores spread widely ( 0.450.81), with GPT-4o as judge yielding the highest value. PI is the most volatile: Gemini-2.5-Pro & Claude-4-Sonnet produce near-zero or negative win deltas, while GPT-5 & GPT-4o are clearly positive, suggesting family/self-preference or rubric alignment effects (auxiliary evaluations in Appendix C). RNR saturates near the ceiling for GPT-4o & GPT-5 judges ( 0.960.98) and is lower for others ( 0.790.82), indicating judgedependent sensitivity: PI > GTEval > RNR. These differences imply that conclusions drawn from single judge can shift both the absolute level and the ordering of models. In practice, we recommend evaluating with multiple judges and applying HH/PP calibration when feasible, or normalizing per-judge before aggregation, to avoid over-interpreting judge-specific biases. Empirically, Claude4-Sonnet behaves as conservative yet stable judge, yielding non-saturated, well-separated scores across metrics, which is why we use Claude-4-Sonnet as judge in the results reported in Figure 3. 6.3 Human-Judge Correlation We validate judge reliability by correlating Claude-4-Sonnet judge scores with blinded human expert annotations on ChatbotArena. We stratify 100 episodes per metric (GTEval, PI) by judge score and conversation length, then compute Spearmans ρ (Kokoska & Zwillinger, 2000), Pearsons (Kowalski, 2018), and Kendalls τ (Kendall, 1938). As MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness shown in Figure 5, GTEval shows strong alignment with humans, while PI exhibits moderate correlation, consistent with the added difficulty of pairwise judgments. Overall, these results indicate that judge scores correlate with human perceptions of user-proxy quality. 6.4 Telemetry, Scaling, and Cost Figures 6, 7, and 8 summarize the practical side of evaluation. Per-episode telemetry  (Fig. 6)  shows that token usage is dominated by the judge, with the user-proxy & assistant contributing smaller but non-negligible share; datasets differ markedly, OASST1 drives the highest token counts while ClariQ yields the largest end-to-end latency. Figure 6. Avg per-episode telemetry for GPT-4o as user-proxy & assistant, and judged by Claude-4-Sonnet across 4 datasets on 6 metrics  (Fig. 3)  ; only non-cached episodes considered. Left: Token usage by role (darker: input, lighter: output). Right: Cumulative latency per episode. Figure 7. Throughput vs. concurrency for GTEVAL on ChatbotArena (async backend, cache off); only the judge varies. Userproxy and assistant fixed to GPT-4o. Figure 8. Cost-quality trade-off for PI: cost per evaluation (USD) vs. PI ( better). Judge = Claude-4-Sonnet; assistant = GPT4o; temperature = 0; cache off. Markers denote user-proxies; labels denote datasets. Dashed line: Pareto frontier. See Table 4 for model pricing. Scaling on an async backend (cache off) reveals clear judgedependent throughput  (Fig. 7)  : GPT-4o as judge attains the highest episodes/min and continues to benefit up to high concurrency, Claude-4-Sonnet scales steadily to mid/high throughput, and Gemini-2.5-Pro plateaus earlier. The costquality frontier  (Fig. 8)  clarifies trade-offs for PI evaluation: User-proxies Gemini-2.5-Pro and Claude-4-Sonnet offer attractive Pareto points (good PI at moderate cost), and GPT-5 generally incurs higher cost with weaker PI gain. Overall, user-proxy Gemini-2.5-Pro, along with Claude-4Sonnet as judge, provides balanced choice for large runs, where one maximizes throughput and another prioritizes peak PI quality."
        },
        {
            "title": "7 CONCLUSION & FUTURE WORK",
            "content": "MIRRORBENCH introduces principled, systems-oriented framework for evaluating the human-likeness of user-proxy agents, explicitly decoupled from downstream task success. The framework provides modular six-layer stack with typed interfaces, metadata-aware registries, compatibilitychecking planner, multi-backend execution, caching, and observability; it supports pluggable proxies, datasets, tasks, and metrics, producing variance-aware, reproducible results at scale. Empirically, we report (GTEVAL, PI, RNR) alongside human-anchored lexical diversity (MATTR, HD-D, YULES K), revealing realism-diversity tension across domains, and show that absolute scores and model orderings can shift with the choice of judge, underscoring the need for calibration (HH/PP) and multi-judge reporting. Finally, telemetry (tokens, latency) and cost analyses make the practical trade-offs of large-scale evaluation explicit. Limitations. Our results rely on LLM-as-judge metrics that can exhibit model-family bias; HH/PP controls help, but residual bias and prompt sensitivity remain. Our experiments use single seed; assistant models are largely fixed; and coverage is limited to four English-centric datasets. Finally, diversity metrics capture surface variation and do not fully reflect discourse phenomena (e.g., repair, initiative). Future Work. (i) Judges: multi-judge ensembles, stronger calibration/normalization, and significance across runs; (ii) Metrics: discourse and interaction-level realism (turntaking, self-correction, persistence), robustness to prompt/goal perturbations; (iii) Datasets: broader domains (customer support, safety-critical, adversarial contexts) and multilingual corpora; (iv) Systems: distributed backends and first-class report generation; (v) Reproducibility & openness: live-updating benchmark for the research community. We aim for MIRRORBENCH to serve as practical evaluation harness and standardized benchmark for measuring user-proxy realism. MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness"
        },
        {
            "title": "REFERENCES",
            "content": "Ahmad, A., Hillmann, S., and Möller, S. Simulating user diversity in task-oriented dialogue systems using large language models, 2025. URL https://arxiv.org/ abs/2502.12813. Aliannejadi, M., Zamani, H., Crestani, F., and Croft, W. B. Asking clarifying questions in open-domain information-seeking conversations. In Proceedings of the 42nd International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR19, pp. 475484, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450361729. doi: 10.1145/3331184.3331265. URL https://doi. org/10.1145/3331184.3331265. Aliannejadi, M., Kiseleva, J., Chuklin, A., Dalton, J., and Burtsev, M. Building and evaluating open-domain diIn EMNLP, alogue corpora with clarifying questions. 2021. Anthropic. System Card: Claude Opus 4 & Claude Sonnet 4. Technical report, Anthropic, May 2025. https://www-cdn.anthropic.com/ URL 6d8a8055020700718b0c49369f60816ba2a7c285. pdf. Accessed: 2025-10-22. Balog, K. and Zhai, C. User simulation for evaluating In Proceedings of the information access systems. Annual International ACM SIGIR Conference on Research and Development in Information Retrieval in the Asia Pacific Region, SIGIR-AP 23, pp. 302305, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400704086. doi: 10.1145/ 3624918.3629549. URL https://doi.org/10. 1145/3624918.3629549. Carmona, C. P., Szava-Kovats, R., and Pärtel, M. Estimating probabilistic dark diversity based on the hydoi: 10. pergeometric distribution. 1101/636753. URL https://www.biorxiv.org/ content/early/2019/05/15/636753. bioRxiv, 2019. Chotlos, J. W. Iv. statistical and comparative analysis of individual written language samples. Psychological Monographs, 56(2):75, 1944. Covington, M. A. and McFall, J. D. Cutting the gordian knot: The moving-average typetoken ratio (mattr). Journal of Quantitative Linguistics, 17:100 94, 2010. URL https://api.semanticscholar. org/CorpusID:18924254. deepmind-media/Model-Cards/ Gemini-2-5-Pro-Model-Card.pdf. Accessed: 2025-10-22. Hathidara, A., Yu, J., and Schreiber, S. Disambiguationcentric finetuning makes enterprise tool-calling llms more realistic and less risky, 2025. URL https://arxiv. org/abs/2507.03336. Heaps, H. S. Information Retrieval: Computational and Theoretical Aspects. Academic Press, Inc., USA, 1978. ISBN 0123357500. Hurst, A., Lerer, A., Goucher, A. P., Perelman, A., Ramesh, A., Clark, A., Ostrow, A., Welihinda, A., Hayes, A., Radford, A., et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. Kendall, M. G. new measure of rank correlation. Biometrika, 30:8193, 1938. URL https: //api.semanticscholar.org/CorpusID: 120478295. Kim, S., Shin, J., Cho, Y., Jang, J., Longpre, S., Lee, H., Yun, S., Shin, S., Kim, S., Thorne, J., and Seo, M. Prometheus: Inducing fine-grained evaluation capability in language models. In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=8euJaTveKw. Kokoska, S. and Zwillinger, D. CRC Standard Probability and Statistics Tables and Formulae, Student Edition. 03 2000. ISBN 9781482273847. doi: 10.1201/b16923. Section 14.7. Köpf, A., Kilcher, Y., von Rütte, D., Anagnostidis, S., Tam, Z. R., Stevens, K., Barhoum, A., Nguyen, D. M., Stanley, O., Nagyfi, R., ES, S., Suri, S., Glushkov, D. A., Dantuluri, A. V., Maguire, A., Schuhmann, C., Nguyen, H., and Mattick, A. J. Openassistant conversations - democratizing large language model alignment. In Thirty-seventh Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https: //openreview.net/forum?id=VSJotgbPHF. Kowalski, C. J. On the effects of non-normality on the distribution of the sample product-moment correlation coefficient. Journal of the Royal Statistical Society Series C: Applied Statistics, 21(1):112, 12 2018. ISSN 00359254. doi: 10.2307/2346598. URL https://doi. org/10.2307/2346598. Google. System Card: report, Gemini 2025. 2.5 Technical https://storage.googleapis.com/ Google, Pro. URL Laban, P., Hayashi, H., Zhou, Y., and Neville, J. Llms get lost in multi-turn conversation, 2025. URL https: //arxiv.org/abs/2505.06120. MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness Li, J., Cheng, X., Zhao, X., Nie, J.-Y., and Wen, J.-R. HaluEval: large-scale hallucination evaluation benchmark for large language models. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 64496464, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023. emnlp-main.397. URL https://aclanthology. org/2023.emnlp-main.397/. Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., and Zhu, C. G-eval: NLG evaluation using gpt-4 with better human alignment. In Bouamor, H., Pino, J., and Bali, K. (eds.), Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 25112522, Singapore, December 2023a. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main. 153. URL https://aclanthology.org/2023. emnlp-main.153/. Liu, Y., Iter, D., Xu, Y., Wang, S., Xu, R., and Zhu, C. G-eval: Nlg evaluation using gpt-4 with better human alignment. arXiv preprint arXiv:2303.16634, 2023b. Lu, J., Holleis, T., Zhang, Y., Aumayer, B., Nan, F., Bai, H., Ma, S., Ma, S., Li, M., Yin, G., Wang, Z., and Pang, R. ToolSandbox: stateful, conversational, interactive evaluation benchmark for LLM tool use capabilities. In Chiruzzo, L., Ritter, A., and Wang, L. (eds.), Findings of the Association for Computational Linguistics: NAACL 2025, pp. 11601183, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-195-7. doi: 10.18653/v1/2025. findings-naacl.65. URL https://aclanthology. org/2025.findings-naacl.65/. Mccarthy, P. and Jarvis, S. Vocd: theoretical and empirical evaluation. language testing, 24, 459-488. Language Testing - LANG TEST, 24:459488, 10 2007. doi: 10. 1177/0265532207080767. Moritz, P., Nishihara, R., Wang, S., Tumanov, A., Liaw, R., Liang, E., Elibol, M., Yang, Z., Paul, W., Jordan, M. I., and Stoica, I. Ray: distributed framework for emerging ai applications. In Proceedings of the 13th USENIX Conference on Operating Systems Design and Implementation, OSDI18, pp. 561577, USA, 2018. USENIX Association. ISBN 9781931971478. OpenAI. gpt-oss-120b & gpt-oss-20b model card, 2025. URL https://arxiv.org/abs/2508.10925. OpenTelemetry. Opentelemetry. //opentelemetry.io/, 2025. APIs, SDKs, and tools for traces, metrics, and logs. https: Collection of Schatzmann, J. and Young, S. The hidden agenda user simulation model. Trans. Audio, Speech and Lang. Proc., 17(4):733747, May 2009. doi: 10.1109/TASL.2008.2012071. URL https://doi. org/10.1109/TASL.2008.2012071. ISSN 1558-7916. Shankar, S., Zamfirescu-Pereira, J., Hartmann, B., Parameswaran, A., and Arawjo, I. Who validates the validators? aligning llm-assisted evaluation of llm outputs with human preferences. In Proceedings of the 37th Annual ACM Symposium on User Interface Software and Technology, UIST 24, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400706288. doi: 10.1145/3654777.3676450. URL https://doi. org/10.1145/3654777.3676450. Tanaka-Ishii, K. and Aihara, S. Computational constancy measures of texts-yules and rényis entropy. Comput. Linguist., 41(3):481502, September 2015. ISSN 08912017. doi: 10.1162/COLI_a_00228. URL https:// doi.org/10.1162/COLI_a_00228. Wang, K., Li, X., Yang, S., Zhou, L., Jiang, F., and Li, H. Know you first and be you better: Modeling human-like user simulators via implicit profiles. In Che, W., Nabende, J., Shutova, E., and Pilehvar, M. T. (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2108221107, Vienna, Austria, July 2025. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.1025. URL https:// aclanthology.org/2025.acl-long.1025/. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Ichter, B., Xia, F., Chi, E. H., Le, Q. V., and Zhou, D. Chain-ofthought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2022. Curran Associates Inc. ISBN 9781713871088. Zheng, L., Chiang, W.-L., Sheng, Y., Zhuang, S., Wu, Z., Zhuang, Y., Lin, Z., Li, Z., Li, D., Xing, E., Zhang, H., Gonzalez, J. E., and Stoica, I. Judging LLM-as-a-judge In Thirty-seventh with MT-bench and chatbot arena. Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2023. URL https: //openreview.net/forum?id=uccHPGDlao. OpenAI. GPT-5 System Card. Technical report, OpenAI, August 2025. URL https://cdn.openai.com/ gpt-5-system-card.pdf. Accessed: 2025-10-22. Zhou, X., Su, Z., Eisape, T., Kim, H., and Sap, M. Is this the real life? is this just fantasy? the misleading success of simulating social interactions with LLMs. In MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness Al-Onaizan, Y., Bansal, M., and Chen, Y.-N. (eds.), Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pp. 2169221714, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main. 1208. URL https://aclanthology.org/2024. emnlp-main.1208/. Zhu, R., Zhu, H., Li, Y., Zhou, S., Cai, S., Lazuka, M., and Ash, E. Dialogueforge: Llm simulation of humanchatbot dialogue, 2025. URL https://arxiv.org/ abs/2507.15752. LAYER-BY-LAYER ARCHITECTURE"
        },
        {
            "title": "DETAILS",
            "content": "As summarized in Section 3.1, the framework is designed in 6-layers starting from low-level execution backends & persistence to the high-level abstract API interface and CLI. Each of the layers in the architecture shown in Figure 1 are built on top of layers beneath it. Few modules in the midupper layers in the architecture also utilizes their sibling modules on the same layer. Here, we briefly describe each layer and the modules built as part of them. A.1 Execution Backends & Persistence This layer is responsible for all the execution and data management. The layer consists of two high-level modules explained below. Execution Backend: It is responsible for taking decomposed units of tasks as input and send them for execution on the respective execution engine. The framework provides abstractions to support multiple execution backends to execute evaluation jobs at scale. The abstract interface provides uniform schema for implementations to extend the support for new execution engines. The framework supports fully-functional synchronous & asynchronous execution backends, and provides stub implementation to extend for Ray backend. Persistence: All evaluation artifacts are persisted in SQLite database organized around three-level hierarchy: runs define experiment configurations, units specify individual evaluation tasks (parameterized by user-proxy, dataset, metric, and seed), and episodes capture per-trial results including execution duration, metric values, and telemetry. Aggregate statistics (mean, standard deviation, confidence intervals) are materialized in separate metrics table to support efficient querying, while composite scores across multiple metrics are stored in scorecards for holistic comparisons. The complete schema is detailed in Table 1. A.2 Core Engine All the core functionalities like data models, registry builder, configuration and manifest management are contained in Layer 2 of the architecture. Since Layer 1 provides uniform abstractions to produce and consume objects, this layer considers backend-agnostic implementation. Data Models: All the data structures for messages, episodes, evaluation units and run manifests are defined with typed data models using TypedDict and pydantic. We formally explain the context of these different data objects later. Registry Builder: The framework provides certain highlevel components which requires module-specific registries. This layer provides the abstract registry builder that supports creating metadata-aware registry for the downstream modules. Such registries can avoid silent errors since we can ensure certain constraints on the classes being registered with the metadata. Configuration & Manifest Management: typical evaluation job requires configuring user-proxy, datasets, metrics, scorecards etc. We define standard configuration schema for each of such components. Moreover, upon invocation of job or during dryrun, the system decomposes evaluation task into small evaluation units. In order to support parallelization (for async and remote backends), we persist self-contained information about the executable units as manifest files. A.3 Orchestration This layer coordinates the evaluation workflow by managing component lifecycles, execution planning, and runtime operations. The layer consists of five orchestration modules. Pipeline Planner: It validates job configurations and generates execution plans by decomposing evaluation jobs into granular evaluation units (user-proxy, dataset, metric, seed combinations). The planner performs compatibility filtering based on component metadata to ensure valid combinations and produces reproducible plan manifests. Component Registries: It maintains metadata-aware registries for all pluggable components including user-proxy adapters, datasets, metrics, model clients, and judges. Registries support dynamic component discovery and instantiation through decorator-based registration patterns. Run Controller: It orchestrates end-to-end evaluation runs by managing execution flow, coordinating backend selection (synchronous, asynchronous, or Ray), tracking progress, and handling run lifecycle including resumption of interrupted runs. Cache Layer: It provides SQLite-based caching for model MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness Table 1. Complete MIRRORBENCH Run Database Schema Table runs units episodes metrics Field run_id created_at status engine planner_version summary_json notes run_id unit_id user_proxy dataset metric seed judge status run_id unit_id episode_id status duration_s artifact_path summary metric_values telemetry_json Type TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT TEXT Description Unique identifier for the experiment run (primary key) ISO-8601 timestamp of run creation Run status: created, running, completed, failed Execution engine identifier (e.g., sync, async) Version of the planner used for the run JSON-encoded run-level summary statistics User-provided annotations or metadata Foreign key to parent run Unique identifier for the evaluation unit (composite primary key) User-proxy agent implementation identifier Dataset name (e.g., oasst1, clariq) Evaluation metric (e.g., gteval) TEXT TEXT TEXT INTEGER Random seed for reproducibility TEXT TEXT Optional judge model for LLM-based metrics Unit execution status: completed pending, running, TEXT TEXT TEXT TEXT REAL TEXT TEXT TEXT TEXT outcome: success, Foreign key to parent run Foreign key to parent unit Unique episode identifier (primary key) Episode timeout Wall-clock execution time in seconds File path to serialized conversation artifact Human-readable episode summary JSON-encoded per-metric scores for this episode JSON-encoded execution telemetry (token counts, API calls) failure, TEXT run_id TEXT unit_id TEXT metric REAL mean standard_deviation REAL confidence_interval REAL REAL p_value INTEGER Number of valid episodes sample_size TEXT extras Foreign key to parent run Foreign key to parent unit Registered metric name (primary key) Sample mean across episodes Sample standard deviation 95% confidence interval half-width Statistical significance (e.g., vs. baseline) JSON-encoded additional statistics telemetry scorecards run_id unit_id key value run_id name score weights missing_metrics extras TEXT TEXT TEXT TEXT TEXT TEXT REAL TEXT TEXT TEXT Foreign key to parent run Foreign key to parent unit Telemetry api_errors) Aggregated telemetry value (JSON or scalar) (e.g., key total_tokens, Foreign key to parent run Scorecard identifier (e.g., realism, diversity) Weighted composite score JSON-encoded metric weights used for aggregation JSON list of metrics excluded due to missing data JSON-encoded additional scorecard metadata MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness responses to reduce redundant API calls and improve evaluation efficiency. The cache uses content-based keys (hashing model name, messages, and configuration) and supports namespace isolation and TTL-based expiration. turn conversational interactions by executing sequences of user-proxy responses within episodic contexts. This driver supports conversation-level evaluations where humanlikeness is assessed across extended interactions. Observability Layer: It provides structured logging via structlog and optional OpenTelemetry integration for distributed tracing and metrics collection. The layer supports multiple output formats (JSON, text) and configurable log levels for debugging and monitoring. A.4 Plugin Components This layer contains the core pluggable components that implement domain-specific evaluation logic. All components follow standardized interfaces and register through the registry system. Model Clients: It wraps provider SDKs (OpenAI, LangChain, Azure, etc.) with unified interface for LLM invocation. Clients automatically capture telemetry (tokens, latency, cost) and support transparent caching through wrapper composition. A.6 API & Interface This layer provides user-facing interfaces for interacting with the framework through programmatic APIs and command-line tools. Runner Facade: It provides simplified programmatic API for executing evaluation runs from Python code. The facade abstracts orchestration complexity and exposes high-level methods for planning, execution, and result retrieval with progress tracking. Command-Line Interface (CLI): It exposes framework functionality through comprehensive CLI supporting workflow commands (plan, dryrun, run), reporting (report json), run management (runs inspect, runs delete), and cache operations (cache stats, cache purge). User-Proxy Adapters: It adapts diverse agent frameworks and LLM configurations into standardized AgentSession interface. Adapters normalize differences in message formats, tool usage patterns, and execution models across frameworks. Report Generation: It produces structured evaluation summaries with aggregated statistics including means, confidence intervals, and significance testing. The module supports JSON report output with planned support for HTML visualization. Datasets: It loads and normalizes evaluation datasets from various sources (HuggingFace, JSONL files, etc.) into standardized episode objects. Loaders support dataset splitting, sampling limits, and provide metadata about available splits and reference statistics. Metrics: It implements human-likeness measurements including lexical diversity metrics (MATTR, HD-D, YULES K) and LLM-as-judge metrics (GTEVAL, PI, RNR). Metrics declare compatibility requirements through metadata (task types, judge dependencies, reference statistics needs). A.5 Task Drivers This layer implements interaction protocols that orchestrate how the artificial conversation between user-proxy & assistant agent is synthesized and how it is engaged with the evaluation metrics. Task drivers bridge the gap between high-level evaluation specifications and low-level execution. Default Single-Turn Task Driver: It executes single-turn interactions where the user-proxy receives episode context and generates single response. This driver is suitable for simple prompt-completion tasks and turn-level utterance evaluation. Mirror Conversation Task Driver: It orchestrates multi-"
        },
        {
            "title": "B DETAILS ON BENCHMARKING DATASETS",
            "content": "As stated in the Section 5, we use four open datasets. Table 2 provides detailed information about each of the datasets used for this paper. B.1 Sampling Datasets for Benchmarking In Table 2, notice that the number of samples we use for evaluating user-proxy agents (approx. 200) does not exhaust all the samples of the underlying open-source datasets. Instead, we perform stratified sampling over the original datasets to extract the final benchmarking datasets. For each dataset, we first normalize conversations into alternating user-assistant turn sequences, retaining only English dialogues with at least two turns. We then define dataset-specific stratification keys to ensure broad coverage: OASST1 conversations are bucketed by the number of user turns (short, medium, long), ChatbotArena samples are stratified by language, turn count, and whether the dialogue contains multiple user interactions, QULAC entries are grouped by topic type and facet category, and ClariQ dialogues are distributed across topic buckets and clarification-pair counts. Within each stratum, we allocate samples proportionally to the population size while enforcing minimum-per-stratum threshold to prevent underrepresentation. Using fixed MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness Table 2. Datasets used for the evaluation. All datasets are open-source and include real human user utterances. Dataset Domain Description Samples Avg Turns ChatbotArena (Zheng et al., 2023) ClariQ (Aliannejadi et al., 2021) OASST1 (Köpf et al., 2023) QULAC (Aliannejadi et al., 2019) General-purpose, open-domain conversations where users engage with anonymous LLMs on diverse tasks including creative writing, technical questions, reasoning, and casual dialogue. Extracted from winning model interactions. Information-seeking dialogues where users pose ambiguous queries and assistants request clarifications to refine search intent. Task-oriented conversations grounded in specific search facets with concise user responses. Multilingual instruction-following dialogues covering diverse tasks (coding, Q&A, creative writing, advice). Linear conversation paths extracted from tree-structured human-generated responses. English-only subset used. Query clarification dialogues in web search contexts. Short user-assistant exchanges covering faceted queries (multiple interpretations) and navigational queries (specific resource seeking). Concise user utterances. 195 200 200 200 2.5 7. 3.3 2.0 random seed for reproducibility, we sample up to 200 samples per dataset, yielding balanced benchmark that spans diverse conversational structures, topical domains, and interaction patterns without oversampling any single mode. AUXILIARY USER-PROXY EVALUATIONS Similar to Figure 3, where the judge is fixed to Claude-4Sonnet, we perform auxiliary user-proxy evaluations  (Fig. 9)  with the judge fixed to GPT-4o (assistant still fixed to GPT4o). Findings. Using GPT-4o as the judge largely preserves the qualitative ordering seen with Claude-4-Sonnet as judge: Gemini-2.5-Pro and Claude-4-Sonnet remain strongest on GTEVAL/PI/RNR, while GPT-OSS-120B and GPT-5 trail. Relative to Claude-4-Sonnet, GPT-4o as judge yields systematically higher absolute scores (especially on RNR), compressing gaps between proxies and making them less distinguishable."
        },
        {
            "title": "METRICS",
            "content": "Below, we provide the implementation details for the metrics and other associated components in the evaluation pipeline. Additionally, in Table 3, we provide details of the machine we used to run all the experiments mentioned in this paper. Tokenization. All lexical metrics use the GPT-4o tokenizer (via tiktoken) to ensure consistency with modern LLM vocabularies. User utterances in the dialogue sample are concatenated with space separators before tokenization. Multiple Judge Calls. By default, each judge metric runs independent samples per episode (c is metric parameter to be set) with different random seeds (for PIs order randomization). Final scores are averaged; individual samples and reasoning traces are stored in metadata for audit. Statistical Reporting. Aggregated metrics report mean, standard deviation, and 95% confidence intervals using Students t-distribution (Equation 4). The framework optionally supports bootstrap confidence intervals (1000 iterations) for non-parametric estimation. Aggregation Across Episodes. For each metric and proxy-dataset pair, the framework computes per-episode scores {s1, s2, . . . , sn} and aggregates them using mean, standard deviation, and confidence intervals. For lexical metrics, z-scores are computed per-episode using the human baseline statistics, then aggregated. For judge metrics, raw scores are averaged first; if calibration is enabled, HH and PP statistics are computed separately and applied to normalize the aggregated results. Handling Missing References. Episodes missing human reference conversations (required for GTEval, PI, and zscore computation) are excluded from metric computation. The framework logs these exclusions and reports the count of valid episodes in the metadata. This ensures that all reported statistics are based on complete, valid comparisons. Stability Filters. Lexical metrics require minimum token count (default: 5) to produce stable estimates. Episodes below this threshold are flagged as unstable and optionally excluded from aggregation. This prevents outlier scores from very short utterances (e.g., single-word responses) from skewing results. Judge Caching. Judge responses are cached using content-based keys (hash of model name, messages, temperature, and metric parameters) with namespace isolation per metric. Default TTL is 30 days. This enables reproducible experiments and cost reduction in iterative evaluation. LLM Settings. All the LLMs used during the evaluation have fixed temperature=0 and max_tokens=2048. Moreover, the LLM clients have by default retry with exponential backoff\" scheme enabled where all MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness Figure 9. Human-likeness of user-proxy LLMs across four datasets: higher is better for judge metrics; diversity scores are z-scored to humans (0 is best). Judge & assistant fixed to GPT-4o. Table 3. Experimental Configuration and Dependency Versions Category Component Hardware & Infrastructure OS CPU RAM GPU CUDA Linux Standard Standard GPT-OSS-120B only GPT-OSS-120B only Runtime & Storage Python SQLite Interpreter Database API Providers (accessed OctDec 2024) OpenAI Anthropic Vertex AI Self-hosted GPT-4o GPT-5 Claude-4-Sonnet Gemini-2.5-Pro GPT-OSS-120B Core Library Versions Version/Configuration SUSE Linux Enterprise Server 15 SP4 (kernel 5.14+) Intel Xeon Platinum 8468 (16+ cores) 64 GB DDR4 4 NVIDIA H200 (140GB VRAM each) or equivalent CUDA 12.4+ 3.12.0+ 3.45.0+ (WAL mode enabled) gpt-4o-2024-08-06 gpt-5-2025-08-07 claude-sonnet-4-20250514 2025-06-17 vLLM 0.10.1+gptoss, BF16 precision Model Clients Data Utilities Logging Telemetry openai langchain-openai pydantic datasets tiktoken nltk tenacity structlog opentelemetry-sdk opentelemetry-exporter-otlp 1.40.0+ 0.1.0+ 2.5.0+ 4.1.1+ (HuggingFace) 0.7.0+ (tokenization) 3.9.2+ (sentence segmentation) 8.2.3+ (retry logic) 24.1.0+ 1.37.0+ 1.37.0+ MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness Table 4. Model Pricing (USD per million tokens). Pricing as of 25 October 2025 from the providers public APIs. Model Input (prompt) Output (completion) GPT-4o GPT-5 Claude 4 Sonnet Gemini 2.5 Pro $2.50 $1.25 $3.00 $1. $10.00 $10.00 $15.00 $10.00 backoff_duration=2s and max_retries=5. Telemetry & Observability. The framework captures perinvocation metrics (latency-to-first-token, generation time, token counts, cost) and organizes traces into hierarchical spans (run, unit, episode) annotated with run/unit identifiers, status, and dataset metadata. Structured logs use structlog with JSON formatting and contextual bindings. Model Pricing. Cost estimates reported in our experiments are computed using the provider-published pricing as of 25 October 2025, shown in Table 4. All costs are calculated in USD per million tokens for both input (prompt) and output (completion) tokens. Token counts are obtained directly from provider API responses. The total cost CUSD for each episode is computed as CUSD = tokensinput priceinput + tokensoutput priceoutput Later, we aggregate across all invocations within unit. Note that pricing may vary by region, deployment tier, or provider discounts; we report costs using standard public API rates. PROMPTS, CONFIG, MANIFEST &"
        },
        {
            "title": "REPORT",
            "content": "This appendix provides the exact artifacts required to reproduce our runs: judge prompts, task-driver prompts, job configuration, and the emitted plan manifest. For reproducibility, we report the prompt, model identifiers, and key decoding parameters (temperature, max tokens). Please act as an impartial judge and choose the most realistic USER among the two conversations provided below. You should choose the conversation which has USER turns written by real user in your opinion. Based on your experience & knowledge of human conversation, determine which USER in which conversation sounds more like real human interacting with chatbot. Note that your judgement should be based on style, tone and behavior of USER rather than the response quality of their utterances. Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. Do not allow the length of the USER messages to influence your evaluation. Do not let ASSISTANT messages influence your decision. Your evaluation must only be dependent on USER messages only. Be as objective as possible. Before giving your final evaluation verdict, you should first think through and produce your reasoning step-by-step. Then, based on your reasoning, give your final verdict on which conversation has USER turns that sound more like real human interacting with chatbot. If Conversation is more real, then verdict would be A\". If Conversation is more real, then verdict would be B\". If both are equally real or non-real, then verdict would be Tie\". Your output must be of following JSON format: {{reasoning\": verdict\": A\"B\"Tie\"}}. reason>\", <short [Conversation A] {conversation_a} [Conversation B] {conversation_b} Output ONLY valid JSON, no additional text. Figure 10. Judge prompt for Pairwise Indistinguishability Metric E.1 System Prompts GTEval Judge Prompt We provide system prompts used across the evaluation pipeline (metric judges, user-proxy, assistant). PI Judge Prompt [System] You are an expert evaluator of conversational AI systems. Your task is to compare two conversations and assess whether the user responses in the proxy conversation exhibit similar style, tone, and realism to the user responses in the real conversation. MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness ## Real Conversation: {real_conversation} ## Proxy Conversation (where user turns are generated by user proxy agent): {proxy_conversation} ## Evaluation Criteria: 1. **Style Similarity**: Do the proxy user responses match the conversational style of real user responses (formality, tone, verbosity)? 2. **Realism**: Do the proxy user responses sound natural and human-like? 3. **Contextual Appropriateness**: Are the proxy user responses appropriate given the conversation context? Note: You should not evaluate based on the content of the responses, only their style, realism, contextual appropriateness, and tone. ## Instructions: - Focus exclusively on comparing USER responses (ignore assistant responses) - Consider the overall conversation flow, not just individual turns - Evaluate style, tone, and naturalness without encouraging unsafe behaviour - If the comparison cannot be performed safely, explain the reason and provide score of 0.0 - Otherwise, provide detailed reasoning about the comparison and assign score from 0.0 to 1.0 where: - 0.0 = Proxy user responses are completely unlike real user responses - 0.5 = Proxy user responses are somewhat similar but have noticeable differences - 1.0 = Proxy user responses are highly similar in style and realism Provide your evaluation as JSON object with the following structure: {{ reasoning\": Detailed explanation of your evaluation (2-3 sentences)\", score\": <float between 0.0 and 1.0> }} Output ONLY valid JSON, no additional text. Figure 11. Judge prompt for GTEval Metric RNR Judge Prompt [System] Please act as an impartial judge and provide verdict on the realism of the USER turns in the conversation provided below based on the provided rubric. You should avoid checking the responses of the ASSISTANT messages since they are not relevant to the task. Your evaluation should follow the rubric provided. Note that your judgement should be based on style, tone and behavior of USER rather than the response quality of their utterances. [Rubric for scoring USER realism] Provide verdict on how human-like the USER is on (NO\" or YES\") using this rubric (verdict NO\" means not real, verdict YES\" means real): 1. Concise and real-user like language 2. Does not sound scripted or artificial 3. Real-user like tone and style Return JSON: {{reasoning\": <1-2 sentences>\", verdict\": <NO\" or YES\">}}. [Conversation] {conversation} Output ONLY valid JSON, no additional text. Figure 12. Judge prompt for RNR Metric User-Proxy System Prompt You are simulating real human user for the MIRRORBENCH evaluation harness. Respond with the next USER turn only. Do not write assistant messages, notes, or any other analysis. Your utterance should be like real user and the context should be based on the following information provided. Task description: {task_description} Match the length, tone, and specificity of real user If you are unsure, respond naturally utterances. based on the assistants previous messages like how real human would. Note that your response MUST not contain anything other than the USER utterance. Do not include any prefixes like User: or Human: as well. Just the raw message content. Figure 13. System prompt for User-Proxy MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness"
        },
        {
            "title": "Assistant System Prompt",
            "content": "You are the assistant in MIRRORBENCH replay. The user-proxy agent is attempting to reproduce the USER side of the real conversation provided below. But the user-proxy does not have access to the real conversation hitory. Instead, it only has access to the conversation summary. You need to respond as the assistant. But we are providing you with the real conversation history as context, so you can respond consistently same as (or similar to) the original assistant in the real conversation (you may paraphrase lightly for safety). If user-proxy deviates from the original USER turn or the original response would violate policy, reply helpfully using your own knowledge while remaining consistent with the persona demonstrated so far. Always follow ethical content policies. Paraphrase sensitive content instead of quoting it verbatim, and refuse politely if request is disallowed. Here is the real conversation for context (the USER turns are from the original conversation): {real_conversation} Now, we will provide you with ongoing conversation with the user-proxy. Please respond as the assistant in this conversation. The datasets block binds the ChatbotArenas MIRRORBENCH corresponding split to local JSONL path and caps evaluation at 200 examples for controlled runtime. Metrics are declared under metrics with their own clients and prompts shown in E.1, allowing the judge to differ from the proxy/assistant. We show two metrics, GTEval and Pairwise Indistinguishability, both routed to Claude-4-Sonnet at temperature=0. Each metric includes num_judge_samples (for repeated scoring) and compute_controls=true, which triggers the HH/PP control comparisons used for calibration and bias checks. Finally, the task_drivers section binds the dataset to the Mirror Conversation Driver, which synthesizes multi-turn dialogues by alternating the user-proxy and assistant models; here, the assistant is also GPT-4o at temperature=0, ensuring low-variance responses for per-proxy comparability. Together, these blocks yield self-contained manifest that our planner expands into (proxy, dataset, metric, seed) units, enabling exact replay and variance-aware aggregation across runs. We illustrate an example of such manifest file in Figure 16, which is manifest produced using config shown in Figure 15. the CLI command mirrorbench run -c Once config.yaml is invoked, the evaluation job starts and executes through all the units. Once the run is completed, another command mirrorbench report json <run-name> -output report.json can be executed to generate final evaluation report as shown in Figure 17. Figure 14. System prompt for Assistant Figure 13 and 14 shows system prompts for user-proxy and assistant respectively. Note that once filled, the system prompt would go into chat history as system message. Thus, we do not see the chat history-related placeholder in these system prompts. E.2 Evaluation Config, Manifest & Report Figure 15 shows the example job configuration. The the run block fixes execonfiguration is declarative: cution semantics (backend, concurrency, timeouts), reproducibility (seeds), and infrastructure knobs (cache and logging). the async backend with max_concurrency=8, while caching is enabled to deduplicate repeated LLM calls across units. The user_proxies block registers concrete proxy via lightweight adapter and model client (here, LangChains ChatOpenAI wrapper for GPT-4o), keeping proxy identity and invocation details separate from orchestration. In our example, MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness Figure 15. Job configuration (abridged). YAML that fully specifies MIRRORBENCH run: backend and concurrency, proxy/assistant clients, dataset binding, judge metrics (with calibration controls), and the mirror-conversation driver. MIRRORBENCH: An Extensible Framework to Evaluate User-Proxy Agents for Human-Likeness Figure 17. Final run report (JSON, abridged). Aggregate metric scores with 95% CIs, calibration controls (HH/PP), unit grid, and run metadata for the completed evaluation. Figure 16. Plan manifest (JSON, abridged). Fully resolved, replayable spec listing the (proxy, dataset, metric, seed) units, driver params, versions, and config_hash, enabling deterministic reruns."
        }
    ],
    "affiliations": [
        "SAP"
    ]
}