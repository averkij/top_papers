{
    "paper_title": "Muon Outperforms Adam in Tail-End Associative Memory Learning",
    "authors": [
        "Shuche Wang",
        "Fengzhuo Zhang",
        "Jiaxiang Li",
        "Cunxiao Du",
        "Chao Du",
        "Tianyu Pang",
        "Zhuoran Yang",
        "Mingyi Hong",
        "Vincent Y. F. Tan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The Muon optimizer is consistently faster than Adam in training Large Language Models (LLMs), yet the mechanism underlying its success remains unclear. This paper demystifies this mechanism through the lens of associative memory. By ablating the transformer components optimized by Muon, we reveal that the associative memory parameters of LLMs, namely the Value and Output (VO) attention weights and Feed-Forward Networks (FFNs), are the primary contributors to Muon's superiority. Motivated by this associative memory view, we then explain Muon's superiority on real-world corpora, which are intrinsically heavy-tailed: a few classes (tail classes) appear far less frequently than others. The superiority is explained through two key properties: (i) its update rule consistently yields a more isotropic singular spectrum than Adam; and as a result, (ii) on heavy-tailed data, it optimizes tail classes more effectively than Adam. Beyond empirical evidence, we theoretically confirm these findings by analyzing a one-layer associative memory model under class-imbalanced data. We prove that Muon consistently achieves balanced learning across classes regardless of feature embeddings, whereas Adam can induce large disparities in learning errors depending on embedding properties. In summary, our empirical observations and theoretical analyses reveal Muon's core advantage: its update rule aligns with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions than Adam."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 0 3 0 6 2 . 9 0 5 2 : r Muon Outperforms Adam in Tail-End Associative Memory Learning Shuche Wang1, Fengzhuo Zhang1,, Jiaxiang Li2, Cunxiao Du3 Chao Du3 Tianyu Pang3 Zhuoran Yang4 Mingyi Hong2 Vincent Y. F. Tan1 1National University of Singapore 2University of Minnesota 3Sea AI Lab 4Yale University shuche.wang@u.nus.edu fzzhang@u.nus.edu li003755@umn.edu cnsdunm@gmail.com {duchao, tianyupang}@sea.com zhuoran.yang@yale.edu mhong@umn.edu vtan@nus.edu.sg Abstract The Muon optimizer is consistently faster than Adam in training Large Language Models (LLMs), yet the mechanism underlying its success remains unclear. This paper demystifies this mechanism through the lens of associative memory. By ablating the transformer components optimized by Muon, we reveal that the associative memory parameters of LLMs, namely the Value and Output (VO) attention weights and Feed-Forward Networks (FFNs), are the primary contributors to Muons superiority. Motivated by this associative memory view, we then explain Muons superiority on real-world corpora, which are intrinsically heavy-tailed: few classes (tail classes) appear far less frequently than others. The superiority is explained through two key properties: (i) its update rule consistently yields more isotropic singular spectrum than Adam; and as result, (ii) on heavy-tailed data, it optimizes tail classes more effectively than Adam. Beyond empirical evidence, we theoretically confirm these findings by analyzing one-layer associative memory model under class-imbalanced data. We prove that Muon consistently achieves balanced learning across classes regardless of feature embeddings, whereas Adam can induce large disparities in learning errors depending on embedding properties. In summary, our empirical observations and theoretical analyses reveal Muons core advantage: its update rule aligns with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions than Adam."
        },
        {
            "title": "Introduction",
            "content": "The effectiveness of Adam (Kingma & Ba, 2015) across diverse training scenarios has made it one of the most widely used optimizers for neural networks, serving as cornerstone of the tremendous successes of Large Language Models (LLMs). Building on this foundation, Muon (Jordan et al., 2024) has emerged as matrixparameter optimizer designed to surpass Adam. Empirical studies show that Muon is nearly 2 times faster than Adam across wide range of model sizes and architectures (Liu et al., 2025; Jordan et al., 2024). Its key innovation is to replace the raw gradient with the sum of its normalized orthogonal factors, which can be interpreted as performing steepest descent with respect to the spectral norm (Bernstein & Newhouse, 2024). However, despite its empirical success, rigorous understanding of why and how Muon outperforms Adam in transformers remains incomplete. In particular, the steepest gradient descent interpretation does not clarify why optimization with respect to the spectral norm, as in Muon, should outperform optimization with respect to the infinity norm (for vectors), as in Adam. Consequently, convergence analyses of Muon derived from this interpretation fail to account for its observed superiority over Adam (Li & Hong, 2025; Shen et al., 2025). Equal contribution. Project Lead. Fengzhuo conducts this work during his internship at Sea AI Lab. 1 This paper takes the first step toward understanding the mechanisms underlying Muons superiority over Adam in training LLMs. Specifically, we ask the following two questions: 1. Which transformer components benefit most from Muons matrix-normbased optimization compared to Adam? 2. What structural features of the transformer allow Muon to optimize these components more effectively? To answer the first question, we apply Muon to different transformer components. Our experiments consistently show that the more rapid convergence of the validation loss using the Muon optimizer compared to Adam is primarily due to the formers focus on the value-output (VO) matrices of the attention mechanism and the Feed-Forward Networks (FFN) blocks. This leads to our first key insight: VO and FFN blocks, which serve as the primary associative memory stores in the model (Geva et al., 2020; Bietti et al., 2023), are the main beneficiaries of Muons optimization strategy. Building on this, we address the second question linking Muons update mechanism to the learning dynamics of associative memory. Prior work suggests that the behavior of these memory components can be modeled as sum of outer products representing stored facts (Meng et al., 2022a). Since Muons update assigns equal update magnitudes to each outer product of the gradient corresponding to orthogonal singular directions, we hypothesize that it optimizes associative memories more effectively than Adam because: (i) Muons spectral normalization procedure balances the rates of learning of these outer products. (ii) Thus, when training on heavy-tailed data (i.e., where few facts appear much more frequently than the rest), Muon reduces the dominance of frequent (head) facts and enables more effective learning from infrequent (tail) facts compared to Adam. We validate these hypotheses through combination of empirical analysis and theoretical modeling. Empirically, we conduct two experiments. First, we measure the singular value spectra of the weight matrices and show that Muon consistently yields more isotropic representations than Adam, indicating that its normalization prevents spectral energy from concentrating in dominant components. Second, we evaluate the performance of both optimizers on knowledge-intensive, heavy-tailed task to demonstrate the practical benefit of Muons more balanced updates: while both optimizers perform well on head classes (frequent in training data), Muon outperforms Adam on tail classes (rare in training data), leading to more stable and uniform convergence. Theoretically, we focus on one-layer linear associative memory model to rigorously explain these empirical findings. Under class imbalance in the training data, mimicking heavy-tailed distribution, we show that Muon maintains balanced learning across classes, regardless of the feature embeddings. In contrast, we prove that Adams performance is unstable and strongly dependent on the embedding structure, which can lead to large disparities in learning error across classes. By closely examining the parameter updates, we find that the singular spectrum of weight matrices trained by Muon is nearly isotropic, whereas Adams is uneven. Summarizing the empirical and theoretical findings, we identify clear mechanism underlying Muons superiority: The Muon update rule is aligned with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions as compared with Adam."
        },
        {
            "title": "2 Preliminaries",
            "content": "Muon (Jordan et al., 2024) is an optimizer tailored for matrix parameters that replaces the raw (or momentum) gradient with the sum of its normalized orthogonal factors, producing scale-invariant, norm-controlled update direction. For weight matrix Rmn at step t, let Gt = L(Wt) denote its gradient. Muon maintains momentum accumulator of gradients as Bt = µBt1 + Gt with B0 = 0, and µ [0, 1). At each step, Muon computes the Singular Value Decomposition (SVD) of Bt as Bt = UtStV with Ut Rmrt, Vt Rnrt, where rt = rank(Bt), and form the nearest (semi)orthogonal matrix Ot = UtV . Then Muon updates the parameter as Wt+1 = Wt ηtOt. In practice, one can approximate Ot using fixed number (e.g., 5) of NewtonSchulz iterations applied to Bt(B Bt)1/2, which avoids the full SVD while preserving the scale normalization effect. Detailed introduction of Muon is in the related works section (Appendix B). 2 Transformers serve as the backbone of LLMs, predicting the probability of the next token given sequence of tokens. sequence of tokens is embedded into matrix (0) RdN . The first layer takes (0) as the input, and each subsequent layer takes the previous layers output as its input. Every layer ℓ [L] processes its input through two sequential components: an attention module and FFN module. The attention module computes (ℓ) = (ℓ1) + (cid:88) h= (ℓ) O,hW (ℓ) V,hX (ℓ1)sm(cid:0)X (ℓ1),W (ℓ), K,h (ℓ) Q,hX (ℓ1)(cid:1), (2.1) where sm() is the column-wise softmax operator, is the number of attention heads, (ℓ) capture token relationships, and (ℓ) V,h Rdvd, (ℓ) forward module then updates the representation as K,h Rdkd O,h Rddv apply linear transformations. The feedQ,h, (ℓ) (ℓ) = (ℓ) + ff(H (ℓ), (ℓ) in , (ℓ) out) = (ℓ) + (ℓ) outσ(W (ℓ) in (ℓ)), (2.2) where σ() is the element-wise activation function, and (ℓ) gated variant replaces the standard form with in Rdf d, (ℓ) out Rddf are learnable parameters. ffgate(H (ℓ), (ℓ) in , (ℓ) out, (ℓ) gate) = (ℓ) out (cid:0)σ(W (ℓ) in (ℓ)) (W (ℓ) gateH (ℓ))(cid:1), where is the Hadamard product, and (ℓ) hidden state of the last token, (L) logits EheadX (L) 1 , which has vocabulary of size of K. gate Rdf is an additional mapping. After layers, the final 1 , is projected by the language model head Ehead RKd to produce Associative memory refers to architectures that store and retrieve patterns based on learned associations between inputs and outputs. Recent research has examined linear associative memory in LLMs. Specifically, consider triplet (s, r, o), where is the subject, the relation, and the object (e.g., =The United Nations headquarters, =is located in, =New York City). linear associative memory maps key vector es encoding (s, r) to value vector eo encoding o, such that eo = es holds for all possible (s, r, o). Under the orthogonality of embeddings es and eo, can be expressed as = (cid:80) , where the summation is taken over the indexes of facts. Prior work has investigated associative memory in both attention and FFN modules. In the attention module, Bietti et al. (2023) showed that the parameter WO can serve as linear associative memory when WV is fixed. Since WO and WV play symmetric roles, we also treat WV as part of the associative memory parameters. In FFN, works on knowledge editing (Geva et al., 2020; Dai et al., 2021; Meng et al., 2022a,b) have identified the module as functioning as an associative memory, which can be well approximated by linear associative memory models. Thus, throughout this paper, we refer to WO, WV , and FFN in LLMs as the associative memory parameters. eoie si"
        },
        {
            "title": "3.1 Associative Memories Are Main Beneficiaries of Muon",
            "content": "In this section, we identify the transformer components that benefit most from Muon by measuring validation loss on the FineWeb dataset using 160M NanoGPT model. We adopt twostage protocol. First, in the Independent Blocks setting, we apply Muon to single block at time while keeping all other blocks on Adam, covering the attention projections WQ, WK, WV , WO and the feed-forward matrices Win, Wout. Second, in the Combined Configurations setting, we apply Muon to the most impactful subsets identified in the first stage to examine whether partial application can recover the performance gains of full Muon. As introduced in Section 2, we evaluate both gated and non-gated FFN variants of NanoGPT. The experimental details are in Appendix E. Figure 1 presents our results. We first examine the independent-block experiments for attention. In both ungated and gated FFN settings (Figures 1(a) and 1(b)), the VO weights WV , WO (Muon on VO / Adam on QK and FFN) show substantially larger gains under Muon than the QK weights WQ, WK (Muon on QK/Adam on VO and FFN). Notably, applying Muon to only WV or only WO already yields much larger 3 (a) Independent Blocks with Non-gated FFN (b) Independent Blocks with Gated FFN (c) Combined Configuration with Non-gated FFN (d) Combined Configuration with Gated FFN Figure 1: Validation loss comparison on the 160M NanoGPT model with ungated and gated FFN. Panels (a) and (b) show the Independent Blocks results, where individual components are optimized separately, for models with ungated and gated FFN, respectively. Panels (c) and (d) show the Combined Configurations results, where multiple components are optimized jointly, again for ungated and gated FFN models. gains than applying it to QK. Between the two, WO performs comparably in the gated FFN setting and better in the ungated setting. For the FFN, we find that Win, Wgate, and Wout all benefit from Muon, with Wout yielding stronger improvements than Win. After identifying the importance of each module, the combined configurations aim to quantify their contributions to the full Muon. Guided by the independent-block findings, we first observe that VO+FFN already closely tracksand in our runs nearly recoversthe full-Muon trajectory in Figures 1(c) and 1(d). This indicates that applying Muon to QK contributes little to its overall performance. Importantly, this effect is not due to the logit explosion reported by Team et al. (2025) in large Mixture of Experts (MoE) models; logit values for our setting do not explode, as reported in Appendix F.1. The small remaining gap between full Muon and VO+FFN may stem from the fact that VO+FFN adopts the same learning rate as full Muon without further tuning. To isolate the contributions of WO and WV within VO+FFN, we perform ablations starting from the VO+FFN setting: we keep Muon on FFN and on only one of WO or WV , reverting the other to Adam (i.e., V+FFN and O+FFN). Both ablations degrade performance, with the V+FFN variant dropping more, indicating that WO is more influential than WV . We apply the same analysis to FFN. The results reveal architectural sensitivity: in the ungated setting (Figure 1(c)), VO+Wout nearly recovers the fullMuon trajectory, whereas in the gated setting (Figure 1(d)) the same combination falls short. Nevertheless, both analyses underscore the central role of Wout in FFN. Overall, applying Muon to VO+FFN is critical for recovering full-Muon performance, though the extent of recovery still depends on architectural design (ungated vs gated). The results from training 0.7B model in Appendix F.2 show similar findings. Observation 1: Muon is most effective when applied to VO and FFN; in particular, applying Muon to only VO+FFN almost recovers the full-Muon trajectory. Remark 3.1. We emphasize that this observation is not trivial consequence of parameter counting; although QK and VO are equal in size, VO proves substantially more influential. As introduced in Section 2, prior works discover that the common role of VO and FFN is that they both serve as the associative memories for transformers, which store facts and knowledge. Furthermore, Bietti et al. (2023) and Meng et al. (2022a) show that the linear associative memories well approximate them. Specifically, for set of facts represented by key-value pairs {(esi , eoi)}, the memory matrix can be constructed as sum of outer products, i.e., = (cid:80) . eoi si Learning linear associative memories is particularly well-suited to Muons update mechanism. Intuitively, the gradient of the loss with respect to the linear associative memory weight can be expressed as sum of outer products. Muon computes its update (without momentum) by taking the SVD of the gradient, = SV = (cid:80) . Comparing this with the linear associative memory (cid:80) , we see that Muon updates all orthogonal facts at the same rate. Later, we will see that the singular values encode the frequencies of knowledge in the training data in Sections 3.3 and 4. This implies that Muon can learn both frequent and infrequent facts uniformly. , and forming the orthogonal factor = = (cid:80) siuiv eoie si uiv We verify this insight from two perspectives. First, from the view of weight spectra, the weight matrices learned with Muon exhibit more isotropic singular-value spectrum than those learned with Adam, indicating that knowledge, regardless of its frequency, is represented with comparable magnitude. Second, at the level of overall knowledge acquisition, Muon yields more balanced learning across entities and frequencies (head and tail) than Adam. We examine these two consequences in the following sections."
        },
        {
            "title": "3.2 Muon Consistently Learns More Isotropic Weights Than Adam",
            "content": "To validate that Muon can shape the weight matrices more evenly across directions, we conducted spectral analysis of them. For weight matrix with non-zero singular values σ = (σ1, σ2, . . . , σn), we define the normalized singular energy distribution = (q1, q2, . . . , qn), where each component qi is qi = σ2 j=1 σ2 . This distribution represents the fraction of energy captured by each corresponding singular vector. Based on this, we introduce several metrics to characterize the isotropy of the spectrum: normalized SVD entropy defined as Hnorm(σ) = 1 i=1 qi log qi), Top-k energy fraction defined as TopEk(σ) = (cid:80)k , and eigenvalue quantile ratio defined as {σ2 }). Detailed explanations of these metrics are in Appendix E.2. Intuitively, more isotropic weights correspond to larger values of normalized SVD entropy and effective rank, and smaller Top-k energy fraction and eigenvalue quantile ratio. i=1 qi log qi, effective rank defined as eRank(σ) = exp ( (cid:80)n i=1:Q75/25(σ) = Q3({σ2 })/Q1({σ2 / (cid:80)n /(cid:80)n j=1 σ2 i=1 σ2 }n (cid:80)n log The spectral analysis in Figure 2, focusing on the key associative memory components from Observation 1, shows that Muon systematically reshapes the learned weight matrices relative to Adam. The results, averaged over 10 random seeds, demonstrate that: (i) In both gated and ungated FFN architectures, Muon produces much more isotropic singular spectrum than Adam from the start of training, whereas Adams isotropy fluctuates significantly over the course of optimization. (ii) The isotropy of Muon is stable across random initializations, as indicated by the negligible error bars in Figure 2, while Adam is highly sensitive to initialization. These findings suggest that Muon consistently promotes richer and more diverse features in the models most critical memory components, conclusion we summarize below. The results for other weights are in Appendix F.3. Observation 2: Muon consistently yields more isotropic weight matrices with broadly distributed spectral energy than Adam, both throughout training and across random initializations, thereby supporting richer feature representations. Empirically, we also find that Muon learns more isotropic QK weights than Adam. However, as discussed in Section 3.1, QK weights are not part of the linear associative memory mechanism and are therefore not expected to benefit from the isotropic property of the weight matrices. 5 (a) VO(Non-gated FFN) (b) VO(Gated FFN) (c) Wout(Non-gated FFN) (d) Wout(Gated FFN) Figure 2: Spectral Dynamics of Transformer Weight Matrices During Training. Each panel reports four metrics characterizing singular value distributions: SVD entropy, Top10E, eRank, and Q75/Q25 ratio. The four subplots correspond to different weight matrix groups: (a) VO, (b) VO (Gated FFN), (c) Wout, and (d) Wout (Gated FFN). Our results differ fundamentally from the spectral analysis in Liu et al. (2025) for three reasons. First, we decompose the parameters according to associative memories, whereas Liu et al. (2025) aggregates them, obscuring the essential components driving Muons behavior. Second, we investigate the instability of Adam under random initialization (i.e., random seeds), which we further establish theoretically in Section 4. Finally, our analysis focuses on dense architectures, while Liu et al. (2025) centers on Mixture-of-Experts (MoE) models."
        },
        {
            "title": "3.3 Muon Acquires Knowledge More Evenly Compared To Adam",
            "content": "Our previous findings indicate that the Muon optimizer is particularly important for the associative memory components of the model, where it learns more isotropic weights. To examine the overall effects of learning associative memories, we turn to knowledge-intensive question-answering (QA) task. The task is based on synthetic QA dataset containing biographical information (e.g., name, birthday, and company) for over 200,000 individuals (Allen-Zhu & Li, 2024). To capture the heavy-tailed nature of real-world knowledge, we control the frequency of each individuals appearance in the training set so that it follows power-law distribution (Figure 3(a)), thereby inducing varying levels of difficulty in learning knowledge about different individuals. 160M NanoGPT model is trained to answer questions about this biographical information. The performance is evaluated via the First Token Accuracy (FTA) on the answers, following Allen-Zhu & 6 (a) Sample/class (b) Muon (c) Adam (d) SGD+Momentum (e) Muon(VO,FFN)/Adam(QK) (f) Muon(QK)/Adam(VO,FFN) Figure 3: Performance comparison of different optimizers for transformers with non-gated FFN on heavytailed knowledge task. (a) Sample distribution per class, following power law. (bd) Performance of Muon, Adam, and SGD+Momentum. (e) Muon applied to VO and FFN, with Adam on QK. (f) Muon applied to QK, with Adam on VO and FFN. Li (2024). Further details on the dataset are provided in Appendix E.3. We include SGD as baseline for Adam and Muon. The results in Figure 3 lead to an unequivocal conclusion about the efficacy of different optimizers under In high-frequency (head) classes, all optimizers perform well, with Muon, Adam, and data imbalance. even SGD+Momentum rapidly reaching near-perfect accuracy (Figure 3(bd)). Consistent with prior work on heavy-tailed distributions (Kunstner et al., 2024), Adam maintains clear advantage over SGD, which struggles with tail classes. Our key finding, however, is that Muon substantially outperforms Adam on low-frequency (tail) data, achieving faster and more uniform convergence across all frequencies. Moreover, the consistently tighter error bars for Muonespecially relative to Adamreflect lower variance and more stable learning process. Furthermore, the hybrid configurations in Figure 3(ef) clarify where Muon matters most. Applying Muon to VO+FFN (with QK on Adam) yields strong gains on rare classes and markedly reduces the headtail gap, whereas applying Muon only to QK (with VO+FFN on Adam) yields only limited improvement. This mirrors Observation 1: VO+FFN is the most effective target set, as it concentrates the models associative memory. Results for the gated FFN, which show the same pattern, are provided in Appendix F.5. We summarize these findings as Observation 3. Observation 3: In heavy-tailed, knowledge-intensive tasks, Muon matches Adams strong performance in the head classes while substantially improving learning on tail classes, narrowing the head-tail gap and accelerating convergence."
        },
        {
            "title": "4 Case Study of One-Layer Models",
            "content": "We now analyze three optimizersAdam, Muon, and Gradient Descent (GD) (as baseline)to complement the preceding empirical observations. We first introduce an abstraction that captures their key dynamics and then present both empirical and theoretical results. As shown in Eqns. (2.1) and (2.2), structural property of associative memory parameters is that their output is added directly to the hidden states, which 7 (a) Average Angles Between Ei/ (cid:101)Ei (b) One-step Optimization Results (c) Multi-step Optimization Results Figure 4: (a) Average angles between Ei or (cid:101)Ei in FFN at layers 5, 10, 15, 20, 25 of Llama3-8b-instruct. (b) Results of one-step GD, SignGD, and Muon with both coupled and decoupled embeddings. For GD, the outcomes under the two embedding types coincide. (c) Results of multi-step GD, SignGD, and Muon with both coupled and decoupled embeddings. are subsequently processed by the language model head. Motivated by this property, our abstraction retains the associative memory and language model head, while replacing all preceding modules with given feature embeddings. Consider triplets {(si, ri, oi)}K i=1, where subject-relation pairs (si, ri) and objects oi are embedded into the columns of matrices RdsK and (cid:101)E RdoK, respectively. linear associative memory Rdods predicts the object for query Ek with probabilities fW (Ek) = sm( (cid:101)EW Ek) RK. The objective is to minimize the population cross-entropy loss L(W ) = (cid:80)K k=1 pk log[fW (Ek)]k, where pk is the frequency or probability of the k-th triplet. We analyze three optimizers initialized at W0 = 0, all simplified by disabling momentum for clarity. (i) GD: Wt+1 = Wt ηW L(Wt). (ii) Adam: Following prior work (Kunstner et al., 2024; Bernstein & Newhouse, 2024), we set β1 = β2 = 0, reducing it to SignGD: Wt+1 = Wt η sign(W L(Wt)). (iii) Muon: The update is Wt+1 = WtηUtV is the SVD of L(Wt). This simplified form, UtV , is the projection of the gradient onto the nearest orthogonal matrix. We then state the assumptions for our results. , where UtΣtV Assumption 4.1. The embeddings and (cid:101)E are orthonormal, i.e., EE = (cid:101)E (cid:101)E = IK,K. The unit-norm requirement rules out feature-level imbalance, which would otherwise couple with the imbalance induced by pk and complicate the analysis. Our techniques can be directly applied even without this unit-norm requirement. The orthogonality assumption is intuitively plausible, as different concepts are independent and do not influence one another. We empirically verify this on Llama3-8b-instruct (Dubey et al., 2024). Following Fang et al. (2024), we extract Ei and (cid:101)Ei in FFN across layers for 3, 000 knowledge items of Counterfact (Meng et al., 2022a) and compute average angles between them (see Appendix E.4 for details). As shown in Figure 4(a), these angles are near 90, confirming approximate orthogonality. For independent concepts, orthogonality requires dr, ds K. For simplicity, we set dr = ds = in what follows. Assumption 4.2. The first triplets share the same probability and together contribute total mass of α, i.e., pk = α/L for [L]. The remaining triplets also share the same probability and together contribute total mass of 1 α, i.e., pk = (1 α)/(K L) for > L. This assumption states that the data imbalance is between two classes among the triplets. Defining β = L/K, the ratio α/β quantifies the degree of balance: if α > β, the first triplets appear more frequently during learning, and vice versa. This simplified two-class setting is sufficient to capture the primary differences between optimizers; the multi-class case follows directly from our proof by extending the SVD calculation."
        },
        {
            "title": "4.1 Experimental Results",
            "content": "Under Assumptions 4.1 and 4.2, we evaluate GD, SignGD, and Muon for α = 0.8, β = 0.2, considering two embeddings for and (cid:101)E: (i) support-decoupled: the supports (indices of non-zero entries) of different Ei or 8 (cid:101)Ei are disjoint; (ii) support-coupled: supports may overlap. We study two optimization protocols, initializing W0 = 0dods: (i) one-step: take single update with scaled step size to obtain range of L(W ) values; (ii) multi-step: run multiple updates to reduce L(W ), varying the number of steps. Experimental details are in Appendix E.5. To quantify learning imbalance across knowledge items, we examine the relationship between population loss L(W ) and maximal probability gap (W ) := maxi,j[K][fW (Ei)]i [fW (Ej)]j, where [fW (Ei)]i denotes the probability assigned to the correct item i. larger (W ) indicates greater imbalance. Across both optimization-step protocols and embeddings (Figures 4(b), 4(c)), we observe that (i) For all optimizers, (W ) first increases and then decreases as L(W ) decreases. Early in training, when correct probabilities are near 0, imbalance is pronounced; later, when all items are well learned (e.g., probabilities 0.9), imbalance diminishes. (ii) For both embedding regimes, GD and Muon behave consistently: GD exhibits substantial imbalance, whereas Muon remains much more balanced across items. (iii) SignGD also demonstrates unstable behavior; its imbalance resembles GD in the coupled embedding case and Muon in the decoupled embedding case. Because one-step and multi-step experiments align qualitatively, we first analyze the one-step setting for clarity. This simplification is common in theoretical studies of neural network dynamics (Ba et al., 2022; Dandi et al., 2023), and our techniques extend directlyalbeit with more algebrato the multi-step case. As demonstration, Theorem 4.4 provides multi-step analysis of Muon."
        },
        {
            "title": "4.2 Theoretical Results",
            "content": "For the one-step analysis, define the smallest correct-class probability across all knowledge items, under the condition that at least one item achieves the correct-class probability of at least 1 ϵ as (cid:110) ϱϵ opt = inf η0 min k[K] [fWη (Ek)]k (cid:12) (cid:12) (cid:12) max k[K] [fWη (Ek)]k 1 ϵ, Wη = W0 η Gopt(W0) (cid:111) . (4.1) where opt {GD, SignGD, Muon} and Gopt(W0) denotes the parameter update of optimizer opt at W0; and Wη denotes the parameter obtained after one step of optimizer opt with step size η starting from W0, i.e., Wη = W0 η Gopt(W0). Specifically, GGD(W0) = L(W0), GSignGD(W0) = sign(W L(W0)), and GMuon(W0) = U0norm(Σ0)V opt [0, 1 ε] and (W ) are related as (W ) = 1 ϵ ϱϵ opt 1 ϵ, opt achieves balanced learning across facts; in contrast, when ϱϵ is the SVD of L(W0). Note that ϱϵ 0 where U0Σ0V 0 opt 0. When ϱϵ opt 0, imbalanced learning ensues. Theorem 4.3. If Assumptions 4.1 and 4.2 hold, with fixed α, β such that α = β, and goes to infinity, we obtain the following results for one-step GD, Muon, and Adam. For GD, for any (cid:101)E and satistifying Assumption 4.1, we have GD = O(ϵr(α,β)K r(α,β)1), where r(α, β) = min ϱϵ (cid:26) α(1 β) β(1 α) , β(1 α) α(1 β) (cid:27) < 1. For Muon, for any (cid:101)E and satistifying Assumption 4.1, we have ϱϵ Muon 1 ϵ (cid:18) 1 + (cid:19)(cid:19) (cid:18) log , and GMuon(W0) = (cid:101)EE + (cid:18) 1 (cid:101)EJK,KE (cid:19) , where JK,K RKK is the matrix with all elements equal to 1. The big-O notation for matrices means that for = O(B), each entry satisfies Aij = O(Bij) for all i, j. For Adam, there exist (cid:101)E and satisfying Assumption 4.1 such that ϱϵ SignGD 1 ϵ. There also exist (cid:101)E and satisfying Assumption 4.1 such that SignGD = O(ϵ0.7K 0.3), and ϱϵ σmin σmax (cid:0)GSignGD(W0)(cid:1) (cid:0)GSignGD(W0)(cid:1) 25%, where σmax and σmin are the largest and smallest singular values, respectively. Interpretation of Theorem 4.3. These theoretical results align with Observations 2 and 3, and Figures 4(b) and 4(c): Muon maintains balanced learning with near-isotropic updates, GD is highly sensitive to 9 data imbalance, and Adam varies widely across embeddings. At the one-step update, when the maximum correct-class probability across items is at least 1 ϵ, the item with the minimum correct-class probability satisfies: (i) Muon: 1 ϵ(cid:0)1 + O( log )(cid:1), which indicates learning is essentially balanced across items with near-isotropic update (singular values nearly equal); (ii) GD: O(cid:0)ϵr(α,β)K r(α,β)1(cid:1), which is strongly controlled by data imbalance via r(α, β) (balanced when = 1, severe imbalance when 1); (iii) Adam: embedding dependent; it can match Muon with disjoint supports (e.g., (cid:101)E = = IK,K), achieving 1 ϵ, but can drop to O(ϵ0.7K 0.3) with overlap; its update may exhibit pronounced spectral decay (σmin/σmax 25%), unlike the near-uniform singular values of Muon. detailed discussion of Theorem 4.3 is provided in Appendix D. In the following, we extend our techniques of one-step analysis to the multi-step analysis of Muon. Parallel to Eqn. (4.1), we define the infimum correct-class probability for the multi-step optimizer as ϱϵ opt = inf t{mink[K][fWt(Ek)]k maxk[K][fWt(Ek)]k 1 ϵ, where Wt = Wt1 ηt Gopt(Wt1)}. Here, we assume that the learning rates {ηt}t1 are determined by fixed schedule prior to optimization. Although the quantity implicitly depends on this schedule, we omit it from the notation for ϱϵ opt for brevity. We emphasize that different schedules may affect the value of that attains the infimum in ϱϵ opt, but they do not influence the balance behavior that we present. Theorem 4.4. If Assumptions 4.1 and 4.2 hold, then multi-step Muon achieves ϱϵ Muon 1 ϵ (cid:18) 1 + (cid:19)(cid:19) (cid:18) log , and GMuon(Wt) = (cid:101)EE + (cid:18) 1 (cid:19) (cid:101)EJK,KE for any 0. The proof is provided in Appendix H. We note that the multi-step analysis of Muon shares similar characteristics as the one-step version in Theorem 4.3."
        },
        {
            "title": "5 Conclusion",
            "content": "Our work takes the first step toward unveiling why and how Muon outperforms Adam. Through ablations of Muons effect on different Transformer components and by relating these results to the balanced learning of associative memories, we conclude that the Muon update rule is aligned with the outer-product structure of linear associative memories, enabling more balanced and effective learning of tail classes in heavy-tailed distributions. Intuitively, this property of Muon may extend beyond outer products to higher-order tensor products, an exciting direction for future work."
        },
        {
            "title": "References",
            "content": "Ruslan Abdulkadirov, Pavel Lyakhov, and Nikolay Nagornov. Survey of optimization algorithms in modern neural networks. Mathematics, 11(11):2466, 2023. Zeyuan Allen-Zhu and Yuanzhi Li. Physics of language models: Part 3.3, knowledge capacity scaling laws. arXiv preprint arXiv:2404.05405, 2024. Orly Alter, Patrick Brown, and David Botstein. Singular value decomposition for genome-wide expression data processing and modeling. Proceedings of the National Academy of Sciences, 97(18):1010110106, 2000. Kang An, Yuxing Liu, Rui Pan, Yi Ren, Shiqian Ma, Donald Goldfarb, and Tong Zhang. Asgo: Adaptive structured gradient optimization. arXiv preprint arXiv:2503.20762, 2025. Jimmy Ba, Murat Erdogdu, Taiji Suzuki, Zhichao Wang, Denny Wu, and Greg Yang. High-dimensional asymptotics of feature learning: How one gradient step improves the representation. Advances in Neural Information Processing Systems, 35:3793237946, 2022. Jeremy Bernstein and Laker Newhouse. Old optimizer, new norm: An anthology. arXiv preprint arXiv:2409.20325, 2024. Alberto Bietti, Vivien Cabannes, Diane Bouchacourt, Herve Jegou, and Leon Bottou. Birth of transformer: memory viewpoint. Advances in Neural Information Processing Systems, 36:15601588, 2023. Xiangyi Chen, Sijia Liu, Ruoyu Sun, and Mingyi Hong. On the convergence of class of adam-type algorithms for non-convex optimization. In International Conference on Learning Representations, 2019. URL https: //openreview.net/forum?id=H1x-x309tm. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. Knowledge neurons in pretrained transformers. arXiv preprint arXiv:2104.08696, 2021. Yatin Dandi, Florent Krzakala, Bruno Loureiro, Luca Pesce, and Ludovic Stephan. How two-layer neural networks learn, one (giant) step at time. arXiv preprint arXiv:2305.18270, 2023. Alexandre Defossez, Leon Bottou, Francis Bach, and Nicolas Usunier. simple convergence proof of adam and adagrad. arXiv preprint arXiv:2003.02395, 2020. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv e-prints, pp. arXiv2407, 2024. Junfeng Fang, Houcheng Jiang, Kun Wang, Yunshan Ma, Shi Jie, Xiang Wang, Xiangnan He, and TatSeng Chua. Alphaedit: Null-space constrained knowledge editing for language models. arXiv preprint arXiv:2410.02355, 2024. Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. Transformer feed-forward layers are key-value memories. arXiv preprint arXiv:2012.14913, 2020. John Hopfield. Neural networks and physical systems with emergent collective computational abilities. Proceedings of the national academy of sciences, 79(8):25542558, 1982. Keller Jordan, Yuchen Jin, Vlado Boza, You Jiacheng, Franz Cecista, Laker Newhouse, and Jeremy Bernstein. Muon: An optimizer for hidden layers in neural networks, 2024. URL https://kellerjordan. github. io/posts/muon, 6, 2024. Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Rame, Morgane Rivi`ere, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. 11 Diederik Kingma and Jimmy Lei Ba. Adam: method for stochastic gradient descent. In ICLR: international conference on learning representations, pp. 115, 2015. Teuvo Kohonen. Correlation matrix memories. IEEE transactions on computers, 100(4):353359, 2009. Dmitry Kovalev. Understanding gradient orthogonalization for deep learning via non-euclidean trust-region optimization. arXiv preprint arXiv:2503.12645, 2025. Frederik Kunstner, Alan Milligan, Robin Yadav, Mark Schmidt, and Alberto Bietti. Heavy-tailed class imbalance and why adam outperforms gradient descent on language models. Advances in Neural Information Processing Systems, 37:3010630148, 2024. Tim Tsz-Kit Lau, Qi Long, and Weijie Su. Polargrad: class of matrix-gradient optimizers from unifying preconditioning perspective. arXiv preprint arXiv:2505.21799, 2025. Omer Levy, Minjoon Seo, Eunsol Choi, and Luke Zettlemoyer. Zero-shot relation extraction via reading comprehension. arXiv preprint arXiv:1706.04115, 2017. Haochuan Li, Alexander Rakhlin, and Ali Jadbabaie. Convergence of adam under relaxed assumptions. Advances in Neural Information Processing Systems, 36:5216652196, 2023. Jiaxiang Li and Mingyi Hong. note on the convergence of muon and further. arXiv e-prints, pp. arXiv 2502, 2025. Jingyuan Liu, Jianlin Su, Xingcheng Yao, Zhejun Jiang, Guokun Lai, Yulun Du, Yidao Qin, Weixin Xu, Enzhe Lu, Junjie Yan, et al. Muon is scalable for llm training. arXiv preprint arXiv:2502.16982, 2025. Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. Locating and editing factual associations in gpt. Advances in neural information processing systems, 35:1735917372, 2022a. Kevin Meng, Arnab Sen Sharma, Alex Andonian, Yonatan Belinkov, and David Bau. Mass-editing memory in transformer. arXiv preprint arXiv:2210.07229, 2022b. Eshaan Nichani, Jason Lee, and Alberto Bietti. Understanding factual recall in transformers via associative memories. arXiv preprint arXiv:2412.06538, 2024. Antonio Orvieto, Samuel Smith, Albert Gu, Anushan Fernando, Caglar Gulcehre, Razvan Pascanu, and Soham De. Resurrecting recurrent neural networks for long sequences. In International Conference on Machine Learning, pp. 2667026698. PMLR, 2023. Yan Pan and Yuanzhi Li. Toward understanding why adam converges faster than sgd for transformers. arXiv preprint arXiv:2306.00204, 2023. Thomas Pethick, Wanyun Xie, Kimon Antonakopoulos, Zhenyu Zhu, Antonio Silveti-Falls, and Volkan Cevher. Training deep learning models with norm-constrained lmos. arXiv preprint arXiv:2502.07529, 2025. Olivier Roy and Martin Vetterli. The effective rank: measure of effective dimensionality. In 2007 15th European signal processing conference, pp. 606610. IEEE, 2007. Naoki Sato, Hiroki Naganuma, and Hideaki Iiduka. Analysis of muons convergence and critical batch size. arXiv preprint arXiv:2507.01598, 2025. Ishaan Shah, Anthony Polloreno, Karl Stratos, Philip Monk, Adarsh Chaluvaraju, Andrew Hojel, Andrew Ma, Anil Thomas, Ashish Tanwer, Darsh Shah, et al. Practical efficiency of muon for pretraining. arXiv preprint arXiv:2505.02222, 2025. Wei Shen, Ruichuan Huang, Minhui Huang, Cong Shen, and Jiawei Zhang. On the convergence analysis of muon. arXiv preprint arXiv:2505.23737, 2025. 12 Chongjie Si, Debing Zhang, and Wei Shen. Adamuon: Adaptive muon optimizer. arXiv preprint arXiv:2507.11005, 2025. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Bhavya Vasudeva, Puneesh Deora, and Christos Thrampoulidis. On generalization of spectral gradient descent: case study on imbalanced data. In High-dimensional Learning Dynamics 2025, 2025. David Willshaw, Peter Buneman, and Hugh Christopher Longuet-Higgins. Non-holographic associative memory. Nature, 222(5197):960962, 1969. Greg Yang, James Simon, and Jeremy Bernstein. spectral condition for feature learning. arXiv preprint arXiv:2310.17813, 2023. Yushun Zhang, Congliang Chen, Naichen Shi, Ruoyu Sun, and Zhi-Quan Luo. Adam can converge without any modification on update rules. Advances in neural information processing systems, 35:2838628399, 2022. Yushun Zhang, Congliang Chen, Tian Ding, Ziniu Li, Ruoyu Sun, and Zhiquan Luo. Why transformers need adam: hessian perspective. Advances in neural information processing systems, 37:131786131823, 2024a. Zeyu Zhang, Akide Liu, Ian Reid, Richard Hartley, Bohan Zhuang, and Hao Tang. Motion mamba: Efficient and long sequence motion generation. In European Conference on Computer Vision, pp. 265282. Springer, 2024b. Dongruo Zhou, Jinghui Chen, Yuan Cao, Ziyan Yang, and Quanquan Gu. On the convergence of adaptive gradient methods for nonconvex optimization. arXiv preprint arXiv:1808.05671, 2018. Fangyu Zou, Li Shen, Zequn Jie, Weizhong Zhang, and Wei Liu. sufficient condition for convergences In Proceedings of the IEEE/CVF Conference on computer vision and pattern of adam and rmsprop. recognition, pp. 1112711135, 2019."
        },
        {
            "title": "A Notations",
            "content": "Let [N ] for the set {1, . . . , }. For matrix RdN , Xi is its i-th column and X:,1 is its last column. IK,K is the identity matrix, IK is all-ones vector and JK,K is the all-ones matrix. denotes the element-wise product."
        },
        {
            "title": "B Related Works",
            "content": "Adam, proposed by Kingma & Ba (2015), was designed to make GD adaptive to the complex optimization landscape of neural networks. Existing works analyze Adam from two primary perspectives: online optimization and feature learning. The online convex optimization view focuses on Adams properties when optimizing convex or non-convex loss functions. From this perspective, Chen et al. (2019) and Zhou et al. (2018) derive non-convex convergence results for Adam, and series of subsequent works continuously relaxed the required assumptions for Adams convergence while tightening its convergence rate. For instance, Zou et al. (2019) proposes set of easy-to-verify sufficient conditions for Adams update rules to guarantee convergence. Defossez et al. (2020) derives the tightest dependency on the heavy ball momentum parameters. More recently, Zhang et al. (2022) demonstrates that Adam can converge without modification of its procedures, and Li et al. (2023) relaxes the smoothness assumption by employing an adaptive Lipschitz constant for gradients. The feature learning view, on the other hand, highlights the relationship between deep learning characteristics and Adam, focusing more on how Adams mechanisms influence the properties of learned features within deep networks. For example, Pan & Li (2023) examines the sharpness of GD and Adam and relates Adams superiority to its low sharpness. Kunstner et al. (2024) finds that Adam is better at learning heavy-tailed distributions than GD. Furthermore, Zhang et al. (2024a) shows that Adam is adaptive to heterogeneous Hessian structures, thus optimizing faster than GD. More literature on Adam is included in the survey by Abdulkadirov et al. (2023). Muon, proposed by Jordan et al. (2024), applies spectral normalization of the gradient to update parameters. At high level, Muon can be understood as steepest descent with respect to the matrix operator norm (Bernstein & Newhouse, 2024). Alternatively, it can be viewed as maximizing the feature update subject to parameter update constraint (Yang et al., 2023). Experiments show that Muon consistently outperforms Adam across diverse model sizes and architectures, including dense transformers and Mixtureof-Experts (Liu et al., 2025; Jordan et al., 2024). Building on this, Si et al. (2025) introduces an adaptive variant of Muon. To explain its advantages, Lau et al. (2025) introduces unifying preconditioning framework, distinguishing optimizers that address curvature anisotropy (like Adam) from those that address gradient anisotropy (like Muon), and proposes generalized optimizer class named PolarGrad. Sato et al. (2025) and Shah et al. (2025) examine the critical batch size of Muon, while other works analyze its convergence in convex and non-convex settings (Li & Hong, 2025; An et al., 2025; Kovalev, 2025; Pethick et al., 2025; Shen et al., 2025). Concurrently, Vasudeva et al. (2025) study Muon on shallow ViTs for computer vision, grounding their results for gradient descent and Muon in linear regression. In contrast, we investigate Muon in the context of LLMs, focusing on its effects on associative memory in next-token prediction. Associative Memories have long history in neural network design and knowledge storage (Hopfield, 1982; Kohonen, 2009; Willshaw et al., 1969). They have inspired architectures capable of retaining long histories, including RNNs (Orvieto et al., 2023) and Mamba (Zhang et al., 2024b). With the success of transformers, recent work has examined them through the lens of associative memories. Geva et al. (2020) and Dai et al. (2021) show that feed-forward modules store knowledge in Wout, while Bietti et al. (2023) demonstrates that the attention output matrix WO also encodes associations of knowledge. Building on these findings, series of works edit knowledge directly by modifying these weights (Meng et al., 2022b; Fang et al., 2024). Beyond empirical results, theoretical analyses have further clarified how transformers leverage associative memories: Bietti et al. (2023) conducts dynamic analysis of memory formation, while Nichani et al. (2024) constructs explicit associative memory mechanisms in both attention and feed-forward modules."
        },
        {
            "title": "C Steepest Descent View Understanding Muon and Adam",
            "content": "Bernstein & Newhouse (2024) showed that many popular deep learning optimizers can be understood through the unifying framework of steepest descent, once their exponential moving averages (EMAs) are disabled. This perspective shifts the focus from heuristic or second-order motivations to more fundamental, geometric view: the choice of an optimizer is equivalent to choosing specific norm to measure the size of the weight update. The Steepest Descent Framework. The core idea is to find weight update, w, that minimizes local quadratic approximation of the loss function. This is formulated as the following optimization problem: = argmin (cid:20) gw + (cid:21) , λ 2 where is the gradient of the loss, λ > 0 is sharpness parameter that controls the step size, and is chosen norm. The solution to this problem can be expressed as: = η d, where the step size η = and the update direction = arg maxt=1 gt. Here, denotes the dual λ norm of (defined as = supx1 yx). The key insight is that different choices of the norm lead to different update directions d, recovering the update rules of well-known optimizers. Muon as Steepest Descent under Spectral Norm. The update rule of the Muon optimizer is derived by applying the steepest descent framework to weight matrices equipped with the spectral norm, denoted in the paper as the ℓ2ℓ2 operator norm (defined as its largest singular value, Aℓ2ℓ2 = σmax(A) = supx2=1 Ax2). For gradient matrix G, the problem is to find the update that solves: = argmin (cid:20) G, WF + ℓ2ℓ2 (cid:21) . λ 2 The solution to this problem is directly determined by the Singular Value Decomposition (SVD) of the gradient, = UΣV. The resulting update direction, which maximizes alignment with the gradient under the spectral norm constraint, is shown to be UV. The corresponding dual norm of the gradient, , ℓ2ℓ2 which scales the step size, is found to be tr(Σ), the sum of the singular values. Combining these components yields the final steepest descent update rule: = tr(Σ) λ UV. This demonstrates that Muons core operation is principled descent step where the singular vectors of the gradient determine the direction, and the sum of its singular values scales the step size. Adam as Steepest Descent under ℓ Norm. Adam can be understood as steepest descent on the flattened parameter vector when the space is equipped with the vector infinity norm (ℓ) (defined as the maximum absolute value of its elements, = maxi xi). For gradient vector g, the optimization problem is to find the update that solves: = argmin (cid:20) gw + w2 (cid:21) . λ 2 The update direction that maximizes alignment with the gradient under the infinity norm constraint , which scales the xi). Combining is the sign of the gradient, sign(g). The corresponding dual norm of the gradient, step size, is the ℓ1 norm, g1 (the sum of the absolute values of its elements, x1 = (cid:80) these components yields the final steepest descent update rule: This reveals that Adams fundamental operation corresponds to descent step where each parameter moves with the same magnitude, determined only by its gradients sign. = g1 λ sign(g). Detailed Discussion of the Theorem 4.3 The proof of Theorem 4.3 is provided in Appendix G. We now explain the results for the three optimizers separately. For GD, the quantity r(α, β) 1 measures the imbalance of the data distribution: r(α, β) = 1 corresponds to perfectly balanced data, while r(α, β) 1 indicates severe imbalance. The results show that if one set of (s, r, o) triplets is learned with the correct-class probability [fW (Ek)]k of at least 1ϵ, then there exists another triplet whose correct-class probability is O(ϵr(α,β)K r(α,β)1). Thus, GD is highly sensitive to data imbalance: as the training distribution becomes more imbalanced, the dispersion of correct-class probabilities across items increases, i.e., the maximal probability gap (W ) grows and mink[K][fW (Ek)]k decreases. This mirrors the message in Figure 4(b), 4(c), and Figure 3(d) in Section 3.3. In contrast, Muon learns in balanced fashion, unaffected by data imbalance for any embeddings (cid:101)E and E. Our results show that when the best-learned triplet achieves correct-class probability of at least 1 ϵ, the worst-learned triplet has comparable correct-class probability at least 1 ϵ(1 + O(log K/K)). This justifies Observation 3. Furthermore, consistent with Observation 2, Muons update GMuon rule allocates equal strength to all update directions; equivalently, the singular values of GMuon(W0) are nearly identical. Our analysis shows that Adams performance is unstable with respect to the embeddings (cid:101)E and E, as reflected by the large error bars in Observations 2 and 3. Adams element-wise normalization disrupts the inherent matrix structure of the gradient. When embeddings of different triplets have disjoint supports (e.g., (cid:101)E = = IK,K), Adam can optimize parameters in balanced manner. However, when embeddings overlap, the sign operator in Adam can introduce imbalance. In particular, the worst-optimized triplet may then have correct-class probability O(ϵ0.7K 0.3). These exponents (0.3, 0.7) are intrinsic to Adams update under certain embeddings and are independent of α or β. Moreover, the Adam update GSignGD(W0) exhibits pronounced spectral decayfor example, its smallest singular value can be less than 25% of the largestunlike the nearly uniform singular values of Muon. This spectral decay explains the poor isotropy reported in Observation 2."
        },
        {
            "title": "E Experimental Details",
            "content": "E.1 Experimental Details of Training on FineWeb When training 160M models on FineWeb, we disable weight decaying and Nesterov acceleration for both Adam and Muon. Thus, we only compare their performance along. To set the learning rate, we conduct grid search on 1 101, 5 102, 2 102, 1 102, 5 103, 2 103, 1 103, 5 104, 2 104. When conducting the Independent Blocks and Combined Configuration experiments in Section 3.1, we just fix the learning rate of Muon. We set β1 = 0.8, β2 = 0.95 for Adam and set β = 0.95 for Muon. When training 0.7B models on FineWeb, we conduct grid search of learning rate on 2 103, 1 103, 5 104, 2 104. We set β1 = 0.9, β2 = 0.95 for Adam and set β = 0.95 for Muon. We do not adopt group query attention in the structure; thus, the parameter sizes of WQ, WK, WV , and WO are the same. We conduct experiments on 8 A100 with 80 GB memory. E."
        },
        {
            "title": "Isotropicity Metrics Explanations",
            "content": "Normalized SVD Entropy. This metric, adapted from Alter et al. (2000), quantifies the uniformity of the singular energy distribution. higher entropy value indicates more isotropic matrix where energy is distributed evenly across many directions. It is defined as the Shannon entropy of the distribution q, normalized by the maximum possible entropy: Hnorm(σ) = 1 (cid:80)n i=1 qi log qi. log Effective Rank. The effective rank (Roy & Vetterli, 2007) provides continuous measure of the number of significant singular dimensions used by the matrix. It is calculated as the exponentiation of the unnormalized Shannon entropy, which corresponds to the perplexity of the energy distribution: eRank(σ) = exp ( (cid:80)n i=1 qi log qi). Top-k Energy Fraction. This metric measures the concentration of energy within the Top-k principal singular components. Assuming the singular values are sorted in descending order (σ1 σ2 σn), it is the cumulative sum of the first energy fractions: TopEk(σ) = (cid:80)k (cid:80)n . i=1 σ2 j=1 σ2 16 Eigenvalue Quantile Ratio. To measure the spread of the singular energy distribution while being robust to extreme outliers, we compute the ratio of the 75th percentile (Q3) to the 25th percentile (Q1) of the eigenvalues {σ }n i=1: Q75/25(σ) = Q3({σ2 }) }) . Q1({σ2 E.3 Dataset Details for the Heavy-Tail Knowledge Task Following Allen-Zhu & Li (2024), the foundation of our knowledge-intensive task is set of question-answering (QA) pairs derived from synthetically generated biographies. Each biography is constructed from combination of seven key attributes: name, birthdate, birthplace, educational institution, major, employer, and workplace. The attribute values are sampled from predefined lists, creating diverse set of entities. Specifically, we use approximately 400 first names, 1000 surnames, 300 educational institutions, 100 majors, and 300 employers. Each synthetic individual is assigned unique combination of these attributes, forming distinct biographical profile. For example, generated biography might look like this: Ashton Hilda Older has birthday that falls on February 01, 2063. Miami, FL is the birthplace of he. He is an alumnus of Saddleback College. He has General Literature education. He works closely with BlockFi. For professional growth, he chose to relocate to Jersey City. This text is generated by combining the structured attributes (name, date, location, etc.) with set of sentence templates. predefined set of QA templates is then used to generate the final training data. These templates contain placeholders corresponding to the biographical attributes. By formatting these templates with the information from each synthetic biography, we generate collection of concrete QA pairs for each entity. For example, for the entity Ashton Hilda Older, we can generate the following six QA pairs: 1. What is the birth date of Ashton Hilda Older? 4. What major did Ashton Hilda Older study? Answer: February 01, 2063. Answer: General Literature. 2. What is the birth city of Ashton Hilda Older? 5. Which company did Ashton Hilda Older work Answer: Miami, FL. 3. Which university did Ashton Hilda Older study? for? Answer: BlockFi. Answer: Saddleback College. 6. Where did Ashton Hilda Older work? Answer: Jersey City. To evaluate the optimizers on knowledge-intensive task with data imbalance, we constructed synthetic dataset where the number of question-answering (QA) samples per class follows power-law distribution. This is designed to simulate real-world scenarios where few entities (the head) are highly represented, while most entities (the tail) are rare. The generation process is controlled by an integer parameter, m. The classes are organized into + 1 groups, indexed from = 0 to m. Group contains Ng classes, where N0 = 1 and Ng = 2g1 for > 0. Each class within group is allocated specific number of selections, Sg = 2mg. For each selection, we generate nqa unique QA pairs by formatting templates with biographical information corresponding to that class. Thus, the total number of QA samples for any given class in group is Sg nqa. This structure ensures that the single class in group 0 has the most samples, while the numerous classes in group have the fewest. In our experiment, we set the parameters to = 15 and nqa = 6. This results in dataset with total of 215 = 32, 768 classes. The number of samples per class ranges from 196, 608 for the head class (group 0) down to just 6 for each of the 16, 384 tail classes (group 15). The final distribution is visualized in Figure 3(a) in the main text. 17 To evaluate the models performance on this pure memory task, we measure the First Token Accuracy (FTA) on the answers. This metric assesses the models ability to correctly recall information by checking if the first generated token of the answer matches the ground truth. Furthermore, to understand how optimizers handle data imbalance, we analyze the FTA across different data frequency groups, from highfrequency (head) to low-frequency (tail) data. E.4 Experimental Details About Angles Between Associative Memories Embeddings Following Fang et al. (2024), we analyze the associative memories in the FFN modules. To obtain Ei, we use the activations within the feed-forward modules, and for (cid:101)Ei, we take the corresponding module outputs. We evaluate knowledge items from two widely used datasets: Counterfact (Meng et al., 2022a) and ZsRE (Levy et al., 2017). Results on Counterfact are shown in Figure 4(a), while results on ZsRE are provided in Figure 10 in Appendix F.6. E.5 Experimental Details of One-layer Models We set the hyperparameters as = = 999, α = 0.8, β = 0.2. For the support-decoupled setting, we set and (cid:101)E as identity matrices. For the support-coupled setting, we set and (cid:101)E according to the construction presented in the proof of Theorem 4.3 in Appendix G."
        },
        {
            "title": "F Additional Experimental Results",
            "content": "F.1 MaxLogit per Layer on the 160M NanoGPT model via Muon Optimizer In this subsection, we present the MaxLogit values for each layer of the 160M NanoGPT model trained using the Muon Optimizer. Following Gemma 3 (Kamath et al., 2025), we introduce RMSNorm to the attention mechanism. The attention mechanism in our model is defined as follows: = softmax( (cid:101)Q (cid:101)K )V, (cid:101)Q = RMSNorm(Q), (cid:101)K = RMSNorm(K) where RMSNorm is defined as RMSNorm(x) = defined as: (cid:80)d 1 i=1 x2 , with being the dimension of x. MaxLogit is Smax = max i,j (cid:101)qi (cid:101)kj representing the maximum value in the attention scores before softmax normalization. The MaxLogit values for each layer are summarized in Table 1. Table 1: MaxLogit values per layer on the 160M NanoGPT model via Muon Optimizer. Layer 2 3 4 5 6 8 9 10 11 12 MaxLogit 8.396 6.880 6.009 7.676 6.349 5.890 7.688 6.314 6.205 5.613 6.033 6. Recent reports Team et al. (2025) have shown potential MaxLogit explosion phenomenon, where Smax grows steadily (often near-linearly) during training, leading to overly peaked attention, gradient spikes, and degraded optimizer comparisons. We included this measurement to rule out the possibility that Muons comparatively smaller impact on the QK blocks (relative to VO/FFN) is simply due to suppressing such an instability. In our 160M setting, with RMSNorm applied to both and (following Gemma 3), the per-layer MaxLogit values remain moderate and show no runaway growth. Thus, for this model size and normalization scheme, differences in Muons effectiveness across components cannot be attributed to avoiding MaxLogit explosion in attention. 18 F.2 Scaling to the 0.7B NanoGPT Model To evaluate the scalability of our findings, we extend our experiments from the 160M model to larger 0.7B parameter model. This section presents the results of this scaled-up analysis, examining whether the advantages of Muon observed in the smaller model persist at larger scale. (a) Non-gated FFN (b) Gated FFN Figure 5: Validation loss comparison on the 0.7B NanoGPT model. (a) Combined configuration with nongated feed-forward networks.(b) Combined configuration with gated feed-forward networks. Figure 6: Spectral Dynamics of Weight Matrices During Training on the 0.7B NanoGPT model. 19 Figure 7: Spectral Dynamics of Weight Matrices During Training on the 0.7B NanoGPT model with the Gated FFN. Figure 5 shows the validation loss curves for various optimizer configurations. Consistent with our findings on the 160M model, applying Muon to all components achieves the lowest validation loss, outperforming Adam baseline. The hybrid experiments further reinforce our earlier conclusions: applying Muon to only the VO and FFN components yields performance nearly identical to that of the full Muon optimizer, whereas applying it only to the QK components offers little advantage over Adam. The spectral dynamics, shown in Figures 6 and 7, also align with Observation 2. For the VO, Win, Wgate (in model with Gated FFN) and Wout matrices, Muon leads to higher SVD entropy and eRank compared to Adam, indicating that it encourages the learning of more distributed, higher-dimensional representations. Overall, these results demonstrate that the benefits of Muon and the underlying mechanisms scale to larger models. F.3 Additional Results about Spectral Dynamics of Transformer Weight Matrices During Training To complement the main-text analysis  (Fig. 2)  , we also evaluate spectral dynamics during training for the 160M NanoGPT model with both non-gated and gated feed-forward networks  (Fig. 8)  . The analysis includes Win for both configurations, as well as the gate matrix Wgate for the gated version. The conclusions are consistent across all three matrices and mirror the non-gated setting: with Muon, SVD entropy and eRank increase, while Top-k energy and the Q75/25 ratio decrease, consistent with Observation 2 in the main text. 20 (a) Win (Non-Gated FFN) (b) Win (Gated FFN) (c) Wgate Figure 8: Spectral Dynamics of FFN Weight Matrices During Training on the 160M NanoGPT model. Each panel reports four metrics characterizing singular value distributions: SVD entropy, Top10E, eRank, and Q75/Q25 ratio. The subplots correspond to different weight matrices: (a) Win (non-gated), (b) Win (gated), and (c) Wgate (gated). F.4 Detailed Experiment Results about Heavy-Tail Imbalance Knowledge Task To complement the qualitative trends shown in Section 3.3  (Fig. 3)  , we report the exact First Token Accuracy (FTA) for selected tail groups at three training checkpoints (2k, 5k, 10k steps). We focus on groups = 11, 13, 15, which represent increasingly rare (midtail, tail, extreme tail) frequency bands in the power-law distribution (recall that larger implies fewer samples per class). The tables contrast full Muon, Adam, SGD+Momentum, and two hybrid configurations (Muon applied only to VO&FFN or only to QK). The numbers highlight: (i) Muons rapid convergence on rare groups (already strong by 2k, near-saturated by 5k), (ii) Adams persistent headtail gap, and (iii) the dominant contribution of applying Muon to VO&FFN for tail generalization (the VO&FFN hybrid closely tracks full Muon, whereas the QK-only hybrid lags). These quantitative results substantiate Observation 3 that Muon delivers more balanced learning. Table 2: Heavy-tail knowledge task: Group performance by optimizer (2,000 steps) Group 11 13 15 Optimizer Muon 0.854 0.029 0.386 0.029 0.140 0.027 Adam 0.312 0.043 0.146 0.015 0.090 0.031 SGD+Mom. Muon(VO, FFN) Muon(QK) 0.472 0.041 0.814 0.022 0.156 0.037 0.154 0.032 0.256 0.030 0.120 0.012 0.086 0.037 0.114 0.023 0.082 0.013 Table 3: Heavy-tail knowledge task: Group performance by optimizer (5,000 steps) Group 11 13 15 Optimizer Muon 0.996 0.006 0.964 0.023 0.320 0.028 Adam 0.936 0.039 0.298 0.074 0.110 0.027 SGD+Mom. Muon(VO, FFN) Muon(QK) 0.970 0.007 0.992 0.005 0.314 0.021 0.354 0.032 0.934 0.015 0.148 0.013 0.118 0.019 0.254 0.026 0.084 0. 21 Table 4: Heavy-tail knowledge task: Group performance by optimizer (10,000 steps) Group 11 13 15 Optimizer Muon 1.000 0.000 1.000 0.000 0.976 0. Adam 1.000 0.000 0.890 0.042 0.264 0.048 SGD+Mom. Muon(VO, FFN) Muon(QK) 1.000 0.000 1.000 0.000 0.422 0.023 0.940 0.034 0.998 0.002 0.294 0.013 0.286 0.039 0.954 0.021 0.126 0.021 F.5 Additional Experiment Results about Heavy-Tail Imbalance Knowledge Task with Gated Feed-Forward Networks This subsection complements the main heavy-tail results in Section 3.3 by studying the gated feed-forward networks (Gated FFN) variant. We follow the same presentation order as in the main text: first an overview figure (sample distribution and learning curves under different optimizers), then tables reporting the exact First-Token Accuracy (FTA) for tail groups {11, 13, 15} at three training checkpoints (2k, 5k, 10k steps). The findings mirror the non-gated setting: (i) full Muon consistently outperforms Adam and SGD+Momentum on rare classes and reaches high accuracy earlier; (ii) the VO&FFN-hybrid (Muon applied to VO and FFN while Adam is used for QK) closely tracks full Muon, indicating that VO&FFN are the primary levers for tail generalization; (iii) the QK-only hybrid offers limited gains. Overall, the gated FFN does not change the qualitative conclusions about where Muon helps most. See Fig. 9 and Tables 57 for details. (a) Sample/class (b) Muon (c) Adam (d) SGD+Momentum (e) Muon(VO, FFN)/Adam(QK) (f) Muon(QK)/Adam(VO,FFN) Figure 9: Performance comparison of different optimizers on heavy-tailed knowledge task with gated feedforward networks. (a) The distribution of samples per class follows power law. (b-d) Performance of Muon, Adam, and SGD+Momentum optimizers. (e) Muon (VO, FFN)/Adam (QK). (f) Muon (QK)/Adam (VO, FFN). 22 Table 5: Heavy-tail knowledge task with the Gated FFN: Group performance by optimizer (2,000 steps) Group 11 13 15 Optimizer Muon 0.896 0.009 0.478 0.034 0.178 0. Adam 0.214 0.063 0.116 0.030 0.086 0.013 SGD+Mom. Muon(VO, FFN) Muon(QK) 0.330 0.042 0.892 0.021 0.146 0.018 0.140 0.019 0.458 0.037 0.110 0.007 0.090 0.020 0.166 0.017 0.074 0.009 Table 6: Heavy-tail knowledge task with the Gated FFN: Group performance by optimizer (5,000 steps) Group 11 13 15 Optimizer Muon 0.998 0.002 0.990 0.010 0.510 0.039 Adam 0.928 0.024 0.216 0.052 0.092 0.015 SGD+Mom. Muon(VO, FFN) Muon(QK) 0.960 0.032 0.990 0.010 0.252 0.016 0.290 0.046 0.968 0.028 0.156 0.024 0.098 0.013 0.468 0.016 0.080 0.016 Table 7: Heavy-tail knowledge task with the Gated FFN: Group performance by optimizer (10,000 steps) Group 11 13 Optimizer Muon 1.000 0.000 1.000 0.000 0.994 0.006 Adam 0.998 0.002 0.948 0.027 0.244 0.085 SGD+Mom. Muon(VO, FFN) Muon(QK) 1.000 0.000 1.000 0.000 0.322 0.011 0.946 0.026 1.000 0.000 0.304 0.017 0.274 0.042 0.990 0.010 0.148 0.015 F.6 Additional Results about Angles Between Associative Memories Embeddings Figure 10: Average angles between es or eo for items in ZsRE at layers 5, 10, 15, 20, 25 of Llama3-8b-instruct. Proof of Theorem 4.3 We separately derive the results for GD, Muon, and Adam in the following proof. For all of them, we define ηϵ opt = inf (cid:110) η (cid:12) (cid:12) (cid:12) 1 max k[K] (cid:2)fW (Ek)(cid:3) (cid:111) . ϵ, where = W0 η Gopt(W0) (G.1) The quantity ηϵ less than ϵ. From the definition, we have that opt represents the minimal step size for at least one triplet to be learned with error probability ϱϵ opt min k[K] (cid:2)fηϵ optGopt(Ek)(cid:3) k. Step 1: Calculations of GD. We define the score of k-th object for the k-th subject-relation pair with the parameter as s(k, k, ) = At W0 = 0do,ds , we have that kW Ek) exp( (cid:101)E k=1 exp( (cid:101)E Ek) (cid:80)K . Proposition I.1 shows that the gradient is s(k, k, W0) ="
        },
        {
            "title": "1\nK",
            "content": "for all k, [K]. L(W0) = α (cid:101)E1:LE 1:L + 1 α (cid:101)EL+1:KE L+1:K α LK (cid:101)EJK,LE 1:L 1 α (K L)K (cid:101)EJK,KLE L+1:K. (G.2) From the gradient, it is easy to see that the first triplets (s, r, o) share the same learning behavior, and the last triplets also share the same behavior. Thus, we calculate the results for = 1 and = + 1. The calculation for = 1 depends on evaluating its score function, which takes the form η (cid:101)E k[W L(W0)]E1, for {1, . . . , K}. Based on the gradient in (G.2) and the orthonormality of the embeddings, it evaluates to α/L for the case = 1, and to 0 for all = 1. This leads to numerator in the softmax score of exp(η α/L), while the denominator sum consists of one term exp(η α/L) and 1 terms of exp(0) = 1. similar calculation for = + 1 shows that the argument of the exponent for the correct object, η (cid:101)E L+1[W L(W0)]EL+1, evaluates to η (1 α)/(K L). By defining γ1 = α/(βK) and γ2 = (1 α)/((1 β)K) based on the problem setup (L = βK), we have that (cid:2)fηW L(E1)(cid:3) 1 = exp(ηγ1) exp(ηγ1) + 1 , (cid:2)fηW L(EL+1)(cid:3) L+1 = exp(ηγ2) exp(ηγ2) + , where γ1 and γ2 are defined as Then we derive that γ1 = α βK , γ2 = 1 α (1 β)K . ηϵ GD = 1 max{γ1, γ2} log (cid:2)(ϵ1 1)(K 1)(cid:3). (G.3) To calculate the desired quantity, we define the quantity r(α, β) to evaluate the balance of data as r(α, β) = min{γ1/γ2, γ2/γ1} = min (cid:26) α(1 β) β(1 α) , β(1 α) α(1 β) (cid:27) . Some basic calculations show that (cid:2)fηϵ 1 min k[K] GDGGD(Ek)(cid:3) = ϵ ϵ + (1 ϵ)r(α,β)ϵ1r(α,β)(K 1)r(α,β)1 . (G.4) When < 1, with the fact that 1 x+1 = 1 + O(x2) as 0, we have that min k[K] (cid:2)fηϵ GDGGD(Ek)(cid:3) = O(ϵr(α,β)K r(α,β)1). 24 Thus, the proof for the convergence of GD has been established. Step 2: Calculations of Muon. For Muon, we first calculate the SVD of the gradient. In fact, we can write the gradient in Eqn. (G.2) as L(W0) = (cid:101)E diag (cid:26) (cid:18) α IL, 1 α IKL (cid:19)"
        },
        {
            "title": "1\nK",
            "content": "IK (cid:20) α , 1 α (cid:21)(cid:27) I KL = (cid:101)EXE. The SVD calculation of = ΣV can be directly derived from Proposition I.3. Thus, the SVD of the gradient is L(W0) = ( (cid:101)E )Σ(E ). The update quantity GMuon(W0) = U0norm(Σ0)V 0 of Muon is GMuon(W0) = (cid:101)E1:LRL,L1R 1:L + (cid:101)EL+1:KRKL,KL1R L,L1E 1 K(cid:2)α2(K L)3 + (1 α)2L3(cid:3) + (cid:113) (cid:0)(K L) (cid:101)E1:LIL (cid:101)EL+1:KIKL (cid:1) KL,KL1E L+1:K (cid:18) (K L)α E 1:L L(1 α) (cid:19) KLE L+1:K 1:L + (cid:101)EL+1:KE = (cid:101)E1:LE (cid:18) (1 β)2α (cid:26) 1 1 λ β + L+1:K (cid:19) (cid:101)E1:LJL,LE 1:L + 1 1 β (cid:18) β2(1 α) λ (cid:19) (cid:101)EL+1:KJKL,KLE L+1:K β(1 α) (cid:101)E1:LJL,KLE L+1:K α(1 β) (cid:101)EL+1:KJKL,LE 1:L (cid:27) , (G.5) where λ = (cid:112)α2(1 β)3 + (1 α)2β3, the matrices RL,L1 and RKL,KL1 are defined in Proposition I.3, and the second equality results from the following facts RL,L1R L,L1 = IL,L 1 ILI , RKL,KL1R KL,KL1 = IKL,KL 1 IKLI KL. Although the gradient is composed of heterogeneous components from (cid:101)E1:L, E1:L and (cid:101)EL+1:K, EL+1:K, we can bound the convergence rate of [fηGMuon(Ek)]k for any k: an upper (resp. lower) bound is obtained by increasing (resp. decreasing) the coefficient of (cid:101)EkE for = k. In fact, Eqn. (G.5) implies that there exists constant > 0 such that the dynamics of the fastestand slowest-learning triplets are bounded by those along the following two update directions. while decreasing (resp. increasing) that of (cid:101)EkE G+ Muon(W0) = Muon(W0) = (cid:18) (cid:18) 1 + 1 2C 2C (cid:19) ( (cid:101)E1:LE 1:L + (cid:101)EL+1:KE L+1:K) (cid:19) ( (cid:101)E1:LE 1:L + (cid:101)EL+1:KE L+1:K) + C (cid:101)EJK,KE (cid:101)EJK,KE. Concretely, the rate of score increase for the correct object of the k-th triplet, which is given by the term (cid:101)E [GMuon(W0)]Ek in the exponent of the softmax score, is bounded. The rate for the fastest-learning triplet is lower-bounded by the corresponding rate derived from G+ Muon(W0), while the rate for the slowestlearning triplet is upper-bounded by that from Muon(W0) and Muon(W0) to calculate the desired quantity. Following the similar procedures for GD to derive Eqn. (G.4), we have that for any η such that maxk[K] 1 ϵ (where Wη = W0 η GMuon(W0)), the following holds Muon(W0). Thus, we only need to focus on G+ (cid:2)fWη (Ek)(cid:3) 1 min k[K] (cid:2)fWη (Ek)(cid:3) ϵ ϵ + (1 ϵ)r(K)ϵ1r(K)(K 1)r(K)1 , (G.6) 25 where r(K) = (K 2C)/(K + 2C). We further have that (1 ϵ)r(K)ϵ1r(K)(K 1)r(K)1 = (1 ϵ) exp (cid:20) = (1 ϵ) 1 + = (1 ϵ) + ϵ 1 ϵ ϵ 1 ϵ log (cid:18) 4C (cid:18) + 2C 4C + 2C (cid:18) log (cid:19) (cid:18) log , (cid:19)(cid:19) log(K 1) log(K 1) (cid:19) + (cid:19)(cid:21) (cid:18) (log K)2 2 (G.7) where the first equality results from the basic calculations, the second equality results from that exp(x) = 1 + + O(x2) when 0. Combining Eqn. (G.6) and (G.7), we have that ϱϵ Muon 1 ϵ (cid:18) 1 + (cid:18) log (cid:19)(cid:19) . Thus, we prove the desired results for Muon. Step 3: Calculations of Adam. The proof of the results for Adam is conducted under two cases. We will construct different embeddings (cid:101)E and in these two cases. In the first case, we set (cid:101)E = = IK,K. With such embedding and sufficiently large K, we have that GSignGD(W0) = sign(W L(W0)) = 2IK,K JK,K. Under such setting, all triplets share the same dynamic. Thus, we have that ϱϵ SignGD = 1 ϵ. In the second case, we set (cid:101)E and as block-wise diagonal matrices. Here we set the block size as 3, i.e., requiring that mod 3 = 0. Such requirement can be satisfied infinitely often when . Then the sufficient and necessary condition of Assumption 4.1 is that each 3 3 block contains an orthonormal basis. To achieve this, we define the following matrix. R(a, b, c) = cos cos cos sin sin cos cos sin sin cos sin cos cos + cos sin sin cos sin + cos cos sin cos sin sin . cos sin sin sin cos It is obvious that R(a, b, c)R(a, b, c) = I3,3. Then we set (cid:101)E and as (cid:101)E = IK/3,K/3 R(3.638, 2.949, 5.218), = IK/3,K/3 R(1.715, 0.876, 3.098), where is the Kronecker product. With these specifications and sufficiently large K, the Adam update matrix is GSignGD(W0) = IK/3,K/3 + JK/3,K/3 B, where and are specified as = 2 0 0 2 0 2 2 2 2 , = 1 1 1 1 1 1 1 1 . These show that the diagonal block of GSignGD(W0) is + = 1 1 1 1 1 1 1 1 1 . 26 Since the k-th and (k + 3)-th triplets share the same learning dynamics for all [K 3], we focus on the learning dynamics of = 1, 2, 3. We have that R(3.638, 2.949, 5.218) (A + B) R(1.715, 0.876, 3.098) 1.46552253 0.0732561 0.0544114 = 1.0132908 0.11179563 1.00709257 1.26935805 1.54147329 0.89611102 , R(3.638, 2.949, 5.218) R(1.715, 0.876, 3.098) = 0.19288146 1.24460331 1.4058011 0.20112175 1.2977753 1.46585978 0.12780259 0.82466989 0. . From the last columns of these two matrices, following the similar procedures for GD to derive Eqn. (G.3), we have that ηϵ SignGD 1 1.541 + 0.930 log (cid:2)(ϵ1 1)(K 1)(cid:3) = 1 2.471 log (cid:2)(ϵ1 1)(K 1)(cid:3). Then, from the first columns of these matrices, we have that 1 min k[K] (cid:2)fηϵ SignGDGSignGD(Ek)(cid:3) ϵ ϵ + (1 ϵ)rϵ1r(K 1)r1 , where = 1.466+0.202 2.471 = 1.668 2.471 . Thus, we have that SignGD O(ϵrK r1) O(ϵ0.7K 0.3). ϱϵ Then we calculate the singular values of GSignGD(W0). We define the eigen vectors of IK,K as (cid:101)U , i.e., (cid:101)U IK/3,K/3 (cid:101)U = diag(K/3, 0 , 0). Using the orthogonal invariance of singular values, GSignGD(W0) shares the singular values with the following matrix ( (cid:101)U I3,3)(cid:0) GSignGD(W0)(cid:1)( (cid:101)U I3,3) = IK/3,K/3 + ( (cid:101)U IK/3,K/3 (cid:101)U ) = diag(A KB/3, A, , A). Thus, the singular values of are also the singular values of GSignGD(W0). We have that (cid:0)GSignGD(W0)(cid:1) (cid:0)GSignGD(W0)(cid:1) σmin(A) σmax(A) σmin σmax 25%. Thus, we conclude the proof of Theorem 4.3. Proof of Theorem 4.4 The proof of Theorem 4.4 takes two steps. In the first step, we derive the share form of Wt along the whole optimization trajectory. In the second step, we build the desired results on the basis of step 1. Throughout the proof, we will write Muon as Wt for the ease of presentation. Step 1: Derive the shared forms of Wt and GMuon. We will derive the forms of Wt along the optimization trajectory via the induction method. We first state our hypothesis and then prove it. Hypothesis 1 . For any optimization step index [T ], the parameters Wt can be expressed as Wt = (cid:101)EXtE, Xt = Λt + Ct, 27 where Λt and Ct are Λt = diag(at IL, bt IKL), Ct = (cid:20) c11 c21 JL,L c12 JKL,L c22 JL,KL JKL,KL (cid:21) , where at, bt, c11 i, [2]. , c12 , c21 , c22 are real numbers such that (1) at = bt 0, and (2) cij = O(at/K) for When = 0, it is obvious to verify that W0 = 0do,ds satisfying this hypothesis with at = bt = c11 = c21 = c22 = 0. Then we assume that this hypothesis holds for {1, , t}, and we will prove that it holds for + 1. Since Wt+1 = Wt ηt+1Utnorm(Σt)V satisfies the hypothesis. We define the score of k-th object for the k-th subject-relation pair with the parameter as , we need to show that ηt+1Utnorm(Σt)V = c12 s(k, k, ) = kW Ek) exp( (cid:101)E k=1 exp( (cid:101)E Ek) (cid:80)K . According to the symmetry of Wt, we have that s(k, k, Wt) = s(1, 1, Wt) for all L. s(k, k, Wt) = s(K, K, Wt) for all > L. s(k, k, Wt) = s(2, 1, Wt) for all k, L, = k. s(k, k, Wt) = s(K, 1, Wt) for all L, > L. s(k, k, Wt) = s(K 1, K, Wt) for all k, > L, = k. s(k, k, Wt) = s(1, K, Wt) for all > L, L. Thus, Proposition I.1 shows that the gradient of Wt is L(Wt) = (cid:101)E(Γt + Bt)E, where Γt and Bt are defined as (cid:18) α Γt = diag (cid:0)1 + s(2, 1, Wt) s(1, 1, Wt)(cid:1)IL, 1 α s(2, 1, Wt) JL,L Bt = (cid:20) α α s(K, 1, Wt) JKL,L 1α 1α KL s(1, K, Wt) JL,KL KL s(K 1, K, Wt) JKL,KL (cid:21) . (cid:0)1 + s(K 1, K, Wt) s(K, K, Wt)(cid:1)IKL (cid:19) , Thus, Proposition I.2 shows that GMuon(Wt) = (cid:101)E (cid:18) diag(IK) + (cid:20) C11 JL,L C21 JKL,L C22 JKL,KL C12 JL,KL (cid:21) (cid:19) E, where C11 = (cid:101)U1,1 (cid:101)V1,1 + (cid:101)U1,2 (cid:101)V1,2 1 βK , C21 = (cid:101)U2,1 (cid:101)V1,1 + (cid:101)U2,2 (cid:101)V1,2 (cid:112)β(1 β)K , C12 = (cid:101)U1,1 (cid:101)V2,1 + (cid:101)U1,2 (cid:101)V2,2 (cid:112)β(1 β)K , C22 = (cid:101)U2,1 (cid:101)V2,1 + (cid:101)U2,2 (cid:101)V2,2 1 (1 β)K . where (cid:101)U , (cid:101)V R22 are the orthonormal matrices defined in Proposition I.2. Since Wt+1 = Wtηt+1GMuon(Wt), it is obvious that at+1 = bt+1. The orthonormality of (cid:101)U and (cid:101)V implies that (cid:101)Ui,j, (cid:101)Vi,j 1. Thus, we have (cid:101)U1,1 (cid:101)V1,1 + (cid:101)U1,2 (cid:101)V1,2 1 βK = (cid:19) . (cid:18) 1 28 This further implies that c1,1 t+1 = O(at+1/K). The proofs for other cij t+1 are similar. This completes the proof. Step 2: Establish the convergence results. We note that this analysis is very similar to the proof of Muon in Theorem 4.3. Concretely, for Wt, the coefficients at, bt, c11 from multiple-step optimization share the same property with those of the one-step results. It means that there exists constant > 0 such that the dynamics of the fastestand slowest-learning triplets are bounded by those along the following two update directions in only one step. , c22 , c12 , c21 G+ Muon = Muon = (cid:18) (cid:18) 1 + 1 (cid:19) (cid:19) 2C 2C ( (cid:101)E1:LE 1:L + (cid:101)EL+1:KE L+1:K) ( (cid:101)E1:LE 1:L + (cid:101)EL+1:KE L+1:K) +"
        },
        {
            "title": "C\nK\nC\nK",
            "content": "(cid:101)EJK,KE (cid:101)EJK,KE. The remaining analysis is then exactly the same as that of Theorem 4.3. Thus, we conclude the proof of Theorem 4.4."
        },
        {
            "title": "I Supporting Propositions",
            "content": "Proposition I.1. We define the score of k-th object for the k-th subject-relation pair with the parameter as s(k, k, ) = When the parameter is trained with loss kW Ek) exp( (cid:101)E k=1 exp( (cid:101)E Ek) (cid:80)K . L(W ) = (cid:88) k=1 pk log (cid:2)fW (Ek)(cid:3) k, the gradient of is L(W ) = (cid:88) k=1 pk (cid:110)(cid:2)1 s(k, k, )(cid:3) (cid:101)EkE s(k, k, ) (cid:101)EkE (cid:111) . (cid:88) k=k Proof of Proposition I.1. The proof just follows from the basic calculus. Thus, we omit them here. Proposition I.2. Let = Λ + RKK. The matrix Λ = diag(a IL, IKL) is diagonal matrix whose first diagonal elements are and the last elements are with a, > 0. The matrix is block-wise constant matrix defined as = (cid:20) c11 JL,L c21 JKL,L c22 JKL,KL c12 JL,KL (cid:21) . Then = ΣV . Here Σ, V, are defined as follows. All of them can be decomposed into three blocks, each corresponding to subspace. The first subspace is S1 = (cid:26)(cid:20) 0KL (cid:21) (cid:12) (cid:12) (cid:12) (cid:12) xIL = 0, and RL (cid:27) . The dimension of this space is 1. The singular value of corresponding to this subspace is a. The block of columns in both and that forms an orthonormal basis for this subspace is given by (cid:21) (cid:20) RL,L1 0KL,L1 , 29 where the columns of the matrix RL,L1 RL(L1) form an orthonormal basis for the subspace {x RLxIL = 0}. The second subspace is S2 = (cid:26)(cid:20)0L (cid:21) (cid:12) (cid:12) (cid:12) (cid:12) yIKL = 0, and RKL (cid:27) . The dimension of this space is 1. The singular value of corresponding to this subspace is b. The block of columns in both and that forms an orthonormal basis for this subspace is given by (cid:20) 0L,KL1 RKL,KL (cid:21) , where the columns of the matrix RKL,KL1 R(KL)(KL1) form an orthonormal basis for the subspace {y RKLyIKL = 0}. The remaining 2-dimensional subspace is induced by 2 2 matrix defined as = (cid:21) (cid:20)α β δ γ = (cid:101)U diag(s1, s2) (cid:101)V , where the elements of are defined as α = + Lc11, β = (cid:112)L(K L) c12, γ = (cid:112)L(K L) c21, δ = + (K L)c22. The singular values s1, s2 are (cid:115) s1,2 = 2 4 , = α2 + β2 + γ2 + δ2, = (αδ βγ)2. The singular values of in this subspace are s1 and s2. The corresponding right singular vectors (vi) and left singular vectors (ui), which form columns of and respectively, are given by: vi = (cid:101)V1,ie1 + (cid:101)V2,ie2, ui = (cid:101)U1,ie1 + (cid:101)U2,ie2 for = 1, 2, where the vectors e1 and e2 are defined as e1 = (cid:21) (cid:20) 1 IL 0KL , (cid:20) e2 = In summary, the SVD of is 0L 1 KL IKL (cid:21) . Σ = diag(a IL1, IKL1, s1, s2), (cid:21) (cid:21) (cid:20) (cid:20) RL,L1 0KL,L1 (cid:20) (cid:20) RL,L1 0KL,L1 , , (cid:21) (cid:20) 0L,KL1 RKL,KL1 (cid:20) 0L,KL1 RKL,KL1 = = (cid:21) , , v1, v2 (cid:21) (cid:21) . , u1, Proof of Proposition I.2. We first prove the results for S1. For any vector in S1, it is direct to verify that (cid:21) (cid:20) 0KL = a2 (cid:21) (cid:20) 0KL . Thus, the singular value of corresponding to the subspace spanned by the vector [v, 0 the corresponding columns of form an orthonormal basis for S1. For the calculation, we have that KL] is a, and (cid:21) (cid:20) 0KL = (cid:21) (cid:20) 0KL . Thus, the corresponding left singular vectors (columns of U) are identical to the right singular vectors for this subspace. similar calculation can be done for S2. The remaining vectors are orthogonal to both S1 and S2 and thus take the form of vi = p1e1 + p2e2, ui = p3e1 + p4e2 for = 1, 2 with p1, p2, p3, p4 R. By solving the equation Xvi = λvi, we can show that the corresponding singular values and coefficients p1, p2, p3, p4 coincide with those in the SVD of , as can be verified by simple calculations. Thus, we conclude the proof of Proposition I.2. Proposition I.3. Let = [a Then the SVD of = ΣV is that , KL] RK, and = diag(x) 1IK RKK, where a, > 0. (cid:18) Σ = diag IL1, IKL1, (cid:114) = = (cid:20) (cid:20) RL,L1 0KL,L1 (cid:20) (cid:20) RL,L1 0KL,L1 (cid:21) (cid:21) , , (cid:20) 0L,KL1 RKL,KL1 (cid:20) 0L,KL1 RKL,KL1 (cid:19) , , 0 a2 (K L) + b2 (cid:21) , , v1, v2 (cid:21) (cid:21) (cid:21) , u1, u2 . Here, the columns of the matrix RL,L1 RL(L1) form an orthonormal basis for the subspace of vectors in RL orthogonal to IL. Similarly, the columns of RKL,KL1 R(KL)(KL1) form an orthonormal basis for the subspace of vectors in RKL orthogonal to IKL. These correspond to the subspaces S1 and S2 defined as: S1 = (cid:26)(cid:20) 0KL (cid:21) (cid:12) (cid:12) (cid:12) (cid:12) xIL = 0, and RL (cid:27) , S2 = (cid:26)(cid:20)0L (cid:21) (cid:12) (cid:12) (cid:12) (cid:12) yIKL = 0, and RKL (cid:27) . The vectors v1, v2, u1, u2 are (cid:21) (cid:19) (cid:20) 0L IKL v1 = v2 = u1 = u2 = (cid:18) 1 (cid:112)a2(K L) + b2L 1 (cid:112)a2(K L) + b2L (cid:18) (K L) (cid:18) 1 (cid:112)KL(K L) 1 IK. (cid:21) (cid:20) IL 0KL (cid:20) IL 0KL (cid:21) (cid:20) IL 0KL (cid:20) 0L IKL (cid:20) 0L IKL (cid:21) + (cid:21) (cid:19) (cid:21) (cid:19) Proof of Proposition I.3. This proposition is direct corollary of Proposition I.2. The matrix = diag(x) 1IK is an instance of the general form Λ + from Proposition I.2. The diagonal part is Λ = diag(x) = diag(a IL, IKL). The off-diagonal part is = 1IK x. We can write in block form: = 1 (cid:21) (cid:20) IL IKL (cid:2)aI bI KL (cid:3) = 1 (cid:20) aJL,L aJKL,L bJKL,KL bJL,KL (cid:21) . This corresponds to setting the block-wise constants in Proposition I.2 to: Substituting these into the formulas for α, β, γ, δ from Proposition I.2 gives: c11 = a/K, c12 = b/K, c21 = a/K, c22 = b/K. α = + L(a/K) = a(K L)/K β = (cid:112)L(K L)(b/K) γ = (cid:112)L(K L)(a/K) δ = + (K L)(b/K) = bL/K 31 These coefficients define the 2 2 matrix from Proposition I.2 for this specific case. We now analyze this matrix . key observation is that its determinant is zero: det(M ) = αδ βγ = a(K L) bL (cid:18) L(K L) 2 (cid:19) (b)(a) = 0. Since the determinant is zero, one of its singular values must be zero. The other singular value, s1, can be calculated from the squared Frobenius norm (sum of squares of elements), which is also the sum of squared singular values (s 1 + s2 2): 1 + 02 = α2 + β2 + γ2 + δ2 = s2 a2(K L)2 2 + L(K L)b2 + L(K L)a2 2 + b2L2 2 = a2(K L) + b2L . This confirms the singular values stated in the proposition. The singular vectors v1, v2, u1, u2 can be derived by performing the SVD on this specific 2 2 matrix ."
        }
    ],
    "affiliations": [
        "National University of Singapore",
        "Sea AI Lab",
        "University of Minnesota",
        "Yale University"
    ]
}