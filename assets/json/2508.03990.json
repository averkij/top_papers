{
    "paper_title": "Are Today's LLMs Ready to Explain Well-Being Concepts?",
    "authors": [
        "Bohan Jiang",
        "Dawei Li",
        "Zhen Tan",
        "Chengshuai Zhao",
        "Huan Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Well-being encompasses mental, physical, and social dimensions essential to personal growth and informed life decisions. As individuals increasingly consult Large Language Models (LLMs) to understand well-being, a key challenge emerges: Can LLMs generate explanations that are not only accurate but also tailored to diverse audiences? High-quality explanations require both factual correctness and the ability to meet the expectations of users with varying expertise. In this work, we construct a large-scale dataset comprising 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We introduce a principle-guided LLM-as-a-judge evaluation framework, employing dual judges to assess explanation quality. Furthermore, we show that fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) can significantly enhance the quality of generated explanations. Our results reveal: (1) The proposed LLM judges align well with human evaluations; (2) explanation quality varies significantly across models, audiences, and categories; and (3) DPO- and SFT-finetuned models outperform their larger counterparts, demonstrating the effectiveness of preference-based learning for specialized explanation tasks."
        },
        {
            "title": "Start",
            "content": "Are Todays LLMs Ready to Explain Well-Being Concepts? Bohan Jiang1, Dawei Li1, Zhen Tan1, Chengshuai Zhao1, Huan Liu1 1School of Computing and Augmented Intelligence, Arizona State University, USA {bjiang14, daweili5, ztan36, czhao93, huanliu}@asu.edu 5 2 0 2 6 ] . [ 1 0 9 9 3 0 . 8 0 5 2 : r Abstract Well-being encompasses mental, physical, and social dimensions essential to personal growth and informed life decisions. As individuals increasingly consult Large Language Models (LLMs) to understand well-being, key challenge emerges: Can LLMs generate explanations that are not only accurate but also tailored to diverse audiences? High-quality explanations require both factual correctness and the ability to meet the expectations of users with varying expertise. In this work, we construct large-scale dataset comprising 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We introduce principle-guided LLMas-a-judge evaluation framework, employing dual judges to assess explanation quality. Furthermore, we show that finetuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) can significantly enhance the quality of generated explanations. Our results reveal: (1) The proposed LLM judges align well with human evaluations; (2) explanation quality varies significantly across models, audiences, and categories; and (3) DPOand SFT-finetuned models outperform their larger counterparts, demonstrating the effectiveness of preference-based learning for specialized explanation tasks. Introduction Well-being is multi-dimensional concept without single clear and universally accepted definition (Alexandrova 2017). In general, people describe well-being as how people feel and how they function both on personal and social level, and how they evaluate their lives as whole, pointing to complex interplay of mental, physical, and social dimensions (Topp et al. 2015). Gaining clear understanding of well-being concepts is vital for self-reflection, decisionmaking, and personal growth (Diener 2000). Recent Large Language Models (LLMs) are increasingly becoming primary sources of knowledge for individuals seeking insights on well-being and its related concepts (Xiong et al. 2024; Wu et al. 2024). As users turn to LLMs for such guidance, the quality of the explanations they receive plays critical role. However, generating highquality explanations for well-being concept is challenging task. good explanation requires more than just factual accuracy; it must be tailored to the users specific needs and level of expertise (Cho and Choi 2018). Due to the knowledge gap between domain experts and the general public, it Figure 1: Illustration of the explanation capability of LLMs. is difficult to find one-size-fits-all explanation (Keil 2006). For example, layperson requires accessible language, realworld examples, and actionable advice. In contrast, domain expert would prefer technical terminology, critical nuance, and evidence-based substantiation (Jarden and Roache 2023). The unexamined quality of LLM-generated explanations, coupled with the difficulty of the task, presents significant research challenge. In this paper, we pioneer the exploration of the following Research Question: Are Todays LLMs Ready to Explain Complex Well-Being Concepts? To address this, our work provides the first large-scale, systematic investigation of existing LLMs capabilities in explaining well-being concepts. We follow comprehensive research pipeline (Figure 2), beginning with the curation of large-scale dataset, collecting 43,880 explanations from 10 diverse LLMs for 2,194 concepts. Those concepts are chosen from well-being-related literature (Diener 2000; Topp et al. 2015; TOV 2018). We then propose novel evaluation framework that adapts the principle-guided LLMas-a-judge paradigm (Zheng et al. 2023), using two distinct judge models guided with fine-grained, audience-level principles to assess explanation quality. Finally, we investigate pathways for improvement by fine-tuning an open-source model using both Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) (Rafailov et al. 2023) to create specialized, high-performing explanation models. Our empirical results first validate that the principleguided evaluation framework provides reliable judgments that align with human evaluators. Our analysis of the 10 baseline LLMs reveals significant performance disparities, with larger models consistently outperforming smaller ones (Figure 1), but even top models, such as o4-mini (OpenAI 2025) and Gemini-2.5-flash (Comanici et al. 2025), exhibit shared weaknesses in providing practical advice and nuanced analysis. We also find that generating explanations for domain experts is particularly challenging (Figure 1), leading to higher likelihood of factual inaccuracies. Crucially, we demonstrate that both SFT and DPO substantially improve the capabilities of smaller model, with the DPO-tuned models performance surpassing that of its much larger variants, proving the value of our curated preference data. In summary, our main contributions are as follows: Novel Datasets: We develop the first well-being concept explanation dataset. It consists of 43,880 LLM-generated concept explanations for 2,194 distinct mental, physical, and social well-being concepts. We also provide audienceaware, specific fine-tuning datasets for SFT and DPO. Fine-Grained Evaluation: We propose LLM-as-ajudge framework with fine-grained audience-level principles as guidance. We evaluate each concept explanation using both direct scoring and comparative ranking. We also adopt co-judge strategy to mitigate evaluation bias. Empirical Experiments: We conduct comprehensive experiments on ten pre-trained and two fine-tuned LLMs. We reveal the nuanced performance differences among these models. We demonstrate the explanation quality improvements of fine-tuned models over larger baselines. Practical Implications: We provide in-depth analyses to probe model size effects, audience effects, and principlewise variation. We rank LLMs per audience-level principle and point out the common weaknesses of LLMs."
        },
        {
            "title": "Related Work",
            "content": "LLMs for Well-Being LLMs are increasingly being developed as proactive agents to promote human well-being (Lin et al. 2020; Chen et al. 2024; Reategui-Rivera, Smiley, and Finkelstein 2025). major line of research involves intelligent chatbots designed to address the shortage of conventional mental health services by offering scalable and effective solutions, from initial diagnoses to follow-up support in clinical domains (Prakash and Das 2020; Jo et al. 2023; Nie et al. 2024). In the educational area, similar chatbot technologies are used to enhance student well-being by serving as intelligent teaching assistants that improve the learning experience, answer queries, and support student success (Chae et al. 2023; Grossman et al. 2019; Gao et al. 2025). Beyond direct user support, another body of work utilizes LLMs as protective safeguards for societal well-being. This research focuses on combating the negative psychological and social impacts of harmful online content. LLMs are being deployed to detect and mitigate misinformation (Chen and Shu 2023; Hu et al. 2024), disinformation (Jiang et al. 2024b; Zhang et al. 2025), and hate speech (Shen et al. 2025; Meguellati et al. 2025), thereby aiming to create safer digital environments. While previous work focuses on using LLMs to promote or protect well-being, it presupposes that these models have coherent grasp of the concept itself. There is lack of research investigating whether LLMs can correctly understand and articulate the nuances of well-being concepts. This work aims to address this gap by systematically evaluating LLM-generated well-being concept explanations. Evaluation of LLM-Generated Content Traditional Assessment Metric: Traditional metrics like BLEU (Papineni et al. 2002) and ROUGE (Lin 2004) rely heavily on exact matching to evaluate models generation quality. Subsequent methods, such as BERTScore (Zhang et al. 2020) and BARTScore (Yuan, Neubig, and Liu 2021), improve upon this by using contextual embeddings, but remain incapable of capturing nuanced features (Post 2018). LLM-as-a-judge: The advanced capabilities of LLMs have inspired paradigm shift towards dynamic reference-free assessment (Wang et al. 2023). LLM-as-a-judge, as one of the leading evaluation paradigms, has been widely adopted due to its ability to conduct nuanced evaluations like humans (Zheng et al. 2023; Li et al. 2024). It has been used in domains like academic writing (Liu and Shah 2023), code generation (McAleese et al. 2024), and social science (Jiang et al. 2024a), to evaluate the quality of LLM-produced openended generation. However, recent studies have revealed various biases and vulnerabilities of the LLM-as-a-judge paradigm, raising concerns in this technique (Li et al. 2025). Principle-Guided Evaluation: To address these limitations, researchers proposed principle-guided evaluation with LLM-as-a-judge (Li et al. 2024), where set of comprehensive and well-designed rules or rubrics is given to the LLM judge for improving the assessments fairness and reliability. Following studies further improve it by providing domain (Ye et al. 2023) or sample-level principles (Kim et al. 2025; Gunjal et al. 2025; Viswanathan et al. 2025), instructing LLM judges with more fine-grained guidelines. Building on this line of work, we introduce audience-level principles: tailored guidelines that align the judges perspective with the needs of distinct explanation audience groups. (e.g., general public and domain experts)."
        },
        {
            "title": "Methods",
            "content": "Collecting Concept Explanation Dataset To systematically evaluate the quality of LLM-generated explanations for well-being concepts, we conduct rigorous data collection procedure comprising the following steps: Step 1: Well-Being Concept Selection. We start by compiling comprehensive list of well-being concepts across three primary dimensions: mental, physical, and social wellbeing. Initial concepts are identified based on their relevance, popularity, and coverage in related literature on human well-being (Diener 2000; Topp et al. 2015; TOV 2018). We further expand this list through cross-referencing synonyms and related terms from Wikipedia and the Oxford English Dictionary. The final dataset consists of 451 mental, 1,011 physical, and 732 social well-being concepts. Figure 2: Overview of the research pipeline. Step 2: Generation Model Selection. We select ten diverse large language models (LLMs) for generating concept explanations. These include four larger API-based proprietary models known for their advanced capabilities: GPT-4.1-mini (Achiam et al. 2023). OpenAI-o4-mini (OpenAI 2025). Gemini-2.5-flash (Comanici et al. 2025). Deepseek-V3 (Liu et al. 2024). Additionally, six smaller open-source LLMs are included: Qwen-3 (1.7B, 4B, 8B, and 14B) (Yang et al. 2025). LLaMA-3.2-instruct (1B and 3B) (Grattafiori et al. 2024). This combination provides comprehensive coverage across different scales, architectures, and training paradigms. Step 3: Generation Prompt Design. To ensure consistency and emulate realistic user-LLM interactions, we design standardized prompt template: Tell me about [concept]. Provide clear explanation suitable for [audience type]. We iteratively refine this template through pilot testing, using two audience categories: general public and domain experts, to guide LLMs in generating targeted explanations. Step 4: Concept Explanation Generation. Applying the finalized prompt template, we query each of the 2,194 concepts against all 10 selected LLMs, resulting in total of 43,880 concept explanations (2,194 concepts 10 LLMs 2 audience types). To minimize variability and randomness in model outputs, all generations are conducted using deterministic setting with LLMs temperature = 0. Fine-Tuning Specialized Model To validate whether the collected dataset is suitable for finetuning specialized model for better well-being concept explanation, we investigate two distinct fine-tuning strategies: Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO). These methods are applied separately to the same pre-trained base model (Qwen-3-4B). In both strategies, we denote the model being trained as Mθ. Supervised Fine-Tuning. SFT aims to adapt the base model Mθ to generate outputs that conform to the format and style of high-quality explanations. Step 1: SFT Data Preparation. We construct our SFT dataset DSF by applying filtering process to each wellbeing concept explanation to select high-quality responses. We first make an assumption based on previous work for given prompt , the quality of the response from larger LLMs generally outperforms those from the smaller LLMs (Askell et al. 2021; Kim et al. 2023). Therefore, an explanation Ei,j,k (for concept ci, generation model Mj, and audience ak) is included in DSF only if it is generated by larger LLM (e.g., Gemini-2.5-flash). Step 2: SFT Objective. The base model Mθ is then finetuned on the curated dataset DSF . For each concept, we use the standardized prompt template containing the concept ci and audience type ak as the input prompt Pi and the corresponding high-quality explanation as the target output Ei. The SFT objective is to train Mθ by minimizing the negative log-likelihood loss LSF (θ): LSF (θ) = Ei (cid:88) t=1 (cid:88) (Pi,Ei) DSF log (Ei,tPi, Ei,<t; θ) (1) where Ei,t is the t-th token of the target explanation Ei. Direct Preference Optimization. DPO directly optimizes the model based on human preference data (Rafailov et al. 2023). It is designed to explicitly teach the model to distinguish between high-quality and low-quality responses. Step 1: DPO Data Preparation. The preference dataset DDP consists of pairs of preferred and dispreferred responses for each input prompt . Similar to the way we construct DSF , we create pool of good and bad explanations for every well-being concept in our collected data: Good Explanation (Eg): it only includes well-being concept explanations generated by the larger LLMs. Bad Explanation (Eb): it only includes well-being concept on specific principle is the average score from J1 and J2: explanations generated by the smaller LLMs. As result, the final dataset DDP is composed of multiple (P, Eg, Eb) for each concept and audience type. Step 2: DPO Objective. The policy model Mθ is optimized by DPO to increase the likelihood of explanation Eg over Eb. This is guided by frozen reference model Mref , which is the original pre-trained base model Mθ. The DPO loss function is defined as: (cid:104) log σ (cid:16) β (cid:17)(cid:17)(cid:105) θ LDP O(θ) = E(P,Eg,Eb) DDP log πb πb (cid:16) log πg θ πg where πg moid function, and β controls deviation from πref . θ πθ(EgP ), πb (2) ref πref (EbP ), σ is the sigref ref"
        },
        {
            "title": "Assessing Concept Explanation Quality",
            "content": "We employ principle-guided LLM-as-a-judge paradigm to assess explanation quality, leveraging two powerful Large Reasoning Models (LRMs) as judges, = {J1, J2}, where J1 is Gemini-2.5-Pro (Comanici et al. 2025) and J2 is DeepSeek-R1 (Guo et al. 2025). Judges will assess the quality using Direct Scoring and Comparative Ranking based on the predefined audience-level principles. Step 1: Fine-Grained Evaluation Criteria. To enhance consistency and interpretability of the evaluation process, inspired by previous work (Ye et al. 2023), we carefully define evaluation criteria with fine-grained, audience-level principles tailored to two types of audiences. For the general public without sufficient domain knowledge: Accuracy: Provide an accurate definition of the concept. Accessibility: Use of simple, everyday language. Conciseness: Brief and direct explanations without unnecessary verbosity. Demonstration: Use of relatable analogies, stories, or realworld examples. Utility: Provision of actionable and practical advice. For domain experts with sufficient domain knowledge: Accuracy: Provide an accurate definition of the concept. Terminology: Use of professional, field-specific jargon. Depth: Comprehensive and nuanced analysis of concepts. Critique: Identification of limitations and controversies. Substantiation: Inclusion of references and citations from peer-reviewed literature. Note that Accuracy is presented in both scenarios because of its importance for analyzing any inaccurate or hallucinated definition in the generated concept explanation. Step 2: Direct Scoring. In this method, judges assign score to each explanation per principle. For given concept explanation Ei,j,k (for concept ci, generation model Mj, and audience ak), each judge Jl provides score Sl(Ei,j,k, v) [1, 5] for each principle v. The final score for an explanation S(Ei,j,k, v) = 1 J (cid:88) l=1 Sl(Ei,j,k, v). (3) To assess models performance on specific principle for given audience, we aggregate the scores S(Ei,j,k, v) across all concepts. The total quality score for model Mj on principle for audience ak is: QDS(Mj, k, v) = (cid:88) i=1 S(Ei,j,k, v). (4) Note that LLMs assign Accuracy scores for the generated explanation by comparing the definition from Wikipedia and Dictionary as ground truth (i.e., the maximum score). Step 3: Comparative Ranking. In this method, judges compare each generated explanation against baseline reference Eref (i.e., Qwen-3-14B) per principle. For each principle v, the comparison yields an outcome O(Ei,j,k, v) {win, loss, tie}. conflict between judges on any given principle (e.g., J1 outputs win and J2 outputs loss) will result in tie for that specific principle. models performance is then quantified by its win rate for each principle and each audience type. The win rate for model Mj on principle for audience ak is calculated as: (Mj, k, v) = {Ei,j,k O(Ei,j,k, v) = win} (5) For Accuracy, the judge assigns an outcome based on which explanation (Eref and Ei,j,k) is closer to the ground-truth definition. For example, if the baseline references explanation Eref is closer to Wikipedias definition, the final outcome will be loss. Results In this section, we present our empirical results to answer the following research questions: RQ1: Does the proposed principle-guided LLM-as-ajudge framework provide human-level evaluation? RQ2: How do the capabilities of LLMs differ when explaining well-being concepts in different scenarios? RQ3: To what extent can fine-tuning via SFT and DPO improve LLMs well-being concept explanation abilities? Validations of our evaluation framework (RQ1) To validate the reliability of our principle-guided LLMas-judge paradigm and answer RQ1, we conduct human evaluations to assess the explanations using the comparative ranking strategy with identical evaluation principles. Specifically, we compare the LLMs judgements against human annotations on held-out set of 50 explanations per model. We compute the overall win rate for each LLM by calculating the average win rate among all audience-level principles. The inter-rater agreement is measured using Cohens kappa (Cohen 1960) on the mental, physical, and social well-being concept categories. We report the results for Figure 3: Cohens kappa scores between LLM-as-a-judge and human annotators. Dashed lines at 0.6 and 0.8 indicate substantial (0.61 to 0.80) and almost-perfect (0.81 to 1) agreement, respectively. general public and domain expert explanations separately. Figure 3 visualizes the level of agreement for all evaluated LLMs. We observe that LLM-as-a-judge is more reliable when evaluating concept explanations for the general public, evidenced by overall higher Cohens kappa scores. Moreover, they have more agreement on evaluation results from larger LLMs and extremely smaller LLMs (LLaMA-3.2-1B, 3B, and Qwen-3-1.7B). This indicates that it is easy for LLMs to recognize extremely good and bad well-being concept explanations. However, their judgments become slightly unreliable (i.e., moderate to substantial agreement) when dealing with relatively moderate quality explanations. Another finding is that there is no significant inter-rater agreement discrepancy between the three well-being categories. Differences in Well-being Concept Explanation Capability (RQ2) To answer RQ2, we conduct comprehensive analyses of the evaluated pretrained LLMs. Our results reveal significant disparities in LLMs capabilities based on several factors. Model size effect: larger LLMs are more capable of well-being concept explanation. Figure 4 compares each LLMs Direct Scoring results across five evaluation principles for the general public (Figure 4a) and domain experts (Figure 4b). In both cases, the four larger API-based LLMs (GPT-4.1-mini, o4-mini, DeepSeek-v3, and Gemini2.5-flash) form substantially larger radar polygons than the smaller open-source models, indicating clear scale effect. In Table 1, the top four larger LLMs achieve overall win rates exceeding 87% for the general public audience and 88% for the domain expert audience against the baseline model. DeepSeek-v3 emerges as the top performer for the general public with an 88.9% win rate, while o4-mini leads for the domain expert audience with 91.5% win rate. In contrast, the performance of the smaller open-source LLMs scales with parameter count but remains significantly lower. This performance divide is visually confirmed by the radar charts in Figure 4, where the Larger LLMs consistently form large and outer performance polygon, while the smaller models are clustered in much smaller area, indicating lower scores across all evaluation principles. Audience effect: generating high-quality well-being concept explanation for domain experts is challenging. While LLMs can effectively adapt their concept explanations to the target audience, they are struggling to provide good explanations for domain experts with specialized background knowledge. comparative analysis of explanation for the general public (Figures 4a) and domain experts (Figure 4b) reveals two findings: (1) The quality of concept explanations for domain experts is worse than those generated for the general public, reflecting on the overall smaller radar polygons and Accuracy decrease. For example, DeepSeekv3 falls from 4.72 to 3.41 (27.8%), while o4-mini plunges from 4.73 to 3.72 (21.4%). This systematic decline indicates that, when asking LLMs to generate explanations for domain experts, they are more likely to hallucinate or generate factually inaccurate details. (2) The performance disparity between smaller and larger LLMs is increasing in expertoriented concept explanations. This phenomenon can be further confirmed by the higher win rates of domain experts compared to the general public. We speculate that the two findings are possibly due to the limited learning capacity of smaller models when there is lack of high-quality, professional, and jargon-rich data that would enable more nuanced explanations of well-being concepts. Well-being category effect: explaining social well-being concepts is more difficult. Besides model scale and audience type, we observe that Physical well-being explanations (lower-left quadrants in Figure 4a and 4b) achieve the highest overall quality: all four larger LLMs score above 4.5 and nearly 4 in direct scoring on Accessibility and Terminology, respectively. In contrast, Social well-being explanations show the greatest variability in the radar chart and lowest win rates among the three well-being concept categories  (Table 1)  . Mental well-being explanations sit between these extremes: nearly all LLMs show the median win rates among three well-being concept categories  (Table 1)  . (a) Scores of concept explanations for the general public. (b) Scores of concept explanations for domain experts. Figure 4: Direct Scoring comparisons of LLMs well-being concept explanation. In Figure (a) and (b), Overall Performance is calculated by averaging scores from mental, physical, and social well-being concepts explanations. Principle-wise analysis: larger LLMs present unified weakness in Utility and Depth despite diverse strengths. As shown in Figure 4, while the larger LLMs consistently outperform smaller models across all evaluation principles, they exhibit shared weakness in providing practical advice (Utility) for the general public and generating nuanced analyses (Depth) for domain experts. At the same time, each of these models demonstrates particular strengths in specific principles. GPT-4.1-mini excels on Accessibility and Terminology, o4-mini achieves the highest scores for factual Accuracy in both settings, DeepSeek-v3 is good at providing clear Demonstration and Concise explanations, and Gemini-2.5-flash can generate Accurate definitions as well as providing evidence and references (Substantiation). Although larger LLMs generally perform worse on Utility and Depth, Gemeni-flash-2.5 and DeepSeek-v3 demonstrate relatively better performances. Based on their overall performance (Figure 4), we list the winner for each evaluation principle. For the principles of general public: Accuracy: o4-mini and DeepSeek-v3 Accessibility: GPT-4.1-mini Conciseness: o4-mini and DeepSeek-v3 Demonstration: DeepSeek-v3 and GPT-4.1-mini Utility: Gemini-2.5-flash and DeepSeek-v3 For the principles of domain experts: Accuracy: Gemini-flash-2.5 Terminology: o4-mini and GPT-4.1-mini Depth: o4-mini Critique: DeepSeek-v3 and GPT-4.1-mini Substantiation: Gemini-flash-2.5 Performances of Fine-Tuned Well-Being Concept Explanation Models (RQ3) To respond to RQ3, we fine-tune the Qwen-3-4B base model using SFT and DPO. We compare their performance back on the same evaluation set. In particular, we begin with pool of 600 well-being concepts, split evenly into 300 for training and 300 held out for evaluation. For each training concept, we collect four good and two bad explanations: SFT uses only the good examples, while DPO uses paired good and bad examples. We then apply both SFT and DPO to the Qwen-3-4B model and evaluate all LLMs on the evaluation set using our Direct Scoring and Comparative Ranking. Improvements on direct scoring results. As shown in Table 2, both fine-tuning strategies achieve substantial gains over the pre-trained Qwen-3-4B model. Qwen-3-4B-SFT increases the general public score by 0.44 points (+16.1%) to 3.18 and the expert score by 0.32 points (+13.0%) to 2.79, completely outperforming the Qwen-3-4B and 8B and nearly matching the Qwen-3-14B performance. Qwen-3-4B-DPO improves even further, adding 0.51 points (+18.6%) to 3.25 for the general public and 0.38 points (+15.4%) to 2.85 for the domain expert. Improvements on comparative ranking results. Table 3 presents win rate increases on the evaluation set. Qwen-34B-SFT achieves the general public win rate of 72.2% and the expert win rate of 81.4%, positioning between the larger Qwen-3 variants (8B and 14B). On the other hand, Qwen3-4B-DPO further increases the general publics win rate to 75.9% and the experts to 83.4%, surpassing Qwen-3-14B for domain expert concept explanations. Although they are still not comparable with larger API-based LLMs, these results demonstrate that both SFT and DPO can bring smaller Model GPT-4.1-mini o4-mini DeepSeek-v3 Gemini-2.5-flash LLaMA-3.2-1B-Instruct LLaMA-3.2-3B-Instruct Qwen-3-1.7B Qwen-3-8B Qwen-3-14B Qwen-3-4B (baseline) General Public Domain Expert Mental Physical Social Overall Mental Physical Social Overall Larger API-based Models 92.3 90.8 91.7 91. 84.7 85.3 85.9 83.8 88.5 87.8 88.9 87.3 Smaller Open-source Models 18.7 55.3 48.3 80.5 88.7 0.0 7.5 25.1 13.6 53.2 65.9 0.0 12.9 38.5 28.2 66.2 77.7 0. 90.3 91.8 90.5 89.2 14.1 45.8 26.5 68.3 81.3 0.0 88.5 87.4 89.1 86.2 12.4 35.2 22.7 65.0 78.4 0.0 92.7 94.4 93.6 91.5 22.3 72.1 52.4 82.1 90.2 0. 86.9 88.2 87.6 85.3 9.2 31.4 17.2 63.1 68.4 0.0 90.0 91.5 90.6 88.7 15.2 49.8 32.0 71.2 80.0 0.0 Table 1: Comparative Ranking comparisons of LLMs well-being concept explanation. All results indicate win rates (%) against the Qwen-3-4B baseline on the whole dataset. Bold and underline values indicate the best and second-best results, respectively. Model General Public Domain Expert Model General Public Domain Expert Larger API-based Models Larger API-based Models GPT-4.1-mini o4-mini DeepSeek-v3 Gemini-2.5-flash 4.17 4.21 4.25 4.19 Smaller Open-source Models LLaMA-1B-Inst. LLaMA-3B-Inst. Qwen-3-1.7B Qwen-3-8B Qwen-3-14B 1.98 2.72 2.11 2.98 3.26 4.00 4.05 4.10 4.02 1.60 2.38 1.55 2.62 2. GPT-4.1-mini o4-mini DeepSeek-v3 Gemini-2.5-flash 88.1 87.2 88.3 87.0 Smaller Open-source Models LLaMA-1B-Inst. LLaMA-3B-Inst. Qwen-3-1.7B Qwen-3-8B Qwen-3-14B 13.5 38.4 20.7 66.5 77.5 89.2 90.7 89.8 88. 16.0 52.6 30.4 70.5 79.3 Baseline & Fine-tuned Models Baseline & Fine-tuned Models 2.74 Qwen-3-4B 3.18 (+16.1%) Qwen-3-4B-SFT Qwen-3-4B-DPO 3.25 (+18.6%) 2.47 2.79 (+13.0%) 2.85 (+15.4%) 0.0 Qwen-3-4B 72.2 (+72.2%) Qwen-3-4B-SFT Qwen-3-4B-DPO 75.9 (+75.9%) 0.0 81.4 (+81.4%) 83.4 (+83.4%) Table 2: Comparison of Direct Scoring results on the evaluation set. Averaged scores of all principles are presented. Relative gains over Qwen-3-4B are shown in parentheses. Table 3: Comparative Ranking results against Qwen-3-4B on the evaluation set. Overall win rates (%) are reported and relative gains over Qwen-3-4B are shown in parentheses. LLMs performance up to the level of their larger variants after fine-tuning on our datasets. DPO generally achieves better performance than SFT. Although both fine-tuning approaches significantly improve Qwen-3-4Bs explanation quality, Qwen-3-4B-DPO consistently outperforms Qwen-3-4B-SFT across both Direct Scoring and Comparative Ranking (Table 2 and 3). We attribute this to DPOs preference-driven training objective, which directly optimizes the model to prefer higher-quality explanations over lower-quality ones, rather than merely mimicking good examples. Thus, DPO captures more subtle signals from good and bad examples than standard maximum likelihood (i.e., SFT). Conclusion and Future Work In this paper, we systematically evaluate whether LLMs are ready to explain complex well-being concepts. We build large-scale dataset of well-being concept explanations, develop principle-guided evaluation framework, and test the efficacy of the small fine-tuned models using both SFT and DPO. Our findings reveal shared weaknesses of LLMs. We point out that LLM can struggle with factual Accuracy when explaining concepts to experts. Finally, we demonstrate that both SFT and DPO substantially improve smaller models. Future work could explore the efficacy of other tuning techniques, such as Proximal Policy Optimization (PPO) (Schulman et al. 2017), Constrained Policy Optimization (CPO) (Achiam et al. 2017), and Group Relative Policy Optimization (GRPO) (Shao et al. 2024). Moreover, researchers can follow our research pipeline to collect and evaluate more LLM-generated concept explanations for other types of audiences (e.g., K12 students) or from different domains (e.g., physics). References Achiam, J.; Adler, S.; Agarwal, S.; Ahmad, L.; Akkaya, I.; Aleman, F. L.; Almeida, D.; Altenschmidt, J.; Altman, S.; Anadkat, S.; et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774. Achiam, J.; Held, D.; Tamar, A.; and Abbeel, P. 2017. Constrained policy optimization. In International conference on machine learning, 2231. PMLR. Alexandrova, A. 2017. philosophy for the science of wellbeing. Oxford University Press. Askell, A.; Bai, Y.; Chen, A.; Drain, D.; Ganguli, D.; Henighan, T.; Jones, A.; Joseph, N.; Mann, B.; DasSarma, N.; et al. 2021. general language assistant as laboratory for alignment. arXiv preprint arXiv:2112.00861. Chae, H.; Kim, M.; Kim, C.; Jeong, W.; Kim, H.; Lee, J.; and Yeo, J. 2023. TUTORING: instruction-grounded conversational agent for language learners. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, 1641316415. Chen, C.; and Shu, K. 2023. Can llm-generated misinformation be detected? arXiv preprint arXiv:2309.13788. Chen, Y.; Zhang, X.; Wang, J.; Xie, X.; Yan, N.; Chen, H.; and Wang, L. 2024. Structured dialogue system for mental health: An llm chatbot leveraging the pm+ guidelines. In International Conference on Social Robotics, 262271. Springer. Cho, Y.; and Choi, I. 2018. Writing from sources: Does audience matter? Assessing Writing, 37: 2538. Cohen, J. 1960. coefficient of agreement for nominal scales. Educational and psychological measurement, 20(1): 3746. Comanici, G.; Bieber, E.; Schaekermann, M.; Pasupat, I.; Sachdeva, N.; Dhillon, I.; Blistein, M.; Ram, O.; Zhang, D.; Rosen, E.; et al. 2025. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, arXiv preprint and next generation agentic capabilities. arXiv:2507.06261. Diener, E. 2000. Subjective well-being: The science of happiness and proposal for national index. American psychologist, 55(1): 34. Gao, W.; Liu, Q.; Yue, L.; Yao, F.; Lv, R.; Zhang, Z.; Wang, H.; and Huang, Z. 2025. Agent4edu: Generating learner response data by generative agents for intelligent education systems. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 2392323932. Grattafiori, A.; Dubey, A.; Jauhri, A.; Pandey, A.; Kadian, A.; Al-Dahle, A.; Letman, A.; Mathur, A.; Schelten, A.; Vaughan, A.; et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Grossman, J.; Lin, Z.; Sheng, H.; Wei, J. T.-Z.; Williams, J. J.; and Goel, S. 2019. MathBot: Transforming online resources for learning math into conversational interactions. AAAI 2019 Story-Enabled Intelligence. Gunjal, A.; Wang, A.; Lau, E.; Nath, V.; Liu, B.; and Hendryx, S. 2025. Rubrics as Rewards: Reinforcement arXiv preprint Learning Beyond Verifiable Domains. arXiv:2507.17746. Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Hu, B.; Sheng, Q.; Cao, J.; Shi, Y.; Li, Y.; Wang, D.; and Qi, P. 2024. Bad actor, good advisor: Exploring the role of large language models in fake news detection. In Proceedings of the AAAI conference on artificial intelligence, volume 38, 2210522113. Jarden, A.; and Roache, A. 2023. What is wellbeing? Jiang, B.; Li, D.; Tan, Z.; Zhou, X.; Rao, A.; Lerman, K.; Bernard, H. R.; and Liu, H. 2024a. Assessing the impact of conspiracy theories using large language models. arXiv preprint arXiv:2412.07019. Jiang, B.; Tan, Z.; Nirmal, A.; and Liu, H. 2024b. Disinformation detection: An evolving challenge in the age of llms. In Proceedings of the 2024 siam international conference on data mining (sdm), 427435. SIAM. Jo, E.; Epstein, D. A.; Jung, H.; and Kim, Y.-H. 2023. Understanding the benefits and challenges of deploying conversational AI leveraging large language models for public health intervention. In Proceedings of the 2023 CHI conference on human factors in computing systems, 116. Keil, F. C. 2006. Explanation and understanding. Annu. Rev. Psychol., 57(1): 227254. Kim, S.; Bae, S.; Shin, J.; Kang, S.; Kwak, D.; Yoo, K. M.; and Seo, M. 2023. Aligning large language models through synthetic feedback. arXiv preprint arXiv:2305.13735. Kim, S.; Suk, J.; Cho, J. Y.; Longpre, S.; Kim, C.; Yoon, D.; Son, G.; Cho, Y.; Shafayat, S.; Baek, J.; et al. 2025. The BiGGen Bench: Principled Benchmark for Fine-grained Evaluation of Language Models with Language Models. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), 58775919. Li, D.; Jiang, B.; Huang, L.; Beigi, A.; Zhao, C.; Tan, Z.; Bhattacharjee, A.; Jiang, Y.; Chen, C.; Wu, T.; Shu, K.; Cheng, L.; and Liu, H. 2024. From Generation to Judgment: Opportunities and Challenges of LLM-as-a-judge. arXiv preprint arXiv: 2411.16594. Li, D.; Sun, R.; Huang, Y.; Zhong, M.; Jiang, B.; Han, J.; Zhang, X.; Wang, W.; and Liu, H. 2025. Preference leakage: contamination problem in llm-as-a-judge. arXiv preprint arXiv:2502.01534. Lin, C.-Y. 2004. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, 7481. Lin, Z.; Xu, P.; Winata, G. I.; Siddique, F. B.; Liu, Z.; Shin, J.; and Fung, P. 2020. Caire: An end-to-end empathetic chatbot. In Proceedings of the AAAI conference on artificial intelligence, volume 34, 1362213623. Liu, A.; Feng, B.; Xue, B.; Wang, B.; Wu, B.; Lu, C.; Zhao, C.; Deng, C.; Zhang, C.; Ruan, C.; et al. 2024. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437. Liu, R.; and Shah, N. B. 2023. Reviewergpt? an exploratory study on using large language models for paper reviewing. arXiv preprint arXiv:2306.00622. McAleese, N.; Pokorny, R. M.; Uribe, J. F. C.; Nitishinskaya, E.; Trebacz, M.; and Leike, J. 2024. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215. Meguellati, E.; Zeghina, A.; Sadiq, S.; and Demartini, G. 2025. LLM-Based Semantic Augmentation for Harmful Content Detection. In Proceedings of the International AAAI Conference on Web and Social Media, volume 19, 1190 1209. Nie, J.; Shao, H.; Fan, Y.; Shao, Q.; You, H.; Preindl, M.; and Jiang, X. 2024. LLM-based conversational AI therapist for daily functioning screening and psychotherapeutic intervention via everyday smart devices. arXiv preprint arXiv:2403.10779. OpenAI. 2025. Openai o3 and o4-mini system card. Technical report, OpenAI. Papineni, K.; Roukos, S.; Ward, T.; and Zhu, W.-J. 2002. Bleu: method for automatic evaluation of machine transIn Proceedings of the 40th annual meeting of the lation. Association for Computational Linguistics, 311318. Post, M. 2018. call for clarity in reporting BLEU scores. arXiv preprint arXiv:1804.08771. Prakash, A. V.; and Das, S. 2020. Intelligent conversational agents in mental healthcare services: thematic analysis of user perceptions. Pacific Asia Journal of the Association for Information Systems, 12(2): 1. Rafailov, R.; Sharma, A.; Mitchell, E.; Manning, C. D.; Ermon, S.; and Finn, C. 2023. Direct preference optimization: Your language model is secretly reward model. Advances in neural information processing systems, 36: 5372853741. Reategui-Rivera, C. M.; Smiley, A.; and Finkelstein, J. 2025. LLM-Based Chatbot to Reduce Mental Illness Stigma in Healthcare Providers. In 2025 IEEE 15th Annual Computing and Communication Workshop and Conference (CCWC), 0000100007. IEEE. Schulman, J.; Wolski, F.; Dhariwal, P.; Radford, A.; and Klimov, O. 2017. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347. Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang, H.; Zhang, M.; Li, Y.; Wu, Y.; et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Shen, X.; Wu, Y.; Qu, Y.; Backes, M.; Zannettou, S.; and Zhang, Y. 2025. HateBench: Benchmarking Hate Speech Detectors on LLM-Generated Content and Hate Campaigns. arXiv preprint arXiv:2501.16750. Topp, C. W.; Østergaard, S. D.; Søndergaard, S.; and Bech, P. 2015. The WHO-5 Well-Being Index: systematic review of the literature. Psychotherapy and psychosomatics, 84(3): 167176. TOV, W. 2018. Well-being concepts and components. Handbook of subjective well-being, 115. Viswanathan, V.; Sun, Y.; Ma, S.; Kong, X.; Cao, M.; Neubig, G.; and Wu, T. 2025. Checklists Are Better Than arXiv Reward Models For Aligning Language Models. preprint arXiv:2507.18624. Wang, J.; Liang, Y.; Meng, F.; Sun, Z.; Shi, H.; Li, Z.; Xu, J.; Qu, J.; and Zhou, J. 2023. Is ChatGPT Good NLG Evaluator? Preliminary Study. In Proceedings of the 4th New Frontiers in Summarization Workshop, 111. Wu, S.; Cachia, J. Y.; Han, F.; Yao, B.; Xie, T.; Zhao, X.; and Wang, D. 2024. Like Sunnie More Than Expected!: Exploring User Expectation and Perception of an Anthropomorphic LLM-based Conversational Agent for Well-Being Support. arXiv preprint arXiv:2405.13803. Xiong, H.; Bian, J.; Li, Y.; Li, X.; Du, M.; Wang, S.; Yin, D.; and Helal, S. 2024. When search engine services meet large language models: visions and challenges. IEEE Transactions on Services Computing. Yang, A.; Li, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Gao, C.; Huang, C.; Lv, C.; et al. 2025. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Ye, S.; Kim, D.; Kim, S.; Hwang, H.; Kim, S.; Jo, Y.; Thorne, J.; Kim, J.; and Seo, M. 2023. Flask: Fine-grained language model evaluation based on alignment skill sets. arXiv preprint arXiv:2307.10928. Yuan, W.; Neubig, G.; and Liu, P. 2021. BARTScore: Evaluating Generated Text as Text Generation. In Ranzato, M.; Beygelzimer, A.; Dauphin, Y. N.; Liang, P.; and Vaughan, J. W., eds., Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, 2726327277. Zhang, C.; Feng, Z.; Zhang, Z.; Qiang, J.; Xu, G.; and Li, Y. 2025. Is LLMs Hallucination Usable? LLM-based Negative Reasoning for Fake News Detection. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 39, 10311039. Zhang, T.; Kishore, V.; Wu, F.; Weinberger, K. Q.; and Artzi, Y. 2020. BERTScore: Evaluating Text Generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Zheng, L.; Chiang, W.; Sheng, Y.; Zhuang, S.; Wu, Z.; Zhuang, Y.; Lin, Z.; Li, Z.; Li, D.; Xing, E. P.; Zhang, H.; Gonzalez, J. E.; and Stoica, I. 2023. Judging LLM-as-aJudge with MT-Bench and Chatbot Arena. In Advances in Neural Information Processing Systems 36: Annual Conference on Neural Information Processing Systems 2023, NeurIPS 2023, New Orleans, LA, USA, December 10 - 16, 2023."
        }
    ],
    "affiliations": [
        "School of Computing and Augmented Intelligence, Arizona State University, USA"
    ]
}