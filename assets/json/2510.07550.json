{
    "paper_title": "TRAVL: A Recipe for Making Video-Language Models Better Judges of Physics Implausibility",
    "authors": [
        "Saman Motamed",
        "Minghao Chen",
        "Luc Van Gool",
        "Iro Laina"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite impressive visual fidelity, modern video generative models frequently produce sequences that violate intuitive physical laws, such as objects floating, teleporting, or morphing in ways that defy causality. While humans can easily detect such implausibilities, there remains no robust method for quantitatively assessing physical realism in video. In this work, we explore whether Video-Language Models (VLMs) can be trained to serve as reliable judges of physical plausibility. We find that existing VLMs struggle to identify physics violations, exposing fundamental limitations in their temporal and causal reasoning. To address this, we introduce TRAVL, a fine-tuning recipe that combines a balanced training dataset with a trajectory-aware attention module to improve motion encoding and discrimination in VLMs. To evaluate physical reasoning more rigorously, we propose ImplausiBench, a benchmark of 300 videos (150 real, 150 generated) that removes linguistic biases and isolates visual-temporal understanding. Performance is reported both with gold-standard human judgments and stricter LLM-as-judge metrics. Together, TRAVL and ImplausiBench offer a unified framework for probing and improving physical plausibility in multimodal models, shedding light on a challenging and underexplored aspect of visual-temporal understanding."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 ] . [ 1 0 5 5 7 0 . 0 1 5 2 : r TRAVL: RECIPE FOR MAKING VIDEO-LANGUAGE MODELS BETTER JUDGES OF PHYSICS IMPLAUSIBILITY Saman Motamed1,2 Minghao Chen2 Luc Van Gool1 Iro Laina2 1 INSAIT, Sofia University \"St. Kliment Ohridski\", Bulgaria 2 Visual Geometry Group, University of Oxford sam-motamed.github.io/projects/TRAVL Figure 1: Video Language Models (VLMs) often struggle with fine-grained understanding of physics realism. We propose fine-tuning recipe that helps VLMs become better judges of physics implausibility. ABSTRACT Despite impressive visual fidelity, modern video generative models frequently produce sequences that violate intuitive physical laws, such as objects floating, teleporting, or morphing in ways that defy causality. While humans can easily detect such implausibilities, there remains no robust method for quantitatively assessing physical realism in video. In this work, we explore whether Video-Language Models (VLMs) can be trained to serve as reliable judges of physical plausibility. We find that existing VLMs struggle to identify physics violations, exposing fundamental limitations in their temporal and causal reasoning. To address this, we introduce TRAVL, fine-tuning recipe that combines balanced training dataset with trajectory-aware attention module to improve motion encoding and discrimination in VLMs. To evaluate physical reasoning more rigorously, we propose ImplausiBench, benchmark of 300 videos (150 real, 150 generated) that removes linguistic biases and isolates visual-temporal understanding. Performance is reported both with gold-standard human judgments and stricter LLM-as-judge metrics. Together, TRAVL and ImplausiBench offer unified framework for probing and improving physical plausibility in multimodal models, shedding light on challenging and underexplored aspect of visual-temporal understanding."
        },
        {
            "title": "Introduction",
            "content": "Modern video generation models [16, 25, 41, 9] have achieved remarkable visual quality, yet they frequently produce sequences that violate intuitive physical lawsfor example, objects may float, vanish, or morph in implausible ways. While such anomalies are easily detected by humans, quantitatively assessing physical realism in generated videos remains an open challenge [34, 6, 33, 59]. Existing evaluation metrics like FVD [47] and CLIPSIM [39] prioritize perceptual similarity rather than physical plausibility. This raises natural question: can video-language models (VLMs) be trained to serve as reliable judges of physical correctness in video? Motivated by the strong physics priors encoded in large language models [13, 44], we explore whether motion-aware visual grounding can enhance VLMs ability to detect implausible dynamics. balanced dataset of plausible and implausible videos, ensuring robustness to distributional biases and improving generalization across diverse motion scenarios. Despite recent advances, VLMs still struggle to reason about physical plausibility and motion. Several studies highlight these limitations: MotionBench [19] reports poor performance on fine-grained motion tasks involving multiobject interactions; Foresight-to-Forethought [52] shows that VLMs fail to predict outcomes in interactive physical scenarios; and Buschoff et al. [43] finds that fine-tuning on narrow physics domain (e.g., falling blocks) fails to generalize to broader settings. Complementing these findings, recent benchmarks evaluating physical reasoning, such as PhysBench [14], reveal that even the most capable models, including GPT-4o, perform well below human level [43, 5], particularly on tasks involving dynamic interactions. To compensate, hybrid systems like PhysAgent [14] inject symbolic or perceptual priors. Other efforts, such as Impossible Videos [3], highlight the challenge of designing blind tests for implausibility detection, though structural and linguistic biases limit their use as reliable evaluation set. In this work, we instead use such datasets as part of training material while shifting evaluation to more carefully constructed protocols. Beyond benchmarks, architectural limitations also hinder physical reasoning. Current VLMs such as InternVideo [49], LLaVA-Video [60], Qwen2-VL [48], and Video-ChatGPT [31]typically encode sparsely sampled frames independently via frozen image encoders like CLIP [39] or SigLIP [58]. These representations are projected into the language model through simple adapters, discarding motion continuity and temporal context. As result, these models often fail to recognize violations of physical laws, such as levitation, teleportation, or object morphing [2, 3, 14]. Addressing these shortcomings requires both better temporal grounding mechanisms and evaluation protocols that isolate genuine visual reasoning. To address these challenges, we present both fine-tuning recipe and an evaluation framework tailored to physical reasoning in video-language models. We introduce TRAVL (TRajectory-Aware Vision-Language learning), modular method that augments VLMs with motion-informed self-attention. TRAVL enhances visual encoding through two key mechanisms: (1) intra-frame spatial attention, which captures physically meaningful structure and relations within each framecrucial for detecting anomalies like deformation, disappearance, or size inconsistencies; and (2) trajectory-aware temporal attention, which restricts inter-frame attention to follow sparse, object-level motion paths computed via CoTracker [23]. This attention structure encourages the model to align visual tokens along both spatial structure and coherent motion, resulting in video representations that are more grounded in physical dynamics. TRAVL is lightweight and architecture-agnostic: it introduces no changes to the vision encoder or language model, and only fine-tunes small number of attention and projection layers. Moreover, TRAVL is trained on To rigorously evaluate physical reasoning capabilities, we introduce ImplausiBench, benchmark explicitly designed to eliminate linguistic shortcuts and isolate visualtemporal understanding. ImplausiBench contains 300 videos (150 real, 150 generated), organized into paired plausible and implausible variants of the same scenario (sharing the same starting frame) and annotated with multiple-choice questions. Each question set was adversarially stress-tested in blind evaluation protocol, where off-the-shelf LLMs attempted to answer without viewing the video; whenever models exploited linguistic cues, we revised the multiple-choice answers until shortcut success was eliminated. In contrast, prior benchmarks such as Impossible Videos[3] did not apply such blind testing, leaving them vulnerable to linguistic or positional biases. By construction, ImplausiBench ensures that progress reflects grounded video reasoning rather than surface-level patterns. Covering broad spectrum of implausibility types including teleportation, levitation, deformation, duplication, and state changes, ImplausiBench serves as high-fidelity diagnostic for evaluating whether VLMs truly understand physical plausibility in video. Summary of Contributions. We propose TRAVL, modular fine-tuning recipe with trajectory-aware self-attention to enhance motion and physics understanding in VLMs. We curate balanced training dataset with plausible and implausible videos with focus on physics reasoning. We propose ImplausiBench, new benchmark of 300 videos that rigorously evaluates physical plausibility under both human and LLM-judge metrics."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Advancements and Limitations in Video-Language Models The development of Video-Language Models (VLMs) has been propelled by large-scale vision-language pretraining frameworks such as CLIP [39], ALIGN [21], and SigLIP [58]. These models form the backbone of more sophisticated video-capable architectures including InternVL [12], Video-ChatGPT [31], LLaVA-Video [60], and Qwen2.5VL [38], which perform well on standard video-language tasks such as captioning, retrieval, and Q/A. They have been evaluated on variety of benchmarks including MMBench [29], MVBench [28], MTVQA [45], MSRVTT-QA [54], MEGA-Bench [11], VBench [20], Video-Bench [35], SEED-Bench [26], and TempCompass [30]. However, many of these benchmarks evaluate 2 Figure 2: Overview of our proposed TRAVL framework. Given input video frames, we apply vision encoder followed by trajectory-aware masked self-attention, which integrates spatial and temporal context using patch trajectories tracked by CoTracker. The enriched features are projected into the language models embedding space. Only the trajectory attention and vision-to-language projector are fine-tuned; the vision encoder and language model are kept frozen. static understanding or treat frames independently, limiting insight into dynamic scene comprehension. Recent works [2, 14] highlight that VLMs struggle with temporal coherence, motion continuity, and dynamic physical reasoning, motivating methods that inject stronger temporal grounding. solidity, gravity). It uses real-world and synthetic videos and is strictly diagnostic (no training is allowed). In contrast, ImplausiBench uses multiple-choice Q/A format designed to probe causal and counterfactual reasoning, with adversarial distractors to prevent shortcut exploitation. 2.2 Incorporating Trajectory-Based Temporal Modeling Trajectory-aware modeling has proven effective for capturing fine-grained motion in variety of vision tasks. For instance, Motionformer [36] uses trajectory attention to improve action recognition. FLATTEN [15], pixelaligned trajectory attention [53], and VideoJAM [10] enhance temporal consistency in video editing and generation, while OmnimatteZero [42] improves training-free video inpainting using trajectory-aware attention. While these approaches showcase the benefits of motion-aware modeling, they are not designed to enhance physical plausibility reasoning in VLMs. Our work bridges this gap by integrating trajectory-guided attention into VLMs, enabling them to better track motion, detect temporal and spatial inconsistencies, and reason about physical implausibility. 2.3 Evaluating Physical Reasoning in VLMs Benchmarks for Physical Reasoning. Several benchmarks have been developed to evaluate physical reasoning in both general AI systems and vision-language models. InfLevel [50] draws from infant cognition studies and uses violation-of-expectation paradigm to evaluate whether models can detect core physical violations (e.g., continuity, Melnik et al. [32] provide taxonomy of physical reasoning benchmarks based on reasoning type (descriptive, predictive, explanatory, counterfactual) and level of interaction. Key passive benchmarks include: IntPhys [40]: tests implausibility via frame prediction mismatch. CoPhy [7]: evaluates prediction under modified initial conditions. CLEVRER [56]: includes causal/counterfactual Q/A based on synthetic CLEVR videos. Physion [8]: uses 3D simulations to test physical reasoning under gravity/collision. While these benchmarks are valuable, they often rely on simplified synthetic data with minimal motion complexity. ImplausiBench extends this space by focusing on highlevel physical plausibility in complex, real and generated videos with multiple objects and natural dynamics. Physical Bongard Problems [51] test abstract physical concepts (e.g., stability, containment) through symbolic visual puzzles. Although the format differs from our visually grounded Q/A setting, the shared goal is interpretable physical understanding. Virtual Tools [1] and PHYRE [4] involve interactive tasks in 2D physics simulations. These are excellent for studying 3 planning under physical constraints but are less applicable to VLMs, which operate in passive video understanding setting without agent interaction. Evaluating VLMs on Plausibility. PhysBench [14] introduces comprehensive test suite for evaluating object dynamics and spatial interactions in real-world videos, while KiVA [57] probes visual analogy-making in synthetic videos inspired by developmental psychology. However, neither benchmark explicitly targets implausible or counterfactual scenarios. Impossible Videos [3] moves closer to our goal by evaluating whether models can detect physically, socially, or biologically implausible events via multiple-choice questions on generated videos. However, as we show in Section 4, their format is vulnerable to linguistic and positional biases that allow LLMs [46, 55] to succeed without robust visual grounding. To address the limitations of prior benchmarks, we introduce ImplausiBench, 300-video benchmark for evaluating physical plausibility in VLMs using paired plausible and implausible videos across diverse domains (e.g., cooking, sports, vehicles, shadows, reflections). Unlike earlier efforts, ImplausiBench: targets both plausible and implausible temporal dynamics (e.g., levitation, teleportation, morphing, duplication) in real and generated videos, is rigorously designed to prevent shortcut exploitation via linguistic biases, and applies LLM-as-a-judge evaluation [17, 61, 27] to normalize scoring across architectures, validated against full human evaluation."
        },
        {
            "title": "3 Method",
            "content": "Understanding whether video obeys the laws of physics often requires reasoning about both spatial configurations and object motion across time. For example, detecting implausibilities like objects hovering, teleporting, duplicating, or disappearing demands joint understanding of structure and dynamics. To address this, we introduce TRAVL, general-purpose fine-tuning recipe for pretrained videolanguage models. TRAVL incorporates trajectory-aware masked attention to enhance temporal and physical reasoning in VLMs. We first describe the attention mechanism itself (Section 3.1), followed by its integration into existing VLM architectures (Section 3.2), and the fine-tuning dataset design that balances real and generated implausibilities (Section 3.3). 3.1 TRAVL Modern VLMs typically begin with vision encoder such as CLIP or SigLIP, which divides each frame into grid of non-overlapping patches and maps each patch to highdimensional embedding. These visual embeddings are projected into the language model input space through lightweight adapters, enabling joint video-text reasoning. However, most VLMs encode each frame independently, discarding motion continuity and lacking mechanisms to capture spatial-temporal dynamics. As result, they often fail to detect physically implausible motion patterns such as teleportation, deformation, or discontinuous trajectories. Goal. TRAVL introduces motion-aware attention into VLMs by combining intra-frame spatial attention with trajectory-guided temporal attention. Sparse patch trajectories, extracted using CoTracker [23], guide temporal connections, while spatial attention contextualizes patch structure within each frame. This dual attention design enables reasoning about both geometry (e.g., size, shape, occlusion) and continuity (e.g., persistence, gravity), without modifying the underlying vision or language backbones. We follow the patchification scheme of the vision encoder: e.g., 16 16 patches for CLIP (256 tokens per frame) or 27 27 for SigLIP (729 tokens per frame). Given frames, we extract patch embeddings zt,p Rd, where = 1..T , = 1..P . To preserve layout and order, we add sine-cosine 2D spatial encodings and 1D temporal encodings prior to attention. Intra-Frame Spatial Attention. Self-attention across all patches = 1..P within frame models intra-frame structure: yt,p = (cid:88) p=1 softmax (cid:19) (cid:18) t,pkt,p vt,p. The goal is to enhance detection of anomalies like duplication and deformation, aided by spatial positional encodings. Patchwise Trajectory Masking. To enforce temporal coherence, we track patch centers across frames and initialize new queries every frames for emerging objects. This produces sparse binary mask {0, 1}T linking patches that share motion trajectories. The mask restricts temporal attention to physically plausible continuities (e.g., rolling ball across time). Trajectory-Guided Temporal Attention. Temporal self-attention is restricted to valid links in M: (cid:88) yi = j:Mi,j =1 softmax (cid:16) kj (cid:17) vj. This enforces object persistence, enabling detection of implausibilities like teleportation or sudden morphing. Following both spatial and temporal attention, enriched patch embeddings are projected to the language space through learnable adapter. The vision encoder and language model remain frozen; only TRAVLs attention and projection modules are trained. Figure 2 shows an overview of TRAVLs main components. 4 Figure 3: Fine-tuning data pipeline. Our dataset is built in three stages: Stage 1 (Plausible Captioning): GPT-4o generates initial captions for real (plausible) videos, verified by human reviewers. Stage 2 (Feedback-Augmented Captioning): Human annotators provide short temporal feedback for each implausible video, which is combined with the original real caption to create complete description using GPT-4o. Stage 3 (QA Generation): Based on the final caption, GPT-4o produces temporally grounded question-answer pairs per video. This pipeline enables fine-grained supervision across controlled set of plausible and implausible variants. 3.2 Model Integration: TRAVL Across Architectures We validate TRAVL on two representative VLMs, demonstrating its modular integration in both pooled and dense token settings. Video-ChatGPT. Video-ChatGPT pools 256 CLIP patch tokens from 100 frames into temporal and spatial summaries before projection. With TRAVL, we replace pooling with intra-frame spatial attention and trajectory-guided temporal attention over sparse CoTracker masks. The resulting enriched tokens are passed through lightweight projection. Only these new modules are trained; CLIP and the LLM stay frozen. LLaVA-NeXT. LLaVA-NeXT encodes 64 frames via SigLIP into 729 patch tokens per frame. The original spatial pooling is replaced with TRAVLs spatial and chunked temporal attention (e.g., 416 frame windows), guided by sparse trajectories. The attended features are fused, pooled, and projected. TRAVL thus preserves input-output format while injecting motion-awareness. Our ablations confirmed that both spatial-only and temporal-only modules improved plausibility detection, but the full TRAVL design yielded the best results. 3.3 Fine-tuning Dataset To train TRAVL-equipped VLMs, we curate dataset that balances plausible and implausible videos while retaining broad video-language coverage. Our design emphasizes natural failure cases from generative models and balanced question types to ensure physically grounded learning. Figure 3 shows our training data generation pipeline. Physics-IQ Scenarios with Synthetic Violations. We begin with 66 base scenarios from Physics-IQ [34], each captured from three views (198 plausible videos). Using Runway, Pika, Sora, Kling, and Luma, we generate 894 variants conditioned on captions and first frames. Unlike prior works that induce violations through prompt engineering, we do not modify prompts to force implausibility. Instead, we capture natural failure cases of diffusion models. Human annotators review every generation, discarding approximately 70 cases where outputs were either plausible or static. For each retained implausible clip, annotators provide structured temporal descriptions of the violations as they occur (e.g., duck disappears midair, then reappears in different location). These fine-grained annotations are used to guide GPT-4o in producing detailed captions and generating balanced Q/A pairs. This annotation pipeline ensures that implausible events are faithfully represented. Impossible Videos. Since the multiple-choice format of Impossible Videos [3] has been shown to admit languageonly shortcuts (see Table 1), we do not use it as an evaluation benchmark. Instead, we repurpose 535 clips from its Physics category as training material. To broaden the coverage of implausibility types, we also generate 92 additional clips with Pika, applying the same human verification and temporally grounded annotation pipeline described above. This ensures consistency in how implausible events are identified, described, and paired with balanced Q/A prompts. Real-World QA from Video-ChatGPT. To maintain generalization beyond synthetic distortions, we include 5 Figure 4: Example from ImplausiBench. For each scenario, we include both real (plausible) and generated (implausible) video that share the same initial scene and visual style. Each pair is annotated with shared multiplechoice question containing three plausible, three implausible, and one None of the above option. The correct answer depends on which version of the video is shownensuring that models must ground their predictions in visual-temporal evidence rather than language alone. 1,763 diverse clips from the Video-ChatGPT training set, paired with their original QA annotations. We filter out long clips (>800 frames) to ensure patch trajectories remain temporally meaningful. Dataset Statistics. The final dataset contains 3,482 videos and 19,708 QA pairs. key design principle is balancing the types of questions across both plausible and implausible videos to avoid dataset skew. In particular, plausibility-style questions (e.g., Does the video look real or implausible?) are deliberately posed not only for implausible clips but also for real ones. By ensuring that every QA type is mirrored across both categories, we prevent models from exploiting correlations between question form and video class. This balanced QA distribution requires models to ground their answers in visual evidence rather than annotation patterns. Additional details and examples are provided in Supplementary section B."
        },
        {
            "title": "ImplausiBench",
            "content": "In this section, we present ImplausiBench, diagnostic benchmark designed to test whether video-language models can detect physically implausible events using visualtemporal cues alone. It consists of paired plausible and implausible videos constructed to minimize language-only shortcuts and isolate grounded physical reasoning. Benchmark Construction. ImplausiBench comprises 150 real-world videos depicting physically plausible scenes. For each, we synthesize an implausible counterpart using state-of-the-art diffusion-based video models (e.g., Pika [37], Runway [41], Kling [24], CogVideo [16], LTX [18], Pyramid-Flow [22]), conditioned on GPT-4ogenerated caption and the first frame of the real video. If the generated result remains plausible after manual inspection, we regenerate until clear physical violation is introduced. The resulting videos capture broad spectrum of implausibility types, loosely grouped into six categories: motion anomalies (e.g., levitation, reversal), object continuity violations (e.g., teleportation, disappearance), structural transformations (e.g., deformation, splitting), unnatural interactions (e.g., passing through solids), appearance shifts (e.g., sudden color or size changes), and implausible state changes (e.g., self-filling, melting). These failure modes align with key principles in intuitive physics and reflect typical breakdowns in generative video models. Multiple Choice Format. Each plausibleimplausible video pair is annotated with single shared multiple-choice question (MCQ) containing seven answer options: three describing plausible outcomes, three describing implausible ones, and one None of the above option. Unlike prior benchmarks that rely heavily on automated generation, all answer options in ImplausiBench are manually curated by annotators to ensure clarity, precision, and grounding in the visual content. To guard against shortcut exploitation, we perform blind test validation: off-the-shelf LLMs are asked to answer the MCQs without access to the video. Whenever models succeed above chance by exploiting linguistic or positional patterns, we revise the answer set until such shortcuts are eliminated. This rigorous process makes ImplausiBench resistant to language-only biasesa key limitation of datasets like Impossible Videosand ensures that correct answers depend on visually grounded reasoning. Due to the intensive manual effort required to design, review, and validate each question, the benchmark is intentionally limited to 300 videos, prioritizing annotation quality over scale. Table 1: Blind test multiple-choice accuracy (no video shown). Random chance is 14.3% for ImplausiBench (7 options) and 20% for Impossible Videos (5 options). Model Impossible Videos ImplausiBench (implausible) ImplausiBench (plausible) 51.2% GPT-4o Qwen2.5-7B 46% 20% Random 22% 20% 14.3% 21.3% 18.6% 14.3% Comparison to Existing Benchmarks. We apply the blind test protocol to both ImplausiBench and the Physics category of the Impossible Videos benchmark [3]. In Impossible Videos, each MCQ presents one plausible and four implausible answers, with prompts that implicitly bias models toward selecting an implausible option, even when no video is shown. This evaluation format permits shortcut exploitation: as Table 1 shows, GPT-4o and Qwen2.5 achieve well above chance-level accuracy without visual input. ImplausiBench avoids this pitfall by balancing plausible and implausible choices, including None of the above option, and filtering out easy distractors. As result, blind-test accuracy drops closer to chance. Why We Do Not Use Impossible Videos for Evaluation. Since Impossible Videos allows models to exploit linguistic and positional biases without grounding in visual evidence, we do not rely on it for evaluation. Instead, we repurpose its videos for training, where implausible content remains valuable for supervision. ImplausiBench, by contrast, enforces stricter correctness (models must succeed on both plausible and implausible versions of each scenario) and is adversarially constructed to resist such shortcuts. This makes it more rigorous benchmark for assessing physical reasoning and visual grounding in VLMs."
        },
        {
            "title": "5 Results",
            "content": "Evaluation Protocol. We evaluate models on ImplausiBench, split into Generated and Real subsets (150 videos each). For each subset we report accuracy in % under two metrics: (i) Human metric, where annotators watched each video and judged whether the models caption correctly described it, and (ii) an LLM-judge metric. To ensure comparability across models with open-ended vs. multiple-choice formats, we adopt an LLM-as-a-judge protocol [17, 61, 27]. Each model answers the same openended prompt (Do the events in the video appear to be real, following physics principles, or are they implausible? Why?); GPT-4o then maps the response to the benchmarks multiple-choice options. The LLM-judge is explicitly instructed to be strict: if an answer is partially correct or omits critical details about the violation, it often defaults to the None of the above option rather than granting partial credit by picking the closest answer to the caption. This conservative scoring reduces the risk of inflating model performance but also leads to lower absolute scores compared to human judgment. To anchor results, Table 2: Evaluation on ImplausiBench, split into Implausible and Real subsets (150 videos each). Numbers are accuracies in %. As the gold standard, we report the Human evaluation metric, based on user judgments of correctness for each VLM output. For comparison, we also report the LLM-judge evaluation, which provides stricter automatic assessment. Implausible (150) Human LLM Human Real (150) Model GPT-4o Gemini 2.5 Pro Proprietary 32.7 41.3 Open-Source LLM 64.0 78. 74.7 76.0 55.3 26.0 31.3 62.7 23.3 31.3 21.3 29.3 12.0 4.7 0.0 2.7 7.3 2.7 22.0 28.7 84.7 100.0 96.7 92.7 72.0 39.3 42.7 98.7 45.3 47.3 Qwen2.5VL InternVideo2.5 Video-ChatGPT Pre-trained Video-ChatGPT SFT Video-ChatGPT TRAVL LLaVA-NeXT Pre-trained LLaVA-NeXT SFT LLaVA-NeXT TRAVL 18.7 12.7 0.0 6.0 12.0 3.3 34.0 52.7 human annotators reviewed every model output, providing the gold standard Human metric. Importantly, while the LLM-judge yields stricter scores, the relative trends between models are preserved, making it reliable and cautious proxy for large-scale evaluation. Scoring on ImplausiBench. We award credit separately on the Generated and Real subsets  (Table 2)  . This design makes performance on synthetic violations (Generated) and naturally plausible videos directly comparable, while separating human-verified correctness from automated judging. dcjbfjuhcbguncgnjrihgcckcigjfvtv TRAVL Improves Implausibility Detection. Across both backbones, adding TRAVL yields consistent gains on the Generated subset. On the Real subset, pretrained models can appear stronger, but this is misleading: they achieve high scores by defaulting to plausible predictions while failing almost entirely on implausible cases. fairer comparison is against the SFT baseline, which is trained on the same data distribution but without TRAVL. Relative to SFT, TRAVL improves performance on both subsets under both Human and LLM-judge metrics. For instance, LLaVA-NeXT with TRAVL outperforms SFT by 18.7% on implausible videos and 2.0% points on real ones (Human metric). Similar improvements hold for Video-ChatGPT. These results confirm that spatial and trajectory-guided temporal attention modules strengthens motion grounding and detection of physical violations, while preserving general plausibility understanding. 5.1 Ablation Studies To better understand the contributions of TRAVLs components, we ablate its two attention modules: spatial selfattention and trajectory-guided temporal attention. Both variants are trained with the same settings as TRAVL but 7 LLaVA-NeXT), temporal attention is applied over short video chunks (416 frames) to maintain tractability, which limits long-range reasoning; future work could explore memory-efficient attention to enable full-sequence modeling. Finally, our mediated evaluation relies on GPT-4o to judge model outputs, introducing dependency on another language models interpretation. Despite these limitations, TRAVL provides lightweight and extensible strategy for integrating temporal structure into VLMs, and ImplausiBench offers high-fidelity benchmark for assessing visual-temporal physical understanding."
        },
        {
            "title": "7 Conclusion",
            "content": "We introduced TRAVL, trajectory-aware fine-tuning framework that improves physical reasoning in VLMs by integrating spatial and trajectory-aware temporal attention and plausibility supervision. TRAVL enables pretrained VLMs to better detect implausible motion patterns with minimal modifications to their vision or language backbones. We demonstrated its effectiveness on both VideoChatGPT and LLaVA-NeXT, showing consistent gains in physical plausibility judgment. To enable more rigorous evaluation, we proposed ImplausiBench, benchmark designed to eliminate linguistic shortcuts and isolate visual-temporal understanding. Our blind test protocol confirms that ImplausiBench is significantly more robust to shortcut exploitation than existing benchmarks such as Impossible Videos, offering clearer signal of grounded physical reasoning."
        },
        {
            "title": "8 Acknowledgment",
            "content": "This research was partially funded by the Ministry of Education and Science of Bulgaria (support for INSAIT, part of the Bulgarian National Roadmap for Research Infrastructure). The authors would like to thank Raha Ahmadi for supporting this project by helping with ImplausiBench dataset. Table 3: Ablation on LLaVA-NeXT. Evaluation on ImplausiBench. Numbers are accuracies in %. Model Implausible (150) LLM Human Real (150) Human LLM Pretrained LLaVA-NeXT LLaVA-NeXT SFT Temporal-only Attention Spatial-only Attention TRAVL (Ours) 3.3 34.0 46.0 42.7 52.7 2.7 22.0 24.0 26.7 28.7 98.7 45.3 41.3 48.7 47.3 62.7 23.3 22.0 30.7 31. with only one component active at time. This reveals whether improvements in implausibility detection arise primarily from intra-frame spatial grounding or trajectoryguided temporal attention. The results are shown in Table Table 3. Findings. Both spatial-only and temporal-only variants improve over supervised fine-tuning, but neither matches the full TRAVL model. This indicates that spatial and temporal attention provide complementary benefits: spatial attention enhances detection of implausible structures (e.g., overlaps, deformations), while temporal attention improves motion continuity tracking. Together, they yield the strongest overall gains in plausibility reasoning. Binary Classification Results. We also evaluate models in binary plausibility classification setup, where the task is to label each video as plausible or implausible. This metric does not probe reasoning quality, but provides complementary view of discrimination ability. As shown in Table 4, TRAVL improves implausibility detection while maintaining plausible video accuracy, with ablated variants again performing between SFT and full TRAVL. Table 4: Binary classification accuracy (%) of LLaVANeXT models on ImplausiBench. Model Real (Plausible) Implausible LLaVA-NeXT Pre-trained LLaVA-NeXT SFT LLaVA-NeXT Temporal-only Attention LLaVA-NeXT Spatial-only Attention LLaVA-NeXT TRAVL (Ours) 98.7 45.3 52.0 53.3 57.3 10.0 83.3 82.7 84.7 84."
        },
        {
            "title": "6 Limitations and Future Work",
            "content": "While TRAVL advances temporal modeling and physical plausibility detection in VLMs, some limitations remain. Our fine-tuning dataset is modest in size and limited in diversity relative to real-world video content; expanding to broader categories of physical implausibility and scenarios, potentially via automated generation pipelines or simulation environments, could improve generalization. TRAVL also depends on externally generated patch trajectories, introducing computational overhead and sensitivity to visual artifacts such as occlusion or blur, and integrating learned or differentiable tracking directly into the model In dense-input settings (e.g., may improve robustness."
        },
        {
            "title": "References",
            "content": "[1] K. R. Allen, K. A. Smith, and J. B. Tenenbaum. Rapid trialand-error learning with simulation supports flexible tool use and physical reasoning. Proceedings of the National Academy of Sciences, 117(47):2930229310, 2020. 3 [2] Piyush Bagad, Makarand Tapaswi, and Cees Snoek. Do video-language foundation models have sense of time? arXiv preprint arXiv:2301.02074, 2023. 2, 3 [3] Zechen Bai, Hai Ci, and Mike Zheng Shou. Impossible videos. arXiv preprint arXiv:2503.14378, 2025. 2, 4, 5, 7, 13 [4] Anton Bakhtin, Laurens van der Maaten, Justin Johnson, Laura Gustafson, and Ross Girshick. Phyre: new benchmark for physical reasoning. In Advances in Neural Information Processing Systems (NeurIPS), volume 32, 2019. 3 [5] Vahid Balazadeh, Mohammadmehdi Ataei, Hyunmin Cheong, Amir Hosein Khasahmadi, and Rahul Krishnan. Synthetic vision: Training vision-language models to understand physics. arXiv preprint arXiv:2412.08619, 2024. 2 [6] Hritik Bansal, Clark Peng, Yonatan Bitton, Roman Goldenberg, Aditya Grover, and Kai-Wei Chang. Videophy-2: challenging action-centric physical commonsense evaluation in video generation. arXiv preprint arXiv:2503.06800, 2025. [7] Fabien Baradel, Natalia Neverova, Julien Mille, Greg Mori, and Christian Wolf. Cophy: Counterfactual learning of physical dynamics, 2019. 3 [8] Daniel M. Bear, Elias Wang, Damian M. Mrowca, Felix J. Binder, Hsiao-Yu Fish Tung, R. T. Pramod, Craig Holdaway, Shuangfei Tao, Kevin Smith, Fangyuan Sun, et al. Physion: Evaluating physical prediction from vision in humans and machines, 2021. 3 [9] T. Brooks et al. Video generation models as world https://openai.com/research/ simulators. video-generation-models-as-world-simulators, 2024. Accessed: 2025-05-15. 1 [10] Hila Chefer, Uriel Singer, Amit Zohar, Yuval Kirstain, Adam Polyak, Yaniv Taigman, Lior Wolf, and Shelly Sheynin. Videojam: Joint appearance-motion representations for enhanced motion generation in video models. arXiv preprint arXiv:2502.02492, 2025. 3 [11] Jiacheng Chen, Tianhao Liang, Sherman Siu, Zhengqing Wang, Kai Wang, Yubo Wang, Yuansheng Ni, Wang Zhu, Ziyan Jiang, Bohan Lyu, Dongfu Jiang, Xuan He, Yuan Liu, Hexiang Hu, Xiang Yue, and Wenhu Chen. Mega-bench: Scaling multimodal evaluation to over 500 real-world tasks, 2024. 2 [12] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. [13] Anoop Cherian, Radu Corcodel, Siddarth Jain, and Diego Romeres. Llmphy: Complex physical reasoning using large language models and world models, 2024. 2 [14] Wei Chow, Jiageng Mao, Boyi Li, Daniel Seita, Vitor Guizilini, and Yue Wang. Physbench: Benchmarking and enhancing vision-language models for physical world understanding. arXiv preprint arXiv:2501.16411, 2025. 2, 3, 4 [15] Yuren Cong, Mengmeng Xu, Christian Simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. Flatten: optical flow-guided attention for consistent text-to-video editing. arXiv preprint arXiv:2310.05922, 2023. 3 [16] Yujun Du, Yong Zhang, Zhuoqian Chen, Qianxi Zhang, Meng Ye, Ying Zhu, Liang Pan, Shixiang Gu, Zhendong Wang, and Zhoujun Lin. Cogvideox: Scaling up video generation with multimodal pretraining and diffusion decoding, 2024. 1, 6 [17] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. 4, 7 [18] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. [19] Wenyi Hong, Yean Cheng, Zhuoyi Yang, Weihan Wang, Lefan Wang, Xiaotao Gu, Shiyu Huang, Yuxiao Dong, and Jie Tang. Motionbench: Benchmarking and improving finegrained video motion understanding for vision language models. arXiv preprint arXiv:2501.02955, 2025. 2 [20] Ziqi Huang, Yinan He, Jiashuo Yu, Fan Zhang, Chenyang Si, Yuming Jiang, Yuanhan Zhang, Tianxing Wu, Qingyang Jin, Nattapol Chanpaisit, Yaohui Wang, Xinyuan Chen, Limin Wang, Dahua Lin, Yu Qiao, and Ziwei Liu. Vbench: Comprehensive benchmark suite for video generative models. CVPR, 2024. 2 [21] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc V. Le, Yunhsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision, 2021. 2 [22] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video generative modeling. arXiv preprint arXiv:2410.05954, 2024. 6 [23] Nikita Karaev, Iurii Makarov, Jianyuan Wang, Natalia Neverova, Andrea Vedaldi, and Christian Rupprecht. Cotracker3: Simpler and better point tracking by pseudolabelling real videos, 2024. 2, 4 [24] Kling Team. Kling ai: Next-generation video generation model, 2024. https://klingai.com. 6 [25] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, Kathrina Wu, Qin Lin, Junkun Yuan, Yanxin Long, Aladdin Wang, Andong Wang, Changlin Li, Duojun Huang, Fang Yang, Hao Tan, Hongmei Wang, Jacob Song, Jiawang Bai, Jianbing Wu, Jinbao Xue, Joey Wang, Kai Wang, Mengyang Liu, Pengyu Li, Shuai Li, Weiyan Wang, Wenqing Yu, Xinchi Deng, Yang Li, Yi Chen, Yutao Cui, Yuanbo Peng, Zhentao Yu, Zhiyu He, Zhiyong Xu, Zixiang Zhou, Zunnan Xu, Yangyu Tao, Qinglin Lu, Songtao Liu, 9 Dax Zhou, Hongfa Wang, Yong Yang, Di Wang, Yuhong Liu, Jie Jiang, and Caesar Zhong. Hunyuanvideo: systematic framework for large video generative models, 2024. 1 [26] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. 2 [27] Dawei Li, Bohan Jiang, Liangjie Huang, Alimohammad Beigi, Chengshuai Zhao, Zhen Tan, Amrita Bhattacharjee, Yuxuan Jiang, Canyu Chen, Tianhao Wu, et al. From generation to judgment: Opportunities and challenges of llm-as-a-judge. arXiv preprint arXiv:2411.16594, 2024. 4, [28] Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Lou, Limin Wang, and Yu Qiao. Mvbench: comprehensive multimodal video understanding benchmark. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), page 2219522206. IEEE, June 2024. 2 [29] Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, Kai Chen, and Dahua Lin. Mmbench: Is your multi-modal model an all-around player?, 2023. 2 [30] Yuanxin Liu, Shicheng Li, Yi Liu, Yuxiang Wang, Shuhuai Ren, Lei Li, Sishuo Chen, Xu Sun, and Lu Hou. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. 2 [31] Muhammad Maaz, Hanoona Rasheed, Salman Khan, and Fahad Shahbaz Khan. Video-chatgpt: Towards detailed video understanding via large vision and language models. arXiv preprint arXiv:2306.05424, 2023. 2 [32] Andreas Melnik, Raphael Schiewer, Markus Lange, Alexandru Muresanu, Maysam Saeidi, Animesh Garg, and Helge Ritter. Benchmarks for physical reasoning ai, 2023. 3 [33] Fanqing Meng, Jiaqi Liao, Xinyu Tan, Wenqi Shao, Quanfeng Lu, Kaipeng Zhang, Yu Cheng, Dianqi Li, Yu Qiao, and Ping Luo. Towards world simulator: Crafting physical commonsense-based benchmark for video generation. arXiv preprint arXiv:2410.05363, 2024. [34] Saman Motamed, Laura Culp, Kevin Swersky, Priyank Jaini, and Robert Geirhos. Do generative video modarXiv preprint els understand physical principles? arXiv:2501.09038, 2025. 1, 5, 12 [35] Xinyu Ning, Li Sun, and Mingkui Tan. Video-bench: unified benchmark for video understanding. arXiv preprint arXiv:2306.11034, 2023. 2 [36] Mandela Patrick, Dylan Campbell, Yuki M. Asano, Ishan Misra, Florian Metze, Christoph Feichtenhofer, Andrea Vedaldi, and João F. Henriques. Keeping your eye on the ball: Trajectory attention in video transformers, 2021. 3 [37] Pika Labs. Pika 1.0: Text-to-video generation, 2024. https://pika.art/blog/pika-1-0. 6 [38] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2025. 2 [39] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. 1, 2 [40] Raphaël Riochet, Miguel Yahia Castro, Mathieu Bernard, Adam Lerer, Rob Fergus, Veronique Izard, and Emmanuel Dupoux. Intphys: framework and benchmark for visual intuitive physics reasoning, 2018. 3 [41] Runway Team. Runway: Multimodal generative models for creative video, 2024. https://research.runwayml. com. 1, 6 [42] Dvir Samuel, Matan Levy, Nir Darshan, Gal Chechik, and Rami Ben-Ari. Omnimattezero: Fast training-free omnimatte with pre-trained video diffusion models, 2025. 3 [43] Luca Schulze Buschoff, Konstantinos Voudouris, Elif Akata, Matthias Bethge, Joshua Tenenbaum, and Eric Schulz. Testing the limits of fine-tuning to improve reasoning in vision language models. arXiv e-prints, pages arXiv2502, 2025. [44] Parshin Shojaee, Kazem Meidani, Shashank Gupta, Amir Barati Farimani, and Chandan K. Reddy. LLM-SR: Scientific equation discovery via programming with large language models. In The Thirteenth International Conference on Learning Representations, 2025. 2 [45] Jingqun Tang, Qi Liu, Yongjie Ye, Jinghui Lu, Shu Wei, Chunhui Lin, Wanqing Li, Mohamad Fitri Faiz Bin Mahmood, Hao Feng, Zhen Zhao, Yanjie Wang, Yuliang Liu, Hao Liu, Xiang Bai, and Can Huang. Mtvqa: Benchmarking multilingual text-centric visual question answering, 2024. 2 [46] OpenAI Team. Gpt-4o system card, 2024. 4 [47] Thomas Unterthiner, Sjoerd van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges, 2018. 1 [48] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 2 [49] Yi Wang, Kunchang Li, Yizhuo Li, Yinan He, Bingkun Huang, Zhiyu Zhao, Hongjie Zhang, Jilan Xu, Yi Liu, Zun Wang, et al. Internvideo: General video foundation models via generative and discriminative learning. arXiv preprint arXiv:2212.03191, 2022. [50] Luca Weihs, Andrew Yuile, Renée Baillargeon, Cynthia Fisher, Gary Marcus, Roozbeh Mottaghi, and Aniruddha Kembhavi. Benchmarking progress to infant-level physical reasoning in ai. Transactions on Machine Learning Research, 2022. 3 [51] Erik Weitnauer and Helge Ritter. Physical bongard problems. In Artificial Intelligence Applications and Innovations: 8th IFIP WG 12.5 International Conference, AIAI 10 2012, Halkidiki, Greece, September 27-30, 2012, Proceedings, Part 8, pages 157163. Springer, 2012. 3 [52] Yilin Wu, Ran Tian, Gokul Swamy, and Andrea Bajcsy. From foresight to forethought: Vlm-in-the-loop policy steering via latent alignment. arXiv preprint arXiv:2502.01828, 2025. 2 [53] Zeqi Xiao, Wenqi Ouyang, Yifan Zhou, Shuai Yang, Lei Yang, Jianlou Si, and Xingang Pan. Trajectory attention for fine-grained video motion control, 2024. [54] Jun Xu, Tao Mei, and Ting Yao. Video question answering via attribute-augmented attention network learning. In Proceedings of the 25th ACM international conference on Multimedia, pages 461469, 2017. 2 [55] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. 4 [56] Kexin Yi, Chuang Gan, Yunzhu Li, Pushmeet Kohli, Jiajun Wu, Antonio Torralba, and Joshua B. Tenenbaum. Clevrer: Collision events for video representation and reasoning, 2019. 3 [57] Eunice Yiu, Maan Qraitem, Anisa Noor Majhi, Charlie Wong, Yutong Bai, Shiry Ginosar, Alison Gopnik, and Kate Saenko. Kiva: Kid-inspired visual analogies for testing large multimodal models. arXiv preprint arXiv:2407.17773, 2024. 4 [58] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV). IEEE, October 2023. 2 [59] Chenyu Zhang, Daniil Cherniavskii, Andrii Zadaianchuk, Antonios Tragoudaras, Antonios Vozikis, Thijmen Nijdam, Derck WE Prinzhorn, Mark Bodracska, Nicu Sebe, and Efstratios Gavves. Morpheus: Benchmarking physical reasoning of video generative models with real physical experiments. arXiv preprint arXiv:2504.02918, 2025. [60] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video instruction tuning with synthetic data. arXiv preprint arXiv:2410.02713, 2024. 2 [61] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 4, 7 11 TRAVL: Supplementary Material This supplementary material provides expanded details supporting our main paper. We begin by describing the GPT-4o-based evaluation protocol we use to score openended VLM responses against multiple-choice ground truth. We then present qualitative visualizations from both the Impossible Videos and our newly proposed ImplausiBench datasets, highlighting model successes and failures across different training stages. Quantitative results are further broken down to analyze tradeoffs between plausibility sensitivity and implausibility detection. We outline the structure of our fine-tuning dataset and provide the prompt design used for generating temporal and physics-based QA pairs in Pseudocode 2. In Section C, we detail the construction of ImplausiBench. Finally, we document TRAVLs model architecture, training setup, practical observations, and how it compares with prior physical reasoning benchmarks. To view the example videos referenced throughout, please open result_viewer.html in the supplementary folder. Understanding the PlausibilityImplausibility Tradeoff. Table 2 reports accuracy as percentage of correct predictions out of 150 videos for both plausible and implausible variants in ImplausiBench. Untuned models such as Video-ChatGPT and LLaVA-NeXT show high accuracy on plausible videos but low accuracy on implausible ones, indicating strong tendency to default to plausible interpretationseven when physical violations are present. With TRAVL fine-tuning, both models improve significantly on implausible videos: Video-ChatGPT increases from 0.0% to 12.0%, and LLaVA-NeXT from 3.3% to 52.7%. However, this comes at the cost of reduced accuracy on plausible videos. This suggests increased sensitivity to physical inconsistencies, but also higher rate of false positives on real videos. However, TRAVL still performs better on both plausible and implausible videos compared to the same backbone trained via SFT. This shift in behavior may be partly due to the fine-tuning data, which is skewed toward implausible examples and contains more limited set of real, plausible scenarios. Expanding the range of plausible examples during training may help to better calibrate model confidence across both types of videos."
        },
        {
            "title": "A Results in Detail",
            "content": "B Fine-Tuning Dataset LLM-as-a-Judge Evaluation. To evaluate whether vision-language models (VLMs) open-ended response corresponds to the correct multiple-choice answer, we adopt an LLM-as-a-judge protocol with GPT-4o. Each VLM is first prompted to provide an open-ended explanation of physical plausibility. GPT-4o then receives this explanation together with the corresponding multiple-choice question and candidate options, and is instructed to select the option that best matches the VLMs reasoning. Importantly, GPT-4o is not told the ground truth during evaluation; its role is to strictly map the VLMs free-form output to one of the benchmarks predefined answers. We validated this judge protocol using blind probes (no video input) to ensure it does not exploit language bias. To prevent partial credit, we also include None of the above fallback option in every question. In Pseudocode 1 we show the exact function used to construct judge prompts. Visualizations of VLM Outputs. Figure 6 showcases qualitative examples from both Video-ChatGPT and LLaVA-NeXT backbones, along with their supervised and TRAVL fine-tuned variants. The examples span four implausible videos from the Impossible Videos benchmark and four plausibleimplausible video pairs from our ImplausiBench benchmark. We highlight diverse model behaviors, including cases where different versions succeed (check mark) or fail (cross) in answering the open-ended prompt: Do the events in the video appear to follow physics principles or not? Why? For full visualizations and model outputs, please view result_viewer.html in the supplementary folder. Our fine-tuning dataset comprises 3,482 unique videos and 19,708 question-answer (QA) pairs. The dataset integrates four sources: (1) the Video-ChatGPT training set, (2) the Physics-IQ benchmark along with newly generated implausible variants, (3) 535 clips from the Physics category of Impossible Videos, and (4) 92 additional implausible clips generated with Pika 1.5. Together, these sources provide broad balance of real and generated content, and expose models to diverse motion contexts and implausibility types. Video-ChatGPT Subset. We include 1,763 videos from the original Video-ChatGPT training set, filtering to those shorter than 800 frames so that motion trajectories remain temporally coherent. Each video is captioned with GPT-4o, and we regenerate QAs to provide richer detail. In addition to general video-understanding queries, we introduce plausibility-oriented questions so that both plausible and implausible clips are associated with comparable QA types. This balancing prevents models from learning shortcut correlations between question style and video category. Physics-IQ Scenarios. We take 66 base scenarios from Physics-IQ [34], each recorded from three viewpoints (198 total plausible videos). These scenarios illustrate core physics principles in short real-world clips. To expand this set, we generate 894 implausible variants using imageto-video models (Pika, Runway, Sora, Kling, and Luma), conditioned on the first frame and caption of the original scenario. Human annotators review all generations, discarding around 70 plausible or static cases, and label the retained clips by violation type (e.g., floating, teleporta12 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 def ask_gpt(client, model, question, options, caption, video_name, attempt_limit=3): options_text = \"n\".join([f\"{k}. {v}\" for k, in options.items()]) prompt_messages = [ { \"role\": \"user\", \"content\": ( \"You are reasoning assistant evaluating the output of \" \"video-language model.nn\" \"The VLM model has watched video and described the video as:n\" f\"{caption}nn\" f\"Based on the above answer and analyzing its reasoning to the \" f\"question of: {question}, select which of the following \" \"multiple-choice options best matches the model's reasoning.n\" \"Your judgment should be based only on the VLM's output.n\" \"Respond with the letter of the best matching option.nn\" f\"Options:n{options_text}\" ) } ] Pseudocode 1: LLM-as-judge prompt tion). Annotators also provide concise temporal feedback (e.g., duck disappears midair), which GPT-4o incorporates into detailed captions and 36 QA pairs per video. prevents models from exploiting phrasing such as what makes this implausible? and ensures that implausibility is only reflected in the answers. Below is the exact prompt: Impossible Videos Scenarios. As shown in Table 1, the multiple-choice questions in Impossible Videos [3] can be solved by LLMs using linguistic biases alone, making it unsuitable for evaluation. Instead, we repurpose 535 videos from the physics category as training data. Each video is captioned with GPT-4o, which is given access to the correct physical violation. These captions are then passed through the same QA-generation pipeline, producing 36 QAs per video. Additional Implausible Videos. Finally, we generate 92 diverse implausible clips using Pika 1.5. Captions are sampled from GPT-4o to cover broad range of everyday scenarios. Each clip is manually inspected to ensure the presence of clear implausibility, captioned accordingly, and passed through the same QA pipeline. This set complements Physics-IQs object-limited scenarios with more generic violations. B.1 QA Generation Prompts To create fine-grained temporal and physical reasoning QA pairs, we used GPT-4o with structured prompts. Each prompt takes as input (1) the scenario name and (2) manually verified caption describing the video. We design separate instructions for plausible and implausible videos to avoid leakage of implausibility cues in the questions. Implausible Videos. For videos containing physically unrealistic events, the prompt explicitly instructs GPT-4o that the clip is implausible. The generated answers must clearly explain why, but the questions remain neutral. This Plausible Videos. For real videos, the prompt is nearly identical, except that it specifies the clips are physically realistic. In this case, the answers must highlight why the events follow physical principles, again without the questions giving away plausibility."
        },
        {
            "title": "C ImplausiBench Construction",
            "content": "To construct ImplausiBench, we selected 150 real-world videos spanning diverse range of everyday scenarios, including food preparation, vehicles, animals, nature, and household activities. We first used GPT-4o to generate captions for each real video and manually verified their correctness to ensure high-quality textual descriptions. Next, we created implausible counterparts for each video by prompting state-of-the-art image-to-video models using the first frame of the real video and guiding them to generate physically unrealistic continuations. To evaluate model understanding of physical plausibility, we designed multiple-choice questions for each video pair. These questions were constructed with the explicit goal of minimizing blind-test accuracy of language models (LLMs). This involved manual crafting of challenging distractors and iterative refinement to prevent models from relying on linguistic shortcuts alone. for example, Gemini 2.5-pro, This makes ImplausiBench particularly challenging benchmark: the bestperforming model in our experiments, only achieved 41% on implausible videos. We envision this benchmark as valuable progress indicator for future models aspiring to reason about physical realism in videos. 13 Some qualitative examples are shown in Figure 6. To view them in video format, please view supplementary file result_viewer.html."
        },
        {
            "title": "D TRAVL Details and Model Specifications",
            "content": "Video-ChatGPT Integration. TRAVL is inserted between the frozen CLIP encoder and the language adapter. We apply spatial self-attention within each frame (256 tokens) and trajectory-aware temporal attention across tracked patches (100 tokens). The resulting features are aggregated and projected via 2-layer MLP to form the final 356-token sequence, which is passed to the language model. LLaVA-NeXT Integration. TRAVL receives 64729 SigLIP patch tokens. We apply intra-frame spatial attention for each set of 729 tokens, followed by inter-frame temporal attention over patch-aligned trajectories using sparse flow masks. To manage memory, we chunk temporal attention into overlapping windows of 416 frames. Features are then projected and passed to the frozen LLaVA adapter. Given that implausible actions happen suddenly (levitation, multiplication, vanishing, etc.), 16 frames is enough to detect such events. However, it is worth noting that other types of reasoning such as longer video understanding could be hurt by this shorter attention chunking. Batch size: 8 for Video-ChatGPT, 2 for LLaVA-"
        },
        {
            "title": "NeXT",
            "content": "Hardware: 4x NVIDIA H200 GPUs Epochs:"
        },
        {
            "title": "E Observations",
            "content": "In this section, we share key observations made during the development and experimentation of TRAVL. Our aim is to highlight practical insights and challenges that arose while adapting trajectory-aware attention for videolanguage models (VLMs). We hope these reflections are useful to researchers working on related problems in multimodal learning, video understanding, and physical reasoning, and that they serve as roadmap for future iterations of TRAVL. Many of the issues we encountered relate to data scale, architecture compatibility, and training efficiency, which we discuss below in detail. Fine-tuning Frame Rate. In our current fine-tuning dataset, we retain each videos original frame rate (FPS). natural extension is to augment the dataset by re-encoding videos at different FPS values. This would expose the model to greater variety of temporal resolutions and increase the number of training frames, potentially improving the robustness of temporal attention and enhancing downstream performance. Trajectory-Guided Sparse Attention Masking. To enable temporal reasoning over object motion, we construct sparse attention mask using CoTracker to track the center of each spatial patch across time. Each video frame is divided into grid of patches that matches the resolution of the vision encoder (e.g., 27 27 for LLaVA-NeXT). To account for newly appearing objects or major scene changes, we reinitialize set of track points at the center of each patch every frames. Each tracked point is assigned patch ID at every visible frame, and we construct binary attention mask that connects patch pairs sharing common trajectory. This sparse mask is then used to constrain temporal self-attention, enabling the model to focus on motion-consistent features while significantly reducing computational cost. Impact of Token Count. We explored increasing the number of tokens per frame in the Video-ChatGPT + TRAVL setup. The original vision-language projector in Video-ChatGPT is trained on 356 tokens, derived from spatial and temporal pooling of CLIP patch features. To increase token granularity, we experimented with reducing the pooling stride, thus preserving more patch tokens across time. However, we consistently found that these configurations underperformed compared to the original 356-token setup. We hypothesize that this degradation stems from mismatch with the pretrained projector, which is specialized for the 356-token format. Without reinitializing or retraining the projector from scratch, deviating from this token structure appears to hinder alignment and performance. Trajectory Mask Calculation. Pseudocode 3 describes how we compute the patch-based trajectory mask. Attention Mechanism. Pseudocode 4 describes the masked attention mechanism using the sparse trajectory mask. Training Details. Optimizer: AdamW Learning rate: 1 104 for attention modules, 5 105 for projector Scaling the Dataset. Our current dataset is currently modest in its coverage of different scenarios. Future efforts should focus on expanding the dataset not only in terms of the diversity of implausibility types, but also with more varied and complex plausible videos. As shown in Table 2, while improving overall implausibility detection, TRAVL hurts the models performance on plausible videos compared to pretrained model. We attribute this effect to the limited distribution of plausible content in the training set. key bottleneck in scaling arises from the lack of timestamped captions in the Video-ChatGPT training data. Without temporally grounded annotations, we cannot easily repurpose long videos into shorter segments with accurate supervision. To preserve coherent motion trajectories, we limit our training set to videos under 800 framesensuring that sampled frames are not spaced so far apart that motion becomes ambiguous or incoherent. Addressing this constraint remains an open challenge for future work, and we believe that improved timestamp alignment or synthetic supervision could unlock much larger and more balanced fine-tuning corpora. Trajectory Masks. We used CoTracker to generate sparse trajectory masks for TRAVL. To account for new objects entering the scene or significant scene changes, we reinitialize patch tracking every frames. While effective, this approach introduces computational overhead, especially when applied on-the-fly during training. Due to speed constraints, we limited tracking to single point per patch (i.e., the center pixel). However, denser trackinge.g., tracking multiple points per patchcould potentially yield richer motion cues and further enhance the models understanding of dynamic interactions. Exploring more efficient or precomputed trajectory pipelines is an important direction for future work. Beyond Implausibility. Our benchmark focuses on detecting violations. future direction is to generate and evaluate physically grounded captions, affordance predictions, or causal reasoning in video. 15 16 18 19 20 Figure 5: Qualitative examples from TRAVL. The first two pages show frames from Impossible Videos, while the remaining illustrate plausible and implausible variants from ImplausiBench. These examples were selected to showcase representative successes (check mark) and failures (cross) across different models, as identified through manual inspection. 21 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 prompt_messages = [ { \"role\": \"user\", \"content\": f\"\"\" You are an expert in video-language reasoning. Your task is to generate 3 to 6 question-answer (Q/A) pairs for the given video scenario and caption. All videos in this batch are implausible they contain physically unrealistic events. The answers must explicitly state this and explain why the scene is implausible, based only on the caption. Questions should focus on: - General video understanding (overall events, including what appears implausible) - Physical realism (phrased neutrally, e.g., Do the events appear realistic or implausible?) - Physical behavior (object interactions, motion, deformations) - Temporal reasoning (what happens first, next, last) Instructions: - Generate 3 to 6 Q/A pairs per scenario. Never fewer, never more. - Include at least one neutral question on physical realism. - DO NOT ask What makes the video implausible? or similar. Implausibility should only appear in the answers. - Questions must sound natural and varied. - Answers must be detailed, grounded only in the caption, and list all reasons for implausibility. Output Format: Q1: <question 1> A1: <answer 1> Q2: <question 2> A2: <answer 2> ... Video Scenario: {scenario} Video Description: {caption} \"\"\" } ] Pseudocode 2: Training Q/A generation prompt 22 Figure 6: Qualitative examples from ImplausiBench. Each row shows real video (left) and its implausible counterpart (right). Pairs share seven-option MCQ designed to prevent language-only shortcuts. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 1 2 3 4 5 6 7 8 9 10 11 12 13 14 def generate_attention_mask(video, cotracker, k=10): # video: [T, 3, 384, 384]; reinit tracking every frames T, _, H, = video.shape # grid size 729 patches = 27 = * queries, q_times = [], [] for in range(0, T, k): for in range(G): for in range(G): = (j + 0.5) * (W / G) = (i + 0.5) * (H / G) queries.append([t, x, y]) q_times.append(t) tracks, vis = cotracker(video[None], queries=queries, t_valid=q_times) patch_ids = ((tracks[0, ..., 1] // (H // G)).long() * + (tracks[0, ..., 0] // (W // G)).long()) mask = torch.zeros((T * P, * P), dtype=torch.bool) for in range(len(queries)): = q_times[n] p0 = patch_ids[q, n] for in range(T): if vis[0, t, n] > 0.5: pt = patch_ids[t, n] mask[q * + p0, * + pt] = True mask[q * + p0, * + p0] = True mask.fill_diagonal_(True) return mask Pseudocode 3: Mask generation code def apply_travl_attention(patch_tokens, flow_mask): # patch_tokens: [B, T, P, D] where T=frames, P=patches, D=dim # mask: [B, T*P, T*P] binary mask spatial_out = [] for in range(T): frame_tokens = patch_tokens[:, t] frame_attn = self_attend(frame_tokens) spatial_out.append(frame_attn) # shape [B, P, D] # spatial attention spatial_out = torch.stack(spatial_out, dim=1) # shape [B, T, P, D] flat_tokens = spatial_out.view(B, T*P, D) attended = masked_temporal_attention(flat_tokens, mask) # [B, T*P, D] return attended.view(B, T, P, D) Pseudocode 4: Attention module code"
        }
    ],
    "affiliations": [
        "INSAIT, Sofia University \"St. Kliment Ohridski\", Bulgaria",
        "Visual Geometry Group, University of Oxford"
    ]
}