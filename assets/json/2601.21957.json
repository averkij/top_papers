{
    "paper_title": "PaddleOCR-VL-1.5: Towards a Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing",
    "authors": [
        "Cheng Cui",
        "Ting Sun",
        "Suyin Liang",
        "Tingquan Gao",
        "Zelun Zhang",
        "Jiaxuan Liu",
        "Xueqing Wang",
        "Changda Zhou",
        "Hongen Liu",
        "Manhui Lin",
        "Yue Zhang",
        "Yubo Zhang",
        "Yi Liu",
        "Dianhai Yu",
        "Yanjun Ma"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce PaddleOCR-VL-1.5, an upgraded model achieving a new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortions, including scanning, skew, warping, screen-photography, and illumination, we propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the model's capabilities by incorporating seal recognition and text spotting tasks, while remaining a 0.9B ultra-compact VLM with high efficiency. Code: https://github.com/PaddlePaddle/PaddleOCR"
        },
        {
            "title": "Start",
            "content": "PaddleOCR-VL-1.5: Towards Multi-Task 0.9B VLM for Robust In-the-Wild Document Parsing Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, Yue Zhang, Yubo Zhang, Yi Liu, Dianhai Yu, Yanjun Ma PaddlePaddle Team, Baidu Inc. paddleocr@baidu.com Official Website: https://www.paddleocr.com Source Code: https://github.com/PaddlePaddle/PaddleOCR Models: https://huggingface.co/PaddlePaddle"
        },
        {
            "title": "Abstract",
            "content": "We introduce PaddleOCR-VL-1.5, an upgraded model achieving new state-of-the-art (SOTA) accuracy of 94.5% on OmniDocBench v1.5. To rigorously evaluate robustness against real-world physical distortionsincluding scanning, skew, warping, screen-photography , and illuminationwe propose the Real5-OmniDocBench benchmark. Experimental results demonstrate that this enhanced model attains SOTA performance on the newly curated benchmark. Furthermore, we extend the models capabilities by incorporating seal recognition and text spotting tasks, while remaining 0.9B ultra-compact VLM with high efficiency. 6 2 0 2 9 2 ] . [ 1 7 5 9 1 2 . 1 0 6 2 : r Figure 1 Performance of PaddleOCR-VL-1.5 on OmniDocBench v1.5 and Real5-OmniDocBench."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 PaddleOCR-VL-1.5 2.1 Architecture . . . 2.2 Training Recipe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 Dataset 3.1 Layout Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3.2 PaddleOCR-VL-1.5-0.9B . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 Evaluation 4.1 Document Parsing . 4.2 New Capabilities . . . . . . 4.3 Inference Performance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 Conclusion Comparison of PaddleOCR-VL-1.5 and 1.0 Models Details of the Real5-OmniDocBench Benchmark Supported Languages Inference Performance on Different Hardware Configurations Real-world Samples E.1 Real-word Document Parsing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . E.2 Layout Analysis . E.3 Text Recognition . . . E.4 Table Recognition . . . . . . . E.5 Formula Recognition . E.6 Seal Recognition . E.7 Text Spotting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 4 6 9 9 9 11 14 14 15 19 20 24 25 26 31 36 42 43 46 2 1. Introduction As the primary repository of human knowledge, documents are growing exponentially in both volume and complexity, establishing document parsing as pivotal technology in the era of artificial intelligence. The ultimate objective of document parsing [1, 2, 3, 4] extends beyond mere text recognition; it aims to reconstruct the deep structural and semantic layout of document. By meticulously distinguishing text blocks, decoding complex formulas and tables, and deducing the logical reading order, advanced parsing lays the groundwork for Large Language Models (LLMs) [5, 6, 7]. Crucially, this capability empowers Retrieval-Augmented Generation (RAG) systems [8] to ingest high-fidelity knowledge, thereby enhancing their reliability in downstream applications. The field has witnessed surge of innovation following October 2025, with several significant document parsing solutions emerging to push the boundaries of document intelligence. Notably, PaddleOCR-VL [9] established high performance baseline, surpassing contemporary SOTA metrics with only 0.9 billion parameters and demonstrating strong multi-scenario generalization. Concurrently, DeepSeek-OCR [10] leverages an optical 2D mapping methodology to enable highratio vision-to-text compression, offering robust end-to-end parsing capabilities. MonkeyOCR v1.5 [11] further enhances the three-stage parsing framework, while HunyuanOCR [12] extends expert OCR capabilities through unified architecture supporting translation and extraction. Despite these advancements, critical gap remains: most existing models are primarily optimized for \"digital-born\" or cleanly scanned documents. Real-world scenarios involving extreme physical distortionssuch as aggressive skewing, non-rigid warping of pages, screencapture moir√© patterns, and erratic lightingremain significant hurdles that even state-of-the-art solutions have yet to fully overcome. To bridge this gap, we present PaddleOCR-VL-1.5, high-performance, resource-efficient document parsing solution that significantly enhances both general precision and real-world robustness. Building upon the proven 0.9B ultra-compact architecture, PaddleOCR-VL-1.5 introduces several critical advancements: Firstly, we upgrade the layout engine to PP-DocLayoutV3. Unlike previous Layout Analysis methods (e.g., Dolphin [3], MinerU2.5 [2], or even PP-DocLayoutV2 [13]), PPDocLayoutV3 is specifically engineered to handle non-planar document images. It can directly predict multi-point bounding boxes for layout elementsas opposed to standard two-point boxesand determine logical reading orders for skewed and warped surfaces within single forward pass, significantly reducing cascading errors. Secondly, we expand the models core capabilities. While maintaining the efficient NaViTstyle dynamic resolution encoder and the ERNIE-4.5-0.3B [5] language backbone, we have integrated new tasks including seal recognition and text spotting. Systematic optimizations in text, table, and formula recognition have further propelled the model to new performance milestone. Thirdly, we construct Real5-OmniDocBench to evaluate in-the-wild robustness. Recognizing the lack of benchmarks for physical distortions, we curated this dataset based on OmniDocBench v1.5 [14]. It comprises five distinct scenarios: scanning, warping, screen photography, illumination, and skew. By maintaining strict one-to-one correspondence with the original ground-truth annotations, Real5-OmniDocBench serves as rigorous benchmark for assessing model resilience in practical applications. Comprehensive benchmarking confirms that PaddleOCR-VL-1.5 establishes new state3 of-the-art (SOTA) standard. On the OmniDocBench v1.5 benchmark, our model achieves breakthrough accuracy of 94.5%, maintaining its position as the official top-ranked solution. More importantly, on the newly curated Real5-OmniDocBench, the model sets new record with an overall accuracy of 92.05%. Despite its compact 0.9B scale, it significantly outperforms massive general VLMs, such as Qwen3-VL-235B [6] and Gemini-3 Pro [15], highlighting its exceptional parameter efficiency. Furthermore, our model expands its capabilities to text spotting and seal recognition, attaining leading performance across diverse and challenging benchmarks. These results collectively validate its superior robustness and generalization in complex, realworld scenarios. Appendix details the specific upgrades and changes in PaddleOCR-VL-1.5 compared to its predecessor. 2. PaddleOCR-VL-1.5 2.1. Architecture PaddleOCR-VL-1.5 introduces an enhanced framework capable of handling both Document Parsing and Text Spotting, as depicted in Figure 2. Figure 2 The overview of PaddleOCR-VL-1.5. For the Document Parsing task, PaddleOCR-VL-1.5 adopts robust two-stage framework. In the initial stage, PP-DocLayoutV3 performs sophisticated layout analysis. Beyond standard axis-aligned detection, it is specifically optimized for real-world complexity by employing multi-point localization (e.g., quadrilaterals or polygons). This allows for the precise boundary anchoring of semantic regions even under severe perspective tilt or physical curvature, while simultaneously establishing the logical reading order. In the second stage, the PaddleOCRVL-1.5-0.9B model takes these geometrically-rectified or localized regions as input to perform 4 high-fidelity recognition across diverse modalities, including text, complex tables, mathematical formulas, charts and seals. To conclude the pipeline, lightweight post-processing engine orchestrates these outputs into structured formats such as Markdown and JSON, while providing advanced capabilities such as cross-page table merging and heading hierarchy refinement. For the Spotting task, the framework simplifies its workflow by directly utilizing the PaddleOCR-VL-1.5-0.9B model for end-to-end text detection and recognition. This approach enables end-to-end text detection and recognition across wide spectrum of domainsranging from standard documents, identification cards, and ancient manuscripts to unconstrained scenarios like advertising posters, dialogue screenshots, signboards, and multilingual texts. 2.1.1. PP-DocLayoutV3: Unified Layout Analysis To address the challenges of complex physical distortionsincluding skew, warping, and illuminationand to overcome the high latency inherent in autoregressive Vision-Language Models (VLMs), we introduce PP-DocLayoutV3. This version represents significant architectural evolution from its predecessor by transitioning from standard rectangular detection to robust instance segmentation framework, while simultaneously integrating reading order prediction into unified, end-to-end Transformer architecture. Building upon the high-efficiency RT-DETR object detector [16], PP-DocLayoutV3 adopts mask-based detection head. This allows the model to predict precise, pixel-accurate masks for layout elements rather than simple bounding boxes. Such capability is critical for isolating document components in non-ideal scenarios, such as skewed or warped pages, where traditional axis-aligned boxes frequently overlap or capture excessive background noise. Unlike the decoupled pointer network employed in PP-DocLayoutV2 [9], PP-DocLayoutV3 integrates Reading Order Prediction directly into the Transformer decoder layers. By merging detection, segmentation, and ordering into single vision-centric model, PP-DocLayoutV3 eliminates the need for redundant post-processing and separate feature extraction steps. Figure 3 The unified architecture of PP-DocLayoutV3, featuring parallel heads for instance segmentation and relational reading order prediction. The core architectural innovation of PP-DocLayoutV3 is the integration of Reading Order Prediction directly into the Transformer decoder. Specifically, our model extends the RT-DETR framework to simultaneously optimize geometric localization and logical sequencing. Following the query-based paradigm, the decoder iteratively refines ùëÅ object queries ùëÑ = {ùëûùëñ}ùëÅ RùëÅ ùëë. ùëñ=1 The reading order is then derived from the refined query embeddings of the final decoder layer through Global Pointer Mechanism. We project the refined queries into shared relational space to compute the pairwise precedence score ùëÜùëñ, ùëó: ùëÜùëñ, ùëó = ùëì (ùëûùëñ, ùëû ùëó) ùëì (ùëû ùëó, ùëûùëñ) ùëë‚Ñé , where ùëì (ùëûùëñ, ùëû ùëó) = (ùëäùëûùëûùëñ)(ùëäùëòùëû ùëó) (1) where ùëäùëû, ùëäùëò Rùëëùëë‚Ñé are learnable projection matrices and ùëë‚Ñé denotes the hidden dimension. The resulting relation matrix ùëÜ RùëÅ ùëÅ is constrained to be anti-symmetric such that ùëÜùëñ, ùëó = ùëÜ ùëó,ùëñ, where ùëÜùëñ, ùëó > 0 implies element ùëñ precedes element ùëó. During inference, to derive globally consistent sequence from these pairwise relations, we implement Voting-based Ranking strategy. We first apply the sigmoid function ùúé() to the relation matrix ùëÜ and mask the diagonal elements. The absolute precedence votes ùëâùëó for each element ùëó is computed by aggregating the probabilities of other elements preceding it: ùëâùëó = ùëÅ ùëñ=1,ùëñ ùëó ùúé(ùëÜùëñ, ùëó). (2) The final reading order is determined by sorting the elements in ascending order of their total votes ùëâùëó. This joint optimization ensures that the logical sequence is highly sensitive to the refined object features, leading to superior performance on complex, multi-column, and non-standard document layouts. By merging detection, segmentation, and ordering into single vision-centric model, PPDocLayoutV3 eliminates the need for redundant post-processing and separate feature extraction. The model produces the complete document structure in single forward pass, where the multihead system concurrently outputs classification labels, bounding box coordinates, pixel-accurate segments, and the logical reading sequence. 2.1.2. PaddleOCR-VL-1.5-0.9B: Element-level Recognition and Text Spotting The PaddleOCR-VL-1.5-0.9B inherits the lightweight architecture of PaddleOCR-VL-0.9B [9], integrating Native Resolution Visual Encoder [17], an Adaptive MLP Connector, and the Lightweight ERNIE-4.5-0.3B Language Model [5]. In this update, the models capabilities have been expanded to include Seal Recognition and Text Spotting. Consequently, the model now supports comprehensive set of six core tasks: OCR, Formula Recognition, Table Recognition, Chart Recognition, Seal Recognition, and Text Spotting. Compared to its predecessor, PaddleOCR-VL-1.5-0.9B demonstrates significant enhancements in recognition accuracy for complex tables and mathematical formulas. Furthermore, the model incorporates finer-grained optimizations for rare characters, ancient Chinese texts, multilingual tables, and text decorations such as underlines and emphasis marks. 2.2. Training Recipe The following sections introduce the training details of these two modules: PP-DocLayoutV3 for layout analysis and PaddleOCR-VL-1.5-0.9B for element recognition and text spotting. 6 2.2.1. Layout Analysis The training of PP-DocLayoutV3 evolves from the two-stage decoupled process used in PPDocLayoutV2 [9] to more sophisticated end-to-end joint optimization strategy. This approach allows the detection, instance segmentation, and reading order modules to share unified feature representation, leading to better alignment between spatial localization and logical sequencing. The model is initialized with the pre-trained weights of PP-DocLayout_plus-L [13], we scaled our training corpus to over 38k high-quality document samples. Each sample underwent rigorous manual annotation to provide ground truth, include the coordinates, categorical label and absolute reading order for every layout elements. To achieve the environmental robustness, we designed specialized Distortion-Aware Data Augmentation pipeline. Unlike standard augmentations, this pipeline specifically simulates complex physical deformations found in real-world mobile photography. We utilize the AdamW optimizer with weight decay of 0.0001. The learning rate is set to constant 2 104 to ensure stable convergence of the integrated Global Pointer and Mask heads. The model is trained for 150 epochs with total batch size of 32. In contrast to the previous version, all componentsincluding the RT-DETR backbone and the integrated reading order transformerare trained simultaneously. This end-to-end supervision ensures that the learned queries in the Transformer decoder capture both the geometric boundaries and the topological relationships of the document elements. 2.2.2. Element-level Recognition and Text Spotting Building upon the architecture described in Section 2.1.2, PaddleOCR-VL-1.5-0.9B introduces progressive training paradigm using PaddleFormers [18], which is high-performance training toolkit for LLMs and VLMs built on the PaddlePaddle framework [19]. While we retain the effective post-adaptation strategy and initialization settings from our previous version, the training methodology has been significantly upgraded to enhance data scale, task diversity, and model robustness. The overview of the three stages is presented in Table 1. Settings Pre-training Post-training Training Samples Max Resolution Sequence length Trainable components Batch sizes Data Augmentation Maximum LR Epoch 46M 1280 28 28 16384 All 128 Yes 5 105 1 5.6M 1280 28 28 16384 All 128 Yes 8 106 1 Table 1 Training settings for PaddleOCR-VL-1.5-0.9B. Pre-training: Enhanced Vision-Language Alignment. While the fundamental objective remains aligning visual features with textual semantics, this stage undergoes substantial data upgrade compared to PaddleOCR-VL-0.9B [9], scaling the pre-training dataset from 29 million to 46 million image-text pairs. This expansion represents qualitative leap in data distribution rather than mere quantitative increase. Specifically, to enhance the generalization of the visual backbone and support the newly introduced capabilities, we incorporate broader spectrum of multilingual documents and complex real-world scenarios. Furthermore, we intentionally inject 7 large-scale pre-training data related to seal recognition and text spotting during this alignment phase. Specifically, the maximum resolution for the spotting task is increased to 2048 28 28 pixels, enabling the model to achieve more precise localization and recognition of text. By introducing these task-specific priors early in the training pipeline, the model establishes robust foundation capable of capturing intricate visual patterns and effectively supporting the fine-grained localization and recognition tasks required in subsequent stages. Post-training: Instruction Fine-tuning with New Capabilities. In this stage, we inherit the four fundamental instruction tasks from PaddleOCR-VL-0.9BOCR, Table, Formula, and Chart Recognitionensuring backward compatibility and high performance on standard document elements. The key innovation in PaddleOCR-VL-1.5-0.9B lies in the addition of two specialized tasks: 1. Seal Recognition: We introduce specific instruction to handle official seals and stamps, addressing challenges such as curved text, blur images and background interference. 2. Text Spotting (Grounded OCR): Unlike standard OCR, which solely outputs textual content, the text spotting task requires the model to simultaneously predict the text and its precise spatial location following the natural reading order. To accommodate complex layouts found in real-world scenarios (e.g., rotated text, common scene, or dense forms), we adopt 4-point quadrilateral representation rather than the traditional 2-point bounding box. The 4-point format defines text region using four vertices: Top-Left (TL), TopRight (TR), Bottom-Right (BR), and Bottom-Left (BL). This formulation provides superior flexibility in localizing inclined and irregular text shapes that standard axis-aligned rectangle cannot tightly enclose. Formally, for given text instance, the target sequence is constructed by appending eight location tokens to the text tokens: ùëå = Text <LOC_ùë•TL><LOC_ùë¶TL> . . . <LOC_ùë¶BL> (3) Here, we introduce set of tokens {<LOC_0>, . . . , <LOC_1000>} to the models vocabulary to represent normalized coordinates. Unlike treating coordinates as plain numerical text, these dedicated special tokens allow the model to learn specific embeddings for spatial information and prevent tokenization fragmentation. For example, recognized instance of the word \"DREAM\" is represented as: DREAM <LOC_253> <LOC_286> <LOC_346> <LOC_298> <LOC_345> <LOC_339> <LOC_252> <LOC_330> This unified representation enables the model to perform end-to-end recognition and fine-grained localization within single generation pass. To enhance generalization and unify diverse label styles, we introduce Reinforcement Learning stage leveraging Group Relative Policy Optimization(GRPO) [20]. By executing parallel rollouts and calculating relative advantages within each group, GRPO facilitates robust policy updates and mitigates style inconsistency. This process is supported by dynamic data screening protocol that prioritizes challenging samples with high reward potential and entropy uncertainty, ensuring the model focuses on non-trivial, high-value learning cases. 3. Dataset 3.1. Layout Analysis To ensure robust model performance across diverse real-world document scenarios, we curate in-house dataset for layout analysis. The data sources encompass 38k document images across diverse domains, including Academic papers, Textbooks, Market Analysis, Financial reports, Slides, Newspapers, Supplementary Teaching Materials, Examination Papers, and various Invoices and Receipts. The dataset features meticulous manual annotations across 25 distinct component categories: Paragraph Title, Image, Text, Number, Abstract, Content, Figure Title, Display Formula, Table, Reference, Doc Title, Footnote, Header, Algorithm, Footer, Seal, Chart, Formula Number, Aside Text, Reference Content, Header Image, Footer Image, Inline Formula, Vertical Text, and Vision Footnote. All documents are manually annotated with element-level boundaries and their corresponding reading order, enabling effective training and evaluation for both layout element detection and reading order restoration. This high-quality ground truth ensures that the model can accurately reconstruct both the spatial structure and the logical flow of complex documents. The data curation process incorporates specific data mining strategies aimed at expanding dataset diversity and identifying hard cases to improve model robustness. This workflow begins with clustering-based sampling applied to an extensive internal data pool, utilizing visual features to ensure representative distribution and minimize redundancy. Subsequently, hard-case mining pipeline is executed using PP-DocLayoutV2 [9] for dual-threshold inference. Samples exhibiting significant discrepancy in detection density between high and low confidence thresholds are categorized as unstable cases. This methodology facilitates the systematic discovery of non-conventional layout structuresincluding comics, CAD drawings, and highaspect ratio screenshotswhich diverge from standard document formats. These instances are further refined through human-in-the-loop process. Integrating these diverse scenarios into the dataset broadens the models representation of characteristics and enhances its adaptive capacity in complex real-world document domains. 3.2. PaddleOCR-VL-1.5-0.9B The data construction strategy for PaddleOCR-VL-1.5-0.9B is driven by two core objectives: enhancing model robustness on challenging samples and expanding the breadth of supported capabilities. Consequently, our data preparation pipeline is divided into two distinct parts: (1) Hard Example Mining (Section 3.2.1), which focuses on identifying and weighting highuncertainty samples to refine the models decision boundaries; and (2) New Capability Data Construction (Section 3.2.2), which involves curating specialized datasets to unlock new skills such as text spotting, seal recognition, and advanced multilingual support. 3.2.1. Data Selection Strategy: Uncertainty-Aware Cluster Sampling To maximize the efficiency of the instruction fine-tuning stage (Stage 2), we propose data curation strategy designed to balance visual diversity and sample difficulty. Instead of uniform random sampling, we employ an Uncertainty-Aware Cluster Sampling (UACS) mechanism. This approach ensures that the training data covers wide spectrum of visual scenarios while allocating more training budget to \"hard\" cases where the model exhibits high uncertainty. 1. Visual Feature Clustering. First, to guarantee the diversity of visual layouts across the six tasks (OCR, Table, Formula, Chart, Seal, and Spotting), we utilize the CLIP [21] visual encoder to 9 extract high-dimensional semantic embeddings for all candidate images. For each task, we apply K-Means clustering to partition the dataset into ùêæ distinct visual clusters {ùê∂1, ùê∂2, . . . , ùê∂ùêæ }. This step groups samples with similar visual structures (e.g., solid line tables vs. wireless tables) together. 2. Uncertainty Estimation. For each cluster ùê∂ùëñ, we estimate its difficulty by measuring the models prediction uncertainty. Specifically, we randomly sample subset of images from ùê∂ùëñ and perform multiple inference passes with stochastic decoding using the pre-trained model from Stage 1. We calculate an uncertainty score ùëÜùëñ based on the divergence of the generated outputs. higher ùëÜùëñ indicates that the model is inconsistent or unconfident regarding the samples in this cluster. 3. Weighted Sampling Plan. Based on the uncertainty score, we formulate sampling plan to determine the number of samples ùëÅùëñ to draw from each cluster ùê∂ùëñ. Inspired by the principle of hard example mining, we adopt polynomial weighting scheme to amplify the focus on harder clusters. Specifically, the allocated sample count ùëÅùëñ for cluster ùê∂ùëñ is determined by: ùëÅùëñ = min (cid:32) (cid:36) (ùëÜùëñ + ùõº) ùõΩ (cid:205)ùêæ ùëó= (ùëÜ ùëó + ùõº) ùõΩ (cid:37) (cid:33) ùëÅ total , ùê∂ùëñ (4) where ùëÜùëñ is the average uncertainty score of cluster ùê∂ùëñ, and ùê∂ùëñ denotes the total number of available samples in that cluster. The parameters ùõº and ùõΩ are the smoothing and power factors, respectively (set to ùõº = 1.0, ùõΩ = 2.0 based on empirical observations). ùëÅ total represents the total sampling budget. This strategy allows us to dynamically up-sample complex scenarios (e.g., distorted seals, dense tables) while maintaining representative baseline for simpler cases. As detailed in the previous section, we employ the Uncertainty-Aware Cluster Sampling (UACS) strategy to select the most effective training samples based on visual clustering and inference variance. 3.2.2. Data Construction for New Capabilities In addition to data quality control, we expanded the VLMs capabilities by integrating data spanning wider array of tasks, languages, and document types. This expansion focuses on these key dimensions: Spotting, Specialized Text (Seals), OCR Enhancement, and Complex Tables, Formulas, and Chart. Spotting: We collected large-scale and diverse set of images covering wide range of realworld scenarios, such as financial research reports, tabular documents, handwritten materials, classical texts, and other complex document and natural scene images. During the annotation process, we jointly employed PP-OCRv5 [22] to generate initial recognition results, followed by an IoU-based cross-filtering strategy to eliminate low-quality and inconsistent samples, while for subset of samples with ambiguous or inaccurate labels, multimodal models including PaddleOCR-VL [9] and Qwen3-VL [6] were further leveraged for label refinement, thereby substantially improving the overall annotation quality and robustness of the dataset. Seal: We combined synthetic and real-world images of contracts, invoices, and commemorative seals to build high-quality dataset. Labels were generated using Qwen3-VL [6] and refined through fine-tuning-based re-labeling process. Challenging cases were manually corrected to ensure final annotation accuracy and robustness. OCR: We have significantly enhanced the models capabilities by refining the datasets 10 precision and enlarging the functional scope. This includes systematically correcting formula representations and line-breaking logic, alongside extending support for education-specific markings like emphasis dots and underlines to capture instructional semantics. Furthermore, the integration of Bengali and Chinas Tibetan scripts broadens the models linguistic versatility, ensuring robust performance across diverse writing systems and educational contexts. Formula: Formula dataset incorporates CV-simulated artifacts, such as Gaussian illumination and harmonic moir√©, to replicate physical conditions like scanning, warping, screen-capture, and geometric skewing. These samples encompass wide spectrum of environmental variables, including light fluctuations and complex document distortions encountered in realistic scenarios. Table: Tables has been expanded to cover an extensive range of scenarios, including financial reports, academic papers, and complex industrial forms. Integration of diverse structures, such as registration and catalog tables. key focus is placed on the precise recognition of cell-level formulas and multilingual content within dense table environments. These advancements ensure high-fidelity conversion into structured formats, even when handling intricate cell structures and professional notations. 4. Evaluation To thoroughly assess the effectiveness of PaddleOCR-VL-1.5, we conducted evaluations on the document parsing benchmark OmniDocBench v1.5 and its derived real-world dataset, Real5OmniDocBench. Furthermore, we expanded the evaluation scope by incorporating text spotting and seal recognition tasks to provide more comprehensive analysis of the models performance in practical and complex scenarios. 4.1. Document Parsing This section details the evaluation of end-to-end document parsing capabilities using the following two benchmarks, aiming to measure its overall performance in real-world document scenarios. OmniDocBench v1.5 To comprehensively evaluate the document parsing capabilities, we conducted extensive experiments on the OmniDocBench v1.5 [2] benchmark. It is an expansion of version v1.0, adding 374 new documents for total of 1,355 document pages. It features more balanced distribution of data in both Chinese and English, as well as richer inclusion of formulas and other elements. Compared to version v1.0, the evaluation method has been updated. While text and reading order are still evaluated using Edit Distance, and tables are evaluated using Tree-Edit-Distance-based Similarity (TEDS), formulas are now assessed using the Character Detection Matching (CDM) [23] method. This metric provides more objective and robust evaluation of the correctness of predicted formulas. The overall metric is weighted combination of the metrics for text, formulas, and tables. Table 2 demonstrates that PaddleOCR-VL-1.5 achieves SOTA performance, consistently outperforming existing pipeline tools, general VLMs, and specialized document parsing models across all key metrics. Notably, PaddleOCR-VL-1.5 exhibits substantial performance leap over its predecessor, PaddleOCR-VL, raising the overall score from 92.86% to top-ranking 94.50%. Specifically, it achieves increases of 2.99%, 1.87%, and 0.1% in the CDM Score, Table-TEDS, and Reading Order scores, respectively. Furthermore, our model establishes new SOTA results in all sub-tasks, including reduced Text-Edit distance of 0.035, an improved Formula-CDM 11 Model Type Methods Parameters Overall TextEdit FormulaCDM TableTEDS TableTEDS-S Reading OrderEdit Pipeline Tools Marker-1.8.2 [24] Mineru2-pipeline [25] PP-StructureV3 [22] General VLMs GPT-4o [7] InternVL3-76B [26] InternVL3.5-241B [27] GPT-5.2 [28] Qwen2.5-VL-72B [29] Gemini-2.5 Pro [30] Qwen3-VL-235B-A22B-Instruct [6] Gemini-3 Pro [15] Specialized VLMs Dolphin [3] OCRFlux-3B [31] Mistral OCR [32] POINTS-Reader [4] olmOCR-7B [33] Dolphin-1.5 [3] MinerU2-VLM [25] Nanonets-OCR-s [34] MonkeyOCR-pro-1.2B [1] Deepseek-OCR [10] MonkeyOCR-3B [1] dots.ocr [35] MonkeyOCR-pro-3B [1] MinerU2.5 [2] PaddleOCR-VL [9] PaddleOCR-VL-1.5 - - - - 76B 241B - 72B - 235B - 0.3B 3B - 3B 7B 0.3B 0.9B 3B 1.9B 3B 3.7B 3B 3.7B 1.2B 0.9B 0.9B 71.30 75.51 86.73 75.02 80.33 82.67 85.50 87.02 88.03 89.15 90.33 74.67 74.82 78.83 80.98 81.79 83.21 85.56 85.59 86.96 87.01 87.13 88.41 88.85 90.67 92.86 94.50 0.206 0.209 0.073 0.217 0.131 0.142 0.123 0.094 0.075 0.069 0. 0.125 0.193 0.164 0.134 0.096 0.092 0.078 0.093 0.084 0.073 0.075 0.048 0.075 0.047 0.035 0.035 76.66 76.55 85.79 79.70 83.42 87.23 86.11 88.27 85.82 88.14 89.18 67.85 68.03 82.84 79.20 86.04 80.78 80.95 85.90 85.02 83.37 87.45 83.22 87.25 88.46 91.22 94.21 57.88 70.90 81.68 67.07 70.64 75.00 82.66 82.15 85.71 86.21 88. 68.70 75.75 70.03 77.13 68.92 78.06 83.54 80.14 84.24 84.97 81.39 86.78 86.78 88.22 90.89 92.76 71.17 79.11 89.48 76.09 77.74 81.28 87.35 86.22 90.29 90.55 90.29 77.77 80.23 78.04 81.66 74.77 84.10 87.66 85.57 89.02 88.80 85.92 90.62 90.63 92.38 94.76 95.79 0.250 0.225 0.073 0.148 0.113 0.125 0.099 0.102 0.097 0.068 0. 0.124 0.202 0.144 0.145 0.121 0.080 0.086 0.108 0.130 0.086 0.129 0.053 0.128 0.044 0.043 0.042 Table 2 Comprehensive evaluation on OmniDocBench v1.5. Performance metrics are cited from the official leaderboard [14], except for Gemini-3 Pro, GPT-5.2, Qwen3-VL-235B-A22B-Instruct and our model, which were evaluated independently. score of 94.21%, and leading scores of 92.76% and 95.79% in Table-TEDS and Table-TEDS-S, respectively. These improvements, particularly in maintaining high reading ordering score of 0.042, underscore the models enhanced precision in text recognition, formula extraction, and complex table structure analysis. Real5-OmniDocBench Real5-OmniDocBench1 is brand-new benchmark oriented toward real-world scenarios, which we constructed based on the OmniDocBench v1.5 dataset. The dataset comprises five distinct scenarios: scanning, warping, screen photography, illumination, and Skew. Apart from the Scanning category, all images were manually acquired via handheld mobile devices to closely simulate real-world conditions. Each subset maintains one-toone correspondence with the original OmniDocBench, strictly adhering to its ground-truth annotations and evaluation protocols. Given its empirical and realistic nature, this dataset serves as rigorous benchmark for assessing the robustness of document parsing models in practical applications. Figure 4 illustrates the visualization of representative samples from the proposed dataset. As illustrated in Table 3, PaddleOCR-VL-1.5 demonstrates consistent superiority across all evaluated scenarios, setting new SOTA record with an overall accuracy of 92.05%. Despite its compact 0.9B parameter scale, the model significantly outperforms massive general VLMs, such as Qwen3-VL-235B and Gemini-3 Pro, highlighting its exceptional parameter efficiency for document-centric tasks. Notably, in the highly challenging Skewing category, PaddleOCRVL-1.5 achieves an accuracy of 91.66%, representing 14.19% absolute improvement over its 1https://huggingface.co/datasets/PaddlePaddle/Real5-OmniDocBench 12 predecessor. This substantial performance leap underscores its superior robustness against extreme geometric distortions and validates its reliability for complex document parsing in unconstrained environments. Detailed comparisons across sub-items including text, formulas, tables, and reading order can be found in Appendix B. Figure 4 Samples of Real5-OmniDocBench. Model Type Methods Parameters Overall Scanning Warping Screen Photography Illumination Skew Pipeline Tools Maker-1.8.2 [24] PP-StructureV3 [22] General VLMs GPT-5.2 [28] Qwen2.5-VL-72B [29] Gemini-2.5 Pro [30] Qwen3-VL-235B-A22B-Instruct [6] Gemini-3 Pro [15] Specialized VLMs Dolphin-1.5 [3] Dolphin [3] Deepseek-OCR [10] MinerU2-VLM [25] MonkeyOCR-pro-1.2B [1] MonkeyOCR-3B [1] MonkeyOCR-pro-3B [1] Nanonets-OCR-s [34] PaddleOCR-VL [9] MinerU2.5 [2] dots.ocr [35] PaddleOCR-VL-1.5 - - - 72B - 235B - 0.3B 0.3B 3B 0.9B 1.9B 3.7B 3.7B 3B 0.9B 1.2B 3B 0.9B 60.10 64. 78.66 86.92 88.21 88.904 89.24 61.48 61.78 73.99 76.95 77.15 78.29 79.49 84.19 85.54 85.61 86.38 92.05 70.27 84.68 84.43 86.19 89.25 89.43 89.47 83.39 72.16 86.17 83.60 84.64 84.65 86.94 85.52 92.11 90.06 86.87 93.43 58.98 59. 76.26 87.77 87.63 89.99 88.90 50.50 60.35 67.20 73.73 76.59 77.27 78.90 83.56 85.97 83.76 86.01 91.25 63.65 66.89 76.75 86.48 87.11 89.27 88.86 69.76 64.29 75.31 78.77 80.24 80.71 82.44 84.86 82.54 89.41 87.18 91.76 66.31 73. 80.88 87.25 87.97 89.27 89.53 75.61 67.29 78.10 80.51 82.11 83.16 84.71 85.01 89.61 89.57 87.57 92.16 41.27 37.98 75.00 86.90 89.07 86.56 89.45 28.16 44.83 63.01 68.16 62.18 65.67 64.47 81.98 77.47 75.24 84.27 91.66 Table 3 Comprehensive evaluation of document parsing on Real5-OmniDocBench, Appendix provides more detailed metrics of this benchmark. 13 4.2. New Capabilities 4.2.1. Text Spotting To thoroughly assess the models end-to-end text spotting capability (detection + recognition), we establish comprehensive OCR benchmark spanning 10 key dimensions. In addition to standard settings such as common scenes (Common) and multilingual recognition (Japanese), the benchmark is designed to reflect practical deployment challenges by deliberately sampling more difficult cases, including degraded or low-quality images (Blur), highly variable handwriting in both Chinese and English (Handwrite_ch/en), structured and layout-sensitive table content (Table), and culturally significant historical materials such as ancient documents and Traditional Chinese (Ancient). As summarized in Table 4, our model delivers the highest spotting accuracy across all 9 dimensions, consistently outperforming strong baselines and demonstrating robust generalization under diverse visual conditions and text styles. These results indicate that the proposed approach remains reliable not only in regular document scenarios but also in challenging, real-world settings that require precise localization and faithful transcription. Dataset Overall Ancient Blur Common Handwrite _ch Handwrite _en Printing _ch Printing _en Table Japanese HunyuanOCR [12] Rex-Omni [36] PaddleOCR-VL-1.5 0.6290 0.6682 0.8621 0.6164 0.4251 0.8523 0.6392 0.6936 0. 0.5222 0.6112 0.7713 0.7984 0.8147 0.8952 0.7665 0.7812 0.9163 0.6213 0.6961 0.8669 0.5956 0.6088 0.8689 0.4419 0.7185 0. 0.6593 0.6642 0.8461 Table 4 Comparison of text spotting performance on the in-house benchmark. Overall denotes the average accuracy across all 9 evaluation dimensions. 4.2.2. Seal Recognition To evaluate the effectiveness of our model in complex seal recognition tasks, we constructed specialized benchmark comprising 300 high-quality images. This evaluation set covers diverse range of seal shapes (e.g., circular, oval, and rectangular) and challenging real-world scenarios, such as overlapping text, low-contrast impressions, and distorted backgrounds. We employ the Normalized Edit Distance (NED) as the primary evaluation metric to assess recognition accuracy at the character level. As illustrated in Table 5, PaddleOCR-VL-1.5 demonstrates clear advantage in seal recognition. Despite its compact size (0.9B parameters), it achieves an NED of 0.138, outperforming the 235B-parameter Qwen3-VL by large margin (0.382). This highlights the models effectiveness in handling specialized document elements. Model Parameters NED () Qwen2.5-VL-72B [29] Qwen3-VL-235B-A22B-Instruct [6] PaddleOCR-VL-1.5 72B 235B 0.9B 0.396 0.382 0.138 Table 5 Comparison of seal recognition performance on in-house-seal benchmark. 4.3. Inference Performance To speed up inference, we optimize the execution workflow of PaddleOCR-VL-1.5 by introducing an asynchronous, multi-threaded design, following the same strategy adopted in PaddleOCR-VL. The entire workflow is decomposed into three consecutive stages: input preparation (primarily 14 converting PDF pages into images), layout analysis, and VLM inference. Each stage runs in its own dedicated thread, and intermediate results are exchanged between adjacent stages via queue-based buffers. This pipelined architecture enables concurrent execution across stages, thereby increasing parallelism and improving overall throughput. For the VLM inference stage in particular, mini-batches are formed dynamically: batch is launched either when the queue size reaches preset capacity or when the oldest queued item has waited longer than specified time limit. This batching strategy makes it possible to group content blocks from multiple pages into single inference call, which substantially increases parallel efficiency, especially when processing large collections of documents. In addition, we deploy PaddleOCRVL-1.5-0.9B on high-performance inference and serving frameworks, i.e., FastDeploy [37], vLLM [38], and SGLang [39]. Key runtime parameters, including max-num-batched-tokens and gpu-memory-utilization, are carefully tuned to strike balance between maximizing inference throughput and controlling GPU memory usage. Table 6 summarizes the end-to-end inference efficiency of different OCR methods under various deployment backends on the OmniDocBench v1.5 dataset. PaddleOCR-VL-1.5 achieves the best overall performance across all metrics. With the FastDeploy backend, it reaches 1.4335 pages/s and 2016.6 tokens/s on single NVIDIA A100 GPU, surpassing its predecessor PaddleOCR-VL by 16.9% and 18.6%, respectively. These results verify that PaddleOCR-VL-1.5 provides state-of-the-art inference speed and throughput, making it well-suited for large-scale, real-world document understanding applications. Methods Backend Total Time (s) Pages/s Tokens/s MonkeyOCR-pro-1.2B [1] dots.ocr [35] MinerU2.5 (mineru=2.5.2) [2] DeepSeek-OCR [10] PaddleOCR-VL PaddleOCR-VL PaddleOCR-VL-1.5 PaddleOCR-VL-1.5 PaddleOCR-VL-1.5 vLLM (v0.10.2) vLLM (v0.14.0) vLLM (v0.10.2) vLLM (v0.8.5) vLLM (v0.10.2) FastDeploy (v2.3) vLLM (v0.10.2) SGLang (v0.5.2) FastDeploy (v2.3) 2152.7 3236.2 1356.5 2130.5 1325.5 1104.5 1184.3 1342.0 944. 0.6292 0.2791 0.9984 0.6358 1.0216 1.2261 1.1433 1.0091 1.4335 949.8 374.3 1415.1 897.4 1419.9 1700.5 1605.6 1418.9 2016.6 Table 6 End-to-End Inference Performance Comparison on OmniDocBench v1.5. PDF documents were processed in batches of 512 on single NVIDIA A100 GPU. The reported end-to-end runtime includes both PDF rendering and Markdown generation. All methods rely on their built-in PDF parsing modules and default DPI settings to reflect out-of-the-box performance. Tokenization and special processing details follow the protocol introduced in [9]. 5. Conclusion This work introduces PaddleOCR-VL-1.5, achieving record SOTA accuracy of 94.5% on OmniDocBench v1.5 and demonstrating superior general precision in document parsing. key advancement of this version is its exceptional robustness in unconstrained real-world environments. The model effectively overcomes critical hurdles such as aggressive skewing, non-rigid page warping, and erratic lightingscenarios where traditional solutions often fail. Furthermore, it expands its functional versatility with the integration of Seal Recognition and Text Spotting. By delivering high-fidelity data foundation, PaddleOCR-VL-1.5 will significantly enhance the reliability and performance of downstream RAG systems and Large Language Model applications in complex, real-world deployment."
        },
        {
            "title": "References",
            "content": "[1] Zhang Li, Yuliang Liu, Qiang Liu, Zhiyin Ma, Ziyang Zhang, Shuo Zhang, Zidun Guo, Jiarui Zhang, Xinyu Wang, and Xiang Bai. Monkeyocr: Document parsing with structurerecognition-relation triplet paradigm. arXiv preprint arXiv:2506.05218, 2025. [2] Junbo Niu, Zheng Liu, Zhuangcheng Gu, Bin Wang, Linke Ouyang, Zhiyuan Zhao, Tao Chu, Tianyao He, Fan Wu, Qintong Zhang, et al. Mineru2. 5: decoupled vision-language model for efficient high-resolution document parsing. arXiv preprint arXiv:2509.22186, 2025. [3] Hao Feng, Shu Wei, Xiang Fei, Wei Shi, Yingdong Han, Lei Liao, Jinghui Lu, Binghong Wu, Qi Liu, Chunhui Lin, et al. Dolphin: Document image parsing via heterogeneous anchor prompting. arXiv preprint arXiv:2505.14059, 2025. [4] Yuan Liu, Zhongyin Zhao, Le Tian, Haicheng Wang, Xubing Ye, Yangxiu You, Zilin Yu, Chuhan Wu, Xiao Zhou, Yang Yu, et al. Points-reader: Distillation-free adaptation of vision-language models for document conversion. arXiv preprint arXiv:2509.01215, 2025. [5] Baidu-ERNIE-Team. Ernie 4.5 technical report, 2025. [6] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [7] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [8] Patrick Lewis, Ethan Perez, Aleksandra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman Goyal, Heinrich K√ºttler, Mike Lewis, Wen-tau Yih, Tim Rockt√§schel, et al. Retrievalaugmented generation for knowledge-intensive nlp tasks. Advances in neural information processing systems, 33:94599474, 2020. [9] Cheng Cui, Ting Sun, Suyin Liang, Tingquan Gao, Zelun Zhang, Jiaxuan Liu, Xueqing Wang, Changda Zhou, Hongen Liu, Manhui Lin, et al. Paddleocr-vl: Boosting multilingual document parsing via 0.9 ultra-compact vision-language model. arXiv preprint arXiv:2510.14528, 2025. [10] Haoran Wei, Yaofeng Sun, and Yukun Li. Deepseek-ocr: Contexts optical compression. arXiv preprint arXiv:2510.18234, 2025. [11] Jiarui Zhang, Yuliang Liu, Zijun Wu, Guosheng Pang, Zhili Ye, Yupei Zhong, Junteng Ma, Tao Wei, Haiyang Xu, Weikai Chen, et al. Monkeyocr v1. 5 technical report: Unlocking robust document parsing for complex patterns. arXiv preprint arXiv:2511.10390, 2025. [12] Hunyuan Vision Team, Pengyuan Lyu, Xingyu Wan, Gengluo Li, Shangpin Peng, Weinong Wang, Liang Wu, Huawen Shen, Yu Zhou, Canhui Tang, et al. Hunyuanocr technical report. arXiv preprint arXiv:2511.19575, 2025. [13] Ting Sun, Cheng Cui, Yuning Du, and Yi Liu. Pp-doclayout: unified document layout detection model to accelerate large-scale data construction. arXiv preprint arXiv:2503.17213, 2025. 16 [14] Linke Ouyang, Yuan Qu, Hongbin Zhou, Jiawei Zhu, Rui Zhang, Qunshu Lin, Bin Wang, Zhiyuan Zhao, Man Jiang, Xiaomeng Zhao, et al. Omnidocbench: Benchmarking diverse pdf document parsing with comprehensive annotations. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2483824848, 2025. [15] Google DeepMind. Gemini 3.0. https://blog.google/products-and-platforms/p roducts/gemini/gemini-3-collection/, 2025. [16] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, and Jie Chen. Detrs beat yolos on real-time object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1696516974, 2024. [17] Mostafa Dehghani, Basil Mustafa, Josip Djolonga, Jonathan Heek, Matthias Minderer, Mathilde Caron, Andreas Steiner, Joan Puigcerver, Robert Geirhos, Ibrahim Alabdulmohsin, et al. Patch npack: Navit, vision transformer for any aspect ratio and resolution. Advances in Neural Information Processing Systems, 36:22522274, 2023. [18] PaddlePaddle Authors. Paddleformers. https://github.com/PaddlePaddle/Padd leFormers, 2025. [19] Yanjun Ma, Dianhai Yu, Tian Wu, and Haifeng Wang. Paddlepaddle: An open-source deep learning platform from industrial practice. Frontiers of Data and Domputing, 1(1):105115, 2019. [20] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Yang Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [21] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [22] Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, et al. Paddleocr 3.0 technical report. arXiv preprint arXiv:2507.05595, 2025. [23] Bin Wang, Fan Wu, Linke Ouyang, Zhuangcheng Gu, Rui Zhang, Renqiu Xia, Botian Image over text: Transforming formula recognition Shi, Bo Zhang, and Conghui He. evaluation with character detection matching. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1968119690, June 2025. [24] Vik Paruchuri. Marker. https://github.com/datalab-to/marker, 2025. Accessed: 2025-09-25. [25] opendatalab. Mineru2.0-2505-0.9b. https://huggingface.co/opendatalab/Miner U2.0-2505-0.9B, 2025. [26] Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. 17 [27] Weiyun Wang, Zhangwei Gao, Lixin Gu, Hengjun Pu, Long Cui, Xingguang Wei, Zhaoyang Liu, Linglin Jing, Shenglong Ye, Jie Shao, et al. Internvl3. 5: Advancing open-source multimodal models in versatility, reasoning, and efficiency. arXiv preprint arXiv:2508.18265, 2025. [28] OpenAI. Gpt-5.2 system card, 2025. URL https://cdn.openai.com/pdf/3a4153c8-c 748-4b71-8e31-aecbde944f8d/oai_5_2_system-card.pdf. [29] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [30] Google DeepMind. Gemini 2.5. https://blog.google/technology/google-deepm ind/gemini-model-thinking-updates-march-2025/, 2025. [31] chatdoc com. Ocrflux. https://github.com/chatdoccom/OCRFlux, 2025. Accessed:2025-09-25. [32] Mistral AI Team. Mistral-ocr. https://mistral.ai/news/mistral-ocr?utm_sourc e=ai-bot.cn, 2025. [33] Jake Poznanski, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Aman Rangapur, Christopher Wilhelm, Kyle Lo, and Luca Soldaini. olmocr: Unlocking trillions of tokens in pdfs with vision language models. arXiv preprint arXiv:2502.18443, 2025. [34] Souvik Mandal, Ashish Talewar, Paras Ahuja, and Prathamesh Juvatkar. Nanonets-ocr-s: model for transforming documents into structured markdown with intelligent content recognition and semantic tagging, 2025. [35] rednote-hilab. dots.ocr: Multilingual document layout parsing in single vision-language model, 2025. [36] Qing Jiang, Junan Huo, Xingyu Chen, Yuda Xiong, Zhaoyang Zeng, Yihao Chen, Tianhe Ren, Junzhi Yu, and Lei Zhang. Detect anything via next point prediction. arXiv preprint arXiv:2510.12798, 2025. [37] PaddlePaddle Authors. Fastdeploy. https://github.com/PaddlePaddle/FastDepl oy, 2025. [38] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th symposium on operating systems principles, pages 611626, 2023. [39] Lianmin Zheng, Liangsheng Yin, Zhiqiang Xie, Chuyue Livia Sun, Jeff Huang, Cody Hao Yu, Shiyi Cao, Christos Kozyrakis, Ion Stoica, Joseph Gonzalez, et al. Sglang: Efficient execution of structured language model programs. Advances in neural information processing systems, 37:6255762583, 2024."
        },
        {
            "title": "Appendix",
            "content": "A. Comparison of PaddleOCR-VL-1.5 and 1.0 Models Category Capability Item V1 V1.5 V1.5 Description Layout Analysis Text Recognition Fundamental Table Recognition stability Improved warped/skewed added CAD and comics. Gains in vertical text, special characters, and emphasis marks. Improvements for borderless for scenes; tables and invoices. Formula Recognition Chart Recognition Reading Order Better in skewed formulas and illumination scenarios. Capability remains consistent with the previous version. Significant boost for irregular layouts. Skewed Docs Scanned Docs Adaptability Warped Docs Screen Photo Illumination Seal Recognition Text Spotting New Features Dramatic improvement for high-angle tilted documents. Stability for low-quality scans is significantly enhanced. Supports complex physical deformation and folded paper. Suppresses interference from reflections and Moir√© patterns. Superior performance in uneven or weak lighting. Recognition of various official seals and stamps. Localization and recognition of multiple character sets. Cross-page Table Merginig Merges split tables while maintaining consistency. Heading Hierarchy Title hierarchy recognition across multi-page documents. Table A1 Comprehensive functional evolution and robustness comparison between PaddleOCR-VL and PaddleOCR-VL-1.5. The star ratings only indicate the relative performance of the two versions and do not represent their absolute accuracy. 19 B. Details of the Real5-OmniDocBench Benchmark Real5-OmniDocBench2 is brand-new benchmark oriented toward real-world scenarios, which we constructed based on the OmniDocBench v1.5 [14] dataset. PaddleOCR-VL-1.5 achieves stateof-the-art (SOTA) results across all sub-scenarios within Real5-OmniDocBench, demonstrating its robust parsing capabilities for real-world documents. detailed comparison of PaddleOCRVL-1.5 against other advanced document parsing models across various metrics on this dataset is provided in this Appendix. As shown in Table A2, under the scaning scenario, PaddleOCR-VL-1.5 achieves state-ofthe-art performance across all key metrics, consistently outperforming existing pipeline tools, general vision-language models, and specialized document parsing models. Compared to its predecessor, PaddleOCR-VL, the new version maintains compact parameter size of 0.9B while raising the overall score from 92.11% to leading 93.43%. Notably, PaddleOCR-VL-1.5 sets new records in all sub-tasks within this scenario, including Formula-CDM score of 93.04% and Table-TEDS score of 90.97%, both significantly surpassing larger models such as Qwen3-VL-235B and Gemini-3 Pro. Additionally, the model achieves exceptionally low Text-Edit Distance (0.037) and Reading Order score (0.045), further demonstrating its high accuracy in text recognition, formula extraction, and complex table structure analysis. Overall, PaddleOCR-VL-1.5 delivers new breakthrough in the Real5-OmniDocBench-scaning scenario. Model Type Methods Parameters Overall TextEdit FormulaCDM TableTEDS Reading OrderEdit Pipeline Tools Maker-1.8.2 [24] PP-StructureV3 [22] General VLMs GPT-5.2 [28] Qwen2.5-VL-72B [29] Gemini-2.5 Pro [30] Qwen3-VL-235B-A22B-Instruct [29] Gemini-3 Pro [15] Specialized VLMs Dolphin [3] Dolphin-1.5 [3] MinerU2-VLM [25] MonkeyOCR-pro-1.2B [1] MonkeyOCR-3B [1] Nanonets-OCR-s [34] Deepseek-OCR [10] dots.ocr [35] MonkeyOCR-pro-3B [1] MinerU2.5 [2] PaddleOCR-VL [9] PaddleOCR-VL-1.5 - - - 72B - 235B - 322M 0.3B 0.9B 1.9B 3.7B 3B 3B 3B 3.7B 1.2B 0.9B 0.9B 70.27 84.68 84.43 86.19 89.25 89.43 89. 72.16 83.39 83.60 84.64 84.65 85.52 86.17 86.87 86.94 90.06 92.11 93.43 0.223 0.094 0.142 0.110 0.073 0.059 0.071 0.154 0.097 0.094 0.123 0.100 0.106 0.078 0.083 0.103 0.052 0.039 0.037 77.03 84.34 85.68 86.14 87.44 89.01 88. 64.58 76.25 79.76 84.17 84.16 88.09 83.59 83.27 86.29 88.22 90.35 93.04 56.05 79.06 81.78 83.41 87.62 85.19 87.37 67.27 83.65 80.44 82.13 79.81 79.11 82.69 85.68 84.86 87.16 89.90 90.97 0.238 0.092 0.109 0.114 0.098 0.066 0. 0.130 0.090 0.091 0.145 0.143 0.106 0.085 0.081 0.141 0.050 0.048 0.045 Table A2 Comprehensive evaluation of document parsing on Real5-OmniDocBench-scaning As shown in Table A3, PaddleOCR-VL-1.5 exhibits notable robustness in the warping scenario, achieving an overall score of 91.25%, which is higher than the larger Qwen3-VL-235B model (89.99%). Its Formula-CDM score of 90.94% and Table-TEDS score of 88.10% indicate strong ability to preserve document structure under significant geometric distortion. 2https://huggingface.co/datasets/PaddlePaddle/Real5-OmniDocBench 20 Model Type Methods Parameters Overall TextEdit FormulaCDM TableTEDS Reading OrderEdit Pipeline Tools Maker-1.8.2 [24] PP-StructureV3 [22] General VLMs GPT-5.2 [28] Gemini-2.5 Pro [30] Qwen2.5-VL-72B [29] Gemini-3 Pro [15] Qwen3-VL-235B-A22B-Instruct [29] Specialized VLMs Dolphin-1.5 [3] Dolphin [3] Deepseek-OCR [10] MinerU2-VLM [25] MonkeyOCR-pro-1.2B [1] MonkeyOCR-3B [1] MonkeyOCR-pro-3B [1] Nanonets-OCR-s [34] MinerU2.5 [2] PaddleOCR-VL [9] dots.ocr [35] PaddleOCR-VL-1.5 - - - - 72B - 235B 0.3B 322M 3B 0.9B 1.9B 3.7B 3.7B 3B 1.2B 0.9B 3B 0.9B 58.98 59. 76.26 87.63 87.77 88.90 89.99 50.50 60.35 67.20 73.73 76.59 77.27 78.90 83.56 83.76 85.97 86.01 91.25 0.349 0.376 0.239 0.092 0.086 0.086 0.051 0.383 0.316 0.328 0.202 0.196 0.164 0.168 0.121 0.154 0.093 0.087 0.053 72.71 68. 80.90 86.50 88.85 88.10 89.06 47.24 61.06 73.59 77.72 78.85 79.08 79.55 86.24 85.92 85.45 85.03 90.94 39.08 47.40 71.80 85.59 83.06 87.20 85.95 42.52 51.58 60.80 63.65 70.52 69.18 73.94 76.57 80.71 81.77 81.74 88.10 0.390 0. 0.165 0.109 0.102 0.087 0.064 0.309 0.247 0.226 0.173 0.221 0.211 0.212 0.124 0.104 0.092 0.093 0.063 Table A3 Comprehensive evaluation of document parsing on Real5-OmniDocBench-warping. In the screen photography scenario presented in Table A4, PaddleOCR-VL-1.5 attains an overall score of 91.76%, demonstrating competitive performance among specialized vision-language models. The model achieves Formula-CDM score of 90.88%, outperforming MinerU2.5 (87.55%) and dots.ocr (85.34%), and shows effective handling of Moire patterns and reflections typically encountered in screen-captured documents. Model Type Methods Parameters Overall TextEdit FormulaCDM TableTEDS Reading OrderEdit Pipeline Tools Maker-1.8.2 [24] PP-StructureV3 [22] General VLMs GPT-5.2 [28] Qwen2.5-VL-72B [29] Gemini-2.5 Pro [30] Gemini-3 Pro [15] Qwen3-VL-235B-A22B-Instruct [29] Specialized VLMs Dolphin [3] Dolphin-1.5 [3] Deepseek-OCR [10] MinerU2-VLM [25] MonkeyOCR-pro-1.2B [1] MonkeyOCR-3B [1] MonkeyOCR-pro-3B [1] PaddleOCR-VL [9] Nanonets-OCR-s [34] dots.ocr [35] MinerU2.5 [2] PaddleOCR-VL-1.5 - - - 72B - - 235B 322M 0.3B 3B 0.9B 1.9B 3.7B 3.7B 0.9B 3B 3B 1.2B 0.9B 63.65 66.89 76.75 86.48 87.11 88.86 89. 64.29 69.76 75.31 78.77 80.24 80.71 82.44 82.54 84.86 87.18 89.41 91.76 0.290 0.204 0.208 0.100 0.103 0.084 0.068 0.232 0.205 0.220 0.139 0.148 0.122 0.124 0.103 0.112 0.081 0.062 0.050 72.73 73.26 79.27 87.46 85.30 87.33 88. 58.66 61.80 77.68 79.02 80.78 81.33 81.55 83.58 86.65 85.34 87.55 90.88 47.21 47.82 71.73 82.00 86.31 87.65 85.85 57.38 68.00 70.26 71.17 74.74 73.04 78.13 74.36 79.09 84.26 86.83 89.38 Table A4 Comprehensive evaluation of document parsing on Real5-OmniDocBench-screen-photography. 0.325 0. 0.148 0.102 0.117 0.087 0.071 0.195 0.177 0.169 0.123 0.179 0.177 0.177 0.107 0.117 0.079 0.053 0.059 Table A5 evaluates performance under illumination variations, where PaddleOCR-VL-1.5 reaches an overall score of 92.16%. This result not only marks significant improvement over the previous PaddleOCR-VL (89.61%) but also surpasses top-tier general VLMs such as Gemini-3 Pro (89.53%). The models Formula-CDM score of 91.80% and Table-TEDS of 89.33% underscore 21 its high sensitivity and accuracy in low-contrast or unevenly lit environments. Model Type Methods Parameters Overall TextEdit FormulaCDM TableTEDS Reading OrderEdit Pipeline Tools Maker-1.8.2 [24] PP-StructureV3 [22] General VLMs GPT-5.2 [28] Qwen2.5-VL-72B [29] Gemini-2.5 Pro [30] Qwen3-VL-235B-A22B-Instruct [29] Gemini-3 Pro [15] Specialized VLMs Dolphin [3] Dolphin-1.5 [3] Deepseek-OCR [10] MinerU2-VLM [25] MonkeyOCR-pro-1.2B [1] MonkeyOCR-3B [1] MonkeyOCR-pro-3B [1] Nanonets-OCR-s [34] dots.ocr [35] MinerU2.5 [2] PaddleOCR-VL [9] PaddleOCR-VL-1.5 - - - 72B - 235B - 322M 0.3B 3B 0.9B 1.9B 3.7B 3.7B 3B 3B 1.2B 0.9B 0.9B 66.31 73.38 80.88 87.25 87.97 89.27 89.53 67.29 75.61 78.10 80.51 82.11 83.16 84.71 85.01 87.57 89.57 89.61 92.16 0.259 0.158 0.191 0.087 0.083 0.060 0.073 0.197 0.159 0.192 0.135 0.144 0.118 0.120 0.099 0.068 0.065 0.049 0. 74.80 77.75 84.41 86.44 86.13 87.81 87.78 61.42 70.04 81.71 80.72 82.07 83.63 84.13 87.94 85.07 88.36 86.66 91.80 50.03 58.19 77.37 84.03 86.11 86.05 88.14 60.10 72.69 71.81 74.29 78.67 77.62 82.02 76.96 84.44 86.87 87.02 89. Table A5 Comprehensive evaluation of document parsing on Real5-OmniDocBench-illumination. 0.337 0.126 0.134 0.097 0.103 0.070 0.080 0.173 0.133 0.156 0.123 0.172 0.168 0.171 0.112 0.076 0.062 0.055 0.051 Under the challenging skew detailed in Table A6, PaddleOCR-VL-1.5 maintains its dominance with an overall score of 91.66%, once again outperforming general VLMs including Gemini-3 Pro (89.45%). It particularly excels in complex structural recovery, evidenced by Table-TEDS score of 91.00% and Text-Edit distance reduced to 0.047, demonstrating its superior ability to rectify and parse slanted document layouts. Model Type Methods Parameters Overall TextEdit FormulaCDM TableTEDS Reading OrderEdit Pipeline Tools PP-StructureV3 [22] Maker-1.8.2 [24] General VLMs GPT-5.2 [28] Qwen3-VL-235B-A22B-Instruct [29] Qwen2.5-VL-72B [29] Gemini-2.5 Pro [30] Gemini-3 Pro [15] Specialized VLMs Dolphin-1.5 [3] Dolphin [3] MonkeyOCR-pro-1.2B [1] Deepseek-OCR [10] MonkeyOCR-pro-3B [1] MonkeyOCR-3B [1] MinerU2-VLM [25] MinerU2.5 [2] PaddleOCR-VL [9] Nanonets-OCR-s [34] dots.ocr [35] PaddleOCR-VL-1.5 - - - 235B 72B - - 0.3B 322M 1.9B 3B 3.7B 3.7B 0.9B 1.2B 0.9B 3B 3B 0.9B 37.98 41. 75.00 86.56 86.90 89.07 89.45 28.16 44.83 62.18 63.01 64.47 65.67 68.16 75.24 77.47 81.98 84.27 91.66 0.557 0.536 0.257 0.077 0.077 0.077 0.080 0.553 0.500 0.292 0.327 0.251 0.248 0.230 0.305 0.192 0.121 0.087 0.047 44.37 60. 80.27 83.96 87.26 87.89 88.33 25.60 51.34 66.25 73.27 69.06 69.23 74.45 81.78 78.81 85.78 85.73 91.00 25.27 17.23 70.47 83.41 81.14 86.99 88.06 14.18 33.22 49.46 48.48 49.42 52.59 53.07 74.39 72.83 72.22 75.74 88.69 0.417 0. 0.167 0.091 0.091 0.104 0.092 0.419 0.321 0.317 0.231 0.301 0.300 0.191 0.151 0.193 0.133 0.094 0.061 Table A6 Comprehensive evaluation of document parsing on Real5-OmniDocBench-skewing variation. 22 C. Supported Languages PaddleOCR-VL-1.5 supports total of 111 languages. Compared to PaddleOCR-VL, PaddleOCRVL-1.5 adds recognition capabilities for Chinas Tibetan script and Bengali. Table A7 lists the correspondence between each language category and the specific supported languages/scripts. Language Category Specific Languages Chinese English Korean Japanese Thai Greek Tamil Telugu Bengali* Chinese English Korean Japanese Thai Greek Tamil Telugu Bengali* Chinas Tibetan script* Chinas Tibetan script* Arabic Latin Cyrillic Arabic, Persian, Uyghur, Urdu, Pashto, Kurdish, Sindhi, Balochi French, German, Afrikaans, Italian, Spanish, Bosnian, Portuguese, Czech, Welsh, Danish, Estonian, Irish, Croatian, Uzbek, Hungarian, Serbian (Latin), Indonesian, Occitan, Icelandic, Lithuanian, Maori, Malay, Dutch, Norwegian, Polish, Slovak, Slovenian, Albanian, Swedish, Swahili, Tagalog, Turkish, Latin, Azerbaijani, Kurdish, Latvian, Maltese, Pali, Romanian, Vietnamese, Finnish, Basque, Galician, Luxembourgish, Romansh, Catalan, Quechua Russian, Belarusian, Ukrainian, Serbian (Cyrillic), Bulgarian, Mongolian, Abkhazian, Adyghe, Kabardian, Avar, Dargin, Ingush, Chechen, Lak, Lezgin, Tabasaran, Kazakh, Kyrgyz, Tajik, Macedonian, Tatar, Chuvash, Bashkir, Malian, Moldovan, Udmurt, Komi, Ossetian, Buryat, Kalmyk, Tuvan, Sakha, Karakalpak Devanagari Hindi, Marathi, Nepali, Bihari, Maithili, Angika, Bhojpuri, Magahi, Santali, Newari, Konkani, Sanskrit, Haryanvi Table A7 Supported Languages/scipts (*indicates newly added languages/scipts ) 23 D. Inference Performance on Different Hardware Configurations We evaluated the inference throughput and latency of PaddleOCR-VL-1.5 across multiple hardware configurations, with the detailed results presented in Table A8. In our experiments, all PDF pages are rendered at 72 DPI, which provides good trade-off between memory efficiency and the visual fidelity required for reliable OCR. We note that the experiments on different hardware platforms were conducted without extensive parameter tuning or system-level optimization; therefore, the reported performance figures should be regarded as conservative and still leave room for further improvement. All models were evaluated using three deployment backends, namely FastDeploy v2.3.0, vLLM v0.10.2, and SGLang v0.5.2. Across these frameworks, PaddleOCR-VL-1.5 consistently delivers high and stable inference efficiency, demonstrating strong generalization across diverse hardware configurations and execution engines, as well as good compatibility with heterogeneous computing environments. Hardware Backend Total Time (s) Pages/s Tokens/s Avg. VRAM Usage (GB) H800 A100 H20 L20 RTX 3060 RTX 4090D FastDeploy vLLM SGLang FastDeploy vLLM SGLang FastDeploy vLLM SGLang FastDeploy vLLM SGLang FastDeploy vLLM SGLang vLLM SGLang vLLM SGLang 556.4 761.8 868.5 671.3 981.4 1100.9 743.7 796.1 862. 845.0 998.2 1126.8 1179.9 1245.5 1504.3 2531.8 2587.7 923.5 1079.5 2.4320 1.7772 1.5589 2.0160 1.3797 1. 1.8206 1.7007 1.5702 1.6023 1.3565 1.2018 1.1477 1.0873 0.9003 0.5351 0.5235 1.4667 1.2548 3404.5 2488.0 2185. 2826.0 1926.1 1722.5 2545.0 2382.0 2204.5 2248.4 1890.7 1680.7 1607.8 1520.6 1260.1 748.1 730.5 2040.1 1750. Table A8 End-to-End Inference Performance 64.8 46.2 48.9 62.1 43.5 48.9 77.2 75.0 74.3 41.0 25.1 30.2 21.8 13.5 19. 11.8 11.7 16.3 20.0 24 E. Real-world Samples This appendix demonstrates the robustness and versatility of PaddleOCR-VL-1.5 in processing diverse, high-complexity real-world scenarios. Section E.1 demonstrates the real-world document parsing capability of PaddleOCR-VL-1.5. Figures A1A5 demonstrate the robust performance of PaddleOCR-VL-1.5 to parse real-world documents across diverse conditions, including varying illumination, geometric skew, screen-tophoto noise, and warped scanned surfaces. Figures A6A9 in Section E.2 illustrate the robustness of PaddleOCR-VL-1.5 in layout analysis across challenging real-world conditions, including skewed or curved geometries, screen-tophoto noise, and illumination variations. Furthermore, Figure A10 highlights its extended generalizability to specialized domainssuch as comics, CAD drawings, and multi-stamped documentswhere earlier version of the model faced limitations. Section E.3 evaluates the text recognition performance of PaddleOCR-VL-1.5 under diverse constraints. As shown in Figure A11, the model exhibits improved sensitivity to text decorations, including underlines, emphasis marks, and wavy patterns, surpassing its predecessor. Figure A12 and Figure A13 demonstrate enhanced robustness in identifying special characters and long-tail cases, such as vertical orientations and character-level ambiguities. The models table recognition abilities are demonstrated in section E.4. Figure A14 illustrates the robustness of the model in processing complex layouts, including tables from academic textbooks and those containing embedded images or mathematical formulas. The proficiency of the model in multilingual table recognition is presented in Figure A15. Furthermore, Figure A16 demonstrates its extended capability for cross-page table detection and merging, addressing the challenges of multi-page document parsing.\" Figures in section E.5 detail the formula recognition performance. As illustrated in Figure A17 , the updated model exhibits superior performance in mathematical expression recognition, specifically regarding sub/superscript accuracy, multi-line formula segmentation, and lower overall error rate. In section E.6, seal recognition represents novel capability of this model update. As illustrated in Figures A18A20, the model accurately extracts content from various types of seals, demonstrating high precision even when faced with complex background interference and cluttered environments. Figure A21 in section E.7 highlights the newly integrated text spotting capability of the model, which enables simultaneous localization and recognition. The results demonstrate superior robustness across challenging layouts, ranging from multi-column magazine pages and complex tables to irregular handwritten content. 25 E.1. Real-word Document Parsing Figure A1 The Markdown Output for Illumination. 26 Figure A2 The Markdown Output for Skew. Figure A3 The Markdown Output for Screen Photography. 28 Figure A4 The Markdown Output for Scanning. 29 Figure A5 The Markdown Output for Warping. E.2. Layout Analysis E.2.1. Layout Analysis for Real-world Documents Figure A6 Comparison of Layout Analysis Results between PaddleOCR-VL and PaddleOCR-VL-1.5 for Warping. 31 Figure A7 Comparison of Layout Analysis Results between PaddleOCR-VL and PaddleOCR-VL-1.5 for Screen Photography. Figure A8 Comparison of Layout Analysis Results between PaddleOCR-VL and PaddleOCR-VL-1.5 for Skew. 33 Figure A9 Comparison of Layout Analysis Results between PaddleOCR-VL and PaddleOCR-VL-1.5 for Illumination. 34 E.2.2. Layout Analysis for New Scenarios Figure A10 Comparison of Layout Analysis Results between PaddleOCR-VL and PaddleOCR-VL-1.5 for New Scenarios. 35 E.3. Text Recognition E.3.1. Text Recognition for Text decoration Figure A11 Markdown Output Comparison between PaddleOCR-VL and PaddleOCR-VL-1.5 on Text Decoration Documents. 36 E.3.2. Text Recognition for Special characters Figure A12 Markdown Output Comparison between PaddleOCR-VL and PaddleOCR-VL-1.5 on Special Characters Documents. 37 E.3.3. Text Recognition for Long-tail Scenarios Figure A13 Markdown Output Comparison between PaddleOCR-VL and PaddleOCR-VL-1.5 on Long-tail Scenarios Documents. 38 E.4. Table Recognition E.4.1. Table Recognition for General Tables Figure A14 Markdown Output Comparison between PaddleOCR-VL and PaddleOCR-VL-1.5 on General Tables. 39 E.4.2. Table Recognition for Multiple Languages Figure A15 Markdown Output Comparison between PaddleOCR-VL and PaddleOCR-VL-1.5 on Multilingual Tables. E.4.3. Table Recognition for Cross-Page Tables Figure A16 Markdown Output Comparison between PaddleOCR-VL and PaddleOCR-VL-1.5 on Cross-Page Tables. 41 E.5. Formula Recognition Figure A17 Markdown Output Comparison between PaddleOCR-VL and PaddleOCR-VL-1.5 on various types of Formulas. E.6. Seal Recognition Figure A18 The markdown output for various types of Seals 1. 43 Figure A19 The markdown output for various types of Seals 2. 44 Figure A20 The markdown output for various types of Seals 3. 45 E.7. Text Spotting Figure A21 Text spotting results on various types of documents."
        }
    ],
    "affiliations": [
        "Baidu Inc."
    ]
}