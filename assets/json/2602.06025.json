{
    "paper_title": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory",
    "authors": [
        "Haozhen Zhang",
        "Haodong Yue",
        "Tao Feng",
        "Quanyu Long",
        "Jianzhu Bao",
        "Bowen Jin",
        "Weizhi Zhang",
        "Xiao Li",
        "Jiaxuan You",
        "Chengwei Qin",
        "Wenya Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \\textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \\textsc{Low}/\\textsc{Mid}/\\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes."
        },
        {
            "title": "Start",
            "content": "Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Haozhen Zhang * 1 Haodong Yue * 2 Tao Feng 3 Quanyu Long 1 Jianzhu Bao 1 Bowen Jin 3 Weizhi Zhang 4 Xiao Li 5 Jiaxuan You 3 Chengwei Qin 6 Wenya Wang"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction 6 2 0 2 5 ] . [ 1 5 2 0 6 0 . 2 0 6 2 : r Memory is increasingly central to Large Language Model (LLM) agents operating beyond single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard querycritical information. Although runtime memory utilization is natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost tradeoff. In this work, we present BudgetMem, runtime agent memory framework for explicit, queryaware performancecost control. BudgetMem structures memory processing as set of memory modules, each offered in three budget tiers (i.e., LOW/MID/HIGH). lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as compact neural policy trained with reinforcement learning. Using BudgetMem as unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracycost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes. Code is available at https://github.com/ViktorAxelsen/BudgetMem *Equal contribution 1Nanyang Technological University 2Tsinghua University 3University of Illinois Urbana-Champaign 4University of Illinois Chicago 5Sun Yat-sen University 6 The Hong Kong University of Science and Technology (Guangzhou). Correspondence to: Haozhen Zhang <haozhen001@e.ntu.edu.sg>, Chengwei Qin <chengweiqin@hkust-gz.edu.cn>. Preprint. February 6, 2026. 1 Memory has become core component of modern Large Language Model (LLM) agents, enabling them to retain and reuse information beyond single context window for long-horizon interaction, personalization, and knowledgeintensive reasoning (Hu et al., 2025). Nevertheless, most prior work centers on offline, query-agnostic memory construction, where past context is preprocessed, compressed, or indexed in fixed manner without conditioning on the downstream query (Kang et al., 2025; Fang et al., 2025; Zhong et al., 2024; Xu et al., 2025; Packer et al., 2023; Chhikara et al., 2025). This build once, use always paradigm can be wasteful and brittle: it spends computation regardless of what particular query needs, while potentially omitting crucial information for specific queries. Instead, an intuitive alternative is on-demand memory extraction, where computation is triggered at runtime based on the current query. This flexibility comes at cost: it pushes memory processing to runtime, making cost and latency first-class concerns. In practice, industrial LLM systems increasingly provide explicit, often tiered compute controls (e.g., thinking modes (Singh et al., 2025), reasoning levels (OpenAI, 2025), or heavier-model options (Anthropic, 2025)), reflecting the need to balance quality against runtime cost. This motivates key question for agent memory: how can we enable explicit and controllable performancecost trade-offs for runtime memory extraction? Unfortunately, enabling performance-cost trade-offs for runtime agent memory is fundamentally challenging. Most existing trade-off mechanisms operate offline, while ondemand memory pushes these decisions to runtime, where each query raises quality-cost choice. This exposes two core questions. First, where should budgets be applied? Existing systems often treat memory as monolithic pipeline with fixed compute setting, making trade-offs coarse and difficult to control (Kang et al., 2025; Zhong et al., 2024). This motivates defining an appropriate budgeting unit for runtime settings, i.e., which modular part(s) of the memory extraction process should be assigned budgets, so that computation can be controlled in targeted and effective way. Second, how should budgets be realized? Prior work provides little systematic guidance on trade-offs for runLearning Query-Aware Budget-Tier Routing for Runtime Agent Memory time memory, often resorting to ad hoc cost reduction or simply increasing compute (Yan et al., 2025a). As result, even after budgeting scheme is specified, it remains unclear how to operationalize budget control, which design axes best capture meaningful trade-offs, and how these choices behave across different budget regimes. Addressing these questions requires moving beyond one-off heuristics or compute-heavy escalation toward more systematic view of performancecost control for runtime agent memory. To address the above challenges, we propose BudgetMem, runtime agent memory framework that enables explicit, controllable performancecost trade-offs for on-demand memory extraction. BudgetMem views runtime memory extraction as multi-stage modular pipeline and makes computation controllable at the module level. Specifically, BudgetMem standardizes how each module is invoked by exposing common budget-tier interface, so that learned router can select among budget tiers within modules while keeping the overall extraction structure fixed. Building on this modular backbone, BudgetMem provides three budget tiers (i.e., LOW/MID/HIGH) for each module, offering different quality-cost trade-offs. We instantiate budget tiers through three complementary tiering strategies: implementation tiering (varying the module implementation), reasoning tiering (varying inference behavior), and capacity tiering (varying the modules model capacity). To navigate these tiered choices, BudgetMem employs shared lightweight router that performs budget-tier routing as the query is processed: at each module, it selects tier based on the available context (i.e., the query and intermediate module states). We train the router with reinforcement learning under cost-aware reward that trades off task performance against memory extraction cost, forming controllable performancecost behavior. This design yields practical and controllable runtime memory system, while also enabling unified comparison of different budget realization strategies. In experiments on LoCoMo, LongMemEval, and HotpotQA, BudgetMem delivers strong gains over competitive baselines in performance-first (high-budget) settings, and exhibits clear performancecost trade-off curves as budgets tighten. Moreover, our analyses disentangle the relative strengths of different budget tiering strategies, offering insights into which mechanisms provide the best returns under different budget regimes. Our contributions are summarized as follows: We introduce BudgetMem, modular runtime agent memory framework that enables explicit performance cost control for on-demand memory extraction via budget-tiered modules. We propose budget-tier routing, learning shared lightweight router with reinforcement learning to select budget tiers during extraction and further instantiate three complementary budget realization strategies (i.e., implementation, reasoning, and capacity tiering) within unified framework. Experiments on LoCoMo, LongMemEval, and HotpotQA demonstrate strong performance and clear performancecost trade-offs, and our further analyses provide valuable insights into when different strategies deliver the best returns across budget regimes. 2. Related Work 2.1. Memory-Augmented LLM Agents Memory-augmented LLM agents typically maintain an external memory store to overcome finite context windows and support long-horizon use. large portion of prior work emphasizes offline or ahead-of-time memory construction, where past interactions are periodically summarized/compressed and indexed, and later accessed via retrieval at query time (Zhong et al., 2024; Packer et al., 2023; Lee et al., 2024; Kang et al., 2025; Fang et al., 2025). Representative designs organize memories chronologically and hierarchically (e.g., event summaries and persona profiles) with information retrieval and heuristic update rules such as recency-based decay (Zhong et al., 2024; Kang et al., 2025). More recent work enriches memory with agentic updates and structure, e.g., constructing metadata-rich notes and linking them into graphs for scalable retrieval and evolution (Xu et al., 2025), or using LLM-based memory managers to retrieve similar entries and apply discrete operations (i.e., add, update, delete, and no-op), with variants that build structured memories such as knowledge graphs (Chhikara et al., 2025). Several methods further introduce learning-based memory management (often via reinforcement learning) to optimize memory operations using downstream task signals (Yan et al., 2025b; Wang et al., 2025). On-demand utilization approaches also push beyond retrieve-then-answer by invoking deeper planning over memories (Yan et al., 2025a). Despite these advances, runtime memory remains costly and is rarely framed as explicit performancecost control: most work relies on fixed pipelines or studies efficiency mainly in offline construction (Fang et al., 2025). In contrast, we focus on runtime memory extraction and systematically compare controllable performancecost trade-offs under budgets. 2.2. Inference-Time Performance-Cost Trade-offs in"
        },
        {
            "title": "LLM Systems",
            "content": "A growing body of work studies how to trade LLM quality against runtime cost and latency by exposing controllable compute knobs. Existing approaches broadly fall into three categories. (i) Algorithmic and systems-level optimizations reduce inference cost without changing the task, including 2 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory faster decoding (Fu et al., 2024; Cai et al., 2024; Li et al., 2024b), early-exit or adaptive-depth inference (Schuster et al., 2022), pruning/sparsity (Ma et al., 2023; Frantar & Alistarh, 2023; Sun et al., 2023), quantization (Xiao et al., 2023a; Liu et al., 2024), and serving-side optimizations such as caching/batching and KV-cache efficiency for long contexts (Zhang et al., 2023; Xiao et al., 2023b; Ge et al., 2023; Li et al., 2024a). (ii) Reasoning-level controls vary inference behavior under different budgets, e.g., direct generation vs. chain-of-thought (Wei et al., 2022), self-refinement and reflection loops (Yao et al., 2022; Shinn et al., 2023; Madaan et al., 2023), or bounded deliberation via limits on steps, samples, or search (Wang et al., 2022). (iii) Capacity-level controls vary effective model capacity, including mixture-ofexperts activation (Shazeer et al., 2017; Fedus et al., 2022) and distillation-based deployments (Gu et al., 2023; Agarwal et al., 2024), closely related to LLM routing across backends under budget constraints (Chen et al., 2024; Feng et al., 2024; Zhang et al., 2025; Jin et al., 2025). While these directions offer practical mechanisms for controllable trade-offs, they have been less systematically explored in runtime agent memory, where computation is spent on memory extraction rather than answer generation. In this work, we take first step toward bringing explicit performancecost control to runtime agent memory by introducing budget-tiered framework and studying complementary budget realization strategies in unified setting. 3. Problem Setup and Method Overview In this work, we study runtime agent memory extraction, where the system selectively processes raw historical records at query time to construct compact memory for answering the current user query. Unlike offline pipelines that pre-compress or pre-structure the entire history, we keep past records intact and defer memory computation until query arrives, avoiding irreversible information loss introduced by query-agnostic preprocessing. Before detailing the methodology, we briefly summarize the task setup from the perspectives of inputs and outputs. Inputs. Let denote the LLM agents history, e.g., prior conversations, logs, or documents. We first perform lightweight, task-agnostic segmentation of into chunk store = {ci}N i=1, where each ci is short text chunk1. Given user query q, retriever returns subset of potentially relevant chunks: Cq = R(q, C), Cq C. (1) where (q, Cq) serves as input to runtime memory pipeline. 1This step only segments raw text for indexing and does not perform any offline extraction, summarization, or rewriting. Outputs. The system produces (i) an extracted memory that is compact and useful for the query, and (ii) final answer ˆy generated by an LLM conditioned on (q, m): = fmem(q, Cq), ˆy = fans(q, m). (2) Here fmem is learned, budget-controlled memory extraction procedure (our focus), while fans is an answer generator (e.g., fixed LLM). Budgeted runtime extraction. key challenge is that runtime extraction occurs online and can be expensive; thus we seek explicit and controllable performancecost tradeoffs. To this end, we design modular memory pipeline in which each module exposes small set of budget tiers (LOW/MID/HIGH). shared lightweight router makes budget-tier routing decisions as the query is processed, selecting which tier to use for each module based on the available context (the query and intermediate signals). The extracted memory produced by the final module is then used to answer the query, and the end-to-end process yields task-level performance signal and an extraction cost signal for optimizing the router in an end-to-end manner. 4. BudgetMem In this section, we first introduce the BudgetMem modular pipeline backbone (Sec. 4.1), then formalize budget tiers and present three tiering strategies (Sec. 4.2), and finally detail how we learn budget-tier routing with lightweight router optimized via reinforcement learning (Sec. 4.3). 4.1. Modular Runtime Memory Pipeline As shown in Figure 1, BudgetMem implements fmem as multi-stage modular pipeline that progressively refines retrieved raw chunks into query-focused memory. We instantiate the pipeline with simple, interpretable module composition (filtering parallel extraction summarization) for our dialogue-centric evaluation, serving as concrete backbone for studying budget control. Note that while we use this instantiation in our experiments, the framework itself does not assume specific module set, and can be adapted to other modular memory pipelines. Pipeline structure. Given (q, Cq), BudgetMem executes fixed modular pipeline: Mfil {Ment, Mtmp, Mtop} Msum. (3) At high level, Mfil (i.e., filter module) refines the retrieved context, the parallel modules {Ment, Mtmp, Mtop} (i.e., entity, temporal, and topic module) extract complementary intermediate contexts, and Msum aggregates them into the final memory. The output of an earlier module is fed into 3 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Figure 1. BudgetMem overview. Given user query q, we retrieve raw chunks Cq from chunked history (without offline memory preprocessing) and process them with modular pipeline (filter entity/temporal/topic summary). Each module exposes LOW/MID/HIGH budget tiers instantiated by one of three strategies (implementation, reasoning, capacity). shared lightweight router selects tiers module-wise based on the query and intermediate states, and is trained with reinforcement learning using task and cost rewards to yield controllable performancecost trade-offs. (Cq: retrieved raw chunks, Cq: filtered chunks, e, t, p: extracted contexts, m: extracted memory) later modules, forming intermediate states that the pipeline refines over stages. Since our focus is on budget-tier control and routing rather than the specific module implementations, we leave module details to Appendix A.4. Module interfaces and intermediate states. We represent each module invocation by context that includes the query and the currently available signals. The filtering module produces refined chunk set Cq = Mfil(q, Cq). (4) where Cq can be viewed as more focused subset or reweighted set of chunks. Next, three extraction modules operate in parallel on the filtered context: = Ment(q, Cq), = Mtmp(q, Cq), = Mtop(q, Cq). (5) where e, t, denote extracted contexts (e.g., entity-centric facts, temporal cues, topical summaries) that can be represented as text, structured fields, or compact natural-language notes. Finally, the summarization module aggregates these outputs into the extracted memory: = Msum(q, e, t, p), (6) which is then provided to the answer generator (i.e., ˆy = fans(q, m)). Budget-tier interface (conceptual). Each module in the pipeline is exposed through common budget-tier interface, meaning that the module can be invoked under 4 different computation budgets (LOW/MID/HIGH) while preserving the same inputoutput contract. This standardization enables module-level budget control and lays the foundation for learning budget-tier routing (Sec. 4.3) and for comparing different tiering strategies (Sec. 4.2). 4.2. Budget Tiers and Tiering Strategies BudgetMem equips each module in the pipeline with small set of budget tiers to expose explicit qualitycost options during runtime extraction. Specifically, every module provides three tiers, LOW/MID/HIGH, which correspond to increasing computational budget and (typically) improved extraction quality. Importantly, budget tiers are defined within each module via common budget-tier interface (Sec. 4.1), enabling the router to make fine-grained, modulewise budget decisions without altering the pipeline structure. Tiering strategies. key question is how to realize these tiers in practice. Motivated by common performancecost knobs used in modern LLM systems (Sec. 2.2), we study three complementary tiering strategies that capture different sources of cost variation: (i) implementation tiering, which varies the module implementation (from lightweight heuristics to learned task-specific models to LLM-based processing); (ii) reasoning tiering, which varies the inference behavior while keeping the underlying model backbone fixed (e.g., direct generation vs. more deliberative inference patterns); and (iii) capacity tiering, which varies the model capacity used inside module (e.g., smaller vs. larger LMs). These strategies are orthogonal: they trade computation through different mechanisms (algorithmic procedure, reaLearning Query-Aware Budget-Tier Routing for Runtime Agent Memory soning patterns, and model size), enabling direct comparison of their trade-off characteristics in unified framework. Realizing LOW/MID/HIGH. For implementation tiering, LOW uses lightweight rule-based or pattern-based processing, MID uses compact learned model specialized for the module function (typically BERT-based models (Devlin et al., 2019)), and HIGH upgrades the module to LLM-based processing for higher-quality extraction. For reasoning tiering, tiers are realized by progressively more computeintensive inference behaviors (e.g., LOW: direct; MID: CoT-style (Wei et al., 2022); HIGH: multi-step/reflectionstyle (Shinn et al., 2023)). For capacity tiering, tiers correspond to increasing model sizes used to implement the same module function. Concrete instantiations for each module under each strategy are provided in Appendix A.4. 4.3. Learning Module-Wise Budget-Tier Routing Given tiered modules, BudgetMem learns shared lightweight router that performs budget-tier routing throughout the modular pipeline. As the query is processed stageby-stage, the router selects budget tier for each module invocation, yielding sequence of tier decisions for single query. Since the memory extraction process can involve non-differentiable components, we formulate routing as sequential decision problem and optimize the router with reinforcement learning. Policy, state, and action. Let πθ denote the router policy with parameters θ. At each module invocation step k, the router observes state sk that summarizes the available context, and outputs an action ak {LOW, MID, HIGH} selecting the tier for the current module. Concretely, we construct sk from: (i) the query q, (ii) the current module input (i.e., the output from the previous module), and (iii) module descriptor indicating which module is being routed2. The chosen action ak determines the budget tier at which the module is executed, producing the next intermediate signal(s) and thus the next state. Episode and objective. complete run of the modular pipeline for one query constitutes an episode. After executing all routed modules, we obtain an extracted memory and generate an answer ˆy = fans(q, m), from which we compute task performance reward rtask [0, 1]. In addition, the routed module executions incur an extraction cost, which we convert into cost reward rcost. We optimize the router under cost-aware objective that trades off task performance and extraction cost: = rtask + λ α rcost, (7) where λ controls the performancecost trade-off preference, and α is scaling factor used to align reward magnitudes (described below). We use standard policy optimization algorithm (e.g., PPO (Schulman et al., 2017)) to optimize πθ; algorithmic details are provided in Appendix A.3. Cost modeling. We define the raw extraction cost as the sum of per-module costs along the routed pipeline: craw = (cid:88) c(Mk, ak). (8) For LLM-based tiers, we measure c() by token usage multiplied by the corresponding input/output token prices; for non-LLM tiers (e.g., lightweight heuristics), the cost is treated as negligible in comparison. To make the cost term commensurate with rtask, we apply sliding-window normalization to map costs to bounded scale (Zhang et al., 2025). Specifically, we maintain recent window of raw costs and normalize craw using robust quantiles: = craw Q5 Q95 Q5 , rcost = 1 clip(c, 0, 1), (9) where Q5 and Q95 denote the 5th and 95th percentiles computed over the sliding window. Reward-scale alignment. In practice, we observe that rtask and rcost can exhibit different variances during training, which may cause the higher-variance term to dominate policy updates. To mitigate this, we introduce simple variance-based alignment factor: α = std(rtask) std(rcost) + ϵ , (10) where std() is computed over recent training rewards and ϵ is small constant. The final reward in Eq. 7 then balances (i) the controllable trade-off via λ and (ii) stabilized optimization via α. 5. Experimental Setup 5.1. Datasets, Metrics, and Baselines Datasets. We evaluate BudgetMem on three benchmarks that stress long-horizon memory and long-context question answering (QA): LoCoMo (Maharana et al., 2024) and LongMemEval (Wu et al., 2024), which are widely used for agent memory evaluation, and HotpotQA (Yang et al., 2018)3 as representative long-context QA task. Metrics. We report both task performance and cost to characterize performancecost trade-offs. For task performance, we use F1-score (F1) and LLM-as-a-judge (Judge) 2We implement the state as compact embedding of these 3we follow the evaluation protocol adopted in prior work (Yu components; details are deferred to Appendix A.3. et al., 2025) 5 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Table 1. Experimental results on LoCoMo, LongMemEval, and HotpotQA datasets using F1-score (F1), LLM-as-a-Judge (Judge), and Cost under the performance-first setting. Methods LoCoMo LongMemEval HotpotQA Avg. F1 Judge Cost F1 Judge Cost F1 Judge Cost F1 Judge Cost LLaMA-3.3-70B-Instruct 22.48 ReadAgent 22.27 MemoryBank 26.43 A-MEM 22.31 LangMem 11.04 Mem0 30.62 MemoryOS 33.88 LightMem BudgetMem-IMP BudgetMem-REA BudgetMem-CAP 38.75 40.92 43.05 31.05 28.98 32.96 25.96 28.18 34.55 40. 50.32 52.23 54.62 Qwen3-Next-80B-A3B-Instruct ReadAgent MemoryBank A-MEM LangMem Mem0 MemoryOS LightMem 22.45 23.53 27.65 20.89 10.77 35.43 32.85 31.37 34.71 38.54 23.40 25.32 38.85 42.83 0.57 0.73 2.88 0.48 2.89 1.97 1.50 1.80 2.90 2. 0.24 0.25 2.88 0.14 1.15 0.75 0.70 20.75 26.74 21.74 12.00 27.70 12.97 26.74 37.47 40.53 40.24 27.72 10.56 10.82 11.01 25.71 13.35 27.70 27.72 32.67 33.17 17.00 42.08 33.50 48.51 56.00 58.00 60. 20.75 28.22 31.19 14.00 36.14 33.00 47.52 13.68 3.94 80.02 16.60 13.57 38.83 5.28 0.71 0.67 0.80 4.97 3.45 21.00 3.99 4.96 15.84 3.39 15.33 22.25 43.25 22.78 28.03 34.50 45.73 49.31 51.12 53. 18.05 18.64 40.54 20.77 24.72 41.21 41.29 30.08 23.75 54.69 22.66 36.72 43.36 58.37 65.77 61.93 64.85 25.78 31.25 50.39 21.09 37.89 53.52 55.42 4.19 7.75 13.32 10.95 4.30 13.32 10.10 1.35 0.99 0. 1.75 1.79 8.34 17.56 2.02 11.68 8.56 19.52 23.75 30.47 19.03 22.26 26.03 35.45 41.84 44.19 45.72 22.74 17.58 26.34 17.56 20.40 30.00 33.95 29.62 28.47 40.27 21.87 35.66 37.14 49.21 57.36 57.39 59. 25.97 31.39 40.04 19.50 33.12 41.79 48.59 6.14 4.14 36.55 9.34 6.92 18.04 5.63 1.29 1.52 1.38 2.32 1.83 10.74 2.82 2.71 9.42 4.21 40.14 40.19 41.22 BudgetMem-IMP 29.18 35.84 BudgetMem-REA BudgetMem-CAP 31.01 Bold indicates the best score within each base model block. Avg. is averaged over datasets for each metric. Cost is computed by summing all tokens across model calls and applying the models respective inputand output-token prices indicates no training using this base model (transfer evaluation only). 54.38 53.34 53.18 38.66 44.57 43.64 54.60 61.06 60.42 52.00 59.00 56.00 46.67 57.67 58.70 57.42 70.83 72. 0.63 0.17 0.22 0.80 1.11 0.61 0.30 0.26 0.17 0.58 0.51 0.33 to assess the correctness of model prediction against the ground-truth answer. For cost, we measure memory extraction cost by aggregating API usage (input & output tokens) for all model calls made by BudgetMem during extraction, and converting token counts into monetary cost using the corresponding service pricing4. (Lee et al., 2024), Baselines. We compare BudgetMem against diverse set of strong memory-augmented baselines: (1) ReadAgent (2) MemoryBank (Zhong et al., 2024), (3) A-MEM (Xu et al., 2025), (4) LangMem (LangChain, 2025), (5) Mem0 (Chhikara et al., 2025), (6) MemoryOS (Kang et al., 2025), and (7) LightMem (Fang et al., 2025). These baselines cover broad range of memory architectures, providing comprehensive comparison against prior agent memory systems. More details are provided in Appendix A. 5.2. Implementation Details We use LLaMA-3.3-70B-Instruct (Grattafiori et al., 2024) and Qwen3-Next-80B-A3B-Instruct (Yang et al., 2025) as the base LLMs, accessed via an API service. We train the 4https://www.together.ai/pricing budget-tier router with LLaMA as the memory-extraction backbone, then directly test the same trained router with Qwen as the backbone without retraining. For the retrieval and chunking, we split each history or document into fixedlength text chunks (with default chunk size of 256 for LoCoMo and LongMemEval; 1024 for HotpotQA) and use Contriever (Izacard et al., 2021) as the default retriever. For each query, we retrieve candidate set of chunks and then apply the same top-K budget (K=5) for downstream memory processing, so that BudgetMem and all baselines operate under matched retrieval context size. As for training, we follow 6/2/2 split for train/validation/test across datasets, and keep the same data splits and evaluation protocols for all methods to ensure fair comparison. We employ the Proximal Policy Optimization (PPO) (Schulman et al., 2017) as the default RL algorithm and use Adam optimizer with batch size of 32 for up to 600 training steps. Appendix and provide additional hyperparameters, prompts, and implementation details. 6. Experimental Analysis We organize experiments as follows. In Sec. 6.1, we report performance-first results (λ = 0) and compare 6 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Figure 2. Performancecost trade-offs across tiering strategies on LoCoMo. By varying the cost weight λ, BudgetMem traces smooth, controllable frontiers that shift toward higher performance as budget increases, and envelop baselines in both lowand high-cost regimes. three variants: BudgetMem-IMP, BudgetMem-REA, and BudgetMem-CAP. Sec. 6.2 varies λ to trace performance cost curves, Sec. 6.3 ablates cost modeling and reward design, and Sec. 6.4 provides further analysis. single benchmark. This trend holds for both LLaMA and Qwen, showing that BudgetMem delivers broadly strong performance across diverse evaluation settings under the performance-first regime. 6.1. Main Results 6.2. Exploring Trade-off Across Tiering Strategies As shown in Table 1, we evaluate BudgetMem under the performance-first setting on LoCoMo, LongMemEval, and HotpotQA, comparing against diverse set of representative memory systems. The results support three key findings. Effectiveness. BudgetMem delivers substantial improvements across all three datasets, consistently outperforming prior methods in both F1 and LLM-Judge. For example, on LongMemEval with the LLaMA-3.3-70B backbone, BudgetMem-CAP achieves Judge score of 60.50, surpassing the strongest baseline LightMem (48.51) by large margin, demonstrating stronger long-context evidence utilization and higher answer quality. Strong performance under controlled cost. Even in the performance-first regime, BudgetMem remains costefficient, achieving higher quality without incurring excessive overhead. Notably, on HotpotQA with Qwen3-Next80B-A3B, BudgetMem-CAP achieves the best Judge score (72.08) at cost of 0.22, while BudgetMem-REA reaches comparable Judge score (70.83) at an even lower cost (0.17). This efficiency stems from BudgetMems runtime, on-demand design: rather than processing the entire history offline, it retrieves query-relevant raw chunks and spends extraction compute only when needed. We note an exception on LoCoMo, where histories are shorter and offline pipelines incur less overhead, so cost differences across methods become less pronounced. Strongest overall performance on aggregate. When aggregating results across datasets, BudgetMem variants consistently rank at the top within each backbone block, indicating strong overall effectiveness rather than gains limited to As shown in Figure 2, we systematically compare the performancecost distributions on LoCoMo across the three tiering axes. As the budget is relaxed (smaller λ), BudgetMem exhibits smooth upward-leftward trend in the costperformance, yielding continuous and controllable trade-off frontier that envelopes prior baselines in both lowand high-cost regimes, achieving higher Judge at comparable cost or lower cost at comparable performance. Notably, implementation and capacity tiering span broader cost range: implementation tiering delivers rapid Judge gains under moderate budgets, while capacity tiering continues to push the frontier outward as the budget increases, attaining the best quality in the high-budget regime. In contrast, reasoning tiering exhibits pronounced cost concentration: because it primarily adjusts inference behaviors (e.g., direct/reflection) while holding the underlying model capacity fixed, the additional cost is dominated by relatively stable token overhead, leading to narrower spread in cost. This pattern suggests that the reasoning axis acts more like fine-grained quality knob within limited cost bandwidth, providing meaningful improvements at similar cost, but offering less room for exploring extremely low-budget settings or extrapolating to very high-budget performance than implementation/capacity tiering. Overall, BudgetMem consistently advances the Pareto frontier from budget-constrained to performance-first regimes, and the differences among tiering axes further indicate that implementation/capacity tiering is better suited for widening budget coverage and extrapolating the boundary, whereas reasoning tiering is most effective for refining quality within relatively concentrated cost region. 7 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory 6.3. Ablating Reward-scale Alignment As shown in Figure 3, we ablate the reward-scale alignment (Sec. 4.3) under the capacity tiering strategy on LoCoMo. Because the task reward and cost reward can differ markedly in scale and variability, removing alignment makes optimization unstable and can bias learning toward the cost term, yielding degenerate low-cost policy. Empirically, without reward-scale alignment (and with cost weight λ=0.3), the router collapses to selecting the LOW tier for most modules, substantially reducing answer quality and driving the Judge score to the lowest level. In contrast, with reward-scale alignment enabled, the router learns more graded use of tiers, producing much smoother and betterbehaved performancecost frontier. This ablation highlights that reward-scale alignment is important for balancing learning signals, enabling meaningful costperformance control rather than trivial cost minimization. Figure 3. Ablation of reward-scale alignment under capacity tiering strategy on LoCoMo. 6.4. Discussion Budget tier selection ratio. Figure 4 analyzes modulelevel routing behavior on LongMemEval using the capacity tiering strategy by reporting the selection ratios of LOW/MID/HIGH tiers under different cost weights λ. Overall, the router exhibits clear and interpretable budget response: as cost pressure increases, it systematically shifts probability mass from higher-cost tiers to cheaper ones, providing module-level evidence that BudgetMem allocates computation in cost-aware manner. More concretely, with small λ (e.g., 0.1), the router assigns most modules to MID, prioritizing quality. At λ = 0.3, it increases the share of LOW while maintaining substantial MID fraction, reflecting controlled reduction in compute. Under larger λ, the policy further concentrates on LOW across modules to meet stricter cost preferences. Overall, these trends confirm that BudgetMem modulates tier usage in predictable way as the budget preference changes. Retrieval-size sensitivity analysis. Figure 5 shows cost and Judge score as we vary the number of retrieved raw Figure 4. Budget-tier selection ratios. Module-wise LOW/MID/HIGH routing ratios on LongMemEval under varying cost weights λ using the capacity tiering strategy. chunks on LoCoMo, evaluated under all three tiering strategies. Increasing the retrieval size predictably raises cost due to longer inputs and additional processing, and it often improves Judge score by providing more supporting evidence, reflecting the standard trade-off between evidence coverage and computational overhead. However, the benefit is not monotonic. In our setting, retrieving 5 chunks provides the best balance between cost and quality. Retrieving too many chunks can degrade performance: additional chunks introduce more redundant or weakly relevant content, increasing noise and potentially distracting the LLM, which lowers Judge score. Conversely, retrieving too few chunks provides insufficient evidence and limits downstream gains. Overall, these results highlight retrieval size as an important practical knob for balancing evidence sufficiency against context noise and cost. Figure 5. Retrieval-size sensitivity on LoCoMo. Cost and Judge versus the number of retrieved raw chunks, evaluated under all three tiering strategies. 7. Conclusion We present BudgetMem, runtime agent memory framework for explicit performancecost control in on-demand memory extraction. BudgetMem equips each module in modular memory pipeline with LOW/MID/HIGH budget tiers and learns lightweight router to select tiers under cost-aware objective. Using BudgetMem as Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory unified testbed, we compare three complementary tiering strategiesimplementation, reasoning, and capacityand characterize their trade-off behaviors across budgets. Experiments on LoCoMo, LongMemEval, and HotpotQA show strong performance in performance-first settings and improved performancecost frontiers under tighter budgets, together with insights on when each strategy provides the best returns."
        },
        {
            "title": "Acknowledgements",
            "content": "This research/project is supported by the NTU Start-Up Grant (#023284-00001), Singapore, the MOE AcRF Tier 1 Seed Grant (RS37/24, #025041-00001), Singapore, and the Youth S&T Talent Support Programme of GDSTA (SKXRC2025462)."
        },
        {
            "title": "Impact Statement",
            "content": "BudgetMem targets practical deployment challenges for memory-augmented agents by making memory computation explicitly controllable under budget constraints. This can benefit applications that require predictable latency or cost, and can lower barriers to using agent memory in resourcelimited settings. BudgetMem is an architectural and training framework and does not introduce new data sources beyond what memoryaugmented agents already use. Potential risks are therefore similar to existing memory systems, such as occasional retrieval of irrelevant context or exposure of benign but unintended historical details. In practice, standard safeguards e.g., sensible retention policies, basic access control, and routine evaluation of failure casescan mitigate these issues. Overall, we expect BudgetMems primary impact to be improving the usability and controllability of agent memory under real-world budget constraints."
        },
        {
            "title": "References",
            "content": "Agarwal, R., Vieillard, N., Zhou, Y., Stanczyk, P., Garea, S. R., Geist, M., and Bachem, O. On-policy distillation of language models: Learning from self-generated mistakes. In The twelfth international conference on learning representations, 2024. Anthropic. Claude code. https://github.com/ anthropics/claude-code, 2025. GitHub repository. Cai, T., Li, Y., Geng, Z., Peng, H., Lee, J. D., Chen, D., and Dao, T. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. Chen, S., Jiang, W., Lin, B., Kwok, J., and Zhang, Y. Routerdc: Query-based router by dual contrastive learning for assembling large language models. Advances in Neural Information Processing Systems, 37:6630566328, 2024. Chhikara, P., Khant, D., Aryan, S., Singh, T., and Yadav, D. Mem0: Building production-ready ai agents with scalable long-term memory. arXiv preprint arXiv:2504.19413, 2025. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 conference of the North American chapter of the association for computational linguistics: human language technologies, volume 1 (long and short papers), pp. 41714186, 2019. Fang, J., Deng, X., Xu, H., Jiang, Z., Tang, Y., Xu, Z., Deng, S., Yao, Y., Wang, M., Qiao, S., et al. Lightmem: Lightweight and efficient memory-augmented generation. arXiv preprint arXiv:2510.18866, 2025. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity. Journal of Machine Learning Research, 23(120):139, 2022. Feng, T., Shen, Y., and You, J. Graphrouter: graph-based router for llm selections. arXiv preprint arXiv:2410.03834, 2024. Frantar, E. and Alistarh, D. Sparsegpt: Massive language models can be accurately pruned in one-shot. In International conference on machine learning, pp. 1032310337. PMLR, 2023. Fu, Y., Bailis, P., Stoica, I., and Zhang, H. Break the sequential dependency of llm inference using lookahead decoding. arXiv preprint arXiv:2402.02057, 2024. Ge, S., Zhang, Y., Liu, L., Zhang, M., Han, J., and Gao, J. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. Grattafiori, A., Dubey, A., Jauhri, A., Pandey, A., Kadian, A., Al-Dahle, A., Letman, A., Mathur, A., Schelten, A., Vaughan, A., et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Gu, Y., Dong, L., Wei, F., and Huang, M. Minillm: Knowledge distillation of large language models. arXiv preprint arXiv:2306.08543, 2023. Hu, Y., Liu, S., Yue, Y., Zhang, G., Liu, B., Zhu, F., Lin, J., Guo, H., Dou, S., Xi, Z., et al. Memory in the age of ai agents. arXiv preprint arXiv:2512.13564, 2025. Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Izacard, G., Caron, M., Hosseini, L., Riedel, S., Bojanowski, P., Joulin, A., and Grave, E. Unsupervised dense information retrieval with contrastive learning. arXiv preprint arXiv:2112.09118, 2021. Jin, B., Collins, T., Yu, D., Cemri, M., Zhang, S., Li, M., Tang, J., Qin, T., Xu, Z., Lu, J., et al. Controlling performance and budget of centralized multi-agent llm system with reinforcement learning. arXiv preprint arXiv:2511.02755, 2025. Kang, J., Ji, M., Zhao, Z., and Bai, T. Memory os of ai agent. arXiv preprint arXiv:2506.06326, 2025. LangChain. https://github.com/ langchain-ai/langmem, 2025. GitHub repository. Langmem. Lee, K.-H., Chen, X., Furuta, H., Canny, J., and Fischer, I. human-inspired reading agent with gist memory of very long contexts. arXiv preprint arXiv:2402.09727, 2024. Li, Y., Huang, Y., Yang, B., Venkitesh, B., Locatelli, A., Ye, H., Cai, T., Lewis, P., and Chen, D. Snapkv: Llm knows what you are looking for before generation. Advances in Neural Information Processing Systems, 37:22947 22970, 2024a. Li, Y., Wei, F., Zhang, C., and Zhang, H. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024b. Liu, Z., Oguz, B., Zhao, C., Chang, E., Stock, P., Mehdad, Y., Shi, Y., Krishnamoorthi, R., and Chandra, V. Llm-qat: Data-free quantization aware training for large language models. In Findings of the Association for Computational Linguistics: ACL 2024, pp. 467484, 2024. Ma, X., Fang, G., and Wang, X. Llm-pruner: On the structural pruning of large language models. Advances in neural information processing systems, 36:2170221720, 2023. Madaan, A., Tandon, N., Gupta, P., Hallinan, S., Gao, L., Wiegreffe, S., Alon, U., Dziri, N., Prabhumoye, S., Yang, Y., et al. Self-refine: Iterative refinement with selffeedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. Maharana, A., Lee, D.-H., Tulyakov, S., Bansal, M., Barbieri, F., and Fang, Y. Evaluating very long-term conversational memory of llm agents. arXiv preprint arXiv:2402.17753, 2024. OpenAI. Codex. https://github.com/openai/ codex, 2025. GitHub repository. Schulman, J., Wolski, F., Dhariwal, P., Radford, A., and Klimov, O. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. Schuster, T., Fisch, A., Gupta, J., Dehghani, M., Bahri, D., Tran, V., Tay, Y., and Metzler, D. Confident adaptive language modeling. Advances in Neural Information Processing Systems, 35:1745617472, 2022. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q., Hinton, G., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. Shinn, N., Cassano, F., Gopinath, A., Narasimhan, K., and Yao, S. Reflexion: Language agents with verbal reinforcement learning. Advances in Neural Information Processing Systems, 36:86348652, 2023. Singh, A., Fry, A., Perelman, A., Tart, A., Ganesh, A., El-Kishky, A., McLaughlin, A., Low, A., Ostrow, A., Ananthram, A., et al. Openai gpt-5 system card. arXiv preprint arXiv:2601.03267, 2025. Sun, M., Liu, Z., Bair, A., and Kolter, J. Z. simple and effective pruning approach for large language models. arXiv preprint arXiv:2306.11695, 2023. Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., Narang, S., Chowdhery, A., and Zhou, D. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171, 2022. Wang, Y., Takanobu, R., Liang, Z., Mao, Y., Hu, Y., McAuley, J., and Wu, X. Mem-{alpha}: Learning memory construction via reinforcement learning. arXiv preprint arXiv:2509.25911, 2025. Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., Le, Q. V., Zhou, D., et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Wu, D., Wang, H., Yu, W., Zhang, Y., Chang, K.-W., and Yu, D. Longmemeval: Benchmarking chat assistants on long-term interactive memory. arXiv preprint arXiv:2410.10813, 2024. Xiao, G., Lin, J., Seznec, M., Wu, H., Demouth, J., and Han, S. Smoothquant: Accurate and efficient post-training In Internaquantization for large language models. tional conference on machine learning, pp. 3808738099. PMLR, 2023a. Packer, C., Fang, V., Patil, S., Lin, K., Wooders, S., and Gonzalez, J. Memgpt: Towards llms as operating systems. 2023. Xiao, G., Tian, Y., Chen, B., Han, S., and Lewis, M. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023b. 10 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Xu, W., Liang, Z., Mei, K., Gao, H., Tan, J., and Zhang, Y. A-mem: Agentic memory for llm agents. arXiv preprint arXiv:2502.12110, 2025. Yan, B., Li, C., Qian, H., Lu, S., and Liu, Z. General agentic memory via deep research. arXiv preprint arXiv:2511.18423, 2025a. Yan, S., Yang, X., Huang, Z., Nie, E., Ding, Z., Li, Z., Ma, X., Kersting, K., Pan, J. Z., Schutze, H., et al. Memoryr1: Enhancing large language model agents to manage and utilize memories via reinforcement learning. arXiv preprint arXiv:2508.19828, 2025b. Yang, A., Li, A., Yang, B., Zhang, B., Hui, B., Zheng, B., Yu, B., Gao, C., Huang, C., Lv, C., et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Yang, Z., Qi, P., Zhang, S., Bengio, Y., Cohen, W., Salakhutdinov, R., and Manning, C. D. Hotpotqa: dataset for diverse, explainable multi-hop question answering. In Proceedings of the 2018 conference on empirical methods in natural language processing, pp. 23692380, 2018. Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan, K. R., and Cao, Y. React: Synergizing reasoning and acting in language models. In The eleventh international conference on learning representations, 2022. Yu, H., Chen, T., Feng, J., Chen, J., Dai, W., Yu, Q., Zhang, Y.-Q., Ma, W.-Y., Liu, J., Wang, M., et al. Memagent: Reshaping long-context llm with multi-conv rl-based memory agent. arXiv preprint arXiv:2507.02259, 2025. Zhang, H., Feng, T., and You, J. Router-r1: Teaching llms multi-round routing and aggregation via reinforcement learning. In The Thirty-ninth Annual Conference on Neural Information Processing Systems, 2025. Zhang, Z., Sheng, Y., Zhou, T., Chen, T., Zheng, L., Cai, R., Song, Z., Tian, Y., Re, C., Barrett, C., et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. Zhong, W., Guo, L., Gao, Q., Ye, H., and Wang, Y. Memorybank: Enhancing large language models with long-term memory. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 38, pp. 1972419731, 2024. 11 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory A. More Implementation Details A.1. Dataset Statistics Table 2 summarizes the dataset statistics used in our experiments, including the train/validation/test split sizes (the number of training queries) and the average context length (in tokens) for LoCoMo (Maharana et al., 2024), LongMemEval (Wu et al., 2024), and HotpotQA (Yang et al., 2018). Note that for HotpotQA, we follow the evaluation protocol and dataset construction in prior work (Yu et al., 2025): we concatenate evidence and distractor documents to form long context, and use the same preprocessed test queries released in that setup. Table 2. Dataset statistics. Dataset #Train/Val/Test Avg. context len. LoCoMo (Maharana et al., 2024) LongMemEval (Wu et al., 2024) HotpotQA (Yang et al., 2018; Yu et al., 2025) 1,236/446/304 282/94/94 5,250/1,750/ 18.0K tokens 122.1K tokens 26.0K tokens A.2. Evaluation Details Tables 3 and 4 report the evaluation configuration. Table 3 lists hyperparameters shared across all datasets. Table 4 further specifies dataset-dependent evaluation settings. Table 3. Hyperparameter Settings (shared across all datasets)"
        },
        {
            "title": "Hyperparameter Value",
            "content": "LLM Decoding Temperature Embedding Dim (query/memory/desc) LLM-Judge Model Capacity Tiering Module Tier (LOW/MID/HIGH for Llama) Capacity Tiering Module Tier (LOW/MID/HIGH for Qwen) 32 5 Batch Size Retrieval Top-K 0.0 768 openai/gpt-oss-120b L: meta/llama-3.2-3b-instruct; M: meta/llama-3.1-8b-instruct; H: meta/llama-3.3-70b-instruct L: qwen/qwen2.5-7b-instruct; M: qwen/qwq-32b; H: qwen/qwen3-next-80b-a3b-instruct Table 4. Evaluation hyperparameters (dataset-specific)."
        },
        {
            "title": "LoCoMo LongMemEval HotpotQA",
            "content": "Chunking / chunk size Max Tokens for LLM Model 256 32 256 512 1024 512 A.3. Details on PPO Training Objectives We train the shared budget-tier router policy πθ with Proximal Policy Optimization (PPO) to make query-aware, module-wise tier selections along the fixed modular runtime memory pipeline: Mfil {Ment, Mtmp, Mtop} Msum. Episode, actions, and states. single query constitutes one PPO episode: given (q, Cq), the router sequentially selects budget tier for each module invocation, executes the routed pipeline to obtain extracted memory m, and then produces the final answer ˆy = fans(q, m). At each routing step k, the action ak chooses the module tier. We use 3-way action space per module {LOW, MID, HIGH}. The state sk is compact embedding of (i) the query q, (ii) the current module input (i.e., the intermediate output from the previous stage), and (iii) module descriptor that indicates which module is being routed. We use single unified actorcritic network shared across modules; the module identity is injected via the module-descriptor embedding, enabling parameter sharing while retaining module-specific routing behavior. 12 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Reward: performancecost trade-off with normalization and alignment. After finishing all routed module executions for query, we compute task reward rtask [0, 1] from answer quality (F1 or LLM-judge), and cost reward rcost from the extraction cost incurred by routed module calls. We optimize the cost-aware scalar reward as follows: = rtask + λ α rcost. (11) Where. rtask [0, 1] is the task reward computed from answer quality (F1 or LLM-as-a-judge); rcost is the (normalized) cost reward derived from the extraction cost incurred by routed module calls; λ controls the performancecost trade-off; and α is the reward-scale alignment factor. We compute rcost by applying sliding-window normalization to raw costs and mapping them to bounded reward (Eq. 9), and compute α via variance-based reward-scale alignment (Eq. 10). PPO objective with joint (multi-module) likelihood ratio. Let log πθ(ak sk) be the per-module log-probability under the current policy. Because each query episode yields sequence of module actions, we use the joint action probability over the routed trajectory: log πθ(a s) = (cid:88) log πθ(ak sk), (cid:16) ρ = exp log πnew θ log πold θ (cid:17) . (12) Where. At routing step k, sk is the router state embedding and ak {LOW, MID, HIGH} is the chosen budget-tier action for the current module; πθ() is the router policy parameterized by θ and log πθ(ak sk) is the corresponding log-probability; = {sk}k and = {ak}k denote the full stateaction sequence over query episode, with joint log-likelihood log πθ(a s); and ρ is the PPO likelihood ratio between the updated policy (superscript new) and the behavior policy used to collect rollouts. We then apply the standard PPO clipped surrogate objective with ratio ρ, together with value-function loss and an entropy bonus: = Lpolicy + cv Lvalue ce H. (13) Where. Lpolicy is the PPO clipped surrogate policy loss; Lvalue is the value-function regression loss; is the policy entropy; and cv and ce are the coefficients for the value loss and entropy bonus, respectively. We estimate advantages with one-step Monte-Carlo return (the episode terminates after the routed pipeline completes), and use the critic as baseline. For stability, we average entropy across module decisions (rather than summing), preventing the entropy term from scaling with the number of routed modules. Optimization and hyperparameters. We use PPO as the default RL optimizer for router training. Unless otherwise specified, we use Adam with learning rate 3 104, clip ϵ = 0.2, value-loss coefficient cv = 0.5, entropy coefficient ce = 0.01, 4 PPO epochs per update, and gradient-norm clipping at 0.5. We train with batch size 32 and up to 600 training steps. The router operates on 768-d embeddings (query/memory/descriptor), consistent with our shared hyperparameter settings. A.4. Module Implementation Details Unified budget-tier interface. We follow the unified budget-tier interface: each module exposes three tiers (LOW/MID/HIGH). Budget tiers only alter the modules internal compute and inferential strength, enabling modulewise routing without changing the overall pipeline structure. To control for prompt effects across tiers, we standardize prompting by using each modules HIGH-tier prompt template consistently for LOW/MID/HIGH. A.4.1. TIERING STRATEGY: IMPLEMENTATION TIERING Budgeted tiers with fixed contract. Implementation tiering realizes LOW/MID/HIGH by increasing the internal compute budget of module while keeping its inputoutput contract unchanged. textscLow uses cheap symbolic pipelines (rules, regex, lightweight NLP such as spaCy) for fast and controllable processing; MID upgrades to compact learned specialist (typically BERT-style encoders (Devlin et al., 2019)) that improves semantic modeling at moderate cost; HIGH further upgrades to LLM-based processing for the strongest open-ended reasoning and cross-context integration, at the highest cost. 13 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Filter Module. The filter module realizes budget tiering through monotonic upgrade of its internal relevance estimator, moving from inexpensive symbolic signals to semantic representations and finally to LLM-based evaluation. At LOW, it relies on sparse lexical/pattern matching with lightweight heuristic scoring, which is fast and deterministic and thus suitable for aggressive cost control. At MID, it introduces representation-based relevance (e.g., embeddingand BERT-style similarity), so that semantic matches beyond exact overlap can be recovered under moderate compute budget. At HIGH, it further upgrades to LLM-based scoring over candidates, thereby enabling context-sensitive discrimination when relevance depends on implicit references, paraphrases, or multi-hop semantics; importantly, across tiers the module keeps the same output contract (a ranked top-k candidate set), so improvements can be attributed to increased internal compute rather than interface changes. Entity Module. The entity relation module follows the same budget ladder, but instantiates it for structured relational cue extraction. At LOW, it uses lightweight rules and shallow NLP cues (e.g., regex patterns and spaCy NER/dependency signals) to extract explicit relational traces with high controllability, although coverage is limited when relations are implicit or span multiple chunks. At MID, it replaces heuristics with compact task-specific extractortypically BERT-style relation extraction (e.g., REBEL-like triplet generation)which improves normalization and coverage of subjectrelationobject tuples under moderate budget. At HIGH, it employs an LLM to perform open-domain relation completion and abstraction, so that missing links can be recovered and paraphrased relations can be consolidated into consistent tuples across chunks; nevertheless, the module continues to emit standardized relation list, ensuring that downstream fusion remains unchanged. Temporal Module. The temporal relation module is tiered as explicit cue detection learned temporal extraction generative temporal inference, aligning budget increases with progressively stronger temporal modeling. At LOW, it identifies explicit time expressions and constructs eventtime anchors using lightweight NLP, which is reliable when temporal cues are directly stated. At MID, it adopts compact learned extractor (e.g., BERT-style) to select temporally relevant relations and normalize them into consistent form. At HIGH, it upgrades to LLM-based temporal reasoning, enabling implicit ordering, cross-chunk alignment, and global consistency checking when the timeline must be inferred rather than read off; across all tiers, the output contract is preserved as standardized set of temporal facts/anchors. Topic Module. The topic module implements budget tiering by progressively strengthening discourse-level abstraction from cheap lexical cues to learned representations and finally LLM-based topic reasoning. At LOW, it extracts controllable topical signals from inexpensive lexical/statistical cues (e.g., keywords, TF-IDF-like patterns, and simple transition markers), providing low-cost but coarse description of topical content and shifts. At MID, it incorporates compact learned representations (e.g., BERT-style encoders and lightweight segment/topic modeling optionally assisted by NLP structure), which yields more stable topical units and more reliable transition detection under moderate compute. At HIGH, it uses an LLM to produce higher-level topic relations and evolution narratives, improving abstraction and interpretability for long or heterogeneous contexts; critically, each tier still outputs the same standardized topic-cue list so that downstream summarization can consume it uniformly. Summary Module. The summary module realizes budget tiering by varying the depth of evidence fusion while keeping the final memory format unchanged. At LOW, it produces deterministic evidence digest (e.g., concise aggregation/concatenation of retrieved memories and cues), minimizing compute and distortion. At MID, it outputs structured synthesis that organizes key evidence, entities, temporal anchors, and topic cues into parseable template, thereby improving usability and reducing brittleness without invoking expensive generative reasoning. At HIGH, it upgrades to LLM-based integrative summarization, where the model fuses multi-source cues, resolves conflicts, and generates an explanatory final memory representation that better supports complex queries; across tiers, the summary remains contract-compatible, so tier effects reflect the allocated compute budget rather than changes in downstream expectations. A.4.2. TIERING STRATEGY: INFERENCE BEHAVIOUR Budgeted inference via behaviourally tiered reasoning. We realize reasoning tiering by allocating different compute budgets to module through progressively stronger inference behaviours, while keeping the module interface fixed. Specifically, LOW executes direct inference; MID executes CoT-style inference; and HIGH executes multi-step/reflectionstyle inference that emphasizes iterative refinement and global consistency. Across tiers, each module consumes the same conditional inputs (the query and candidate memories or intermediate cues) and produces the same standardized artifact (e.g., ranked subset or cue list); therefore, tier differences reflect changes in internal reasoning depth and token footprint rather than changes in pipeline structure. Filter Module. Across tiers, the filter module ranks retrieved chunks by relevance and returns the top-k. LOW performs Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory direct scoring to maximize efficiency and determinism; MID adopts CoT-style scoring to improve semantic calibration beyond surface overlap; and HIGH adopts multi-step/reflection-style scoring to better resolve cases where relevance depends on implicit constraints or cross-memory interactions. Entity Module. The entity module outputs the same relation list format across tiers; tiers only affect relation extraction strength. LOW extracts relations via direct inference and thus prioritizes precision on explicitly stated facts; MID uses CoT-style inference to improve coverage and normalization of relation tuples; and HIGH uses multi-step/reflection-style inference to consolidate paraphrases, reconcile competing candidates, and enforce relation consistency across memories. Temporal Module. The temporal module always outputs the same set of temporal facts/anchors; tiers only change how well it infers and normalizes them. LOW identifies explicit anchors via direct inference; MID uses CoT-style inference to select and normalize query-relevant temporal constraints; and HIGH uses multi-step/reflection-style inference to improve cross-chunk alignment, infer implicit ordering, and promote global timeline coherence. Topic Module. Across tiers, the topic module preserves the topic-cue list format, while producing progressively higher-level topic abstractions.LOW produces coarse topical cues via direct inference; MID uses CoT-style inference to yield more stable topic relations and transitions; and HIGH uses multi-step/reflection-style inference to improve abstraction quality and maintain global coherence over long and heterogeneous contexts. Summary Module. Across tiers, the summary module preserves the final memory interface, while improving evidence aggregation and fusion. LOW performs direct synthesis with minimal inference; MID performs CoT-style synthesis that better organizes entity/temporal/topic cues into coherent structure; and HIGH performs multi-step/reflection-style synthesis to integrate multi-source evidence, reduce inconsistencies, and improve overall faithfulness under complex queries. A.4.3. TIERING STRATEGY: MODULES MODEL CAPACITY For capacity tiering, we operationalize the LOW/MID/HIGH budgets by swapping the backbone LLM used inside the same module, while keeping the module interface and inference behavior fixed. To control for prompt effects across tiers, we further standardize the prompting by using the same HIGH-tier prompt template for each module at all budget levels. In our implementation, tiers correspond to increasing model capacities drawn from widely used open-weight families: LOW instantiates modules with small model (e.g., Llama-3.2-3B-Instruct and qwen/qwen2.5-7b-instruct), MID uses medium model (e.g., Llama-3.1-8B-Instruct and qwen/qwq-32b), and HIGH uses large model (e.g., Llama-3.3-70B-Instruct and qwen3-next-80b-a3b-instruct). Filter Module. The filter module preserves an identical relevance scoring and selection procedure across tiers, and varies only the capacity of the scoring backbone. Given the query and set of candidate memories, it assigns relevance scores and returns ranked top-k subset for downstream extraction. Under LOW/MID/HIGH, this scoring is executed by increasingly larger backbones, which improves semantic calibration and robustness under the same top-k contract. Entity Module. The entity module keeps the same extraction objective and output format across tiers, producing standardized list of entity-centric relations from the filtered evidence. Capacity tiering affects only the model used to instantiate the extractor: larger backbones better recover implicit relations, while the emitted relation list remains interfacecompatible with downstream fusion. Temporal Module. The temporal module follows fixed temporal extraction procedure for all tiers and outputs standardized set of temporal facts/anchors. Capacity tiering changes only the underlying backbone used to infer and normalize temporally relevant constraints from the filtered memories, trading temporal fidelity and consistency against runtime cost without modifying the module contract. Topic Module. The topic module maintains fixed topic-relation extraction function and emits standardized topic-cue list across all tiers. By varying only the backbone capacity, larger models can induce more stable topical structure and discourse transitions from the same evidence, while the interface and downstream consumption remain unchanged. Summary Module. The summary module aggregates the upstream module outputs into final compact memory representation under an identical summarization interface across tiers. Capacity tiering instantiates the summarizer with increasingly larger backbones, improving synthesis robustness and conflict handling under complex evidence, while keeping the final memory format and downstream expectations fixed. 15 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Table 5. LoCoMo (LLaMA-based models). Per-category results in terms of F1-score / Judge score (%) and Cost ($). Method Single-hop (F1/J) Multi-hop (F1/J) Temporal (F1/J) Open-domain (F1/J) Avg. (F1/J) Cost ($) Baselines MemoryBank ReadAgent MemoryOS A-MEM Mem0 LangMem LightMem 31.45/38.44 28.67/36.56 33.77/41.88 33.69/41.56 13.61/34.06 25.90/29.68 39.13/41.90 BudgetMem-IMP (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0.9 43.76/57.81 43.61/55.94 46.55/59.38 23.72/31.25 25.19/31.56 28.76/36.25 25.77/32.81 BudgetMem-REA (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0.9 44.95/57.19 47.55/61.88 46.44/60.00 42.34/57.19 41.61/57.50 40.64/51.88 40.55/53.12 BudgetMem-CAP (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0. 47.55/60.62 45.73/59.06 45.76/64.06 32.35/60.31 40.52/46.88 29.73/39.37 26.78/34.38 19.82/28.26 25.65/42.75 34.91/34.06 27.60/42.03 12.22/32.61 28.22/36.23 33.01/35.25 34.01/42.75 32.98/43.48 34.37/44.93 23.99/32.61 25.47/36.23 26.51/37.68 24.73/36.23 30.32/47.10 32.83/49.28 30.65/43.48 30.16/42.03 24.97/47.10 32.51/47.10 31.82/44.20 34.15/50.72 32.20/45.65 33.26/52.17 40.53/46.38 27.53/39.86 24.32/30.43 21.44/28.99 5.30/9.23 5.55/4.62 20.49/16.15 11.18/6.15 2.79/1.54 6.14/6.15 37.94/42. 36.88/38.46 37.93/40.77 44.57/43.08 14.72/12.31 16.64/13.85 18.09/14.62 22.27/19.23 44.48/44.62 37.07/39.23 44.36/44.62 40.64/40.00 40.65/43.08 41.72/49.23 35.03/39.23 45.19/43.08 44.32/46.15 44.93/43.85 34.46/38.46 30.63/24.62 28.80/13.85 25.11/12.31 12.52/20.00 17.06/32.50 23.50/37.50 13.92/20.00 13.22/52.50 25.67/25.00 40.83/47.17 21.09/55.00 23.13/55.00 26.81/55.00 10.48/30.00 9.52/27.50 9.37/32.50 19.05/37.50 33.77/55.00 32.16/60.00 37.28/65.00 25.15/50.00 22.02/50.00 24.19/47.50 21.46/47. 30.76/57.50 37.64/67.50 39.01/70.00 45.32/60.00 22.25/47.50 22.80/50.00 18.87/47.50 22.27/28.98 22.48/31.05 30.62/34.55 26.43/32.96 11.04/28.18 22.31/25.96 33.88/40.76 38.75/50.32 38.80/50.00 42.21/52.55 21.07/27.55 22.48/28.66 24.82/31.85 24.39/31.05 40.92/52.23 41.16/54.30 41.95/53.50 38.22/49.84 36.51/51.75 38.03/50.00 36.27/47.93 43.05/54.62 41.95/53.98 42.41/57.64 40.79/52.71 34.45/40.76 27.91/32.80 24.76/29.46 0.73 0.57 1.97 2.88 2.89 0.48 1. 1.80 1.75 1.26 0.45 0.48 0.48 0.48 2.90 2.65 2.72 2.33 2.34 2.33 2.32 2.40 2.48 2.48 2.00 0.58 0.28 0.28 B. More Experimental Results For LoCoMo (Maharana et al., 2024), we report the per-category performance in terms of F1-score, Judge score, and Cost. The results are computed separately for each category defined in the dataset, providing fine-grained analysis of model performance across different long-context conversational scenarios. Following prior practice, we exclude Category 5 (adversarial) from our evaluation. To facilitate clear comparison across model families, the results are reported in two separate tables, corresponding to LLaMA-based models and Qwen-based models, respectively. The detailed category-wise results on LoCoMo are summarized in Tables 5 and 6. For LongMemEval (Wu et al., 2024), we present the category-level evaluation results, including F1-score, Judge score, and Cost. Each category is evaluated independently to reflect the models ability to retain, retrieve, and reason over long-term contextual information under different memory-intensive settings. The results are organized into two separate tables for LLaMA-based and Qwen-based models, respectively, enabling direct comparison between the two model families. The complete per-category results for LongMemEval are reported in Tables 7 and 8. For HotpotQA (Yang et al., 2018), we provide the per-category performance measured by F1-score, Judge score, and Cost. The results are organized according to the predefined categories of the dataset, enabling detailed examination of model behavior across different multi-hop reasoning types. For clarity, we report the results of LLaMA-based models and Qwen-based models in separate tables. The full category-wise results on HotpotQA are shown in Tables 9 and 10. 16 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Table 6. LoCoMo (Qwen-based models). Per-category results (F1/Judge, %) and cost ($). Method Single-hop (F1/J) Multi-hop (F1/J) Temporal (F1/J) Open-domain (F1/J) Avg. (F1/J) Cost ($) Baselines MemoryBank 30.50/44.37 22.80/36.23 MemoryOS ReadAgent A-MEM LangMem Mem0 LightMem 6.44/6.15 42.05/45.65 26.59/15.38 42.98/60.00 3.93/3.85 29.04/36.88 23.71/40.58 17.81/9.23 32.02/45.31 26.66/46.38 4.99/6.15 24.49/27.18 24.55/30.43 3.54/3.03 11.52/28.77 15.25/31.08 39.45/52.81 23.08/38.41 23.93/23.08 BudgetMem-IMP (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0.9 46.52/62.90 28.91/46.09 38.39/40.98 45.58/59.29 30.12/43.94 44.09/45.24 46.92/61.08 31.00/44.78 43.87/39.68 30.09/35.94 26.52/36.23 21.25/16.15 27.79/34.06 25.12/34.06 21.37/13.08 25.98/32.81 26.14/35.51 26.23/19.23 26.05/32.19 25.48/32.61 22.62/16.92 BudgetMem-REA (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0.9 45.47/63.44 32.41/45.65 37.57/34.62 44.49/60.94 34.63/50.72 40.52/36.92 45.17/18.24 40.63/11.93 33.14/6.07 44.20/62.81 33.05/46.38 41.70/38.46 43.78/63.44 34.60/47.83 46.00/44.62 44.54/64.06 32.25/45.65 43.29/41.54 46.47/63.44 37.43/50.00 38.96/40.00 BudgetMem-CAP (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0. 46.18/63.12 33.14/46.38 40.37/34.62 44.82/62.50 35.96/47.10 42.45/39.23 45.64/66.25 32.73/44.20 43.34/40.77 46.15/64.38 32.92/45.65 43.50/44.62 45.88/61.25 34.27/45.65 40.92/37.69 44.79/65.00 36.50/46.38 42.38/39.23 45.21/62.19 31.86/43.48 40.73/35.38 25.88/45.00 35.23/42.81 25.56/45.00 28.12/52.50 31.21/25.00 12.19/46.15 27.77/42.50 30.63/55.88 24.22/52.63 31.40/63.89 25.04/55.00 23.74/55.00 25.17/52.50 24.87/55.00 33.31/60.00 30.39/55.00 28.46/5.67 33.41/60.00 26.94/60.00 28.80/47.50 26.84/55.00 32.19/57.50 28.65/55.00 29.54/60.00 29.97/55.00 28.81/50.00 32.66/57.50 34.88/57.50 23.53/34.71 35.43/38.85 22.45/31.37 27.65/38.54 20.89/23.40 10.77/25.32 32.85/42. 40.14/54.38 40.58/52.63 41.89/53.27 27.15/33.12 25.62/31.05 26.01/31.85 25.14/30.57 40.19/53.34 41.00/53.34 40.52/55.25 40.55/53.98 41.15/55.89 40.58/54.30 41.68/55.10 41.22/53.18 41.35/53.82 41.30/55.73 41.66/55.57 41.21/52.23 41.65/55.10 40.69/52.23 0.25 0.75 0.24 2.88 0.14 1.15 0.70 0.80 0.52 0.46 0.07 0.10 0.09 0.09 1.11 0.62 0.61 0.61 0.61 0.61 0. 0.61 0.61 0.61 0.61 0.56 0.56 0.55 C. Prompts C.1. Answer Prompt for the Locomo Dataset"
        },
        {
            "title": "Answer Prompt for the Locomo Dataset",
            "content": "Based on the above context, write an answer in the form of short phrase for the (cid:44) following question. Answer with exact words from the context whenever possible. Question: {} Short answer: 17 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Table 7. LongMemEval (LLaMA-based models). Per-category results (F1/Judge, %) and cost ($). Method SS-User (F1/J) Multi-Session (F1/J) SS-Pref (F1/J) Temporal (F1/J) Know.-Update (F1/J) SS-Assist. (F1/J) Avg. (F1/J) Cost ($) Baselines MemoryBank 22.64/17.86 16.91/12.50 ReadAgent 13.03/24.14 MemoryOS 22.05/17.86 A-MEM 22.31/25.00 Mem0 15.92/10.34 LangMem 74.31/82.14 LightMem BudgetMem-IMP (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0.9 41.52/55.17 22.23/41.38 20.80/43.10 25.07/37.93 18.35/46.55 17.78/43.10 23.29/39. BudgetMem-REA (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0.9 40.51/53.45 38.49/44.83 42.38/48.28 34.90/51.72 35.84/46.55 36.48/48.28 33.66/44.83 BudgetMem-CAP (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0.9 36.78/50.00 36.19/53.45 35.16/41.38 39.59/53.45 29.28/34.48 25.30/31.03 33.77/34.48 35.87/42.86 60.71/67.86 22.88/75.00 67.86/64.29 73.48/82.14 18.75/35.71 8.10/50.00 67.40/71.43 59.74/75.00 64.14/67.86 61.35/75.00 64.11/71.43 59.95/67.86 58.05/67. 73.21/82.14 77.11/82.14 76.79/82.14 77.11/85.71 77.11/85.71 77.98/82.14 76.79/85.71 75.60/82.14 72.01/82.14 69.64/78.57 80.36/85.71 60.97/67.86 59.88/67.86 60.97/60.71 76.33/79.17 11.10/16.67 14.52/8.33 22.35/33.33 16.83/42.86 15.71/33.33 76.89/83.33 14.60/8.33 19.47/41.67 19.89/41.67 13.76/25.00 13.96/25.00 23.12/58.33 19.79/28.57 38.26/89.29 17.99/50.00 25.04/42.86 23.49/66.67 21.98/42.86 22.41/66.67 25.23/42.86 23.38/58.33 25.03/42.86 20.59/50.00 24.45/42.86 22.95/58.83 25.87/42.86 24.92/58.33 45.28/89.29 22.77/50.00 40.46/92.86 24.45/58.33 44.55/89.29 20.55/41.67 30.37/75.00 17.52/25.00 42.88/89.29 18.81/41.67 40.64/89.29 20.70/50.00 34.55/92.86 17.94/33.33 39.73/92.86 19.46/58.33 36.80/82.14 21.65/33.33 42.49/89.29 19.56/50.00 39.59/89.29 18.44/16.67 29.14/67.86 17.58/33.33 36.48/71.43 18.28/41.67 23.56/53.57 18.72/33. 8.11/19.23 2.77/15.38 2.11/6.52 12.44/15.38 4.78/23.08 3.05/8.70 36.99/70.00 28.98/41.30 6.00/8.70 5.35/8.70 5.28/10.87 5.31/8.70 6.03/8.70 5.98/8.70 31.39/36.96 35.54/41.30 23.65/36.96 25.87/39.13 32.30/30.43 32.91/39.13 20.66/32.61 34.38/43.48 30.71/47.83 24.54/30.43 32.31/41.30 14.07/17.39 14.67/21.74 14.98/17.39 3.94 24.75/43.33 26.74/32.67 26.14/53.33 20.75/27.72 13.68 15.77/46.43 12.97/33.50 38.83 23.93/26.67 21.74/33.17 80.02 44.46/70.00 27.70/42.08 13.57 12.00/17.00 16.60 9.23/14.29 5.28 16.88/20.83 26.74/48.51 20.67/35.71 37.47/56.00 19.44/42.86 23.83/40.50 14.26/39.29 22.66/39.50 21.21/46.43 24.98/40.00 21.19/50.00 23.22/41.50 19.53/53.57 22.47/41.00 18.99/46.43 24.03/39. 25.75/50.00 40.53/58.00 28.79/42.86 41.29/56.50 17.10/42.86 38.34/55.00 24.75/46.43 35.63/54.50 30.46/42.86 40.01/53.50 26.24/39.29 39.67/55.50 27.95/46.43 35.09/54.00 31.08/57.14 40.24/60.50 29.31/39.29 38.19/57.00 7.84/21.43 33.81/48.50 38.80/46.43 42.25/57.00 11.36/25.00 26.98/38.50 10.62/17.86 26.79/38.50 17.29/32.14 28.62/36.50 0.71 0.54 0.58 0.58 0.58 0.58 0.58 0.67 0.66 0.62 0.67 0.68 0.68 0.61 0.80 0.63 0.13 0.10 0.10 0.10 0.10 C.2. Answer Prompt for the HotpotQA Dataset Answer Prompt for the HotpotQA Dataset Based on the following context, answer the question. The question may require (cid:44) reasoning across multiple pieces of information. {context} Question: {question} Instructions: - Read the context carefully and identify relevant information - If the answer can be found in the context, provide short, precise answer - Output your answer within <answer></answer> tags <answer>your answer here</answer> 18 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Table 8. LongMemEval (Qwen-based models). Per-category results (F1/Judge, %) and cost ($). Method SS-User (F1/J) Multi-Session (F1/J) SS-Pref (F1/J) Temporal (F1/J) Know.-Update (F1/J) SS-Assist. (F1/J) Avg. (F1/J) Cost ($) Baselines MemoryBank 10.41/12.50 13.45/27.59 MemoryOS 13.65/14.29 ReadAgent 14.66/7.14 A-MEM 16.68/10.34 LangMem 17.93/14.29 Mem0 65.70/85.71 LightMem BudgetMem-IMP (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0.9 32.47/32.76 29.02/44.83 12.73/37.93 12.54/49.29 12.16/43.10 11.07/39.66 11.39/44.83 BudgetMem-REA (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0.9 40.99/48.28 40.67/41.38 35.64/34.48 37.32/37.93 38.58/41.38 33.56/37.93 40.91/44. BudgetMem-CAP (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0.9 30.47/34.48 38.31/41.38 31.56/34.48 36.57/41.38 29.30/29.31 34.46/36.21 33.79/34.48 5.08/14.29 25.24/75.00 41.39/64.29 46.81/53.57 10.43/25.00 78.84/85.71 17.84/50.00 43.92/85.71 53.92/85.71 33.08/85.71 25.26/75.00 24.85/78.57 22.57/75.00 22.44/78.57 63.24/85.71 49.64/85.71 49.31/85.71 47.10/85.71 57.88/82.14 53.60/85.71 54.31/85.71 49.77/85.71 58.60/85.71 48.16/85.71 54.31/85.71 51.08/92.86 49.14/85.71 56.25/89. 15.39/33.33 17.59/42.86 8.83/8.33 65.14/79.17 13.58/28.57 21.06/45.83 13.81/66.66 42.07/92.86 31.27/92.86 23.06/60.71 9.20/57.14 11.79/46.43 8.40/53.57 12.63/64.29 38.56/89.29 43.22/82.14 37.05/75.00 43.43/85.71 47.13/85.71 38.49/89.29 34.74/85.71 14.54/75.00 13.47/25.00 11.93/58.33 15.18/16.67 13.64/33.33 20.66/41.67 18.49/12.50 17.08/66.67 20.27/50.00 15.20/66.67 15.64/58.33 13.42/50.00 15.33/75.00 14.38/58.33 17.38/66.67 16.30/66.67 16.03/58.33 16.77/66.67 16.96/66.67 16.60/41.67 15.68/66. 16.55/58.33 43.91/92.86 17.89/66.67 40.21/82.14 16.16/66.67 44.08/92.86 46.93/85.71 17.38/66.67 43.26/100.00 14.58/33.33 12.89/50.00 42.23/92.86 13.38/50.00 44.92/92.86 9.36/23.08 2.57/4.35 2.75/15.38 3.60/19.23 4.14/0.00 4.95/18.31 36.26/80.00 11.49/26.09 21.50/34.78 6.74/17.39 2.20/8.70 2.19/10.87 2.27/8.70 2.15/8.70 20.89/43.48 13.64/39.13 20.87/47.83 14.92/39.13 16.52/43.48 21.91/47.83 24.34/43.48 16.55/47.83 20.49/47.83 23.01/47.83 17.30/43.48 12.96/28.26 14.38/32.61 16.89/32.61 3.45 12.58/56.67 10.56/28.22 14.64/42.86 13.35/33.00 15.84 12.49/46.67 27.72/20.75 4.97 13.62/33.33 10.82/31.19 21.00 3.99 7.44/10.71 11.01/14.00 4.96 32.34/53.33 25.71/36.14 3.39 29.16/22.50 27.70/47. 29.01/53.57 29.18/52.00 21.62/46.43 29.53/55.50 6.32/46.43 14.90/46.00 13.45/42.86 11.41/44.00 13.40/46.43 11.84/42.00 13.76/42.86 10.91/42.00 14.02/42.86 11.39/44.50 27.56/46.43 35.84/59.00 26.16/42.86 32.57/54.50 31.23/50.00 32.56/54.00 26.22/53.57 31.60/55.50 35.06/60.71 35.62/58.00 24.12/46.43 32.04/55.50 26.05/50.00 34.52/58.00 22.86/46.43 32.01/56.00 26.98/46.43 34.51/57.00 27.61/53.57 32.19/57.50 27.76/46.43 33.69/56.50 16.15/46.43 27.82/50.50 11.06/42.86 28.42/52.00 14.54/46.43 30.68/52.50 0.30 0.41 0.13 0.15 0.15 0.15 0.15 0.26 0.17 0.17 0.17 0.17 0.17 0.17 0.17 0.17 0.17 0.17 0.22 0.17 0. C.3. Answer Prompt for the LongMemEval Dataset"
        },
        {
            "title": "Answer Prompt for the LongMemEval Dataset",
            "content": "I will give you several history chats between you and user. Please answer the question based on the relevant chat history. History Chats:{} Current Date: {} Question: {} Short Answer: 19 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Table 9. HotpotQA (LLaMA-based models). Overall results (F1/Judge, %) and cost ($). Method Overall (F1/J) Cost ($) Baselines MemoryBank ReadAgent MemoryOS A-MEM Mem0 LangMem LightMem 22.25/23.75 15.33/30.08 34.50/43.36 43.25/54.69 28.03/36.72 22.78/22.66 45.73/58.37 BudgetMem-IMP (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0. 49.31/65.77 51.40/63.28 48.49/60.94 40.95/45.70 37.40/44.14 37.52/43.36 39.01/44.53 BudgetMem-REA (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0.9 51.12/61.93 51.23/63.55 53.93/66.36 57.17/68.40 56.86/66.07 56.66/68.40 55.04/66.20 BudgetMem-CAP (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0.9 53.87/64.85 38.62/50.00 31.71/36.27 24.65/32.57 22.16/28.90 30.92/37.62 35.29/36.57 7.75 4.19 13.32 26.74 4.30 10.95 10. 1.35 1.23 1.15 1.14 1.37 1.37 1.37 0.99 0.95 0.77 0.79 0.82 0.77 0.80 0.93 0.84 0.10 0.11 0.11 0.10 0.10 20 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Table 10. HotpotQA (Qwen-based models). Overall results (F1/Judge, %) and cost ($). Method Overall (F1/J) Cost ($) Baselines MemoryBank 18.64/31.25 MemoryOS ReadAgent A-MEM LangMem Mem0 LightMem 1.79 41.21/53.52 11.68 1.75 18.05/25.78 8.34 40.54/50.39 4.32 20.77/21.09 2.02 24.72/37.89 8.56 41.29/55.42 BudgetMem-IMP (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0. 46.67/57.42 32.33/42.58 34.01/42.97 33.51/44.14 30.17/41.02 31.53/42.97 32.26/42.58 BudgetMem-REA (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0.9 57.67/70.83 50.74/58.50 60.26/69.89 57.04/68.75 59.28/69.79 58.79/70.00 59.96/71.43 BudgetMem-CAP (COE) COE=0 0.05 0.1 0.3 0.5 0.7 0.9 58.70/72.08 59.57/71.91 60.03/71.51 59.61/71.98 64.24/74.75 58.63/69.41 59.90/71.05 0.63 0.26 0.26 0.30 0.30 0.30 0. 0.17 0.32 0.18 0.19 0.19 0.19 0.12 0.22 0.18 0.19 0.18 0.20 0.17 0.15 21 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory C.4. LLM JUDGE GENERAL PROMPT"
        },
        {
            "title": "LLM JUDGE GENERAL PROMPT",
            "content": "You are an expert judge evaluating the quality of an answer for QA task. Your goal is to determine whether the model's answer correctly and sufficiently answers the given question. Read the following information carefully: [Question] {question} [Ground Truth Answers] {ground_truth} [Model Answer] {model_answer} Your evaluation criteria: 1. Correctness: - Is the model answer factually consistent with ANY of the correct answers? - Does it avoid contradictions or introducing false information? 2. Relevance: - Does the answer address the question directly without unnecessary content? 3. Completeness: - Does the answer include all essential information needed to fully answer the (cid:44) - Partial answers are allowed but should receive lower scores. question? Scoring Rules: - Score = 1.0 if the answer is fully correct. - Score = 0.5 if the answer is partially correct but incomplete or slightly (cid:44) - Score = 0.0 if the answer is incorrect, irrelevant, or contradicts the ground (cid:44) inaccurate. truth. Output Format (STRICT): Return your output as JSON dictionary with two fields: {{ \"explanation\": \"<brief explanation of your reasoning>\", \"score\": <0.0 0.5 1.0> }} Be concise and objective. Do not include anything outside the JSON. 22 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory C.5. FILTER MODULE PROMPT FILTER MODULE PROMPT (Low) **Role:** You are relevance scoring system. **Task:** Given query and an ordered list of memories, output score for each memory indicating how directly and usefully it helps answer the (cid:44) query. (cid:44) **Scoring Rules:** - 10: directly answers the query or provides essential constraints/info to answer it (cid:44) - 7{9: clearly relevant and helpful, but not fully sufficient alone - 4{6: partially relevant; some usefulness but missing key connection - 1{3: weak/tangential relevance; mostly not useful - 0: completely irrelevant **Input Format:** The input consists of: 1. `<query>` section containing the user's question. 2. `<memories>` section containing individual `<memory>` elements. Each memory is formatted as: ``` <memory index=\"N\" [date_time=\"...\" session_id=\"...\" dia_id=\"...\"]> memory content text </memory> ``` Where: - `index` is the memory's position in the list. - `date_time`, `session_id`, `dia_id` are optional metadata attributes. - The text between the tags is the memory content to be scored. **Output Format:** Your entire response must be **only** the following line: `<answer>[s0, s1, s2, ...]</answer>` - The array must contain exactly one integer score per memory, in the same order as the input indices. (cid:44) - If there are no memories, output: `<answer>[]</answer>` **Input:** <query>{query}</query> <memories>{memories_text}</memories> 23 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory Filter Module Prompt (Mid) **Role:** You are relevance scoring system. **Task:** Given query and an ordered list of memories, output score for each memory indicating how directly and usefully it helps answer the (cid:44) query. (cid:44) **Scoring Rules:** - 10: directly answers the query or provides essential constraints/info to answer it (cid:44) - 7{9: clearly relevant and helpful, but not fully sufficient alone - 4{6: partially relevant; some usefulness but missing key connection - 1{3: weak/tangential relevance; mostly not useful - 0: completely irrelevant **Input Format:** The input consists of: 1. `<query>` section containing the user's question. 2. `<memories>` section containing individual `<memory>` elements. Each memory is formatted as: ``` <memory index=\"N\" [date_time=\"...\" session_id=\"...\" dia_id=\"...\"]> memory content text </memory> ``` Where: - `index` is the memory's position in the list. - `date_time`, `session_id`, `dia_id` are optional metadata attributes. - The text between the tags is the memory content to be scored. **Output Format:** Your entire response must be **only** the following line: `<answer>[s0, s1, s2, ...]</answer>` - The array must contain exactly one integer score per memory, in the same order as the input indices. (cid:44) - If there are no memories, output: `<answer>[]</answer>` Let's think step by step to score the relevance of each memory to the query. **Input:** <query>{query}</query> <memories>{memories_text}</memories> Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory FILTER MODULE PROMPT (High) **Role:** You are relevance scoring system. **Task:** Given query and an ordered list of memories, output score for each memory indicating how directly and usefully it helps answer the (cid:44) query. (cid:44) **Scoring Rules:** - 9{10: directly answers the query or contains essential key facts - 6{8: clearly helpful and strongly related to the query - 3{5: somewhat related or only partially helpful - 1{2: very weakly related (e.g., superficial keyword overlap only) - 0: completely irrelevant or not useful for the query **Input Format:** The input consists of: 1. `<query>` section containing the user's question. 2. `<memories>` section containing individual `<memory>` elements. Each memory is formatted as: ``` <memory index=\"N\" [date_time=\"...\" session_id=\"...\" dia_id=\"...\"]> memory content text </memory> ``` Where: - `index` is the memory's position in the list. - `date_time`, `session_id`, `dia_id` are optional metadata attributes. - The text between the tags is the memory content to be scored. To complete the task systematically, please follow the steps reasoning (cid:44) framework outlined below: **Reasoning Steps:** **PLAN:** - Analyze the query's intent and define relevance criteria. - Identify key scoring dimensions (e.g., topical alignment, factual (cid:44) support). **ACT:** - Evaluate each memory against the criteria. - For each memory, briefly justify the provisional score based on its (cid:44) content. **REFLECT:** - Review all scores for consistency and calibration. - Make adjustments if needed to ensure fairness and coherence. **Output Format:** First write your reasoning following the PLAN ACT REFLECT structure. Then, your final response must be **only** the following line: `<answer>[s0, s1, s2, ...]</answer>` - The array must contain exactly one integer score per memory, in the same order as the input indices. (cid:44) - If there are no memories, output: `<answer>[]</answer>` **Input:** <query>{query}</query> <memories>{memories_text}</memories> 25 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory C.6. ENTITY MODULE RELATION PROMPT ENTITY MODULE RELATION PROMPT (Low) **Role:** You are specialist in entity semantic relationship extraction. **Task:** From the provided memories, extract **only** concrete (cid:44) relationships between entities that are central to answering the given query. (cid:44) **Extraction Criteria:** - Focus on factual connections where the entities and their relationship directly help answer the query. (cid:44) - Ignore background entities, anecdotes, or side topics. - Each extracted relationship should be concise, useful fact. **Input Format:** The `<memories>` section contains list of `<memory>` tags. Each memory is formatted as: ``` <memory [date_time=\"...\" session_id=\"...\" dia_id=\"...\"]> memory content text </memory> ``` Where: - `date_time`, `session_id`, `dia_id` are optional metadata attributes. - The text within the tags is the content to analyze for relationships. **Output Format:** Your entire response must be JSON array of relationship strings, formatted as follows: (cid:44) `<answer>[\"relationship1\", \"relationship2\", ...]</answer>` Each relationship string must be in the format: `\"EntityA - relation - (cid:44) If no relevant relationships are found, output: `<answer>[]</answer>` EntityB (optional context)\"` **Input:** <query>{query}</query> <memories>{memories_text}</memories> 26 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory ENTITY MODULE RELATION PROMPT (Mid) **Role:** You are specialist in entity semantic relationship extraction. **Task:** From the provided memories, extract **only** concrete (cid:44) relationships between entities that are central to answering the given query. (cid:44) **Extraction Criteria:** - Focus on factual connections where the entities and their relationship directly help answer the query. (cid:44) - Ignore background entities, anecdotes, or side topics. - Each extracted relationship should be concise, useful fact. **Input Format:** The `<memories>` section contains list of `<memory>` tags. Each memory is formatted as: ``` <memory [date_time=\"...\" session_id=\"...\" dia_id=\"...\"]> memory content text </memory> ``` Where: - `date_time`, `session_id`, `dia_id` are optional metadata attributes. - The text within the tags is the content to analyze for relationships. **Output Format:** Your entire response must be JSON array of relationship strings, formatted as follows: (cid:44) `<answer>[\"relationship1\", \"relationship2\", ...]</answer>` Each relationship string must be in the format: `\"EntityA - relation - (cid:44) If no relevant relationships are found, output: `<answer>[]</answer>` EntityB (optional context)\"` Let's think step by step to extract the relationships between entities. **Input:** <query>{query}</query> <memories>{memories_text}</memories> 27 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory ENTITY MODULE RELATION PROMPT (High) **Role:** You are specialist in entity semantic relationship extraction. **Task:** From the provided memories, extract **only** concrete (cid:44) relationships between entities that are central to answering the given query. (cid:44) **Extraction Criteria:** - Focus on factual connections where the entities and their relationship directly help answer the query. (cid:44) - Ignore background entities, anecdotes, or side topics. - Each extracted relationship should be concise, useful fact. **Input Format:** The `<memories>` section contains list of `<memory>` tags. Each memory is formatted as: ``` <memory [date_time=\"...\" session_id=\"...\" dia_id=\"...\"]> memory content text </memory> ``` Where: - `date_time`, `session_id`, `dia_id` are optional metadata attributes. - The text within the tags is the content to analyze for relationships. To complete the task systematically, please follow the steps reasoning (cid:44) framework outlined below: **Reasoning Steps:** **PLAN:** - Analyze the query to determine its precise intent. - Identify which entity types and relationship types are essential for (cid:44) answering it. **ACT:** - From the memories, extract candidate relations that involve only query-relevant entities. (cid:44) - Format each relation as concise, factual statement. **REFLECT:** - **CHECK:** Verify each extracted relation: - Does it directly help answer the query? - Is it clearly supported by the memory content? - **REGENERATE (if needed):** If important query-relevant relations are (cid:44) missing, re-extract or rewrite them to better align with the query intent. (cid:44) **Output Format:** First write your reasoning following the PLAN ACT REFLECT structure. Then, your final response must be **only** the following line: `<answer>[\"relationship1\", \"relationship2\", ...]</answer>` Each relationship string must be in the format: `\"EntityA - relation - (cid:44) If no relevant relationships are found, output: `<answer>[]</answer>` EntityB (optional context)\"` **Input:** <query>{query}</query> <memories>{memories_text}</memories> 28 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory C.7. TEMPORAL MODULE RELATION PROMPT TEMPORAL MODULE RELATION PROMPT (Low) **Role:** You are specialist in temporal information extraction. **Task:** From the provided memories, extract **only** specific temporal (cid:44) facts that are necessary for answering the given query. **Extraction Criteria:** - Focus on precise temporal information that directly constrains or clarifies the answer. (cid:44) - Ignore vague, irrelevant, or background time references. - Each extracted item must represent clear, factual temporal statement. **Input Format:** The `<memories>` section contains list of `<memory>` tags. Each memory is formatted as: ``` <memory [date_time=\"...\" session_id=\"...\" dia_id=\"...\"]> memory content text </memory> ``` Where: - `date_time`, `session_id`, `dia_id` are optional metadata attributes. - The text within the tags is the content to analyze for temporal (cid:44) information. **Output Format:** Your entire response must be JSON array of temporal fact strings, formatted as follows: (cid:44) `<answer>[\"temporal_fact1\", \"temporal_fact2\", ...]</answer>` Each string should clearly express temporal fact (e.g., \"Event occurred in March 2023\", \"Process lasted 3 days\", \"A happened before B\"). (cid:44) If no relevant temporal information is found, output: `<answer>[]</answer>` **Input:** <query>{query}</query> <memories>{memories_text}</memories> 29 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory TEMPORAL MODULE RELATION PROMPT (Mid) **Role:** You are specialist in temporal information extraction. **Task:** From the provided memories, extract **only** specific temporal (cid:44) facts that are necessary for answering the given query. **Extraction Criteria:** - Focus on precise temporal information that directly constrains or clarifies the answer. (cid:44) - Ignore vague, irrelevant, or background time references. - Each extracted item must represent clear, factual temporal statement. **Input Format:** The `<memories>` section contains list of `<memory>` tags. Each memory is formatted as: ``` <memory [date_time=\"...\" session_id=\"...\" dia_id=\"...\"]> memory content text </memory> ``` Where: - `date_time`, `session_id`, `dia_id` are optional metadata attributes. - The text within the tags is the content to analyze for temporal (cid:44) information. **Output Format:** Your entire response must be JSON array of temporal fact strings, formatted as follows: (cid:44) `<answer>[\"temporal_fact1\", \"temporal_fact2\", ...]</answer>` Each string should clearly express temporal fact (e.g., \"Event occurred in March 2023\", \"Process lasted 3 days\", \"A happened before B\"). (cid:44) If no relevant temporal information is found, output: `<answer>[]</answer>` Let's think step by step to extract the temporal information. **Input:** <query>{query}</query> <memories>{memories_text}</memories> 30 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory TEMPORAL MODULE RELATION PROMPT (High) **Role:** You are specialist in temporal information extraction. **Task:** From the provided memories, extract **only** specific temporal (cid:44) facts that are necessary for answering the given query. **Extraction Criteria:** - Focus on precise temporal information that directly constrains or clarifies the answer. (cid:44) - Ignore vague, irrelevant, or background time references. - Each extracted item must represent clear, factual temporal statement. **Input Format:** The `<memories>` section contains list of `<memory>` tags. Each memory is formatted as: ``` <memory [date_time=\"...\" session_id=\"...\" dia_id=\"...\"]> memory content text </memory> ``` Where: - `date_time`, `session_id`, `dia_id` are optional metadata attributes. - The text within the tags is the content to analyze for temporal (cid:44) information. To complete the task systematically, please follow the steps reasoning (cid:44) framework outlined below: **Reasoning Steps:** **PLAN:** - Analyze the query's intent and define what constitutes relevant temporal information. (cid:44) - Identify key temporal dimensions (e.g., dates, durations, sequences, (cid:44) constraints). **ACT:** - Extract temporal facts from each memory based on the defined criteria. - For each fact, briefly justify its relevance to the query. **REFLECT:** - Review all extracted facts for consistency and query relevance. - Make adjustments if needed to ensure factual support and coherence. **Output Format:** Your entire response must be JSON array of temporal fact strings, formatted as follows: (cid:44) `<answer>[\"temporal_fact1\", \"temporal_fact2\", ...]</answer>` Each string should clearly express temporal fact (e.g., \"Event occurred in March 2023\", \"Process lasted 3 days\", \"A happened before B\"). (cid:44) If no relevant temporal information is found, output: `<answer>[]</answer>` **Input:** <query>{query}</query> <memories>{memories_text}</memories> 31 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory C.8. TOPIC MODULE RELATION PROMPT TOPIC MODULE RELATION PROMPT (Low) **Role:** You are specialist in topic relationship extraction. **Task:** From the provided memories, extract **only** topic relationships (cid:44) that are central to answering the given query. **Extraction Criteria:** - Focus on thematic connections and topic transitions that help understand the conversation flow. (cid:44) - Identify main topics discussed and how they relate to each other. - Extract topic shifts, topic continuations, and thematic relationships. **Input Format:** The `<memories>` section contains list of `<memory>` tags. Each memory is formatted as: ``` <memory [date_time=\"...\" session_id=\"...\" dia_id=\"...\"]> memory content text </memory> ``` Where: - `date_time`, `session_id`, `dia_id` are optional metadata attributes. - The text within the tags is the content to analyze for topic (cid:44) relationships. **Output Format:** Your entire response must be JSON array of topic relationship strings, formatted as follows: (cid:44) `<answer>[\"topic_relationship1\", \"topic_relationship2\", ...]</answer>` Each string should clearly express topic relationship (e.g., \"Topic (cid:44) leads to Topic B\", \"Discussion shifts from to Y\", \"Topic is related to Topic through Z\"). (cid:44) If no relevant topic relationships are found, output: `<answer>[]</answer>` **Input:** <query>{query}</query> <memories>{memories_text}</memories> 32 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory TOPIC MODULE RELATION PROMPT (Mid) **Role:** You are specialist in topic relationship extraction. **Task:** From the provided memories, extract **only** topic relationships (cid:44) that are central to answering the given query. **Extraction Criteria:** - Focus on thematic connections and topic transitions that help understand the conversation flow. (cid:44) - Identify main topics discussed and how they relate to each other. - Extract topic shifts, topic continuations, and thematic relationships. **Input Format:** The `<memories>` section contains list of `<memory>` tags. Each memory is formatted as: ``` <memory [date_time=\"...\" session_id=\"...\" dia_id=\"...\"]> memory content text </memory> ``` Where: - `date_time`, `session_id`, `dia_id` are optional metadata attributes. - The text within the tags is the content to analyze for topic (cid:44) relationships. **Output Format:** Your entire response must be JSON array of topic relationship strings, formatted as follows: (cid:44) `<answer>[\"topic_relationship1\", \"topic_relationship2\", ...]</answer>` Each string should clearly express topic relationship (e.g., \"Topic (cid:44) leads to Topic B\", \"Discussion shifts from to Y\", \"Topic is related to Topic through Z\"). (cid:44) If no relevant topic relationships are found, output: `<answer>[]</answer>` Let's think step by step to extract the topic relationships. **Input:** <query>{query}</query> <memories>{memories_text}</memories> Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory TOPIC MODULE RELATION PROMPT (High) **Role:** You are specialist in topic relationship extraction. **Task:** From the provided memories, extract **only** topic relationships (cid:44) that are central to answering the given query. **Extraction Criteria:** - Focus on thematic connections and topic transitions that help understand the conversation flow. (cid:44) - Identify main topics discussed and how they relate to each other. - Extract topic shifts, topic continuations, and thematic relationships. **Input Format:** The `<memories>` section contains list of `<memory>` tags. Each memory is formatted as: ``` <memory [date_time=\"...\" session_id=\"...\" dia_id=\"...\"]> memory content text </memory> ``` Where: - `date_time`, `session_id`, `dia_id` are optional metadata attributes. - The text within the tags is the content to analyze for topic (cid:44) relationships. To complete the task systematically, please follow the steps reasoning (cid:44) framework outlined below: **Reasoning Steps:** **PLAN:** - Analyze the query's intent to determine what topic relationships are relevant. (cid:44) - Identify key thematic elements and potential connections. **ACT:** - Extract topic relationships from each memory based on content analysis. - Identify thematic shifts, continuations, and interconnections. **REFLECT:** - Review all extracted relationships for relevance to the query. - Ensure relationships are clearly supported by the memory content. **Output Format:** First write your reasoning following the PLAN ACT REFLECT structure. Then, your final response must be **only** the following line: `<answer>[\"topic_relationship1\", \"topic_relationship2\", ...]</answer>` Each string should clearly express topic relationship (e.g., \"Topic (cid:44) leads to Topic B\", \"Discussion shifts from to Y\", \"Topic is related to Topic through Z\"). (cid:44) If no relevant topic relationships are found, output: `<answer>[]</answer>` **Input:** <query>{query}</query> <memories>{memories_text}</memories> Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory C.9. SUMMARY MODULE PROMPT SUMMARY MODULE PROMPT (Low) **Role:** You are specialist in information synthesis. **Task:** Based strictly on the provided entity, temporal, and topic (cid:44) relations, integrate, extract, and reorganize this knowledge to create concise summary that clearly highlights the most useful information for answering the query. (cid:44) (cid:44) **Input Format:** The input includes: 1. `<query>` defining the subject and scope. 2. An `<Entity Relations>` section containing one `<entity>` tag per (cid:44) 3. `<Temporal Relations>` section containing one `<temporal>` tag per (cid:44) 4. `<Topic Relations>` section containing one `<topic>` tag per topic (cid:44) relationship string. temporal fact. relationship. **Synthesis Guidelines:** - Do **not** answer the query directly. - Explain what information is available and how it should be used to (cid:44) formulate an answer. **Output Format:** Your entire response must end with the following line: `<answer>your summary text here</answer>` The content inside `<answer>` must be plain text. **Input:** <query>{query}</query> <Entity Relations>{entity_text}</Entity Relations> <Temporal Relations>{temporal_text}</Temporal Relations> <Topic Relations>{topic_text}</Topic Relations> 35 Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory SUMMARY MODULE PROMPT (Mid) **Role:** You are specialist in information synthesis. **Task:** Based strictly on the provided entity, temporal, and topic (cid:44) relations, integrate, extract, and reorganize this knowledge to create concise summary that clearly highlights the most useful information for answering the query. (cid:44) (cid:44) **Input Format:** The input includes: 1. `<query>` defining the subject and scope. 2. An `<Entity Relations>` section containing one `<entity>` tag per (cid:44) 3. `<Temporal Relations>` section containing one `<temporal>` tag per (cid:44) 4. `<Topic Relations>` section containing one `<topic>` tag per topic (cid:44) relationship string. temporal fact. relationship. structure. **Synthesis Guidelines:** - **Integrate** relevant entity, temporal, and topic facts into coherent (cid:44) - **Extract** key information that directly supports or constrains the (cid:44) - **Reorganize** content for clarity and logical flow. - Do **not** answer the query directly. - Explain what information is available and how it should be used to answer. (cid:44) formulate an answer. **Output Format:** Your entire response must end with the following line: `<answer>your summary text here</answer>` The content inside `<answer>` must be plain text. Let's think step by step to synthesize the summary. **Input:** <query>{query}</query> <Entity Relations>{entity_text}</Entity Relations> <Temporal Relations>{temporal_text}</Temporal Relations> <Topic Relations>{topic_text}</Topic Relations> Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory SUMMARY MODULE PROMPT (High) **Role:** You are specialist in information synthesis. **Task:** Based strictly on the provided entity, temporal, and topic (cid:44) relations, integrate, extract, and reorganize this knowledge to create concise summary that clearly highlights the most useful information for answering the query. (cid:44) (cid:44) **Input Format:** The input includes: 1. `<query>` defining the subject and scope. 2. An `<Entity Relations>` section containing one `<entity>` tag per (cid:44) 3. An `<Temporal Relations>` section containing one `<temporal>` tag per (cid:44) 4. `<Topic Relations>` section containing one `<topic>` tag per topic (cid:44) relationship string. temporal fact. relationship. **Synthesis Guidelines:** - Do **not** answer the query directly. - Explain what information is available and how it should be used to (cid:44) formulate an answer. To complete the task systematically, please follow the steps reasoning (cid:44) framework outlined below: **Reasoning Steps:** **PLAN:** - Analyze the query's intent to determine the core information requirements. - Identify how to organize and prioritize the provided relations. **ACT:** - Integrate entity, temporal, and topic facts into coherent structure. - Extract and highlight key information most relevant to answering the (cid:44) query. **REFLECT:** - Review the summary for clarity, completeness, and focus on the query. - Ensure the summary effectively explains how the information should be (cid:44) used. **Output Format:** First write your reasoning following the PLAN ACT REFLECT structure. Then, your final response must be **only** the following line: `<answer>your summary text here</answer>` The content inside `<answer>` must be plain text. **Input:** <query>{query}</query> <Entity Relations>{entity_text}</Entity Relations> <Temporal Relations>{temporal_text}</Temporal Relations> <Topic Relations>{topic_text}</Topic Relations>"
        }
    ],
    "affiliations": [
        "Nanyang Technological University",
        "Sun Yat-sen University",
        "The Hong Kong University of Science and Technology (Guangzhou)",
        "Tsinghua University",
        "University of Illinois Chicago",
        "University of Illinois Urbana-Champaign"
    ]
}