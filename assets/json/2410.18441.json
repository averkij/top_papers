{
    "paper_title": "The Nature of Mathematical Modeling and Probabilistic Optimization Engineering in Generative AI",
    "authors": [
        "Fulu Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we give an in-depth analysis on the mathematical problem formulations and the probabilistic optimization explorations for some of the key components in Transformer model [33] in the field of generative AI. We explore and discuss some potential further enhancement for current state of the art methods for some key underlying technologies of generative AI models from algorithmic and probabilistic optimization perspective. In particular, we present an optimal solution for sub-word encoding (SWE) based on similar initial settings as that of byte-pair encoding (BPE) algorithm in [9] with similar objectives as that of WordPiece approach in [28, 31] to maximize the likelihood of the training data. We also present cross entropy optimization method to optimize hyperparameters for word2vec model [17]. In addition, we propose a factored combination of rotary positional encoding (RoPE) [32] and attention with linear biases (ALiBi) [23] with a harmonic series. We also present a probabilistic FlashAttention [6, 7] (PrFlashAttention) method with a probability distribution over block distances in the matrix to decide which block is likely to participate in a given round of attention computation while maintaining the lower triangle shape of the tensor for autoregressive language models by re-shaping the tensors. Finally, we present staircase adaptive quantization (SAQ) of key-value (KV) cache for multi-query attention (MQA) based on the framework presented in [16] to have gradual quantization degradation while achieving reasonable model quality and cost savings."
        },
        {
            "title": "Start",
            "content": "Fulu Li Contact: fulu@alum.mit.edu"
        },
        {
            "title": "Abstract",
            "content": "In this paper, we give an in-depth analysis on the mathematical problem formulations and the probabilistic optimization explorations for some of the key components in Transformer model [33] in the field of generative AI. We explore and discuss some potential further enhancement for current state of the art methods for some key underlying technologies of generative AI models from algorithmic and probabilistic optimization perspective. In particular, we present an optimal solution for sub-word encoding (SWE) based on similar initial settings as that of byte-pair encoding (BPE) algorithm in [9] with similar objectives as that of WordPiece approach in [28, 31] to maximize the likelihood of the training data. We also present cross entropy optimization method to optimize hyperparameters for word2vec model [17]. In addition, we propose factored combination of rotary positional encoding (RoPE) [32] and attention with linear biases (ALiBi) [23] with harmonic series. We also present probabilistic FlashAttention [6, 7] (PrFlashAttention) method with probability distribution over block distances in the matrix to decide which block is likely to participate in given round of attention computation while maintaining the lower triangle shape of the tensor for autoregressive language models by re-shaping the tensors. Finally, we present staircase adaptive quantization (SAQ) of key-value (KV) cache for multi-query attention (MQA) based on the framework presented in [16] to have gradual quantization degradation while achieving reasonable model quality and cost savings. 1. Introduction In the year of 1953, Albert Einstein, arguably the most notable scientific giant in the 20 century, once commented on the development of Western Science and he concluded that the development of Western Science in based on two great achievements: the invention of the formal logical system by Greek philosophers, and the discovery of the possibility to find out causal relationships by systematic experiments (Renaissance). Large language models (LLMs) made great strides or breakthroughs with deep neural networks via unsupervised learning in recent years mainly due to the discovery of the possibility to find out causal attention (essentially paradigm shift for the study of applied science without the need to find out clear causal relationship) by Transformer models [33] with massive data sets and massive parallel computations with graphics processing units (GPUs) or tensor processing units (TPUs). th Mathematically, language model is framework and/or mechanism to compute the joint probability and/or conditional probability of natural language texts for some given language [21, 34]. In this paper, we start with the Transformer model [33] and give an in-depth analysis on the mathematical problem formulations and the probabilistic optimization approaches for some of the key components in the Transformer model in generative AI. We explore and discuss some potential further enhancement for current state of the art methods for some key underlying technologies of generative AI models from algorithmic and probabilistic optimization engineering perspective. Notably, maximum-likelihood estimation is often used as the de facto mathematical tool for parameter estimation such as the weights and bias in feed-forward network (FFN) or multi-layer perceptron (MLP) in the blocks/layers of Transformer model [33]. For non-independent variables, which is often the case in practice, maximizing log likelihood to minimize the cost or loss function with or without constraints in machine learning can be unsolvable in closed form. That is probably why we often have to resort to using iterative procedures to train large language models (LLMs) in epochs, updating the weights and bias values of the neurons/nodes in deep neural networks after each round of belief propagation. Gradient descent, which was first proposed by Augustin-Louis Cauchy in 1847 [13], is mathematical method for unconstrained optimization. More precisely, it is first-order iterative approach to minimize differentiable multivariate objective function such as the cost or loss function in machine learning. The basic idea is to take repeated steps in the opposite direction of the gradient of the function at the current point, 1 which is the direction of the steepest descent. Gradient descent method is very useful in machine learning for minimizing the cost or loss function as the objective function to optimize. The convergence properties of gradient descent method for non-linear optimization problems were first studies by Haskell Curry in 1944 [5]. As an extension of gradient descent, stochastic gradient descent, which uses mini-batch sampling during the optimization process, serves as the most basic algorithm used for training most deep neural networks today [12, 21]. In this paper, we analyze and explore the mathematical problem formulations and probabilistic optimization engineering aspects for some of the most critical components of large language models (LLMs) underpinning todays generative AI, in particular the Transformer model [33]. The major contributions of the paper include: a) We present systematic analysis on the mathematical problem formulations and probabilistic optimization explorations for some of the major components of Transformer model [33] underpinning todays generative AI; b) We explore further enhancement with algorithmic and probabilistic optimization solutions for the state of the art approaches in the related areas: in particular, we present an optimal solution for sub-word encoding (SWE) based on similar initial settings as that of byte-pair encoding (BPE) algorithm [9] with similar objectives as that of WordPiece approach [28, 31] to maximize the likelihood of the training data. We also present cross entropy optimization method to optimize hyperparameters for word2vec model in [17]. In addition, we propose factored combination of rotary positional encoding (RoPE) [32] and attention with linear biases (ALiBi) [23] with harmonic series. d) c) e) We also present probabilistic FlashAttention [6, 7] (PrFlashAttention) method with probability distribution over block distances in the matrix to decide which block is likely to participate in given round of attention computation while maintaining the lower triangle shape of the tensor for autoregressive language models by re-shaping the tensors. f) Finally, we present staircase adaptive quantization (SAQ) of key-value (KV) cache for multi-query attention (MQA) based on the framework presented in [16] to have gradual quantization degradation while achieving reasonable model quality and cost savings. The rest of the paper is organized as follows: We discuss related notions behind some engineering practices in generative AI in Section 2. We gave an in-depth analysis on mathematical modeling and probabilistic optimization engineering of LLM in generative AI in Section 3. We discuss some further enhancements for pre-training and post-training of LLM in Section 4. The summary and future directions are given in Section 5. 2. Related Notions Behind Some Engineering Practices In probability theory, the law of large numbers states that the average of the results obtained from large number of independent random samples converges to the true value. As mentioned earlier, in the training of deep neural networks during the maximum likelihood estimation of those parameters, i.e., the weights and bias of those neural nodes in the deep neural network, stochastic gradient descent method is used in the iterative optimization process. According to the law of large numbers, the gradients computed from these mini-batches of training data set are expected to fluctuate around the true gradient for the whole training data set. Therefore, the mini-batch gradient on average is expected to indicate an adequate direction for changing the parameters during stochastic gradient descent optimization process [5, 12, 13, 21]. In probability theory and statistics, variance is the expected value of the squared deviation from the mean of random variable. Layer normalization in the Transformer model [33] is used for regularization, where the mean and variance of the parameters in each layer are computed in order to make sure that the resulting outputs of the given layer have well-behaved distribution, with the value of expectation/mean at 0.0 and the value of variance at 1.0 [21]. Depending on the real application scenarios, most of the generative pre-trained transformer (GPT) models adopted decoder-only Transformer model [19], where the query matrix of and the key matrix of are all of full rank due to the autoregressive nature of contiguous lower triangle shape along the diagonal in the corresponding matrix during attention computation. matrix is of full rank if its rank is the same as its smaller dimension. Notably, matrix of full rank can carry more information that matrix of the same size with lower rank, where some rows or columns in the matrix are not of linear independence. That is probably 2 why decoder-only transformer models become the state of the art in natural language generation, in particular for generative AI. The logistic sigmoid function also has its root in probability theory and it was used as an activation function in deep neural networks, where the value of the logistic function is in the range of (0, 1) and can be interpreted as probability. Another popular activation function for deep neural networks, named rectified linear unit (ReLU), achieves better gradient propagation when training deep neural networks. Non-linearity is essential for neural networks as it allows the algorithm to deduce complex patterns in the data. Nonlinearity is accomplished by activation functions [30]. In retrospect, there was the argument that single layer perceptron can not solve XOR classification if there is linear activation function for the neural network, which speaks for itself about the importance of the introduction of non-linear activation functions to deep neural networks today. ReLU was first used by Alston Householder in 1941 as mathematical abstraction of biological neural networks [11]. In probability theory, the output of the softargmax function can be used to represent categorical distribution, where probability distribution is over different possible outcomes. For the softmax function is the Transformer model [33], it is essentially normalized exponential function that converts the vector representation in the neural networks back to the probability distribution over the tokens, which facilitates next token prediction for generative AI. Now let us take look at the rationale behind some emergent ability of some generative AI model when the size of the language model, i.e., the number of parameters of the deep neural networks underpinning the given language model, is large enough and the size of the training data for the given neural networks is large enough. Let be the number of times that event occurs during independent trials and let denote the probability that event occurs for every trial. According to Bernoullis law of large numbers [2], for any positive real number of , we have μ A ϵ lim n> Pr{ μ < ϵ} = 1 (1) An interesting observation is that when the number of parameters of the language model in the deep neural networks is large enough and the size of the training data for the neural networks is large enough, the event that some emergent ability for given generative AI model may appear is of high possibility, which could be the new form of law of large numbers in the field of large language models (LLMs) for generative AI. 3. Mathematical Modeling of LLM for Generative AI In the following, we discuss the probabilistic optimization formulations for autoregressive language model that is widely used in GPT-like models (generative pre-trained transformer) [19], pre-training of the foundation model [21], fine-tuning of the model with additional domain knowledge of (question, answer) pairs [25], reinforcement learning with human feedback (RLHF) [20, 27], direct preference optimization (DPO) [24] and identity preference optimization (IPO) [1] for alignment in generative AI. Mathematically, autoregressive language model (ALM) generating sequence of text by predicting next token given previous tokens is based on conditional probability, where the objective of the model is to estimate the joint probability of the given sequence of tokens. According to the chain rule of probability, we have: Pr(s) = Pr(s1, s2, . . . , sT ) = Pr(st s1, s2, . . . , st1) (2) where the token sequence of is given by model predicts the probability of each token of based on all the previous tokens of sequence [21, 34]. , is the length of the token sequence, the in the = (s1, s2, . . . , sT ) s1, s2, . . . st st t=1 The pre-training process of LLM with deep neural networks such as the Transformer model [33] is to minimize the cross-entropy loss of the training data set, i.e., the sum of the negative log-likelihood of of true tokens at each position in the sequence of the training data set, and we have the probabilistic optimization objective as follows: ( min θ t=1 log Pr(st s1, s2, . . . , st1; θ )) (3) 3 θ where is the model parameters, is the probability distribution over the vocabulary [3, 34], the function of softmax() in the Transformer mode converts vector of numbers into vector of probabilities, where the probability of is the probability of the true label token index at position given all preceding tokens of Pr(st s1, s2, . . . , st1) . s1, s2, . . . , st1 Pr () On the other hand, after the pre-training of the foundation model, the model is further fine-tuned with additional domain knowledge of (question, answer) pairs, the probabilistic optimization objective of finetuning is to maximize the log-likelihood of the correct answers of the additional training data set as follows: ( max θ i=1 log Pr(Ai Qi; θ )) (4) θ ith is the where is the model parameters, number of total number of (question, answer) pairs, Pr(Ai Qi) given question of and Ai Qi . question and answer pair respectively [25], is the is the probability of the correct answer of Ai Qi After the fine-tuning of the model with domain knowledge of (question, answer) pairs, reinforcement learning with human feedback (RLHF) is often used to further improve the models instruction-following to estimate the quality of the models capabilities [20]. The basic idea is to construct reward model of outputs based on pair-wise of (prompt, completion) training data. The policy/language model is optimized based on methods such as proximal policy optimization (PPO) [27] and the probabilistic optimization objective is to minimize the cross entropy loss, which incentivizes it to make predictions that are closer to actual human ratings/feedbacks: min Πθ (𝔼[R(Q, A) β KL(Πθ(A Q) Πθold(A Q))]) (5) indicates the expectation value of in probability theory, i.e., the first moment, where indicates the prompt and denotes the completion, is hyperparameter that controls the strength of the Kullback is the old policy/ Leibler (KL) divergence penalty and language model. is the current policy/language model, β β > 0 Πθ Πθold R(x) 𝔼[x] , KL divergence is also called relative entropy and mathematically it is defined as: KL(W ) = xχ (W(x) log( W(x) (x) )) (6) The combination of RLHF and PPO has led to great success in practice such as InstructGPT and GPT4 [1]. However, as pointed out in [8] that RLHF is often slow and quite unstable in practice, in particular in distributed learning environment. Direct preference optimization (DPO) [24] and its variants such as identity preference optimization (IPO) [1] are becoming popular alternative solutions without the rewarding stage, while performing reinforcement learning (RL) to learn the policy with single maximum likelihood objective [24]. The loss that DPO is trying to optimize, given an empirical data set of language model to optimize, is given by [1]: , as function of Πθ , i.e., the policy/ min Πθ (𝔼(x,yw,yl)D[log(σ (τ log( Πθ(yw x) Πθ(yl x) ) τ log( Πref (yw x) Πref (yl x) )))]) (7) denotes some reference policy, is hyperparameter that controls the strength of the Kullback denotes the sigmoid function and plays the role of τ Πref where Leibler (KL) divergence penalty and normalization and the data set of = (xi, yw,i yl,i)N i=1 and , τ > 0 σ ( . ) is given by: where is given prompt, xi preference of yw,i over yl,i . yw,i are two completions given the prompt of , (8) xi yw,i yl,i yl,i indicates the The key findings in [24, 1] is that when the Bradley-Terry model that represents the preference function as sigmoid of the difference of rewards perfectly fits the preference data and the optimal reward policy is obtained from preference optimization loss function, then the optimization of RLHF objective in Equation (5) perfectly coincides with the optimization of DPO objective in Equation (7). Azar et al in [1] further simplified DPO optimization with sampled loss for IPO as follows: min Πθ (𝔼(x,yw,yl)D[τ (log( Πθ(yw x) Πθ(yl x) ) log( Πref (yw x) Πref (yl x) ) τ1 2 )2]) (9) where is hyperparameter that controls the strength of the KullbackLeibler (KL) divergence penalty and τ , is the policy/language model to optimize, is given prompt, τ > 0 Πref yw denotes some reference policy, and are two completions given the prompt of . Πθ yl In the following, we focus on some of the widely-used models for generative AI such as the Transformer model [33]. 3.1. The Transformer Model As discussed in [33], Transformer model typically has the following major components: (a) tokenizer that converts sequence of text into sequence of tokens; (b) an embedding layer that converts tokens and positions of tokens into vector/tensor representations; (c) transformer layers that conduct repeated transformations on vector representations via deep learning process, extracting more and more language semantics information, which typically consists of multi-head attention (MHA) layer and feedforward network (FFN) layer; (d) LayerNormalization block that computes the mean and variance of the parameters at each layer such that the outputs of the given layer has well-behaved distribution with mean of zero and variance of 1.0; (e) An un-embedding layer that converts the vector representation back to the probability distribution over the tokens with softmax function; (f) Residual connections that bypass one or more layers of neural network computations to improve the flow of gradients during back propagation in order to facilitate deeper neural networks. The identity shortcuts of residual connections essentially skip blocks of layers to preserve features of the propagated signal; (g) Next token prediction based on some effective algorithm, etc. algorithms such as top algorithm or top k In the following, we present some enhancements for some of the state of the art methods for some of the key components in the implementation of the Transformer model [33]. 3.1.1. Subword Encoding to Maximize the Likelihood of of the Training Data For the tokenizer, byte pair encoding (BPE) algorithm [9] is widely used for sub-word encoding to deal with rare and unseen word issues. The basic idea for BPE algorithm is to initialize each word unit/token with one character in the text and greedily merge the adjacent pair with the highest frequency until the number of remaining tokens/words reaches given size. However, by only focusing on merged-token frequencies when merging two adjacent pair with the highest frequency may lead to reduced total number of appearances for each word vocabulary item in the training data. Essentially, it is tradeoff between average word length and the total number of words appearance in the training data while avoiding out-of-vocabulary issues and capturing linguistic meanings as much as possible. Another widely-used sub-word encoding approach named WordPiece [28] algorithm starts by initializing all Unicode characters of the collection for given language as tokens. Then it combines two tokens (not necessarily adjacent) in such way that the likelihood of the training data is maximized [28]. Song et al presented fast WordPiece algorithm, whose computational complexity is linear with respect to the input length [31]. Notably, both of BPE [9] and WordPiece [28, 31] methods efficiently addressed the open-ended vocabulary problem by allowing word to be represented as sequence of characters if necessary, splitting word into multiple sub-words. Essentially, subword encoding approaches effectively interpolate between word level inputs for frequent words and character level inputs for rare words [21]. The key differences between BPE [9] and WordPiece [28] are two-folds: (a) the initial settings are different in that BPE initializes each word unit/token with one character in the given input text to learn the word vocabulary while WordPiece initializes each word unit/token with one character among all basic Unicode characters for given language; (b) the criteria used to merge two word units/tokens are different in that BPE merges two adjacent word units/tokens with the highest frequency in the given learning text while WordPiece combines two of the word units/tokens (not necessarily adjacent) that maximizes the likelihood of the training data. 5 Please note that both BPE and WordPiece are heuristic greedy algorithms, which make it difficult to judge why the vocabulary size of large language model of , say about 200k, is much larger than that of large language model of , say around 50k, if the training data sets for the two language models are largely the same and both large language models are using decoder-only Transformer model [33] for training of their foundation models. To answer this question, we present an optimal solution for sub-word encoding (SWE) based on -step shortest path algorithm with similar initial-settings to that of BPE as well as an enhanced BPE algorithm (eBPE) to maximize the likelihood of training data. B Following the assumptions in WordPiece [28, 31], we do not focus on word semantics, instead we are more interested in word appearances in the training data set. For the sake of simplicity, we assume that the given learning data set for word vocabulary is the same as the training data set of the language model for BPE. The basic idea is that we follow the logistics of BPE [9] including the initial settings to initialize each word unit/ token with one character in the given learning text and the merging operations of adjacent word units/tokens while not based on word frequencies in the given learning text but based on the objective to maximize the likelihood of training data, assuming the learning data set for word vocabulary and the training data set is the same for now. We formulate the optimal sub-word encoding (SWE) problem to maximize the likelihood of the training data with final number of tokens of in the learning text as -step shortest path problem to find the optimal solution, where the number of unique words in the final tokens is the actual size of the vocabulary. k Usually, for the application of sub-word encoding, the first step is to split the text into words based on spaces and append special end-of-word symbol such as _ to each word. This special symbol is important as it marks word boundaries, which prevents the algorithm from confusing the end of one word with the start of another word, in particular for BPE algorithm [9] and its variants. Borrowing some interesting concepts from Riemann Hypothesis, we consider end-of-word spaces/special end-of-word symbols as trivial stops and we consider stops in-between characters within word as nontrivial stops. Assuming we have an initial sequence of individual word unit/tokens, each of which is either character that belongs to an original word in the learning text/training data or special end-of-word symbol, say _, our goal is to merge them into subwords or words based on the original order of those tokens in the sequence such that the likelihood of the training data is maximized. Let be the final number of tokens/ words, where the number of unique words in the final tokens/words is the actual size of the vocabulary, let stand for the number of original words in the learning text/training data, let be the number of sub-words/ tokens/steps within the word in the learning text/training data, let learning text/training data, let the symbol of indicate the concatenation of the sub-words, and we have original word in the learning text/training data, let denote the ki stand for the wi ith ith original original word in the sub-word of the the wi, ith jth i=1 ki 1, ki (10) [1,...,w] (11) (12) (13) wi = wi,1 . . . wi,ki Condition (10), Condition (11) , Condition (12) and Equation (13) indicate that all word units/tokens in the initial sequence of learning text/training data are covered in the -step path from the beginning of the sequence to the end of sequence and trivial stops /end-of-word symbols must be visited once and only once, where each original word has at least one step/one token. We also need to emphasize that the final number of tokens of for the learning text/training data must be greater than or equal to the number of original words in the learning text/training data. Please note that to find out the total number of word appearances in the learning text/training data, we need to first find out the unique words in the current tokens. Let be the number of unique words in the current ith tokens, let stand for the denote the number of appearances for the word of in the learning text/training data, let be the total number of word appearances in the learning text/training data, and we have unique word in the current tokens, let ntotal aui ui ui u ntotal = aui i=1 (14) si Let stand for the corresponding token/word of the sequence to the end of sequence in the learning text/training data, let for the word of in the learning text/training data, let be the cost of the can be defined as follows: asi ith ci si ith denote the number of appearances step in the -step path, which step in the -step path from the beginning of the ci = 1 asi + 1 (15) Notably, we are using the inverse of the sum of the number of corresponding-word appearances for the corresponding step and one as the cost of given step to avoid the zero appearance issue. For -step path, let cp be the total cost of given path, which can be defined as follows: cp = ci i=1 (16) Based on the cost definition of step in given path and the cost definition of given path in Equation (15) and Equation (16), we can apply Bellman-Ford shortest path algorithm to find the -step shortest path that satisfies Condition (10), Condition (11), Condition (12) and Equation (13) as the optimal solution to maximize the likelihood of the training data. While there is some good justification for the optimal solution of the -step shortest path that satisfies given Conditions in (10)(13) based on Bellman-Ford shortest algorithm to maximize the likelihood of of the training data, it may not be an ideal solution for an online tokenization algorithm due to its complexity of O(k 3) , where is the number of steps and is the input length in terms of the number of characters in the learning text/training data. In the following, we present an enhanced byte-pair encoding (eBPE) algorithm in which two adjacent token pair are merged where the number of merged-token/word appearances in the learning text/training data is the highest. We repeat this merging operation process until the final number of tokens in the learning text/training data reaches the value of . k More formally, based on similar initial settings as that of byte-pair encoding (BPE) algorithm in [9] with similar objectives as that of WordPiece approach in [28, 31] to maximize the likelihood of the training data, we have the optimal solution for sub-word encoding (SWE) based on Bellman-Ford shortest path algorithm to maximize the likelihood of the training data as follows in Figure 1: 1. Initialize each word unit/token with one character in the input text and mark each end-of-word with special symbol. Initialize the value of the final number of tokens of . 2. 3. Compute pair-wise step cost based on Equation (15). 4. Apply the relaxation procedures of Bellman-Ford shortest path algorithm to find the -step shortest path from the beginning of the token sequence to the end of the token sequence based on the input text and pair-wise cost functions (Equation (15)) and compute the cost of the path based on Equation (16). 5. Choose the the -step shortest path that satisfies Conditions (10) , Condition (11), Condition (12) and Equation (13). 6. Output the final tokens 7. The unique tokens/words in the final tokens constitute the vocabulary from the learning text/training data. k Figure 1: An Optimal Solution of Sub-word Encoding (SWE) based on Bellman-Ford Shortest Path Algorithm to Maximize the Likelihood of the Training Data. Based on similar initial settings as that of byte-pair encoding (BPE) algorithm in [9] with similar objectives as that of WordPiece approach in [28, 31] to maximize the likelihood of the training data, we have the enhanced byte-pair encoding (eBPE) algorithm to maximize the likelihood of the training data as follows in Figure 2: 7 1. Initialize each word unit/token with one character in the input text and mark each end-of-word with special symbol. Initialize the value of the final number of tokens of . 2. 3. Compute the number of merged-token appearances in the learning text/training data for any two adjacent tokens in the current token sequence. 4. Merge two adjacent token pair whose number of merged-token/ word appearances in the learning text/training data is the highest. 5. Repeat Step 3 and Step 4 until the final number of tokens in the learning text/training data reaches the value of . 6. Output the final tokens 7. The unique tokens/words in the final tokens constitute the vocabulary from the learning text/training data. k Figure 2: An Enhanced Byte-Pair Encoding (eBPE) Algorithm to Maximize the Likelihood of the Training Data. 3.1.2. Optimization of Hyperparameters for Word2vec Approach Word2vec [17] is widely used as the state of the art model for obtaining vector representations of words, where the objective for these vectors is to capture information about the meaning of the word based on the surrounding words as much as possible such that those vectors can be useful for predicting the surrounding words in sentence or document. As stated in [17], more formally, the objective of the skip-gram model is to maximize the average log probability as follows: 1 (17) log(Pr(wt+j wt)) t=1 jc, j0 w1, w2, w3, . . . , wT where is the sequence length of word sequence of , is the size of the training context window. For skip-gram model, it is trying to predict surrounding words with window of radius of for every word in the window. Continuous bag of words (CBOW) and skip-gram are two of the most popular frameworks used in word2vec model to get word embeddings [17]. For continuous bag of words (CBOW) model, it is trying to predict the middle word with the input of its surrounding words within given window. The basic idea is that word vectors are positioned in the vector space in such way that words that share some common contexts, i.e., close to each other, in the text corpus are located close to one another in the vector space. In the following discussions, we only focus on the skip-gram model (Equation (17)) but the principle can be equally applicable to continuous bag of words (CBOW) model as well. As pointed out in [14] that much of the superior performance of word2vec [17] in downstream tasks, compared with other similar approaches, is mainly due to the choice of specific hyperparameters such as the size of the training context window, i.e., the hyperparameter of , the dimensionality of the vector, i.e., the hyperparameter of , the word frequency threshold for sub-sampling of high-frequency words, i.e., the hyperparameter of , the word count threshold to be considered as part of the training vocabulary for lowfrequency word, i.e., the hyperparameter of sf . mc Existing hyperparameter optimization methods include: (a) grid search, i.e., essentially an exhaustive search over given hyperparameter space; (b) random search, which can explore much more potential values than grid search for continuous hyperparameters; (c) Bayesian optimization, which employs probabilistic model between hyperparameter values and objectives evaluated on given data set and it has shown to outperform both grid search and random search with fewer evaluations; (d) gradient-based optimization, which computes the gradients with respect to hyperparameters when applicable for some learning algorithms and then optimizes the hyperparameters based on gradient decent philosophy; (e) evolutionary optimization, which employs an evolutionary algorithm to find the best possible hyperparameters for given learning algorithm; (f) population-based methods, which updates the hyperparameters as well as the weights of neural networks during the training of the models with multiple independent learning processes with different hyperparameteres of the learning algorithm; (g) early stopping-based approach, which starts as random 8 search over the hyperparameters space and successively prunes low-performing ones until there is only one model remaining. Notably, it is combinatorial optimization problem for the selection of the size of the training context window of , the dimensionality of the vector of , the word frequency threshold for sub-sampling of highfrequency words of , the word count threshold to be considered as part of the training vocabulary for lowfrequency word of , for the hyperparameters of word2vec algorithm. Therefore, it is not practical to use some common approach like grid search for the selection of hyperparamters for word2vec model/algorithm. sf mc In the following, we present probabilistic optimization framework with cross entropy optimization method for the selection of hyperparameters in word2vec [17] model/algorithm. The basic idea of cross entropy (CE) method [26] is to translate the deterministic optimization problem into corresponding stochastic one and then use rare event simulation techniques to find the optimal solution. As discussed in [26], cross entropy (CE) method differs from other well-known random search algorithms for global optimization such as simulated annealing, tabu search, and genetic algorithms, which are local search heuristics and employ the notion of local neighborhood structures. Cross entropy (CE) method employs multi-extremal optimization process based on Kullback-Leibler cross-entropy, importance sampling, etc. Therefore, cross entropy (CE) method represents global random search procedure rather than local one. Our work differs from existing hyperparameter optimization methods in that we use cross-entropy optimization with probability distribution over hyperparameter space, where some rare event simulation techniques are used to find the optimal values of the hyperparameters for given learning algorithm with given training data set. The cross-entropy operation is achieved by virtue of probability update after the performance evaluation after each round of randomly-generated hyperparameter samples based on probability distributions, where small portion of top performers are given higher probabilities for the next round. The iterative optimization process ends when the convergence conditions are achieved or predefined early-stop criteria is met. We also need to emphasize that for very large training data set, we can randomly choose small subset of the training data set for cross entropy optimization for the selection of the hyperparameters of word2vec model/ algorithm, then we use the selected hyperparameters to train the model with the whole training data set. We have some basic notations for the cross entropy optimization of hyperparameters in word2vec model/ algorithm in [17] as follows: Let be the hyperparameter for the size of the training context window. Let stand for the hyperparameter for the dimensionality of the vector. Let denote the word frequency threshold stand for the minimum count threshold for low-frequency for high-frequency word for sub-sampling. Let stand for the performance metric word to be considered as part of the training vocabulary. Let of trained word2vec model for given tuple value of ( ), given training data set for word similarity and/or analogy tasks, etc. The details of some related performance metric can be found in [14]. We also assume that the proper value range of is given as are all integers and [a, b] are , d2 > d1 d, d1, d2 a, b, ℤ , where all integers and [ f1, f2] . We also assume that the proper value range . Let stand for . We also assume that the proper value range of is given as . We also assume that the proper value range of is given as , > a, b, , where [d1, d2] sf are all positive real numbers and are all integers and sf , f1, f2 ℜ+ F(c, d, sf , mc) d, d1, d2 ℤ c, d, sf , mc , where mc sf , m2 > m1 mc, m1, m2 γt , f2 > f1 sf , f1, f2 of is given as the benchmark value of [m1, m2] mc , where F(c, d, sf , mc) mc, m1, m2 ℤ round and we have its definition as follows: (18) (F(c, d, sf , mc) ) ρ} γt = min{ : for the th Pr (c,d,sf ,mc)t1 where the purpose of the hyperparameters optimization of word2vec model/algorithm is to maximize the performance metric for word similarity and/or analogy tasks, etc. of the word2vec model, normally takes value of 0.01 so that the event of obtaining high performance is not too rare, the tuple value of ( ) is c, d, sf , mc ) respectively, randomly chosen based on the probability distribution of each variable of ( round. th c, d, sf , mc (c, d, sf , mc)t1 Essentially, is the top -quantile of the performers of the randomly generated ( round. represents the set of randomly generated values of ( (t 1)th ) values in the c, d, sf , mc ) in the c, d, sf , mc γt ρ ρ As discussed in [26], there are several ways to set the termination conditions. Normally, if for some say , and we have , = 5 γt = γt1 = . . . = γtl (19) then we stop the hyperparameter optimization process for word2vec model/algorithm for the given training data set."
        },
        {
            "title": "M\nith",
            "content": "stand for the number of sample tuple values of ( randomly generated sample tuple value of ( Let the probability that the given beginning. We assume uniform distributions of the values for tuple value of (c, d, sf , mc)i ith is an indicator function, c, d, sf , mc c, d, sf , mc H{} ) in given round, let ), denote (c, d, sf , mc)i is the qi is being chosen and is initialized as zeros at the , in the ranges of with with uniform distribution. The , [a, b] [d1, d2] [a, b] (c, d, sf , mc) qi [a, b] , respectively. For example, randomly choosing value of in given range of , [ f1, f2] [m1, m2] uniform distribution is like generating random number in the range of updated value of can be estimated as: qi qe = H{F((c,d,sf ,mc)i)γ} k=1 H{F((c,d,sf ,mc)k )γ} (20) While there are solid theoretical justifications for Equation (20), we refer interested readers to [26] for the details. In order to have smoothed update procedure, normally we have + (1 α) qt1 = α qe qt (21) where empirically value of between 0.4 and 0.9, i.e., α the value of in the previous round, qt round according to Equation (20), is is the estimated value of based on the performance in the previous qe stands for the value of in the current round. 0.4 α 0.9 qi , gives the best results [26], qt1 qi qi Let be the set of tuple values of of the randomly generated tuple values of (c, d, sf , mc) whose performance are in the top -quantile of the performers ρ (c, d, sf , mc) in given round and we have qt normalized = {(c, d, sf , mc)i H{F((c,d,sf ,mc)i)γ}} (22) for the top -quantile of the performers is given as follows: ρ qn = qt (c,d,sf ,mc)iB qt (23) Let be the factor of favorability, i.e., the total number of samples chosen from those top performers and we have = 10 , towards those top performers in given round and let be Ns Ns = ρ (24) Ns , where samples that are randomly chosen from those For the next round, we have (c, d, sf , mc) samples, i.e., , [ f1, f2] [m1, m2] random tuple numbers of (M Ns) respectively, with uniform distributions. number of tuple values of indicates the number of tuple value elements in the value set of . The rest of the , , respectively with uniform distributions, which can be accomplished by generating , , are randomly chosen from the ranges of in the range of , [a, b] [d1, d2] (c, d, sf , mc) (c, d, sf , mc) samples of (M Ns) [m1, m2] [d1, d2] [ f1, f2] [a, b] , , , In summary, we have the algorithm of cross entropy optimization for the hyperparameters optimization (CEHPO) in word2vec model/algorithm [17] as follows: For given training data set and given set of hyperparameters to optimize within their corresponding value ranges, we have the detailed description of the cross entropy optimization process for hyperparameters optimization in word2vec model/algorithm [17], .i.e., the CEHPO algorithm, in Figure 3. The cross entropy optimization process will stop when the convergence conditions are satisfied or pre-defined early-stop criteria is met. 10 and initialize as zeros. qi = 1. Set 2. Generating range of [d1, d2] for mc random numbers in the range of [a, b] sf for , in the range of [ f1, f2] , with uniform distribution for the tuple values of for , in the range of for , in the c for , [a, b] for , in the range , with uniform distribution for the tuple values of [ f1, f2] ) if it is the first time. for , in the range of (M Ns) [d1, d2] mc random numbers in the range of sf [m1, m2] ( c, d, sf , mc 3. Generating in the range of of for [m1, m2] ( ) if it is not the first time. c, d, sf , mc 4. is determined by Equation (24). Ns random samples of 5. Ns -quantile performers of probability distribution of 6. Calculate according to Equation (18). γt 7. Update according to Equation (20) and Equation (21). 8. Update normalized according to Equation (23). 9. If for some stop, otherwise, reiterate from Step 3. (c, d, sf , mc) (c, d, sf , mc) qi , say , such that = 5 qn qi are drawn from those top ρ in the last round based on γt = γt1 = . . . = γtl , then Figure 3: the Cross-Entropy HyperParameter Optimization (CEHPO) Algorithm. 3.1.3. Rotary Positional Embedding (RoPE) and Attention with Linear Biases (ALiBi) As of today, rotary positional embedding approach (RoPE) presented in [32] is probably the most widely used positional embedding method for Transformer models in generative AI applications, in particular after LlaMA2 adopted this positional embedding approach. The RoPE approach works well in most scenarios but it may have some challenges for input length extrapolation, where sometimes the sequence length for inference is longer than the maximum training sequence length, in particular when the training data set is not large enough. Attention with Linear Biases (ALiBi) method presented in [23] does not add positional embeddings to word embeddings but it adds biases to query-key attention scores with penalty that is proportional to their distance, which facilitates the extrapolation performance, in particular when the training data set is not large enough and the inductive biases have some significant impact on extrapolation during inference. Rotary positional embedding method [32] uses Eulers formula in complex analysis to transform the addition operation of the static position encoding proposed in the original Transformer framework [33] into multiplication operation, where the related parameters naturally become part of the learning process. Notably, both rotary positional embedding approach of RoPE and ALiBi method incorporate relative position information, where relative position information is used in RoPE for positional embedding calculation and the relative distance information is used in ALiBi for added linear biases for the query-key attention score. In the following, we propose factored combination of ALiBi and RoPE for Transformer-based generative AI applications in order to obtain the benefits of extrapolation by ALiBi as well as the benefits of RoPE in other application scenarios. As mentioned earlier, the method of ALiBi [23] does not change the token embeddings, it can be used along with RoPE [32] with some twists due to the fact that simple combination of ALiBi and RoPE may lead to the signal from relative position information being too strong as both ALiBi and RoPE uses relative position information in their respective approach. In [23], ALiBi uses geometric series decaying factor as head11 specific slope among different heads for multi-head attention (MHA) and it uses arithmetic series as function of the distance. The static, non-learned biases works well for input length extrapolation, where sometimes the sequence length for inference is longer than the maximum training sequence length. As ith described in [23], attention with linear biases (ALiBi) is defined as follows for the attention score of the query of in each head: qi softmax(qiK + . [ (i 1), (i 2), . . . , 2, 1,0]), query and ith where is the qi the attention layer in each head for multi-head attention given the first keys and hyperparameter of head-specific slope that is picked before training. The proposed values of and is the head dimension, the scaled-dot-product is computed by ℜid , is follow qi ℜ1d (25) 1 geometric series, where for model with 8 heads, the geometric series is in the form of 2 , 1 22 refer interested readers to [23] for suggestions on how to use the geometric series with other number of heads. 1 28 , . . . , . We As discussed in [23], their experiments with trainable parameters of slopes of did not achieve strong extrapolation results. By using the same slopes of geometric series of and adding another harmonic series of factors for the added linear biases as follows may lead to reasonable results. We propose to use harmonic series to add different scalar factor at different positions in the form of , 1 2 3 1 1 2 So, Equation (25) can be rewritten as follows: 1 (i + 1) , . . . , , where the sum of all those scalar factor values is k=1 ( 1 1 + 1 ) = + 1 . softmax(qiK + . [ (i 1) 1 2 , (i 2) 2 3 , . . . , 2 (i 2)(i 1) , 1 (i 1) , 0 (i + 1) (26) ]), The intuition is that we may need to properly adjust the linear biases introduced by ALiBi when we use both ALiBi and RoPE for Transformer-based generative AI applications as both ALiBi and RoPE use relative position information in their computation for eventual attention scores. We will evaluate the effectiveness of Equation (26) for ALiBi along with RoPE as one of our future directions for Transformer-based generative AI applications. 4. Pre-Training and Post-Training of LLM In this section, we focus on some techniques to speed up the pre-training of foundation models and to accelerate the inference process after pre-training, in particular on attention computation. 4.1. Probabilistic FlashAttetnion Attention computation is arguably the most critical component of the Transformer model [33]. Due to the quadratic nature of the attention computation complexity, lot of efforts have been made to speed up the attention computation such as the efforts by OpenAI team in [4]. One widely-used attention computation approach is called FlashAttention in [6, 7]. The essence of FlashAttention method for attention computation in Transformer model for large language model (LLM) is the application of tiling by splitting large matrix into tiles to have finer granularity to deal with I/O with differentiated memory (HBM, SRAM, etc.) constraint hierarchy in GPU. FlashAttention [6, 7] is of exact attention computation in its current form. As discussed in [22], fast causal attention computation for sparse FlashAttention can be more efficient. We considers attention computation in Transformer model for LLM in probabilistic way. The presented probability density function (PDF) with respect to the block/tile distance in the matrix follows constrained harmonic deduction philosophy. The presented PrFlashAttention dynamically and probabilistically skips less-related rows/columns in Query/Key , in the tensor shape of (Q/K) matrix along tensor dimension, say the number of Head dimension of (Batch, Head, Context Length, Head Dimension) during attention computation while supporting causal masks for auto-regressive models by reshaping the tensors. In the following, we discuss the probabilistic model for the presented approach of PrFlashAttention and how the masks for each row in the query matrix of and each column in the key matrix of are calculated. The presented probabilistic model for PrFlashAttention is defined as follows: For block distance, say, , within given range, say, , the probability is set as one, otherwise, it follows harmonic deduction series. (n) = 1, 1 (n k)(n + 1) , { if 0 ; if > ; (27) , we have the sum of probability series as: > , where is the context length, Br and Bc are block sizes for matrix Notably, for the second part, when 1 1 (N Br, Bc) and matrix respectively. Let nq be defined as: stand for the number of blocks in each row of matrix , the normalized probability for each row can Pr (rq) = nq i=1 Pr (i ) nq , (28) Pr (i ) where Similarly, let nk column can be defined as: is the probability of the ith block in the qth row of . rq denote the number of blocks in each column of matrix , the normalized probability for each Pr (ck) = nk i=1 Pr (i ) nk , (29) Pr (i ) is the probability of the th where We use weighted combination of pre-computed row/column probability and random number to make the selection process dynamic: = , (30) column of ck block in the ith . dq Pr (rq) + (1 w) is the decision factor for the row, is random number between 0 and 1, is the weight qth where dq between 0 and 1. = is the decision factor for the where dk between 0 and 1. The adjusted sparsity value is also weighted combination: dk Pr (ck) + (1 w) th , (31) column, is random number between 0 and 1, is the weight sadj = ps + (1 w) , (32) is the percentile value of the normalized row/column probabilities among rows/columns in the where ps matrix, is the targeted dropping percentage and is the weight between 0 and 1. 4.2. Adaptive Quantization of KV Cache for Multi-Query Attention Multi-query attention (MQA) has been proposed mostly to speed up the inference process. The basic idea is to keep the original number of heads for query matrix of in multi-head attention (MHA) but have only one heads share the same set of and head for key matrix of and value matrix of , which means all the heads, where computed keys and values vectors are cached, without the re-computation of the same key and value vectors at each attention block. In general, Multi-query attention (MQA) with key-value (KV) cache has neutral effect on model quality and training speed, but can greatly speed up the inference. K In the following, we present Staircase Adaptive Quantization (SAQ) for key-value (KV) cache in multiquery attention (MQA) to further alleviate the problem of KV cache based on the framework in [16] by means of gradual quantization degradation to speed up the inference while achieving reasonable model performance. For -bit integer, the quantization and de-quantization process can be expressed as follows [16]: 13 Q(X ) = zX sX (33) = Q(X ) . sX + zX (34) Q(X ) where zero-point and operation. indicates the quantized tensor of , sX = (max min )/(2B 1) is the de-quantized tensor of , is the scaling factor and the symbol of Q(X ) zX = min . is the is the rounding Notably, key and value cache of newly generated tokens arrive sequentially in time. Following similar settings in [16], during the pre-fill phase, exact (full precision) key and value tensors are passed to the next layers, even though only the quantized KV cache is retained in memory to reduce the memory footprint and to prevent some re-computations in the decoding phase. We assume that we have lprompt number of key tokens and lprompt number of value tokens in the pre-fill stage. We also assume that full precision is expressed as 16-bit quantization such as fp16 (float point 16) that is commonly used in the implementation of tensors, so we can have lower quantization choices such as 8-bit quantization, 4-bit quantization, 2-bit quantization, etc. Let be the number of quantization choices qn quantization choices and and represents the number of B1 Bi can be of arbitrary length, in the pre-fill stage we split the quantization bits of the full precision. Since ith indicate the number of bits for the lprompt sequence of lprompt tokens into qn segments, i.e., S1, S2, . . . , Sqn , each of which corresponds to different quantization level, with the segment of S1 assume that all of the segments, i.e., corresponds to the full precision. For the sake of simplicity, we , are of equal sizes, say segment size of . Please note that S1, S2, . . . , Sqn the size of the segment of , which corresponds to the one with the lowest quantization level, i.e. 2-bit"
        },
        {
            "title": "Sqn",
            "content": "quantization or 1-bit quantization, could be open-ended as the token sequence grows longer and longer unless it is truncated due to the constraint of cache memory. From the token sequence perspective, the quantization level downgrades by half in terms of quantization bits every tokes, which looks like staircase. We present the algorithm for Staircase Adaptive Quantization (SAQ) to have gradual quantization degradation for KV cache in both pre-fill stage and decoding stage based on the framework in [16] for multiquery attention (MQA) to speed up the inference. The details of the presented algorithm for Staircase Adaptive Quantization (SAQ) of KV cache for multi-query attention (MQA) in the phase of pre-fill and decoding are described in the Appendix A. 5. Summary and Future Directions With rapid progress in the field of generative AI, it is often good idea to reflect on some of the fundamental mathematical modeling and probabilistic optimization tools that powered this AI revolution so that we can make further enhancement systematically down the road. In this paper, we give an in-depth analysis on the mathematical problem formulations and the probabilistic optimization explorations for some of the key components in Transformer model [33] in the field of generative AI. We explore and discuss some potential further enhancement for current state of the art methods for some key underlying technologies of generative AI models from algorithmic and probabilistic optimization perspective. In particular, we present an optimal solution for sub-word encoding (SWE) based on similar initial settings as that of byte-pair encoding (BPE) algorithm in [9] with similar objectives as that of WordPiece approach in [28, 31] to maximize the likelihood of the training data. We also present cross entropy optimization method to optimize hyperparameters for word2vec model in [17]. In addition, we propose factored combination of rotary positional encoding (RoPE) [32] and attention with linear biases (ALiBi) [23] with harmonic series. We also present probabilistic FlashAttention [6, 7] (PrFlashAttention) method with probability distribution over block distances in the matrix to decide which block is likely to participate in given round of attention computation while maintaining the lower triangle shape of the tensor for autoregressive language models by re-shaping the tensors. Finally, we present staircase adaptive quantization (SAQ) of key-value (KV) cache for multi-query attention (MQA) based on the framework presented in [16] to have gradual quantization degradation while achieving reasonable model quality and cost savings. We will conduct extensive experiments for the proposed approaches as one of our future directions."
        },
        {
            "title": "Reference",
            "content": "[1] M. Azar, M. Rowland, B. Piot, D. Guo, D. Calandriello, M. Valko, R. Munos, General Theoretical Paradigm to Understand Learning from Human Preferences, https://arxiv.org/pdf/2310.12036, 2023. [2] J. Bernoulli, Wahrscheinlichkeitsrechnung (Ars conjectandi, 1713), Ostwalds Klassiker der exakten Wissenschaften, W. Engelmann, Leipzig, 1899. [3] T. Brown, B. Mann, N. Ryder, M. Subbiah, J. D. Kaplan, P. Dhariwal, A. Neelakantan, P. Shyam, G. Sastry, A. Askell, et al, Language models are few-shot learners, Advances in neural information processing systems, 33:1877 1901, 2020. [4] R. Child, S. Gray, A. Radford, and I. Sutskever. Generating Long Sequences with Sparse Transformers. arXiv: 1904.10509, 2019. [5] H. B. Curry, \"The Method of Steepest Descent for Non-linear Minimization Problems\". Quart. Appl. Math. 2 (3): 258261, 1944. [6] T. Dao, Daniel Y. Fu, Stefano Ermon, Atri Rudra, Christopher Ré, FlashAttention: Fast and Memory-Efficient Exact Attention with IO-Awareness, https://arxiv.org/pdf/2205.14135, 2022. [7] T. Dao, FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning, https://arxiv.org/pdf/ 2307.08691, 2023. [8] K. Ethayarajh, W. Xu, N. Muennighoff, D. Jurafsky, D. Kiela, KTO: Model Alignment as Prospect Theoretic Optimization, https://arxiv.org/pdf/2402.01306, 2024. [9] P. Gage, New Algorithm for Data Compression, The User Journal. 1994. [10] G. Hinton, O. Vinyals, and J. Dean, Distilling the knowledge in neural network, https://arxiv.org/abs/ 1503.02531, 2015. [11] A. Householder, \"A theory of steady-state activity in nerve-fiber networks: I. Definitions and preliminary lemmas\", The Bulletin of Mathematical Biophysics. 3 (2): 6369, 1941. [12] D. P. Kingma, J. L. Ba, Adam: Method for Stochastic Optimization, in the Proc. of ICLR 2015. [13] C. Lemarechal, \"Cauchy and the Gradient Method\", Doc Math Extra: 251254, 2012. [14] O. Levy, Y. Goldberg, I. Dagan, Improving Distributional Similarity with Lessons Learned from Word Embeddings, https://aclanthology.org/Q15-1016.pdf, 2015. [15] Y. Lipman, R. T. Q. Chen, H. Ben-Hamu, M. Nickel, M. Le, Flow Matching for Generative Modeling, https:// arxiv.org/pdf/2210.02747, 2023. [16] Z. Liu, J. Yuan, H. Jin, S. Zhong, Z. Xu, V. Braverman, B. Chen, X. Hu, KIVI: Tuning-Free Asymmetric 2bit Quantization for KV Cache, https://arxiv.org/abs/2402.02750, 2024. [17] T. Mikolov, I. Sutskever, K. Chen, G. Corrado, J. Dean, Distributed Representations of Words and Phrases and their Compositionality, https://arxiv.org/pdf/1310.4546, 2013. [18] L. Muttenthaler, K. Greff , F. Born, B. Spitzer , S. Kornblith , M. C. Mozer , K. Müller, T. Unterthiner and A. K. Lampinen, Aligning Machine and Human Visual Representations across Abstraction Levels, https://arxiv.org/pdf/ 2409.06509, 2024. [19] OpenAI, Learning to reason with llms, https://openai.com/index/learning-to-reason-with-llms/, 2024. [20] L. Ouyang, J. Wu, X. Jiang, D. Almeida, C. Wainwright, P. Mishkin, C. Zhang, S. Agarwal, K. Slama, A. Ray, et al, Training language models to follow instructions with human feedback, Advances in Neural Information Processing Systems, 35:2773027744, 2022. [21] G. PaaB, S. Giesselbach, Foundation Models for Natural Language Processing, Springer Press, 2023. [22]M. Pagliardini, Daniele Paliotta, Martin Jaggi, François Fleuret, Faster Causal Attention Over Large Sequences Through Sparse Flash Attention, https://arxiv.org/pdf/2306.01160, 2023. [23] O. Press, N. A. Smith, M. Lewis, Train Short, Test Long: Attention with Linear Biases Enables Input Length Extrapolation, https://arxiv.org/pdf/2108.12409, 2021. [24] R. Rafailov, A. Sharma, E. Mitchell, S. Ermon, C. D. Manning, C. Finn, Direct Preference Optimization: Your Language Model is Secretly Reward Model, https://arxiv.org/pdf/2305.18290, 2023. [25] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer, Journal of Machine Learning Research, 21:167, 2020. [26]R. Rubinstein, The Cross-Entropy Method for Combinatorial and Continuous Optimization, Methodology And Computing in Applied Probability, 1999. [27] J. Schulman, F. Wolski, P. Dhariwal, A. Radford, and O. Klimov, Proximal policy optimization algorithms, arXiv preprint arXiv:1707.06347, 2017. [28] M. Schuster and K. Nakajima. Japanese and Korean Voice Search. In: 2012 IEEE ICASSP, 2012. [29] N. Shazeer, Fast transformer decoding: One write-head is all you need, https://arxiv.org/abs/1911.02150, 2019. [30] N. Shazeer, GLU variants improve transformer, https://arxiv.org/abs/2002.05202, 2020. [31] X. Song, A. Salcianu, Y. Song, D. Dopson, D. Zhou, Fast WordPiece Tokenization, https://arxiv.org/pdf/ 2012.15524, 2020. 15 [32] J. Su, Y. Lu, S. Pan, A. Murtadha, B. Wen, Y. Liu, RoFormer: Enhanced Transformer with Rotary Position Embedding, https://arxiv.org/pdf/2104.09864, 2021. [33] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, L. Kaiser, I. Polosukhin, Attention Is All You Need, https://arxiv.org/pdf/1706.03762.pdf, 2017. [34] J. Wang, Tutorial on LLM Reasoning: Relevant methods behind ChatGPT o1, https://github.com/ openreasoner/openr/blob/main/reports/Tutorial-LLM-Reasoning-Wang.pdf, 2024. Appendix A: Pseudo Code for Staircase Adaptive Quantization (SAQ) Algorithm of KV Cache in Multi-Query Attention (MQA) Algorithm: SAQ Pre-fill and Decoding Algorithm Parameters: group size , segment size , quantization options qn , quantization bits Bi Procedure : Preﬁll Input: ℜlpromptd , XK = WK XV = WV , XVg = XV[: lprompt ] XVr = XV[lprompt :] sn = lprompt //S if : sn qn XVg,i = XV[lprompt (i + 1) : lprompt ] for in range[1: sn ] XVg,sn1 = XV[: lprompt (sn 1) ] else: XVg,i = XV[lprompt (i + 1) : lprompt ] for in range[1: qn 2 ] , numGroup = //G ) for in range[1: sn ] , numGroup = //G ) for in range[1: qn 1 ] XVg,qn1 = XV[: lprompt (qn 1) ] if sn qn : Q(XVg,i) = GrpQuant(XVg,i , dim = token, qbits = Bi+1 else: Q(XVg,i) = GrpQuant(XVg,i , dim = token, qbits = Bi+1 Q(XKg), XKr = Kquant(XK) KV cache Q(XKg), XKr, Q(XVg), XVr Return XK, XV end 16 Procedure : Decoding Input: KV cache, ℜ1d tQ = tWQ, tK = tWK, tV = tWV Q(XKg), XKr, Q(XVg), XVr KV cache XKr = Concat([XKr, tK], dim = token) XVr = Concat([XVr, tV], dim = token) if : len(XKr) = = Q(XKr), = Kquant(XKr) sn = len(Q(XKg))//S if : sn (qn 2) Q(XKg,i) = Q(XKg)[i : (i 1) S], dim = token , for in range[1: ] sn XKg,i = GrpDeQuant(Q(XKg,i) , dim = channel, qbits = Bi+1 , numGroup = //G ) for in range[1: ] sn Q(XKg,i) = GrpQuant(XKg,i , dim = channel, qbits = Bi+2 else: , numGroup = //G ) for in range[1: ] sn Q(XKg,i) = Q(XKg)[i : (i 1) S], dim = token , for in range[1: qn 2 ] Q(XKg,qn1) = Q(XKg)[: (qn 2) ] XKg,i = GrpDeQuant(Q(XKg,i) ] , dim = channel, qbits = , numGroup = //G ) for in range[1: Bi+1 qn 2 XKg,qn1 = GrpDeQuant(Q(XKg,i) , dim = channel, qbits = , numGroup = Bi+ (sn qn + 2)S //G ) Q(XKg,i) = GrpQuant(XKg,i , dim = channel, qbits = Bi+2 , numGroup = //G ) for in range[1: qn 2 ] Q(XKg,qn1) = GrpQuant(XKg,qn1 , dim = channel, qbits ="
        },
        {
            "title": "Bqn",
            "content": ", numGroup = (sn qn + 2)S //G ) if : sn (qn 1) Q(XKg) = Concat([Q(XKg,sn), . . . , Q(XKg,1, Q(XKr)], dim = token) else: Q(XKg) = Concat([Q(XKg,qn1), . . . , Q(XKg,1), Q(XKr)], dim = token) XKr empty tensor 17 end if : len(XVr) > Q(XV ) = GrpQuant(XVr[: R] Q(XVg) = Concat([Q(XVg), Q(X Vr)], dim = token) , dim = token, qbits = , numGroup = //G ) B2 if : (len(Q(XVg)) % S) = = 0 sn = len(Q(XVg))//S if sn (qn 1) : Q(XVg,i) = Q(XVg)[i : (i 1) S], dim = token , for in range[2: ] sn XVg,i = GrpDeQuant(Q(XVg,i) , dim = token, qbits = Bi , numGroup = //G ) for in range[2: ] sn Q(XVg,i) = GrpQuant(XVg,i , dim = token, qbits = Bi+1 , numGroup = //G ) for in range[2: ] sn else: Q(XVg,i) = Q(XVg)[i : (i 1) S], dim = token Q(XVg,qn1) = Q(XVg)[: (qn 2) ] , for in range[2: qn 2 ] XVg,i = GrpDeQuant(Q(XVg,i) qn 1 ] , dim = token, qbits = Bi , numGroup = //G ) for in range[2: Q(XVg,i) = GrpQuant(XVg,i , dim = token, qbits = , numGroup = ) for in range[2: //G Bi+1 qn 1 ] if : sn (qn 1) Q(XVg) = Concat([Q(XVg,sn), . . . , Q(XVg,1, Q(XVr)], dim = token) else: Q(XVg) = Concat([Q(XVg,qn1), . . . , Q(XKg,1), Q(XVr)], dim = token) XVr XVr[S :] end = Concat([tQQ(XKg)T, tQXT Kr], dim = token) Ag = Softmax(A)[: S], Ar = Softmax(A)[S :] tO = AgQ(XVg) + Ar XVr KV cache Q(XKg), XKr, Q(XVg), XVr 18 return tO end function Kquant XK ℜld ( ): = % , XKg = XK[: r] XKr = XK[l :] sn = //S if : sn (qn 1) XKg,i = XK[(l r) (i + : (l r) (i 1) ] for in range[1: ] sn else: XKg,i = XK[(l r) : (l r) (i 1) ] for in range[1: qn 2 ] XKg,qn1 = XK[: (l r) (qn 2) ] if : sn (qn 1) Q(XKg,i) = GrpQuant(XKg,i , dim = channel, qbits = , numGroup = Bi+1 //G ) for in range[1: ] sn else: Q(XKg,i) = GrpQuant(XKg,i , dim = channel, qbits = , numGroup = ) for in range[1: //G Bi+1 qn 2 ] Q(XKg,qn1 ) = GrpQuant(XKg,qn1 , dim = channel, qbits = , numGroup ="
        },
        {
            "title": "Bqn",
            "content": "(l (qn 2) r)//G ) if : sn (qn 1) Q(XKg) = Concat([Q(XKg,1), . . . , Q(XKg,sn )], dim = token) else: Q(XKg) = Concat([Q(XKg,1), . . . , Q(XKg,qn1 )], dim = token) return Q(XKg), XKr end"
        }
    ],
    "affiliations": []
}