{
    "paper_title": "Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance",
    "authors": [
        "Jingwei Zuo",
        "Maksim Velikanov",
        "Ilyas Chahed",
        "Younes Belkada",
        "Dhia Eddine Rhayem",
        "Guillaume Kunsch",
        "Hakim Hacid",
        "Hamza Yous",
        "Brahim Farhat",
        "Ibrahim Khadraoui",
        "Mugariya Farooq",
        "Giulia Campesan",
        "Ruxandra Cojocaru",
        "Yasser Djilali",
        "Shi Hu",
        "Iheb Chaabane",
        "Puneesh Khanna",
        "Mohamed El Amine Seddik",
        "Ngoc Dung Huynh",
        "Phuc Le Khac",
        "Leen AlQadi",
        "Billel Mokeddem",
        "Mohamed Chami",
        "Abdalgader Abubaker",
        "Mikhail Lubinets",
        "Kacper Piskorski",
        "Slim Frikha"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 0 3 ] . [ 1 8 4 4 2 2 . 7 0 5 2 : r Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance 2025-07-"
        },
        {
            "title": "Falcon LLM Team",
            "content": "https://huggingface.co/tiiuae https://github.com/tiiuae/falcon-h1 Abstract: In this report, we introduce Falcon-H1, new series of large language models (LLMs) featuring novel hybrid architecture designs that are optimized for both high performance and efficiency across broad spectrum of use cases. Unlike previous Falcon models, which were built solely on either Transformer or Mamba architectures, the Falcon-H1 series is based on parallel hybrid architecture that combines the strengths of the Transformer-based attention mechanism with State Space Models (SSMs), known for their superior long-context memory and computational efficiency. We also systematically revisited nearly every aspect of model design, data strategy, and training dynamicschallenging several conventional practices in the domain. To support wide range of deployment scenarios, the Falcon-H1 series is released in rich set of configurations, including both base and instruction-tuned models at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameter scales. Quantized versions of the instruction-tuned models are also available. In total, over 30 model checkpoints can be accessed via Hugging Face Hub. Our comprehensive evaluations demonstrate that Falcon-H1 models consistently set new performance benchmarks through exceptional parameter and training efficiency. The flagship FalconH1-34B-Instruct rivals or outperforms leading models up to the 70B scale, such as Qwen3-32B, Qwen2.5-72B and Llama3.3-70B, despite being approximately half the size and trained on fraction of the data. This parameter efficiency is even more pronounced at smaller scales, where our 1.5B-Deep model achieves performance competitive with state-of-the-art 7B10B models, FalconH1-0.5B delivers performance on par with typical 7B models from 2024. These models demonstrate leadership across wide range of tasks, including reasoning, mathematics, multilingual, instructionfollowing, and scientific knowledge. Combined with support for extended context windows of up to 256K tokens and multilingual coverage across 18 languages, Falcon-H1 models are well-suited for wide array of applications. All Falcon-H1 models are released under permissive open-source license1, reinforcing our commitment to accessible, high-impact AI research and development."
        },
        {
            "title": "1 Introduction",
            "content": ""
        },
        {
            "title": "2 Architecture",
            "content": "4 2.1 Channel Allocation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 7 2.2 SSM-Specific Parameters Ablations . . . . . . . . . . . . . . . . . . . . . . . . . . . . 2.3 Challenging Conventional Components . . . . . . . . . . . . . . . . . . . . . . . . . . 11 1https://falconllm.tii.ae/falcon-terms-and-conditions.html Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance"
        },
        {
            "title": "3 Pretraining",
            "content": "17 3.1 Pretraining Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17 3.1.1 Data Sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 3.1.2 Data Strategy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 3.2 Training Dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.2.1 Training Stability . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23 3.2.2 Effective Learning Rate and Effective Weight Decay . . . . . . . . . . . . . . 24 3.2.3 Maximal Update Parametrization (µP ) with Tunable Multipliers . . . . . . . 28 3.2.4 Other Aspects: Batch Scaling, Rampup, Warmup . . . . . . . . . . . . . . . . 31 3.3 Pretraining Infrastructure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 3.3.1 Scaling Dynamics of Data Parallelism . . . . . . . . . . . . . . . . . . . . . . 33 3.3.2 Mixer Parallelism (MP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 3.3.3 Context Parallelism (CP) . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "4 Post-trainining",
            "content": "37 4.1 Post-training Data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 4.2 Supervised Fine-Tuning (SFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 4.3 Direct Preference Optimization (DPO) . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "5 Evaluation",
            "content": "39 5.1 Base Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 5.2 Instruct Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 5.3 Model Efficiency . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "9 Acknowledgments",
            "content": "A Languages used for training Falcon-H1 tokenizers"
        },
        {
            "title": "B Scalar stochastic dynamics with weight decay",
            "content": "C Tuning µP multipliers 53 53 54 54"
        },
        {
            "title": "68\nD.1 Multilingual Evaluations - Base Models\n. . . . . . . . . . . . . . . . . . . . . . . . . 68\nD.2 Multilingual Evaluations - Instruct Models . . . . . . . . . . . . . . . . . . . . . . . . 73\nD.3 Long-context Evaluations - Instruct Models . . . . . . . . . . . . . . . . . . . . . . . 77",
            "content": "2 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance"
        },
        {
            "title": "80\nE.1 Synthetic English Data Topics\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\nE.2 Programming Languages . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80\nE.3 Code Quality Classifier . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81",
            "content": "1."
        },
        {
            "title": "Introduction",
            "content": "The rapid progress in the development of large foundation modelsparticularly large language models (LLMs)has been driven by advances in architectural design, scaling strategies, and training paradigms. Beginning with Transformer-based architectures (Vaswani et al., 2017) and expanding through massive-scale pretraining and techniques such as supervised fine-tuning (SFT) and reinforcement learning from human feedback (RLHF)(Ouyang et al., 2022). key limitation of the vanilla Transformer lies in its quadratic complexity with respect to input sequence length. To address this, recent research has explored more efficient alternatives to traditional attention mechanisms, such as Multi-head Latent Attention (MLA), featured in the DeepSeek series (Liu et al., 2024a,b). In parallel, several novel architectures have been proposed to move beyond Transformers entirely, including Griffin (De et al., 2024), RWKV (Peng et al., 2023), Titans (Behrouz et al., 2024), and Mamba (Gu & Dao, 2023), which offer comparable or superior performance in certain tasks with greater computational or memory efficiency. These advances have given rise to new class of hybrid models that combine attention mechanisms with state-space models (SSMs), taking advantage of their complementary strengths: attention excels at modeling long-range dependencies, while SSMs provide efficient sequence mixing. This hybrid paradigm has gained popularity in models such as Jamba (Lieber et al., 2024; Team et al., 2024), Samba (Ren et al., 2024), Zamba (Glorioso et al., 2024), and Hymba (Dong et al., 2024). Building on these insights, we introduce Falcon-H1an innovative series of large language models that feature novel parallel hybrid architecture integrating Transformer-style attention with Mamba-based state-space models (SSMs). Distinct from other leading open-weight LLM seriesincluding LLaMA (Grattafiori et al., 2024), Mistral (Jiang et al., 2023), Qwen (Yang et al., 2024a,b), and DeepSeek (Liu et al., 2024a,b), Falcon-H1 is explicitly architected around this hybrid design, harnessing the complementary strengths of both mechanisms to deliver faster inference, lower memory usage, and state-of-the-art performance across wide array of benchmarks. The series includes both pre-trained and instruction-tuned variants across seven scales: 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. In addition to bfloat16-precision models, we also provide quantized versions in multiple precisions to support efficient deployment across diverse hardware environments. Notably, our flagship model, Falcon-H1-34B-Instruct, achieves competitive or superior results compared to the strongest open-weight models to date, such as Qwen3-32B, Qwen2.5-72BInstruct and LLaMA3.3-70B-Instructdespite being approximately half the size. Below, we show the key features of Falcon-H1: Innovative Hybrid Architecture: We combine attention and Mamba-2 heads in parallel within our hybrid mixer block. Importantly, the amount of attention and mamba heads can be adjusted independently, allowing for an optimal attention and SSM ratio. This hybrid design enables faster inference, lower memory usage, and strong generalization across tasks. Wide Range of Model Sizes: The Falcon-H1 family includes base, instruction-tuned and quantized variants in various sizes0.5B, 1.5B, 1.5B-deep, 3B, 7B and 34Bdesigned to meet the needs of diverse usages and deployment scenarios, from edge devices to large-scale systems. 3 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Multilingual by Design: Supports 18 languages out of the box, including Arabic (ar), Czech (cs), German (de), English (en), Spanish (es), French (fr), Hindi (hi), Italian (it), Japanese (ja), Korean (ko), Dutch (nl), Polish (pl), Portuguese (pt), Romanian (ro), Russian (ru), Swedish (sv), Urdu (ur), and Chinese (zh) with scalability to 100+ languages, thanks to our multilingual tokenizer trained on diverse language datasets. Compact Models, Big Performance: Falcon-H1-0.5B delivers performance on par with typical 7B models from 2024, while Falcon-H1-1.5B-Deep rivals many of the current leading 7B10B models. Each Falcon-H1 model is designed to match or exceed the performance of models at least twice its size, making them ideal for low-resource and edge deployments without compromising on capability. 256K Context Support: Falcon-H1 models support up to 256K context length, enabling applications in long-document processing, multi-turn dialogue, and long-range reasoning, with exceptional long context performance and greater computational and memory efficiency, FalconH1 provides great balance between performance and resource cost. Robust Data and Training Strategy: Falcon-H1 employs redesigned training approach that maximizes the value of high-quality but limited data. Additionally, the training process scales smoothly across model sizes through customized Maximal Update Parametrization (µP ) recipe, specifically adapted for this novel architecture. 2. Architecture Hybrid models combining SSM and attention mechanisms have emerged recently as promising direction. The classical approach to integrating these components was through sequential designs (Team et al., 2024; Ren et al., 2024; Glorioso et al., 2024), where one module feeds into the other in series across layers. More recently, parallel designs (Dong et al., 2024) have emerged as an alternative integration strategy, where both modules see the same input and their outputs are fused/concatenated before the block projection. For Falcon-H1, we adopt the parallel formulation as shown in Figure 1. Our parallel hybrid design has the freedom to choose the ratio of attention and SSM channels, and we are able to keep small share of attention heads for precision while SSMs handle most of the work. However, since Mamba architecture is relatively new, their architectural hyper-parameters remain under-explored compared to well-established transformer designs. This necessitates systematic investigation of design choices to optimize performance. We therefore perform detailed ablations and coarse grid searches on 300M to 1.5B parameter proxy models to understand the impact of key architectural decisions. We sweep critical settings and record both training loss and throughput metrics. These experiments inform the final configuration summarized in Table 1 and provide insights for the broader SSM research community. Model Params (B) Layers # Vocab dmodel Heads (Q/KV, SSM) dhead (Attn/SSM) dstate Context Len. # Tokens Falcon-H1-0.5B Falcon-H1-1.5B Falcon-H1-1.5B-Deep Falcon-H1-3B Falcon-H1-7B Falcon-H1-34B 0.52 1.55 1.55 3.15 7.59 33.6 36 24 66 32 44 72 32,778 65,536 65,536 65,536 130,048 261,120 1024 2048 1280 2560 3072 5120 8/2, 24 8/2, 48 6/2, 24 10/2, 32 12/2, 24 20/4, 32 64/64 128/64 128/64 128/128 128/128 128/ 128 256 256 256 256 256 16K 128K 128K 128K 256K 256K 2.5T 3T 3T 2.5T 12T 18T Table 1: Model architecture details of the Falcon-H1 series. Embedding and projection layers are untied for all the models. 4 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Figure 1: Falcon-H1 architecture. Attention and SSM run in parallel within each block; their outputs are concatenated before the blocks output projection. The number of SSM/Attention heads can be flexibly tuned."
        },
        {
            "title": "2.1 Channel Allocation",
            "content": "A key aspect of our hybrid design is the concatenation of attention and SSM channels, which introduces an additional degree of freedom: the ability to independently vary the number of attention and SSM channels within each hybrid layer. This flexibility has not been explored in previous hybrid designs. For instance, (Dong et al., 2024) averages the outputs of attention and SSM channels, which requires both to have identical dimensions. To fully leverage this channel allocation flexibility, we conducted systematic study of various allocation strategiesincluding different configurations of MLP channels and the positioning of the MLP block relative to the mixer. Experimental settings. To provide fair comparison across different channel allocation strategies, we adopted the following experimental design. Let dssm, dattn, dMLP denote the variable numbers of inner channels for the SSM, attention, and MLP blocks, respectively. The MLP channel dimension dMLP can be varied freely without any constraints, as shown in Figure 1. For the attention and SSM blocks, we vary the number of query heads while keeping the head dimension, number of key-value (KV) heads, and number of attention groups fixed. Then, we divide the total available channels into 8 chunks, which can be freely allocated across the SSM, attention, and MLP modules. This results in the following parameterization: dssm = αS 4096, dattn = αA 6144, dMLP = αM 4864, (1) 5 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance where the chunk fractions αS, αA, αM are chosen from αS, αA, αM { 1 8 , 2 8 , 8 , 4 8 , 5 8 , 6 8 }, αS + αA + αM = 1. (2) In this experiment, the base amount of channels per chunk in (1) is set differently for the SSM, attention, and MLP blocks, with ratio 4096 : 6144 : 4864 = 2 : 3 : 2.375. The reason behind this setting is to balance parameter count and efficiency of different allocations. Recall that the number of matrix layer parameters that scale with inner channels 2 are given by 3dssmd, 2dattnd, 3dMLPd for the respective blocks, where is the hidden dimension of the model. Then, the ratio 2 : 3 : 2 of base channels would keep the number of parameters constant for different blocks. From this fixed parameter ratio, we slightly increased MLP base channels to take into account the lower computational cost of MLP compared to token mixing attention and SSM. This led to the adjusted ratio of 2 : 3 : 2.375, which we instantiate in practice as 4096 : 6144 : 4864 in this experiment. In addition to channel allocations, another choice we have to make when assembling the hybrid model block is how to position SSM, attention, and MLP blocks with respect to each other. Similarly to parallel and sequential transformer design (Zhao et al., 2019; Chowdhery et al., 2022; He & Hofmann, 2024), we have the same choice of whether each pair of blocks should be processed in parallel or one after another. We have tested 3 configurations: fully parallel (SAM), semi-parallel (SA_M), and fully sequential (S_A_M), with the respective forward passes of single model block SAM: rl+1 = rl + MLP + MLP SA_M: rl+1 = l + MLP S_A_M: rl+1 = (Nl(rl)) + attn (r (N l)), (r (N )), = rl + attn = r + attn (Nl(rl)) + SSM (Nl(rl)) + SSM (Nl(rl)) = rl + SSM l)), (r (N (Nl(rl)) (3) (4) (5) (Nl(rl)) Here rl is the residual at the beginning of lth model block, are intermediate residuals in the middle of sequential model blocks; SSM denote SSM, attention and MLP forward , passes; and Nl, are RMSnorms applied at the beginning of each block and shared for the blocks arranged in parallel to each other. Switching between these 3 block configurations almost does not change the parameter count in the model because SSM/attention/MLP blocks are rearranged as whole without modifying the insides of each block. The only extra parameters come from the necessity to add new RMSnorm layers for each additional sequential computation within the model block. , MLP , attn , l, We have compared all the configurations described above for relatively deep model with = 60 layers, hidden dimension = 1280, resulting in approximately 1.2B parameters. All other training and architecture hyperparameters were identical, and we measured the loss after 70GT of training. The results. Our channel allocation experiments have two parts, focusing on channel allocations and then on block arrangement. First, for fully parallel SAM blocks arrangement we examined all 21 admissible (αS, αA, αM ) partitions, according to (2). The resulting loss values are plotted on figure 2 (left). We see clear separation of magnitude between the impact of the number of attention channels and SSM MLP channel switching. Having more attention channels significantly degrades the performance, while SSM MLP channel switching has noticeable but much weaker effect. We have also confirmed similar behavior for the SA_M block arrangement on smaller number of runs. 2For attention and MLP all parameters that scale with inner channels are located in the matrix layers. However, Mamba2 SSM block contains few vector-like parameters that scale with dssm, for example, the additional RMSnorm applied to inner channels after SSM computation. As the number of such extra parameters is tiny compared to the matrix layer ones, we neglect them when balancing parameter counts for different channel allocations. 6 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Figure 2: (Left): The loss of fully parallel SAM hybrid block configuration for all possible (αS, αA, αM ) channel allocations according to (1),(2). (Right) The loss of all 3 considered block configurations SAM (3), SA_M (4), and S_A_M(5) for fixed optimal attention allocation αA = 1 8 and varied SSM/MLP channel allocation. For the second part of the experiment, we compare all three SAM, SA_M, S_A_M block arrangements while fixing attention channels to the minimum αA = 1 8 and varying SSM/MLP channels so that αS +αM = 7 8 . The resulting loss values are plotted on figure 2 (right). We see that semi-parallel SA_M configuration provides the best results, (αS, αA, αM ) = ( 2 8 ) being the optimal channel allocation. However, the dependence on SSM/MLP allocations is flat near optimum. Interestingly, as block configuration becomes more sequential SAM SA_M S_A_M, the optimal SSM fraction reduces as 3 8 . At the moment, we dont have an explanation of this behavior. 8 1 8 2 Based on the experiment results above, for Falcon-H1 models, we adopted SA_M block configuration with channel allocations roughly following 2 : 1 : 5 ratio, with slight deviation for different model sizes, which is possible thanks to flat dependence on SSM/MLP allocations near optimum. 8 , 8 ,"
        },
        {
            "title": "2.2 SSM-Specific Parameters Ablations",
            "content": "We start by revisiting the Mamba2 design (Dao & Gu, 2024) of SSM block we employ for Falcon-H1 models. At the core of the block is token mixing mechanism that maps an input sequence RL of length to = SSM(x, B, C, dtAlog, D) RL via recurrent mechanism involving hidden state RdstateL ht+1 = At ht + Btdttxt, (6) Here Bt determines the vector Btdttxt to be written into the SSM hidden state, At determines the fraction of previous hidden state to be forgotten, Ct is reading projector, and the scalar controls the direct connection between input and output sequences, by passing the hidden state recurrent computation. The time step dtt controls both the writing intensity via Btdttxt, and forgetting intensity via the parametrization of At = exp(cid:2)eAlog dtt ht + Dxt. (cid:3). yt = It is often convenient to view SSM operation (6) as implementing an linear sequence transformation with casual attention matrix Mts yt = Mts xs, st Mts = t Bs dts i=s+1 Ai + δts, i=t+ := 1. (7) 7 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance The recurrence (6) describes the sequence transformation of single SSM channel. Mamba2 organizes all the dssm = dheadnh channels first into nh heads of dimension dhead, and then further unites the heads into ng groups with some of the parameters shared within each group. Such organization into heads and groups is similar to grouped query attention (GQA) (Ainslie et al., 2023). Specifically, all the parameters Bt, Ct, dtt, At, are broadcasted along the head dimension, and Bt, Ct are further shared within each group. The key distinction of Mamba2 block compared to early SSM designs is that parameters defining recursion (6) are input dependent, allowing for much stronger expressivity. First, the input to the block RdL goes through linear transformation, then selectively through causal depthwise 1D convolution conv1d(), and finally through the SiLU activation function. Denoting concatenation with merged letters, we write the full input transformation as xBC = SiLU (cid:0)conv1d(cid:0) exez eB eCfdt = WxzBCdtu, ex eB eC(cid:1)(cid:1), dt = Softplus(fdt + b), WxzBCdt R(2dssm+2ngdstate+nh)d, = SiLU(ez). (8) (9) Here, Rnh is head-wise bias, and is the gate multiplied element-wise with SSM output yg = z, similarly to the gated MLP. Finally, yg goes into grouped RMSnorm3, followed by the output projection layers Wo Rddssm to produce the output of Mamba2 block. We note that parameters Alog, D, are, however, static learnable weights that are not input dependent. In the remaining paragraph, we describe ablations for various dimensions of the Mamba2 block described above: head dimension dhead = dssm/nh, the number of groups ng, the recurrent state dimension dstate, the depthwise 1-D convolution kernel size, and the scan chunk size used by SSD algorithm (Dao & Gu, 2024) implementing the recursion (6). State dimension vs. group count. Prior work shows that enlarging the SSM state size dstate consistently boosts accuracy, but at non-trivial efficiency cost (Gu & Dao, 2023; Liu et al., 2024d; Stan & Rhodes, 2024; Mitra et al., 2025). Systematic studies of the accuracyefficiency frontier remain sparse. We therefore ran twodimensional grid search over the state dimension dstate and the number of groups ng. To disentangle their effects from model size, we can fix the total number of parameters by fixing budget = dstate ng while varying (dstate, ng). We swept five budgets {4, 16, 64, 256, 1024}. As shown in Fig. 3, validation accuracy rises almost exclusively with larger dstate; varying ng has only marginal impact. Thus, the best configuration within any budget uses the smallest feasible ng and the largest possible dstate. Conversely, training throughput deteriorates with increasing dstate, with pronounced efficiency apex around dstate = 16. Note that all experiments were conducted at sequence length of 2048. Because longer sequences require larger state to retain historical information, we take (ng, dstate) = (1, 256) for the final models as the best compromise. Since Falcon-H1-34B was trained with tensor parallelism (TP) = 4 and mixer parallelism (MP; see 3.3.2), we fixed the number of groups to ng = 2 so that it is divisible by TP/2. Head dimension dhead. To isolate the effect of the SSM head size dhead, we trained variants with dhead {16, 64, 256} while keeping dssm = dheadnh so the parameter count stays roughly constant. We observe 102 change in training cross-entropy, with clear gain at larger heads (see Fig. 4a). 3grouped RMS normalization layer is required here to enable the usage of tensor parallelism (TP), which splits SSM channels across different devices. 8 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance (a) (b) Figure 3: Hyperparameter optimization landscapes for SSM number of groups and state dimension size. (a) Loss surface showing performance relative to global minimum across number of groups and d_state size. (b) Relative throughput surface as fraction of maximum performance. Dashed lines indicate iso-parameter curves (ng ds = constant), implying constant total parameter count. Red stars mark optimal configurations for each computational budget, revealing distinct trade-offs between model quality and efficiency. Throughput is more sensitive: dhead < 32 reduced GPU utilization, whereas dhead 64 maintained optimal efficiency. To our knowledge, no prior work reports an explicit ablation of dhead for SSMs; existing sources simply pick values (e.g., 64 or 128) and discuss kernel limits (Dao, 2024a,b). Larger head dimensions therefore yield more favorable accuracy and efficiency. Depthwise causal 1-D convolution (convdim). To our knowledge, no prior work reports an ablation over the depthwise causal Conv1d kernel size inside Mamba-style SSM blocks; existing papers simply fix = 4 (Pei & others, 2025; Chao et al., 2024). The reference causal_conv1d CUDA kernel itself only supports {2, 3, 4} (Dao-AILab, 2023; Hoang & Mamba contributors, 2024). We therefore re-implemented the kernel to handle sizes up to 32 and swept {2, 4, 8, 16, 32}. Kernel size 4 indeed minimized validation loss, whereas both smaller and larger filters degraded accuracy, so we keep kernel_size = 4 in the final models (see Fig. 4b). Chunk size (cs). The SSD kernel processes long sequence in blocks of cs tokens. The efficiency scales favorably with cs until two limits emerge: 1. Launch overhead. Very small chunks (cs < 64) trigger many kernel launches and under-utilise the GPU. 2. Memory pressure. When cs > 256 the cross-chunk prefix-sum kernel (dA_cumsum) no longer fits in on-chip SRAM and becomes memory-bound, lowering the efficiency. broad plateau therefore appears at cs {128, 256}. This matches the implementation guidance in the Mamba official codebase where the Triton kernels are tuned for power-of-two values and default to cs = 256. We fix cs = 256 for all subsequent experiments to maximize throughput while retaining numerical stability. Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance (a) (b) Figure 4: Model accuracy and computational efficiency across architectural dimensions. (a) Similar analysis for attention head dimensions, showing that larger head dimensions provide both better computational efficiency and lower loss. (b) Loss (purple line, left y-axis) and relative computational efficiency (blue line, right y-axis) as functions of convolution dimension. The analysis reveals an optimal trade-off at dimension 4, where the model achieves minimal loss while maintaining high efficiency. Hidden State Resetting in Mamba. When several documents are concatenated inside one long sequence, the tail of document (0 < Tk) leaks into the head of document k+1 through the product of in (7). This cross-doc leakage violates the independence assumption of languagemodel training and especially causes semantic contamination between unrelated contexts. For attention, simple fix is block-diagonal (segmented) mask that zeroes attention scores across document boundariesoften called cross-document masking (Grattafiori et al., 2024; Team et al., 2025). For recurrent architectures, such cross-document bleeding can be avoided by resetting the hidden state at document boundaries. Let rt = 1 signify that token is the first token of new document in the sequence. This binary indicator is derived from the data loaders position tensor and marks document boundaries within packed sequence. To reset hidden-state on document boundary we need to force At = 0 when is at the document edge, we do so by injecting large negative value 80 channel-wise into the SSM parameter vector before exponentiation at positions where rt = 1: Ai = exp(cid:2)eAlog dti + ri (80)(cid:3) (0, A, ri = 1, ri = 0. Thus, at boundary, ht+1 = 0 ht + xt = xt, perfectly resetting the hidden state in single step with no additional compute. Subsequent tokens use the standard transition weights A. This resetting scheme does not add any compute or memory overhead while remaining gradient-safe because the 80 bias is constant, so gradients propagate normally through exp and eAt. Moreover, it is numerically stable: exp(80) 1035 lies above the FP16/BF16 underflow threshold ( 1045) yet empirically zeros the hidden state without training instabilities. Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance"
        },
        {
            "title": "2.3.1 RoPE Base Frequency",
            "content": "For Falcon-H1 models, we have used an unconventionally high value = 1011 of the RoPE base frequency. Below, we present our reasoning process to arrive at this value. We have started the training of 7B and 34B models with standard value = 104 and sequence length Lseq = 8192. In the middle of the training, we increased sequence length to Lseq = 16384 but observed drop in the model evaluations and revoked the change. This drop was unexpected since we did not change the data mixture, and the only effect of Lseq increase amounts to fewer training samples being cut. Suspecting RoPE base frequency as the main parameter that interacts with the training sequence length, we have increased the base frequency to = 106. This change resulted in an immediate boost in model evaluations, and another boost after increasing sequence length again to Lseq = 16384. To further investigate the impact of base frequency b, we ran sweep on Falcon-H1 0.5B model, depicted in Figure 5a. At smaller b, few orders of magnitude around the model sequence length, the training loss L(b) steeply depends on b, with lower values being extremely suboptimal. Earlier drop of evaluations on 7B/34B models when increasing Lseq can be interpreted as moving up the L(b) curve due to the decrease of relatively to Lseq. At larger the curve L(b) flattens limit . Our chosen and slowly increases, reflecting the NoPE (no positional embeddings). value = 1011 roughly corresponds to the optimum of L(b) curve. We stress, though, that optimal does not need to be estimated very accurately due to the flatness of L(b) for large b. Applying = 1011 to 7B/34B models resulted in another increase of evaluation scores. Finally, let us point out the advantage of using large base frequency when scaling the sequence either during continual pretraining or inference. RoPE assigns different frequencies θk = b2k/dhead to different dimensions within query and key vectors. When the value of is comparable to the original sequence length Lseq, further increase of Lseq requires reassigning frequencies within QK dimensions to allocate some space for smaller θ (or larger wavelengths θ1) needed to handle longer sequences. Different strategies of this reassigning, such as Position Interpolation (Chen et al., 2023), NTK-aware (bloc97, 2023a) or NTK-by-parts (bloc97, 2023b), improve performance on larger sequences but still deform the original assignment θk the model has adapted to during training. Fortunately, using extremely large during training leaves many dimensions effectively unassigned, since the respective large sequences were never seen during training. In that case, no RoPE modifications are required when increasing sequence length beyond the training value, making sequence length extension for Falcon-H1 models extremely simple. An interesting question is whether such large values are optimal only for hybrid models, where SSM part can take care of short-range dependencies, or can also work for transformer models."
        },
        {
            "title": "2.3.2 Width–Depth Trade-offs",
            "content": "Increasing the depth (number of layers) versus the width (hidden dimensionality) of decoder-only model presents distinct trade-offs in expressivity and efficiency. Depth provides more sequential composition of nonlinear transformations, enabling hierarchical feature learning and multi-step reasoning that shallow-but-wide model might require exponentially more neurons to emulate (Chen et al., 2024). In contrast, width expands the models capacity per layer, allowing it to encode more features in parallel. Depth thus increases representational power by adding layers of compositionality, while additional width increases the representational richness at each layer without increasing the number of sequential transformations. 11 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance (a) (b) Figure 5: (a) Dependence of the training loss on RoPE base frequency b. The dotted line shows the training sequence length for reference. First, we tried many base frequencies and measured the loss early in the training at 20GT. Then, we picked 3 characteristic base frequency values and measured the loss in much later training stage at 450GT, with the results being roughly similar to the early measurement. (b) Dependence of the training loss on the number of layers for fixed number of parameters. From an optimization and efficiency perspective, these choices have significant implications. Deep stacks are more sequential, which hampers parallelism and can slow down training and inference: each added layer introduces an additional serial step that cannot be parallelized in time, leading to higher latency. Wider layers, on the other hand, perform more computation in parallel within single layer (e.g., larger matrix multiplications), which modern hardware can exploit efficiently. Memory constraints also differ: very deep models must store activations for many layers during backpropagation (increasing memory usage proportional to depth). By contrast, widening layer adds no additional sequential operations: each layer can encode more information and shortens the longest gradient path, alleviating bottlenecks. The trade-off is higher peak-memory footprint. From training point of view, very deep networks demand careful architectural tweaks to remain trainable, whereas very wide networks may hit other limits such as memory bandwidth or diminishing returns in utilization of parameters. In practice, state-of-the-art LLM architectures balance width and depth to leverage the benefits of both. Modern decoder-only LLMs are typically built with dozens to 100 layers, each with very large hidden size, rather than choosing an extreme in one dimension. For example, LLaMA-3 (Grattafiori et al., 2024) scales from 8 to 70 parameters by increasing both depth and width layers with 12 288-dimensional hidden size. This co-scaling strategy ensures the model has sufficient depth to compose complex behaviors and sufficient width to maintain high capacity and parallel throughput. In fact, many design heuristics keep the networks depth and width growing in tandem with total size. For the Falcon-H1 series, we revisited the widthdepth trade-off under fixed 1.5 parameter budget. We performed joint sweep over hidden width dmodel and depth L, scaling the learning rate inversely with width following simple µP scaling (Yang et al., 2022) (η 1/dmodel) to stabilize training dynamics across configurations and omitted depth-scaling at this stage for simplicity. 12 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance We evaluated five architecture shapes: W1536L87, W1792L63, W2048L48, W2304L37, and W2560L30. Efficiency was measured in training giga-tokens per hour (gtok/h), and quality was assessed via pre-training cross-entropy. As shown in Figure 5b, greater depth yielded consistently higher overall quality. The 87-layer extreme W1536L87 variant clearly outperformed the wider W2560L30 one. From our empirical studies, it even matched/outperformed 3.0 and 7.0 reference models twice to 5 times its size. This accuracy boost came at cost: training throughput dropped by 2530 % gtok/h and inference slowed by comparable margin relative to the shallowest configuration. Because the depthwidth balance remains under-explored, we are releasing two 1.5 variantsFalcon-H1-1.5B (widthbalanced with 24 layers) and Falcon-H1-1.5B-Deep (deeper with 66 layers) to foster further investigation of this trade-off."
        },
        {
            "title": "2.4 Tokenizer",
            "content": "Considering the memory footprint and model performance acorss different model scales (Tao et al., 2024), we decided to train multiple tokenizers with different vocabulary size. Basically, we increase gradually the vocabulary size regarding model size as shown in Table 1. In this section, we show detailed studies and experimental results when building Falcon-H1 tokenizers."
        },
        {
            "title": "2.4.1 Empirical Studies",
            "content": "In the literature, tokenizer with high compression rate (i.e., low fertility score) across languagesincluding non-Latin scriptsis widely regarded as essential. By encoding equivalent text with fewer tokens, such tokenizers improve both training and inference efficiency, requiring fewer iterations to generate the same amount of text. However, compression alone does not fully capture the factors influencing end-to-end LLM performance. To address this, we conducted series of experiments to investigate how various tokenizer training strategies affect both proxy metrics and downstream outcomes. This was primarily evaluated using the fertility score (compression rate) and the average number of bytes per vocabulary token (expressiveness). Beyond these proxy metrics, we also performed full-scale training experiments to directly observe the impact of design choices that might not be evident through compression or expressiveness alone. Specifically, we explored the impact of scaling tokenizer training data, regex splitting patterns, handling of punctuation and digits, and inclusion of LATEX tokens. Will scaling training data impact tokenizer performance? To investigate how scaling training data affects tokenizer performance, we conducted an experients with six tokenizers trained on English text. We varied two factors: the training corpus sizes (1GB, 14GB, and 40GB) and the vocabulary size (65k and 135k). All other training parameters were held constant to isolate the impact of these variables. The results are summarized in Table 2. The relationship between corpus size and performance is non-monotonic and is conditioned on the vocabulary size. For 65k vocabulary, optimal performance is achieved with smaller corpora: the 1GB corpus yields the best compression, while the 14GB corpus maximizes bytes per token. Performance degrades at 40GB. Conversely, for 135k vocabulary, the 14GB corpus surpasses both smaller and larger corpora on both metrics. Conclusion: Simply increasing the training data volume does not guarantee better tokenizer. Instead, there is an optimal range of data that depends on the vocabulary size. 13 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Vocab Size Corpus Size Fertility Score Bytes per Token 65k 135k 1GB 14GB 40GB 1GB 14GB 40GB 1.4350 1.4443 1. 1.3887 1.3344 1.3964 8.2435 8.7274 8.3748 8.8912 9.2688 9.2577 Table 2: Tokenizer performance by corpus and vocabulary size. Best results for each vocabulary size are in bold. The arrows indicate the desired direction for each metric. How important is the Splitting RegEx? The Splitting RegEx is pattern rule used to break raw text into preliminary word-like units before applying the actual tokenization algorithm. In other words, it defines how the tokenizer initially segments text, influencing how efficiently it can compress data into tokens. For this analysis, we compared three publicly available splitting regex patterns used by different tokenizers: GPT-4o, LLaMA-3, and GPT-2. We trained all tokenizers with fixed vocabulary size of 131k on the same dataset, and then measured both the fertility score and the average number of bytes represented in the vocabulary. Splitting RegEx Fertility Score Bytes per Token GPT-2 GPT-4o LLaMA-3 1.3466 1.3209 1.3238 9.1019 8.7924 8.7003 Table 3: Impact of different splitting regex patterns on fertility score and average bytes per token. Best scores for each metric are in bold; arrows indicate the desired optimization direction. From the results shown in Table 3, we observe that while the choice of splitting regex does have measurable impact on both the fertility score and the average bytes per token, the differences between recent regex patterns such as GPT-4o and LLaMA-3 remain relatively small. Conclusion: The differences between splitting regex patterns used by modern tokenizers are minor. For practical purposes, it is advisable to adopt splitting regex from well-established and up-to-date tokenizer. Whether to apply Punctuation and Digits Splitting? Whether to apply punctuation and digit splitting remains an active topic of discussion in the community (Singh & Strouse, 2024), with notable impacts on domains such as mathematics and code. While metrics like fertility score and bytes-per-token are often used to evaluate tokenizers, we find they do not consistently predict downstream model performance. For example, as shown in Table 4, we trained two tokenizers on identical dataone applying individual digit splitting and one without. Although the latter achieves better (lower) fertility score, most recent studies (Yang et al., 2024b; Grattafiori et al., 2024) adopt digit splitting as standard practice, underscoring that such metrics alone may not fully capture tokenizer effectiveness. To resolve this ambiguity, we conducted controlled empirical study focused on downstream task performance. We trained three 1.8B Falcon-Mamba models (Zuo et al., 2024) on shared 14 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Splitting Setting Vocab Size Fertility Score Bytes per Token"
        },
        {
            "title": "Individual digits\nNo digits splitting",
            "content": "135k 135k 1.3209 1.2884 8.7924 8.9688 Table 4: Fertility Score (Compression rate, digits excluded on the test set) and average number of bytes for different splitting regular expressions. 280GT dataset (including 40GT of decay stage). The models differed only in their tokenizer configurations: Splits both digits and punctuation; Splits digits only; No specialized splitting for digits or punctuation. As shown in Figure 6, despite some score fluctuations inherent to training dynamics, the results on the HumanEval code benchmark (Chen et al., 2021) indicate clear trend: enabling both punctuation and digit splitting consistently leads to superior code generation performance. The benefits of punctuation splitting are further supported by qualitative analysis of tokenization in non-Latin languages. In languages like Chinese and Japanese, punctuation marks are often full-width characters that can be incorrectly merged with adjacent words if not explicitly separated. Figure 7 illustrates this phenomenon. Without punctuation splitting, the tokenizer merges characters with punctuation, producing semantically incoherent units. Figure 6: Model performance regarding different splitting strategies. Without Punctuation Splitting (Falcon3 tokenizer) Tokens are incorrectly merged with punctuation. 关闭 此 模式 后 您 将 无法 再 看到 它 如果您 丢失 了 它 则 必须 创建 一个新的 With Punctuation Splitting (Falcon-H1 tokenizer) Punctuation is correctly isolated, preserving word boundaries. 关闭 此 模式 后 您 将 无法 再 看到 它 如果您 丢失 了 它 则 必须 创建 一个新的 Figure 7: Qualitative comparison for Chinese sentence, with or without punctuation splitting. Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Conclusion: Splitting both digits and punctuation seems to be the most effective strategy. This approach enhances performance on code and math tasks and ensures more robust, semantically meaningful tokenization across diverse languages. Moreover, our results underscore that tokenizer design should be guided by the actual model performance, rather than relying solely on proxy metrics such as fertility score. How important are the common LATEX tokens? Given the prevalence of LATEX syntax in scientific and mathematical documents, we hypothesized that incorporating common LATEX commands directly into the tokenizers vocabulary would enhance models mathematical reasoning capabilities. The core principle is that representing frequent commands like frac or sqrt as single, atomic tokens simplifies the prediction task for the model, reducing the sequence length and compositional complexity of mathematical expressions. To test this hypothesis, we curated set of the most frequent LATEX commands, mainly from the Overleaf documentation to ensure comprehensive coverage of formatting, referencing, and mathematical functions. We then conducted controlled experiment by training two 1B-parameter Falcon-Mamba models (Zuo et al., 2024) on an identical, math-heavy data mixture of 280GT (including 40GT decay stage). One model used our baseline tokenizer, while the other used version where unused vocabulary slots were replaced with our curated set of LATEX tokens. The models were evaluated on four different math benchmarks: MATH-Hard (Hendrycks et al., 2021b), gsm8k (Cobbe et al., 2021a), math_qa (Amini et al., 2019) and minerva-math. As shown in Figure 8, the model trained with the LATEX-augmented tokenizer demonstrated consistent and notable performance improvement across most benchmarks. Figure 8: Performance on mathematical benchmarks for two 1B models during the training decay stage. The model trained with tokenizer augmented with specialized LATEX tokens (blue) consistently outperforms the baseline model (orange). Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Conclusion: Enriching the tokenizers vocabulary with domain-specific tokens like LATEX is an effective strategy for boosting model performance on specialized tasks. By providing native vocabulary for mathematical syntax, we directly facilitate the models ability to process and reason about complex mathematical problems."
        },
        {
            "title": "2.4.2 Final Tokenizer Design and Implementation",
            "content": "Our final tokenization strategy employs the Byte Pair Encoding (BPE) algorithm (Sennrich et al., 2016), trained on an extensive multilingual corpus covering over 121 languages 27 to ensure broad linguistic support. We developed suite of tokenizers with vocabulary sizes of 32K, 65K, 130K, and 261K, each tailored to specific model scale  (Table 1)  . primary design principle is to scale the vocabulary size in proportion to the models overall architecture (Tao et al., 2024). This balance is critical for preventing the embedding layer from becoming disproportionately large, thereby maintaining computational efficiency during fine-tuning and inference, particularly in resourceconstrained environments. The design of these tokenizers incorporates key findings from our empirical studies. Specifically, we implement both digit and punctuation splitting, which we found essential for improving performance on code-related tasks and ensuring accurate segmentation in non-Latin languages. Furthermore, based on our experiments demonstrating improved mathematical capabilities, we manually inject common LATEX commands directly into the vocabulary. To facilitate adaptability for downstream tasks, we reserve 1,024 special tokens across all tokenizers in the suite, providing end-users with the flexibility to customize the vocabulary for specific applications. summary of the final trained tokenizers along with their vocabulary size is shown in the table 5."
        },
        {
            "title": "Model",
            "content": "Falcon-H1-32k Falcon-H1-65k Falcon-H1-131k Falcon-H1-262k 32,768 65,536 131,048 261,120 Falcon-H1-0.5B* Falcon-H1-1.5B*, Falcon-H1-3B* Falcon-H1-7B* Falcon-H1-34B* Table 5: List of available tokenizers trained for Falcon-H1 3. Pretraining"
        },
        {
            "title": "3.1 Pretraining Data",
            "content": "Capabilities of language models are known to come mainly from the training data, and that stays true for Falcon-H1 series. We have expanded our data corpus to more than 20 Teratokens, among which up to 18 Teratokens were used for training Falcon-H1 models. As shown in Table 1, each Falcon-H1 model comes with different compute or token budget, considering their different learning capability and model capacity. The data mixtures are also designed differently across model scales. Noting that, although most Falcon-H1 models continued to show performance improvements toward the end of training, we chose to finalize training based on the allocated compute budget for each model. The raw pretraining corpus was constructed from multiple sources, including web data, highquality curated corpora, code, educational math content, and in-house synthetic data. We con17 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance ducted an extensive evaluation and studies of the data mixture, performing exhaustive tests and applying data optimization strategies. Notably, beyond the domain-specific or knowledge-intensive data typically emphasized in many LLM training pipelines, we observed that the organization of knowledge within the dataset has significant impact on model performance. strong correlation was found between factors such as training epochs, the number of unique high-quality data tokens, and the proportion of each data type within training epoch. However, isolating and analyzing these factors independently is impractical, as they are interdependent and would substantially increase the complexity of the analysis during pretraining."
        },
        {
            "title": "3.1.1 Data Sources",
            "content": "English Web data. Starting from FineWeb (Penedo et al., 2024a), we applied further quality filtering to improve knowledge density and training stability, using small language models as quality judges with carefully designed prompts. After processing the entire FineWeb dataset, we retained approximately 11T tokens. Multilingual data. Apart from English, Falcon-H1 models support 17 languages (except the 0.5B model, which is English-only): Czech (cs), German (de), Spanish (es), French (fr), Hindi (hi), Italian (it), Japanese (ja), Korean (ko), Dutch (nl), Polish (pl), Portuguese (pt), Romanian (ro), Russian (ru), Swedish (sv), Urdu (ur), and Chinese (zh). The multilingual data corpus draws from diverse sourcesmainly Common Crawl and range of curated datasets. For multilingual web data from Common Crawl, language identification was first performed at the HTML level using pycld2, then refined post-extraction with fasttext and trafilatura. The data was segmented into five similarly sized partitions (413 dumps each), covering 2012 to mid-2024. All dumps from 20222024 were included, with older dumps sampled randomly due to lower multilingual content. Processing followed the heuristics-based pipeline of (Penedo et al., 2023), with language-specific tuning of Gopher Quality filtering, line-wise filtering, and stop words (Malartic et al., 2024). rule-based toxicity filter was applied using human-curated lists of offensive words. Native/proficient speakers rated each word (0: non-toxic, 1: context-dependent, 2: always toxic), and documents were filtered based on cumulative toxicity scores for which the formula was refined through human feedback. The processed data was re-partitioned (two to five parts per language) and deduplicated via MinHash at the part level. For languages with limited web data from above-mentioned data sources, we supplemented with additional public datasets: CulturaY (Thuat Nguyen & Nguyen, 2024) (Hindi, Korean, Urdu) and FineWeb2 (Penedo et al., 2024b) (Dutch, Romanian, Swedish, Korean, Urdu), processed through the same pipeline for consistency. The resulting multilingual web dataset for the 17 pre-trained languages totaled over 3,000 GigaTokens (GT), portion of which was used during the pretraining phase. To further enhance the multilingual data quality and diversity, we extract multilingual data from some highly curated data sources, including Wikipedia, academic preprints (arXiv, PubMed Central), online forums (Reddit, HackerNews, OpenSubtitles (Tiedemann, 2016), Ubuntu IRC, YouTube, StackOverflow), open-source books from Gutenberg (gut), public datasets like Europarl Corpus (eur), Gardian (Gardian, 2024), Makhzan (mak), and proprietary data. Code data. Code data is widely considered as an important source for boosting models general and reasoning capabilities. We have internally curated an extensive code corpus that contains file-level, repository-level, and High Quality (HQ) code splits. For the file-level split, we used an internally scraped and curated dataset, spanning 67 programming languages (listed in Appendix E.2). The data was sourced from GitHub repositories created up to May 2024 and from notebooks within the Meta Kaggle Code dataset 4. Notebooks were converted to scripts using 4https://www.kaggle.com/datasets/kaggle/meta-kaggle-code 18 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance JupyText 5 after which all samples underwent heuristic filtering (Lozhkov et al., 2024). Language labeling followed the method in (Penedo et al., 2023), retaining files classified under one of 18 target languages, with relaxed acceptance threshold (0.15), given that non-English content can typically appear in comments. Fuzzy deduplication was performed using MinHash and Local Sensitive Hashing (LSH) (Broder, 1997), computing 256 hashes per document with 5-grams and Jaccard similarity threshold of 0.85. Personally Identifiable Information (PII), such as email addresses and IP addresses, was redacted using pipeline inspired by DataTrove 6 replacing tokens with standardized placeholders (e.g., >>EMAIL_ADDRESS<<, >>IP_ADDRESS<<). The repository-level split containing long context code data was built with repository-level code samples (Guo et al., 2024; Hui et al., 2024), which were constructed by concatenating all source files from given repository, enabling cross-file code comprehension. MinHash deduplication was performed at the repository level (prior to filtering), to preserve the logical structure of each repository. Files were concatenated in alphabetical order for effective duplicate detection, and later shuffled to mitigate bias and ensure balanced representation. For both file-level and repository-level code data, we curated high-quality (HQ) split by applying code quality classifiers. For non-Python languages, we used CodeBERT-based classifier 7 covering 19 programming languages (see the full list in Appendix E.3), selecting samples that met predefined quality threshold. For Python, we applied both the CodeBERT-based classifier and specialized Python scorer 8, retaining only samples that met the threshold for both classifiers. Additionally, we supplemented this corpus with the OpenCoder annealing corpus (Huang et al., 2024b), which includes algorithmic data, synthetic QA, and synthetic code snippet datasets 9. We further preprocessed those data following the same methodology used for our file-level corpus. Math data. We used combination of open-source datasets and in-house crawled or retrieved math data from the Web. The open-source datasets include Proof-Pile-2 (Azerbayev et al., 2023), FineMath (Liu et al., 2024c), InfiMM-WebMath-40B (Han et al., 2024), and OpenCoder FineWeb Math corpus (Huang et al., 2024a), largely consisting of math data extracted and filtered from Common Crawl. For the in-house math data, following similar approach to (Shao et al., 2024), we first used fastText classifier trained on OpenWebMath (Paster et al., 2023) to retrieve OpenWebMathlike pages. The classifier was then iteratively refined with extracted math data to expand coverage, targeting top math-related domains from Common Crawl. All math data underwent decontamination process to remove overlaps with popular math benchmarks such as GSM8K (Cobbe et al., 2021a) and MATH (Hendrycks et al., 2021a). Synthetic data. We used combination of external open datasets and large volume of inhouse generated synthetic data. External sources include subsets of Nemotron-CC (Su et al., 2024) (diverse_qa_pairs, extract_knowledge) and Cosmopedia v2 (Ben Allal et al., 2024). From our experiments, we found that fully synthetic data generated without grounding on seed samples often lacks diversity and suffers from inconsistent quality. To address this, our in-house synthetic data was primarily created by rewriting curated raw dataincluding web, code, books, Wikipedia, arXiv, math sources, etc. We employed diverse set of internal and external open models across various scales and architectures, considering the compute cost, data quality, and content diversity. Instead of focusing only on questionanswer pairs, our generation strategies included enhancing writing style, increasing knowledge density, filtering redundant tokens, and applying iterative quality control to the generated samples. This rewriting process helped structure and formalize the 5https://jupytext.readthedocs.io/ 6https://github.com/huggingface/datatrove/blob/main/src/datatrove/pipeline/formatters/pii.py 7https://huggingface.co/devngho/code_edu_classifier_v2_microsoft_codebert-base 8https://huggingface.co/HuggingFaceTB/python-edu-scorer 9https://huggingface.co/datasets/OpenCoder-LLM/opc-annealing-corpus 19 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance underlying knowledge, reduce noise, and ultimately improve training stability and efficiency. Apart from rewriting raw samples, we also generate raw question-answer pairs from DeepMinds mathematics dataset (Saxton et al., 2019), which are further enhanced with context-enriched questions, and correct chain-of-thought solutions generated in way similar to STaR (Zelikman et al., 2022). To further increase the knowledge intensity of our pretraining corpus, we also generated synthetic textbooks using carefully constructed topic hierarchies extracted from Wikipedia, covering over 30K topics. Starting from 99 root categories (see Table 41 in Appendix), we crawled the Wikipedia category graph up to depth of 5 (or until reaching article nodes), followed by deduplication and pruning of irrelevant topics. For each hierarchy, we first generated table of contents, then created structured content for each unitincluding comprehensive explanations, relevant examples, and diverse exercises. Long context data. Long-context data naturally appears in sources such as web pages, repositorylevel code, books, and academic papers (e.g., arXiv). To enhance the models ability to handle extended sequences, we applied range of restructuring strategies across target lengths of 32K, 128K, and 256K tokens. These include Fill-in-the-Middle (FIM), where random sections are removed from documents, and section reordering, where segments are shuffled and the model is tasked with reconstructing the original orderrequiring combination of long-context reasoning, memory, and coherence understanding. Additionally, we created small set of synthetic long-context samples with questionanswer pairs to further improve capabilities such as in-context learning, complex pattern retrieval, etc."
        },
        {
            "title": "3.1.2 Data Strategy",
            "content": "Data validation. We carefully inspect each data source prior to injecting it into the training pipeline to obtain detailed understanding of its quality and its impact on domain-specific tasks as well as on overall model performance. More specifically, to derive strong, interpretable signals for each data source, we train multiple 0.5B-scale models from scratch, subsequently evaluating their performance on carefully selected domain benchmarks. An alternative strategy involves training single model from scratch and then modifying the data mixture during the learning rate decay stage to assess data quality with reduced computational costs (Grattafiori et al., 2024). However, in our experiments, this approach sometimes failed to yield clear or robust conclusions regarding data quality. This is primarily due to additional confounding factors, e.g., data distribution shift between the stable and decay stage, correlations in data used across these stages, and data mixtures within each of these stages, etc. To this end, we systematically train 0.5B models from scratch using either individual data sources or well-studied combinations of them. This enables us to isolate extra factors and examine multiple dimensions: absolute data quality, relative quality in comparison to existing datasets, interactions and correlations between different data sources and formats, and their respective impacts on various domain-specific tasks. Once the data is validated at the 0.5B scale, it will be passed to the models at medium and large model scales, being retained or adjusted based on the observed improvements in model performance. Deterministic data loading and check-pointing. When adopting multiple data sources into training, we implemented deterministic dataloader that reads samples sequentially from each source rather than sampling them randomly. This design offers several advantages: a) deterministic training behavior, enabling precise comparisons across different runs; b) flexible continual pretraining through coordinated model and data checkpointing across multiple data sources, without disrupting the loading order; c) dynamic data mixture updates during pretraining without needing to restructure data files or data classes; and d) improved control over multi-epoch training, allowing custom epoch sizes across diverse data sources on the fly - we define one training epoch as 20 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance full pass over all unique tokens from given data source. Data mixture. Effective data organization strategies proved critical to the final performance of our models. These strategies include choices around the data mixture, pre-training stages, and multiepoch training. In practice, these factors are tightly interconnected and require joint optimization, taking into account compute budgets, model scales, data source quality, and data volumes. One common concern when reducing the proportion of web data is its potential impact on model generalization and knowledge diversity. However, we found this concern might be somewhat overstated. While popular web datasets like FineWeb (Penedo et al., 2024a) or RefinedWeb (Penedo et al., 2023) offer broad topical diversity, their knowledge density is relatively low, even when enhanced by domain-specific filtering such as FineWeb-Edu (Penedo et al., 2024a) or FineFineWeb (M-A-P et al., 2024). With highly knowledge-intensive data sources and rewritten web samples, the raw web data can be significantly reduced without impacting model generalization and knowledge diversity. Through extensive experiments on data quality and validation, we iteratively refined and converged on an optimal data mixture across the pretraining process. Starting with an initial mixture validated on smaller models, we progressively adjusted the mixture throughout the training by incorporating newly optimized and carefully evaluated data at various model scales. This process led to the final data configurations for the 34B and 7B models  (Table 6)  , where web data accounted for only about 15% and 12.35%, respectivelysubstantially increasing the share of rewritten data (over raw web, code, curated sources, math data, etc.). For the smaller 3B, 1.5B, and 0.5B models, we maintained constant data mixture throughout the whole pretraining stage, leveraging the dynamic data preparation carried out during the pretraining of the larger models. 34B 7B"
        },
        {
            "title": "Raw data\nWeb\nCurated\nCode\nMath",
            "content": "Rewritten data Web & Curated Code & Math Synthetic data* 99.47 40.00 25.00 20.00 14.47 0.23 0.00 0.23 0."
        },
        {
            "title": "End",
            "content": "43.45 14.60 15.93 10.05 2.87 52.05 20.36 31.69 4."
        },
        {
            "title": "Start",
            "content": "81.07 25.00 26.00 20.00 10.07 10.56 0.00 10.56 8."
        },
        {
            "title": "End",
            "content": "42.26 12.35 16.47 10.74 2.70 53.04 18.12 34.92 4.70 3B"
        },
        {
            "title": "Mix",
            "content": "39.70 11.60 11.68 14.00 2.42 56.80 22.08 34.72 3.50 1.5B"
        },
        {
            "title": "Mix",
            "content": "23.20 10.20 4.75 8.00 0.25 69.80 23.75 46.05 7.00 0.5B"
        },
        {
            "title": "Mix",
            "content": "11.50 6.50 0.00 5.00 0.00 75.50 20.50 55.00 13.00 Total 100.00 * Fully synthetic samples, not derived from rewriting existing raw data. 100.00 100. 100.00 100.00 100.00 100.00 Table 6: Falcon-H1 data mixtures across model sizes. For the 34B and 7B models, both start and end-of-training mixtures are shown. For the 3B, 1.5B, and 0.5B models, single, static data mix was used. All values are in percent (%). To mitigate the risk of overfitting or bias towards specific domain tasks and to preserve balanced performance across diverse skill areas, we perform frequent checkpoint evaluations over diverse domain tasks throughout the pretraining stage. Additionally, whenever the data mixture changed, we perform frequent vibe checks on intermediate model checkpoints. This combined strategy ensures that the model maintains strong general capabilities while avoiding unintended 21 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance specialization or biases toward particular domains. Data organization and scheduling. primary challenge in large-scale pretraining is the profound imbalance between the vast quantities of web-scale data and the relative scarcity of highquality, curated datasets. Specialized corpora like mathematical texts, while crucial for model capability, comprise small fraction of the total training data corpus. To ensure these high-quality sources maintain their influence, we employ an aggressive up-sampling strategy enabled by multiepoch training. This approach decouples the effective training data size from the raw token count of our finite, high-quality corpora. This extensive reuse of data challenges the common practice of single-epoch training, often adopted to mitigate the risk of model memorization. However, our empirical investigation suggests that concerns about memorization may be overstated in large-scale regimes. By estimating the models memorization windowthe temporal span over which it retains specific training exampleswe found it possible to reuse high-quality samples multiple times without compromising generalization. As illustrated in Figure 9, this window can be approximated by analyzing the training loss on tokens seen at various points in the past. This finding is particularly relevant as most literature on memorization and catastrophic forgetting focuses on smaller models or shorter training durations. Figure 9: Models memorization window and loss trajectives Beyond data composition, we found data scheduling to be critical factor. Counter-intuitively, our experiments revealed that an \"anti-curriculum\" approach yielded superior results compared to conventional curriculum learning. In this paradigm, we introduce data of all complexity levelsfrom simple text to advanced mathematical problemsfrom the very beginning of training. 22 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance We hypothesize that this early and continuous exposure provides the model with more effective learning trajectory to develop the internal features required to master complex tasks. Across models of various scales, our experiments confirmed that maintaining this optimized data mixture from the outset outperforms curriculum strategies that reserve high-quality data for later training stages. This finding holds when sufficient volume of high-quality data is available. Long-context training. We initiated pretraining with an 8K context window, with the majority of pretraining conducted at 16K context length. An exception is Falcon-H1-0.5B, which was primarily trained at 4K context due to its limited capacity for handling longer sequences. Thanks to our infrastructure optimizations, throughput degradation at 16K context was minimal. Moreover, we observed performance boost compared to training with 4K or 8K context lengths. For long-context extension, we maintained the overall data mixture largely unchanged, but slightly increased the proportion of long-context samples within each data source. Following the learning rate decay phase, we continued training with fixed minimum learning rate during long-context extension. We empirically found that performing this extension after the decay phase yielded no significant performance difference compared to extending it before decay. However, the post-decay approach was substantially more compute-efficient due to the reduced training throughput of long sequences. For Falcon-H1-34B and Falcon-H1-7B, we applied 60GT at both 32K and 128K context, and 25GT at 256K context. For Falcon-H1-3B and Falcon-H1-1.5B, the same token counts were used for both the 32K and 128K context stages."
        },
        {
            "title": "3.2.1 Training Stability",
            "content": "During early experiments with Falcon-H1 we observed severe loss spikes from the beginning of the training. These spikes created two practical problems: (i) Spiky loss curves distort ablation results: variant may look inferior simply because spike coincides with the learning-rate decay, reversing the true ordering; and (ii) continuing with such spikes would force us to choose learning rates well below the optimum, which would slow convergence. Eliminating the instability therefore became prerequisite for any meaningful ablation. Spike-like behaviour has been reported before in Falcon2 (Malartic et al., 2024), Falcon-Mamba (Zuo et al., 2024), Jamba (Lieber et al., 2024), and YuLan (Hu et al., 2024b). Falcon2 attributed the issue to depth, whereas YuLan pointed to three potential triggers: exploding residual paths, unstable layer-norm statistics, and extreme attention scores. common practice is to employ batch-skipping: whenever the loss on batch exceeds userdefined multiple of the running median, that batch is dropped and the optimiser moments remain unchanged. This heuristic can suppress spikes caused by handful of outlier examples, yet it is largely ineffective for dynamics-induced instabilities and only only postpones the problem. Isolating the Source. Our initial experiments were conducted on pure Mamba2 baseline. Loss spikes were already present in this setting, showing that the SSM pathway is sufficient to trigger the instabilityalthough it may not be the sole contributor. To test whether the spikes were driven by width or depth, we compared two vanilla Mamba2 variants trained under identical conditions: Wide: larger hidden dimension and more SSM heads, but fewer layers; 23 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Deep: smaller hidden dimension, comparable total parameter count, but more layers. All other hyperparameters were kept fixed, and the learning rate was scaled as per µP scaling. The wide model exhibited pronounced loss spikes, whereas the deep model trained smoothly. This finding implicates widthrelated dynamics inside the SSMspecifically the larger number of headsas primary driver of the observed instability, and motivated the analysis reported in the next sections. Diagnosing the SSM Dynamics. We logged parameter statistics throughout training and found that spiking runs displayed wider range of edtt values. Recall from (6) that dtt = softplus( edtt+ b) controls both forgetting of and writing to SSM hidden state, as can be seen from (6). Therefore, large positive edtt values have two antagonistic effects: linearly enlarging the information from the current token written into the hidden state while exponentially forgetting the information from the previous tokens. Whenever the modeling objective requires both the recent token and its long-range context, gradient descent is pulled in opposite directions: (i) increase edts so that dts increases and amplifies the contribution of token through Bs dts in (7). (ii) keep Qs j=s Aj with < from collapsing so that earlier information still reaches position and propagates further. Because these antagonistic signals arrive at different scales and at different optimization steps, the parameter overshoots and then over-corrects, producing the characteristic loss spikes. Mitigation. We tested three interventions: Clip Alog: no effect. Clip negative dt: no effect. Clip positive dt: completely removed spikes. This confirms our previous hypothesis: spikes are caused by writing to the hidden state. However, we recognize that such clipping may restrict expressiveness. softer alternative is to multiply the dt activation by constant 0 < α < 1. This attenuation preserves the full parameter range while preventing early excursions into the unstable regime. With attenuation enabled we can train Falcon-H1 at relatively high learning rates without observing any loss spikes. The attenuation factor is part of the µP forward multipliers (see: 3.2.3) and will be tuned as well with other multipliers."
        },
        {
            "title": "3.2.2 Effective Learning Rate and Effective Weight Decay",
            "content": "Parameter norms. To initiate the discussion of the joint effect of AdamW learning rate (LR) η and weight decay (WD) λ on the model training, we start with the behavior of parameter norms of matrix layers, to which WD was applied during training. Figure 10 (left) shows that parameter norms grow indefinitely with no WD λ = 0 while stabilizing at constant level when λ > 0. The value of parameter norms after stabilizing depends on LR, WD, and also batch size B, raising the question of the precise form of this dependence. 24 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Leaving the dependence of parameter norms on batch size to the future work, we have found that the dependence of on (η, λ) is well described, for all matrix layers , by simple scaling rule . (10) η λ Moreover, if the norm is normalized in mean fashion as 2 = 1 mn for different layers in the architecture are extremely close to each other, see figure 10 (left). j=1 2 ij, its values i= Pn Pm In fact, the scaling (10) is natural and is observed in simple model of stochastic dynamics where Brownian expansion due to noise in the gradients dominates the attraction to the optimal parameter value. We provide such toy model in section and show that in the relevant regime of hyperparameters, the dependence weight norm on LR and WD has the functional form (η, λ)2 = C1 η λ + 1 λ2 , (11) where the constants C1, C2 are determined by the variance of the gradient noise and steepness of the loss function near the optimal value of the parameters. Consider the region of (η, λ) typically used in the model pertaining. One extreme scenario is large noise variance and flat loss landscape, where the weight norm is mostly determined by the balance between weight decay contraction and 1 λ2 and the Brownian expansion due to stochastic optimizer updates. Then, we have C1 weight norms scaling is given by (10). The opposite extreme scenario is small noise variance and steep loss landscape, where the weight norm is mostly determined by the balance between weight decay contraction and attraction of the parameters to the optimal value . Then, we would have C1 η λ C2 To determine which of the two above scenarios better describes the behavior of the norms during the actual model training, we performed 2d sweep over (η, λ) values and measured the norms by the end of the constant LR stage of WSD schedule, just before LR decay. The results are depicted in the figure 10 and show that the scaling (10) indeed describes parameter norms very well, suggesting that the noise plays the dominant role in the dynamics of the matrix layers. 1 λ2 and the weight norms scaling be λ1. η λ It is convenient to assign to both LR and WD simple, intuitive role Effective learning rate. they play in the training process. Weight decay squeezes the learnable parameters and, therefore, controls their norms. Learning rate determines the speed of learning, and also the noise level in the presence of label noise (Liu et al., 2025). As result, the final loss has steep dependence on the LR, making it the most important parameter to be tuned. λ This simple intuition does not hold for weight decay as we saw in the previous section: the combination η controls the parameter norms instead of WD λ itself. Moreover, the LR intuition also does not hold, as we show in Figure 11 (Left): changing λ has similar effect on the loss curve as changing η. We argue that simple LR and WD intuition can be kept if we switch original η, λ with effective learning rate (ELR) and effective weight decay (EWD) defined as (ELR) ηeff = pηλ, (EWD) λeff = λ η . (12) Essentially, the EWD definition was already verified in the previous section due to parameter norms scaling (10), equivalent to λ1 eff . Here we take λ1 as an intuitive functional form of parameter norm scaling that can be observed, for example, for minimizers of L2 regularized loss with strong regularization λ, or in the weak noise scenario of (11). Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Figure 10: Behavior of weight norms of embedding Wemb, unembedding Wunemb, and input/output projections Win, Wout layers of pure Mamba2 model. (Left) Evolution of weight norms during training of 1B model. In addition to no WD run, we show WD sweep (solid) and LR sweep (dashed), both of which display similar impact on weight norms. Learning rate is measured in the units of 105; and the jump of norms at 10GT corresponds to batch size doubling within rampup. (Right) Average weight norm across all model layers at two-dimensional (η, λ) sweep on 300M model. The combinations ηλ are used as coordinate axes to demonstrate that weight norms are scaled as in (10). LR and WD are measured in the units of η0 = 256105 and λ0 = 0.1. Empty cells correspond to diverged runs. η and λ heuristic way to obtain ELR definition (12) is to look at an optimizer part δWt of single step update Wt+1 = Wt ηδWt ληWt of linear layer , and define ELR as meaningful measure of the update strength associated with δWt. We start with proposition that relative magnitude of the parameters change ηδWt is more meaningful measure than the absolute magnitude ηδWt. Wt Then, recall that for Adam the update is given by ratio δWt = G1,t of gradient moments G1,t, G2,t. An important empirical observation is that each component of the ratio has roughly the same magnitude around 0.1, almost independent from LR and WD used in the training. As result, we can treat δWt as constant, leading to simplified update strength measure ηδWt Wt . Next, assume that the parameter norms have already stabilized at their stationary state given by the scaling Wt ηλ, which is λ . This further changes update strength measure to exactly our ELR definition (12). Wt η Wt G2,t+ϵ η η Now, let us examine the properties of ELR on the noise level during training, that we can roughly measure as difference of the loss just before and after learning rate decay Lbefore LRD Lafter LRD = Lnoise. In the figure 11 (right) we plot Lnoise for different values of ELR ηeff and EWD λeff. We see that keeping ηeff = const and changing λeff has weak effect on Lnoise while keeping λeff = const and changing ηeff affects the noise level strongly. We can roughly interpret this observation as the noise level being the function of only the effective learning rate Lnoise(η, λ) = Lnoise(ηeff). Essentially, this shows that ELR actually controls the noise level and closes our earlier statement that learning rate and weight decay intuition hold for EWD and ELR. Finally, let us point out to an additional property of EWD and ELR that we call log-scale orthogonality and which is useful for (η, λ) sweeps often done within empirical tuning of training hyperparameters. robust strategy for such sweeps is to use log-scaled grid, for example, with the powers of 2: (η, λ) {η02i, λ02j}n i,j=0, which ensures good balance between precision and coverage. Such experiment design naturally corresponds to the Euclidean metric, but in logcoordinates (log η, log λ). In this metric, the gradients of EWD and ELR turn out to be orthogonal, 26 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Figure 11: Effect of LR η and WD λ of the training loss curve of 300M pure Mamba2 model. (Left) LR and WD sweeps on left and right subplots show that increasing (or decreasing) either LR or WD has similar effect on the loss curve. (Right) Noise level measure as the loss gap before and after LR decay Lnoise = Lbefore LRD Lafter LRD at two-dimensional (η, λ) sweep. Again, the use of ELR and EWD for coordinate axis shows that the noise is mostly determined by ηeff = ηλ. unlike in the standard Euclidean metric on the (η, λ) plane. (log)ηeff (log)λeff = ηeff log η λeff log η + ηeff log λ λeff log λ = 0. (13) With the above orthogonality property, the sweeps on log-scale grids contain directions that maximally change the noise level (via ELR) while keeping the parameter norms constant (via EWD), and vice versa. This makes possible to effectively measure the effect of noise level and parameter norms on training dynamics, and we will utilize it for our µP sweeps in the next section. To summarize, ELR and EWD defined in (12) have the following useful properties (A) Parameter norms are fully determined by EWD λeff (B) Noise level is fully determined by ELR ηeff. (C) Log-scale orthogonality. Directions of increase of ELR and EWD are orthogonal on (log η, log λ) plane, allowing to separately and efficiently measure their effects within sweeps on log-scale grids. We emphasize, however, that the above conclusions are rough approximations based on limited set of experiments performed for Falcon-H1 training, and accurate investigation of these questions would be an exciting direction for future work. Incorporating parameter norms scaling into the power scheduler. Recent work (Shen et al., 2024; Bjorck et al., 2025) noticed that for longer training durations , the optimal learning rate of the WSD scheduler scales as ηopt with the exponent roughly in the range [0.3, 0.5]. As next step, (Shen et al., 2024) suggests to downscale the learning rate of the stable stages ), referred to as power scheduler starting from specified activation time t0 as η(t) = η0 (PS). Our internal tests support these conclusions with PS schedule achieving better loss than learning rate tuned WSD at different training token scales. min(1, t0 27 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance However, we also decided to incorporate our insights on the role of weight decay into the schedule. Specifically, let us relate the PS scaling of learning rate and weight decay to the scaling of ELR and EWD (η(t) 1 2 λ(t) const(t) = (ηeff(t) 1 λeff(t) 1 4 4 (14) Now, we combine two observations. On the one hand, we observed that the final loss is much more sensitive to ELR than to EWD. In that case, we can mostly attribute the effectiveness of the PS scaling (14) due to the scaling of ELR rather than EWD. On the other hand, we still observed substantial gains from tuning weight decay µP multipliers, suggesting that it is beneficial to have tuned parameter norms of all the layers in the architecture. Then, if we would like to keep parameter norms at the optimal level throughout the whole training, we should not scale EWD, which controls the parameter norms. This brings us to Effective Power Scheduler (EPS) scaling (ηeff(t) 1 λeff(t) const(t) = (η(t) 1 λ(t) 1 4 4 (15) While we have observed an improved convergence speed of EPS over PS, systematic study is required to properly incorporate weight norm control into the training schedule. In particular, EPS schedule we propose rests on the assumption that parameter norms should not be scaled during long training runs, which is not guaranteed to be the optimal choice."
        },
        {
            "title": "3.2.3 Maximal Update Parametrization (µP ) with Tunable Multipliers",
            "content": "Background. Maximal update parametrization (µP ) was proposed in (Yang & Hu, 2022). It combines several ideas of how different factors affect the ability of the model to learn non-trivial internal representations as the models width increases. One such factor is parameterizing the weight matrix with scalar multiplier m, so that the forward pass becomes = mW x. Another is initialization variance and learning rate of different layers in the models architecture. The impact of these factors is formulated rigorously in the infinite width limit , where unique scaling of main parameters with is required to ensure nontrivial feature learning. The multiplier m, initialization variance σ2, learning rate η, and weight decay λ must scale with certain powers of width d: = m0da, σ = σ0db, η = η0dc, λ = λ0de, (16) with specific values of scaling exponents (a, b, c, e) depending on the location of the layer in the model architecture and the type of optimizer used, e.g. SGD or Adam. For example, for AdamW optimizer, µP scaling of hidden layers are (a, b, c, e) = (0, 1 2 , 1, 1) while for output(LM head) layer (a, b, c, e) = (0, 1, 1, 1). The subsequent work (Yang et al., 2023; Bordelon et al., 2023) also explores the scaling with model depth to enable feature learning in the limit . The main practical implication of µP is zero-shot hyperparameter (HP) transfer (µTransfer) (Yang et al., 2022). Essentially, if one finds the optimal hyperparameters at reference, typically small, model size dref , the optimal HPs at larger target size could be roughly obtained from reference model values by using scaling rules (16). µTransfer were previously applied for tuning and transferring LLM HPs in (Yang et al., 2022; Dey et al., 2023; Hu et al., 2024a; Dey et al., 2025; Liu et al., 2023c). An important aspect of model parametrization is the presence of an exact symmetry transformation that rescales (m, σ, η, λ) such that both the forward pass and the optimizer update stay 28 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Architecture part Full model Hybrid block SSM (Mamba2) block Forward pass Shapes Multiplier scaling munembWunembN ((FL(. . . F1(membWembX) . . .))) Wemb Rddvoc Wunemb Rdvocd memb 1 munemb d1 Fl(rl) = l = rl + attn l (r + MLP (Nl(rl)) + SSM (N l)) (Nl(rl)) eC = mCWCr ex = mxWxr eB = mBWBr xBC = SiLU(conv1d(ex eB eC)) ez = mzWzr dt = Softplus(mdtWdtr + bdt) ySSM = SSM(x, B, C, dt) SiLU(ez) SSM(r) = mSSMWSSMN SSM(ySSM) Attention block = WQr = mkeyWKr = WV attn (r) = mattnWattn GQA(Q, K, V) MLP block yMLP = SiLU(mgateWgater) (Wupr) MLP(r) = mMLPWdownyMLP rl, RdLseq Wx Rdssm Wz Rdssm WB Rdssm WC Rdssm nssm nssm statenssm statenssm d d nattn nssm Wdt Rnssm WSSM Rddssm Wattn Rddattn nattn WQ Rdattn nattn WK Rdattn nattn WV Rdattn Wup RdMLPd Wgate RdMLPd Wdown RddMLP d mB (dssm d)1 mx d1 mz d1 statenssm mC d1 mdt d1 mSSM (dssm nssm )1 mattn (dattn nattn mkey d2(dattn d)1 ) 1 2 mMLP (dMLPd)1 mgate d1 Table 7: This table summarizes the location of all the forward µP multipliers used in Falcon-H1 models, the rules to scale multipliers with model size, and the shapes of the relevant parameters or activations required for the scaling. Specific forms of scaling rule in the last column can be straightforwardly derived from µP Desideratum, see, for example, (Yang et al., 2024c). GQA(Q, K, V) is Grouped Query Attention; SSM(x, B, C, dt) is recurrent sequence transformation described in section 2.2; , , SSM, are RMS normalization layers. unchanged. Specifically, for AdamW optimizer (with ε = 0), such scaling transformation with parameter is p1m, σ pσ, η pη, λ p1λ. (17) This symmetry has practical implication for µTransfer as it allows to remove in (16) the scaling of either learning rate or forward multiplier (by choosing = (d/dref )a or = (d/dref )c). The motivation to remove one of the scalings comes from training infrastructure: multiplier must be implemented directly in the forward pass of the model, while scaling of η, λ is typically implemented via optimizer parameter groups. For Falcon-H1 models, we relocate the µP scaling (16) from learning rate/weight decay to 2 , 1, 1) for the hidden layers 2 , 0, 0). As result, all the models in the series can be fine-tuned or forward multipliers. For example, the original scaling (a, b, c, e) = (0, 1 becomes (a, b, c, e) = (1, 1 continuously pretrained with the same learning rate and weight decay parameters. Our approach. Our core idea is to augment µTransfer scaling for transferring HPs across model sizes with tuning the µP multipliers at the base model size. To motivate the approach, let us rewrite the general scaling (16) for model parameter (i) from the point of view of the base model with width dref m(i) = m(i) ref (cid:19)a (cid:18) dref , σ(i) = σ(i) ref (cid:19)b (cid:18) dref , η(i) = η(i) ref (cid:19)c (cid:18) dref , λ(i) = λ(i) ref (cid:19)e (cid:18) dref . (18) The classical µTransfer uses the Standard parametrization at the base model size, corresponding to no forward multipliers m(i) ref = η ref = 1 and use of global LR η and WD λ for all the layers, η(i) 29 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance"
        },
        {
            "title": "22\nWemb\n20\nWunemb\n2−0.5\nWin\n2−2\nWout\n2−0.5\nWup\n20.5\nWgate\n2−0.5 Wdown",
            "content": "L = 66 = 1280 dssm = 64 nssm = 16 dssm state = 128 dattn = 64 nattn = 12 dMLP = 3840 22.5 memb Wemb 25 Wunemb munemb 22 mMLP Win 21 Wout mattn mSSM 21.5 Wup 20.5 Wgate mgate 22 mkey Wdown 22 mx 21.5 mz 21.5 mB 21 mC 21.5 mdt 21.5 22 21.5 Mixer MLP SSM 21 23 22 20.5 22 20.5 Wconv1d 20 bconv1d 20.5 bdt Alog 22.5 21 21.5 21. 23 Table 8: Base model shapes and its µP multipliers after tuning procedure described in section C. The notations for forward multipliers, as well as notations for most of the learnable parameters, were introduced in table 7. Win = WxzBCdtQKV is merged weight matrix combining all the initial projections of both SSM and attention mixers, and Wout = WSSMattn is similarly merged weight matrix that combines output projections of SSM and attention. We have merged these matrices in the context of µP multipliers to reduce the total number of multipliers to be tuned. Finally, and Alog are learnable parameters of Mamba2 sequence transformation. and λ(i) ref = λ. However, this strategy is somewhat contradictory: it uses specialized HPs at target size > dref that respect the limiting feature learning scaling, while using global HPs for the base model as if dref were tiny and base model was very far from limiting behavior. This assumption does not look reasonable in practice. For Falcon-H1 series, we have used the base model with dref = 1280 while the largest 34B model has = 5120, just an 4 factor from the base model. ref /η and λ(i) With such premise, we have decided to individually tune HPs of the different layers to respect the limiting tendency that could be affecting the model at dref . First, we group model parameters according to their role in the architecture, e.g. LM head or MLP down projection, to tune LR and WD multipliers η(i) ref /λ. Second, we introduce minimal set of forward multipliers that produce all possible transformations of the activations throughout the whole forward pass of Falcon-H1 architecture10. The minimal property of our set removes redundant multipliers that otherwise would be tuned in vain. For example, having both key and query multipliers mKK and mQQ is redundant because only their scalar product QK mQmKQK is used in attention and having only mK is sufficient to cover all possible scalings of attention scores. Finally, we dont tune initialization variances σ(i) because the symmetry (17) reduces one degree of freedom in the HP set (m, σ, η, λ), and our preliminary sweeps on initialization variances have shown weak dependence of the loss on σ(i). This resulted in 35 multipliers to be tuned for 10Formally, to have fully complete set of forward multipliers we would also need to add multipliers to SSM inputs eB and fdt. However, these multipliers are effectively taken care of by learnable parameters of SSM and Alog that can adapt to the right scale during training, unlike matrix parameters with non-adaptive norms as described in section 3.2.2. 30 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Figure 12: Sensitivity of the loss with respect to all 35 µP multipliers we have tuned. The multipliers are organized into 4 groups as in table 8, with matrix layers ELR and EWD multipliers are shown in the middle plot side-by-side for comparison. All 4 multiplier groups have different magnitudes of the effect on the loss, and the y-axis limits were adjusted accordingly. ELR multipliers have the strongest impact, followed by foraward multipliers, then EWD multipliers, and lastly LR multipliers of vector-like layers. Although the sensitivities w.r.t. to the last group are low, note that the final multiplier values are quite far from those of the matrix layers (see table 8), and, therefore, separating LRs of vector-like and matrix-like layers (as was done in our tuning) still yields significant performance improvement. Falcon-H1 architecture. The final values of tuned multipliers, as well as all base model shapes and global LR/WD, can be found in table 8, and the location of forward multipliers is detailed in table 7. We describe our procedure to tune multipliers in section C. It naturally leads to the estimation of the sensitivity of the loss on each multiplier around the optimum ( log2 m)2 that we display on figure 12 to provide intuition behind the impact of each of the 35 tuned multipliers."
        },
        {
            "title": "3.2.4 Other Aspects: Batch Scaling, Rampup, Warmup",
            "content": "In this section, we briefly report interesting observations related to training dynamics that we have incorporated into Falcon-H1 training. Batch scaling. First, following (Zuo et al., 2024) we have used batch scaling that scales the learning rate if the batch size is changed η(b) = ηref bref , (19) where square root is suitable scaling for Adam optimizer (Malladi et al., 2022). We have observed that scaling (19) better preservers optimal learning than no batch scaling at all. However, more careful studies are required for robust transfer of HPs with batch size, taking into account, for example, scaling of parameter norms with batch size to combine it with ELR/EWD picture discussed in section 3.2.2. Rampup. At the beginning of the training, we have used the batch size rampup that linearly increases batch size over the specified duration, which we set around 50GT for different Falcon-H1 model sizes. In figure 13 we considered 3 strategies and their impact on the training loss: no rampup, rampup without batch scaling, rampup with batch scaling (19). No rampup was observed to have the worst loss, while also amplifying training instabilities. Shortly after the rampup period, batch scaling 31 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Figure 13: Rampup and warmup related observations on 1.5B pure Mamba2 model. (Top Left) Train loss trajectories during rampup with and without batch scaling. No batch scaling run exhibits loss jumps at the moments of batch size increase. These jumps could be associated with decrease in the noise level. (Top Right) Loss after learning rate decay at 70GT on LR sweep for different rampup strategies. (Bottom Left) Evolution of the loss for runs with and without BS scaling and 3 different learning rates. To take into account the noise level, we measure the loss after LR decay, which is performed at several positions of the training trajectory. To be able to display losses at very distant moments of training, we subtract the loss of the reference run. We see that, for all considered learning rates, the runs with batch scaling have better trend with longer training duration. (Bottom Right) Evolution of the loss gap between runs with different warmup durations. Unlike the previous plot, the loss is measured directly (without LR decay) because warmup duration does not seem to affect noise level, as can be seen from the last predecay measurement at 60GT and after decay measurement at 70GT being very close. We see that at early stages ( 16GT) loss vs warmup duration curve shifts with measurement time, reflecting that short warmup runs simply had more high LR steps at the beginning of the training. However, at later stages ( 16GT) the curve stabilizes, suggesting well-defined optimal warmup duration with long-lasting effect on the training. showed worse loss than classical rampup without batch scaling, see figure 13 (top right). However, the gap between them closes at long training durations, and rampup batch scaling eventually outperforms its no batch scaling version, see figure 13 (bottom left). This later trend is especially interesting because BS and no BS runs have exactly the same hyperparameters after the rampup period. One interpretation would be that batch scaling during rampup directs the training trajectory to better region of parameter space, and the model continues to learn in this region 32 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance for very long time. Warmup. Another classical technique used at the beginning of training is to linearly increase LR to the target value in the earliest stages of the training, typically for few gigatokens. In figure 13 (bottom right) we have tested the impact of warmup duration on the loss at later training stage. Similarly to the rampup batch scaling, we observe that warmup duration has long-lasting impact on the loss, with relatively short optimal duration of 0.1GT."
        },
        {
            "title": "3.3 Pretraining Infrastructure",
            "content": "Pretraining was conducted using Mambatron, our in-house distributed training framework. This framework is an evolution of Gigatron, the codebase previously used to train the Falcon model series, and has been specifically optimized to support the unique hybrid architecture of FalconH1. Building upon foundation of standard 3-dimensional parallelism (3DP), we introduced two key innovations tailored for this hybrid design. First, we redesigned Context Parallelism (CP) to efficiently manage and scale the long sequence lengths inherent to the hybrid attention-SSM architecture. Second, we developed novel strategy termed Mixer Parallelism (MP), which is specifically designed to parallelize computations across the distinct attention and SSM heads. This approach significantly accelerates both training and inference throughput. The Falcon-H1 model series was trained on large-scale infrastructure comprising 4,096 NVIDIA H100 GPUs. To optimize resource utilization, we employed dynamic node allocation strategy, enabling the simultaneous training of six Falcon-H1 models. The parallelism configurations for each model are detailed in Table 9. Batch Size Context Len. Stage DP TP PP CP MP"
        },
        {
            "title": "Models",
            "content": "Falcon-H1-0.5B Falcon-H1-1.5B (1.5B-Deep) Falcon-H1-3B Falcon-H1-7B 4M 4M 8M 8M Falcon-H1-34B 26M 4K, 16K 16K, 32K 131K 16K, 32K 131K 16K, 32K 131K 262K 16K 32K 131K 262K 64 256 64 256 256 128 64 448 192 48 24 1 1 1 1 1 2 2 4 4 4 4 1 1 1 1 1 1 1 1 2 2 2 1 1 4 1 4 1 4 8 1 2 8 16 Table 9: 5D Parallelism Configurations for Falcon-H1s Training"
        },
        {
            "title": "3.3.1 Scaling Dynamics of Data Parallelism",
            "content": "While Data Parallelism (DP) is fundamental technique for distributed training, its throughput scaling is not without limits. When scaling the number of DP workers (NDP) while keeping the global batch size (Bg) constant, throughput gains diminish significantly once communication overhead outweighs computation. This occurs because, to maintain fixed Bg, the number of gradient accumulation steps (K) per optimizer update must decrease inversely with NDP, where Bg = NDP Bµ for micro-batch size Bµ. 33 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance To formally analyze this behavior, we model the time for single optimizer step as: Tstep(NDP) tµ + tsync(NDP) (1) Here, tµ is the constant time for forward-backward pass on single micro-batch, and tsync is the latency of the gradient all-reduce operation, which can grow with NDP due to network complexity. As NDP increases, the computation term (K tµ) shrinks, while the communication term (tsync) becomes the dominant component. Consequently, the overall throughput, given by Bg/Tstep, deviates from ideal linear scaling. Substituting the terms, we get: Throughput(NDP) Bg Bg NDPBµ tµ + tsync(NDP) (2) Theoretically, linear scaling could be maintained by increasing Bg proportionally with NDP, keeping constant. However, significantly changing the global batch size from the value determined during hyperparameter tuning can destabilize training dynamics, thus impacting model convergence and final performance. Therefore, our strategy involves pragmatic trade-off. We cap the DP size at value where communication overhead remains manageable, and increase the global batch size only up to critical point that balances high throughput with stable model convergence. This ensures efficient hardware utilization without compromising the integrity of the training regime."
        },
        {
            "title": "3.3.2 Mixer Parallelism (MP)",
            "content": "During pre-training, we developed novel distributed training paradigm to boost the efficiency of training our largest models. Leveraging the parallel architecture of our decoder layers, where attention and Mamba layers are executed sequentially, we partitioned the Tensor Parallel (TP) world into two distinct groups: one dedicated to Mamba operations, the other to attention operations. This design allows these computations to run concurrently, followed by an all-reduce operation to synchronize their outputs. We refer to this strategy as Mixer Parallelism (MP). It significantly improves training throughput by optimizing both computational speed and memory efficiency while also boosting inference efficiency particularly for scenarios involving small batch sizes and sequence lengths. Mixer Parallelism can be implemented using two distinct approaches. In naive Mixer Parallelism, predefined TP groups are assigned exclusively to single mixer type (Attention or Mamba). In contrast, interleaved Mixer Parallelism distributes different mixer types across TP groups in an alternating fashion, achieving more balanced distribution of computational overhead from the slower mixer layers. Figure 14 shows how different Mixer Parallelisms are implemented. Training Efficiency with MP. To evaluate the efficacy of Mixer Parallelism for the models training, we conducted experiments using 2B hybrid model, configured with data parallelism of 4, tensor parallelism of 4, and context length of 2048. We measured the training throughput for baseline without MP against both the naive and interleaved variants. The results, summarized in Table 10, demonstrate the clear superiority of the interleaved MP strategy. By effectively balancing the computational load, interleaved Mixer Parallelism achieves substantial 1.43x speedup over the baseline. Inference Efficiency with MP. To evaluate the efficacy of Mixer Parallelism for inference scenarios, we conducted comprehensive throughput analysis across 3B and 7B parameters models. Based on its superior load-balancing properties, we focused exclusively on the interleaved MP variant for these experiments. Our evaluation was performed on single node equipped with two NVIDIA H100 GPUs, configured with Tensor Parallelism size of 2. We systematically varied two 34 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Figure 14: Diagram illustrating the Mixer Parallelism (MP) strategies. Each row represents full decoder layer. Output projection and all reduce operations are applied at the end of each layer. (Top) Without MP, all GPUs compute both mixer types sequentially. (Middle) Naive MP statically assigns GPUs to one mixer type for all layers. (Bottom) Interleaved MP alternates assignments per layer, achieving better load balancing."
        },
        {
            "title": "MP Variant",
            "content": "Throughput (Gtok/hr) Speedup (ratio) None (Baseline) Naive MP Interleaved MP 0.2339 0.2640 0.3343 1.00 1.13 1.43 Table 10: Training throughput and speedup comparison for different Mixer Parallelism variants. key parameters to simulate diverse workloads: the batch size (from 1 to 128) and the number of generated output tokens (from 4096 to 32768). constant prefill size of 8 tokens was used for all runs. The implementation was integrated into custom fork of the vLLM library (Kwon et al., 2023). We report in figure 15 throughput results of this experiment on all our model sizes. The dashed curves represent configurations using Mixer Parallelism, while matching colors indicate experiments with the same number of generated tokens. The results show that Mixer parallelism significantly accelerates inference for low-latency scenarios characterized by small batch sizes and short generated sequences. Though this advantage diminishes and reverses for larger batches and longer generation sequences. This observation is consistent across all model sizes. We provide the community an easy way to test the implementation of Mixer parallelism in vLLM fork 11. 11https://tiiuae.github.io/Falcon-H1/deployment/ 35 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Figure 15: Throughput comparison for Mixer Parallelism across different model sizes (3B and 7B)."
        },
        {
            "title": "3.3.3 Context Parallelism (CP)",
            "content": "To keep GPU memory flat while scaling to long contexts, the model shards each sequence horizontally across the cp_world of devices and lets every device work on one contiguous chunk of length Q. Self-Attention chunks. For the attention blocks we reuse RingAttention (Liu et al., 2023a): each rank holds its local querykeyvalue slice and circulates K/V tensors around the ring so that the full score matrix is produced without ever materialising the whole sequence on one GPU. Memory is therefore O(Q) per rank instead of O(T ) for the full length . SSM (Mamba-2) chunks. For the SSM layers we follow the chunk-wise state-passing schedule described in Section 8.2 of Mamba-2 (Dao & Gu, 2024) and illustrated in their Figure 7 (right). In words: 1. Initial state. Rank waits for the final hidden state produced by rank 1 (rank 0 starts from zeros or an optional user-supplied context state). 2. Local work. Using that state and its own input slice xj, the rank runs the SSM kernel on its private chunk and produces the chunks output tokens yj and the next hidden state hj+1. The computation is entirely local and no cross-rank dependency exists. 3. State hand-off. If following rank exists, hj+1 is sent asynchronously to rank j+1; otherwise the pipeline finishes. The only communication is single tensor of shape [B d_state] per boundarywhere is the micro-batch size on the rank, the number of SSM heads, and dstate the width of each state vectorso bandwidth remains linear in the number of GPUs. CausalConv1D chunks. The preceding causal convolution stage is split in the same fashion: each rank gets the last k1 timesteps from its left neighbour, performs its local depth-wise convolution, and forwards the k1 boundary activations to the next rank. 36 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance 4. Post-trainining In this section, we detail the two-stage post-training procedure for our Falcon-H1 models, which consists of Supervised Fine-Tuning (SFT) followed by Offline Reinforcement Learning."
        },
        {
            "title": "4.1 Post-training Data",
            "content": "Compared to the previous Falcon model series, we have significantly expanded the scope and quality of the supervised fine-tuning data. This expansion spans multiple domains and includes millions of high-quality samples. Notably, our efforts focus on improving the models capabilities in complex reasoning, mathematical and scientific problem-solving, instruction-following, and function calling. Some of the key improvements include: Mathematical Problem-Solving: We enhanced both the breadth and difficulty of mathematical problems by curating and rewriting high-quality solutions. Inspired by OpenMathInstruct2 (Toshniwal et al., 2024) and AceMath (Liu et al., 2024e), we synthesized math problems across diverse sub-domains, carefully filtering out incorrect solutions. Additional synthetic examples were generated using data from the pretraining math corpus, ensuring high correctness and varying difficulty levels. Scientific Problem-Solving: We improved the models performance in scientific reasoning, particularly across STEM domains. Problem-solution pairs were extracted or synthesized from existing pretraining corpora, with subsequent refinements to ensure consistent formatting and data quality. Conversational and Instruction-Following: We enhanced the models conversational abilities and instruction-following by improving personalization, stylistic variety and multiround long conversation capability. The post-training corpus is based on both license-permissive open datasets and proprietary data sources. From open data, we partially adopted and refined the datasets such as OpenMathInstruct2 (Toshniwal et al., 2024), Tulu3 (Lambert et al., 2024), Smoltalk (Ben Allal et al., 2024), and hermes-function-calling-v1 (\"interstellarninja\"). In many cases, questions and solutions were further optimized to improve clarity and correctness. Proprietary data includes high-quality extracted, refined, and synthetic sample pairs from internal high-quality sources on both STEM and non-STEM domains, along with human-annotated examples targeting specific skills. This hybrid strategy ensures diverse, accurate, and skill-aligned training data. We find that data quality and structure have much greater impact on post-training performance than data volume alone. We carefully decontaminated the post-training data against popular benchmarks to prevent unintended data leakage and ensure fair evaluation."
        },
        {
            "title": "4.2 Supervised Fine-Tuning (SFT)",
            "content": "We performed extensive data mixture and duration testing during SFT to balance the models performance across different domains. Compared to pretraining, SFT is more sensitive to data mixture. The resulting checkpoints that we released offer strong general performance but can be further adjusted for specific use cases. Our SFT process is divided into two distinct stages: primary stage with 16k context length, followed by long-context stage extending to 128k. The initial 16k stage was conducted for 3 GT, 37 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance succeeded by an additional 3GT for the long-context stage. During the total 6GT of SFT, we repeated different data sources for different amount of times (epochs) depending on their volume and weight in the mixture. The most repeated data source was Tulu3, with around 50% weight in the data mixture and around 3.5 epochs over the SFT duration. The other data sources were repeated for fewer than 2 epochs. For the main 16k stage, we employed Warmup-Stable-Decay (WSD) learning rate schedule. The decay phase followed an exponential profile, reducing the learning rate by factor of eight to minimum value of ηmin = η/8. The subsequent 128k long-context stage proceeded with constant learning rate equal to this minimal value, ηmin. The 128k stage was omitted for our smallest 0.5B parameter model due to the inherent limitations in processing long sequences by models of that scale. The key hyperparameters for our SFT setup are summarized in Table 11. Hyperparameter / Setting Value / Details Sequence Length Batch Size (b) Learning Rate (η) AdamW Parameters Learning Rate Schedule LR Decay Long Context Stage Epochs per Data Source 16k for the main stage; 128k for long context stage 1 million tokens (MT) 128 106 β1 = 0.9; β2 = 0.95; no weight decay WSD: 50MT warmup, 1.5GT stable, 1.5GT decay Exponential schedule from η to ηmin = η/8 +3GT with constant learning rate ηmin 3.5 Table 11: Hyperparameters for the Supervised Fine-Tuning (SFT) stage. Finally, we note that we have used slightly different batch sizes, in the range of 0.25MT to 4MT, for different model sizes to meet the desired GPU allocation on the cluster and the job run time12. Then, for given batch size b, we have used square root batch scaling (19), taking the values from table 11 as reference values bref , ηref . With such scaling, we have observed minimal impact of batch size on the final performance of the SFT model."
        },
        {
            "title": "4.3 Direct Preference Optimization (DPO)",
            "content": "For the DPO stage, we utilized fork of the AllenAI open-instruct repository 13. Similar to the SFT stage, our data mixture was built upon Tulu3 (Lambert et al., 2024), supplemented by several other open-source and in-house preference datasets. We employed the standard DPO loss function (dpo_loss_type: dpo_norm). The hyperparameters for the DPO stage are outlined in Table 12. Hyperparameter / Setting Value / Details Batch Size Learning Rate Learning Rate Schedule DPO Loss Parameter (β) AdamW Parameters 256 5 106 Linear decay to zero over 2 epochs, with warmup ratio of 0.1 5 PyTorch default (β1 = 0.9, β2 = 0.999) Table 12: Hyperparameters for the Direct Preference Optimization (DPO) stage. 12For instance, the 128k long-context stage, being computationally intensive due to context parallelism, necessitated larger batch sizes to enable higher degrees of data parallelism (DP) for quicker job execution. 13https://github.com/allenai/open-instruct 38 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance An important aspect of our DPO strategy was the stopping criterion. Instead of training for the full two epochs, at which point the learning rate schedule concludes, we found that stopping at approximately one epoch yielded superior results. This approach was empirically determined to be more effective than either completing the full two-epoch schedule or using linear scheduler that terminates after single epoch. 5. Evaluation To provide rigorous and reproducible comparison, we benchmarked the Falcon-H1 series against comprehensive suite of leading models, including Qwen3 (Yang et al., 2025), Qwen2.5 (Yang et al., 2024b), Gemma3 (Team et al., 2025), Falcon3 (Team, 2024), Llama3 (Grattafiori et al., 2024) and Llama4 14. Our evaluation pipeline is built upon foundation of established opensource frameworks: lm-evaluation-harness (Gao et al., 2024), evalchemy (Raoof et al., 2025), evalplus (Liu et al., 2023b) and helmet (Yen et al., 2025). Standardization and Reproducibility. Our methodology was standardized across all models to ensure fair comparison. All evaluations were conducted within the same Docker environment to eliminate system-level variance. For the Qwen3 series, we disabled the \"thinking mode\" on all benchmarks to align its inference process with that of other models. Framework-Specific Settings. To ensure stability, all evalchemy evaluations were pinned to specific commit hash (f735e77). For relevant mathematical benchmarks within this framework, we standardized the number of generation turns to 16 and applied an identical system prompt across all models. Furthermore, all final math results were post-processed using Math-Verify (Kydlíček) for consistent verification. For all other frameworks, we adhered to their default settings to maintain comparability with established results in the literature."
        },
        {
            "title": "5.1 Base Models",
            "content": "For base models, we report in Table 13 the benchmarks and settings used for the evaluations. Basically, we check the models capabilities in five main domains: General, Math, Science, Code, and Multilingual. Noting that some benchmarks contains cross-domain tasks and do not fall neatly into single category. Falcon-H1-0.5B-Base. The results presented in Table 14 establish Falcon-H1-0.5B as new benchmark for sub-1B parameter base models. Despite possessing the smallest footprint in the comparison, it leads on every Math, Science, and Code benchmark, often by substantial margins (e.g., GSM8k: 60.20 vs. 50.04; MATH-lvl5: 15.18 vs. 9.29). The model further confirms its strength in structured, knowledge-intensive reasoning by securing the highest scores on BBH and MMLU. Its performance on commonsense benchmarks like HellaSwag and Winogrande is competitive but surpassed by larger models in the 1B-1.6B class, suggesting design that prioritizes deep reasoning capabilities over broad world knowledge. Ultimately, Falcon-H1-0.5Bs results demonstrate that with targeted design choices, even 0.5B model can achieve highly competitive, non-trivial performance on complex tasks. Falcon-H1-1.5B-Base and Falcon-H1-1.5B-Deep-Base. The evaluation results for our 1.5Bscale models, presented in Table 15, highlight the significant benefits of increased model depth. As discussed in Section 2.3.2, the Falcon-H1-1.5B-Deep variant, which features more layers while maintaining similar parameter count, establishes itself as the clear state-of-the-art model in its class. Its performance is highly competitive, often rivaling that of current leading 7B to 10B models 14https://huggingface.co/meta-llama/Llama-4-Scout-17B-16E 39 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Benchmark General Settings BBH (Suzgun et al., 2022) ARC-C (Clark et al., 2018) HellaSwag (Zellers et al., 2019) Winogrande (Sakaguchi et al., 2021) MMLU (Hendrycks et al., 2020) logprobs, 3-shot logprobs, 25-shot logprobs, 10-shot logprobs, 5-shot logprobs, 5-shot Math GSM8k (Cobbe et al., 2021b) MATH lvl5 (Hendrycks et al., 2021a) strict match, 5-shot math verify, logprobs, 4-shot Science GPQA (Rein et al., 2023) MMLU-Pro (Wang et al., 2024) MMLU-stem (Hendrycks et al., 2020) logprobs, 5-shot logprobs, 5-shot logprobs, 5-shot Code HumanEval (Chen et al., 2021) HumanEval+ (Liu et al., 2023b) MBPP (Austin et al., 2021) MBPP+ (Liu et al., 2023b) pass@1 pass@1 pass@1 pass@ Multilingual Framework lm-eval-harness lm-eval-harness lm-eval-harness lm-eval-harness lm-eval-harness lm-eval-harness lm-eval-harness lm-eval-harness lm-eval-harness lm-eval-harness evalplus evalplus evalplus evalplus Multi-Hellaswag (Dac Lai et al., 2023) MGSM (Shi et al., 2022) Multi-MMLU (Dac Lai et al., 2023) logprobs, 0-shot flexible extract, 8-shot native CoT logprobs, 5-shot lm-eval-harness lm-eval-harness lm-eval-harness Table 13: Evaluation settings and benchmark sources for base models. Tasks General BBH MMLU ARC-C HellaSwag Winogrande Math GSM8k MATH lvl5 Science GPQA MMLU-Pro MMLU-stem Code HumanEval HumanEval+ MBPP MBPP+ Falcon-H10.5B Qwen30.6B Qwen2.50.5B Gemma31B Llama3.21.2B Falcon31.6B 40.22 55.04 46.93 56.30 59. 60.20 15.18 29.70 30.04 57.12 35.98 31.10 52.12 43.39 36.07 52.64 44.80 53.51 60.54 50.04 9.29 29.11 22.99 50. 31.71 27.44 51.06 42.33 32.62 47.61 35.32 51.79 56.83 34.80 4.23 27.94 18.98 43.74 29.27 25.00 40.74 34.66 30.26 26.33 39.33 62.94 62. 2.20 1.21 24.66 11.31 27.59 6.71 5.49 12.70 9.52 30.72 32.39 39.42 65.73 62.75 7.05 0.98 23.57 11.80 30. 18.90 16.46 35.98 29.89 35.24 45.14 47.87 62.30 61.17 34.95 3.40 27.85 16.11 40.06 10.37 9.15 12.43 9.52 Table 14: Performance of the 0.5B+ Base models. 40 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance such as Qwen2.5-7B (Yang et al., 2024b) and Falcon3-7B/10B (Team, 2024). This architectural choice proves particularly impactful for reasoning-intensive tasks; the deep model substantially outperforms its shallower counterpart and other peers on Science and Math benchmarks like MATHlvl5 (+4.38 points) and MMLU-Pro (+5.54 points). This strong performance continues into the Code and Multilingual domains, where FalconH1-1.5B-Deep consistently leads or is highly competitive. Although it is surpassed by Qwen31.7B (Yang et al., 2025) on specific benchmarks like HumanEval and GSM8k, its dominant overall profile underscores the efficacy of our deep architecture. The result also suggests that while benchmark categories are distinct, the underlying skills required to solve them are often shared. strong, generalizable reasoning capability, which our deeper model demonstrates, appears to be critical polyvalent skill that confers benefits across multiple domains. Tasks General BBH MMLU ARC-C HellaSwag Winogrande Math GSM8k MATH lvl5 Science GPQA MMLU-Pro MMLU-stem Code HumanEval HumanEval+ MBPP MBPP+ Multilingual Multi-Hellaswag Multi-MMLU MGSM Falcon-H11.5B-Deep Falcon-H11.5B Qwen31.7B Qwen2.51.5B Gemma31B Llama3.21.2B Falcon31.6B 52.37 66.29 55.89 69.72 67.09 68.69 24.77 32.80 41.07 67.43 52.44 46.34 70.90 60. 50.36 52.00 60.33 46.57 61.81 53.24 66.76 65.59 52.01 20.39 29.11 35.53 63.37 50.00 42.68 65.08 55.03 46.62 46.51 50. 43.05 62.46 55.72 67.09 66.30 70.74 16.39 29.45 33.81 61.53 67.68 60.98 67.72 58.99 46.47 - - 40.55 61.13 54.27 67.86 64. 63.00 8.84 28.36 28.72 54.93 35.37 29.27 60.05 49.47 42.89 48.09 45.13 30.26 26.33 39.33 62.94 62.59 2.20 1. 24.66 11.31 27.59 6.71 5.49 12.70 9.52 46.14 26.50 - 30.72 32.39 39.42 65.73 62.75 7.05 0.98 23.57 11.80 30. 18.90 16.46 35.98 29.89 41.61 28.22 4.73 35.24 45.14 47.87 62.30 61.17 34.95 3.40 27.85 16.11 40.06 10.37 9.15 12.43 9. 31.42 31.56 9.40 Table 15: Performance of the 1B+ Base models. Falcon-H1-3B-Base. At the 3B-4B parameter scale, Falcon-H1-3B showcases exceptional training efficiency. Despite being trained on only 2.5T tokensan order of magnitude less data than the 36T tokens reportedly used for Qwen3 (Yang et al., 2025)our model delivers highly competitive performance profile, as shown in Table 16. While Qwen3-4Bs extensive training confers an advantage on many general, science, and coding benchmarks, Falcon-H1-3Bs resource-efficient approach enables it to achieve state-of-the-art capabilities in tasks like advanced mathematical reasoning. It secures leading scores on the challenging MATH-lvl5 benchmark (25.83) and the multilingual MGSM (64.00). Crucially, its performance had not yet plateaued at the conclusion of training, suggesting that its already strong results represent conservative estimate of its full potential and that our targeted data strategy can achieve specialized excellence with significantly less computational cost. 41 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Tasks General BBH MMLU ARC-C HellaSwag Winogrande Math GSM8k MATH lvl5 Science GPQA MMLU-Pro MMLU-stem Code HumanEval HumanEval+ MBPP MBPP+ Multilingual Multi-Hellaswag Multi-MMLU MGSM Falcon-H13B Qwen34B Qwen2.53B Gemma34B Llama3.23B Falcon33B 53.17 68.39 61.35 73.85 68.11 68.31 25.83 32.63 40.58 69.55 59.15 53.66 71.43 57. 55.15 54.78 64.00 56.88 72.92 64.33 75.74 72.30 81.65 24.47 34.90 46.18 75.58 74.39 68.90 74.60 63.76 56.69 64.91 - 46.40 65.56 56.57 74.60 71.03 74.60 16.09 28.44 32.12 62.23 42.68 35.37 59.52 50.53 54.71 55.13 59.93 40.41 59.41 58.36 77.62 72. 37.60 6.95 29.78 28.34 51.70 33.54 28.05 60.05 51.32 61.03 52.57 - Table 16: Performance of the 3B+ Base models. 39.45 55.94 51.02 76.39 72. 27.82 1.74 28.78 25.08 47.67 29.27 26.22 48.94 39.42 53.89 45.45 20.33 44.02 56.77 55.12 67.13 65.11 64.67 11. 29.78 29.03 55.34 36.59 31.71 51.85 42.06 36.30 38.31 31.80 Falcon-H1-7B-Base. At the 7B+ parameter scale, as detailed in Table 17, Falcon-H1-7B establishes new state-of-the-art benchmark, particularly in complex, knowledge-intensive domains. It demonstrates clear leadership in advanced reasoning by securing top scores on MMLU (77.38), MATH-lvl5 (34.67), and the challenging GPQA science benchmark (36.58). Furthermore, it excels in code generation tasks, leading on MBPP and MBPP+, and also achieves the best performance in multilingual mathematics on MGSM. This consistent top-tier performance underscores its exceptional capabilities in structured reasoning and specialized knowledge application. The competitive landscape at this scale also reveals distinct strengths among other models. The Qwen models, for instance, exhibit strong aptitude for coding, with Qwen3-8B leading on HumanEval benchmarks. Meanwhile, Gemma3-12B, despite its larger size, primarily excels on commonsense reasoning tasks like HellaSwag and Winogrande. This distribution of results highlights key finding: while larger models may show advantages in general commonsense tasks, the architectural and data choices of Falcon-H1-7B make it superior model for high-value, reasoning-focused applications in science, math, and code, while also maintaining an advantage over its counterpart, Qwen3-8B, on general tasks requiring more world knowledge. Falcon-H1-34B-Base. In the 34B parameter class, we evaluate Falcon-H1-34B against highly competitive field that includes models up to the 70B scale, as well as the Llama4-scout-17B MoE model (109B). Qwen3-32B is not included since no base model checkpoint was released. As shown in Table 18, despite this challenging comparison, Falcon-H1-34B demonstrates state-of-the-art, parameter-efficient performance, distinguishing itself in specialized, complex domains, even when compared against significantly larger models. It secures the top position on challenging reasoning benchmarks such as BBH (69.36) and Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Tasks General BBH MMLU ARC-C HellaSwag Winogrande Math GSM8k MATH lvl5 Science GPQA MMLU-Pro MMLU-stem Code HumanEval HumanEval+ MBPP MBPP+ Multilingual Multi-Hellaswag Multi-MMLU MGSM Falcon-H17B Qwen38B Qwen2.57B Gemma312B Llama3.18B Falcon37B Falcon310B 60.61 77.38 65.19 81.26 79. 73.46 34.67 36.58 48.38 77.20 67.68 63.41 78.57 67.20 65.16 67.55 74.53 58.44 76.63 67.75 79.60 76.80 83.02 28. 35.65 48.25 78.53 87.80 82.32 75.13 64.02 62.13 68.71 67.87 53.72 74.17 63.91 80.20 76.01 83.09 22.58 32.30 43.55 71. 57.32 48.78 76.72 63.49 58.74 64.07 71.07 54.33 74.23 67.58 84.22 79.79 71.19 17.22 34.56 42.72 68.51 45.12 36.59 73.02 59. 70.62 67.14 - 46.52 65.17 57.68 81.97 77.11 49.51 6.57 31.46 32.71 55.72 39.02 31.71 61.38 51.32 61.72 53.58 41. 50.88 69.98 62.71 76.69 73.64 76.95 20.09 35.07 39.23 67.71 50.00 43.29 67.99 57.14 46.58 - 52.20 59.30 73.22 67.49 79.64 79. 82.11 25.38 35.40 42.45 70.85 51.83 44.51 73.54 61.38 50.91 53.17 59.00 Table 17: Performance of the 7B+ Base models. MATH-lvl5 (40.71). Furthermore, its leadership on the GPQA benchmark (42.70), all code generation tasks (HumanEval, HumanEval+), and multilingual mathematics (MGSM) underscores its exceptional and well-rounded capabilities in both reasoning and knowledge-intensive applications. The performance of other models at this scale reveals interesting trade-offs. The larger Qwen2.5-72B and Llama3.1-70B models show an advantage on general knowledge and commonsense reasoning tasks like MMLU, ARC-C, and HellaSwag. However, Falcon-H1-34B remains highly competitive, often securing the second-best score. This pattern reinforces key finding from our smaller models: while increased scale can confer advantages on broad-knowledge tasks, the specialized architecture and data strategy of Falcon-H1 enable it to deliver superior, parameter-efficient performance on complex reasoning and code generation tasks. This positions Falcon-H1-34B as leading model in its class and establishes it as far more cost-efficient alternative to 70B+ models for developers seeking powerful base for fine-tuning or specialized reasoning applications. Overall Remarks of Falcon-H1 Base models. key finding across our suite of base models is the achievement of state-of-the-art performance with remarkable training efficiency. Despite being trained on modest 2.5T to 18T tokens, the Falcon-H1 series consistently challenges and often surpasses competitor models trained on substantially larger datasets. Our models establish clear leadership in complex, reasoning-intensive domains such as mathematics, science, and code generation across all evaluated scales. Notably, the performance of these models, particularly the smaller variants, had not yet plateaued at the conclusion of pretraining, indicating significant headroom for further optimizations with extended training. The resulting models exhibit wellbalanced performance profile, positioning them as powerful and efficient foundations for fine-tuning on specialized downstream applications. 43 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Tasks General BBH MMLU ARC-C HellaSwag Winogrande Math GSM8k MATH lvl Science GPQA MMLU-Pro MMLU-stem Code HumanEval HumanEval+ MBPP MBPP+ Multilingual Multi-Hellaswag Multi-MMLU MGSM Falcon-H134B Qwen2.572B Qwen2.532B Gemma327B Llama3.170B Llama4scout 69.36 83.46 71.25 85.68 82. 76.50 40.71 42.70 57.18 83.82 70.12 64.63 83.33 70.37 72.62 76.76 82.40 67.77 85.96 72.44 87.57 83.74 89.76 38. 42.28 60.22 84.81 59.15 51.22 87.04 70.63 71.20 78.54 82.20 67.45 83.18 70.48 85.13 82.32 90.14 36.40 39.68 58.05 82. 59.76 51.83 83.07 68.78 - - - 61.60 78.32 70.31 86.19 82.40 81.35 25.38 35.82 49.64 76.59 48.78 40.85 76.19 61. 74.01 72.41 - Table 18: Performance of the 34B+ Base models. 62.78 78.49 69.20 87.78 85.32 80.52 18.81 36.49 47.07 70.35 57.32 50.61 78.84 66. 74.65 71.10 70.73 61.71 77.98 62.97 84.01 78.93 83.24 27.19 35.99 50.16 72.57 57.32 48.78 77.78 64.29 72.02 72.38 75. For the full evaluation results on the models multilingual capabilities, please refer to the dedicated Appendix section D.1, where we report each models performance by language."
        },
        {
            "title": "5.2 Instruct Models",
            "content": "For instruction-tuned models, we report in Table 19 the benchmarks and settings used for the evaluations. For these models, we expanded the evaluation scope to cover broader range of domains and tasks. While we provide general categorization for clarity, it is important to note that some benchmarks are cross-domain and do not fall neatly into single category. Falcon-H1-0.5B-Instruct model was evaluated exclusively on non-multilingual tasks, as it was trained only on English data. Long context benchmarks are only reported on 34B scale models for simplicity. For both the multilingual and long-context evaluations, this section reports the average scores for each task category. detailed, per-language and per-task breakdown of these results is available in Appendix D.2 and Appendix D.3, respectively. Falcon-H1-0.5B-Instruct. At the sub-1B parameter scale, the Falcon-H1-0.5B-Instruct model sets new state-of-the-art benchmark, demonstrating clear and consistent advantage in complex, reasoning-intensive domains as shown in Table 20. The models superiority is most pronounced in Math, where it achieves sweeping dominance across all five benchmarks. Its performance on GSM8k (68.39) and MATH-500 (58.40) is particularly notable, substantially outperforming all competitors. This strength in structured reasoning extends to the Science and Code categories, where it secures top scores on the majority of tasks, including MMLU-Pro, HumanEval, and CRUXEval. Furthermore, its leading performance on IFEval (72.07) confirms its exceptional ability to adhere to complex instructions. However, the competitive landscape is not uniform. Competitors 44 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Benchmark General BBH (Suzgun et al., 2022) ARC-C (Clark et al., 2018) TruthfulQA (Lin et al., 2021) HellaSwag (Zellers et al., 2019) MMLU (Hendrycks et al., 2020) Math Settings logprobs, 3-shot logprobs, 0-shot logprobs, 0-shot logprobs, 0-shot logprobs, 5-shot GSM8k (Cobbe et al., 2021b) MATH-500 (Lightman et al., 2023) AMC-23 AIME-24 (AIME) AIME-25 (AIME) strict match, 5-shot accuracy average accuracy, 16 repetitions average accuracy, 16 repetitions average accuracy, 16 repetitions Science GPQA (Rein et al., 2023) GPQA_Diamond (Rein et al., 2023) MMLU-Pro (Wang et al., 2024) MMLU-stem (Hendrycks et al., 2020) logprobs, 5-shot average accuracy, 3 repetitions logprobs, 5-shot logprobs, 5-shot Code HumanEval (Chen et al., 2021) HumanEval+ (Liu et al., 2023b) MBPP (Austin et al., 2021) MBPP+ (Liu et al., 2023b) LiveCodeBench (Jain et al., 2024) CRUXEval (Gu et al., 2024) Instruction Following & Others IFEval (Zhou et al., 2023) Alpaca-Eval (Li et al., 2023) MTBench (Bai et al., 2024) LiveBench (White et al., 2024) Multilingual pass@1 pass@1 pass@1 pass@1 accuracy pass@1, input & output average inst & prompt avg accuracy LC winrate turn 1 & 2 average global_average Multi-Hellaswag (Dac Lai et al., 2023) MGSM (Shi et al., 2022) Multi-MMLU (Dac Lai et al., 2023) logprobs, 0-shot flexible extract, 8-shot native CoT logprobs, 5-shot Long Context HELMET-LongQA (Yen et al., 2025) HELMET-RAG (Yen et al., 2025) HELMET-Recall (Yen et al., 2025) default default default Framework lm-eval-harness lm-eval-harness lm-eval-harness lm-eval-harness lm-eval-harness lm-eval-harness evalchemy evalchemy evalchemy evalchemy lm-eval-harness evalchemy lm-eval-harness lm-eval-harness evalplus evalplus evalplus evalplus evalchemy evalchemy lm-eval-harness evalchemy evalchemy evalchemy lm-eval-harness lm-eval-harness lm-eval-harness helmet helmet helmet Table 19: Evaluation settings and benchmark sources for instruction-tuned models. Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance like Gemma3-1B and Qwen3-0.6B show an edge on certain coding (MBPP, LiveCodeBench) and preference-based instruction following (Alpaca-Eval, LiveBench) benchmarks. Similarly, on general commonsense tasks like HellaSwag and ARC-C, larger models in the comparison group hold an advantage. This specialized profile suggests that our fine-tuning strategy for Falcon-H1-0.5BInstruct prioritizes deep, multi-step reasoning and precise instruction execution over performance on conversational or broad-knowledge benchmarks. This trade-off firmly establishes the model as the leading choice for applications requiring robust, complex problem-solving at an efficient scale. Tasks Falcon-H1-0.5B Qwen3-0.6B Qwen2.5-0.5B Gemma3-1B Llama3.2-1.2B Falcon3-1.6B General BBH ARC-C TruthfulQA HellaSwag MMLU Math GSM8k MATH-500 AMC-23 AIME-24 AIME-25 Science GPQA GPQA_Diamond MMLU-Pro MMLU-stem Code HumanEval HumanEval+ MBPP MBPP+ LiveCodeBench CRUXEval Instruction Following IFEval Alpaca-Eval MTBench LiveBench 42.91 37.80 44.12 51.93 53.40 68.39 58.40 33.13 3.75 4.38 29.95 27.95 31.03 54.55 51.83 45.12 42.59 33.07 7.05 25.75 72.07 10.79 7.06 20.80 32.95 31.06 51.65 42.17 42. 42.61 46.00 27.97 2.71 1.67 26.09 25.08 16.95 39.30 41.46 37.19 56.08 47.08 9.78 23.63 62.16 9.59 5.75 27.78 33.26 33.28 46.19 52.38 46.07 38.51 27.80 12.50 0.62 0. 26.85 24.24 18.73 39.83 36.59 32.32 46.83 39.68 2.94 14.88 32.11 3.26 4.71 14.27 35.86 34.13 42.17 42.24 40.87 42.38 45.40 19.22 0.42 1.25 28.19 21.55 14.46 35. 40.85 37.20 57.67 50.00 5.09 12.70 61.48 17.87 7.03 18.79 33.21 34.64 42.08 55.30 45.93 44.28 13.20 7.19 1.46 0.00 26.59 25.08 16.20 39.16 34.15 29.88 33.60 29.37 2.35 0. 55.34 9.38 6.37 14.97 34.47 43.09 42.31 58.53 46.10 44.05 19.80 6.87 0.41 0.21 26.76 31.31 18.49 39.64 22.56 20.73 20.63 17.20 0.78 15.58 54.26 6.98 6.03 14. Table 20: Performance of the 0.5B+ Instruct models. Falcon-H1-1.5B-Instruct and Falcon-H1-1.5B-Deep-Instruct. At the 1.5B parameter scale, the Falcon-H1 instruct models establish clear dominance, with both the deep and shallow architecture variants setting new state-of-the-art benchmarks. As shown in Table 21, Falcon-H1-1.5BDeep-Instruct achieves comprehensive leadership across nearly all evaluated domains, securing the top position on the vast majority of General, Math, Science, Code, and Multilingual tasks, reaching level of performance competitive with current state-of-the-art 7B models, like Qwen3-8B (Yang et al., 2025), Qwen2.5-7B (Yang et al., 2024a) (check Table 23). This sweeping success highlights the profound impact of our deep architectural design combined with instruction tuning. The performance gap is particularly striking in complex reasoning, where the model substantially outperforms all peers on benchmarks like GSM8k (82.34) and MATH-500 (77.80). The shallower Falcon-H11.5B-Instruct model also delivers an exceptionally strong performance, consistently securing the 46 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance second-best score across most benchmarks and often outperforming larger models like Qwen3-1.7B. Its leadership on Alpaca-Eval further underscores its well-rounded instruction-following capabilities. The overwhelming evidence from these evaluations confirms that the Falcon-H1-1.5B-Instruct models, particularly the deep variant, are leading choices for sophisticated, reasoning-driven tasks, delivering performance that often transcends their parameter scale. Falcon-H11.5B-Deep Falcon-H11.5B Qwen31.7B Qwen2.51.5B Gemma31B Llama3.21B Falcon31.6B Tasks General BBH ARC-C TruthfulQA HellaSwag MMLU Math GSM8k MATH-500 AMC-23 AIME-24 AIME-25 Science GPQA GPQA_Diamond MMLU-Pro MMLU-stem Code HumanEval HumanEval+ MBPP MBPP+ LiveCodeBench CRUXEval 54.43 43.86 50.48 65.54 66.11 82.34 77.80 56.56 14.37 11.04 33.22 40.57 41.89 67.30 73.78 68.90 68.25 56.61 23.87 52.32 Instruction Following IFEval Alpaca-Eval MTBench LiveBench Multilingual Multi-Hellaswag Multi-MMLU MGSM 83.50 27.12 8.53 36.83 53.14 53.00 60.00 46.47 42.06 49.39 63.33 62. 74.98 74.00 46.09 12.50 9.58 26.34 35.19 37.80 64.13 68.29 61.59 64.81 56.35 17.61 39.57 80.66 28.18 8.46 34.13 49.38 48.06 58.00 35.18 34.81 45.98 49.27 57. 69.83 73.00 43.59 11.25 8.12 27.68 33.33 23.54 54.30 67.68 60.96 58.73 49.74 14.87 18.88 70.77 21.89 7.61 40.73 37.89 39.60 52.40 42.41 40.53 47.05 62.23 59. 57.47 48.40 24.06 2.29 1.25 26.26 25.59 28.35 54.04 56.10 50.61 64.81 56.08 12.52 34.76 45.33 9.54 7.10 21.65 42.93 45.90 45.20 35.86 34.13 42.17 42.24 40. 42.38 45.40 19.22 0.42 1.25 28.19 21.55 14.46 35.39 40.85 37.20 57.67 50.00 5.09 12.70 61.48 17.87 7.03 18.79 41.77 34.91 - 33.21 34.64 42.08 55.30 45. 44.28 13.20 7.19 1.46 0.00 26.59 25.08 16.20 39.16 34.15 29.88 33.60 29.37 2.35 0.06 55.34 9.38 6.37 14.97 39.78 35.24 29.73 34.47 43.09 42.31 58.53 46. 44.05 19.80 6.87 0.41 0.21 26.76 31.31 18.49 39.64 22.56 20.73 20.63 17.20 0.78 15.58 54.26 6.98 6.03 14.10 32.04 32.25 15.33 Table 21: Performance of the 1B+ Instruct models. Falcon-H1-3B-Instruct. At the 3B-4B scale, Falcon-H1-3B-Instruct emerges as top-performing and highly versatile model, demonstrating clear strengths in reasoning, science, and instruction following  (Table 22)  . It leads on the majority of General knowledge benchmarks like MMLU and BBH, and dominates the Science category. This shows that after instruction tuning, the model excels at applying its knowledge. Furthermore, its state-of-the-art scores on IFEval and MTBench highlight its superior ability to understand and follow complex instructions. While other models show specialized strengths, such as Qwen3-4B in mathematics, Falcon-H1-3B remains highly competitive across all areas. It also shows distinct advantage in Code generation on MBPP 47 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance and excels in Multilingual tasks. This balanced and powerful performance across many different domains establishes Falcon-H1-3B-Instruct as premier, all-around model in its class. Falcon-H13B Qwen34B Qwen2.53B Gemma34B Llama3.23B Falcon33B Tasks General BBH ARC-C TruthfulQA HellaSwag MMLU Math GSM8k MATH-500 AMC-23 AIME-24 AIME-25 Science GPQA GPQA_Diamond MMLU-Pro MMLU-stem Code HumanEval HumanEval+ MBPP MBPP+ LiveCodeBench CRUXEval Instruction Following IFEval Alpaca-Eval MTBench LiveBench Multilingual Multi-Hellaswag Multi-MMLU MGSM 53.69 49.57 53.19 69.85 68.30 84.76 74.20 55.63 11.88 13.33 33.89 38.72 43.69 69. 76.83 70.73 79.63 67.46 26.81 56.25 85.05 31.09 8.72 36.86 58.34 54.90 63.90 51.07 37.71 51.75 55.31 67.01 80.44 85.00 66.88 22.29 18.96 28.02 40.74 29.75 67. 84.15 76.83 68.78 59.79 39.92 69.63 84.01 36.51 8.45 51.34 43.12 50.70 68.90 46.55 43.77 58.11 64.21 65.09 57.54 64.20 39.84 6.25 3.96 28.69 35.69 32.76 59. 73.78 68.29 72.75 60.85 11.74 43.26 64.26 17.37 7.79 27.32 50.81 52.90 57.30 50.01 44.88 51.68 47.68 59.53 77.41 76.40 48.12 6.67 13.33 29.19 28.62 29.71 52. 67.07 61.59 77.78 66.93 21.14 52.13 77.01 39.64 8.24 36.70 54.48 51.10 - 41.47 44.88 50.27 63.74 61.74 77.26 41.20 22.66 11.67 0.21 28.94 29.97 27.44 51. 54.27 50.00 62.17 50.53 2.74 17.75 74.00 19.69 7.96 26.37 50.93 48.40 62.20 45.02 48.21 50.06 64.24 56.76 74.68 54.20 29.69 3.96 2.29 28.69 29.29 29.71 56. 52.44 45.73 61.90 55.29 3.13 44.38 69.10 14.82 7.79 26.01 37.51 38.90 42.10 Table 22: Performance of the 3B+ Instruct models. Falcon-H1-7B-Instruct. At the 7B-12B parameter scale, Falcon-H1-7B-Instruct demonstrates highly competitive and well-rounded performance profile, outperforming the larger Gemma312B model (Team et al., 2025) on majority of benchmarks  (Table 23)  . When compared to its direct counterparts like Qwen3-8B (Yang et al., 2025), its strengths in knowledge-intensive domains become particularly evident. The model leads on all four Science benchmarks and on key General reasoning tasks such as MMLU and ARC-C. It also exhibits strong practical skills, achieving top scores on HumanEval and HumanEval+ for code generation and leading across all three Multilingual benchmarks. While competitors like Qwen3-8B and Gemma3-12B show advantages in specific areas, particularly on several Math and preference-based benchmarks (e.g., Alpaca-Eval, LiveBench), the collective results highlight key distinction. The broad and deep capabilities of Falcon-H1-7BInstruct across science, reasoning, code, and multilingualism make it an exceptionally effective and Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance versatile model for wide range of sophisticated, instruction-driven applications. Falcon-H17B Qwen38B Qwen2.57B Gemma312B Llama3.18B Falcon37B Falcon310B Tasks General BBH ARC-C TruthfulQA HellaSwag MMLU Math GSM8k MATH-500 AMC-23 AIME-24 AIME-25 Science GPQA GPQA_Diamond MMLU-Pro MMLU-stem Code HumanEval HumanEval+ MBPP MBPP+ LiveCodeBench CRUXEval 62.28 59.98 59.91 75.92 76.83 81.65 73.40 56.72 16.04 13. 36.33 56.90 51.75 77.61 86.59 81.10 80.69 68.78 35.03 66.51 Instruction Following IFEval Alpaca-Eval MTBench LiveBench Multilingual Multi-Hellaswag Multi-MMLU MGSM 85.35 40.23 8.85 45.74 67.75 67.83 73.50 47.47 42.06 53.19 60.56 71.56 78.92 83.80 70.78 28.33 19.17 25.84 43.10 34.64 66.89 84.75 79.27 71.96 62.70 45.60 72. 83.43 46.13 8.74 56.19 47.30 50.44 65.20 53.76 41.38 62.41 63.40 73.64 71.95 75.80 53.91 12.29 9.58 31.79 33.00 43.23 69.36 82.32 73.78 79.63 68.25 32.68 56. 75.25 29.48 8.45 37.13 58.74 61.20 66.10 63.36 51.96 61.02 55.63 72.50 87.49 86.20 66.88 22.50 18.75 33.98 37.71 39.88 66.54 84.76 75.61 85.71 72.22 30.92 67. 81.51 43.55 8.69 49.23 66.53 65.22 - 48.58 52.39 52.99 71.28 68.67 82.49 45.80 22.81 5.42 0.42 32.72 31.31 36.42 59.31 68.29 61.59 68.25 55.03 15.85 21. 77.04 25.48 8.29 31.73 60.74 55.53 70.70 52.12 54.35 55.58 71.81 70.81 81.05 69.00 40.00 8.75 6.25 31.21 37.21 40.73 67.43 71.95 65.85 77.25 65.87 12.72 55. 76.59 27.56 8.73 32.35 47.81 50.62 56.30 58.09 54.44 55.05 75.57 74.01 85.06 68.60 45.78 9.79 5.42 33.39 34.68 44.05 70.57 82.32 75.00 73.28 64.02 19.77 59. 78.84 24.31 8.46 34.30 52.77 53.67 64.80 Table 23: Performance of the 7B+ Instruct models. Falcon-H1-34B-Instruct. At the 34B scale, Falcon-H1-34B-Instruct demonstrates that exceptional performance does not require massive parameter counts. As shown in Table 24, our 34B model consistently competes with and often outperforms models twice its size. Its primary strength lies in its deep knowledge and reasoning, leading across the entire Science category and on general reasoning benchmarks like HellaSwag. Furthermore, its top score on MTBench confirms its high-quality conversational abilities. While larger models leverage their scale to gain an edge in mathematics and some coding benchmarks, this profile highlights Falcon-H1-34Bs remarkable parameter efficiency. It delivers state-of-the-art results in science and general reasoning while being significantly smaller, making it powerful and cost-effective choice for knowledge-intensive applications. Long-Context Capabilities of Falcon-H1. Given the efficiency benefits of Mamba-based architectures in long-context scenarios, systematic evaluation of Falcon-H1s capabilities in this area is crucial. We benchmarked Falcon-H1-34B-Instruct against several larger models using the 49 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Falcon-H134B Qwen332B Qwen2.572B Qwen2.532B Gemma327B Llama3.370B Llama4scout Tasks General BBH ARC-C TruthfulQA HellaSwag MMLU Math GSM8k MATH-500 AMC-23 AIME-24 AIME-25 Science GPQA GPQA_Diamond MMLU-Pro MMLU-stem Code HumanEval HumanEval+ MBPP MBPP+ LiveCodeBench CRUXEval 70.68 61.01 65.27 81.94 84.05 83.62 83.80 69.38 23.75 16.67 41.53 49.66 58.73 83.57 87.20 81.71 83.86 71.43 49.71 73. Instruction Following IFEval Alpaca-Eval MTBench LiveBench Multilingual Multi-Hellaswag Multi-MMLU MGSM 89.37 48.32 9.20 46.26 74.55 77.76 76. 62.47 48.98 58.58 68.89 80.89 88.78 82.00 67.34 27.71 19.79 30.20 49.49 54.68 81.64 90.85 85.37 86.24 71.96 45.01 78.45 86.97 64.21 9.05 63.05 58.39 66.20 71. 72.52 46.59 69.80 68.79 84.42 82.26 83.60 67.34 17.29 15.21 37.67 44.95 56.63 82.59 87.20 80.49 89.68 75.40 54.60 75.63 86.35 49.29 9.16 54.03 69.48 78.26 72. 68.72 44.54 70.28 73.95 82.80 78.47 82.20 68.75 17.92 11.46 34.31 40.74 56.35 82.37 90.24 82.32 87.83 74.07 49.12 73.50 81.79 39.26 9.09 52.92 65.90 73.56 73. 67.28 54.52 64.26 57.25 78.01 90.37 90.00 77.81 27.50 22.71 36.49 47.47 47.81 73.55 86.59 78.05 88.36 74.07 39.53 74.82 83.19 56.16 8.75 55.41 69.64 71.60 77. 69.15 63.65 66.15 70.24 82.08 93.71 70.60 39.38 12.92 1.25 31.99 42.09 53.29 74.88 83.53 79.87 88.09 73.81 40.31 69.53 89.94 38.27 8.98 53.11 62.30 74.58 83. 64.90 56.14 62.74 65.03 80.40 90.37 83.20 69.06 27.92 8.96 31.80 51.18 55.58 75.20 85.40 78.70 81.50 64.80 40.12 68.32 86.32 36.26 8.98 54.21 64.64 76.67 86. Table 24: Performance of the 34B-scale Instruct models. 50 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance HELMET suite (Yen et al., 2025), with category-level results presented in Table 25 and full breakdown in Appendix D.3. The findings highlight Falcon-H1s strong, parameter-efficient performance, particularly in the demanding Retrieval-Augmented Generation (RAG) task at extreme context lengths. While remaining competitive with 70B-class models at shorter sequences, our model uniquely achieves the top score on RAG at 131k tokens (62.21), surpassing all competitors, including those more than twice its size. This suggests superior ability to synthesize information from retrieved documents, key capability for practical RAG systems. The results also reveal nuanced performance profile. On tasks reliant on pure recall and long-form question answering (longQA), Falcon-H1 is surpassed by competitors like Qwen3-32B and Llama-3.3-70B at extreme context lengths. We attribute this performance gap not to architectural limitations but to our training data composition, which indicates substantial room for improvement with more curated long-context data. Crucially, despite this trade-off, Falcon-H1 still broadly outperforms the much larger Qwen2.5-72B-Instruct model across most long-context tasks. This, combined with its stateof-the-art RAG performance, positions Falcon-H1-34B as highly effective and parameter-efficient choice for real-world, long-context systems. Seq. Length HELMET-RAG 8k 16k 32k 65k 131k HELMET-Recall 8k 16k 32k 65k 131k HELMET-longQA 8k 16k 32k 65k 131k Falcon-H134B-Instruct Qwen2.5-72BInstruct 72.17 81.46 67.96 67.08 62.21 100.00 100.00 97.50 80.69 56.63 32.87 34.64 35.09 32.45 33.81 72.21 80.42 70.08 63.25 42.33 100.00 100.00 98.38 71.75 38. 35.20 39.13 39.22 36.71 32.94 Qwen332B 69.25 77.92 64.83 61.96 57.08 100.00 100.00 100.00 96.50 86.13 31.63 35.68 41.15 47.47 53.52 Llama-3.3-70BInstruct 74.29 82.33 70.21 69.08 55.38 100.00 100.00 99.63 98.81 82.19 33.67 39.75 47.53 48.57 46.06 Table 25: HELMET performance metrics at various sequence lengths. The best result in each row is in bold, and the second-best is underlined. Overall Remarks on Falcon-H1 Instruct models. The Falcon-H1 instruct series demonstrates consistent pattern of state-of-the-art performance and exceptional parameter efficiency across all evaluated scales. Our smaller models, particularly the Falcon-H1-1.5B-Deep-Instruct, redefine the performance baseline in their class, delivering reasoning capabilities in math and science that are competitive with leading 7B and 10B models. This makes them powerful and efficient alternatives for resource-constrained and edge environments. As we scale up, the Falcon-H1-7B and 34B models establish themselves as leaders in knowledge-intensive and reasoning-focused domains. They consistently excel in science, code generation, and multilingual understanding, often outperforming competitor models with more than double their parameter count. Furthermore, the Falcon-H1-34B shows distinct advantage in practical long-context applications, leading on the 131k RAG benchFalcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance mark. While larger competitor models show an edge in certain math and preference-based tasks, the Falcon-H1 series consistently provides superior balance of deep reasoning, broad knowledge, and high efficiency, positioning it as premier choice for sophisticated, real-world applications."
        },
        {
            "title": "5.3 Model Efficiency",
            "content": "We conducted comparative evaluation of prefill (input) and generation (output) throughput between Falcon-H1-34B and Qwen2.5-32B15. All experiments were conducted on H100 GPUs using the vLLM framework (Kwon et al., 2023) with tensor parallel size of 2. The performance of each phase was measured as follows: Prefill throughput test: Input sequence length was varied (2k to 262k tokens), while the output was fixed at 2,048 generated tokens per sequence with batch size of 32. Generation Throughput Test: Input sequence length was fixed at 4,096 tokens with batch size of 32, while the output generation length was varied (2k to 262k tokens). Figure 16: Model efficiency comparison between Falcon-H1-34B and Qwen2.5-32B. As shown in Table 16, the results demonstrate the superior scalability of the Falcon-H1 hybrid architecture. While Qwen2.5-32B exhibits marginal throughput advantage at shorter context lengths, Falcon-H1-34B becomes significantly more efficient as the context grows. At the longest sequence lengths tested, Falcon-H1-34B achieves up to 4x improvement in input throughput and an 8x speedup in output throughput. This performance profile makes the model exceptionally well-suited for long-context input and generation use cases. The initial advantage of the Transformer-based model at short contexts is likely attributable to the highly mature optimizations of attention mechanisms within modern inference frameworks compared to current State-Space Model (SSM) implementations. As theoretically, Mamba-based or hybrid architectures are more efficient, we believe this gap highlights promising direction for future work. We invite the community to contribute to optimizing SSM implementations, which we see as critical step in advancing the next generation of efficient large language models. 15Experiments were conducted prior to the release of Qwen3; however, we anticipate no significant efficiency differences between Qwen2.5-32B and Qwen3-32B. 52 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance 6. Model Integrations To ensure broad accessibility and facilitate immediate adoption, Falcon-H1 is deeply integrated into the open-source AI ecosystem. Table 26 summarizes the key platforms and tools supported at the time of this reports release. This list is continually expanding, with the most up-to-date information available on our project website16."
        },
        {
            "title": "General Usage",
            "content": "Fine-tuning vLLM (Kwon et al., 2023), Hugging Face (transformers (Wolf et al., 2020), PEFT (Mangrulkar et al., 2022), TRL (von Werra et al.)) Llama-Factory (Zheng et al., 2024), OUMI (Oumi Community), Axolotla, Unsloth (Daniel Han & team, 2023)"
        },
        {
            "title": "Local Deployment",
            "content": "llama.cppb, LM-Studioc, Jand, Docker Model APIe, Ollamaf, *, Apple MLXg, *"
        },
        {
            "title": "Quantization",
            "content": "AutoGPTQ (ModelCloud.ai & qubitium@modelcloud.ai, 2024) * Integrations validated internally; official support pending merge into main libraries at the time of the report release. https://github.com/axolotl-ai-cloud/axolotl https://lmstudio.ai/ https://docs.docker.com/ai/model-runner/ https://github.com/ml-explore/mlx-lm https://github.com/ollama/ollama https://github.com/skypilot-org/skypilot https://github.com/ggml-org/llama.cpp https://github.com/menloresearch/jan Table 26: Key Ecosystem Integrations for the Falcon-H1 Series. 7. Conclusion In this report, we introduced the Falcon-H1 series, new family of models built on an innovative hybrid Mamba-Transformer architecture. Our design goal was to achieve state-of-the-art performance with exceptional resource efficiency, and our comprehensive evaluations confirm the success of this approach. key advantage of Falcon-H1 is its ability to deliver superior performance while using significantly less training dataonly 2.5T to 18T tokensand offering up to 8x faster inference in long-context scenarios. This performance gain is particularly impactful at smaller scales, where our 1.5B-Deep model delivers capabilities competitive with leading 7B-10B models, making it ideal for edge deployments. At the larger end, our flagship 34B model challenges and often surpasses 70B+ competitors, particularly on knowledge-intensive tasks and practical applications like RAG at extreme context lengths. The success of this series is rooted in several key innovations: flexible hybrid architecture allowing for an optimal attention-SSM ratio, robust multilingual design, and customized training strategy that maximizes the value of high-quality data. We also observed that the models performance had not yet saturated by the end of pretraining, indicating significant headroom for future gains. Ultimately, by providing powerful, efficient, and versatile foundation for wide range of applications, the Falcon-H1 series demonstrates more sustainable and accessible path toward developing high-performance artificial intelligence. Our future work will focus on several primary areas. First, we will prioritize data enhancement. We plan to iteratively refine both our base and instruction-tuned models by curating broader and more diverse high-quality datasets. Second, we will continue our architectural and algorithmic innovation, with focus on effective knowledge compression and scaling to contexts beyond 256k. 16https://tiiuae.github.io/Falcon-H1/ 53 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Finally, we will continue to scale our models strategically, exploring novel techniques to deepen their core reasoning capabilities. Through these efforts, we aim to continue developing models that are not only state-of-the-art but also fundamentally more efficient and accessible, thereby contributing to the sustainable advancement of artificial intelligence. 8. Authors Core Contributors Jingwei Zuo, Maksim Velikanov, Ilyas Chahed, Younes Belkada, Dhia Eddine Rhayem, Guillaume Kunsch*, Hakim Hacid"
        },
        {
            "title": "Contributors",
            "content": "Quantization and Integration: Hamza Yous, Brahim Farhat, Ibrahim Khadraoui Data & Evaluation: Mugariya Farooq, Giulia Campesan, Ruxandra Cojocaru, Yasser Djilali, Shi Hu, Iheb Chaabane, Puneesh Khanna, Mohamed El Amine Seddik, Ngoc Dung Huynh, Phuc Le Khac, Leen AlQadi, Billel Mokeddem, Mohamed Chami, Abdalgader Abubaker Platform & Infrastructure Support: Mikhail Lubinets, Kacper Piskorski, Slim Frikha *Individual who has departed from our team; work was conducted at TII. 9. Acknowledgments We extend our sincere gratitude to the contributors and maintainers from the open-source AI community who were instrumental in integrating Falcon-H1 into the ecosystem. For their invaluable help with fine-tuning libraries, we thank Wing Lian for the support on Axolotl, Daniel Han for the support on Unsloth, Yaowei Zheng for the support on Llama-Factory, and the OUMI maintainers for their assistance. For their support with local deployment tools, we are grateful to Prince Canuma and Awni Hannun for their work on the Apple MLX integration, and to Georgi Gerganov, compilade, and Gabe Goodhart for their extensive efforts on llama.cpp and Ollama integration. We also thank the Jan maintainers for their help. Finally, for their crucial support on core libraries, we thank Arthur Zucker for his guidance on Hugging Face Transformers integration and the vLLM maintainers for their help in incorporating Falcon-H1 into their high-performance inference library."
        },
        {
            "title": "References",
            "content": "European Parliament Proceedings Parallel Corpus 1996-2011, Release v7. https://www.statmt. org/europarl/. Project Gutenberg. https://www.gutenberg.org/. Makhzan Dataset. https://github.com/zeerakahmed/makhzan/. AIME. AIME problems and solutions, 2025. URL https://https://artofproblemsolving.com/ wiki/index.php/AIME_Problems_and_Solutions. 54 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. GQA: Training generalized multi-query transformer models from multi-head checkpoints. In The 2023 Conference on Empirical Methods in Natural Language Processing, 2023. URL https://openreview.net/forum?id=hmOwOZWzYE. Aida Amini, Saadia Gabriel, Shanchuan Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. MathQA: Towards interpretable math word problem solving with operation-based formalisms. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 23572367, Minneapolis, Minnesota, 6 2019. Association for Computational Linguistics. doi: 10.18653/v1/N19-1245. URL https://aclanthology.org/N19-1245. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Zhangir Azerbayev, Hailey Schoelkopf, Keiran Paster, Marco Dos Santos, Stephen McAleer, Albert Jiang, Jia Deng, Stella Biderman, and Sean Welleck. Llemma: An open language model for mathematics. arXiv preprint arXiv:2310.10631, 2023. Ge Bai, Jie Liu, Xingyuan Bu, Yancheng He, Jiaheng Liu, Zhanhui Zhou, Zhuoran Lin, Wenbo Su, Tiezheng Ge, Bo Zheng, et al. Mt-bench-101: fine-grained benchmark for evaluating large language models in multi-turn dialogues. arXiv preprint arXiv:2402.14762, 2024. Ali Behrouz, Peilin Zhong, and Vahab Mirrokni. Titans: Learning to memorize at test time. arXiv preprint arXiv:2501.00663, 2024. Loubna Ben Allal, Anton Lozhkov, Guilherme Penedo, Thomas Wolf, and Leandro von Werra. Smollm-corpus, 7 2024. URL https://huggingface.co/datasets/HuggingFaceTB/ smollm-corpus. Johan Bjorck, Alon Benhaim, Vishrav Chaudhary, Furu Wei, and Xia Song. Scaling optimal LR across token horizons. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=WYL4eFLcxG. bloc97. Ntk-aware scaled rope allows llama models to have extended (8k+) context size without any fine-tuning and minimal perplexity degradation, 2023a. URL https://www.reddit.com/r/ LocalLLaMA/comments/14lz7j5/ntkaware_scaled_rope_allows_llama_models_to_have/. bloc97. Add ntk-aware interpolation \"by parts\" correction, 2023b. URL https://github.com/ jquesnelle/scaled-rope/pull/1. Blake Bordelon, Lorenzo Noci, Mufan Bill Li, Boris Hanin, and Cengiz Pehlevan. Depthwise hyperparameter transfer in residual networks: Dynamics and scaling limit, 2023. URL https: //arxiv.org/abs/2309.16620. Andrei Broder. On the resemblance and containment of documents. 06 1997. doi: 10.1109/ SEQUEN.1997.666900. Rong Chao, Wenze Ren, Wen-Yuan Ting, Hsin-Yi Lin, Yu Tsao, and Fan-Gang Zeng. An investigation of incorporating mamba for speech enhancement. arXiv preprint arXiv:2405.06573, 2024. URL https://arxiv.org/abs/2405.06573. Accepted to IEEE SLT 2024. 55 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Lijie Chen, Binghui Peng, and Hongxun Wu. Theoretical limitations of multi-layer transformer. December 2024. URL https://arxiv.org/abs/2412.02975. , 4 Dec 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, Dave Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William Hebgen Guss, Alex Nichol, Alex Paino, Nikolas Tezak, Jie Tang, Igor Babuschkin, Suchir Balaji, Shantanu Jain, William Saunders, Christopher Hesse, Andrew N. Carr, Jan Leike, Josh Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. 2021. Shouyuan Chen, Sherman Wong, Liangjian Chen, and Yuandong Tian. Extending context window of large language models via positional interpolation, 2023. URL https://arxiv.org/abs/ 2306.15595. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, Parker Schuh, Kensen Shi, Sasha Tsvyashchenko, Joshua Maynez, Abhishek Rao, Parker Barnes, Yi Tay, Noam Shazeer, Vinodkumar Prabhakaran, Emily Reif, Nan Du, Ben Hutchinson, Reiner Pope, James Bradbury, Jacob Austin, Michael Isard, Guy Gur-Ari, Pengcheng Yin, Toju Duke, Anselm Levskaya, Sanjay Ghemawat, Sunipa Dev, Henryk Michalewski, Xavier Garcia, Vedant Misra, Kevin Robinson, Liam Fedus, Denny Zhou, Daphne Ippolito, David Luan, Hyeontaek Lim, Barret Zoph, Alexander Spiridonov, Ryan Sepassi, David Dohan, Shivani Agrawal, Mark Omernick, Andrew M. Dai, Thanumalayan Sankaranarayana Pillai, Marie Pellat, Aitor Lewkowycz, Erica Moreira, Rewon Child, Oleksandr Polozov, Katherine Lee, Zongwei Zhou, Xuezhi Wang, Brennan Saeta, Mark Diaz, Orhan Firat, Michele Catasta, Jason Wei, Kathy Meier-Hellstern, Douglas Eck, Jeff Dean, Slav Petrov, and Noah Fiedel. Palm: Scaling language modeling with pathways, 2022. URL https://arxiv.org/abs/2204.02311. Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord. Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457, 2018. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021a. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021b. Viet Dac Lai, Chien Van Nguyen, Nghia Trung Ngo, Thuat Nguyen, Franck Dernoncourt, Ryan Rossi, and Thien Huu Nguyen. Okapi: Instruction-tuned large language models in multiple languages with reinforcement learning from human feedback. arXiv e-prints, pp. arXiv2307, 2023. 56 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Michael Han Daniel Han and Unsloth team. Unsloth, 2023. URL http://github.com/unslothai/ unsloth. Tri Dao. State space duality (mamba-2) part i: The model. https://tridao.me/blog/2024/ mamba2-part1-model/, 2024a. we also choose similar dimensions as modern Transformers, e.g. = 64 or = 128.. Tri Dao. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR), 2024b. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality. arXiv preprint arXiv:2405.21060, 2024. Dao-AILab. causal-conv1d: Causal depthwise conv1d in cuda with pytorch interface. https: //github.com/Dao-AILab/causal-conv1d, 2023. Features: kernel size 2, 3, 4; supports fp32/fp16/bf16. Soham De, Samuel Smith, Anushan Fernando, Aleksandar Botev, George Cristian-Muraru, Albert Gu, Ruba Haroun, Leonard Berrada, Yutian Chen, Srivatsan Srinivasan, et al. Griffin: Mixing gated linear recurrences with local attention for efficient language models. arXiv preprint arXiv:2402.19427, 2024. Nolan Dey, Gurpreet Gosal, Zhiming, Chen, Hemant Khachane, William Marshall, Ribhu Pathria, Marvin Tom, and Joel Hestness. Cerebras-gpt: Open compute-optimal language models trained on the cerebras wafer-scale cluster, 2023. URL https://arxiv.org/abs/2304.03208. Nolan Dey, Bin Claire Zhang, Lorenzo Noci, Mufan Li, Blake Bordelon, Shane Bergsma, Cengiz Pehlevan, Boris Hanin, and Joel Hestness. Dont be lazy: Completep enables compute-efficient deep transformers, 2025. URL https://arxiv.org/abs/2505.01618. Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, et al. Hymba: hybridhead architecture for small language models. arXiv preprint arXiv:2411.13676, 2024. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. The language model evaluation harness, 07 2024. URL https://zenodo.org/records/12608602. Gardian. curated research corpus for agricultural advisory ai applications, 2024. URL https: //huggingface.co/datasets/CGIAR/gardian-ai-ready-docs. Paolo Glorioso, Quentin Anthony, Yury Tokpanov, James Whittington, Jonathan Pilault, Adam arXiv preprint Ibrahim, and Beren Millidge. Zamba: compact 7b ssm hybrid model. arXiv:2405.16712, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. 57 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Alex Gu, Baptiste Rozière, Hugh Leather, Armando Solar-Lezama, Gabriel Synnaeve, and Sida Wang. Cruxeval: benchmark for code reasoning, understanding and execution. arXiv preprint arXiv:2401.03065, 2024. Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y. K. Li, Fuli Luo, Yingfei Xiong, and Wenfeng Liang. Deepseek-coder: When the large language model meets programming the rise of code intelligence, 2024. URL https: //arxiv.org/abs/2401.14196. Xiaotian Han, Yiren Jian, Xuefeng Hu, Haogeng Liu, Yiqi Wang, Qihang Fan, Yuang Ai, Huaibo Huang, Ran He, Zhenheng Yang, et al. Infimm-webmath-40b: Advancing multimodal pre-training for enhanced mathematical reasoning. arXiv preprint arXiv:2409.12568, 2024. Bobby He and Thomas Hofmann. Simplifying transformer blocks. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id= RtDok9eS3s. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and arXiv preprint Jacob Steinhardt. Measuring massive multitask language understanding. arXiv:2009.03300, 2020. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021b. Scott Hoang and Mamba contributors. Clarification on how to interpret kernel size for conv1d https://github.com/state-spaces/mamba/issues/523, 2024. GitHub issue dis- (#523). cussing the meaning and limits of the conv1d kernel size. Shengding Hu, Yuge Tu, Xu Han, Chaoqun He, Ganqu Cui, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, Weilin Zhao, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. arXiv preprint arXiv:2404.06395, 2024a. Yiwen Hu, Huatong Song, Jia Deng, Jiapeng Wang, Jie Chen, Kun Zhou, Yutao Zhu, Jinhao Jiang, Zican Dong, Wayne Xin Zhao, and Ji-Rong Wen. Yulan-mini: An open data-efficient language model, dec 2024b. URL https://arxiv.org/abs/2412.17743. Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, J. Yang, J. H. Liu, Chenchen Zhang, Linzheng Chai, Ruifeng Yuan, Zhaoxiang Zhang, Jie Fu, Qian Liu, Ge Zhang, Zili Wang, Yuan Qi, Yinghui Xu, and Wei Chu. Opencoder: The open cookbook for top-tier code large language models. 2024a. URL https://arxiv.org/pdf/2411.04905. Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, Yang, JH Liu, Chenchen Zhang, Linzheng Chai, et al. Opencoder: The open cookbook for top-tier code large language models. arXiv preprint arXiv:2411.04905, 2024b. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei 58 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report, 2024. URL https: //arxiv.org/abs/2409.12186. \"Teknium\" \"interstellarninja\". Hermes-function-calling-dataset-v1. URL https://huggingface. co/NousResearch/hermes-function-calling-v1. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Hynek Kydlíček. Math-Verify: Math Verification Library. URL https://github.com/ huggingface/math-verify. Nathan Lambert, Jacob Morrison, Valentina Pyatkin, Shengyi Huang, Hamish Ivison, Faeze Brahman, Lester James Miranda, Alisa Liu, Nouha Dziri, Shane Lyu, et al. Tulu 3: Pushing frontiers in open language model post-training. arXiv preprint arXiv:2411.15124, 2024. Xuechen Li, Tianyi Zhang, Yann Dubois, Rohan Taori, Ishaan Gulrajani, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Alpacaeval: An automatic evaluator of instruction-following models. https://github.com/tatsu-lab/alpaca_eval, 5 2023. Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, et al. Jamba: hybrid transformermamba language model. arXiv preprint arXiv:2403.19887, 2024. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. arXiv preprint arXiv:2305.20050, 2023. Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv preprint arXiv:2109.07958, 2021. Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixtureof-experts language model. arXiv preprint arXiv:2405.04434, 2024a. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024b. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ring attention with blockwise transformers for nearinfinite context, 2023a. URL https://arxiv.org/abs/2310.01889. Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023b. URL https: //openreview.net/forum?id=1qvx610Cu7. Yan Liu, Renren Jin, Ling Shi, Zheng Yao, and Deyi Xiong. Finemath: fine-grained mathematical evaluation benchmark for chinese large language models. arXiv preprint arXiv:2403.07747, 2024c. Yue Liu, Yunjie Tian, Yuzhong Zhao, Hongtian Yu, Lingxi Xie, Yaowei Wang, Qixiang Ye, Jianbin Jiao, and Yunfan Liu. Vmamba: Visual state space model. arXiv preprint arXiv:2401.10166, 2024d. doi: 10.48550/arXiv.2401.10166. URL https://arxiv.org/abs/2401.10166. NeurIPS 2024 Spotlight. Zhengzhong Liu, Aurick Qiao, Willie Neiswanger, Hongyi Wang, Bowen Tan, Tianhua Tao, Junbo Li, Yuqi Wang, Suqi Sun, Omkar Pangarkar, Richard Fan, Yi Gu, Victor Miller, Yonghao Zhuang, Guowei He, Haonan Li, Fajri Koto, Liping Tang, Nikhil Ranjan, Zhiqiang Shen, Xuguang Ren, Roberto Iriondo, Cun Mu, Zhiting Hu, Mark Schulze, Preslav Nakov, Tim Baldwin, and Eric P. Xing. Llm360: Towards fully transparent open-source llms, 2023c. URL https://arxiv.org/ abs/2312.06550. Zihan Liu, Yang Chen, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Acemath: AdarXiv preprint vancing frontier math reasoning with post-training and reward modeling. arXiv:2412.15084, 2024e. Ziming Liu, Yizhou Liu, Jeff Gore, and Max Tegmark. Neural thermodynamic laws for large language model training, 2025. URL https://arxiv.org/abs/2505.10559. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. M-A-P, Ge Zhang, Xinrun Du, Zhimiao Yu, Zili Wang, Zekun Wang, Shuyue Guo, Tianyu Zheng, Kang Zhu, Jerry Liu, Shawn Yue, Binbin Liu, Zhongyuan Peng, Yifan Yao, Jack Yang, Ziming Li, Bingni Zhang, Minghao Liu, Tianyu Liu, Yang Gao, Wenhu Chen, Xiaohuan Zhou, Qian Liu, Taifeng Wang, and Wenhao Huang. Finefineweb: comprehensive study on fine-grained domain web corpus, December 2024. URL [https://huggingface.co/datasets/m-a-p/FineFineWeb] (https://huggingface.co/datasets/m-a-p/FineFineWeb). Quentin Malartic, Nilabhra Roy Chowdhury, Ruxandra Cojocaru, Mugariya Farooq, Giulia Campesan, Yasser Abdelaziz Dahou Djilali, Sanath Narayan, Ankit Singh, Maksim Velikanov, Basma El Amel Boussaha, et al. Falcon2-11b technical report. arXiv preprint arXiv:2407.14885, 2024. Sadhika Malladi, Kaifeng Lyu, Abhishek Panigrahi, and Sanjeev Arora. On the SDEs and scaling rules for adaptive gradient algorithms. In Alice H. Oh, Alekh Agarwal, Danielle Belgrave, and Kyunghyun Cho (eds.), Advances in Neural Information Processing Systems, 2022. URL https: //openreview.net/forum?id=F2mhzjHkQP. Sourab Mangrulkar, Sylvain Gugger, Lysandre Debut, Younes Belkada, Sayak Paul, and Benjamin Bossan. Peft: State-of-the-art parameter-efficient fine-tuning methods. https://github.com/ huggingface/peft, 2022. 60 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Saptarshi Mitra, Rachid Karami, Haocheng Xu, Sitao Huang, and Hyoukjun Kwon. Characterizing state space model (ssm) and ssm-transformer hybrid language model performance with long context length. arXiv preprint arXiv:2507.12442, 2025. doi: 10.48550/arXiv.2507.12442. URL https://arxiv.org/abs/2507.12442. ModelCloud.ai and qubitium@modelcloud.ai. Gptqmodel. https://github.com/modelcloud/ gptqmodel, 2024. Contact: qubitium@modelcloud.ai. Oumi Community. Oumi: an Open, End-to-end Platform for Building Large Foundation Models. URL https://github.com/oumi-ai/oumi. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Keiran Paster, Marco Dos Santos, Zhangir Azerbayev, and Jimmy Ba. Openwebmath: An open dataset of high-quality mathematical web text. arXiv preprint arXiv:2310.06786, 2023. Yan Ru Pei and others. Let ssms be convnets: State-space modeling with optimal tensor contractions. arXiv preprint arXiv:2501.13230, 2025. URL https://arxiv.org/abs/2501.13230. Guilherme Penedo, Quentin Malartic, Daniel Hesslow, Ruxandra Cojocaru, Alessandro Cappelli, Hamza Alobeidli, Baptiste Pannier, Ebtesam Almazrouei, and Julien Launay. The refinedweb dataset for falcon llm: outperforming curated corpora with web data, and web data only. arXiv preprint arXiv:2306.01116, 2023. Guilherme Penedo, Hynek Kydlíček, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024a. Guilherme Penedo, Hynek Kydlíček, Vinko Sabolčec, Bettina Messmer, Negar Foroutan, Martin Jaggi, Leandro von Werra, and Thomas Wolf. Fineweb2: sparkling update with 1000s of languages, 12 2024b. URL https://huggingface.co/datasets/HuggingFaceFW/fineweb-2. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, et al. Rwkv: Reinventing rnns for the transformer era. arXiv preprint arXiv:2305.13048, 2023. Negin Raoof, Etash Kumar Guha, Ryan Marten, Jean Mercat, Eric Frankel, Sedrick Keh, Hritik Bansal, Georgios Smyrnis, Marianna Nezhurina, Trung Vu, Zayne Rea Sprague, Mike Merrill, Liangyu Chen, Caroline Choi, Zaid Khan, Sachin Grover, Benjamin Feuer, Ashima Suvarna, Shiye Su, Wanjia Zhao, Kartik Sharma, Charlie Cheng-Jie Ji, Kushal Arora, Jeffrey Li, Aaron Gokaslan, Sarah Pratt, Niklas Muennighoff, Jon Saad-Falcon, John Yang, Asad Aali, Shreyas Pimpalgaonkar, Alon Albalak, Achal Dave, Hadi Pouransari, Greg Durrett, Sewoong Oh, Tatsunori Hashimoto, Vaishaal Shankar, Yejin Choi, Mohit Bansal, Chinmay Hegde, Reinhard Heckel, Jenia Jitsev, Maheswaran Sathiamoorthy, Alex Dimakis, and Ludwig Schmidt. Evalchemy, 6 2025. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. 61 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling. arXiv preprint arXiv:2406.07522, 2024. Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale. Communications of the ACM, 64(9):99106, 2021. David Saxton, Edward Grefenstette, Felix Hill, and Pushmeet Kohli. Analysing mathematical reasoning abilities of neural models. In International Conference on Learning Representations, 2019. Rico Sennrich, Barry Haddow, and Alexandra Birch. Neural machine translation of rare words with subword units, 2016. URL https://arxiv.org/abs/1508.07909. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Yikang Shen, Matthew Stallone, Mayank Mishra, Gaoyuan Zhang, Shawn Tan, Aditya Prasad, Adriana Meza Soria, David D. Cox, and Rameswar Panda. Power scheduler: batch size and token number agnostic learning rate scheduler, 2024. URL https://arxiv.org/abs/2408. 13359. Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, et al. Language models are multilingual chain-of-thought reasoners. arXiv preprint arXiv:2210.03057, 2022. Aaditya K. Singh and DJ Strouse. Tokenization counts: the impact of tokenization on arithmetic in frontier llms, 2024. URL https://arxiv.org/abs/2402.14903. Matei-Ioan Stan and Oliver Rhodes. Learning long sequences in spiking neural networks. Scientific Reports, 14(1):21957, 2024. doi: 10.1038/s41598-024-71678-8. URL https://www.nature.com/ articles/s41598-024-71678-8. Dan Su, Kezhi Kong, Ying Lin, Joseph Jennings, Brandon Norick, Markus Kliegl, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Nemotron-cc: Transforming common crawl into refined long-horizon pretraining dataset, 2024. URL https://arxiv.org/abs/2412.02595. Mirac Suzgun, Nathan Scales, Nathanael Schärli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. arXiv preprint arXiv:2210.09261, 2022. Chaofan Tao, Qian Liu, Longxu Dou, Niklas Muennighoff, Zhongwei Wan, Ping Luo, Min Lin, and Ngai Wong. Scaling laws with vocabulary: Larger models deserve larger vocabularies. arXiv preprint arXiv:2407.13623, 2024. Falcon-LLM Team. The falcon 3 family of open models, December 2024. Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. 62 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Jamba Team, Barak Lenz, Alan Arazi, Amir Bergman, Avshalom Manevich, Barak Peleg, Ben Aviram, Chen Almagor, Clara Fridman, Dan Padnos, et al. Jamba-1.5: Hybrid transformermamba models at scale. arXiv preprint arXiv:2408.12570, 2024. Huu Nguyen Thuat Nguyen and Thien Nguyen. Culturay: large cleaned multilingual dataset of 75 languages, 2024. Jörg Tiedemann. Finding alternative translations in large corpus of movie subtitle. In Proceedings of the Tenth International Conference on Language Resources and Evaluation (LREC16), pp. 35183522, 2016. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data. arXiv preprint arXiv:2410.01560, 2024. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, Shengyi Huang, Kashif Rasul, and Quentin Gallouédec. TRL: Transformer Reinforcement Learning. URL https://github.com/huggingface/trl. Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. arXiv preprint arXiv:2406.01574, 2024. Colin White, Samuel Dooley, Manley Roberts, Arka Pal, Ben Feuer, Siddhartha Jain, Ravid Shwartz-Ziv, Neel Jain, Khalid Saifullah, Siddartha Naidu, et al. Livebench: challenging, contamination-free llm benchmark. arXiv preprint arXiv:2406.19314, 4, 2024. Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-Art Natural Language Processing. pp. 3845. Association for Computational Linguistics, 10 2020. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6. An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, arXiv preprint Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv:2407.10671, 2024a. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, arXiv preprint Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv:2412.15115, 2024b. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jing Zhou, Jingren Zhou, Junyang Lin, Kai Dang, Keqin Bao, Kexin Yang, Le Yu, Lianghao Deng, Mei Li, Mingfeng Xue, Mingze Li, Pei Zhang, Peng Wang, Qin Zhu, Rui Men, Ruize Gao, Shixuan Liu, Shuang Luo, Tianhao Li, Tianyi Tang, Wenbiao Yin, Xingzhang Ren, Xinyu Wang, Xinyu Zhang, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yinger 63 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Zhang, Yu Wan, Yuqiong Liu, Zekun Wang, Zeyu Cui, Zhenru Zhang, Zhipeng Zhou, and Zihan Qiu. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. Greg Yang and Edward J. Hu. Feature learning in infinite-width neural networks, 2022. URL https://arxiv.org/abs/2011.14522. Greg Yang, Edward J. Hu, Igor Babuschkin, Szymon Sidor, Xiaodong Liu, David Farhi, Nick Ryder, Jakub Pachocki, Weizhu Chen, and Jianfeng Gao. Tensor programs v: Tuning large neural networks via zero-shot hyperparameter transfer, 2022. URL https://arxiv.org/abs/ 2203.03466. Greg Yang, Dingli Yu, Chen Zhu, and Soufiane Hayou. Tensor programs vi: Feature learning in infinite-depth neural networks, 2023. URL https://arxiv.org/abs/2310.02244. Greg Yang, James B. Simon, and Jeremy Bernstein. spectral condition for feature learning, 2024c. URL https://arxiv.org/abs/2310.17813. Howard Yen, Tianyu Gao, Minmin Hou, Ke Ding, Daniel Fleischer, Peter Izsak, Moshe Wasserblat, and Danqi Chen. Helmet: How to evaluate long-context language models effectively and thoroughly. In International Conference on Learning Representations (ICLR), 2025. Eric Zelikman, Yuhuai Wu, Jesse Mu, and Noah D. Goodman. Star: Bootstrapping reasoning with reasoning. In Conference on Neural Information Processing Systems, 2022. Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics, 2019. Guangxiang Zhao, Xu Sun, Jingjing Xu, Zhiyuan Zhang, and Liangchen Luo. Muse: Parallel multiscale attention for sequence to sequence learning, 2019. URL https://arxiv.org/abs/1911. 09483. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. LlamaFactory: Unified Efficient Fine-Tuning of 100+ Language Models. Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations). Association for Computational Linguistics, 2024. URL https://arxiv.org/abs/2403.13372. Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023. Jingwei Zuo, Maksim Velikanov, Dhia Eddine Rhaiem, Ilyas Chahed, Younes Belkada, Guillaume Kunsch, and Hacid Hakim. Falcon mamba: The first competitive attention-free 7b language model. 2024. URL https://arxiv.org/abs/2410.05355. 64 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance A. Languages used for training Falcon-H1 tokenizers af an as az bar bg bo bxr ckb cy dsb eml et fi ga gn he hsb hy ie is jbo kk ko kv la li lt Afrikaans Aragonese Assamese Azerbaijani Bavarian Bulgarian Tibetan Russia Buriat Central Kurdish Welsh Lower Sorbian Emiliano-Romagnol Estonian Finnish Irish Guarani Hebrew Upper Sorbian Armenian Interlingue Icelandic Lojban Kazakh Korean Komi Latin Limburgish Lithuanian Malagasy Mongolian Malay Burmese Nahuatl languages Nepali Norwegian Nynorsk Odia Pampanga # Code Language 1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55 58 61 64 67 70 73 76 79 82 85 mg 88 mn 91 ms 94 my nah 97 100 ne 103 nn 106 or 109 pam 112 pnb Western Panjabi 115 qu 118 sv 121 vi Quechua Swedish Vietnamese als ar ast azb bcl bh bpy ca cs da dv eo eu fr gd gom hi ht ia ilo it jv km krc kw lb lmo lv # Code Language 2 5 8 11 14 17 20 23 26 29 32 35 38 41 44 47 50 53 56 59 62 65 68 71 74 77 80 83 86 mk 89 mr 92 mt 95 myv nap 98 101 new 104 no 107 os 110 pl 113 ps 116 ro 119 th Swiss German Arabic Asturian South Azerbaijani Central Bikol Bihari languages Bishnupriya Catalan Czech Danish Divehi Esperanto Basque French Scottish Gaelic Goan Konkani Hindi Haitian Creole Interlingua Iloko Italian Javanese Khmer Karachay-Balkar Cornish Luxembourgish Lombard Latvian Macedonian Marathi Maltese Erzya Neapolitan Newari Norwegian Ossetic Polish Pashto Romanian Thai am arz av ba be bn bs ce cv de el es fa fy gl gu hr hu id io ja ka kn ku ky lez lo Amharic Egyptian Arabic Avaric Bashkir Belarusian Bangla Bosnian Chechen Chuvash German Greek Spanish Persian Western Frisian Galician Gujarati Croatian Hungarian Indonesian Ido Japanese Georgian Kannada Kurdish Kyrgyz Lezghian Lao Maithili Malayalam Western Mari # Code Language 3 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 54 57 60 63 66 69 72 75 78 81 84 mai 87 ml 90 mrj 93 mwl Mirandese 96 mzn Mazanderani Low German 99 Dutch 102 nl Occitan 105 oc Punjabi 108 pa Piedmontese 111 pms Portuguese 114 pt Russian 117 ru Turkish 120 tr nds Table 27: Language Codes and Corresponding Languages B. Scalar stochastic dynamics with weight decay To model the behavior of parameter norms 2 = ij we look at single entry Wij = and consider its evolution during training. Then, behavior of the typical values of x2 could serve as proxy for behavior of the parameter norms 2. ij 2 The update of parameter at iteration using AdamW with learning rate η and weight decay λ can be written as Here At = g1,t average (element-wise) squared gradient g2. g2,t+ε is the Adam update rule that uses moving average of the gradient g1 and moving xt+1 = xt ηAt ηλxt. (20) 65 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Generally, updates At depend on the dynamics of the rest of model beyond our selected parameter x, have stochastic nature w.r.t. randomness caused by sampling of the samples in batch, and correlated across nearby iterations due to moving averages used in the update rule. Yet, to get an intuitive picture of the role of AdamW parameters η, λ we consider toy model of this update At = h(xt x) + ξt, (21) where ξt is i.i.d. noise with zero mean and variance E[ξ2 ] = σ2, and describes steepness of the loss landscape for our chosen parameter x. Then, evolution (20) of xt becomes stationary linear stochastic equation. Let us now track the evolution of the first two moments of xt. Taking the expectation of (20) and its square we get the update of the moments E[xt+1] = (1 ηh ηλ)E[xt] + ηhx E[x2 t+1] = (1 ηh ηλ)2E[x2 ] + 2(1 ηh ηλ)ηhE[xt]x + (ηhx)2 + η2σ (22) (23) As , the dynamics of first and second moment converge to the stationary state E[xt] x, E[x2 ] = x2,. With direct calculation, we get ] x2, that can be found by setting E[xt+1] = E[xt] = and E[x2 t+1] = E[x2 = + λ x, x,2 = ησ2 (λ + h)(2 ηλ ηh) + x2 . (24) (25) Now, let us try to map the toy model described above to the scenario of LLM training, where we find substantial simplification of the second moment in the stationary state. Typical values of learning rate and weight decay used in pretraining are η 103 and λ 0.1. These values imply that the product ηλ is very small, and we can drop it compared to the terms that are of the order of 1. It is also reasonable to assume that 1, which also implies λ 1 for practical values of λ. Indeed, the steepness roughly corresponds to quadratic approximation of the loss w.r.t. to the chosen parameter L(x) 1 2 h(x x)2. Then, due to large number of parameters in the model, the sensitivity to single parameter should be relatively small, implying 1. Having two assumptions ηλ 1 and λ 1 the second moment simplifies to x,2 1 η λ σ2 + 2(hx)2 ηλ ! . (26) The obtained result for x,2 has clear interpretation as it fully separates signal and noise contributions. The first term describes the balance between weight decay contraction and Brownian motion expansion due to the noise in the updates. The second term describes the balance between, again, weight decay contraction, and attraction of the parameter to its optimal value x. Importantly, these two terms have different scaling w.r.t. learning rate η and weight decay λ. Implication for the parameter norms. Recall that our parameter was just single entry of the parameter matrix . Different entries would have its own values of σ, h, but the same η, λ as those are global hyperparameters of the training algorithm. Then, we could write the dependence of parameters norm 2 on learning rate and weight decay as 2 = ij 2 ij = ! σ2 ij 2 η λ + (hijx ij)2 ij 1 λ2 = η λ + C2 1 λ2 . (27) 66 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance As the dependence of the parameter norms on η, λ can be accurately measured for the actual LLM training, we could check which of the two scalings more accurately describes the experimental data. As we have seen in section 3.2.2, the scaling 2 η λ very well captures the experiments values, 1 implying C1 λ2 for typical values of η, λ. This suggest that, at least for the most of the parameters, the noise in the updates dominates the attraction to the optimal values. η λ C2 C. Tuning µP multipliers To jointly tune all of our = 35 multipliers, we have used stagewise procedure comprised of micro-sweeps over each multiplier on each stage. With slight abuse of notations, denote Mn = {m(1) } the set of all µP multipliers at stage n. At each stage, we repeat the following steps: , . . . , m(M ) 1. Start the stage by running training job with Mn, and measure the final loss Ln that will serve as baseline loss for the current stage. 2. For each multiplier = 1 . . . run log-scale microsweep: two training jobs with increased n,+ = pm(i) n, = p1m(i) ith multiplier m(i) , and measure the respective n,+ and L(i) losses L(i) n,. The scaling factor is chosen to balance exploration and precision. We have used = 2 at the beginning of the tuning to converge faster to the vicinity of the optimal set of multipliers, while switching to = 2 towards the end of the tuning for the increased precision. and decreased m(i) 3. For each multiplier m(i) n,, Ln, L(i) n,+) to pick the value of the next stage multiplier m(i) we manually inspected decreased, baseline, and increased losses (L(i) n+1. Most of the time we pick this value from {m(i) n,+} depending on which of the respective losses is lower. However, sometimes we have also picked other values, for example, at earlier stages we could pick the value beyond maximal m(i) n,+ or minimal m(i) n, to increase exploration. Also, someq n+ if two losses Ln, L(i) n,+ turned times we picked an intermediate value, for example, out roughly the same and significantly better than L(i) n,, m(i) , m(i) m(i) m(i) n,. 4. Construct the next stage set of multipliers Mn+1 = {m(1) on the step 3, then go to step 1 to start stage + 1. n+1, . . . , m(M ) n+1} from the values picked The above procedure is quite simple, and we expect it can be improved in the future to achieve faster convergence to the optimal set of multipliers. However, the simplicity and manual elements of our procedure was important to build an intuition behind the roles of different multipliers in the training process, as we were also checking the whole training curve for spikes, the noise level (the loss difference before and after LR decay), and any possible anomalies in the training. Another benefit of our procedure is the possibility to measure the sensitivity of the loss to each multiplier. Indeed, at the end of the procedure we obtain set of multipliers that is close to optimality, and the dependence of the loss on the logarithm of each multiplier can be roughly approximated by quadratic function L(m) 2 (log log m)2 + L. We fit our three loss measurements (L(i) n,+) with this quadratic dependence to estimate the sensitivity = ( log m)2 L, reported in Figure 12. n,, Ln, L(i) 2 67 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Effective learning rate and weight decay. Recall that we have 4 types of µP multipliers, grouped as in table 8 1. Forward multipliers m(i). 2. LR multipliers η(j)/η for matrix-like layers with enabled weight decay. 3. WD multipliers λ(j)/λ for the same matrix-like layers with enabled weight decay. 4. LR multipliers η(k)/η for vector-like layers with disabled weight decay. Our tuning procedure in the space of multipliers has coordinate-aligned structure, similar to coordinate descent. Therefore, it is beneficial for its convergence and interpretation to make different coordinates as independent as possible. We expect that forward multipliers and LR multipliers for vector-like layers are already quite independent from each other. However, as discussed in section 3.2.2, LR and WD multipliers for the same matrix-like layer are expected to be strongly coupled. We aim to decrease the coupling by changing the coordinate axes to ELR and EWD (12), and η(j)λ(j)/ tuning ELR multiplier ηλ and EWD multiplier η instead of plane LR and WD multipliers. Specifically, for ELR micro-sweep at stage we change LR and WD values as , pλ(j) (η(j) , p1λ(j) ) (pη(j) , λ(j) ). Accordingly, 2 ( log λeff )2 instead of the raw LR/WD in figure 12 we also report sensitivities sensitivities. ), and for EWD micro-sweep as (η(j) ( log ηeff )2 and ) (pη(j) , λ(j) λ(j) η(j) (cid:14)q λ Other settings, and applied scaling relations. The shapes of the model used for multiplier tuning are reported in table 7. As for the training job settings, we have used WSD learning rate schedule with 65GT of the stable stage, and 10GT of exponential decay that reduces LR 32 times. The global LR and WD values are η = 256 106 and λ = 0.1, sequence length 2048, and the global batch size of 4 million tokens (MT). As we expect all the hyperparameters to be well-tuned by the end of multiplier tuning, it is essential to correctly transfer them to other model and training settings. Here are the scaling relations we have applied for the Falcon-H1 training. For the µP multipliers, we have the scaling version that scales forward multipliers, as described in table 7, while keeping LR and WD values unchanged. We did not use scaling of multipliers with model depth L, for simplicity and due to time constraints, but it can be directly applied following recent work (Yang et al., 2023; Dey et al., 2025). When changing the batch size, we have used the square root scaling of the learning rate (19). When changing the total training duration, we have applied our effective power scheduler (EPS) for the learning rate and weight decay of the stable stage of WSD, as described in section 3.2.2. The activation is the power scheduler should be set close to the duration of the stable stage our tuning jobs - 65GT. However, EPS can be activated later to ensure higher adaptivity in the later stages of training, for example, if curriculum learning or long context extension at the end of the training are used. D. Detailed evaluation results D.1 Multilingual Evaluations - Base Models 68 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Task Falcon-H11.5B-deep Falcon-H11.5B Qwen31.7B Qwen2.51.5B Gemma31B Llama3.21.2B Falcon31.6B Multilingual Hellaswag 41.63 50.44 58.52 57.25 35.61 54.39 50.89 56.57 48.17 51.20 49.32 50.36 Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Average Multilingual MMLU Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Chinese (zh) Average 43.92 54.24 57.33 56.54 40.69 55.11 53.21 57.04 52.49 50.35 51.92 51.23 52.00 Multilingual GSM (MGSM) German (de) Spanish (es) French (fr) Japanese (ja) Russian (ru) Chinese (zh) Average 64.40 68.00 61.20 42.80 64.40 61.20 60.33 39.01 46.55 54.69 53.68 33.22 50.21 47.09 51.99 43.74 48.04 44.53 46.62 40.17 48.49 51.73 50.66 37.72 48.46 46.54 51.53 46.67 44.71 44.53 46.87 46. 48.40 61.20 57.60 31.60 54.00 52.00 50.80 40.93 47.18 54.20 53.07 34.01 50.53 45.46 52.41 42.86 48.16 42.36 46.47 - - - - - - - - - - - - - - - - - - - - 38.55 43.48 51.98 50.30 30.43 46.36 41.81 50.60 35.71 45.31 37.25 42.89 42.67 49.88 53.65 52.67 34.47 50.84 48.31 53.23 44.74 47.57 45.16 53.94 48. 41.60 54.40 44.00 34.00 44.40 52.40 45.13 40.62 45.56 52.22 50.93 35.63 49.11 47.24 50.62 43.46 45.77 46.40 46.14 26.30 27.27 26.92 25.73 26.04 25.90 26.98 27.09 27.03 26.36 25.90 26.49 26.50 - - - - - - - 34.96 41.47 48.13 46.07 32.97 44.23 42.22 45.84 39.04 42.12 40.69 41.61 26.03 28.68 29.00 28.33 27.24 28.27 29.00 28.61 27.03 28.50 28.78 29.25 28. 6.00 6.00 4.80 2.80 3.60 5.20 4.73 28.69 30.82 37.49 37.45 28.26 31.84 29.99 33.69 29.63 29.08 28.65 31.42 27.29 32.94 35.98 35.22 27.93 32.28 30.67 33.99 30.29 28.50 29.76 33.89 31.56 8.40 14.40 14.40 2.00 3.60 13.60 9.40 Table 28: Performance comparison of Base models on multilingual tasks (1B-2B scale). All scores are percentages. The best average score is in bold, and the second-best is underlined. Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Task Falcon-H13B Qwen34B Qwen2.53B Gemma34B Llama-3.23B Falcon33B Multilingual Hellaswag Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Average Multilingual MMLU Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Chinese (zh) Average 45.42 56.29 62.98 62.52 39.35 58.90 56.43 61.39 53.09 55.48 54.79 55.15 46.98 57.82 60.08 58.81 42.53 58.17 56.48 59.37 55.54 53.53 54.98 53.05 54.78 Multilingual GSM (MGSM) German (de) Spanish (es) French (fr) Japanese (ja) Russian (ru) Chinese (zh) Average 64.80 72.00 70.40 51.20 64.00 61.60 64.00 49.63 58.09 64.69 63.76 41.12 61.86 56.52 63.25 53.68 57.39 53.67 56. 57.70 66.33 68.65 68.28 52.64 68.10 65.99 68.79 65.50 65.01 64.78 67.22 64.91 - - - - - - - 49.50 57.29 60.15 59.32 40.40 58.34 56.33 59.85 52.76 55.47 52.50 54.71 49.50 57.29 60.15 59.32 40.40 58.34 56.33 59.85 52.76 55.47 52.50 59.63 55.13 58.80 68.00 62.40 45.60 62.00 62.80 59.93 52.14 61.36 67.09 66.62 46.26 64.44 63.17 65.85 60.48 59.76 64.14 61. 47.14 54.22 55.68 54.92 45.28 54.42 53.69 54.97 53.99 51.58 53.61 51.36 52.57 - - - - - - - 43.59 54.74 61.75 59.99 41.03 57.78 55.59 59.40 50.63 53.99 54.39 53.89 39.24 47.33 49.06 48.44 37.50 47.84 46.96 48.45 45.69 44.56 45.76 44.54 45.45 23.20 24.40 22.00 12.80 16.80 22.80 20.33 29.52 35.01 48.95 48.33 28.71 36.45 31.60 47.73 31.21 31.26 30.60 36. 29.77 39.98 47.47 46.89 29.95 39.84 36.11 46.56 35.91 32.51 34.70 40.07 38.31 28.80 46.80 48.00 8.80 14.80 43.60 31.80 Table 29: Performance comparison of Base models on multilingual tasks (3B-4B scale). All scores are percentages. The best average score is in bold, and the second-best is underlined. 70 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Task Falcon-H17B Qwen38B Qwen2.57B Gemma312B Llama3.18B Falcon37B Falcon310B Multilingual Hellaswag Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Average 54.57 66.28 72.71 71.73 47.65 69.71 67.47 70.92 64.65 64.52 66.61 65.16 Multilingual MMLU Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Chinese (zh) Average 60.93 69.19 71.75 70.85 57.06 70.68 69.74 70.93 68.48 67.74 67.82 65.40 67.55 Multilingual GSM (MGSM) German (de) Spanish (es) French (fr) Japanese (ja) Russian (ru) Chinese (zh) Average 75.20 83.20 77.20 62.00 76.40 73.20 74.53 54.24 64.09 70.30 68.86 45.82 66.86 62.51 68.44 59.64 62.39 60.32 62.13 62.06 70.11 71.82 71.56 57.92 71.78 70.06 71.67 69.52 69.20 68.71 70.13 68. 75.60 82.00 77.60 58.00 81.20 32.80 67.87 52.23 60.58 68.82 67.89 38.72 63.57 60.76 67.94 49.32 61.60 54.65 58.74 58.19 65.69 68.19 67.88 49.65 66.68 65.73 68.48 63.02 65.47 62.92 66.96 64.07 74.00 75.20 70.80 59.60 74.40 72.40 71.07 62.62 71.81 76.82 75.32 53.57 74.01 73.36 75.28 70.74 68.95 74.31 70.62 61.96 68.32 69.24 69.27 59.03 69.08 69.04 68.96 69.09 67.10 68.44 66.13 67. - - - - - - - 50.14 63.13 69.71 67.95 45.75 66.37 64.21 68.06 59.48 60.25 63.91 61.72 46.98 55.48 57.31 56.93 43.93 56.51 55.54 57.30 53.87 52.93 54.11 52.17 53.58 41.20 52.80 40.00 28.00 44.40 42.80 41.53 31.11 44.52 67.57 66.96 30.15 51.34 40.14 65.98 38.09 37.55 38.91 46.58 - - - - - - - - - - - - - 51.20 71.20 65.60 24.00 38.80 62.40 52.20 34.21 51.18 71.36 70.25 31.09 56.61 45.87 69.49 42.91 43.17 43.85 50.91 35.33 55.57 66.99 67.09 35.81 59.45 51.94 66.98 51.06 44.80 49.14 53.82 53.17 60.00 73.60 69.20 34.80 49.20 67.20 59.00 Table 30: Performance comparison of Base models on multilingual tasks (7B-12B scale). All scores are percentages. The best average score is in bold, and the second-best is underlined. Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Task Falcon-H134B Qwen2.572B Gemma327B Llama4scout Llama3.170B Multilingual Hellaswag Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Average Multilingual MMLU Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Chinese (zh) Average 62.95 74.30 79.68 78.18 54.53 76.86 75.54 78.26 72.47 71.10 74.93 72. 70.83 78.23 79.65 79.67 67.16 79.46 78.69 79.64 78.42 76.65 78.13 74.60 76.76 Multilingual GSM (MGSM) 82.40 85.60 80.00 76.80 88.00 81.60 82.40 German (de) Spanish (es) French (fr) Japanese (ja) Russian (ru) Chinese (zh) Average 63.59 73.06 78.75 77.19 54.08 76.06 73.93 77.68 66.87 70.88 71.08 71.20 73.52 80.28 80.63 81.03 68.94 81.03 80.32 80.88 79.03 79.08 79.03 78.78 78.54 83.20 86.00 77.60 80.80 83.60 82.00 82. 66.36 75.46 79.80 78.52 57.31 77.58 76.77 78.87 74.04 71.84 77.60 74.01 68.10 73.31 74.56 74.13 65.46 74.12 74.33 74.47 73.97 71.96 73.54 70.94 72.41 - - - - - - - 64.08 73.55 77.96 76.84 55.99 75.17 74.71 76.77 72.17 70.87 74.10 72.02 67.43 73.78 74.33 74.60 64.68 74.22 74.02 73.95 73.60 72.55 74.23 71.27 72.38 76.00 77.60 72.40 70.80 79.20 78.80 75. 65.19 76.53 80.86 78.92 58.46 78.36 77.65 80.37 74.38 72.66 77.78 74.65 65.46 72.10 73.33 73.45 63.38 73.79 72.73 73.78 72.52 71.28 72.08 69.39 71.10 71.20 79.60 69.20 61.20 74.40 68.80 70.73 Table 31: Performance comparison of Base models on multilingual tasks (34B scale). All scores are percentages. The best average score is in bold, and the second-best is underlined. 72 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance D.2 Multilingual Evaluations - Instruct Models Task Falcon-H11.5B-deep Falcon-H11.5B Qwen31.7B Qwen2.51.5B Gemma31B Llama3.21.2B Falcon31.6B Multilingual Hellaswag 44.22 53.83 61.50 60.54 37.35 57.96 52.94 60.01 50.44 54.16 51.58 53.14 Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Average Multilingual MMLU Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Chinese (zh) Average 45.13 55.02 57.85 57.47 40.74 55.94 54.54 57.55 53.83 51.93 53.02 51.53 53.00 Multilingual GSM (MGSM) German (de) Spanish (es) French (fr) Japanese (ja) Russian (ru) Chinese (zh) Average 61.20 70.00 50.40 54.40 57.60 66.40 60.00 41.51 49.97 57.67 56.51 34.98 53.54 49.98 55.80 45.81 50.66 46.80 49. 41.21 50.57 53.09 51.91 38.05 50.56 48.97 52.09 48.32 46.84 47.10 47.16 48.06 61.20 71.20 57.60 41.20 55.60 61.20 58.00 36.19 37.95 41.92 41.45 30.68 40.05 36.89 41.64 36.01 38.22 35.80 37.89 35.53 43.92 45.40 44.90 28.98 45.63 41.63 30.76 40.63 40.59 37.62 50.38 39.60 50.80 60.80 45.20 44.80 56.00 56.80 52.40 39.20 43.04 52.07 50.50 30.24 46.32 41.76 50.89 35.70 45.85 36.67 42. 41.51 48.04 51.42 51.08 33.27 49.04 46.72 51.82 43.32 45.89 42.82 52.95 45.90 42.00 49.20 46.80 34.40 43.60 55.20 45.20 37.65 42.09 47.22 46.29 33.42 44.62 40.11 45.66 39.81 42.27 40.39 41.77 32.52 36.11 36.25 36.06 31.66 35.54 35.90 34.94 34.92 35.37 34.76 33.66 34.91 - - - - - - - 33.38 40.65 46.02 44.04 32.89 43.09 39.19 44.22 37.10 38.36 38.60 39. 30.00 36.75 38.29 37.66 32.39 37.01 36.60 36.04 32.81 35.27 34.87 37.68 35.24 32.40 33.60 34.00 19.60 29.20 29.60 29.73 29.49 31.31 38.79 38.52 28.50 32.76 29.67 35.22 30.07 29.22 28.88 32.04 27.48 33.91 37.49 36.87 27.92 33.71 31.00 35.80 30.97 29.12 30.46 33.60 32.25 15.20 24.40 24.00 3.20 5.20 20.00 15.33 Table 32: Performance comparison of Instruct models on multilingual tasks (1B-2B scale). All scores are percentages. The best average score is in bold, and the second-best is underlined. 73 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Task Falcon-H13B Qwen34B Qwen2.53B Gemma34B Llama3.23B Falcon33B Multilingual Hellaswag Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Average Multilingual MMLU Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Chinese (zh) Average 49.37 59.27 66.48 65.88 42.32 62.44 58.83 64.68 55.55 59.04 57.83 58.34 47.57 56.87 59.88 58.99 42.42 57.89 55.99 59.29 55.12 54.14 55.30 53.83 54.90 Multilingual GSM (MGSM) German (de) Spanish (es) French (fr) Japanese (ja) Russian (ru) Chinese (zh) Average 63.60 71.60 66.40 59.60 54.00 68.40 63. 40.48 44.51 46.99 46.82 33.66 45.45 43.19 46.70 40.99 42.83 42.63 43.12 41.65 58.70 56.14 59.38 36.39 57.34 56.59 26.74 56.48 54.39 53.83 55.88 50.70 68.80 73.20 67.20 65.20 65.60 73.20 68.90 46.57 51.41 60.17 60.09 33.33 55.11 51.36 59.53 42.35 53.42 45.54 50.81 48.21 55.35 57.83 57.61 38.45 56.35 54.34 58.13 50.81 54.39 50.23 58.34 52.90 58.00 64.80 54.40 51.20 54.40 62.40 57. 48.69 54.22 60.26 59.55 43.19 58.01 53.96 60.11 53.95 54.26 53.08 54.48 46.24 52.27 53.69 53.07 44.27 53.49 52.30 52.54 51.75 50.48 52.03 50.13 51.10 - - - - - - - 40.52 52.90 58.80 56.92 40.32 55.04 51.96 56.39 47.12 49.12 51.17 50.93 41.03 50.76 52.76 51.52 40.09 51.24 50.31 52.25 47.94 46.22 48.60 47.77 48.40 63.60 71.60 62.00 51.60 62.80 61.60 62. 29.83 36.47 51.19 50.98 28.47 38.47 32.35 50.44 32.15 31.22 31.01 37.51 30.39 40.85 48.45 47.32 30.17 41.55 37.47 47.96 36.81 33.32 33.88 39.80 38.90 41.60 63.20 60.80 14.00 20.40 52.40 42.10 Table 33: Performance comparison of Instruct models on multilingual tasks (3B-4B scale). All scores are percentages. The best average score is in bold, and the second-best is underlined. 74 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Task Falcon-H17B Qwen38B Qwen2.57B Gemma312B Llama3.18B Falcon37B Falcon310B Multilingual Hellaswag Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Average 56.73 69.51 75.94 75.08 49.26 72.32 70.52 74.38 66.25 66.26 68.97 67.75 Multilingual MMLU Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Chinese (zh) Average 60.44 69.13 71.96 71.04 56.52 70.23 69.83 71.55 68.81 67.96 68.65 65.82 67.83 Multilingual GSM (MGSM) German (de) Spanish (es) French (fr) Japanese (ja) Russian (ru) Chinese (zh) Average 73.20 82.00 72.40 70.80 67.60 74.80 73.50 44.19 48.34 51.06 52.35 37.34 49.67 47.61 51.15 44.87 47.25 46.45 47. 49.11 58.63 59.55 53.14 36.61 58.99 57.66 23.60 58.32 55.51 43.71 65.66 50.44 72.00 68.40 60.00 61.60 60.80 68.40 65.20 52.23 60.57 68.81 67.89 38.72 63.57 60.75 67.93 49.32 61.60 54.64 58.74 56.23 63.65 66.17 65.30 46.15 65.19 63.05 64.44 59.27 63.29 60.43 65.89 61.20 64.80 69.20 61.60 64.80 61.60 74.80 66.10 59.19 67.64 72.53 70.86 51.93 69.86 67.64 71.28 66.45 65.17 69.21 66. 59.40 66.48 67.29 67.35 57.41 67.35 67.02 67.67 66.55 64.58 66.31 63.94 65.22 - - - - - - - 49.44 62.29 68.58 66.79 45.66 64.55 63.18 66.47 59.21 59.17 62.73 60.74 47.17 57.62 59.62 59.51 45.31 58.27 57.01 58.80 55.78 55.21 56.57 55.09 55.53 71.20 79.60 69.60 56.00 76.40 71.60 70.70 32.19 46.58 68.74 68.14 29.92 52.72 41.54 66.85 39.65 39.60 39.98 47. 35.34 52.44 64.14 63.78 34.33 56.02 49.28 63.80 48.33 42.48 46.79 50.12 50.62 60.00 80.40 73.20 30.80 36.00 57.60 56.30 35.51 52.95 74.00 72.71 31.90 59.14 47.52 72.18 44.17 44.80 45.54 52.77 36.26 55.68 67.71 67.45 35.24 60.26 52.79 67.27 51.85 45.85 50.02 53.64 53.67 68.40 84.80 76.40 45.20 46.80 67.20 64.80 Table 34: Performance comparison of Instruct models on multilingual tasks (7B-12B scale). All scores are percentages. The best average score is in bold, and the second-best is underlined. 75 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Task Falcon-H134B Qwen332B Qwen2.572B Qwen2.532B Gemma327B Llama4-Scout Llama3.370B Multilingual Hellaswag 64.33 76.96 80.39 80.56 57.01 79.07 77.08 80.25 73.95 73.92 76.47 74.55 Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Average Multilingual MMLU Arabic (ar) German (de) Spanish (es) French (fr) Hindi (hi) Italian (it) Dutch (nl) Portuguese (pt) Romanian (ro) Russian (ru) Swedish (sv) Chinese (zh) Average 72.08 78.97 80.25 80.38 68.34 79.97 79.56 80.49 79.15 77.31 78.85 75.41 77.76 Multilingual GSM (MGSM) German (de) Spanish (es) French (fr) Japanese (ja) Russian (ru) Chinese (zh) Average 75.20 80.80 70.80 78.40 68.00 84.80 76. 52.39 59.48 62.87 62.45 49.25 61.38 59.56 62.03 57.83 56.58 58.50 58.39 65.71 71.32 74.04 72.49 56.77 74.71 67.34 31.56 72.12 73.06 69.09 73.61 66.20 73.20 76.00 67.60 69.20 68.00 76.80 71.80 63.20 70.93 76.03 75.19 53.40 73.61 70.84 75.71 66.06 69.53 69.83 69.48 73.67 78.57 81.03 81.05 69.07 81.04 79.74 81.25 78.76 78.20 78.51 78.22 78.26 72.80 79.20 60.80 74.80 63.20 83.20 72. 60.75 67.63 72.68 72.60 48.50 70.21 67.75 72.73 60.83 66.75 64.46 65.90 68.17 75.71 76.86 76.88 61.39 76.88 75.58 76.21 73.60 74.41 73.54 74.94 73.56 72.80 76.80 61.20 81.20 68.40 81.20 73.60 62.40 70.52 74.94 73.97 55.37 73.64 70.72 74.65 69.93 68.31 71.61 69.64 66.30 71.96 73.65 73.86 63.95 73.70 72.54 74.24 73.07 71.52 72.82 69.54 71.60 78.00 84.80 74.80 76.80 72.40 80.40 77. 55.39 62.57 69.10 68.13 45.25 65.44 65.53 67.87 62.80 59.93 63.30 62.30 68.70 75.61 76.86 76.50 67.37 76.32 76.00 77.06 75.54 74.31 76.11 73.50 74.58 86.00 88.40 82.80 78.00 84.40 83.60 83.87 58.44 64.68 69.59 68.62 50.47 66.97 68.05 69.99 63.34 64.55 66.39 64.64 70.51 77.89 79.47 79.08 68.22 78.82 78.70 78.99 77.73 75.88 78.10 74.97 76.67 88.00 89.60 82.80 84.40 88.40 88.00 86. Table 35: Performance comparison of Instruct models on multilingual tasks (34B scale). All scores are percentages. The best average score is in bold, and the second-best is underlined. 76 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance D.3 Long-context Evaluations - Instruct Models Dataset Metric citation_prec citation_rec str_em citation_prec citation_rec qampari_rec_top5 exact_match exact_match substring_exact_match exact_match rougeL_f1 f1 substring_exact_match alce_asqa alce_asqa alce_asqa alce_qampari alce_qampari alce_qampari banking77 clinic150 hotpotqa infbench_choice infbench_qa infbench_sum json_kv msmarco_rerank_psg NDCG@10 multi_lexsum narrativeqa nlu nq popqa ruler_niah_mk_2 ruler_niah_mk_3 ruler_niah_mv trec_coarse trec_fine triviaqa f1 rougeL_f1 exact_match substring_exact_match substring_exact_match ruler_recall ruler_recall ruler_recall exact_match exact_match substring_exact_match FalconH1-34BInstruct 61.49 61.90 49.68 23.67 23.42 28.80 82.00 91.00 66.67 55.00 20.75 25.02 100 76.94 32.06 22.85 74.00 63.33 65.33 100 100 100 69.00 34.00 93.33 Qwen332B 70.05 63.37 44.10 29.01 25.60 29.20 87.00 91.00 69.33 46.00 24.01 33.12 100 81.92 34.17 24.89 79.00 57.83 61.33 100 100 100 69.00 42.00 88. Qwen2.572BInstruct 65.04 69.52 46.98 28.36 27.81 29.40 86.00 91.00 67.67 61.00 20.39 31.65 100 85.40 34.07 24.21 83.00 66.33 61.67 100 100 100 72.00 46.00 93.17 Llama3.3-70BInstruct 49.47 66.00 51.10 20.48 18.81 37.00 84.00 85.00 71.33 50.00 22.14 30.25 100 84.32 31.84 28.86 83.00 61.17 69.00 100 100 100 63.00 34.00 95.67 Table 36: Performance of HELMET tasks at sequence length 8192. 77 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Dataset Metric citation_prec citation_rec str_em citation_prec citation_rec qampari_rec_top5 exact_match exact_match substring_exact_match exact_match rougeL_f1 f1 substring_exact_match alce_asqa alce_asqa alce_asqa alce_qampari alce_qampari alce_qampari banking77 clinic150 hotpotqa infbench_choice infbench_qa infbench_sum json_kv msmarco_rerank_psg NDCG@10 multi_lexsum narrativeqa nlu nq popqa ruler_niah_mk_2 ruler_niah_mk_3 ruler_niah_mv trec_coarse trec_fine triviaqa f1 rougeL_f1 exact_match substring_exact_match substring_exact_match ruler_recall ruler_recall ruler_recall exact_match exact_match substring_exact_match FalconH1-34BInstruct 49.85 47.75 52.15 21.90 20.91 29.40 86.00 95.00 65.33 55.00 28.40 24.92 100 66.15 33.68 20.52 77.00 57.00 67.50 100 100 100 66.00 43.00 93.00 Qwen332B 71.23 65.46 46.20 28.08 25.96 31.40 90.00 96.00 64.33 49.00 31.48 34.91 100 70.24 34.61 26.57 76.00 56.50 60.83 100 100 100 78.00 51.00 86.50 Qwen2.572BInstruct 55.52 62.08 43.77 28.85 27.88 30.60 91.00 94.00 65.67 56.00 32.13 34.08 100 74.89 34.51 29.27 86.00 67.33 62.33 100 100 100 85.00 51.00 93.67 Llama3.3-70BInstruct 48.14 62.54 50.17 17.34 15.64 42.40 91.00 95.00 69.33 55.00 32.46 32.11 100 76.01 32.70 31.78 85.00 61.50 64.33 100 100 100 70.00 39.00 95.67 Table 37: Performance of HELMET tasks at sequence length 16384. Dataset Metric citation_prec citation_rec str_em citation_prec citation_rec qampari_rec_top5 exact_match exact_match substring_exact_match exact_match rougeL_f1 f1 substring_exact_match alce_asqa alce_asqa alce_asqa alce_qampari alce_qampari alce_qampari banking77 clinic150 hotpotqa infbench_choice infbench_qa infbench_sum json_kv msmarco_rerank_psg NDCG@10 multi_lexsum narrativeqa nlu nq popqa ruler_niah_mk_2 ruler_niah_mk_3 ruler_niah_mv trec_coarse trec_fine triviaqa f1 rougeL_f1 exact_match substring_exact_match substring_exact_match ruler_recall ruler_recall ruler_recall exact_match exact_match substring_exact_match FalconH1-34BInstruct 34.98 25.52 45.18 12.13 11.31 16.80 87.00 97.00 60.67 56.00 29.26 22.87 99 50.35 34.62 19.99 84.00 60.83 58.00 97.00 95.00 99.00 64.00 44.00 92.33 Qwen332B 61.34 56.04 43.72 25.43 24.99 31.40 90.00 97.00 57.67 62.00 36.47 35.47 100 54.98 32.59 24.97 80.00 57.00 57.50 100 100 100 64.00 50.00 87. Qwen2.572BInstruct 53.16 57.80 42.07 24.18 22.54 27.60 90.00 98.00 64.33 57.00 32.36 35.21 98 60.66 36.73 28.29 82.00 63.00 59.33 100 96.00 99.50 90.00 47.00 93.67 Llama3.3-70BInstruct 45.29 55.78 46.37 11.12 10.69 45.00 92.00 96.00 66.00 68.00 41.08 31.58 100 65.71 33.27 33.50 87.00 59.17 59.50 100 99.00 99.50 70.00 46.00 96.17 Table 38: Performance of HELMET tasks at sequence length 32768. 78 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Dataset Metric citation_prec citation_rec str_em citation_prec citation_rec qampari_rec_top5 exact_match exact_match substring_exact_match exact_match rougeL_f1 f1 substring_exact_match alce_asqa alce_asqa alce_asqa alce_qampari alce_qampari alce_qampari banking77 clinic150 hotpotqa infbench_choice infbench_qa infbench_sum json_kv msmarco_rerank_psg NDCG@10 multi_lexsum narrativeqa nlu nq popqa ruler_niah_mk_2 ruler_niah_mk_3 ruler_niah_mv trec_coarse trec_fine triviaqa f1 rougeL_f1 exact_match substring_exact_match substring_exact_match ruler_recall ruler_recall ruler_recall exact_match exact_match substring_exact_match FalconH1-34BInstruct 10.91 4.09 18.98 2.82 1.75 0.40 92.00 96.00 63.00 56.00 23.32 21.59 87 30.60 35.48 18.03 83.00 57.67 51.67 76.00 66.00 93.75 80.00 60.00 96.00 Qwen332B 47.02 45.37 45.68 16.15 14.03 29.20 92.00 98.00 55.33 65.00 42.56 38.51 99 37.83 35.20 34.85 83.00 55.00 52.50 94.00 97.00 96.00 84.00 64.00 85.00 Qwen2.572BInstruct 31.50 34.93 44.18 7.76 6.09 19.60 93.00 97.00 54.67 62.00 18.15 33.99 54 40.71 33.21 29.98 82.00 56.50 52.50 80.00 54.00 99.00 90.00 55.00 89.33 Llama3.3-70BInstruct 40.17 43.70 41.12 8.57 8.07 37.60 96.00 97.00 61.00 74.00 38.74 30.11 100 41.04 33.58 32.97 86.00 60.83 59.00 99.00 97.00 99.25 79.00 52.00 95.50 Table 39: Performance of HELMET tasks at sequence length 65536. Dataset Metric citation_prec citation_rec str_em citation_prec citation_rec qampari_rec_top5 exact_match exact_match substring_exact_match exact_match rougeL_f1 f1 substring_exact_match alce_asqa alce_asqa alce_asqa alce_qampari alce_qampari alce_qampari banking77 clinic150 hotpotqa infbench_choice infbench_qa infbench_sum json_kv msmarco_rerank_psg NDCG@10 multi_lexsum narrativeqa nlu nq popqa ruler_niah_mk_2 ruler_niah_mk_3 ruler_niah_mv trec_coarse trec_fine triviaqa f1 rougeL_f1 exact_match substring_exact_match substring_exact_match ruler_recall ruler_recall ruler_recall exact_match exact_match substring_exact_match FalconH1-34BInstruct 3.00 0.53 9.12 2.77 1.44 0.40 93.00 96.00 56.00 62.00 24.82 19.44 49 19.63 34.18 14.60 87.00 53.83 48.17 43.00 40.00 94.50 66.00 46.00 90.83 Qwen332B 41.31 37.40 43.70 11.31 9.43 21.00 97.00 93.00 47.67 75.00 46.94 38.90 82 29.75 34.52 38.61 89.00 49.00 48.17 84.00 84.00 94.50 85.00 67.00 83. Qwen2.572BInstruct 3.07 2.76 35.58 0.00 0.00 4.40 90.00 96.00 30.67 61.00 13.16 30.67 15 24.86 32.48 24.67 85.00 38.33 34.67 43.00 15.00 82.25 92.00 48.00 65.67 Llama3.3-70BInstruct 6.40 4.80 37.38 1.53 1.13 18.20 87.00 92.00 43.67 58.00 45.26 32.79 87 25.47 32.59 34.93 82.00 47.33 47.50 66.00 79.00 96.75 66.00 63.00 83.00 Table 40: Performance of HELMET tasks at sequence length 131072. 79 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance E. Training Data E.1 Synthetic English Data Topics"
        },
        {
            "title": "Acoustics\nAdministrative law\nAlgebra\nAnalytical chemistry\nAnimal law\nAstrophysics\nBranches of biology\nBranches of genetics\nBranches of philosophy\nBranches of psychology\nBusiness law\nCalculus",
            "content": "Chinese law Civil law (common law)"
        },
        {
            "title": "Social law\nSpecialist law enforcement\nagencies\nStatistics\nStatutory law by topic\nSubfields of chemistry\nSubfields of computer science\nSubfields of economics\nSubfields of physics\nTax law\nTheory of relativity\nTort law\nTypes of accounting\nTypes of marketing\nWorkplace",
            "content": "Table 41: List of topics crawled from Wikipedia category tree starting nodes. E.2 Programming Languages The 67 included programming languages are: Agda, Assembly, Batchfile, BibTex, C, C#, C++, CMake, COBOL, Coq, CSS, Dart, Dockerfile, F, Fortran, Go, Haskell, HTML, Idris, Isabelle, 80 Falcon-H1: Family of Hybrid-Head Language Models Redefining Efficiency and Performance Isabelle ROOT, Java, JavaScript, JSON, Julia, Kotlin, LabVIEW, Lean, Literate Agda, Lua, Makefile, Maple, Markdown, Mathematica, MATLAB, Nix, NumPy, Objective-C, Objective-C++, Octave, Pascal, Pep8, Perl, PHP, Pickle, PowerShell, Python, Q#, R, Ruby, Rust, SAS, Scala, Scilab, Shell, Swift, SystemVerilog, TeX, TypeScript, VBA, Verilog, VHDL, VisualBasic._NET, XML, XSLT, YAML. E.3 Code Quality Classifier The code quality classifier we run over the multi-programming language corpus supports the following programming languages: Assembly, C, C#, C++, CSS, Dart, Go, HTML, Java, JavaScript, Kotlin, Lua, PHP, PowerShell, Python, Ruby, Rust, Shell, SQL. Then, these are the only programming languages included in the HQ code corpus."
        }
    ],
    "affiliations": []
}