{
    "paper_title": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading",
    "authors": [
        "Qianjin Yu",
        "Keyu Wu",
        "Zihan Chen",
        "Chushu Zhang",
        "Manlin Mei",
        "Lingjun Huang",
        "Fang Tan",
        "Yongsheng Du",
        "Kunlin Liu",
        "Yurui Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, DeepSeek-R1 (671B) (DeepSeek-AIet al., 2025) has demonstrated its excellent reasoning ability in complex tasks and has publiclyshared its methodology. This provides potentially high-quality chain-of-thought (CoT) data for stimulating the reasoning abilities of small-sized large language models (LLMs). To generate high-quality CoT data for different LLMs, we seek an efficient method for generating high-quality CoT data with LLM-Adaptive questiondifficulty levels. First, we grade the difficulty of the questions according to the reasoning ability of the LLMs themselves and construct a LLM-Adaptive question database. Second, we sample the problem database based on a distribution of difficulty levels of the questions and then use DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) to generate the corresponding high-quality CoT data with correct answers. Thanks to the construction of CoT data with LLM-Adaptive difficulty levels, we have significantly reduced the cost of data generation and enhanced the efficiency of model supervised fine-tuning (SFT). Finally, we have validated the effectiveness and generalizability of the proposed method in the fields of complex mathematical competitions and code generation tasks. Notably, with only 2k high-quality mathematical CoT data, our ZMath-32B surpasses DeepSeek-Distill-32B in math reasoning task. Similarly, with only 2k high-quality code CoT data, our ZCode-32B surpasses DeepSeek-Distill-32B in code reasoning tasks."
        },
        {
            "title": "Start",
            "content": "Rethinking the Generation of High-Quality CoT Data from the Perspective of LLM-Adaptive Question Difficulty Grading Qianjin Yu1 Keyu Wu1 Zihan Chen1 Chushu Zhang1 ManLin Mei 1 Fang Tan1 Yongsheng Du1 Kunlin Liu1 Yurui Zhu1 Lingjun Huang1 1Intelligent System Department, Zhongxing Telecom Equipment(ZTE), Changsha, Hunan, China yuqianjin58@gmail.com, wukeyu9029@gmail.com 5 2 0 2 6 1 ] A . [ 1 9 1 9 1 1 . 4 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recently, DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) has demonstrated its excellent reasoning ability in complex tasks and has publicly shared its methodology. This provides potentially high-quality chain-of-thought (CoT) data for stimulating the reasoning abilities of smallsized large language models (LLMs). To generate high-quality CoT data for different LLMs, we seek an efficient method for generating highquality CoT data with LLM-Adaptive question difficulty levels. First, we grade the difficulty of the questions according to the reasoning ability of the LLMs themselves and construct an LLM-Adaptive question database. Second, we sample the problem database based on distribution of difficulty levels of the questions and then use DeepSeek-R1 (671B) (DeepSeek-AI et al., 2025) to generate the corresponding highquality CoT data with correct answers. Thanks to the construction of CoT data with LLMAdaptive difficulty levels, we have significantly reduced the cost of data generation and enhanced the efficiency of model supervised finetuning (SFT). Finally, we have validated the effectiveness and generalizability of the proposed method in the fields of complex mathematical competitions and code generation tasks. Notably, with only 2k high-quality mathematical CoT data, our ZMath-32B surpasses DeepSeekDistill-32B in math reasoning task. Similarly, with only 2k high-quality code CoT data, our ZCode-32B surpasses DeepSeek-Distill-32B in code reasoning tasks."
        },
        {
            "title": "Introduction",
            "content": "Since the release of DeepSeek-R1 (DeepSeek-AI et al., 2025), long chain-of-thought reasoning has gained widespread popularity in both foundational AI LLMs and wide range of industrial AI applications. However, the deployment of full-capacity R1-class models (e.g., DeepSeek-R1 with 671B parameters) poses substantial computational challenges, rendering their utilization infeasible for Figure 1: Constrcution of CoT Data with/without LLMAdaptive question difficulty grading. For LLms of different parameters, the former consistently outperforms the latter in reasoning performance on the mathematical competition dataset AIME24 (of America, 2024). edge devices and real-time systems due to prohibitive resource demands. This limitation has spurred intensive research into developing compact (<70B parameters) models capable of sustaining extended CoT reasoning, which is core competency requirement for mathematical problem-solving, code generation, and scientific analysis. Thanks to the shared reasoning process of DeepSeek-R1, we can get high-quality CoT data to boost the reasoning abilities of small-parameter LLMs. Recently, many methods for generating CoT data based on DeepSeek-R1 have been widely studied in the community. (Labs, 2025; Team, 2025c) enhance the reasoning capabilities of LLMs by using massive CoT data, enabling their reasoning abilities to reach competitive levels. (Ye et al., 2025; Muennighoff et al., 2025) aim to trigger the reasoning capabilities of large models by constructing small batch of high-quality CoT data, yet they are unable to achieve further improvements in reasoning performance. (Wen et al., 2025a) focuses on refining reasoning abilities through multi-stage curriculum learning and rejection sampling. However, these approaches rarely consider the adaptive relationship between the Base LLM and its training data during data distillation. Therefore, we rethink the question:\"What constitutes high-quality CoT data?\" and provide comprehensive answer from the perspective of LLM-Adaptive Question Difficulty Grading. Based on the above discussion, we propose method for constructing high-quality CoT data based on LLM-Adaptive Question Difficulty Grading, as shown in Figure 1. Our method efficiently creates LLM-Adaptive CoT datasets, significantly enhancing reasoning abilities of LLMs across varying parameters without requiring resourceintensive fine-tuning approaches such as curriculum learning or rejection sampling. In contrast, LLMs trained on data without adaptive difficulty grading struggle to improve or may experience degraded performance under the same cost constraints. First, we evaluate and grade the difficulty levels of the reasoning questions by analyzing the intrinsic reasoning capabilities of the LLMs. Based on this adaptive difficulty grading, we develop an adaptive question database that covers various difficulty levels. Next, we sample questions from this adaptive library, guided by carefully designed distribution across different levels of difficulty. Finally, utilizing the powerful reasoning capabilities of the DeepSeek-R1 (671B) (DeepSeekAI et al., 2025), we generate corresponding highquality CoT data that covers both mathematical reasoning and code generation tasks. In summary, the main contributions of this work are as follows: Adaptive Difficulty Evaluation: We analyze the intrinsic reasoning capabilities of LLMs to effectively evaluate and classify reasoning questions into adaptive difficulty levels. Comprehensive Adaptive Problem Library: Based on the adaptive difficulty levels, we construct an extensive problem library covering diverse difficulty categories and carefully sample questions according to well-designed difficulty distribution. High-Quality CoT Data Generation: Leveraging the DeepSeek-R1 model (671B) (DeepSeekAI et al., 2025), we generate high-quality chainof-thought (CoT) datasets that cover mathematical reasoning and code generation tasks, ensuring consistent accuracy and detailed reasoning. Comprehensive Evaluation: We conduct extensive experiments on mathematical reasoning and code generation tasks using LLMs with different parameters, demonstrating the effectiveness and generalization of our proposed method for highquality chain-of-thought (CoT) data generation."
        },
        {
            "title": "2.1 Chain-of-Thought (CoT) Data Generation",
            "content": "Current research focuses on three primary strategies for generating high-quality CoT data: (1) Manual annotation by domain experts to create goldstandard reasoning chains, primarily for benchmarking (Li et al., 2024; Huang et al., 2024; Gao et al., 2024); (2) Prompt engineering leveraging LLMs in-context learning capacity to elicit stepby-step rationales, though constrained by model biases (Wu et al., 2024; Maiti et al., 2025; Whitney et al., 2024); (3) Automated generation using selfalignment frameworks (Mahene et al., 2024; Liu et al., 2025). While such methods show promise, particularly in boosting small sized LLMs via supervised fine-tuning, key challenges persist in ensuring the diversity, correctness, and coherence of generated reasoning chains (Muennighoff et al., 2025; Ye et al., 2025). To address these limitations, recent advances integrate rejection sampling to filter low-quality reasoning paths and employ iterative refinement of teacher models. For instance, some approaches (Labs, 2025; Team, 2025c) leverage DeepSeek-R1 as the teacher reasoning model to improve step-by-step rationale generation, coupled with GPT-4o-mini for mathematical solution verification. Despite these improvements, scaling highquality CoT generation across broader domains and difficulty levels remains an open challenge, particularly in maintaining robustness against error propagation in multi-step reasoning."
        },
        {
            "title": "2.2 LLM-Adaptive Difficulty Grading",
            "content": "Traditional data generation approaches typically rely on static difficulty labels or heuristic rules, which inadequately account for the continuously evolving capabilities of large language models (LLMs). Inspired by adaptive assessment techniques in educational settings, this strategy automatically calibrates training data to align with the models current competence, thereby optimizing learning efficiency. Prior studies have explored alternative methods, such as employing LLMgenerated scoring to adjust difficulty (Team, 2025a; Xie et al., 2024; Lee and Song, 2024) or adopting curriculum learning frameworks (Wen et al., 2025a; Min et al., 2024; Yuan et al., 2025) that treat longform QA as inherently challenging tasks. However, these approaches suffer from critical limitations, including inaccurate difficulty categorization and insufficient granularity in difficulty stratification. For instance, coarse-grained curriculum designs often oversimplify difficulty levels (e.g., categorizing questions merely by length), while LLM-based scoring methods struggle to capture nuanced reasoning demands. Such shortcomings highlight the need for more sophisticated, fine-grained adaptive frameworks to bridge the gap between data difficulty and model capability."
        },
        {
            "title": "3 Method",
            "content": "In this section, we introduce in detail our method for constructing high-quality CoT data with LLMAdaptive question difficulty grading. As shown in 2, our approach contains three components, described separately in the following subsections: (1) Distribution Construction, (2) LLM-Adaptive Question Difficulty Grading & Distribution Sampling, and (3) LLM-Adaptive CoT Generation."
        },
        {
            "title": "3.1 Distribution Construction",
            "content": "To efficiently sample questions with modeladaptive difficulty grading, we require an effective reference distribution. To this end, we propose two alternative approaches for constructing question-difficulty distribution. Option1 leverages the Base LLM (defined as SLLM ) to characterize the true difficulty-level distribution (defined as Peval) over the evaluation datasets. Option2 employs customized distribution (defined as PC) based on human-defined priors. Detailed descriptions of both methods are presented below. Peval = (cid:40) Easy, Grader(Q, R), if answer is True if answer is False (1) where Grader(Q, R) denotes the Difficulty grading given by the PRM-Grader based on the reponse of the SLLM . The details of the Result-Verifier and PRM-Grader can be seen in 3. Option2 Inspired by the idea of curriculum learning, we hypothesize that during the model finetuning process, the model learns relatively difficult questions more easily compared to very difficult ones. Therefore, we also propose curriculumlearning-based customized distribution. Specifically, we classify question difficulty into five levels, with the number of samples at each difficulty level decreasing as the difficulty increases. The distribution can be formally defined by the following equation: PC ="
        },
        {
            "title": "Ni\nNtotal",
            "content": "= wi j=1 wj (cid:80)5 , (2) wi > wi+1, = 1, 2, . . . , 4 where Ni denotes the number of questions at difficulty level i, Ntotal denotes the total number of questions, and wi represents the weight assigned to difficulty level i. The constraint wi > wi+1 illustrates that the assigned number of samples decreases as question difficulty increases. We propose two methods for constructing modeladaptive difficulty distributions: Option1 derives difficulty levels from the actual performance of the SLLM , while Option2 uses curriculum-learninginspired human-defined distribution. Subsequent experiments in section 4 will analyze and compare these approaches in detail. Next, we will use the distributions to guide the selection of LLMAdaptive Questions."
        },
        {
            "title": "3.2 LLM-Adaptive Question Difficulty\nGrading & Distribution Sampling",
            "content": "Option1 To obtain the actual difficulty-level distribution Peval from the SLLM for the evaluation data DBeval, we first perform answer verification through Result-Verifier. Those questions correctly answered by SLLM are defined as easy-level problems. Then, we utilize PRM-Grader to grade the difficulty levels of the questions that the SLLM answers incorrectly. The specific grading formulation is shown as follows: After constructing the question difficulty distribution based on either the evaluation-set distribution or curriculum-learning-inspired principles, we further need to build specialized candidate question database from large-scale data space using the model-adaptive difficulty grading method. Following such construction, we can perform sampling according to the predefined difficulty distribution to obtain the final high-quality questions. First, we Figure 2: The Framework for CoT Data Generation via LLM-Adaptive Question Difficulty Grading , comprising three core components: Distribution Construction, LLM-Adaptive Question Difficulty Grading & Distribution Sampling, and LLM-Adaptive Chain-of-Thought (CoT) Generation illustrate our LLM-Adaptive Question Difficulty Grading method for building the candidate question database (defined as DBAdaptive) in detail. Then, we describe the process of Distribution Sample to acquire the LLM-Adaptive Questions. LLM-Adaptive Question Difficulty Grading First, we collect original questions from largescale open-source datasets, each accompanied by standardized answers, thus constructing an initial question-answer database (defined as DBraw). Next, we generate responses for these questions using the SLLM and record their reply trajectories and results. Then, we apply the Result-Verifier customized to specific tasks. For mathematical reasoning, we adopt Math Verifier, directly comparing the LLM-generated and standard answers; for code generation tasks, correctness is verified by executing the produced code against suite of test cases, passing all tests indicating correctness. According to equation 1 , verified correct responses are labeled as easy and directly added to the candidate question database. For responses deemed incorrect, we label these questions as difficult and further utilize the PRM-Grader, assigning difficulty levels into five categories. Specifically, the PRM-Grader computes an average score (ranging from 0 to 1) reflecting the response trajectory of SLLM , mapping this score onto five discrete difficulty levels, with lower scores indicating relatively higher difficulty. Ultimately, the questions categorized by these difficulty levels are collectively included within our candidate question database, thereby completing the construction of DBAdaptive in LLM-Adaptive manner. Distribution Sample After constructing the candidate question database, we employ distributionbased sampler, guided by the question-difficulty distribution established in Section 3.1, to sample high-quality, model-adaptive questions from this database as preliminary inputs for obtaining highquality CoT data. This procedure is formally defined as follows: DBsample Sampler (cid:0)DBAdaptive, Peval PC (cid:1) , (3) where DBsample represents the sampled questions, DBAdaptive denotes the candidate question database, and Peval, PC indicates the difficultylevel distribution determined in Section 3.1."
        },
        {
            "title": "3.3 LLM-Adaptive CoT Generation",
            "content": "After obtaining the DBSample, we directly employ the TLLM to generate responses and associated reasoning processes for these sampled questions. Subsequently, we apply the Result-Verifier to examine and validate the correctness of these generated responses. The implementation of ResultVerifier here is identical to that described in Section 3.2. The TLLM in our experiments is DeepSeekR1(671B) (DeepSeek-AI et al., 2025). Following the verification process, we select only those questions whose corresponding responses and reasoning processes have been validated as correct, thereby forming high-quality CoT dataset COTAdaptive. Finally, this rigorously-constructed CoT dataset serves as training data for fine-tuning SLLM to get the final reasoning LLM RLLM ."
        },
        {
            "title": "4.1 Setup",
            "content": "Datasets and Metrics Our training datasets consist of high-quality mathematical reasoning problems sourced from NuminaMath (LI et al., 2024), historical AIME problems (of America, 2024), and OlympicArena (Huang et al., 2024), as well as challenging code generation tasks from TACO (Li et al., 2023) and CodeForces (Penedo et al., 2025). Benchmarks To give reasonable result, we evaluate our trained models on the following authoritative benchmarks: AIME24 and AIME25 (of America, 2024) comprise challenging mathematics competition problems from the American Invitational Mathematics Examination of 2024 and 2025. MATH500 (Lightman et al., 2023) is representative subset of 500 mathematical problems from the comprehensive MATH dataset. GPQA (Rein et al., 2024) is dataset focused on graduate-level physics questions designed to evaluate advanced problem-solving skills. LiveCodeBench (EASY, MEDIUM, HARD) (Jain et al., 2024) contains competitive coding problems sourced from various platforms categorized by three difficulty levels. Settings Our training framework builds on previous advancements in s1-1k(Muennighoff et al., 2025), LIMO(Ye et al., 2025), and Light-R1(Wen et al., 2025b), implemented through the LLamaFactory(Zheng et al., 2024) to leverage its proven scalability. The framework incorporates the Deepseek-R1 template, flash-attention2(Dao, 2024) and Liger-Kernel(Hsu et al., 2024) to improve computational efficiency while minimizing memory requirements. All experiments are conducted on Model DS-distill-7B Zmath-7B phi4-14B Zmath-14B DS-distill-32B Sky-32B-Preview Zmath-32B MATH AIME 24 56.67 60 30 50.0 66.67 43.33 73.33 AIME 25 33.3 43. 16.67 36.67 50.0 23.33 56.67 MATH 500 89.4 93.2 79.2 89.4 89.8 90 94."
        },
        {
            "title": "GPQA",
            "content": "49.49 49.49 54.55 63.13 59.6 50.0 63.13 Table 1: Comparison of LLMs with different parameters on Math Reasoning Benchmarks Model"
        },
        {
            "title": "EASY MEDIUM HARD",
            "content": "DS-distill-7B Zcode-7B phi4-14B Zcode-14B 79.21 81.0 72.4 89.96 DS-distill-32B"
        },
        {
            "title": "92.11\nSky-32B-Preview 84.23\n96.06",
            "content": "Zcode-32B 41.09 39.58 29.91 41.99 74.92 46.53 75.53 11.11 10.11 5.19 8. 30 8.89 31.85 Table 2: Comparison of LLMs with different parameters on Code Generation Benchmarks 28 H800 GPU cluster, with performance evaluations executed using the Skythought benchmarking suite(Team, 2025a). The core hyperparameters for the initial experiments included context length of 16384, learning rate of 5e-6, batch size of 128, and 10 training epochs. Baselines We take the three representative baselines below for comparison: phi-4: phi-4(Abdin et al., 2024) is 14B LLM developed by Microsoft. Phi-4 demonstrates exceptional performance in complex reasoning tasks, particularly in mathematics, where it achieves 80.4% on the MATH benchmark and 80.6% on MGSM. DeepSeek-Distill-R1: series of LLMs developed by Deepseek, distilled from the DeepseekR1 using 800k training instances(DeepSeek-AI et al., 2025). Our implementation primarily utilizes the 7B and 32B variants distilled on the Qwen architecture. Sky-T1-32B-Preview: This model exhibits characteristics similar to Distill-R1-32B, but was developed by the Sky-T1 team using 10,000 distillation samples that specifically target mathematical and coding capabilities(Team, 2025b). Grading Methods"
        },
        {
            "title": "4.2 Results and Analysis",
            "content": "Main Results Table 1 and Table 2 present the results of controlled experiments evaluating LLM performance on mathematical and coding benchmark datasets, respectively. Synthetic mathematical and coding data were generated using three base LLMs, DS-distill-7B, phi4-14B, and DS-distill-32B, and subjected to supervised fine tuning (SFT) to produce series of LLMs at 7B, 14B, and 32B parameter scales. These LLMs were grouped into two categories: Zmath for mathematical tasks and Zcode for coding tasks. The Zmath-14B LLM was trained on approximately 16,000 distilled data points derived from Deepseek-R1. The distilled data set was manually verified to ensure the accuracy of the answers. After training, the LLM output was aligned with the Deepseek-r1 format. Compared to the baseline phi4-14B LLM, Zmath-14B demonstrated substantial improvements: an average gain of 20 points on the AIME (2024, 2025) and Livecode-easy benchmarks; an approximate increase of 10 points on the GPQA and Livecode-medium datasets.Notably, Zmath-14B outperformed the Sky-32B-Preview LLM on mathematical tasks, highlighting its superior capability in this domain. To explore the upper limit of our adaptive data synthesis approach, we trained LLMs using only 2,000 Chain of Thought (CoT) mathematical and coding data points on the DS-distill-32B. The results were as follows: Zmath-32B achieved an average improvement of 5 points across all mathematical benchmarks, significantly exceeding the performance of DS-distill-32B. On the Livecode Bench, Zcode-32B recorded an average gain of 2.14 points over DS-distill-32B. We hypothesize that further enhancements in coding performance, relative to mathematical gains, may necessitate larger training corpus. Comparable performance improvements were observed with the smaller Zmath-7B and Zcode-7B models, consistent with the trends observed in their 32B counterparts. These findings underscore the efficacy of our data synthesis method across varying model scales and task domains. Ablation Studies We conducted four ablation studies using the code datasets to comprehensively validate the effectiveness of our proposed LLMNo-Grading UT-Grading PRM-Grading 92.11 93.19 96. 74.92 71.60 75.53 30.00 28.15 31.85 Table 3: Performance (%) of DS-distill-32B trained on 2K data using different difficulty grading methods (None, UT-based, and PRM-based), evaluated on LiveCode-Bench. Adaptive difficulty grading approach from various perspectives. (1) Comparison of Difficulty Grading Methods (PRM vs. UT) This experiment verifies the effectiveness of different difficulty grading methods. We compared two difficulty grading methods: Process Reward Model (PRM)-based grading, which assigns 0-1 scores divided into five levels (lower scores indicate higher difficulty), and Unit Test (UT)-based grading, which categorizes problems into five levels based on the percentage of passed test cases (lower pass rates indicate higher difficulty). As shown in Table 3, we compare the results of DeepSeek-Distill-32B trained with UTgraded 2K data, PRM-graded 2K data, and the baseline evaluated on LiveCodeBench (easy-mediumhard), demonstrating the superior effectiveness of the PRM-based difficulty grading method. (2) Distribution Transfer Experiment To examine whether different models exhibit unique preferences for difficulty distributions, we perform distribution transfer experiment. Specifically, we transfer the PRM-based difficulty distribution derived from the DeepSeek-Distill-32B model to train the DeepSeek-Distill-7B model. We then compare its performance with counterpart trained using the 7B models own PRM-based distribution. As shown in Table 4, the 7B model benefits more from training on its self-derived difficulty distribution, suggesting that model-specific difficulty adaptation plays critical role in performance optimization. (3) Influence of Training Data Size To investigate the influence of the size of the training data on reasoning performance, we trained the DeepSeekDistill-32B model using 1K and 2K PRM-graded math examples. As shown in Table 5, increasing the training data size from 1K to 2K led to consistent performance improvements across all four math benchmarks. CoT Source Baseline-7B +CoT from 32B +CoT from 7B MATH AIME 24 56.67 56.67 60. AIME 25 33.30 40.00 43.33 MATH 500 89.40 92.00 93."
        },
        {
            "title": "GPQA",
            "content": "49.49 45.96 49.49 Table 4: Performance (%) of 7B LLM with training data distributions derived from 32B and 7B LLMs on math benchmarks. The 7B/32B LLM is DS-distill-7B/32B. Training Setup MATH MATH 500 AIME AIME"
        },
        {
            "title": "GPQA",
            "content": "No PRM fine-tuning +1K PRM-graded data +2K PRM-graded data 89.80 95.50 94.60 66.67 73.33 73.33 50.00 53.33 56.67 59.60 60.61 63.13 Table 5: Performance (%) of DeepSeek-Distill-32B trained on varying sizes of PRM-graded math data. (4) Impact of Training Data Distribution Standard We compare two strategies for defining training data distributions: Option 1 leverages the base LLM to infer the true difficulty distribution from the evaluation dataset, while Option 2 relies on human-defined prior distributions. As shown in Tab. 6, using the model-inferred evaluation distribution (Option 1) achieves stronger results on most benchmarks, indicating that aligning training distribution with evaluation difficulty leads to better generalization."
        },
        {
            "title": "MATH",
            "content": "MATH 500 AIME 24 AIME"
        },
        {
            "title": "GPQA",
            "content": "Baseline-7B +Option 1 (LLM) +Option 2 (Human) 89.40 93.20 90.80 56.67 60.00 63.33 33.30 43.33 33.33 49.49 49.49 48.99 Table 6: Performance (%) of DS-distill-7B trained on 2K PRM-graded data using different training distribution strategies. Option 1 uses LLM-inferred evaluationset distribution; Option 2 adopts human-defined priors."
        },
        {
            "title": "5 Conclusion",
            "content": "In this paper, we propose general and efficient method for constructing high-quality Chain-ofThought (CoT) datasets. Firstly, we build question database more aligned with the Base LLM itself by leveraging method that adaptively grade question difficulty. This database possess potential source of high-quality questions. Next, we use the difficulty distribution from either LLM performance on evaluation datasets or curriculumlearning-inspired difficulty levels, to sample crucial questions for improving the reasoning capability. Finally, these selected questions are employed to generate Chain-of-Thought data through the teacher LLM (DeepSeek-R1), forming COT dataset that is adaptively graded according to question difficulty aligned with the Base LLM. Benefiting from our constructed COT data, we effectively refine LLMs through supervised finetuning (SFT), achieving improved reasoning abilities across LLMs of different parameter scales. In the future, we plan to integrate our approach for constructing high-quality COT data with reinforcement learning or reject sampling, further enhancing the reasoning abilities of the models."
        },
        {
            "title": "References",
            "content": "Marah Abdin, Jyoti Aneja, Harkirat Behl, Sébastien Bubeck, Ronen Eldan, Suriya Gunasekar, Michael Harrison, Russell J. Hewett, Mojan Javaheripi, Piero Kauffmann, James R. Lee, Yin Tat Lee, Yuanzhi Li, Weishung Liu, Caio C. T. Mendes, Anh Nguyen, Eric Price, Gustavo de Rosa, Olli Saarikivi, Adil Salim, Shital Shah, Xin Wang, Rachel Ward, Yue Wu, Dingli Yu, Cyril Zhang, and Yi Zhang. 2024. Phi-4 technical report. arXiv preprint arXiv:2412.08905. Tri Dao. 2024. FlashAttention-2: Faster attention with better parallelism and work partitioning. In International Conference on Learning Representations (ICLR). DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, Huazuo Gao, Hui Qu, Hui Li, Jianzhong Guo, Jiashi Li, Jiawei Wang, Jingchang Chen, Jingyang Yuan, Junjie Qiu, Junlong Li, J. L. Cai, Jiaqi Ni, Jian Liang, Jin Chen, Kai Dong, Kai Hu, Kaige Gao, Kang Guan, Kexin Huang, Kuai Yu, Lean Wang, Lecong Zhang, Liang Zhao, Litong Wang, Liyue Zhang, Lei Xu, Leyi Xia, Mingchuan Zhang, Minghua Zhang, Minghui Tang, Meng Li, Miaojun Wang, Mingming Li, Ning Tian, Panpan Huang, Peng Zhang, Qiancheng Wang, Qinyu Chen, Qiushi Du, Ruiqi Ge, Ruisong Zhang, Ruizhe Pan, Runji Wang, R. J. Chen, R. L. Jin, Ruyi Chen, Shanghao Lu, Shangyan Zhou, Shanhuang Chen, Shengfeng Ye, Shiyu Wang, Shuiping Yu, Shunfeng Zhou, Shuting Pan, S. S. Li, Shuang Zhou, Shaoqing Wu, Shengfeng Ye, Tao Yun, Tian Pei, Tianyu Sun, T. Wang, Wangding Zeng, Wanjia Zhao, Wen Liu, Wenfeng Liang, Wenjun Gao, Wenqin Yu, Wentao Zhang, W. L. Xiao, Wei An, Xiaodong Liu, Xiaohan Wang, Xiaokang Chen, Xiaotao Nie, Xin Cheng, Xin Liu, Xin Xie, Xingchao Liu, Xinyu Yang, Xinyuan Li, Xuecheng Su, Xuheng Lin, X. Q. Li, Xiangyue Jin, Xiaojin Shen, Xiaosha Chen, Xiaowen Sun, Xiaoxiang Wang, Xinnan Song, Xinyi Zhou, Xianzu Wang, Xinxia Shan, Y. K. Li, Y. Q. Wang, Y. X. Wei, Yang Zhang, Yanhong Xu, Yao Li, Yao Zhao, Yaofeng Sun, Yaohui Wang, Yi Yu, Yichao Zhang, Yifan Shi, Yiliang Xiong, Ying He, Yishi Piao, Yisong Wang, Yixuan Tan, Yiyang Ma, Yiyuan Liu, Yongqiang Guo, Yuan Ou, Yuduan Wang, Yue Gong, Yuheng Zou, Yujia He, Yunfan Xiong, Yuxiang Luo, Yuxiang You, Yuxuan Liu, Yuyang Zhou, Y. X. Zhu, Yanhong Xu, Yanping Huang, Yaohui Li, Yi Zheng, Yuchen Zhu, Yunxian Ma, Ying Tang, Yukun Zha, Yuting Yan, Z. Z. Ren, Zehui Ren, Zhangli Sha, Zhe Fu, Zhean Xu, Zhenda Xie, Zhengyan Zhang, Zhewen Hao, Zhicheng Ma, Zhigang Yan, Zhiyu Wu, Zihui Gu, Zijia Zhu, Zijun Liu, Zilin Li, Ziwei Xie, Ziyang Song, Zizheng Pan, Zhen Huang, Zhipeng Xu, Zhongyu Zhang, and Zhen Zhang. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. 2024. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985. Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, and Yanning Chen. 2024. Liger kernel: Efficient triton kernels for llm training. arXiv preprint arXiv:2410.10989. Zhen Huang, Zengzhi Wang, Shijie Xia, Xuefeng Li, Haoyang Zou, Ruijie Xu, Run-Ze Fan, Lyumanshan Ye, Ethan Chern, Yixin Ye, et al. 2024. Olympicarena: Benchmarking multi-discipline cognitive reasoning for superintelligent ai. Advances in Neural Information Processing Systems, 37:1920919253. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando SolarLezama, Koushik Sen, and Ion Stoica. 2024. Livecodebench: Holistic and contamination free evaluation of large language models for code. Bespoke Labs. 2025. Bespoke-stratos: The unreareasoning distillation. sonable effectiveness of https://www.bespokelabs.ai/blog/bespoke-stratosthe-unreasonable-effectiveness-of-reasoningdistillation. Accessed: 2025-01-22. Jung Lee and Yeong-Tae Song. 2024. College exam In 2024 IEEE/ACIS grader using llm ai models. 27th International Conference on Software Engineering, Artificial Intelligence, Networking and Parallel/Distributed Computing (SNPD), pages 282289. IEEE. Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. 2024. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13:9. Jia LI, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Costa Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, Zihan Qin, Bin Dong, Li Zhou, Yann Fleureau, Guillaume Lample, and Stanislas Polu. 2024. Numinamath. [https://huggingface.co/ AI-MO/NuminaMath-CoT](https://github.com/ project-numina/aimo-progress-prize/blob/ main/report/numina_dataset.pdf). Rongao Li, Jie Fu, Bo-Wen Zhang, Tao Huang, Zhihong Sun, Chen Lyu, Guang Liu, Zhi Jin, and Ge Li. 2023. Taco: Topics in algorithmic code generation dataset. arXiv preprint arXiv:2312.14852. Lightman, Hunter, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. 2023. Lets verify step by step. arXiv preprint arXiv:2305.20050. Beiming Liu, Zhizhuo Cui, Siteng Hu, Xiaohua Li, Haifeng Lin, and Zhengxin Zhang. 2025. Llm evaluation based on aerospace manufacturing expertise: Automated generation and multi-model question answering. arXiv preprint arXiv:2501.17183. Anthony Mahene, Daniel Pereira, Vincent Kowalski, Elizabeth Novak, Catherine Moretti, and Josephine Laurent. 2024. Automated dynamic data generation for safety alignment in large language models. Authorea Preprints. Aniruddha Maiti, Samuel Adewumi, Temesgen Alemayehu Tikure, Zichun Wang, Niladri Sengupta, Anastasiia Sukhanova, and Ananya Jana. 2025. Comparative analysis of openai gpt-4o and deepseek r1 for scientific text categorization using prompt engineering. arXiv preprint arXiv:2503.02032. Yingqian Min, Zhipeng Chen, Jinhao Jiang, Jie Chen, Jia Deng, Yiwen Hu, Yiru Tang, Jiapeng Wang, Xiaoxue Cheng, Huatong Song, et al. 2024. Imitate, explore, and self-improve: reproduction report on slow-thinking reasoning systems. arXiv preprint arXiv:2412.09413. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Candès, and Tatsunori Hashimoto. 2025. s1: Simple test-time scaling. Mathematical Association of America. 2024. Aime. Guilherme Penedo, Anton Lozhkov, Hynek Kydlíˇcek, Loubna Ben Allal, Edward Beeching, Agustín Piqueres Lajarín, Quentin Gallouédec, Nathan Habib, Lewis Tunstall, and Leandro von Werra. 2025. Codeforces. https://huggingface. co/datasets/open-r1/codeforces. David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel R. Bowman. 2024. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, Zheyan Luo, Zhangchi Feng, and Yongqiang Ma. 2024. Llamafactory: Unified efficient fine-tuning In Proceedings of the of 100+ language models. 62nd Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), Bangkok, Thailand. Association for Computational Linguistics. NovaSky Team. 2025a. o1 preview model within 450. ai.github.io/posts/sky-t1. Accessed: 2025-01-09. Sky-t1: Train your own https://novaskyNovaSky Team. 2025b. achieve more: Cut reasoning costs by 50 https://novaskyai.github.io/posts/reduce-overthinking. Accessed: 2025-01-23. Think less, OpenThoughts Team. 2025c. https://open-thoughts.ai. Open Thoughts. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. 2025a. Light-r1: Curriculum sft, dpo and rl for long arXiv preprint cot from scratch and beyond. arXiv:2503.10460. Liang Wen, Yunke Cai, Fenrui Xiao, Xin He, Qi An, Zhenyu Duan, Yimin Du, Junchen Liu, Lifu Tang, Xiaowei Lv, Haosheng Zou, Yongchao Deng, Shousheng Jia, and Xiangzheng Zhang. 2025b. Light-r1: Curriculum sft, dpo and rl for long arXiv preprint cot from scratch and beyond. arXiv:2503.10460. Claire Whitney, Edward Jansen, Victor Laskowski, and Charles Barbieri. 2024. Adaptive prompt regeneration and dynamic response structuring in large language models using the dynamic query-response calibration protocol. Authorea Preprints. Siwei Wu, Zhongyuan Peng, Xinrun Du, Tuney Zheng, Minghao Liu, Jialong Wu, Jiachen Ma, Yizhi Li, Jian Yang, Wangchunshu Zhou, et al. 2024. comparative study on reasoning patterns of openais o1 model. arXiv preprint arXiv:2410.13639. Wenjing Xie, Juxin Niu, Chun Jason Xue, and Nan Guan. 2024. Grade like human: Rethinking automated assessment with large language models. arXiv preprint arXiv:2405.19694. Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, and Pengfei Liu. 2025. Limo: Less is more for reasoning. Siyu Yuan, Zehui Chen, Zhiheng Xi, Junjie Ye, Zhengyin Du, and Jiecao Chen. 2025. Agent-r: Training language model agents to reflect via iterative selftraining. arXiv preprint arXiv:2501.11425."
        }
    ],
    "affiliations": [
        "Intelligent System Department, Zhongxing Telecom Equipment(ZTE), Changsha, Hunan, China"
    ]
}