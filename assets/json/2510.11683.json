{
    "paper_title": "Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models",
    "authors": [
        "Nianyi Lin",
        "Jiajie Zhang",
        "Lei Hou",
        "Juanzi Li"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "A key challenge in applying reinforcement learning (RL) to diffusion large language models (dLLMs) lies in the intractability of their likelihood functions, which are essential for the RL objective, necessitating corresponding approximation in each training step. While existing methods approximate the log-likelihoods by their evidence lower bounds (ELBOs) via customized Monte Carlo (MC) sampling, the forward computational graphs of all MC samples need to be retained for the gradient computation of non-linear terms in the RL objective, resulting in significant memory overhead. This constraint restricts feasible sample sizes, leading to imprecise likelihood approximations and ultimately distorting the RL objective. To overcome this limitation, we propose \\emph{Boundary-Guided Policy Optimization} (BGPO), a memory-efficient RL algorithm that maximizes a specially constructed lower bound of the ELBO-based objective. This lower bound is carefully designed to satisfy two key properties: (1) Linearity: it is formulated in a linear sum where each term depends only on a single MC sample, thereby enabling gradient accumulation across samples and ensuring constant memory usage; (2) Equivalence: Both the value and gradient of this lower bound are equal to those of the ELBO-based objective in on-policy training, making it also an effective approximation for the original RL objective. These properties allow BGPO to adopt a large MC sample size, resulting in more accurate likelihood approximations and improved RL objective estimation, which in turn leads to enhanced performance. Experiments show that BGPO significantly outperforms previous RL algorithms for dLLMs in math problem solving, code generation, and planning tasks. Our codes and models are available at \\href{https://github.com/THU-KEG/BGPO}{https://github.com/THU-KEG/BGPO}."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 2 3 8 6 1 1 . 0 1 5 2 : r Boundary-Guided Policy Optimization for Memory-efficient RL of Diffusion Large Language Models Nianyi Lin*, Jiajie Zhang*, Lei Hou, Juanzi Li Tsinghua University"
        },
        {
            "title": "Abstract",
            "content": ""
        },
        {
            "title": "Introduction",
            "content": "A key challenge in applying reinforcement learning (RL) to diffusion large language models (dLLMs) lies in the intractability of their likelihood functions, which are essential for the RL objective, necessitating corresponding approximation in each training step. While existing methods approximate the log-likelihoods by their evidence lower bounds (ELBOs) via customized Monte Carlo (MC) sampling, the forward computational graphs of all MC samples need to be retained for the gradient computation of non-linear terms in the RL objective, resulting in significant memory overhead. This constraint restricts feasible sample sizes, leading to imprecise likelihood approximations and ultimately distorting the RL objective. To overcome this limitation, we propose Boundary-Guided Policy Optimization (BGPO), memory-efficient RL algorithm that maximizes specially constructed lower bound of the ELBO-based objective. This lower bound is carefully designed to satisfy two key properties: (1) Linearity: it is formulated in linear sum where each term depends only on single MC sample, thereby enabling gradient accumulation across samples and ensuring constant memory usage; (2) Equivalence: Both the value and gradient of this lower bound are equal to those of the ELBO-based objective in on-policy training, making it also an effective approximation for the original RL objective. These properties allow BGPO to adopt large MC sample size, resulting in more accurate likelihood approximations and improved RL objective estimation, which in turn leads to enhanced performance. Experiments show that BGPO significantly outperforms previous RL algorithms for dLLMs in math problem solving, code generation, and planning tasks. Our codes and models are available at https://github.com/THU-KEG/BGPO. *Equal contribution. Work done when JZ interned at Zhipu. 1 Recently, diffusion large language models (dLLMs) have emerged as promising alternatives to conventional autoregressive models (ARMs), demonstrating competitive performance across various language modeling tasks (Nie et al., 2025b; Ye et al., 2025; Gong et al., 2025b; Cheng et al., 2025). Unlike ARMs, which generate sequences in left-toright, token-by-token manner, dLLMs iteratively unmask tokens in parallel, offering the potential for significant inference acceleration (DeepMind, 2025; Inception Labs et al., 2025; Song et al., 2025; Wu et al., 2025). Despite these advancements, existing works primarily focus on pre-training and supervised fine-tuning of dLLMs, while leveraging reinforcement learning (RL) to further enhance dLLMs remains challenging problem, even though RL has demonstrated significant efficacy in improving various capabilities of LLMs (OpenAI, 2024; DeepSeek-AI et al., 2025). key challenge in applying RL to dLLMs lies in the intractability of their likelihood functions (Zhu et al., 2025; Zhao et al., 2025a; Tang et al., 2025). Specifically, the iterative, non-sequential generation process precludes exact calculation of the log-likelihoods for generated responses (Zhu et al., 2025; Zhao et al., 2025a; Tang et al., 2025), which are essential for different RL algorithms (Schulman et al., 2017; Shao et al., 2024). In light of this, recent works have explored approximating the log-likelihoods by their evidence lower bounds (ELBOs) via customized Monte Carlo (MC) sampling (Zhu et al., 2025). While increasing the MC sample size can yield highly accurate approximations (Ho et al., 2020; Song et al., 2021), this approach incurs substantial memory overhead during RL training. In particular, computing the gradient of the non-linear functions in the RL objective necessitates storing the forward computational graphs for all MC samples, dramatically increasing Figure 1: Left: Comparison of memory usage of previous ELBO-based RL method (VRPO-OL) and our BGPO using different Monte Carlo sample size nt for the RL objective approximation. The max response length is set to 512. Middle and right: Performance of LLaDAs with different RL algorithms on mathematical tasks. memory consumption. As result, practical implementations can only adopt relatively small sample sizes (e.g., nt = 4 as illustrated in the left of Figure 1) due to hardware constraints, which directly amplifies errors in log-likelihood approximation and introduces substantial bias and variance for the estimated objective and its gradients, ultimately degrading performance. To address this limitation, we propose BoundaryGuided Policy Optimization (BGPO), memoryefficient RL algorithm for dLLMs that supports large MC sample sizes for log-likelihood and RL objective approximation. Specifically, BGPO maximizes constructed lower bound of the ELBObased objective. This lower bound is carefully designed to satisfy two critical properties: (1) Linearity: it is formulated in linear sum where each term associates with single MC sample, thereby enabling gradient accumulation across samples and ensuring constant memory usage irrespective of sample size; (2) Equivalence: Both the value and gradient of the lower bound are equal to those of the ELBO-based objective in on-policy training, ensuring that the lower bound can also effectively approximate the original RL objective. These properties allow BGPO to adopt large MC sample size to obtain more accurate approximation for the RL objective, thereby achieving better performance. To validate the effectiveness of BGPO, we conduct RL experiments with LLaDA-8B-Instruct on math problem solving, code generation, and planning tasks. The results show that BGPO significantly improves the performance of LLaDA-8BInstruct across all tasks, and also outperforms previous RL algorithms for dLLMs. Further analysis demonstrates that increasing the MC sample size effectively reduces the bias and variance of gradients and improves model performance. Notably, BGPO achieves these improvements with only marginal increases in average training step time, despite its larger sample size. In summary, our main contributions include: (1) We propose BGPO, memory-efficient RL algorithm for dLLMs that supports large MC sample sizes in the approximation of log-likelihoods and the RL objective; (2) We theoretically prove the equivalence of the BGPO objective and the ELBObased objective in on-policy training, demonstrating that BGPO also provides an effective approximation of the original RL objective; (3) Through comprehensive experiments, we validate the efficacy of BGPO and demonstrate the value of larger MC sample sizes in boosting model performance. We hope our work establishes firm foundation for future research on RL for dLLMs."
        },
        {
            "title": "2 Preliminary",
            "content": "2.1 Masked Diffusion Language Models Masked dLLMs employ non-autoregressive generation paradigm, generating text through progressive denoising. At their core lies mask predictor pθ (Austin et al., 2021a; Ou et al., 2025), which learns the data distribution through forwardreverse framework. Starting from the original text at = 0, the forward process gradually masks the input tokens until the sequence is fully masked at = 1. Following LLaDA (Nie et al., 2025b), at time (0, 1), each token is replaced by the mask token with probability and remaining unmasked with probability 1 t. Conversely, the reverse process employs the mask predictor to recover this sequence by iteratively predicting the masked tokens as time reverses from 1 to 0. In conditional generation scenarios, the prompt always remains unmasked, and the forward-reverse 2 process is only applied to the response y. where 2.2 Challenges of Applying RL to dLLMs. Reinforcement learning (RL) has proved effective for improving language models, and the basic objective is to maximize: (θ) =ExD,yπθ(x)A(x, y) πθ(yx) =ExD,yπθold πθold =ExD,yπθold (x)R(x, y), (x) (yx) A(x, y), (1) where πθ, πθold, and A(x, y) denote the current policy, old policy, and sequence-level advantage, respectively, and R(x, y) = elog πθ(yx)log πθold (yx)A(x, y). (2) However, applying RL to dLLMs is nontrivial, since the iterative denoising generation makes the exact computation of log πθ(yx) intractable. To address this, recent works have developed several methods to approximate log πθ(yx). diffu-GRPO (Zhao et al., 2025a) adopts singlepass estimation, simply making log πθ(yx) = (cid:80)y i=1 log pθ(yix), where yi is the i-th token of and is randomly masked prompt. Though efficient, it introduces notable bias relative to the exact policy. Alternatively, VRPO (Zhu et al., 2025) proposes to approximate log πθ(yx) by using its evidence lower bound (ELBO): Bπθ (yx) EtU [0,1],ytq(t,y,x)ℓπθ (yt, t, yx), (3) where q(t, y, x) denotes the forward masking process for the response at time t, and ℓπθ (yt, t, yx) 1 i=1 yt (cid:88) 1[yi = M] log pθ(yiyt, x). (4) Specifically, they estimate Bπ(yx) via customized Monte Carlo sampling: ˆBπθ (yx) = 1 nt nt(cid:88) j=1 ℓπθ (yt(j), t(j), yx), (5) i.i.d. q(t(j), y, x) where t(j) i.i.d. U[0, 1] and yt(j) are sampled timestamp and corresponding partially masked responses. Substituting ˆBπθ (yx) into Eq. 1 yields an approximated RL objective: ˆJ (θ) = ExD,yπθold (x) ˆR(x, y), (6) 3 ˆR(x, y) = ˆBπθ (yx) ˆBπold (yx)A(x, y). (7) Notably, previous works have shown that when the sample size nt is large enough, the bias of ˆBπ(yx) for well-trained model relative to log πθ(yx) will become negligible (Ho et al., 2020; Song et al., 2021). However, using large nt in training requires huge amount of GPU memory: Each time ˆR(x, y) is computed, nt forward passes of pθ need to be executed (i.e., Eq. 4 and 5), and all the nt computational graphs must be retained in the memory for calculating the gradient of the exponential function in Eq. 7. Therefore, in practice, the sample size can only remain small (e.g., nt = 4), which results in inaccurate approximations for the likelihoods as well as the final objective, seriously affecting the final performance. To break through this limitation, we propose Boundary-Guided Policy Optimization (BGPO), memory-efficient RL algorithm for dLLMs that supports large Monte Carlo sample size, thereby enabling more accurate approximations and achieving better performance. detailed introduction is provided in the following sections."
        },
        {
            "title": "3 BGPO",
            "content": "Following Zhu et al. (2025), our BGPO algorithm also uses the estimated ELBO ˆBπθ (yx) to approximate log πθ(yx). The main difference is that instead of directly maximizing the approximated objective ˆJ (θ), BGPO turns to maximize constructed tight lower bound of ˆJ (θ): ˆJlb(θ) = ExD,yπθold ˆRlb(x, y), (8) (x) where ˆRlb(x, y) ˆR(x, y). Specifically, ˆRlb(x, y) is carefully designed so that it satisfies the following two properties1: Linearity: ˆRlb(x, y) is formulated as (cid:80)nt j=1 gj, where gj is function about the partially masked sample yt(j) at time t(j). Therefore, we can backpropagate the gradient of gj separately for each yt(j) and update the policy after all backpropagations, so that the memory usage becomes irrelevant to the MC sample size nt. Equivalence: In on-policy training (i.e., πθold = πθ), the value and gradient of ˆRlb(x, y) are always equal to those of ˆR(x, y), making ˆJlb(θ) 1For simplicity, we mainly discuss ˆRlb and ˆR in this section, while all their properties can be directly applied to ˆJlb and ˆJ , without influence by the expectation function. be equivalent to ˆJ (θ) and also an effective approximation for the original RL objective (θ). These two properties allow BGPO to use larger MC sample size in the likelihood approximation, which effectively reduces the bias and variance of ˆJlb(θ) and its gradient, leading to better performance. In the following, we will introduce the construction of ˆRlb(x, y) in detail. 3.1 Linear Lower Bound Construction The construction of ˆRlb(x, y) is different based on the sign of the advantage A(x, y): For A(x, y) 0, we construct ˆRlb(x, y) using Taylor expansion; For A(x, y) < 0, we construct ˆRlb(x, y) using Jensons inequality. Lemma 1. [First-order Taylor Expansion] For any δ R, the exponential function satisfies eδ 1 + δ. When A(x, y) 0, we apply the first-order Taylor expansion in Eq. 7, which yields: ˆR(x, y) = ˆBπθ (yx) ˆBπold (yx)A(x, y) (cid:17) (cid:16) 1 nt j=1 dj (cid:80)nt A(x, y) = 1 + dj A(x, y) 1 nt nt(cid:88) j=1 = nt(cid:88) j=1 (1 + dj)A(x, y) nt , (9) where dj = ℓπθ (yt(j), t(j), yx) ℓπθold (yt(j), t(j), yx). (10) Lemma 2. [Jensens Inequality] For convex function and finite set {xi}n i=1, we have (cid:88) (cid:88) (cid:32) (cid:33) xi (xi). (11) 1 1 i=1 i=1 When A(x, y) < 0, by applying Jensens Inequality in Eq. 7, we have: ˆR(x, y) = (cid:16) 1 nt (cid:80)nt j=1 dj (cid:17) A(x, y) 1 nt nt(cid:88) j=1 edj A(x, y) Putting everything together and letting: gj = (1+dj )A(x,y) nt , edj A(x,y) nt , if A(x, y) 0; if A(x, y) < 0, (13) ˆRlb(x, y) is constructed as linear sum of gj: ˆRlb(x, y) = nt(cid:88) j=1 gj. (14) As shown in Algorithm 1, the linearity of ˆRlb(x, y) (as well as ˆJlb(θ)) enables us to separate the gradient backpropagations for each yt(j), thus keeping the memory usage constant and allowing larger sample size nt. 3.2 Proof of Equivalence In on-policy training where πθ = πθold, the value of ℓπθ is equal to ℓπθold , which means the value of dj is always 0. By applying this in Eq. 7 and 14, we can find the values of ˆRlb(x, y) and ˆR(x, y) are both equal to A(x, y). Moreover, the gradient of ˆRlb(x, y) is also the same as that of ˆR(x, y) when dj = 0. Specifically, by applying the chain rule of the derivative, we have: θ ˆR(x, y) = θ (cid:18) (cid:16) 1 nt (cid:17) (cid:80)nt j=1 dj (cid:19) A(x, y) (cid:16)(cid:80)nt j=1 = dj nt (cid:17) A(x, y)θ (cid:16)(cid:80)nt j=1 dj (cid:17) nt dj =0 = nt(cid:88) j= A(x, y)θdj nt . (15) Similarly, when A(x, y) 0, we have: θ ˆRlb(x, y) = θ (cid:32) nt(cid:88) i=1 (cid:33) (1 + dj)A(x, y) nt = nt(cid:88) j= A(x, y)θdj nt , (16) and when A(x, y) < 0, we have: θ ˆRlb(x, y) = θ (cid:32) nt(cid:88) (cid:33) edj A(x, y) nt i=1 A(x, y)edj θdj nt = nt(cid:88) i= = nt(cid:88) j=1 edj A(x, y) nt . (12) 4 dj =0 = nt(cid:88) j=1 A(x, y)θdj nt . (17) i=1 πθold (x(b)) and compute advantages {A(x, y(i))}G i=1 using Eq. 18 Sample response {y(i)}G for = 1 to do Sample nt timestamp {t(j)}nt for = 1 to nt do Update the old policy πθold πθ and sample batch Db from for each prompt Db do Algorithm 1 BGPO Input: dataset D; initial policy model πθ; hyperparameters: G, nt, η. 1: for iteration = 1, 2, . . . , do 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for Output: πθ Sample partially masked response y(i) Compute gj using Eq. 13 and let Lj gj Backpropagate the gradient of Lj end for Update the policy θ θ ηθ j=1 [0, 1] end for end for t(j) q(t(j), y(i), x) ( graident accumulation) Therefore, ˆRlb(x, y) and ˆR(x, y) (as well as ˆJlb(θ) and ˆJ (θ)) are equivalent in terms of both value and gradient in on-policy training. This means like ˆJ (θ), ˆJlb(θ) is also an effective approximation of (θ), and using large sample size nt can reduce the bias and various of ˆJlb(θ) and its gradient, leading to better model performance. 3.3 Final Loss of BGPO In practice, we adopt group-based advantage estimation. Specifically, for each prompt x, we sample responses y(1), . . . , y(G) from πθold(x). Let r(x, y(i)) denotes the reward of y(i). The advantage of y(i) is defined as: A(x, y(i)) = r(x,y(i))mean({r(x,y(j))}G std({r(x,y(j))}G j=1) j=1) . (18) Accordingly, the loss for BGPO is formulated as: LBGPO = {y(i)}G xD, i=1πθold (x) (cid:34) 1 G (cid:88) i=1 (cid:35) ˆRlb(x, y(i)) . (19) Finally, we summarize our BGPO algorithm in Algorithm 1."
        },
        {
            "title": "4 Experiment",
            "content": "In this section, we empirically validate the efficacy of BGPO through extensive RL experiments. 4.1 Setup Models. We employ LLaDA-8B-Instruct (Nie et al., 2025b), state-of-art dLLM that has undergone pre-training and supervised fine-tuning, as our initial policy model. 5 Datasets. We conduct RL experiments in three domains: math problem solving, code generation, and planning tasks (Ye et al., 2025). For math problem solving, we train the model on mix of the training splits of MATH (Hendrycks et al., 2021) and GSM8K (Cobbe et al., 2021), and evaluate on the respective test sets. For code generation, we use 16K medium-difficulty problems filtered from DeepCoder (Luo et al., 2025) as the training set, and adopt MBPP (Austin et al., 2021b) and HumanEval (Chen et al., 2021) as the test sets. For planning tasks, we train and evaluate on Countdown (Pan et al., 2025) and Sudoku (Arel, 2025), adopting the same training and test splits as d1 (Zhao et al., 2025a). Implementation Details. We implement BGPO based on the VeRL (Sheng et al., 2025) framework. The maximum response lengths for math problem solving, coding generation, and planning tasks are set to be 512, 512, and 256, respectively. The batch size, rollout group size G, and learning rate are set to 16, 8, and 5 107, respectively. The MC sample size nt is set to 32 for Sudoku and 16 for other tasks. See Table 4 for more detailed hyperparameters. Following Zhao et al. (2025a), we evaluate the trained models (including baselines) every 20 steps and report results from the best-performing checkpoint. All experiments are conducted on 8 H800 GPUs. Baselines. We mainly compare BGPO with two representative RL algorithms for dLLMs that are introduced in Section 2: (1) diffu-GRPO (Zhao et al., 2025a), which approximates the log-likelihoods with single-pass mean-field estimation; (2) VRPOOL, which is the online version of VRPO (Zhu Model Mathematics Coding Planning MATH500 GSM8K HumanEval MBPP Sudoku Countdown Prior works with LLaDA d1-LLaDA (Zhao et al., 2025a) wd1 (Tang et al., 2025) LLaDA-IGPO (Zhao et al., 2025b) LLaDA-1.5 (Zhu et al., 2025) RL from LLaDA-8B-Instruct LLaDA-8B-Instruct (Nie et al., 2025b) + diffu-GRPO (Zhao et al., 2025a) + VRPO-OL (Zhu et al., 2025) + BGPO (ours) 40.2 39.0 42.8 42.6 39.6 43.1 44.1 45. 82.1 82.3 83.6 83.3 79.3 82.1 83.3 84.3 - - - 45.0* 45.1 47.0 44.8 47.6 - - - 40.0* 39.1 40.3 41.5 41. 16.7 25.2 - - 12.0 26.7 26.1 26.9 32.0 46.1 - - 19.5 53.1 84.8 87.5 Table 1: Performance comparison between BGPO and different baselines on mathematics, coding, and planning tasks. \"*\" indicates we re-evaluate the model using the same code environment. Figure 2: Training reward dynamics of BGPO, diffu-GRPO, and VRPO-OL across different tasks. et al., 2025) that adopts ELBO-based likelihood approximation and uses the objective in Eq 6. We set the MC sampling sizes of VRPO-OL to the maximum that H800 can support, i.e., nt = 4 for math and planning tasks and nt = 2 for code generation, since the prompts of coding tasks are longer. Besides, we also present the results from several prior works as references, including d1 (Zhao et al., 2025a), wd1 (Tang et al., 2025), LLaDAIGPO (Zhao et al., 2025b), and LLaDA 1.5 (Zhu et al., 2025), though their training setting are partially different from ours. 4.2 Main Results Table 1 presents the performance of BGPO and different baselines on math problem solving, code generation, and planning tasks. As shown in the table, our BGPO algorithm achieves significant improvement over LLaDA-8B-Instruct, and also outperforms previous RL algorithms (i.e., diffuGRPO and VRPO-OL) on all tasks, indicating that BGPO can produce more accurate approximation of the RL objective compared to these baselines. Specifically, BGPO improves the performance of LLaDA-8B-Instruct by about 5.5% and 2.5% on mathematical and coding tasks, respectively, and dramatically improves the performance on Sudoku and Countdown by 14.9% and 68.0%. Moreover, the model trained with BGPO also outperforms all previous LLaDA-based models, such as d1 and LLaDA-1.5, achieving state-of-the-art results. In addition, Figure 2 shows the reward dynamics of BGPO, diffu-GRPO, and VRPO-OL during the training on different tasks. The reward of BGPO is higher than the other two baselines in most steps. Particularly, BGPO exhibits notably faster reward increase and significantly higher reward on the Countdown task, where the exploration space is relatively simple. These phenomena demonstrate that the larger MC sample size of BGPO brings more accurate optimization direction, which aims to maximize the expectation of rewards. 4.3 Effect of Increasing MC Sample Sizes To demonstrate the effect of increasing the MC sample size nt in approximating the RL objective, we train LLaDA-8B-Instruct on math problem solving using BGPO with different nt. As shown in 6 Model MATH500 GSM8K LLaDA-8B-Instruct + BGPO (nt = 1) + BGPO (nt = 2) + BGPO (nt = 4) + BGPO (nt = 8) + BGPO (nt = 16) 39.6 43.5 44.1 43.7 45.3 45. 79.3 83.5 82.5 82.7 83.9 84.3 Table 2: Performance of BGPO with different Monte Carlo sampling size nt on mathematics benchmarks. Figure 3: Standard deviation (std) of gradients of different RL algorithms with different MC sampling size nt. The std is normalized by the absolute value of each parameter to unify the scale. Figure 4: Gradient bias (normalized by the absolute value of parameter) with different MC sampling size nt. Table 2, the model performance consistently improves as nt increases from 1 to 16, implying that larger MC sample sizes can produce more approximations of the RL objective. To further illustrate this, we compare the standard deviation (i.e., root of variance) and bias of the loss gradients of different RL algorithms with dif2. Specifically, we compute the gradient ferent nt of batch 8 times with different MC sampling sizes, and then calculate the standard deviation for each parameter, normalized by the absolute value of the parameter to unify the scale. For the bias calculation, we use the gradient of BGPO with nt = 256 to simulate the golden gradient, and also normalize the bias of parameters by their absolute value. As shown in Figure 3 and 4, the gradient variance and bias of diffu-GRPO are quite large, since it adopts single-pass estimation and also partially masks the prompt. In contrast, the gradient variance and bias of VRPO-OL and our BGPO gradually decrease as the MC sampling size nt increases, and BGPO can even obtain smaller variance and bias by using larger nt, since the memory overhead of BGPO remains constant, regardless of nt. This enables BGPO to have more accurate optimization direction and more stable training, resulting in better model performance. 4.4 Out-of-domain Performance To evaluate the out-of-domain generalization capability of BGPO, we train the models on math and coding tasks, respectively, and evaluate them on other tasks. As presented in Table 3, the model trained on math tasks improve the performance on the planning tasks, and the model trained on coding tasks achieves improvement on both math and planning tasks, demonstrating the good generalizability of BGPO. 4.5 Training Speed Comparison potential concern for BGPO is that the large MC sample size may substantially increase the running time of each RL step, affecting the training efficiency. To allay this concern, we compare the averaged training step time of BGPO and baseline algorithms on math problem solving, with the maximum response length to be 512. As shown in Figure 5, even BGPO adopts much larger MC sample size (i.e., 4 of VRPO-OL), its averaged training step time only slightly increased. This is because the dominant time cost of each step lies in the response rollout phase (i.e., sampling responses for each prompt) rather than the objective computation and policy updating phases."
        },
        {
            "title": "5 Related Work",
            "content": "5.1 Diffusion Large Language Models 2We do not directly compare the variance and bias of the loss since the value of loss is always 0 in on-policy training. Diffusion large language models (dLLMs), which generate text through masked diffusion (Austin 7 Model Mathematics Coding Planning MATH500 GSM8K HumanEval MBPP Sudoku Countdown LLaDA-8B-Instruct +BGPO (train on math tasks) +BGPO (train on coding tasks) 39.6 45.7 40.8 79.3 84.3 80. 45.1 44.2 47.6 39.1 38.6 41.7 6.3 8.6 9.2 14.5 21.1 21.5 Table 3: Out-of-domain performance of BGPO. The in-domain results are in gray. the log-likelihoods of dLLMs through single-pass mean-field estimation. Following wd1 (Tang et al., 2025; Zhao et al., 2025b) and IGPO (Zhao et al., 2025b) also adopt this approximation approach. Though efficient, the single-pass estimation introduces notable bias relative to the exact likelihoods. Alternatively, VRPO (Zhu et al., 2025) in LLaDA 1.5 approximates the log-likelihoods by their ELBOs, which is estimated via Monte Carlo (MC) sampling. Theoretically, this method can produce highly accurate approximations by using large MC sample size. However, the practical sample size used in training is severely constrained by the GPU memory limit, since the computational graphs of all samples need to be retained for the gradient calculation of the non-linear function in the RL objective. While our BGPO algorithm addresses this memory-inefficiency limitation and supports large MC sample sizes, thereby effectively reducing the bias and variance of approximations and achieving better performance."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose BGPO, memoryefficient RL algorithm for dLLMs that supports large Monte Carlo sample size for approximating the sequence-level log-likelihoods and the final objective, thereby effectively reducing the bias and variance of approximations and leading to better model performance. We theoretically prove the equivalence of our BGPO objective and the previous ELBO-based objective, and conduct extensive experiments to validate the efficacy of BGPO. We hope that our work lays solid foundation for future research on RL of dLLMs."
        },
        {
            "title": "7 Limitations",
            "content": "In this work, we only conduct experiments on 8Blevel models, since there are no larger open-source dLLMs, and our computational resources are also limited. Nonetheless, we believe our BGPO algorithm can be well applied to larger dLLMs due to Figure 5: Training speed comparison between baselines. et al., 2021a; Sahoo et al., 2024; Shi et al., 2024; Ou et al., 2025; Nie et al., 2025a), have recently achieved significant advances, demonstrating performance comparable to similarly-sized autoregressive models. Among existing open-source dLLMs, DiffuLLaMA (Gong et al., 2025a), Dream (Ye et al., 2025), and SDAR (Cheng et al., 2025) are adapted from pre-trained autoregressive LLMs, while LLaDA (Nie et al., 2025b) is trained from scratch using bidirectional attention by maximizing the ELBOs of log-likelihoods, presenting complete process of pre-training and supervised fine-tuning of dLLMs. Moreover, several commercial dLLMs like Mercury (Inception Labs et al., 2025), Gemini Diffusion (DeepMind, 2025), and Seed Diffusion (Song et al., 2025) not only achieve leading performance in code generation but also offer significantly faster inference, demonstrating the practical viability of dLLMs and their promising alternative to autoregressive LLMs. 5.2 Reinforcement Learning for dLLMs Applying RL to dLLMs presents unique challenges compared to autoregressive models. The iterative, non-sequential generation process of dLLMs makes their likelihood functions intractable, necessitating the approximation of log-likelihoods for policy optimization. For instance, d1 (Zhao et al., 2025a) proposed diffu-GRPO, which approximates its solid theoretical foundation."
        },
        {
            "title": "8 Ethical Considerations",
            "content": "All the models and datasets used in this work are publicly published with permissible licenses."
        },
        {
            "title": "References",
            "content": "Arel. 2025. Arels sudoku generator. https://www. ocf.berkeley.edu/arel/sudoku/main.html. Accessed: 2025-09-23. Jacob Austin, Daniel D. Johnson, Jonathan Ho, Daniel Tarlow, and Rianne van den Berg. 2021a. Structured denoising diffusion models in discrete state-spaces. In Advances in Neural Information Processing Systems 34: Annual Conference on Neural Information Processing Systems 2021, NeurIPS 2021, December 6-14, 2021, virtual, pages 1798117993. Jacob Austin, Augustus Odena, Maxwell I. Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. 2021b. Program synthesis with large language models. CoRR, abs/2108.07732. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and 39 others. 2021. Evaluating large language models trained on code. CoRR, abs/2107.03374. Shuang Cheng, Yihan Bian, Dawei Liu, Yuhua Jiang, Yihao Liu, Linfeng Zhang, Wenghai Wang, Qipeng Guo, Kai Chen, Biqing Qi*, and Bowen Zhou. 2025. Sdar: synergistic diffusionautoregression paradigm for scalable sequence generation. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training verifiers to solve math word problems. CoRR, abs/2110.14168. DeepMind. 2025. Gemini diffusion. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and 81 others. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. CoRR, abs/2501.12948. Shansan Gong, Shivam Agarwal, Yizhe Zhang, Jiacheng Ye, Lin Zheng, Mukai Li, Chenxin An, Peilin Zhao, Wei Bi, Jiawei Han, Hao Peng, and Lingpeng Kong. 2025a. Scaling diffusion language models via adaptation from autoregressive models. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Shansan Gong, Ruixiang Zhang, Huangjie Zheng, Jiatao Gu, Navdeep Jaitly, Lingpeng Kong, and Yizhe Zhang. 2025b. Diffucoder: Understanding and improving masked diffusion models for code generation. CoRR, abs/2506.20639. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. 2021. Measuring mathematical problem solving with the MATH dataset. In Proceedings of the Neural Information Processing Systems Track on Datasets and Benchmarks 1, NeurIPS Datasets and Benchmarks 2021, December 2021, virtual. Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 6-12, 2020, virtual. Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, Stefano Ermon, Aditya Grover, and Volodymyr Kuleshov. 2025. Mercury: Ultra-fast language models based on diffusion. Michael Luo, Sijun Tan, Roy Huang, Ameen Patel, Alpay Ariyak, Qingyang Wu, Xiaoxiang Shi, Rachel Xin, Colin Cai, Maurice Weber, Ce Zhang, Li Erran Li, Raluca Ada Popa, and Ion Stoica. 2025. Deepcoder: fully open-source 14b coder at o3-mini level. https://pretty-radio-b75.notion.site /DeepCoder-A-Fully-Open-Source-14B-Coder -at-O3-mini-Level-1cf81902c14680b3bee5eb3 49a512a51. Notion Blog. Shen Nie, Fengqi Zhu, Chao Du, Tianyu Pang, Qian Liu, Guangtao Zeng, Min Lin, and Chongxuan Li. 2025a. Scaling up masked diffusion models on text. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. Shen Nie, Fengqi Zhu, Zebin You, Xiaolu Zhang, Jingyang Ou, Jun Hu, Jun Zhou, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. 2025b. Large language diffusion models. CoRR, abs/2502.09992. OpenAI. 2024. Learning to reason with llms. https: //openai.com/index/learning-to-reason-wit h-llms/. Accessed: 2025-05-07. Jingyang Ou, Shen Nie, Kaiwen Xue, Fengqi Zhu, Jiacheng Sun, Zhenguo Li, and Chongxuan Li. 2025. Your absorbing discrete diffusion secretly models the conditional distributions of clean data. In The Thirteenth International Conference on Learning Representations, ICLR 2025, Singapore, April 24-28, 2025. OpenReview.net. 9 Jiayi Pan, Junjie Zhang, Xingyao Wang, Lifan Yuan, Hao Peng, and Alane Suhr. 2025. Tinyzero. https://github.com/Jiayi-Pan/TinyZero. Accessed: 2025-01-24. Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, and Lingpeng Kong. 2025. Dream 7b: Diffusion large language models. CoRR, abs/2508.15487. Siyan Zhao, Devaansh Gupta, Qinqing Zheng, and Aditya Grover. 2025a. d1: Scaling reasoning in diffusion large language models via reinforcement learning. CoRR, abs/2504.12216. Siyan Zhao, Mengchen Liu, Jing Huang, Miao Liu, Chenyu Wang, Bo Liu, Yuandong Tian, Guan Pang, Sean Bell, Aditya Grover, and 1 others. 2025b. Inpainting-guided policy optimization for diffusion large language models. arXiv preprint arXiv:2509.10396. Fengqi Zhu, Rongzhen Wang, Shen Nie, Xiaolu Zhang, Chunwei Wu, Jun Hu, Jun Zhou, Jianfei Chen, Yankai Lin, Ji-Rong Wen, and Chongxuan Li. 2025. Llada 1.5: Variance-reduced preference optimization for large language diffusion models. CoRR, abs/2505.19223. Subham S. Sahoo, Marianne Arriola, Yair Schiff, Aaron Gokaslan, Edgar Marroquin, Justin T. Chiu, Alexander Rush, and Volodymyr Kuleshov. 2024. Simple and effective masked diffusion language models. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. 2017. Proximal policy optimization algorithms. CoRR, abs/1707.06347. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. CoRR, abs/2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. 2025. Hybridflow: flexible and efficient RLHF framework. In Proceedings of the Twentieth European Conference on Computer Systems, EuroSys 2025, Rotterdam, The Netherlands, 30 March 2025 - 3 April 2025, pages 12791297. ACM. Jiaxin Shi, Kehang Han, Zhe Wang, Arnaud Doucet, and Michalis K. Titsias. 2024. Simplified and generalized masked diffusion for discrete data. In Advances in Neural Information Processing Systems 38: Annual Conference on Neural Information Processing Systems 2024, NeurIPS 2024, Vancouver, BC, Canada, December 10 - 15, 2024. Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. 2021. Score-based generative modeling through stochastic differential equations. In 9th International Conference on Learning Representations, ICLR 2021, Virtual Event, Austria, May 3-7, 2021. OpenReview.net. Yuxuan Song, Zheng Zhang, Cheng Luo, Pengyang Gao, Fan Xia, Hao Luo, Zheng Li, Yuehang Yang, Hongli Yu, Xingwei Qu, Yuwei Fu, Jing Su, Ge Zhang, Wenhao Huang, Mingxuan Wang, Lin Yan, Xiaoying Jia, Jingjing Liu, Wei-Ying Ma, and 3 others. 2025. Seed diffusion: large-scale diffusion language model with high-speed inference. CoRR, abs/2508.02193. Xiaohang Tang, Rares Dolga, Sangwoong Yoon, and Ilija Bogunovic. 2025. wd1: Weighted policy optimization for reasoning in diffusion language models. CoRR, abs/2507.08838. Chengyue Wu, Hao Zhang, Shuchen Xue, Zhijian Liu, Shizhe Diao, Ligeng Zhu, Ping Luo, Song Han, and Enze Xie. 2025. Fast-dllm: Training-free acceleration of diffusion LLM by enabling KV cache and parallel decoding. CoRR, abs/2505.22618. 10 Task Response length Diffusion step Block size MC sample size nt diffu-GRPO VRPO-OL BGPO Mathematics Coding Sudoku Countdown 512 / 512* 512 / 512* 256 / 256* 256 / 256* 256 / 512* 512 / 512* 128 / 256* 128 / 256* 32 / 32* 32 / 32* 32 / 32* 32 / 32* 1 1 1 1 4 2 4 4 16 16 32 16 Table 4: Detailed hyperparameters for different tasks. \"*\" denotes the different hyperparameters used in evaluation."
        },
        {
            "title": "A Detailed hyperparameters",
            "content": "We present detailed hyperparameters of BGPO on different tasks in Table 4. Following previous works, we adopt block-wise decoding strategy in both training and evaluation. The choices of response length, diffusion step, and block size also follow Zhu et al. (2025) and Zhao et al. (2025a) for obtaining the best performance."
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}