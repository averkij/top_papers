{
    "paper_title": "Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting",
    "authors": [
        "Yoonwoo Jeong",
        "Cheng Sun",
        "Frank Wang",
        "Minsu Cho",
        "Jaesung Choe"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses a significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), a novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into a generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in a generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate ~43.7x speedup on 512-D feature maps. Code will be made publicly available."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 2 ] . [ 1 7 2 9 0 2 . 2 1 5 2 : r 2025-12Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting Yoonwoo Jeong1,2,, Cheng Sun1, Frank Wang1, Minsu Cho2, Jaesung Choe1 1NVIDIA, 2POSTECH"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in computer vision have successfully extended Open-vocabulary segmentation (OVS) to the 3D domain by leveraging 3D Gaussian Splatting (3D-GS). Despite this progress, efficiently rendering the high-dimensional features required for open-vocabulary queries poses significant challenge. Existing methods employ codebooks or feature compression, causing information loss, thereby degrading segmentation quality. To address this limitation, we introduce Quantile Rendering (Q-Render), novel rendering strategy for 3D Gaussians that efficiently handles high-dimensional features while maintaining high fidelity. Unlike conventional volume rendering, which densely samples all 3D Gaussians intersecting each ray, Q-Render sparsely samples only those with dominant influence along the ray. By integrating Q-Render into generalizable 3D neural network, we also propose Gaussian Splatting Network (GS-Net), which predicts Gaussian features in generalizable manner. Extensive experiments on ScanNet and LeRF demonstrate that our framework outperforms state-of-the-art methods, while enabling real-time rendering with an approximate 43.7 speedup on 512-D feature maps. Code will be made publicly available. 1. Introduction 3D Gaussian Splatting (3D-GS) [Kerbl et al., 2023] has emerged as powerful representation for the neural rendering task, offering an explicit set of 3D Gaussians combined with efficient splatting [Zwicker et al., 2001] and tile-based rasterization for real-time rendering. This pipeline delivers both high-quality reconstruction and real-time frame ratescapabilities that have quickly made it foundation for numerous 3D vision and graphics applications. Beyond photorealistic rendering, recent works [Jun-Seong et al., 2025; Lyu et al., 2024; Qin et al., 2024; Wu et al., 2024; Ye et al., 2024] have begun leveraging 3D Gaussians as medium for scene understanding. prevalent approach is to distill knowledge from powerful 2D foundation modelssuch as CLIP [Ilharco et al., 2021; Radford et al., 2021], SAM [Kirillov et al., 2023; Ravi et al., 2024], and DINO [Caron et al., 2021; Oquab et al., 2023; SimÃ©oni et al., 2025]into pre-optimized 3D Gaussians by embedding highdimensional features. In this work, we focus on open-vocabulary segmentation (OVS) using OpenCLIP [Ilharco et al., 2021], which outputs 512-dimensional language-aligned features for arbitrary text or image queries. The original volume rendering algorithm [Kerbl et al., 2023] samples all Gaussians intersecting ray, regardless of their actual contribution to the output, i.e. rendered pixel color. For RGB rendering this overhead is manageable, but for high-dimensional embeddings (e.g., 512-D CLIP features) it becomes computationally heavy. To resolve this problem, series of studies compress the dimensionality of 512-D CLIP features into 3-D or 6-D features or codebooks [Jun-Seong et al., 2025; Qin et al., 2024; Wu et al., 2024; Zhou et al., 2024]. While effective, this strategy is not fundamental solution and can potentially loss the original information that was stored in the high-dimensional features. Moreover, the distribution of the optimized 3D Gaussians potentially have noisy or local minima due to its per-scene optimization scheme. Accordingly, it is challenging to properly embed high-dimensional feature vectors on the top of these 3D Gaussians. Based on this observation, we hypothesize that not all Gaussians are influentialonly partial fraction of 3D Gaussians meaningfully affect the high-dimensional feature rendering along ray. This observation Work has been done during an internship at NVIDIA Corresponding author & Project lead: jchoe@nvidia.com 2025 NVIDIA. All rights reserved. Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting Figure 1: Quantile rendering. (a) Unlike volume rendering [Kerbl et al., 2023; Mildenhall et al., 2021] that densely samples and blends all 3D Gaussians along the rays, (b) our Quantile render selectively samples and blends sparse set of quantile Gaussians those with dominant influence along the ray, which can efficiently render high-dimensional feature maps from Gaussian features. motivates Quantile Rendering (Q-Render), our transmittance-aware and efficient rendering algorithm for high-dimensional Gaussian features. Instead of densely accumulating every rasterized 3D Gaussian, Q-Render adaptively selects small set of quantile Gaussiansthose that dominate the rays transmittance profileand renders only these representatives. This quantile-based selection cuts redundant computation, and approximates the original signals for downstream tasks that may require to render high-dimensional feature maps. In this work, integrating Q-Render into 3D neural network [Chen et al., 2024; Choy et al., 2019; Wu et al., 2024], we build Gaussian Splatting Network (GS-Net) that operates on 3D Gaussians to predict Gaussian features. Typically, Q-Render serves as an efficient bridge between 2D supervision and the 3D neural network, allowing backward gradients [Rumelhart et al., 1986] to flow from image-space losses to the 3D neural networks predictions. Moreover, with this integration, Q-Renders sparse sampling becomes even more advantageous: the inductive bias of the 3D neural network tends to predict spatially smooth Gaussian features, meaning that densely sampling all Gaussians along each ray is unnecessary. Instead, the sparsely selected quantile Gaussians are sufficient to faithfully render high-dimensional feature maps while significantly reducing computational overhead during the rendering process and its backward computation. We validate our method on two open-vocabulary 3D semantic segmentation benchmarks: (1) ScanNet and (2) LeRF-OVS, where CLIP-based embeddings are stored directly in 3D Gaussians. In both cases, Q-Render achieves state-of-the-art results, underscoring its value as scalable bridge between 2D foundation models and 3D Gaussian representations. Quantile Rendering sparse, transmittance-guided sampling strategy that selects only the most representative Gaussians along each ray for efficiency. Gaussian Splatting Network 3D neural network that predicts high-dimensional featuremetric Gaussians from optimized 3D Gaussians, with Q-Render enabling efficient and effective feature distillation from 2D foundation models. Extensive Validation comprehensive experiments on open-vocabulary 3D semantic segmentation benchmarks with superior performances against recent studies. 2 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting 2. Related works High-dimensional Features on Gaussian representation. There is growing body of this field that adopts 3D Gaussian Splatting [Kerbl et al., 2023] as the underlying representation, incorporating these language-aligned embeddings into per-Gaussian memory [Jun-Seong et al., 2025; Qin et al., 2024; Shi et al., 2024; Wu et al., 2024; Zhou et al., 2024]. The core idea is to utilize rendering pipeline of vanilla 3DGS [Kerbl et al., 2023] to sample 3D Gaussians for distillation from 2D image embeddings, capability that point clouds inherently lack. Nonetheless, their memory-based approaches force per-scene feature optimization, which limits their application in practice. Moreover, lifting high-dimensional embeddings (e.g., CLIP 512-D feature) into 3D Gaussians demands significant computational resources. Neural scene representation networks. Recent research has explored treating neural scene representations as 3D models, using neural networks to process them and solve various 3D tasks. PeRFception [Jeong et al., 2022] trains networks to process Plenoxels [Fridovich-Keil et al., 2022] for classification and semantic segmentation. SPARF [Hamdi et al., 2023] improves Plenoxel in few-shot setup by training networks that process few-shot Plenoxels and output the enhanced version. SplatFormer [Chen et al., 2024] adopts Point Transformer V3 [Wu et al., 2024] to improve the robustness of 3D-GS under out-of-distribution poses. To the best of our knowledge, we are the first to address language and grouping tasks using networks that process 3D-GS. 3. Preliminary 3D Gaussian Splatting. 3D-GS [Kerbl et al., 2023] represents 3D scenes as set of 3D Gaussians where each 3D Gaussian is parameterized as center position ğœ‡ R3, covariance matrix Î£ R33, opacity ğ›¼ R, and spherical harmonics coefficients with ğ‘‘ degree sph R(ğ‘‘+1)23. In particular, the covariance matrix of the anisotropic Gaussian , rotation in quaternion representation R4, where distribution is decomposed into scaling factors R3 Î£ = RSSR. Given set of ğ‘ numbers of 3D Gaussians ğ’¢ = {gğ‘–}ğ‘ , 3D-GS [Kerbl et al., 2023] organizes the rendering pipeline as: (1) tiling strategy; (2) splatting algorithm [Zwicker et al., 2001]; and (3) volume rendering [Mildenhall et al., 2021]. In details, volume rendering proceeds with alpha-blending through densely rasterized 3D Gaussians along ray ğ‘ as: ğ‘–=1 = {ğœ‡ğ‘–, sğ‘–, rğ‘–, ğ›¼ğ‘–, sphğ‘–}ğ‘ ğ‘–=1 >0 C[ğ‘ ] = ğ‘–ğ‘†ğ‘Ÿ ğ‘¤ğ‘–cğ‘– s.t. ğ‘¤ğ‘– = ğ‘‡ğ‘–ğ›¼ ğ‘–, ğ‘‡ğ‘– = (1 ğ›¼ ğ‘—), ğ›¼ ğ‘– = ğ›¼ğ‘– Gğœ‡ğ‘–,Î£ğ‘–(ğ‘¢) (1) ğ‘—ğ‘†ğ‘Ÿ,:ğ‘–1 where is rendered image, C[ğ‘ ] is rendered pixel color at the ray ğ‘ , cğ‘– is the emitted color of the ğ‘–-th , ğ‘†ğ‘Ÿ is an ordered sequence of indices of densely sampled Gaussians along ğ‘ , ğ‘‡ğ‘– Gaussians obtained from sphğ‘– is the transmittance at ğ‘–-th Gaussian along ğ‘ , and G() is the Gaussian function evaluated at pixel location ğ‘¢ by projecting the Gaussian parameters. For training, 3D-GS optimizes parameters ğ’¢ by minimizing rendering loss â„’rgb = C1 + ğœ†SSIM SSIM(C, C) where is ground truth image and ğœ†SSIM is the hyperparameter adjusting the SSIM loss. High-dimensional Gaussian features. Recent studies [Lyu et al., 2024; Qin et al., 2024; Wu et al., 2024; Ye et al., 2024; Zhou et al., 2024] propose to register high-dimensional features for each 3D Gaussian g. These embedded features store 3D scene information such as language attributes, mask identities, etc. These methods commonly start from optimized 3D Gaussians ğ’¢ following the original 3D-GS paper [Kerbl et al., 2023]. Then, while freezing ğ’¢, this method 3 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting Figure 2: Overview. Given optimized 3D Gaussians ğ’¢, our network is trained to predict Gaussian features â„± that are aligned with the language embedding space from CLIPs vision encoder. Typically, the proposed Q-Render accelerates the training and inference speed by transforming predicted Gaussian features into rendered feature maps. allocates ğ¶-dimensional feature vector Rğ¶ for every 3D Gaussian such that these feature vectors are optimized to minimize feature rendering losses. 4. Methodology , 3D neural network As shown in Figure 2, given set of ğ‘ numbers of optimized 3D Gaussians ğ’¢ = {gğ‘–}ğ‘ ğ‘–=1 predicts set of ğ¶-dimensional Gaussian features â„± = {fğ‘–}ğ‘ where Rğ¶. Through our Quantile Rendering, the Gaussian features are rendered into ğ¶-dimensional feature maps. The 3D neural network is trained to minimize the rendering loss between the rendered feature maps and the target 2D feature maps extracted from CLIPs vision encoder. ğ‘–=1 4.1. 3D neural network Conventional 3D neural networks for pointcloud [Choe et al., 2021; Choy et al., 2019; Ding et al., 2023; Lee et al., 2025; Wu et al., 2024; Yang et al., 2024] often voxelize 3D points to efficiently process scene-scale 3D points. They transform scattered points into sparse voxel grids with unique spatial locations, enabling single-pass inference over an entire 3D scene thanks to its efficiency. In contrast to handling pointclouds, modeling scene with continuous 3D Gaussians [Kerbl et al., 2023] introduces overlapping regions since each Gaussian has volumetric extent. To resolve this issue, we follow SplatFormer [Chen et al., 2024] that introduces voxelization on 3D Gaussians by sampling its center location ğœ‡ to ensure compatibility with typical 3D backbones: Point Transformer v3 (PTv3) [Wu et al., 2024] and MinkUnet [Choy et al., 2019]. These models are designed to predict voxel features such that we proceed with the de-voxelization steps to convert predicted voxel features into predicted Gaussian features â„±, which will be used for rendering procedures. 4.2. Quantile rendering To train the 3D neural network using the knowledge from 2D foundation models, rendering process is necessary. Volume rendering [Kerbl et al., 2023; Mildenhall et al., 2021] can be an option, but it may require large computation power [Qin et al., 2024; Wu et al., 2024; Zhou et al., 2024] when rendering high-dimensional Gaussian features. This is because volume rendering repeatedly accumulates high-dimensional Gaussian features â„± along the ray, which quickly becomes prohibitively expensive as dimensionality grows as described in Table 1. To address this, we introduce quantile-based sampling strategy. Quantilesstatistical cutpoints that divide distribution into intervals of equal probabilityenable sparse yet representative sampling. By approximating the distribution in traditional volume rendering with carefully selected subset of Gaussians, our Quantile Rendering achieves efficient rendering while preserving representativeness. Specifically, given 4 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting Algorithm 1 Quantile Rendering Require: Optimized 3D Gaussians ğ’¢, predicted Gaussian features â„±, the number of Quantile Gaussians ğ¾, indices of rasterized 3D Gaussians ğ¼ at target ray. Ensure: Rendered feature vector at the target ray ğ‘„ Rğ¶. 1: procedure QuantileRender(ğ’¢, â„±, ğ¾, ğ¼) ğ‘‡ 1, ğ‘‡ ğ‘„ 1, ğ‘„ 0, ğ‘˜ 0 2: for ğ‘– in ğ¼ do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: gğ‘– ğ’¢[ğ‘–], fğ‘– â„±[ğ‘–] ğ‘‡test ğ‘‡ (1 ğ›¼ ğ‘–) if ğ‘‡test < 1 ğ‘˜+1 ğ¾+1 then ğ‘˜ ğ‘˜ + 1 ğ‘¤ğ‘„ ğ‘‡ ğ‘„ ğ›¼ ğ‘– ğ‘„ ğ‘„ + ğ‘¤ğ‘„ fğ‘– ğ‘‡ ğ‘„ ğ‘‡ ğ‘„ (1 ğ›¼ ğ‘–) while ğ‘‡test < 1 ğ‘˜+1 ğ¾+1 do ğ‘˜ ğ‘˜ + 1 end while end if if ğ‘‡test < 1 break ğ¾+1 then end if ğ‘‡ ğ‘‡test end for ğ‘„ ğ‘„ 1ğ‘‡ ğ‘„ return ğ‘„ 22: end procedure gğ‘– = {ğœ‡ğ‘–, sğ‘–, rğ‘–, ğ›¼ğ‘–, sphğ‘–} Sampling Quantile Gaussian Alpha-blending on Quantile Gaussians Normalizing feature vector hyperparameter ğ¾, the algorithm samples ğ¾ Quantile Gaussians along each ray. The detailed procedure is presented in Algorithm 1. We first rasterize and splat 3D Gaussians following [Kerbl et al., 2023] to obtain the indices of rasterized 3D Gaussians ğ¼ at the target ray ğ‘ as described in Section 3. Then, our Q-Render takes place to run three sub-steps: Quantile Gaussian sampling, alpha-blending, and feature normalization. Sampling Quantile Gaussian. Given hyperparameter ğ¾, we partition the transmittance ğ‘‡ R[0,1] into ğ¾+1 evenly distributed segments at each ray, and track how much the transmittance changes by passing through each Gaussian. Once crossing segment boundary( line 6 of Algorithm 1), we determine that this Gaussian as Quantile in this interval. Alpha-blending on Quantile Gaussians. With these Quantile Gaussians that show the relatively big transmittance change, we participate such Gaussians in alpha-blending. As summarized in Table 1, this selective blending reduces the time complexity from ğ’ª(ğ‘ ğ¶) (volume rendering) to ğ’ª(ğ‘ + ğ¾ğ¶), where ğ‘ is the number of Gaussians for each pixel and ğ¶ is the feature dimension. Unlike top-ğ¾ sampling proposed by concurrent study [Jun-Seong et al., 2025], which requires additional complexity to sort values, ğ’ª(ğ‘ log ğ¾ + ğ¾ğ¶). Normalizing feature vector. In the volume rendering [Kerbl et al., 2023; Mildenhall et al., 2021], transmittance ğ‘‡ is initialized as one and then the remaining transmittance ğ‘‡ closes to zero as proceeding with the alpha blending along ray. However, alpha-blending on sub-sampled Gaussians may have remaining transmittance that has relatively higher than 0. To approximate the original distribution in volume rendering that returns zero-close final transmittance value, we forcefully set the final transmittance ğ‘‡ ğ‘„ as zero by normalizing the accumulated feature ğ‘„ as ğ‘„ ğ‘„ 1ğ‘‡ ğ‘„ . 5 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting Table 1: Complexity comparison table. ğ¾ intervals along each ray, ğ‘ is the number of Gaussians per pixel and ğ¶ is the feature dimension. It shows the efficiency of our Q-Render, which avoids any multiplier for the large ğ‘ . Precise inference time is in Figure 6. V-Render [Kerbl et al., 2023] top-ğ¾ [Jun-Seong et al., 2025] Q-Render (ours)"
        },
        {
            "title": "Complexity",
            "content": "ğ’ª(ğ‘ ğ¶) ğ’ª(ğ‘ log ğ¾ + ğ¾ğ¶) ğ’ª(ğ‘ + ğ¾ğ¶) Figure 3: Comparison of transmittance distribution across different Gaussians sampling algorithms. Our Q-Render effectively approximates the distribution of the transmittance distribution of the original 3D-GS. We used ğ¾ = 10 for visualization. Finally, our Quantile rendering has been formulated as ğ‘„ = QuantileRender(ğ’¢, â„±, ğ¾, ğ¼) where ğ¼ is the indices of rasterized 3D Gaussians. As illustrated in Figure 3, Q-Render well approximates the transmittance tendency from the volume rendering [Kerbl et al., 2023] while top-ğ¾ sampling strategy [Jun-Seong et al., 2025] shows different the tendency. In short, our method avoids the overhead and outperforms top-ğ¾ in both efficiency and accuracy as stated in Table 6. Furthermore, the inductive bias of the 3D neural network promotes spatially smooth Gaussian feature predictions. Accordingly rendering dense sampling along rays becomes redundant. Our sparse quantile selection remains sufficient for high-fidelity feature mapping while significantly reducing computational overhead during both rendering and backward passes. Such an approximation is bounded by ğ‘‚(1/ğ¾) where we provide detailed theoretical justification of Q-Render as Riemann sum approximation in Appendix C. 4.3. Training loss We validate our GS-Net in open-vocabulary 3D semantic segmentation task with two datasets, ScanNet dataset and LeRF-OVS dataset. In this task, CLIP [Ilharco et al., 2021; Radford et al., 2021] is one of the popular vision language models that predicts 512-D feature vector from given input image. Following the previous studies [Jun-Seong et al., 2025; Qin et al., 2024; Wu et al., 2024], we set the distillation target as CLIP vision encoders feature vectors. We first extract image patches, a.k.a. masks {m}, using Grounded-SAM2 [Ren et al., 2024], and extract CLIP embeddings from the corresponding patch CLIP. Given pair of training data }, the 3D neural network is trained to minimize discrepancy between our rendered feature vector ğ‘„ {mğ‘–, CLIP ğ‘– and CLIP embedding CLIP in the format of the contrastive loss â„’ as follows: â„’ = log exp(sim(f ğ‘„, CLIP )) ğ‘–=ğ‘— exp(sim(f ğ‘„, CLIP ğ‘— ğ‘– where sim(, ) is the cosine similarity function. , )) (2) Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting Table 2: Open vocabulary 3D semantic segmentation performances in the ScanNet dataset. Method Per-scene optim. 19 classes 15 classes mIoU () mAcc () mIoU () mAcc () mIoU () mAcc () 10 classes LangSplat [Qin et al., 2024] OpenGaussian [Wu et al., 2024] Dr.Splat [Jun-Seong et al., 2025] GS-Mink (ours) GS-PTv3 (ours) 1.47 22.60 23. 50.75 48.99 10.23 34.41 35.42 62.00 60.36 2.00 24.21 25. 53.54 52.39 11.85 37.58 34.64 66.39 66.05 3.24 34.74 36. 64.95 62.57 16.79 51.58 53.29 79.34 77.70 Figure 4: Qualitative results on the open-vocabulary 3D semantic segmentation task. 5. Experiments We provide experiments to verify the efficacy of our GS-Net on open-vocabulary 3D semantic segmentation task using the ScanNet dataset and LeRF-OVS dataset. Given set of open-vocabulary text queries, we extract their text features using CLIPs text encoder. For fair comparison, we used the sam CLIP model provided by the recent work [Jun-Seong et al., 2025]. We then compute the cosine similarity between Gaussian features and these text features, assigning each Gaussian the labels with the highest similarity scores. Following the evaluation protocol of OpenGaussian [Kerbl et al., 2023], we assess mIoU and mAcc on predefined sets of 19, 15, and 10 categories by theirs. In contrast to OpenGaussian, which freezes the original pointclouds and disables densification and pruning, we enable both densification and pruning to fully leverage the capacity of 3D-GS. As result, per-Gaussian labels are required for evaluation that is extracted from pointcloud labels. To assign semantic labels to each Gaussian, we use the Mahalanobis distance to find the ğ¾ nearest neighbors, similar to Dr.Splat [Jun-Seong et al., 2025]. Furthermore, we found that filtering points based on opacity and Gaussian influence leads to more reliable labels, as reported in section B.2. For fair comparison, we reproduce the results of LangSplat [Qin et al., 2024], OpenGaussian [Wu et al., 2024], and Dr.Splat [Jun-Seong et al., 2025] using our training and evaluation setup. 5.1. ScanNet dataset We compare GS-Net against baselines [Qin et al., 2024; Wu et al., 2024] on the ScanNetv2 [Dai et al., 2017] dataset, which includes 1,513 scenes and 124,505 frames of indoor captures featuring various household 7 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting Table 3: Open-vocabulary semantic segmentation performances in the LeRF-OVS dataset."
        },
        {
            "title": "Method",
            "content": "Feature dim. mIoU () mAcc () LangSplat [Qin et al., 2024] LEGaussians [Shi et al., 2024] OpenGaussian [Wu et al., 2024] SuperGSeg [Liang et al., 2024] GS-Mink (ours) GS-Mink (ours) 3 8 6 64 6 512 9.7 16.2 38.4 35. 38.6 45.8 12.4 23.8 51.4 52.0 52.3 56.9 Figure 5: Our qualitative results in the LeRF-OVS dataset. objects. When training our generalized network, we use the same 10 validation scenes as OpenGaussian [Wu et al., 2024] and train on the remaining 1,503 scenes. Additionally, we train GS-Net to overfit single scene to ensure fair comparison with the baselines. We pre-optimize all 3D-GS [Kerbl et al., 2023] using the original implementation across all scenes to train and evaluate GS-Net. In Table 2, we compare two GS-Net models, GS-Mink and GS-PTv3 where each model has MinkUnet [Choy et al., 2019] and PTv3 [Wu et al., 2024] as baseline 3D neural networks, against previous baselines [Jun-Seong et al., 2025; Qin et al., 2024; Wu et al., 2024]. Although the evaluation scenes are not used for training GS-Net, GS-Net achieves significant performance improvements or comparable results to previous baselines. Furthermore, when overfitted to single scene, GS-Mink and GS-PTv3 improve mIoU by 12.08%ğ‘ and 12.73%ğ‘, respectively. Interestingly, while GS-PTv3 outperforms GS-Mink in the overfitting scenario, this trend reverses when training for generalization. We found that GS-PTv3 is more prone to overfitting during training, suggesting that architectural improvements for handling Gaussians could mitigate this issue. As shown in Figure 4, our GS-Net produces much clearer semantic segmentation results, further demonstrating the superiority of our method. 5.2. LeRF-OVS dataset. We compare GS-Mink with previous baselines on LeRF-OVS dataset [Kerr et al., 2023; Qin et al., 2024] where the scenes has sampled for the open vocabulary semantic segmentation task such ramen, teatime, kitchen, etc. we maintain to use the same ground truth masks and corresponding text captions for the fair comparison. We use GS-Mink as our baselines and train the model with this dataset. As demonstrated in Table 3, our method outperforms recent studies while having 6-D compressed Gaussian embeddings and 512-D Gaussian embeddings as the same dimensionality as CLIPs image embeddings. For visualization, we also provide our qualitative results in Figure 5. 8 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting 3D neural network Feature renderer ğ¾ 19 classes mIoU () V-Render [Kerbl et al., 2023] top-ğ¾ [Jun-Seong et al., 2025] MinkUNet Q-Render (ours) - 5 10 20 40 50 5 10 20 40 50 49.02 37.84 40.22 43.59 45.70 44. 49.98 50.75 50.65 50.85 50.28 Figure 6: Comparison of (a) FPS and (b) mIoU by varying the number of Gaussians participating on rendering. Our Q-Render allows real-time rendering while preserving the mIoU performance. For the fair speed comparison, while using our implementation with the same rasterized Gaussians and 512-D Gaussian features, we only change the sampling strategies among the rasterized Gaussians: V-Render [Kerbl et al., 2023; Mildenhall et al., 2021] uses all intersecting Gaussians, top-ğ¾ [Jun-Seong et al., 2025] sorts and selects ğ¾ Gaussians, and ours collect ğ¾ Quantile Gaussians. 5.3. Control experiments Feature renderer. We conduct ablation study by replacing our Q-Render of our GS-Net with other feature rendering algorithms, such as volume render [Kerbl et al., 2023], and the top-ğ¾ render [Jun-Seong et al., 2025]. The comparison results are described in Figure 6 in terms of rendering speed and accuracy in open-vocabulary semantic segmentation. In general, Q-Render achieves superior or comparable performance compared to other feature rendering algorithms. In terms of rendering speed, our Q-Render demonstrate up to 1.5 faster speed in comparison with volume rendering (V-Render). Moreover, top-ğ¾ rendering shows remarkable speed drops as ğ¾ increases. Such phenomenon is related to our complexity analysis in Table 1. For the detailed ablation study about ğ¾, the number of quantile Gaussians, we achieve the best performance when setting ğ¾ = 40. Nonetheless, the performance becomes converged as ğ¾ 10, and reaches 50mIoU. Furthermore, Table 1 shows that top-ğ¾ render has relatively big performance drop when setting smaller ğ¾. We deduce that this is because of the discrepancy in the transmittance profile as we visualized in Figure 3. In another analysis, our Q-render with ğ¾ = 40 outperforms the performance of the volume render. Theoretically speaking, our Q-render is designed to approximate the original transmittance profile by the volume render as stated in section 4.2. Though we do not have concrete experimental supports, we guess that this is related to the potential noise in the optimized 3D Gaussians ğ’¢ which have some difficulties in representing the precise geometry information due to the limited training images [Zhu et al., 2024] or the 3D Gaussian representation itself [Huang et al., 2024]. Grid size. Throughout the paper, we used grid size of 10.0cm for both GS-Mink and GS-PTv3. We also conducted ablation studies by varying the grid size to 10.0, 5.0, 2.0, 1.0, 0.50, and 0.25 cm. The results of open vocabulary semantic segmentation experiments with different grid sizes are presented in Table 4. We observed that reducing the grid size to 5.0cm shows the best performance. While the performance started to dramatically drop after setting grid size smaller than 1.0 cm. We deduce this phenomenon from the model architecture 9 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting Table 4: Ablation study for the grid size. Note that the scene scale is aligned metric-unit as described in section of the appendix. So the grid size is determined as below. 3D neural network Feature renderer Grid Size (cm) 19 classes 15 classes mIoU () mAcc () mIoU () mAcc () mIoU () mAcc () 10 classes GS-Mink GS-PTv3 Q-Render (K=40) 10.0 5.0 2.0 1.0 0.5 0.25 10.0 5.0 2.0 47. 50.39 50.28 45.00 41.12 34.36 43.71 48.64 48.99 58.46 62.14 61.58 55.41 49.72 42.42 55.96 59.42 60.36 49. 53.14 53.00 48.27 44.40 37.34 49.94 51.35 52.39 62.85 66.39 65.98 60.99 55.32 47.47 63.09 64.83 66.05 59. 63.37 63.29 60.29 57.05 49.03 59.60 62.57 62.57 75.33 78.30 78.34 75.45 71.46 61.78 74.79 78.18 77.70 Table 5: Rendering speed on ScanNet scene0006_00 (frame 0). Note that 512 is implemented by for-loop iterations, leveraging the original baseline code to render 512 dimension feature map. LangSplat OpenGaussian GS-Mink (ours) Feature dim. 3 FPS () 112.12 6 - 512 0.65 - 6 512 3 6 71.13 0.83 172.52 80.98 28.42 (43.7) and voxelization strategy. While we follow the Gaussian voxelization scheme by [Chen et al., 2024], it only sample one voxel per Gaussian though each Gaussian has volumetric shapes. Moreover, the 3D neural network baselines have some receptive field extent. Once we reduce the grid size smaller than specific numbers, some voxels may not aggregated together due to the receptive field limitations. Based on this analysis, we believe that exploring model designs or voxelization type can become potential future research directions for the further improvements. In Table 5, we evaluate the inference speed of recent methods and our GS-Mink on Inference time. scene0000_00 from ScanNet. For fair comparison, we measure frames per second (FPS) using the same feature dimension settings as in LangSplat (3 dim) and OpenGaussian (6 dim), and utilize the full Gaussian scene. Our method achieves the highest rendering speed among the evaluated approaches. We modify the official implementation by [Qin et al., 2024] and [Wu et al., 2024] to render 512-D feature maps by for-loop iterations (we denoted these implementations as 512 in Table 5). It turns out that our Q-Render achieves upto 43.7 speed gains when rendering 512-D feature maps. Information loss after voxelization. We visualize the voxelization results in Figure 7. The voxelization itself potentially brings information loss, and changes the original 3D Gaussian parameters, such as opacity, spherical harmonics, etc. To estimate the amount of information loss, we conduct an experiment that renders images from (1) original Gaussians, or (2) voxelized & de-voxelized Gaussians. Figure 7 shows that PSNR drops from 19.89 to 15.19 when ğ›¿=0.10. Due to these reasons, we de-voxelize the estimated voxel features from the 3D neural network into Gaussian features. Then, we proceed with our Q-rendering. We used [Sun et al., 2025] to directly render images from sparse voxels without de-voxelization steps. 6. Conclusion The core contribution of this work is Quantile Rendering (Q-Render), transmittance-aware strategy that resolves the computational bottleneck of embedding high-dimensional features in 3D Gaussian Splatting. Unlike conventional rendering that densely accumulates all intersections, Q-Render adaptively samples only 10 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting Figure 7: Information loss after voxelization. It shows that rendered image from de-voxelized Gaussians achieved higher fidelity compared to rendered image directly from sparse voxels (w/o de-voxelization). Table 6: Performance change on the number of ğ¾. We use model trained with ğ¾ = 40 and use different numbers of ğ¾ during inference. ğ¾ mIoU () mAcc () 5 20 40 50 39.16 48.43 42.18 53.94 44.94 56. 45.81 56.87 45.71 56.94 Table 7: Comparison of GS-Net performance across different input 3D-GS models. GS-Net v2 utilizes 3D Gaussians pre-trained with an additional depth loss as input. Method Training 3D-GS 19 classes 15 classes 10 classes mIoU () mAcc () mIoU () mAcc () mIoU () mAcc () GS-Net v1 GS-Net v2 (Ours) Rendering + Depth Rendering 28. 50.75 38.85 62.00 31.02 53.54 44. 66.39 42.58 64.94 57.92 79.34 the influential quantile Gaussians that dominate rays transmittance profile. This sparse sampling drastically reduces computational overhead, enabling efficient rendering of full 512-dimensional feature maps with up to 43.7x speedup. Consequently, Q-Render achieves state-of-the-art performance on open-vocabulary segmentation benchmarks, establishing it as scalable bridge between 2D foundation models and 3D representations. Limitations and Future Work. Although Q-Render has shown efficient approximation of volumetric rendering, there still exists several limitations and failure cases. Limitation 1: Dynamic ğ¾ Selection. Theoretically, the optimal number of samples ğ¾ should vary depending on the distribution of transmittance along ray. However, in this work, we use fixed ğ¾ across all experiments for simplicity. To investigate the sensitivity of our model to ğ¾, we conducted an analysis where we trained the model with ğ¾ = 40 and performed inference with varying ğ¾ {5, 10, 20, 40, 50}. As shown in Table 6, the performance significantly drops when the inference ğ¾ differs from the training configuration. This underscores the necessity of an adaptive ğ¾ selection strategy. Although we explored two adaptive strategies in Appendix E.1, they incur high computational costs. Thus, we leave the development of efficient, adaptive sampling strategies for future work. Limitation 2: Dependence on 3D-GS. Our current framework assumes that input 3D Gaussians are obtained through per-scene optimization, which inherently limits practical scalability. However, emerging generalizable 3D-GS approaches that eliminate the need for per-scene optimization, such as DepthSplat [Xu et al., 2025], WorldMirror [Liu et al., 2025], and DepthAnything3 [Li et al., 2024] offer promising path to resolve this issue. Furthermore, we observe that the quality of the input 3D Gaussians significantly impacts downstream performance. As shown in Table 7, GS-Net v1, which utilizes the original 3D-GS optimized without depth supervision, yields suboptimal results. Conversely, GS-Net v2 takes as input more recent 3D-GS implementation trained with additional depth loss. This improvement leads to substantial performance gains, highlighting the 11 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting critical importance of geometric accuracy in the input representation. We belive advancements in 3D-GS will also involve the improvement of GS-Net. Limitation 3: Dependence on 3D Network Architecture. As reported in Appendix E.3, we observe that performance highly depends on the choice of backbone network. In particular, MinkowskiNet [Choy et al., 2019] and PTv3 [Wu et al., 2024] exhibit strong sensitivity to the voxel grid resolution, as shown in Table 4. This suggests that developing more efficient and Gaussian-aware architecture could further improve the performance of our GS-Net framework. Potential directions include introducing Gaussian-friendly operators, reducing or eliminating voxelization, and designing modules that are robust to noise in Gaussian parameters."
        },
        {
            "title": "Appendix",
            "content": "A. Resolving up-to-scale with monocular depth The scale of 3D scene is critical factor in 3D scene understanding. However, when camera poses are estimated solely from multi-view images, recovering the absolute scene scalereferred to as the up-to-scale problembecomes challenging. For instance, COLMAP [SchÃ¶nberger and Frahm, 2016; SchÃ¶nberger et al., 2016], widely used tool for extracting extrinsic and intrinsic camera parameters in neural rendering pipelines, does not inherently address this issue. Since our method relies on optimized Gaussian parameters Î˜, the up-to-scale problem naturally arises when handling diverse 3D scenes captured purely from images. To overcome this issue, we employ DepthAnythingV2 [Yang et al., 2025], an off-the-shelf monocular depth estimation method trained to produce metric-scale depth maps from single images. Similar to the depth alignment process in [Chung et al., 2024; Kerbl et al., 2024], we fix the original Gaussian parameters Î˜ and optimize the global scene scale ğ‘ to approximate the metric scale: arg min ğ‘ ğ‘â„ ğ‘–= invDmono ğ‘– ğ‘ invDrendered ğ‘– 1, (3) where ğ‘â„ is the number of known images, invDmono is ğ‘–-th inverse depth map from DepthAnythingV2 [Yang et al., 2025], and invDrendered is the corresponding rendered inverse depth map using 3DGS. After obtaining the scene scale ğ‘, we update the Gaussian parameter Î˜ as Î˜ = {ğœƒğ‘–}ğ‘ . Note that only the mean and the scale attributes of each Gaussian are changed. We provide an example result in Figure 8. ğ‘ sğ‘–, rğ‘–, ğ›¼ğ‘–, sphğ‘–}ğ‘ ğ‘–=1 = { 1 ğ‘ ğœ‡ğ‘–, 1 ğ‘–=1 ğ‘– ğ‘– Unlike the original 3DGS [Kerbl et al., 2023], which recently introduced scale-alignment method, we avoid pruning initial COLMAP points that could affect the distribution of our optimized Gaussian parameters Î˜. This ensures fair comparison with recent studies such as Gaga [Lyu et al., 2024]. Furthermore, we use metric depth estimator, depth-anything-v2-metric-hypersim-vitl.pth in the official repository, whereas 3DGS employs an inverse depth estimator, depth-anything-v2-vitl.pth. Instead of normalizing the estimated depth maps, we directly take their reciprocal to obtain inverse depth maps ğ·mono. B. Data preprocessing B.1. 3D Gaussian Splatting optimization We use the original implementation with the proposed hyperparameters, such as training iterations and upsampling frequency, for pre-optimizing 3D-GS. For the initial points, we follow the initialization strategies used by OpenGaussian [Wu et al., 2024] and Dr.Splat [Jun-Seong et al., 2025], utilizing the point clouds provided by the original ScanNetv2 dataset instead of using COLMAP [SchÃ¶nberger and Frahm, 2016; SchÃ¶nberger et al., 2016]. Note that we maintain the densification and pruning processes of 3D-GS to preserve its capacity. 12 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting Figure 8: Visualization of 3D Gaussians with different conditions: (left) 3D Gaussians that are trained from ground truth pointcloud, (middle) 3D Gaussians that are trained from COLMAP points, then are applied with scene scale ğ‘, (right) 3D Gaussians that are trained from COLMAP points without considering scene scale ğ‘. Figure 9: Generated Gaussian Splatting labels from point cloud labels. Applying both opacity and distance thresholding allows much clearer label generation for Gaussian Splatting. B.2. Pseudo ground truth acquisition Since we enable the densification and pruning processes of 3D-GS, the number and coordinates of each Gaussian differ from the original point clouds. To evaluate the predicted Gaussians, we need to create Gaussian-specific labels derived from the original point labels. As shown in Figure 9, for each Gaussian, we first explore the ğ¾ nearest neighbor points and assign the label based on the most frequent label among these neighbors. However, we observed that due to the large number of Gaussians, many Gaussians contribute minimally to the scene (i.e., their opacity is below 0.1), and we filtered out these less significant Gaussians. Despite this, we found that some isolated Gaussians (i.e., \"floaters\") still received labels, resulting in noisy ground truth (GT) labels. To address this, we ignored labels if all the nearest points had Mahalanobis distance above 0.1, effectively removing such noisy assignments. As result, we were able to obtain clearer and more consistent Gaussian labels. C. Theoretical Analysis of Quantile Rendering Based on the definitions provided in Section 4.2 and Algorithm 1 of the manuscript, we provide the theoretical analysis of Quantile Rendering (Q-Render) as an approximation of the volume rendering in 3D-GS [Kerbl et al., 2023] using Riemann sums. 13 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting C.1. Preliminaries: Riemann Sums and the Right Rule Let ğ‘“ : [ğ‘, ğ‘] be bounded function defined on closed interval [ğ‘, ğ‘]. We define partition ğ‘ƒ of the interval into ğ‘ sub-intervals: ğ‘ = ğ‘¢0 < ğ‘¢1 < ğ‘¢2 < < ğ‘¢ğ‘ = ğ‘ The width of the ğ‘˜-th sub-interval [ğ‘¢ğ‘˜1, ğ‘¢ğ‘˜] is denoted by ğ‘¢ğ‘˜ = ğ‘¢ğ‘˜ ğ‘¢ğ‘˜1. Riemann Sum approximates the definite integral of ğ‘“ over [ğ‘, ğ‘] by summing the areas of rectangles: ğ‘† = ğ‘ ğ‘˜=1 ğ‘“ (ğ‘¢* ğ‘˜)ğ‘¢ğ‘˜ (4) where ğ‘¢* ğ‘˜ is sample point chosen from the sub-interval [ğ‘¢ğ‘˜1, ğ‘¢ğ‘˜]. The Right Riemann Sum specifically chooses the right endpoint of each sub-interval as the sample point: Thus, the approximation becomes: ğ‘¢* ğ‘˜ = ğ‘¢ğ‘˜. ğ‘†ğ‘Ÿğ‘–ğ‘”â„ğ‘¡ = ğ‘ ğ‘˜=1 ğ‘“ (ğ‘¢ğ‘˜)ğ‘¢ğ‘˜ (5) (6) When the partition is uniform, ğ‘¥ = (ğ‘ ğ‘)/ğ‘ , due to the Right endpoint approximation theorem, the Right Rule converges linearly: ğ‘ ğ‘ ğ‘“ (ğ‘¥) ğ‘‘ğ‘¥ ğ‘†right ğ‘€ (ğ‘ ğ‘)2 2ğ‘ , where ğ‘€ is the maximum value of the absolute value of ğ‘“ (ğ‘¥) ğ‘¥ over the interval. C.2. Volume Rendering as an Integral We observe that the discrete volume rendering process is fundamentally equivalent to Riemann sum approximation of continuous path integral. Theoretically, volume rendering assumes that the accumulated transmittance ğ‘‡ monotonically decreases from 1 to 0 along the ray, creating bounded integration domain. Different neural rendering approaches can be characterized by how they partition this domain: NeRF [Mildenhall et al., 2021]: The integral is approximated in the spatial domain (ğ‘¡). Query points are sampled uniformly or hierarchically (coarse-to-fine) along the ray. The Riemann partition is defined by the distance between adjacent samples (ğ›¿ğ‘– = ğ‘¡ğ‘–+1 ğ‘¡ğ‘–), where the opacity ğ›¼ğ‘– is derived from the learned volumetric density ğœ. 3D-GS [Kerbl et al., 2023]: The integral is evaluated using set of discrete primitives sorted by depth. Here, the rasterized Gaussians act as the representative samples for intervals along the ray. The rendering equation accumulates the alpha-blended contribution of each intersecting Gaussian, effectively performing Riemann summation where the step size and weight are determined by the Gaussians covariance and opacity. Quantile Rendering (Ours): Unlike the spatial sampling in NeRF or the dense accumulation in 3D-GS, our method shifts the Riemann partition to the transmittance domain (ğ‘‡ ). By sampling intervals of equal probability in transmittance space, we ensure that the summation is performed only over the most significant contributors, offering more efficient approximation of the underlying integral. 14 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting C.3. Volume Rendering as Continuous Integral Consider camera ray parameterized by distance ğ‘¡ [0, ). Let ğœ(ğ‘¡) denote the volume density and ğ‘(ğ‘¡) the feature (or color) emitted at position ğ‘¡. The transmittance is defined as ğ‘‡ (ğ‘¡) = exp ( ğ‘¡ 0 ) ğœ(ğ‘ ) ğ‘‘ğ‘  . Its derivative satisfies ğ‘‘ğ‘‡ ğ‘‘ğ‘¡ = ğœ(ğ‘¡)ğ‘‡ (ğ‘¡) = ğ‘‘ğ‘‡ ğ‘‘ğ‘¡ ğ‘‘ğ‘¡ = ğœ(ğ‘¡)ğ‘‡ (ğ‘¡) ğ‘‘ğ‘¡. The continuous volume rendering integral is ğ¶vol = 0 ğ‘(ğ‘¡) ğœ(ğ‘¡)ğ‘‡ (ğ‘¡) ğ‘‘ğ‘¡. (7) The Quantile render operates by analyzing the change in Transmittance ğ‘‡ rather than spatial distance ğ‘¡. We can reformulate the volume rendering integral Eq. 7 by performing change of variables from spatial distance ğ‘¡ to transmittance ğ‘¢ = ğ‘‡ (ğ‘¡). Following [Kerbl et al., 2023; Mildenhall et al., 2021], ğ‘¢ and ğ‘¡ are noted as: When ğ‘¡ = 0, ğ‘¢ = 1 (Ray starts with full transmittance starting from 1). When ğ‘¡ , ğ‘¢ 0 (Transmittance converges to 0). Since ğ‘‡ is strictly decreasing from 1 to 0, the substitution transforms Eq. 7 into ğ‘¢ = ğ‘‡ (ğ‘¡), ğ‘‘ğ‘¢ = ğœ(ğ‘¡)ğ‘‡ (ğ‘¡) ğ‘‘ğ‘¡ ğ¶vol = ğ‘¢=1 ğ‘(ğ‘¢) (ğ‘‘ğ‘¢) = 1 0 ğ‘(ğ‘¢) ğ‘‘ğ‘¢. Thus, volume rendering is exactly the integral of function ğ‘(ğ‘¢) over the transmittance interval [0, 1]: ğ¶vol = 1 0 ğ‘(ğ‘¢) ğ‘‘ğ‘¢. (8) C.4. Quantile Rendering as Right Riemann Sum As described in Section 4.2 of the manuscript, the Quantile Rendering algorithm partitions the Transmittance ğ‘‡ [0, 1] into ğ¾ + 1 evenly distributed segments. The algorithm selects specific Quantile Gaussian whenever the accumulated transmittance drops. In detail, the Quantile Rendering partitions the transmittance domain [0, 1] into 0 = ğ‘¢ğ¾+1 < ğ‘¢ğ¾ < < ğ‘¢1 < ğ‘¢0 = 1. For each interval [ğ‘¢ğ‘˜1, ğ‘¢ğ‘˜], Q-render selects the Gaussian whose transmittance crossing corresponds to the right endpoint ğ‘¢ğ‘˜. Thus the algorithm evaluates ğ‘(ğ‘¢) at ğ‘¢ğ‘˜. Therefore, Q-render implements exactly the Right Riemann Sum for Eq. 8: This replaces the dense per-Gaussian accumulation with sparse, quantile-driven sampling strategy. ğ¶ right ğ‘„ = ğ¾+1 ğ‘˜=1 ğ‘(ğ‘¢ğ‘˜) ğ‘¢. (9) Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting C.5. Approximation Error of Quantile Rendering Following [Kerbl et al., 2023; Mildenhall et al., 2021], the integrand ğ‘(ğ‘¢) is differentiable and satisfies ğ‘(ğ‘¢) ğ‘€ ğ‘¢ [0, 1]. Applying the classical Right Rule error bound with ğ‘ = 0, ğ‘ = 1, and ğ‘ = ğ¾ + 1 gives: Thus the quantile approximation converges at rate ğ‘‚(1/ğ¾). ğ¶vol ğ¶ right ğ‘„ ğ‘€ 2(ğ¾ + 1) ğ‘€ 2ğ¾ . C.6. Convergence of Q-render with Transmittance Normalization Q-render includes final normalization step to correct for residual transmittance: ğ¶ğ‘„ = ğ¶ right ğ‘„ 1 ğ‘‡ğ‘„ , where ğ‘‡ğ‘„ is the remaining transmittance after processing all ğ¾ quantile Gaussians. Since each quantile removes at least ğ‘¢, we have Hence the normalization factor satisfies ğ‘‡ğ‘„ ğ‘¢ = 1 ğ¾ + 1 . 1 1 ğ‘‡ğ‘„ 1 1 1 ğ¾+1 = ğ¾ + 1 ğ¾ . Combining with eq. 10 yields ğ¶vol ğ¶ğ‘„ ğ¾ + 1 ğ¾ ğ‘€ 2(ğ¾ + 1) = ğ‘€ 2ğ¾ . (10) (11) (12) (13) (14) Final Theorem. Under the assumptions that ğ‘ is differentiable and ğ‘(ğ‘¢) ğ‘€ [Kerbl et al., 2023; Mildenhall et al., 2021], Q-render converges to volume rendering with rate ğ¶vol ğ¶ğ‘„ ğ‘€ 2ğ¾ (15) and the approximation error vanishes linearly as ğ¾ . D. Implementation Details For optimization, we use the default optimizers provided by MinkUNet (Adam) and PTv3 (AdamW). The learning rate is adjusted using PyTorchs ReduceLROnPlateau scheduler, reducing by factor of 10 when plateau is detected. Training is conducted with batch size of 4 across 8 A100-80GB GPUs while computing rendering loss from 4 randomly chosen training viewpoints. Both MinkUNet and PTv3 follow their default configurations. 16 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting E. Additional Experiments E.1. Adaptive Gaussian Sampling Strategy We additionally implement two adaptive variants of Q-Render, namely Learned-K and Stratified-K, to assess whether adaptive selection of ğ¾ can effectively mitigate the sensitivity to this hyperparameter. Learned-K. In this variant, we train the model with three candidate values for ğ¾, specifically {10, 20, 40}. Inspired by the mIoU-prediction head in SAM2, we introduce an additional similarity-prediction head that estimates the expected similarity between the predicted features and the target features for each candidate ğ¾. This head takes the transmittance profile as input and outputs the predicted similarity scores. Therefore, similar to SAM2, we use an additional loss that computes the difference between predicted similarity and the actual similarity. â„’ğ‘ ğ‘–ğ‘š(f ğ‘„, CLIP ğ‘– , {ğ‘‡ğ‘›}ğ‘ ğ‘›=1) = Sim-HeadÎ˜({ğ‘‡ğ‘›}ğ‘ ğ‘›=1) sim(f ğ‘„, CLIP ğ‘– )2 2, (16) where {ğ‘‡ğ‘›}ğ‘ ğ‘›=1 the CLIP feature used in training. are transmittance values along the ray, ğ‘„ is rendered feature via Q-Render, and CLIP is ğ‘– In addition, we apply the original loss for three ğ¾s during training to optimize features for all ğ¾s. During inference, the model evaluates these similarity scores and dynamically selects the value of ğ¾ that yields the highest expected similarity. This allows the model to adaptively determine ğ¾ per input without relying on manually fixed value. Stratified-sampling. In the second variant, we compute the mean and standard deviation of the transmittance values along the depth dimension. Rather than uniformly partitioning the transmittance for sampling, we instead draw samples based on the z-score under Gaussian distribution parameterized by the estimated mean and standard deviation. This enables denser sampling at depths where objects are more likely to contribute, analogous to the stratified sampling strategy used in NeRF, and improves robustness under diverse transmittance distributions. Specifically, we uniformly partition the transmittance interval in the ğ‘§-score space and then map the samples back to the original transmittance domain. Consequently, unlike the original Q-Renderwhich uniformly partitions the transmittance valuesthis variant adapts the sampling locations according to the underlying transmittance distribution, encouraging the model to draw more Gaussians from regions where they are more densely concentrated. Results. We compare these two adaptive variants against the original Q-Render in terms of segmentation performance and rendering FPS. Due to the large memory footprint of the Learned-K model, we use voxel size of 0.5 for all experiments. For fair comparison, we set ğ¾ = 40 for both the stratified-K variant and the original Q-Render. As shown in Table 8, the Learned-K model slightly underperforms the other strategies, likely because the model must additionally learn the similarity-prediction head, which makes it harder to focus on improving feature quality. We also observe that the stratified-K variant performs on par with the original Q-Render. However, the most critical observation is that both adaptive variants achieve nearly half the FPS of Q-Render. This slowdown arises because they require two passes along each ray: one to estimate the transmittance statistics and another to perform rendering. Despite their increased computational cost, we do not observe any clear performance improvement over Q-Render. Therefore, we adopt Q-Render as our rendering algorithm of choice, as it provides an efficient and effective approximation of V-Render. 17 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting Table 8: Open vocabulary 3D semantic segmentation performances in the ScanNet dataset."
        },
        {
            "title": "FPS",
            "content": "19 classes 15 classes mIoU () mAcc () mIoU () mAcc () mIoU () mAcc () 10 classes Learned-K Stratified-K Q-Render (ours) 14.31 15.14 32. 40.14 41.30 41.12 49.16 49.52 49.72 43.85 44.67 44. 55.60 55.78 55.32 55.84 56.66 57.05 68.53 70.10 71. Table 9: Comparison between Dr.Splat and GS-Mink on MipNeRF360 benchmark. Scene bicycle garden treehill Dr.Splat GS-Mink mIoU mAcc mIoU mAcc 0.2112 0.5543 0.3012 0.6114 0. 0.2713 0.2236 0.6721 0.2063 0.3165 0.7813 0.2585 Avg. 0.3359 0. 0.3673 0.4521 E.2. Open Vocabulary Segmentation on Outdoor Scenes Since our evaluation benchmarks are all indoor datasets, we also demonstrate the effectiveness of our proposed method on outdoor benchmarks. However, to the best of our knowledge, there are currently no benchmarks that jointly cover multi-view and outdoor scenes for open-vocabulary segmentation (OVS). To enable comparison in outdoor settings, we manually annotated 3 scenes from MipNeRF360 [Barron et al., 2022] outdoor scenes. Specifically, we use SAM2 to obtain an initial mask corresponding to the queried object and then manually refine the segmentation by providing additional positive and negative point prompts. This process allows for reliable ground-truth masks for evaluating outdoor multi-view OVS performance across all baselines. We compare our framework with the state-of-the-art method, Dr.Splat [Jun-Seong et al., 2025], using the manually annotated MipNeRF360 benchmark. As reported in Table 9, Q-Render consistently outperforms Dr.Splat, corroborating our main findings and demonstrating robust generalization to outdoor scenarios. Qualitative comparisons are visualized in Figure 10. As observed in the first column, our method achieves clear separation between the target object and its surroundings. However, we acknowledge that VRAM constraints necessitated the use of larger voxel sizes. We leave the development of more memory-efficient network architectures for 3D-GS to future work. E.3. More network architecture To evaluate the generalizability of our approach across different backbones, we conducted additional experiments using two representative point-based architectures: PointNet++ [Qi et al., 2017] and PointNeXT [Qian et al., 2022]. PointNet++ utilizes hierarchical feature learning via local neighborhood grouping to capture fine-grained geometry, while PointNeXT advances this design with residual connections and improved scaling strategies. However, as reported in Table 10, both point-based models exhibit inferior performance compared to voxel-based baselines (MinkUNet and PTv3). We conjecture that this disparity stems from the handling of point density. Since 3D Gaussians are often densely clustered around object surfaces, the k-nearest neighbor (k-NN) search used in point-based networks tends to limit the metric receptive field, thereby hindering the aggregation of broader contextual information. In contrast, voxel-based networks are inherently more robust to such density variations, resulting in superior performance. 18 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting Figure 10: Our qualitative results in the MipNeRF360 dataset. Table 10: Comparison of various 3D network backbone to GS-Net. 3D arch. Rendering 19 classes 15 classes mIoU () mAcc () mIoU () mAcc () mIoU () mAcc () 10 classes PointNet++ [Qi et al., 2017] PointNeXT [Qian et al., 2022] PTv3 [Wu et al., 2024] MinkUNet [Choy et al., 2019] Q-render 39.42 37.89 48.99 50.75 50.62 45.80 60.36 62.00 42.88 41.15 52.39 53.54 56.66 52.09 66.05 66.39 52.21 51.57 64.95 64.95 69.14 63.83 79.34 79. E.4. Thorough Comparison with OpenGaussian To better understand which components of our method contribute most to the performance improvement, we conduct an ablation by modifying each part of the pipeline individually and comparing the results against OpenGaussian. Specifically, we introduce two hybrid models: one that uses the GS-Net feature extractor while retaining the original rendering module of OpenGaussian, and another that keeps the OpenGaussian feature extractor but replaces the renderer with Q-Render. As shown in Table 11, incorporating Q-Render yields consistent performance gain for both OpenGaussian and GS-Net. Moreover, replacing the OpenGaussian codebooks with GS-Net features leads to substantial improvement. We conjecture that preserving highdimensional features is key factor, as OpenGaussian encodes only 6-dimensional attributes, whereas GS-Net leverages 512-dimensional CLIP features that retain richer semantic information. E.5. Sensitivity under Gaussian Perturbation We also investigate the sensitivity of GS-Net to perturbations applied to the input 3D-GS representation. Specifically, on top of the predicted opacity, we inject Gaussian noise with varying scales into the Gaussian opacity values and evaluate the resulting segmentation performance. To avoid extreme cases where the opacity becomes negative values, we apply the noise before the sigmoid activation is applied. As shown in Table 12, GS-Net remains highly robust under mild noise levels: performance degradation at noise scales of 0.25 and 0.5 is negligible, demonstrating that Q-Render retains sufficient stability even when the opacity field is moderately corrupted. However, as the noise scale increases beyond 1.0, we observe marginal drop in both mIoU and mAcc, and the performance eventually collapses at extreme noise levels such as 4.0. These results indicate that while Q-Render is resilient to realistic perturbations in the opacity field, strong distortions that heavily corrupt 19 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting Table 11: Comparison of V-Render and Q-Render under different feature extractors."
        },
        {
            "title": "Feature Extractor Rendering",
            "content": "19 classes 15 classes mIoU () mAcc () mIoU () mAcc () mIoU () mAcc () 10 classes"
        },
        {
            "title": "OpenGaussian",
            "content": "GS-Net (Mink) V-Render Q-Render V-Render Q-Render 22.60 23.10 49.02 50.75 34.41 35. 58.75 62.00 24.21 26.18 50.41 53.54 37.58 38.12 63.65 66.39 34.74 36. 61.04 64.94 51.58 56.12 76.21 79.34 Table 12: Robustness of Q-Render to Gaussian noise added to opacity. Noise Scale 19 classes mIoU () mAcc () 0 0.25 0.5 1 2 4 50.75 50.13 50.18 47.13 38.13 16.12 62.00 60.94 60.88 57.10 44.11 32.19 Figure 11: Comparison between V-Render and Q-Render on RGB reconstruction using pre-trained 3D-GS model without fine-tuning. Q-Render shows only minor PSNR degradation, demonstrating that it serves as an efficient and accurate approximation of V-Render in the RGB domain. the underlying geometry inevitably degrade performance. Overall, this experiment suggests that our pipeline maintains robust performance under practical levels of noise in 3D-GS inputs. E.6. Rendering RGB Since our Q-Render is not restricted to CLIP features, it could also be applied to RGB image rendering. Using this setup, we quantify how much information is lost when replacing V-Render with Q-Render. To isolate the behavior of the renderer itself, we directly apply Q-Render to pre-trained 3D-GS models without any fine-tuning. As shown in Fig. 11, Q-Render exhibits only very slight drop in PSNR compared to V-Render, indicating that it provides an effective approximation of the original renderer even in the RGB domain. This suggests that Q-Render is broadly applicable beyond our semantic feature rendering setup. In particular, we believe Q-Render can be seamlessly integrated into various 3D-GS variantsincluding dynamic or time-varying Gaussianssince these models also assume fixed Gaussian positions at each rendering step. 20 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting Table 13: Peak inference memory on scene0000_00 (ScanNet, 100 frames). Method Feature Dim. Peak Memory (GB) LangSplat OpenGaussian Dr.Splat Q-Render (ğ¾=40) 3 6 512 512 7.18 16.13 61.13 27.18 E.7. Memory Footprint Comparison We also compare the VRAM requirements of each baseline on scene0000_00 from ScanNet (100 frames). As summarized in Table 13, LangSplat and OpenGaussian require little memory due to their low-dimensional feature representations (3D and 6D). However, this compression inevitably introduces information loss, which correlates with their reduced segmentation performance. Dr.Splat relies on 512D features and stores per-view Gaussian visibility masks, which causes memory usage to grow proportionally with the number of frames and leads to more than 61,GB of peak consumption. Q-Render also uses 512D features, and its cache-free per-ray accumulation avoids this overhead, reducing the peak memory to 27.18,GB at ğ¾=40 while preserving high-dimensional semantic capacity. F. Pseudo-code for Voxelization, Voxel Feature Composition, and De-voxelization To ensure reproducibility, we provide detailed pseudo-code of our implementation. Listing 1 outlines the voxelization pipeline that groups Gaussians into voxels for efficient processing. Listing 2 details the organization of voxel positions and features as inputs for the 3D network. Finally, Listing 3 describes the de-voxelization process, where voxel features are redistributed to predict Gaussian-level features. 21 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting from typing import List, Tuple from typing_extensions import Literal # Global variable indicating the shape to be sampled SAMPLE_SHAPE: Literal[\"volume\", \"tri-plane\", \"tri-line\", \"center\"] = \"center\" def voxelization(gaussians, grid_size): \"\"\" Convert set of 3D Gaussians into set of unique voxels and their associated features. This function does the following: 1. Counts how many voxels will be needed for each Gaussian based on the current SAMPLE_SHAPE (e.g., \"volume\", \"tri-plane\", etc.). 2. Generates voxel coordinates and features for all Gaussians. 3. Removes duplicates, resulting in unique set of voxel coordinates, their corresponding features, and an index map that indicates how each original voxel maps to the new unique list. Args: gaussians : List[dict] list of 3D Gaussian parameters. Each Gaussian is typically dictionary containing at least \"scale\" key (and possibly position or other attributes) needed by the downstream functions. grid_size : float hyperparameter defining the size of each voxel. Returns: uni_voxels_xyz : List[Tuple[float, float, float]] The (x, y, z) coordinates of the unique voxels. uni_voxel_feats : List The feature vectors corresponding to each unique voxel. inverse_indices : List[int] list of indices that maps each original voxel back to its corresponding index in the unique voxel list. num_voxels_per_g: List[int] list of number of voxels that are sampled from each 3D Gaussian. Notes: - The function relies on three helper functions which are assumed to be defined: 1) `count_voxels_per_g(gaussians, grid_size)` Determines the number of voxels for each Gaussian given the sampling shape. 2) `compose_voxel_features(gaussians, num_voxels_per_g)` Produces the (x, y, z) coordinates and any features for each voxel. 3) `return_unique_voxels(voxels_xyz, voxel_feats)` Removes duplicate voxel coordinates and returns list of unique coordinates, their combined features, and an inverse index array. - If you are implementing this in parallel computing environment (e.g., CUDA), each step may be adapted for batch or GPU-based processing. \"\"\" # 1. Determine how many voxels are required for each Gaussian. num_voxels_per_g = count_voxels_per_g(gaussians, grid_size) # 2. Generate voxel coordinates (xyz) and their associated feature vectors. # These features might include Gaussian-related attributes or other data. voxels_xyz, voxel_feats = compose_voxel_features(gaussians, num_voxels_per_g) # 3. Deduplicate the generated voxels: # # # - Get unique voxel coordinates, - Aggregate/merge features for any duplicates, - Obtain the inverse index mapping from original voxels to unique. uni_voxels_xyz, uni_voxel_feats, inverse_indices = return_unique_voxels(voxels_xyz, voxel_feats) return uni_voxels_xyz, uni_voxel_feats, inverse_indices, num_voxels_per_g Listing 1: Our voxelization process for 3D Gaussians. 22 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting from torch import Tensor def compose_voxel_features(gaussians, num_voxels_per_g, grid_size): \"\"\" Generate per-voxel feature vectors for set of 3D Gaussians. This function iterates through each Gaussian and computes: 1. The xyz location of each voxel in the Gaussian. 2. Mahalanobis distance-based influence (voxel opacity). 3. Additional attributes such as RGB. Args: gaussians : List[dict] list of Gaussian definitions num_voxels_per_g : List[int] or torch.Tensor Number of voxels associated with each Gaussian (e.g., from `count_voxels_per_g` or another precomputation). grid_size : float The size of each voxel cell, used to help determine voxel coordinates. Returns: voxel_features : List[torch.Tensor] list of per-voxel feature tensors having [voxel_xyz, voxel_rgb, voxel_opacity]. Notes: The function `get_voxel_location(...)` is assumed to provide the precise xyz coordinate of each voxel. Implementation details are omitted for readibility. \"\"\" voxel_features = [] voxel_xyzs = [] # Iterate over Gaussians for g_idx, gaussian in enumerate(gaussians): # Retrieve the number of voxels expected for this Gaussian num_voxels = num_voxels_per_g[g_idx] # Extract Gaussian parameters for clarity mu = gaussian[\"mean\"] # Center location of 3D Gaussian inv_cov = gaussian[\"inverse_3d_covariance_matrix\"] opacity_factor = gaussian[\"opacity\"] color_rgb = gaussian[\"rgb\"] # Generate features for each voxel in this Gaussian for voxel_idx in range(num_voxels): # 1. Get the 3D coordinate for the current voxel voxel_xyz = get_voxel_location(voxel_idx, gaussian, grid_size) voxel_xyzs.append(voxel_xyz) # 2. Compute the Mahalanobis distance, Eq. 1 of the main manuscript dist = (voxel_xyz - mu).T @ inv_cov @ (voxel_xyz - mu) # 3. Compute voxel opacity using Gaussian attenuation factor voxel_opacity = opacity_factor * torch.exp(-0.5 * dist) # 4. Concatenate features feature_vec = torch.cat([voxel_xyz, color_rgb, voxel_opacity], dim=1) voxel_features.append(feature_vec) return voxel_xyzs, voxel_features Listing 2: Pseudocode for the compose-voxel-features function. 23 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting from typing_extensions import Literal from torch_scatter import scatter, segment_csr # Global reduction mode (could also be passed as parameter) REDUCE: Literal[\"mean\", \"max\"] = \"max\" def de_voxelization(uni_voxels_pred, inverse_indices, num_voxels_per_g): \"\"\" Aggregate network predictions from unique voxels back to full voxel predictions, and then further reduce them into per Gaussian predictions. This function performs two main steps: 1. Unique Voxels -> All Voxels: Uses `scatter` to expand the predictions from the unique-voxel level back to the original list of all voxels, by indexing through `inverse_indices`. 2. All Voxels -> Gaussians: Uses `segment_csr` to combine per-voxel predictions for each Gaussian. This is done by summing the number of voxels per Gaussian in `num_voxels_per_g`, which creates offsets used by `segment_csr` to group voxel predictions into Gaussian predictions. Args: uni_voxels_pred : torch.Tensor tensor containing the predicted values (e.g., features or logits) for each unique voxel. Its shape could be (N_unique, C), where N_unique is the number of unique voxels and is the dimensionality of the prediction (e.g. channels). inverse_indices : torch.Tensor 1D tensor of indices mapping every original voxel to its corresponding index in the unique voxel array. num_voxels_per_g : torch.Tensor 1D tensor indicating how many voxels belong to each Gaussian. Returns: gs_pred : torch.Tensor tensor containing the final predictions at the Gaussian level, with shape (G, C). Notes: - If your pipeline needs different reduction mode (e.g. `\"sum\"`) or you want to pass it in as parameter, replace `REDUCE` accordingly. \"\"\" # 1. Expand predictions from unique voxels to all voxels using `scatter`. # # # # # - 'uni_voxels_pred' has shape (N_unique, C). - 'inverse_indices' has shape (M, ) with >= N_unique (the total number of voxels). - 'scatter' will produce new tensor of shape (M, C), where each entry is the value from the appropriate unique voxel. The method of combination (mean, max, etc.) is determined by `REDUCE`. voxels_pred = scatter(uni_voxels_pred, index=inverse_indices, reduce=REDUCE) # 2. Combine per-voxel predictions into Gaussian-level predictions using `segment_csr`. # # # - We first compute offsets with cumulative sums of the number of voxels in each Gaussian. - The 'segment_csr' function then takes the voxel predictions and segments them into groups corresponding to each Gaussian, reducing via the same `REDUCE` rule. offset = torch.cumsum(num_voxels_per_g, dim=0) gs_pred = segment_csr(voxels_pred, ptr=offset, reduce=REDUCE) return gs_pred Listing 3: Our de-voxelization process for 3D Gaussians. Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting"
        },
        {
            "title": "References",
            "content": "[1] Jonathan Barron, Ben Mildenhall, Dor Verbin, Pratul Srinivasan, and Peter Hedman. Mip-nerf 360: Unbounded anti-aliased neural radiance fields. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 54705479, 2022. 18 [2] Mathilde Caron, Hugo Touvron, Ishan Misra, HervÃ© JÃ©gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the International Conference on Computer Vision (ICCV), 2021. 1 [3] Yutong Chen, Marko Mihajlovic, Xiyi Chen, Yiming Wang, Sergey Prokudin, and Siyu Tang. Splatformer: Point transformer for robust 3d gaussian splatting. arXiv preprint arXiv:2411.06390, 2024. 2, 3, 4, 10 [4] Jaesung Choe, Byeongin Joung, Francois Rameau, Jaesik Park, and In So Kweon. Deep point cloud reconstruction. arXiv preprint arXiv:2111.11704, 2021. [5] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 30753084, 2019. 2, 4, 8, 12, 19 [6] Jaeyoung Chung, Jeongtaek Oh, and Kyoung Mu Lee. Depth-regularized optimization for 3d gaussian splatting in few-shot images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 811820, 2024. 12 [7] Angela Dai, Angel Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias NieÃŸner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 58285839, 2017. 7 [8] Runyu Ding, Jihan Yang, Chuhui Xue, Wenqing Zhang, Song Bai, and Xiaojuan Qi. Pla: Language-driven open-vocabulary 3d scene understanding. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 70107019, 2023. 4 [9] Sara Fridovich-Keil, Alex Yu, Matthew Tancik, Qinhong Chen, Benjamin Recht, and Angjoo Kanazawa. Plenoxels: Radiance fields without neural networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 55015510, 2022. 3 [10] Abdullah Hamdi, Bernard Ghanem, and Matthias NieÃŸsner. Sparf: Large-scale learning of 3d sparse radiance fields from few input images. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV) Workshops, pages 29302940, October 2023. [11] Binbin Huang, Zehao Yu, Anpei Chen, Andreas Geiger, and Shenghua Gao. 2d gaussian splatting for geometrically accurate radiance fields. In ACM SIGGRAPH 2024 conference papers, pages 111, 2024. 9 [12] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, et al. Openclip, 2021. 1, 6 [13] Yoonwoo Jeong, Seungjoo Shin, Junha Lee, Chris Choy, Anima Anandkumar, Minsu Cho, and Jaesik Park. Perfception: Perception using radiance fields. Advances in Neural Information Processing Systems, 35: 2610526121, 2022. 3 [14] Kim Jun-Seong, GeonU Kim, Kim Yu-Ji, Yu-Chiang Frank Wang, Jaesung Choe, and Tae-Hyun Oh. Dr. splat: Directly referring 3d gaussian splatting via direct language embedding registration. In CVPR, pages 1413714146, 2025. 1, 3, 5, 6, 7, 8, 9, 12, 18 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting [15] Bernhard Kerbl, Georgios Kopanas, Thomas LeimkÃ¼hler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Trans. Graph., 42(4):1391, 2023. 1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 13, 14, 15, 16 [16] Bernhard Kerbl, Andreas Meuleman, Georgios Kopanas, Michael Wimmer, Alexandre Lanvin, and George Drettakis. hierarchical 3d gaussian representation for real-time rendering of very large datasets. ACM Transactions on Graphics (TOG), 43(4):115, 2024. 12 [17] Justin Kerr, Chung Min Kim, Ken Goldberg, Angjoo Kanazawa, and Matthew Tancik. Lerf: Language embedded radiance fields. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1972919739, 2023. 8 [18] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 40154026, 2023. 1 [19] Junha Lee, Chunghyun Park, Jaesung Choe, Yu-Chiang Frank Wang, Jan Kautz, Minsu Cho, and Chris Choy. Mosaic3d: Foundation dataset and model for open-vocabulary 3d segmentation. arXiv preprint arXiv:2502.02548, 2025. [20] Mingye Li, Weicai Zhu, Yini Wang, Jing Wang, Chao Li, Siyu Wang, Kai Han, and Ruimao Zhang. Depth anything v3: Removing the gaps between zero-shot, single-dataset, and multi-dataset depth estimation. arXiv preprint arXiv:2406.05943, 2024. 11 [21] Siyun Liang, Sen Wang, Kunyi Li, Michael Niemeyer, Stefano Gasperini, Nassir Navab, and Federico Tombari. Supergseg: Open-vocabulary 3d segmentation with structured super-gaussians. arXiv preprint arXiv:2412.10231, 2024. 8 [22] Yifan Liu, Zhiyuan Min, Zhenwei Wang, Junta Wu, Tengfei Wang, Yixuan Yuan, Yawei Luo, and Chunchao Guo. Worldmirror: Universal 3d world reconstruction with any-prior prompting. arXiv preprint arXiv:2510.10726, 2025. URL https://arxiv.org/abs/2510.10726. 11 [23] Weijie Lyu, Xueting Li, Abhijit Kundu, Yi-Hsuan Tsai, and Ming-Hsuan Yang. Gaga: Group any gaussians via 3d-aware memory bank. arXiv preprint arXiv:2404.07977, 2024. 1, 3, 12 [24] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65(1):99106, 2021. 2, 3, 4, 5, 9, 14, 15, [25] Maxime Oquab, TimothÃ©e Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 1 [26] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. Advances in neural information processing systems, 30, 2017. 18, 19 [27] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Abed Al Kader Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. In Advances in Neural Information Processing Systems (NeurIPS) 35, 2022. 18, 19 26 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting [28] Minghan Qin, Wanhua Li, Jiawei Zhou, Haoqian Wang, and Hanspeter Pfister. Langsplat: 3d language gaussian splatting. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2005120060, 2024. 1, 3, 4, 6, 7, 8, [29] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 1, 6 [30] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman RÃ¤dle, Chloe Rolland, Laura Gustafson, et al. Sam 2: Segment anything in images and videos. arXiv preprint arXiv:2408.00714, 2024. 1 [31] Tianhe Ren, Shilong Liu, Ailing Zeng, Jing Lin, Kunchang Li, He Cao, Jiayu Chen, Xinyu Huang, Yukang Chen, Feng Yan, Zhaoyang Zeng, Hao Zhang, Feng Li, Jie Yang, Hongyang Li, Qing Jiang, and Lei Zhang. Grounded sam: Assembling open-world models for diverse visual tasks, 2024. 6 [32] David Rumelhart, Geoffrey Hinton, and Ronald Williams. Learning representations by backpropagating errors. nature, 323(6088):533536, 1986. 2 [33] Johannes Lutz SchÃ¶nberger and Jan-Michael Frahm. Structure-from-motion revisited. In Conference on Computer Vision and Pattern Recognition (CVPR), 2016. 12 [34] Johannes Lutz SchÃ¶nberger, Enliang Zheng, Marc Pollefeys, and Jan-Michael Frahm. Pixelwise view selection for unstructured multi-view stereo. In European Conference on Computer Vision (ECCV), 2016. 12 [35] Jin-Chuan Shi, Miao Wang, Hao-Bin Duan, and Shao-Hua Guan. Language embedded 3d gaussians for open-vocabulary scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 53335343, 2024. 3, 8 [36] Oriane SimÃ©oni, Huy V. Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, MichaÃ«l Ramamonjisoa, Francisco Massa, Daniel Haziza, Luca Wehrstedt, Jianyuan Wang, TimothÃ©e Darcet, ThÃ©o Moutakanni, Leonel Sentana, Claire Roberts, Andrea Vedaldi, Jamie Tolan, John Brandt, Camille Couprie, Julien Mairal, HervÃ© JÃ©gou, Patrick Labatut, and Piotr Bojanowski. DINOv3. August 2025. URL https://ai.meta.com/research/publications/dinov3. 1 [37] Cheng Sun, Jaesung Choe, Charles Loop, Wei-Chiu Ma, and Yu-Chiang Frank Wang. Sparse voxels rasterization: Real-time high-fidelity radiance field rendering. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1618716196, 2025. 10 [38] Xiaoyang Wu, Li Jiang, Peng-Shuai Wang, Zhijian Liu, Xihui Liu, Yu Qiao, Wanli Ouyang, Tong He, and Hengshuang Zhao. Point transformer v3: Simpler faster stronger. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 48404851, 2024. 2, 3, 4, 8, 12, [39] Yanmin Wu, Jiarui Meng, Haijie Li, Chenming Wu, Yahao Shi, Xinhua Cheng, Chen Zhao, Haocheng Feng, Errui Ding, Jingdong Wang, et al. Opengaussian: Towards point-level 3d gaussian-based open vocabulary understanding. arXiv preprint arXiv:2406.02058, 2024. 1, 3, 4, 6, 7, 8, 10, 12 [40] Haofei Xu, Songyou Peng, Fangjinhua Wang, Hermann Blum, Daniel Barath, Andreas Geiger, and Marc Pollefeys. Depthsplat: Connecting gaussian splatting and depth. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 11 27 Quantile Rendering: Efficiently Embedding High-dimensional Feature on 3D Gaussian Splatting [41] Jihan Yang, Runyu Ding, Weipeng Deng, Zhe Wang, and Xiaojuan Qi. Regionplc: Regional point-language contrastive learning for open-world 3d scene understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1982319832, 2024. 4 [42] Lihe Yang, Bingyi Kang, Zilong Huang, Zhen Zhao, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything v2. Advances in Neural Information Processing Systems, 37:2187521911, 2025. [43] Mingqiao Ye, Martin Danelljan, Fisher Yu, and Lei Ke. Gaussian grouping: Segment and edit anything in 3d scenes. In European Conference on Computer Vision, pages 162179. Springer, 2024. 1, 3 [44] Shijie Zhou, Haoran Chang, Sicheng Jiang, Zhiwen Fan, Zehao Zhu, Dejia Xu, Pradyumna Chari, Suya You, Zhangyang Wang, and Achuta Kadambi. Feature 3dgs: Supercharging 3d gaussian splatting to enable distilled feature fields. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2167621685, 2024. 1, 3, 4 [45] Zehao Zhu, Zhiwen Fan, Yifan Jiang, and Zhangyang Wang. Fsgs: Real-time few-shot view synthesis using gaussian splatting. In European conference on computer vision, pages 145163. Springer, 2024. 9 [46] Matthias Zwicker, Hanspeter Pfister, Jeroen Van Baar, and Markus Gross. Ewa volume splatting. In Proceedings Visualization, 2001. VIS01., pages 29538. IEEE, 2001. 1,"
        }
    ],
    "affiliations": [
        "NVIDIA",
        "POSTECH"
    ]
}