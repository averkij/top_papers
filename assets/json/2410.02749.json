{
    "paper_title": "Training Language Models on Synthetic Edit Sequences Improves Code Synthesis",
    "authors": [
        "Ulyana Piterbarg",
        "Lerrel Pinto",
        "Rob Fergus"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in a single pass. One explanation for this is the scarcity of open-sourced edit data. While high-quality instruction data for code synthesis is already scarce, high-quality edit data is even scarcer. To fill this gap, we develop a synthetic data generation algorithm called LintSeq. This algorithm refactors existing code into a sequence of code edits by using a linter to procedurally sample across the error-free insertions that can be used to sequentially write programs. It outputs edit sequences as text strings consisting of consecutive program diffs. To test LintSeq, we use it to refactor a dataset of instruction + program pairs into instruction + program-diff-sequence tuples. Then, we instruction finetune a series of smaller LLMs ranging from 2.6B to 14B parameters on both the re-factored and original versions of this dataset, comparing zero-shot performance on code synthesis benchmarks. We show that during repeated sampling, edit sequence finetuned models produce more diverse programs than baselines. This results in better inference-time scaling for benchmark coverage as a function of samples, i.e. the fraction of problems \"pass@k\" solved by any attempt given \"k\" tries. For example, on HumanEval pass@50, small LLMs finetuned on synthetic edit sequences are competitive with GPT-4 and outperform models finetuned on the baseline dataset by +20% (+/-3%) in absolute score. Finally, we also pretrain our own tiny LMs for code understanding. We show that finetuning tiny models on synthetic code edits results in state-of-the-art code synthesis for the on-device model class. Our 150M parameter edit sequence LM matches or outperforms code models with twice as many parameters, both with and without repeated sampling, including Codex and AlphaCode."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 5 1 ] . [ 2 9 4 7 2 0 . 0 1 4 2 : r a"
        },
        {
            "title": "TRAINING LANGUAGE MODELS ON SYNTHETIC EDIT\nSEQUENCES IMPROVES CODE SYNTHESIS",
            "content": "Ulyana Piterbarg, Lerrel Pinto, Rob Fergus New York University up2021@cims.nyu.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Software engineers mainly write code by editing existing programs. In contrast, large language models (LLMs) autoregressively synthesize programs in single pass. One explanation for this is the scarcity of open-sourced edit data. While highquality instruction data for code synthesis is already scarce, high-quality edit data is even scarcer. To fill this gap, we develop synthetic data generation algorithm called LintSeq. This algorithm refactors existing code into sequence of code edits by using linter to procedurally sample across the error-free insertions that can be used to sequentially write programs. It outputs edit sequences as text strings consisting of consecutive program diffs. To test LintSeq, we use it to refactor dataset of instruction + program pairs into instruction + program-diff-sequence tuples. Then, we instruction finetune series of smaller LLMs ranging from 2.6B to 14B parameters on both the re-factored and original versions of this dataset, comparing zero-shot performance on code synthesis benchmarks. We show that during repeated sampling, edit sequence finetuned models produce more diverse programs than baselines. This results in better inference-time scaling for benchmark coverage as function of samples, i.e. the fraction of problems pass@k solved by any attempt given tries. For example, on HumanEval pass@50, small LLMs finetuned on synthetic edit sequences are competitive with GPT-4 and outperform models finetuned on the baseline dataset by +20% (3%) in absolute score. Finally, we also pretrain our own tiny LMs for code understanding. We show that finetuning tiny models on synthetic code edits results in state-of-the-art code synthesis for the on-device model class. Our 150M parameter edit sequence LM matches or outperforms code models with twice as many parameters, both with and without repeated sampling, including Codex and AlphaCode."
        },
        {
            "title": "INTRODUCTION",
            "content": "The successes of large language models (LLMs) are difficult to overstate. However, consistent and correct zero-shot generation in code synthesis remains out-of-reach for all but the largest models (Abdin et al., 2024; Groeneveld et al., 2024; Dubey et al., 2024). Compared to other reasoning tasks, this setting has two challenging properties, namely solutions are both structured and long-form. Humans tackle problems that have these properties by leveraging abstract mental models, first developing plan for their solution that reflects the settings structure and then executing the plan one step at time (Gopnik, 1982; Kirsh, 2009). For example, software engineer might employ object-oriented programming when creating new code-base by developing class object and then gradually adding new functionality to this class as their code-base becomes more complex. In contrast, LLMs are trained to autoregressively synthesize entire programs from scratch. This makes repeatedly editing program with an LLM extremely expensive current state-of-the-art, LLM-powered code editing tools like Cursor repeatedly prompt models to rewrite entire programs during every edit generation call (Sanger, 2024). LLM outputs also suffer from degrading quality as sequence lengths grow and exhibit limited diversity across samples (Chen et al., 2021; Li et al., 2022b; Roziere et al., 2023; Lozhkov et al., 2024). The consequence of these pathologies is that there We open-source our code and tiny code LM models to https://lintseq.github.io/. 1 Figure 1: Code synthesis with LMs trained on synthetic code edit sequences. Left: An example generation from an LM trained to synthesize code as stream of static-error-free edits. Right: Comparing zero-shot HumanEval coverage (in %) as function of FLOPs for large external LLMs vs the smaller LMs that we instruction finetune in this paper. Repeatedly sampling from edit sequence LMs yields coding problem solutions that are competitive with GPT-4 and GPT-4-Omni, and have total cost similar to sampling once from the best open-source LLMs (see Appendix F.4). does not exist reliable trade-off between zero-shot generation quality and inference-time compute cost under the current paradigm of autoregressive code synthesis, particularly for smaller models. In this paper, we claim that these issues can be mitigated at the data-level by reparameterizing code synthesis as sequential edit problem. Rather than training models for single-step generation of entire programs, we propose that models be trained to generate code by predicting code edit sequences. This objective has major obstacle: while high-quality solution data for code synthesis is scarce, open-source edit data with good coverage over the distribution of all error-free code diffs is nonexistent (Muennighoff et al., 2023). To address this, we introduce an algorithm titled LintSeq that re-factors existing programs into sequences of static error-free code edits. LMs trained on data generated with our algorithm output code edits that effect interdependent lines of program. LintSeq is parameter-free. It consists of two phases: backward sampling phase, which takes source file as input and uses static program verifier to sample sequences of error-free program states that begin with this file and end with empty programs; and forward edit computation phase, which reverses each sequence of programs, employing the Unix diff (Thompson & Ritchie, 1975) operator to compute deltas between consecutive versions of the source file, and outputs edit sequences. The static program verifier used by LintSeq in its backward sampling phase is also referred to as linter. To test the impact of finetuning LMs to synthesize code with edit sequences via LintSeq instruction data, we conduct series of experiments comparing these models to those finetuned on standard version of the same instruction data. We evaluate LMs zero-shot on the code synthesis benchmarks HumanEval (Chen et al., 2021) and MBPP (Austin et al., 2021) by computing coverage, the proportion of problems solved by any attempt, as function of samples. We do not allow LMs to use chain-ofthought reasoning (Wei et al., 2022) during evaluation. Our results are five-fold: 1. Across models ranging in scale from 150M to 14B parameters, instruction finetuning LMs on edit sequences vs full programs improves the quality and diversity of synthesized code. 2. The improved diversity of samples means that pass@k performance increases smoothly as function of inference-time compute, allowing for better trade-off between the two. 3. Tiny edit sequence LMs have state-of-the-art performance in their model class  (Table 1)  . 4. For smaller LLMs, repeatedly sampling from edit sequence models results in HumanEval coverage that is competitive with GPT-4 models and has similar cumulative cost to sampling once per problem from open-source LLMs like Llama 3.1 405B (Figures 1 and 4). 5. Ablating the linter from edit sampling during data generation hurts the downstream quality of programs synthesized by edit sequence models (Figures 5 and 6)."
        },
        {
            "title": "2 LINTSEQ: CODE SYNTHESIS AS A SEQUENTIAL EDIT PROBLEM",
            "content": "LintSeq is an algorithm for synthetic data generation that samples across sequences of insertions that can be used to sequentially write program, leveraging linter to repeatedly check program 2 Figure 2: Visualizing LintSeq, an algorithm for re-factoring programs into sequences of edits. This algorithm samples across all of the static error-free line insertions that can be used to write program chunk-by-chunk. It uses the Unix-diff operator to express generated edit sequences as text strings. states for static errors during generation. linter is type of standard code analysis tool that verifies the correctness of programs. The LintSeq algorithm is loosely inspired by recent work on discrete diffusion methods for text generation, where decoding is non-autoregressive (Li et al., 2022a). The key hypothesis underlying LintSeq is as follows: by training LMs on code edit sequences with teacher-forced supervised learning, we can potentially achieve better trade-off between generation quality and compute at inference-time while still benefiting from the training and sampling efficiency of the autoregressive modeling paradigm. In this section, we provide formalism for the edit sequence re-parameterization of code synthesis and we formally introduce LintSeq. 2.1 REPARAMETERIZING CODE DATASETS WITH EDITS We operate in the textual supervised learning setting in this paper, where we have access to code dataset of example programs y, each of which may be optionally paired with corresponding natural language instruction that describes the programs function, i.e. = {(xi, yi)}N i=1. Let (, ) denote the Unix diff operator (Thompson & Ritchie, 1975), which computes text difference between pair of strings by performing line-by-line matching and returns string summarizing the detected differences. The diff operator is implemented by popular version control and software development systems to help programmers track edits or code diffs between versions of text files. single edit computed with the diff operator may consist of multiple line deletions and/or line insertions. Fix program in the dataset D. Consider sequence σy of text strings corresponding to programs or program states that terminates at y. σy = (y1, . . . , yj1, y) (1) We can equivalently re-express σy as an edit sequence δy of length by first computing diff between an empty program ϵ and the first program in the sequence, and then computing diffs between all pairs of consecutive programs, as shown below. δy = ((ε, y1), (y1, y2), (y2, y3), . . . , (yj1, y)) (2) If is dataset such that for every pair (x, y) D, there exists pair (x, δy) D, then we say that is an edit sequence re-factoring of D. 2.2 GENERATING LINTER-GUIDED SYNTHETIC EDIT SEQUENCES Recall from above that single program edit computed by the diff operator (, ) can consist of any number of deletions and insertions. LintSeq is an algorithm for computing edit sequence re-factorings such that all data (x, δy) have particular property: every edit in δy consists of insertions only. There are two phases in LintSeq: backward sampling phase that is used to compute 3 Figure 3: Empirics of processing code data with LintSeq. Left: Lines per example in dataset of instruction finetuning data for Python synthesis before and after processing with LintSeq via the linter pylint (see Section 3.2). LintSeq processing adds lines of diff metadata to examples (see Appendix A). Right: The corresponding edit counts per synthetic code edit sequence. On dataset of short programs (14 lines of code, on average), the mean LintSeq edit sequence contains four edits. program state sequences σy, and forward edit sequence computation phase that is used to re-express σy as edit sequences δy. An illustration of these phases is shown in Figure 2. Full examples of edit sequences generated with LintSeq are provided in Appendix (Figures 9 and 10). Phase I: Backward Sampling In the backward sampling phase of LintSeq, for each of the pairs (x, y) D, we generate sequences of intermediate program states σy that begin with the empty program and terminate at the original program y. These sequences are generated in reverse or backwards using simple procedure that we dub linter-guided sampling. Starting with the program y, we sequentially generate each predecessor program in σy from its successor by following these steps: (1) delete line from the current program by sampling uniformly at random; (2) run linter or other verifier on the remaining code; (3) if the deletion induced new errors, remove all affected lines; and (4) repeat steps 2 and 3 until no errors are caught by the linter. We repeat these steps until all lines have been removed from the original program y, at which point σy has been generated. Phase II: Forward Edit Computation Once program state sequences σy have been generated for each (x, y) D, we run the forward edit computation phase of our algorithm. In this phase, we apply Equation 2 from above to compute an edit sequence δy for each σy. Starting from the last program that was added to σy, we use the diff operator to compute edits between each pair of consecutive programs in σy up to the original program y. Finally, we pair each edit sequence δy with its instruction (if present) to yield an edit sequence re-factoring of with size sN . 2.3 PROPERTIES OF LINTSEQ DATA Synthetic edit sequences generated by LintSeq have few other important properties. Let δy be an arbitrary j-length edit sequence in generated with LintSeq, δy = ((ε, y1), . . . , (yj1, y)). First, we observe that there is simple correspondence between δy and the original program used to generate it: can be re-constructed by starting with an empty program, and successively applying each edit in δy to this program one-by-one. In other words, the edit sequence δy resolves to y. Furthermore, by construction, every prefix subsequence of δy resolves to sub-program of that is error-free, i.e. that throws no errors when checked with the linter or the verifier used during generation. These two properties, in conjunction with the uniform sampling step used in the first phase of the algorithm, show that LintSeq samples examples across all possible static error-free sequences of line insertions that can be used to sequentially write program from-scratch. We show an example of program synthesis dataset statistics before and after LintSeq processing in Figure 3. In the worst case, re-expressing program as an edit sequence increases the length of training example by token count that is constant in the number of program lines (Appendix A). 2.4 PRACTICALITIES OF TRAINING LANGUAGE MODELS ON LINTSEQ DATA LintSeq can be run on any code data. It is agnostic to the contents of program, and only depends on knowledge of the programming language that file is written in and the existence of linter or another kind of verifier for written program files. We use teacher-forced supervised learning (Williams & Zipser, 1989) to train models on LintSeq data, concatenating edit sequences into single string by interleaving edits with special tokens, <diff>, and computing instruction-conditioned losses over the resultant sequences. During inference, finetuned models can be prompted to synthesize programs with edit sequences by appending these special tokens to the ends of prompts. More details are provided in Appendix A. Synthetic data generation with LintSeq is controlled by single hyperparameter: the number of edit sequences that are sampled for each example in the source code dataset D. Edit sequence sampling can optionally be constrained to avoid repetitions, though this may yield dataset with fewer than sD examples if there are programs in with no non-empty and static error-free sub-programs."
        },
        {
            "title": "3 EXPERIMENTS",
            "content": "To study LintSeq and the impact of re-parameterizing program synthesis as sequential edit generation problem, we conduct multi-pronged set of instruction finetuning experiments. These experiments study code synthesis in Python and are designed to answer the following questions: How does finetuning tiny LMs on re-factorized code edit data generated with LintSeq impact benchmark coverage compared to finetuning on the original code data? Do performance improvements hold across model scales, families, and tokenizers? How does ablating linter-guidance from LintSeq to finetune on randomly sampled edit sequences impact code synthesis? Similar to previous works (Chen et al., 2021), we evaluate models by computing zero-shot coverage statistics on code synthesis benchmarks with and without repeated sampling. We expound upon our motivation for evaluating models in this manner in Appendix B.3.1. 3.1 PRETRAINING TINY LMS FOR CODE UNDERSTANDING We begin our investigations by pre-training two tiny decoder-only transformers, TinyCodeLM-150M and TinyCodeLM-400M, for Python code understanding on 72 billion tokens of text. Pretraining our own language models grants us data contamination-free test-bed to study code synthesis with edit sequences, rapidly evaluate LintSeq, and broadly re-examine the trade-off between inference-time compute and generation quality in code synthesis for models that can be updated on-device. We rely on open-source data and libraries to pretrain our models (Penedo et al., 2024; Lozhkov et al., 2024; Soldaini et al., 2024; Groeneveld et al., 2024). Our pretraining data mix is inspired by Code Llama (Roziere et al., 2023), and reflects code-skewed mixture of web text and raw Python sampled from FineWeb and TheStack, respectively (Penedo et al., 2024; Li et al., 2023). The architecture of our models respectively mimics the two smallest versions of GPT-2 (Radford et al., 2019), but integrates the transformer architecture changes proposed by the OLMo framework. This includes the absence of bias terms and the addition of non-parametric layer norms (Ba, 2016), as well as the use of SwiGLU (Shazeer, 2020), rotary positional embeddings (Su et al., 2024), and the GPT-NeoX-20B tokenizer (Black et al., 2022). We train both models for two epochs with batch size of 524,288 tokens on an NVIDIA H100 node with four GPUs. Our experiments are supported by Pytorch FSDP (Zhao et al., 2023). More details on our pretraining procedures are in Appendix C. 3.2 GENERATING SYNTHETIC DATASET WITH LINTSEQ Next, to support our finetuning experiments, we prepare large baseline dataset of paired instruction and program data. We re-factorize the programs in this dataset into code edit sequences with LintSeq. To that end, we first pool the Python portions of two open-source instruction datasets for code synthesis: the GPT 3.5/4-based Magicoder instruction tuning dataset and the StarCoder2-15B-based self-alignment training dataset (Wei et al., 2024b;a). These datasets are generated with the OSSInstruct approach by Wei et al. (2024b) and have undergone decontamination for the benchmarks that we evaluate on in this paper. We conduct de-duplication on the pooled data to check for repeated examples. Furthermore, we strip any chain-of-thought-like natural language explanations from completion data. The resultant dataset has over 88,900 instruction-Python program pairs. With our baseline dataset prepared, we run LintSeq to generate = 5 synthetic edit trajectory samples for each instruction-program pair. As described in Section 2.4, we concatenate each synthetic edit 5 Table 1: Summary of temperature-tuned coding benchmark results for LMs with 0.2B parameters. Scores annotated with () indicate external model evaluations that we ran using the procedure described in Appendix B, and all other scores are as reported by model authors. We list models in order of increasing HumanEval pass@1 score. HumanEval MBPP(+) Model PolyCoder AlphaCode Codex SmolLM-Instruct TinyCodeLM-Instruct TinyCodeLM-LintSeqInstruct Size pass@1 2.1 160M 4.3 89M 8.2 85M 7.6 135M 150M 9.1 150M 12.8 pass@10 3.4 12.2 12.8 14.4() 13.2 20.6 pass@1 - - - 10.1() 11.5 13.6 - - - 14.6() 16.4 24.4 pass@10 Open-Source (cid:32) (cid:35) (cid:35) (cid:32) (cid:32) (cid:32) Table 2: Summary of temperature-tuned coding benchmark results for 0.2B to 0.4B parameter language models. Annotations, model order, and evaluation procedure are the same as in Table 1. HumanEval MBPP(+) Model PolyCoder TinyCodeLM-Instruct SmolLM-Instruct AlphaCode CodeT5+ Codegen-Mono Codex TinyCodeLM-LintSeqInstruct Size pass@1 3.0 400M 400M 9.6 360M 11.3 302M 11.6 220M 12.0 350M 12.8 300M 13.2 400M 13.4 pass@10 5.3 18.5 19.3() 18.8 20.7 23.1 20.4 20. pass@1 - 15.5 19.4() - - 9.4() - 19.4 pass@10 Open-Source - 22.2 23.1() - - 15.2() - 29.9 (cid:32) (cid:32) (cid:32) (cid:35) (cid:32) (cid:32) (cid:35) (cid:32) trajectory into single string by interleaving consecutive edits with special reserved edit token. Inspired by Muennighoff et al. (2024), we do not restrict against edit sequence repetitions. We use the popular Python linter pylint to guide edit sampling during generation. Examples of generated edit sequences and experiments testing the effect of varying are in Appendix E. 3.3 FINETUNING LANGUAGE MODELS ON LINTSEQ EDIT SEQUENCES We now probe the impact of instruction finetuning variety of autoregressive LMs to synthesize code with edit sequences, compared to standard generation of full programs. We use two code synthesis benchmarks to support our model evaluations: HumanEval (Chen et al., 2021) and Mostly Basic Programming Problems (MBPP) (Austin et al., 2021). Using both the code edit re-factorized and baseline instruction datasets obtained in section 3.2, we run pairs of finetuning experiments with six different models. In each experiment pair, we finetune an LM on both datasets for an equal number of optimizer steps and with the same learning rate schedule, saving intermediate checkpoints throughout finetuning. We run full benchmark evaluations on HumanEval and MBPP on each saved checkpoint1, performing no prompt tuning. Then, we compare the best inference-time scaling behavior, i.e. benchmark coverage (pass@k) as function of samples (k), obtained by edit sequence finetuning vs standard finetuning for each model. more detailed description of the computed metrics as well as full specification of the evaluation and finetuning procedures is provided in Appendices and D. 3.3.1 TINYCODELM We run our first two pairs of finetuning experiments on TinyCodeLM-150M and TinyCodeLM-400M. These models were not pretrained on code synthesis instruction data, nor were they pretrained on any diff-like edit data. Our experimental results are summarized in Tables 1 and 2, where we compare the temperature-tuned performance of our models to the reported benchmark coverage of existing 1To process the generations of edit sequence LMs into executable programs, we simply resolve each of the predicted code edits one-by-one. This procedure is visualized in Figure 1 and described in Appendix A.2. 6 Figure 4: HumanEval and MBPP coverage with repeated sampling (pass@k vs. k) achieved by instruction finetuning Gemma 2, Phi-3, and Llama 3.1 language models on dataset of LintSeq edit sequence re-factored vs standard Python code (temperature = 1, top-p = 0.95). code LMs at similar parameter scales. We also report the inference-time scaling of benchmark coverage as function of samples for our finetuned models in Appendix Tables 8 and 9. For both the 150M and 400M parameter versions of TinyCodeLM, we find that finetuning LMs to synthesize code with edits via LintSeq data dramatically improves benchmark performance compared to baseline finetuning. Indeed, the edit sequence variants of TinyCodeLM outperform all existing code language models of comparable scale that we are aware of, including AlphaCode (Li et al., 2022b), Codex (Chen et al., 2021), CodeT5+ (Wang et al., 2023b), and the recent SmolLM-Instruct (Ben Allal et al., 2024). Our smaller edit sequence-finetuned model is particularly strong for its size, roughly matching or out-performing models with twice as many parameters including the 300M parameter version of Codex and the 302M-parameter version of AlphaCode (Tables 1 and 2). 3.3.2 GEMMA 2, PHI-3, AND LLAMA 3.1 The results above raise natural question: do performance improvements from finetuning LMs to synthesize code with edit sequences hold for other model scales, architectures, and tokenizers? To test this, we conduct four additional pairs of instruction finetuning experiments on LMs from three model families, Gemma 2, Phi-3, and Llama 3.1, ranging in size from 2.6B to 14B. We employ pretrained-only model weights, if available. The results of these experiments are in Figure 4, where we plot zero-shot benchmark coverage as function of samples for instruction finetuned models. Raw coverage scores are reported in Appendix Tables 10 and 11. Most of our findings echo those of Section 3.3.1. Aggregating across sample counts, we find that finetuning models to synthesize code with edits improves overall zero-shot performance on HumanEval and MBPP compared to finetuning on the original data. This suggests that re-factoring code with edit sequences is an architectureand tokenizer-independent mechanism for improving downstream LM outputs. Furthermore, as shown in Figure 4 and Tables 10 and 11, we find that the degree by which edit sequence LMs outperform baseline model variants increases with repeated sampling for all tested models, culminating in an average absolute gain in pass@50 of +20% ( 3%) on HumanEval and +12% ( 2%) on MBPP. This observation confirms the hypothesis posed in Section 2, showing that training LMs to synthesize code with edits using LintSeq data improves the relationship between cumulative inference-time compute and zero-shot performance. At pass@1, however, our results are slightly more mixed than in Section 3.3.1. For Phi-3 models, we observe either no difference or decrease in score between each pair of model-data variants. One explanation for this is bias: the Phi-3 models have been previously instruction finetuned and were likely to have been trained on standard code data, putting LM variants finetuned to generate edit sequence re-factorized code at comparative disadvantage due to distributional shift. 3.4 ABLATING LINTER-GUIDANCE The backward sampling phase of LintSeq uses linter or other verifier to group interdependent lines of program together during edit sequence generation, ensuring that all code edits resolve to 7 Figure 5: HumanEval and MBPP pass@k vs. achieved by finetuning TinyCodeLM models on linter-guided vs randomly sampled code edit sequences (temperature = 1, top-p = 0.95). programs that are free of static errors. We conclude our experiments by testing the importance of this design choice with TinyCodeLM models. To do this, we replace the backwards procedure described in Section 2.2 with exclusively random sampling; during each step of the algorithm, we first sample the number of lines to delete from the current program uniformly at random, before sampling set of lines with the desired count. Using this linter-ablated version of the algorithm, we generate new synthetic edit sequence dataset with the same size as the LintSeq dataset used in all previous finetuning experiments, i.e. with = 5 example sequences per sample in the source dataset. The average number of edits per example in this dataset (ERandSeqInstruct = 3.9) is empirically similar to its linter-guided counterpart (ELintSeqInstruct = 3.8, see Figure 3). We employ the same procedure as the one used in Section 3.3 to instruction finetune TinyCodeLM models on the dataset of randomly sampled edit sequences. In Figure 5, we compare the temperature 1 inference-time scaling laws on HumanEval and MBPP obtained by finetuning models on randomly sampled vs static error-free edit sequences. Raw model scores are also provided in Appendix F, Tables 8 and 9. Ablating linter-guidance results in decline in benchmark coverage. On HumanEval, linter ablation reduces absolute pass@50 score on TinyCodeLM-150M (22.6% (cid:55) 17.7%) and on TinyCodeLM-400M (26.8% (cid:55) 22.0%). MBPP pass@50 is similarly affected, dropping for both models (34.5% 30.2% and 39.6% (cid:55) 34.5%). These results suggest that the error-free nature of edits in LintSeq instruction finetuning data does indeed have positive impact on the coding problem coverage of sampled solutions. To conclude our analysis, we probe whether training models on error-free edit sequences also has an effect on the presence of errors in across all generated programs, aside from its positive effects on coverage. To assess this, we run the Python linter pylint over each of the synthesized programs used to compute the reported temperature 1 pass@k metrics, checking code for static errors. In Figure 6, we plot the total proportions of synthesized program samples with at least one static error across finetuned model variants. On both benchmarks, LMs trained on randomly sampled edits (dark grey) appear to generate buggy code with much higher frequency than all other models. Furthermore, on HumanEval, we find that LintSeq models (indigo) synthesize programs with static errors at higher frequency than baseline models (light grey), despite their higher coverage of benchmark coding problems. This additional finding suggests that model performance gains from LintSeq cannot simply be attributed to improvement in static error frequency across code training on re-factored code must be helping models write generally better, more diverse programs. In summary, the error-free nature of the linter-guided edits sampled in LintSeq appears to indeed be important for improving both the quality and diversity of sampled programs (Figure 5), as well as the overall correctness (Figure 6) of code synthesized by language models trained on edit sequences. Figure 6: Comparing static error frequency in synthesized code samples across baseline vs edit sequence instruction finetuned model variants (n = 50, temperature = 1, top-p = 0.95)."
        },
        {
            "title": "4 RELATED WORK",
            "content": "Foundation Models for Code Code synthesis is one of the oldest problems in computer science. Neural language model-based approaches such as Codex, AlphaCode, CodeT5+, CodeGen, StarCoder, and Code Llama have recently proven to be extremely competitive with previous methods (Chen et al., 2021; Li et al., 2022b; Wang et al., 2023b; Nijkamp et al., 2022; Li et al., 2023; Roziere et al., 2023). Today, foundation models trained on web text and code data dominate, and LLM-powered code editing tools like Github Copilot and Cursor are used by thousands of engineers every day (Heaven, 2024). Many general-purpose LLMs are also trained on code data. While the largest of these LLMs show strong performance on coding benchmarks, generations continue to suffer from limited meaningful output diversity, prompt sensitivity, and degrading quality on long-contexts (Achiam et al., 2023; Gemini Team et al., 2023; Dubey et al., 2024). Smaller models also lag behind (Abdin et al., 2024; Gemma Team et al., 2024; Ben Allal et al., 2024). As of the writing of this paper, directly prompting LLMs to generate code diffs results in low quality edits across models (Sanger, 2024). We claim that this is the result of data problem and we attempt to address it in this work. Finetuning on Synthetic Data LLM post-training methods like supervised finetuning have been shown to be extremely powerful for improving model performance across tasks (Wei et al., 2021). However, high-quality datasets of paired instruction-response examples are extremely expensive to curate. One possible solution lies in synthetic data generation methods like Self-Instruct, wherein an LLM is prompted to generate instructions and/or responses from examples (Wang et al., 2022). Such data have been used extensively for improving LLM performance through self-refinement and/or knowledge distillation on coding tasks (Chaudhary, 2023; Roziere et al., 2023; Abdin et al., 2024; Lozhkov et al., 2024). We employ post-processed instruction data for code synthesis created with method from this family, OSS-Instruct (Wei et al., 2024b), as the base of our experiments on re-factorizing code with code edit sequences via LintSeq. Unlike Self-Instruct-like synthetic data generation methods, our algorithm does not employ an LLM for data generation, and instead generates examples of error-free edit sequences from existing code data by using simple linter. Finetuning on Edits Several works have investigated finetuning code LLMs on edit data. Notably, Muennighoff et al. (2023) instruction tune models on 4TB dataset of GitHub commits pairing code changes with human instructions. Relatedly, Li et al. (2024) use GitHub commit data sourced from Python repositories to generate code editing instruction data with GPT 3.5/ChatGPT. Both of these works specifically focus on better-equipping LLMs for natural language-prompted code editing tasks, in which model is explicitly prompted to generate an edit in response to natural language specification. Our work differs in three important ways: first, we study edit sequences rather than single edits; second, we train LLMs to predict edits implicitly during code synthesis; third, our synthetic edit generation algorithm does not rely on the existence of any kind of commit data. On Device Language Models As the capabilities of LLMs have improved, so to have those of small language models. Recent projects like SmolLM (Ben Allal et al., 2024) and OpenELM (Mehta et al., 2024) re-examine the potential of tiny language models that can be run and even updated on-device, i.e. on smart phone or laptop. The representations learned by such models during pretraining are weaker than those of scaled-up LLMs (Kaplan et al., 2020). This is particularly true for harder tasks that involve reasoning, such as code synthesis (Gemma Team et al., 2024; Abdin et al., 2024). To our knowledge, the most recent open-source work studying small language models pretrained entirely for code understanding is from several years ago (Xu et al., 2022; Nijkamp et al., 2022; Wang et al., 2021; 2023b). The 150M and 400M parameter TinyCodeLM models pretrained in this paper belong to the on device model family and build upon previous works. These models provide an efficient test-bed for experiments on LM code synthesis that is updated to recent advancements in high throughput pretraining and to improvements in open-source data quality. Inference-Time Compute Scaling The performance of language models can be boosted during inference by using scaled-up sample counts, hand-engineered prompting schema, and/or search (Brown et al., 2024; Snell et al., 2024). These methods dramatically increase inference costs. Their effectiveness is tightly linked to the expressivity of learned model representations and the diversity of outputs across samples. Our experiments with smaller language models are inspired by these works we study whether it is possible to (1) improve the expressivity of representations for code synthesis across LM parameter scales during finetuning, and (2) take advantage of this property to improve the inference-time performance of smaller LMs by larger margins during repeated sampling."
        },
        {
            "title": "5 DISCUSSION, LIMITATIONS, AND CONCLUSION",
            "content": "This paper introduces an algorithm, LintSeq, for generating synthetic code edit sequences from existing programs. LintSeq enables LLM reasoning settings like code synthesis to be re-parameterized at the data-level as sequential edit generation tasks. The algorithm is parameter-free, requires only CPU to run, and makes no assumptions about the content or structure of code files. Re-parameterizing code generation with edits has few immediate benefits. For example, it makes code generation with LLMs much more controllable at the prompt-level (Appendix A.3) and it reduces the cost of predicting useful and correct code insertions with models, since synthetic edit-trained LLMs do not need to be prompted to re-synthesize entire programs from scratch (Section 2.4). In our experiments with LintSeq, we also show the following: 1. Tiny LMs can be efficiently finetuned to synthesize Python programs with edit sequences via LintSeq data. This results in state-of-the-art code benchmark performance for models that can be trained on device (Sections 3.1 and 3.3.1). 2. Across other tested models from the Phi, Gemma, and Llama families, finetuning on LintSeq data also improves the diversity of zero-shot generations, boosting the inference-time scaling of coverage on HumanEval and MBPP at fixed sample counts (Section 3.3.2). 3. On HumanEval, the cumulative inference cost of repeatedly sampling from small edit sequence LLMs is similar to sampling once from larger LLMs and yields coverage that is competitive with GPT-4, GPT-4-Omni, and Llama 3.1 405B (Figure 1, Appendix F.4). 4. Ablating linter-guidance from LintSeq hurts the quality, diversity, and correctness of code synthesized by instruction finetuned models (Section 3.4). There are several limitations to our work. First, as currently formulated, LintSeq can only be used to generate synthetic sequences of insertion edits. This is consequence of the parameter-free nature of the algorithm every edit in LintSeq sequence reflects an existing line of code in the source file used to generate it. As result, LintSeq edit data can only be used to train models for synthesis-like tasks, rather than for refinement. Second, our experiments with LintSeq study code synthesis in Python only. We look forward to testing LintSeq with other programming languages, verifiers, and problems in future work. One other exciting setting where LintSeq might improve the tradeoff between model generation quality and inference-time compute is in mathematical reasoning and formal theorem proving."
        },
        {
            "title": "ETHICS STATEMENT",
            "content": "This work explores data-driven mechanisms for improving the quality of language model-generated code. Our synthetic data generation method relies on open-source data and our experiments leverage open-source software and resources. It is important to acknowledge that all language models for code synthesis have the potential to be misused whether intentionally or unintentionally for generation of code with vulnerabilities and/or malicious behaviors. Any and all model generated code has the potential to be harmful and must not be executed without precautions."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "This work was supported by grants from NSF award 2339096 and ONR awards N00014-21-1-2758 and N00014-22-1-2773. We are grateful to Shenglong Wang and NYU High Performance Computing for their support of this project. UP is funded by an NSF GRFP Award, and LP is funded by the Packard Fellowship. We would like to thank Nate Rahn, Mahi Shafiullah, and David Brandfonbrener for helpful comments and discussions."
        },
        {
            "title": "REFERENCES",
            "content": "Marah Abdin, Sam Ade Jacobs, Ammar Ahmad Awan, Jyoti Aneja, Ahmed Awadallah, Hany Awadalla, Nguyen Bach, Amit Bahree, Arash Bakhtiari, Harkirat Behl, et al. Phi-3 technical report: highly capable language model locally on your phone. arXiv preprint arXiv:2404.14219, 2024. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Bo Adler, Niket Agarwal, Ashwath Aithal, Dong Anh, Pallab Bhattacharya, Annika Brundyn, Jared Casper, Bryan Catanzaro, Sharon Clay, Jonathan Cohen, et al. Nemotron-4 340b technical report. arXiv preprint arXiv:2406.11704, 2024. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. JL Ba. Layer normalization. arXiv preprint arXiv:1607.06450, 2016. Loubna Ben Allal, Niklas Muennighoff, Logesh Kumar Umapathi, Ben Lipkin, and Leandro von Werra. framework for the evaluation of code generation models. https://github.com/ bigcode-project/bigcode-evaluation-harness, 2022. Loubna Ben Allal, Anton Lozhkov, and Elie Bakouch. Smollm - blazingly fast and remarkably powerful. https://huggingface.co/blog/smollm, 2024. Accessed: 2024-09-02. Sid Black, Stella Biderman, Eric Hallahan, Quentin Anthony, Leo Gao, Laurence Golding, Horace He, Connor Leahy, Kyle McDonell, Jason Phang, et al. Gpt-neox-20b: An open-source autoregressive language model. arXiv preprint arXiv:2204.06745, 2022. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. Sahil Chaudhary. Code alpaca: An instruction-following llama model for code generation. https: //github.com/sahil280114/codealpaca, 2023. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Google Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Google Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivi`ere, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. Alison Gopnik. Words and plans: Early language and the development of intelligent action. Journal of Child Language, 9(2):303318, 1982. Dirk Groeneveld, Iz Beltagy, Pete Walsh, Akshita Bhagia, Rodney Kinney, Oyvind Tafjord, Ananya Harsh Jha, Hamish Ivison, Ian Magnusson, Yizhong Wang, et al. Olmo: Accelerating the science of language models. arXiv preprint arXiv:2402.00838, 2024. Will Douglas Heaven. How ai assistants are already changing the way code gets https://www.technologyreview.com/2023/12/06/1084457/ made. ai-assistants-copilot-changing-code-software-development-github-openai/, 2024. Accessed: 2024-09-20. 11 Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. David Kirsh. Problem solving and situated cognition. The Cambridge Handbook of Situated Cognition, pp. 264306, 2009. Denis Kocetkov, Raymond Li, Loubna Ben Allal, Jia Li, Chenghao Mou, Carlos Munoz Ferrandis, Yacine Jernite, Margaret Mitchell, Sean Hughes, Thomas Wolf, et al. The stack: 3 tb of permissively licensed source code. arXiv preprint arXiv:2211.15533, 2022. Kaixin Li, Qisheng Hu, James Zhao, Hui Chen, Yuxi Xie, Tiedong Liu, Michael Shieh, and Junxian He. Instructcoder: Instruction tuning large language models for code editing. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 4: Student Research Workshop), pp. 5070, 2024. Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, et al. Starcoder: may the source be with you! arXiv preprint arXiv:2305.06161, 2023. Xiang Li, John Thickstun, Ishaan Gulrajani, Percy Liang, and Tatsunori Hashimoto. Diffusion-lm improves controllable text generation. Advances in Neural Information Processing Systems, 35: 43284343, 2022a. Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser, Remi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago, et al. Competition-level code generation with alphacode. Science, 378(6624):10921097, 2022b. Jiawei Liu, Chunqiu Steven Xia, Yuyao Wang, and Lingming Zhang. Is your code generated by chatGPT really correct? rigorous evaluation of large language models for code generation. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. URL https: //openreview.net/forum?id=1qvx610Cu7. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. Sachin Mehta, Mohammad Hossein Sekhavat, Qingqing Cao, Maxwell Horton, Yanzi Jin, Chenfan Sun, Seyed Iman Mirzadeh, Mahyar Najibi, Dmitry Belenko, Peter Zatloukal, and Mohammad Rastegari. OpenELM: An efficient language model family with open training and inference framework. In Workshop on Efficient Systems for Foundation Models II @ ICML2024, 2024. URL https://openreview.net/forum?id=XNMbTkxroF. Niklas Muennighoff, Qian Liu, Armel Zebaze, Qinkai Zheng, Binyuan Hui, Terry Yue Zhuo, Swayam Singh, Xiangru Tang, Leandro Von Werra, and Shayne Longpre. Octopack: Instruction tuning code large language models. arXiv preprint arXiv:2308.07124, 2023. Niklas Muennighoff, Alexander Rush, Boaz Barak, Teven Le Scao, Nouamane Tazi, Aleksandra Piktus, Sampo Pyysalo, Thomas Wolf, and Colin Raffel. Scaling data-constrained language models. Advances in Neural Information Processing Systems, 36, 2024. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. Codegen: An open large language model for code with multi-turn program synthesis. arXiv preprint arXiv:2203.13474, 2022. Guilherme Penedo, Hynek Kydlıˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. arXiv preprint arXiv:2406.17557, 2024. Ulyana Piterbarg, Lerrel Pinto, and Rob Fergus. diff history for neural language agents. In Forty-first International Conference on Machine Learning, 2024. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. 12 Jie Ren, Samyam Rajbhandari, Reza Yazdani Aminabadi, Olatunji Ruwase, Shuangyan Yang, Minjia Zhang, Dong Li, and Yuxiong He. {ZeRO-Offload}: Democratizing {Billion-Scale} model training. In 2021 USENIX Annual Technical Conference (USENIX ATC 21), pp. 551564, 2021. Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Tal Remez, Jeremy Rapin, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. Aman Sanger. Editing files at 1000 tokens per second. https://www.cursor.com/blog/ instant-apply, 2024. Accessed: 2024-09-02. Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters. arXiv preprint arXiv:2408.03314, 2024. Luca Soldaini, Rodney Kinney, Akshita Bhagia, Dustin Schwenk, David Atkinson, Russell Authur, Ben Bogin, Khyathi Chandu, Jennifer Dumas, Yanai Elazar, et al. Dolma: An open corpus of three trillion tokens for language model pretraining research. arXiv preprint arXiv:2402.00159, 2024. Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Ken Thompson and Dennis Ritchie. unix Programmers Manual. Bell Telephone Laboratories, 1975. Guanhua Wang, Heyang Qin, Sam Ade Jacobs, Connor Holmes, Samyam Rajbhandari, Olatunji Ruwase, Feng Yan, Lei Yang, and Yuxiong He. Zero++: Extremely efficient collective communication for giant model training. arXiv preprint arXiv:2306.10209, 2023a. Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah Smith, Daniel Khashabi, and Hannaneh Hajishirzi. Self-instruct: Aligning language models with self-generated instructions. arXiv preprint arXiv:2212.10560, 2022. Yue Wang, Weishi Wang, Shafiq Joty, and Steven CH Hoi. Codet5: Identifier-aware unified pre-trained encoder-decoder models for code understanding and generation. arXiv preprint arXiv:2109.00859, 2021. Yue Wang, Hung Le, Akhilesh Deepak Gotmare, Nghi DQ Bui, Junnan Li, and Steven CH Hoi. Codet5+: Open code large language models for code understanding and generation. arXiv preprint arXiv:2305.07922, 2023b. Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned language models are zero-shot learners. arXiv preprint arXiv:2109.01652, 2021. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35:2482424837, 2022. Yuxiang Wei, Federico Cassano, Jiawei Liu, Yifeng Ding, Naman Jain, Harm de Vries, Leandro von Werra, Arjun Guha, and Lingming Zhang. Starcoder2-instruct: Fully transparent and permissive self-alignment for code generation. https://huggingface.co/blog/sc2-instruct, 2024a. Accessed: 2024-09-08. Yuxiang Wei, Zhe Wang, Jiawei Liu, Yifeng Ding, and Lingming Zhang. Magicoder: Empowering code generation with oss-instruct. In Forty-first International Conference on Machine Learning, 2024b. Ronald Williams and David Zipser. learning algorithm for continually running fully recurrent neural networks. Neural computation, 1(2):270280, 1989. 13 Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Perric Cistac, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-of-the-Art Natural Language Processing. In Association for Computational Linguistics, pp. 3845, October 2020. URL https://www.aclweb.org/anthology/2020.emnlp-demos.6. Frank Xu, Uri Alon, Graham Neubig, and Vincent Josua Hellendoorn. systematic evaluation of large language models of code. In Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming, pp. 110, 2022. Yanli Zhao, Andrew Gu, Rohan Varma, Liang Luo, Chien-Chin Huang, Min Xu, Less Wright, Hamid Shojanazeri, Myle Ott, Sam Shleifer, et al. Pytorch fsdp: experiences on scaling fully sharded data parallel. arXiv preprint arXiv:2304.11277, 2023."
        },
        {
            "title": "A MORE ON EDIT SEQUENCES AND DIFFS",
            "content": "A.1 READING UNIX DIFFS We provide guide to reading Unix-style diffs below in Figure 7. The diff shown in this figure is computed using the Python library difflib, which is the implementation that we use to compactly represent edits in our synthetic data generation experiments. Note that the total extra tokens present in an insertion edit sequence representation of program scales with the number of program lines L, and can be upper-bounded as Tdiff L((chars in decorator)+(extra chars per line in body)) = 16L. Figure 7: The anatomy of Unix diff: diagrammatic visualization of the different parts of Unix-style diff, as computed by difflib. The body of diff can consist of multiple line deletions, followed by multiple line insertions. The decorator portion of the diff respectively indicates the location and size of these deletions and insertions, if any. Like the diff shown above, the edits in synthetic edit sequences generated by LintSeq consist of line insertions only. A.2 RESOLVING EDIT SEQUENCES During inference, LMs that have been finetuned on LintSeq instruct data will synthesize code via edit sequences, outputting text strings that consist of sequence of consecutive Python diffs interleaved with newline characters and <diff> tokens, similar to Piterbarg et al. (2024). Each of these diffs will be structured as shown in Figure 7, if correctly formatted by the language model. Resolving an edit sequence generated by language model into an executable Python program is simple: starting with an empty program, we consecutively apply the line insertions and/or deletions in the body of each diff to the lines of the program specified in its decorator. We continue this process until all of the diffs in the generated edit sequence have been parsed and resolved. Figure 1 shows code edit sequence generation from LintSeq instruction finetuned LM and the corresponding resolved, executable Python program. A.3 CONTROLLABILITY OF CODE SYNTHESIS WITH EDIT SEQUENCE LMS The structure of Unix-style diffs affects the downstream controllability of code synthesis with models that have been trained on edit sequence re-parameterized programs. As shown in Figure 7, the first line of every diff is decorator that describes the location and the numbers of lines changed by the edit. During inference, autoregressive language models that have been trained on Unix-style diffs with this format can be prompted to predict an edit in any desired target location within the program being synthesized by intervening on model generation. A.4 FUTURE WORK: SEARCHING IN EDIT SPACE If we apply the lens of reinforcement learning or search to this setting, we might say that reparameterizing the code data used to train language model re-parameterizes the models generative action space. It is possible that combining edit sequence LMs with more sophisticated decoding mechanisms, inference-time search, and/or interactive post-training may result in even larger improvements to the quality of generated code than those of the zero-shot code synthesis settings studied in this paper. We look forward to testing this hypothesis in future work."
        },
        {
            "title": "B EVALUATION",
            "content": "HumanEval (Chen et al., 2021) and Mostly-Basic Programming Problems (MBPP) (Austin et al., 2021) are two of the most studied benchmarks for evaluating code LMs (Liu et al., 2023). These benchmarks probe the code synthesis capabilities of models, and consist of pairs of natural language program descriptions and test-cases. We employ the extended MBPP test cases released as MBPP(+) by Liu et al. (2023) to add additional rigour to our testing procedure. All of the code LMs that we compare our models against evaluate HumanEval performance using the original set of benchmark test cases; for consistency, we employ these same test cases in our evaluations when comparing the performance of our models to the reported scores of external LMs. During testing on both HumanEval and MBPP(+), LMs are prompted to generate outputs using the natural language descriptions of target programs. Their outputs are then evaluated on the paired test cases. generation is considered correct if and only if it passes all of the test cases upon execution, subject to fixed timeout setting. Previous works on code synthesis with language models report scores on HumanEval and MBPP(+) across samples. The most common of these metrics is known as pass@k (Chen et al., 2021; Austin et al., 2021; Li et al., 2022b; Wang et al., 2023b). This is the metric that we use to report and compare model performance throughout this paper. Our implementation of HumanEval and MBPP(+) evaluations mimics the Big Code Evaluation Harness by Ben Allal et al. (2022). We do not allow models to use chain-of-thought during generation. B.1 PROMPTING The primary goal of this paper is to introduce method for re-factorizing code synthesis with LMs by finetuning them on synthetic instruction data. As result, we evaluate all models using minimal prompt formats, performing no prompt tuning (see Figures 9 and 10). Examples of the prompt formats that we use during evaluation are shown in Figure 8. Figure 8: Examples of formatted HumanEval and MBPP(+) prompts used in model evaluations. We finetune all tested models on example outputs exclusively corresponding to Python code, and as result, we do not use Markdown formatting to separate Python code from natural language in either our instruction data nor in our inference-time prompts. To evaluate models on HumanEval, we use both the default Python version prompt format in the original benchmark dataset, where natural language program description is provided to an LM within docstring, as well as the equivalent, fully natural language prompt format from HumanEvalPack (Muennighoff et al., 2023). The latter format is similar to the structure of the instructions in our finetuning datasets. To evaluate models on MBPP(+), we use the default prompts from the MBPP benchmark dataset, formatted with specification of the target function name and arguments both inside and outside of the natural language instruction, as shown in Figure 8. During LM benchmark evaluations, we test models on each of the format variants described above and report scores on the better performing variant for each benchmark only. 16 B.2 GENERATION AND PARSING During generation, we continue decoding until an end-of-sequence token is output by an LM. We treat all LM outputs as either Python code or sequences of Python code edits, depending on whether an LM was finetuned on standard instruct or LintSeq instruct data. In the latter case, we post-process outputs by resolving the output edit sequences using the procedure described in Appendix A.2. B.3 EVALUATING MODEL CHECKPOINTS B.3.1 PHILOSOPHY There is well-known trade-off between the temperature used for sampling from autoregressive code LMs and the benchmark coverage achievable by models, i.e. the proportion of problems pass@k for which an LM is able to generate at least one output that passes all test cases given tries. This trade-off was first described by Chen et al. (2021). Informally, increasing the sampling temperature increases the width of the distribution from which tokens are sampled, producing more diverse but noisier (and possibly lower quality) generations. For larger repeated sample counts, the pass@k score typically increases with sampling temperature up to some threshold, beyond which the negative effects of noise overpower the positive effects of diversity. The benchmark coverage achievable by an LM at any temperature and in the limit of samples, i.e. on pass@k for , ultimately depends on both the power and expressivity of the code language models learned representation. From practical perspective, while smaller language models may have weaker representational power than larger models, the representational expressivity of the former may enable them to overtake the latter at fixed computational budgets by leveraging extra compute at inference-time, e.g. generating larger number of samples per problem and using the provided test cases to check each one for correctness before returning an output (Brown et al., 2024; Snell et al., 2024). For example, an LLM that has an 85% pass@1 score on an arbitrary task may be more expensive in total serving cost (see Figure 1) than smaller LM with 90% pass@50 score on the same task. small LM can only have this property, however, if it exhibits reliable trade-off between generation quality and inference-time sampling cost across tasks. In other words, its representation must be sufficiently expressive. B.3.2 COMPUTING COVERAGE (PASS@K) Our goal is to probe whether re-parameterizing code synthesis with edit sequences can improve the expressivity of smaller LLM representations, boosting benchmark coverage as function of samples-per-problem. Hence, we primarily compare finetuned models by evaluating them with the procedures described above on HumanEval and MBPP(+) at high temperature and large sample count, computing pass@k for {1, 5, 10, 20, 50} with = 50 samples2 at temperature 1, top-p 0.95. We compute pass@k statistics with the same procedure as Chen et al. (2021). The results of these evaluations are reported throughout the paper and shown in Figures 4, 5 and Tables 8, 9, 10, 11. In each of these Figures and Tables, we identify the most performant checkpoint from each model-data finetuning run by comparing pass@50 score at temperature 1 on HumanEval and MBPP(+) across checkpoints. Many existing state-of-the-art code synthesis LMs only report temperature-tuned pass@k scores on HumanEval, including Codex, AlphaCode, and Codegen-Mono (Chen et al., 2021; Li et al., 2022b; Nijkamp et al., 2022). Thus, in Tables 1 and 2, we select the single best overall checkpoint of each TinyCodeLM model by averaging HumanEval and MBPP(+) pass@50 score. Then, we temperaturetune each checkpoints pass@1 and pass@10 scores when reporting results. On HumanEval, we test temperatures τ {0.0, 0.2, 0.4, 0.8, 1.0}. On MBPP(+), we sweep over smaller temperature range, τ {0.0, 0.1, 1.0}. We perform the same temperature tuning procedure when reporting external model benchmark scores as well, i.e. the scores annotated with () in Tables 1 and 2. When running benchmark evaluations with these external code LMs, we stray from the prompt formatting, generation, and parsing procedures described in Appendices B.1 and B.2; instead, in the interest of fair evaluation, we reproduce the conventions reported by model authors to report other scores. 2These are the largest sample counts that are feasible to compute on our hardware given the scope of our experiments."
        },
        {
            "title": "C PRETRAINING",
            "content": "We rely on data and libraries open-sourced by the HuggingFace, FineWeb, StarCoder, Dolma, OLMo, and PyTorch FSDP projects to pretrain our models (Wolf et al., 2020; Penedo et al., 2024; Lozhkov et al., 2024; Soldaini et al., 2024; Groeneveld et al., 2024; Zhao et al., 2023). C.1 MODEL ARCHITECTURES AND PRETRAINING HYPERPARAMETERS Table 3: Architectural and pretraining hyperparameters of our on device 150M and 400M parameter TinyCodeLM models, pretrained on mixture of Web text and code for Python understanding. Transformer Architecture Model Family Tokenizer Attention Bias Attention Dropout Hidden Activation Hidden Size Intermediate Size Number of Attention Heads Number of Hidden Layers Number of Key-Value Heads Vocabulary Size Positional Encodings Mixed Precision Weight Tying Flash Attention 2 Optimizer Learning Rate Weight Decay Betas Epsilon TinyCodeLM Smallest, 150M Parameters decoder-only OlmoForCausalLM GPT-NeoX-20B-OLMo False 0.0 SwiGLU 768 3072 12 12 12 50304 Rotary (RoPE) BFLOAT16 True True Small, 400M Parameters decoder-only OlmoForCausalLM GPT-NeoX-20B-OLMo False 0.0 SwiGLU 1024 4096 16 24 16 50304 Rotary (RoPE) BFLOAT16 True True AdamW 0.0003 0.01 (0.9, 0.95) 1.0e-05 AdamW 0.0003 0.01 (0.9, 0.95) 1.0e-05 Learning Rate Scheduler Number of Warm-Up Steps Alpha-f (αf ) Total Epochs of Pretraining cosine (with warmup) 100 0.1 cosine (with warmup) 100 0.1 2 C.2 PRETRAINING DATA MIX Table 4: Pretraining data mix used to train both TinyCodeLM models. Datasets were tokenized and prepared using HuggingFace and Dolma tooling (Wolf et al., 2020; Soldaini et al., 2024). Pretraining Data Source FineWeb (Penedo et al., 2024) The Stack (Kocetkov et al., 2022) Subset 10BT Sample Python Only Tokens Documents 14.9M 10.4BT 24.2M 61.8BT"
        },
        {
            "title": "D INSTRUCTION FINETUNING",
            "content": "D.1 BASELINE INSTRUCTION DATASET Table 5 displays the data sources that are used to prepare the dataset described in Section 3.2. These data are pooled and preprocessed into instruction-program pairs by stripping away Markdown formatting and natural language explanations from completions (Figure 9 and 10). In our experiments, we use the resultant data to finetune baseline models, comparing their performance to those of LMs finetuned on edit sequences generated with LintSeq from the same set of instruction-program pairs. HuggingFace Instruction Data Source bigcode/self-oss-instruct-sc2-exec-filter-50k ise-uiuc/Magicoder-OSS-Instruct-75K Subset Examples 50,661 38,284 Full Python 88, Table 5: Instruction data mix used to prepare the baseline instruction dataset in Section 3.2. D.2 PROCEDURES AND HYPERPARAMETERS We instruction finetune all models with Microsoft DeepSpeed using the ZeRO++ protocol for stage three sharding. For the largest of these models, we also use CPU parameter offloading to accelerate experiments (Wang et al., 2023a; Ren et al., 2021). When finetuning models on LintSeq data, we add new token <diff> to tokenizers (Section 2.4) and resize model embeddings accordingly. In our experiments with Gemma 2, Phi-3, and Llama 3.1 models, we use HuggingFace to access and load pretrained model weights and tokenizers. As mentioned in the main body of the paper, we instruction finetune pretrained-only weights if open-sourced and available. This is the case for Gemma 2 and Llama 3.1 only, as of the writing of this paper. Across all of the finetuning experiments conducted in this paper, we train model-data variants with the same batch size and for an equal number of total optimizer steps. This optimizer step count corresponds to ten epochs of finetuning with the baseline instruction tuning dataset described in Section 3.2. We save intermediate checkpoints at equal optimizer step intervals in all experiments. In order to tune the peak learning rates used in each set of model experiments, we run full sweep α {6e-4, 3e-4, 1e-4, 5e-5, 1e-5, 5e-6} in the baseline instruction data setting for each model. We select peak learning rate values by tracking the best-achieved downstream benchmark performance across models. The chosen values are displayed in Table 6. All other finetuning hyperparameters are kept fixed at the settings in Table 7 across experiments. TinyCodeLM Gemma 2 Phi-3 Llama 3.1 Peak Learning Rate (α) 3e-4 3e5e-5 5e-5 1e-5 150M 400M 2B 3.8B 14B 8B 1e-5 Table 6: Peak learning rates used to instruction finetune models. Learning Rate Scheduler Warmup Ratio Weight Decay Total Batch Size Batch Loss Reduction Mixed Precision Max Sequence Length Total Optimizer Steps Hyperparameter Setting linear 0.001 0.01 512 sum BFLOAT16 1024 1740 Table 7: All other instruction finetuning settings, re-used across experiments."
        },
        {
            "title": "E MORE ON SYNTHETIC DATA GENERATION WITH LINTSEQ",
            "content": "E.1 EXAMPLES OF GENERATED SYNTHETIC EDIT TRAJECTORIES Figure 9: LintSeq edit sequence samples vs baseline instruction-program data, example A. Figure 10: LintSeq edit sequence samples vs baseline instruction-program data, example B. E.2 TUNING LINTSEQ EXAMPLE COUNT Figure 11: Probing the effect of varying the number of edit sequences sampled with LintSeq per instruction-example pair during data generation: Using the source dataset described in Section 3.2, we sweep over the value of the LintSeq parameter used during synthetic data generation to yield three different edit sequence instruction datasets with {1, 5, 10}. We finetune TinyCodeLM models on each of these datasets, and compare the resultant HumanEval and MBPP(+) performance vs samples (i.e. pass@k vs k) at temperature 1. The most performant values is = 5."
        },
        {
            "title": "F ADDITIONAL RESULTS",
            "content": "F.1 PRETRAINING TINYCODELM Figure 12: Evaluating the zero-shot Python synthesis capabilities of TinyCodeLM-150M during pretraining on HumanEval and MBPP(+). Figure 13: Evaluating the zero-shot Python synthesis capabilities of TinyCodeLM-400M during pretraining on HumanEval and MBPP(+). 21 F.2 FINETUNING TINYCODELM F.2. INFERENCE-TIME SCALING LAWS Table 8: HumanEval fixed-temperature coverage scaling results achieved by finetuning TinyCodeLM models (zero-shot, temperature = 1, top-p = 0.95). HumanEval Model Variant tinycodeLM-Instruct tinycodeLM-RandSeqInstruct tinycodeLM-LintSeqInstruct tinycodeLM-Instruct tinycodeLM-RandSeqInstruct tinycodeLM-LintSeqInstruct Size 150M 150M 150M 400M 400M 400M Linter Guided - - pass@1 6.2 4.0 6.4 6.8 7.2 7.2 pass@5 10.4 9.7 14. 11.4 12.7 14.9 pass@10 12.3 11.9 17.7 pass@20 14.5 14.4 20.4 13.7 15.6 18.2 16.2 18.6 21.5 pass@50 18.3 17.7 (-0.6) 22.6 (+4.3) 18.9 22.0 (+3.1) 26.8 ( +7.9) Table 9: MBPP(+) fixed-temperature coverage scaling results achieved by finetuning TinyCodeLM models (zero-shot, temperature = 1, top-p = 0.95). MBPP(+) Model Variant tinycodeLM-Instruct tinycodeLM-RandSeqInstruct tinycodeLM-LintSeqInstruct tinycodeLM-Instruct tinycodeLM-RandSeqInstruct tinycodeLM-LintSeqInstruct Size 150M 150M 150M 400M 400M 400M Linter Guided - - pass@1 7.3 4.3 7.5 9.3 5.5 13.1 pass@5 16.4 13.1 18. 17.9 15.9 25.2 pass@10 20.4 18.4 23.4 pass@20 24.6 23.7 28.5 22.2 21.5 29.9 26.8 27.0 34.4 pass@50 30.6 30.2 (-0.4) 34.5 (+3.9) 32.4 34.5 (+2.1) 39.6 (+7.2) 22 F.3 FINETUNING GEMMA 2, PHI-3, AND LLAMA 3.1 F.3.1 INFERENCE-TIME SCALING LAWS Table 10: HumanEval fixed-temperature coverage scaling results achieved by finetuning Gemma 2, Phi-3, and Llama 3.1 models (zero-shot, temperature = 1, top-p = 0.95). HumanEval Model Variant Gemma-2-Instruct Gemma-2-LintSeqInstruct Parameters 2.6B 2.6B pass@1 16.1 21.6 pass@5 26.3 35.0 pass@10 30.5 41. pass@20 34.6 48.3 Phi-3-Mini-Instruct Phi-3-Mini-LintSeqInstruct Llama-3.1-Instruct Llama-3.1-LintSeqInstruct Phi-3-Med-Instruct Phi-3-Med-LintSeqInstruct 3.8B 3.8B 8B 8B 14B 14B 40.1 38.3 34.2 37.6 52.8 44.7 48.8 63.6 50.3 61. 67.9 73.5 52.1 72.0 55.6 69.2 72.0 80.2 55.0 79.0 59.7 75. 75.0 84.5 pass@50 40.2 58.5 (+18.3) 57.3 86.6 (+29.3) 63.4 82.3 (+18.9) 77.4 90.2 (+12.8) Table 11: MBPP(+) fixed-temperature coverage scaling results achieved by finetuning Gemma 2, Phi-3, and Llama 3.1 models (zero-shot, temperature = 1, top-p = 0.95). MBPP(+) Model Variant Gemma-2-Instruct Gemma-2-LintSeqInstruct Parameters 2.6B 2.6B pass@1 22.8 27.7 pass@5 34.2 41.3 pass@10 37.8 46. pass@20 41.2 50.5 Phi-3-Mini-Instruct Phi-3-Mini-LintSeqInstruct Llama-3.1-Instruct Llama-3.1-LintSeqInstruct Phi-3-Med-Instruct Phi-3-Med-LintSeqInstruct 3.8B 3.8B 8B 8B 14B 14B 35.2 39.3 38.4 38.5 40.3 40.0 45.9 57.5 50.2 56. 51.5 60.9 49.1 62.4 53.8 61.6 54.2 67.0 52.0 65.9 56.6 65. 56.3 71.5 pass@50 44.6 54.7 (+10.1) 56.1 69.4 (+13.3) 59.4 69.8 (+10.4) 59.0 75.9 (+16.9) F.4 COMPUTING HUMANEVAL COVERAGE VS CUMULATIVE INFERENCE-TIME FLOPS In Figure 1, we plot HumanEval coverage as function of cumulative inference-time FLOPs, comparing the performance and total cost of repeatedly sampling from our instruction finetuned Phi-3 and Llama 3.1 models vs sampling single generation per problem from very large models like Llama 3.1 405B (Dubey et al., 2024) and Nemotron 4 340B (Adler et al., 2024). We use the approximations below, drawn from Kaplan et al. (2020), to conservatively estimate the cumulative inference costs of synthesizing solutions to all of the 164 HumanEval benchmark problems across different models. The models that we compare are all dense transformers, where the majority of the parameters are used in matrix multiplications. FLOPs per token 2 (Nmodel-params + 2 Lmodel-layers Ccontext) Total FLOPs FLOPs per token Tavg-total-tokens-per-sample Ksamples Mproblems FLOPs per token Tavg-total-tokens-per-sample Ksamples For our instruction finetuned models, we determine the quantities Tavg-total-tokens-per-sample by computing token counts over all sets of samples per problem that we obtained to compute the coverage statistics in Figure 4 and Table 10 above. These token statistics are provided in the table below. Table 12: HumanEval total tokens per sample for finetuned Gemma 2, Phi-3, and Llama 3.1 models (zero-shot, temperature = 1, top-p = 0.95). These counts reflect prompt and completion tokens. They are computed using the same samples whose coverage statistics are reported in Table 10. Model Variant Gemma-2-Instruct Gemma-2-LintSeqInstruct Parameters 2.6B 2.6B Avg. Total Tokens Per HumanEval Sample (n = 50) 172 205 Phi-3-Mini-Instruct Phi-3-Mini-LintSeqInstruct Llama-3.1-Instruct Llama-3.1-LintSeqInstruct Phi-3-Med-Instruct Phi-3-Med-LintSeqInstruct 3.8B 3.8B 8B 8B 14B 14B 218 178 201 230 247 Note that edit sequence (i.e. LintSeqInstruct finetuned) LMs have slightly higher average token counts per sample due to presence of diff descriptor tokens in generations (see Appendix A). We report zero-shot HumanEval coverage for external models by using the evaluation statistics from Dubey et al. (2024) (Table 18, column one). To estimate cumulative inference-time FLOPs for these models, we employ the approximation expression above and estimate Tavg-total-tokens-per-sample 200, reflecting an ensemble over the per sample token counts of standard Instruct finetuned models shown in Table 12. Note that this ensembled statistic reflects program generations without chain-of-thought only. As result, we believe it to be conservative estimator."
        }
    ],
    "affiliations": [
        "New York University"
    ]
}