{
    "paper_title": "COIG-Writer: A High-Quality Dataset for Chinese Creative Writing with Thought Processes",
    "authors": [
        "Yunwen Li",
        "Shuangshuang Ying",
        "Xingwei Qu",
        "Xin Li",
        "Sheng Jin",
        "Minghao Liu",
        "Zhoufutu Wen",
        "Tianyu Zheng",
        "Xeron Du",
        "Qiguang Chen",
        "Jiajun Shi",
        "Wangchunshu Zhou",
        "Jiazhan Feng",
        "Wanjun Zhong",
        "Libo Qin",
        "Stephen Huang",
        "Wanxiang Che",
        "Chenghua Lin",
        "Eli Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models exhibit systematic deficiencies in creative writing, particularly in non-English contexts where training data is scarce and lacks process-level supervision. We present COIG-Writer, a novel Chinese creative writing dataset that captures both diverse outputs and their underlying thought processes through systematic reverse-engineering of high-quality texts. Unlike existing datasets that provide only input-output pairs, COIG-Writer comprises 1,665 meticulously curated triplets spanning 51 genres, each containing: (1) a reverse-engineered prompt, (2) detailed creative reasoning documenting decision-making processes, and (3) the final text. Through comprehensive experiments, we identify a two-component model of creative writing: narrative logic (provided by process supervision) and linguistic expression (maintained by general-purpose data). Our findings reveal three critical insights: (1) Process supervision is highly effective but requires stabilization with general data. A ratio of at least one creative sample to twelve general samples is needed to achieve optimal performance; below this threshold, the win rate progressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities are culturally-bound with no cross-lingual transfer (89.26pp gap between Chinese and English performance), and (3) lexical diversity inversely correlates with creative quality (TTR paradox), suggesting high diversity signals compensatory behavior for logical deficiencies. These findings establish that creative excellence emerges from the interaction between logical scaffolding and linguistic grounding, analogous to how mathematical reasoning enhances but cannot replace linguistic competence in foundation models."
        },
        {
            "title": "Start",
            "content": "COIG-Writer: High-Quality Dataset for Chinese Creative Writing with Thought Processes M-A-P, 2077AI"
        },
        {
            "title": "Abstract",
            "content": "5 2 0 2 6 1 ] . [ 1 3 6 7 4 1 . 0 1 5 2 : r Large language models exhibit systematic deficiencies in creative writing, particularly in nonEnglish contexts where training data is scarce and lacks process-level supervision. We present COIG-Writer, novel Chinese creative writing dataset that captures both diverse outputs and their underlying thought processes through systematic reverse-engineering of high-quality texts. Unlike existing datasets that provide only input-output pairs, COIG-Writer comprises 1,665 meticulously curated triplets spanning 51 genres, each containing: (1) reverse-engineered prompt, (2) detailed creative reasoning documenting decision-making processes, and (3) the final text. Through comprehensive experiments, we identify two-component model of creative writing: narrative logic (provided by process supervision) and linguistic expression (maintained by general-purpose data). Our findings reveal three critical insights: (1) Process supervision is highly effective but requires stabilisation with general data. ratio of at least one creative sample to twelve general samples is needed to achieve optimal performance; below this threshold, the win rate progressively degrades (from 62.75% down to 35.78%)., (2) creative capabilities are culturally-bound with no cross-lingual transfer (89.26pp gap between Chinese and English performance), and (3) lexical diversity inversely correlates with creative quality (TTR paradox), suggesting high diversity signals compensatory behavior for logical deficiencies. These findings establish that creative excellence emerges from the interaction between logical scaffolding and linguistic grounding, analogous to how mathematical reasoning enhances but cannot replace linguistic competence in foundation models. Project Homepage: https://COIG-Writer.github.io/ 1. Introduction Process supervision has transformed structured reasoning, for example, pushing math competition benchmarks to about 93% accuracy [20, 23] and enhancing multi-step reasoning [18, 28], yet creative writing, which accounts for about 40% of LLM applications [1, 24], lacks comparable methodological advances. We hypothesize this gap stems from fundamental misunderstanding: creative writing is not monolithic but compositional, requiring both narrative logic (structural planning) and linguistic expression (stylistic realization). Current creative writing models exhibit systematic failures across three dimensions. First, narrative structures converge to predictable templatesrepetitive narratives with limited variation dominate outputs [30]. Second, stylistic diversity collapsesdistinct authorial voices homogenize into what practitioners term AI flavor [5]. Third, cultural authenticity deteriorates catastrophically in non-English contextsChinese models produce Western narrative Figure 1. Overview of COIG-Writer construction and evaluation. Stage 1 (Data Construction): High-quality Chinese texts spanning 51 genres are collected and filtered, followed by expert reverse-engineering to extract creative prompts and reasoning processes, yielding 1,665 validated triplets. Stage 2 (Human Evaluation): Models trained on COIG-Writer undergo rigorous human preference evaluation through pairwise comparisons, with analysis of win rates and lexical diversity (TTR) to assess creative writing quality. structures with superficial cultural markers rather than authentic qi-cheng-zhuan-he (beginningdevelopment-turn-conclusion) progression [7]. We introduce COIG-Writer, Chinese creative writing dataset that uniquely captures the reasoning process underlying creative decisions. Our 1,665 expert-curated triplets span 51 genres, each containing: (1) reverse-engineered prompts, (2) detailed creative reasoning chains, and (3) final texts. While existing datasets prioritize either scale (e.g. WritingPrompts [9] (300K samples), ROCStories [22] (100K samples)) or breadth (e.g. COIG [35] (67K samples), LCCC [27] (12M samples)), they provide only input-output pairs without process data. COIG-Writer uniquely combines multi-genre coverage with explicit reasoning chains, enabling process-level learning of creative decision-making. Figure 1 illustrates our two-stage construction pipeline: (i) systematic collection and filtering of high-quality texts, followed by (ii) expert reverse-engineering to extract the implicit creative reasoning. Our experiments reveal three key findings: (1) Process supervision achieves 62.75% win rate in Chinese creative writing, but this requires stabilization ratio of approximately one creative-process sample to twelve general-purpose samples. Below this threshold, performance degrades monotonically (with win rates rising from 35.78% to 62.75% as the ratio is approached). (2) No cross-lingual transfer occurs: English performance drops to 46.46%, with pure COIGWriter models generating Chinese text for 12.18% of English prompts. (3) Lexical diversity inversely correlates with qualityhighest Type-Token Ratio (TTR) (0.678) corresponds to lowest preference scores (37.25%). These findings support two-component model of creative writing: narrative logic (enhanced by process supervision) and linguistic expression (maintained by general data). Neither component alone sufficesthe optimal configuration requires both. Contributions: Reverse-engineering methodology: We develop systematic approach to extract reasoning chains from high-quality texts through multi-stage validation (LLM filtering + expert annotation). The methodology achieves 70% acceptance rate and generalizes to other creative domains. COIG-Writer dataset: 1,665 Chinese creative writing triplets spanning 51 genres, with average lengths of 283/1,089/2,214 characters (prompt/reasoning/article). Each triplet 2 undergoes 6-dimensional quality evaluation (score 50), representing expert annotations. Empirical validation of compositional hypothesis: Through controlled experiments, we demonstrate: (1) process supervision improves Chinese creative writing from 35.78% to 62.75% but requires stabilization ratio of approximately 1:12 (creative to general samples), (2) creative capabilities are language-specific with 16.29% performance gap between Chinese and English, and (3) lexical diversity inversely correlates with quality (TTR paradox). 2. The COIG-Writer Dataset Figure 2. The data curation pipeline of COIG-Writer. Our methodology consists of three main stages: (1) Genre scope definition through expert consultation, (2) Multi-stage source text collection and filtering, and (3) Reverse-engineering of thought processes with comprehensive quality control. To address these challenges, we introduce COIG-Writer, Chinese creative writing dataset that uniquely captures the reasoning process underlying creative decisions. Our 1,665 expertcurated triplets span 51 genres, each containing: (1) reverse-engineered prompts, (2) detailed creative reasoning chains, and (3) final texts. In the following section, we will elaborate on the datasets construction methodology, covering our systematic text collection, the reverseengineering of creative thought processes, and the multi-stage quality assurance pipeline While existing datasets prioritize either scale or breadth, they provide only input-output pairs without process data. COIG-Writer uniquely combines multi-genre coverage with explicit reasoning chains, enabling process-level learning of creative decision-making. Figure 1 illustrates our two-stage construction pipeline: (i) systematic collection and filtering of high-quality texts, followed by (ii) expert reverse-engineering to extract the implicit creative reasoning. 3 2.1. Data Collection Methodology Genre Taxonomy and Scope Definition. We established our genre taxonomy by aggregating categories from writing websites, merging categories, and removing duplicates. The selection process followed two core principles: (1) representational diversity to capture the rich spectrum of Chinese literary traditions, and (2) practical relevance to include contemporary forms with real-world applications. Our final taxonomy encompasses 51 specific genres organized across eight primary domains: Functional Writing (e.g., proposal planning, tutorial guides), Communicative Writing (e.g., social media content, advertising copy), Non-fictional Writing (e.g., essays, reviews), Fiction (spanning traditional genres like Wuxia to modern science fiction), Internet Culture (e.g., subcultural expressions, fan fiction), Poetry (classical and contemporary forms), Scripts (drama, debate), and Role-playing Writing (character-driven narratives). Annotator Recruitment and Training. We recruited 100 university students from diverse academic backgrounds, including literature and linguistics programs, humanities disciplines, and STEM fields. This interdisciplinary composition ensures broad perspective coverage while maintaining literary sensitivity. All annotators underwent standardized 8-hour training program covering: (1) quality assessment criteria, (2) reverse-engineering techniques, (3) reasoning process articulation, and (4) cultural sensitivity guidelines. Source Text Collection and Initial Filtering. Source texts were systematically collected from diverse online platforms including literary forums, social media platforms, professional blogs, and cultural websites. To ensure temporal relevance and avoid potential contamination with foundation model training data, we strictly limited collection to content published after October 2022, verified through rigorous URL tracing and cross-platform timestamp validation. Each collected text underwent five-dimensional initial assessment: Content Completeness (structural integrity), Format Standardization (presentation quality), Error Correction (linguistic accuracy), Logical Consistency (narrative coherence), and Creativity Assessment (originality and engagement). Texts were further evaluated using engagement metrics (likes, shares, comments) as proxy indicators for quality and appeal. Automated Quality Screening. We developed specialized LLM-based quality screening system using carefully designed prompts with Qwen3-235B-A22B [31]. The system evaluates texts across two dimensions through structured prompting: Article_quality (linguistic fluency, structural coherence, factual accuracy) and Article_creativity (originality, expressiveness, cultural resonance). 2.2. Thought Process Construction Reverse-Engineering Methodology. For each qualified article, annotators employed our systematic three-step reverse-engineering protocol to extract the implicit creative reasoning underlying the final text: (1) Prompt Reconstruction. Annotators analyze the articles core attributes across multiple dimensions: thematic focus (subject matter and conceptual depth), stylistic markers (tone, voice, linguistic register), structural organization (narrative flow, argument structure), and cultural grounding (idioms, references, contextual assumptions). Based on this analysis, annotators reverse-engineer plausible prompt that could have inspired the article. The reconstructed prompt must balance two competing requirements: sufficient specificity to constrain the creative space toward the target output, while maintaining adequate interpretative freedom to enable genuine creative decision-making. (2) Reasoning Process Articulation. Using both the original article and reconstructed prompt, annotators systematically document the hypothetical creative decision-making pathway connecting initial inspiration to final output. The reasoning process must explicitly address five critical decision categories: (a) initial interpretation and planning (understanding the prompt, establishing creative goals, conceptualizing the overall approach), (b) structural and stylistic choices (organizational framework, narrative perspective, tonal register, rhetorical strategies), (c) cultural and contextual considerations (selection of culturally resonant elements, audience adaptation, contextual knowledge embedding), (d) narrative development strategies (plot progression techniques, character development, argument construction, thematic elaboration), and (e) revision and refinement thoughts (metacognitive reflections on improving coherence, enhancing impact, resolving tensions). (3) Coherence Validation. Each resulting triplet (Article, Reverse Inspiration Prompt, Reasoning Process) undergoes self-consistency checks to ensure logical coherence across all components. Validation criteria include: (i) the prompt plausibly motivates the articles characteristics without being overly deterministic, (ii) each reasoning step logically follows from the prompt and previous decisions, (iii) the reasoning provides sufficient justification for major creative choices in the final article, and (iv) no contradictions exist between prompt requirements, reasoning explanations, and article content. Triplets failing these checks are flagged for iterative refinement. Multi-Dimensional Quality Evaluation. Each data triplet undergoes systematic evaluation across six interdependent dimensions spanning three components: Article (quality: fluency, coherence, cultural appropriateness; creativity: originality, expressiveness, engagement), Prompt (quality: clarity, specificity, generative potential; creativity: innovation, cultural grounding, complexity), and Reasoning (quality: logical consistency, completeness, clarity; creativity: insight depth, decision justification, authenticity). These dimensions enforce cohesionpoor article-reasoning alignment directly impacts quality scores. Triplets must satisfy dual thresholds to advance: cumulative score 50 and individual dimension scores 8, ensuring both overall excellence and consistent quality across all facets. 2.3. Quality Assurance and Final Validation Human-in-the-Loop Validation. Eight graduate-level domain experts in Chinese literature conducted manual validation following standardized calibration sessions. Each triplet underwent tiered review based on complexity: 2 reviewers for standard samples, 4 for samples requiring specialized cultural or stylistic knowledge. Review criteria encompassed: (i) semantic consistency across the triplet components, (ii) cultural and linguistic authenticity, (iii) reasoning process coherence, and (iv) contribution to genre diversity. Initial review achieved 70% acceptance rate, with rejected samples entering iterative refinement unless they contained factual errors (e.g., anachronistic references) or violated content guidelines, which warranted removal. This multi-stage validation pipeline produced final corpus of 1,665 verified triplets. Bias Mitigation and Diversity Assurance. We implemented five strategies to ensure dataset diversity: (1) balanced genre representation with minimum 15 samples per category, (2) geographic diversity across source platforms, (3) temporal spread throughout the collection period, (4) stylistic variety within each genre, and (5) regular bias audits during curation. These measures minimize systematic bias and promote equitable representation across all dimensions. 5 2.4. Evaluation Benchmark Construction We constructed comprehensive benchmark to systematically evaluate model performance on creative writing tasks. Test Query Development. Two computational linguistics postgraduate students developed 104 evaluation queries covering all 51 genres (minimum two queries per genre). Each query specifies three elements: target genre, creative constraints (length, style, theme), and cultural/contextual requirements. Our expert panel validated all queries for clarity, precision, and appropriate difficulty levels. Human Evaluation Protocol. Four trained graduate evaluators assessed model outputs using standardized 4-point scale (03) across five dimensions: Content Quality, Creative Merit, Cultural Appropriateness, Task Fulfillment, and Overall Preference. To ensure consistency, each evaluator assessed outputs from five specific models, achieving high inter-rater agreement and minimizing evaluation bias. (a) Main category distribution (b) Length distributions Figure 3. Dataset composition of COIG-Writer. (a) Distribution across 7 main categories encompassing 51 specific genres. Communication (28.9%) and Novel (28.0%) constitute the majority, followed by Non-fiction (14.6%) and Functional Writing (13.3%). (b) Length distributions for prompts (Query), reasoning processes (Thought), and articles (Answer) demonstrate the varying complexity across the dataset. 2.5. Dataset Statistics and Analysis COIG-Writer contains 1,665 high-quality triplets with substantial diversity. Average character lengths are 283 for prompts, 1,089 for reasoning processes, and 2,214 for articles, with maximum article length reaching 31,071 characters. The dataset spans 7 main categories with 51 specific genres. Communication and Novel categories each represent 30% of the dataset, followed by Non-fiction (14.6%) and Functional Writing (13.3%), as shown in Figure3a. Genres include poetry, social media content, fiction, and specialized forms like Xianxia and military novels (see Appendix B). Length distributions (Figure 3b) show articles ranging from 1231,071 characters, reasoning processes from 2524,094 characters, and prompts from 302,642 characters. This logarithmic distribution, concentrated between 10010,000 characters, reflects natural variation in creative writing genres and enables learning from both concise and elaborate examples. 6 3. Experiments and Analysis 3.1. Experimental Setup Model Configurations. We investigate five configurations that systematically vary the ratio of COIG-Writer data (DCW, 1,665 samples) to general-purpose data (DG). This design enables us to empirically determine the stabilization threshold required for process supervision to enhance creative writing capabilities. To ensure cross-lingual stability, we construct DG from two complementary sources: 10k Chinese samples from the DeepSeek-R1 distilled dataset [21] and 10k English samples from OpenThoughts [14], large-scale reasoning dataset. This yields balanced bilingual pool of 20k samples. We create training mixtures by sampling equal amounts from each language source: 1k per language (2k total), 5k per language (10k total), and 10k per language (20k total, using the complete pool). Table 1 summarizes the five experimental configurations, with DG quantities selected to span ratios from 1:1.2 to 1:12 (creative to general samples). Table 1. Training configurations and data composition."
        },
        {
            "title": "Model",
            "content": "COIG-Writer General Total MCW MCW+1k MCW+5k MCW+10k MG 1,665 1,665 1,665 1,665 0 0 2,000 10,000 20,000 20,000 1,665 3,665 11,665 21,665 20,000 Training Configuration. All models initialize from Qwen2.5-7B-Instruct [31] and undergo supervised fine-tuning for 3 epochs. We employ AdamW optimizer with learning rate ùúÇ = 2 105, global batch size ùêµ = 32, and linear learning rate warmup over 10% of training steps. The maximum sequence length is set to 8,192 tokens. Evaluation Protocol. We construct comprehensive evaluation benchmark consisting of 557 test queries spanning all 51 genres in our taxonomy, with 204 Chinese queries and 353 English queries. Each query specifies the target genre, creative constraints (style, theme), and cultural or contextual requirements. Human evaluation follows rigorous pairwise comparison protocol. Four graduate-level evaluators (separate from the annotation team) assess model outputs using blind comparisons, where neither the model identities nor the training configurations are disclosed. 3.2. Main Results Human Preference Evaluation. Table 2 reports pairwise win rates across model configurations. Chinese Creative Writing: Substantial Effectiveness of COIG-Writer. For Chinese creative writingthe native language of the COIG-Writer datasetour results demonstrate substantial effectiveness. MCW+10k achieves statistically significant win rate of 62.75% against the baseline MG (ùëù < 0.001), establishing it as the only configuration to meaningfully outperform general-purpose training. This 25.5 percentage point improvement represents substantial gain attributable to the specialized creative writing data when properly balanced with generalpurpose samples. 7 Table 2. Pairwise win rates (%) on creative writing tasks. Bold values indicate win rates > 55%. Each cell (ùëñ, ùëó) shows win rate of row ùëñ vs. column ùëó. Chinese (Original Dataset Language) English (Cross-lingual Transfer) Model MCW MCW+1k MCW+5k MCW+10k MG MCW MCW+1k MCW+5k MCW+10k MG MCW MCW+1k MCW+5k MCW+10k MG 60.78 67.65 74.02 64. 39.22 60.29 67.65 57.84 32.35 39.71 58.33 50.00 25.98 32.35 41.67 37.25 35.78 42.16 50.00 62.75 61.47 72.80 75.92 76.49 38.53 64.59 67.71 69. 27.20 35.41 50.71 57.79 24.08 32.29 49.29 53.54 23.51 30.03 42.21 46.46 (a) Chinese results (b) English results Figure 4. Distribution of character counts across model variants. Box plots show median, IQR (box), whiskers (1.5IQR), and outliers (dots). Both languages show MCW producing shortest outputs, with Chinese texts generally shorter due to character density. Performance exhibits monotonic improvement with increasing general data proportions: MCW+5k reaches parity (50.00%), while MCW+1k and MCW underperform at 42.16% and 35.78% respectively. This pattern suggests critical threshold of approximately 20k general samples (1:12 ratio of creative to general data) necessary to stabilize the creative enhancements introduced by specialized data. The strong performance in Chinese validates the effectiveness of processsupervised creative writing data for the language domain it was designed for. English Creative Writing: Limited Cross-lingual Transfer. In contrast, English results demonstrate limited cross-lingual transfer of creative writing capabilities. The baseline MG maintains dominance with win rates ranging from 53.54% against MCW+10k to 76.49% against MCW. The monotonic improvement with increasing general data (from 23.51% to 46.46%) indicates that Chinese-centric creative data, while highly effective for its native language, does not transfer effectively to English generation. The Two-Component Model of Creative Writing. Our results reveal that creative writing quality emerges from two distinct components that must be balanced: Narrative Logic. Provided by COIG-Writer through explicit reasoning chains, enabling coherent plot development, consistent character behavior, and structured storytelling. This component ensures logical connections between paragraphs and maintains thematic consistency. 8 Linguistic Expression. Maintained by general-purpose data, ensuring natural phrasing, stylistic fluency, and cultural idiomaticity. This component provides the surface realization that makes text feel naturally written rather than artificially generated. The failure of MCW (35.78% win rate) demonstrates that logic alone is insufficientqualitative analysis reveals well-structured narratives expressed in stilted, unnatural language. Conversely, MGs fluent surface but poor performance indicates that linguistic variety without logical scaffolding produces what annotators described as logical disconnection between paragraphs despite fluent expressionbeautiful nonsense that reads well locally but lacks global coherence. Generation Length Analysis. Table 3 and Figure 4 present output length characteristics across model variants. For Chinese generation, MCW+10k produces outputs of comparable length to the baseline (1,120.2 vs 1,137.3 characters) while achieving superior win rates, indicating that performance gains stem from content quality rather than mere verbosity. The MCW and MCW+1k models generate substantially shorter outputs (960.4 and 949.7 characters respectively), correlating with their inferior performance. In English tasks, the baseline produces the longest outputs (4,069.9 characters) and achieves highest win rates, suggesting positive correlation between generation length and quality in this domain. The MCW model generates the shortest responses (3,037.8 characters, 25.4% fewer than baseline), corresponding with its poorest performance (23.51% win rate). Notably, while MCW+10k approaches baseline length (98.3% of baseline characters), it still underperforms in preference evaluations, indicating that factors beyond lengthlikely coherence and cultural appropriatenessdetermine English generation quality. Table 3. Average generation length across model configurations. Chinese English Model Tokens Chars Tokens Chars MCW MCW+1k MCW+5k MCW+10k MG 606.9 602.9 699.4 710.7 730.3 960 950 1,099 1,120 1,137 1,195 1,382 1,533 1,577 1,577 3,038 3,690 3,988 4,002 4, The distribution analysis (Figure 4) reveals that variance in output length decreases as more general data is incorporated, with MCW exhibiting the highest variability across both languages. This suggests that specialized creative data alone leads to less predictable generation behavior, while mixing with general data stabilizes output characteristics. Table 4. Type-Token Ratio analysis reveals inverse correlation with creative quality. Chinese English Model Mean Median Mean Median MCW MCW+1k MCW+5k MCW+10k MG 0.522 0.578 0.576 0.593 0.678 0.513 0.570 0.576 0.586 0.671 0.562 0.571 0.574 0.590 0.590 0.515 0.554 0.561 0.579 0.571 (a) Chinese: wide TTR range (0.5220.678) (b) English: narrow TTR range (0.5620.590) Figure 5. The TTR Paradox. Higher lexical diversity correlates with lower creative quality. MG achieves highest TTR (0.678) but loses to MCW+10k (TTR = 0.593) with only 37.25% win rate, challenging conventional assumptions about diversity metrics. Lexical Diversity Analysis. We measure Type-Token Ratio (TTR) to test whether lexical diversity correlates with generation quality. For Chinese text, we apply jieba segmentation before computing TTR. Table 4 and Figure 5 reveal an inverse correlation between lexical diversity and quality. In Chinese, MG shows highest TTR (0.678) but lowest win rate (37.25%) against MCW+10k (TTR=0.593). For English, despite identical TTR (0.590), MCW+10k underperforms by 7 percentage points. This inverse relationship aligns with our two-component model: high TTR in MG indicates vocabulary variation without narrative coherencemanual inspection reveals frequent topic shifts and inconsistent terminology. Lower TTR in MCW+10k reflects deliberate term reuse for thematic consistency. 3.3. Qualitative Analysis Coherence and Instruction Adherence. Manual inspection of 557 test samples reveals systematic failure modes. For Chinese tasks, MG exhibits logical disconnection between paragraphs despite fluent surface form, while MCW produces unformatted text blocks without proper segmentation. MCW+10k successfully maintains narrative coherence while following complex instructions. In the \"Wu Song Fights Tiger\" reinterpretation task requiring critical commentary, MCW+10k correctly incorporates the meta-narrative critique, while MCW defaults to literal retelling and MG generates tangentially related content. Cross-Lingual Contamination. critical failure emerges in English generation: MCW produces Chinese text in 12.18% of English prompts (43/353), compared to 1.13% for MCW+10k and 1.42% for MG. Contamination correlates inversely with general data proportion, with intermediate rates for MCW+1k (1.70%) and MCW+5k (1.42%). Genre-Specific Performance. Performance varies significantly across 51 genres. Abstract tasks (homophonic wordplay \"XiLaNai\", experimental \"crazy literature\") fail across all models with <15% success rate, producing overly formal outputs lacking stylistic authenticity. Structured formats show differential improvement: advertisements and slogans benefit from MCW+10ks incorporation of classical poetry and idioms, while MCW+1k and MCW+5k produce simplified 10 vocabulary. Technical genres (\"instruction manuals\", \"proposals\") show no distinguishable quality differences in human evaluation. 3.4. Discussion Our findings reveal compositional structure underlying creative writing capability, with important implications for data-scarce languages: Language-Specific Effectiveness and Data Scarcity. The divergent results between Chinese (62.75% win rate) and English (46.46% win rate) reflect fundamental differences in data availability rather than inherent limitations of process supervision. Chinese creative writing with explicit reasoning chains remains severely underrepresented in general pretraining corpora, making COIG-Writers 1,665 samples valuable and distinctive contribution. The 25.5 percentage point improvement in Chinese demonstrates that even relatively small specialized datasets can substantially enhance capabilities when they address genuine data gaps. Conversely, English general corpora already contain abundant creative writing examples from diverse sources (published literature, online fiction, creative writing communities), diminishing the marginal value of additional specialized data. This suggests that the effectiveness of domain-specific datasets should be evaluated relative to their representation in existing general-purpose corporaspecialized data provides maximal benefit for underrepresented domains and languages. Stabilization Threshold. The monotonic performance improvement (35.78%62.75%) with increasing general data establishes minimum 22k sample requirement for process supervision effectiveness in Chinese. This 1:12 ratio (creative:general data) suggests narrative logic forms necessary but minority component, analogous to how mathematical reasoning enhances but cannot replace linguistic competence [19]. Future work scaling the creative writing dataset alongside general data could further elucidate whether this ratio remains optimal across different dataset sizes. Cultural and Reasoning-Level Specificity. The performance gap between languages demonstrates that creative patterns are culturally encoded at the reasoning level, not merely at the vocabulary level. The 12.18% Chinese generation on English prompts by MCW indicates that Chinese narrative structures (four-character idioms, implicit progression) constitute incompatible features for English generation. This finding provides strong evidence that creative writing competencies are culturally and linguistically bound, contradicting hypotheses of universal creative skill transfer. TTR as Diagnostic. The inverse correlation between lexical diversity and quality reveals compensatory behavior: models lacking process supervision increase vocabulary variation to mask logical deficiencies. This suggests TTR could serve as an early warning for training imbalancesabnormally high diversity signaling insufficient narrative coherence. Implications. These findings suggest: (1) scaling creative datasets provides maximal benefit for underrepresented languages and domains where general corpora lack sufficient creative examples, (2) cross-lingual transfer requires reasoning-level adaptation beyond translation, particularly when source and target languages have divergent narrative conventions, and (3) evaluation should separately assess narrative logic and linguistic expression. The substantial effectiveness in Chinese validates process supervision as viable approach for enhancing creative capabilities in data-scarce scenarios. Limitations. Our experimental design varied general-purpose data quantities by sampling equal amounts from each language: +1k per language (2k total), +5k per language (10k total), and +10k per language (20k total, the complete bilingual pool), while holding the COIG-Writer dataset fixed at 1,665 samples. This design choice, while revealing the stabilization threshold for combining creative and general data, limits our ability to assess whether scaling the creative writing dataset itself would yield further improvements. Future work should investigate whether increasing COIG-Writer data beyond 1,665 samples could enhance Chinese performance or enable more effective cross-lingual transfer. Additionally, the current study cannot definitively distinguish whether the limited English effectiveness stems from the datasets Chinese-centric cultural framing or simply from insufficient creative writing examples for English. Larger-scale experiments varying both creative and general data quantities would provide clearer insights into the optimal data composition for creative writing tasks. 4. Related Work Creative Writing Datasets and Evaluation. English creative writing has benefited from substantial dataset development. The WritingPrompts dataset [9], has provided foundational data for hierarchical neural story generation. More recently, Fein et al. [10] introduced LitBench, the first standardized creative writing benchmark, featuring 2,480 human-labeled story comparisons and training corpus of 43,827 pairs. LitBench demonstrated that Bradley-Terry reward models outperform zero-shot large language model (LLM) evaluators (78% vs. 73% human agreement). However, existing English datasets such as ROCStories [22] and poetry corpora [12, 15] target specific genres or limited creative aspects, neglecting process-oriented data and cross-genre diversity. By contrast, high-quality Chinese creative writing resources are critically scarce. Existing datasets target general tasks: LCCC [27] provides 12M dialogue pairs, LCSTS [16] contains 2.4M summarization pairs, while instruction tuning datasets COIG [35] and COIG-CQIA [2] focus on general instruction-following rather than creative writing. Process-Oriented Learning and Creative Writing. Process supervision improves LLMs on tasks with explicit structure: chain-of-thought prompting [28], self-consistency [26], and zeroshot CoT [18]. However, creative writing requires long-horizon narrative control, stylistic decision-making, and culturally informed choices that go beyond stepwise logical inference [4]. Prior computational creativity methodsfrom rules/templates [3, 11, 29] to outline/plan-first pipelines [32, 34]mainly cover high-level structure rather than the fine-grained thought signals (e.g., motif development, pacing, voice) that guide human composition. Quality Issues and Evaluation Challenges. AI-generated creative writing consistently exhibits identifiable \"AI flavor,\" characterized by weak logical coherence [33], monolithic stylistic expression [8], superficial observations [25], inappropriate ornate vocabulary [17], and formulaic narratives [13]. These systematic issues suggest fundamental shortcomings in current training methods rather than mere scaling limitations. Furthermore, evaluating creative content remains inherently challenging. Traditional automatic metrics like BLEU and ROUGE fail to capture the diversity and nuanced qualities inherent in creative writing [9]. Human evaluation, while more accurate, is expensive, subjective, and difficult to scale [6]. Recent LLM-based evaluation approaches [36] partially address scalability but inherit biases from underlying models, especially 12 when assessing culturally-specific creative content. 5. Conclusion We present COIG-Writer, Chinese creative writing dataset of 1,665 triplets spanning 51 genres with reverse-engineered prompts, reasoning processes, and final texts. Our experiments reveal two-component model where narrative logic (from process supervision) and linguistic expression (from general data) must be balanced for quality generation. Three findings support this model: (1) Process supervision requires minimum 22k general samplesbelow this threshold, performance degrades monotonically (35.78%62.75%). (2) Creative capabilities are language-specific, with Chinese models achieving 62.75% win rate but only 46.46% in English. (3) Lexical diversity inversely correlates with qualityhighest TTR (0.678) yields lowest preference scores. These results demonstrate that creative excellence requires both logical scaffolding and linguistic grounding. While smaller than English datasets, COIG-Writer enables mechanism discovery rather than scale optimization. The identified compositional structure suggests future work should separately optimize narrative logic and linguistic expression rather than treating creativity as monolithic. Process supervision proves necessary but insufficienteffective creative AI requires careful balance between structure and expression."
        },
        {
            "title": "Contributions and Acknowledgements",
            "content": "Multimodal Art Projection (M-A-P) is non-profit open-source AI research community, ran by donation. The community members are working on research topics in wide range of spectrum, including but not limited to the pre-training paradigm of foundation models, largescale data collection and processing, and the derived applications on coding, reasoning and music generation. Core Contributors (Equal Contribution) Yunwen Li, CUHK-Shenzhen, M-A-P Shuangshuang Ying, M-A-P Xingwei Qu, The University of Manchester"
        },
        {
            "title": "Contributors",
            "content": "Xin Li, Nanyang Technological University Sheng Jin, Zhejiang University Minghao Liu, 2077AI, M-A-P Zhoufutu Wen, M-A-P Tianyu Zheng, M-A-P Xeron Du, M-A-P Qiguang Chen, Harbin Institute of Technology Jiajun Shi, M-A-P Wangchunshu Zhou, M-A-P Jiazhan Feng, M-A-P Wanjun Zhong, M-A-P"
        },
        {
            "title": "Advisors",
            "content": "Libo Qin, Central South University Stephen Huang, Peking University Wanxiang Che, Harbin Institute of Technology Chenghua Lin, The University of Manchester"
        },
        {
            "title": "Corresponding Authors",
            "content": "Eli Zhang, University of Waterloo Chenghua Lin, The University of Manchester"
        },
        {
            "title": "1 Anthropic. Anthropic economic index: Understanding ai‚Äôs effects on the economy, 2025.",
            "content": "URL https://www.anthropic.com/economic-index. Accessed: 2025-09-16."
        },
        {
            "title": "9 Angela Fan, Mike Lewis, and Yann Dauphin. Hierarchical neural story generation. arXiv",
            "content": "preprint arXiv:1805.04833, 2018."
        },
        {
            "title": "11 Pablo Gerv√°s. Computational approaches to storytelling and creativity. AI Magazine, 30(3):",
            "content": "4949, 2009."
        },
        {
            "title": "14 Etash Guha, Ryan Marten, Sedrick Keh, Negin Raoof, Georgios Smyrnis, Hritik Bansal,\nMarianna Nezhurina, Jean Mercat, Trung Vu, Zayne Sprague, Ashima Suvarna, Benjamin",
            "content": "15 Feuer, Liangyu Chen, Zaid Khan, Eric Frankel, Sachin Grover, Caroline Choi, Niklas Muennighoff, Shiye Su, Wanjia Zhao, John Yang, Shreyas Pimpalgaonkar, Kartik Sharma, Charlie Cheng-Jie Ji, Yichuan Deng, Sarah Pratt, Vivek Ramanujan, Jon Saad-Falcon, Jeffrey Li, Achal Dave, Alon Albalak, Kushal Arora, Blake Wulfe, Chinmay Hegde, Greg Durrett, Sewoong Oh, Mohit Bansal, Saadia Gabriel, Aditya Grover, Kai-Wei Chang, Vaishaal Shankar, Aaron Gokaslan, Mike A. Merrill, Tatsunori Hashimoto, Yejin Choi, Jenia Jitsev, Reinhard Heckel, Maheswaran Sathiamoorthy, Alexandros G. Dimakis, and Ludwig Schmidt. Openthoughts: Data recipes for reasoning models, 2025. URL https://arxiv.org/abs/2506.04178."
        },
        {
            "title": "16 Baotian Hu, Qingcai Chen, and Fangze Zhu. Lcsts: A large scale chinese short text summa-",
            "content": "rization dataset. arXiv preprint arXiv:1506.05865, 2015."
        },
        {
            "title": "23 OpenAI. Learning to reason with llms. Technical report, OpenAI, 2024. URL https:\n//openai.com/index/learning-to-reason-with-llms/. Accessed: 2024-09-12.\n24 OpenAI. How people are using chatgpt, 2025. URL https://openai.com/index/how-p",
            "content": "eople-are-using-chatgpt/. Accessed: 2025-09-15."
        },
        {
            "title": "25 Melissa Roemmele. Inspiration through observation: Demonstrating the influence of auto-",
            "content": "matically generated text on creative writing. arXiv preprint arXiv:2107.04007, 2021."
        },
        {
            "title": "29 Geraint A Wiggins. A preliminary framework for description, analysis and comparison of",
            "content": "creative systems. Knowledge-based systems, 19(7):449458, 2006."
        },
        {
            "title": "32 Kevin Yang and Dan Klein. Fudge: Controlled text generation with future discriminators.",
            "content": "arXiv preprint arXiv:2104.05218, 2021."
        },
        {
            "title": "33 Kevin Yang, Dan Klein, Nanyun Peng, and Yuandong Tian. Doc: Improving long story",
            "content": "coherence with detailed outline control. arXiv preprint arXiv:2212.10077, 2022."
        },
        {
            "title": "36 Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao\nZhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with\nmt-bench and chatbot arena. Advances in neural information processing systems, 36:46595‚Äì46623,\n2023.",
            "content": "17 A. Use of Large Language Models During the writing process, LLM was employed to polish and refine certain parts of the manuscript. The tool was used to improve sentence fluency and enhance clarity of expression, while preserving the original academic arguments and logical structure, thus ensuring that the overall language is more standardized and aligned with academic writing conventions. B. Complete Genre Distribution and Statistics This appendix provides comprehensive statistics for all 51 genres in the COIG-Writer dataset. Tables 510 present detailed breakdowns by category, including sample counts, percentages, and length distributions for Articles (A.L.), Reverse Inspiration Prompts (R.I.P.L.), and Reasoning Processes (R.P.L.). All lengths are measured in Chinese characters. Table 5. Overview statistics across main categories. Length values shown as (minmeanmax). Category Overall Communication Novel Non-fiction Functional Poetry Funny Literature Script Count % Article Length Prompt Length Reasoning Length 122,21431,071 1,665 100.0 121,5849,422 28.9 28.0 613,66931,071 14.6 2252,05212,766 1481,2819,057 13.3 38210695 7.7 4.1 1773012,117 3.4 3693,10813,339 481 467 243 221 128 68 302832,643 402861,754 413321,222 412261,718 412482,492 412031,118 301861,007 424052,643 2521,0894,094 3021,1623,816 3531,1352,964 4531,0662,661 4371,0854,094 4428672,935 2527031,826 5531,2073,647 Table 6. Funny Literature Genre Count % A.L. (minmeanmax) R.I.R.L. (minmeanmax) R.P.L. (minmeanmax) Funny Funny Subculture Esports Funny Fiction Anime/Manga Fan Fiction Subcultural Identity Expression Fan Circle Funny Literature Anti-Mainstream Consumption Funny Literature Internet Jargon Transnational/Crosslanguage Funny Literature 31 1.86% 171305.4512117 18 1.08% 241519.941008 8 0.48% 168506.25 30144.03691 99291.831007 67283.75542 252701.551826 476698.61923 351750.621033 4 0.24% 332486.75638 90153.25262 546794.75 3 0.18% 415543.67730 77108.67160 535627677 2 0.12% 194215 100153.5207 711774.5838 1 0.06% 1 0.06% 448448448 323323323 979797 105105105 536536536 643643 C. Reasoning Behavior Analysis To understand the mechanisms underlying performance differences between model configurations, we analyze reasoning patterns during generation by categorizing model behaviors into four types: normal writing, deep reasoning, self-exploration, and self-reflection. 18 Table 7. Communication Practical Writing Genre Count % A.L. (minmeanmax) R.I.R.L. (minmeanmax) R.P.L. (minmeanmax) Social Media Content Creation Advertising Copy Blog Post Debate Script Popular Science Speech Draft Slogan Product Review 124 7.45% 232426.58842 42383.491754 5451273.512940 74 4.44% 30410.424305 62 3.72% 4803164.379422 59 3.54% 5901167.632825 50 3.00% 1462178.746570 49 2.94% 7251974.927177 12261.231394 47 2.82% 16 0.96% 1912986.944635 44169.5708 302884.692978 129394.481461 6611150.451783 100257410 7741313.752320 44242.76751 6031053.043816 43329.161118 7281288.222485 446957.871718 40168.79570 57247.62641 7201315.312196 Table 8. Novel Genre Count % A.L. (minmeanmax) R.I.R.L. (minmeanmax) R.P.L. (minmeanmax) Fiction/Story Everyday Stories Costume Novels Mystery/Inference Stories Wuxia Novels Science Fiction Stories Xuanhuan Novels Fairy Tale Fantasy/Magic Stories Xianxia Novels Emotional Stories Military Novels Sports Novels Game Novels 104 6.25% 1223641.4527124 612129.528137 50 3.00% 40 2.40% 3446193.131071 31 1.86% 1104875.7119030 44349.191035 5821198.532964 41322.161222 6491217.422068 71442.58985 3531079.21785 42302.35765 4231176.352187 30 1.80% 11984586.7723797 28 1.68% 7793233.5712280 27 1.62% 4674925.0724706 25 1.50% 3641192.882544 2582925.649290 25 1.50% 24 1.44% 1704291.8820078 23 1.38% 3943119.2612147 23 1.38% 15153417.177095 22 1.32% 6223903.2710358 15 0.90% 10674309.9311492 49341.17586 6851126.932023 53293.36958 6031226.682935 90354.041005 3941192.522237 465772.961256 118198.52435 54283.08451 4171182.922098 498984.881881 6591131.31975 234362.13637 9391575.352784 72411221681 6291095.81506 177340892 253422.67 89385.421106 45263.7784 Chinese Reasoning Patterns. As illustrated in Figure 7, models trained with COIG-Writer data demonstrate significantly enhanced reasoning capabilities. The MCW+10k configuration exhibits balanced distributions across all reasoning types, with increased frequencies of deep reasoning and self-exploration phases compared to the baseline. This balanced reasoning profile correlates with superior creative performance, suggesting that explicit process supervision enables models to engage in more sophisticated creative planning and reflection. English Reasoning Patterns. Figure 8 reveals contrasting patterns for English generation. The baseline model maintains consistent deep reasoning throughout generation, while COIGWriter variants exhibit disrupted reasoning flows with excessive self-reflection but limited deep reasoning. This misalignment between the Chinese-oriented reasoning patterns encoded in COIG-Writer and the requirements of English creative writing explains the performance degradation, providing mechanistic evidence for the lack of cross-lingual transfer in creative capabilities. 19 Table 9. Functional Practical Writing Genre Count % A.L. (minmeanmax) R.I.R.L. (minmeanmax) R.P.L. (minmeanmax) Argumentative Essay Academic Abstract Proposal Planning Open Letter Apology Letter Eulogy Tutorial Guide Interview Questions Product Manual 67 4.02% 5511066.212309 62 3.72% 1961219.19057 33 1.98% 1481667.483951 24 1.44% 1911122.965704 11 0.66% 3181506.453677 10 0.60% 3951872.77032 7 0.42% 2212352.575276 5 0.30% 2038211448 2 0.12% 6211796.52972 46239.96806 48204.05478 598938.031568 440980.482728 41337.092492 7151415.393138 44220.25486 5481177.252427 581762.27954 92211.27445 69011531696 155258.3656 122399.861359 9361218.141491 437795.21044 7952444.54094 137235.8357 50164.5279 Table 10. Non-fiction Writing Genre Count % A.L. (minmeanmax) R.I.R.L. (minmeanmax) R.P.L. (minmeanmax) Essay Reviews Travel Writing Historical Stories Biography 73 4.38% 3741859.078585 22516494891 58 3.48% 54 3.24% 48319467733 34 2.04% 3592179.446856 24 1.44% 8004625.6212766 41279.991381 453918.671630 42193.55932 5021167.412661 80180.31804 5251016.331587 41275.03426 6751055.881930 43294.461718 5881212.122041 Figure 6. Complete distribution of all 51 genres in COIG-Writer, showing the full diversity of creative writing categories covered. 20 Figure 7. Reasoning behavior analysis on Chinese creative writing. Models trained with COIGWriter data exhibit balanced distributions across reasoning types, while baseline models show predominant normal writing with minimal deep reasoning. Figure 8. Reasoning behavior analysis on English creative writing. The baseline model maintains consistent deep reasoning patterns, while COIG-Writer variants show disrupted reasoning flows, explaining their inferior performance. 21 D. Prompts D.1. Prompt for pre-analyzing answer usability During the annotation process, it was observed that annotators spent substantial time on annotation only to find that the quality of the answers was unsatisfactory at the final scoring stage. Consequently, pre-analysis step was introduced to examine the usability of the answers. D.2. Prompt for analyzing answers Provide comprehensive analysis of the answer to help annotators quickly grasp the general idea of the answer and accelerate the annotation process."
        },
        {
            "title": "Prompt for analyzing answers",
            "content": "ÊàëÂ∏åÊúõ‰Ω†ÊâÆÊºî‰∏Ä‰ΩçËµÑÊ∑±ÁöÑÂàõ‰ΩúËÄÖÂíåÂàÜÊûêÂ∏àËØ∑ÂØπ‰ª•‰∏ãÁâáÊÆµËøõË°å‰∏ì‰∏öÂàÜÊûê {} ËØ∑‰ªé‰ª•‰∏ãÊñπÈù¢ËøõË°åËØ¶ÁªÜÂàÜÊûê 1. ÂÜÖÂÆπÁªìÊûÑÂàÜÊûêÊñáÊú¨ÁöÑÊï¥‰ΩìÊû∂ÊûÑÊÆµËêΩÁªÑÁªáÂíåÈÄªËæëÊµÅÁ®ãÊåáÂá∫ÁªìÊûÑÁöÑ‰ºòÁº∫ÁÇπÂèä ÂØπÊï¥‰ΩìÊïàÊûúÁöÑÂΩ±Âìç 2. ËØ≠Ë®ÄÈ£éÊ†ºËØÑ‰º∞ËØ≠Ë®ÄÁöÑËâ≤ÂΩ©ËäÇÂ•èÂè•ÂºèÂèòÂåñÂíåËØçÊ±áÈÄâÊã©Ëøô‰∫õÂÖÉÁ¥†Â¶Ç‰ΩïÂ°ëÈÄ† ‰∫ÜÊñáÊú¨ÁöÑÊï¥‰ΩìÈ£éÊ†ºÂíåËØ≠Ë∞É 3. ‰øÆËæûÊâãÊ≥ïËØÜÂà´Âπ∂ËØÑ‰ª∑‰ΩøÁî®ÁöÑ‰øÆËæûËÆæÂ§áÂ¶ÇÊØîÂñªÈöêÂñªÊéíÊØîÁ≠âÂèäÂÖ∂ÊïàÊûú 4. ÊúâÊïàÊÄßËØÑ‰º∞ÊñáÊú¨Âú®ÂÆûÁé∞ÂÖ∂ÊÑèÂõæÊñπÈù¢ÁöÑÊúâÊïàÁ®ãÂ∫¶Â¶Ç‰ΩïÂì™‰∫õÈÉ®ÂàÜÁâπÂà´ÊúâÂäõÊàñËñÑ Âº± 5. ‰∏ì‰∏öÊ¥ûÂØü‰ªé[ÊñáÊú¨Á±ªÂûã]Âàõ‰ΩúÈ¢ÜÂüüÁöÑ‰∏ì‰∏öËßíÂ∫¶ÊåáÂá∫ËøôÁØáÊñáÊú¨‰∏≠ÁöÑÁã¨ÁâπÂÖÉÁ¥†Êàñ ÂàõÊñ∞ÁÇπ ËØ∑Á°Æ‰øù‰Ω†ÁöÑÂàÜÊûêÊó¢ÊúâÁêÜËÆ∫‰æùÊçÆÂèàÊúâÂÆûÁî®‰ª∑ÂÄºËÉΩÂ∏ÆÂä©ÊàëÁêÜËß£ËøôÁØáÊñáÊú¨ÁöÑÂàõ‰ΩúÊäÄÂ∑ßÂíå ÊïàÊûú D.3. Prompt for analyzing answers and queries Provide comprehensive analysis of the connection between answer and query, offer reliable ideas for annotators to write thoughts, and accelerate the annotation process. D.4. Prompt for evaluating querys Score the quality and creativity of the queries provided by annotators based on the given answers, screen out some low-quality data in advance, and reduce the pressure of manual quality inspection. D.5. Prompt for evaluating answers Score the quality and creativity of the answers provided by annotators based on the given query, filter out some low - quality data in advance, and reduce the pressure of manual quality inspection."
        },
        {
            "title": "Prompt for analyzing answers and queries",
            "content": "‰Ω†ÊòØ‰∏Ä‰ΩçËµÑÊ∑±ÂÜÖÂÆπÂàÜÊûê‰∏ìÂÆ∂‰∏ìÊ≥®‰∫éËß£ÊûêÊñáÊú¨Âàõ‰ΩúËÉåÂêéÁöÑÊÄùÁª¥ËøáÁ®ãÁªìÊûÑËÆæËÆ°ÂíåÂÜô‰Ωú ÊäÄÂ∑ßËØ∑Âü∫‰∫éÊèê‰æõÁöÑÈóÆÈ¢ò/‰∏ªÈ¢ò(query)ÂíåÂØπÂ∫îÁöÑÂõûÁ≠îÂÜÖÂÆπ(answer)ÂØπËøôÁØáÂõûÁ≠îËøõË°åÂÖ® Èù¢‰∏ì‰∏öÁöÑÂàõ‰ΩúÊÄùË∑ØÂàÜÊûêËØ∑Êèê‰æõ: - Query: {} - Answer: {} ËØ∑‰ªé‰ª•‰∏ãÁª¥Â∫¶ËøõË°åÂàÜÊûê: 1. ÈúÄÊ±ÇÁêÜËß£‰∏éÂÆö‰Ωç - ÂØπÂéüÂßãÈóÆÈ¢ò/‰∏ªÈ¢òÁöÑÁêÜËß£Ê∑±Â∫¶ÁõÆÊ†áÂèó‰ºóËØÜÂà´‰∏éÂÜÖÂÆπÂÆö‰ΩçÊ†∏ÂøÉÈóÆÈ¢òÊèêÂèñ‰∏éÂõûÂ∫îÁ≠ñ Áï• 2. ÂÜÖÂÆπÊû∂ÊûÑËÆæËÆ° - Êï¥‰ΩìÊ°ÜÊû∂‰∏éÁªìÊûÑÂ∏ÉÂ±ÄÂºÄÂ§¥‰∏éÁªìÂ∞æÁöÑËÆæËÆ°ÊÑèÂõæÂèäÊïàÊûú‰∏ª‰ΩìÈÉ®ÂàÜÁöÑÈÄªËæëÂ±ïÂºÄÊñπÂºèÊÆµËêΩ‰πãÈó¥ÁöÑË°îÊé•‰∏éÂ±ÇÊ¨°ÂÖ≥Á≥ª 3. Ë°®ËææÊäÄÂ∑ßËøêÁî® - ËØ≠Ë®ÄÈ£éÊ†º‰∏éË°®ËææÁâπÁÇπ‰øÆËæûÊâãÊ≥ï‰∏éÂè•ÂºèÁªìÊûÑ‰∏ì‰∏öÊúØËØ≠ÁöÑËøêÁî®‰∏éËß£ÈáäÂèô‰∫ãÁ≠ñÁï•‰∏é ËØªËÄÖÂºïÂØºÊñπÂºè 4. ËÆ∫ËØÅÊñπÊ≥ïÁ≠ñÁï•ËÆ∫ÁÇπÊûÑÂª∫‰∏éÊîØÊåÅÊñπÂºèËÆ∫ÊçÆÈÄâÊã©‰∏éËøêÁî®ÊïàÊûúÂèçÈ©≥Â§ÑÁêÜ‰∏éÂ§öËßíÂ∫¶ ÊÄùËÄÉËØ¥ÊúçÂäõÊûÑÂª∫ÊäÄÂ∑ß 5. ÂàõÊñ∞‰∏é‰ª∑ÂÄºÂëàÁé∞ - Áã¨ÁâπËßÅËß£‰∏éÂàõÊñ∞ÁÇπÂÆûÁî®ÊÄßÂª∫ËÆÆÁöÑËÆæËÆ°‰∏éÂëàÁé∞ÁêÜËÆ∫‰∏éÂÆûË∑µÁöÑÂπ≥Ë°°Â§ÑÁêÜÁü•ËØÜÊ∑±Â∫¶ ‰∏éÂπøÂ∫¶ÁöÑÂ±ïÁ§∫ÊñπÂºè Êï¥‰ΩìÊïàÊûúËØÑ‰º∞ - ÂÜÖÂÆπ‰∏éÂéüÂßãÈúÄÊ±ÇÁöÑÂåπÈÖçÂ∫¶‰ø°ÊÅØÂØÜÂ∫¶‰∏éÂèØËØªÊÄßÂπ≥Ë°°‰∏ì‰∏öÊÄß‰∏éÈÄö‰øóÊÄßÁöÑÁªìÂêàÊΩúÂú® ÂΩ±Âìç‰∏éÂ∫îÁî®‰ª∑ÂÄº ËØ∑ËÆ∞‰Ωè‰Ω†ÁöÑÂàÜÊûêÊó®Âú®Êè≠Á§∫ËøôÁØáÂõûÁ≠îËÉåÂêéÁöÑÂàõ‰ΩúÊÄùË∑ØÁªÑÁªáÁ≠ñÁï•ÂíåË°®ËææÊäÄÂ∑ßÂ∏ÆÂä©Áî®Êà∑ ÁêÜËß£Âàõ‰ΩúËÄÖÂ¶Ç‰ΩïËß£ËØªÈúÄÊ±ÇÁªÑÁªáÂÖÉÁ¥†ËÆæËÆ°Êû∂ÊûÑÂπ∂ÊúÄÁªàÂëàÁé∞ÂÜÖÂÆπËøôÂ∞ÜÊúâÂä©‰∫éÁî®Êà∑Â≠¶ ‰π†Âπ∂ÊéåÊè°È´òË¥®ÈáèÂÜÖÂÆπÂàõ‰ΩúÁöÑÊÄùÁª¥Ê®°ÂºèÂíåÊñπÊ≥ïËÆ∫ÊèêÂçáËá™Ë∫´ÁöÑÂÜÖÂÆπÂàõ‰ΩúËÉΩÂäõ D.6. Prompt for evaluating thoughts Score the quality and creativity of the thoughts provided by annotators based on the given query, screen out some low - quality data in advance, and reduce the pressure of manual quality inspection. E. Case Study To illustrate the models performance on nuanced creative texts, this case study presents short, atmospheric horror story written in Chinese and its corresponding English translation generated by the model. This example is intended to highlight the models ability to preserve the originals tone, critical details, and narrative pacing."
        },
        {
            "title": "Baseline Case",
            "content": "This case examines generative output from the baseline model to illuminate its inherent limitations in maintaining narrative coherence. Notably, the baseline demonstrates proficiency in producing text with polished, fluent stylistic registerconsistent with its strengths in surfacelevel language generation. However, deeper reading reveals critical deficits in structural 23 and semantic cohesion: sentences often suffer from broken inter-clausal semantic links, while paragraphs lack coherent narrative thread, leading to disjointed content that fails to sustain logical progression. Specifically, the generated text frequently shifts between ideas without transitional reasoning or contextual grounding, resulting in fragmented output where successive segments appear tangential or even contradictory. This discrepancy between stylistic fluency and logical coherence in the baselines output not only exemplifies common failure mode in current generation frameworks but also underscores the need for more robust modeling of inter-utterance and inter-paragraph semantic dependencieskey gaps this work aims to address."
        },
        {
            "title": "Prompt for evaluating querys",
            "content": "‰Ω†ÁöÑ‰ªªÂä°ÊòØÊ†πÊçÆ‰ª•‰∏ãÊ†áÂáÜ,Âü∫‰∫éÁªôÂÆöÁöÑanswer,ÂØπqueryËøõË°åÁ≤æÁ°ÆÁöÑËØÑ‰º∞ËæìÂá∫Â∫îÂåÖÊã¨‰∏§ ‰∏™ÈÉ®ÂàÜË¥®ÈáèÂíåÂàõÊÑèÊÄßËØ∑ÈÅµÂæ™‰ª•‰∏ãÁªÜÂåñÁöÑËØÑÂàÜÊ†áÂáÜ‰ª•Á°Æ‰øùËØÑ‰º∞ÁöÑÁªÜËá¥ÊÄß 1. Ë¥®ÈáèÊÄß(ËØÑÂàÜ1-10): ËØÑ‰º∞ÈóÆÈ¢òÁöÑÊ∏ÖÊô∞Â∫¶ÂíåÂÆåÊï¥ÊÄßÁùÄÈáç‰∫éÈóÆÈ¢òÊòØÂê¶ÊòéÁ°Æ‰∏îËá™ÂåÖÂê´ËÉΩÂê¶Âú®‰∏çÈúÄË¶ÅÈ¢ùÂ§ñ‰ø°ÊÅØ ÁöÑÊÉÖÂÜµ‰∏ãÂæóÂà∞ÂÆåÊï¥ÁöÑÁ≠îÊ°à 9-10ÂàÜ: ÂÆåÂÖ®Ê∏ÖÊô∞Á≤æÂáÜ‰∏îËá™ÂåÖÂê´Êó†Ê≠ß‰πâÈóÆÈ¢òÁªìÊûÑÂêàÁêÜËÉΩÂ§üÁõ¥Êé•Ëß£ÈáäÂíåÂõû Á≠îÊó†ÈúÄÈ¢ùÂ§ñÊæÑÊ∏ÖqueryÂá†‰πéÂØπÈΩê‰∫Üansweranswer‰∏≠Âá∫Áé∞ÁöÑÂ§ßÈÉ®ÂàÜÂØπË±°ÊàñÊ¶Ç ÂøµÂá∫Áé∞Âú®query‰∏≠queryÂæàÊ∏ÖÊô∞ÁöÑÊèèËø∞‰∫ÜÈúÄÊ±Ç 7-8ÂàÜ: Âü∫Êú¨Ê∏ÖÊô∞‰∏îÊòì‰∫éÁêÜËß£Ê≠ß‰πâÂæàÂ∞ëÂü∫Êú¨ÂÆåÊï¥Â∞ΩÁÆ°Â∞èÁöÑÊæÑÊ∏ÖÂèØËÉΩÊúâÂä©‰∫é ÊèêÈ´òÁ≤æÁ°ÆÊÄßqueryËæÉÂ•ΩÁöÑÂØπÈΩê‰∫Üansweranswer‰∏≠Âá∫Áé∞ÁöÑÈÉ®ÂàÜÂØπË±°ÊàñÊ¶ÇÂøµÂá∫Áé∞ Âú®query‰∏≠queryËæÉÊ∏ÖÊô∞ÁöÑÊèèËø∞‰∫ÜÈúÄÊ±Ç 5-6ÂàÜ: ÂèØ ÁêÜ Ëß£ ‰ΩÜ Áº∫ ‰πè ‰∏Ä ÂÆö Ê∏Ö Êô∞ Â∫¶ Â≠ò Âú® Êòæ Ëëó ÁöÑ Ê≠ß ‰πâ Êàñ ÁªÜ ÂæÆ Áº∫ Â§± ÂèØ ËÉΩ ÈúÄ Ë¶Å ÊæÑÊ∏Ö‰ª•Á°Æ‰øùÂáÜÁ°ÆÂõûÁ≠îqueryÂá†‰πéÊ≤°ÂØπÈΩêansweranswer‰∏≠Âá∫Áé∞ÁöÑÂÜÖÂÆπÊ≤°Âá∫Áé∞ Âú®query‰∏≠ 3-4ÂàÜ: Áõ∏ÂΩìÊ®°Á≥äÊàñÁº∫Â§±ÂÖÉÁ¥†ÈúÄË¶ÅÂ§ßÈáèËß£ÈáäÊ∏ÖÊô∞Â∫¶ÈóÆÈ¢òÂ¢ûÂä†ÂõûÁ≠îÈöæÂ∫¶ 1-2ÂàÜ: ÈùûÂ∏∏‰∏çÊ∏ÖÊô∞Ê®°Á≥äÊàñ‰∏çÂÆåÊï¥Èöæ‰ª•Ëß£ÈáäÊàñÊó†Ê≥ïÁõ¥Êé•ÂõûÁ≠î 2. ÂàõÊÑèÊÄß(ËØÑÂàÜ1-10): ËØÑ‰º∞ÈóÆÈ¢òÁöÑÂéüÂàõÊÄßÂàõÊñ∞ÊÄùÁª¥ÂíåÂêØÂèëÊΩúÂäõÁùÄÈáç‰∫éÈóÆÈ¢òÊòØÂê¶ÊâìÁ†¥Â∏∏ËßÑÊÄùÁª¥Ê®°ÂºèËÉΩÂê¶ ÊøÄÂèëÁã¨ÁâπËßÜËßíÂíåÊ∑±Â∫¶ÊÄùËÄÉ 9-10ÂàÜ: ÂçìË∂äÂàõÊÑèÊèêÂá∫ÂÖ®Êñ∞ËßÜËßíÊàñÂâçÊâÄÊú™ËßÅÁöÑÈóÆÈ¢òÊ°ÜÊû∂Â∑ßÂ¶ôËøûÊé•‰∏çÂêåÈ¢ÜÂüüÊåë ÊàòÊ†πÊ∑±ËíÇÂõ∫ÁöÑÂÅáËÆæ‰øÉ‰ΩøÊÄùÁª¥ËåÉÂºèËΩ¨Êç¢ 7-8ÂàÜ: ÊòæËëóÂàõÊÑè‰ª•Êñ∞È¢ñÊñπÂºèÈáçÊûÑÁÜüÊÇâÈóÆÈ¢òËûçÂêà‰∏çÂêåÈ¢ÜÂüüÁü•ËØÜÊèêÂá∫‰ª§‰∫∫ÊÑèÂ§ñ ‰ΩÜÊúâÊÑè‰πâÁöÑÈóÆÈ¢òËßíÂ∫¶ÈºìÂä±Ë∑≥Âá∫Â∏∏ËßÑÊÄùÁª¥Ê°ÜÊû∂ 5-6ÂàÜ: ‰∏≠Á≠âÂàõÊÑèÂú®‰º†ÁªüÈóÆÈ¢òÂü∫Á°Ä‰∏äÊúâÊâÄÂàõÊñ∞Êèê‰æõÁï•ÂæÆÂá∫‰∫∫ÊÑèÊñôÁöÑÈóÆÈ¢òÊÉÖÂ¢É ÈºìÂä±‰∏ÄÂÆöÁ®ãÂ∫¶ÁöÑÈùûÁ∫øÊÄßÊÄùÁª¥‰ΩÜÊï¥‰ΩìÊ°ÜÊû∂ËæÉ‰∏∫Â∏∏ËßÅ 3-4ÂàÜ: ÊúâÈôêÂàõÊÑè‰∏ªË¶ÅÈÅµÂæ™Â∏∏ËßÑÈóÆÈ¢òÊ®°ÂºèÈóÆÈ¢òÂΩ¢ÂºèÊàñÂÜÖÂÆπÁï•ÊúâÂèòÂåñ‰ΩÜÊÄùË∑ØÂ∏∏ ËßÅÂæàÂ∞ëÊøÄÂèëÈùûÂ∏∏ËßÑÊÄùËÄÉ 1-2ÂàÜ: ÊûÅÂ∞ëÂàõÊÑèÂÆåÂÖ®ÈÅµÂæ™Ê†áÂáÜÂåñ‰º†ÁªüÁöÑÈóÆÈ¢òÊ®°ÂºèÊó†‰ªª‰ΩïÊñ∞È¢ñÂÖÉÁ¥†ÊàñÁã¨ÁâπËßÜ Ëßí‰∏çÈºìÂä±ÂàõÈÄ†ÊÄßÊÄùÁª¥ ËØ∑‰∏•Ê†ºÊåâ‰ª•‰∏ãÊ†ºÂºèÂõûÂ§ç‰∏çË¶ÅÂåÖÂê´ÂÖ∂‰ªñÂÜÖÂÆπ {{ \"quality\": 1-10, \"creative\": 1-10, }} Ê†πÊçÆ‰ª•‰∏äÊ†áÂáÜÂØπ‰ª•‰∏ãÈóÆÈ¢òËøõË°åËØÑ‰º∞ answer:{ } query: { } ËæìÂá∫Ê†ºÂºèÁ§∫‰æã: ```json { \"quality\": 8, \"creative\": 8 } ```"
        },
        {
            "title": "Prompt for evaluating answers",
            "content": "‰Ω†ÁöÑ‰ªªÂä°ÊòØÊ†πÊçÆ‰ª•‰∏ãÊ†áÂáÜ,Âü∫‰∫éÁªôÂÆöÁöÑquery,ÂØπanswerËøõË°åÁ≤æÁ°ÆÁöÑËØÑ‰º∞ËæìÂá∫Â∫îÂåÖÊã¨‰∏§ ‰∏™ÈÉ®ÂàÜË¥®ÈáèÂíåÂàõÊÑèÊÄßËØ∑ÈÅµÂæ™‰ª•‰∏ãÁªÜÂåñÁöÑËØÑÂàÜÊ†áÂáÜ‰ª•Á°Æ‰øùËØÑ‰º∞ÁöÑÁªÜËá¥ÊÄß 1. Ë¥®ÈáèÊÄß(ËØÑÂàÜ1-10): ËØÑ‰º∞ÊÄùËÄÉÁöÑË¥®ÈáèÈÄªËæëÊÄßÂíåÂÆåÊï¥ÊÄß 9-10ÂàÜ: ÊÄùËÄÉÂÖ®Èù¢Ê∑±ÂÖ•ÈÄªËæë‰∏•ÂØÜÂáÜÁ°ÆÊääÊè°ÈóÆÈ¢òÊ†∏ÂøÉÂàÜÊûêËßíÂ∫¶Â§öÂÖÉËÆ∫ËØÅÊúâ ÂäõÂÖÖÂàÜÂõûÂ∫îÈóÆÈ¢òÁöÑÂêÑ‰∏™ÊñπÈù¢ 7-8ÂàÜ: ÊÄùËÄÉËæÉ‰∏∫ÂÆåÊï¥ÈÄªËæëÂü∫Êú¨Ê∏ÖÊô∞Ê∂µÁõñÈóÆÈ¢ò‰∏ªË¶ÅÊñπÈù¢Êúâ‰∏ÄÂÆöÊ∑±Â∫¶‰ΩÜÂú®Êüê ‰∫õÁªÜËäÇ‰∏äÂèØËøõ‰∏ÄÊ≠•ÊãìÂ±ï 5-6ÂàÜ: ÊÄùËÄÉÂü∫Êú¨ÂêàÁêÜ‰ΩÜ‰∏çÂ§üÂÖ®Èù¢Êúâ‰∏ÄÂÆöÂàÜÊûê‰ΩÜÁº∫‰πèÊ∑±Â∫¶ÂØπÈóÆÈ¢òÁöÑÁêÜËß£ÂíåÂõûÂ∫î Â≠òÂú®ÈÉ®ÂàÜ‰∏çË∂≥ 3-4ÂàÜ: ÊÄùËÄÉÂ≠òÂú®ÊòéÊòæÁº∫Èô∑ÈÄªËæëËæÉÂº±ÈÅóÊºèÂÖ≥ÈîÆÊñπÈù¢ÂØπÈóÆÈ¢òÁêÜËß£ÊúâÈôêÊàñÂÅèÁ¶ªÈóÆ È¢òÊ†∏ÂøÉ 1-2ÂàÜ: ÊÄùËÄÉË¥®Èáè‰Ωé‰∏ãÈÄªËæëÊ∑∑‰π±ÂàÜÊûêËÇ§ÊµÖÊú™ËÉΩÊúâÊïàÂõûÂ∫îÈóÆÈ¢òÊàñ‰∏•ÈáçËØØËß£ÈóÆ È¢òÊÑèÂõæ 2. ÂàõÊÑèÊÄß(ËØÑÂàÜ1-10): ËØÑ‰º∞ÊÄùËÄÉÂú®ÂéüÂàõÊÄßÂêØÂèëÊÄßÊñπÈù¢ÁöÑË°®Áé∞ 9-10ÂàÜ: ÊÄùËÄÉÊûÅÂÖ∑ÂàõÊñ∞ÊÄßÊèêÂá∫Áã¨ÁâπËßÅËß£ÂíåÂÖ®Êñ∞ËßÜËßíÊâìÁ†¥Â∏∏ËßÑÊÄùÁª¥Ê°ÜÊû∂ËûçÂêàÂ§ö È¢ÜÂüüÁü•ËØÜ‰∫ßÁîüÂØåÊúâÂêØÂèëÊÄßÁöÑÊ¥ûËßÅ 7-8ÂàÜ: ÊÄùËÄÉÊúâÊòéÊòæÂàõÊñ∞ÂÖÉÁ¥†Â±ïÁé∞ÈùûÂ∏∏ËßÑÊÄùÁª¥Ë∑ØÂæÑÊèê‰æõÊñ∞È¢ñÁöÑÂàÜÊûêËßíÂ∫¶Ë∂ÖË∂ä Ë°®Èù¢Â±ÇÊ¨°ÂºïÂèëÊ∑±Â∫¶ÊÄùËÄÉ 5-6ÂàÜ: ÊÄùËÄÉÂåÖÂê´‰∏ÄÂÆöÂàõÊñ∞ÁÇπÊúâËá™Â∑±ÁöÑËßÅËß£ÊÄùË∑ØËæÉ‰∏∫Â∏∏ËßÅ‰ΩÜÊúâÁã¨Âà∞‰πãÂ§ÑËÉΩÂú® ‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÊãìÂ±ïÈóÆÈ¢òËÆ®ËÆ∫Á©∫Èó¥ 3-4ÂàÜ: ÊÄùËÄÉÂàõÊñ∞ÊÄßÊúâÈôê‰∏ªË¶ÅÊ≤øÁî®Â∏∏ËßÑÂàÜÊûêÊñπÊ≥ïËßÇÁÇπËæÉ‰∏∫‰º†ÁªüÂæàÂ∞ëË∑≥Âá∫Êó¢ÂÆö Ê°ÜÊû∂ÊÄùËÄÉ 1-2ÂàÜ: ÊÄùËÄÉÂá†‰πéÊó†ÂàõÊñ∞ÂÆåÂÖ®‰æùÂæ™Ê†áÂáÜÂåñÊÉØÊÄßÁöÑÊÄùÁª¥Ê®°ÂºèÊú™ËÉΩÊèê‰æõ‰ªª‰ΩïÊñ∞È≤ú ËßÜËßíÊàñÁã¨ÁâπÂàÜÊûê ËØ∑‰∏•Ê†ºÊåâ‰ª•‰∏ãÊ†ºÂºèÂõûÂ§ç‰∏çË¶ÅÂåÖÂê´ÂÖ∂‰ªñÂÜÖÂÆπ query:{ } answer: { } ËæìÂá∫Ê†ºÂºèÁ§∫‰æã: ```json { \"quality\": 8, \"creative\": 8 } ```"
        },
        {
            "title": "Prompt for evaluating thoughts",
            "content": "‰Ω†ÁöÑ‰ªªÂä°ÊòØÊ†πÊçÆ‰ª•‰∏ãÊ†áÂáÜ,Âü∫‰∫éÁªôÂÆöÁöÑquery,ÂØπthoughtËøõË°åÁ≤æÁ°ÆÁöÑËØÑ‰º∞ËæìÂá∫Â∫îÂåÖÊã¨‰∏§ ‰∏™ÈÉ®ÂàÜË¥®ÈáèÂíåÂàõÊÑèÊÄßËØ∑ÈÅµÂæ™‰ª•‰∏ãÁªÜÂåñÁöÑËØÑÂàÜÊ†áÂáÜ‰ª•Á°Æ‰øùËØÑ‰º∞ÁöÑÁªÜËá¥ÊÄß 1. Ë¥®ÈáèÊÄß(ËØÑÂàÜ1-10): ËØÑ‰º∞ÊÄùËÄÉÁöÑË¥®ÈáèÈÄªËæëÊÄßÂíåÂÆåÊï¥ÊÄß 9-10ÂàÜ: ÊÄùËÄÉÂÖ®Èù¢Ê∑±ÂÖ•ÈÄªËæë‰∏•ÂØÜÂáÜÁ°ÆÊääÊè°ÈóÆÈ¢òÊ†∏ÂøÉÂàÜÊûêËßíÂ∫¶Â§öÂÖÉËÆ∫ËØÅÊúâ ÂäõÂÖÖÂàÜÂõûÂ∫îÈóÆÈ¢òÁöÑÂêÑ‰∏™ÊñπÈù¢ 7-8ÂàÜ: ÊÄùËÄÉËæÉ‰∏∫ÂÆåÊï¥ÈÄªËæëÂü∫Êú¨Ê∏ÖÊô∞Ê∂µÁõñÈóÆÈ¢ò‰∏ªË¶ÅÊñπÈù¢Êúâ‰∏ÄÂÆöÊ∑±Â∫¶‰ΩÜÂú®Êüê ‰∫õÁªÜËäÇ‰∏äÂèØËøõ‰∏ÄÊ≠•ÊãìÂ±ï 5-6ÂàÜ: ÊÄùËÄÉÂü∫Êú¨ÂêàÁêÜ‰ΩÜ‰∏çÂ§üÂÖ®Èù¢Êúâ‰∏ÄÂÆöÂàÜÊûê‰ΩÜÁº∫‰πèÊ∑±Â∫¶ÂØπÈóÆÈ¢òÁöÑÁêÜËß£ÂíåÂõûÂ∫î Â≠òÂú®ÈÉ®ÂàÜ‰∏çË∂≥ 3-4ÂàÜ: ÊÄùËÄÉÂ≠òÂú®ÊòéÊòæÁº∫Èô∑ÈÄªËæëËæÉÂº±ÈÅóÊºèÂÖ≥ÈîÆÊñπÈù¢ÂØπÈóÆÈ¢òÁêÜËß£ÊúâÈôêÊàñÂÅèÁ¶ªÈóÆ È¢òÊ†∏ÂøÉ 1-2ÂàÜ: ÊÄùËÄÉË¥®Èáè‰Ωé‰∏ãÈÄªËæëÊ∑∑‰π±ÂàÜÊûêËÇ§ÊµÖÊú™ËÉΩÊúâÊïàÂõûÂ∫îÈóÆÈ¢òÊàñ‰∏•ÈáçËØØËß£ÈóÆ È¢òÊÑèÂõæ 2. ÂàõÊÑèÊÄß(ËØÑÂàÜ1-10): ËØÑ‰º∞ÊÄùËÄÉÂú®ÂéüÂàõÊÄßÂêØÂèëÊÄßÊñπÈù¢ÁöÑË°®Áé∞ 9-10ÂàÜ: ÊÄùËÄÉÊûÅÂÖ∑ÂàõÊñ∞ÊÄßÊèêÂá∫Áã¨ÁâπËßÅËß£ÂíåÂÖ®Êñ∞ËßÜËßíÊâìÁ†¥Â∏∏ËßÑÊÄùÁª¥Ê°ÜÊû∂ËûçÂêàÂ§ö È¢ÜÂüüÁü•ËØÜ‰∫ßÁîüÂØåÊúâÂêØÂèëÊÄßÁöÑÊ¥ûËßÅ 7-8ÂàÜ: ÊÄùËÄÉÊúâÊòéÊòæÂàõÊñ∞ÂÖÉÁ¥†Â±ïÁé∞ÈùûÂ∏∏ËßÑÊÄùÁª¥Ë∑ØÂæÑÊèê‰æõÊñ∞È¢ñÁöÑÂàÜÊûêËßíÂ∫¶Ë∂ÖË∂ä Ë°®Èù¢Â±ÇÊ¨°ÂºïÂèëÊ∑±Â∫¶ÊÄùËÄÉ 5-6ÂàÜ: ÊÄùËÄÉÂåÖÂê´‰∏ÄÂÆöÂàõÊñ∞ÁÇπÊúâËá™Â∑±ÁöÑËßÅËß£ÊÄùË∑ØËæÉ‰∏∫Â∏∏ËßÅ‰ΩÜÊúâÁã¨Âà∞‰πãÂ§ÑËÉΩÂú® ‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÊãìÂ±ïÈóÆÈ¢òËÆ®ËÆ∫Á©∫Èó¥ 3-4ÂàÜ: ÊÄùËÄÉÂàõÊñ∞ÊÄßÊúâÈôê‰∏ªË¶ÅÊ≤øÁî®Â∏∏ËßÑÂàÜÊûêÊñπÊ≥ïËßÇÁÇπËæÉ‰∏∫‰º†ÁªüÂæàÂ∞ëË∑≥Âá∫Êó¢ÂÆö Ê°ÜÊû∂ÊÄùËÄÉ 1-2ÂàÜ: ÊÄùËÄÉÂá†‰πéÊó†ÂàõÊñ∞ÂÆåÂÖ®‰æùÂæ™Ê†áÂáÜÂåñÊÉØÊÄßÁöÑÊÄùÁª¥Ê®°ÂºèÊú™ËÉΩÊèê‰æõ‰ªª‰ΩïÊñ∞È≤ú ËßÜËßíÊàñÁã¨ÁâπÂàÜÊûê ËØ∑‰∏•Ê†ºÊåâ‰ª•‰∏ãÊ†ºÂºèÂõûÂ§ç‰∏çË¶ÅÂåÖÂê´ÂÖ∂‰ªñÂÜÖÂÆπ query:{ } answer: { } ËæìÂá∫Ê†ºÂºèÁ§∫‰æã: ```json { \"quality\": 8, \"creative\": 8 } ```"
        },
        {
            "title": "Prompt",
            "content": "Âàõ‰Ωú‰∏ÄÁØáÊÇ¨ÁñëÁü≠ÁØáÂ∞èËØ¥ÊñáÂ≠óÈ£éÊ†ºÊÅêÊÄñÊÉäÊÇö‰ΩÜÁªìÂ∞æËçíËØûÂ§èÂ§©‰∏ª‰∫∫ÂÖ¨Â∫îÊóßÂèã‰πãÁ∫¶ÂéªÈÖí ÂêßÂú®ÈÖíÂêßÂç´ÁîüÈó¥ÂèëÁé∞ÊÅêÊÄñÂ∞∏‰ΩìÊúãÂèã‰πüÂèòÂæóÊÅêÊÄñÂú®ÈÄÉÁîüËøáÁ®ã‰∏≠ÂæóÂà∞È™∑È´ÖÂ∏ÆÂä©ÂèàÈô∑ÂÖ• Ê∑∑‰π±ÁöÑËΩ¶Á•∏ÊúÄÂêéÂõûÂà∞ÊÅêÊÄñËµ∑ÂßãÁÇπ‰∏ªÊó®ÊòØÈÄöËøáÁ¶ªÂ•áÁöÑÊÉÖËäÇÂíåÊÅêÊÄñÊ∞õÂõ¥ÂºïÂèëËØªËÄÖÂØπ‰∫éÊú™ Áü•Âç±Èô©ÁöÑÊÅêÊÉß‰∏éÊÄùËÄÉ Ëø∑Èõæ"
        },
        {
            "title": "The Mist",
            "content": "ÈòøÊù∞Âø´Âá∫Êù•ÊàëÊâØÁùÄÂóìÂ≠êÂú®Âç´ÁîüÈó¥Èó®Âè£ÂñäÁì∑Á†ñÁºùÈöôÈáåÊ∏óÂá∫ÁöÑÈúâÊñëÂú®ÊòèÈªÑÁÅØÂÖâ‰∏ãÂÉèÂáù Âõ∫ÁöÑË°ÄËøπËÄÅÂèãÁöÑÊâãÊú∫Â±èÂπï‰∫ÆËµ∑Êó∂ÊàëÁúãÂà∞‰ªñÊ≠£Ëπ≤Âú®Ê¥óÊâãÂè∞ËæπÂêéÈ¢à‰∏äÊ¥áÁùÄ‰∏ÄÂúàÊ∑±Ëâ≤Ê∂≤ ‰Ωì \"Ajie, get out here!\" yelled at the bathroom door, my throat raw. In the dim yellow light, the mold seeping from between the tiles looked like clotted bloodstains. When my old friends phone screen lit up, saw him squatting by the sink, ring of dark liquid soaking the back of his neck. Â∞∏‰Ωì. . . . . . ‰ªñÁ™ÅÁÑ∂Áõ¥Ëµ∑Ë∫´ÊåáÁî≤ÁºùÈáåÂç°ÁùÄÁ¢éÂèëÂ∞±Âú®. . . . . . ÈÇ£‰∏™. . . . . . Â•≥Âéï. . . . . . ËØùÊ≤° ËØ¥ÂÆå‰ªñÂøΩÁÑ∂ÂÉµ‰ΩèÈïúÂ≠êÈáåÊàëÁöÑÂÄíÂΩ±Ê≠£ÁõØÁùÄ‰ªñÂêéËÖ∞ÁöÑÈùíÁ¥´Ëâ≤ÁòÄÁóïÈÇ£ÊòØÊàë‰∏âÂ§©ÂâçÂ∏Æ‰ªñÊåâ ËøáÁöÑÊ∑§Ë°Ä \"A body...\" he said, suddenly straightening up, his fingernails jammed with bits of hair. \"Its in... in the... womens restroom...\" Before he could finish, he froze. In the mirror, my reflection was staring at the purplish-blue bruise on his lower backthe same bruise had massaged for him three days ago. Ëµ∞Âªä‰º†Êù•ËÑöÊ≠•Â£∞ÊàëÊäì‰Ωè‰ªñÁöÑÊâãËÖïÂ∞±Ë∑ëÂ§úÁè≠‰øùÂÆâÊ≠£Áî®Ê∂àÊØíÊ∞¥Êì¶Êã≠Èó®ÊääÊâãÁúãËßÅÊàë‰ª¨ ÂÜ≤ËøõÂêßÂè∞Êó∂ÂêßÂè∞ÁÅØÁ™ÅÁÑ∂ÁÜÑÁÅ≠ÈòøÊù∞ÁöÑÂñâÁªì‰∏ä‰∏ãÊªöÂä®ÊåáÁºùÈáåÊ∏óÂá∫Ë°ÄÁè†Ê∑∑ÁùÄÈÖíÁ≤æÂë≥ÁöÑÂëï ÂêêÁâ©Ê∫ÖÂú®‰ªñÁöÆÈûã‰∏ä Footsteps echoed from the hallway. grabbed his wrist and we ran. The night-shift security guard was wiping down doorknob with disinfectant. As we scrambled behind the bar, the bar lights abruptly went out. Ajies Adams apple bobbed, and beads of blood seeped from between his fingers. Vomit, reeking of alcohol, splattered onto his leather shoes. Âø´Áúã‰ªñÊåáÂêëËêΩÂú∞Á™óÊö¥Èõ®ÂÄæÁõÜ‰∏≠Â•≥ÂéïÊñπÂêëËÖæËµ∑Âõ¢Âõ¢ÁôΩÈõæÈõæÊ∞îÈáåÈöêÁ∫¶ÊµÆÁé∞Âá∫ È™∑È´ÖËΩÆÂªìÊ£ÆÁôΩËâ≤ÁöÑËÇãÈ™®Âú®Èõ®Âπï‰∏≠Ê≥õÁùÄÁ£∑ÁÅ´Ëà¨ÁöÑÂÖâÊàëÂàöÊÉ≥Êä•Ë≠¶Êï¥Ê†ãÂª∫Á≠ëÁ™ÅÁÑ∂ÂâßÁÉàÊôÉ Âä®Ë≠¶Êä•Âô®ÁãÇÂè´Ê∂àÈò≤ÈÄöÈÅìÈó®ÂìêÂΩìÊíûÂºÄ \"Look!\" he pointed towards the floor-to-ceiling window. In the pouring rain, plumes of white mist were rising from the direction of the womens restroom. skeletal silhouette loomed within the fog, its stark white ribs glowing like phosphorescent fire against the curtain of rain. Just as was about to call the police, the entire building began to shake violentlyalarms blared, and the fire escape door slammed open with clang. 28 Êúâ‰∫∫ÊíûËΩ¶‰∫ÜÈòøÊù∞ÊåáÁùÄÁõëÊéßÂ±èÁîªÈù¢Èáå‰∏ÄËæÜÊïëÊä§ËΩ¶Ê≠£Á¢æËøáÂ∞∏‰ΩìÊÆãËÇ¢Âè∏Êú∫Êà¥ÁùÄÂ¢® ÈïúÂêéËßÜÈïúÈáåÂèçÁùÄÊàëÁöÑËÑ∏Ë≠¶Á¨õÂ£∞‰∏≠Êàë‰ª¨Êå§ËøõÂÅúÂ∞∏Èó¥ÂÜ∑ÂÜªÊüúÈáåÁöÑÂ•≥Â∞∏ÁöÆËÇ§Â∑≤ÂëàÂçäÈÄèÊòé Áä∂Âç¥Âú®Êàë‰ª¨Èù¢ÂâçË£ÇÂºÄ‰∏ÄÈÅìË°ÄÂè£Â≠êÈú≤Âá∫ÈáåÈù¢Ë£πÁùÄÁöÑ. . . . . . Ë£πÂ∞∏Ë¢ã \"Someone crashed!\" Ajie pointed at security monitor. On the screen, an ambulance was running over the mangled limbs of corpse. The driver was wearing sunglasses, and the rearview mirror reflected my own face. Amid the wailing sirens, we squeezed into the morgue. The female corpse in the refrigerated drawer had skin that was already semi-translucent, yet right before our eyes, bloody gash split open, revealing what was wrapped inside... body bag? Ë£π Â∞∏ Ë¢ã Èáå Ëú∑ Áº© ÁùÄ ÁöÑ È™∑ È´Ö Á™Å ÁÑ∂ Êä¨ Â§¥ ÂÆÉ ÁöÑ Áúº Áùõ Á´ü ‰∏é Èòø Êù∞ Âêé ËÖ∞ ÁöÑ ÁòÄ Áóï ‰∏Ä Ê®° ‰∏Ä Ê†∑ Ëøô ÊòØ. . . . . . ÊàëÊë∏ÂêëËá™Â∑±Âè£Ë¢ãÂèëÁé∞‰ªäÂ§©Êó©‰∏äÂú®Âú∞ÈìÅÁ´ô‰π∞ÁöÑÂç†ÂçúÁ∫∏ÁâåÊ≠§ÂàªÊ≠£‰ªéË£§Ë£ÜÈáåÊé¢ Âá∫‰∏ÄÂè™ËãçÁôΩÁöÑÊâãÊåáÊåáÁºùÈáåÂç°ÁùÄ. . . . . . Âç°ÁùÄÈòøÊù∞ÁöÑÊâãÊú∫ The skeleton curled up inside the body bag suddenly looked up. Its eyes were identical to the bruise on Ajies lower back. \"This is...\" reached for my pocket and found the tarot cards had bought at the subway station this morning. pale finger was now poking out from my crotch, and wedged between its knuckles was... was Ajies phone. Èõ®ÂÅúÂêéÂÅúÂ∞∏Èó¥ÊÅ¢Â§çÂÜ∞ÂáâÂØÇÈùôÊàëÁõØÁùÄÈòøÊù∞ÁöÑÂêéËÖ∞ÈÇ£ÈáåÂéüÊú¨ÁöÑÊ∑§ÈùíÂ∑≤ÁªèË§™ÊàêÊöóÁ¥´ ÂÉèÊüêÁßçÊ≠£Âú®Ê∂àÊï£ÁöÑËÉéËÆ∞‰Ω†ËØ¥. . . . . . ÊàëÊåáÂêëÁ™óÂ§ñÈÇ£ÈáåÊÇ¨ÁùÄÂçäÂùóË¢´ËΩ¶ÁÅØÁÖß‰∫ÆÁöÑÈ™®È™ºËøô ÊòØË∞ÅÁöÑÈ™®Â§¥ After the rain stopped, the morgue returned to its icy silence. stared at Ajies lower back, where the original bruise had faded to deep violet, like some kind of disappearing birthmark. \"You think...\" pointed out the window, where half bone, illuminated by car headlights, was suspended in the air. \"Whose bone is that?\" ‰ªñÁ™ÅÁÑ∂Â§ßÁ¨ëËµ∑Êù•‰Ω†ËÆ∞ÂæóÂêóÂéªÂπ¥ÂÜ¨Â§©Êàë‰ª¨Âú®ÈÉäÂ§ñËø∑Ë∑ØÁöÑÊó∂ÂÄô‰Ω†‰∏çÊòØ‰∏ÄÁõ¥ÂæÄ‰∏úËµ∞ Âêó He suddenly burst out laughing. \"Do you remember? Last winter, when we got lost in the countryside, didnt you insist on walking east?\""
        }
    ],
    "affiliations": []
}