{
    "paper_title": "Padding Tone: A Mechanistic Analysis of Padding Tokens in T2I Models",
    "authors": [
        "Michael Toker",
        "Ido Galil",
        "Hadas Orgad",
        "Rinon Gal",
        "Yoad Tewel",
        "Gal Chechik",
        "Yonatan Belinkov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-to-image (T2I) diffusion models rely on encoded prompts to guide the image generation process. Typically, these prompts are extended to a fixed length by adding padding tokens before text encoding. Despite being a default practice, the influence of padding tokens on the image generation process has not been investigated. In this work, we conduct the first in-depth analysis of the role padding tokens play in T2I models. We develop two causal techniques to analyze how information is encoded in the representation of tokens across different components of the T2I pipeline. Using these techniques, we investigate when and how padding tokens impact the image generation process. Our findings reveal three distinct scenarios: padding tokens may affect the model's output during text encoding, during the diffusion process, or be effectively ignored. Moreover, we identify key relationships between these scenarios and the model's architecture (cross or self-attention) and its training process (frozen or trained text encoder). These insights contribute to a deeper understanding of the mechanisms of padding tokens, potentially informing future model design and training practices in T2I systems."
        },
        {
            "title": "Start",
            "content": "Padding Tone: Mechanistic Analysis of Padding Tokens in T2I Models Michael Toker1 Ido Galil1,2 Hadas Orgad1 Rinon Gal2 Yoad Tewel2 1Technion Israel Institute of Technology Gal Chechik2,3 Yonatan Belinkov1 2NVIDIA 3Bar-Ilan University 5 2 0 2 2 ] . [ 1 1 5 7 6 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Text-to-image (T2I) diffusion models rely on encoded prompts to guide the image generation process. Typically, these prompts are extended to fixed length by adding padding tokens before text encoding. Despite being default practice, the influence of padding tokens on the image generation process has not been investigated. In this work, we conduct the first in-depth analysis of the role padding tokens play in T2I models. We develop two causal techniques to analyze how information is encoded in the representation of tokens across different components of the T2I pipeline. Using these techniques, we investigate when and how padding tokens impact the image generation process. Our findings reveal three distinct scenarios: padding tokens may affect the models output during text encoding, during the diffusion process, or be effectively ignored. Moreover, we identify key relationships between these scenarios and the models architecture (cross or self-attention) and its training process (frozen or trained text encoder). These insights contribute to deeper understanding of the mechanisms of padding tokens, potentially informing future model design and training practices in T2I systems."
        },
        {
            "title": "Introduction",
            "content": "Text-to-image (T2I) models consist of two main components: text encoder, which generates representations of the users prompt, and diffusion model, which generates an image based on this representation. To standardize sequence lengths for efficient batch processing in training and inference, input prompts are padded to fixed length with special padding token. Unlike language models, where padding tokens are explicitly masked and thus ignored, the computation process of the T2I models can use these tokens as any other token. Despite their ubiquity, the potential impact of Figure 1: Images generated with FLUX from different segments of the input prompt. Description of each column, from left to right: (1) An image generated using the full prompt (both prompt tokens and padding tokens encoded together), (2) An image generated using only the prompt tokens and clean padding tokens, (3) An image generated using only the prompt-contextual pads encoded with the prompt, while the prompt tokens were replaced with clean pad tokens. padding tokens on image generation outcomes has been overlooked. We introduce two methods to evaluate the influence of tokens on different model components: (1) Intervention in the Text Encoder Output (ITE) and (2) Intervention in the Diffusion Process (IDP). Both methods build on causal mediation analysis, also known as activation patching (Imai et al., 2010; Vig et al., 2020; Zhang and Nanda, 2024). This technique involves perturbing specific inputs or intermediate representations to observe their effect on the output, helping to pinpoint the influential elements. In ITE we selectively perturb specific segments of the text encoders output representations to isolate the contributions of two key elements: prompt tokens and padding tokens. Next, we generate images using the modified prompt representations and analyze the results. The perturbation involves replacing selected token representations with those from prompt that consists solely of padding tokens, referred to as clean pads. These clean pads differ from the original padding tokens, which contain contextual information from the prompt. The method is illustrated in Figure 2. If padding tokens carry meaningful information, we expect two outcomes: (a) replacing the prompt tokens with clean pads should still result in an image reflecting elements of the original prompt, while (b) replacing the padding tokens with clean pads should alter the image either semantically or stylistically. In cases where our analysis with ITE indicates that padding tokens are not used by the text encoder, we further examine the role of padding tokens in the diffusion process. Particularly, we investigate whether significant information is written into the padding token representations throughout the diffusion process. Here we employ IDP, illustrated in Figure 8, to interpret the causal effect of the padding tokens during the diffusion process. We begin with standard prompt padded to fixed length, as well as an only pads prompt. However, in IDP, token replacement occurs before each attention block within the diffusion process and at every diffusion step. We repeat the procedure of selectively replacing either prompt tokens or padding tokens with clean pads, similarly to ITE. Figure 1 illustrates an example of images generated using this method. We analyze six different T2I models and highlight two scenarios where padding tokens are utilized. First, when the text encoder was not frozen during training or fine-tuning, it learns to encode meaningful semantic information into these tokens. Second, in architectures with multi-modal attention mechanismssuch as Stable Diffusion 3 (Esser et al., 2024) and FLUX1padding tokens carry meaningful information throughout the diffusion process, even if the text encoder itself does not directly encode it. Here, the padding tokens seem to act as registers, with information written into their representations to store and recall, similarly to findings from both language models and visionlanguage models (Darcet et al., 2024; Burtsev et al., 2020). 1blackforestlabs.ai To summarize, our main contributions are: 1. We propose two causal methods for analyzing the use of specific tokens in both the text encoder and diffusion model of the T2I pipeline, and apply them to investigate the role of padding tokens. 2. We find that T2I models with frozen text encoders ignore padding tokens. However, when the text encoder is trained or fine-tuned, padding tokens gain semantic significance. 3. We uncover that even when padding tokens are not utilized by the text encoder, for some architectures of the diffusion model, they can still function as registers and play meaningful part in the diffusion process."
        },
        {
            "title": "2 Analysis of Padding in Text Encoding",
            "content": "In the T2I pipeline, the text encoder processes the input prompt = [P1, .., Pk], with prompt length of k. To ensure consistent input length, the prompt is usually padded to fixed length, denoted as . We denote this padded version of the prompt as Pfull, which is concatenation of the prompt tokens and the padding tokens: Pfull = [P1, . . . , Pk, pad, . . . , pad]. (1) The text encoder then processes Pfull, producing constant-length encoded representation, which is subsequently used by the diffusion model for conditional image generation. We denote this encoded full prompt representation as Efull."
        },
        {
            "title": "2.1 Method",
            "content": "Our goal is to evaluate the information encoded in the prompt-contextual padding tokens, and to measure their effect on the generated image. To do so, as illustrated in Figure 2, we generate images using partial representations of Efull that isolate the effect of the padding tokens. We generate images based on modified representations of Efull and compare them to images generated from the full prompt Efull. This enables us to visually express the information from different parts of the text input. Specifically, to remove information coming from subset of the tokens, we replace them with clean padding tokens that were not influenced by the users prompt. To obtain these clean padding tokens, we encode Sclean = [pad, pad, . . . , pad], fixed-length sequence made entirely of padding tokens, and denote their embeddings as Eclean. These encoded pad tokens are then used in constructing the final mixed representation, which commethod Figure 2: ITE: Interpreting information within pad tokens in the text encoder. We first encode the full prompt and an clean pads separately. Next, we keep the tokens we want to interpret and replace all other tokens with clean pad tokens. We then generate an image conditioned on this mixed representation. In the example shown here, we interpret the pad tokens in LLaMA-UNet, revealing semantic information embedded within the pad tokens. is generated in the standard pipeline (left column). Then, we generate an image that demonstrates the information in the non-pad tokens, by replacing the pad tokens with clean pads (middle column). Lastly, we generate an image demonstrating the information within the pad tokens, by replacing the non-pad tokens with clean pads (left column). More formally, the mixed representation for generating an image from the prompt tokens only (middle column) is: Eprompt = (cid:104) E0:k full , Ek:N clean (cid:105) , (2) where Ei:j represents the encoded tokens from index to j, and for representation that generates an image from the prompt-contextual padding tokens only (right column): Epads = (cid:104) E0:k clean, Ek:N full (cid:105) (3)"
        },
        {
            "title": "2.2 Experimental Setup",
            "content": "Models. We use six T2I models. These models can be divided into two categories based on their training approach: those with pretrained frozen text encoders during the training: Stable Diffusion 3 (Esser et al., 2024), Stable Diffusion 2, Stable Diffusion XL (Podell et al., 2024), FLUX; and those with some learned weights as part of the text to image training: LDM (Rombach et al., 2022) and Lavi-Bridge (Zhao et al., 2024) (LLaMA-UNet version). The first group can be divided to two subgroups: models that use vision-language crossattention with the text representations in the diffusion process (Stable Diffusion 2, Stable Diffusion XL) and models that use the text representations as part of vision-language self-attention, allowing Figure 3: Images generated from different segments of the input prompt using ITE. Description of each column, from left to right: (1) An image generated using the full prompt (both prompt tokens and padding tokens encoded together), (2) An image generated using only the prompt tokens and clean padding tokens, (3) An image generated using only the prompt-contextual pads encoded with the prompt, while the prompt tokens were replaced with clean pad tokens. bines both the encoded prompt and clean padding tokens. We use the encoded pad tokens since they contain no information related to the current prompt, while maintaining the same length and distribution of the text encoders output. This allows us to effectively isolate the contribution of the padding tokens that are encoded alongside the full prompt tokens, helping us understand how much of the information in the final representation comes from the prompt itself versus the pads. Figure 3 demonstrated our method. First, we generate an image from the full prompt, which is how the image Figure 4: Average CLIP score over 5,000 images generated from the different representations: full prompt, only prompt, prompt-contextual pads and clean pads. LDM and LLaMA-UNet are the only models achieving high CLIP scores for images generated from padding tokens, indicating their use during text encoding. See Table 4 in the Appendix for standard deviations. text representations to change throughout diffusion (FLUX, Stable Diffusion 3). Appendix provides more information regarding each of the models. Data. Our prompts are based on the Parti dataset (Yu et al., 2022), benchmark containing over 1600 diverse and challenging prompts used to evaluate T2I models. To prevent using prompts that have leaked into the training corpus of the models, we select prompts from eight different challenge categories in Parti, and use GPT-4o2 to generate an alternative set of prompts with similar style and complexity. We then manually review the prompts to ensure their coherence. This process results in 500 new prompts. The complete list of categories, along with the prompt used with GPT, can be found in Appendix A, and the full dataset is included in the supplementary material. Each of the 500 prompts is used to generate 10 images from different random seeds, resulting in 5,000 images for each configuration of model and representation. We investigate three representations: Efull, Eprompt (Eq. 2), Epads (Eq. 3), and Eclean as lower-bound control, with their corresponding images denoted as full, prompt, prompt-contextual pads and clean, respectively. Metrics. To evaluate the generated images, we employ two key metrics: CLIP score (Hessel et al., 2021), which measures how well the generated images align with the prompts, and KID (Kernel Inception Distance) (Binkowski et al., 2018), to evaluate the quality of generated images. KID is used to measure the similarity between the feature 2openai.com/index/hello-gpt-4o distributions of images generated from full representation and generated images after some causal intervention. Unlike FID (Heusel et al., 2017), which is based on Gaussian approximations, KID uses the maximum mean discrepancy (MMD) measure, making it more robust in practice, especially when dealing with smaller sample sizes."
        },
        {
            "title": "2.3 Results",
            "content": "Figure 4 shows the average CLIP score over generations from different representations: full, prompt, prompt-contextual pads and clean. Stable Diffusion (versions 2+3) and FLUX models appear to make little to no use of padding tokens: CLIP scores for the full and prompt representations are nearly identical, while the prompt-contextual padscontaining only padding tokensyield significantly lower scores. In contrast, LLaMA UNet and LDM contain significant semantic information in padding, with higher CLIP score for the prompt-contextual pads, although the degradation in performance from full to prompt is small. Text encoder training objective and its influence on padding usage Our results suggest that the training objective of the text encoder significantly impacts how padding tokens are utilized. Many current T2I models, such as Stable Diffusion and FLUX, employ frozen text encoder, with the diffusion model being trained on its encoded outputs. It may be that because the text encoder is not explicitly trained to process padding tokens for image generation, it does not effectively incorporate them during the textual encoding. As shown in Figure 4 and Table 1, in models that use frozen text encoders, KID Score Pad Segment CLIP Score Model Prompt Pads Flux-schnell LDM LLaMA UNet Stable Diffusion 2 Stable Diffusion 3 0.01 0.88 7.37 0.02 0.01 14.52 4.53 0.48 31.09 15.74 Table 1: KID scores between the images generated from the prompt-contextual pads vs. images generated only from prompt representations. Lower is better. The KID is calculated w.r.t images generated from the full representation. images generated using the prompt representation yield the same CLIP score as those generated using the full representation, while images generated from prompt-contextual padding representations result in very low CLIP score, almost as low as those generated from clean padding. In these models, the prompt KID is very high, meaning that the images are out of distribution. This suggests that in these models, the text encoder does not encode any meaningful information in the padding tokens, which makes them unnecessary for generating the final image. Other models, like LDM and Lavi-Bridge, propose adapting the text encoder specifically for the image generation task. These methods train the text encoder, including the use of padding tokens, on the image generation objective, allowing it to effectively learn how to utilize padding. In these models, the results differ: images generated from full prompt tokens have lower scores compared to those generated using prompt representations, suggesting that the information encoded in the prompt tokens is insufficient to generate the correct images. Furthermore, images generated from the promptcontextual padding tokens in these models yield much higher CLIP scores, even surpassing images generated from full prompt tokens in one of the models. KID of prompt-contextual pads in these models is comparatively low, indicating that the images generated from pads come from closer distribution compared to the images generated from the full representation. Overall, this indicates that padding tokens play an important role in the text encoding process for image generation in these adapted models. 1 2 3 0.30 0.018 0.23 0.018 0.17 0.022 Table 2: Average CLIP scores for different promptcontextual pad segments in LLaMA-UNet: the first 20% of the pads, the next 20%, and then the subsequent 20%. We observe that the semantic information degrades gradually, with most of it concentrated in the initial tokens. How many padding tokens do text encoders use? We focus on the LLaMA-UNet model and analyze padding behavior. We divide the padding tokens into five segments, each containing 20% of the total padding tokens in their natural order. For each segment, we mask both the prompt tokens and pad tokens in the other segments, then generate images from this mixed representation. The CLIP scores can be found in Table 2. Our observations reveal that the information encoded in padding tokens varies based on their proximity to the prompt tokens, with those closer to the prompt carrying more significant information. We hypothesize that this behavior may be due to the text encoders use of causal masking or the positional encoding scheme applied to the padding tokens. Only the padding tokens that are closer to the prompt tokens appear to be utilized effectively. Since LLaMA is language model adapted for image generation using LoRa training, we can load the LoRa with scaling factor, α, to observe how gradually removing LoRa affects the number of used pad tokens. Our results in Figure 5 show that as α decreases, fewer pad tokens are used. This indicates that part of what the LoRa learns involves encoding information in more pad tokens."
        },
        {
            "title": "Process",
            "content": "Even when padding tokens contain no meaningful information after text encoding, the diffusion model might still make use of them during the diffusion process. To generate images from text prompts, T2I models use an attention mechanism to condition the generation process, typically following two common approaches: cross-attention and MM-DiT (Esser et al., 2024) blocks. In crossattention, used in models like Stable Diffusion 2/XL, the model converts image patches into query vectors and text tokens into key and value vecFigure 6: Attention histogram for Stable Diffusion XL and FLUX* for each token reveals that while both models exclude semantic information from padding tokens, FLUX utilizes these tokens, whereas Stable Diffusion does not. *In FLUX, we have removed the long middle part with low attention in order to improve visualization. Figure 5: Images generated from Lavi-bridge with LoRa loaded with scaling factor α (y-axis). We analyze pad token segments: the first column shows the full image, and the next columns show three consecutive 20% of the pads. As α decreases, fewer pad tokens are used. tors. The image patches gather information from the text based on an attention map, but the text representation remains unchanged throughout the process. In contrast, MM-DiT blocks, found in models like FLUX and Stable Diffusion 3, implement multi-modal self-attention, by projecting both image patches and text token representations into query, key, and value vectors. Thus, both the image and text representations update and influence each other during the attention process. We therefore expect that models implementing crossattention where the pads are not used in the text encoder would also not use them in the diffusion process. However, models implementing MM-DiT blocks can potentially aggregate information into the padding tokens, even if initially they contain no information. Motivation: attention maps and qualitative examples. To explore this, we examine the attention maps between image patches and text representations, resulting in an attention map for each token (see example in Figure 7). While in Stable Diffusion XL only the prompt (and the end-of-text) tokens significantly attend to main areas in the image, in FLUX not only prompt tokens, but also many pad tokens contribute much attention to image areas (Figure 6). Moreover, generating images with FLUX and Stable Diffusion XL, with and Figure 7: Attention maps for FLUX diffusion show strong alignment between prompt tokens and semantically relevant image tokens. These maps also reveal high attention for padding tokens with the main objects in the image. without padding (Figure 11, App. E), reveals that FLUX without padding often misses key details, while Stable Diffusion XL remains consistent in its generations."
        },
        {
            "title": "3.1 Method",
            "content": "To interpret the causal effect of tokens during the diffusion process, we develop IDP, illustrated in Figure 8. The diffusion process consists of several diffusion steps, where each step begins with the current latent image representation and the full encoded text representation. Since we look only at models where padding tokens do not carry meaningful information in the text encoder, we hypothesize that the diffusion model might be using these tokens as registers to store and recall information, subsequently passing it to the image patches, similar to the findings of Darcet et al. (2024) in their work on VLMs with image patches. In this case, we conduct the intervention before each attention block to ensure that the attention mechanism fully incorporates the tokens we wish to interpret. We use full prompt and clean pads prompt, whose representations per diffusion layer are denoted as E(l) clean, respectively. We explore two directions: first, we replace the full and E(l) Figure 8: IDP: Interpreting information within pad tokens in the diffusion model. We perform diffusion of two prompts simultaneously: the full prompt and an clean pads. During the diffusion, we keep the tokens we want to interpret (here: the prompt-contextual padding tokens) and replace all other tokens with clean pad tokens. We perform this intervention before each attention block in the diffusion model, through all diffusion steps. We then generate an image conditioned on this mixed representation. In the example shown here, we interpret the pad tokens in FLUX, revealing semantic information embedded within the pad tokens during diffusion. CLIP Reference"
        },
        {
            "title": "Pads\nPrompt\nClean\nFull",
            "content": "0.76 0.022 0.90 0.036 0.46 0.018 1.0 0.0 0.23 0.015 0.33 0.028 0.10 0.009 0.34 0.020 Table 3: Average CLIP scores between images generated (with FLUX) with different IDP interventions and either the full prompt or an image generated from the full prompt. Pad: prompt-contextual pads; Prompt: prompt tokens; Clean: prompt full of pads, used for comparison; Full: prompt with real tokens and pads. prompt tokens with clean pads, using Equation 2. If the images generated from these representations still contain prompt-relevant information, it would suggest that the pads are being utilized. Second, we replace the prompt-contextual pads with the clean pads, as in Equation 3. If the resulting images remain unchanged, this would indicate that the pads are not used for encoding information."
        },
        {
            "title": "3.2 Results",
            "content": "Table 3 shows the results of our intervention in the diffusion process. The table shows average CLIP scores of images generated with different IDP interventions, to assess the role of pad and prompt tokens. First, we compute CLIP scores vis-a-vis the full text prompt (Prompt column). As may be expected, images generated from the prompt tokens are similar to the prompt text to the same extent as images generated from the full prompt. Interestingly, images generated from only the pads are much more similar to the text prompt than images generated from clean prompts, indicating that pad tokens are used by the diffusion model to produce images that relate to the prompt. Next, we compute CLIP scores vis-a-vis images generated from the full prompt (Table 3, Image column). The CLIP score between images generated from full prompts and images generated when using only padding tokens in the diffusion is approximately 76significantly higher than the score for randomly generated images from clean padding prompt. This is further evidence that the padding tokens contain visual information closely related to the content of the prompt tokens. However, the CLIP score when using images generated with IDP from the prompt tokens is still higher, suggesting that some information is lost when only using padding tokens in the diffusion model. Finally, we provide qualitative example in Figure 9 and more examples in Figure 10 (Appendix E), which show that images generated from the prompt-contextual pads with IDP have meaningful semantic information. While images generated solely from prompt tokens typically align with the semantic meaning of the prompt, different visual features are often missing when padding tokens are excluded, while the same features are presented in the padding tokens. It appears that the diffusion model uses padding tokens to create additional visual information, while semantic content remains primarily in the prompt tokens. of text-to-image models, Tang et al. (2023) introduced method to interpret T2I pipelines by analyzing the influence of input words on generated images through cross-attention layers. Chefer et al. (2024) decomposed textual concepts, with focus on the diffusion component. Basu et al. (2024) employed causal tracing to investigate the storage of knowledge in T2I models like Stable Diffusion. Toker et al. (2024) analyzed the text encoder in T2I pipelines, offering view into intermediate representations rather than just its final output. Our work takes unique direction by focusing specifically on padding tokens, which have been largely overlooked in prior research. While previous research has illuminated how prompt tokens guide image generation, we show that padding tokens, often thought to be inert, can play more active roleencoding semantic information or even functioning as registers that influence model computations. This adds new dimension to the interpretation of T2I models, suggesting that even these seemingly unimportant tokens may hold valuable information or operational significance."
        },
        {
            "title": "5 Discussion",
            "content": "This work addresses design decision present in every T2I model that has remained largely unexplored: the choice to include padding tokens during both textual encoding and the diffusion process. As more studies begin integrating large language models (LLMs) into T2I pipelines using techniques like fine-tuning, LoRA, or adapters, the role of padding tokens becomes increasingly crucial. Training these models with padding tokens could influence wide range of methods that assume subject information is encoded in specific tokens (Chefer et al., 2023; Rassin et al., 2023; Hertz et al., 2023; Gal et al., 2022), potentially altering their implementation when padding tokens carry significant semantic information. This factor should be carefully considered when deciding whether to train with or ignore padding tokens. Furthermore, future research could explore how incorporating padding tokens into training might provide computational advantages in more integrated, end-to-end architectures, potentially allowing models to dynamically allocate resources by adjusting the use of padding tokens as needed. Figure 9: Images generated with FLUX from different prompt segments show distinct alignments: prompt tokens produce semantically accurate images, while the visual nuance like cozy emerges only from the promptcontextual pad tokens."
        },
        {
            "title": "4 Related Work",
            "content": "Special tokens and additional computation While padding tokens are generally used for efficient batch processing without fulfilling functional role, other special tokens are known to carry various roles. In transformer language models, attention is often directed to special tokens, including punctuation marks (.), [SEP], or just the first token; this has been referred to as null or no-op attention (Vig and Belinkov, 2019; Kovaleva et al., 2019; Clark et al., 2019; Rogers et al., 2020). Some have added special tokens to enable additional processing, such as registers in vision transformers (Darcet et al., 2024) or memory tokens in language models (Burtsev et al., 2020). More generally, language models benefit from additional computation via chain-of-thought reasoning (Wei et al., 2024). Finally, several studies found it useful to train models to perform additional computation with custom tokens, including filler tokens like ..... (Pfau et al., 2024), so-called pause tokens (Goyal et al., 2024), or meta-tokens for additional reasoning steps (Zelikman et al., 2024). This idea can be traced back to adaptive computation time techniques (Graves, 2016; Banino et al., 2021). Our work contributes to this literature by analyzing the role of padding tokens in T2I models. Interpreting vision-language models. Compared to uni-modal models, VLMs have seen relatively few attempts at interpretation. CLIP (Radford et al., 2021) has been focus of several studies: Goh et al. (2021) identified multimodal neurons responding to specific concepts, while Gandelsman et al. (2023) decomposed its image representations into text-based characteristics. In the realm"
        },
        {
            "title": "Limitations",
            "content": "While we have studied multiple T2I models representing several architectures, our work did not cover the vast space in this area. Our prompt selection offers some variety, but it may not capture all edge cases, potentially overlooking cases where padding tokens are used differently. Additionally, although we rely on widely used metrics like CLIP Score and KID for evaluation, these may not capture all nuances of image quality."
        },
        {
            "title": "Ethical Considerations",
            "content": "In developing our code, we used both Copilot and GPT-4o, but carefully reviewed each line to ensure it aligned with our intended implementation. For writing and rephrasing improvements, we used Wordtune and GPT-4o. Every generated suggestion was carefully reviewed and adjusted to ensure our original intent remained intact. References Andrea Banino, Jan Balaguer, and Charles Blundell. In 8th 2021. Pondernet: Learning to ponder. ICML Workshop on Automated Machine Learning (AutoML). Samyadeep Basu, Nanxuan Zhao, Vlad Morariu, Soheil Feizi, and Varun Manjunatha. 2024. Localizing and editing knowledge in text-to-image generative models. In The Twelfth International Conference on Learning Representations. Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. 2018. Demystifying mmd gans. In International Conference on Learning Representations. Mikhail Burtsev, Yuri Kuratov, Anton Peganov, and Grigory Sapunov. 2020. Memory transformer. arXiv preprint arXiv:2006.11527. Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. 2023. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Transactions on Graphics, 42(4):110. Hila Chefer, Oran Lang, Mor Geva, Volodymyr Polosukhin, Assaf Shocher, Inbar Mosseri, Lior Wolf, et al. 2024. The hidden language of diffusion models. In The Twelfth International Conference on Learning Representations. Kevin Clark, Urvashi Khandelwal, Omer Levy, and Christopher D. Manning. 2019. What does BERT look at? an analysis of BERTs attention. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 276286, Florence, Italy. Association for Computational Linguistics. Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. 2024. Vision transformers need registers. In The Twelfth International Conference on Learning Representations. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. 2024. Scaling rectified flow transformers for highIn Forty-first Internaresolution image synthesis. tional Conference on Machine Learning. Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit Bermano, Gal Chechik, and Daniel CohenOr. 2022. An image is worth one word: Personalizing text-to-image generation using textual inversion. arXiv preprint arXiv:2208.01618. Yossi Gandelsman, Alexei Efros, and Jacob Steinhardt. 2023. Interpreting CLIPs image representation via text-based decomposition. arXiv preprint arXiv:2310.05916. Gabriel Goh, Nick Cammarata, Chelsea Voss, Shan Carter, Michael Petrov, Ludwig Schubert, Alec Radford, and Chris Olah. 2021. Multimodal neurons in artificial neural networks. Distill, 6(3):e30. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. 2024. Think before you speak: Training lanIn The Twelfth guage models with pause tokens. International Conference on Learning Representations. Alex Graves. 2016. for recurrent neural networks. arXiv:1603.08983. Adaptive computation time arXiv preprint Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. 2023. Prompt-to-prompt image editing with cross-attention control. In The Eleventh International Conference on Learning Representations, 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. 2021. CLIPScore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 75147528, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. Gans trained by two time-scale update rule converge to In Advances in Neural local nash equilibrium. Information Processing Systems, volume 30. Curran Associates, Inc. Kosuke Imai, Luke Keele, and Dustin Tingley. 2010. general approach to causal mediation analysis. Psychological methods, 15(4):309. Olga Kovaleva, Alexey Romanov, Anna Rogers, and Anna Rumshisky. 2019. Revealing the dark secrets of BERT. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 43654374, Hong Kong, China. Association for Computational Linguistics. Jacob Pfau, William Merrill, and Samuel Bowman. 2024. Lets think dot by dot: Hidden computation in transformer language models. arXiv preprint arXiv:2404.15758. Jesse Vig and Yonatan Belinkov. 2019. Analyzing the structure of attention in transformer language model. In Proceedings of the 2019 ACL Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pages 6376, Florence, Italy. Association for Computational Linguistics. Jesse Vig, Sebastian Gehrmann, Yonatan Belinkov, Sharon Qian, Daniel Nevo, Simas Sakenis, Jason Huang, Yaron Singer, and Stuart Shieber. 2020. Causal mediation analysis for interpreting neural arXiv preprint nlp: The case of gender bias. arXiv:2004.12265. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. 2024. SDXL: improving latent diffusion models for high-resolution image synthesis. In The Twelfth International Conference on Learning Representations, ICLR 2024, Vienna, Austria, May 7-11, 2024. OpenReview.net. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. 2024. Chain-of-thought prompting elicits reasoning in large language models. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA. Curran Associates Inc. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR. Royi Rassin, Eran Hirsch, Daniel Glickman, Shauli Ravfogel, Yoav Goldberg, and Gal Chechik. 2023. Linguistic binding in diffusion models: Enhancing attribute correspondence through attention map alignment. arXiv preprint arXiv:2306.08877. Anna Rogers, Olga Kovaleva, and Anna Rumshisky. 2020. primer in BERTology: What we know about how BERT works. Transactions of the Association for Computational Linguistics, 8:842866. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. Highresolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, CVPR 2022, New Orleans, LA, USA, June 18-24, 2022, pages 1067410685. IEEE. Raphael Tang, Linqing Liu, Akshat Pandey, Zhiying Jiang, Gefei Yang, Karun Kumar, Pontus Stenetorp, Jimmy Lin, and Ferhan Ture. 2023. What the DAAM: Interpreting stable diffusion using cross attention. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 56445659, Toronto, Canada. Association for Computational Linguistics. Michael Toker, Hadas Orgad, Mor Ventura, Dana Arad, and Yonatan Belinkov. 2024. Diffusion lens: Interpreting text encoders in text-to-image pipelines. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 97139728. Association for Computational Linguistics. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, Ben Hutchinson, Wei Han, Zarana Parekh, Xin Li, Han Zhang, Jason Baldridge, and Yonghui Wu. 2022. Scaling autoregressive models for content-rich textto-image generation. Trans. Mach. Learn. Res., 2022. Eric Zelikman, Georges Raif Harik, Yijia Shao, Varuna Jayasiri, Nick Haber, and Noah Goodman. 2024. Quiet-STar: Language models can teach themselves In First Conference on to think before speaking. Language Modeling. Fred Zhang and Neel Nanda. 2024. Towards best practices of activation patching in language models: Metrics and methods. In The Twelfth International Conference on Learning Representations. Shihao Zhao, Shaozhe Hao, Bojia Zi, Huaizhe Xu, and Kwan-Yee Wong. 2024. Bridging different language models and generative vision modarXiv preprint els for text-to-image generation. arXiv:2403.07860."
        },
        {
            "title": "A Data Creation",
            "content": "We randomly selected 50 samples from each of the following categories in the Parti dataset: Fine-grained Detail Imagination Simple Detail Style and Format Complex Linguistic Structures Perspective Quantity 2. Stable Diffusion 3 utilizes combination of two frozen CLIP text encoders along with frozen T5 encoder. 3. FLUX utilizes frozen T5 text encoder and CLIP encoder. key distinction between FLUX and Stable Diffusion models is that the latter incorporates transformer architecture with self-attention to both the image and text latent representations in the diffusion process. This allows the diffusion model to modify text representations dynamically during the diffusion. The models with trained text encoders: 1. LDM uses BERT text encoder, which is trained jointly with the diffusion model on the image generation task. 2. Lavi-Bridge employs LLaMA that is trained jointly with the diffusion model on the image generation task."
        },
        {
            "title": "D Technical Details",
            "content": "All experiments were conducted using NVIDIA A100 GPUs with 8 cores, ensuring high computational performance and efficiency for our model evaluations. The total computational time across all experiments amounted to approximately 200 GPU hours."
        },
        {
            "title": "E Qualitative Examples",
            "content": "The following figures provide visual examples illustrating the impact of padding tokens in the T2I pipeline, highlighting some key findings from our analysis. For each category, we used the following prompt with GPT-4o: Create an alternative CSV with different prompts of similar style and complexity. For the categories Style and Format, and Simple Detail, we repeated this process twice, generating total of 100 examples for each. In the end, we obtained 500 prompts overall."
        },
        {
            "title": "Different Architectures",
            "content": "To condition the generation process on textual prompt, T2I models typically employ an attention mechanism. There are two popular methods for achieving this: through cross-attention mechanism, used in models like Stable Diffusion 2 and Stable Diffusion XL, and MM-DiT (Esser et al., 2024) blocks, found in models such as FLUX and Stable Diffusion 3. In the cross-attention mechanism, image patches are projected into query vectors while text tokens are projected into key and value vectors and . Essentially, each image patch draws information from the text tokens based on the attention map A: = softmax(QK/ (cid:112) dk), (4) where dk represents the dimensionality of the key vectors. It is important to note that only the image patches extract information from the text tokens, while the text tokens remain constant throughout the computation process. Alternatively, the MMDiT blocks implement self-attention mechanism where both the image patches and text tokens are concatenated into single set and then projected into Q, and vectors. In this formulation, both the image and text draw information from each other, using the following attention map: (cid:112) = softmax([Qtxt, Qimg][Ktxt, Kimg]/ dk), (5) where Qtxt, Ktxt are the text query and key vectors, and Qimg, Kimg are the image query and key vectors. Here, both the image patches and text tokens are updated after the operation."
        },
        {
            "title": "C Models",
            "content": "The models with frozen text encoders are: 1. Stable Diffusion 2 employs single frozen CLIP-based text encoder. Figure 10: Additional examples of images generated from different segments of the input prompt using IDP. Description of each column, from left to right: (1) An image generated using the full prompt (both prompt tokens and padding tokens encoded together), (2) An image generated using only the prompt tokens and clean padding tokens that were not encoded with the prompt, (3) An image generated using only the padding tokens encoded with the prompt, while the prompt tokens were replaced with clean pad tokens. See Figure 8 for further technical details. Figure 11: Examples of images generated from the same prompts with maximum padding and without padding in Stable Diffusion XL and FLUX. Images generated by Stable Diffusion XL maintain consistent quality, while produced by FLUX without padding often miss key details. For example, given the prompt compass beside feather, images with padding typically include textured paper with text or manuscript. In contrast, for the prompt boy visiting zoo, images generated without padding result in vague animal shapes (first column) or hybrids, such as mix between giraffe and horse (third image). However, adding padding leads to more visually coherent animals."
        },
        {
            "title": "F Complementary results",
            "content": "Model flux-schnell ldm LLaMA unet stable diffusion 2 stable diffusion 3 stable diffusion XL Clean Pads 0.039 0.033 0.034 0.037 0.039 0.023 Full 0.037 0.037 0.035 0.033 0.035 0.036 Pads 0.036 0.043 0.034 0.037 0.046 0.043 Prompt 0.036 0.042 0.041 0.034 0.036 0.039 Table 4: Calculated Standard Deviation of CLIP Scores for each Model and different text encoder interventions."
        }
    ],
    "affiliations": [
        "Bar-Ilan University",
        "NVIDIA",
        "Technion Israel Institute of Technology"
    ]
}