{
    "paper_title": "OpenLID-v3: Improving the Precision of Closely Related Language Identification -- An Experience Report",
    "authors": [
        "Mariia Fedorova",
        "Nikolay Arefyev",
        "Maja Buljan",
        "Jindřich Helcl",
        "Stephan Oepen",
        "Egil Rønningstad",
        "Yves Scherrer"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Language identification (LID) is an essential step in building high-quality multilingual datasets from web data. Existing LID tools (such as OpenLID or GlotLID) often struggle to identify closely related languages and to distinguish valid natural language from noise, which contaminates language-specific subsets, especially for low-resource languages. In this work we extend the OpenLID classifier by adding more training data, merging problematic language variant clusters, and introducing a special label for marking noise. We call this extended system OpenLID-v3 and evaluate it against GlotLID on multiple benchmarks. During development, we focus on three groups of closely related languages (Bosnian, Croatian, and Serbian; Romance varieties of Northern Italy and Southern France; and Scandinavian languages) and contribute new evaluation datasets where existing ones are inadequate. We find that ensemble approaches improve precision but also substantially reduce coverage for low-resource languages. OpenLID-v3 is available on https://huggingface.co/HPLT/OpenLID-v3."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 1 ] . [ 1 9 3 1 3 1 . 2 0 6 2 : r OpenLID-v3: Improving the Precision of Closely Related Language Identification An Experience Report"
        },
        {
            "title": "Maja Buljan",
            "content": "Jindˇrich Helcl"
        },
        {
            "title": "Stephan Oepen",
            "content": "Egil Rønningstad Language Technology Group Department of Informatics University of Oslo mariiaf@ifi.uio.no"
        },
        {
            "title": "Abstract",
            "content": "Language identification (LID) is an essential step in building high-quality multilingual datasets from web data. Existing LID tools (such as OpenLID or GlotLID) often struggle to identify closely related languages and to distinguish valid natural language from noise, which contaminates language-specific subsets, especially for low-resource languages. In this work we extend the OpenLID classifier by adding more training data, merging problematic language variant clusters, and introducing special label for marking noise. We call this extended system OpenLID-v3 and evaluate it against GlotLID on multiple benchmarks. During development, we focus on three groups of closely related languages (Bosnian, Croatian, and Serbian; Romance varieties of Northern Italy and Southern France; and Scandinavian languages) and contribute new evaluation datasets where existing ones are inadequate. We find that ensemble approaches improve precision but also substantially reduce coverage for low-resource languages."
        },
        {
            "title": "Introduction",
            "content": "Growing interest in large-scale LLM pre-training data for languages other than English puts spotlight on robust and broad-coverage language identification (LID). Common pre-training datasets are typically distilled from massive collections of web documents, which are characterized by immense diversity in, for example, genres and domains, degrees of (in)formality, juxtaposition of language and non-language content, presence of machine-generated content, code switching, and other sources of variation. For instance, the two largest and linguistically broadest pretraining datasets, FineWeb 2 (Penedo et al., 2025) and HPLT 3.0 (Oepen et al., 2025), apply LID as document-level classification task. Specifically, FineWeb builds on the third-party GlotLID classifier (Kargaran et al., 2023), which supports 1 some 2,000 distinct languages, whereas HPLT has developed custom classifier dubbed OpenLID (Burchell et al., 2023) covering around 200 languages; see Section 2 for further background. In this work, we seek to shed more light on LID performance and challenges in the realm of noisy web documents, with particular emphasis on selected groups of closely related languages. The contributions of this paper are as follows: We train new version of the fully opensource OpenLID system for our experiments and publicly release it as OpenLID-v3.1 This version covers 194 languages plus not-alanguage class. We evaluate OpenLID-v3 on mainstream LID benchmarks such as FLORES+ and UDHR and show that they are not sufficient for evaluating the quality of similar LID.2 In addition, we employ several existing benchmarks for similar languages and create new ones for the BCMS macrolanguage, and for Norwegian Bokmål and Nynorsk. We also report negative results on our efforts with two-step coarse-to-fine classification approach, to be found in Appendix F."
        },
        {
            "title": "2 Related Work",
            "content": "Accurate language identification (LID) is essential for building high-quality multilingual datasets, since documents or segments assigned to an incorrect language can severely contaminate the language-specific subsets, especially for lowresource languages. 1https://github.com/hplt-project/openlid 2https://github.com/hplt-project/ openlid-v3-evaluation"
        },
        {
            "title": "2.1 LID Tools and Methods",
            "content": "Numerous LID systems have been developed over the years, including langid.py (Lui and Baldwin, 2012), Googles Compact Language Detector 2 (CLD2) and its neural network successor CLD3, the HeLI method (Jauhiainen et al., 2016), LanideNN (Kocmi and Bojar, 2017), or AfroLID (Adebara et al., 2022). In recent years, classifiers based on the fastText model (Joulin et al., 2017) have become the de facto standard in large-scale corpus processing due to their efficiency and strong performance across many languages. FastText calculates hidden representation of text by summing the embeddings of words and character n-grams from this text. This representation is passed through linear layer for classification. This approach takes both wordand character-level features into account, making it especially efficient for languages with rich morphology. In this work, we focus on two fastText-based systems: GlotLID (Kargaran et al., 2023) and OpenLID (Burchell et al., 2023), both designed for multilingual scenarios and widely adopted in massivescale data curation pipelines such as FineWeb (Penedo et al., 2024, 2025) or HPLT (de Gibert et al., 2024; Burchell et al., 2025; Oepen et al., 2025). OpenLID and GlotLID differ primarily in their language coverage and training data selection. GlotLID focuses on maximizing the language coverage and supports over 2,000 languages, though it incorporates training data from sources with more restrictive licenses. This extensive coverage results in larger model size, but makes it suitable for use cases involving low-resource languages. In contrast, OpenLID prioritizes fully open-source training data with permissive licenses. The original version (v1) covering 201 languages was later updated (v2) to reduced coverage of 189 languages. The reduction excluded three problematic languages and consolidated certain language varieties under their macrolanguage labels for compatibility with the FLORES+ benchmark."
        },
        {
            "title": "2.2 Broad-Coverage Evaluation Benchmarks",
            "content": "The most widely adopted multilingual language identification benchmarks are FLORES+ and UDHR. FLORES+ 3 is parallel corpus based on FLORES-200 (NLLB Team et al., 2022), originally 3https://huggingface.co/datasets/ openlanguagedata/flores_plus developed for machine translation evaluation. It features two disjoint publicly available data splits, dev and devtest, and currently covers 225 language varieties. In our experiments, we use the former for validation and the latter for evaluation. The UDHR dataset consists of translations of the Universal Declaration of Human Rights across 418 languages,4 which provides formal, declarative text in single domain. The UDHR dataset features test split only. Additionally, FastSpell (Bañón et al., 2024) provides benchmark specifically designed to discriminate between closely related languages in web documents, which addresses some of the limitations that stem from using relatively clean parallel corpora. Finally, Oepen et al. (2025) present manual inspection effort conducted in the context of HPLT 3.0 data collection.5 In that work, human annotators checked the prediction of OpenLID-v2 for web documents. They could label documents as containing unnatural language, porn, web artifacts, and incorrect LID. While the data hasnt been annotated for the correct language, it is possible to construct LID evaluation dataset from the subset of correctly identified samples. Statistics of this resulting dataset, dubbed HPLT-LID, and evaluation results on it are to be found in the Appendix A. We also performed re-annotation of incorrectly classified examples for some languages."
        },
        {
            "title": "Languages",
            "content": "The VarDial workshop has long tradition of shared tasks that focus on language identification in challenging settings, such as the discrimination of closely related languages (e.g. Zampieri et al., 2014; Gaman et al., 2020; Aepli et al., 2023; Chifu et al., 2024). For example, shared tasks have focused on the Bosnian-Croatian-MontenegrinSerbian macrolanguage or on regional languages of Italy. We leverage datasets made available in this context for more fine-grained analysis of OpenLID-v3 (see Section 4)."
        },
        {
            "title": "2.4 Evaluation Metrics",
            "content": "Caswell et al. (2020) argue that the commonly used metric precision (and its derivative, F1-score) are 4We use the udhr-lid version of the dataset available at https://huggingface.co/datasets/cis-lmu/udhr-lid 5https://github.com/hplt-project/release3_ inspection/tree/main/annot_round1 2 misleading when evaluating models on the standard LID datasets, as the models face incomparably larger language imbalance in the real web crawls compared to any reasonable labeled dataset. For example, their baseline model achieving median F1-score of 98% on benchmark data produced set of monolingual corpora with median precision of 5% only when evaluated manually on real data. On the other hand, recall and false positive rate (FPR) are not susceptible to class imbalance, and thus are recommended for model comparison. Since most of our benchmarks are single-label, we report FPR, precision, recall and F1-score for them. For multilabel benchmarks, which are available for some related languages, where short samples may be valid in more than one language, we also report loose (is subset) and exact match metrics (cf. Fedorova et al., 2025).6 These metrics (we refer to them as multilabel classification metrics throughout the paper) are still less strict than our default ones, because they do not depend on the exact number of false positives or negatives in set of predicted labels."
        },
        {
            "title": "3 OpenLID-v3",
            "content": "This work is motivated by our experience with OpenLID-v2 in the context of its application to large-scale web dataset HPLT 3.0, and the results of its manual inspection (Oepen et al., 2025). We identified number of issues with OpenLIDv2, which led us to provide an updated version, OpenLID-v3, with slightly different language inventory. We focus on improving OpenLID-v2 (rather than GlotLID, for example) because of the permissive license of all its training data. The identified issues are as follows: Support for the Latin language, which was present in OpenLID-v1, but removed in v2. OpenLID-v2 contains only Serbian in Cyrillic script, while Latin script is also widely used in Serbian non-governmental public and private communication. As result, OpenLID-v2 erroneously classifies Serbian in Latin script as Bosnian or Croatian; according to the manual inspection, half of HPLT 3.0 Bosnian turned out to be Serbian written in Latin script. Since OpenLID-v2s class inventory is limited to 200 languages, any text that is not natural 6https://github.com/ltgoslo/slide language (e.g., code, broken encoding we label these instances as not-a-language, using zxx_Zxxx throughout the paper) or natural language outside of the 200 the model was trained for (we refer to such instances as other), are still predicted to belong to one of the existing classes. In HPLT 3.0, the problem of not-a-language classes was overcome by rule-based filtering after LID, and the problem of the other class was solved by thresholding the predictions by 0.5 softmax scores. However, some classes can still accumulate large number of not-a-language and other documents. We call this the trash bin phenomenon; in HPLT 3.0, Ligurian was found to be such trash bin language (Oepen et al., 2025). Some highly similar languages showed high confusion when tested on the FLORES+ development split and UDHR (Arabic dialects, Persian languages, Bambara and Dyula, Dzongkha and Tibetan, and other languages listed in Table 10). Finally, the amount of identified data for certain languages in the HPLT dataset is very small compared to its number of speakers, e.g. Bengali and Tamil. See Table 10 for the full list of such languages. When developing OpenLID-v3, we approached these issues in the following ways: We merged 8 Arabic dialects into one Arabic macrolanguage: ara_Arab; and two Persian varieties: pes_Arab and prs_Arab into the Farsi macrolanguage fas_Arab. This follows the OpenLID-v1 approach. Likewise, we merged Bambara and Dyula, which are mutually intelligible (Amuzu and Singler, 2014). We introduced the not-a-language class zxx_Zxxx, using the GlotLID training data labeled as und_* and zxx_*. We extended the training data for several languages, including Latin, and Serbian written in Latin script. We relied on the subsets of the GlotLID training data which were reported by its authors to not be noisy. We also removed one possibly noisy OpenLIDv2 training subset and added the most recent Wikipedia dumps, where adding GlotLID training data showed insufficient improvements during validation. The affected lan3 approach was used to produce HPLT 4.0 datasets."
        },
        {
            "title": "4 Case Studies on Related languages",
            "content": "Results of both OpenLID and GlotLID are high on multilingual benchmarks. However, lower precision for similar languages may be hidden when averaging their scores with those of languages that are easy to distinguish. For this reason, we perform additional evaluation for groups of closely related languages. We choose three groups of such languages, within those with updated training data, based on benchmark availability and our language expertise."
        },
        {
            "title": "Serbian",
            "content": "Bosnian, Croatian and Serbian are, along with Montenegrin, South Slavic languages spoken in the West Balkans; part of group of historically and geographically close languages that are, for the most part, mutually intelligible to speakers of any one of them. They differ by variations in vocabulary and grammatical features most notably, the orthography of the Slavic jat (ˇe) (Mešanovic-Meša, 2011; Karavdic, 2017). Montenegrin is only present in GlotLID class inventory; for this reason, we do not focus on it. The analysis of its GlotLID predictions is to be found in Appendix C. Test data. We use three datasets for the BCMS language group.8 First, BCMS Twitter user dataset (Ljubešic and Rupnik, 2022b), also featured in the BENCHic BCMS benchmark (Rupnik et al., 2023). We use the version with multi-label annotations (Miletic and Miletic, 2024), comprising roughly 123 instances (in the test set), where each instance corresponds to one user and contains many texts written by that user. Second, we use the BCS portion of ParlaSent (Mochtak et al., 2023), comprising roughly 18,000 sentences from transcriptions of parliamentary debates, annotated with the speakers country of origin. Third, we make use of the HPLT-LID data. Since Serbian in Latin script was absent in OpenLID-v2, pre-annotation was only performed for Bosnian and Croatian. In total, 804 sentences were pre-annotated (402 predicted to be Bosnian and 402 predicted to be Croatian). Out of them, 114 Bosnian samples and 13 Croatian were pre-annotated to be false positives. 7https://hplt-project.org/datasets/v4.0 8Exact scores on these benchmarks are to be found in the Appendix C. 4 Figure 1: Model comparison on three LID benchmarks, FPR (lower is better) vs. recall (higher is better). For the full set of metrics see Table 9. FLORES+ refers to the devtest split, and FastSpell refers to our version, which excludes Nynorsk. guages and data sources used are summarized in Table 10 in the Appendix. We also experimented with ensembling OpenLID-v3 and GlotLID by top-1 and top-3 agreement. Top-3 agreement worsened the results, which is expected for single-label model, where only one class gets relatively high softmax score, and all other classes are long tail of small numbers."
        },
        {
            "title": "The list of",
            "content": "language labels supported by OpenLID-v3 is given in Table 15 in Appendix G."
        },
        {
            "title": "3.1 Results",
            "content": "Figure 1 compares the results of our OpenLID-v3 model with the previous version (OpenLID-v2) as well as with GlotLID, focusing on the popular LID benchmarks: FLORES+, UDHR, and FastSpell. Following HPLT 3.0, we experimented wth applying the same softmax thresholding of 0.5 to all three models. OpenLID-v3 is on par with both OpenLID-v2 and GlotLID; the difference with and without thresholding softmax scores is not large among models, but the results are slightly better for OpenLID-v3 with thresholding, and for GlotLID without. This different behavior may be explained by the fact that GlotLIDs trash bins fall outside of the classes we evaluate on. In our further experiments, we use OpenLID-v3 with thresholding, and GlotLID without, unless otherwise specified, including the ensembling approach. Top-1 ensembling resulted in the lowest FPR across all datasets. Based on our experiments, this"
        },
        {
            "title": "Twitter users",
            "content": "OpenLID-v3 OpenLID-v2 GlotLID Ensemble"
        },
        {
            "title": "ParlaSent",
            "content": "OpenLID-v3 OpenLID-v2 GlotLID Ensemble HPLT-LID (reannotated) OpenLID-v3 OpenLID-v2 GlotLID Ensemble Bosnian Prec."
        },
        {
            "title": "FPR",
            "content": "Rec."
        },
        {
            "title": "FPR",
            "content": "Croatian Prec. Rec."
        },
        {
            "title": "FPR",
            "content": "Serbian Latin Prec. Rec. n=21 37.21 16.96 16.95 0 n=153 11.59 0.08 9.59 13. n=5 5.26 1.85 0.01 2.70 76.19 90.48 47.62 0 47.71 57.51 43.79 34.64 40.00 40.00 0.2 20.00 n= 80.00 84.62 36.17 0 n=1,387 87.49 78.05 81.02 89.25 n=7 42.86 21.43 31.58 46.15 0.026 0.017 0.26 0.13 0.26 0.242 0.11 0.03 0.09 0.05 0.03 54.54 50.00 77.27 0 79.16 80.25 90.48 77.21 85.71 42.86 85.71 85.71 n= 100.00 0 82.14 0 n=1,060 95.00 0 97.10 98.38 n=110 100.00 0 100.00 100.00 0 0 0.12 0.019 0 0.006 0.003 0 0 0 0 30.85 0 24.49 0 53.77 0 31.60 28.77 50.00 0 3.64 3.64 0.23 0.8 0.42 0.22 0.44 0.26 0.14 0.15 0.9 0.4 0.146 Table 1: Performance comparison across three benchmark datasets showing false positive rate (FPR), precision (Prec.), and recall (Rec.) for Bosnian, Croatian, and Serbian in Latin script. OpenLID-v3 uses softmax threshold 0.5. Ensemble represents top-1 agreement between OpenLID-v3 and GlotLID. Best values per metric and language are in bold. We performed re-annotation of these false positive samples to their correct labels; the majority of them was found to be Serbian. The annotation was done by native speaker of Croatian with linguistic background. One sample was detected to be valid both in Croatian and Serbian and annotated as multilabel; we excluded it from metric calculation."
        },
        {
            "title": "4.1.1 Quantitative Evaluation\nTable 1 presents FPR, precision and recall per lan-\nguage on Twitter user, ParlaSent and reannotated\nBCS part of HPLT-LID.",
            "content": "None of the models scored high on all three datasets. As HPLT 4.0 developers, we were mostly interested in results on Twitter, since its texts were the closest to the noisy web data, containing hyperlinks, emodjis etc. OpenLID-v3 turned out to be the best for Bosnian and Serbian on it; slightly higher precision of Croatian obtained from OpenLIDv2 comes at cost of lack of Serbian class label, which is not what we aim at. Importantly, GlotLID and OpenLID-v3 always disagree on Twitter data, which emphasizes that ensembling should be done cautiously for particular language groups. For the future work on improving predictions for the Twitter dataset, there is strong need for multilabel training data, silver approach from (Fedorova et al., 2025) might be helpful for future work. Scores on ParlaSent were the highest, especially"
        },
        {
            "title": "Model",
            "content": "Loose Exact Acc. Acc. bos F1 hrv F1 srp F1 OpenLID-v3 GlotLID 46.34 40. 40.65 33.33 57.14 28.17 72.73 55.74 49.15 40.00 Table 2: Loose accuracy, exact match accuracy and loose F1s per languages on multilabel BCS test data (Twitter). Ensemble@1 never agrees (always other, all metrics zero). in precision of Serbian, which proves it to be the easiest dataset. Results of evaluation on reannotated HPLT-LID were similar to those on Twitter: OpenLID-v3 performed the best, and there were high disagreement between it and GlotLID. However, the confusion of Bosnian and Serbian was still high. Table 2 shows multilabel classification metrics on Twitter data. By these metrics, OpenLID-v3 is clear winner over GlotLID."
        },
        {
            "title": "4.1.2 Common Errors",
            "content": "We performed manual analysis of BCS predictions from all models and observed the following seven most frequent error patterns: NE confusion Named entities are common source of confusion . The presence of different country name (e.g. Serbian news article about Croa5 (gold/predicted label) b/h b/s b/x h/b h/s h/x s/b s/h s/x % of total NE confusion lexical overlap historic forms da confusion ungrammatical syntax total ambiguity mislabeled minority rep (other/unknown) 12.0 34.0 - - 10.0 26.0 - 18.0 - - - 66.6 - - 33.3 - 2.0 20.0 - 4.0 32.0 12.0 4.0 4.0 4.0 26.0 36.0 - 10.0 6.0 2.0 16. - - - 100.0 - - - - 2.0 2.0 2.0 - 64.0 20.0 - 10.0 10.0 44.0 - - 6.0 - - 40.0 6.0 12.0 - - 14.0 34.0 - 34.0 8.0 6.0 - - 28.0 26.0 - 32.0 6.1 20.5 5.8 1.7 22.5 19.9 1.2 22. Table 3: V3-ensemble evaluation on ParlaSent; percentage of error occurrences, per sample of 50 mislabeled documents for each mismatched pair of labels. Exceptions are b/s, b/x, and h/s, which have total of 2, 40, and 3 datapoints each, respectively. The bottom row, other, is catch-all for unambiguous documents with clear language markers, which were nevertheless mislabeled. tia), an individual or institution that adheres to the other languages naming convention, or even word type confusion (Tome se niko ne raduje (Nobodys looking forward to that, compared with the common Slovenian name Tome) identified as Slovenian), leads to mislabelling, due to lack of other language markers. Lexical overlap For human reader, the current ortography of jat is strong discriminator between Serbian and Bosnian or Croatian, the former prefering the form in the standard language (e.g. videti, mleko), and the other two using je/ije (vidjeti, mlijeko). However, documents are frequently mislabeled between Bosnian and Serbian, despite the presence of clear e/ije indicators, if the document also contains roots and noun/adjective inflections that are common to both Bosnian and Serbian, whereas Croatian uses diferent surface form of the word (e.g. obavezno (Bosnian, Serbian) vs. obvezatno (Croatian)). The frequency of these error occurrences (lexical overlap in 3) seems to indicate that common roots and shared lexemes are stronger signal than jat ortography. Historic forms Similarly, all models frequently mislabel ParlaSent documents in Croatian as either Bosnian or Serbian, when the speakers slip into historic lexeme forms that were part of the shared Serbo-Croatian language, but are not present in current standard Croatian. Older speakers and colloquial language still frequently use historic forms, which again leads to mislabeling despite the unambiguous grammatical markers. da confusion Another theoretically strong indicator is the difference between the future tense in Croatian (modal verb + infinitive, e.g. (ho)cu glasati, glasat cu) and Serbian (modal verb + da + present simple, e.g. (ho)cu da glasam); both forms are accepted in Bosnian. Frequently, Croatian documents are mislabelled as Serbian when there are multiple occurences of the conjuction da (meaning that; not to be confused with the particle da (yes)). E.g., Ne sumnjam da je ovaj zakon nekima donio dobro ne sumnjam da oni hvale ovaj zakon. (I do not doubt that this legislation brought good to some, and do not doubt that they praise it.) Croatian mislabelled as Serbian vs. Necu da glasam za taj zakon. (I will not / dont want to vote for this legislation.) true Serbian. Ungrammatical syntax Ungrammatical syntax comprises documents in non-standard language; mainly long run-on sentences, with lack of punctuation, incorrect syntax, frequent interjections, and generally language that is highly colloquial in structure and word choice. While this mislabeling occurs across most pairings, these documents are most frequently labeled as not-a-language (zxx_Zxxx), particularly when there is lack of any distinguishing grammatical or lexical language markers. Finally, some datapoints are mislabeled due to extralinguistic factors. Total ambiguity Some documents have no clear language markers between BCS. Most of these documents are mislabeled across models, and would arguably be hard even for human annotator and native speaker to tease apart, due to lack of unambiguous linguistic features. Mislabeled minority representative Mislabeled minority representative is specific to the ParlaSent dataset, where statements are labeled according to the national parliament in which the discussion occurred. In handful of cases, it is clear that the speaker is speaking minority language, and the model has identified it correctly, though it doesnt match the parliament country of origin. We performed quantitative study of these error types on the ParlaSent predictions obtained from the best model (OpenLID-v3 top-1 ensemble with GlotLID). The results are presented in the Table 3. The last row (other/unknown) quantifies unambiguous documents with clear language markers, which were nevertheless mislabeled by the model. This is particularly frequent occurrence for Latin-script Serbian."
        },
        {
            "title": "4.2 Romance Languages of Italy and France",
            "content": "The HPLT 3.0 compilation efforts reported language identification issues connected to Ligurian. 31% of the samples annotated as Occitan in the UDHR dataset were predicted as Ligurian by OpenLID-v2. Upon closer inspection, it turned out that the Occitan part of UDHR contained seven translations three truly Occitan ones and four Francoprovençal ones (see Table 12 in the Appendix). Although Ligurian, Occitan and Francoprovençal are all Romance languages and are spoken in adjacent areas (Southern France, Northwestern Italy and Western Switzerland), they are generally considered to belong to different genealogical sub-groups (Ledgeway and Maiden, 2016; Ramponi, 2024). Since Francoprovençal was missing from OpenLID-v2s class inventory, its instances were erroneously predicted as Ligurian. This suggests that (a) multilingual benchmarks cannot be fully relied on when it comes to closely related, unstandardized languages, and (b) LID tools struggle to identify low-resource languages especially if higher-resourced closely related varieties are present in their training data. We further investigate the performance of OpenLID-v3 on these Romance languages. As we were unable to find test set that covered Francoprovençal, Ligurian and Occitan,9 we use the ITDI dataset instead, which covers several languages and dialects of Italy (Aepli et al., 2022). The metrics were only calculated for Ligurian, Venetian and Friulian, as only these three languages are predicted by both OpenLID-v3 and GlotLID. The results are presented in Table 4. While GlotLID is better than OpenLID-v3 both in precision and recall, ensembling them achieves the best precision and the lowest FPR. For both mod9While there exists an effort on Occitan LID (Miletic and Scherrer, 2022), we could not use it in our experiments, as it contains silver labels only."
        },
        {
            "title": "Model",
            "content": "OpenLID-v3 GlotLID ensemble"
        },
        {
            "title": "FPR",
            "content": "0.01 0.01 0.004 F1 Precision Recall 80.88 82.37 76.68 96.51 97.60 98.47 74.22 75.15 68.95 Table 4: Comparison of OpenLID-v3 and GlotLID on the ITDI test set restricted to Ligurian, Venetian and Friulian (4,744 samples). els, Venetian confusion is the highest with Italian, but OpenLID-v3 predicts fewer Italian false positives and more not-a-language ones (which is never the case for GlotLID). Friulian is mostly predicted correctly, more often with OpenLID-v3 (1,284 true positives) than with GlotLID (1,275). Ligurian is also more often predicted correctly by both models, with Sicilian and Friulian as the top confusion instances. For OpenLID-v3, the third most frequently predicted class is not-a-language. Ensembling the two models removes 45% of Venetian samples, while the number of Ligurian and Friulian ones remains roughly the same. In sum, this case study emphasizes the importance of fine-grained benchmarks for related Romance languages. GlotLID might be good choice as standalone model; however, the most precise results are obtained when it is ensembled with OpenLID-v3, although at the price of removing samples from other easily confused languages such as Venetian."
        },
        {
            "title": "4.3 Scandinavian Languages",
            "content": "Lastly, we focus on the North Germanic languages of Mainland Scandinavia, i.e. Norwegian Bokmål, Norwegian Nynorsk, Danish and Swedish. In addition to FLORES+ and UDHR, which cover all four languages, we experiment with two languagegroup-specific datasets: First, we use SLIDE (Fedorova et al., 2025), multilabel dataset based on clean instances from Universal Dependencies (Nivre et al., 2020) (6,950 samples in total), containing many other languages. Second, we use Nordic DSL (Haas and Derczynski, 2021), single-label dataset containing noisy sentences sourced from Wikipedia and featuring Faroese and Icelandic, in addition to the four aforementioned languages. We did not use the FastSpell dataset for the calculation of metrics, as upon manual examination, we noticed large share of its Nynorsk samples to be valid in both Bokmål and Nynorsk. We re-"
        },
        {
            "title": "SLIDE",
            "content": "OpenLID-v3 OpenLID-v2 GlotLID Ensemble"
        },
        {
            "title": "Nordic DSL",
            "content": "OpenLID-v3 OpenLID-v2 GlotLID Ensemble FLORES+ OpenLID-v3 OpenLID-v2 GlotLID Ensemble"
        },
        {
            "title": "UDHR",
            "content": "OpenLID-v3 OpenLID-v2 GlotLID Ensemble Norwegian Bokmål Rec. Prec. FPR n=2,098 0.04 0.05 0.047 0.027 88.73 87.33 88.32 92.56 87.04 88.04 90.47 84."
        },
        {
            "title": "FPR",
            "content": "0.02 0.01 0.02 0.01 Danish Prec. n=677 Norwegian Nynorsk Rec. Prec. FPR"
        },
        {
            "title": "FPR",
            "content": "Rec. n=1,628 Swedish Prec. n=1,250 Rec."
        },
        {
            "title": "FPR",
            "content": "Other Prec. n=1,745 Rec. 81.81 85.27 80.33 88.85 83.75 80.35 86.26 81.24 0.03 0.03 0.008 0. 89.66 90.47 96.57 97.37 84.64 84.58 81.33 79.55 0.008 0.008 0.009 0.003 96.11 95.88 95.83 98.29 96.80 96.8 97.44 96.8 0.03 0.03 0.03 0. 89.68 89.63 92.07 75.86 95.13 95.58 99.20 99.77 n=14,960 n=14,960 n=14,960 n=14, n=14,960 0.007 0.02 0.01 0.005 96.04 91.85 94.58 97.19 83.22 85.37 94.50 82.78 0.012 0.007 0.006 0.003 94.10 96.12 97.09 98. 93.56 89.78 93.77 91.82 0.017 0.02 0.006 0.004 91.98 91.97 97.20 98.13 96.87 96.75 96.47 95.92 0.002 0.002 0.004 0.001 99.07 98.77 98.16 99. 93.33 92.73 93.69 92.01 0.039 0.04 0.025 0.079 92.72 92.82 95.16 86.32 99.44 99.56 99.36 99.74 n=1,012 n=1, n=1,012 n=1,012 n=212,520 1e-4 1e-4 1e-4 7e-5 4e-5 7e-5 4e-5 4e-5 97.61 96.84 97.10 98. n=62 98.39 96.88 98.41 98.39 96.84 96.84 99.41 96.74 98.39 1.0 1.0 98.39 3e-5 2e-5 1e-5 6e-6 4e-5 7e-5 1e-4 99.31 99.60 99.70 99.80 n=61 98.36 96.77 93.75 1.0 99.21 98.12 99.51 98.91 98.36 98.36 98.36 98.36 7e-5 9e-5 1e-5 1e8e-5 4e-5 4e-5 0 98.42 98.12 99.67 99.70 n=58 96.61 98.28 98.31 1.0 98.22 98.22 97.53 97.23 98.28 98.28 1.0 98. 0 0 0 0 7e-5 1e-4 4e-5 0 1.0 1.0 1.0 1.0 n=61 96.83 95.31 98.39 1.0 99.90 99.90 1.0 99. 3e-3 0.004 0 1e-2 99.99 99.99 1.0 99.98 1.0 99.99 1.0 1.0 n=27,515 1.0 1.0 1.0 1.0 8e-3 0.004 0 8e99.99 99.99 1.0 99.99 99.98 99.97 99.98 1.0 Table 5: Performance comparison across four benchmark datasets showing the false positive rate (FPR), precision (Prec.), and recall (Rec.) for Scandinavian and other languages. OpenLID-v3 uses softmax threshold 0.5. Ensemble represents top-1 agreement between OpenLID-v3 and GlotLID. Best values per metric and language are in bold. annotated this subset and evaluated it separately. Table 5 summarizes the results. We find that GlotLID obtains the best recall, while OpenLID shows better false positive rate and precision. Precision and FPR are further improved by ensembling GlotLID and OpenLID-v3. OpenLID-v3 performs on par or slightly better than OpenLID-v2 on all benchmarks. Furthermore, the confusion matrices (available in the repository10) show that the two Norwegian varieties, Bokmål and Nynorsk, are most easily confused. Appendix provides additional results: Table 14 shows the effect of applying softmax threshold across all models and datasets, and Table 13 shows the effect of ensembling across datasets. Both tables report multilabel classification metrics. HPLT-LID The HPLT 3.0 manual inspection data (Oepen et al., 2025) only covers one Scandinavian language, namely Norwegian Bokmål. 402 documents were annotated as being correctly predicted Bokmål or not. We process this annotation in the same way as we did for BCMS annotation, excluding porn, unnatural language and artifacts. Documents with correctly identified language formed the dataset of documents guaranteed 10https://github.com/ltgoslo/slide/tree/main/ src/eval_logs/vardial_2026 to be Bokmål, with 304 samples in total. 5 documents were annotated to be in language other than Bokmål. We manually reviewed predictions of these 5 false positive Bokmål documents. Both OpenLID-v3 and GlotLID still predict them to be Bokmål. 3 of them are non-fluent Bokmål documents with many grammatical mistakes (articles of the wrong gender, articles and verbs omitted), as if written by foreign speakers, and 2 include codeswitching, with Bokmål still being the predominant language. FastSpell. native speaker of Norwegian relabeled the FastSpell Nynorsk subset in multilabel way, finding 40% of samples to be (also) valid Bokmål. The annotator also reported that many word sequences were not full sentences and some texts were just menu choices from web page. The evaluation of OpenLID-v3 and GlotLID on the resulting dataset is presented in Table 6. While both models performed poorly in Bokmål detection, OpenLID-v3 performed slightly better for Nynorsk and produced fewer Danish and Swedish false positives, but more other false positives than GlotLID. The manual analysis of the predictions showed that errors are often caused by named entities valid in any language using Latin script (Georg Jojuli 2018) hannes Toft), dates (Levering 15."
        },
        {
            "title": "Model",
            "content": "NB F1 NN F1 DA FP SV FP Other FP OpenLID-v3 GlotLID 71.90 79.07 92.19 91.14 16 27 8 52 16 Table 6: GlotLID (no threshold) and OpenLID-v3 (softmax threshold 0.5) on the re-annotated FastSpell Nynorsk data. F1 is for loose F1, FP is for false positives. 406 Nynorsk samples, 163 Bokmål samples. while consisting of Wikipedia data, was heavily preprocessed by its authors. common limitation of all models under evaluation is the unavailability of fully parallel datasets for all languages of interest, which might cause models to overfit to some concepts and named entities that are more common in certain languages. and foreign words (tomter Parque natural cabo de gata). These findings correspond to the error analysis from Fedorova et al. (2025) and further prove that shorter sequences, especially from the web, are the hardest for LID models. In conclusion, we recommend OpenLID-v3 for general Scandinavian language identification if precision is prioritized, and GlotLID if recall is prioritized. For discriminating between Bokmål and Nynorsk, it is recommended to use LID model trained specifically to separate these languages."
        },
        {
            "title": "5 Conclusion",
            "content": "We have evaluated the performance of three LID models (two OpenLID versions and GlotLID) on four multilingual benchmarks (two based on clean text and two based on web documents). Our new model, OpenLID-v3, performs on par or better in precision compared to its predecessors OpenLID-v2 and GlotLID, while the best precision is achieved by top-1 ensembling with GlotLID. We have also shown that reliable evaluation of LID models including similar languages should be done on benchmarks specific to these languages, even if overall metrics on large multilingual benchmarks are high. Since web data may contain short texts valid in more than one language, there is need for more multilabel training data and benchmarks."
        },
        {
            "title": "Limitations",
            "content": "The first limitation that we acknowledge is data mismatch between the evaluation and the intended use we would ideally evaluate on web text. However, large-scale web LID data of sufficient quality were not available at the time of conducting our experiment. We are aware of the initiative by Suarez et al. (2026), but leave evaluation on their dataset for future work. We have done our best to control for data contamination, where there was known intersection of training and test data. However, this was not possible for tests on Nordic DSL, as this dataset,"
        },
        {
            "title": "Ethical Considerations",
            "content": "All the new data annotations were done by the authors voluntarily and without any monetary compensation. We did not check any of OpenLID-v3s training data for inappropriate or biased content. We believe it does not do much harm, since the model is not generative. However, we can imagine some model predictions to be biased, particularly if used not as classifier, but as source of semantic representations. large amount of the current research is focused on collecting data for training large instructiontuned generative models, capable of outputting grammatically and orthographically correct standard language, because of the high demand for such models in the public sector. This may result in loss of data written in low-resource varieties of mainstream languages, along with possibly valuable cultural knowledge."
        },
        {
            "title": "Acknowledgements",
            "content": "This project was supported by the European Union Horizon Europe project no. 101070350 (HPLT). The computations were performed on resources provided by Sigma2 the National Infrastructure for High-Performance Computing and Data Storage in Norway."
        },
        {
            "title": "References",
            "content": "Ife Adebara, AbdelRahim Elmadany, Muhammad Abdul-Mageed, and Alcides Inciarte. 2022. AfroLID: neural language identification tool for African languages. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 19581981, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Noëmi Aepli, Antonios Anastasopoulos, Adrian Chifu, William Domingues, Fahim Faisal, Mihaela Gaman, Radu Tudor Ionescu, and Yves Scherrer. 2022. Findings of the VarDial evaluation campaign 2022. In Proceedings of the Ninth Workshop on NLP for Similar Languages, Varieties and Dialects, Gyeongju, 9 Republic of Korea. International Committee on Computational Linguistics (ICCL). Noëmi Aepli, Çagrı Çöltekin, Rob Van Der Goot, Tommi Jauhiainen, Mourhaf Kazzaz, Nikola Ljubešic, Kai North, Barbara Plank, Yves Scherrer, and Marcos Zampieri. 2023. Findings of the VarDial evaluation campaign 2023. In Tenth Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial 2023), pages 251261, Dubrovnik, Croatia. Association for Computational Linguistics. Milind Agarwal, Md Mahfuz Ibn Alam, and Antonios Anastasopoulos. 2023. LIMIT: Language identification, misidentification, and translation using hierarchical models in 350+ languages. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1449614519, Singapore. Association for Computational Linguistics. Evershed Amuzu and John Singler. 2014. Codeswitching in west africa. International Journal of Bilingualism, 18:329345. Marta Bañón, Gema Ramírez-Sánchez, Jaume Zaragoza-Bernabeu, and Sergio Ortiz Rojas. 2024. FastSpell: The LangId magic spell. In Proceedings the 2024 Joint International Conference on of Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 71337140, Torino, Italia. ELRA and ICCL. Laurie Burchell, Alexandra Birch, Nikolay Bogoychev, and Kenneth Heafield. 2023. An open dataset and model for language identification. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 865879, Toronto, Canada. Association for Computational Linguistics. Laurie Burchell, Ona de Gibert, Nikolay Arefyev, Mikko Aulamo, Marta Bañón, Pinzhen Chen, Mariia Fedorova, Liane Guillou, Barry Haddow, Jan Hajiˇc, Jindˇrich Helcl, Erik Henriksson, Mateusz Klimaszewski, Ville Komulainen, Andrey Kutuzov, Joona Kytöniemi, Veronika Laippala, Petter Mæhlum, Bhavitvya Malik, and 16 others. 2025. An expanded massive multilingual dataset for high-performance language technologies (HPLT). In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1745217485, Vienna, Austria. Association for Computational Linguistics. Isaac Caswell, Theresa Breiner, Daan van Esch, and Ankur Bapna. 2020. Language ID in the wild: Unexpected challenges on the path to thousand-language web text corpus. In Proceedings of the 28th International Conference on Computational Linguistics, pages 65886608, Barcelona, Spain (Online). International Committee on Computational Linguistics. Adrian-Gabriel Chifu, Goran Glavaš, Radu Tudor Ionescu, Nikola Ljubešic, Aleksandra Miletic, Filip Miletic, Yves Scherrer, and Ivan Vulic. 2024. VarDial evaluation campaign 2024: Commonsense reasoning in dialects and multi-label similar language identification. In Proceedings of the Eleventh Workshop on NLP for Similar Languages, Varieties, and Dialects (VarDial 2024), pages 115, Mexico City, Mexico. Association for Computational Linguistics. Ona de Gibert, Graeme Nail, Nikolay Arefyev, Marta Bañón, Jelmer van der Linde, Shaoxiong Ji, Jaume Zaragoza-Bernabeu, Mikko Aulamo, Gema RamírezSánchez, Andrey Kutuzov, Sampo Pyysalo, Stephan Oepen, and Jörg Tiedemann. 2024. new massive multilingual dataset for high-performance language technologies. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LRECCOLING 2024), pages 11161128, Torino, Italia. ELRA and ICCL. Mariia Fedorova, Jonas Sebulon Frydenberg, Victoria Handford, Victoria Ovedie Chruickshank Langø, Solveig Helene Willoch, Marthe Løken Midtgaard, Yves Scherrer, Petter Mæhlum, and David Samuel. 2025. Multi-label Scandinavian lanIn Proceedings of guage identification (SLIDE). the Third Workshop on Resources and Representations for Under-Resourced Languages and Domains (RESOURCEFUL-2025), pages 179189, Tallinn, Estonia. University of Tartu Library, Estonia. Mihaela Gaman, Dirk Hovy, Radu Tudor Ionescu, Heidi Jauhiainen, Tommi Jauhiainen, Krister Lindén, Nikola Ljubešic, Niko Partanen, Christoph Purschke, Yves Scherrer, and Marcos Zampieri. 2020. report on the VarDial evaluation campaign 2020. In Proceedings of the 7th Workshop on NLP for Similar Languages, Varieties and Dialects, pages 114, Barcelona, Spain (Online). International Committee on Computational Linguistics (ICCL). Cyril Goutte, Serge Léger, and Marine Carpuat. 2014. The NRC system for discriminating similar lanIn Proceedings of the First Workshop on guages. Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 139145, Dublin, Ireland. Association for Computational Linguistics and Dublin City University. René Haas and Leon Derczynski. 2021. Discriminating between similar Nordic languages. In Proceedings of the Eighth Workshop on NLP for Similar Languages, Varieties and Dialects, pages 6775, Kiyv, Ukraine. Association for Computational Linguistics. Tommi Jauhiainen, Krister Lindén, and Heidi Jauhiainen. 2016. HeLI, word-based backoff method for language identification. In Proceedings of the Third Workshop on NLP for Similar Languages, Varieties and Dialects (VarDial3), pages 153162, Osaka, Japan. The COLING 2016 Organizing Committee. Tommi Jauhiainen, Marco Lui, Marcos Zampieri, Timothy Baldwin, and Krister Lindén. 2019. Automatic language identification in texts: survey. Journal of Artificial Intelligence Research, 65:675782. 10 Armand Joulin, Edouard Grave, Piotr Bojanowski, and Tomas Mikolov. 2017. Bag of tricks for efficient text classification. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 2, Short Papers, pages 427431, Valencia, Spain. Association for Computational Linguistics. Zenaida Karavdic. 2017. Kako prepoznati bosanski, crnogorski, hrvatski srpski standardni jezik? BOSNIACA-ˇcasopis Nacionalne univerzitetske biblioteke Bosne Hercegovine, (22):3446. Amir Hossein Kargaran, Ayyoob Imani, François Yvon, and Hinrich Schuetze. 2023. GlotLID: Language identification for low-resource languages. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 61556218, Singapore. Association for Computational Linguistics. Tom Kocmi and Ondˇrej Bojar. 2017. LanideNN: Multilingual language identification on character window. In Proceedings of the 15th Conference of the European Chapter of the Association for Computational Linguistics: Volume 1, Long Papers, pages 927936, Valencia, Spain. Association for Computational Linguistics. Adam Ledgeway and Martin Maiden. 2016. The Oxford Guide to the Romance Languages. Oxford University Press. Nikola Ljubešic and Peter Rupnik. 2022a. The news dataset for discriminating between bosnian, croatian and serbian SETimes.HBS 1.0. Slovenian language resource repository CLARIN.SI. Nikola Ljubešic and Peter Rupnik. 2022b. The twitter user dataset for discriminating between bosnian, croatian, montenegrin and serbian twitter-HBS 1.0. Slovenian language resource repository CLARIN.SI. Marco Lui and Timothy Baldwin. 2012. langid.py: An off-the-shelf language identification tool. In Proceedings of the ACL 2012 System Demonstrations, pages 2530, Jeju Island, Korea. Association for Computational Linguistics. Emira Mešanovic-Meša. 2011. Kontrastivna analiza bosanskog, hrvatskog srpskog jezika zakonima Federacije Bosne Hercegovine. Slavistiˇcki komitet BiH. Aleksandra Miletic and Filip Miletic. 2024. gold standard with silver linings: Scaling up annotation for distinguishing bosnian, croatian, montenegrin and In Proceedings of the Fourth Workshop serbian. on Human Evaluation of NLP Systems (HumEval)@ LREC-COLING 2024, pages 3646."
        },
        {
            "title": "Aleksandra Miletic",
            "content": "and Yves Scherrer. 2022. OcWikiDisc: corpus of Wikipedia talk pages in In Proceedings of the Ninth Workshop Occitan. on NLP for Similar Languages, Varieties and Dialects, pages 7079, Gyeongju, Republic of Korea. Association for Computational Linguistics. Michal Mochtak, Peter Rupnik, and Nikola Ljubešic. 2023. The parlasent multilingual training dataset for sentiment identification in parliamentary proceedings. (arXiv:2309.09783). ArXiv:2309.09783 [cs]. Joakim Nivre, Marie-Catherine de Marneffe, Filip Ginter, Jan Hajiˇc, Christopher D. Manning, Sampo Pyysalo, Sebastian Schuster, Francis Tyers, and Daniel Zeman. 2020. Universal Dependencies v2: An evergrowing multilingual treebank collection. In Proceedings of the Twelfth Language Resources and Evaluation Conference, pages 40344043, Marseille, France. European Language Resources Association. NLLB Team, Marta R. Costa-jussà, James Cross, Onur Çelebi, Maha Elbayad, Kenneth Heafield, Kevin Heffernan, Elahe Kalbassi, Janice Lam, Daniel Licht, Jean Maillard, Anna Sun, Skyler Wang, Guillaume Wenzek, Al Youngblood, Bapi Akula, Loic Barrault, Gabriel Mejia Gonzalez, Prangthip Hansanti, and 20 others. 2022. No language left behind: Scaling human-centered machine translation. Preprint, arXiv:2207.04672. Stephan Oepen, Nikolay Arefev, Mikko Aulamo, Marta Bañón, Maja Buljan, Laurie Burchell, Lucas Charpentier, Pinzhen Chen, Mariia Fedorova, Ona de Gibert, Barry Haddow, Jan Hajiˇc, Jindˇrich Helcl, Andrey Kutuzov, Veronika Laippala, Zihao Li, Risto Luukkonen, Bhavitvya Malik, Vladislav Mikhailov, and 13 others. 2025. HPLT 3.0: Very large-scale multilingual resources for LLM and MT. monoand bilingual data, multilingual evaluation, and pre-trained models. Preprint, arXiv:2511.01066. Guilherme Penedo, Hynek Kydlíˇcek, Loubna Ben allal, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, and Thomas Wolf. 2024. The fineweb datasets: Decanting the web for the finest text data at scale. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track. Guilherme Penedo, Hynek Kydlíˇcek, Vinko Sabolˇcec, Bettina Messmer, Negar Foroutan, Amir Hossein Kargaran, Colin Raffel, Martin Jaggi, Leandro Von Werra, and Thomas Wolf. 2025. Fineweb2: One pipeline to scale them all adapting pre-training Preprint, data processing to every language. arXiv:2506.20920. Alan Ramponi. 2024. Language varieties of italy: Technology challenges and opportunities. Transactions of the Association for Computational Linguistics, 12:19 38. Daniel Romic. 2024. Heritage bosnian, croatian, and serbian spoken by second generation speakers in germany he-bcs-ge. Peter Rupnik, Taja Kuzman, and Nikola Ljubešic. 2023. BENCHic-lang: benchmark for discriminating between Bosnian, Croatian, Montenegrin and Serbian. In Tenth Workshop on NLP for Similar Languages, 11 Varieties and Dialects (VarDial 2023), pages 113 120, Dubrovnik, Croatia. Association for Computational Linguistics. Pedro Ortiz Suarez, Laurie Burchell, Catherine Arnett, Rafael Mosquera-Gómez, Sara Hincapie-Monsalve, Thom Vaughan, Damian Stewart, Malte Ostendorff, Idris Abdulmumin, Vukosi Marivate, Shamsuddeen Hassan Muhammad, Atnafu Lambebo Tonja, Hend Al-Khalifa, Nadia Ghezaiel Hammouda, Verrah Otiende, Tack Hwa Wong, Jakhongir Saydaliev, Melika Nobakhtian, Muhammad Ravi Shulthan Habibi, and 78 others. 2026. Commonlid: Reevaluating state-of-the-art language identification performance on web data. Preprint, arXiv:2601.18026. Marcos Zampieri, Liling Tan, Nikola Ljubešic, and Jörg Tiedemann. 2014. report on the DSL shared task 2014. In Proceedings of the First Workshop on Applying NLP Tools to Similar Languages, Varieties and Dialects, pages 5867, Dublin, Ireland. Association for Computational Linguistics and Dublin City University."
        },
        {
            "title": "A Multilingual Benchmarks",
            "content": "In all evaluations on all datasets, metrics were only calculated on subset of languages, present in all models being compared. For GlotLID, we always used its the most recent version at the moment of writing, GlotLID-v3. Table 7 shows statistics of the HPLT-LID dataset. Only those samples were accepted, which were not unnatural language, porn, web artifacts or incorrect LID. One must notice that, while this dataset contains texts from the web, it can not be used as benchmark of noisy web data, because the texts were heavily cleaned and preprocessed before their annotation was performed. We believe however that HPLT-LID may be valuable for the community, because there exist not many LID benchmarks for some languages presented there.. Table 8 presents evaluation of all models (except OpenLID-v2, because the dataset was created with its help) under consideration on HPLT-LID. This is just another proof that there were no regression in performance of OpenLID-v3 compared to OpenLID-v2."
        },
        {
            "title": "B Additional Data Sources",
            "content": "Table 10 shows the difference between OpenLIDv2 and OpenLID-v3 in terms of training data sources. Languages outside OpenLID-v2 class inventory. We discussed adding natural language other class (languages present in glotlid-corpus other than our"
        },
        {
            "title": "Language",
            "content": "# docs Language # docs ast_Latn bos_Latn cat_Latn cmn_Hans ces_Latn deu_Latn ell_Grek eng_Latn fra_Latn fin_Latn glg_Latn hin_Deva 139 49 196 183 183 179 613 126 139 335 182 136 hrv_Latn ita_Latn jpn_Jpan nob_Latn pes_Arab por_Latn rus_Cyrl spa_Latn slk_Latn srp_Cyrl yor_Latn total 188 171 87 304 209 111 362 300 127 15 168 4, Table 7: HPLT 3.0 manual inspection dataset statistics."
        },
        {
            "title": "Model",
            "content": "OpenLID-v3, 0.5 OpenLID-v3 GlotLID, 0.5 GlotLID ensemble"
        },
        {
            "title": "FPR",
            "content": "4e-5 6e-5 4e-5 4e-5 1e-5 F1 Precision Recall 99.84 99.84 99.51 99.55 99.53 99.91 99.85 99.89 99.89 99.96 99.78 99.84 99.16 99.24 99.14 Table 8: Evaluation on HPLT-LID (n=3,873), with and without softmax threshold. 190+) for collecting garbage documents and outof-train-set languages. However, after looking closer into the data, just sampling from about 1,900 those languages and labeling them all as \"other\" did not seem good idea. About 1,500 of those language were low-resource ones where only Bibles were available, reported as non-reliable data by GlotLIDs authors 11."
        },
        {
            "title": "The smallest",
            "content": "After removing non-reliable sources, 366 languages were left. language in OpenLID-v2 dataset is Yiddish with 923 samples only. Out of 366, we have 150 languages with more samples than that. Some of them were quite large like Low German (nds_Latn) with 117,676 samples (many languages in OpenLID-v2 dataset have less), national like Romansh (roh_Latn) or have more than million speakers like Chechnyan (che_Cyrl). For the future OpenLID improvement, we recommend extending the model with these languages as separate labels instead of pushing them all into \"other\". Not all languages with many samples are expected to be common in crawls: some of them 11https://github.com/cisnlp/GlotLID/blob/main/ sources.md"
        },
        {
            "title": "FastSpell",
            "content": "OpenLID-v3, 0.5 OpenLID-v3 OpenLID-v2, 0.5 OpenLID-v2 GlotLID, 0.5 GlotLID ensemble FLORES+ OpenLID-v3, 0.5 OpenLID-v3 OpenLID-v2, 0.5 OpenLID-v2 GlotLID, 0.5 GlotLID ensemble"
        },
        {
            "title": "UDHR",
            "content": "OpenLID-v3, 0.5 OpenLID-v3, OpenLID-v2, 0.5 OpenLID-v2 GlotLID, 0.5 GlotLID ensemble"
        },
        {
            "title": "FPR",
            "content": "F1 Precision Recall n=6,809 2.5e-3 2.9e-3 3.9e-3 4.2e-3 3.0e-3 3.5e-3 1.5e-3 7.6e-5 8.6e-5 8.9e-5 1.0e-4 3.4e-5 3.8e-5 2.6e-5 1.5e-4 1.8e-4 2.5e-4 3.0e-4 6.7e-5 8.7e-5 4.5e-5 91.08 91.22 90.04 90.37 91.32 91.39 90. 95.72 95.23 94.55 94.05 95.19 94.63 97.11 n=155,848 97.72 97.74 98.09 98.11 98.34 98.38 97.01 97.86 97.70 98.62 98.46 99.52 99.46 98.33 n=10,283 96.14 96.21 96.27 96.21 97.39 97.37 95. 97.13 96.87 97.26 96.93 98.96 98.78 98.07 89.56 90.32 89.07 90.11 90.24 91.00 87.78 97.71 97.89 97.98 98.16 97.95 98.07 96.5 96.39 96.22 96.73 96.91 96.76 96.87 94.58 Table 9: Multilingual benchmarks. F1, precision and recall in %. 0.5 stands for thresholding softmax scores at 0.5. Ensembling experiments are done with thresholded OpenLID-v3 and GlotLID without thresholding. FLORES+ refers to the devtest split, and FastSpell refers to our version that excludes Nynorsk. are not alive like Classical Chinese (lzh_Hani) or constructed like Interlingua (ina_Latn). Since OpenLID already featured Esperanto and Latin, we believe other such language are also worth being edited. Also we found more languages present in both Cyrillic and Latin script, like Serbian: Tatar, for example. Training data for zxx_Zxxx. und_* are random sequences generated from different scripts.12 zxx_* are non-linguistic noise in different scripts collected from the web.13 (We experimented with adding them as separated classes first; while our validation data had no such class, some languages outside our class inventory were classified as one of this classes; the choice between the two was random according to manual inspection.) Bosnian, Croatian, Montenegrin and"
        },
        {
            "title": "Serbian",
            "content": "Other benchmarks We are aware of BCS news dataset (SETimes) (Ljubešic and Rupnik, 2022a), comprising roughly 9,000 parallel documents written in each of the languages, from now-inactive news site that published articles in the languages of South-Eastern Europe. However, SETimes data were present among both OpenLID and GlotLIDs training data, but their splitting into sentences seem to be done in different ways. While we succeeded in deduplicating the test data from GloLIDs training instances, we scored suspiciously high with OpenLID (more than 90% F1), which was never observed on other benchmarks. This is clear sign that our deduplication has not worked. Thus, we only report GlotLID results here and leave proper evaluation on SETimes for future work. It is an important benchmark, because it comprises multisentence documents; average document length for each combination of gold and assigned label is given in Table 11. Generally, the common wisdom that longer documents lead to better classification holds here as well, mainly because longer documents give more opportunities for clear grammatical markers and discriminators to arise. Short sentences are much more likely to be mislabeled, across all models and datasets. Note that Serbian (srp, bottom row) doesnt follow the pattern of 12https://github.com/cisnlp/GlotLID/blob/main/ assets/train/gen_und.py 13https://huggingface.co/datasets/cis-lmu/ glotlid-corpus/tree/main/zxx"
        },
        {
            "title": "Justification",
            "content": "the newest Wikipedia commonvoice, GlotStoryBook, leipzigwiki, tatoeba, ud commonvoice, leipzigwiki, tatoeba ud, all OpenLID-v2 dyu data commonvoice, GlotStoryBook, leipzigwiki, tatoeba, ud ast_Latn bam_Latn ben_Beng bod_Tibt GlotStoryBook, tatoeba, the newest Wikipedia cat_Latn dan_Latn GlotStoryBook, leipzigwiki, tatoeba, ud dzo_Tibt frp_Latn HFWikipedia fuv_Latn tatoeba gug_Latn commonvoice, leipzigwiki, tatoeba, ud heb_Hebr leipzigwiki, tatoeba, ud kan_Knda GlotStoryBook, leipzigwiki, tatoeba kin_Latn lat_Latn lij_Latn oci_Latn run_Latn masakhanews, tatoeba, the newest Wikipedia rus_Cyrl som_Latn GlotStoryBook, leipzigwiki, masakhanews, tatoeba srp_Latn ssw_Latn GlotStoryBook, Vukuzenzele, the newest Wikipedia sun_Latn tam_Taml zxx_Zxxx afriqa, commonvoice, GlotStoryBook, tatoeba tatoeba, ud, GlotStoryBook, leipzigwiki the newest Wikipedia removing OpenLIDs pilar leipzigwiki, nusa, tatoeba commonvoice, GlotStoryBook, leipzigwiki, tatoeba, ud zxx_*, und_* commonvoice, GlotStoryBook, leipzigwiki, tatoeba, ud ud, setimes, tatoeba high confusion with other high confusion with dyu_Latn small in HPLT 3.0 high confusion with dzo_Tibt some confusion with oci_Latn high confusion with nob_Latn high confusion with bod_Tibt missed in OpenLID-v2, high confusion with lij_Latn high confusion with other poor in HPLT 3.0 minor confusion with underrepresented Yiddish small in HPLT 3.0 high confusion with run_Latn missed in OpenLID-v2 poor in HPLT 3.0 high confusion with lij_Latn high confusion with kin_Latn high confusion with other high confusion with other missed in OpenLID-v2, high confusion with Croatian and Bosnian high confusion with other high confusion with other small in HPLT 3.0 missed in OpenLID-v2 Table 10: OpenLID-v3 added/removed training data, compared to OpenLID-v2; added data were taken from GlotLID-corpus, if another not specified. (gold/pred) bos hrv srp bos hrv srp 1,922 534 6,129 571 1,833 468 - - 3, Table 11: GlotLID evaluation on SETimes; average document length by character count, across correctly and incorrectly labeled data. longer documents being more likely to be correctly labeled. In addition to performance on Latin-script Serbian generally being lower, this discrepancy is likely due to common lexical overlap between Bosnian and Serbian, exacerbated by document length, and the model seemingly giving preference to the bos label, discussed further in this section. We also acknowledge the existence of the COPA (Rupnik et al., 2023) and Heritage BCS (Romic, 2024) datasets, but, as the Serbian portion of COPA uses the standard Cyrillic script only, it was inappropriate for our evaluation; and Heritage BCS data comprise transcripts of bilingual speakers conversations, there is too much second-language interference in the documents for meaningful evaluation on LID task. Montenegrin GlotLID predictions Interestingly, there are handful of mislabeled documents that GlotLID labels as Montenegrin (cnr) or Chakavian (ckm, dialect of Croatian). Both the first 14 and the second 15 feature nonstandard syntax and repetitive conjunctions and adjectives; both are, respectively, typical of Chakavian and Montenegrin poetry, which feature disproportionately in training data. HPLT-LID reannotation details 201 samples were predicted by OpenLID-v2 to be Serbian Cyrillic, out of which 151 were pre-annotated to be false positives in the corresponding repository, but turned out to be missing annotation rather then false. 21 samples were detected to be translationese from Serbian Cyrillic and annotated with new tag tra_Zxxx; 3 samples were found to be only valid in Montenegrin and annotated with cnr_Latn."
        },
        {
            "title": "France",
            "content": "Table 12 lists the Francoprovençal and Occitan translations of the UDHR and their corresponding URLs. 14Dijagnoza je toˇcna, nisam siguran da je terapija, nisam siguran da je terapija ispravna. 15Bijele noci, pa normalno da su bijele noci smetale, sve što je bijelo smetalo je, mraku se puno bolje rade ovakvi poslovi."
        },
        {
            "title": "URL",
            "content": "Occitan Auvergnat Occitan Languedocien Occitan Provençal Francoprovençal Fribourg Francoprovençal Savoie Francoprovençal Vaud Francoprovençal Valais http://efele.net/udhr/d/udhr_auv.txt http://efele.net/udhr/d/udhr_lnc.txt http://efele.net/udhr/d/udhr_prv.txt http://efele.net/udhr/d/udhr_oci_1.txt http://efele.net/udhr/d/udhr_oci_2.txt http://efele.net/udhr/d/udhr_oci_3.txt http://efele.net/udhr/d/udhr_oci_4.txt Table 12: Occitan and Francoprovençal UDHR translations and their URLs. We have not found significant difference in the performance of OpenLID-v3 and the hierarchical models in any of the related language groups when evaluating on FLORES+ or UDHR. We hypothesize that the capacity of the large LID model is not saturated yet and therefore there is nothing to be gained by using subset of the training data. Instead, effort should be put into securing larger and higher-quality annotated data for these language groups."
        },
        {
            "title": "G List of Supported Languages",
            "content": "Table 15 shows the list of language labels supported by OpenLID-v3."
        },
        {
            "title": "E Scandinavian Languages",
            "content": "Table 13 presents ensemble results combining GlotLID (no threshold) and OpenLID-v3 (softmax threshold 0.5). Table 14 shows performance comparison for different versions of OpenLID with GlotLID on the SLIDE, Nordic DSL, FLORES+ devtest, and UDHR test sets with and without thresholding."
        },
        {
            "title": "F Hierarchical Models",
            "content": "Hierarchical LID systems decompose the classification taks into series of decision steps, typically progressing from broad language groups to the individual languages or variants (Jauhiainen et al., 2019; Agarwal et al., 2023). The motivation for these approaches is that closely related languages are naturally more challenging to discriminate for LID models (Goutte et al., 2014). We trained hierarchical fastText-based models and evaluated their effect on the following language groups: Scandinavian languages (Danish, Faroese, Icelandic, Norwegian Bokmål, Norwegian Nynorsk, and Swedish), South Slavic languages (Bosnian, Croatian, Serbian in Latin script), along with varieties of Arabic (Egyptian, Levantine, Mesopotamian, Moroccan, Najdi, Standard Arabic, Taizzi-Adeni, and Tunisian), and Persian (Dari and Iranian Persian). For each group, we train specialized language classifier on the subset of the OpenLID-v3 corpus composed of these languages. During inference, every time the base model predicts language from one of the four groups, we replace the base model prediction with the prediction of the specialized model. The specialized models follow the same architecture and training procedure as OpenLID-v3. For each language group, we use the corresponding language-specific subset of the OpenLID-v3 training data."
        },
        {
            "title": "Dataset",
            "content": "Loose Acc. Exact Acc. NB F1 DA F1 NN F1 SV F1 Other F1 SLIDE Nordic DSL FLORES+ UDHR 94.62 93.66 99.97 99.99 92.30 93.66 99.97 99.99 92.52 89.41 97.56 98. 90.53 94.89 99.35 99.17 91.52 97.01 98.45 99.13 98.21 95.54 99.95 100.00 91.70 92.54 99.99 100.00 Table 13: Ensembling GlotLID (no threshold) and OpenLID-v3 (softmax threshold 0.5) on Scandinavian languages. Bold indicates improvement over both individual models. Italic indicates improvement over OpenLID-v3 only. Thres. Model Loose Acc. Exact Acc. NB F1 DA F1 NN F1 SV F1 Other F"
        },
        {
            "title": "SLIDE",
            "content": "No 0.5 OpenLID-v3 OpenLID-v2 OpenLID-v1 GlotLID OpenLID-v3 OpenLID-v2 OpenLID-v1 GlotLID samples Nordic DSL (from 50k split) No 0. OpenLID-v3 OpenLID-v2 OpenLID-v1 GlotLID OpenLID-v3 OpenLID-v2 OpenLID-v1 GlotLID samples FLORES+ devtest No 0."
        },
        {
            "title": "UDHR",
            "content": "No 0.5 OpenLID-v3 OpenLID-v2 OpenLID-v1 GlotLID OpenLID-v3 OpenLID-v2 OpenLID-v1 GlotLID samples OpenLID-v3 OpenLID-v2 OpenLID-v1 GlotLID OpenLID-v3 OpenLID-v2 OpenLID-v1 GlotLID samples 94.46 94.39 93.61 97.20 95.55 95.63 94.81 97.09 6,950 94.71 94.32 94.17 96.19 94.30 93.96 93.88 96.10 74, 99.97 99.97 99.97 99.98 99.97 99.97 99.97 99.98 216,568 99.75 99.74 99.79 99.97 99.97 99.97 99.97 99.99 27,757 90.55 90.36 89.77 93.40 91.87 91.83 91.14 93.45 6,950 94.33 94.07 93.91 95.83 94.25 94.16 94.00 95.56 2,098 94.71 94.32 94.17 96.19 94.30 93.96 93.88 96.10 74,800 89.29 88.63 88.73 94.54 89.17 88.49 88.47 94.48 14, 99.97 99.97 99.97 99.98 99.97 99.97 99.97 99.98 216,568 99.75 99.74 99.79 99.97 99.97 99.97 99.97 99.99 27,757 97.53 97.19 97.63 98.24 97.22 96.84 97.53 98.19 1,012 93.85 96.12 83.22 99.20 98.39 98.41 98.41 99.20 62 89.27 89.60 87.56 93.37 91.60 91.58 89.26 93.40 677 93.63 92.89 93.64 95.40 93.83 92.84 93.65 95.51 14, 99.11 98.91 98.86 99.60 99.26 98.86 98.86 99.55 1,012 97.44 85.71 95.31 96.00 98.36 97.56 99.19 98.36 61 93.61 94.15 93.03 94.84 93.52 94.12 93.36 94.52 1,628 94.24 93.79 92.78 96.84 94.36 94.30 93.16 96.85 14,960 98.22 98.08 98.57 98.60 98.32 98.17 98.62 98.60 1,012 82.19 95.80 91.20 99.15 97.44 98.28 95.00 100.00 96.30 95.89 95.51 98.23 97.62 97.38 96.84 98.22 1,250 96.06 95.67 95.37 95.87 96.11 95.65 95.41 95.96 14,960 99.90 99.90 99.75 100.00 99.95 99.95 99.85 100.00 1,012 78.21 73.49 88.41 99.19 98.39 97.60 99.19 100.00 61 93.13 93.11 91.42 98.05 95.16 95.29 93.76 97.73 1,745 97.36 97.30 97.15 97.21 95.96 96.07 96.13 96.86 14, 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 212,520 99.88 99.87 99.90 99.99 99.99 99.99 99.99 100.00 27,515 Table 14: Comprehensive comparison of language identification models across all test datasets with and without softmax thresholding. Bold values indicate best performance for each metric within each dataset/threshold combination. 17 Lang. code Name ace_Arab ace_Latn Achinese Achinese Lang. code Name fon_Latn fra_Latn Fon French afr_Latn Afrikaans frp_Latn Lang. code Name Lang. code Name lij_Latn lim_Latn lin_Latn lit_Latn lmo_Latn ltg_Latn ltz_Latn lua_Latn Ligurian Limburgan (Limburger, Limburgish) Lingala slk_Latn slv_Latn Slovak Slovenian smo_Latn Samoan Lithuanian Lombard Latgalian Luxembourgish (Letzeburgesch) Luba-Lulua sna_Latn snd_Arab som_Latn sot_Latn Shona Sindhi Somali Southern Sotho spa_Latn Spanish (Castilian) (FrancoArpitan provençal) Friulian Nigerian Fulfulde fur_Latn fuv_Latn gaz_Latn West Central Oromo gla_Latn Scottish Gaelic gle_Latn Irish als_Latn amh_Ethi ara_Arab asm_Beng ast_Latn awa_Deva ayr_Latn Tosk Albanian Amharic Arabic Assamese Asturian (Asturleonese, Bable, Leonese) Awadhi Central Aymara glg_Latn gug_Latn Galician Paraguayan Guaraní lug_Latn luo_Latn Ganda Luo (Kenya and Tanzania) (Dholuo) Lushai Standard Latvian srd_Latn srp_Cyrl Sardinian Serbian srp_Latn ssw_Latn Serbian Swati lus_Latn lvs_Latn azb_Arab azj_Latn South Azerbaijani North Azerbaijani guj_Gujr hat_Latn bak_Cyrl bam_Latn ban_Latn Bashkir Bambara Balinese bel_Cyrl bem_Latn ben_Beng bho_Deva bjn_Arab bjn_Latn bod_Tibt bos_Latn bug_Latn bul_Cyrl cat_Latn ceb_Latn Belarusian Bemba (Zambia) Bengali Bhojpuri Banjar Banjar Tibetan Bosnian Buginese Bulgarian Catalan (Valencian) Cebuano hau_Latn heb_Hebr hin_Deva hne_Deva hrv_Latn hun_Latn hye_Armn ibo_Latn ilo_Latn ind_Latn isl_Latn ita_Latn jav_Latn jpn_Jpan kab_Latn (Creole, Gujarati Haitian Haitian Creole) Hausa Hebrew Hindi Chhattisgarhi Croatian Hungarian Armenian Igbo Iloko Indonesian Icelandic Italian Javanese Japanese Kabyle mag_Deva Magahi mai_Deva Maithili mal_Mlym Malayalam mar_Deva Marathi min_Latn Minangkabau mkd_Cyrl Macedonian mlt_Latn Maltese mni_Beng Manipuri mos_Latn Mossi mri_Latn Maori mya_Mymr nld_Latn nno_Latn nob_Latn npi_Deva ces_Latn"
        },
        {
            "title": "Czech",
            "content": "kac_Latn Kachin (Jingpho) nso_Latn cjk_Latn ckb_Arab"
        },
        {
            "title": "Chokwe\nCentral Kurdish",
            "content": "kam_Latn kan_Knda Kamba (Kenya) Kannada cmn_Hans Mandarin Chinese cmn_Hant Mandarin Chinese kas_Arab kas_Deva"
        },
        {
            "title": "Kashmiri\nKashmiri",
            "content": "nus_Latn nya_Latn oci_Latn ory_Orya kat_Geor"
        },
        {
            "title": "Georgian",
            "content": "pag_Latn (Northern Burmese Dutch (Flemish) Norwegian Nynorsk Norwegian Bokmål Nepali (individual language) Pedi Sotho, Sepedi) Nuer Chichewa (Chewa, Nyanja) Occitan (post 1500) Odia (Oriya (individual language)) Pangasinan sun_Latn swe_Latn swh_Latn szl_Latn tam_Taml taq_Latn taq_Tfng tat_Cyrl tel_Telu tgk_Cyrl tha_Thai tir_Ethi tpi_Latn tsn_Latn tso_Latn (indilanguage) Sundanese Swedish Swahili vidual (Kiswahili) Silesian Tamil Tamasheq Tamasheq Tatar Telugu Tajik Thai Tigrinya Tok Pisin Tswana Tsonga tuk_Latn"
        },
        {
            "title": "Turkmen",
            "content": "tum_Latn tur_Latn"
        },
        {
            "title": "Tumbuka\nTurkish",
            "content": "twi_Latn uig_Arab Twi Uighur (Uyghur) ukr_Cyrl"
        },
        {
            "title": "Ukrainian",
            "content": "crh_Latn Crimean (Crimean Turkish)"
        },
        {
            "title": "Tatar",
            "content": "cym_Latn Welsh dan_Latn Danish deu_Latn German dik_Latn Southwestern Dinka dzo_Tibt Dzongkha Standard Estonian Greek ekk_Latn ell_Grek Modern (1453-) English eng_Latn kaz_Cyrl kbp_Latn kea_Latn khk_Cyrl khm_Khmr kik_Latn kin_Latn Kazakh Kabiyè Kabuverdianu Halh Mongolian Khmer Khmer) Kikuyu (Gikuyu) Kinyarwanda (Central pan_Guru pap_Latn pbt_Arab plt_Latn pol_Latn Panjabi (Punjabi) Papiamento Southern Pashto Plateau Malagasy Polish umb_Latn urd_Arab uzn_Latn vec_Latn vie_Latn"
        },
        {
            "title": "Umbundu\nUrdu\nNorthern Uzbek\nVenetian\nVietnamese",
            "content": "por_Latn quy_Latn Portuguese Ayacucho Quechua war_Latn Waray (Philippines) wol_Latn Wolof epo_Latn eus_Latn ewe_Latn fao_Latn Esperanto Basque Ewe Faroese fas_Arab fij_Latn Persian Fijian kmb_Latn kmr_Latn knc_Arab knc_Latn kor_Hang ktu_Latn fil_Latn Filipino (Pilipino) lao_Laoo kir_Cyrl Kirghiz (Kyrgyz) ron_Latn Kimbundu Northern Kurdish Central Kanuri Central Kanuri run_Latn rus_Cyrl sag_Latn san_Deva Romanian (Moldavian, Moldovan) Rundi Russian Sango Sanskrit Korean Kituba (Democratic Republic of Congo) Lao sat_Olck scn_Latn Santali Sicilian shn_Mymr"
        },
        {
            "title": "Shan",
            "content": "xho_Latn Xhosa ydd_Hebr yor_Latn yue_Hant zgh_Tfng zsm_Latn zul_Latn zxx_Zxxx Eastern Yiddish Yoruba Yue Chinese Standard Moroccan Tamazight Standard Malay Zulu No linguistic content (Not applicable) fin_Latn"
        },
        {
            "title": "Finnish",
            "content": "lat_Latn"
        },
        {
            "title": "Latin",
            "content": "sin_Sinh Sinhala (Sinhalese) Table 15: OpenLID-v3 supported languages (model class labels)."
        }
    ],
    "affiliations": [
        "Language Technology Group, Department of Informatics, University of Oslo"
    ]
}