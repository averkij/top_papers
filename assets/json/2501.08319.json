{
    "paper_title": "Enhancing Automated Interpretability with Output-Centric Feature Descriptions",
    "authors": [
        "Yoav Gur-Arieh",
        "Roy Mayan",
        "Chen Agassy",
        "Atticus Geiger",
        "Mor Geva"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automated interpretability pipelines generate natural language descriptions for the concepts represented by features in large language models (LLMs), such as plants or the first word in a sentence. These descriptions are derived using inputs that activate the feature, which may be a dimension or a direction in the model's representation space. However, identifying activating inputs is costly, and the mechanistic role of a feature in model behavior is determined both by how inputs cause a feature to activate and by how feature activation affects outputs. Using steering evaluations, we reveal that current pipelines provide descriptions that fail to capture the causal effect of the feature on outputs. To fix this, we propose efficient, output-centric methods for automatically generating feature descriptions. These methods use the tokens weighted higher after feature stimulation or the highest weight tokens after applying the vocabulary \"unembedding\" head directly to the feature. Our output-centric descriptions better capture the causal effect of a feature on model outputs than input-centric descriptions, but combining the two leads to the best performance on both input and output evaluations. Lastly, we show that output-centric descriptions can be used to find inputs that activate features previously thought to be \"dead\"."
        },
        {
            "title": "Start",
            "content": "Enhancing Automated Interpretability with Output-Centric Feature Descriptions Yoav Gur-Arieh1 Roy Mayan1* Chen Agassy1 Atticus Geiger2 Mor Geva1 1Blavatnik School of Computer Science and AI, Tel Aviv University 2Pr(Ai)2R Group {yoavgurarieh@mail,roymayan@mail,chenagassy@mail,morgeva@tauex}.tau.ac.il, atticusg@gmail.com 5 2 0 2 4 1 ] . [ 1 9 1 3 8 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Automated interpretability pipelines generate natural language descriptions for the concepts represented by features in large language models (LLMs), such as plants or the first word in sentence. These descriptions are derived using inputs that activate the feature, which may be dimension or direction in the models representation space. However, identifying activating inputs is costly, and the mechanistic role of feature in model behavior is determined both by how inputs cause feature to activate and by how feature activation affects outputs. Using steering evaluations, we reveal that current pipelines provide descriptions that fail to capture the causal effect of the feature on outputs. To fix this, we propose efficient, output-centric methods for automatically generating feature descriptions. These methods use the tokens weighted higher after feature stimulation or the highest weight tokens after applying the vocabulary unembedding head directly to the feature. Our output-centric descriptions better capture the causal effect of feature on model outputs than input-centric descriptions, but combining the two leads to the best performance on both input and output evaluations. Lastly, we show that output-centric descriptions can be used to find inputs that activate features previously thought to be dead."
        },
        {
            "title": "Introduction",
            "content": "Understanding how language models represent concepts in real-valued vector space has long been central challenge in NLP (Mikolov et al., 2013; Karpathy et al., 2015; Bau et al., 2019; Mu and Andreas, 2020; Dai et al., 2022; Park et al., 2024a). Recent efforts to scale this process use automated interpretability pipelines, where large language models (LLMs) describe the concepts encoded by features, i.e., small model components such as neurons or directions in activation space, based on inputs * Equal contribution 1 Figure 1: We posit that faithful description of feature should consider both model inputs that activate it (left, marked words cause the highest activations) and the effect it introduces to the models outputs (right). that activate them (Bills et al., 2023; Bricken et al., 2023; Paulo et al., 2024; Choi et al., 2024). However, despite its wide adoption, solely relying on the inputs activating feature to describe it has practical limitations and theoretical pitfalls. First, given the large corpora modern LLMs are trained on, obtaining these examples can be costly and nearly impossible in cases when features are described by data instances that are not publicly available. This practical limitation increases the compute and data needed for automated interpretability. Second, the concept represented by feature is determined by the causal role of that feature in model behavior, namely, how model inputs cause the feature to activate and how feature causes model outputs to change (Mueller et al., 2024). Using only inputs to characterize feature is ungrounded in the causal mechanisms driving model behavior, which introduces pitfalls. For example, different datasets can lead to inconsistent feature descriptions (Bolukbasi et al., 2021) or to classifying features as dead due to lack of activation (Gao et al., 2024; Templeton et al., 2024). Last, common use of feature descriptions is controlling model behavior through steering, i.e., stimulating feature to control the models outputs (Upchurch et al., 2017; Li et al., 2023; Rimsky et al., 2024; Templeton et al., 2024; OBrien et al., 2024a). Therefore, good feature descriptions for steering should be output-centric. To overcome these limitations, we propose two output-centric methods for enhancing automated interpretability pipelines (see Figure 1 for illustration). The first method, called VocabProj, uses the prominent tokens in the projection of feature to the models vocabulary space (Geva et al., 2022b; Bloom and Lin, 2024). The second method, called TokenChange, considers the tokens whose probabilities in the models output distribution change the most when the feature is amplified. Notably, these methods are substantially more computationally efficient than generating descriptions based on activating inputs; VocabProj requires single matrix multiplication, and TokenChange involves running the model on few inputs. We compare the descriptions generated by these methods with those generated based on maximum activating inputs (dubbed MaxAct) using two evaluations: input-based and output-based (see Figure 2 for illustration). The input-based evaluation assesses how accurately description identifies what triggers the feature, whereas the output-based evaluation measures how effectively the description captures the causal impact of the features activation on the models output. Experiments over neuron-aligned and sparse autoencoder (SAE) features from both the residual and MLP layers of multiple LLMs reveal substantial differences between the methods and the descriptions they yield. While MaxAct typically outperforms VocabProj and TokenChange on the input-based evaluation, it is generally worse in capturing the features effect on the models generation. Moreover, the gap between MaxAct and VocabProj in describing the inputs activating given feature is sometimes small, suggesting that the latter can serve as cheap replacement in such cases. Last, ensembles of the three methods consistently achieve the best performance across both evaluations, providing strong empirical evidence for the benefits of incorporating output-centric methods into automated interpretability pipelines. Further analysis sheds light on those benefits. We observe that descriptions generated by outputcentric methods are often abstractions of their inputcentric counterparts, and that the composition of the inputand output-centric descriptions of feature can in some cases provide new meaning (e.g. Figure 1). Additionally, experiments with Gemma-2 SAEs show that output-centric methods can be used to efficiently discover inputs that activate dead features, for which no activating inputs had previously been identified. To summarize, our work makes the following contributions: (a) we propose two-faceted evaluation framework for feature descriptions, examining them through complementary input and output lenses (b) we highlight key drawbacks of using MaxAct, the common method used today in automated interpretability pipelines, to obtain feature descriptions in LLMs, (c) we propose outputcentric methods to mitigate these limitations, (d) our experiments demonstrate the effectiveness of each approach and that their combination yields more faithful feature descriptions, (e) our analysis provides insights into the benefits in combining inputand output-centric methods. We release our code and generated feature descriptions at https://github.com/yoavgur/Feature-Descriptions."
        },
        {
            "title": "2 Problem Setup",
            "content": "We focus on the problem of automatically describing atomic units of computation in LLMs called features. As the exact nature of features is hotly debated topic, we adopt the general framework of Geiger et al. (2024a) which we limit to real-valued features. Any hidden vector Rd in the LLM can be transformed with an invertible featurizer : Rd Rk that maps the vector into space of features. single feature Rk is simply one-hot encoding which can be vectorized using vf = 1(f ). This framework supports variety of features, including neurons (axis-aligned dimensions) in MLPs (Geva et al., 2022b), sets of orthogonal directions (Geiger et al., 2024b; Huang et al., 2024; Park et al., 2024b), sparse linear features from SAEs (Elhage et al., 2022; Templeton et al., 2024; Huben et al., 2024), or even nonlinear features, e.g. onion representations with magnitude-based features (Csordás et al., 2024). During inference, the LLM constructs the vector from the input, which can then be passed through to determine the activation for each feature F(v). The possible values for activations are result of the feature space, e.g. SAE features produced with ReLU only have positive activations. In this work, we consider the problem of automatically labeling the concept represented by feature . Namely, producing humanunderstandable description text sf of the feature 2 . Importantly, we want the method producing sf to be scalable, i.e. automatic and efficient, such that it can be integrated into large-scale pipelines that interpret millions of features in LLMs. This additional requirement excludes approaches that rely, for example, on manual human labeling. key question that arises is how to evaluate whether description faithfully describes its corresponding feature. Here we observe that describing feature is practically two-faceted problem; one can describe what inputs activate the feature, i.e. what inputs yield high feature activations, but they can also describe what this feature promotes in the models output. Consider for example the feature illustrated in Figure 1. The input side indicates that the feature activates mainly on competitive financial and business related sentences. Conversely, the output side shows that the feature amplifies the concept of war when activated. Only when considering the two sides together we see that the feature promotes the concept of war in social and business related scenarios, e.g., trade war, bidding war, and culture war. Notably, this formulation was also discussed in prior works; Geva et al. (2021, 2022a,b) characterized MLP as key-value memories that promote specific concepts, and Antverg and Belinkov (2022); Huang et al. (2023) contended the importance of differentiating between the information encoded by the feature versus used by the model. Despite the dual nature of this problem, existing automated interpretability pipelines (e.g., Bills et al., 2023; Paulo et al., 2024; Choi et al., 2024) have focused on one side of the problem. Namely, describing the inputs that activate the feature, while disregarding the features influence on the models output. For example, Huang et al. (2023) showed that neurons interpreted by Bills et al. (2023) lack causal influence on the concepts expressed in their generated descriptions. Therefore, we offer more holistic approach, accounting for both the input and output of the model."
        },
        {
            "title": "3 Evaluation of Feature Descriptions",
            "content": "We propose to evaluate how faithful description is to its corresponding feature with the following complementary metrics, illustrated in Figure 2. Input-based Evaluation Following Huang et al. (2023); Caden Juang et al. (2024), we evaluate how well the description captures the inputs triggering the feature. Given feature , we feed its description sf generated by some method into an LLM, Figure 2: Illustration of our feature description evaluation, considering the descriptions faithfulness with respect to both the input (middle panel) and output (lower panel) of the model. which is tasked to generate two sets of examples each: activating and neutral. These examples are expected and not expected to activate according to sf , respectively (see for examples and details regarding prompts). We then pass the generated examples through and obtain activation for each example, calculated as the max activation over all token positions in that example. We take the max over all token positions since its reasonable to expect to be activated highly even for just single token, and not at all for the rest. Let mactivating and mneutral be the mean activations obtained for the activating and neutral examples, respectively. The description sf is considered faithful if the mean activation for the activating examples exceeds that of the neutral examples, namely: mactivating > mneutral This evaluation is similar to those implemented in existing automated pipelines, which essentially measure how accurately the description captures the inputs that activate the feature. Output-based Evaluation To assess how faithful sf is with respect to influence on the models outputs, we evaluate sf against outputs generated by when steering versus when steering another feature . Concretely, we feed open-ended prompts, such as <BOS> think that (Chalnev et al., 2024), and let the model generate tokens three times one time while amplifying and two other times by amplifying two different randoms features and . Amplification of feature is done by clamping its activation to 3 high value (Templeton et al., 2024). Since finding an effective yet not destructive amplification level is challenging (Bhalla et al., 2024; Templeton et al., 2024), we run each input with varying levels of amplification while fixing the KL-divergence between the outputs of the steered model and the non-steered model (Paulo et al., 2024), as calculated on single next token prediction, averaged over all open ended prompts. This way we generate three sets of texts Tf , Tf and Tf . Next, we feed sf concatenated with Tf , Tf and Tf to judge LLM, and task it with indicating which of the three sets matches sf . The description sf is faithful if the LLM selects Tf . Namely, we evaluate how accurately the description captures the effect the feature has on the models output. For details, example generations and the exact prompts used, see A."
        },
        {
            "title": "Interpretability Methods",
            "content": "We describe the series of methods for automatically describing features in LLMs. These include the input-centric method prevalent today, two outputcentric methods that describe feature using its corresponding vector vf , and their ensembles. Max Activating Examples (MaxAct) Using the inputs that maximally activate given feature to understand its function has been used extensively (Dalvi et al., 2018; Na et al., 2019; Bolukbasi et al., 2021). More recently, this method has been widely adopted and refined for automatically interpreting features at scale (Bills et al., 2023; Bricken et al., 2023; Paulo et al., 2024; Choi et al., 2024; He et al., 2024a; Huben et al., 2024). The method involves collecting feature activations in across large dataset. For each feature, examples are sampled from the dataset, prioritizing those with the highest activations, along with some examples from other activation quantiles (Bricken et al., 2023). These examples are then fed to an explainer model, which is tasked with generating description of the feature by the examples that activate it. Vocabulary Projection (VocabProj) Building on Geva et al. (2021, 2022a,b), we propose to view the feature as an update to the models output distribution. To interpret contribution, we compute the feature vector 1(f ) = vf Rd and project it to the vocabulary space to obtain vector of logits RV such that: = WU LayerNorm(vf ) where is Ms vocabulary, LayerNorm is the final layer norm, and WU RVd is the models unembedding matrix. We then examine the tokens corresponding to the topand bottom-scoring entries in w, interpreting them as the tokens most promoted or suppressed, respectively. These tokens are then fed to an explainer model that generates description for the feature. For more details and other variants of this method, see B.1. Token Change (TokenChange) This method describes the tokens whose logits in the models output were most affected by amplifying the feature. Specifically, we pass random prompts sampled from some dataset through the model and collect the output logit values for each token position. Next, the feature is clamped to activation value m, and we collect the new logit values (Templeton et al., 2024). We then calculate the mean change in logit value per token across all positions and prompts. The list of tokens most affected by amplifying the feature is provided to an explainer model, which generates description for the feature. While both VocabProj and TokenChange are output-geared methods, VocabProj is correlative and TokenChange causally intervenes in the models generation. Ensembles To capture both the input and output sides of feature, we propose combining the above approaches in two ways: (a) Ensemble Raw: the raw data used by the methods is concatenated and fed to the explainer model. For example, in Ensemble Raw (MaxAct+VocabProj) we would feed the explainer model the activating examples and top tokens in the vocabulary projection. (b) Ensemble Concat: the description is simply concatenation of the descriptions generated by the methods. We also attempted to summarize the descriptions by the different methods with an LLM to produce more cohesive description, but these ensembles performed worse across the board."
        },
        {
            "title": "5.1 Experimental Setting",
            "content": "Features We analyze both features learned through SAEs and neurons in MLP layers, covering four LLMs of different sizes and families: Gemma-2 2B (Team et al., 2024b), Llama-3.1 8B and Llama-3.1 8B Instruct (Dubey et al., 2024), and GPT-2 small (Radford et al., 2019). For Gemma-2, Llama-3.1 and GPT-2 small, we evaluate descrip4 tions of SAE features trained on residual stream and MLP layers: Gemma Scope 16K and 65K (Lieberum et al., 2024), Llama Scope 32K (He et al., 2024b), and OpenAI SAE 32K and 128K (Gao et al., 2024). The activation function used by Gemma Scope is JumpReLU (Rajamanoharan et al., 2024), while both Llama Scope and OpenAI SAE use TopK-ReLU (Makhzani and Frey, 2014). We randomly sample = 40 features per layer from every SAE, resulting in total of 4,160 features for Gemma-2, 2,560 for Llama-3.1 and 1,440 for GPT-2 small. For Llama-3.1 Instruct we inspect sample of = 80 MLP features per layer, with 2,560 features in total. Description Generation We use the methods described in 4 and generate descriptions for each feature, using GPT-4o mini (Hurst et al., 2024) as our explainer model to ensure consistency with descriptions from Neuronpedia (Lin and Bloom, 2023) and Transluce (Choi et al., 2024). For MaxAct, we utilize the publicly available feature descriptions from these repositories. To validate these descriptions are comparable to the ones generated by us, we randomly sampled features and found their descriptions matched those we generate for MaxAct (see B.3 for full analysis). When generating ensembles from raw data (Ensemble Raw), we rely on feature activation data from these same sources, using the top five activating sentences to keep in line with existing methods. Notably, Transluce generated descriptions for Llama-3.1 8B Instruct through more complex process than MaxAct (Choi et al., 2024), creating multiple descriptions from activating examples and selecting the best using simulation scoring (Bills et al., 2023). For clarity, we refer to this method as MaxAct++ and generate the MaxAct descriptions for Llama-3.1 8B Instruct ourselves using the feature activation data from Transluce. For VocabProj and TokenChange, we pass the top and bottom tokens to the explainer model GPT-4o mini (see prompts in B.2). We set = 50 for VocabProj and = 20 for TokenChange. For TokenChange we use = 32 random prompts of 32 tokens each from The Pile (Gao et al., 2020). Description Evaluation For the input-based evaluation, we instruct Gemini 1.5 Pro (Team et al., 2024a) to generate five activating and five neutral sentences with respect to given feature description. For the output-based evaluation, we prompt the model with three different open-ended prompts, letting it generate up to 25 tokens while clamping the features activation value to for all token positions. For each prompt, we run the model four times with increasing clamping values, making the generations progressively more affected by the features output. The values were derived by fixing two target KL-divergence values 0.25 and 0.5, which provide two positive and two negative clamping values for m. We select these values to balance between generating text with sufficient feature effect and long or noisy text that is hard to evaluate. This process results in 12 text generations for each of the sets Tvf , Tv . We then use Gemini 1.5 Pro (Team et al., 2024a) as an LLM judge, as described in 3 (exact prompts in A). , and Tv f"
        },
        {
            "title": "5.2 Results",
            "content": "Table 1 shows the results averaged across layers, and Figure 3 provides breakdown for layer groups in Gemma-2 2B and Llama-3.1 8B Instruct. Similar trends are shown for Llama-3.1 and GPT-2 in C. Combining inputand output-centric methods yields better feature descriptions Table 1 shows that across all models and feature types, MaxAct outperforms VocabProj and TokenChange on the input-based evaluation and vice versa on the outputbased evaluation, often by large margins of up to 15%-30%. This also holds for MaxAct++ on Llama-3.1 8B Instruct, demonstrating that inputand output-centric methods capture different feature information. Second, ensembling inputand output-centric methods boosts performance on both evaluations, with the ensembles combining all three methods consistently outperforming the singlemethods. For instance, for Gemma-2 the ensembles yielded an improvement of 6%-10% over the next best single-method on both metrics. One exception to this trend is MaxAct++, which performs better than all other methods on the input metric, with Ensemble Raw (All) in close second. This is probably due to MaxAct++ being optimized for describing what activates given feature. Overall, this input-output integration not only better describes the causal roles of features but also improves performance on the existing widely-used input-based evaluation. Performance varies by layer and feature type Comparing the results for residual versus MLP features and neurons versus SAE features, we find that output-based performance is substantially lower for MLP features compared to residual features (reaching 45-50 points for MLP vs. 66 points for resid5 Gemma-2 Res. SAE Gemma-2 MLP SAE Llama-3.1 Res. SAE Llama-3.1 Inst. MLP Input Output Input Output Input Output Input Output MaxAct MaxAct++ VocabProj TokenChange - 56.6 2.2 49.2 2.2 50.4 2.2 35.1 2.1 30.3 2.7 71.8 2.6 85.6 1.4 36.9 1.9 39 1.9 50.1 2.2 56.5 2.2 20.9 1.8 37.2 2.1 18.2 2.2 64.2 2.8 71.2 1.8 45.8 1.9 74 1.7 43.8 1.9 44.7 2.2 54.9 2.2 22.3 1.8 40.3 2.2 21.4 2.4 72.0 2.6 89.8 1. - - - - - EnsembleR (MA+VP) 66.9 2.1 EnsembleR (MA+TC) EnsembleR (VP+TC) 53.1 2. 52 2.2 56.6 2.2 38.6 2.1 36.9 2.8 68.9 2.7 86.7 1.3 40.7 1.9 67 2.1 61.9 2.1 56.4 2.2 46.2 2.2 37.2 2.8 68.0 2.7 87.2 1.3 41.7 1.9 63 2.1 24.3 1.9 46.6 2.2 20.9 2.3 67.4 2.7 72.4 1.7 44.3 1.9 EnsembleR (All) EnsembleC (All) 36 2.8 71.2 2.6 86.2 1.3 41.8 1.9 66.6 2.1 64.9 2.1 55.7 2.2 48.7 2.2 57.7 2.2 66.9 2.1 31.6 2.1 49.9 2.2 28.5 2.6 75.4 2.5 84.9 1.4 44.6 1.9 Table 1: Inputand output-based evaluation results of the methods and their ensembles, over different feature types and models, averaged across model layers, along with their respective 95% confidence intervals. For SAE features we take the average over features from SAEs of all sizes. We denote MA for MaxAct, VP for VocabProj, TC for TokenChange, and EnsembleR and EnsembleC for the raw and concatenation based ensembles. ual). This might be explained by the MLP layers introducing gradual changes to the residual stream (Geva et al., 2021, 2022b), potentially making them harder to steer. Additionally, output-based performance of VocabProj is worse in early layers but gradually improves, consistent with prior observations (Nostalgebraist, 2020; Geva et al., 2021; Yom Din et al., 2024). generated descriptions. For the input-based evaluation, longer and more informative description may have higher chance of enabling an LLM to generate sentences with at least one activating token, compared to concise description. Similarly, concise description could be matched to texts generated by the model more easily compared to long and detailed description. VocabProj and TokenChange often provide efficient substitutes for MaxAct major practical drawback of MaxAct is the computational cost required for comprehensively mapping the activating inputs of feature. Considering the performance of VocabProj, TokenChange, and EnsembleR (VP+TC), we observe that (a) they typically outperform MaxAct on the output-based evaluation, which is crucial for assessing the descriptions faithfulness to the features causal effect and its usefulness for steering, and (b) they often perform only slightly worse on the input-based evaluation, e.g. theres only 3.5 point gap between Ensemble Raw (VP+TC) and MaxAct on residual stream SAE features in Gemma-2. These results suggest that VocabProj and TokenChange, which require only 2 inference passes, can often be more efficient and sometimes higher-performing alternative to the widely-used MaxAct method. Description Format Affects Performance Comparing the top-performing ensembles, we observe that Ensemble Raw is generally better on the inputbased evaluation while Ensemble Concat is consistently best on the output-side evaluation. We hypothesize that this could be due to the different description formats of the two ensembling approaches, i.e., concatenating raw outputs versus"
        },
        {
            "title": "6 Analysis",
            "content": "In this section, we compare the feature descriptions obtained by MaxAct, VocabProj and TokenChange and analyze the utility in their combination."
        },
        {
            "title": "6.1 Qualitative Analysis",
            "content": "We manually analyze the descriptions by MaxAct and VocabProj for random sample of 100 features from Gemma Scope 16K, 50 for the MLP layers and 50 for the residual stream. We exclude TokenChange here as we noticed that the descriptions it produces are often similar to those by VocabProj (see examples in E). In the analysis, we consider descriptions that pass both our inputand output-based evaluations. We observed 4 main types of relations between the descriptions: Similar: The tokens in the projection and are highly similar to the tokens in the activating examples, resulting in matching descriptions. Composition: The inputand outputbased descriptions refer to different aspects of the feature, while their composition provides more holistic description of the feature. Abstraction: The tokens in the projection represent more general or broad concept than the one observed in the activating examples. 6 (a) Residual stream SAE features of width 65k from Gemma-2. (b) MLP SAE features of width 65k from Gemma-2. (c) Residual stream SAE features of width 32k from Llama-3.1. (d) MLP features from Llama-3.1 8B Instruct. Figure 3: Performance of the various methods on the proposed metrics, for Gemma-2 2B (upper row), Llama-3.1 8B (lower left), and Llama-3.1 8B Instruct (lower right). For the output metric, the baseline (dashed black line) is 1/3 since the judge LLM picks between three sets of texts."
        },
        {
            "title": "Relation",
            "content": "Example feature Description by MaxAct layer-type/id"
        },
        {
            "title": "Description by VocabProj",
            "content": "Similar 41% 3-MLP-16K/ 4878 Terms and themes related to various genres of storytelling, particularly in horror, drama, and fantasy. blend of themes and genres commonly found in storytelling or media, with specific focus on dramatic, horror, and suspenseful narratives. Composition 23% 19-MLP-16K/ References to political events and milestones. Abstraction 23% 21-RES-16K/ 10714 Information related to bird species and wildlife activities. Concepts related to time measurement such as days, weeks, weekends, and months, indicating it likely pertains to scheduling or planning events. Concepts related to birdwatching and ornithology, focusing on activities such as observing, spotting, and recording bird species in their natural habitats. Different 13% 19-MLP-16K/ 1450 Mentions of notable locations, organizations, or events, particularly in various contexts. Concepts related to self-reflection, purpose, and generalization in various contexts, focusing on the exploration of identity and overarching themes in literature or philosophy. Table 2: Human evaluation results of descriptions by MaxAct and VocabProj for 100 SAE features from Gemma Scope, showing for each relation category the fraction of observed cases and the descriptions of an example feature. Different: The inputand outputbased descriptions refer to different aspects of the feature, which share no clear relation between them. Table 2 shows the fraction of examples classified per category alongside representative feature descriptions. Overall, while inputand output-centric descriptions are often similar (41%), there are many cases where their composition provides broader (23%) or more accurate (23%) description ."
        },
        {
            "title": "6.2 Reviving Dead Features",
            "content": "One drawback of describing features with MaxAct is the dependency on the dataset used to obtain activations (Bolukbasi et al., 2021). particularly interesting case is the classification of dead features, which do not activate for any input from the dataset. Dead features can be prevalent (Voita et al., 2024; Gao et al., 2024; Templeton et al., 2024). For example, we observed they constitute up to 29% of the features in some SAEs in Gemma-2. While dead features could potentially not represent meaningful features, it may be that the dataset used simply does not cover the right inputs for activating them. Here we conduct an analysis that shows that dead features can be revived (i.e. activated) with inputs crafted based on their 7 VocabProj and TokenChange descriptions. Analysis We sampled 1,850 SAE features from Gemma-2 2B equally distributed across layers and types (MLP / residual) and classified as dead based on Neuronpedia. For each feature, we create set of candidate prompts for activating it by: (a) using the feature descriptions by VocabProj and TokenChange and letting Gemini generate 150 sentences that are likely to activate the feature, and (b) gathering the tokens identified by VocabProj and TokenChange and constructing 1,450 sequences of different lengths that randomly combine these tokens. Both the top and bottom tokens obtained using these methods could potentially activate the feature, as they might relate to concepts that the feature promotes or suppresses. We then feed all the generated prompts into the the model and consider feature as revived if any prompt successfully activated it. For implementation details, see D. Results The generated prompts successfully activated 9.1% (85) of MLP SAE features and 62% In 12% (70) of cases, (491) of residual ones. feature was activated using an LLM-generated prompt, while 73% (423) were activated with prompt composed of two tokens: <BOS> and sampled token. Moreover, the revived dead features can often be easily interpreted using VocabProj and TokenChange, while considered faithful based on our output-based metric (see examples in D). Overall, this demonstrates that output-centric methods can address potential oversights that may arise from focusing solely on activating inputs. 2024), refining the prompt given to the explainer model (Paulo et al., 2024), and improving descriptions of residual feature activations via description selection (Choi et al., 2024) similarly to the algorithm by Singh et al. (2023). While all these prior works rely on input-centric, computationally intensive approaches, we propose output-centric efficient methods that require no more than two inference passes of the model. Furthermore, we show that combining inputand output-centric methods leads to improved overall performance. More broadly, our work relates to growing efforts in understanding features encoded in neurons and SAE features. These include steering (Farrell et al., 2024; Chalnev et al., 2024; OBrien et al., 2024b; Templeton et al., 2024), circuit discovery (Marks et al., 2024; Makelov et al., 2024; Balcells et al., 2024), feature disentanglement (Huang et al., 2024; Cohen et al., 2024) and benchmarks like SAEBench.1 However, evaluation of feature descriptions remains relatively underexplored. Rajamanoharan et al. (2024) evaluated latent interpretability for different SAE architectures using an input-centric approach which does not reflect downstream effect in model control. More recently, Paulo et al. (2024) have found negative correlation between multiple input-centric scoring methods and an intervention-based metric. Finally, Bhalla et al. (2024) concurrently evaluated feature descriptions in terms of their downstream effects on the model. However, they focus on evaluating methods for effectively steering models, as opposed to evaluating methods for generating descriptions."
        },
        {
            "title": "8 Conclusion",
            "content": "Bills et al. (2023) introduced an automated interpretability pipeline that used GPT-4 to explain the neurons of GPT-2 based on their activating examples (MaxAct), while employing an input-based evaluation known as simulation scoring. This approach has become common practice for interpreting neurons and learned SAE features of LLMs at scale (Lin and Bloom, 2023; Cunningham et al., 2023; Bricken et al., 2023; Templeton et al., 2024; Gao et al., 2024; He et al., 2024a), which also extends to neuron description pipelines of visual models (Hernandez et al., 2022; Shaham et al., 2024; Kopf et al., 2024). Recently, new methods for generating feature descriptions have been proposed, such as applying variants of activation patching (Kharlapenko et al., While existing automated interpretability efforts describe features based on their activating inputs, we posit that describing feature is two-faceted challenge, requiring the comprehension of both its activating inputs and influence on model outputs. To tackle this challenge at scale, we employ two evaluations input-based and output-based and propose two output-centric methods (VocabProj and TokenChange) for generating feature descriptions. Through extensive experiments we show that output-centric methods offer an efficient solution for automated interpretability, especially when geared towards model steering, and can substantially enhance existing pipelines which rely on input-centric methods. 1https://www.neuronpedia.org/sae-bench/info"
        },
        {
            "title": "Limitations",
            "content": "Although we observe clear trends in the results, the output-based evaluation is fairly noisy. We address this by sampling large numbers of features and using multiple prompts in the evaluation, but future work could focus on reducing this noise further and making the evaluation more efficient. Additionally, we find that the output-centric methods and ensembles are sensitive to the choice of prompt. Since generating feature descriptions using these methods is non-trivial and often involves long texts (especially for the ensembles), improving explainer model prompts to extract relevant information could potentially enhance performance. Regarding the methods evaluated, while we focused on efficient approaches that can automatically scale to millions of features, exploring other methods, such as patching-based methods, could be valuable. Lastly, the output-centric methods we propose are tied to the models vocabulary, which means they can only describe features that can be expressed with tokens from the vocabulary. These methods may struggle in describing features that are not easily or naturally expressed with words, such as positional features."
        },
        {
            "title": "Acknowledgements",
            "content": "We thank the Transluce team, specifically Dami Choi, for sharing their neuron description pipeline data, as well as Johnny Lin from Neuronpedia for sharing their descriptions and model activations data. This work was supported in part by the Gemma 2 Academic Research Program at Google, the Edmond J. Safra Center for Bioinformatics at Tel Aviv University, and the Israel Science Foundation grant 1083/24. Figures 1 and 2 use images from www.freepik.com."
        },
        {
            "title": "References",
            "content": "Omer Antverg and Yonatan Belinkov. 2022. On the pitfalls of analyzing individual neurons in language models. In International Conference on Learning Representations. Daniel Balcells, Benjamin Lerner, Michael Oesterle, Ediz Ucar, and Stefan Heimersheim. 2024. Evolution of sae features across layers in llms. Preprint, arXiv:2410.08869. Anthony Bau, Yonatan Belinkov, Hassan Sajjad, Nadir Durrani, Fahim Dalvi, and James Glass. 2019. Identifying and controlling important neurons in neural machine translation. In International Conference on Learning Representations. Usha Bhalla, Suraj Srinivas, Asma Ghandeharioun, and Himabindu Lakkaraju. 2024. Towards unifying interpretability and control: Evaluation via intervention. Preprint, arXiv:2411.04430. Steven Bills, Nick Cammarata, Dan Mossing, Henk Tillman, Leo Gao, Gabriel Goh, Ilya Sutskever, Jan Leike, Jeff Wu, and William Saunders. 2023. Language models can explain neurons in language models. URL https://openaipublic. blob. core. windows. net/neuron-explainer/paper/index. html.(Date accessed: 14.05. 2023), 2. Bloom and Lin. 2024. Understanding sae features with the logit lens. In AI Alignment Forum. Tolga Bolukbasi, Adam Pearce, Ann Yuan, Andy Coenen, Emily Reif, Fernanda Viégas, and Martin Wattenberg. 2021. An interpretability illusion for bert. Preprint, arXiv:2104.07143. Trenton Bricken, Adly Templeton, Joshua Batson, Brian Chen, Adam Jermyn, Tom Conerly, Nick Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Zac Hatfield-Dodds, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah Burke, Tristan Hume, Shan Carter, Tom Henighan, and Christopher Olah. 2023. Towards monosemanticity: Decomposing language models with dictionary learning. Transformer Circuits Thread. Https://transformercircuits.pub/2023/monosemanticfeatures/index.html. Gonçalo Paulo Caden Juang, Jacob Drori, and Nora Belrose. 2024. Open source automated interpretability for sparse autoencoder features. EleutherAI Blog, July, 30. Sviatoslav Chalnev, Matthew Siu, and Arthur Conmy. 2024. Improving steering vectors by targeting sparse autoencoder features. Preprint, arXiv:2411.02193. Dami Choi, Vincent Huang, Kevin Meng, Daniel Johnson, Jacob Steinhardt, and Sarah Schwettmann. 2024. Scaling automatic neuron description. https: //transluce.org/neuron-descriptions. Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. 2024. Evaluating the ripple effects of knowledge editing in language models. Transactions of the Association for Computational Linguistics, 12:283298. Róbert Csordás, Christopher Potts, Christopher Manning, and Atticus Geiger. 2024. Recurrent neural networks learn to store and generate sequences using non-linear representations. In Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 248262, Miami, Florida, US. Association for Computational Linguistics. 9 Hoagy Cunningham, Aidan Ewart, Logan Riggs, Robert Huben, and Lee Sharkey. 2023. Sparse autoencoders find highly interpretable features in language models. arXiv preprint arXiv:2309.08600. Damai Dai, Li Dong, Yaru Hao, Zhifang Sui, Baobao Chang, and Furu Wei. 2022. Knowledge neurons in pretrained transformers. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8493 8502, Dublin, Ireland. Association for Computational Linguistics. Fahim Dalvi, Nadir Durrani, Hassan Sajjad, Yonatan Belinkov, Anthony Bau, and James R. Glass. 2018. What is one grain of sand in the desert? analyzing individual neurons in deep nlp models. CoRR, abs/1812.09355. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. 2024. The llama 3 herd of models. arXiv preprint arXiv:2407.21783. Nelson Elhage, Tristan Hume, Catherine Olsson, Nicholas Schiefer, Tom Henighan, Shauna Kravec, Zac Hatfield-Dodds, Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg, and Christopher Olah. 2022. Toy models of superposition. Transformer Circuits Thread. Eoin Farrell, Yeu-Tong Lau, and Arthur Conmy. 2024. Applying sparse autoencoders to unlearn arXiv preprint knowledge in language models. arXiv:2410.19278. Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The pile: An 800gb dataset of diverse text for language modeling. Preprint, arXiv:2101.00027. Leo Gao, Tom Dupré la Tour, Henk Tillman, Gabriel Goh, Rajan Troll, Alec Radford, Ilya Sutskever, Jan Leike, and Jeffrey Wu. 2024. Scaling and evaluating sparse autoencoders. Preprint, arXiv:2406.04093. Atticus Geiger, Duligur Ibeling, Amir Zur, Maheep Chaudhary, Sonakshi Chauhan, Jing Huang, Aryaman Arora, Zhengxuan Wu, Noah Goodman, Christopher Potts, and Thomas Icard. 2024a. Causal abstraction: theoretical foundation for mechanistic interpretability. Preprint, arXiv:2301.04709. Atticus Geiger, Zhengxuan Wu, Christopher Potts, Thomas Icard, and Noah D. Goodman. 2024b. Finding alignments between interpretable causal variables In Causal and distributed neural representations. Learning and Reasoning, 1-3 April 2024, Los Angeles, California, USA, volume 236 of Proceedings of Machine Learning Research, pages 160187. PMLR. Mor Geva, Avi Caciularu, Guy Dar, Paul Roit, Shoval Sadde, Micah Shlain, Bar Tamir, and Yoav Goldberg. 2022a. LM-debugger: An interactive tool for inspection and intervention in transformer-based language models. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 1221, Abu Dhabi, UAE. Association for Computational Linguistics. Mor Geva, Avi Caciularu, Kevin Wang, and Yoav Goldberg. 2022b. Transformer feed-forward layers build predictions by promoting concepts in the vocabulary space. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 3045, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Mor Geva, Roei Schuster, Jonathan Berant, and Omer Levy. 2021. Transformer feed-forward layers are keyvalue memories. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 54845495, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Zhengfu He, Wentao Shu, Xuyang Ge, Lingjie Chen, Junxuan Wang, Yunhua Zhou, Frances Liu, Qipeng Guo, Xuanjing Huang, Zuxuan Wu, Yu-Gang Jiang, and Xipeng Qiu. 2024a. Llama scope: Extracting millions of features from llama-3.1-8b with sparse autoencoders. Preprint, arXiv:2410.20526. Zhengfu He, Wentao Shu, Xuyang Ge, Lingjie Chen, Junxuan Wang, Yunhua Zhou, Frances Liu, Qipeng Guo, Xuanjing Huang, Zuxuan Wu, et al. 2024b. Llama scope: Extracting millions of features from arXiv llama-3.1-8b with sparse autoencoders. preprint arXiv:2410.20526. Evan Hernandez, Sarah Schwettmann, David Bau, Teona Bagashvili, Antonio Torralba, and Jacob Andreas. 2022. Natural language descriptions of deep features. In International Conference on Learning Representations. Jing Huang, Atticus Geiger, Karel DOosterlinck, Zhengxuan Wu, and Christopher Potts. 2023. Rigorously assessing natural language explanations of In Proceedings of the 6th BlackboxNLP neurons. Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 317331, Singapore. Association for Computational Linguistics. Jing Huang, Zhengxuan Wu, Christopher Potts, Mor Geva, and Atticus Geiger. 2024. RAVEL: Evaluating interpretability methods on disentangling language model representations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 8669 8687, Bangkok, Thailand. Association for Computational Linguistics. Robert Huben, Hoagy Cunningham, Logan Riggs Smith, Aidan Ewart, and Lee Sharkey. 2024. Sparse autoencoders find highly interpretable features in language models. In The Twelfth International Conference on Learning Representations. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. 2024. Gpt-4o system card. arXiv preprint arXiv:2410.21276."
        },
        {
            "title": "Curt Tigges",
            "content": "Joseph Bloom and David Chanin. 2024. Saelens. https://github.com/jbloomAus/ SAELens. Andrej Karpathy, Justin Johnson, and Li Fei-Fei. 2015. Visualizing and understanding recurrent networks. arXiv preprint arXiv:1506.02078. Dmitrii Kharlapenko, neverix, Neel Nanda, and Arthur Conmy. 2024. Self-explaining SAE features. Laura Kopf, Philine Lou Bommer, Anna Hedström, Sebastian Lapuschkin, Marina M. C. Höhne, and Kirill Bykov. 2024. Cosy: Evaluating textual explanations of neurons. Preprint, arXiv:2405.20331. Quentin Lhoest, Albert Villanova del Moral, Yacine Jernite, Abhishek Thakur, Patrick von Platen, Suraj Patil, Julien Chaumond, Mariama Drame, Julien Plu, Lewis Tunstall, Joe Davison, Mario Šaško, Gunjan Chhablani, Bhavitvya Malik, Simon Brandeis, Teven Le Scao, Victor Sanh, Canwen Xu, Nicolas Patry, Angelina McMillan-Major, Philipp Schmid, Sylvain Gugger, Clément Delangue, Théo Matussière, Lysandre Debut, Stas Bekman, Pierric Cistac, Thibault Goehringer, Victor Mustar, François Lagunas, Alexander Rush, and Thomas Wolf. 2021. Datasets: community library for natural language processing. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 175184, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Kenneth Li, Oam Patel, Fernanda Viégas, Hanspeter Pfister, and Martin Wattenberg. 2023. Inferencetime intervention: Eliciting truthful answers from language model. In Thirty-seventh Conference on Neural Information Processing Systems. Tom Lieberum, Senthooran Rajamanoharan, Arthur Conmy, Lewis Smith, Nicolas Sonnerat, Vikrant Varma, Janos Kramar, Anca Dragan, Rohin Shah, and Neel Nanda. 2024. Gemma scope: Open sparse autoencoders everywhere all at once on gemma 2. In Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP, pages 278300, Miami, Florida, US. Association for Computational Linguistics. Aleksandar Makelov, Georg Lange, and Neel Nanda. 2024. Towards principled evaluations of sparse autoencoders for interpretability and control. In ICLR 2024 Workshop on Secure and Trustworthy Large Language Models. Alireza Makhzani and Brendan Frey. 2014. k-sparse autoencoders. Preprint, arXiv:1312.5663. Samuel Marks, Can Rager, Eric J. Michaud, Yonatan Belinkov, David Bau, and Aaron Mueller. 2024. Sparse feature circuits: Discovering and editing interpretable causal graphs in language models. Preprint, arXiv:2403.19647. Tomás Mikolov, Ilya Sutskever, Kai Chen, Gregory S. Corrado, and Jeffrey Dean. 2013. Distributed representations of words and phrases and their compositionality. In Advances in Neural Information Processing Systems 26: 27th Annual Conference on Neural Information Processing Systems 2013. Proceedings of meeting held December 5-8, 2013, Lake Tahoe, Nevada, United States, pages 31113119. Jesse Mu and Jacob Andreas. 2020. Compositional explanations of neurons. In Advances in Neural Information Processing Systems, volume 33, pages 17153 17163. Curran Associates, Inc. Aaron Mueller, Jannik Brinkmann, Millicent L. Li, Samuel Marks, Koyena Pal, Nikhil Prakash, Can Rager, Aruna Sankaranarayanan, Arnab Sen Sharma, Jiuding Sun, Eric Todd, David Bau, and Yonatan Belinkov. 2024. The quest for the right mediator: history, survey, and theoretical grounding of causal interpretability. CoRR, abs/2408.01416. Seil Na, Yo Joong Choe, Dong-Hyun Lee, and Gunhee Kim. 2019. Discovery of natural language concepts in individual units of cnns. In International Conference on Learning Representations. Neel Nanda and Joseph Bloom. 2022. Transformerlens. https://github.com/TransformerLensOrg/ TransformerLens. Nostalgebraist. 2020. interpreting GPT: the logit lens. Kyle OBrien, David Majercak, Xavier Fernandes, Richard Edgar, Jingya Chen, Harsha Nori, Dean Carignan, Eric Horvitz, and Forough PoursabziSangde. 2024a. Steering language model refusal with sparse autoencoders. Preprint, arXiv:2411.11296. Kyle OBrien, David Majercak, Xavier Fernandes, Richard Edgar, Jingya Chen, Harsha Nori, Dean Carignan, Eric Horvitz, and Forough PoursabziSteering language model reSangde. 2024b. arXiv preprint fusal with sparse autoencoders. arXiv:2411.11296. Johnny Lin and Joseph Bloom. 2023. Neuronpedia: Interactive reference and tooling for analyzing neural networks with sparse autoencoders. Software available from neuronpedia.org. Kiho Park, Yo Joong Choe, Yibo Jiang, and Victor Veitch. 2024a. The geometry of categorical and hierarchical concepts in large language models. Preprint, arXiv:2406.01506. 11 Kiho Park, Yo Joong Choe, and Victor Veitch. 2024b. The linear representation hypothesis and the geometry of large language models. Preprint, arXiv:2311.03658. Gonçalo Paulo, Alex Mallen, Caden Juang, and Nora Belrose. 2024. Automatically interpreting millions of features in large language models. Preprint, arXiv:2410.13928. Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, Ilya Sutskever, et al. 2019. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9. Senthooran Rajamanoharan, Tom Lieberum, Nicolas Sonnerat, Arthur Conmy, Vikrant Varma, János Kramár, and Neel Nanda. 2024. Jumping ahead: Improving reconstruction fidelity with jumprelu sparse autoencoders. Preprint, arXiv:2407.14435. Nina Rimsky, Nick Gabrieli, Julian Schulz, Meg Tong, Evan Hubinger, and Alexander Turner. 2024. Steering llama 2 via contrastive activation addition. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1550415522, Bangkok, Thailand. Association for Computational Linguistics. Tamar Rott Shaham, Sarah Schwettmann, Franklin Wang, Achyuta Rajaram, Evan Hernandez, Jacob Andreas, and Antonio Torralba. 2024. multimodal automated interpretability agent. In Forty-first International Conference on Machine Learning. Chandan Singh, Aliyah Hsu, Richard Antonello, Shailee Jain, Alexander Huth, Bin Yu, and Jianfeng Gao. 2023. Explaining black box text modules in natural language with language models. arXiv preprint arXiv:2305.09863. Gemini Team, Petko Georgiev, Ving Ian Lei, Ryan Burnell, Libin Bai, Anmol Gulati, Garrett Tanzer, Damien Vincent, Zhufeng Pan, Shibo Wang, et al. 2024a. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530. Gemma Team, Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, et al. 2024b. Gemma 2: Improving open language models at practical size. arXiv preprint arXiv:2408.00118. Adly Templeton, Tom Conerly, Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas Turner, Callum McDougall, Monte MacDiarmid, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, and Tom Henighan. 2024. Scaling monosemanticity: Extracting interpretable features from claude 3 sonnet. Transformer Circuits Thread. Paul Upchurch, Jacob Gardner, Geoff Pleiss, Robert Pless, Noah Snavely, Kavita Bala, and Kilian Weinberger. 2017. Deep feature interpolation for image content changes. In 2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 60906099. Elena Voita, Javier Ferrando, and Christoforos Nalmpantis. 2024. Neurons in large language models: Dead, n-gram, positional. In Findings of the Association for Computational Linguistics: ACL 2024, pages 12881301, Bangkok, Thailand. Association for Computational Linguistics. Wolf. 2019. Huggingfaces transformers: State-ofthe-art natural language processing. arXiv preprint arXiv:1910.03771. Alexander Yom Din, Taelin Karidi, Leshem Choshen, and Mor Geva. 2024. Jump to conclusions: Shortcutting transformers with linear transformations. In Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 96159625, Torino, Italia. ELRA and ICCL."
        },
        {
            "title": "A Additional Details on Feature\nDescription Evaluations",
            "content": "Input-based We used the prompt in Figure 4 for generating activating and neutral sentences based on feature description, as per the input metric. Output-based We used the prompt in Figure 5 for tasking the judge LLM with telling the steered text generations apart using feature description, as per the output metric. Figure 12 shows an example of steered text set for the feature with the description urgent global issues such as epidemics and invasions."
        },
        {
            "title": "B Additional Experimental Details",
            "content": "B.1 Variants of VocabProj When implementing VocabProj, presented in 4, there are several variants that generate tokens we can choose from, which are determined by the weight matrices we utilize. There are two points of interest: (a) the projection destination in the model (unembedding matrix WU RdV vs. embedding matrix WE RVd), (b) in the case of SAEs, the source of the feature vector we analyze when applying the SAE on the hidden representation (encoding matrix Wenc Rddsae vs. decoding matrix Wdec Rdsaed). We conducted experiments across all of our subject models (except Llama-3.1 8B Instruct), in order to choose the best variant of this method. 12 Variant Gemma-2 2B (Gemma Scope 16K) Llama-3.1 8B (Llama Scope 32K) GPT-2 small (OpenAI SAE 32K) Dec & Unembed 0.44 (0.31-0.58) Enc & Unembed 0.38 (0.27-0.52) 0.52 (0.38-0.65) Dec & Embed 0.29 (0.17-0.42) Enc & Embed 0.27 (0.17-0.39) 0.14 (0.06-0.25) 0.20 (0.11-0.31) 0.16 (0.08-0.27) 0.29 (0.12-0.50) 0.25 (0.12-0.46) 0.25 (0.08-0.46) 0.21 (0.08-0.42) Table 3: Confidence interval of mean input metric results on the descriptions generated by VocabProj using tokens retrieved by 4 different methods, to compare decoding vs. encoding variants. Variant Gemma-2 2B (Gemma Scope 16K) Llama-3.1 8B (Llama Scope 32K) GPT-2 small (OpenAI SAE 32K) Dec & Unembed 0.41 (0.35-0.47) 0.37 (0.31-0.43) Dec & Embed 0.14 (0.11-0.19) 0.12 (0.08-0.16) 0.20 (0.13-0.28) 0.13 (0.08-0.21) Table 4: Confidence interval of mean input metric results on the descriptions generated by VocabProj using tokens retrieved by 2 different methods, to compare unembedding vs. embedding variants with the decoding matrix. Decode vs. Encode We first wished to tackle the decision of (a). To do so, we conducted smallscale experiment in which we took random sample of SAE features, using the following SAE types: Gemma Scope 16K, Llama Scope 32K and OpenAI SAE 32K; considering both layers (MLP and residual), for each subject model. This resulted in 52 features from Gemma-2 2B, 64 from Llama-3.1 8B, and 24 from GPT-2 small. Due to the small sample size of features, we used bootstrap (9999 resamples of the data with replacements, 95% confidence) to estimate the accuracy of each variant. We used our chosen prompt (see B.2 for more details), to generate descriptions given the tokens retrieved using each of the 4 combinations above. We evaluated the descriptions using our input metric presented in 3. Table 3 shows the confidence interval for each variant on each model. From the table we concluded that generally the decoding matrix variant outperforms the encoding one. Figure 4: Prompt given to the judge LLM for the inputbased evaluation. Unembed vs. Embed We then conducted larger scale experiment to tackle decision (b). We used the same SAEs and models from our previous experiment, taking random sample of 5 features per SAE, considering both layers (MLP and residual), for each subject model. This resulted in 260 features from Gemma-2 2B, 320 from Llama-3.1 8B, and 120 from GPT-2 small. Table 4 shows the confidence interval for each variant on each model. From the table we concluded that the unembedding variant outperforms the embedding one, therefore we chose the decoding-unembedding variant for VocabProj. B.2 Description Generation VocabProj We use the prompt in Figure 6 given to the explainer model for it to generate feature descriptions using VocabProj. We tried different prompts, but didnt observe significant improvement. These include both generic prompts to be used for all subject models (Figures 13 and 14), and more fine-tuned prompts based on vocabulary projection demonstrations for each subject model (see the fine-tuned based prompt in Figure 11, for which we concatenate fewshot examples for each model as seen in Figure 9). 13 Variant Gemma-2 2B (Gemma Scope 16K) Llama-3.1 8B (Llama Scope 32K) GPT-2 small (OpenAI SAE 32K) ALL Neuronpedia 0.49 (0.44-0.55) 0.46 (0.41-0.52) 0.41 (0.36-0.47) 0.46 (0.43-0.49)"
        },
        {
            "title": "MaxAct",
            "content": "0.52 (0.46-0.57) 0.47 (0.42-0.53) 0.44 (0.39-0.50) 0.48 (0.45-0.51) Table 5: Confidence interval of mean input metric results on the descriptions taken from Neuronpedia and those generated by MaxAct. Ensembles To generate Ensemble Raw descriptions, we used variations of the prompt in Figure 10 when the ensemble included MaxAct. To generate Ensemble Raw (VocabProj+TokenChange) we simply concatenate the tokens generated by the two methods and use the VocabProj prompt. B.3 Recreating Neuronpedia Descriptions using MaxAct In order to compare our own generated descriptions to the ones provided in Neuronpedia, we conducted an experiment across all of our subject models (except Llama-3.1 8B Instruct) where we regenerated description based on the activations data provided by Neuronpedia, fed to MaxAct, following their automatic pipeline based on Bills et al. (2023). For given feature, the explainer model gets as input the 5 top-activating sentences in the format of token-activation pairs, and generates description adapting their code2 to our pipeline. We took random sample of 360 SAE features from each model, using the following SAE types: Gemma Scope 16K and 65K, Llama Scope 32K and OpenAI SAE 32K and 128K; considering both layers (MLP and residual). We evaluated both sets of descriptions using our input-based metric, and observed that they reach similar performance. Table 5 shows the confidence interval for the mean input metric evaluating both Neuronpedias descriptions and our recreated descriptions."
        },
        {
            "title": "C Additional Evaluation Results",
            "content": "See Figure 8 and Table 6 for additional results from Llama-3.1 8B and GPT-2 small SAE features, overall following the same trends observed in 5. Results for GPT-2 small are noisier than in other models. This may be due to the models relatively small size and generally lower performance. 2https://github.com/hijohnnylin/ automated-interpretability"
        },
        {
            "title": "Dead Feature Analysis",
            "content": "D.1 Generating Candidate Prompts To generate the candidate prompts, we first generate 150 potentially activating sentences in the same way as when doing so for the output metric, based on VocabProj and MaxAct. We then compile list of tokens using both VocabProj and TokenChange, and create candidate prompts that begin with <BOS> followed by either of the following: single token (1 candidate per token). Two random tokens (250 candidates). Three random tokens (250 candidates). Five random tokens (200 candidates). Twelve random tokens (200 candidates). Twenty-five random tokens (100 candidates). Thirty-two random tokens (50 candidates). D.2 Dead Feature Revival Example As an example of feature deemed to be dead that we managed to revive, and that also has clear and faithful description, we take residual stream SAE feature 64628 in layer 23 of Gemma2 2B. Using VocabProj we can get an explanation for the feature: gaming, focusing on players, gameplay, and game mechanics. Indeed when examining the top tokens when projecting the feature to vocabulary space, they are all related to games, and players. The candidate prompt that managed to trigger this feature is **Player Agency**: Choices and consequences, branching narratives.. We can then see in Figure 7 that this description is faithful when amplifying the feature and examining text generated from open ended prompts, like in the output evaluation. Llama-3.1 MLP SAE GPT2 Res. Mid. SAE Input Output Input Output GPT2 Res. SAE Input Output GPT2 MLP SAE Output Input"
        },
        {
            "title": "MaxAct\nVocabProj\nTokenChange",
            "content": "56.4 2.9 49.6 2.9 43.8 4.4 45.5 4.5 20.2 2.3 48.2 2.9 23 3.8 46.1 4.5 25.4 2.5 53.1 2.9 25.3 3.9 42.8 4.4 EnsembleR (MA+VP) 62.1 2.8 45.8 2.9 56.8 4.4 45.7 4.5 EnsembleR (MA+TC) 65.8 2.8 48.9 2.9 56.2 4.4 46.6 4.5 EnsembleR (VP+TC) 22.6 2.4 50.7 2.9 28.6 4.1 44.3 4.5 42.4 4.5 19.2 3.5 22.4 3.8 56.8 4.5 58.9 4.4 25.3 3.9 43.9 4.5 41.3 4.4 32.6 4.2 6.9 2.3 37.4 4.4 43.7 4.5 6.1 2.1 34.9 4.3 42.6 4.5 46.4 4.5 50.9 4.5 36.1 4.3 49.4 4.5 50.5 4.5 40 4.4 7.1 2.3 43.6 4.5 44.3 4. EnsembleR (All) EnsembleC (All) 62.7 2.8 51.6 2.9 61 4.4 47.2 4.5 39.1 2.8 55.5 2.9 40.1 4.4 49.5 4.5 59.1 4.4 48.7 4.5 50.1 4.5 36.9 4.3 39.2 4.4 46.4 4.5 25.2 3.9 37.7 4.4 Table 6: Inputand output-based evaluation results of the methods and their ensembles, over different feature types and models, averaged across model layers, along with their respective 95% confidence intervals. For SAE features we take the average over features from SAEs of all sizes. We denote MA for MaxAct, VP for VocabProj, TC for TokenChange, and EnsembleR and EnsembleC for the raw and concatenation based ensembles."
        },
        {
            "title": "Analysis",
            "content": "Table 7 shows descriptions generated by MaxAct, VocabProj and TokenChange."
        },
        {
            "title": "F Resources and Packages",
            "content": "In our experiments, we used models, data and code from the following packages: transformers (Wolf, 2019), datasets (Lhoest et al., 2021), TransformerLens (Nanda and Bloom, 2022) and SAELens (Joseph Bloom and Chanin, 2024). All of the experiments were conducted using single A100 80GB or H100 80GB GPU. Figure 5: Prompt given to the judge LLM for the outputbased evaluation. 15 3-MLP-16K/ 4878 19-MLP-16K/ 5635 Example feature Description by MaxAct layer-type/id"
        },
        {
            "title": "Description by TokenChange",
            "content": "Terms and themes related to various genres of storytelling, particularly in horror, drama, and fantasy. blend of themes and genres commonly found in storytelling or media, with specific focus on dramatic, horror, and suspenseful narratives. Categorization or analysis of music and entertainment genres, possibly including content recommendations or thematic associations. References events and milestones. to political 21-RES-16K/ 10714 Information related to bird species and wildlife activities. 19-MLP-16K/ 1450 Mentions of notable locations, organizations, or events, particularly in various contexts. Concepts related to time measurement such as days, weeks, weekends, and months, indicating it likely pertains to scheduling or planning events. Time periods, particularly weeks and weekends, along with some programming or markup elements for building or managing templates or components. Concepts related to birdwatching and focusing on activities ornithology, such as observing, spotting, and recording bird species in their natural habitats. Concepts related to self-reflection, purpose, and generalization in various contexts, focusing on the exploration of identity and overarching themes in literature or philosophy. Enhancing or analyzing bird watching or ornithological data and experiences, possibly improving the tracking of bird sightings and interactions. Recognize and generate variations of the term \"general\" and its context, along with concepts associated with insight and observation. Table 7: Example descriptions by MaxAct, VocabProj and TokenChange for 4 SAE features from GemmaScope. Figure 6: Prompt given to the explainer model for the VocabProj method. Figure 7: Text generated when amplifying feature pronounced to be dead, which we managed to activate using the explanation generated by VocabProj, which was gaming, focusing on players, gameplay, and game mechanics. 16 (a) MLP 32k SAE features from Llama-3.1. (b) Mid residual stream 32k SAE features from GPT-2 small. (c) Residual stream 32k SAE features from GPT-2 small. (d) MLP 32k SAE features from GPT-2 small. Figure 8: Performance of the various methods on the proposed metrics, for Llama-3.1 8B (upper left) and GPT-2 small (upper right and lower row). For the output metric, the baseline (dashed black line) is 1/3 since the judge LLM picks between three sets of texts. Figure 9: Three demonstrations of tokens and their descriptions for each model, added to the base prompt forming fine-tuned prompt. 17 Figure 10: Prompt given to the explainer model for the Ensemble Raw method. 18 Figure 11: The basic fine-tuned prompt VocabProj method. Figure 13: first variant of generic prompt for the VocabProj method. Figure 12: An example of steered text set for the outputbased metric. Figure 14: second variant of generic prompt for the VocabProj method."
        }
    ],
    "affiliations": [
        "Blavatnik School of Computer Science and AI, Tel Aviv University",
        "Pr(Ai)2R Group"
    ]
}