{
    "paper_title": "Dolphin: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback",
    "authors": [
        "Jiakang Yuan",
        "Xiangchao Yan",
        "Botian Shi",
        "Tao Chen",
        "Wanli Ouyang",
        "Bo Zhang",
        "Lei Bai",
        "Yu Qiao",
        "Bowen Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The scientific research paradigm is undergoing a profound transformation owing to the development of Artificial Intelligence (AI). Recent works demonstrate that various AI-assisted research methods can largely improve research efficiency by improving data analysis, accelerating computation, and fostering novel idea generation. To further move towards the ultimate goal (i.e., automatic scientific research), in this paper, we propose Dolphin, the first closed-loop open-ended auto-research framework to further build the entire process of human scientific research. Dolphin can generate research ideas, perform experiments, and get feedback from experimental results to generate higher-quality ideas. More specifically, Dolphin first generates novel ideas based on relevant papers which are ranked by the topic and task attributes. Then, the codes are automatically generated and debugged with the exception-traceback-guided local code structure. Finally, Dolphin automatically analyzes the results of each idea and feeds the results back to the next round of idea generation. Experiments are conducted on the benchmark datasets of different topics and results show that Dolphin can generate novel ideas continuously and complete the experiment in a loop. We highlight that Dolphin can automatically propose methods that are comparable to the state-of-the-art in some tasks such as 2D image classification and 3D point classification."
        },
        {
            "title": "Start",
            "content": "DOLPHIN: Closed-loop Open-ended Auto-research through Thinking, Practice, and Feedback Jiakang Yuan1,2, Xiangchao Yan2, Botian Shi2, Tao Chen1,(cid:66), Wanli Ouyang2 Bo Zhang2,,(cid:66), Lei Bai2,(cid:66), Yu Qiao2, Bowen Zhou2 1Fudan University, 2Shanghai Artificial Intelligence Laboratory 5 2 0 2 ] . [ 1 6 1 9 3 0 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "The scientific research paradigm is undergoing profound transformation owing to the development of Artificial Intelligence (AI). Recent works demonstrate that various AI-assisted research methods can largely improve research efficiency by improving data analysis, accelerating computation, and fostering novel idea generation. To further move towards the ultimate goal (i.e., automatic scientific research), in this paper, we propose DOLPHIN, the first closed-loop open-ended auto-research framework to further build the entire process of human scientific research. DOLPHIN can generate research ideas, perform experiments, and get feedback from experimental results to generate higher-quality ideas. More specifically, DOLPHIN first generates novel ideas based on relevant papers which are ranked by the topic and task attributes. Then, the codes are automatically generated and debugged with the exceptiontraceback-guided local code structure. Finally, DOLPHIN automatically analyzes the results of each idea and feeds the results back to the next round of idea generation. Experiments are conducted on the benchmark datasets of different topics and results show that DOLPHIN can generate novel ideas continuously and complete the experiment in loop. We highlight that DOLPHIN can automatically propose methods that are comparable to the state-of-the-art in some tasks such as 3D point classification. Our homepage: https://unimodal4reasoning.github. io/Dolphin-project-page/. 1. Introduction The fast evolution of Artificial Intelligence (AI) [1, 2, 10, 46] has brought about profound transformation in various fields [16, 24, 48]. In the landscape of scientific research, AI modes are developed and play an important role (cid:66) Corresponding Authors, Project Leader Figure 1. Comparisons of the four steps in the evolutionary trajectory towards auto-research including (a) Entirely human-driven research, (b) AI-assisted research, (c) Semi-automatic research, and (d) Auto-research. in accelerating key research processes, including scientific data collection and processing [11], scientific computation [7, 17], and scientific innovation [27]. Under this trend, the research paradigm is shifting from completely humandriven research to AI-assisted research [3]. More recently, the continuous iteration and upgrading of LLMs has promoted the evolution of AI-assisted scientific research to automatic scientific research [23, 34]. The evolutionary trajectory from human-driven research to automatic research consists of four stages as shown in Fig. 1. 1) The entirely human-driven stage requires humans to complete all aspects including idea generation and experiments. 2) In the AI-assisted research stage, researchers use LLMs-based tools [3, 37] to improve the research effi1 ciency. For example, GPT-researcher [3] can help us to decompose complex tasks and generate research reports using LLMs. 3) The semi-automatic research stage enables automation in certain aspects of scientific research. Recently, several works [20, 34, 36] use LLMs to generate ideas for different topics based on relevant works automatically. 4) The ultimate stage of AI development in science is the automatic scientific research stage, where AI system can automatically conduct the entire research process from conception to completion. Recently, AI-Scientist [23] proposes an open-ended auto-research framework capable of performing the complete scientific workflow, from idea generation to experimental validation and academic paper writing. Despite the encouraging progress made in existing works, several fundamental challenges continue to hinder the advancement of automatic scientific research. First, current studies face significant difficulties in accurately assessing the effectiveness of AI-generated ideas. Most existing works [20, 34, 36] either rely on human studies or directly employ LLMs to evaluate the quality (e.g., novelty) of generated ideas. However, merely focusing on the novelty of an idea itself does not adequately reflect its effectiveness in experimental validation. While works like AI-Scientist [23] consider experimental validation, they evaluate on the self-constructed simple datasets, making it challenging to draw meaningful comparisons with existing methods in the same field. Second, critical limitation in previous works [20, 34, 36] is the absence of feedback mechanism between the experimental validation unit and the idea generation unit process that is fundamental to human research. Human researchers typically iterate their ideas or refine existing approaches based on experimental outcomes, which serves as crucial pathway for improving the quality of research ideas. To address these challenges and facilitate further progress towards automatic scientific research, in this work, we propose DOLPHIN, the first closed-loop open-ended automatic scientific research framework which is composed of three key steps in the research cycle (i.e., idea generation, experimental verification, and results feedback). In each research loop, DOLPHIN first retrieves the papers related to the given topic and generates novel and independent ideas. Then, the codes can be generated and debugged under the guidance of the experimental plan, which is directly generated by LLMs. Finally, DOLPHIN automatically analyzes the experimental results of successfully executed experiments which will guide the next loop of idea generation. Besides, for the two key steps in the auto-research loop (i.e., idea generation and experiment verification), we claim that the quality of ideas and execution rate of codes are crucial for enhancing the efficiency of auto-research. To improve the quality of ideas, inspired by ResearchAgent [4], we find that the relevance of retrieved papers to the topic is positively correlated with the quality of generated ideas. Therefore, instead of directly using the retrieved papers as references for idea generation, DOLPHIN filters these papers by judging the topic relevance and task attribute relevance between the retrieved papers and the input topic. Further, to improve the execution rate of DOLPHIN, we design an error-traceback-guided debugging process that analyzes the local code structure related to the error-traceback to guide the debugging process. To further evaluate the effectiveness of automatic research, we conduct experiments on the benchmarks such as ModelNet40 [45], CIFAR-100 [19], and SST-2 [35], covering different tasks like 2D image classification, 3D point classification and sentiment classification. Our findings reveal that DOLPHIN generates ideas that boost the performance on these benchmarks compared to selected baselines like PointNet [28], WideResNet [12], and BERT-base [8]. We would like to emphasize that, DOLPHIN can propose methods based on PointNet that achieve performance comparable to human-designed state-of-the-art 3D classification methods. Besides, the quality of generated ideas improves through feedback, validating the effectiveness of the proposed closed-loop design. Our contribution can be summarized as follows: We propose DOLPHIN, the first closed-loop open-ended automatic research framework that covers three key steps in the human research cycle, including generating ideas, performing experiments, and feedback. To improve the efficiency of automatic research, we propose task-attribute-guided paper ranking and exceptiontraceback-guided debugging process to improve the quality of generated ideas and the successful rate of code execution, respectively. Experimental results on benchmark datasets show that DOLPHIN can generate high-quality ideas and perform experiments in the loop. We were supervised to observe that DOLPHIN is capable of achieving performance comparable to state-of-the-art methods in certain areas through automated research. 2. Related Works 2.1. AI for Automatic Scientific Research Open-ended Scientific Research. Recent studies [13, 20, 23, 26, 27, 42, 47] have demonstrated that Large Language Models (LLMs) have the potential to generate novel research ideas, finding that has sparked widespread discussion in academia. Yang et al. [47] focuses on the social science domain and develops framework called MOOSE based on LLM prompting, incorporating diverse feedback mechanisms to enhance the quality of generated ideas. Li et al. [20] introduces an innovative chain structure in research domain, aiming to enhance the reasoning capabili2 ties of LLMs. Besides, some iterative optimization strategies [13, 42] can also improve the novelty of ideas. While these approaches strive to improve the novelty of scientific ideas, crucial aspect is the empirical validation of their practical effectiveness. AI-Scientist [23] proposes an endto-end framework that automates the entire process from idea generation to experimental execution and paper writing. However, the experimental validation of its idea generation remains preliminary and lacks evaluation on real datasets or scenarios. Besides, the framework lacks feedback mechanism from experimental validation to idea generation, unlike human researchers who iteratively refine their hypotheses based on experimental outcomes. Scope-limited Scientific Research. Several studies have successfully applied LLMs to specific scientific discovery tasks. For example, AutoML-GPT [52] leverages LLMs for hyperparameter tuning by combining model and data descriptions as prompts to predict training logs. AgentHPO [21] proposes creator-executor framework that conducts experiments for specific hyperparameters and iteratively optimizes them based on historical trials. Similarly, MLCopilot [51] constructs an experience pool from historical data to enable LLMs-based hyperparameter prediction. EvoPrompting [6] improves the in-context prompting examples of LLMs to achieve effective code-level neural architecture design. In contrast to these scope-limited approaches, our method focuses on more open-ended, autonomous scientific discovery, encompassing the entire process from idea generation to final experimental validation, thereby achieving truly closed-loop logic across the complete research lifecycle. 2.2. LLMs-assisted Research Tools As LLMs [1, 2, 5, 10, 38, 46] continue to evolve rapidly, they have emerged as powerful tools for boosting research efficiency. They have been utilized such as report or survey generation [3, 14, 44], code development [15, 37, 50, 53], and data analysis [18, 32]. notable example is GPTResearcher [3], an autonomous agent based on LLMs and multi-agent systems to generate comprehensive research reports on the specified topics by effectively leveraging external knowledge bases. In the code development domain, tools such as GitHub Copilot [37] leverage the power of LLMs to provide intelligent, context-aware code suggestions and completions within Integrated Development Environments (IDEs), which significantly streamline the development process and enhance programming efficiency. The integration of LLMs as research assistants [3] opens up promising avenues for automating routine tasks, accelerating the overall research workflow. This technological breakthrough represents significant step forward in augmenting human research capabilities. 3. Methods In this section, we introduce DOLPHIN, closed-loop openended auto-research framework as shown in Fig. 2, which is mainly composed of an idea generation process, an experiments verification process, and result feedback process. The closed-loop means that the experimental results will be fed back into the idea generation process and the above three processes form research cycle. In the idea generation process, DOLPHIN first retrieves papers based on the input topic and then filters papers that are not relevant to the topic. Then, the retrieved papers serve as references to guide LLMs to generate ideas, which will be further filtered through novelty and independence check process. Subsequently, DOLPHIN formulates experimental plans and proceeds to generate and debug code using specifically designed error-traceback-guided debugging process. Finally, the results will be analyzed and utilized as feedback for the next cycle of ideas generation. In the following section, we will detail the ideas generation process, experiments verification process, and results feedback process in Sec. 3.1, Sec. 3.2, and Sec. 3.3 respectively. 3.1. Ideas Generation Process good beginning is half done. As the beginning of the research cycle, high-quality ideas are crucial to the entire research. promising approach to generating high-quality ideas is to imitate the behaviors of human researchers. They typically first conduct literature reviews and then generate ideas based on the literature [31]. To generate novel ideas, DOLPHIN typically divided the idea generation process into two steps including 1) paper retrieval and ranking, and 2) ideas generation and filtering. Paper Retrieval and Ranking. To generate high-quality ideas, the first step is to retrieve papers that are relevant to the topic. Given research topic (e.g., 3D classification), DOLPHIN begins by searching for relevant papers using Semantic Scholar API1, obtaining the essential information such as titles and abstracts. However, the initially retrieved papers are not always directly related to the input topic, which can limit their usefulness in generating subsequent ideas. For instance, if the input topic is 3D classification, some retrieved papers might pertain to 3D detection [44]. Although these topics are interconnected, they typically focus on different challenges. As result, it is necessary to filter out papers that are irrelevant to the specific topic. To this end, we design paper ranking process that aims to assign higher score to the paper relevant to the input topic. In detail, DOLPHIN ranks the retrieved papers based on two main criteria: 1) relevance to the input topic, and 2) alignment of task attributes with those of the input topic. The task attributes typically define task, including model 1https://www.semanticscholar.org/product/api 3 Figure 2. The overview of DOLPHIN, an closed-loop open-ended auto-research framework. DOLPHIN first generates set of ideas based on the retrieved papers. After filtering ideas, experimental plans will be generated for these filtered ideas. Then, codes can be generated and debugged using the proposed error-traceback-guided debugging process. Finally, the results of successfully executed experiments will be auto-analyzed and reflected into the next round of ideas generation. inputs, model outputs, and other characteristics. Specifically, the LLM is first utilized to extract the task attributes of the input topic and then prompt it to score (i.e., 1-10) each retrieved paper based on the designed criteria. The detailed prompts can be found in our supplementary material and we use gpt-4o-2024-08-06 to retrieve and score papers at this stage. After scoring, DOLPHIN filters out papers with scores below 8, using the remaining papers as the references of the subsequent ideas generation process. Ideas Generation and Filtering. After obtaining retrieved papers that are related to the topic, the next step is to generate novel ideas based on the references. DOLPHIN begins by prompting the LLM to generate novel and non-redundant ideas, each comprising title, brief experiment plan, and summary. However, these generated ideas are not consistently novel, and some ideas are similar to one another. As result, directly performing experiments on such ideas will cost substantial time and computational resources, further reducing research efficiency. To address this, we introduce further idea-filtering procedure that filters out non-novel or redundant ideas. To be specific, DOLPHIN first examines the independence of ideas to ensure non-redundancy. Given ideas [I1, I2, ..., IN ], DOLPHIN first extract the sentence-level embedding [f1, f2, ..., fN ] based on the summary of each ideas. Then, an idea bank is constructed to store embeddings of the ideas that have been checked to be independent. is initialized as empty in the first loop and initialized with previous ineffective ideas in the following loops which will be introduced in Sec. 3.3. When examining the i-th idea, DOLPHIN calculates its cosine similarity with existing ideas stored in and discards the current idea when the max similarity exceeds predetermined threshold (i.e., 0.8). After filtering out redundant ideas, the remaining ideas will undergo the novelty check. Following AI-Scientist [23], DOLPHIN simply prompt the LLMs to decide whether the idea is novel based on the searched papers by Semantic Scholar API1. Only ideas identified as novel will proceed to the subsequent experimental verification process. 3.2. Experimental Verification Process Jump in and get your feet wet. In the research cycle, experimental verification is crucial as it serves as the most effective way to confirm the validity of proposed idea. Most of the existing automatic scientific discovery works [20, 36] directly evaluate the novelty of AI-generated ideas by LLMs or humans. However, although novel ideas can be obtained in this way, their effectiveness cannot be guaranteed due to 4 code structure under the guidance of extracted exception traceback information. After that, the LLM analyzes the exception traceback and local code structure to make necessary modifications, enabling automatic code execution after these adjustments. The debugging process will be repeated until either successful execution is achieved or the predetermined maximum number of debugging times is reached. Subsequently, all successfully implemented ideas will undergo comprehensive analyses in the next phase. 3.3. Results Feedback Process Experience is the best teacher. Human researchers often analyze experimental results to further propose new ideas or improve existing ideas, since insights from previous experiments can be leveraged to effectively enhance the quality of subsequent idea generation. However, recent works either implement feedback mechanisms within the isolated idea generation process [20] or lack feedback mechanisms entirely [23]. To address this limitation, DOLPHIN analyzes experimental results and incorporates the findings into the subsequent round of ideas generation process. DOLPHIN first divides the experimental results into three categories (i.e., improvement, maintenance, and decline) compared to the performance of the reference codes. Our goal is to discourage the development of ideas that have previously led to stagnant or declining performance, while actively promoting the creation of innovative concepts or iterations based on past ideas that enhance the model performance. In detail, DOLPHIN incorporate the embeddings of summaries from ideas that maintain or improve the performance into defined in Sec. 3.1. In this way, ideas will be filtered out if they are similar to previous ideas that cannot improve the performance and avoid redundant verification of the ineffective ideas. Besides, DOLPHIN incorporates performance-enhancing ideas into the idea generation prompt for the next loop. Please refer to the supplementary material for detailed prompts. 4. Experiment 4.1. Experimental Setups Tasks and Datasets. We conduct auto-research on three topics including image classification, 3D classification, and sentiment classification. For 2D classification task, we evaluate our method on CIFAR-100 [19], which is widely-used in computer vision. For 3D classification task, we use ModelNet40 [45] which is 3D CAD dataset and consist of 40 categories. For the sentiment classification task, we use Stanford Sentiment Treebank (SST-2) dataset [35]. More details can be found in supplementary materials. Implementation Details. For the ideas generation process, we use gpt-4o-2024-08-06 [25] as our LLM agent. The total number of retrieved papers is set to 50 and only papers with scores higher than 8 will be treated as references Figure 3. Debugging with traceback-guided local code structure. the lack of closed-loop experimental verification, which is inconsistent with our scientific research goals. In contrast, DOLPHIN screens out truly effective ideas through an experimental verification process. Given an idea generated by the ideas generation process (Sec. 3.1) and reference codes, DOLPHIN first prompts the LLM to generate detailed experimental plans and then modifies the reference codes according to the idea and the generated plans. After modified codes, the experiment will automatically proceed. However, we find that the execution success rate is relatively low since LLMs encounter significant challenges in modifying code with complicated nested relationships (e.g., between class and function), while ensuring complete error-free execution. This will further reduce the efficiency of verifying ideas and research. Based on our observation, we design an exceptiontraceback-guided debugging process as shown in Fig. 3, aiming at assisting the LLMs in comprehending code logic with local code structure. Specifically, to generate the code structure related to the code errors, DOLPHIN first extracts information in exception traceback, including function name, line, and code, since traceback contains the nested information between functions. Note that DOLPHIN only focuses on custom codes, excluding library function calls. Then, DOLPHIN prompts the LLM to generate the"
        },
        {
            "title": "Tasks",
            "content": "Baseline Baseline Avg. Improvement Max Improvement Human designed Number ideas CIFAR-100 Top-1 Acc. (%) 79.5 (WRN-28-10 [49]) 81.2 (WRN-28-10) 81.8 (+0.6) 82.0 (+0.8) 82.2 (ResNeXt [22]) 6 / 40 ModelNet40 OA (%) mAcc. (%) SST-2 Acc. (%) 89.2 (PointNet [28]) 91.0 (PointNet [28]) 92.0 (+1.0) 93.9 (+2.9) 93.8 (GPSFormer [40]) 5 / 40 86.2 (PointNet [28]) 87.6 (PointNet [28]) 88.7 (+1.1) 91.1 (+3.5) 91.8 (GPSFormer [40]) 5 / 40 - 91.0 (BERT-base [8]) 91.8 (+0.8) 92.5 (+1.5) 93.1 (BERT-large [8]) 6 / Table 1. Experimental verifications on 2D image classification, 3D point classification, and sentiment classification tasks. Number ideas refers to the number of ideas that can achieve performance gains. denotes the results of our implementation. Avg. Improvement and Max Improvement represent the average and maximum improvement of all ideas that can improve the baseline performance. for the ideas generation process. We generate 20 ideas in each loop and the threshold of the independence filtering is set to 0.8. We use sentence-transformer/allroberta-large-v1 [33] to extract the summary embedding of each idea. For the experimental verification process, we use deepseek-v2.5 [53] deployed by ollama [39] as our code agent. The maximum number of debugging attempts is set to 5 for experimental efficiency. Following AI-Scientist [23], we use self-reflection to first eliminate some syntax errors before executing the program and use aider as the framework to call LLM agents. The same hyper-parameters and models employed in the ideas generation process are utilized in the results feedback. 4.2. Main Results We evaluate DOLPHINs capabilities across various tasks covering images, point clouds, and language modalities. In this section, we conduct experiments on each task for two loops (i.e., 40 ideas). For more detailed analyses of the closed-loop process, please refer to Sec. 4.3. Detailed implementations for the different tasks can be found in our supplementary material. Results on 2D Image Classification. We first conduct experiments on the image classification task, using WRN-2810 [49] as our baseline model. This model is trained on CIFAR-100 [19] for 200 epochs, and we report the Top-1 accuracy. As shown in Tab. 1, the average improvement and max improvement are 0.6% and 0.8%, respectively. Notably, the idea generated and performed automatically by DOLPHIN can achieve comparable performance to handcrafted methods such as ResNeXt [22] (e.g., 82.0% compared to 82.2%). It should be noted that Transformer-based methods [9] such as ViT are not included in our comparison, due to their heavy dependence on pre-training on large-scale datasets, which, at this stage, requires significant resource investment to validate their effectiveness. Results on 3D Point Classification. Further, we conduct experiments on 3D classification task using PointNet [28] as our baseline. We train the model on ModelNet40 [45] for 200 epochs, and report both overall accuracy and mean accuracy. As illustrated in Tab. 1, total of 5 ideas achieve"
        },
        {
            "title": "Method",
            "content": "Novelty Cost (Avg.) Naive generation Generation with naive retrieval Ours (task attribute filtering) 8 / 20 13 / 20 19 / 20 $0.106 $0.187 $0.184 Table 2. Results of ideas generation process. The novelty is evaluated by gpt-4o-2024-08-06. Cost (Avg.) is the cost per idea including paper retrieval, ideas generation, and novelty check. performance gains in two loops, with an average improvement of 1.0% OA, which shows that DOLPHIN can generate effective ideas and verify the idea by performing experiments. Besides, the maximum improvement can achieve 93.9% overall accuracy, which largely improves the performance compared with the human-designed baseline (i.e., 91.0% achieved by PointNet [28]). More excitingly, such results achieved by auto-research achieve comparable performance to the current state-of-the-art method (i.e., 93.8% achieved by GPSFormer [40]). This method is carefully designed by human researchers based on Transformer architecture. We would like to emphasize that for fair comparison, we compare the 3D methods without pre-training and the voting mechanism. Results on Sentiment Classification. To verify the effectiveness of DOLPHIN on different modalities, we also perform experiments on the NLP task (i.e., sentiment classification). We conduct experiments on SST-2 [35] and report the classification accuracy. We fine-tune the pre-trained BERT-base model [8] as our baseline. It can be seen that DOLPHIN can also generate and perform effective ideas (e.g., 1.5% improvement on SST-2) on NLP task, reducing the performance gap between BERT-base (i.e., 91.0%) and BERT-large (i.e., 93.1%). 4.3. Further Analyses Analysis on Ideas Generation Process. To evaluate the effectiveness of the ideas generation process. We conduct further ablation studies by comparing naive generation, generation with retrieved papers, and our proposed method. Naive generation refers to the direct use of LLMs to generate ideas based on the seed idea and reference codes, similar 6 Keywords Classification Detection Segmentation Completion Naive Filter (ours) 82 109 17 4 38 43 16 0 Table 3. For the 3D classification task, the frequency of each keyword is determined from the retrieved papers, focusing only on those words that appear in the abstracts and titles of papers scoring above 8 points in the ranking process. Naive and Filter refer to naive retrieval and filtering based on task attributes. to the approach used by AI-Scientist [23]. Generation with retrieved papers involves directly searching papers based on the topic and filtering them by their relevance to the input topic. As shown in Tab. 2, the naive generation yields the poorest results, with more than half of the ideas being judged as not novel. Furthermore, the quality of generated ideas is significantly improved when using naive retrieved papers, as this approach more closely aligns with the way human researchers generate ideas. However, as mentioned in Sec. 3.1, this approach tends to retrieve irrelevant papers and will mislead LLMs. As indicated in the first line of Tab. 3, some papers primarily focus on 3D detection or point cloud completion, where the design approach of the model is entirely different from that of the 3D point classification. This phenomenon can be well handled by the designed paper ranking process. As illustrated in Tab. 2 and Tab. 3, the number of novel ideas significantly increased from 8/20 to 19/20, while the proportion of papers related to irrelevant topics substantially decreased. This improvement can be attributed to the availability of more relevant It reference papers during the ideas generation process. is worth noting that the higher occurrence of the keyword segmentation is due to that, many studies concurrently perform both point classification and segmentation tasks. In addition, the average cost per idea is shown in Tab. 2. It can be seen that the cost of each idea is very small. Besides, the cost of generating each idea is relatively higher when retrieving papers. This is mainly due to the retrieval process and the longer prompt required for idea generation (i.e., both the title and abstract are fed into the LLMs as references). Analysis on Experimental Verification Process. The success rate of experiment execution is crucial for improving research efficiency. We further conducted studies on the experimental verification process. As illustrated in Tab. 4, we conduct experiments on three approaches: 1) directly feed the exception traceback to LLM, 2) extract the local code structure based on exception traceback, and then feed the local code structure and traceback to LLM, and 3) extract local code structure according to the information derived from the exception traceback, and then feed the local code structure and traceback to LLM. Firstly, we find that the successful execution rate is relatively low (e.g., 4 / 15) when directly feeding the traceback L.C.S. Traceback"
        },
        {
            "title": "Successful execution",
            "content": "Loop 1 Loop 2 Loop 3 4 / 15 3 / 15 7 / 15 5 / 13 5 / 13 6 / 13 5 / 14 6 / 14 8 / 14 Table 4. Results of successful execution rate. L.C.S. represents local code structure. Traceback denotes using information extracted from exception traceback. The denominator is the number of ideas after the novelty and independence check. Loop Loop 1 Loop 2 Loop 3 Total Improvement rate Cost (Avg.) 2 / 7 0.184 3 / 6 0.203 4 / 8 0. 9 / 21 0.201 Table 5. Performance in different loops. The denominator is the number of successfully executed ideas. Method Accuracy (Avg. class) Overall accuracy Human designed methods PointNet [28] PointNet++ [29] DGCNN [43] PointNeXt [30] OctFormer [41] GPSFormer [40] 86.2 - 90.2 90.8 - 91.8 89.2 91.9 92.9 93.2 92.7 93.8 Methods obtained by DOLPHIN (auto-research) PointNet-CSR 91.1 93. Table 6. Classification on ModelNet40 [45]. The results are obtained from 1024 points without voting. into LLM for debugging since the LLM cannot fully understand the complicated nested relationships in the codes. For example, when dimension mismatch error occurs between networks and feature dimensions, LLMs can easily locate where the error occurs. However, since the feature might be obtained through multiple nested modules, LLMs fail to correct the network dimension. Then, by adding the local code structure according to exception traceback, the success rate does not significantly improve. This is because the exception traceback contains lots of information about called libraries, which makes LLMs generate code structures irrelevant to our custom codes. Further, by guiding LLMs to generate code structures with information extracted from traceback, the execution rate can be largely improved (i.e., 33.3%50.0%). This is due to the extracted information containing custom code information related to the exception, enabling LLMs to focus on relevant functions and variables. Note that to improve efficiency, we only allow maximum of 5 debugging iterations in our experiments. Analysis on Results Feedback Process. To demonstrate the advantages of the closed-loop framework, we further analyze results on different loops. As shown in Tab. 5, we find 7 Figure 4. Case studies for the ideas and codes generated by DOLPHIN (Left) and human researcher (Right). that the quality of generated ideas without feedback is relatively low. The reasons can be divided into two folds: 1) Repeated ideas may be generated without feedback process, resulting in redundant verification of the same idea and decreased experimental efficiency. 2) In the absence of feedback, the model cannot learn what kind of ideas are effective for the specific task. In contrast, DOLPHIN effectively solves these challenges through closed-loop approach, demonstrating progressive improvement in idea quality as the number of iterations increases (e.g., 2/7 improvement rate in Loop 1 4/8 improvement rate in Loop 3). Further, these ideas that improve the performance are different between each round, which further improves the research efficiency and shows the effectiveness of DOLPHIN. Besides, the average cost slightly increases as the iterations continue, since the results will be fed back into the next round of ideas generation process. 4.4. Case Studies We illustrate our approach with an example drawn from 3D point classification task as shown in Fig. 4. It can be Diff. DGCNN PointNet-CSR (Completed by DOLPHIN) Idea Impl. 1) Architecture-level 2) With learnable parameters 3) Repeated blocks 1) Module-level 2) Without learnable parameters 3) Single module Multi-layer Edge with high complexity Single contextual semantic reasoning module with low complexity Results 1) 90.2% mAcc., 92.9% OA 2) 20.86s per epoch 1) 91.1% mAcc., 93.9% OA 2) 6.12s per epoch (> 3x faster) Table 7. The differences between DGCNN [43] proposed by human and PointNet-CSR proposed using DOLPHIN. seen that DOLPHIN can generate codes corresponding to the idea and only in this way, the generated ideas can be effectively verified. Tab. 6 presents the comparison between AIgenerated approach (i.e., PointNet-CSR obtained by DOLPHIN) and previous human-designed methods. Idea automatically generated and performed by DOLPHIN can outperform most human-designed methods and achieve comparable performance to current SOTA [40]. Furthermore, as shown in Tab. 7, we carefully investigate the existing works on 3D classification task up to the submission date, identifying the work most relevant to PointNet-CSR (AI-generated 8 3D work), as illustrated in Fig. 4. The detailed comparison of the idea, implementation, and result can be found in Tab. 7, showing that PointNet-CSR can achieve better and faster performance through more concise architecture. Please refer to our supplementary material for more comparisons between human-designed works and DOLPHINgenerated works. 5. Conclusion and Future Outlook DOLPHIN evaluates idea quality through experimental verification, improving it in closed-loop fashion. Besides, beyond conducting quantitative experiments that demonstrate DOLPHINs capability to generate solutions and results comparable to human-designed approaches, we also conducted in-depth case studies to evaluate the novelty of the ideas and the implementation efficiency of the codes generated by DOLPHIN. These quantitative and qualitative evaluations we conducted for DOLPHIN are essential for gaining further insight into the potential and value inherent in DOLPHIN. In the future, we envision DOLPHIN further advancing AI-driven automated scientific research. By harnessing its ability to generate novel ideas in closed-loop system, we also aspire for DOLPHIN to foster the development of groundbreaking ideas inspired by cross-disciplinary knowledge, ultimately providing innovative solutions for complex scientific challenges."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. [2] Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024. URL: https://www-cdn.anthropic.com/ de8ba9b01c9ab7cbabf5c33b80b7bbc618857627/ Model_Card_Claude_3.pdf. [3] Assafelovic. gpt-researcher, 2023. URL: https : / / github.com/assafelovic/gpt-researcher. [4] Jinheon Baek, Sujay Kumar Jauhar, Silviu Cucerzan, and Sung Ju Hwang. Researchagent: Iterative research idea generation over scientific literature with large language models. arXiv preprint arXiv:2404.07738, 2024. [5] Zheng Cai, Maosong Cao, Haojiong Chen, Kai Chen, Keyu Chen, Xin Chen, Xun Chen, Zehui Chen, Zhi Chen, Pei arXiv preprint Chu, et al. arXiv:2403.17297, 2024. Internlm2 technical report. [6] Angelica Chen, David Dohan, and David So. Evoprompting: language models for code-level neural architecture search. Advances in Neural Information Processing Systems, 36, 2024. [7] Kang Chen, Tao Han, Junchao Gong, Lei Bai, Fenghua Ling, Jing-Jia Luo, Xi Chen, Leiming Ma, Tianning Zhang, Rui Su, et al. Fengwu: Pushing the skillful global mediumrange weather forecast beyond 10 days lead. arXiv preprint arXiv:2304.02948, 2023. [8] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics, pages 41714186, 2019. [9] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2020. [10] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [11] Tao Han, Zhenghao Chen, Song Guo, Wanghan Xu, and Lei Bai. Cra5: Extreme compression of era5 for portable global climate and weather research via an efficient variational transformer. arXiv preprint arXiv:2405.03376, 2024. [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. [13] Xiang Hu, Hongyu Fu, Jinge Wang, Yifeng Wang, Zhikun Li, Renjun Xu, Yu Lu, Yaochu Jin, Lili Pan, and Zhenzhong Lan. Nova: An iterative planning and search approach to enhance novelty and diversity of llm generated ideas. arXiv preprint arXiv:2410.14255, 2024. [14] Yuntong Hu, Zhuofeng Li, Zheng Zhang, Chen Ling, Raasikh Kanjiani, Boxin Zhao, and Liang Zhao. Hireview: Hierarchical taxonomy-driven automatic literature review generation. arXiv preprint arXiv:2410.03761, 2024. [15] Qian Huang, Jian Vora, Percy Liang, and Jure Leskovec. MLAgentBench: Evaluating Language Agents on Machine Learning Experimentation. In ICML, 2024. [16] Shuyi Jia, Chao Zhang, and Victor Fung. Llmatdesign: Autonomous materials discovery with large language models. arXiv preprint arXiv:2406.13163, 2024. [17] John Jumper, Richard Evans, Alexander Pritzel, Tim Green, Michael Figurnov, Olaf Ronneberger, Kathryn Tunyasuvunakool, Russ Bates, Augustin ˇZıdek, Anna Potapenko, et al. Highly accurate protein structure prediction with alphafold. nature, 596(7873):583589, 2021. [18] Alex Kim, Maximilian Muhn, and Valeri Nikolaev. Financial statement analysis with large language models. arXiv preprint arXiv:2407.17866, 2024. [19] Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. [20] Long Li, Weiwen Xu, Jiayan Guo, Ruochen Zhao, Xinxuan Li, Yuqian Yuan, Boqiang Zhang, Yuming Jiang, Yifei Xin, Ronghao Dang, et al. Chain of ideas: Revolutionizing research in novel idea development with llm agents. arXiv preprint arXiv:2410.13185, 2024. [21] Siyi Liu, Chen Gao, and Yong Li. Large language model arXiv preprint agent for hyper-parameter optimization. arXiv:2402.01881, 2024. [22] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1197611986, 2022. [23] Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery . ArXiv, abs/2408.06292, 2024. [24] Renqian Luo, Liai Sun, Yingce Xia, Tao Qin, Sheng Zhang, Hoifung Poon, and Tie-Yan Liu. Biogpt: generative pretrained transformer for biomedical text generation and mining. Briefings in bioinformatics, 23(6):bbac409, 2022. [25] OpenAI. Hello gpt-4o. https : / / openai . com / index/hello-gpt-4o/, 2024. [26] Biqing Qi, Kaiyan Zhang, Haoxiang Li, Kai Tian, Sihang Zeng, Zhang-Ren Chen, and Bowen Zhou. Large language models are zero shot hypothesis proposers. arXiv preprint arXiv:2311.05965, 2023. [27] Biqing Qi, Kaiyan Zhang, Kai Tian, Haoxiang Li, ZhangRen Chen, Sihang Zeng, Ermo Hua, Hu Jinfang, and Bowen Zhou. Large language models as biomedical hypothesis arXiv preprint generators: comprehensive evaluation. arXiv:2407.08940, 2024. [28] Charles Qi, Hao Su, Kaichun Mo, and Leonidas Guibas. Pointnet: Deep learning on point sets for 3d classification In Proceedings of the IEEE conference and segmentation. on computer vision and pattern recognition, pages 652660, 2017. [29] Charles Ruizhongtai Qi, Li Yi, Hao Su, and Leonidas Guibas. Pointnet++: Deep hierarchical feature learning on point sets in metric space. Advances in neural information processing systems, 30, 2017. [30] Guocheng Qian, Yuchen Li, Houwen Peng, Jinjie Mai, Hasan Hammoud, Mohamed Elhoseiny, and Bernard Ghanem. Pointnext: Revisiting pointnet++ with improved training and scaling strategies. Advances in neural information processing systems, 35:2319223204, 2022. [31] Justus Randolph. guide to writing the dissertation literature review. Practical assessment, research, and evaluation, 14(1):13, 2019. [32] Zeeshan Rasheed, Muhammad Waseem, Aakash Ahmad, Kai-Kristian Kemell, Wang Xiaofeng, Anh Nguyen Duc, and Pekka Abrahamsson. Can large language models serve as data analysts? multi-agent assisted approach for qualitative data analysis. arXiv preprint arXiv:2402.01386, 2024. [33] Nils Reimers and Iryna Gurevych. Making Monolingual Sentence Embeddings Multilingual using Knowledge Distillation. In EMNLP, 2020. [34] Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. Can llms generate novel research ideas? large-scale human study with 100+ nlp researchers. arXiv preprint arXiv:2409.04109, 2024. [35] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionalIn Proceedings of the 2013 ity over sentiment treebank. conference on empirical methods in natural language processing, pages 16311642, 2013. [36] Haoyang Su, Renqi Chen, Shixiang Tang, Xinzhe Zheng, Jingzhe Li, Zhenfei Yin, Wanli Ouyang, and Nanqing Dong. Two heads are better than one: multi-agent system has the potential to improve scientific idea generation. arXiv preprint arXiv:2410.09403, 2024. [37] Copilot Team. copilot, 2023. URL: https://github. com/features/copilot. [38] Gemini Team, Rohan Anil, Sebastian Borgeaud, JeanBaptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. [39] Ollama Team. ollama, 2023. URL: https://ollama. com/. [40] Changshuo Wang, Meiqing Wu, Siew-Kei Lam, Xin Ning, Shangshu Yu, Ruiping Wang, Weijun Li, and Thambipillai Srikanthan. Gpsformer: global perception and local structure fitting-based transformer for point cloud understanding. In European Conference on Computer Vision. Springer, 2024. [41] Peng-Shuai Wang. Octformer: Octree-based transformers for 3d point clouds. ACM Transactions on Graphics (TOG), 42(4):111, 2023. [42] Qingyun Wang, Doug Downey, Heng Ji, and Tom Hope. SciMON: Scientific Inspiration Machines Optimized for Novelty. In ACL, 2024. [43] Yue Wang, Yongbin Sun, Ziwei Liu, Sanjay Sarma, Michael Bronstein, and Justin Solomon. Dynamic graph cnn for learning on point clouds. ACM Transactions on Graphics (tog), 38(5):112, 2019. [44] Yidong Wang, Qi Guo, Wenjin Yao, Hongbo Zhang, Xin Zhang, Zhen Wu, Meishan Zhang, Xinyu Dai, Min Zhang, Qingsong Wen, et al. Autosurvey: Large language arXiv preprint models can automatically write surveys. arXiv:2406.10252, 2024. [45] Zhirong Wu, Shuran Song, Aditya Khosla, Fisher Yu, Lin3d guang Zhang, Xiaoou Tang, and Jianxiong Xiao. shapenets: deep representation for volumetric shapes. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 19121920, 2015. [46] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [47] Zonglin Yang, Xinya Du, Junxian Li, Jie Zheng, Soujanya Poria, and E. Cambria. Large Language Models for Automated Open-domain Scientific Hypotheses Discovery. ACL Findings, 2024. [48] Shunyu Yao, Qingqing Ke, Qiwei Wang, Kangtong Li, and Jie Hu. Lawyer gpt: legal large language model with enIn hanced domain knowledge and reasoning capabilities. Proceedings of the 2024 3rd International Symposium on Robotics, Artificial Intelligence and Information Engineer10 ing, page 108112, New York, NY, USA, 2024. Association for Computing Machinery. [49] Sergey Zagoruyko. Wide residual networks. arXiv preprint arXiv:1605.07146, 2016. [50] Kechi Zhang, Jia Li, Ge Li, Xianjie Shi, and Zhi Jin. Codeagent: Enhancing code generation with tool-integrated agent systems for real-world repo-level coding challenges. arXiv preprint arXiv:2401.07339, 2024. [51] Lei Zhang, Yuge Zhang, Kan Ren, Dongsheng Li, and Yuqing Yang. Mlcopilot: Unleashing the power of large language models in solving machine learning tasks. arXiv preprint arXiv:2304.14979, 2023. [52] Shujian Zhang, Chengyue Gong, Lemeng Wu, Xingchao Liu, and Mingyuan Zhou. Automl-gpt: Automatic machine learning with gpt. arXiv preprint arXiv:2305.02499, 2023. [53] Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024."
        },
        {
            "title": "Outlines",
            "content": "In the Appendix section, we provide additional details and qualitative results from the following aspects: Sec. 6: More details of DOLPHIN including prompts, qualitative results of each step, and so on: Sec. 6.1: Ideas generation process. Sec. 6.2: Experimental verification process. Sec. 7: More details and results of performing experiments using DOLPHIN: Sec. 7.1 The selected datasets and implementation on different tasks: * Sec. 7.1.1: Image classification task. * Sec. 7.1.2: 3D classification task. * Sec. 7.1.3: Sentiment classification task. Sec. 7.2: Code implementation differences between human-designed and Dolphin-designed. Sec. 8: More analyses and further works: Sec. 8.1: The extensibility of the research task related to idea generation. Sec. 8.2: Analyses on future works. 6. More details of Dolphin In this section, we provide more details about DOLPHIN including prompts, qualitative results of some processes, algorithms of some processes, and so on. In the following section, we will give more details of the ideas generation process, experimental verification process, and results feedback process, respectively. 6.1. Ideas Generation Process We provide prompts used in paper retrieval, paper ranking, and idea generation process in Fig. 5. We partially refer to the prompt design of previous works [23, 34]. As depicted in our manuscript, the quality of retrieved papers is important to idea generation. Here, we give an example to further illustrate the impact. Given the topic 3D classification, naive retrieval will result in lots of papers related to 3D object detection. As result, we have identified several ideas that are more closely related to detection (e.g., region proposal PointNet), which are significantly influenced by the detection task. Further, we provide the algorithm of independence check in Algorithm 1. To show the effectiveness of the independence check process, we show an example in Fig. 6. It can be seen that although the name and title of the idea are totally different from each other, the technologies used in the two ideas are almost the same. Our idea independence check process can effectively filter the repeated ideas, further improving the auto-research efficiency. Algorithm 1: Independence Check Process Input: List of ideas I, previous paper memory bank B, sentence embedding model S, threshold τ . Output: Idea independence of each paper R. for idea summary in do if len(B) == 0 then R.append(True) end else Extract summary embedding: fs = S(s). Compare fs with summary embeddings in by sim = fs BT R1len(B). if max(sim) < τ then R.append(True) end else R.append(False) end end end Return: Independence list R, len(R) == len(I) 6.2. Experimental Verification Process Fig. 7 shows the local code structure generation prompt, which needs first to extract the exception traceback information. Further, to show how this information can guide the LLM in generating the local code structure, we provide an example for better illustration. As shown in Fig. 8, the LLM tend to copy the original reference code which is useless in the following debugging process. 7. More Details and Results of Experiments In this section, we provide comprehensive overview of the implementation details and dataset information used in our main text. 7.1. Details of Selected Tasks 7.1.1. Image Classification Task Dataset: CIFAR-100. The CIFAR-100 dataset [19] includes colored natural images with resolution of 3232 pixels, categorized into 100 distinct classes. The training and test sets contain 50,000 and 10,000 images, respectively. We adopt standard data augmentation scheme (i.e., RandomCrop, RandomHorizontalFlip, RandomRotation). Implementation Details. We use WRN-28-10 [49] as our baseline. We partially refer to the codebase2. Our 2https : / / github . com / bmsookim / wide - resnet . 12 training process employs the SGD optimizer with the CosineAnuealing scheduler. The initial learning rate is set to 0.1, and we train the model for 200 epochs with batch size of 128. We apply weight decay of 5e-4 and Nesterov momentum with coefficient of 0.9. not be judged without experiments. DOLPHIN which includes ideas generation, experimental verification, and results feedback process can serve as an evaluation protocol. In the future, it can be combined with auto-idea generation works to assess the effectiveness of the idea generation. 7.1.2. 3D Classification Task 8.2. Analysis on Future Works DOPLINE achieves the first closed-loop automatic research framework, we still hope that DOPLINE will possess stronger auto-research capabilities. For example, our ultimate goal is to utilize the capabilities of large models to integrate multi-disciplinary knowledge which is hard to be achieved by human researchers. To achieve this goal, we still need to make efforts in the following aspects: 1) develop more powerful code models that can understand and modify project-level code, and 2) retrieve multi-disciplinary papers that may be related to the given topic. Dataset: ModelNet40. ModelNet40 [45] is synthetic object dataset that contains 12,311 3D CAD models covering 40 categories. The standard training/validation set of ModelNet40 carries 9843/2468 point clouds. Implementation Details. We use PointNet [28] as our baseline. Following PointNet [28], we uniformly sample 1024 points on each object. We use the random scale, random dropout, and point shift during training and train the model for 200 epochs. The initial learning rate is set to 1e3. We use Adam optimizer (weight decay=1e-4) and step learning rate decay (step size=20, gamma=0.7). Our implementation partially refers to codebase3. 7.1.3. Sentiment Classification Task Dataset: SST-2. Stanford Sentiment Treebank (SST) [35] contains 11,855 one-sentence movie reviews extracted from Rotten Tomatoes. SST contains 215,154 unique manually labeled texts of varying lengths. Implementation Details. Our code of sentiment classification tasks refers to the codebase4. We fully fine-tune the BERT-base model for the classification task for 5 epochs with the learning rate 2e-5. The batch size is set to 32. We use the early stop mechanism during training (i.e., stop training if the accuracy of current epoch is lower than that of the previous epoch). 7.2. More Qualitative Results We provide several cases that are automatically generated and evaluated by DOLPHIN as shown in Fig. 9, Fig. 10, and Fig. 11. We show the ideas and modified codes in figures and the performance in the corresponding caption, respectively. 8. More analysis and Further Works 8.1. The Extensibility for the Research Task Recently, lots of works have explored automatic idea generation [20, 34]. One limitation is that the novelty of an idea can only be judged through human scoring or large model scoring. However, in real scientific research, we need ideas that can lead to performance breakthroughs that can pytorch 3https://github.com/yanx27/Pointnet_Pointnet2_ pytorch/tree/master 4https://github.com/YJiangcm/SST2sentimentanalysis 13 Figure 5. Prompts of paper retrieval, paper ranking, and ideas generation. 14 Figure 6. An example of independence check. Figure 7. Prompts of local code structure and debugging. 15 Figure 8. Code structure without extracted traceback information. Figure 9. Idea and codes generated by DOLPHIN which achieves 92.34% OA and 89.54% mAcc. on ModelNet40 (+1.34% OA and +1.94% mAcc. compared to our baseline). 17 Figure 10. Idea and codes generated by DOLPHIN which achieves 92.30% OA and 88.96% mAcc. on ModelNet40 (+1.30% OA and +1.36% mAcc. compared to our baseline). 18 Figure 11. Idea and codes generated by DOLPHIN which achieves 82.05% Acc. on CIFAR-100 (+0.85% Acc. compared to our baseline)."
        }
    ],
    "affiliations": [
        "Fudan University",
        "Shanghai Artificial Intelligence Laboratory"
    ]
}