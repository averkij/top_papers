{
    "paper_title": "Towards General Agentic Intelligence via Environment Scaling",
    "authors": [
        "Runnan Fang",
        "Shihao Cai",
        "Baixuan Li",
        "Jialong Wu",
        "Guangyu Li",
        "Wenbiao Yin",
        "Xinyu Wang",
        "Xiaobin Wang",
        "Liangcai Su",
        "Zhen Zhang",
        "Shibin Wu",
        "Zhengwei Tao",
        "Yong Jiang",
        "Pengjun Xie",
        "Fei Huang",
        "Jingren Zhou"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Advanced agentic intelligence is a prerequisite for deploying Large Language Models in practical, real-world applications. Diverse real-world APIs demand precise, robust function-calling intelligence, which needs agents to develop these capabilities through interaction in varied environments. The breadth of function-calling competence is closely tied to the diversity of environments in which agents are trained. In this work, we scale up environments as a step towards advancing general agentic intelligence. This gives rise to two central challenges: (i) how to scale environments in a principled manner, and (ii) how to effectively train agentic capabilities from experiences derived through interactions with these environments. To address these, we design a scalable framework that automatically constructs heterogeneous environments that are fully simulated, systematically broadening the space of function-calling scenarios. We further adapt a two-phase agent fine-tuning strategy: first endowing agents with fundamental agentic capabilities, then specializing them for domain-specific contexts. Extensive experiments on agentic benchmarks, tau-bench, tau2-Bench, and ACEBench, demonstrate that our trained model, AgentScaler, significantly enhances the function-calling capability of models."
        },
        {
            "title": "Start",
            "content": "2025-09-"
        },
        {
            "title": "Towards General Agentic Intelligence\nvia Environment Scaling",
            "content": "Runnan Fang, Shihao Cai, Baixuan Li, Jialong Wu ((cid:0)), Guangyu Li, Wenbiao Yin, Xinyu Wang, Xiaobin Wang, Liangcai Su, Zhen Zhang, Shibin Wu, Zhengwei Tao, Yong Jiang((cid:0)), Pengjun Xie, Fei Huang, Jingren Zhou Tongyi Lab , Alibaba Group https://tongyi-agent.github.io/blog https://github.com/Alibaba-NLP/DeepResearch"
        },
        {
            "title": "Abstract",
            "content": "Advanced agentic intelligence is prerequisite for deploying Large Language Models in practical, real-world applications. Diverse real-world APIs demand precise, robust function-calling intelligence, which needs agents to develop these capabilities through interaction in varied environments. The breadth of function-calling competence is closely tied to the diversity of environments in which agents are trained. In this work, we scale up environments as step towards advancing general agentic intelligence. This gives rise to two central challenges: (i) how to scale environments in principled manner, and (ii) how to effectively train agentic capabilities from experiences derived through interactions with these environments. To address these, we design scalable framework that automatically constructs heterogeneous environments that are fully simulated, systematically broadening the space of function-calling scenarios. We further adapt two-phase agent fine-tuning strategy: first endowing agents with fundamental agentic capabilities, then specializing them for domainspecific contexts. Extensive experiments on agentic benchmarks, τ-bench, τ2-Bench, and ACEBench, demonstrate that our trained model, AgentScaler, significantly enhances the models function-calling capability."
        },
        {
            "title": "Introduction",
            "content": "Function calling empowers language agents to interface with the real world (Qin et al., 2023; Chen et al., 2024b; Qin et al., 2024; Schick et al., 2023). Yet, their progress is fundamentally constrained by the scarcity of agentic data1, i.e., trajectories generated by autonomous agents interacting with environments via explicit action executions, namely, tool calls (Zhou et al., 2023; Liu et al., 2024a). The community has gradually transitioned from the era of raw corpora and human-curated data to the emerging era of experience (Silver & Sutton, 2025; Wu et al., 2025a; Li et al., 2025; Tao et al., 2025; Geng et al., 2025). Crucially, language agents must experience these interactions themselves in predefined environment, which makes both data collection and reliable supervision highly challenging. Several approaches have been attempted to generate synthetic agentic data. Broadly, previous methods fall into two categories. The first category follows reverse paradigm, in which user queries are generated to match each assistant function call observed at every interaction turn (Yin et al., 2025), though the 5 2 0 2 6 1 ] . [ 1 1 1 3 3 1 . 9 0 5 2 : r Equal contributors. yongjiang.yj@alibaba-inc.com 1In this paper, the terms function-calling, tool, API, MCP are used interchangeably; agentic data refers Jialong Wu (wujialongml@gmail.com) is project leader. (cid:0) to trajectories involving such interactions. resulting trajectories may exhibit limited realism. The second category follows forward paradigm, which we refer to as simulated agenthuman interplay (Chen et al., 2024a; Liu et al., 2024b; Prabhakar et al., 2025a; Barres et al., 2025; Zeng et al., 2025). Such generated trajectories, however, may lack naturalness. In this category, high-level user intent is first formulated to necessitate agent interaction. Agentic data is then constructed in top-down manner based on this intent through humanagent interplay. Yet, the environment is not scalable: the absence of automated environment construction hinders large-scale deployment and inevitably entails some degree of manual intervention. To address these challenges, we pursue the advancement of general agentic intelligence via systematic environment scaling. Our approach follows principled two-stage pipeline: (i) fully simulated environment construction and scaling, responsible for establishing and expanding diverse agentic scenarios, and (ii) agent experience learning, which exploits these environments to foster generalizable intelligence. In designing environment construction and scaling, we follow the principle that the core of an agent lies in its capacity for environment interaction, with each environment instantiated as readwrite database (Barres et al., 2025; Zeng et al., 2025). Specifically, we collect broad spectrum of APIs and organize them into domains using community detection, where each domain represents an environment aligned with specific database structure. Then, we instantiate tools as executable code, thereby achieving programmatic materialization that enables direct operations on the underlying database structures. Finally, we sample from the domain-specific tool graph to generate parameters for the tool sequences and initialize the corresponding database state. We then integrate these components into an overall user intent, grounding tool executions directly on the database. This design enables verifiability at both the environment level and the tool-argument response level. For learning from agent experience, our focus is on training the agents ability to perform tool calls and to respond effectively to users (Ye et al., 2025b; Su et al., 2025). We begin by performing simulated humanagent interactions on the constructed agentic tasks (Prabhakar et al., 2025a), thereby collecting trajectories that serve as the agents experience and perform strict filtering. To facilitate the acquisition of this capability, we adopt two-stage agent experience learning framework: in stage 1, the agent acquires fundamental tool-calling skills across general domains; in stage 2, it is further trained within target vertical domains using domain-specific scenarios, enabling smoother and more context-aligned development of agentic capabilities. Extensive experiments on agentic benchmarks, τ-bench (Yao et al., 2024), τ2-Bench (Barres et al., 2025), and ACEBench (Chen et al., 2025) show the effectiveness of our pipeline and trained models. Based on the above pipeline, we train our family of AgentScaler models (4B, 8B, 30B-A3B), built upon the Qwen-3 (Team, 2025b) series. At each comparable scale (4B, 8B), our models achieve state-of-the-art performance. Notably, AgentScaler-30-A3B sets new state-of-the-art with significantly fewer parameters, delivering results on par with existing 1T-parameter models and leading closed-source systems. We also provide systematic analysis covering model generalization, stability, and the long-horizon tool-calling challenge, offering key insights into the development of general agentic intelligence."
        },
        {
            "title": "2 Environment Build and Scaling",
            "content": "Design Principal In essence, any function call can be interpreted as readwrite operation over an underlying environmental database (Guo et al., 2025). Specifically, each function unc can be assigned an operator type, op( unc) {read, write}, where read-type function perform queries over (e.g., retrieval, inspection, monitoring), while write-type tools induce state transitions in (e.g., modification, generation, actuation). Under this abstraction, tool response is equivalent to evaluating the induced operator on D, i.e., API( unc, α) op( unc)(α; D), where the symbol α denotes the input arguments provided to function call. Furthermore, let Td denote the set of tools within domain d. Tools in the same domain typically exhibit structurally similar readwrite patterns, which can be captured by common 2 database schema Sk. Consequently, the design problem reduces to defining partition of the tool space into domains {T1 . . . , TM}, and assigning to each domain database schema Sk, where Sk specifies the environment for that domain. Figure 1: The overview of the environment automatic build, and agentic task construction."
        },
        {
            "title": "2.1 Environment Automatic Build",
            "content": "Building upon this design principle, we propose systematic pipeline for leveraging diverse set of tools as shown in Figure 1. We begin with scenario collection, which gathers large corpus of real-world tools; proceed to tool dependency graph modeling, which induces well-structured domain partitions and distributions; and finally employ function schema programmatic materialization, which maps tool operations onto database interactions, thereby enabling the construction of the overall environment. Scenario Collection We collected more than 30,000 APIs from ToolBench (Qin et al., 2023; Guo et al., 2024), API-Gen (Prabhakar et al., 2025b) and our internal tool repository. After applying rigorous filtering, including the removal of low-quality APIs and subsequent refinement, we rewrite some API descriptions to incorporate explicit inputoutput specifications (Fang et al., 2025). Building on this, we further constructed tool compositions by systematically exploiting the inputoutput relationships among APIs. This process ultimately resulted in API pools ΘF whose size = (over 30,000), providing reliable foundation for subsequent experiments and analysis. Tool Dependency Graph Modeling We construct tool graph in which nodes are tools and edges encode compositional compatibility induced by function parameters. tool unc consists of description Pf unc and list of parameters Pf unc. For pair of tools, we can extract their respective parameter lists and convert them into vector representations ϕ to compute their cos-similarity. If the similarity exceeds predefined threshold τ, we consider there to be dependency relationship between the two tools. Accordingly, we insert an edge between them in our graph. (cid:110) = (i, j) sim(ϕ(Pf unci ), ϕ(Pf uncj )) > τ, = (cid:111) (1) Domain partitioning then reduces to graph clustering problem. We employ Louvain community detection (Blondel et al., 2008) to identify coherent tool communities that serve as domains. For segmented tool set, since parameter matching relies solely on vectorization and considers only individual parameter information, the overall inter-tool dependencies may be difficult to capture. Therefore, for tools within given domain, we further employ an LLM to systematically examine the dependencies between each pair of tools, thereby further improving the accuracy of edges in the tool graph. In total, we obtained domains (exceeding 1,000). Function Schema Programmatic Materialization We first leverage the parameters of all tools within domain to generate domain-specific database structure, which serves as the underlying state for subsequent tool operations. After obtaining the domain-specific tool set and the corresponding database schema in the previous stage, we can formalize each tool in python code, enabling it to perform readwrite operations over the database schema. Interestingly, when generating database structures and formalizing code within specific domains of τ-bench, we observe through manual inspection that our outputs exhibit high degree of consistency with the official implementations provided by τ-bench (Yao et al., 2024)."
        },
        {
            "title": "2.2 Agentic Task Construction",
            "content": "We construct trajectories via forward simulated agenthuman interplay, which allows us to fully simulate the environment, the user, and the agent. The critical step is to synthesize agentic tasks that elicit human tool usage while ensuring that the resulting trajectories remain verifiable. Concretely, we first initialize an environment state based on the domain-specific database schema, while encouraging as much diversity as possible in the initial state. Next, we sample logically coherent tool sequences from the domains tool graph, specifically by constructing directed dependency graph over APIs and traversing it to obtain valid sequences. Starting from randomly selected initial node, we conduct directed walk until either the maximum execution steps are reached or node with no outgoing edges is encountered. This process yields logically coherent tool sequence. For each step, we generate the corresponding arguments and perform the actual tool call, grounding the operations directly on the database and continuously tracking the evolving database state. This procedure enables verifiability at two complementary granularities: (i) database-level state consistency and (ii) exact matching of tool sequences."
        },
        {
            "title": "3 Agent Experience Learning",
            "content": "We leverage user intent to drive interactions that yield agent experiences, and train the model through two-phase process. Figure 2: The agent interacts with the simulated user and changes the environment state through the generated functions."
        },
        {
            "title": "3.1 Human–Agent Interplay for Experience Collection",
            "content": "Interplay Motivated by Yao et al. (2024), once we have constructed an agentic task, we proceed to perform human-agent interplay in the environment. Specifically, we instantiate simulated user tasked with fulfilling given overall intent. The agent then leverages domain-specific tools to address the users needs, continuing the interaction until the simulated user deems the task complete. This setup enables us to conduct end-to-end simulation, encompassing user simulation, agent, and environment, yielding highly scalable framework. Each completed interaction trace constitutes an agent experience, which can subsequently be used for training. Importantly, since we possess both the gold tool sequences and arguments for the overall intent and the final environment state, we can apply these as supervision signals for experience filtering. Filtering We adopt three-stage funnel-based trajectory filtering framework consisting of validity control, environment state alignment, and function calling exact match. Validity control, removes invalid interaction trajectories to ensure well-formed alternating user assistant exchanges. Additionally, we apply an n-gram-based filtering procedure to eliminate severely repetitive reasoning segments. In such cases, we discard these data points. Environment state alignment retains only those trajectories whose final database state matches the golden state after the interplay, thereby validating the effectiveness of write operations. The filtering granularity at this stage is the database/environment level. Function calling exact match serves as the most stringent filtering stage, where the granularity is the tool sequence. Since tool sequence consisting entirely of read operations without any write operations would cause state-based filtering to fail, we adopt stricter exact match approach for filtering in such cases. trajectory is preserved only if the sequence of invoked tools and arguments exactly matches the overall intent, ensuring high-fidelity supervision. It is worth noting that we do not filter out trajectories in which tool calls return errors. Thanks to the aforementioned filtering framework, such trajectories may still accomplish the intended goal despite intermediate failures. Retaining them in the training data helps improve the robustness of the model."
        },
        {
            "title": "3.2 Agentic Experience Learning",
            "content": "Agentic Fine-tuning Given agent-human interplay experience trajectory = (h0, a1, . . . , an1, hn, a0), where each human instruction is denoted by ht at t-round interaction, and each assistant turn at is decomposed as at = (τt, ρt, yt). Here, τt represents the function call tokens, ρt the tool response tokens, and yt the assistant response tokens. Our training objective is to optimize only the tool calls and assistant responses, while human instructions hi and tool responses ρt are excluded from the loss. Formally, given an autoregressive model pθ(xk x<k), we define the loss as L(θ) = 1 I[ki ] k=1 k=1 I[xk ] log πθ (xi x<k) , (2) where xk denotes the k-th token in the trajectory, πθ is the model distribution, I[] is the indicator function, is the set of tokens belonging to tool calls τ or assistant responses y. In practice, all tokens in ρi and hi are masked out from supervision but remain visible in the context x<k. This ensures that the model conditions on tool responses and human instructions, while gradients are only propagated through assistant-generated tool calls and natural-language responses. Two-stage Experience Learning In the first phase, the agent is trained to acquire fundamental skills for tool usage and user interaction. We focus on general domains where broad set of tools and tasks are available, allowing the agent to develop robust understanding of when and how to invoke function calls, as well as how to integrate tool outputs into coherent user-facing responses. This stage emphasizes 5 breadth and generality, ensuring that the agent builds versatile foundation of agentic behaviors before domain-specific specialization. In the second phase, the agent undergoes fine-grained training in vertical domains, where tasks, tools, and user intents exhibit domain-specific characteristics. By grounding the learning process in realistic scenarios within target domain, the agent refines its ability to select tools, parameterize calls, and produce responses that are accurate, contextually appropriate, and aligned with domain-specific goals. This specialization ensures smoother adaptation of agentic capabilities, enabling the agent to operate effectively in real-world, task-oriented environments."
        },
        {
            "title": "4.1 Setup",
            "content": "Benchmarks We evaluate our methods on three established agentic benchmarks: τ-bench, τ2-Bench, and ACEBench-en. For τ-Bench (covering the retail and airline domains) and τ2-Bench (spanning the retail, airline, and telecom domains), we adopt the passˆ 1 metric for evaluation and additionally analyze the trend of passˆ k, following the protocols in Yao et al. (2024); Barres et al. (2025). For ACEBench-en, we report results across the Normal, Special, and Agent categories, as well as the Overall performance, using the accuracy metric. Baselines We compare our trained series models against the following types: closed-sourced large language model, including Gemini-2.5-pro (Comanici et al., 2025), Claude-Sonnet-4 (Anthropic, 2025), GPT-o3, GPT-o4-mini (OpenAI, 2025b), and GPT-5 (with thinking) (OpenAI, 2025a); open-sourced large language model: GPT-OSS-120B-A5B (Agarwal et al., 2025), Deepseek-V3.1-671B-A37B (DeepSeek-AI, 2024), KimiK2-1T-A32B (Team et al., 2025), Qwen3-Thinking-235B-A22B (Team, 2025b), Seed-OSS-36B (Team, 2025a), Qwen-Coder-30B-A3B (Hui et al., 2024), and xLAM-2 model series (Prabhakar et al., 2025a). Backbones We train the AgentScaler model series by training on Qwen3 models (Team, 2025b) of varying scales. Specifically, AgentScaler-4B and AgentScaler-30B-A3B are trained on Qwen3-Thinking-4B-2507 and Qwen3-Thinking-30B-A3B-2507, respectively, while AgentScaler-8B is trained on Qwen3-8B."
        },
        {
            "title": "4.2 Experimental Results",
            "content": "Main Results From Table 1, we observe that closed-source large language models (LLMs) still maintain clear performance advantage, consistently achieving the highest scores across most domains and benchmarks. This demonstrates the strength of industrial-scale training pipelines and proprietary optimization strategies. Nevertheless, our proposed AgentScaler achieves remarkable level of performance given its lightweight parameter scale. Specifically, it surpasses most open-source baselines with fewer than 1T parameters, establishing new state-of-the-art across τ-bench, τ2-Bench, and ACEBench-en. Notably, AgentScaler-4B achieves performance on par with 30B-parameter models despite using the fewest parameters, highlighting the agentic potential of compact LLMs. Moreover, AgentScaler-30BA3B delivers results that are comparable to trillionparameter open-source models and, in several domains, approach those of closed-source counterparts. These findings highlight the efficiency of our approach: agentic capabilities can be effectively learned and deployed even in relatively compact models, enabling competitive performance without relying on massive parameter counts. This advantage makes AgentScaler particularly wellsuited for practical deployment in resource-constrained or latency-sensitive scenarios. Figure 3: Performance comparison on the Normal, Agent, and Overall subsets of ACEBench-en for twostage training models. 6 Table 1: Main results on τ-bench, τ2-Bench, and ACEBench-en. Model Retail Airline Retail Airline Telecom Normal Special Agent Overall τ-bench τ2-Bench ACEBench-en Gemini-2.5-pro Claude-Sonnet-4 GPT-o3 GPT-o4-mini GPT-5-think GPT-OSS-120B-A5B Deepseek-V3.1-671B-A37B Kimi-K2-1T-A32B Qwen3-Thinking-235B-A22B Seed-OSS-36B Qwen-Coder-30B-A3B xLAM-2-8B-fc-r xLAM-2-32B-fc-r xLAM-2-70B-fc-r Qwen3-Thinking-4B Qwen3-8B Qwen3-14B Qwen3-Thinking-30B-A3B AgentScaler-4B AgentScaler-8B AgentScaler-30B-A3B 68.7 73.9 70.4 70.4 78. 67.8 66.1 73.9 67.8 70.4 68.7 58.2 64.3 67.1 59.1 45.2 45.7 67.8 64.3 50.4 70.4 Closed-Source Large Language Models 44.0 40.0 52.0 46.0 44.0 67.5 67.5 80.2 70.2 81.1 56.0 54.0 64.8 56.0 62.6 27.2 47.4 58.2 46.5 96.7 Open-Source Large Language Models 49.2 40.0 51.2 46. 46.0 48.0 35.2 45.0 45.2 52.5 25.0 31.0 48.0 54.0 42.0 54.0 57.0 64.9 70.6 71.9 68.4 60. 55.3 55.3 61.4 56.1 41.2 48.0 58.8 62.3 58.8 70.2 38.0 46.0 56.5 58.0 52.0 42.0 48.0 52.0 56. 52.0 30.5 30.0 58.0 56.0 44.0 60.0 45.6 38.5 65.8 45.6 41.2 30.7 11.4 16.7 14.0 28.7 23.5 26.9 26. 48.2 45.4 55.3 76.7 79.9 78.3 79.9 76.7 79.1 80.3 78.9 72.1 79.1 74.0 58.8 69.2 57.1 43.3 71.4 66.9 64. 70.3 69.2 76.7 90.0 87.3 86.7 84.0 85.3 84.0 62.0 81.3 84.0 82.0 41.3 0.0 24.7 5.3 84.7 75.3 84.0 86. 76.7 76.7 82.7 63.4 42.5 63.3 60.0 32.5 50.8 40.8 65.0 39.1 58.4 24.1 5.0 13.4 38.4 11.7 29.1 44.2 42. 30.8 44.2 60.0 78.2 76.1 78.2 77.9 72.2 76.0 69.3 77.4 70.2 76.7 57.5 34.8 52.5 36.5 49.5 65.9 68.0 67. 65.9 67.4 75.7 Ablation Study We further conduct an ablation analysis to examine the effect of the proposed two-stage agent experience learning framework on ACEBench-en. As shown in Figure 3, both Stage 1 and Stage 2 training substantially improve performance over the base model (Qwen3-Thinking-30B-A3B) across all subsets. And through multi-steps agent training in Stage 2, the models score on the agent set has further improved, and the overall score has also increased. These results validate the design of the two-phase training pipeline: general foundation learning is critical for establishing tool-usage competence, and subsequent domain-specialization further consolidates and contextualizes these capabilities."
        },
        {
            "title": "5 Analysis",
            "content": "Model Table 2: The results on ACEBench-zh. Our synthetic data approach enables efficient knowledge transfer and strong robustness and generalization. We further evaluate our models on ACEBench-zh, which represents an out-ofdistribution (OOD) scenario relative to the training setup. As shown in Table 2, the AgentScaler models consistently outperform their Qwen baselines across all scales in terms of overall score. In particular, AgentScaler-30B-A3B achieves the best overall score of 81.5, demonstrating strong improvements in both the Normal and Agent subsets, while maintaining competitive performance on the Special subset. Notably, the small Qwen3-4B model demonstrated remarkable improvement in agentic capabilities after the two-stage training, with its score surging from 6.7 to 38.4 and substantial gain of 21.7 points in the overall score. This offers valuable insights into effectively training compact models for complex function calling tasks in real-world applications. Qwen3-Thinking-30B-A3B AgentScaler-30B-A3B Qwen3-Thinking-4B AgentScaler-4B Qwen3-8B AgentScaler-8B ACEBench-zh Agent 35.0 58.4+23.4 73.4 85.3+11.9 34.7 70.8+36.1 43.9 65.6+21.7 6.7 38.4+31. 85.3 70.0-15.3 74.2 81.5+7.3 72.7 79.3+6.6 55.8 64.1+8.3 71.3 73.7+2.4 86.7 83.3-3. 80.3 75.2-5.1 Special Overall Normal AgentScaler shows the strong consistency, stability. To assess the stability of AgentScaler, Figure 4 reports the passˆ metric on the τ2-Bench, which denotes the accuracy achieved when the model Figure 4: Passˆ metric results across all domains in the τ2-Bench. correctly answers the same question in all independent trials. According to the experimental results, the weighted overall score of AgentScaler-30B-A3B consistently surpasses that of Qwen3-Thinking-30B-A3B across all evaluated passˆ settings, indicating substantial performance advantage of our model over Qwen3-Thinking-30B-A3B. Moreover, clear downward trend in scores is observed as increases, suggesting that the stability of existing LLMs remains considerable challenge. Long-horizon tool calling remains fundamental challenge for agentic models. To further analyze the models long-horizon tool-calling capability, we constructed scatter plot on the τ-bench dataset showing the relationship between the number of tool calls in each trajectory and the corresponding trajectory accuracy, with dashed line indicating the trend. As illustrated in Figure 5, there exists clear negative correlation between the number of tool calls and task accuracy. Our AgentScaler models exhibit this trend as well, underscoring that handling extended tool-use chains is still an open problem that we plan to address in future work. Figure 5: Accuracy by tool call count on τ-bench."
        },
        {
            "title": "6.1 Tool-Use Environments",
            "content": "The construction of tool-use environments primarily involves three approaches: real-world environments, LLMsimulated environment, and Simulated Environments based on state config. Using real-world environments (Qin et al., 2023; Song et al., 2023; Mastouri et al., 2025; Wu et al., 2025b) to invoke actual tools yields the most authentic feedback and enhances the models robustness in practical applications. However, this requires frequent calls to MCP services, resulting in high costs and significant time overhead. Moreover, maintaining highly available and stable MCP service is often difficult, posing major challenges for agentic data generation and online RL training of models.Many works use LLMgenerated responses to simulate environments as source of tool responses (Qin et al., 2024; Lu et al., 2024; 8 Sun et al., 2025). By leveraging strong or fine-tuned LLMs, these approaches generate plausible responses given tool call. However, such methods struggle with issues like hallucination and inconsistent response variability. To address the limitations of the above two approaches, some recent work (Ye et al., 2025b; Yao et al., 2024; Barres et al., 2025; Prabhakar et al., 2025b; Ye et al., 2025a) proposes building an offline tool execution environment for LLM training and evaluation. On one hand, offline environments avoid calling real tools, significantly reducing response generation cost and latency. On the other hand, mocked tool usage in such environments can still interact with real databases or state files through actual execution. However, these methods are more commonly applied in LLM evaluation rather than training, as constructing reliable tool suite and high-fidelity execution environment typically requires substantial manual effort. Furthermore, it is difficult to automatically validate the quality of such environments without human involvement, making scalability significant challenge. Our approach enables domain scalability through sampling from toolgraph, and eliminates the need for human intervention via rigorous, rule-based validation pipeline. This makes scalable construction of tool execution environments feasible."
        },
        {
            "title": "6.2 Tool Learning",
            "content": "To enhance the agentic capabilities and tool-calling abilities of models, many works have attempted to improve tool utilization through various approaches. For instance, xLAMs (Prabhakar et al., 2025b; Zhang et al., 2024) and ToolAce (Liu et al., 2024a) leverage large-scale agentic data synthesis pipelines to generate high-quality training data and thereby boost model performance. DiaTool-DPO (Jung et al., 2025) employs DPO to enable models to learn from multi-turn positive and negative trajectories. Meanwhile, Tool-RL (Qian et al., 2025), Tool-N1 (Zhang et al., 2025) utilize reinforcement learning (RL) algorithms to enhance both the tool-calling proficiency and generalization ability of models, further pushing the performance boundaries beyond supervised fine-tuning. Overall, whether relying on agentic data synthesis or online interaction with environments via RL training, stable, reliable, and scalable execution environment is essential. For example, Kimi-K2 (Team et al., 2025) uses tool simulator during Agentic Data Synthesis to obtain observations for multi-turn trajectories. Our method not only leverages accurately simulated tool environments to collect trajectories but also introduces verifiable environmental state changes, making each simulation response more reliable. Furthermore, we propose state change based environment validation strategy, enabling robust filtering mechanism for large-scale agentic data synthesis."
        },
        {
            "title": "7 Conclusion",
            "content": "In this work, we presented principled pipeline for advancing general agentic intelligence through systematic environment scaling and agent experience learning. By programmatically materializing tools as executable code and grounding them in database-structured environments, our approach enables large-scale construction of verifiable trajectories. Building on these environments, we introduced two-stage agent experience learning framework that first equips agents with fundamental tool-usage capabilities and then specializes them for domain-specific contexts. Extensive experiments on three representative benchmarks, τ-bench, τ2-Bench, and ACEBench, demonstrate the effectiveness of our pipeline. Notably, our AgentScaler family achieves state-of-the-art performance among open-source models under 1T parameters, and in several cases reaches parity with much larger or closed-source counterparts. Looking ahead, we believe our work highlights the importance of scalable environment construction and verifiable agentic experience for fostering robust and generalizable language agents. Future directions include integrating reinforcement learning on top of our fully simulated environments and extending our pipeline to broader modalities and real-world deployment scenarios."
        },
        {
            "title": "Limitation",
            "content": "Although our proposed framework has demonstrated promising results, several limitations remain, which point to ongoing efforts and potential directions for future work. Lack of Reinforcement Learning While our approach leverages two-stage supervised fine-tuning (SFT), the simulated environment we construct provides stable and low-latency feedback, which is inherently well-suited for RL optimization. In future work, we aim to integrate RL on top of our simulated environment to further improve the agentic behavior of the model. Model Scale Another limitation of our current work lies in model scale. Our method has so far only been validated on 30B-scale architecture, without extension to larger models exceeding 200B or even trillion-parameter scales. While prior work (Belcak et al., 2025) emphasizes that small language models are the future of agentic AI, we share the view that training agentic capabilities in relatively smaller models is particularly meaningful. Such models are easier to deploy on edge devices, enable broader applicability across diverse scenarios, and offer faster response times."
        },
        {
            "title": "References",
            "content": "Sandhini Agarwal, Lama Ahmad, Jason Ai, Sam Altman, Andy Applebaum, Edwin Arbus, Rahul Arora, Yu Bai, Bowen Baker, Haiming Bao, et al. gpt-oss-120b & gpt-oss-20b model card. arXiv preprint arXiv:2508.10925, 2025. Anthropic. System card: Claude opus 4 & claude sonnet 4, 2025. URL https://www-cdn.anthropic.c om/6d8a8055020700718b0c49369f60816ba2a7c285.pdf. Victor Barres, Honghua Dong, Soham Ray, Xujie Si, and Karthik Narasimhan. tau2-bench: Evaluating conversational agents in dual-control environment. arXiv preprint arXiv:2506.07982, 2025. Peter Belcak, Greg Heinrich, Shizhe Diao, Yonggan Fu, Xin Dong, Saurav Muralidharan, Yingyan Celine Lin, and Pavlo Molchanov. Small language models are the future of agentic ai. arXiv preprint arXiv:2506.02153, 2025. Vincent Blondel, Jean-Loup Guillaume, Renaud Lambiotte, and Etienne Lefebvre. Fast unfolding of communities in large networks. Journal of statistical mechanics: theory and experiment, 2008(10):P10008, 2008. Chen Chen, Xinlong Hao, Weiwen Liu, Xu Huang, Xingshan Zeng, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Yuefeng Huang, et al. Acebench: Who wins the match point in tool usage? arXiv preprint arXiv:2501.12851, 2025. Mingyang Chen, Haoze Sun, Tianpeng Li, Fan Yang, Hao Liang, Keer Lu, Bin Cui, Wentao Zhang, Zenan Zhou, and Weipeng Chen. Facilitating multi-turn function calling for llms via compositional instruction tuning. arXiv preprint arXiv:2410.12952, 2024a. Zehui Chen, Kuikun Liu, Qiuchen Wang, Wenwei Zhang, Jiangning Liu, Dahua Lin, Kai Chen, and Feng Zhao. Agent-flan: Designing data and methods of effective agent tuning for large language models. arXiv preprint arXiv:2403.12881, 2024b. Gheorghe Comanici, Eric Bieber, Mike Schaekermann, Ice Pasupat, Noveen Sachdeva, Inderjit Dhillon, Marcel Blistein, Ori Ram, Dan Zhang, Evan Rosen, et al. Gemini 2.5: Pushing the frontier with advanced reasoning, multimodality, long context, and next generation agentic capabilities. arXiv preprint arXiv:2507.06261, 2025. DeepSeek-AI. Deepseek-v3 technical report, 2024. URL https://arxiv.org/abs/2412.19437. Runnan Fang, Xiaobin Wang, Yuan Liang, Shuofei Qiao, Jialong Wu, Zekun Xi, Ningyu Zhang, Yong Jiang, Pengjun Xie, Fei Huang, et al. Synworld: Virtual scenario synthesis for agentic action knowledge refinement. arXiv preprint arXiv:2504.03561, 2025. Xinyu Geng, Peng Xia, Zhen Zhang, Xinyu Wang, Qiuchen Wang, Ruixue Ding, Chenxi Wang, Jialong Wu, Yida Zhao, Kuan Li, et al. Webwatcher: Breaking new frontiers of vision-language deep research agent. arXiv preprint arXiv:2508.05748, 2025. Zhicheng Guo, Sijie Cheng, Hao Wang, Shihao Liang, Yujia Qin, Peng Li, Zhiyuan Liu, Maosong Sun, and Yang Liu. Stabletoolbench: Towards stable large-scale benchmarking on tool learning of large language models. arXiv preprint arXiv:2403.07714, 2024. Zhicheng Guo, Sijie Cheng, Yuchen Niu, Hao Wang, Sicheng Zhou, Wenbing Huang, and Yang Liu. Stabletoolbench-mirrorapi: Modeling tool environments as mirrors of 7,000+ real-world apis. arXiv preprint arXiv:2503.20527, 2025. Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Kai Dang, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. 11 Sunghee Jung, Donghun Lee, Shinbok Lee, Gaeun Seo, Daniel Lee, Byeongil Ko, Junrae Cho, Kihyun Kim, Eunggyun Kim, and Myeongcheol Shin. Diatool-dpo: Multi-turn direct preference optimization for tool-augmented large language models. arXiv preprint arXiv:2504.02882, 2025. Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, et al. Websailor: Navigating super-human reasoning for web agent. arXiv preprint arXiv:2507.02592, 2025. Weiwen Liu, Xu Huang, Xingshan Zeng, Xinlong Hao, Shuai Yu, Dexun Li, Shuai Wang, Weinan Gan, Zhengying Liu, Yuanqing Yu, et al. Toolace: Winning the points of llm function calling. arXiv preprint arXiv:2409.00920, 2024a. Zuxin Liu, Thai Hoang, Jianguo Zhang, Ming Zhu, Tian Lan, Juntao Tan, Weiran Yao, Zhiwei Liu, Yihao Feng, Rithesh RN, et al. Apigen: Automated pipeline for generating verifiable and diverse function-calling datasets. Advances in Neural Information Processing Systems, 37:5446354482, 2024b. Jiarui Lu, Thomas Holleis, Yizhe Zhang, Bernhard Aumayer, Feng Nan, Felix Bai, Shuang Ma, Shen Ma, Mengyu Li, Guoli Yin, et al. Toolsandbox: stateful, conversational, interactive evaluation benchmark for llm tool use capabilities. arXiv preprint arXiv:2408.04682, 2024. Meriem Mastouri, Emna Ksontini, and Wael Kessentini. Making rest apis agent-ready: From openapi to model context protocol servers for tool-augmented llms. arXiv preprint arXiv:2507.16044, 2025. OpenAI. Introducing gpt-5, 2025a. URL https://openai.com/index/introducing-gpt-5/. OpenAI. Introducing openai o3 and o4-mini, 2025b. URL https://openai.com/index/introducing-o 3-and-o4-mini/. Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, et al. Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601, 2025a. Akshara Prabhakar, Zuxin Liu, Ming Zhu, Jianguo Zhang, Tulika Awalgaonkar, Shiyu Wang, Zhiwei Liu, Haolin Chen, Thai Hoang, Juan Carlos Niebles, et al. Apigen-mt: Agentic pipeline for multi-turn data generation via simulated agent-human interplay. arXiv preprint arXiv:2504.03601, 2025b. Cheng Qian, Emre Can Acikgoz, Qi He, Hongru Wang, Xiusi Chen, Dilek Hakkani-Tür, Gokhan Tur, and Heng Ji. Toolrl: Reward is all tool learning needs. arXiv preprint arXiv:2504.13958, 2025. Yujia Qin, Shihao Liang, Yining Ye, Kunlun Zhu, Lan Yan, Yaxi Lu, Yankai Lin, Xin Cong, Xiangru Tang, Bill Qian, et al. Toolllm: Facilitating large language models to master 16000+ real-world apis. arXiv preprint arXiv:2307.16789, 2023. Yujia Qin, Shengding Hu, Yankai Lin, Weize Chen, Ning Ding, Ganqu Cui, Zheni Zeng, Xuanhe Zhou, Yufei Huang, Chaojun Xiao, et al. Tool learning with foundation models. ACM Computing Surveys, 57 (4):140, 2024. Timo Schick, Jane Dwivedi-Yu, Roberto Dessì, Roberta Raileanu, Maria Lomeli, Eric Hambro, Luke Zettlemoyer, Nicola Cancedda, and Thomas Scialom. Toolformer: Language models can teach themselves to use tools. Advances in Neural Information Processing Systems, 36:6853968551, 2023. David Silver and Richard Sutton. Welcome to the era of experience. Google AI, 1, 2025. Yifan Song, Weimin Xiong, Dawei Zhu, Wenhao Wu, Han Qian, Mingbo Song, Hailiang Huang, Cheng Li, Ke Wang, Rong Yao, et al. Restgpt: Connecting large language models with real-world restful apis. arXiv preprint arXiv:2306.06624, 2023. 12 Hongjin Su, Ruoxi Sun, Jinsung Yoon, Pengcheng Yin, Tao Yu, and Sercan Ö Arık. Learn-byinteract: data-centric framework for self-adaptive agents in realistic environments. arXiv preprint arXiv:2501.10893, 2025. Hao Sun, Zile Qiao, Jiayan Guo, Xuanbo Fan, Yingyan Hou, Yong Jiang, Pengjun Xie, Yan Zhang, Fei Huang, and Jingren Zhou. Zerosearch: Incentivize the search capability of llms without searching. arXiv preprint arXiv:2505.04588, 2025. Zhengwei Tao, Jialong Wu, Wenbiao Yin, Junkai Zhang, Baixuan Li, Haiyang Shen, Kuan Li, Liwen Zhang, Xinyu Wang, Yong Jiang, et al. Webshaper: Agentically data synthesizing via information-seeking formalization. arXiv preprint arXiv:2507.15061, 2025. ByteDance Seed Team. Seed-oss open-source models. https://github.com/ByteDance-Seed/seed-oss, 2025a. Kimi Team, Yifan Bai, Yiping Bao, Guanduo Chen, Jiahao Chen, Ningxin Chen, Ruijue Chen, Yanru Chen, Yuankun Chen, Yutian Chen, et al. Kimi k2: Open agentic intelligence. arXiv preprint arXiv:2507.20534, 2025. Qwen Team. Qwen3 technical report, 2025b. URL https://arxiv.org/abs/2505.09388. Jialong Wu, Baixuan Li, Runnan Fang, Wenbiao Yin, Liwen Zhang, Zhengwei Tao, Dingchu Zhang, Zekun Xi, Gang Fu, Yong Jiang, et al. Webdancer: Towards autonomous information seeking agency. arXiv preprint arXiv:2505.22648, 2025a. Jialong Wu, Wenbiao Yin, Yong Jiang, Zhenglin Wang, Zekun Xi, Runnan Fang, Linhai Zhang, Yulan He, Deyu Zhou, Pengjun Xie, et al. Webwalker: Benchmarking llms in web traversal. arXiv preprint arXiv:2501.07572, 2025b. Shunyu Yao, Noah Shinn, Pedram Razavi, and Karthik Narasimhan. tau-bench: benchmark for tool-agent-user interaction in real-world domains. arXiv preprint arXiv:2406.12045, 2024. Junjie Ye, Zhengyin Du, Xuesong Yao, Weijian Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining Zhu, Zhiheng Xi, Siyu Yuan, Tao Gui, Qi Zhang, Xuanjing Huang, and Jiecao Chen. ToolHop: querydriven benchmark for evaluating large language models in multi-hop tool use. In Wanxiang Che, Joyce Nabende, Ekaterina Shutova, and Mohammad Taher Pilehvar (eds.), Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 29953021, Vienna, Austria, July 2025a. Association for Computational Linguistics. ISBN 979-8-89176-251-0. doi: 10.18653/v1/2025.acl-long.150. URL https://aclanthology.org/2025.acl-long.150/. Junjie Ye, Changhao Jiang, Zhengyin Du, Yufei Xu, Xuesong Yao, Zhiheng Xi, Xiaoran Fan, Qi Zhang, Xuanjing Huang, and Jiecao Chen. Feedback-driven tool-use improvements in large language models via automated build environments. arXiv preprint arXiv:2508.08791, 2025b. Fan Yin, Zifeng Wang, Hsu, Jun Yan, Ke Jiang, Yanfei Chen, Jindong Gu, Long Le, Kai-Wei Chang, Chen-Yu Lee, et al. Magnet: Multi-turn tool-use data synthesis and distillation via graph translation. arXiv preprint arXiv:2503.07826, 2025. Yirong Zeng, Xiao Ding, Yuxian Wang, Weiwen Liu, Wu Ning, Yutai Hou, Xu Huang, Bing Qin, and Ting Liu. Boosting tool use of large language models via iterative reinforced fine-tuning. arXiv e-prints, pp. arXiv2501, 2025. Jianguo Zhang, Tian Lan, Ming Zhu, Zuxin Liu, Thai Hoang, Shirley Kokane, Weiran Yao, Juntao Tan, Akshara Prabhakar, Haolin Chen, et al. xlam: family of large action models to empower ai agent systems. arXiv preprint arXiv:2409.03215, 2024. 13 Shaokun Zhang, Yi Dong, Jieyu Zhang, Jan Kautz, Bryan Catanzaro, Andrew Tao, Qingyun Wu, Zhiding Yu, and Guilin Liu. Nemotron-research-tool-n1: Exploring tool-using language models with reinforced reasoning. arXiv preprint arXiv:2505.00024, 2025. Wangchunshu Zhou, Yuchen Eleanor Jiang, Long Li, Jialong Wu, Tiannan Wang, Shi Qiu, Jintian Zhang, Jing Chen, Ruipu Wu, Shuai Wang, et al. Agents: An open-source framework for autonomous language agents. arXiv preprint arXiv:2309.07870, 2023."
        }
    ],
    "affiliations": [
        "Tongyi Lab, Alibaba Group"
    ]
}