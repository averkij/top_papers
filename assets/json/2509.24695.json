{
    "paper_title": "SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer",
    "authors": [
        "Junsong Chen",
        "Yuyang Zhao",
        "Jincheng Yu",
        "Ruihang Chu",
        "Junyu Chen",
        "Shuai Yang",
        "Xianbang Wang",
        "Yicheng Pan",
        "Daquan Zhou",
        "Huan Ling",
        "Haozhe Liu",
        "Hongwei Yi",
        "Hao Zhang",
        "Muyang Li",
        "Yukang Chen",
        "Han Cai",
        "Sanja Fidler",
        "Ping Luo",
        "Song Han",
        "Enze Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce SANA-Video, a small diffusion model that can efficiently generate videos up to 720x1280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at a remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing a constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at a fixed memory cost, eliminating the need for a traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16x faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating a 5-second 720p video from 71s to 29s (2.4x speedup). In summary, SANA-Video enables low-cost, high-quality video generation."
        },
        {
            "title": "Start",
            "content": "2025-9-30 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer Junsong Chen1,2*, Yuyang Zhao1*, Jincheng Yu1*, Ruihang Chu4, Junyu Chen1, Shuai Yang1 Xianbang Wang3, Yicheng Pan4, Daquan Zhou5, Huan Ling1, Haozhe Liu6, Hongwei Yi1 Hao Zhang1, Muyang Li3, Yukang Chen1, Han Cai1, Sanja Fidler1, Ping Luo2 Song Han1,3, Enze Xie 1NVIDIA 2HKU 3MIT 4THU 5PKU 6KAUST *Equal contribution. Project Page: https://nvlabs.github.io/Sana/Video Abstract: We introduce SANA-Video, small diffusion model that can efficiently generate videos up to 7201280 resolution and minute-length duration. SANA-Video synthesizes high-resolution, high-quality and long videos with strong text-video alignment at remarkably fast speed, deployable on RTX 5090 GPU. Two core designs ensure our efficient, effective and long video generation: (1) Linear DiT: We leverage linear attention as the core operation, which is more efficient than vanilla attention given the large number of tokens processed in video generation. (2) Constant-Memory KV Cache for Block Linear Attention: we design block-wise autoregressive approach for long video generation by employing constant-memory state, derived from the cumulative properties of linear attention. This KV cache provides the Linear DiT with global context at fixed memory cost, eliminating the need for traditional KV cache and enabling efficient, minute-long video generation. In addition, we explore effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs, which is only 1% of the cost of MovieGen. Given its low cost, SANA-Video achieves competitive performance compared to modern state-of-the-art small diffusion models (e.g., Wan 2.1-1.3B and SkyReel-V2-1.3B) while being 16 faster in measured latency. Moreover, SANA-Video can be deployed on RTX 5090 GPUs with NVFP4 precision, accelerating the inference speed of generating 5-second 720p video from 71s to 29s (2.4 speedup). In summary, SANA-Video enables low-cost, high-quality video generation. 5 2 0 2 9 2 ] . [ 1 5 9 6 4 2 . 9 0 5 2 : r Figure 1 An overview of generated videos and inference latency and memory of SANA-Video. The generation latency is measured under 50 denoising steps. Linear attention is more efficient for video generation and our block linear attention maintains fixed memory requirement for long videos. 2025 NVIDIA. All rights reserved. SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer 1. Introduction Video generation is currently highly active field, fueling applications that range from creative content production and digital live streaming to virtual product displays. Recent large-scale models from industry labs, such as Veo3 [1], Kling [2], Wan [3] and Seedance [4], have demonstrated remarkable performance in generating high-fidelity video content. However, this quality comes at the cost of immense computational complexity. Video generation is an exceptionally token-extensive task; for instance, producing single 5-second video at 720p resolution with model like Wan 14B [3] requires to process over 75,000 tokens, taking 32 minutes on H100 GPU. This sheer volume of data leads to prohibitive training costs and extremely slow generation speeds, rendering these powerful models impractical for widespread research and application. Even with large cost, generating long video (>10 s) is hard to realize with these large models due to the full-sequence processing operation. Recent works (e.g., MAGI-1 [5] and SkyReelv2 [6]) explores the long video generation but the efficiency is strictly constrained by the vanilla attention and KV cache. Given these challenges, pivotal question arises: Can we develop high-quality and high-resolution video generator that is computationally efficient and runs very fast on both cloud and edge devices? This paper proposes SANA-Video, small diffusion model designed for both efficient training and rapid inference without compromising output quality. In stark contrast to the massive resource requirements of contemporary models, SANA-Videos training is remarkably cost-effective, requiring only 64 NVIDIA H100 GPUs for 12 days, which represents as little as 1% of the training cost of MovieGen [7] and 10% of that of OpenSora [8]. This efficiency extends to inference, where SANA-Video can generate 5-second, 720p video in just 36 seconds on NVIDIA H100 GPU. By drastically reducing the computational barrier, SANA-Video makes high-quality video generation more accessible and practical for broader range of users and systems. The improvements mainly lie in three key components. Linear DiT. We extend SANA [9] linear DiT design to the video domain, addressing the significant computational bottleneck of traditional self-attention (ğ‘‚(ğ‘ 2)), as shown in Fig. 1(d). By replacing all attention modules with our efficient linear attention, we reduce complexity to ğ‘‚(ğ‘ ), which is crucial for high-resolution video generation and leads to 4 acceleration on 720p video. To enhance our model for video, we make two key improvements. We first integrate Rotary Position Embeddings (RoPE) [10] to improve long-context modeling. In Sec. 3.2, we detail our exploration of the optimal placement for RoPE and how we address the training instability it can introduce. Additionally, we introduce 1D temporal convolution to the Mix-FFN via shortcut connection. This design allows us to effectively leverage pre-trained image models and efficiently adapt them for video generation by aggregating temporal features. Block Linear Attention with KV Cache. The success of SANA-Video in long video generation is mainly inspired by the attribute of causal linear attention [11]. Based on our reformulation of the causal linear attention operation, we reduce the KV cache to small and fixed memory, along with fixed computational cost for each new token. This natively supports long-context operations. Based on the block linear attention module, we introduce two-stage autoregressive model continue-training paradigm, including autoregressive block training with monotonically increasing SNR sampler and the improved self-forcing specially for our long context attention operation, leading to efficient, long, and high quality video generation. Efficient Data Filter and Training. The low training cost is mainly attribute to three aspects: the powerful pre-trained text-to-image (T2I) model, efficient data filtering, and the efficient training strategy. First, SANA-Video is continue pre-trained from SANA [9, 12]-1.6B T2I model with the modification for spatio-temporal modeling (Sec. 3.2). Second, we collect data from diverse data source and design specific data filtering criterion for each data source. In addition, strong VLM [13] serves as our video captioner, producing highly detailed captions (80-100 words), including subject category, color, appearance, actions, expressions, surrounding environment, camera angles, etc. Third, with the highquality video-text pairs, we train SANA-Video in multiple stages from low resolution to high resolution and finally leverage human preferred data for SFT, ensuring the model can efficiently learn the motion and aesthetic appearance. In conclusion, our model achieves latency that is over 13 faster than the state-of-the-art Wan2.1 for 720p video generation (Fig. 1(b)), while delivering competitive results across many benchmarks. Additionally, we quantize and deploy our SANA-Video on RTX 5090 GPUs with the NVFP4 precision, where it takes just 29 seconds to generate 5s 720p video. We hope our model can be efficiently used by everyday users, providing powerful foundation model for fast video generation. SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer Figure 2 Overview of SANA-Video. Fig.(a) high-level block-wise autoregressive training pipeline based on our block causal KV cache. (Details in Sec. 3.3). Fig.(b) Our model pipeline, containing an Autoencoder, Re-writer, Linear DiT, and text encoder. Fig.(c) The detailed design of the added 3D RoPE in linear attention and the temporal convolution in our Linear DiTs Mix-FFN. 2. Preliminaries 2.1. Video Diffusion Model Following SANA [9], we use Rectified Flows (RFs) [14] with SNR sampler as the training objective in Eq. 1. Here, ğ‘ is the conditional embedding, ğœƒ is the model weights, and ğ‘¢(ğ‘¥ğ‘¡ ğ‘¡, ğ‘; ğœƒ) denotes the output velocity predicted by the diffusion model. ğ‘£(ğ‘¥) is the target velocity. In this paper, our SANA-Video is unified framework for Text-to-Image (T2I), Text-to-Video (T2V), and Image-to-Video (I2V) generation by varying condition embeddings. Specifically, for T2I and T2V, ğ‘ is the text prompt and ğ‘¥ is the image or video. For I2V, we use first frame and text prompt as condition ğ‘. By setting the noise of the first frame to zero, SANA-Video can realize I2V without any model modification. Therefore, the joint training of T2I, T2V, and I2V makes SANA-Video unified framework that can perform all tasks with single model. Eğ‘,ğ‘¡,ğ‘¥ ğ‘¢(ğ‘¥ğ‘¡ ğ‘¡, ğ‘; ğœƒ) ğ‘£(ğ‘¥) 2 . (1) 2.2. Autoregressive Long Video Generation Autoregressive diffusion models combine token/block-wise autoregressive chain-rule decomposition with denoising diffusion models, emerging as promising direction for long sequence generation like language [15] and video generation [6, 16, 17]. Specifically, for sequence of ğ‘ blocks ğ‘¥1:ğ‘ = (ğ‘¥1, ğ‘¥2, . . . , ğ‘¥ğ‘ ), the generation process is product of block distribution using the chain rule ğ‘(ğ‘¥1:ğ‘ ) = ğ‘ ğ‘–=1 ğ‘(ğ‘¥ğ‘–ğ‘¥ğ‘—<ğ‘–), with each block distribution ğ‘(ğ‘¥ğ‘–ğ‘¥ğ‘—<ğ‘–) modeled using diffusion process (Eq. 1). This approach leverages the strengths of both autoregressive models and diffusion models to capture sequential dependencies and enable block-wise, high-quality generation. 3. SANA-Video Scaling video generation to higher resolutions and longer sequences dramatically increases the number of tokens, making the ğ‘‚(ğ‘ 2) complexity of self-attention major bottleneck in computation, speed, and memory. This underscores the need for efficient linear attention in video generation. Building upon SANA Linear DiT [9], we introduce Linear Video DiT (Fig. 2(a)) for video generation by integrating two key components: Rotary Position Embeddings (RoPE) and temporal 1D convolution within the Mix-FFN. These designs keep SANAs macro architecture as well as additional temporal modeling (Fig. 2(b)), allowing us to leverage pre-trained image model and efficiently adapting it into powerful video model through continuous pre-training. In addition to the short video generation, we introduce block linear attention module for efficient long video generation. With the re-formulation of linear attention, the block linear attention module and causal Mix-FFN keep constant-memory KV cache and linear computational cost for long video. Based on this KV cache, we design two stage post-training paradigm to unlock the infinite length generation ability, leading to high-quality and efficient long video generation model. 3 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer 3.1. Training strategy Stage1: VAE Adaptation on Text-to-Image (T2I). Training video DiT models from scratch is resource-intensive due to the mismatch between image and video VAEs. We address this by first efficiently adapting existing T2I models to new video VAEs. Specifically, We leverage different video VAEs in generating videos of different resolution. For 480P videos, VAEs with high-compression ratio limits the overall performance, and thus we adopt Wan-VAE [3]. For 720P high-resolution video, we introduce our video VAE, DCAE-V, which provides higher compression ratio for more efficient generation (details in Sec. 3.4). The adaptation of both VAEs is highly efficient, converging within 5-10k training steps, further demonstrating the strong generalization ability of our Linear DiT. Stage2: Continue Pre-Training from T2I Model. Initializing video Linear DiT from pre-trained T2I model [9] is an efficient and effective way to leverage the well-learned visual and textual semantic knowledge. Therefore, we initialize our SANA-Video with model adapted from the first stage and introduce additional designs to model long-context and motion information (Sec. 3.2). The additional temporal designs are tailor-made for linear attention, improving the locality of attention operation. The newly added layers are zero-initialized with skip connection, which minimizes their influence on the pre-trained weights during early training. After this identity initialization, SANA-Video is trained in coarse-to-fine manner. It first trains on low-resolution, short videos (e.g., 192P 2.5 seconds) before moving to higher resolution, longer videos (e.g., 480P 5 seconds) with different data filtering criteria (Appendix D). This coarse-to-fine approach efficiently encourages SANA-Video to fast learn dynamic information with abundant data and then refine details using less, but higher-quality, data. Stage3: Autoregressive Block Training. The continued pre-training makes SANA-Video an efficient small diffusion model, primarily for high-resolution 5-second video generation. To enable the generation of much longer videos, we analyze the attributes of linear attention in Sec. 3.3 and propose constant-memory block KV cache for autoregressive generation. Building on this design, we conduct autoregressive block training in two steps: we first train the autoregressive module and then address exposure bias with our improved self-forcing block training (Sec. 3.3.2). This process results in high-quality, efficient model for long video generation. 3.2. Efficient Linear DiT Pre-Training SANA-Video adopts the SANA [9] as the base architecture and innovatively tailors the Linear Diffusion Transformer blocks to handle the unique challenges of T2V tasks, as depicted in Fig. 2. Several dedicated designs are proposed as follows: Figure 3 Analysis of Linear Attention with RoPE. (a) Visual comparison of attention maps. First two plots compare vanilla softmax attention (Wan) to our linear attention without positional encoding. The latter two plots show our methods effect: applying RoPE after the ReLU kernel results in sparser, more localized attention pattern. (b) Training loss for the QK sum (Eq. 2 denominator). Removing RoPE from the denominator (green line) ensures training stability, as discussed in Sec. 3.2. Linear Attention in Video DiT. Our work extends the SANA [9] architecture by integrating Rotary Position Embeddings (RoPE) [10] into its efficient ReLU (ğœ‘) linear attention blocks. This integration is crucial for enhancing the models ability to handle the sequential and spatial relationships in high-quality video generation. The core of our design lies in applying RoPE after the ReLU activation, specifically as RoPE(ReLU(ğ‘¥)), as shown in Fig. 2. This order is critical because it prevents the ReLU kernel from filtering out the positional information encoded by RoPE. As Fig. 3 shows, this design results in attention maps with clear focus on local regions, which is essential for capturing fine-grained video details. However, applying RoPE directly to queries and keys (as in vanilla attention) can make the linear attention mechanism numerically unstable [18] due to the difference between softmax and ReLU similarity functions. The RoPE transformation can change the non-negative nature of the ReLU output, potentially causing the denominator in the SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer standard linear attention formula (Eq. 2) to become zero. To solve this, we modify the calculation: while the numerator includes RoPE on the queries and keys, we remove RoPE from either the key or the query in the denominator. This ensures the denominator remains positive, guaranteeing training stability (Fig. 3 (b)) while still benefiting from positional encoding. ğ‘‚ğ‘– = RoPE(ğœ‘(ğ‘„ğ‘–))(ğ‘ RoPE(ğœ‘(ğ‘„ğ‘–))(ğ‘ ğ‘—=1 ğ‘—= ) RoPE(ğœ‘(ğ¾ğ‘— ))ğ‘‡ ğ‘‰ğ‘— RoPE(ğœ‘(ğ¾ğ‘— ))ğ‘‡ ) = RoPE(ğœ‘(ğ‘„ğ‘–))(ğ‘ ğœ‘(ğ‘„ğ‘–)(ğ‘ ğ‘—=1 ğ‘—=1 RoPE(ğœ‘(ğ¾ğ‘— ))ğ‘‡ ğ‘‰ğ‘— ğœ‘(ğ¾ğ‘— )ğ‘‡ ) ) , (2) where ğ‘‚ğ‘–, ğ‘„ğ‘–, ğ¾ğ‘– and ğ‘‰ğ‘– denote the output, query, key and value of the ğ‘–th token. Mix-FFN with Spatial-Temporal Mixture. As shown in Fig. 3, we compare the linear attention map in SANA-Video with the softmax attention map in Wan2.1 [3]. We observe that linear attention is much denser and less focused on local details compared to softmax attention. SANA [9] ameliorates the locality problem in image generation with the convolution in Mix-FFN. Building upon the Mix-FFN, we enhance it with temporal 1D convolution. The temporal convolution with shortcut connection is appended to the end of the block (Fig. 2(b)), enabling seamless temporal feature aggregation while preserving initialization. The module helps capture local relationships along the temporal axis, resulting in better motion continuity and consistency in generated videos. As evidenced in our ablation study (Fig. 6(a)), this addition leads to significantly lower training loss and improved motion performance. 3.3. Block Linear Attention This section outlines key components enabling efficient long-video generation. Inspired by the inherent attribute of causal linear attention [11], we explore the constant-memory global KV cache in our block linear attention module, which supports long-context attention with small, fixed GPU memory. Based on this module, we introduce two-stage autoregressive model continue training paradigm: autoregressive block training with monotonically increasing SNR sampler and an improved self-forcing method for our long-context attention. 3.3.1. Block Linear Attention with KV Cache Table 1 For sequence with ğ‘ tokens R1ğ·, memory and compute costs are compared among three attention types. Causal linear attention shows best efficiency while maintains global memory. Metric Causal Full Attention Causal Local Attention Causal Linear Attention Memory Comp. Cost (ğ‘ -th token) Comp. Cost (ğ‘ tokens) ğ‘‚(ğ‘ ğ·) ğ‘‚(ğ‘ ğ·) ğ‘‚(ğ‘ 2 ğ·) ğ‘‚(ğ‘Š ğ·) ğ‘‚(ğ‘Š ğ·) ğ‘‚(ğ‘ ğ‘Š ğ·) ğ‘‚(ğ·2) ğ‘‚(ğ·2) ğ‘‚(ğ‘ ğ·2) Limitation of Causal Vanilla Attention. In view of the training objective (Eq. 1), block-wise causal attention is required to implement autoregressive generation. Recent works [5, 6, 17] use combination of full attention within block and causal attention to previous blocks. To reduce computational costs, they leverage KV cache, which is effective but comes with memory overhead. For each new token R1ğ· with ğ‘ cached conditional tokens, it requires ğ‘‚(ğ‘ ğ·) memory to store the cache and ğ‘‚(ğ‘ ğ·) FLOPs for the attention computation. However, since the computational and memory costs grow linearly, these methods [5, 6, 17] often restrict the attention window to local scope during long video generation. While this maintains stable cost, it comes at the expense of losing global-context information. KV Cache in Block Linear Attention. In contrast to the dramatically increased computational and memory cost in causal vanilla attention, linear attention [11] has significant efficiency advantage, naturally supporting long video generation with global attention while maintaining constant memory. Consider the causal attention setting, linear attention (Eq. 2) output for the ğ‘–th token can be re-formulated as: (ğ‘– ğœ‘(ğ‘„ğ‘–) ğ‘‚ğ‘– = ğ‘—=1 ğœ‘(ğ¾ğ‘—)ğ‘‡ ğ‘‰ğ‘— ğœ‘(ğ‘„ğ‘–) ğ‘—=1 ğœ‘(ğ¾ğ‘—)ğ‘‡ ğ‘‰ğ‘— + ğœ‘(ğ¾ğ‘–)ğ‘‡ ğ‘‰ğ‘– (ğ‘– ğœ‘(ğ‘„ğ‘–) (ğ‘– ğ‘—=1 ğœ‘(ğ¾ğ‘—)ğ‘‡ ğœ‘(ğ‘„ğ‘–) (ğ‘–1 ğ‘—=1 ğœ‘(ğ¾ğ‘—)ğ‘‡ + ğœ‘(ğ¾ğ‘–)ğ‘‡ ) ) = ) ) = (ğ‘–1 ğ‘—=1 ğ‘†ğ‘— + ğ‘†ğ‘– ) ğœ‘(ğ‘„ğ‘–) (ğ‘–1 ğœ‘(ğ‘„ğ‘–) ğ‘—=1 ğœ‘(ğ¾ğ‘—)ğ‘‡ + ğœ‘(ğ¾ğ‘–)ğ‘‡ ) , (3) where ğ‘†ğ‘— = ğœ‘(ğ¾ğ‘—)ğ‘‡ ğ‘‰ğ‘— denotes the attention state for the ğ‘—th token. We omit RoPE here for simplicity. Obviously, as long as the cumulative sum of state ğ‘–1 ğ‘—=1 ğ‘†ğ‘— and the cumulative sum of keys ğ‘–1 ğ‘—=1 ğœ‘(ğ¾ğ‘—)ğ‘‡ are stored, only the attention state for the ğ‘–th token ğ‘†ğ‘– Rğ·ğ· is required to compute. Therefore, the memory cost is only ğ‘–1 ğ‘—=1 ğ‘†ğ‘— Rğ·ğ· and ğ‘–1 ğ‘—=1 ğœ‘(ğ¾ğ‘—)ğ‘‡ Rğ·1, taking ğ‘‚(ğ·2) in total, and the computational cost is only ğ‘‚(ğ·2). In Table 1 and Fig. 4(a), we compare the memory and computational cost among causal full attention, causal local attention and our causal 5 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer Figure 4 Overview of Block Linear Attention. (a) We compare the attention compute mechanism among vanilla attention, linear attention and causal linear attention. (b) The illustration of block causal Mix-FFN in processing the adjacent blocks. linear attention. Since ğ‘ > ğ‘Š >> ğ·, causal linear attention achieves the best efficiency and can still maintain global memory in long video generation. Block Causal Mix-FFN. In addition to linear attention, In addition to linear attention, our designed temporal-spatial Mix-FFN with convolution layers improves locality and must also be causal for long video generation. During inference, the causal temporal convolution layer with kernel size of 3 requires access to the last frame of the previous block. To this end, the last frame of previous block Rğ»ğ‘Š ğ· is also cached (Fig. 4(b)). Therefore, our causal linear DiT module keeps fixed memory cache, containing cumulative sum of attention states and keys from all previous frames, along with the last frame of the previous block. 3.3.2. Autoregressive Block Training The continue training of the autoregressive SANA-Video variant begins with the pre-trained 5s SANA-Video model. To align with the pre-trained models distribution, we propose monotonically increasing SNR sampler. Specifically, we randomly select block and sample timestep for it with the SNR sampler [14]. Then the timesteps for the remaining blocks are sampled via propagated probability [19], ensuring all the timesteps are monotonically increasing, i.e., later blocks have larger timestep than early blocks. This proposed timestep sampler offers two key advantages. First, the monotonically increasing timesteps have much smaller sampling space than random timesteps, which results in faster convergence and better performance. Second, applying the SNR sampler to randomly selected block guarantees that every block is trained with sufficient information. However, monotonically increasing SNR sampler cannot address severe problem in autoregressive generation, i.e., exposure bias, where condition blocks are ground truth during training but are generated content during inference, leading to error accumulation and limiting performance in long video generation. Self-Forcing [17] aims to address this issue in vanilla attention DiT model with autoregressive rollout. In our work, we improve Self-Forcing to better leverage our constant-memory global KV cache. Limited by the increasing VRAM requirement of causal vanilla attention (Fig. 1(c) and Table 1), Self-Forcing uses local attention within designed window size. Consequently, it sets the length of self-generated content to be the same as the pre-trained model (i.e., 5s). In contrast, the block linear attention in SANA-Video supports long-context global KV cache with small and constant GPU memory. This allows us to self-generate much longer video (e.g., 1 min) and select clip for training, which better aligns the conditioning signals between training and inference. 3.3.3. Block Linear Attention during Inference 0 ğ‘†, cumulative sum of keys 0 We follows Self-Forcing [17] for autoregressive inference, with the KV cache update based on our design (Alg. 1). Specifically, we first initialize KV cache as empty and start to denoise the first block. After it is fully denoised, the attention state 0 0 ğœ‘(ğ¾)ğ‘‡ and cache for convolustion in Spatial-Temporal Mix-FFN (conv cache ğ‘“ ) will be stored. For the remaining blocks (e.g., ğ‘›-th block), they will use the existing KV cache to denoise the latent until clean and then update the cumulative attention state ğ‘› 0 ğœ‘(ğ¾)ğ‘‡ . Also, conv cache ğ‘“ will be replaced with the new cache. Such update leverages the global while keeping the memory constant and small, making the long video generation efficient and effective. 0 ğ‘† and cumulative sum of keys ğ‘› 6 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer Algorithm 1 Block Linear Diffusion Inference with Linear KV Cache Require: KV cache Require: Denoise timesteps {ğ‘¡1, . . . , ğ‘¡ğ‘‡ }, noise scheduler Î¨ Require: Number of blocks ğ‘€ Require: Block-wise diffusion model ğºğœƒ (ğºKV ğœƒ returns cumulative sum of state ğ‘†, cumulative sum of key ğœ‘(ğ¾)ğ‘‡ and conv cache ğ‘“ ) ğ‘¡ğ‘— ; ğ‘¡ğ‘—, KV) ğ‘¡ğ‘‡ ğ’© (0, ğ¼) Set ^ğ‘¥ğ‘– if ğ‘— = 1 then 1: Initialize model output Xğœƒ [] 2: Initialize KV cache KV [None, None, None] 3: for ğ‘– = 1, . . . , ğ‘€ do Initialize ğ‘¥ğ‘– 4: for ğ‘— = ğ‘‡, . . . , 1 do 5: 0 ğºğœƒ(ğ‘¥ğ‘– 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: end for 16: return Xğœƒ Xğœƒ.append(^ğ‘¥ğ‘– 0) Update Cache KV ğºKV Sample ğœ– ğ’© (0, ğ¼) ğ‘¡ğ‘—1 Î¨(^ğ‘¥ğ‘– Set ğ‘¥ğ‘– 0, ğœ–, ğ‘¡ğ‘—1) end for ğœƒ (^ğ‘¥ğ‘– end if else 0; 0, KV) 3.4. Deep Compression Video Autoencoder SANA-Video achieves high efficiency and quality for 480P video generation using Wan-VAE. However, even with our efficient linear attention, the generation speed for 720P videos is 2.3 slower. This efficiency drop is even more severe for full attention DiT models (4 for Wan 2.1 1.3B), inspiring us to explore more efficient VAE that can compress more tokens. We fine-tune DCAE [20] into DCAE-V, with spatial down-sampling factor of ğ¹ = 32, temporal factor of ğ‘‡ = 4, and channels ğ¶ = 32. The number of latent channels aligns with our pre-trained T2I model, enabling fast adaptation from an image to video model in the same latent space. The concurrent Wan2.2-5B model also achieves 32 times spatial compression, by combining VAE with spatial down-sampling factor of 16 and patch embedding compression of 2. The advantages of DCAE-V over Wan2.2-VAE are twofold. First, DCAE-Vs 32 latent channels align with our pre-trained T2I model, which improves convergence speed. Second, to achieve the same compression ratio, Wan2.2-VAE would require the model to predict much larger latent dimension (192 vs. 32 in DCAE-V), task that is difficult for small diffusion model (Details in Appendix C.1). As shown in Table 3, DCAE-V exhibits reconstruction performance comparable to other state-of-the-art VAEs like Wan2.1 [3], Wan2.2 [3], and LTX-Video [21]. This high compression allows our model to achieve performance on par with much larger models (e.g., Wan2.1-14B and Wan2.2-5B) while demonstrating significant acceleration, as shown in Table 2. Specifically, SANA-Video can generate 720P 5s video within just 36 seconds, which is 53 acceleration over Wan2.1-14B. When compared to Wan2.2-5B, which shares the same compression ratio as ours, SANA-Video achieves 3.2 acceleration. Table 2 Latency on H100 GPU and VBench evaluation on 720 1280 81 resolution videos. Table 3 Reconstruction capability of different Autoencoders on Panda-70M 192p resolution. Models Latency(s) Total Quality Semantic Autoencoder Ratio PSNR SSIM LPIPS Wan-2.1-14B Wan-2.1-1.3B Wan-2.2-5B SANA-Video-2B 1897 400 116 36 83.73 83.38 83.28 84. 85.77 85.67 85.03 84.63 75.58 74.22 76.28 81.73 F8T4C16 (Wan2.1-VAE) F16T4C48 (Wan2.2-VAE) F32T8C128 (LTX-VAE) 16 21 F32T4C32 (Our DCAE-V) 128 34.41 35.61 32.26 33.25 0.95 0.96 0.93 0. 0.01 0.01 0.04 0.03 3.5. Data Filtering Pipeline To curate our training dataset, we collect public real and synthetic data and implement multi-stage filtering paradigm. First, we use PySceneDetect [22] and FFMPEG to cut raw videos into single-scene short clips. For each video clip, we analyze its aesthetic and motion quality, as well as providing detailed captions. Specifically, the motion quality is measured by Unimatch [23] (optical flow) and VMAF [24] (pixel difference), and only clips with moderate and 7 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer Table 4 Comprehensive comparison of our method with SOTA approaches in efficiency and performance on VBench. The speed is tested on one H100 GPU with BF16 Precision. Latency: Measured with batch size of 1, on 48083281 video, using the models default inference steps for fair comparison. We highlight the best, second best, and third best entries. Methods Text-to-Video MAGI-1 [5] Step-Video [27] CogVideoX1.5 [28] SkyReels-V2 [6] Open-Sora-2.0 [24] Wan2.1-14B [3] Wan2.1-1.3B [3] SANA-Video Image-to-Video MAGI-1 [5] Step-Video-TI2V [27] CogVideoX-5b-I2V [28] HunyuanVideo-I2V [29] Wan2.1-14B [3] SANA-Video Latency (s) Speedup #Params (B) Evaluation scores Total Quality Semantic / I2V 435 246 111 132 465 484 103 60 435 246 111 210 493 60 1.1 2.0 4.4 3.7 1.0 1.0 4.7 8.0 1.1 2.0 4.4 2.3 1.0 8.2 4.5B 30B 5B 1.3B 14B 14B 1.3B 2B 4.5B 30B 5B 13B 14B 2B 79.18 81.83 82.17 82.67 84.34 83.69 83.31 83.71 89.28 88.36 86.70 86.82 86.86 88.02 82.04 84.46 82.78 84.70 85.4 85.59 85.23 84. 82.44 81.22 78.61 78.54 80.82 79.65 67.74 71.28 79.76 74.53 80.12 76.11 75.65 81.35 96.12 95.50 94.79 95.10 92.90 96. clear motion are kept. Furthermore, the average optical flow is used as representation of motion magnitude, injecting into prompt for better motion controllability. Aesthetic quality is measured by pre-trained video aesthetic model (DOVER [25]) and key frame saturation obtained with OpenCV [26], where low aesthetic score and over-saturated videos are removed. Finally, we collect approximately 5,000 human preferred high-quality videos based on stringent motion and aesthetic criteria. The SFT data is collected with diverse but balanced motion and style categories, which can further improve the overall performance. More details are in Appendix D. Figure 5 Data filtering paradigm of SANA-Video. 4. Experiments 4.1. Implementation Details. Pipeline Settings. For DiT model, to best utilize the pre-trained text-to-image model SANA [9], our SANA-Video-2B is almost identical to those of the original SANA [9], including the diffusion transformer model and small decoder-only text encoder. For 480P videos, we leverage Wan2.1-VAE [3] autoencoder. For 720P high-resolution video generation, we fine-tune the DCAE [20] into video deep compression autoencoder (DCAE-V) to facilitate more efficient training and inference. Our final model is trained on 64 H100 GPUs for approximately 12 days. Details are in Appendix B.1. SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer 4.2. Performance Comparison and Analysis The comprehensive efficiency and performance comparison among SANA-Video with state-of-the-art is illustrated in Table 4. We adopt VBench [30] as the performance evaluation metric and the generation latency of 480P 81-frame video as efficiency metric. As shown in Table 4, SANA-Video exhibits remarkable latency of 60 seconds, marking it the fastest model compared. This translates to throughput that is 7.2 faster than MAGI-1 and over 4 faster than Step-Video. In terms of comparison, SANA-Video achieves Total Score of 83.71 on text-to-video generation, comparable with large model Open-Sora-2.0 (14B) and outperforming Wan2.1 (1.3B). In addition, SANA-Video achieves 88.02 Total Score on image-to-video generation, outperformance large DiT models Wan2.1 (14B) and HunyuanVideo-I2V (11B). Furthermore, SANA-Video achieves the best semantic / I2V score across all the methods, demonstrating strong vision-text semantic alignment. 4.3. Ablation Studies We then conduct ablation studies on the crucial architectural modifications discussed in Sec. 3.2. As shown in Fig. 6, we provide training loss curves and latency profiles on H100 GPUs. Figure 6 SANA-Video configuration ablation studies. (a) Training loss curves with and without 3D RoPE. (b) Training loss curves with and without temporal 2D Convolution. (c) Latency comparison of SANA-Video between linear and full attention. (d) Comparison of monotonically increasing versus random timestep sampling in autoregressive block training. Note that monotonically increasing sampling improves consistency across blocks. Linear Attention Module. We incorporate three key designs to enhance our linear attention model. First, we integrate 3D RoPE to focus linear attention on local features  (Fig. 3)  . This improves performance, as evidenced by significantly lower training loss (Fig. 6(a)). Second, to address differences between linear and vanilla attention, we introduce Spatial-Temporal Mix-FFN module. Its training loss curve (Fig. 6(b)) demonstrates that 1D temporal convolution layer significantly enhances performance. Finally, our linear attention design provides significant efficiency advantage. As Fig. 6(c) shows, our models latency becomes lower at higher resolutions, achieving 2 speedup at 480P and 4 at 720P, proving its superior efficiency for high-resolution video generation. Monotonically Increasing SNR Sampler. We compare the proposed monotonically increasing SNR sampler with random timestep sampling in the autoregressive block training. As shown in Fig. 6(d) (two columns are from different blocks), monotonically increasing SNR sampler achieves better quality and more consistency across blocks. Long Video Generation. We compare SANA-Video with previous autoregressive video generation methods on VBench, as shown in Table 5. SANA-Video achieves comparable performance with Self-Forcing [17] while outperforming SkyReel-V2 [6] and CausVid [16]. Table 5 Comparison of autoregressive video generation methods on VBench. Model Total Score Quality Score Semantic Score CausVid SkyReels-V2 Self-Forcing SANA-Video 81.20 82.67 84.31 83.70 84.05 84.70 85. 84.43 69.80 74.53 81.28 80.78 Figure 7 Latency comparison of our model on BF16 and NVFP4 precision. 9 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer 5. Applications and Deployment As pre-training model, SANA-Video can be easily extended to multiple applications of video generation. First, we adapt SANA-Video to several world model applications (Fig. 1 and Appendix E): embodied AI, autonomous driving and game generation. (Details are in Appendix E). Second, we quantize our model to NVFP4 for efficient inference. On-Device Deployment with 4-Bit Quantization. To facilitate efficient edge deployment, we quantize SANA-Video from BF16 to NVFP4 format using SVDQuant [31]. To balance efficiency and fidelity, we selectively quantize the following layers: the QKV and output projections in self-attention, the query and output projections in cross-attention, and the 1x1 convolutions in feed-forward layers. Other components (normalization layers, temporal convolutions, and KV projections in cross-attention) are kept at higher precision to preserve semantic quality and prevent compounding errors. As shown in Fig. 7, this strategy reduces the end-to-end generation time for 720p 5-second video from 71 to 29 on single RTX 5090 GPU, achieving 2.4 latency speedup while maintaining quality indistinguishable from the BF16 baseline. 6. Related Work 6.1. Video Diffusion Model Video generation has become rapidly growing focus in generative AI. Modern approaches typically use VAE to compress videos into latent space, where diffusion modelconditioned on text, images, or bothlearns to generate content. Early studies, such as Make-A-Video [32], PYoCo [33] and Tune-A-Video [34], adapted text-to-image models with additional temporal layers to enable video generation. Works like, MagicVideo [35], SVD [36] and Latent Video Diffusion [37] played pioneering roles in scaling latent diffusion approaches. However, the limited compression rate of VAEs has hindered their ability to generalize to long video sequences. major breakthrough came with Sora [38], which introduced temporal VAE to compress temporal dimensions alongside spatial ones, while adopting transformer-based backbone [39] at scale. Recent efforts have pushed this framework further. For instance, Wan 2.2 [3] incorporated sparse MoE architecture that routes different diffusion steps to specialized experts, while VEO3 [1] extended the paradigm by integrating audio, achieving state-of-the-art performance. The success of MovieGen [7], Seaweed [40], Goku [41], and Waiver [42] further demonstrates the potential of video generation and its broad impact on practical applications. These developments underscore video generation as one of the most dynamic and competitive frontiers in generative AI community. 6.2. Autoregressive Diffusion Model Autoregressive generation dominates the text domain, while diffusion models have become the standard for visual generation. Recent research explores how to combine these paradigms to duplicate the long-term planning capacity of large language models in vision generation. straightforward solution [43] is to jointly train an autoregressive (AR) model for text and diffusion model for vision, but this leaves the visual side reliant solely on diffusion without benefiting from AR modeling. Inspired by block diffusion [15], several works [44, 45, 46, 47, 48] explore ARdiffusion hybrids: MAR [44] disentangles the two, letting AR predict conditions and diffusion reconstruct tokens; ACDiT [45] integrates them via block-wise diffusion with autoregression across blocks, while CausalDiffusion [46] extends this to token-level autoregression. Extending these ideas to video is natural since frames form temporal chunks. FAR [49] generates each frame autoregressively; MarDini [50] employs an AR planner to provide frame-level conditions, with diffusion recovering pixels for tasks such as video interpolation, video extension, and image-to-video generation. Beyond this, MAGI [5] and Skyreel [6] remove the dual-model design, training under the strategy of diffusion forcing [51], where later frames are assigned higher noise levels, thereby enabling infinite autoregressive inter-chunk prediction and high-quality inner-chunk diffusion generation. More recently, self-forcing [17] highlights gap between training (real data diffused with noise) and inference (model-generated conditions), and proposes rollout-based training to align the two, leading to more robust long-term prediction. 6.3. Efficient Attention for Multimodal Generation. Diffusion Transformers (DiT) have emerged as the mainstream architecture for visual content generation. Representative models include PixArt-Î± [52], Stable Diffusion 3 (SD3) [14], and Flux [53], the latter demonstrating the potential of scaling DiT to 12B parameters for high-resolution image synthesis.. To address the computational challenges of vanilla 10 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer attention (ğ‘‚(ğ‘›2))), various methods have replaced it with linear-complexity mechanisms. For instance, DiG [54] uses gated linear attention, PixArt-Î£ [55] designs key-value token compression, while LinFusion [56] involves Mamba-based structure and SANA [9] employs ReLU linear attention approaches to reduce computational overhead. With the rise of video generation, the computational demands of standard quadratic attention have become major bottleneck. To address the high computational cost of 3D video attention, many existing works employ factorized spatial and temporal attention to reduce complexity [32, 57, 58, 59]. Other methods reduce attention complexity by selectively skipping certain token interactions [60, 61, 62, 63, 64, 65]. Simultaneously, other models, such as Mamba-based architectures [66, 67], have explored state-space models and linear-complexity designs for efficient video generation. However, these methods either retain some quadratic complexity due to global self-attention layers or are limited to local attention. In contrast, our model maintains constant-memory KV cache with global attention mechanism, enabling the generation of high-quality, minute-length videos. 7. Conclusion In this paper, we introduce SANA-Video, small diffusion model that can efficiently generate high resolution, highquality and long videos at remarkably fast speed and low hardward requirement. The significance of SANA-Video lies in the following improvements: linear attention as the core operation, leading to remarkable efficiency improvement in token-extensive video generation task; block linear attention with costant-memory KV cache, supporting minute-long video generation with fixed memory cost; effective data filters and model training strategies, narrowing the training cost to 12 days on 64 H100 GPUs. With such small cost, SANA-Video showcases 16 faster speed but competitive performance with modern state-of-the-art small diffusion models. Acknowledgements. We would like to express our heartfelt gratitude to Shuchen Xue from UCAS, Haocheng Xi from UCB, Songlin Yang, Xingyang Li and Wenkun He from MIT for their invaluable insightful discussions on efficient attention designs, as well as Tian Ye from HKUST(GZ) for his expertise on data curation. Their collaborative efforts and constructive discussions have been instrumental in shaping this work. 11 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer A. LLM Usage Our use of large language models (LLMs) was limited to editorial assistance to improve the clarity and readability of this manuscript. Specifically, these tools were used to refine grammar and phrasing, enhance the logical flow between sections, and condense overly verbose passages for conciseness. Crucially, all original research ideas, experimental designs, and data analyses were conceived and executed by the authors; the LLM did not contribute to any scientific or methodological content. B. More Implementation Details B.1. Pipeline Configuration As detailed in Table 6, our SANA-Video-2B model supersedes the original SANA [9] architecture, including the diffusion transformer and small decoder-only text encoder, to best utilize the pre-trained text-to-image models weights. However, we introduce several key modifications to support video generation. We increase the FFN dimension from 5600 to 6720 and the head dimension from 32 to 112 to accommodate 3D RoPE, and we add temporal convolution in the Mix-FFN module to enhance motion performance. To effectively capture latent features from both images and videos, our approach uses different VAEs based on resolution. For 480P videos, we leverage Wan2.1-VAE [3] to prioritize reconstruction quality with lower compression rate (F8T4C16). In contrast, for high-resolution 720P videos, we fine-tune the DCAE [20] into more aggressive deep compression autoencoder, DCAE-V (F32T4C32), to facilitate more efficient training and inference. For conditional feature extraction, we follow SANA by using small decoder-only LLM for efficient text processing. For our training strategy, we also employ multi-aspect augmentation to enable arbitrary aspect ratio generation and facilitate image-video joint training, allowing the model to generate both images and videos from single architecture. The AdamW optimizer [68] is utilized with weight decay of 0.03 and constant learning rate of 5e-5. We use Accelerate FSDP [69] for efficient sharded data parallel training. Our final model is trained on 64 H100 GPUs for approximately 12 days. Table 6 Architecture details of the proposed SANA-Video. Model Width Depth FFN #Heads #Param (M) SANA-Video-2B 20 6720 20 2056 C. More Results Please refer to our project link (https://nvlabs.github.io/Sana/Video/), for the qualitative comparison and our generation results. C.1. VAE Comparison In Sec. 3.4, we analyze the differences and performance of various video VAEs. To select the VAE that best suits our small diffusion model, we conducted generalization experiment. We hypothesize that VAE with better reconstruction ability under perturbation will be better fit, as the diffusion models output during inference may be slightly different from the clean latent distribution seen during VAE training. Specifically, we add Gaussian noise to the encoded latent before decoding it, setting ğ‘¥ ğ‘¡ = ğ‘¥ğ‘¡ + ğœ–ğ‘§, where ğ‘§ ğ’© (0, ğ¼). As the results in Table 7 show, our DCAE-V performs much more robustly under different noise levels. This demonstrates its superior reconstruction generalization, making it the ideal choice for our small diffusion model. C.2. Qualitative Comparison Text-to-Video Generation. We compare the text-to-video generation results with current state-of-the-art small diffusion models Wan2.1-1.3B [3] and Wan2.2-5B [3]. As shown in Fig. 8, SANA-Video has comparable semantic understanding, great motion control, and high aesthetic quality. Image-to-Video Generation. We compare the image-to-video generation results with small diffusion models LTXVideo [21] (2B) and SkyReelv2-I2V [6] (1.3B). As shown in Fig. 9, SANA-Video has the best semantic understanding SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer Table 7 Performance comparison of different VAE models on 1000 samples from Panda-70M with different noise perturbation levels. Model latent shape psnr ssim lpips Wan2.1VAE (ğœ– = 0) Wan2.2VAE (ğœ– = 0) DCAE-V (ğœ– = 0) 16, T/4, H/8, W/8 48, T/4, H/16, W/16 32, T/4, H/32, W/32 Wan2.1VAE (ğœ– = 0.1) Wan2.2VAE (ğœ– = 0.1) DCAE-V (ğœ– = 0.1) 16, T/4, H/8, W/8 48, T/4, H/16, W/16 32, T/4, H/32, W/32 Wan2.1VAE (ğœ– = 0.2) Wan2.2VAE (ğœ– = 0.2) DCAE-V (ğœ– = 0.2) 16, T/4, H/8, W/8 48, T/4, H/16, W/16 32, T/4, H/32, W/32 34.41 35.61 33.25 28.61 30.12 31.91 24.25 25.94 29.34 0.95 0.96 0.94 0.89 0.92 0. 0.78 0.84 0.90 0.01 0.01 0.03 0.06 0.04 0.04 0.16 0.10 0.05 ability (camera remains steady instruction in the first case) as well as the best motion control (slow-motion effect instruction in the second case) and moderate motion magnitude (first case). C.3. More I2V Results Our SANA-Video is unified framework that can perform T2I, T2V and I2V with single model. We visualize the I2V generation results in Fig. 10. The first column is the reference image and the remaining columns are the generated video. Our SANA-Video can generate semantic consistent and temporal smooth videos based on the first frame. C.4. Influence of Motion Score As mentioned in our data pipeline (Sec. D), we use the average optical flow value to represent the motion magitude, which is called motion score in our paper. The motion score is added to the text prompt to better control the motion. In Fig. 11, we compare the impact of motion score in the I2V task, which is more clear with the same reference image. By increasing the motion, SANA-Video can generate videos with larger but still consistent motion. C.5. Long Video Visualization In Fig. 12, we provide an example of our 30-second long video generation. SANA-Video is able to generate motion consistent and semantically aligned long videos. D. Data Processing Pipeline To curate our training dataset, we collect mix of public real and synthetic data, which is then refined through multistage filtering paradigm, as shown in Fig. 5. We use PySceneDetect [22] to cut raw videos into single-scene, 5-second clips, and Qwen-2.5-VL-7B [13] rewrites prompts to ensure prompt-clip alignment. The data is further filtered based on multiple criteria, including motion, aesthetics, and saturation. We use Unimatch [23] and VMAF [24] for motion, DOVER [25] for aesthetics, and OpenCV [26] for saturation. Finally, for the SFT stage, subset of approximately 5,000 high-quality videos is selected based on stringent motion and aesthetic criteria and then classified to ensure balanced and diverse dataset. The details of this data curation process are discussed as follows. Scene Detection and Shot Cut. In the pre-training stage, we focus on generating 5-second short videos with 16 FPS on specific scene. However, the raw videos are commonly long and contains more than one scene. Therefore, we cut the raw videos to small video shots with two steps: PySceneDet [22] to split the scenes and FFmpeg [70] to split videos into short clips. Motion Filtering. Our pre-training dataset comes from multiple sources, and each source of data differs not only in style but also in motion. Motion that is too fast or too slow degrades the motion performance of SANA-Video. Following Open-Sora [8], we apply Unimatch [23] and Vmaf to score the motion of each video. Unimatch can evaluate the optical flow of two given images of the same shape. We select frames from each video every 0.5 seconds, reshape them into 320x576, and calculate the average optical flow over all selected frames. Vmaf, on the other hand, simply computes the 13 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer Figure 8 Qualitative comparison among T2V methods. SANA-Video has comparable motion control and video-text semantic alignment with state-of-the-art small diffusion models. pixel difference of two images; we use FFmpeg [70] to compute the Vmaf over all consecutive frames and normalize them. Due to the variance of different video sources, we analyze the motion scale and set the appropriate motion range individually, ensuring our data has moderate and clear motion. During pre-training, we also append Motion score: {unimatch value} to the text prompt to help control the motion magnitude of the generated videos  (Fig. 11)  . Aesthetics Many Text-to-Image works have proven that high aesthetic data can improve the training efficiency of an image generation model [52]. We believe that this also applies to the video generation model. We use Dover [25] to score each video for its aesthetics. Dover produces three different scores: aesthetic score, technical score, and overall score, among which we use the overall score as the filter metric. Saturation We also observe that some of our data, especially synthetic data and real data converted from HDR to SDR, has unnatural color, appearing in high saturation. To prevent these data from damaging the output quality of SANA-Video, we use OpenCV [26] to compute saturation score of each video. We select frames from each video every 0.5 seconds, convert their color representation from RGB to HSV, where the channel in HSV color representation stands for saturation. By averaging the channel over all pixels and frames, we obtain the saturation score of video. We keep only videos with saturation score lower than threshold set to reasonable value for each data source. 14 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer Figure 9 Qualitative comparison among I2V methods. SANA-Video has better motion control and video-text semantic alignment. Captioning [3] shows that LLM rewritten prompts can produce more accurate and detailed prompts within the same distribution, and thus make the model easier to learn, and consequently enhance the models performance. Moreover, for synthetic data with existing prompts, replacing their prompts with LLM rewritten ones helps reduce the misalignment between the original prompts and their synthetic output. Following [3], we use Qwen-2.5-VL [71] to caption our data as shown in Fig. 13. SFT Data For our final stage of SFT training, we selected approximately 5,000 high-quality videos based on stringent criteria for motion and aesthetics. The motion requirement is fulfilled by the presence of either distinct object motion, camera motion, or both. Ideal object motion is characterized by moderate magnitude and clearly focused action that is free from occlusions. Similarly, any camera movement must be stable and smooth, without jittering, to maintain 3D consistency. The aesthetic criteria are equally comprehensive. Beyond technical qualities like balanced brightness and natural color, we prioritize videos with appealing overall content and layout, demonstrated by thoughtful composition and engaging subject matter. Following this filtering process, the videos were classified into four motion categories (human activities, animal activities, other objects, natural or urban scenes) and three aesthetic styles (realistic, cartoon, cinematic). This strategic sampling across diverse categories is crucial for ensuring both the models high performance and the breadth of its capabilities. The influence of fine-tuning on the SFT data is illustrated in Fig. 14, where both the aesthetic details (the eyes in the first example), and the motion realism (the pipe of the second example) will be improved. E. World Model We fine-tune SANA-Video on several downstream tasks to demonstrate the potential of applying SANA-Video to world model related generation: embodied AI, autonomous driving and game generation. World Model for Embodied AI. The first important downstream task for video generation is embodied AI, where SANA-Video can be used to generate simulation data for robot training. In this task, we leverage AgiBot [72] as the SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer Figure 10 Visualization of image-to-video generation. SANA-Video can keep consistent with the first frame while generating realistic motion. Figure 11 The impact of motion score on I2V task. Higher motion score can lead to larger motion. training data, which contains synchronized views of different camera views. The head-front view is adopted as the target videos and filtered with our data pipeline. The generation results are shown in the first row of Fig. 15. World Model for Autonomous Driving. Video generation model is also good simulator for autonomous vehicle scenarios, and SANA-Video can be used to generate diverse and realistic driving scenes. In this task, we fine-tune SANA-Video on internal driving data, using the front camera with 30 FOV. The generation results are shown in the second row of Fig. 15. World Model for Game Generation. We explore downstream game generation to create interactive video games. Specifically, we use VPT [73] as the training data, containing screen recording videos of players playing Minecraft. The raw videos are cut and processed following our data pipeline in Appendix D. In addition, we train small classifier to identify low-quality data in the scenario. The generation results are shown in the third row of Fig. 15. 16 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer Figure 12 Long video visualization of SANA-Video. Figure 13 An overview of the captioning pipeline. 17 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer Figure 14 Analysis of the influence of SFT. Fine-tuning on the human preferred SFT data can improve the video details and adherence to the laws of physics. Figure 15 Visualization of world model task generation. 18 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer"
        },
        {
            "title": "References",
            "content": "[1] Google DeepMind. Veo 3. https://https://deepmind.google/models/veo/, 2025. [2] Kuaishou. Kling ai. https://klingai.kuaishou.com/, 2024. [3] Ang Wang, Baole Ai, Bin Wen, Chaojie Mao, Chen-Wei Xie, Di Chen, Feiwu Yu, Haiming Zhao, Jianxiao Yang, Jianyuan Zeng, et al. Wan: Open and advanced large-scale video generative models. arXiv preprint arXiv:2503.20314, 2025. [4] Yu Gao, Haoyuan Guo, Tuyen Hoang, Weilin Huang, Lu Jiang, Fangyuan Kong, Huixia Li, Jiashi Li, Liang Li, Xiaojie Li, et al. Seedance 1.0: Exploring the boundaries of video generation models. arXiv preprint arXiv:2506.09113, 2025. [5] Hansi Teng, Hongyu Jia, Lei Sun, Lingzhi Li, Maolin Li, Mingqiu Tang, Shuai Han, Tianning Zhang, WQ Zhang, Weifeng Luo, et al. Magi-1: Autoregressive video generation at scale. arXiv preprint arXiv:2505.13211, 2025. [6] Guibin Chen, Dixuan Lin, Jiangping Yang, Chunze Lin, Juncheng Zhu, Mingyuan Fan, Hao Zhang, Sheng Chen, Zheng Chen, Chengchen Ma, et al. Skyreels-v2: Infinite-length film generative model. arXiv preprint arXiv:2504.13074, 2025. [7] Adam Polyak, Amit Zohar, Andrew Brown, Andros Tjandra, Animesh Sinha, Ann Lee, Apoorv Vyas, Bowen Shi, Chih-Yao Ma, Ching-Yao Chuang, et al. Movie gen: cast of media foundation models. arXiv preprint arXiv:2410.13720, 2024. [8] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [9] Enze Xie, Junsong Chen, Junyu Chen, Han Cai, Haotian Tang, Yujun Lin, Zhekai Zhang, Muyang Li, Ligeng Zhu, Yao Lu, and Song Han. SANA: Efficient high-resolution text-to-image synthesis with linear diffusion transformers. In ICLR, 2025. [10] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [11] Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and FranÃ§ois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In ICML, 2020. [12] Enze Xie, Junsong Chen, Yuyang Zhao, Jincheng Yu, Ligeng Zhu, Chengyue Wu, Yujun Lin, Zhekai Zhang, Muyang Li, Junyu Chen, et al. Sana 1.5: Efficient scaling of training-time and inference-time compute in linear diffusion transformer. In ICML, 2025. [13] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [14] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas MÃ¼ller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In ICML, 2024. [15] Marianne Arriola, Aaron Gokaslan, Justin Chiu, Zhihan Yang, Zhixuan Qi, Jiaqi Han, Subham Sekhar Sahoo, and Volodymyr Kuleshov. Block diffusion: Interpolating between autoregressive and diffusion language models. arXiv preprint arXiv:2503.09573, 2025. [16] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast autoregressive video diffusion models. In CVPR, 2025. [17] Xun Huang, Zhengqi Li, Guande He, Mingyuan Zhou, and Eli Shechtman. Self forcing: Bridging the train-test gap in autoregressive video diffusion. arXiv preprint arXiv:2506.08009, 2025. [18] Jianlin Su. Transformer upgrade path: 2. rotary position encoding with comprehensive advantages. https://kexue.fm/ archives/8265, Mar 2021. [19] Mingzhen Sun, Weining Wang, Gen Li, Jiawei Liu, Jiahui Sun, Wanquan Feng, Shanshan Lao, SiYu Zhou, Qian He, and Jing Liu. Ar-diffusion: Asynchronous video generation with auto-regressive diffusion. In CVPR, 2025. [20] Junyu Chen, Han Cai, Junsong Chen, Enze Xie, Shang Yang, Haotian Tang, Muyang Li, Yao Lu, and Song Han. Deep compression autoencoder for efficient high-resolution diffusion models. arXiv preprint arXiv:2410.10733, 2024. [21] Yoav HaCohen, Nisan Chiprut, Benny Brazowski, Daniel Shalem, Dudu Moshe, Eitan Richardson, Eran Levin, Guy Shiran, Nir Zabari, Ori Gordon, et al. Ltx-video: Realtime video latent diffusion. arXiv preprint arXiv:2501.00103, 2024. 19 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer [22] Brandon Castellano. PySceneDetect. [23] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 45(11):1394113958, 2023. [24] Xiangyu Peng, Zangwei Zheng, Chenhui Shen, Tom Young, Xinying Guo, Binluo Wang, Hang Xu, Hongxin Liu, Mingyan Jiang, Wenjun Li, et al. Open-sora 2.0: Training commercial-level video generation model in $200 k. arXiv preprint arXiv:2503.09642, 2025. [25] Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2014420154, 2023. [26] G. Bradski. The OpenCV Library. Dr. Dobbs Journal of Software Tools, 2000. [27] Guoqing Ma, Haoyang Huang, Kun Yan, Liangyu Chen, Nan Duan, Shengming Yin, Changyi Wan, Ranchen Ming, Xiaoniu Song, Xing Chen, et al. Step-video-t2v technical report: The practice, challenges, and future of video foundation model. arXiv preprint arXiv:2502.10248, 2025. [28] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [29] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [30] Fan Zhang, Shulin Tian, Ziqi Huang, Yu Qiao, and Ziwei Liu. Evaluation agent: Efficient and promptable evaluation framework for visual generative models. arXiv preprint arXiv:2412.09645, 2024. [31] Muyang Li, Yujun Lin, Zhekai Zhang, Tianle Cai, Xiuyu Li, Junxian Guo, Enze Xie, Chenlin Meng, Jun-Yan Zhu, and Song Han. Svdquant: Absorbing outliers by low-rank components for 4-bit diffusion models. arXiv preprint arXiv:2411.05007, 2024. [32] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. [33] Songwei Ge, Seungjun Nah, Guilin Liu, Tyler Poon, Andrew Tao, Bryan Catanzaro, David Jacobs, Jia-Bin Huang, Ming-Yu Liu, and Yogesh Balaji. Preserve your own correlation: noise prior for video diffusion models. In Proceedings of the IEEE/CVF International Conference on Computer Vision (ICCV), pages 2293022941, October 2023. [34] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 76237633, 2023. [35] Daquan Zhou, Weimin Wang, Hanshu Yan, Weiwei Lv, Yizhe Zhu, and Jiashi Feng. Magicvideo: Efficient video generation with latent diffusion models. arXiv preprint arXiv:2211.11018, 2022. [36] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [37] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 2256322575, 2023. [38] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators. OpenAI Blog, 1(8):1, 2024. [39] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [40] Team Seawead, Ceyuan Yang, Zhijie Lin, Yang Zhao, Shanchuan Lin, Zhibei Ma, Haoyuan Guo, Hao Chen, Lu Qi, Sen Wang, et al. Seaweed-7b: Cost-effective training of video generation foundation model. arXiv preprint arXiv:2504.08685, 2025. [41] Shoufa Chen, Chongjian Ge, Yuqi Zhang, Yida Zhang, Fengda Zhu, Hao Yang, Hongxiang Hao, Hui Wu, Zhichao Lai, Yifei Hu, et al. Goku: Flow based video generative foundation models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 2351623527, 2025. 20 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer [42] Yifu Zhang, Hao Yang, Yuqi Zhang, Yifei Hu, Fengda Zhu, Chuang Lin, Xiaofeng Mei, Yi Jiang, Zehuan Yuan, and Bingyue Peng. Waver: Wave your way to lifelike video generation. arXiv preprint arXiv:2508.15761, 2025. [43] Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. [44] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37:5642456445, 2024. [45] Jinyi Hu, Shengding Hu, Yuxuan Song, Yufei Huang, Mingxuan Wang, Hao Zhou, Zhiyuan Liu, Wei-Ying Ma, and Maosong Sun. Acdit: Interpolating autoregressive conditional modeling and diffusion transformer. arXiv preprint arXiv:2412.07720, 2024. [46] Chaorui Deng, Deyao Zhu, Kunchang Li, Shi Guang, and Haoqi Fan. Causal diffusion transformers for generative modeling. arXiv preprint arXiv:2412.12095, 2024. [47] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Beyond next-token: Next-x prediction for autoregressive visual generation. arXiv preprint arXiv:2502.20388, 2025. [48] Sucheng Ren, Qihang Yu, Ju He, Xiaohui Shen, Alan Yuille, and Liang-Chieh Chen. Flowar: Scale-wise autoregressive image generation meets flow matching. arXiv preprint arXiv:2412.15205, 2024. [49] Yuchao Gu, Weijia Mao, and Mike Zheng Shou. Long-context autoregressive video modeling with next-frame prediction. arXiv preprint arXiv:2503.19325, 2025. [50] Haozhe Liu, Shikun Liu, Zijian Zhou, Mengmeng Xu, Yanping Xie, Xiao Han, Juan PÃ©rez, Ding Liu, Kumara Kahatapitiya, Menglin Jia, et al. Mardini: Masked autoregressive diffusion for video generation at scale. arXiv preprint arXiv:2410.20280, 2024. [51] Boyuan Chen, Diego MartÃ­ MonsÃ³, Yilun Du, Max Simchowitz, Russ Tedrake, and Vincent Sitzmann. Diffusion forcing: Next-token prediction meets full-sequence diffusion. Advances in Neural Information Processing Systems, 37:2408124125, 2024. [52] Junsong Chen, Jincheng Yu, Chongjian Ge, Lewei Yao, Enze Xie, Yue Wu, Zhongdao Wang, James Kwok, Ping Luo, Huchuan Lu, et al. Pixart-ğ›¼: Fast training of diffusion transformer for photorealistic text-to-image synthesis. arXiv preprint arXiv:2310.00426, 2023. [53] Black Forest Labs. Flux. https://github.com/black-forest-labs/flux, 2024. [54] Lianghui Zhu, Zilong Huang, Bencheng Liao, Jun Hao Liew, Hanshu Yan, Jiashi Feng, and Xinggang Wang. Dig: Scalable and efficient diffusion models with gated linear attention. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 76647674, 2025. [55] Junsong Chen, Chongjian Ge, Enze Xie, Yue Wu, Lewei Yao, Xiaozhe Ren, Zhongdao Wang, Ping Luo, Huchuan Lu, and Zhenguo Li. Pixart-ğœ: Weak-to-strong training of diffusion transformer for 4k text-to-image generation. In European Conference on Computer Vision, pages 7491. Springer, 2024. [56] Songhua Liu, Weihao Yu, Zhenxiong Tan, and Xinchao Wang. Linfusion: 1 gpu, 1 minute, 16k image. arXiv preprint arXiv:2409.02097, 2024. [57] Haoxin Chen, Menghan Xia, Yingqing He, Yong Zhang, Xiaodong Cun, Shaoshu Yang, Jinbo Xing, Yaofang Liu, Qifeng Chen, Xintao Wang, et al. Videocrafter1: Open diffusion models for high-quality video generation. arXiv preprint arXiv:2310.19512, 2023. [58] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. [59] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. International Journal of Computer Vision, 133(5):30593078, 2025. [60] Haocheng Xi, Shuo Yang, Yilong Zhao, Chenfeng Xu, Muyang Li, Xiuyu Li, Yujun Lin, Han Cai, Jintao Zhang, Dacheng Li, et al. Sparse videogen: Accelerating video diffusion transformers with spatial-temporal sparsity. In ICML, 2025. 21 SANA-Video: Efficient Video Generation with Block Linear Diffusion Transformer [61] Shuo Yang, Haocheng Xi, Yilong Zhao, Muyang Li, Jintao Zhang, Han Cai, Yujun Lin, Xiuyu Li, Chenfeng Xu, Kelly Peng, et al. Sparse videogen2: Accelerate video generation with sparse attention via semantic-aware permutation. NeurIPS, 2025. [62] Xingyang Li*, Muyang Li*, Tianle Cai, Haocheng Xi, Shuo Yang, Yujun Lin, Lvmin Zhang, Songlin Yang, Jinbo Hu, Kelly Peng, Maneesh Agrawala, Ion Stoica, Kurt Keutzer, and Song Han. Radial attention: ğ’ª(ğ‘› log ğ‘›) sparse attention with energy decay for long video generation. NeurIPS, 2025. [63] Jintao Zhang, Chendong Xiang, Haofeng Huang, Jia Wei, Haocheng Xi, Jun Zhu, and Jianfei Chen. Spargeattn: Accurate sparse attention accelerating any model inference. In ICML, 2025. [64] Peiyuan Zhang, Yongqi Chen, Haofeng Huang, Will Lin, Zhengzhong Liu, Ion Stoica, Eric Xing, and Hao Zhang. Vsa: Faster video diffusion with trainable sparse attention. NeurIPS, 2025. [65] Peiyuan Zhang, Yongqi Chen, Runlong Su, Hangliang Ding, Ion Stoica, Zhenghong Liu, and Hao Zhang. Fast video generation with sliding tile attention. ICML, 2025. [66] Hongjie Wang, Chih-Yao Ma, Yen-Cheng Liu, Ji Hou, Tao Xu, Jialiang Wang, Felix Juefei-Xu, Yaqiao Luo, Peizhao Zhang, Tingbo Hou, et al. Lingen: Towards high-resolution minute-length text-to-video generation with linear computational complexity. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 25782588, 2025. [67] Yu Gao, Jiancheng Huang, Xiaopeng Sun, Zequn Jie, Yujie Zhong, and Lin Ma. Matten: Video generation with mamba-attention. arXiv preprint arXiv:2405.03025, 2024. [68] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [69] Accelerate: An Extension of PyTorch for enabling easy deployment of models for large-scale training, 2022. [70] FFmpeg Developers. FFmpeg. http://ffmpeg.org, 2025. [71] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. [72] AgiBot World Colosseum contributors. Agibot world colosseum. https://github.com/OpenDriveLab/AgiBotWorld, 2024. [73] Bowen Baker, Ilge Akkaya, Peter Zhokov, Joost Huizinga, Jie Tang, Adrien Ecoffet, Brandon Houghton, Raul Sampedro, and Jeff Clune. Video pretraining (vpt): Learning to act by watching unlabeled online videos. Advances in Neural Information Processing Systems, 35:2463924654, 2022."
        }
    ],
    "affiliations": [
        "HKU",
        "KAUST",
        "MIT",
        "NVIDIA",
        "PKU",
        "THU"
    ]
}