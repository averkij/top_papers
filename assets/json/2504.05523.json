{
    "paper_title": "Pretraining Language Models for Diachronic Linguistic Change Discovery",
    "authors": [
        "Elisabeth Fittschen",
        "Sabrina Li",
        "Tom Lippincott",
        "Leshem Choshen",
        "Craig Messner"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown potential as tools for scientific discovery. This has engendered growing interest in their use in humanistic disciplines, such as historical linguistics and literary studies. These fields often construct arguments on the basis of delineations like genre, or more inflexibly, time period. Although efforts have been made to restrict inference to specific domains via fine-tuning or model editing, we posit that the only true guarantee is domain-restricted pretraining -- typically, a data- and compute-expensive proposition. We show that efficient pretraining techniques can produce useful models over corpora too large for easy manual inspection but too small for \"typical\" LLM approaches. We employ a novel date-attribution pipeline in order to obtain a temporally-segmented dataset of five 10-million-word slices. We train two corresponding five-model batteries over these corpus segments, efficient pretraining and Llama3-8B parameter efficiently finetuned. We find that the pretrained models are faster to train than the finetuned baselines and that they better respect the historical divisions of our corpus. Emphasizing speed and precision over a-historical comprehensiveness enables a number of novel approaches to hypothesis discovery and testing in our target fields. Taking up diachronic linguistics as a testbed, we show that our method enables the detection of a diverse set of phenomena, including en masse lexical change, non-lexical (grammatical and morphological) change, and word sense introduction/obsolescence. We provide a ready-to-use pipeline that allows extension of our approach to other target fields with only minimal adaptation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 2 3 2 5 5 0 . 4 0 5 2 : r Preprint. Under review."
        },
        {
            "title": "Pretraining Language Models for Diachronic Linguistic\nChange Discovery",
            "content": "Elisabeth Fittschen1, Sabrina Li2, Tom Lippincott2, Leshem Choshen3, Craig Messner2 1University of Hamburg, Germany 2Center for Digital Humanities, Johns Hopkins University, USA 3IBM Research, MIT, USA Correspondence: elisabeth.fittschen@studium.uni-hamburg.de"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown potential as tools for scientific discovery. This has engendered growing interest in their use in humanistic disciplines, such as historical linguistics and literary studies. These fields often construct arguments on the basis of delineations like genre, or more inflexibly, time period. Although efforts have been made to restrict inference to specific domains via fine-tuning or model editing, we posit that the only true guarantee is domain-restricted pretrainingtypically, dataand compute-expensive proposition. We show that efficient pretraining techniques can produce useful models over corpora too large for easy manual inspection but too small for typical LLM approaches. We employ novel date-attribution pipeline in order to obtain temporally-segmented dataset of five 10-million-word slices. We train two corresponding five-model batteries over these corpus segments, efficient pretraining and Llama3-8B parameter efficiently finetuned. We find that the pretrained models are faster to train than the finetuned baselines and that they better respect the historical divisions of our corpus. Emphasizing speed and precision over a-historical comprehensiveness enables number of novel approaches to hypothesis discovery and testing in our target fields. Taking up diachronic linguistics as testbed, we show that our method enables the detection of diverse set of phenomena, including en masse lexical change, non-lexical (grammatical and morphological) change, and word sense introduction/obsolescence. We provide ready-to-use pipeline that allows extension of our approach to other target fields with only minimal adaptation."
        },
        {
            "title": "Introduction",
            "content": "Certain fields of study invest heavily in the epistemological importance of boundaries that demarcate their objects of study into groups. These distinctions range from those as straightforward as the arrow of time (e.g. diachronic change in linguistics) to those derived from traditional means of practice (e.g. specific forms of poetry in literary studies).1 Such methodological investments are somewhat at odds with the dominant modern technology for language research, pretraining large language models (LLMs). LLMs are at least in part successful due to their omnivorous nature (Kaplan et al., 2020), they develop general skills by consuming as diverse and as large corpus as possible (Polo et al., 2024). Our target fields are inherently characterized by both limited data and specific interest in the case of our particular exemplar field, diachronic linguistics, in language rather than general model capability. Whether desirable or not, LLMs have some limited ability to divide information 1Models and datasets are found here, the code repository is found here. 1 Preprint. Under review. (for example, produce haiku and not limerick in zero-shot setting) (Cheng et al., 2024; Ifergan et al., 2024). However, prompting or other information elicitation techniques offer little immediate evidence that particular generation or string likelihood evaluation was performed using only knowledge appropriate to the desired period or division. The most convincing solution to this problem is also the most straightforward. To ensure models weights contain no out of domain information, you must simply train your own model on restricted corpus of your own choosing. We show that pretraining under tight academic budget of data (and compute) proves surprisingly effective when performed using the efficient methods provided by the BabyLM challenge community (Hu et al., 2024). Although those techniques are designed for data efficiency and cognitively-plausible pretraining, we find that BabyLlama-2 (Tastet & Timiryasov, 2024) is also an efficient and effective recipe for academic pretraining. To leverage this efficiency, we develop multiple-model approach that shifts the paradigm of diachronic linguistics research by offering access to corpus-level hypotheses concerning lexical change, non-lexical (grammatical and morphological) change, and word sense introduction/obsolescence that were previously obscure or unavailable. Specifically, we train series of 5 models each with pretraining dataset of 10 million tokens drawn from consecutive historical periods. We evaluate these pretrained models, as well as larger models finetuned on the same data slices, using standard metrics and evaluation sets. Moreover, as pretraining contrastive LLMs presents new paradigm, we showcase its potential with novel word-sense preference evaluation set, qualitative analysis and use case examples. We find that: 1. Our models train nearly two times faster than DoRA finetuned models while retaining adequate performance for many tasks 2. The finetuned models leak information across time periods in way that our models do not, jeopardizing tasks such as lexical sense-change analysis that require precise boundaries 3. When properly utilized, our full battery of models can be used to generate hypotheses about grammatical and lexical change across our corpus 4. This technique is likely useful to multiple domains, and can be adopted for automated hypothesis discovery in other fields."
        },
        {
            "title": "2 Related work",
            "content": "Multiple works suggest that LLMs could serve linguistic studies in flexible ways not imagined before (Warstadt & Bowman, 2022), such as simulating human subject responses (Wilcox et al., 2020; Trott, 2024; Aher et al., 2022) or modeling language acquisition (Hu et al., 2024; Warstadt et al., 2023). We draw upon this thinking, calling for pretraining as now feasible and promising way of contrasting corpora and aiding linguist queries, broadly, and specifically in semantic change. Lexical semantic change is an entire field dedicated to finding words that changed in meaning, those often do use an LLM, albeit not causal LLM (Periti & Montanelli, 2024). As such works use existing LLMs large part of the field is dedicated to processing and aligning those embeddings (Schlechtweg et al., 2020). We know of no work utilizing LLM automation to aid morphological, grammatical, orthographical and other linguistic changes beyond lexical semantic change. In that sense, our work is distinct from previous works."
        },
        {
            "title": "2.1 Efficient finetuning",
            "content": "Increased finetuning costs have lead to the development of parameter-efficient rank-adaptor finetuning techniques like LoRA (Hu et al., 2022) and DoRA (Liu et al., 2024). As these techniques are the most similar in compute and token demands to our approach we use DoRA finetuning to set our baseline. 2 Preprint. Under review. 2.2 Domain-specific language modeling Previous approaches to domain specific modeling that employ embedding models have typically chosen BERT-like architectures, and have employed finetuning (Hosseini et al., 2021; Qiu & Xu, 2022) and pretraining (Beck & ollner, 2023; Manjavacas & Fonteyn, 2022). However, beyond being non-causal, these strategies employ large sets of data, limiting their ability to be adapted for smaller domains of interest. One project trains recenthistory-aware (2011 to 2022) model on GPT2, but does so in order to detect knowledge-level analogies rather than provide methodological lever, and also employs large dataset (Drinkall et al., 2024). 2.3 Evaluating and guaranteeing historically-specific model knowledge Datasets for evaluating diachronic model knowledge have previously focused on historical performance, both linguistic (Manjavacas & Fonteyn, 2022) and at the level of knowledge (Dhingra et al., 2022; Piryani et al., 2024) , with these latter sets typically being structured in QA form."
        },
        {
            "title": "3.1 Setup",
            "content": "Training data. We employ multistage pipeline to prepare time-bound slices for pretraining. This pipeline integrates three sources to accurately estimate the publication date of each document found in the Project Gutenberg collection. (1) Author information sourced from WikiData, WD (2) Work metadata found in the Project Gutenberg Catalog, PGC (3) Inference performed by LLMs We first define historical range that will structure our inquiry, the years 1750-1940, inclusive. We use fuzzy string matching system, described in Appendix B, to align authors to works. We acquire final publication dates for each author-associated work collected in the previous fuzzy matching step by prompting an instruction-tuned LLM. In order to evaluate the efficacy of this work-date attribution approach, we selected variety of openand closedsource LLMs and calculated performance against gold-annotated test set consisting of set of works published from 1550-1850. While the closed-source LLMs perform best, Llama3.3-70B (Grattafiori et al., 2024), quantized to 4 bits using the BitsAndBytes library Belkada et al. (2023) performs well enough to justify its use (see Appendix for details of the evaluation process). We prompt this model to provide date of writing for each of the PG works for each author in the set produced by the fuzzy author matching stage. Finally, we split this corpus into 5 sections using the date information generated by the previous step. We set the boundaries for these slices by negotiating between ideal priori boundaries (say, 50 years or 30 years) and our desire to obtain 10 million training tokens for each split. We further reserve 5 million tokens for testing and 1 million for validation during training. This results in 5 equal subcorpora for the time periods 1750-1820, 1820-1850, 1850-1880, 1880-1910, and 1910-1940."
        },
        {
            "title": "3.2 Procedure",
            "content": "Model training. We employ two training approaches over each split of the historical data: (1) finetuned models adapted from larger pretrained model, and (2) pretrained models trained solely on the small historical datasets. We train the finetuned models using DoRA adapters on top of Llama3 8B backbone. We train our experiment pretrained models using the BabyLlama-2 recipe, which employs distillation approach. Ultimately, pretraining the BabyLlama-2 models was quicker and more efficient than the DoRA finetuning process. More details regarding the training procedures can be found in Appendix A. 3 Preprint. Under review. 3.3 Evaluation We evaluate the trained BabyLlama-2 models (pretrained), DoRA models (finetuned), and two baseline LLMs (The pretrained versions of BabyLlama-2 and Llama3-8B) using modified version of the BabyLM evaluation pipeline, perplexity, and novel cloze evaluation set. Text Sense Year They had bunch of crazy ideas that would never work tried to call the operator but the phone was dead You know how it is. Im not into ironing. Its not my thing Lets go where theres some life. Whatta ya say? Hey baby, Im down 1599 1882 1936 1952 Table 1: Cloze task examples and the year when the word sense first appeared Perplexity. To verify the strength of our temporal boundaries and ensure that we capture time-specific linguistic information, we calculate perplexity for each model on test set drawn from its own timeslice as well as ones drawn from each other timeslice. We also use perplexity as measure of general fluency. BabyLM Evaluation Pipeline: BLiMP. The BabyLM evaluation pipeline provided by Choshen et al. (2024) is version of EleutherAIs lm-evaluation-harness (Gao et al., 2023), modified to support the evaluation of models trained over token-limited corpus, in our case the overlap in-vocabulary for all of the time slices. Specifically, this set consists of samples where every word appears twice in the model training sets (maximally filtered). The pipeline supports evaluation over BLiMP, GLUE (Wang et al., 2018) and EWoK (Ivanova et al., 2024) of which we solely utilize BLiMP (Warstadt et al., 2020). Concretely, BLiMP tests the models ability to understand different linguistic phenomena, which we aggregate to measure linguistic model performance. We also closely analyze the results of specific BLIMP phenomena to explore BLiMPs capacity to uncover historically-specific linguistic preferences learned by our models in 4.2. Novel word sense cloze evaluation set We construct this dataset using the Oxford English Dictionary (OED), which catalogs most English words and their respective word senses. For each word sense the OED provides the year of its first registered usage, as well as list of curated example sentences illustrating the word sense in context. To generate usable cloze task for next-token prediction models without the ability to follow instructions, the masked words need to be located at the end of the sentences. We select sentences where the word in question appears within the last 10% of characters. For practicality, we further restrict the dataset to words the OED doesnt consider exceptionally rare, specifically ones appearing once every thousand to million words (Table 12 in the appendix). During evaluation, the dataset is filtered akin to the BLiMP task, such that sentences with uncommon words, which have less than two occurrences in any training set, are filtered out. Evaluation is performed by generating the top one-word responses. This is achieved using custom LogitsProcessor, which redistributes the probability mass of tokens initiating new word to the EOS token. In combination with probability-based beam search (length penalty set to zero) this method efficiently approximates the top-k responses. We were unable to find similar approach in the literature. Example tasks are shown in Table 1. More details on sense distribution and evaluation details can be found in Appendix C."
        },
        {
            "title": "3.4 Analysis",
            "content": "For the exploratory analysis, we contrast the log perplexity of the different models. This is done by first min-max normalizing the perplexities over sentence. Despite the models having different baseline perplexities, their normalized log perplexity follows similar trajectory, with the exception of words particularly characteristic (domain-specific) for models dataset. This phenomenon is shown in Table 2, where significant shift can be seen for station, which lowers in perplexity as the railway system is widely adopted during 4 Preprint. Under review. Model Sentence 1750 to 1820 with whom he talked in the station at fort wayne interested him 1820 to 1850 with whom he talked in the station at fort wayne interested him 1850 to 1880 with whom he talked in the station at fort wayne interested him 1880 to 1910 with whom he talked in the station at fort wayne interested him 1910 to 1940 with whom he talked in the station at fort wayne interested him Table 2: Normalized perplexities for different models, lighter red signifies higher surprisal. (a) Finetuned models with Llama3 8B baseline (b) Pretrained models with BabyLlama2 baseline Figure 1: Cross-time perplexities 1840s and 50s. We use this perplexity data to generate candidates for word sense change, motivated by the notion that words whose later sense has not yet emerged should have higher perplexity for earlier models."
        },
        {
            "title": "4.1 Perplexity",
            "content": "The baseline and finetuned models are fluent, but lack historical specificity. The baseline models show slight time-period preferences but are logically a-historical (Figure 1). The DoRA-adapted models have overall low perplexity Models trained on earlier time slices show strong preference for data from their respective time slice, whereas those trained on later slices do not show such prominent specialization. The pretrained models are less fluent, but are specialized to their historical period. The BabyLlama-2 models uniformly produce their lowest perplexities when measured against their periods corresponding test set. Cross-evaluation of the models against the other reserved test sets (Figure 1) yields encouraging signs that the linguistic information captured by the pretrained models follows an appropriate historical arc, and that there is little to no information from any of the untargeted time slices. Perplexity is low at the relevant period and increases linearly both when testing older and newer texts."
        },
        {
            "title": "4.2 BLiMP",
            "content": "The pretrained models perform reasonably despite underperforming the baseline models. The two unmodified baseline models earn aggregate scores of 0.74 (BabyLlama2) and 0.82 (Llama3-8B) on our most filtered version of the BLiMP dataset. This is expected in both cases, as our model was trained over different mix of 10,000,000 tokens (in the case of the former) and far fewer tokens (in the case of the later). Nonetheless, our model slices approach the general competence of baseline BabyLlama2. 5 Preprint. Under review. Model 1750-1820 18201850-80 1880-1910 1910-40 pretrained 0.67 0.80 finetuned 0.68 0.81 0.69 0. 0.72 0.84 0.72 0.84 Table 3: Aggregate maximally filtered BLiMP accuracy across all timeslices. Model 1750-1820 18201850-80 1880-1910 1910-40 pretrained 0.00 0.92 finetuned 0.00 0.96 0.33 0. 0.82 1.00 0.91 0.99 Table 4: Accuracy for maximally filtered BLiMP only NPI licenser present task across all timeslices. Our pretrained models begin to prefer only to even in later slices. The finetuned models consistently outperform the pretrained models  (Table 3)  . This is to be expected given that these models have access to much larger sample of linguistic information. However, beyond verifying that the models are usable models of language, we care about the contrastive differences between them. We note an increase in BLIMP competency over time. Interestingly, this is not sign of incompetence, but rather newly found ability to distinguish general linguistic change. The pretrained models capture historically specific grammatical change in way the finetuned models do not. Table 4 collects the timeslice-relative performance of each modeling strategy on the BLiMP only NPI licenser present task. For comparison, baseline BabyLlama2 and Llama3-8B score 0.76 and 0.90 on this task, respectively. Negative polarity items (NPIs) are words that indicate negative sentence (Penka & Zeijlstra, 2010). This specific BLiMP phenomenon evaluates whether given model prefers the typically negatively polar construction only...ever to the typically positively polar even...ever, as in the example taken from the filtered BLiMP test set found below: Only Nina ever falls asleep. *Even Nina ever falls asleep. The underlying conditions that license the use of an NPI are complex, and continue to be the subject of research. However, scholars generally agree that licensing conditions vary between languages and across diachronic periods of single tongues (Herburger, 2023; Labelle & Espinal, 2014; Penka & Zeijlstra, 2010; Zeijlstra, 2016). Our models capture this diachronic sensitivity. While the finetuned DoRA models demonstrate preference for only...ever to even..ever across all timeslices, the pretrained BabyLlama2 models only start to prefer the former construction in the later corpus sub-periods. More precisely, the earlier models seem to prefer associating even with specifically negative polarity. Examining the earliest and latest training corpora reveals plentiful attributions of general only...ever (235 and 228 respectively) and even...ever constructions (137 and 113 respectively). The latest corpus slice contains far more unambiguously negative-context uses of only...ever (e.g. only girl that ever, only time he ever) than the earliest slice. Here, only is employed more broadly, for example as form of just (e.g. only thus much: if you have ever had any cause to believe him impressed with your idea). Per the pretrained models, this change in preference increases monotonically over time with final sharp discontinuous rise. The finetuned models offer no such insight, likely due to the linguistic priors they retain from their historically-agnostic pretraining phase. Catastrophic forgetting does not completely obliterate these priors, endangering the historical specificity of their predictions. The token count and sourcing limitations of the corpus slices might render extending the diachronic conclusions offered by our models to the whole of English fraught. However, even if this insight is corpus-specific, the baseline models do not register any such change at all. By offering the capacity to discover diachronic narratives, and 6 Preprint. Under review. (a) Finetuned models with Llama3 8B baseline (b) Pretrained models with BabyLlama2 baseline Figure 2: Model performance on the top 100 completion cloze task compare trends across corpora, our modeling approach opens up avenues to further study that the finetuned models silently pass over. The pretrained models capture time period-specific changes in lexical meaning in way the finetuned models do not. While the finetuned models generally succeed more in completing given cloze sentence across the entirety of the corpus, they achieve this in part by integrating inappropriate timeslice information (Figure 2), rendering them less useful for contrasting corpora and studying change. This is shown in Figure 3, where the Leakage or recall over future senses is measured. The finetuned models consistently outperform over future senses. More in Appendix D. Figure 3: Probability of Leakage, over pretrained and finetuned models. Performing error analysis over each slice of both the pretrained and finetuned models confirms that finetuned models from early timeslices perform inappropriately well on future cloze tasks, while the pretrained models do not. Table 5 presents two examples of this phenomenon for models trained on the 1750-1820 timeslice. The first example, pound is ranked as most likely by the finetuned model and unlikely by the pretrained model. Given the intended time period specialization, there is no way that this particular car-centric sense should even be obliquely available in the training data, making this pure historical error, where the finetuned version relies on its prior linguistic information to make determination. The second example, centering on silver as an elliptical for silver medal is correctly completed by the finetuned model, but also ranked as reasonably likely by pretrained version. However, this example includes collocation of gold, that could steer model from any of our time slices towards higher probability of silver. We find that this trend continues throughout the time slices. While the pretrained models are at times inappropriately performant, it is only on this particular subset of cloze tasks that makes correct answer likely by other means. In contrast, the finetuned model consistently performs out of its bounds on both this set of examples as well as those similar to the first sample that are more obviously out of bounds. The pretrained models enable novel hypotheses about lexical changes across our corpus. In addition to these two types of errors, manual inspection reveals third, more exciting form. Table 6 contains an example of this error type, drawn from cloze task that centers 7 Preprint. Under review. Sentence Definition Year Pretrain Finetune Im going to sell my car... No more police towing [it] ..to car pound. place in which vehicles impounded by the police or other authorities are kept... 1970 Hill ... which won three gold and silver. Elliptical for silver medal n. 1960 7 0 Table 5: Two examples for time slice 1750-1820 with their rank per model. Sentence Definition They have nowhere to go. This ishow do the Americans say it?the end of the line. V. direction or course of movement. the end of the line ( transferred and figurative ). Cf. the end of the road at end n.. Sense Year 1948 Table 6: The new sense of line is accepted by the finteuned (rank #1) and pretrained (#14). on the phrase end of the line. Both pretrained and finetuned rank the correct completion within the top 20 completions. However, the pretrained models ranking differs from the error types examined above. While we cannot rule out that the finetuned model is achieving accuracy due to its future knowledge, we can do so for the pretrained counterpart (for example, collocation search of the training data reveals that this exact construction is never used). Additionally, unlike the second type of error (gold...silver) nothing in the context makes line likely conclusion. Thus, the surprising performance of the pretrained model is best explained as prefiguration of construction to come. The way line is used in the 1750-1820 slice of the corpus predicts its ability to be used in this particular construction in the future. Examining the training corpus reveals numerous uses of line in hereditary (i.e. end of ones line) writing (i.e. the line ended) and military (i.e. the British line) contexts, all uses logically associated with the action of ending. Some uses, especially in writing, seem sufficient to support this construction. In sense, they pave the way for end of the line, detail captured by the pretrained, but not by the finetuned models. Reintroducing the axis of time in the form of the full cross-time battery of models further enhances our methods ability to detect subtle changes in use. Table 7 shows the rank of the correct word cholera for each of the models when completing the context sentence: The potatoes failed, the pigs were affected with disease which the people called cholera This sense of cholera is attributed to 1837, and concerns specific hog disease originally grouped with the human malady due to its surface-level similarities. Only the earliest pretrained model considers cholera acceptable in this context, while the finetuned models rank it highly across all time-slices. Collocation of cholera in the earliest slice reveals that the nature of the disease had not yet solidified in discourse. Manual inspection shows that multiple forms of cholera, morbus and infantus, both ailments unrelated to the modern understanding of the disease, share lexical space with phenomenological descriptions of their symptomatic similarity (fever, diarrhea etc.). By the next two time slices, the term begins to coalesce around the common 19th century understanding of cholera as specific, communicable human disease capable of producing mass illness events, as demonstrated by collocations like epidemic and plague. The pretrained models capture this moment of conceptual solidification, while the DoRA baselines offer no such insight. To sum, these findings on changes we know happened hint at potential uses even beyond linguistic change, such as historical studies of social and knowledge change. 8 Preprint. Under review. 1750-1820 1820-1850 1850-1880 18801910-1940 Pretrained Finetuned 41 18 NaN 19 NaN 11 NaN NaN 11 Table 7: Rank of cholera completion. Llama3-8B ranks it 8, BabyLlama-2 ranks it 57. NaN indicates it is outside the top k. 4.3 Diachronic analysis The efficiency and historical certainty of our modeling approach enable numerous novel ways to analyze linguistic change. For example, contrasting the information provided by each timeslice model allows more flexible automated hypotheses generation than the cloze approach utilized above. Discovering sense trajectories of interest. Word senses can be said to have trajectories across our corpus slice periods, as judged by their acceptability by the pretrained model trained on given slice. For example, one would expect earlier models to be perplexed by the word car, in the sense of an automobile, and expect the later models would accept it. To track how this sort of shift occurs, we set the 1910-1940 models normalized per-word perplexity scores as baseline, and retrieve all usages where the perplexity difference decreased continually with time. For tractability, we subset this group to those with the largest change in acceptability between the first and last models. This approach captures distinctions in usage over time, and separates synchronically distinct senses of words. Figure 4 depicts the trajectories for the word station. (See the full data used in this analysis in the supplementary materials.) Two senses of the word emerge after applying our filtering approach. The first sense is associated with railroad station, and the second with stopover or encampment site. While both of these senses becomes more acceptable as time goes on, they follow distinct trajectories. The rail-related sense becomes precipitously more acceptable in the 1820-1850 timeslice, no doubt due to the adoption of rail technology during that period. In contrast, the camp/stopover sense begins its trajectory from place of relative acceptability, and then proceeds to become smoothly even more acceptable as time passes. Figure 4: Natural appearances of station with descending probability trajectory and manually labelled for sense. These observations lead to any number of hypotheses about the interaction between these senses that could be pursued by further means over larger corpora. For example, this information allows the question of whether the already-acceptable usage of station as camp or stop grew more acceptable due to the influence of the emerging rail-related sense. We offer some further analytical directions in Appendix E."
        },
        {
            "title": "5 Conclusion and future work",
            "content": "Our modeling approach leverages efficient pretraining in order to offer novel, boundaryguaranteed, form of linguistic hypothesis discovery across comparative corpora. While we believe this approach has massive potential for the specific use case examined above, diachronic change, we also believe that it is extensible, and can be leveraged in similar way across different corpus divisions and fields. Further work could verify these beliefs by testing our approachs ability to detect linguistic shifts across synchronic boundaries. 9 Preprint. Under review. Additionally, increasing the size of the training corpora could lead to increased model knowledge, allowing for the discovery of knowledge-level hypotheses relevant to disciplines like literary studies and history."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Haim Dubossarsky for feedback concerning the use and evaluation of our modeling strategy. Elisabeth Fittschen acknowledges the support of Hamburglobal, whose stipend partially funded her exchange program."
        },
        {
            "title": "References",
            "content": "Gati Aher, RosaI. Arriaga, and A. Kalai. Using large language models to simulate multiple In International Conference on Machine humans and replicate human subject studies. Learning, 2022. URL https://api.semanticscholar.org/CorpusId:251719353. Christin Beck and Marisa ollner. Ghisberttraining bert from scratch for lexical semantic investigations across historical german language stages. In Proceedings of the 4th Workshop on Computational Approaches to Historical Language Change, pp. 3345, 2023. Younes Belkada, Tim Dettmers, Artidoro Pagnoni, Sylvain Gugger, and Sourab Mangrulkar. Making llms even more accessible with bitsandbytes, 4-bit quantization and qlora, 2023. Jeffrey Cheng, Marc Marone, Orion Weller, Dawn Lawrie, Daniel Khashabi, and Benjamin Van Durme. Dated data: Tracing knowledge cutoffs in large language models. arXiv preprint arXiv:2403.12958, 2024. Leshem Choshen, Ryan Cotterell, Michael Y. Hu, Tal Linzen, Aaron Mueller, Candace Ross, Alex Warstadt, Ethan Wilcox, Adina Williams, and Chengxu Zhuang. [call for papers] the 2nd BabyLM Challenge: Sample-efficient pretraining on developmentally plausible corpus. Computing Research Repository, arXiv:2404.06214, 2024. URL https: //arxiv.org/abs/2404.06214. Bhuwan Dhingra, Jeremy Cole, Julian Martin Eisenschlos, Daniel Gillick, Jacob Eisenstein, and William Cohen. Time-aware language models as temporal knowledge bases. Transactions of the Association for Computational Linguistics, 10:257273, 2022. Felix Drinkall, Eghbal Rahimikia, Janet Pierrehumbert, and Stefan Zohren. Time machine GPT. In Kevin Duh, Helena Gomez, and Steven Bethard (eds.), Findings of the Association for Computational Linguistics: NAACL 2024, pp. 32813292, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-naacl.208. URL https://aclanthology.org/2024.findings-naacl.208/. Leo Gao, Jonathan Tow, Baber Abbasi, Stella Biderman, Sid Black, Anthony DiPofi, Charles Foster, Laurence Golding, Jeffrey Hsu, Alain Le Noach, Haonan Li, Kyle McDonell, Niklas Muennighoff, Chris Ociepa, Jason Phang, Laria Reynolds, Hailey Schoelkopf, Aviya Skowron, Lintang Sutawika, Eric Tang, Anish Thite, Ben Wang, Kevin Wang, and Andy Zou. framework for few-shot language model evaluation, 12 2023. URL https://zenodo.org/records/10256836. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Elena Herburger. On the history of npis and negative concord. Canadian Journal of Linguistics/Revue canadienne de linguistique, 68(4):555589, 2023. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network. arXiv preprint arXiv:1503.02531, 2015. Kasra Hosseini, Kaspar Beelen, Giovanni Colavizza, and Mariona Coll Ardanuy. Neural language models for nineteenth-century english. arXiv preprint arXiv:2105.11321, 2021. 10 Preprint. Under review. Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. ICLR, 1(2):3, 2022. Michael Y. Hu, Aaron Mueller, Candace Ross, Adina Williams, Tal Linzen, Chengxu Zhuang, Leshem Choshen, Ryan Cotterell, Alex Warstadt, and Ethan Gotlieb Wilcox (eds.). The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning, Miami, FL, USA, November 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.conll-babylm.0/. Maxim Ifergan, Leshem Choshen, Roee Aharoni, Idan Szpektor, and Omri Abend. Beneath the surface of consistency: Exploring cross-lingual knowledge representation sharing in llms. arXiv preprint arXiv:2408.10646, 2024. Anna Ivanova, Aalok Sathe, Benjamin Lipkin, Unnathi Kumar, Setayesh Radkani, Thomas Clark, Carina Kauf, Jennifer Hu, RT Pramod, Gabriel Grand, et al. Elements of world knowledge (ewok): cognition-inspired framework for evaluating basic world knowledge in language models. arXiv preprint arXiv:2405.09605, 2024. Jared Kaplan, Sam McCandlish, Tom Henighan, Tom Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361, 2020. Marie Labelle and Teresa Espinal. Diachronic changes in negative expressions: The case of french. Lingua, 145:194225, 2014. Alexandre Lacoste, Alexandra Luccioni, Victor Schmidt, and Thomas Dandres. Quantifying the carbon emissions of machine learning. arXiv preprint arXiv:1910.09700, 2019. Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weight-decomposed low-rank adaptation. In Forty-first International Conference on Machine Learning, 2024. Enrique Manjavacas and Lauren Fonteyn. Adapting vs. pre-training language models for historical languages. Journal of Data Mining & Digital Humanities, (Digital humanities in languages), 2022. Doris Penka and Hedde Zeijlstra. Negation and polarity: An introduction. Natural Language & Linguistic Theory, 28:771786, 2010. Francesco Periti and Stefano Montanelli. Lexical semantic change through large language models: survey. ACM Computing Surveys, 56(11):138, 2024. Bhawna Piryani, Jamshid Mozafari, and Adam Jatowt. Chroniclingamericaqa: large-scale question answering dataset based on historical american newspaper pages. In Proceedings of the 47th International ACM SIGIR Conference on Research and Development in Information Retrieval, pp. 20382048, 2024. Felipe Maia Polo, Seamus Somerstep, Leshem Choshen, Yuekai Sun, and Mikhail Yurochkin. Sloth: scaling laws for llm skills to predict multi-benchmark performance across families. arXiv preprint arXiv:2412.06540, 2024. Wenjun Qiu and Yang Xu. Histbert: pre-trained language model for diachronic lexical semantic analysis. arXiv preprint arXiv:2202.03612, 2022. Dominik Schlechtweg, Barbara McGillivray, Simon Hengchen, Haim Dubossarsky, and Nina Tahmasebi. SemEval-2020 task 1: Unsupervised lexical semantic change detection. In Aurelie Herbelot, Xiaodan Zhu, Alexis Palmer, Nathan Schneider, Jonathan May, and Ekaterina Shutova (eds.), Proceedings of the Fourteenth Workshop on Semantic Evaluation, pp. 123, Barcelona (online), December 2020. International Committee for Computational Linguistics. doi: 10.18653/v1/2020.semeval-1.1. URL https://aclanthology.org/2020. semeval-1.1/. 11 Preprint. Under review. Jean-Loup Tastet and Inar Timiryasov. BabyLlama-2: Ensemble-distilled models consistently outperform teachers with limited data. In Michael Y. Hu, Aaron Mueller, Candace Ross, Adina Williams, Tal Linzen, Chengxu Zhuang, Leshem Choshen, Ryan Cotterell, Alex Warstadt, and Ethan Gotlieb Wilcox (eds.), The 2nd BabyLM Challenge at the 28th Conference on Computational Natural Language Learning, pp. 292301, Miami, FL, USA, November 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024. conll-babylm.26/. Sean Trott. Can large language models help augment english psycholinguistic datasets? In unknown, 2024. URL https://api.semanticscholar.org/CorpusId:267091853. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel Bowman. GLUE: multi-task benchmark and analysis platform for natural language understanding. In Tal Linzen, Grzegorz Chrupała, and Afra Alishahi (eds.), Proceedings of the 2018 EMNLP Workshop BlackboxNLP: Analyzing and Interpreting Neural Networks for NLP, pp. 353355, Brussels, Belgium, November 2018. Association for Computational Linguistics. doi: 10.18653/v1/W18-5446. URL https://aclanthology.org/W18-5446/. Alex Warstadt and Samuel Bowman. What artificial neural networks can tell us about human language acquisition. In Algebraic structures in natural language, pp. 1760. CRC Press, 2022. Alex Warstadt, Alicia Parrish, Haokun Liu, Anhad Mohananey, Wei Peng, Sheng-Fu Wang, and Samuel R. Bowman. BLiMP: The benchmark of linguistic minimal pairs for English. Transactions of the Association for Computational Linguistics, 8:377392, 2020. doi: 10.1162/ tacl 00321. URL https://aclanthology.org/2020.tacl-1.25/. Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan Cotterell. Findings of the BabyLM challenge: Sample-efficient pretraining on developmentally plausible corpora. In Alex Warstadt, Aaron Mueller, Leshem Choshen, Ethan Wilcox, Chengxu Zhuang, Juan Ciro, Rafael Mosquera, Bhargavi Paranjabe, Adina Williams, Tal Linzen, and Ryan Cotterell (eds.), Proceedings of the BabyLM Challenge at the 27th Conference on Computational Natural Language Learning, pp. 134, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.conll-babylm.1. URL https://aclanthology.org/2023.conll-babylm.1/. Ethan Gotlieb Wilcox, Jon Gauthier, Jennifer Hu, Peng Qian, and R. Levy. On the predictive power of neural language models for human real-time comprehension behavior. ArXiv, abs/2006.01912, 2020. URL https://api.semanticscholar.org/CorpusId:219261016. Hedde Zeijlstra. Diachronic developments in the domain of negation. Language and Linguistics Compass, 10(6):284295, 2016."
        },
        {
            "title": "A Additional model and training details",
            "content": "DoRa adapters of rank 16 were chosen for efficiency purposes. We adopt the hyperparamers used in the original paper  (Table 8)  . We finetune using next-token prediction loss on the data slices. We train the models for three epochs, with one training run taking around eight hours on single A100 GPU, which emits 0.11KG CO2 an hour (Lacoste et al., 2019). We choose the best model checkpoints based on evaluation loss. Notably, models trained on data after 1850 reached optimal performance after single epoch, while those trained over earlier periods continued to show improvement during the second epoch. We train the pretrained models using the BabyLlama2 training recipe. We adopt the same Llama-345M model  (Table 10)  and training hyperparameters  (Table 9)  as the original paper. BabyLlama2 uses distillation strategy where the logits of two trainer models are used to train student model. Notably, the teacher and student models are of the same size. We 12 Preprint. Under review. Hyperparameters (DoRA) LLaMA3-8B Rank α Dropout Optimizer LR LR Scheduler Batch size Warmup Steps Epochs Where 16 32 0.05 AdamW 1 104 Linear 16 100 3 Q, K, V, Up, Down Table 8: Hyperparameter configurations of DoRA for LLaMA3-8B. Hyperparameter Learning rate Number of epochs Batch size Weight decay Distillation α Value 7 104 8 128 5 0.5 Table 9: Training and distillation hyperparameters of BabyLlama-2 initialize Byte-Pair-Encoding tokenizer for each time slice and train two teacher models over the training data for eight epochs. We select the model with the best validation score (consistently epoch four during our runs). Training teacher model took around 32 minutes on single A100 GPU. From the two teachers, we then distill student model using the distillation loss after Hinton et al. (2015), with = αLCE + (1 α)LKL This loss is made up in equal parts of the normal next token prediction loss and the loss over the soft trainer logits. We train the student over eight epochs; the last epoch consistently having the lowest evaluation loss. Training the student model took 3 hours and 20 minutes on an A100."
        },
        {
            "title": "B Attribution pipeline details",
            "content": "We extract from WD all entities with an occupation of author or writer that also have birth dates that fall within this range. We further constrain this subset by filtering it to only include authors WD indicates were known to write in English. We then fuzzily match this set of authors to those in PGC. The first pass uses Levenshtein distance matching with predefined threshold in combination with any extractable birth and death information to match PGC authors to the list sourced from WD. The optional second pass uses only fuzzy string matching with stricter predefined threshold, and matches any remaining PGC authors to an author from WD. This second pass allows for the inclusion of authors without WD-provided date information, compensating for the further loss in certainty with tighter regulation of name similarity. The result of this stage is mapping between WD authors and PGC authors with an associated list of their works found in PG. To validate open source and propriety LLM performance on work-date attribution we manually annotated sample (n=1054) of known-author works with their date of writing using publication information sourced from internet repositories like the HathiTrust collection (This material is available in the supplement). We then used one open weight model (Llama3.3-70B quantized to 4 bits) and two proprietary models (GPT-4, GPT-4o) to zero-shot attribute the dates of works using the following prompt: When was the work {} by {} written? Answer just with the year. 13 Preprint. Under review. Hyperparameter Vocabulary size Number of layers Number of heads Number of KV heads Embedding dimension Hidden dimension Total parameters Value 16,000 32 15 5 960 2560 345M Table 10: BabyLlama-2 Model Architecture. Where the first {} was replaced with the work title and the second {} by the work author. We then evaluated performance with tolerance of +/- 1 year to account for the historically common practice of assigning publication date to copyright year. Noting systematic error in the results provided by the best performing model at this stage (GPT-4o) we collected the set of erroneously attributed texts produced by this model and undertook another round of hand annotation on this set, spending additional effort to source historical materials (publishing industry trade journals, library records) that could disambiguate questionable attributions or provide evidence of earlier publications not in the digitized record. We then re-evaluated the models with tolerances of +/- 1 and 10 years, allowing match to either date attribution to be acceptable. Additionally, we evaluated the models after disqualifying scores with extreme difference (+/- 50 years) from their ground score, to assess the impact of having more certain source of information (say, author birth and death dates) that pre-restricts correct answers to tighter range. Table 11 shows that while the closed-source models perform the best under these conditions, the open source model performs well enough to serve as first point of departure. +/-1 +/-10 DQ +/-1 DQ +/-10 Llama3.3-70B 0.63 0.74 GPT-4 0.82 GPT-4o 0.81 0.89 0.84 0.70 0.87 0.96 0.88 0.99 0. Table 11: Performance on work date attribution per LLM. +/- indicates year delta tolerance threshold, DQ indicates that extreme variations from the ground scores (+/-50) were not considered Notably, this approach is flexible broader diachronic slices justify tolerating more variance."
        },
        {
            "title": "C Cloze evaluation set details",
            "content": "The cloze evaluation set contains 50.4 thousand examples. Of which 14.6 thousand examples remain after filtering, large portion is of old english origin as can be seen in Figure 5. Evaluation was performed over the top 100 word completion task. If the word appeared within the top 100 words (case insensitive) the completion was considered successful. For evaluation the senses were grouped by time slice. In Figures 2 and 6, each model was evaluated over each time slice. In the leakage reports (Figures 3 and 7) the model performance was contrasted between the senses created before and after the models respective training cutoff."
        },
        {
            "title": "D Additional model performance information",
            "content": "We include an overview of mean reciprocal rank over time, for more detailed insight into model performance (Figure 6). As well as figure showing model leakage dived by model recall (Figure 7). Here, it can be clearly seen that the finetuned models performance on 14 Preprint. Under review. Figure 5: Count of cloze tasks for per time slice for the set filtered for our data (14.6 thousand examples). Band Freq./mil. % in OED 8 7 6 5 4 3 2 1 >1,000 100 1,000 10 100 1 10 0.1 1 0.01 0.1 <0.01 0.02% 0.18% 1% 4% 11% 20% 45% 18% Table 12: Word Frequency Bands and their respective counts per million words and the percentage of non-obsolete OED entries future time slices is unprecedented also when correcting for the relatively weak performance of the pretrained models. (a) Baseline (DoRA) MRR (b) BabyLlama MRR Figure 6: Model performance on the cloze task"
        },
        {
            "title": "E Further analytical commentary",
            "content": "In second, more cumulative analysis  (Table 8)  , words with consistently high perplexity differences were highlighted. The underlying reason for these variations is varied. Some words show semantic shifts, such as car (automobile) plane (airplane) and inspector (detective), while others are part of novel word combinations, which had gained popularity such as skirt in the context of hobble skirt or Victoria in the context Queen Victoria. 15 Preprint. Under review. Figure 7: Probability of leakage corrected for model recall. Figure 8: Cumulative perplexity results. While these insights cannot be pinpointed to single phenomenon, they offer valuable insights into the training corpora."
        }
    ],
    "affiliations": [
        "Center for Digital Humanities, Johns Hopkins University, USA",
        "IBM Research, MIT, USA",
        "University of Hamburg, Germany"
    ]
}