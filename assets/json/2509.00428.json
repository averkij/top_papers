{
    "paper_title": "Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation",
    "authors": [
        "Xuechao Zou",
        "Shun Zhang",
        "Xing Fu",
        "Yue Li",
        "Kai Li",
        "Yushe Cao",
        "Congyan Lang",
        "Pin Tao",
        "Junliang Xing"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Controllable face generation poses critical challenges in generative modeling due to the intricate balance required between semantic controllability and photorealism. While existing approaches struggle with disentangling semantic controls from generation pipelines, we revisit the architectural potential of Diffusion Transformers (DiTs) through the lens of expert specialization. This paper introduces Face-MoGLE, a novel framework featuring: (1) Semantic-decoupled latent modeling through mask-conditioned space factorization, enabling precise attribute manipulation; (2) A mixture of global and local experts that captures holistic structure and region-level semantics for fine-grained controllability; (3) A dynamic gating network producing time-dependent coefficients that evolve with diffusion steps and spatial locations. Face-MoGLE provides a powerful and flexible solution for high-quality, controllable face generation, with strong potential in generative modeling and security applications. Extensive experiments demonstrate its effectiveness in multimodal and monomodal face generation settings and its robust zero-shot generalization capability. Project page is available at https://github.com/XavierJiezou/Face-MoGLE."
        },
        {
            "title": "Start",
            "content": "Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation Xuechao Zou1, Shun Zhang1, Xing Fu2, Yue Li3, Kai Li4, Yushe Cao4, Congyan Lang1, Pin Tao4, Junliang Xing4 1Beijing Jiaotong University, 2Ant Group, 3Qinghai University, 4Tsinghua University Equal contribution. Corresponding authors 5 2 0 2 0 3 ] . [ 1 8 2 4 0 0 . 9 0 5 2 : r Figure 1: Mixture of Global and Local Experts with Diffusion Transformer (Face-MoGLE) is unified and flexible framework for high-quality and controllable face generation. It supports text-to-face synthesis (left), mask-to-face synthesis (right), and multimodal face generation guided jointly by text and masks (middle). By harmonizing global context modeling with local detail refinement, Face-MoGLE produces highly photorealistic results with enhanced semantic consistency and visual fidelity. Abstract Controllable face generation poses critical challenges in generative modeling due to the intricate balance required between semantic controllability and photorealism. While existing approaches struggle with disentangling semantic controls from generation pipelines, we revisit the architectural potential of Diffusion Transformers (DiTs) through the lens of expert specialization. This paper introduces Face-MoGLE, novel framework featuring: (1) Semanticdecoupled latent modeling through mask-conditioned space factorization, enabling precise attribute manipulation; (2) mixture of global and local experts that captures holistic structure and region-level semantics for fine-grained controllability; (3) dynamic gating network producing time-dependent coefficients that evolve with diffusion steps and spatial locations. Face-MoGLE provides powerful and flexible solution for high-quality, controllable face generation, with strong potential in generative modeling and security applications. Extensive experiments demonstrate its effectiveness in multimodal and monomodal face generation settings and its robust zero-shot generalization capability. Project page is available at https://github.com/XavierJiezou/Face-MoGLE. CCS Concepts Information systems Multimedia content creation; Computing methodologies Image processing. Keywords Mixture of Experts, Diffusion Transformer, Face Generation arXiv Preprint, arXiv, 2025 Zou et al."
        },
        {
            "title": "1 Introduction\nFace generation has become a central task in computer vision,\nwith wide-ranging applications in digital content creation [21, 37],\nvirtual reality [15], and human-computer interaction [40]. Beyond\nentertainment, this technology holds significant promise in security\nand public welfare. For example, it can assist criminal investigations\nby synthesizing suspect portraits from forensic sketches or textual\ndescriptions [29], and support the search for missing persons by\nreconstructing plausible appearances from partial visual cues. Gen-\nerating realistic and contextually appropriate faces offers profound\npotential for numerous practical uses.",
            "content": "Face generation involves the core challenges of achieving both high image fidelity and controllability over various facial attributes. Controllable face generation, in particular, focuses on generating facial images that can manipulate specific attributes, such as identity, expression, or appearance. Early efforts in controllable face generation were dominated by generative adversarial networks (GANs) [11, 20, 21], which enable high-resolution synthesis but often suffer from issues such as mode collapse, training instability, and limited adaptability to complex or multi-modal conditions. Flowbased methods [24] offer invertibility and likelihood estimation but fall short in sample quality. Recently, diffusion models [13, 44, 48] have become the de facto standard for high-fidelity image synthesis due to their strong generative performance and compatibility with conditional guidance, making them particularly effective for controllable face generation. Recently, several works [3, 4, 32, 59, 61, 62] have explored fine-tuning pre-trained diffusion models to support conditional inputs such as sketches or semantic maps, further enhancing controllability. Despite the success of diffusion models, most state-of-the-art models rely on U-Net backbones [44], which suffer from two inherent limitations. First, the convolutional inductive bias of U-Nets restricts their ability to model long-range dependencies essential for holistic facial consistency, while their entangled feature representations conflate structural and textural information, hindering precise attribute control. Second, existing methods [17, 33] tightly couple semantic masks with generation by directly concatenating masks and latent codes, which propagates mask errors to texture synthesis and limits fine-grained control over local regions. This coupling also imposes impractical requirements for pixel-perfect masks during inference, compromising zero-shot generalization. These dual limitations motivate our architectural redesign to strengthen global modeling while decoupling semantic guidance from low-level synthesis. While pre-trained foundational diffusion transformers (DiT) [9, 26, 38] have recently demonstrated stronger generalization and higher fidelity compared to U-Net-based diffusion models, their application to face generationespecially under complex, multimodal conditional settingsremains largely underexplored. To address the limitations of existing controllable face generation methods, we propose Face-MoGLE, novel framework that decouples semantic masks into independent binary components, each corresponding to distinct facial attribute such as hair, face contour, or nose. This semantic decoupling enables precise, regionspecific control and lays the foundation for targeted refinement. To effectively model both global structure and local detail, we introduce Mixture of Experts (MoE) design: global experts capture holistic relationships across facial regions (e.g., ensuring spatial alignment between hair and face), while local experts focus on fine-grained features within individual regions (e.g., refining the texture of eyebrows or hair strands). These experts are seamlessly integrated into the Diffusion Transformer (DiT) backbone, which provides powerful foundation for high-fidelity image synthesis and temporal-aware conditioning throughout the denoising process. Together, these components enable Face-MoGLE to achieve both semantically controllable and visually realistic face generation. In summary, our main contributions are as follows: We propose unified and modular generation framework based on the Diffusion Transformer (DiT), which decouples semantic masks into binary components to enable structured and disentangled condition modeling. We design Mixture of Global and Local Experts (MoGLE) architecture, where global experts capture holistic facial structures and local experts refine region-specific details for improved semantic alignment and visual fidelity. We introduce diffusion-aware dynamic gating network that adaptively blends expert outputs with spatial and temporal awareness, enabling fine-grained control throughout the denoising process. Experimental results showed that Face-MoGLE significantly outperformed state-of-the-art (SOTA) controllable face generation models across multiple benchmarks. Compared to strong diffusion baselines such as PixelFace+ [8] and DDGI [23], our model achieved better FID scores and higher condition consistency. We extend the FFHQ-Text [64] dataset with high-quality semantic segmentation masks to enable precise region-level control for multimodal input tasks. Testing on this expanded dataset confirms Face-MoGLEs robust zero-shot generalization. Notably, FaceMoGLE produces images with high perceptual realism that can evade SOTA face forgery detectors, underscoring its promise in generative and security applications. As illustrated in Fig. 1, even without explicit single-modal training, it delivers strong results in Mask2Face and Text2Face tasks without retraining or architectural changes. Overall, by leveraging architectural innovations and explicit condition disentanglement, Face-MoGLE offers powerful and flexible solution for high-quality, controllable face generation."
        },
        {
            "title": "2 Related Work\n2.1 Diffusion Model\nDiffusion models [6, 13, 35] have emerged as a powerful class of\ngenerative models, demonstrating superior performance to GANs\nin image synthesis, which had long been dominated by GAN-based\napproaches [22, 27, 52, 56]. Inspired by the physical diffusion pro-\ncess [47], these models learn to generate data by reversing a gradual\nnoising process. In the forward pass, data is progressively corrupted\nwith Gaussian noise over multiple timesteps, while the model is\ntrained to recover the original sample through a denoising process.\nThe denoising diffusion probabilistic model proposed by Ho et\nal. [13] laid the foundation for this approach with impressive image\nsynthesis performance. Later, Nichol and Dhariwal proposed the\nclassifier-free guidance method [14], which enabled conditional\ngeneration without relying on external classifiers. Recent advance-\nments have transitioned from modeling in pixel space to latent",
            "content": "Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation arXiv Preprint, arXiv, 2025 space. LDM [44] employs U-Net [45] architecture for efficient denoising in compressed latent space. More recent works replace the convolutional U-Net with vision transformers [7, 38], leveraging their global attention mechanisms and geometry-aware positional encodings to capture spatial dependencies better. These Diffusion Transformers (DiTs) [9, 26, 38, 58] have demonstrated strong scalability, with performance improvements that correlate with model capacity and training compute, establishing them as the new stateof-the-art in diffusion-based generation tasks."
        },
        {
            "title": "2.2 Face Generation\nFace generation has become a cornerstone task in computer vision,\nwith increasing demands for controllability and high-fidelity synthe-\nsis. Current approaches can be broadly categorized into GAN-based,\ndiffusion-based, and hybrid methods, and are typically applied to\nunimodal (e.g., mask or text) or multimodal settings.",
            "content": "In unimodal face generation, mask-to-face and text-to-face are two representative tasks. For mask-to-face generation, methods like MaskGAN [27] use semantic masks to enable interactive and diverse facial manipulation. INADE [50] introduces stochastic sampling on class distributions to enhance diversity, while E2Style [55] focuses on efficient and accurate StyleGAN inversion. SemFlow [54] further unifies image synthesis and segmentation using rectified flow, achieving reversible transformations. In text-to-face generation, clip2latent [39] and GCDP [36] employ CLIP-based guidance and semantic layout generation to improve text-image alignment. E3FaceNet [60] additionally introduces 3D awareness and geometric regularization to improve realism and view consistency. In multimodal face generation, the core challenge is aligning diverse conditionssuch as text, masks, or sketcheswhile preserving image fidelity. TediGAN [56] and PixelFace+ [8] enable flexible content creation using both textual and visual inputs. MM2Latent [30] directly maps multimodal signals to the GAN latent space for efficient generation. Diffusion-based methods like Collaborative Diffusion [17] and UaC [33] support plug-and-play multimodal synthesis. DDGI [23] integrates GAN inversion with diffusion features to handle multi-condition face generation. While prior works have made notable progress in specific settings, they often struggle to balance fine-grained semantic control with high-quality synthesis, especially under zero-shot or compositional generalization. In contrast, our proposed Face-MoGLE achieves strong results across unimodal and multimodal tasks."
        },
        {
            "title": "2.3 Mixture of Experts\nThe Mixture of Experts (MoE) model is a neural architecture that\nenhances scalability and specialization by partitioning the input\nspace among several expert networks. Each expert learns to model a\nsubset of the data distribution, while a gating network dynamically\nassigns tokens to relevant experts based on input semantics. The\nconcept was first introduced by Hinton et al. [18], who proposed a\nsupervised learning framework in which each expert specializes\nin a distinct region of the input space. This early work provided a\ntheoretical link between modular neural networks and competitive\nlearning, and laid the foundation for modern sparse expert systems.\nSubsequent advances have integrated MoEs into deep learn-\ning. Notably, Shazeer et al. [46] introduced the Sparsely-Gated",
            "content": "Mixture-of-Experts, demonstrating large-scale training efficiency. GShard [28] and Switch Transformers [10] refined these ideas, improving training stability and enabling trillion-parameter models. In the vision domain, V-MoE [43] brought sparse expert routing into Vision Transformers [7]. Inspired by these works, our Face-MoGLE leverages mixture of global and local experts within diffusion transformer. This enables high-fidelity, controllable face generation by dynamically selecting experts throughout the denoising process."
        },
        {
            "title": "3 Method\n3.1 Overall Pipeline\nWe build upon the DiT [38] architecture, utilizing FLUX [26] as our\nfoundational model, to introduce Mixture of Global and Local Ex-\nperts (MoGLE)‚Äîa simple yet powerful framework for fine-grained\ncontrollable face generation. This design accepts multimodal con-\ntrol conditions, aiming to harmonize global context modeling with\nlocal detail refinement for semantic masks within a unified model.",
            "content": "Figure 2: Training pipeline of the diffusion transformer. Training. The training process is entirely conducted in the latent space, where both the forward diffusion and reverse denoising occur. This design avoids direct modeling in pixel space, which significantly improves computational efficiency [44]. As illustrated in Fig. 2, the model is conditioned on multimodal signals, including text prompt and semantic mask, and is trained to predict the noise added to the latent image tokens during the diffusion process. Let the input text prompt be P, the facial image be Rùêª ùëä 3, and the semantic mask be Rùêª ùëä 3. These inputs are transformed into token sequences via the following encoders: Cùëù = Etext (P), = Eimage (X), Cùëö = Emask (M), (1) arXiv Preprint, arXiv, 2025 Zou et al. where Cùëù Rùêø ùëë , Rùêøùëë , and Cùëö Rùêøùëë denote the token sequences output by the text, image, and mask encoders, respectively. ùêø and ùêø are the token lengths, and ùëë, ùëë are the embedding dimensions. The text encoder Etext jointly leverages CLIP [41] and H5 [42], combining strong generalization with stylistic richness. The image encoder Eimage is based on the encoder of pretrained VAE [25, 53], which maps the image from pixel space to latent token representations. Both encoders are kept frozen during training. In contrast, the mask encoder Emask is carefully designed component intended to enhance fine-grained and structured semantic control. We propose Mixture of Global and Local Experts (MoGLE) architecture to obtain rich and flexible token-level representations from semantic masks. This design addresses the lack of spatial controllability in pretrained text-to-image generation DiTs [26, 44]. Details can be found in Section 3.2. At each training step, timestep ùë° {1, . . . ,ùëá } is sampled, and Gaussian noise is added to the image tokens: (2) Zùë° = ùõºùë° + ùùê (0, I), 1 ùõºùë° ùùê, where ùõºùë° denotes the cumulative product of the forward noise schedule. The resulting noisy tokens Zùë° Rùêøùëë are fed into the denoising network. To improve robustness, we apply drop probability of 0.1 independently to each condition (i.e., text prompt or semantic mask) during training, following previous works [51, 58, 59]. This allows the model to gracefully handle cases where one or both modalities are absent (i.e., set to ), thereby enabling flexible and controllable face generation. Our denoising module is diffusion transformer composed of ùëÅ stacked blocks, each consisting of frozen transformer backbone and trainable Low-Rank Adaptation (LoRA) [16, 51] modules for efficient fine-tuning: ÀÜùùê = ùëìùúÉ (cid:0)Zùë° , ùë°, Cùëù, Cùëö (cid:1) , where ùëìùúÉ denotes the denoising network, with only the LoRA modules updated during training. The model is optimized by minimizing the mean squared error between the predicted and true noise: (3) LMSE = Eùë°,Z,ùùê (cid:2) ÀÜùùê ùùê 2 2 (cid:3) . (4) This objective guides the model to iteratively denoise latent image tokens while attending to textual and semantic signals. Sampling. During inference, we adopt an improved sampling procedure [9, 26, 51] to iteratively denoise latent image tokens, initialized from pure Gaussian noise. The text prompt and semantic mask are encoded the same way as during training, using the frozen text encoder and the trained mask encoder, respectively. These condition tokens guide the denoising process toward generating semantically faithful and structurally aligned images. To support flexible generation, our model allows either condition to be dropped at test time by setting it to an empty input. This mirrors the condition drop strategy used during training and enables diverse use cases, such as semantic face synthesis without text, or text-to-face generation from text alone. After denoising completes, the generated latent tokens are decoded into the final image using the pretrained VAE [25, 53] decoder. The output image reflects highlevel semantics and spatial structure derived from the conditioning inputs, enabling high-quality and controllable face generation."
        },
        {
            "title": "3.2 Mixture of Global and Local Experts",
            "content": "Global and Local Experts. To obtain expressive and controllable semantic representations from the input mask, we design Mixture of Global and Local Experts architecture, as shown in Fig. 3. The motivation is twofold: (1) to extract global structural priors from the full-face layout; and (2) to model region-specific semantics for enhanced controllability and fidelity in face generation. Given semantic mask Rùêª ùëä 3, we first decouple it into ùëõ binary masks {M(ùëñ ) }ùëõ ùëñ=1, each representing semantic region (e.g., face, hair, nose). All masks are passed through shared frozen VAE encoder EVAE [25, 53], producing sequence of latent tokens: (ùëñ ) ùëö = EVAE (M(ùëñ ) ) Rùêøùëë, (5) where M(0) denotes the full mask, used to derive global context. (ùëñ ) ùëö is processed by its corresponding expert: Each token sequence ùëñ = 0, 1, . . . , ùëõ (ùëñ ) ùëö = Expertùëñ (C (ùëñ ) ùëö ) Rùêøùëë . The global expert captures high-level spatial priors, while local experts focus on fine-grained, region-specific semantics. This cooperative modeling enables the system to maintain structural consistency while improving the fidelity of generated faces. ùëñ = 0, 1, . . . , ùëõ (6) Dynamic Gating Network. To dynamically integrate expert outputs across the diffusion process, we introduce diffusion-aware gating network ùëîùúÉ , illustrated in Fig. 4. This module takes the current noisy latent tokens Zùë° , learned timestep embedder Etime (ùë°), (0) ùëö , and produces normalized weights: and the global mask token [ùúî (t) ] = ùëîùúÉ (Zùë° , Etime (ùë°), (0) ùëö ) , ùúî (t) s.t. ùúî (t) + ùúî (t) = 1, ùúî (t) , ùúî (t) [0, 1] (7) , . . . , ùúî (t) ùëõ ùëñ=1 and ùúî (t) where ùúî (t) denote the spatial weight maps for the global expert and the ùëñ-th local expert at timestep ùë°, respectively. Unlike static fusion, our gating mechanism produces spatially varying weights that evolve during the denoising process. The final semantic embedding is computed as: Cùëö = ùúî (t) (0) ùëö + ùëõ ùëñ=1 ùúî (t) (ùëñ ) ùëö . (8) To better understand the gating behavior, we visualize the spatial weight maps predicted for both global and selected local experts in Fig. 5. These maps reveal that different semantic regions are adaptively emphasized at stages of the diffusion process, validating the gating networks semantic awareness and spatial adaptivity."
        },
        {
            "title": "4 Experiments\n4.1 Datasets\nIn this study, we employ two benchmark datasets: MM-CelebA-\nHQ [56], an enriched version of CelebAMask-HQ [27] featuring\nhigh-resolution facial images annotated with attributes, utilized for\nboth training and evaluation; and FFHQ-Text [64], a meticulously\ncurated subset of FFHQ [22], comprising high-quality images of\nfemale faces accompanied by nuanced textual descriptions. We\nfurther refine this dataset into a multimodal variant, referred to as\nMM-FFHQ-Female, designed explicitly for zero-shot evaluation. In",
            "content": "Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation arXiv Preprint, arXiv, 2025 Figure 3: Architecture of the Mixture of Global and Local Experts (MoGLE) designed for semantic mask embedding. accordance with [17], the initial 27,000 pairs from MM-CelebA-HQ are allocated for training, while the remaining 3,000 serve as the test set. To produce semantic masks for FFHQ-Text, we leverage two pretrained facial parsing modelsFaRL [63] and SegFace [34]. For masks with overall accuracy (OA) below 0.8, we conduct manual annotation, whereas for those achieving OA 0.8, we adopt randomized sampling strategy, comprising 90% from FaRL and 10% from SegFace. More details of datasets are available in Appendix B."
        },
        {
            "title": "4.3 Evaluation Metrics",
            "content": "Image Quality . The visual fidelity of generated images is crucial indicator of model performance. We employ the following Figure 4: Structure of our dynamic gating network. Figure 5: Visualization of global and partial local weight map. arXiv Preprint, arXiv, 2025 Zou et al. Table 1: Comparison of face generation methods on the MM-CelebA-HQ dataset across different tasks. Method Venue Paradigm Image Generation Quality Condition Alignment FID KID CMMD Mask Text Multimodal Face Generation TediGAN [56] UaC [33] Collaborative [17] PixelFace+ [8] DDGI [23] Face-MoGLE (Ours) CVPR-21 CVPR-23 CVPR-23 ACM MM-23 CVPR-24 - GAN Diffusion Diffusion GAN GAN & Diffusion Diffusion 83.35 75.35 24.48 65.53 46.68 74.75 63.78 13.50 53.90 - 1.562 1.982 0.734 1.273 - 22.24 10.87 0. INADE [50] E2Style [55] SemFlow [54] Face-MoGLE (Ours) CVPR-21 TIP-22 NeurIPS-24 - Mask-to-Face Generation GAN GAN Flow Diffusion 21.09 38.44 56.65 11.24 21.22 41. 19.63 8.29 Text-to-Face Generation clip2latent [39] GCDP [36] E3-FaceNet [60] Face-MoGLE (Ours) BMVC-22 ICCV-23 ICML-24 - GAN Diffusion GAN Diffusion 63.89 72.67 70.89 38.55 43.81 47.82 34.81 21.85 0.636 1.871 1.129 1. 0.399 1.410 1.456 2.749 2.46 3.41 3.22 2.61 - 2.44 2.57 2.36 2.30 2. 5.05 4.68 4.64 4.94 23.90 25.52 24.51 26.16 - 26.32 24.85 24.75 25.65 24. 27.56 25.94 27.94 26.91 IR -0.1446 -0.4001 -0.1265 0.6403 - 0.7014 -0.0234 0.1649 -0. 0.0398 1.0129 0.4563 0.8150 0.9527 Figure 6: Visualization results from different methods. The figure compares three generation paradigms: multimodal face synthesis (left) and two unimodal tasks mask-to-face and text-to-face generation (middle and right). From top to bottom, each row highlights attribute-specific synthesis: straight hair, hat, lipstick, necklace, and earrings. Our method demonstrates superior alignment with the textual descriptions, semantic mask, and higher visual fidelity compared to other baseline models. Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation arXiv Preprint, arXiv, 2025 Table 2: Comparison of multimodal face generation methods on the MM-FFHQ-Female dataset under zero-shot setting. Method Venue Paradigm Image Generation Quality Condition Alignment FID KID CMMD Mask Text TediGAN [56] UaC [33] Collaborative [17] PixelFace+ [8] Face-MoGLE (Ours) CVPR-21 CVPR-23 CVPR-23 ACM MM-23 - GAN Diffusion Diffusion GAN Diffusion 122.47 86.58 93.31 76.45 92.72 46.57 62.84 38.95 62. 31.27 1.091 1.796 2.178 1.917 1.238 3.32 3.11 3.85 3.08 2.77 25.03 26.97 23.03 26.60 28. IR -0.4847 -0.8851 -1.2101 -0.2616 0.1801 widely adopted metrics: Fr√©chet Inception Distance (FID) [12] quantifies the distributional discrepancy between generated and real images in the Inception feature space, with lower values indicating higher quality. Kernel Inception Distance (KID) [2], similar in spirit to FID, relies on kernel-based methods to provide an unbiased and more stable estimate. For readability, we multiply the KID by 1,000. CLIP Maximum Mean Discrepancy (CMMD) [19] measures the alignment of conditional distributions, making it particularly suitable for conditional generation tasks. Text Consistency. To evaluate how well the generated images align semantically with the input textual descriptions, we utilize the CLIP Score [41]. This metric leverages the CLIP vision-language model to compute similarity between text and image embeddings, with higher scores indicating more substantial semantic alignment. For better readability, we report the scores in percentage format. Mask Consistency. We use DINO Structure Distance to measure how well the generated image aligns with source images under the guidance of the input semantic mask, which compares the selfsimilarity matrices of features from the DINO-ViT [5]. smaller value indicates higher mask (structural) consistency. Human Preference. To reflect subjective human perception, we incorporate the Image Reward (IR) score [57], derived from learned aesthetic scoring model. This metric estimates the visual appeal and coherence of generated images and has been shown to correlate closely with human judgments. Deepfake Detection. To further assess the realism of our synthesized facial images, we evaluate their ability to evade deepfake detection. It is measured using Area Under the Receiver Operating Characteristic Curve (AUC), Equal Error Rate (EER), and Average Precision (AP) [1, 49]. AUC and EER values close to 0.5 indicate increased similarity to authentic images and detector confusion, while lower AP reflects reduced confidence in fake identification."
        },
        {
            "title": "4.4 Comparison with State-of-the-Art Methods\nTo evaluate the effectiveness of Face-MoGLE, we conduct compre-\nhensive comparisons with representative state-of-the-art methods\nacross three face generation tasks. As shown in Table 1, our model\nconsistently performs well across key metrics of generated image\nquality, condition alignment, and human perceptual preference.",
            "content": "4.4.1 Multimodal Face Generation. Face-MoGLE achieves the best results in FID (22.24), KID (10.87), CMMD (0.477), mask alignment (2.44), and also ranks first in text alignment (26.32), indicating strong generation fidelity and semantic controllability. Since DDGI [23] has not released its code, we directly copy the results reported in its paper. Compared to the strongest diffusion-based baseline Collaborative [17], our method brings consistent improvements across all metrics, validating the effectiveness of the proposed framework. 4.4.2 Monomodal Face Generation. Mask2Face Generation. For mask-to-face generation, Face-MoGLE again achieves the best results in FID (19.63), KID (8.29), and CMMD (0.399). The mask alignment score (2.57) matches top-performing methods, demonstrating the models strength in preserving spatial structure while generating perceptually compelling faces. Text2Face Generation. In the text-to-face setting, Face-MoGLE achieves superior performance in terms of generation fidelity and condition consistency. Specifically, it obtains the best results across all image quality metrics, with an FID of 34.81, KID of 21.85, and CMMD of 0.636. Compared to other methods, Face-MoGLE is more effective at generating realistic and condition-consistent facial images from text descriptions, highlighting the effectiveness of our diffusion-based approach. 4.4.3 Visualization and Human Preference. We use the IR score [57] to assess the perceptual quality of the generated faces. As shown in Table 1, Face-MoGLE achieves the highest IR scores in multimodal (0.7014) and mask-to-face (0.0398) generation, and ranks second in text-to-face (0.9527), indicating that the generated images are both semantically aligned and preferred by human observers. Fig. 6 highlights these results, where Face-MoGLE effectively resolves cross-modal conflicts and generates more natural features in multimodal and mask-to-face tasks. Although slightly behind in text-to-face IR, it achieves better geometric accuracy and visualtextual consistency, striking balance between structural fidelity and semantic alignment. More results are available in Appendix C."
        },
        {
            "title": "4.5 Ablation Studies\n4.5.1 Effect of Harmonizing Global and Local Experts. We conduct\nablation studies on three configurations: Global Expert (holistic\nsemantic mask), Local Experts (decoupled binary masks), and the\nCombined Global + Local Experts, as summarized in Table 3. The\nGlobal Expert alone yields a moderate FID of 30.36 but shows poor\nsemantic mask alignment (Mask: 2.47). In contrast, Local Experts,\nwhile achieving the best text alignment (Text: 27.07) due to precise",
            "content": "arXiv Preprint, arXiv, 2025 Zou et al. Table 3: Effect of harmonizing global and local experts. Expert Composition FID KID Mask Text 26.30 Only Global Only Local 27.07 26.32 Global & Local 18.16 20.45 10.87 30.36 33.62 22. 2.47 4.87 2.44 mapping between facial regions and textual semantics, suffer from the highest FID (33.62) due to the lack of holistic spatial context. Our unified framework, which dynamically integrates global and local features via gating network, outperforms both, achieving significantly improved FID of 22.24 and enhanced mask consistency (2.44). These results highlight the effectiveness of hierarchical expert fusion: global experts ensure topological coherence, while local experts provide fine-grained semantic control, jointly promoting visual fidelity and structural precision. Table 4: Impact of various gating mechanisms. Gating Mechanism FID KID Mask Text 26.53 w/o Diffusion Scalar Gating 26.72 26.32 Matrix Gating 2.37 4.74 2.44 25.74 43.48 22. 13.12 30.58 10.87 Impact of Various Gating Mechanisms. We evaluate three 4.5.2 gating strategies: static weights, time-dependent scalar weights, and our spatiotemporal matrix weights. As Table 4 shows, matrix gating achieves the best FID (22.24) and KID (10.87), outperforming scalar gating by 48.9% and static weights by 13.6%. The severe degradation under scalar gating (FID:43.48) stems from its inability to handle spatial conflicts, whereas static weights (FID:25.74) lack temporal adaptability across diffusion stages. Our method resolves these through pixel-wise weight maps that evolve spatially and temporally, achieving optimal balance between semantic control (Mask:2.44) and photorealism. This conclusively demonstrates the necessity of spatiotemporal dynamics in expert fusion. Table 5: Joint contribution of expert and gating. Expert Gating FID KID Mask Text 33.25 26.55 31.30 22.24 23.07 14.87 19.11 10.87 26.71 26.38 26.64 26.32 2.49 3.20 2.60 2.44 Joint Contribution of Expert and Gating. We ablate the in4.5.3 dividual and joint effects of global-local experts and dynamic gating. As shown in Table 5, using only experts (FID:26.55) or gating (FID:31.30) yields partial improvements over the baseline (FID:33.25), while their combined use achieves optimal FID (22.24). This synergy arises because experts decompose facial semantics into multiple binary components (mask error drops from 3.20 to 2.44), while the gating network dynamically aligns these components across space and time. Although the baseline shows marginally higher text alignment (Text:26.71 vs. 26.32), its overly smoothed outputs lack semantic precision, whereas our full model balances photorealism and control. This validates that global-local experts and adaptive gating are mutually essential for high-fidelity and controllable generation. 4.5.4 Zero-Shot Generalization Validation. As shown in Table 2, Face-MoGLE achieves superior zero-shot generalization on the MMFFHQ-Female dataset, outperforming existing methods on most metrics. Our framework attains state-of-the-art image quality (FID: 62.93, KID: 31.27), mask consistency (Mask: 2.77), text alignment (CLIP: 28.06), and human preference (Image Reward: 0.1801). While TediGAN shows better CMMD performance (1.091 vs. ours 1.238), this can be attributed to its StyleGAN backbone being pre-trained on the FFHQ dataset. Compared to the best diffusion-based baseline (UaC), our method reduces FID by 27.3% and KID by 32.9%, demonstrating the effectiveness of our mask-decoupling strategy and global-local MoE architecture. The dynamic gating network enables adaptive feature fusion during the diffusion process, contributing to robust performance on unseen semantic combinations. These results validate Face-MoGLEs capability to synthesize high-fidelity faces under zero-shot conditions without task-specific fine-tuning. 4.5.5 Evaluation with Deepfake Detection. We evaluate the ability of our synthesized faces to evade deepfake detectors, as shown in Table 6. Specifically, we test against NPR [49], general-purpose detector, and Wavelet-CLIP [1], which focuses on face-specific artifacts. Face-MoGLE achieves near-random AUC on NPR (0.50 vs. Collaboratives 0.51) and significantly outperforms Collaborative on Wavelet-CLIP (0.46 vs. 0.75). In light of these results, we stress that this evaluation is conducted solely for defensive research purposes. Societal impacts and responsible AI are discussed in Appendix A. Table 6: Performance comparison of two deepfake detection models against different face generation methods. Each cell is shown as NPR [49] / Wavelet-CLIP [1] detection results. Method TediGan [56] UaC [33] Collaborative [17] PixelFace+ [8] Face-MoGLE (Ours) AUC 0.73 / 0.81 0.45 / 0.96 0.51 / 0.75 0.28 / 0.87 0.50 / 0.46 EER 0.35 / 0.26 0.53 / 0.11 0.51 / 0.32 0.65 / 0.20 0.50 / 0. AP 0.71 / 0.76 0.43 / 0.96 0.50 / 0.79 0.37 / 0.89 0.52 / 0."
        },
        {
            "title": "5 Conclusion\nIn this work, we introduced a mixture of global and local experts\nwith a diffusion transformer for controllable face generation. Our\nmethod effectively decouples semantic mask information and dy-\nnamically selects experts to enhance image synthesis fidelity and\ncondition alignment. Through extensive experiments, we demon-\nstrated improvements in multimodal and monomodal generation\nand robust generalization. Future work may explore more efficient\narchitectures and applications in real-world scenarios.",
            "content": "Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation arXiv Preprint, arXiv, 2025 References [1] Lalith Bharadwaj Baru, Rohit Boddeda, Shilhora Akshay Patel, and Sai Mohan Gajapaka. 2025. Wavelet-Driven Generalizable Framework for Deepfake Face Forgery Detection. In WACV. 16611669. [2] Miko≈Çaj Bi≈Ñkowski, Dougal J. Sutherland, Michael Arbel, and Arthur Gretton. 2018. Demystifying MMD GANs. In ICLR. 136. [3] Bocheng, YuhangMa, wuliebucha, Shanyuan Liu, Ao Ma, Xiaoyu Wu, Dawei Leng, and Yuhui Yin. 2024. HiCo: Hierarchical Controllable Diffusion Model for Layout-to-image Generation. In NeurIPS. [4] Tim Brooks, Aleksander Holynski, and Alexei Efros. 2023. Instructpix2pix: Learning to follow image editing instructions. In CVPR. 1839218402. [5] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv√© J√©gou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. 2021. Emerging Properties in Self-Supervised Vision Transformers. In ICCV. 96509660. [6] Prafulla Dhariwal and Alexander Nichol. 2021. Diffusion models beat gans on image synthesis. In NeurIPS, Vol. 34. 87808794. [7] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. 2021. An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale. In ICLR. 121. [8] Xiaoxiong Du, Jun Peng, Yiyi Zhou, Jinlu Zhang, Siting Chen, Guannan Jiang, Xiaoshuai Sun, and Rongrong Ji. 2023. PixelFace+: Towards Controllable Face Generation and Manipulation with Text Descriptions and Segmentation Masks. In ACM MM. 46664677. [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas M√ºller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, Dustin Podell, Tim Dockhorn, Zion English, and Robin Rombach. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In ICML. 28 pages. [10] William Fedus, Barret Zoph, and Noam Shazeer. 2022. Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity. JMLR 23, 120 (2022), 139. [11] Erik H√§rk√∂nen, Aaron Hertzmann, Jaakko Lehtinen, and Sylvain Paris. 2020. Ganspace: Discovering interpretable gan controls. In NeurIPS, Vol. 33. 98419850. [12] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. 2017. GANs trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS. 66296640. [13] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. In NeurIPS, Vol. 33. 68406851. [14] Jonathan Ho and Tim Salimans. 2021. Classifier-Free Diffusion Guidance. In NeurIPS. 18. [15] Fangzhou Hong, Mingyuan Zhang, Liang Pan, Zhongang Cai, Lei Yang, and Ziwei Liu. 2022. AvatarCLIP: Zero-Shot Text-Driven Generation and Animation of 3D Avatars. TOG 41, 4, Article 161 (2022), 19 pages. [16] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. 2022. LoRA: Low-Rank Adaptation of Large Language Models. In ICLR. 113. [17] Ziqi Huang, Kelvin CK Chan, Yuming Jiang, and Ziwei Liu. 2023. Collaborative diffusion for multi-modal face generation and editing. In CVPR. 60806090. [18] Robert Jacobs, Michael Jordan, Steven Nowlan, and Geoffrey Hinton. 1991. Adaptive mixtures of local experts. Neural computation 3, 1 (1991), 7987. [19] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. 2024. Rethinking FID: Towards Better Evaluation Metric for Image Generation . In CVPR. 93079315. [20] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2018. Progressive Growing of GANs for Improved Quality, Stability, and Variation. In ICLR. [21] Tero Karras, Samuli Laine, and Timo Aila. 2019. style-based generator architecture for generative adversarial networks. In CVPR. 44014410. [22] Tero Karras, Samuli Laine, and Timo Aila. 2019. Style-Based Generator Architecture for Generative Adversarial Networks. In CVPR. 44014410. [23] Jihyun Kim, Changjae Oh, Hoseok Do, Soohyun Kim, and Kwanghoon Sohn. 2024. Diffusion-driven gan inversion for multi-modal face image generation. In CVPR. 1040310412. [24] Durk Kingma and Prafulla Dhariwal. 2018. Glow: Generative flow with invertible 1x1 convolutions. In NeurIPS, Vol. 31. 110. [25] Diederik Kingma and Max Welling. 2014. Auto-encoding variational {Bayes}. In ICLR. 114. [26] Black Forest Labs. 2024. FLUX. https://github.com/black-forest-labs/flux. [27] Cheng-Han Lee, Ziwei Liu, Lingyun Wu, and Ping Luo. 2020. Maskgan: Towards diverse and interactive facial image manipulation. In CVPR. 55495558. [28] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. 2021. {GS}hard: Scaling Giant Models with Conditional Computation and Automatic Sharding. In ICLR. 123. [29] Fang Liu, Xiaoming Deng, Yu-Kun Lai, Yong-Jin Liu, Cuixia Ma, and Hongan Wang. 2019. SketchGAN: Joint Sketch Completion and Recognition With Generative Adversarial Network. In CVPR. [30] Debin Meng, Christos Tzelepis, Ioannis Patras, and Georgios Tzimiropoulos. 2024. MM2Latent: Text-to-facial image generation and editing in GANs with multimodal assistance. In ECCV. 120. [31] Konstantin Mishchenko and Aaron Defazio. 2024. Prodigy: An Expeditiously Adaptive Parameter-Free Learner. In ICML. 3577935804. [32] Chong Mou, Xintao Wang, Liangbin Xie, Yanze Wu, Jian Zhang, Zhongang Qi, and Ying Shan. 2024. T2i-adapter: Learning adapters to dig out more controllable ability for text-to-image diffusion models. In AAAI, Vol. 38. 42964304. [33] Nithin Gopalakrishnan Nair, Wele Gedara Chaminda Bandara, and Vishal Patel. 2023. Unite and conquer: Plug & play multi-modal synthesis using diffusion models. In CVPR. 60706079. [34] Kartik Narayan, Vibashan VS, and Vishal Patel. 2024. Segface: Face segmentation of long-tail classes. arXiv preprint arXiv:2412.08647 (2024). [35] Alexander Quinn Nichol and Prafulla Dhariwal. 2021. Improved denoising diffusion probabilistic models. In ICML. 81628171. [36] Minho Park, Jooyeol Yun, Seunghwan Choi, and Jaegul Choo. 2023. Learning to generate semantic layouts for higher text-image correspondence in text-to-image synthesis. In ICCV. 75917600. [37] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. 2021. Styleclip: Text-driven manipulation of stylegan imagery. In ICCV. 2085 2094. [38] William Peebles and Saining Xie. 2023. Scalable diffusion models with transformers. In ICCV. 41954205. [39] Justin N. M. Pinkney and Chuan Li. 2022. clip2latent: Text driven sampling of pre-trained StyleGAN using denoising diffusion and CLIP. In BMVC. 112. [40] Prajwal, Rudrabha Mukhopadhyay, Vinay P. Namboodiri, and C.V. Jawahar. 2020. Lip Sync Expert Is All You Need for Speech to Lip Generation In the Wild. In ACM MM. 484492. [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. 2021. Learning Transferable Visual Models From Natural Language Supervision. In ICML, Vol. 139. 87488763. [42] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2020. Exploring the Limits of Transfer Learning with Unified Text-to-Text Transformer. JMLR 21, 140 (2020), 167. [43] Carlos Riquelme, Joan Puigcerver, Basil Mustafa, Maxim Neumann, Rodolphe Jenatton, Andr√© Susano Pinto, Daniel Keysers, and Neil Houlsby. 2021. Scaling vision with sparse mixture of experts. In NeurIPS, Vol. 34. 85838595. [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bj√∂rn Ommer. 2022. High-Resolution Image Synthesis With Latent Diffusion Models. In CVPR. 1068410695. [45] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. 2015. U-net: Convolutional networks for biomedical image segmentation. In MICCAI. 234241. [46] Noam Shazeer, *Azalia Mirhoseini, *Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. 2017. Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer. In ICLR. 119. [47] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. 2015. Deep unsupervised learning using nonequilibrium thermodynamics. In ICML. 22562265. [48] Jiaming Song, Chenlin Meng, and Stefano Ermon. 2021. Denoising Diffusion Implicit Models. In ICLR. 120. [49] Chuangchuang Tan, Yao Zhao, Shikui Wei, Guanghua Gu, Ping Liu, and Yunchao Wei. 2024. Rethinking the up-sampling operations in cnn-based generative network for generalizable deepfake detection. In CVPR. 2813028139. [50] Zhentao Tan, Menglei Chai, Dongdong Chen, Jing Liao, Qi Chu, Bin Liu, Gang Hua, and Nenghai Yu. 2021. Diverse semantic image synthesis via probability distribution modeling. In CVPR. 79627971. [51] Zhenxiong Tan, Songhua Liu, Xingyi Yang, Qiaochu Xue, and Xinchao Wang. 2024. Ominicontrol: Minimal and universal control for diffusion transformer. arXiv preprint arXiv:2411.15098 3 (2024). [52] Hao Tang, Dan Xu, Yan Yan, Philip H.S. Torr, and Nicu Sebe. 2020. Local ClassSpecific and Global Image-Level Generative Adversarial Networks for SemanticGuided Scene Generation. In CVPR. 78707879. [53] Aaron van den Oord, Oriol Vinyals, and Koray Kavukcuoglu. 2017. Neural discrete representation learning. In NeurIPS. 63096318. [54] Chaoyang Wang, Xiangtai Li, Lu Qi, Henghui Ding, Yunhai Tong, and MingHsuan Yang. 2024. Semflow: Binding semantic segmentation and image synthesis via rectified flow. In NeurIPS, Vol. 37. 138981139001. [55] Tianyi Wei, Dongdong Chen, Wenbo Zhou, Jing Liao, Weiming Zhang, Lu Yuan, Gang Hua, and Nenghai Yu. 2022. E2Style: Improve the efficiency and effectiveness of StyleGAN inversion. TIP 31 (2022), 32673280. [56] Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu. 2021. Tedigan: Textguided diverse face image generation and manipulation. In CVPR. 22562265. [57] Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. 2023. ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation. In NeurIPS, Vol. 36. 1590315935. arXiv Preprint, arXiv, 2025 Zou et al. [58] Sicheng Xu, Guojun Chen, Yu-Xiao Guo, Jiaolong Yang, Chong Li, Zhenyu Zang, Yizhong Zhang, Xin Tong, and Baining Guo. 2024. Vasa-1: Lifelike audio-driven talking faces generated in real time. In NeurIPS, Vol. 37. 660684. [59] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. 2023. Ip-adapter: Text compatible image prompt adapter for text-to-image diffusion models. arXiv preprint arXiv:2308.06721 (2023). [60] Jinlu Zhang, Yiyi Zhou, Qiancheng Zheng, Xiaoxiong Du, Gen Luo, Jun Peng, Xiaoshuai Sun, and Rongrong Ji. 2024. Fast text-to-3D-aware face generation and manipulation via direct cross-modal mapping and geometric regularization. In ICML. 6060560625. [61] Kai Zhang, Yawei Li, Wangmeng Zuo, Lei Zhang, Luc Van Gool, and Radu Timofte. 2022. Plug-and-Play Image Restoration With Deep Denoiser Prior. TPAMI 44, 10 (2022), 63606376. [62] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. 2023. Adding conditional control to text-to-image diffusion models. In ICCV. 38363847. [63] Yinglin Zheng, Hao Yang, Ting Zhang, Jianmin Bao, Dongdong Chen, Yangyu Huang, Lu Yuan, Dong Chen, Ming Zeng, and Fang Wen. 2022. General facial representation learning in visual-linguistic manner. In CVPR. 1869718709. [64] Yutong Zhou. 2021. Generative adversarial network for text-to-face synthesis and manipulation. In ACM MM. 29402944."
        },
        {
            "title": "A Societal Impacts and Responsible AI",
            "content": "Our research focuses on controllable face generation, based on diffusion transformer architecture combined with mixture of global and local experts, aiming to support variety of optimistic application scenarios. This technology is not intended to mislead or deceive. However, similar to other generative models, it may still be misused for impersonating individuals. We strongly oppose any behavior that produces deceptive or harmful facial content. While acknowledging the potential risks of misuse, we also recognize the significant positive potential of this technology. Our method can be broadly applied in digital creativity, virtual human interaction, and personalized content generation. Additionally, it holds promise in public-interest applications such as generating portraits of missing children or criminal suspects (e.g., by reconstructing facial contours from the semantic mask and inferring other attributes from textual descriptions), thereby contributing to public safety and social welfare. We are committed to the responsible development of AI technologies that benefit humanity. To mitigate potential misuse and provide necessary safeguards, we are also exploring the application of our method in advancing face forgery detection. Specifically, we use the generated facial images as training data to support the development of generalpurpose face forgery detection models. Preliminary experiments show that incorporating data generated by our method improves the generalization ability of these models. We will continue to share our latest progress with the research community actively."
        },
        {
            "title": "B Dataset Details",
            "content": "B.1 MM-CelebA-HQ This dataset contains 30,000 high-resolution facial images, each annotated with corresponding semantic segmentation map and ten natural language descriptions. The semantic segmentation maps label each pixel into one of 19 categories, including background, face skin, nose, eyeglasses, left eye, right eye, left eyebrow, right eyebrow, left ear, right ear, inner mouth, upper lip, lower lip, hair, hat, earring, necklace, neck, and clothing. These segmentation labels provide pixel-level structural and contextual information useful for supervised learning and evaluation in image generation tasks. Each image is also paired with 10 unique text descriptions that capture detailed visual characteristics, such as facial features, expressions, accessories, age, and gender. During training, one of the 10 descriptions is randomly selected, while during testing, the first description is always used to ensure consistency. The dataset serves as comprehensive benchmark for text-to-image generation and multimodal learning, supporting tasks such as conditional image synthesis, semantic-guided generation, and multimodal learning. B.2 MM-FFHQ-Female FFHQ-Text [64] is smaller-scale but highly specialized dataset that comprises 760 high-quality female face images from the FFHQ (Flickr-Faces-HQ) [22] dataset. Each image is paired with 9 distinct natural language descriptions, detailing fine-grained facial attributes such as makeup style, hairstyle, facial expression, skin tone, and accessories. These descriptions emphasize subtle details and variations, making the dataset particularly suitable for evaluating text-to-image generation and manipulation tasks that require high sensitivity to nuanced text cues. To produce semantic masks for FFHQ-Text, we leverage two pretrained facial parsing modelsFaRL [63] and SegFace [34]. For masks with overall accuracy (OA) below 0.8, we conduct manual annotation, whereas for those achieving OA 0.8, we adopt randomized sampling strategy, comprising 90% from FaRL and 10% from SegFace. randomly sampled textual description for each image is used during zero-shot evaluation to ensure consistency across experiments. The combination of detailed textual annotations, semantic masks, and high-resolution facial images enables comprehensive studies on fine-level semantic alignment and learning in multimodal models. We will release this dataset to promote community development."
        },
        {
            "title": "C More Results",
            "content": "C.1 Multimodal Face Generation Fig. 7 demonstrates the comparative results of multimodal face generation between our method and several state-of-the-art multimodal generation methods. As shown, our method consistently produces more realistic and semantically faithful faces, effectively integrating multiple modalities such as text descriptions and segmentation masks. Noticeably, our generated faces exhibit finer facial details and more accurate modality alignment than other methods. C.2 Mask-to-Face Generation In Fig. 8, we present comprehensive comparative visualization of mask-to-face generation performance. Although our method is not explicitly designed for the mask-to-face generation task, it still achieves commendable structural alignment with the input semantic masks, yielding compelling images of superior fidelity. C.3 Text-to-Face Generation Fig. 9 illustrates the comparative results of text-to-face generation methods. It is evident from the examples provided that our method surpasses previous techniques in capturing subtle textual cues and translating them accurately into visual facial features. Compared to Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation arXiv Preprint, arXiv, 2025 Figure 7: Comparative results of multimodal face generation on the MM-CelebA-HQ dataset. other methods, our generated faces better reflect the described attributes, demonstrating notable improvement in text consistency. C.4 Zero-shot Generalization MM-FFHQ-Female. As shown in Fig. 10, our method produces significantly more faithful and realistic generations than baseline methods. The results demonstrate consistency with the input prompts while preserving fine-grained semantic structures with the mask. C.5 Ablation Studies Fig. 11 illustrates the visual results of our ablation studies. The figure includes variations such as Only Global, Only Local, w/o Diffusion, Scalar Gating, and our final best-performing method. arXiv Preprint, arXiv, 2025 Zou et al. Figure 8: Comparative results of mask-to-face generation on the MM-CelebA-HQ dataset. Mixture of Global and Local Experts with Diffusion Transformer for Controllable Face Generation arXiv Preprint, arXiv, 2025 Figure 9: Comparative results of text-to-face generation on the MM-CelebA-HQ dataset. arXiv Preprint, arXiv, 2025 Zou et al. Figure 10: Comparative results of zero-shot generalization on the MM-FFHQ-Female dataset. Figure 11: Comparative results of ablation studies on the MM-CelebA-HQ dataset."
        }
    ],
    "affiliations": [
        "Ant Group",
        "Beijing Jiaotong University",
        "Qinghai University",
        "Tsinghua University"
    ]
}