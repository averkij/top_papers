{
    "paper_title": "MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D",
    "authors": [
        "Wei Cheng",
        "Juncheng Mu",
        "Xianfang Zeng",
        "Xin Chen",
        "Anqi Pang",
        "Chi Zhang",
        "Zhibin Wang",
        "Bin Fu",
        "Gang Yu",
        "Ziwei Liu",
        "Liang Pan"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Texturing is a crucial step in the 3D asset production workflow, which enhances the visual appeal and diversity of 3D assets. Despite recent advancements in Text-to-Texture (T2T) generation, existing methods often yield subpar results, primarily due to local discontinuities, inconsistencies across multiple views, and their heavy dependence on UV unwrapping outcomes. To tackle these challenges, we propose a novel generation-refinement 3D texturing framework called MVPaint, which can generate high-resolution, seamless textures while emphasizing multi-view consistency. MVPaint mainly consists of three key modules. 1) Synchronized Multi-view Generation (SMG). Given a 3D mesh model, MVPaint first simultaneously generates multi-view images by employing an SMG model, which leads to coarse texturing results with unpainted parts due to missing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete 3D texturing, we introduce the S3I method, specifically designed to effectively texture previously unobserved areas. 3) UV Refinement (UVR). Furthermore, MVPaint employs a UVR module to improve the texture quality in the UV space, which first performs a UV-space Super-Resolution, followed by a Spatial-aware Seam-Smoothing algorithm for revising spatial texturing discontinuities caused by UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the Objaverse T2T benchmark and the GSO T2T benchmark, based on selected high-quality 3D meshes from the Objaverse dataset and the entire GSO dataset, respectively. Extensive experimental results demonstrate that MVPaint surpasses existing state-of-the-art methods. Notably, MVPaint could generate high-fidelity textures with minimal Janus issues and highly enhanced cross-view consistency."
        },
        {
            "title": "Start",
            "content": "MVPaint: Synchronized Multi-View Diffusion for Painting Anything 3D Wei Cheng1 Chi Zhang1 Juncheng Mu2,4 Xianfang Zeng1 Xin Chen1 Anqi Pang1 Liang Pan2 Zhibin Wang1 Bin Fu1 Gang Yu1 Ziwei Liu3 4 2 0 N 4 ] . [ 1 6 3 3 2 0 . 1 1 4 2 : r 1 Tencent PCG 2 Shanghai AI Laboratory 3 S-Lab, NTU 4 Tsinghua University http://mvpaint.github.io Figure 1. MVPaint generates multi-view consistent textures with arbitrary UV unwrapping and high generation versatility."
        },
        {
            "title": "Abstract",
            "content": "Texturing is crucial step in the 3D asset production workflow, which enhances the visual appeal and diversity of 3D assets. Despite recent advancements in Textto-Texture (T2T) generation, existing methods often yield subpar results, primarily due to local discontinuities, inconsistencies across multiple views, and their heavy dependence on UV unwrapping outcomes. To tackle these challenges, we propose novel generation-refinement 3D texturing framework called MVPaint, which can generate high-resolution, seamless textures while emphasizing multiview consistency. MVPaint mainly consists of three key modules. 1) Synchronized Multi-view Generation (SMG). Given 3D mesh model, MVPaint first simultaneously generates multi-view images by employing an SMG model, which leads to coarse texturing results with unpainted parts * Equal Contribution, Project Lead, Corresponding Author due to missing observations. 2) Spatial-aware 3D Inpainting (S3I). To ensure complete 3D texturing, we introduce the S3I method, specifically designed to effectively texture previously unobserved areas. 3) UV Refinement (UVR). Furthermore, MVPaint employs UVR module to improve the texture quality in the UV space, which first performs UV-space Super-Resolution, followed by Spatial-aware Seam-Smoothing algorithm for revising spatial texturing discontinuities caused by UV unwrapping. Moreover, we establish two T2T evaluation benchmarks: the Objaverse T2T benchmark and the GSO T2T benchmark, based on selected high-quality 3D meshes from the Objaverse dataset and the entire GSO dataset, respectively. Extensive experimental results demonstrate that MVPaint surpasses existing state-of-the-art methods. Notably, MVPaint could generate high-fidelity textures with minimal Janus issues and highly enhanced cross-view consistency. 1 1. Introduction 3D texture generation remains complex and critical aspect of asset creation, especially valuable in applications like gaming, animation, and virtual/augmented/mixed reality. Despite the scarcity of specialized 3D training data and the high computational demands of texture modeling, recent breakthroughs in text-to-image technologies [15, 17, 22, 44] have significantly advanced the field. These technologies facilitate Text-to-Texture (T2T) generation [6, 34, 43, 60] and their integration with 3D shapes [35, 40, 49], enhancing the visual diversity and realism of 3D models. However, achieving consistent and seamless textures across various viewing angles remains challenging, often hampered by local discontinuities and cross-view inconsistencies. Recently, many texture generation methods have focused on leveraging 2D diffusion priors for guiding the generation process, which often utilizes conditional controls [62] (e.g., depth) to produce more fitting textures. TEXTure [43] and Text2Tex [6] sample series of camera viewpoints for iteratively rendering depth maps, which are then used to generate high-quality images through pretrained depth-to-image diffusion model. To avoid inconsistent textures due to using multiple independent generation processes, SyncMVD [34] introduced method combining multi-view single-step denoising with UV space synchronization. However, its reused attention process is limited to nearby views, which frequently leads to Janus problems. Paint3D [60] developed coarse-to-fine texture generation strategy, starting with coarse texture obtained by iteratively painting from camera viewpoints, followed by inpainting and super-resolution in UV space. Similarly, Meta 3D TextureGen [4] also employs UV position maps for UV inpainting and enhancement. Despite achieving remarkable 3D texturing results, both tools [4, 60] depend heavily on continuous mesh UV unwrapping. Discontinuities in texture often arise in scenarios where the UV atlases are randomly packed within the UV images. Consequently, many challenges remain in 3D texturing: Multi-View Consistency: Ensuring consistency across multiple viewpoints to prevent local style discontinuities and the presence of numerous seams. Diverse Texture Details: Avoiding overly smooth textures that lack detail, while aiming for highresolution outputs. UV Unwrapping Robustness: Developing method that does not rely heavily on UV unwrapping results to achieve robust automated generation. To address these challenges, we propose MVPaint, coarse-to-fine 3D texture generation framework capable of producing high-fidelity, seamless 3D textures while ensuring multi-view consistency and reducing dependence on UV unwrapping quality. MVPaint mainly consists of three stages for texture generation. (1) First, we employ the Synchronized Multi-view Generation (SMG) model that uses multi-view diffusion model with cross-attention [51] and UV synchronization to initiate 3D texture generation conditioned on given textural instruction, which effectively avoids the Janus problem and produces highly consistent multi-view images at low resolution. Following, we upsample and refine the coarse multi-view images by adding vivid texture details, subsequently projecting them into UV space (1K resolution) for further enhancement. (2) Second, we propose the Spatial-aware 3D Inpainting (S3I) method to ensure complete 3D texturing, particularly for areas that were not observed in the first stage. Specifically, S3I resolves the inpainting process in 3D space by considering the spatial relations among 3D points uniformly sampled from mesh surfaces. (3) Third, we introduce UV Refinement (UVR) module, comprising series of tailored texture enhancement operations in UV space. UVR first employs super-resolution module to upscale the UV map to 2K resolution. Afterward, we introduce Spatial-aware Seamsmoothing Algorithm (SSA) to revise spatial discontinuous textures, especially for repairing the seams caused by UV unwrapping. Consequently, high-quality 3D UV textures could be obtained. To facilitate the evaluation of T2T generation, we establish two benchmarks: the Objaverse [12] T2T benchmark and the GSO [16] T2T benchmark. The Objaverse T2T benchmark comprises 1000 high-quality 3D meshes curated from the Objaverse dataset. Given that most T2T models are trained on subset of the Objaverse dataset, we further establish the GSO T2T benchmark, which leverages all 1032 3D models from the GSO dataset to assess the generalizability of T2T models. For each 3D mesh, textual annotations are generated utilizing large language model (LLM). Extensive experimental results on the Objaverse [12] and the GSO [16] T2T benchmarks demonstrate that MVPaint could outperform existing State-of-The-Art (SoTA) methods for 3D texture generation. We would like to emphasize that MVPaint is robust 3D texturing method, significantly reducing occurrences of failed generations, such as missing areas, large inconsistencies, over-smoothness, and Janus issues. Qualitative texturing results of MVPaint could be visualized in Fig. 1. Our contributions could be summarized as follows: 1) We propose robust 3D texturing framework, entitled MVPaint, for generating diverse, high-quality, seamless 3D textures while ensuring multi-view consistency. 2) Various 3D texturing models, operations, and strategies, including SMG, S3I, and UVR modules, have been proposed, studied, and utilized in this work. We believe these contributions will significantly advance future research in 3D texture generation. 3) We conduct extensive experiments on the Objaverse and the GSO T2T benchmark, demonstrating that MVPaint achieves impressive 3D texture generation results, surpassing existing SoTA methods. Figure 2. The Framework Overview of MVPaint. Given an input mesh, Stage 1 of MVPaint utilizes synchronized multi-view generation (SMG) model, consisting of control-based T2MV model and an I2I model, for 3D texture initialization. In Stage 2, the synchronized views are reprojected back to UV space, where inpainting is performed on the 3D point cloud to fill the holes (shown in red dots), hence completing the UV map. In Stage 3, the completed UV map undergoes super-resolution, adding finer details, followed by seam detection and 3D-aware smoothing to achieve complete, seamless, and multi-view consistent 3D texture. 2. Related Work Multi-View Generation. The generation of coherent multiview images from diverse inputs, such as text, images, and meshes, has become crucial research area. This enables the creation of 3D assets with consistent appearances across various perspectives. Groundbreaking studies like Zero-1-to-3 [33] and Consistent-1-to-3 [58] have employed viewpoint-conditioned diffusion models to synthesize novel views of objects, ensuring seamless transitions between the generated perspectives. Building upon these advancements, Zero123++ [46] and MVDream [47] have adopted an approach that tiles multiple view images into single canvas for generation, respectively producing consistent hexa-view and tetra-view images. 3D Texture Generation. Traditionally, texture generation relies on manual or procedural techniques [29, 31, 50, 54], which were effective for basic applications but lacked complexity. The introduction of global optimization techniques [25, 53] allows for more detailed textures that better matched 3D model geometries. AI-based 3D texture generation is initially dominated by generative adversarial networks (GANs) [18, 36, 41, 64], and then the focus has shifted towards latent diffusion models (LDM) [21, 44], with models like Stable Diffusion [39, 44] showing promising results. With large-scale text-image prior models, texture generation methods [10, 34, 43] leverage textimage correlations to update rendered 3D views by extracting gradients from CLIP [37, 42, 45]. Iterative methods [13, 38, 43] enhance texture quality and consistency across 3D models by rendering depth or normal maps, with the help of ControlNet [62]. Other methods [4, 60] generate multi-view images with depth or normal conditioned sparse views and then apply inpainting and refinement directly on UV map with position-map-controlled diffusion network. However, it is challenging for UV diffusion models to directly generate correct and 3D-continuous texture patches, as they are frequently packed to separate UV regions. Recently, multi-path diffusion [3] synchronizes the wrapped latent [34] or images [27, 61] during multiple single-view DDIM [48] processes. Nonetheless, these methods [27, 34] tend to get trapped to the multi-face issues also known as the Janus problem which is typical phenomenon in 3D generation using 2D priors. More discussions are provided in Sec. A. 3. Our Approach Given an untextured mesh = (V, F) and texture prompt c, where = {vi} representing the set of 3D vertex vi R3 and = {fi} representing the set of triangular faces each defined by triplet of vertices, MVPaint aims to generate high-quality texture map (2K level) represented as multi-channel UV image RHW conditioned on the texture prompt c. To achieve multi-view consistency, high-quality, and seamless 3D textures, MVPaint utilizes three major stages, including 1) Synchronized Multi-view Generation (SMG) model - for simultaneously generating 3 where zMV is latent for MV generation. Multi-View Synchronization. Existing T2I-model-based texturing methods [3, 30, 34] enhance the alignment of texture predictions from different views by integrating their denoising processes on the latent according to the shared UV space. Although significant differences across different views could lead to overly smoothed textures, the synchronization operation could eliminate minor discrepancies by adjusting the generation results of different views based on the same UV map. Unfortunately, the latent of T2MV model DMV usually has low resolution (e.g., 32 32), which complicates establishing robust mapping to the UV space, especially when UV unwrapping is complex, hence hindering the effective capture of intricate visual relationships across different views. Rather than synchronizing in latent space, we propose the Synchronized Multi-view Generation (SMG) model, which fuses the multi-view diffusion path by aligning the multi-view generation in the decoded image domain. Given the latent zMV of DMV at time step t, clean intermediate 0t (denoted as MV in the following for simplicity) state zMV is obtained by removing the noise from zMV . Afterward, MV is converted into image space with larger resolution (e.g., 256 256) by using the pretrained decoder Gψ() of the VAE in DMV. Subsequently, we generate the synchronized UV map sync by fusing the multi-view generation with inverse UV mapping, applying weights based on the cosine angle between the view direction vi and the surface normal nuv represented in the UV space. Afterward, the synchronized multi-view images are generated by rasterizing the UV map, which is then encoded as synchronized latent MV sync using the pretrained encoder Fϕ() of the VAE in DMV. Consequently, the multi-view consistency could be enhanced, and the synchronized multi-view images Il could be obtained as follows: Nsync Nsync Il Nsync sync = Fϕ(I MV = DMV({zMV; MV ), Nsync Nsync sync }, c, Pl N; τp) , sync) , = R(T where sync = (cid:88) cos(vi, nuv)T i, i = R1(I i), = Gψ(z MV) , (2) (3) R() and R1() denote the UV rasterization and UV mapping function, respectively. Notably, using single synchronization step could be adequate, while many synchronized diffusion steps may result in unstable generations. The effectiveness of synchronization could be visualized in Fig. 3. Synchronized Refinement. To enhance and upscale multiview images Il , we utilize an Image-to-Image (I2I) generation module DI2I() for synchronized texture refinement, which generates high-fidelity images Ih at higher Nsync Nsync Figure 3. The Effectiveness of Synchronization on Multi-view Image Generation. Although T2MV models generate Janusproblem-free results, they still suffer from texture misalignment from different views. In contrast, the proposed SMG model can effectively enforce multi-view consistency for T2MV generation. dense view images as the initial texturing (see Sec. 3.1); 2) Spatial-aware 3D Inpainting (S3I) model - to inpaint and enhance the texturing based on spatial relations (see Sec. 3.2); and 3) UV Refinement (UVR) module - to conduct upsampling and refinement for generating the final UV texture (see Sec. 3.3);. The overview of MVPaint is illustrated in Fig. 2. 3.1. Synchronized Multi-View Generation Building on the success of using 2D diffusion priors in recent Text-to-Image (T2I) models [15, 17, 22, 44], many 3D texturing methods [6, 34, 43, 60] initialize their textures using depth-conditioned 2D generation models. Specifically, they typically begin by rendering depth maps from various viewpoints sampled around the given 3D mesh model M, then using pre-trained depth-to-image diffusion model to generate multi-view images based on the text instructions. Despite leveraging 2D generation priors, existing methods often overlook 3D priors during generation, leading to low-quality multi-view results plagued by issues such as multi-view inconsistencies, the Janus problem, and over-smoothed textures lacking vivid details. Text-to-Multi-View for 3D Texture Generation. In contrast, we utilize Text-to-Multi-View (T2MV) diffusion model DMV, where the multi-view prior acts as generalizable 3D prior [47], to generate consistent multi-view images in single forward process conditioned on the text instruction c. In particular, we leverage the T2MV model DMV by training control model [62] τp to guide the generation process using the depth or normal map Pl {1, 2, , } (abbr. as Pl N) from the corresponding view. Formally, the generation of initial low-resolution multi-view images Il {1, 2, , } (abbr. as Il N) could be formulated as: = Il Il {1, 2, , } = DMV(zMV, c, Pl N; τp), (1) 4 Figure 4. Spatial-aware 3D Inpainting could effectively accomplish texture completion for 3D structures with complex geometries and large unobserved areas. Figure 5. Spatial-aware Seam-smoothing Algorithm could revise texture seams from 2D UV unwrapping by smoothing color vectors using their 3D neighbors. resolution (1K level) while preserving multi-view consistency. Specifically, we leverage two pretrained control models [44, 62], including texture refinement model τt and geometry enforcement model τg, which adopts the synchronized denoising mechanism [34] to ensure consistency across different views. Formally, the high-quality multiview image refinement could be formulated as: Ih Nsync = DI2I(zI, Il Nsync , Ph N, st, sg; τt, τg), (4) where zI is random initialized latent for single image generation, Ph is the set of high-resolution geometric (e.g., depth or normal) maps, and st, sg is the user-defined strength of the two control models. 3.2. Texture Inpainting in 3D Space Nsync Although high-quality multi-view images Ih cover most of the mesh surface, there are still unobserved regions that need to be inpainted. After performing UV mapping R1(), multi-view images Ih are projected into the UV Nsync space, achieving an incomplete UV map Ti. To address the artifacts caused by complex self-occlusion issues during UV mapping R1(), we refine the projection area by identifying and limiting the regions affected by occlusion (check Sec. B.4 for details). For UV map completion, existing methods [34, 60] mostly perform inpainting directly in UV space. However, adjacent 3D areas are frequently mapped to non-adjacent 2D regions within Ti, which becomes more pronounced when Ti is highly fragmented. 3D Point Cloud Inpainting. In light of this, we propose Spatial-aware 3D Inpainting (S3I) module (shown in Stage 2 of Fig.2) to inpaint texture in the 3D space, which generates full-coverage 3D texture Tc conditioned on the incomplete texture Ti, while enforcing 3D geometry-aware spatial consistency. Specifically, we first generate dense colored point cloud Puv RNuv6 by concatenating the 3D point coordinates generated based on each pixel from valid regions in Ti and their corresponding RGB values. Note that unpainted pixels from valid UV areas will also be used for generating 3D points Pu Puv with zeroinitialized color vector (i.e., (0, 0, 0)). Accordingly, inpainting the UV texture could be reformulated as predicting suitable color vector for each point Pu conditioned on the set of colored 3D points Pv Puv generated from the visible area in Tc. We highlight that the learning-free approach S3I is unaffected by the UV unwrapping results. Spatial-aware Color Propagation. To address the 3D point inpainting problem, we propose Spatial-aware Color Propagation (SCP) algorithm, which iteratively propagates the color value from Pv to Pu. In each iteration, the knearest neighbors Npi = {qj Pv 1 k} of each point pi Pu are selected, and the color vector of pi is then estimated by applying weighted sum of the color vectors from each neighbor within Npi . For each neighboring point qj Npi , the aggregation weight wj is computed by considering both the Euclidean distance dj and the surface normal similarity between qj and pi: wj = 1/dj 1/dj (cid:80) (nj ni), (5) where nj and ni are surface normal of qj and pi, respectively. () is robust mapping function, defined as: (x) = 1 108, x, 10, if 1 < 0.5, if 0.5 < 0.9, if 0.9 1. (6) After iterations, all 3D points will be painted, resulting in the full-coverage UV texture Tc (1K level). Please refer to the supplementary (Sec. B.2) for algorithm details. The effectiveness of S3I could be visualized in Fig. 4. 3.3. UV Refinement Even though full-coverage UV texture Tc has been acquired, texture details may appear flawed caused by projecFigure 6. Qualitative Results on Text-conditioned 3D Texture Generation. MVPaint could constantly generate high-quality 3D textures, while existing methods frequently provide flawed results. Note that the input text prompts are simplified for better presentation. tion errors during R1() and the interpolation filling process. To achieve high-quality UV map T, we propose UV Refinement (UVR) module (shown in Stage 3 of Fig.2), mainly consisting of 1) super-resolution module for UV texture upsampling and refinement; 2) and spatial-aware seam-smoothing algorithm for repairing the texture seams caused by UV upsampling. UV Space Super-Resolution. In order to generate more aesthetically pleasing and higher-resolution texture mappings, we perform super-resolution on Tc in UV space. We use an Image-to-Image Upscale (UP) diffusion model DUP for super-resolution in UV space with an upscale model τup: Tup = DUP(zUV, Tc; τup), (7) where zUV is random initialized latent for high-resolution UV generation. The upscale model τup can be selected as either tiling control network or an upscale network. Please refer to Sec. B.5 for detailed model descriptions and distinctions. Spatial-aware Seam-smoothing Algorithm. Although more delicate and intricate textures could be received in the upsampled UV map Tup, the discontinuities caused by UV unwrapping frequently lead to abrupt changes. To resolve this problem, we introduce Spatial-aware Seamsmoothing Algorithm (SSA) to revise the seams in 3D point cloud space. First, we extract the binary image Tvalid to mark the valid pixels from Tup, based on which we perform connectivity analysis and edge extraction for detecting the seam mask mseam. Similar to the projection operations in S3I, we resample Tup into 3D colored point cloud Pup RN26, followed by applying the SS algorithm for seam repairing. Specifically, we extract subsets Pseam and Pn-seam from Pup using the seam mask mseam. Afterward, we construct kd-tree with Pn-seam and then refine Pseam with neighboring points from Pn-seam. Finally, by calculating the normal vector cosine similarity and the distance for weighted coloring, we obtain the final high-quality seamless texture (2K level). The effectiveness of SSA are illustrated in Fig. 5. More algorithm details are provided in Sec. B.3. 4. Experiments In this section, extensive experiments have been conducted to evaluate the effectiveness of MVPaintin generating high-quality 3D textures from texture instructions. Implementation Details. We utilize the MVDream [47] as the base model of T2MV DMV of SMG, and we add the control module τp and train it with the same training scheme of ControlNet [62]. Different from other controlled MVDream methods [32] which only controls single view, we densely control multi-view for better shape alignment. For synchronized refinement, we choose the SDXL [39] as the based model for I2I refinement DI2I, where two pre-trained ControlNets [14, 56], τt and τg are deployed. During refinement, the per-view latents zI are with 128 128 resolution, and they are synchronized on Tsync with 512 512 resolution. In all SMG processes, models are worked on = 8 views with evenly distributed azimuth angle and interleaved elevation of 30. 4.1. Text-Instructed 3D Texture Generation Dataset Details. We filter out 104k Objaverse [12] samples with valid single texture maps and bake the texture 6 Table 1. Quantitative Results on the Objaverse T2T Benchmark. Method FID KID CLIP User Study Overall Seamless Consistency TEXTure [43] Paint3D [60] SyncMVD [34] Ours 28.03 25.28 26.99 20.89 7.60 5.19 5.72 3.45 20.30 19.27 20. 19.87 3.81 3.85 3.96 4.19 3.66 3.36 3.85 4.25 3.31 3.51 3. 3.98 Table 2. Quantitative Results on the GSO T2T Benchmark. Method FID KID CLIP User Study Overall Seamless Consistency TEXTure [43] Paint3D [60] SyncMVD [34] 24.76 37.29 26.96 5.50 10.24 5.37 Ours 20. 3.12 23.44 21.21 23.08 23.25 3.78 3.24 4.01 4.13 3.97 3.14 4. 4.51 3.65 3.45 3.71 4.21 with Xatlas [59] wrapping, where 102k samples are selected for training and the rest for validation and evaluation. The training data consists of rendered per-view RGB images IN and their corresponding control proxy images Pl N. The textual annotations are derived by instructing the CogVLM-2 model [23] to describe category, texture, and appearance of the 3D objects, utilizing the multi-view images IN as the model input. Afterward, keywords are summarized using another LLM [26]. Evaluation Benchmarks. To comprehensively evaluate arbitrary types of meshes for the text-to-texture (T2T) generation, we build two evaluation benchmarks: (1) the Objaverse T2T benchmark. To achieve diverse Objaverse T2T benchmark, we integrate the Paint3D test set [60], containing 301 artist-crafted meshes from Objaverse, along with 3D scans and complex scenes, such as 3D character models, which leads to an extensive test set with total of 1000 models. (2) the GSO T2T benchmark. The Google Scanned Objects (GSO) dataset provides curated collection of 1032 3D scanned common household items, each captured at high resolution to capture intricate details. As 3D texture generation methods are primarily trained on 3D objects from Objaverse, the GSO dataset could be used to assess their generalizability. Therefore, we establish the GSO T2T benchmark using the complete GSO dataset. Evaluation metrics. After generating 3D textures, 512resolution images of the mesh are rendered with the generated textures from 16 fixed viewpoints at the same elevation, i.e., 15 for fairness. Then, we compare the rendered images with the true image distribution generated using the ground truth textures. For thorough evaluation, we use common generative metrics: Frechet Inception Distance (FID) [20], Kernel Inception Distance (KID) [5], and CLIP score [19] to assess image distribution, quality, and richness. The KID values are scaled by 103 in all tables. User Study. To complement the quantitative results based on the generative metrics, we also conduct user study to capture human preferences regarding the generated 3D textures. 10 participants are invited to evaluate the textured Figure 7. Ablation Study on SMG Designs. meshes in an interface that allows free navigation and observation of the 3D models. Each participant is asked to rate the following aspects on scale of 1 to 5: overall quality, seam visibility, and overall consistency reflecting their preferences for each criterion. Evaluation Results. We select all existing open-source SoTA methods on T2T for comparison, including TEXTure [43], Paint3D [60], and SyncMVD [34]. The quantitative results on the Objaverse T2T benchmark are reported in Tab. 1. Our method, MVPaint achieves the best scores in terms of FID and KID, outperforming previous SoTA methods by over 4.3 and 1.7, respectively, while TEXTure offers the best CLIP score. It is worth noting that the TEXTure often encounters the Janus problem, potentially resulting in high CLIP score. According to the user study, MVPaint outperforms previous SoTA methods, achieving the highest ratings across all evaluated aspects, including overall quality, seam visibility, and consistency. Since none of the T2T methods incorporate the GSO dataset in their training, the GSO T2T benchmark could be used to evaluate their generalizability. The quantitative results on the GSO T2T benchmark are reported in Tab. 2. Similar to the results observed in the Objaverse T2T benchmark, MVPaint achieves the best objective FID, KID performance, subjective user study scores, and second-best CLIP scores on the GSO T2T benchmark. As Paint3D incorporates crucial submodule that is trained on the Objaverse dataset, there is noticeable decline in performance when evaluated on the GSO T2T benchmark, compared to its performance on the Objaverse T2T benchmark. In con7 Figure 8. Diverse Texturing on the Same Model. We use GPT4 [1] to generate 38 random texturing prompts without cherry-picking. Table 3. Ablation Study on Objaverse [12] Benchmark. Method FID KID CLIP w/o MV Sync. w/o MV Diff w/o Geo. Refinement w/o 3D Inpainting w/o Seam Smoothing Full Design 21.42 27.63 21. 20.91 20.82 20.89 3.72 5.82 3.67 3.56 3.54 3.45 19.90 20.44 20. 19.87 19.92 19.87 trast, MVPaint maintains its ability to produce high-quality 3D textures on the GSO benchmark, despite being trained on dataset curated from Objaverse. The qualitative comparisons could be visualized in Fig. 6. 4.2. Ablation SMG Designs. To validate the effectiveness of SMG module from Stage 1, we conduct detailed ablation study over its three key designs, including the T2MV diffusion model, multi-view synchronization, and geometry-aware refinement. Specifically, in w/o MV Sync. we omit the synchronization module and use Il for T2MV model. In w/o MV Diff we omit the whole T2MV modol DMV and its output Il , and use the τg in the DI2I to generate from scratch. In w/o Geo. Refinement, we drop the geometry refinement control model τg and only use the tiling model τt. The quantitative results of the combined benchmarks are reported in Tab. 3 which testify to the efficacy of the proposed designs. rather than Il Nsync Nsync The qualitative results are illustrated in Fig. 7. Without multi-view synchronization, inconsistent MV images result in poor initializations for subsequent refinement. In the absence of MV images, the refinement network struggles to identify the orientation of the untextured mesh across differ8 Figure 9. Application of MVPaint. Texturing for generated 3D assets from MeshXL [7] and MeshAnything [8, 9]. ent views, leading to the Janus problem. Without geometryguided refinement (τg), the refinement merely adds details to coarse MV images and exacerbates initialization artifacts. With the full design, MVPaint could produce multiview consistent results, free of the Janus problem. Quantitative ablation results on the GSO benchmark are presented in Tab.S1, with further analysis in Sec. C.2. 3D Inpainting and Seam Smoothing. We also validate the effectiveness of 3D inpainting (from Stage 2) and seam smoothing (from Stage 3), and the quantitative results are reported in Tab. 3. Their quantitative results are very close, with the FID showing slight variations within 0.1 and the KID consistently remaining below 0.2. Although these refinement operations have minimal impact on metrics, they could effectively eliminate texture artifacts. 4.3. Application MVPaint could generate faithful 3D textures based on text instructions, hence supporting various related applications, such as: 1) Generating Diverse 3D Texture: Given certain 3D mesh model, MVPaint could generate 3D textures with large variations conditioned on different text prompts. As shown in Fig. 8, MVPaint generates 38 different 3D textures for the single unicorn model. 2) Texturing AI-Generated 3D Meshes: MVPaint generates 3D textures independently of the UV unwrapping quality, enabling it to produce high-quality textures even when the AI-generated 3D meshes contain minor artifacts. We demonstrate the textures generated by MVPaint on 3D meshes created by MeshXL [7] and MeshAnything [8, 9] in Fig. 9. 5. Conclusion In this paper, we introduce MVPaint, comprehensive framework for generating 3D textures from text, consisting of three key stages: synchronized multi-view generation, 3D space texture inpainting, and UV refinement. Utilizing synchronized multi-view diffusion, MVPaint initializes 3D textures based on generated multi-view images, ensuring high cross-view consistency. Subsequently, areas not covered by these multi-view images are textured through inpainting in 3D space. Finally, refinement module enhances and upscales the 3D mesh in the UV space, producing high-quality UV textures at 2K resolution. Extensive experiments demonstrate that MVPaint consistently produces high-quality 3D textures, outperforming existing SoTA texturing methods."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. In arXiv preprint arXiv:2303.08774, 2023. 8 [2] Stability AI. Stable diffusion 2, 2022. Accessed: 2024-1024. 17 [3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. Multidiffusion: fusing diffusion paths for controlled image generation. In ICML, 2023. 3, 4 [4] Raphael Bensadoun, Yanir Kleiman, Idan Azuri, Omri Harosh, Andrea Vedaldi, Natalia Neverova, and Oran Gafni. Meta 3d texturegen: Fast and consistent texture generation for 3d objects. In arXiv preprint arXiv:2407.02430, 2024. 2, 3, 12, 15, [5] Mikołaj Binkowski, Danica Sutherland, Michael Arbel, and Arthur Gretton. Demystifying mmd gans. In ICLR, 2018. 7 [6] Dave Zhenyu Chen, Yawar Siddiqui, Hsin-Ying Lee, Sergey Tulyakov, and Matthias Nießner. Text2tex: Text-driven texture synthesis via diffusion models. In CVPR, 2023. 2, 4, 12 [7] Sijin Chen, Xin Chen, Anqi Pang, Xianfang Zeng, Wei Cheng, Yijun Fu, Fukun Yin, Yanru Wang, Zhibin Wang, Chi Zhang, et al. Meshxl: Neural coordinate field for generative 3d foundation models. In arXiv preprint arXiv:2405.20853, 2024. 8, 9 [8] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, et al. Meshanything: Artist-created mesh generaIn arXiv preprint tion with autoregressive transformers. arXiv:2406.10163, 2024. 8, 9 9 [9] Yiwen Chen, Yikai Wang, Yihao Luo, Zhengyi Wang, Zilong Chen, Jun Zhu, Chi Zhang, and Guosheng Lin. Meshanything v2: Artist-created mesh generation with adjacent mesh tokenization. In arXiv preprint arXiv:2408.02555, 2024. 8, 9 [10] Yiwen Chen, Chi Zhang, Xiaofeng Yang, Zhongang Cai, Gang Yu, Lei Yang, and Guosheng Lin. It3d: Improved textto-3d generation with explicit view synthesis. In AAAI, 2024. [11] Xiaoliang Dai, Ji Hou, Chih-Yao Ma, Sam Tsai, Jialiang Wang, Rui Wang, Peizhao Zhang, Simon Vandenhende, Xiaofang Wang, Abhimanyu Dubey, et al. Emu: Enhancing image generation models using photogenic needles in haystack. In arXiv preprint arXiv:2309.15807, 2023. 12, 17 [12] Matt Deitke, Dustin Schwenk, Jordi Salvador, Luca Weihs, Oscar Michel, Eli VanderBilt, Ludwig Schmidt, Kiana Ehsani, Aniruddha Kembhavi, and Ali Farhadi. Objaverse: universe of annotated 3d objects. In CVPR, 2023. 2, 6, 8, 16, 18 [13] Kangle Deng, Timothy Omernick, Alexander Weiss, Deva Ramanan, Jun-Yan Zhu, Tinghui Zhou, and Maneesh Agrawala. Flashtex: Fast relightable mesh texturing with lightcontrolnet. In ECCV, 2024. 3 [14] Diffusers. https : Controlnet depth sdxl 1.0. / / huggingface . co / diffusers / controlnet - depth-sdxl-1.0, 2023. 6 [15] Ming Ding, Zhuoyi Yang, Wenyi Hong, Wendi Zheng, Chang Zhou, Da Yin, Junyang Lin, Xu Zou, Zhou Shao, Hongxia Yang, et al. Cogview: Mastering text-to-image generation via transformers. In NeurIPS, 2021. 2, 4 [16] Laura Downs, Anthony Francis, Nate Koenig, Brandon Kinman, Ryan Hickman, Krista Reymann, Thomas McHugh, and Vincent Vanhoucke. Google scanned objects: highIn ICRA, quality dataset of 3d scanned household items. 2022. 2, [17] Oran Gafni, Adam Polyak, Oron Ashual, Shelly Sheynin, Devi Parikh, and Yaniv Taigman. Make-a-scene: Scenebased text-to-image generation with human priors. In ECCV, 2022. 2, 4 [18] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and In NeurIPS, Yoshua Bengio. Generative adversarial nets. 2014. 3 [19] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation metric for image captioning. In EMNLP, 2021. 7 [20] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. In NeurIPS, 2017. 7 [21] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. In NeurIPS, 2020. [22] Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. In JMLR, 2022. 2, 4 [23] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. Cogvlm2: Visual language modIn arXiv preprint els for image and video understanding. arXiv:2408.16500, 2024. 7 [24] Edward Hu, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. In ICML, 2022. 13, 17 [25] Jingwei Huang, Justus Thies, Angela Dai, Abhijit Kundu, Chiyu Jiang, Leonidas Guibas, Matthias Nießner, Thomas Funkhouser, et al. Adversarial texture optimization from rgbd scans. In CVPR, 2020. 3 [26] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume LamIn arXiv preprint ple, Lucile Saulnier, et al. Mistral 7b. arXiv:2310.06825, 2023. 7 [27] DaDong Jiang, Xianghui Yang, Zibo Zhao, Sheng Zhang, Jiaao Yu, Zeqiang Lai, Shaoxiong Yang, Chunchao Guo, Xiaobo Zhou, and Zhihui Ke. Flexitex: Enhancing texIn arXiv preprint ture generation with visual guidance. arXiv:2409.12431, 2024. 3 [28] Sagi Katz, Ayellet Tal, and Ronen Basri. Direct visibility of point sets. In SIGGRAPH, 2007. 15 [29] Johannes Kopf, Chi-Wing Fu, Daniel Cohen-Or, Oliver Deussen, Dani Lischinski, and Tien-Tsin Wong. Solid texture synthesis from 2d exemplars. In SIGGRAPH, 2007. 3 [30] Yuseung Lee, Kunho Kim, Hyunjin Kim, and Minhyuk Sung. Syncdiffusion: Coherent montage via synchronized joint diffusions. In NeurIPS, 2023. 4 [31] Sylvain Lefebvre and Hugues Hoppe. Appearance-space texture synthesis. In ACM TOG, 2006. 3 [32] Zhiqi Li, Yiming Chen, Lingzhe Zhao, and Peidong Liu. Mvcontrol: Adding conditional control to multi-view diffusion for controllable text-to-3d generation. In arXiv preprint arXiv:2311.14494, 2023. [33] Ruoshi Liu, Rundi Wu, Basile Van Hoorick, Pavel Tokmakov, Sergey Zakharov, and Carl Vondrick. Zero-1-to-3: Zero-shot one image to 3d object. In ICCV, 2023. 3 [34] Yuxin Liu, Minshan Xie, Hanyuan Liu, and Tien-Tsin Wong. Text-guided texturing by synchronized multi-view diffusion. In arXiv preprint arXiv:2311.12891, 2023. 2, 3, 4, 5, 7, 12, 13, 16, 17 [35] Xiaoxiao Long, Yuan-Chen Guo, Cheng Lin, Yuan Liu, Zhiyang Dou, Lingjie Liu, Yuexin Ma, Song-Hai Zhang, Marc Habermann, Christian Theobalt, et al. Wonder3d: SinIn CVPR, gle image to 3d using cross-domain diffusion. 2024. 2 [36] Mehdi Mirza. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784, 2014. 3 [37] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky, and Tiberiu Popa. Clip-mesh: Generating textured meshes from text using pretrained image-text models. In SIGGRAPH Asia, 2022. [38] Sai Raj Kishore Perla, Yizhi Wang, Ali Mahdavi-Amiri, and Hao Zhang. Easi-tex: Edge-aware mesh texturing from single image. In ACM TOG, 2024. 3 [39] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and 10 Robin Rombach. Sdxl: els for high-resolution image synthesis. arXiv:2307.01952, 2023. 3, 6, 17 Improving latent diffusion modIn arXiv preprint [40] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. In ICLR, 2023. [41] Alec Radford. Unsupervised representation learning with deep convolutional generative adversarial networks. In arXiv preprint arXiv:1511.06434, 2015. 3 [42] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, 2021. 3 [43] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. In SIGGRAPH, 2023. 2, 3, 4, 7, 12, 13, 16, 17 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, 2022. 2, 3, 4, 5 [45] Aditya Sanghi, Hang Chu, Joseph Lambourne, Ye Wang, Chin-Yi Cheng, Marco Fumero, and Kamal Rahimi Malekshan. Clip-forge: Towards zero-shot text-to-shape generation. In CVPR, 2022. 3 [46] Ruoxi Shi, Hansheng Chen, Zhuoyang Zhang, Minghua Liu, Chao Xu, Xinyue Wei, Linghao Chen, Chong Zeng, and Hao Su. Zero123++: single image to consistent multi-view diffusion base model. In arXiv preprint arXiv:2310.15110, 2023. 3 [47] Yichun Shi, Peng Wang, Jianglong Ye, Long Mai, Kejie Li, and Xiao Yang. Mvdream: Multi-view diffusion for 3d generation. In ICLR, 2024. 3, 4, 6, 13, [48] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3 [49] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. In ICLR, 2024. 2 [50] Greg Turk. Texture synthesis on surfaces. In SIGGRAPH, 2001. 3 [51] Vaswani. Attention is all you need. In NeurIPS, 2017. 2 [52] Peng Wang and Yichun Shi. Imagedream: Image-prompt In arXiv preprint multi-view diffusion for 3d generation. arXiv:2312.02201, 2023. 17 [53] Li-Yi Wei and Marc Levoy. Texture synthesis over arbitrary manifold surfaces. In SIGGRAPH, 2001. 3 [54] Li-Yi Wei, Sylvain Lefebvre, Vivek Kwatra, and Greg Turk. State of the art in example-based texture synthesis. In Eurographics STAR, 2009. 3 [55] Kailu Wu, Fangfu Liu, Zhihan Cai, Runjie Yan, Hanyang Wang, Yating Hu, Yueqi Duan, and Kaisheng Ma. Unique3d: High-quality and efficient 3d mesh generation from single image. In arXiv preprint arXiv:2405.20343, 2024. [56] Xinsir. Controlnet https : / / huggingface . co / xinsir / controlnet - tile - sdxl-1.0, 2023. 6 tile sdxl 1.0. [61] Hongkun Zhang, Zherong Pan, Congyi Zhang, Lifeng Zhu, and Xifeng Gao. Texpainter: Generative mesh texturing with multi-view consistency. In SIGGRAPH, 2024. 3 [62] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In CVPR, 2023. 2, 3, 4, 5, 6, 12, [63] Longwen Zhang, Ziyu Wang, Qixuan Zhang, Qiwei Qiu, Anqi Pang, Haoran Jiang, Wei Yang, Lan Xu, and Jingyi Yu. Clay: controllable large-scale generative model for creating high-quality 3d assets. In ACM TOG, 2024. 12, 13, 17 [64] Jun-Yan Zhu, Taesung Park, Phillip Isola, and Alexei Efros. Unpaired image-to-image translation using cycleconsistent adversarial networks. In ICCV, 2017. 3 [57] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Ip-adapter: Text compatible image prompt adapter In arXiv preprint Yang. for text-to-image diffusion models. arXiv:2308.06721, 2023. 17 [58] Jianglong Ye, Peng Wang, Kejie Li, Yichun Shi, and Heng Wang. Consistent-1-to-3: Consistent image to 3d view synthesis via geometry-aware diffusion models. In 3DV, 2024. [59] Jonathan Young. xatlas: Library for Mesh Parameterization. GitHub repository, 2018. 7, 15 [60] Xianfang Zeng, Xin Chen, Zhongqi Qi, Wen Liu, Zibo Zhao, Zhibin Wang, Bin Fu, Yong Liu, and Gang Yu. Paint3d: Paint anything 3d with lighting-less texture diffusion models. In CVPR, 2024. 2, 3, 4, 5, 7, 12, 13, 15, 16,"
        },
        {
            "title": "Appendix",
            "content": "A. Detailed Discussion on Related Works A.1. Baseline Methods In this section, we review the most representative baseline that we compared during the evaluation and discuss its strengths and weaknesses. Other great close-source methods, such as Meta 3D TextureGen [4], are removed from the comparison scoop. We omit the SOTA method Text2Tex [6] in this report due to the extensive evaluation in previous literature [34, 60]. TEXTure. TEXTure [43] presents method for generating 3D textures from textual descriptions using pretrained depth-to-image diffusion model. TEXTure employs an iterative approach to ensure consistent texturing from multiple viewpoints by dividing rendered images into keep, refine, and generate regions. It supports texture transfer and editing through both text prompts and user input. However, the method can produce global inconsistencies when handling complex geometries or viewpoints that do not fully capture the model, which the authors identify as areas for future improvement. Paint3D. Paint3D [60] is two-stage generative method. In the first stage, it utilizes ControlNet to generate textures from individual viewpoints. In the second stage, it innovatively proposes direct texture generation in UV space. Paint3D [60] employs UV position map as the control signal to train ControlNet [62], leveraging the generative capabilities of existing diffusion models to produce corresponding textures. However, due to significant differences between texture map samples and image samples, the diffusion network inherently lacks robust texture generation capabilities. Moreover, during the control generation process using the UV position map, the diffusion model tends to assign similar colors to atlas textures that are closer in 2D mapping space, rather than to the 3D space represented by the control channels. As result, Paint3D [60] performs poorly on most automatically unwrapped complex texture maps, leading to numerous instances of misaligned textures. SyncMVD. SyncMVD [34] proposes zero-shot texture generation method. It addresses the limitations of asynchronous diffusion that plague traditional project-andinpaint techniques. While previous approaches generate textures from individual views without adequate synchronization, leading to inconsistencies, SyncMVD synchronizes the diffusion processes for multi-view generation. This innovative method facilitates early consensus in texture generation by sharing denoised content across overlapping views during each denoising step. As result, SyncMVD [34] achieves consistent textures that exhibit remarkable details and coherence across various perspectives. We highlight that SyncMVD does not utilize multi-view diffusion model for generating multi-view images. Instead, SyncMVD retains the iterative camera pose and subject framework generated by single-view diffusion, which often leads to multi-faces problems due to the lack of constraints between multiple views. A.2. Border Related Methods In this section, we discuss the other related works that share similar designs that are out of our evaluations scope. Meta 3D TextureGen. Meta 3D TextureGen [4] proposes two-stage texture generation method. In the first stage, normal and position controls are used to generate aesthetically pleasing multi-view images, which are then weighted and projected based on the mesh surface normals and view directions to obtain an initial UV texture map. In the second stage, the process continues in the UV space, where normal and position controls are again employed for inpainting. Finally, texture enhancement is applied to perform super-resolution and enrich texture details. Through this innovative approach, Meta 3D TextureGen can generate high-definition textures that are rich in detail and aesthetically pleasing with Emu [11] base model. However, due to the relatively independent multi-view generation in the first stage, there may be lack of consistency between the textures across different views and may be potentially vulnerable to the Janus problem. Additionally, the limited number of viewpoints in the first stage can lead to significant unobserved areas for objects with complex occlusions. This poses challenges for the subsequent texture inpainting and enhancement processes, which are performed entirely in UV space. Since the code for Meta 3D TextureGen has not been open-sourced, we do not include it in our experimental comparisons. Unique3D. Unique3D [55] is an innovative image-to-3D framework capable of efficiently generating high-quality 3D meshes from single-view images, demonstrating both high fidelity and robust generalization capabilities. It integrates multi-view diffusion models, multi-scale upsampling strategies, and the proposed ISOMER algorithm, enabling the generation of detail-rich textured meshes in 30 seconds. Similar to our method, Unique3D uses coarse-tofine manner to generate multi-view high-resolution images. Differently, Unique3D targets simultaneous geometry and texture generation, and 3D consistency is encouraged but not strictly aligned. For example, it upscales 256 256 images with multi-view ControlNet [62] and further boosts the resolution to 2k with view-separate upscaling model without any synchronization. CLAY. CLAY [63] is large-scale 3D generation model that can produce high-quality 3D geometry and materials from text or image inputs. It employs multi-resolution VAE and minimal latent diffusion transformer, supporting 12 Algorithm 1: Spatial-aware 3D Inpainting Input: colored points: set of points with color information color mask: boolean array where true indicates points with valid color n: number of nearest neighbors for KNN search Output: updated colored points: set of points with updated color information Function update colored points(colored points, colored mask): points colored points.points colors colored points[color mask].colors normals calculate surface normals of points unknown points points[color mask] unknown normals normals[color mask] tree KDTree(points, n) distances, indices tree(unknown points) neighbors normals Index Select(normals, indices) cos Cosine Similarity(unknown normals, neighbors normals) distance score Normalize(1 / distances) weight cos * distance score coloring round 0 while stage == uncolored or coloring round > 0 do for point in unknown points do neighbors KNN(points, n) new color Weighted-Average(weight, neighbors) colored points.assign color(point, new color) Calculate total number of colored points if coloring progress then Increment coloring round else Decrease coloring round or exit loop if no further progress return colored points various control modalities such as multi-view images and voxels to facilitate precise 3D asset creation. The texturing module of CLAY [63] is also built upon MVDream [47], they modify it by adding additional channels and modalities to support physical-based rendering (PBR), ControlNet [62] to achieve view control, and uses LoRA [24]-based fine-tuning. CLAY [63] generates the 4 orthogonal views of images, and similar to Paint3D [60] inpaint and upscale the PBR images in UV space. We argued inpainting directly on UV space is vulnerable to complicated UV unwrapping, 13 especially since the coarse UV is generated from low coverage of 4 views at resolution of 256 256 with great loss in detail. Note that CLAY focuses on PBR generation, and it has the image-prompt ability, whereas we more focus on text-to-texture generation. As the implementation is not publicly available, we exclude CLAY in our evaluation. B. Implementation Details B.1. Control Strength of I2I Finer Painting In the Synchronized Multi-view Generation (SMG) process, we employed refinement model composed of two control modules. The first module, denoted as τt, is designed to provide low-resolution multi-view (MV) images that are free of Janus artifacts and maintain multi-view consistency for the refinement stage. The second module, τg, offers geometric guidance, ensuring that the multi-view refinement process is fully aligned with the underlying mesh structure. These two components are controlled by the parameters st and sg, which allow for flexible user-defined configurations. If the user is satisfied with the generated MV images, relatively high st can be set to maintain consistency, while sg can be reduced to minimize divergence, as demonstrated in the case of the Blue Hippo in the Fig. S1. Conversely, if the user is dissatisfied with the MV images or desires more creative diversity, lower st and higher sg can be used, as shown in the Unicorn case. It is important to note that without any st, our refinement model may face Janus issues similar to those observed in SyncMVD [34] and TEXTure [43] illustrated by the Blue Hippo case in Fig. S1. B.2. Spatial-aware 3D Inpainting In Sec. 3.2, we briefly described the execution flow of the algorithm. Here, we provide more detailed explanation of the algorithm. Spatial-aware 3D Inpainting (S3I) is designed to inpaint unobserved regions after multi-view projection. We sample the mesh into dense point cloud, enabling us to apply coloring at the point cloud level to capture structural information. S3I is learning-free method that propagates color from observed regions to unobserved regions. Since the propagation is based on the k-nearest neighbors (KNN) algorithm, there can be errors in propagation across planes, particularly in seam areas. To address this, we calculate the normal vector for each point, incorporating it into the weight calculation to prevent color diffusion across non-coplanar surfaces. The pseudocode for the detailed algorithm execution is as follows: B.3. Spatial-Aware Seam-Smoothing Algorithm. In Sec. 3.3, we introduce the implementation concept It is of the Spatial-Aware Seam-Smoothing algorithm. Figure S1. High Flexibility in Proposed SMG Design. With two refinement modules τg, τt and their corresponding control strength sg, st, MVPaint can provide user versatile choices of texture generation. With larger strength of st, the generated results will have more alignment with coarse MV images. With larger strength of sg, the generated results will have more creativity (see the Unicorn case) while with higher risk of Janus problem (see the Blue Hippo case). 14 Algorithm 2: KNN Seam Smoothing Algorithm Input: colored points: set of points with color information, where each row is [xyzrgb] seam mask: boolean array where true indicates seam points n: number of nearest neighbors for KNN search Output: new color: Smoothed color for seam points Function knn seam smooth(colored points, seam mask, neighbors): non seam points colored points[seam mask] normals calculate surface normals of colored points seam normals normals[seam mask] non seam normals normals[seam mask] colors non seam points.colors tree KDTree(non seam points, n) distances, indices tree(seam points) seam neighbors normals Index Select(non seam normals, indices) cos Cosine Similarity(seam normals, seam neighbors normals) distance score Normalize(1 / distances) weight cos * distance score for point in seam points do neighbors KNN(colored points, n) new color Weighted-Average(weight, neighbors.colors) colored points.assign color(point, new color) return colored points.colors used to correct color discontinuities at seams after superresolution in the UV space. Similar to S3I, it employs knearest neighbor (KNN) search to perform weighted color averaging. The detailed pseudocode for the algorithm is as follows. B.4. Unprojection Reduction Algorithm After generating consistent multi-view images, we apply weighted projection to obtain the texture UV map. However, due to occlusion in 3D objects, this can lead to projection errors and artifacts in the UV map, as shown in Figure S3. To address this, we determine occlusion relationships and reduced the affected projection areas. The specific algorithm workflow is as follows: we first extract the 3D coordinates for each valid pixel in the UV map, generating 3D point cloud. Then, using the occlusion detection algorithm proposed in [28], we mark occluded points for each view. Finally, these occlusion marks are mapped back onto the UV map, and regions marked as occluded are excluded from projection. B.5. Discussion on UV Space Tiling Depending on the specific generation requirements, our UV upscale model DUP can be replaced by refinement network DTILE. Similar to the tiling network in the second stages in Paint3D [60] and Meta 3D TextureGen [4], DTILE is diffusion model controlled by tiling control module τtile and position map control module τpos. The position map is UV map Tpose where the 3D position of the corresponding mesh surface replaces the channel values. Formally, this optional module can be written as TTILE = DTILE(zUV, Tc, Tpose; τtile, τpos). (S1) Different from previous literature [4, 60], we find the UV tiling process is very vulnerable to continuity or the UV wrapping, leading to obvious seams on 3D mesh when UV atlas is packed randomly like Xatlas [59]. Thus, we only treat this module as an opt for DUP if users want to pursue extreme details. We give an example of comparison of UV upscaling and tiling in Fig. S2, where tiling can increase the upper bound of generation quality, while still suffering from stability. Thus, we use DUP as our default super-resolution model, and it is very important to point out that all the results in this work are generated by DUP. C. Detailed Evaluations C.1. Detailed Evaluation Settings Evaluation Elevation Selection. During the comparative experiments, we observed that each method predefined its optimal elevation angle, complicating the selection of unified rendering perspective for comparison. Our approach Figure S2. Discussion on UV upscale or tiling. Given groundtruth low-resolution UV map (first row), UV upscaling DUP generates sharp and clean textures (second row), UV tiling DTILE will add intricate but acceptable details (the left case in the third row) or wrong details (the right case in the third row). also report the quantitative results of the ablation study on SMG designs on GSO benchmark in Tab. 3. Different from the results in in-domain analysis in Tab. 3 where w/o MV Diff influence the most to the quantitative results, in crossdomain benchmark which mostly consists of scan objects, the influence of each design is much more even. What accords with the Objaverse [12] benchmark results is that the full design achieves the best objective metrics and subjective metrics except for CLIP scores. Table S1. Quantitative Results on SMG Designs on GSO Benchmark. Method FID KID CLIP User Study Overall Seamless Consistency w/o MV Sync. w/o MV Paint w/o Geo. Refinement Full Design 25.72 25.23 25.56 20.02 5.31 5.17 5. 3.12 22.49 23.36 22.49 23.25 3.98 4.04 3.85 4.13 4.19 3.99 3. 4.51 4.04 3.84 4.14 4.21 D. Additional Experiments D.1. Ablation on View Selection We conducted ablation experiments on the choice of the number of viewpoints based on the main papers ablation study. Specifically, we tested several configurations: = 4 with elevations ϕ = 0; = 8 with elevation ϕ = 0; = 16 with elevation at ϕ = 0; = 8 with elevations interleaved ϕ = 30; and = 16 with elevations interleaved between ϕ = 30. All the view azimuths θ are uniformly distributed in all experiments. Our experiments were conducted on the Objaverse [12] dataset, we use the testing elevation ϕ = 15 and evaluation metrics. We organize quantitative results and report in Tab. S2. From the results, we observe that interleaved elevations at ϕ = 30 yield better viewpoint coverage, achieving improved metrics at novel elevation of 15. Additionally, we found that increasing from 4 to 8 results in significant metric improvements, while further increasing to 16 leads to decline in metrics. This is attributed to the high number of independently generated viewpoints during the refinement phase, which causes excessive overlap and leads to over-smoothed or blurry textures, resulting in lower performance metrics. Table S2. Quantitative Results on View Number on Objaverse [12] Benchmark. View Setting FID KID CLIP = 4, ϕ = 0 = 8, ϕ = 0 = 16, ϕ = 0 = 8, ϕ = 30 = 16, ϕ = 30 35.48 23.45 25. 20.89 21.58 11.24 4.12 4.51 3.45 3.87 23.21 20.48 21.45 19.87 20.21 Figure S3. Illustration of Projection Error. When the Unprojection Reduction Algorithm is not used, occlusions may cause the color of the occluding object to be projected onto the occluded areas, resulting in artifacts. However, by applying the Unprojection Reduction Algorithm, this issue can be effectively resolved by preventing incorrect color projections onto occluded regions. involved setting the elevation angle for Paint3D and TEXTure to 30, while SyncMVD utilized rendering perspective comprising eight evenly distributed views at 0and two views at 60, which we retained. Our proposed method also employed four views at 30and four views at 30. Consequently, we selected 15 as the testing elevation and densely rendered 16 perspectives to ensure the validity of the evaluation. We also conduct additional ablation study on such setting design in Sec. D.1. C.2. Quantitative and Qualitative Results Additional Qualitative Results of T2T Evaluation. We provide additional qualitative results to showcase MVPaints outcomes, including extra comparisons with baseline methods on border categories in Fig. S4). From the qualitative results, we can conclude that SyncMVD [34] can generate good results on general lifeless objects like shoes, bags, etc. While it has severe Janus problems on generating objects with heads, see the Santa, mouse, and eagle cases. Paint3D [60]s performance is highly related to UV wrapping or texture complexity. When given easy prompts are given like copper cup or black boots, Paint3D [60] can produce decent results when texture complexity is high like Santa or eagle it generates results with artifacts and seams. TEXTure [43] generally generates texture with high image saturation with large-scale artifacts and severe Janus problems, which accords with the user study in Tab. 1 and Tab.2 where TEXTure [43] has the least user appealing scores, even though it has good subject evaluation results. Quantitative Results of Ablation on SMG Designs. We 16 D.2. Runtime Evaluation image prompt ability. We conducted runtime tests to comprehensively evaluate the proposed methods performance. Initially, we performed detailed analysis of the average time required for each module, as illustrated in Tab. S3. The total runtime of our pipeline was measured at 97.79 seconds, with the majority of the computational load concentrated in the multiview refinement step, which accounted for nearly 80% of the overall runtime with SDXL [39] model. Table S3. Runtime Evaluation of MVPaint pipeline. Stage I. SMG T2MV I2I II. S3I III. UVR SR Fix Seams Overall Time (s) 10.51 78.04 0.82 8.30 0. 97.79 runtime comparison of our method against several SOTA approaches is also conducted. Experiments were executed on uniform hardware environment utilizing an single H800 GPU, ensuring consistency in testing conditions. Notably, the model loading time and the rendering time for intermediate results or video outputs were excluded from the reported statistics. As illustrated in the following Tab. S4, despite employing greater number of steps and higher multi-view resolutions, our runtime remains on par with SOTA methods. Table S4. Runtime Comparison between the MVPaint and SOTA methods. Methods TEXTure [43] Paint3D [60] SyncMVD [34] MVPaint Time (s) 142.52 104.36 87.32 97. E. Limitations MVPaint achieves strong spatial consistency and highresolution textures, but it also has certain limitations, primarily in three areas: the aesthetic problem and the lack of Aesthetic Problem. Compared to Meta 3D TextureGen [4], our model exhibits certain deficiencies in the aesthetic quality of texture generation. This can be attributed partly to Meta 3D TextureGens [4] use of an aesthetically optimized diffusion model, Emu [11], which generates each viewpoint independently. While this approach ensures aesthetic appeal, it may compromise the consistency of the textures. In contrast, our chosen T2MV model, MVDream [47], is based on SD2 [2], which lacks diversity and aesthetic quality in its texture outputs. To address this limitation, we propose the I2I refinement method, which employs higher-resolution and more aesthetically pleasing model, SDXL [39], to supplement details or re-render the multiview images. This significantly enhances the aesthetic quality. However, increasing the re-rendering intensity raises the probability of encountering the Janus Problem, thereby limiting our ability to generate highly aesthetically pleasing textures. We believe that the emergence of superior T2MV models in the future will help mitigate the aesthetic shortcomings of our current prototype. Lack of Image Prompt Capability. Similar to Meta 3D TextureGen [4] and SyncMVD [34], our MVPaint focuses on the generation of textures from text, without emphasizing the optimization of input diversity; consequently, we do not address image prompts in this paper. In contrast, some single-view methods, such as TEXTure [43] and Paint3D [60], leverage existing diffusion models along with plug-and-play image prompt module IP-Adapter [57]. We propose two potential solutions for incorporating image prompts into MVPaint. One approach is to train LowRank Adaptation (LoRA) [24] model akin to CLAY [63], enabling image prompt functionality. Alternatively, we could directly replace the foundational model of T2MV with an image-to-multiview (I2MV) model, such as ImageDream [52]. 17 Figure S4. Additional results with border categories. The 3D models are from GSO [16] and Objaverse [12], and the text prompt is abbreviated."
        }
    ],
    "affiliations": [
        "Tencent PCG",
        "Shanghai AI Laboratory",
        "S-Lab, NTU",
        "Tsinghua University"
    ]
}