{
    "paper_title": "MIVE: New Design and Benchmark for Multi-Instance Video Editing",
    "authors": [
        "Samuel Teodoro",
        "Agus Gunawan",
        "Soo Ye Kim",
        "Jihyong Oh",
        "Munchurl Kim"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent AI-based video editing has enabled users to edit videos through simple text prompts, significantly simplifying the editing process. However, recent zero-shot video editing techniques primarily focus on global or single-object edits, which can lead to unintended changes in other parts of the video. When multiple objects require localized edits, existing methods face challenges, such as unfaithful editing, editing leakage, and lack of suitable evaluation datasets and metrics. To overcome these limitations, we propose a zero-shot $\\textbf{M}$ulti-$\\textbf{I}$nstance $\\textbf{V}$ideo $\\textbf{E}$diting framework, called MIVE. MIVE is a general-purpose mask-based framework, not dedicated to specific objects (e.g., people). MIVE introduces two key modules: (i) Disentangled Multi-instance Sampling (DMS) to prevent editing leakage and (ii) Instance-centric Probability Redistribution (IPR) to ensure precise localization and faithful editing. Additionally, we present our new MIVE Dataset featuring diverse video scenarios and introduce the Cross-Instance Accuracy (CIA) Score to evaluate editing leakage in multi-instance video editing tasks. Our extensive qualitative, quantitative, and user study evaluations demonstrate that MIVE significantly outperforms recent state-of-the-art methods in terms of editing faithfulness, accuracy, and leakage prevention, setting a new benchmark for multi-instance video editing. The project page is available at https://kaist-viclab.github.io/mive-site/"
        },
        {
            "title": "Start",
            "content": "4 2 0 2 7 1 ] . [ 1 7 7 8 2 1 . 2 1 4 2 : r MIVE: New Design and Benchmark for Multi-Instance Video Editing Samuel Teodoro1* Agus Gunawan1* Soo Ye Kim Jihyong Oh3 Munchurl Kim1 2Adobe Research 1KAIST {sateodoro, agusgun, mkimee}@kaist.ac.kr 3Chung-Ang University sooyek@adobe.com jihyongoh@cau.ac.kr https://kaist-viclab.github.io/mive-site/ Figure 1. Given video, instance masks, and target instance captions, our MIVE framework enables faithful and disentangled edits in (a) singleand (b)-(c) multi-instance levels, as well as an applicability to more fine-grained (d) partial instance level without the need for additional training. Unlike previous methods, our MIVE does not rely on global edit captions, but leverages individual instance captions. Each object mask is color-coded to match its corresponding edit caption. Zoom-in for better visualization."
        },
        {
            "title": "Abstract",
            "content": "Recent AI-based video editing has enabled users to edit videos through simple text prompts, significantly simplifying the editing process. However, recent zero-shot video editing techniques primarily focus on global or single-object edits, which can lead to unintended changes in other parts of the video. When multiple objects require localized edits, existing methods face challenges, such as unfaithful editing, editing leakage, and lack of suitable evaluation datasets and metrics. To overcome these limitations, we propose zero-shot Multi-Instance Video Editing framework, called MIVE. MIVE is general-purpose mask-based framework, not dedicated to specific objects (e.g., people). MIVE introduces two key modules: (i) Disentangled Multiinstance Sampling (DMS) to prevent editing leakage and (ii) Instance-centric Probability Redistribution (IPR) to ensure precise localization and faithful editing. Addition- *Co-first authors (equal contribution) Co-corresponding authors ally, we present our new MIVE Dataset featuring diverse video scenarios and introduce the Cross-Instance Accuracy (CIA) Score to evaluate editing leakage in multi-instance video editing tasks. Our extensive qualitative, quantitative, and user study evaluations demonstrate that MIVE significantly outperforms recent state-of-the-art methods in terms of editing faithfulness, accuracy, and leakage prevention, setting new benchmark for multi-instance video editing. The project page is available at https://kaistviclab.github.io/mive-site/. 1. Introduction The popularity of short-form videos on social media has grown significantly [27, 52]. However, editing these videos is often time-consuming [48] and can require professional assistance [33]. These challenges have driven the development of AI-based video editing (VE) tools [27], supported by advances in generative [4, 44, 66] and visual language models [41]. These tools allow users to specify desired edits testing of multi-instance VE across diverse viewpoints, instance sizes, and numbers of instances. The global editing metrics used in these methods further fail to accurately measure local editing quality, essential for multi-instance VE. From the above, four critical challenges persist in multiinstance VE: (i) Attention leakage. The absence of local control in pre-trained T2I models, imprecise input conditions [25], and the use of single global edit caption [70] hinder previous methods from effectively disentangling edit prompts; (ii) Lack of versatile multi-instance VE methods. While EVA [63] exhibits per-object editing capabilities, it is not general-purpose VE framework; (iii) Unfaithful editing. The lack of techniques to enhance editing faithfulness in multi-instance VE tasks often results in inaccurate editing; (iv) Lack of evaluation dataset and metrics. Recent methods [25, 63] are tested on limited datasets using metrics inadequate for evaluating local VE quality. To overcome these, we present MIVE, generalpurpose zero-shot Multi-Instance Video Editing framework to achieve faithful edits and reduce attention leakage by disentangling multi-instance edits. MIVE integrates easily into existing T2I models and achieves multi-instance editing capabilities through two key modules: (i) Inspired by prior works [46, 55], we design Disentangled Multiinstance Sampling (DMS) which significantly reduces attention leakage; and (ii) To enhance editing faithfulness and increase the likelihood of objects appearing within their masks, we introduce Instance-centric Probability Redistribution (IPR) in the cross-attention layers. Moreover, we evaluate MIVE on our proposed dataset, called MIVE Dataset, of 200 videos using standard metrics and novel metric, the Cross-Instance Accuracy (CIA) Score, to quantify attention leakage. Our contributions are as follows: We propose novel general-purpose mask-based zeroshot Multi-Instance Video Editing framework, called MIVE, which enables multi-instance editing for videos; We propose to disentangle multi-instance video editing through our (i) Disentangled Multi-instance Sampling (DMS) that reduces editing leakage and (ii) Instancecentric Probability Redistribution (IPR) that enhances editing localization and faithfulness; We propose new evaluation benchmark that includes novel evaluation metric, called the Cross-Instance Accuracy (CIA) Score, and new dataset, called MIVE Dataset, consisting of 200 videos with varying numbers and sizes of instances accompanied with instance-level captions and masks. The CIA Score is designed to quantify attention leakage in multi-instance editing tasks; Our extensive experiments validate the effectiveness of our MIVE in disentangling multiple edits across various instances and achieving faithful editing, significantly outperforming the most recent SOTA video editing methods. Figure 2. Limitations of previous SOTA methods. (a) ControlVideo [70] relies on single global captions, and (b) GAV [25] depends on bounding box conditions that can sometimes overlap. Both are susceptible to unfaithful editing (red arrow) and attention leakage (blue arrow). with simple text prompts [2, 40], making the editing process faster and more accessible. Recent VE methods often leverage pre-trained text-toimage (T2I) models [44]. Compared to alternative approaches, such as training models on large-scale datasets [10, 71] or fine-tuning on single videos [2, 32, 58], zeroshot methods [7, 11, 16, 25, 26, 40, 62, 70] continue to gain traction due to their efficiency and the availability of pre-trained T2I models. Most zero-shot approaches focus on global editing that modifies the entire scene [62, 70] or single-object editing that can unintentionally affect other parts of video [6, 32]. In some cases, however, users may require precise editing of particular objects without altering other parts of the video, such as replacing explicit content (e.g., cigarettes) to create family-friendly versions [68]. Local VE aims to address this problem by accurately manipulating specific objects in videos. However, adapting pre-trained T2I models for this task is challenging since they lack fine-grained control and require additional training [71] or attention manipulation [32, 63] to enable spatial control. The problem is further exacerbated when the models need to perform multiple local edits simultaneously using single long caption as observed in Fig. 2-(a) for ControlVideo [70]. This approach often leads to: (i) unfaithful editing (e.g., the red bag not transforming into yellow purse) and (ii) attention leakage [61] where the intended edit for specific object unintentionally affects other object regions (e.g., both wall and statue becoming yellow). Recently, Ground-A-Video (GAV) [25] demonstrated that simultaneous multi-object VE is feasible using T2I model [44] fine-tuned on grounding conditions [29]. However, GAV still suffers from attention leakage [61], especially when the objects bounding boxes overlap (e.g., the statue also gaining flower crown in Fig. 2-(b)). EVA [63], concurrent work, attempts to address the leakage by assigning edit prompts to object masks. However, it is designed for human-centric videos [63] and does not demonstrate diverse object (e.g., vehicles) editing capability. Furthermore, both GAV [25] and EVA [63] have been tested on limited datasets, insufficient for comprehensive 2 Figure 3. The overall framework of Multi-Instance Video Editing (MIVE). Our Disentangled Multi-instance Sampling (DMS, Sec. 3.2) consists of latent parallel sampling (LPS, blue box), latent fusion (yellow box) to fuse different instance latents, re-inversion (red box) to harmonize the latents after fusion, and noise parallel sampling (NPS, green box). In addition, our Instance-centric Probability Redistribution (IPR, Sec. 3.3) provides better spatial control. 2. Related Works Zero-shot text-guided video editing. Recent advancements in diffusion models [20, 49, 50] have accelerated the evolution of both text-to-image (T2I) [12, 38, 42, 44, 45] and text-to-video (T2V) [4, 15, 17, 21, 53, 57, 67] models for generative tasks. These breakthroughs have led to the development of numerous video editing (VE) frameworks, where pre-trained models serve as backbones. Most VE methods [11, 16, 25, 26, 40, 58, 70] rely on pre-trained T2I models [29, 44, 69], as T2V models remain less accessible to the public [5] or are computationally expensive to run [53]. To facilitate VE, recent methods have either fine-tuned T2I models on large video datasets [15, 56, 71], optimized on single input video [32, 58], or leveraged zero-shot inference [11, 16, 25, 26, 40, 62, 70]. Our work falls within zero-shot VE, allowing editing without additional training. Zero-shot VE methods often prioritize temporal consistency through various techniques such as modifying the UNets self-attention layers [6, 11, 54, 58, 62], applying optimization strategies [6, 62], smoothing input latents [25, 70], linearly combining features [16], or transforming input latents [26]. Our approach focuses more on achieving faithful editing and reducing attention leakage while adopting previous method [11] to achieve temporal consistency. Local video editing and techniques from image generation. Similar to image generation and editing, the need for fine-grained control in videos has led to the development of several local VE methods [25, 32, 63, 71]. These methods usually adopt spatial control techniques from image generation such as training additional adapters [1, 29, 55, 64, 69] or employing zero-shot techniques, e.g., optimization [3, 8, 9, 60], attention modulation [28], and multi-branch sampling [46]. In particular, AVID [71] uses masks and retrains the Stable Diffusion (SD) inpainting model [44], while Video-P2P [32] leverages an attention control method [18], enabling both to achieve finer control. Extending the local control for multi-instance VE scenarios is relatively underexplored, with few works [25, 63] tackling this challenge by similarly adopting image generation techniques [28, 29] to localize multi-instance VE. GAV [25] leverages GLIGEN [29], framework that uses bounding boxes and integrates gated self-attention layers within the transformer blocks, allowing spatial control. However, GLIGEN requires retraining the gated self-attention layers for every new T2I model (e.g. SDv1.5 and SDv2.1), reducing its flexibility. EVA [63] uses masks and discrete text embeddings for spatial control, alongside attention modulation [28] for the spatio-temporal attention (STA). However, inaccurate feature correspondences [65] in the STA amplify flickering since attention modulation, if not controlled well, causes artifacts [28]. Our work similarly modulates attention values but only within the cross-attention layers, enabling spatial control without affecting the STA. 3. Proposed Method 3.1. Overall Framework In this work, we tackle multi-instance video editing (VE) by disentangling the edits for multiple instances. Given set of input frames = 1:N and set of instance target edits = {gi}M i=1, where each target edit gi = {mi, ci} consists of instance masks mi = m1:N and corresponding edit captions ci, we modify each instance based on ci, ensuring 3 that the region outside the masks m1:N remain unedited. Figure 3 illustrates the overall framework of MIVE. MIVE falls under the category of inversion-based VE [40] (see Supplemental Sec. for preliminaries). Starting with 0 = E(f 1:N ) generated using the VAE the initial latents z1:N encoder [44], we employ the non-inflated U-Net to invert the latents using DDIM Inversion [49]. This yields sequence of inverted latents, {zt}T }T t=0, that we store for later use with representing the number of denoising steps. t=0 = {z1:N For the sampling, we introduce the Disentangled Multiinstance Sampling (DMS, Sec. 3.2), inspired by image generation methods [46, 55], to disentangle the multi-instance VE process and minimize attention leakage. Each instance is independently modified using latent parallel sampling (blue box), and the multiple denoised instance latents are harmonized through noise parallel sampling preceded by latent fusion and re-inversion (green, yellow, and red boxes, respectively). We employ the 3D U-Net [58] and 3D ControlNet [69] following [25, 70] during sampling, with depth maps = d1:N obtained via MiDas [43] as conditions. Our DMS requires each edited instance to appear within its mask mi. Vanilla cross-attention [44] can not ensure the target edit stays within mi (see Fig. 7-(b)). To address this, we introduce Instance-centric Probability Redistribution (IPR, Sec. 3.3) in the cross-attention, boosting the accuracy of edit placement inside mi. Finally, we employ FLATTEN [11] to ensure temporal consistency. 3.2. Disentangled Multi-instance Sampling To disentangle the multi-instance VE process and reduce attention leakage, we propose our Disentangled Multiinstance Sampling (DMS). As shown in Fig. 3, DMS comprises of two sampling strategies: (1) Latent Parallel Sampling (LPS) shown in blue box and (2) Noise Parallel Sampling (NPS) shown in green box preceded by latent fusion (yellow box) and re-inversion (red box). In the LPS, we independently edit each instance using its target caption ci and its mask mi through DDIM [49] denoising as follows: {ˆz1:N t1 }i = DDIM({ˆz1:N }i, ϵθ({ˆz1:N }i, ci, mi, e, t), t). (1) Then, we fuse the denoised instance latents {ˆz1:N the inverted latents z1:N {z1:N t1 }i with t1 to obtain the edited instance latents t1 }i using masking as: {z1:N t1 }i = {ˆz1:N t1 }i mi + z1:N t1 (1 mi). (2) LPS requires that the edited instance appears within mi since we are replacing the background with the inverted latents at each step. This is achieved through our proposed IPR (Sec. 3.3), providing the necessary spatial control. While we can perform multi-instance VE by solely using LPS until the end of denoising, the resulting instances 4 have artifacts because they are sampled independently (see Fig. 6-(c)). Thus, we propose to fuse the multiple instance latents { z1:N } obtained from LPS during intermediate samt pling steps as follows: = ΣM z1:N i=1({z1:N }i mi) + z1:N mB. (3) t1 . We can use ˆz1:N where mB = 1 ΣM i=1mi denotes background mask. We then perform NPS (details in next paragraph) to yield the intermediate latents ˆz1:N t1 and continue alternating between LPS and latent fusion with NPS until the end of denoising. However, the results still contain artifacts, e.g., noise and blur (see Fig. 6-(d)). Thus, we propose performing re-inversion using DDIM for steps after the latent fusion to obtain z1:N t+l followed by NPS from timestep + to to yield the final fused latent z1:N . Unlike the initial inversion, our re-inversion uses the 3D U-Net and ControlNet to avoid causing additional artifacts (see Supp.). The goal of our NPS is to harmonize the independent instance latents obtained from the LPS. In this sampling strategy, we use the re-inverted fused latents z1:N to estimate each instance noise ˆn1:N , ci, mi, e, t) using the instance caption ci. We also estimate the noise n1:N = ϵθ(z1:N , c, t) for the background using the inverted latents z1:N and an empty caption c. We then comt and n1:N to single noise through masking, and bine ˆn1:N perform one DDIM step to obtain the latents ˆz1:N t1 : = ϵθ(z1:N t t1 = DDIM(ˆz1:N ˆz1:N , ΣM i=1(ˆn1:N mi)+ n1:N mB, t) (4) 3.3. Instance-centric Probability Redistribution Our sampling requires the edited objects to appear within their masks, highlighting the need for spatial control in the U-Net because the vanilla cross-attention [44] struggles to localize edits (see Fig. 7-(b)). To address this, we propose Instance-centric Probability Redistribution (IPR), inspired by attention modulation [28]. Our IPR further achieves faithful editing with less artifacts, unlike [28] that uses maximization-minimization, potentially disrupting the spatial distribution of initial attention scores and causing artifacts (see Fig. 7-(c)). Fig. 4 shows our IPR (bottom part) and comparative illustration (top part) versus others. In the bottom part of Fig. 4, we focus on single instance editing with target caption = ci. The caption c, with text tokens, is encoded into text embeddings using pretrained CLIP model [41], which are then used as the key in the cross-attention. Each value of key Kj={S,T ,E,P } corresponds to either start of sequence S, multiple text , an end of sequence E, and multiple padding tokens. The cross-attention map between the query image features Rhwd and can be expressed as: = Softmax(QKT / d) [0, 1]hwn, (5) and 1λr, respectively. The attention probabilities, AI,jT and AI,j=E, of and are updated as follows: AI,jT = AI,jT + λS λr / NT AI,j=E = AI,j=E + λS (1 λr) (7) where NT denotes the number of text tokens. Increasing λr may lead to enhancing the editing details for certain tokens, while decreasing it may increase the overall editing fidelity. λr = 0.6 is empirically set for all experiments. 4. Evaluation Data and Metric 4.1. MIVE Dataset Construction Available datasets are unsuitable for multi-instance video editing (VE) tasks. TGVE [59] and TGVE+ [47] contain limited number of videos, have few objects with few instances per object class, and have no instance masks. The datasets (subsets of DAVIS [39], TGVE [59], or Internet videos) used in GAV [25] and EVA [63] are either partially accessible or unavailable. DAVIS [39] treats multiple instances of the same object class as single entity and is, thus, not suitable for multi-instance VE. To address this gap, we introduce the MIVE Dataset, new evaluation dataset for multi-instance VE tasks. Our MIVE Dataset features 200 diverse videos from VIPSeg [37], video panoptic segmentation dataset, with each video center-cropped to 512 512 region. Since VIPSeg lacks source captions, we use LLaVA [31] to generate captions and use Llama 3 [14] to summarize the captions to fewer tokens. We then manually insert tags in the captions to establish object-to-mask correspondence. Finally, we use Llama 3 to generate the target edit captions by swapping or retexturing each instance similar to [71]. Table 1 compares our MIVE Dataset to other VE datasets. EVAs dataset is not publicly available, so we use the statistics in their paper. Compared to others, MIVE Dataset offers the most number of evaluation videos and the greatest diversity in terms of the number of objects per video and the number of instances per object class. Our dataset also demonstrates wide range of instance sizes, from small objects covering as little as 25 pixels (0.01%) per video to large objects occupying 98.68% of the video. See Supp. Sec. C.1 for more details. 4.2. Cross-Instance Accuracy Score SpaText [1] and InstanceDiffusion [55] assess local textual alignment using the Local Textual Faithfulness, the cosine similarity between CLIP [41] text embeddings of an instance caption and image embeddings of the cropped instance. While this measures text-instance alignment, it overlooks potential cross-instance information leakage, where caption for an instance influences others. We observe that this score is sometimes higher for instances that Figure 4. comparative illustration of our IPR versus others (top) and details of our IPR (bottom). where = {Ai,j; hw, n}, corresponds to the i-th image feature, and to j-th text token ej. We propose to manipulate each attention map Ai,j through redistribution to localize edits and achieve faithful editing with less artifacts. In our IPR, we split Ai,j of an instance into two sets depending on the mask m: outside the instance AO,j = {Ai,j; mi = 0} and inside the instance AI,j = {Ai,j; mi = 1}. Our IPR is based on several experimental observations (see Supp. Sec. E.2 for details): (i) Manipulating the attention probabilities of the padding tokens Ai,jP may lead to artifacts, so we avoid altering these; (ii) Increasing tokens probability decreases editing faithfulness, while decreasing it and reallocating those values to the and tokens increases editing faithfulness. Thus, for AO,j, we zero out and tokens probabilities, reallocating them to to prevent edits outside the mask (blue dashed box in Fig. 4, bottom). In addtition, we redistribute the attention probability for the object region inside the mask by reducing the value of and redistributing it to and (red dashed box in Fig. 4, bottom). We reduce AI,j=S by λS, which decays linearly to 0 from = to = 1, formulated as: λS = (t / ) (min(mean(AI,j=S), min(AI,j=S)) + ) (6) where is warm-up value that is linearly decayed from λ to 0 in the early sampling steps < 0.1T to increase the editing fidelity especially for small and difficult objects. Increasing λ tends to enhance faithfulness but may introduce artifacts. We empirically found that λ = 0.4 is the best trade-off between faithfulness and artifact-less results. Then, we redistribute λS to and with the ratios of λr 5 Dataset Name Number of Clips Frames per Clip Objects per Clip Number of Number of Number of Object Number of Instances TGVE [59] & TGVE+ [47] EVA [63] MIVE Dataset (Ours) 76 26 200 32 128 16 32 12 1-2 1-2 3-12 Classes No Info No Info 110 per Object Class 1-2 1-2 1-20 Instance Captions Masks Instance Range of Average Instance Mask Size Per Video (%) No Masks No Info 0.01 98.68 Table 1. Comparison between our multi-instance video editing dataset with other text-guided video editing datasets. should be unaffected by an instance caption (see Supp. Sec. C.2). To address this, we propose new metric called CrossInstance Accuracy (CIA) Score that we define as follows: For each cropped instance i, we compute the cosine similarity S(Ii, Cj) between its CLIP image embeddings Ii and the text embeddings Cj for all instance captions (i, {1, ..., n}), producing an similarity matrix given by = [S(Ii, Cj)] for i, = 1, 2, ..., n. For each row in S, we set the highest similarity score to 1 and all others to 0, ideally resulting in matrix with 1s along the diagonal and 0s elsewhere, indicating that each cropped instance aligns best with its caption. The CIA is calculated as the mean of diagonal elements as: CIA = (1 / n) Σn i=1S(Ii, Ci). (8) 5. Experiments Implementation details. We conduct our experiments on single NVIDIA RTX A6000 GPU. We use Stable Diffusion [44] v1.5 with ControlNet [69] and depth [43] as condition. We perform = 50 DDIM denoising steps after doing 100 inversion steps with an empty text following [11]. Our CFG [19] scale is 12.5. We use RAFT [51] to estimate the optical flows for FLATTEN [11] that we adopt in our framework. We apply our IPR and alternate between LPS and NPS for the first 40 steps, switching to only NPS for the last 10 steps. Specifically, in the first 40 steps, we apply 9 LPS steps and and 1 NPS step with = 3 re-inversion steps, repeating this cycle four times. In the last 10 steps, we deactivate IPR and perform NPS with = 2 re-inversion steps. Evaluation metrics. To evaluate the edited frames, we report standard metrics: (i) Global Temporal Consistency (GTC) [11, 26, 62, 63]: average cosine similarity of CLIP [41] image embeddings between consecutive frames, (ii) Global Textual Faithfulness (GTF) [11, 16, 26, 63]: average similarity between the frames and global edit prompts, and (iii) Frame Accuracy (FA) [62, 63]: percentage of frames with greater similarity to the target than the source prompt. Global metrics assess overall frame quality but they overlook nuances in individual instance edits, essential for multi-instance tasks. To address this, we use Local Temporal Consistency (LTC), Local Textual Faithfulness (LTF) [1, 55], and Instance Accuracy (IA) computed using the cropped edited instances. Metric computation details are in Supp. Sec. C.3. We also quantify cross-instance leakage through our proposed CIA (Sec. 4.2) and background leakage through Background Preservation (BP) [71] that is the L1 distance between input and edited frame backgrounds. 5.1. Experimental Results We compare our framework against recent zero-shot video editing (VE) methods: (i) five global editing: ControlVideo [70], FLATTEN [11], RAVE [26], FreSCo [62], TokenFlow [16], and (ii) one multi-object editing: GAV [25]. We use single global source and edit captions for the global editing methods. We use combination of global and local source and target captions along with bounding box conditions for GAV. We exclude the concurrent work EVA [63] as the code is not yet publicly available and Video-P2P [32] since it edits one instance at time, accumulating error when used in multi-instance editing scenarios (see Supp. Sec. D.4). For each baseline, we use their default settings across all videos. Qualitative comparison. We present qualitative comparisons in Fig. 5. As shown, ControlVideo and GAV suffer from attention leakage (e.g. yellow affecting the alien in Video 1 of Fig. 5-(b) and -(g)) while FLATTEN, RAVE, TokenFlow, FreSCo, and GAV exhibit unfaithful editing for all the examples (Fig. 5-(c) to Fig. 5-(g)). Moreover, ControlVideo and FreSCo exhibit mismatch in editing instances as shown in Fig. 5-(b), Video 2 (incorrectly turning the man on the left to woman in red dress) and Fig. 5- (f), Video 1 (incorrectly turning the washing machine on the left to yellow washing machine), respectively. In contrast, our method confines edits within the instance masks, faithfully adheres to instance captions, and does not exhibit mismatch in instance editing. Quantitative comparison. Table 2 shows the quantitative comparisons. The SOTA methods yield (i) low LTF and IA scores, indicating less accurate instance-level editing, and (ii) low CIA scores, reflecting significant attention leakage. However, they score better in temporal consistency-related metrics (GTC and LTC) due to either (i) smooth results with less textures and details (Fig. 5-(b), Video 2) or (ii) unfaithful editing (Fig. 5-(c) to Fig. 5-(g)) that simply reconstructs the input video, preserving structure and motion without alIn contrast, our MIVE achieves top tering the instances. scores in GTF and key multi-instance VE metrics (LTF, IA, CIA, and BP), while maintaining competitive LTC score. Our FA ranks second to ControlVideo, which we attribute to CLIPs limitations in compositional reasoning [23, 36], preventing it from capturing complex relationships between the instances in the global caption and frames. This poten6 Method Venue Editing Scope Global Scores GTC GTF FA LTC Local Scores LTF Leakage Scores IA CIA (Ours) BP TC User Study TF Leakage ControlVideo [70] FLATTEN [11] RAVE [26] TokenFlow [16] FreSCo [62] GAV [25] MIVE (Ours) ICLR24 ICLR24 CVPR24 ICLR24 CVPR24 ICLR24 - Global Global Global Global Global Local, Multiple Local, Multiple 0.9743 0.9679 0.9675 0.9686 0.9541 0.9660 0.9604 0.2738 0.2388 0.2727 0.2569 0.2527 0.2566 0. 0.8856 0.2637 0.5777 0.5622 0.4202 0.5504 0.8557 0.9548 0.9507 0.9551 0.9478 0.9324 0.9514 0.9478 0.1960 0.1881 0.1869 0.1868 0.1860 0.1893 0.2138 0.4941 0.2469 0.3512 0.3501 0.2962 0.3703 0.6419 0.4967 0.5111 0.4945 0.5307 0.5172 0.5492 0.7100 72.8690 62.8136 64.8703 68.6688 85.1843 60.0773 54. 6.97 32.32 10.45 7.61 3.55 8.90 30.20 14.00 2.45 3.61 3.16 1.81 7.74 67.23 6.26 9.74 4.52 4.26 3.42 9.74 62.06 Table 2. Quantitative comparison for multi-instance video editing. The best and second best scores are shown in red and blue, respectively. Figure 5. Qualitative comparison for three videos (with increasing difficulty from left to right) in our MIVE dataset. (a) shows the colorcoded masks overlaid on the input frames to match the corresponding instance captions. (b)-(f) use global target captions for editing. (g) uses global and instance target captions along with bounding boxes (omitted in (a) for better visualization). Our MIVE in (h) uses instance captions and masks. Unfaithful editing examples are shown in red arrow and attention leakage are shown in green arrow."
        },
        {
            "title": "Methods",
            "content": "LTC LTF IA CIA BP R 0.9460 0.2072 0.5587 0.6663 54.6597 Only NPS 0.9483 0.2068 0.5716 0.6688 50.8549 Only LPS LPS + NPS w/o Re-Inversion 0.9485 0.2080 0.5776 0.6783 52.3240 Ours, Full 54.3452 0.9478 0.2138 0.6419 0.7100 No Modulation [44] Dense Diffusion [28] Ours, Full 0.9535 0.2060 0.5225 0.6553 50.1319 0.9482 0.2136 0.6215 0.6891 59.2100 0.9478 0.2138 0.6419 0.7100 54.3452 Table 3. Ablation study results on DMS (Sec. 3.2) and IPR (Sec. 3.3). LPS and NPS denotes Latent Parallel Sampling and Noise Parallel Sampling, respectively. The best and second best scores are shown in red and blue, respectively. tially leads to higher FA scores despite mismatches in the instance edits (e.g., edits are switched for the two people in Video 2 of Fig. 5-(b)). Note that our MIVE also shows the best BP performance without using an inpainting model, as GAV does, indicating its effectiveness in isolating instance edits while preserving and preventing leakage in the background. Additional demo videos are available in the Supp. User study. We conduct user study to compare our method with baselines and report the results in Tab. 2. We select 50 videos from our dataset covering diverse scenarios (varying number of objects per clip, number of instances per object class, and instance size). We task 31 participants to select the method with the best temporal consistency (TC), textual faithfulness (TF), and minimal editing leakage (Leakage). Our method outperforms baselines, with 67.23% chance of being selected as the best for TF and 62.06% for minimal leakage. MIVE also achieves second best TC, with 30.20% selection rate. This discrepancy with our GTC and LTC scores may stem from CLIPs focus on consecutive frame consistency, while human evaluators prioritize the overall visual stability across instances and frames. Details of our user study are in the Supp. 5.2. Ablation Studies For our Ablation Studies, we report key multi-instance VE metrics in Tab. 3 and provide the Global Scores in the Supp. Ablation study on DMS. We conduct an ablation study on our DMS, presenting qualitative results in Fig. 6 and quantitative results in Tab. 3. From Fig. 6, we observe the following: (b) using only NPS results in suboptimal and unfatifhul editing; (c) using only LPS produces noisy, blurry textures with poor details; (d) performing alternating LPS and NPS produces similar artifacts, but slightly sharper edges compared to (c), emphasizing the importance of alternating LPS and NPS; and (e) adding re-inversion produces faithful edits with less artifacts, sharper boundaries, and enhanced details, demonstrating the effectiveness of our re-inversion technique. Table 3 reinforces these findings, with our full method achieving the highest editing faithfulness, as reflected in LTF, IA, and CIA, though with slight reduction Figure 6. Ablation study on DMS (Sec. 3.2). Figure 7. Ablation study on IPR (Sec. 3.3). in BP and LTC. Overall, our final model strikes balanced trade-off between faithfulness, BP and LTC. Ablation study on IPR. We also conduct an ablation study on our IPR, with qualitative results presented in Fig. 7 and quantitative results in Tab. 3. As shown in Fig. 7-(b), the model tends to reconstruct the input frames when modulation is omitted. The reconstruction thereby preserves the high LTC of the input but greatly suffers from unfaithful editing (low LTF and IA) and leakage (low CIA). Introducing modulation via DenseDiffusion [28] (Fig. 7-(c)) slightly reduces LTC but enhances faithfulness and produces less leakage, though it produces severe blurring and smoothing artifacts due to the change in spatial distribution of attention maps. These artifacts in turn produce worse BP. In contrast, Our IPR produces faithful edits (best LTF and IA) and less leakage (best CIA), while also producing less artifacts (better BP) than DenseDiffusion with slight reduction in LTC. 8 6. Conclusion In this work, we introduce MIVE, novel general-purpose mask-based multi-instance video editing framework featuring Disentangled Multi-instance Sampling (DMS) and Instance-centric Probability Redistribution (IPR). Our method achieves faithful and disentangled edits with minimal attention leakage, outperforming existing SOTA methods in both qualitative and quantitative analyses on our newly proposed MIVE Dataset. We further propose new Cross-Instance Accuracy (CIA) Score to quantify attention leakage. Our user study supports MIVEs robustness and effectiveness with participants favoring our method."
        },
        {
            "title": "References",
            "content": "[1] Omri Avrahami, Thomas Hayes, Oran Gafni, Sonal Gupta, Yaniv Taigman, Devi Parikh, Dani Lischinski, Ohad Fried, and Xi Yin. Spatext: Spatio-textual representation for controllable image generation. In CVPR. IEEE, 2023. 3, 5, 6, 13 [2] Omer Bar-Tal, Dolev Ofri-Amar, Rafail Fridman, Yoni Kasten, and Tali Dekel. Text2live: Text-driven layered image and video editing. In ECCV, pages 707723. Springer, 2022. 2 [3] Omer Bar-Tal, Lior Yariv, Yaron Lipman, and Tali Dekel. MultiDiffusion: Fusing diffusion paths for controlled image generation. In ICML, pages 17371752, 2023. 3 [4] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, pages 2256322575, 2023. 1, 3 [5] Tim Brooks, Bill Peebles, Connor Holmes, Will DePue, Yufei Guo, Li Jing, David Schnurr, Joe Taylor, Troy Luhman, Eric Luhman, et al. Video generation models as world simulators, 2024. 3 [6] Duygu Ceylan, Chun-Hao Huang, and Niloy Mitra. Pix2video: Video editing using image diffusion. In ICCV, pages 2320623217, 2023. 2, [7] Wenhao Chai, Xun Guo, Gaoang Wang, and Yan Lu. Stablevideo: Text-driven consistency-aware diffusion video editing. In ICCV, pages 2304023050, 2023. 2 [8] Hila Chefer, Yuval Alaluf, Yael Vinker, Lior Wolf, and Daniel Cohen-Or. Attend-and-excite: Attention-based semantic guidance for text-to-image diffusion models. ACM Trans. Graph., 42(4):110, 2023. 3 [9] Minghao Chen, Iro Laina, and Andrea Vedaldi. Trainingfree layout control with cross-attention guidance. In WACV, pages 53435353, 2024. 3 [10] Jiaxin Cheng, Tianjun Xiao, and Tong He. Consistent videoto-video transfer using synthetic dataset. In ICLR, 2024. 2 [11] Yuren Cong, Mengmeng Xu, christian simon, Shoufa Chen, Jiawei Ren, Yanping Xie, Juan-Manuel Perez-Rua, Bodo Rosenhahn, Tao Xiang, and Sen He. FLATTEN: optical FLow-guided ATTENtion for consistent text-to-video editing. In ICLR, 2024. 2, 3, 4, 6, 7, 16, [12] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. NeurIPS, 34:87808794, 2021. 3, 12 [13] Ankit Dhiman, Manan Shah, Rishubh Parihar, Yash Bhalgat, Lokesh Boregowda, and Venkatesh Babu. Reflecting reality: Enabling diffusion models to produce faithful mirror reflections. arXiv preprint arXiv:2409.14677, 2024. 29 [14] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. 5, 12, 13 [15] Patrick Esser, Johnathan Chiu, Parmida Atighehchian, Jonathan Granskog, and Anastasis Germanidis. Structure and content-guided video synthesis with diffusion models. In ICCV, pages 73467356, 2023. 3 [16] Michal Geyer, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Tokenflow: Consistent diffusion features for consistent video editing. In ICLR, 2024. 2, 3, 6, 7, 16, 20 [17] Yingqing He, Tianyu Yang, Yong Zhang, Ying Shan, and Qifeng Chen. Latent video diffusion models for high-fidelity long video generation. arXiv preprint arXiv:2211.13221, 2022. 3 [18] Amir Hertz, Ron Mokady, Jay Tenenbaum, Kfir Aberman, Yael Pritch, and Daniel Cohen-Or. Prompt-to-prompt imarXiv preprint age editing with cross attention control. arXiv:2208.01626, 2022. [19] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. 6, 12 [20] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. 3 [21] Jonathan Ho, William Chan, Chitwan Saharia, Jay Whang, Ruiqi Gao, Alexey Gritsenko, Diederik Kingma, Ben Poole, Mohammad Norouzi, David Fleet, et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. 3 [22] Seongmin Hong, Kyeonghyun Lee, Suh Yoon Jeon, Hyewon Bae, and Se Young Chun. On exact inversion of dpm-solvers. In CVPR, pages 70697078, 2024. 29 [23] Yushi Hu, Benlin Liu, Jungo Kasai, Yizhong Wang, Mari Ostendorf, Ranjay Krishna, and Noah Smith. Tifa: Accurate and interpretable text-to-image faithfulness evaluation with question answering. In ICCV, pages 2040620417, 2023. 6, 20, 21 [24] Ondˇrej Jamriˇska, ˇSarka Sochorova, Ondˇrej Texler, Michal Lukaˇc, Jakub Fiˇser, Jingwan Lu, Eli Shechtman, and Daniel S`ykora. Stylizing video by example. ACM Transactions on Graphics (TOG), 38(4):111, 2019. 28 [25] Hyeonho Jeong and Jong Chul Ye. Ground-a-video: Zeroshot grounded video editing using text-to-image diffusion models. In ICLR, 2024. 2, 3, 4, 5, 6, 7, 16, 20, [26] Ozgur Kara, Bariscan Kurtkaya, Hidir Yesiltepe, James Rehg, and Pinar Yanardag. Rave: Randomized noise shuffling for fast and consistent video editing with diffusion models. In CVPR, pages 65076516, 2024. 2, 3, 6, 7, 16, 20 9 [27] Jini Kim and Hajun Kim. Unlocking creator-ai synergy: Challenges, requirements, and design opportunities in aipowered short-form video production. In Proceedings of the CHI Conference on Human Factors in Computing Systems, pages 123, 2024. 1 [28] Yunji Kim, Jiyoung Lee, Jin-Hwa Kim, Jung-Woo Ha, and Jun-Yan Zhu. Dense text-to-image generation with attention modulation. In ICCV, pages 77017711, 2023. 3, 4, 8, 22, 25 [29] Yuheng Li, Haotian Liu, Qingyang Wu, Fangzhou Mu, Jianwei Yang, Jianfeng Gao, Chunyuan Li, and Yong Jae Lee. Gligen: Open-set grounded text-to-image generation. In CVPR, pages 2251122521, 2023. 2, 3 [30] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, pages 740755. Springer, 2014. [31] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In CVPR, pages 2629626306, 2024. 5, 12, 13 [32] Shaoteng Liu, Yuechen Zhang, Wenbo Li, Zhe Lin, and Jiaya Jia. Video-p2p: Video editing with cross-attention control. In CVPR, pages 85998608, 2024. 2, 3, 6, 21 [33] Ying Liu, Dickson KW Chiu, and Kevin KW Ho. Short-form videos for public library marketing: performance analytics of douyin in china. Applied Sciences, 13(6):3386, 2023. 1 [34] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver++: Fast solver for guided sampling of diffusion probabilistic models. arXiv preprint arXiv:2211.01095, 2022. 29 [35] Cheng Lu, Yuhao Zhou, Fan Bao, Jianfei Chen, Chongxuan Li, and Jun Zhu. Dpm-solver: fast ode solver for diffusion probabilistic model sampling in around 10 steps. NeurIPS, 35:57755787, 2022. 29 [36] Zixian Ma, Jerry Hong, Mustafa Omer Gul, Mona Gandhi, Irena Gao, and Ranjay Krishna. Crepe: Can vision-language foundation models reason compositionally? In CVPR, pages 1091010921, 2023. 6, 20, [37] Jiaxu Miao, Xiaohan Wang, Yu Wu, Wei Li, Xu Zhang, Yunchao Wei, and Yi Yang. Large-scale video panoptic segmentation in the wild: benchmark. In CVPR, pages 21033 21043, 2022. 5, 12 [38] Alexander Quinn Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav Shyam, Pamela Mishkin, Bob Mcgrew, Ilya Sutskever, and Mark Chen. GLIDE: Towards photorealistic image generation and editing with text-guided diffusion models. In ICML, pages 1678416804, 2022. 3 [39] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alex Sorkine-Hornung, and Luc Van Gool. The 2017 davis challenge on video object segmentation. arXiv preprint arXiv:1704.00675, 2017. 5 [40] Chenyang Qi, Xiaodong Cun, Yong Zhang, Chenyang Lei, Xintao Wang, Ying Shan, and Qifeng Chen. Fatezero: Fusing attentions for zero-shot text-based video editing. In ICCV, pages 1593215942, 2023. 2, 3, 4 [41] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML, pages 87488763. PMLR, 2021. 1, 4, 5, 6, [42] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical text-conditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1 (2):3, 2022. 3 [43] Rene Ranftl, Katrin Lasinger, David Hafner, Konrad Schindler, and Vladlen Koltun. Towards robust monocular depth estimation: Mixing datasets for zero-shot cross-dataset transfer. IEEE TPAMI, 44(3):16231637, 2020. 4, 6 [44] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image synthesis with latent diffusion models. In CVPR, pages 10684 10695, 2022. 1, 2, 3, 4, 6, 8, 12, 17, 22 [45] Chitwan Saharia, William Chan, Saurabh Saxena, Lala Li, Jay Whang, Emily Denton, Kamyar Ghasemipour, Raphael Gontijo Lopes, Burcu Karagol Ayan, Tim Salimans, et al. Photorealistic text-to-image diffusion models with deep language understanding. NeurIPS, 35:3647936494, 2022. 3 [46] Takahiro Shirakawa and Seiichi Uchida. Noisecollage: layout-aware text-to-image diffusion model based on noise cropping and merging. In CVPR, pages 89218930, 2024. 2, 3, 4 [47] Uriel Singer, Amit Zohar, Yuval Kirstain, Shelly Sheynin, Adam Polyak, Devi Parikh, and Yaniv Taigman. Video editing via factorized diffusion distillation. In ECCV, pages 450 466. Springer, 2024. 5, 6 [48] Than Htut Soe. Automation in video editing: Assisted workflows in video editing. In AutomationXP@ CHI, 2021. 1 [49] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. In ICLR, 2021. 3, 4, 12 [50] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. In ICLR, 2021. 3 [51] Zachary Teed and Jia Deng. Raft: Recurrent all-pairs field In ECCV, pages 402419. transforms for optical flow. Springer, 2020. 6 [52] Baptist Vandersmissen, Frederic Godin, Abhineshwar Tomar, Wesley De Neve, and Rik Van de Walle. The rise of mobile and social short-form video: an in-depth measurement study of vine. In Workshop on Social Multimedia and Storytelling (SoMuS 2014), pages 110, 2014. 1 [53] Jiuniu Wang, Hangjie Yuan, Dayou Chen, Yingya Zhang, Xiang Wang, and Shiwei Zhang. Modelscope text-to-video technical report. arXiv preprint arXiv:2308.06571, 2023. 3 [54] Jiangshan Wang, Yue Ma, Jiayi Guo, Yicheng Xiao, Gao Huang, and Xiu Li. Cove: Unleashing the diffusion feature correspondence for consistent video editing. arXiv preprint arXiv:2406.08850, 2024. 3 [55] Xudong Wang, Trevor Darrell, Sai Saketh Rambhatla, Rohit Girdhar, and Ishan Misra. Instancediffusion: Instance-level control for image generation. In CVPR, pages 62326242, 2024. 2, 3, 4, 5, 6, 13, 29 10 [70] Yabo Zhang, Yuxiang Wei, Dongsheng Jiang, Xiaopeng Zhang, Wangmeng Zuo, and Qi Tian. Controlvideo: Training-free controllable text-to-video generation. In ICLR, 2024. 2, 3, 4, 6, 7, 16, 20, [71] Zhixing Zhang, Bichen Wu, Xiaoyan Wang, Yaqiao Luo, Luxin Zhang, Yinan Zhao, Peter Vajda, Dimitris Metaxas, and Licheng Yu. Avid: Any-length video inpainting with diffusion model. In CVPR, pages 71627172, 2024. 2, 3, 5, 6, 12, 13 [56] Xiang Wang, Hangjie Yuan, Shiwei Zhang, Dayou Chen, Jiuniu Wang, Yingya Zhang, Yujun Shen, Deli Zhao, and Jingren Zhou. Videocomposer: Compositional video synthesis with motion controllability. NeurIPS, 36, 2024. 3 [57] Yaohui Wang, Xinyuan Chen, Xin Ma, Shangchen Zhou, Ziqi Huang, Yi Wang, Ceyuan Yang, Yinan He, Jiashuo Yu, Peiqing Yang, et al. Lavie: High-quality video generation with cascaded latent diffusion models. arXiv preprint arXiv:2309.15103, 2023. 3 [58] Jay Zhangjie Wu, Yixiao Ge, Xintao Wang, Stan Weixian Lei, Yuchao Gu, Yufei Shi, Wynne Hsu, Ying Shan, Xiaohu Qie, and Mike Zheng Shou. Tune-a-video: One-shot tuning of image diffusion models for text-to-video generation. In ICCV, pages 76237633, 2023. 2, 3, 4 [59] Jay Zhangjie Wu, Xiuyu Li, Difei Gao, Zhen Dong, Jinbin Bai, Aishani Singh, Xiaoyu Xiang, Youzeng Li, Zuwei Huang, Yuanxi Sun, et al. Cvpr 2023 text guided video editing competition. arXiv preprint arXiv:2310.16003, 2023. 5, 6 [60] Jinheng Xie, Yuexiang Li, Yawen Huang, Haozhe Liu, Wentian Zhang, Yefeng Zheng, and Mike Zheng Shou. Boxdiff: Text-to-image synthesis with training-free box-constrained diffusion. In ICCV, pages 74527461, 2023. [61] Fei Yang, Shiqi Yang, Muhammad Atif Butt, Joost van de Weijer, et al. Dynamic prompt learning: Addressing crossattention leakage for text-based image editing. NeurIPS, 36: 2629126303, 2023. 2 [62] Shuai Yang, Yifan Zhou, Ziwei Liu, and Chen Change Loy. Fresco: Spatial-temporal correspondence for zero-shot video translation. In CVPR, pages 87038712, 2024. 2, 3, 6, 7, 16, 20, 28 [63] Xiangpeng Yang, Linchao Zhu, Hehe Fan, and Yi Yang. Eva: Zero-shot accurate attributes and multi-object video editing. arXiv preprint arXiv:2403.16111, 2024. 2, 3, 5, 6 [64] Zhengyuan Yang, Jianfeng Wang, Zhe Gan, Linjie Li, Kevin Lin, Chenfei Wu, Nan Duan, Zicheng Liu, Ce Liu, Michael Zeng, et al. Reco: Region-controlled text-to-image generation. In CVPR, pages 1424614255, 2023. 3 [65] Chang-Han Yeh, Chin-Yang Lin, Zhixiang Wang, Chi-Wei Hsiao, Ting-Hsuan Chen, and Yu-Lun Liu. Diffir2vrzero: Zero-shot video restoration with diffusion-based image restoration models. arXiv preprint arXiv:2407.01519, 2024. 3 [66] Lijun Yu, Yong Cheng, Kihyuk Sohn, Jose Lezama, Han Zhang, Huiwen Chang, Alexander Hauptmann, MingHsuan Yang, Yuan Hao, Irfan Essa, et al. Magvit: Masked generative video transformer. In CVPR, pages 1045910469, 2023. [67] Sihyun Yu, Kihyuk Sohn, Subin Kim, and Jinwoo Shin. Video probabilistic diffusion models in projected latent space. In CVPR, pages 1845618466, 2023. 3 [68] Asim Sinan Yuksel and Fatma Gulsah Tan. Deepcens: deep learning-based system for real-time image and video censorship. Expert Systems, 40(10):e13436, 2023. 2 [69] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In ICCV, pages 38363847, 2023. 3, 4, 6, 12 11 MIVE: New Design and Benchmark for Multi-Instance Video Editing"
        },
        {
            "title": "Supplementary Material",
            "content": "A. Preliminaries We briefly introduce how inversion-based video editing is achieved in this subsection. Given set of input frames 1:N , each frame is encoded into clean latent code zi 0 = E(f i) using the encoder of the Latent Diffusion Model (LDM) [44]. DDIM inversion [12, 49] is applied to map the clean latent zi 0 to noised latent ˆzi through reversed diffusion timesteps : 1 using the U-Net ϵθ of the LDM: ˆzi t1 ˆzi = αt 1 αt1ϵθ(It1) αt1 + 1 αtϵθ(It1), (9) where αt denotes noise scheduling parameter [44] and It = (ˆzt, t, c, e) represents the noisy latent at timestep with text prompt as input. After inversion, the latents ˆzi at = are used as input to the DDIM denoising process [49] to perform editing: ˆzi t1 = αt1 ˆzi 1 αtϵθ(It) + αt 1 αt1ϵθ(It). (10) ControlNet [69] condition can be added as additional guidance for the sampling and can be obtained from any structured information (e.g., depth maps). The edited frame ˆf = D(ˆzi 0) is obtained using the decoder of the LDM. classifier-free guidance [19] scale of scf = 1 and larger scale scf 1 are used during inversion and denoising, respectively. B. Demo Videos We provide sample input and edited videos for our method along with baseline methods in our project page: https: //kaist-viclab.github.io/mive-site/. C. Dataset and Metrics Additional Details C.1. MIVE Dataset Construction To create our MIVE Dataset, we center-crop 512 512 region from each video in VIPSeg [37]. We only select the videos where all instances remain visible across frames. We also exclude videos with fewer than 12 frames. To ensure diversity, we remove videos that contain only one of the 40 most frequently occurring object classes (e.g., only persons). This process yields final subset of 200 videos from VIPSeg. Since VIPSeg does not include source captions, we generate video captions using visual language model (LLaVA 12 [31]) and an LLM (Llama 3 [14]). We show our caption generation pipeline in Fig. 8 of this Supplemental. First, we prompt LLaVA to describe the scene of each frame in the video and to include the known objects from the video in the caption. From all the frame captions, we choose one that includes the most instances as the representative caption of the video. We then prompt Llama 3 [14] to summarize this initial caption into more concise format with fewer tokens and provide the output in JSON format (ommitted in Fig. 8 to simplify visualization). Although LLaVA and Llama provide useful initial caption for each video, not all objects are accurately captured in each video. Therefore, we manually refine the caption and add the starting and ending tags for each instance token to serve as reference for their corresponding segmentation masks. To generate target caption for each instance, we use Llama 3 to prompt edits, such as retexturing or swapping instances similar to [71]. We instruct Llama 3 to generate five candidates for the target caption of each instance and we randomly select one to create final instance caption. Finally, we modify the original source caption by replacing the source instance captions with the final target instance captions to generate the global target caption. Although we only use the thing instances for our task, we still generate captions for the stuff objects and background elements. This setup allows for future extensions of our dataset, where an LLM can be employed to create target edits for these objects. We show sample frames and captions in Figures 9 and 10. C.2. Cross-Instance Accuracy (CIA) Score Our Cross-Instance Accuracy (CIA) Score is proposed to address the shortcomings of existing video editing metrics, particularly their inability of accounting for potential editing leakage (commonly termed as attention leakage in literatures). The Global Textual Faithfulness (GTF) and Frame Accuracy (FA) metrics cannot account for cases where the text prompt for one instance leaks to another instance, as they cannot capture the nuances of individual instances within the global captions and frames. The Instance Accuracy (IA) only determines whether cropped instance is more aligned with its target caption compared to its source caption, without considering whether or not another instance caption is affecting the cropped instance. The Local Textual Faithfulness (LTF) only quantifies the alignment of the instances target caption and its corresponding cropped instance, but it also overlooks potential attention leakage from other instance captions. While Background Preservation (BP) can measure modifications and attention leakage Use Case Number of Clips Number of Number of Frames per Clip Objects per Clip Number of Object Number of Instances Range of Average Instance Mask Size Per Video (%) per Object Class Classes MIVE Dataset (full set) For Editing (Things without Stuff) For User Study For Demo 200 200 50 40 12 46 12 46 13 46 13 46 3-12 1-9 2-9 2-9 110 54 38 1-20 1-17 1-16 1-16 0.01 98.68 0.02 77.35 0.05 75.62 0.05 69.74 Table 4. Statistics of our multi-instance video editing dataset in various use cases. (i) MIVE Dataset is the full set of our dataset including both stuff and thing categories. (ii) For Editing, we only edit objects in the thing categories, thus, decreasing some statistics. (iii) For User Study, we only select 50 videos that cover diverse scenarios. (iv) For the Demo, we only select 40 videos that cover diverse scenarios. Figure 8. MIVE Dataset caption generation pipeline for each video. Yellow box: The process starts by prompting LLaVA [31] to generate caption for each video that includes all instances in the video. Since LLaVa can only accept images, we perform the prompting for each frame and select one representative caption that includes the most instances. Red box: We utilize Llama 3 [14] to summarize the initial caption generated by LLaVa. Blue box: We manually include all of the instances of interest that are not included in the caption and manually add tags to map the instance captions to corresponding segmentation masks. Purple box: We utilize Llama 3 to generate target captions by retexturing or swapping instances similar to [71] for each instance. in the background, it does not take into consideration the leakage in instances that should be unaffected by given instance caption. We further observe that the Local Textual Faithfulness between cropped instance and another target instance caption, which should not affect the cropped instance, is sometimes higher than the score between the cropped instance and its corresponding target instance caption (red text in Fig. 11). The nature of our problem, along with the limitations and observations presented above, motivated us to propose new evaluation metric called the Cross-Instance Accuracy (CIA) Score that can account for attention leakage across instances in video editing tasks. We visualize the computation for our CIA Score in Fig. 11, and provide detailed explanation of how to calculate the CIA Score in Sec. 4.2 of our main paper. C.3. Local Metrics Computation To calculate the local scores, we crop each instance using the bounding boxes inferred from their masks and add padding to preserve their aspect ratios. The Local Textual Faithfulness (LTF) is calculated as the average cosine similarity between the CLIP [41] image embeddings of each cropped instance and the text embeddings of its instance caption, following [1, 55]. Local Temporal Consistency (LTC) is similarly measured as the average cosine similar13 Figure 9. Sample frames and captions from our MIVE Dataset (Part 1). The colored texts are the instance target captions. For each video, the instance masks are color-coded to correspond with the instance target captions in the global target caption. ity between cropped instances across consecutive frames. Instance Accuracy (IA) is the percentage of instances with higher similarity to the target instance caption than to the source instance caption. D. Comparison with State-of-the-Art Methods D.1. Qualitative Comparison We present more visual comparison in Figures 12, 13, and 14. The videos are selected with increasing difficulty based on the number of instances to edit. 14 Figure 10. Sample frames and captions from our MIVE Dataset (Part 2). The colored texts are the instance target captions. For each video, the instance masks are color-coded to correspond with the instance target captions in the global target caption. D.2. Quantitative Results Based on Instance Sizes (ii) the number of instances. and Numbers of Instances In this part, we analyze whether the baseline methods and our MIVE exhibit biases based on: (i) the instance sizes or 15 Quantitative results based on instance sizes. Table 5 Figure 11. Visualization of the Cross-Instance Accuracy (CIA) Score computation. We calculate the Local Textual Faithfulness (LTF) between each cropped instance and all the instance captions. For each row, we assign 1 to the maximum LTF (shown in red) and 0 to the rest. The CIA Score is calculated as the mean of the diagonal elements (shown in green). Method Venue Editing Scope Local Scores (Small) Local Scores (Medium) Local Scores (Large) LTC LTF IA CIA LTC LTF IA CIA LTC LTF IA CIA ControlVideo [70] FLATTEN [11] RAVE [26] TokenFlow [16] FreSCo [62] GAV [25] MIVE (Ours) ICLR24 ICLR24 CVPR24 ICLR24 CVPR24 ICLR24 - Global Global Global Global Global Local, Multiple Local, Multiple 0.9546 0.9519 0.9547 0.9486 0.9288 0.9529 0.9537 0.1684 0.1789 0.1752 0.1783 0.1790 0.1803 0.1794 0.3479 0.4230 0.3953 0.4517 0.4044 0.4224 0. 0.3875 0.4215 0.4310 0.4434 0.4283 0.4680 0.6059 0.9516 0.9457 0.9527 0.9406 0.9226 0.9498 0.9441 0.1852 0.1902 0.1830 0.1876 0.1892 0.1932 0.1997 0.3248 0.2338 0.2932 0.3297 0.2937 0.3298 0.4647 0.5220 0.5766 0.5444 0.6085 0.5945 0.5913 0.6883 0.9580 0.9547 0.9581 0.9522 0.9383 0.9550 0. 0.2048 0.1876 0.1910 0.1880 0.1852 0.1889 0.2243 0.5845 0.2371 0.3684 0.3536 0.2709 0.3740 0.7414 0.5003 0.4949 0.4815 0.5090 0.4958 0.5420 0.7331 Table 5. Quantitative comparison based on instance size. We only show Local Scores since these are the only scores that can be computed depending on the instance size. We follow the categorization of instance size from COCO dataset [30], where: (i) small instance has area < 322, (ii) medium instance has area between 322 and 962, and (iii) large instance has area > 962. The best and second best scores are shown in red and blue, respectively. presents comparison between our method and baselines across various instance sizes. To categorize instance sizes, we follow the COCO dataset [30], where: (i) small instances have the areas < 322, (ii) medium instances have the areas between 322 and 962, and (iii) large instances have the areas > 962. Across all videos, there are 69 small instances, 297 medium instances, and 434 large instances. To compute the area of each instance in the video, we average its area across all frames. As shown in Tab. 5, the Local Temporal Consistency scores align with the findings in the main paper (Sec. 5.1), where RAVE and ControlVideo achieve the best and second-best performance, respectively, while our MIVE method demonstrates competitive results. For medium and large instance sizes, our method achieves the best performance in textual faithfulness (LTF and IA) and attention leakage (CIA). In the medium-sized instance scenario, our MIVE shows LTF, IA, and CIA improvement of approximately 0.006, 13.5%, and 8%, respectively, compared to the second-best method. Similarly, in the large-sized instance scenario, MIVE achieves an improvement of approximately 0.02, 16%, and 19%, respec16 Figure 12. Qualitative comparison with SOTA video editing methods (Part 1). (a) shows the color-coded masks overlaid on the input frames to match the corresponding instance captions. (b)-(f) use global target captions for editing. (g) uses global and instance target captions along with bounding boxes (omitted in (a) for better visualization). Our MIVE in (h) uses instance captions and masks. Unfaithful editing examples are shown in red arrow and attention leakage are shown in green arrow. tively, over the second-best method. The greater improvement in large-sized instances indicates that our method performs better on larger objects likely due to our reliance on masks as condition for editing. This reasoning is further supported by the results of GAV. GAV, which uses bounding boxes, tends to have more overlaps in large-instance scenarios, leading to worse performance compared to the global editing baselines. These findings highlight the advantage of using masks over bounding boxes as conditions for multiinstance video editing. In the small instance size scenario, the LTF scores of all methods tend to be lower compared to medium and large instances. This highlights the challenges of editing small instances in diffusion-based video editing methods, which are caused by the downsampling in the VAE of the LDM [44]. For small instances, our MIVE achieves the best performance in attention leakage (CIA) and competitive performance in editing faithfulness (LTF and IA). For LTF, 17 Figure 13. Qualitative comparison with SOTA video editing methods (Part 2). (a) shows the color-coded masks overlaid on the input frames to match the corresponding instance captions. (b)-(f) use global target captions for editing. (g) uses global and instance target captions along with bounding boxes (omitted in (a) for better visualization). Our MIVE in (h) uses instance captions and masks. Unfaithful editing examples are shown in red arrow and attention leakage are shown in green arrow. the difference between our method and the best-performing method (GAV) is minimal (0.001). It is worth noting that the LTF of our method for small instances could potentially be improved by increasing the value of λ in our Instancecentric Probability Redistribution (IPR). However, for IA, our MIVE scores 5% lower than the best method, despite achieving higher LTF. To better understand this issue, we analyze the qualitative results of TokenFlow, as shown in our demo and in Fig. 12, Fig. 13, and Fig. 14. TokenFlow often produces artifacts in multi-instance editing scenarios. For example, in video 2 (middle column) of Fig. 13, where the target instance caption is baseball (source instance caption: golf ball), TokenFlow transforms the golf ball into grassy area, while other methods fail to edit this region, thereby preserving the original golf ball. Interestingly, the IA score of TokenFlow for this edited golf ball Figure 14. Qualitative comparison with SOTA video editing methods (Part 3). (a) shows the color-coded masks overlaid on the input frames to match the corresponding instance captions. (b)-(f) use global target captions for editing. (g) uses global and instance target captions along with bounding boxes (omitted in (a) for better visualization). Our MIVE in (h) uses instance captions and masks. Unfaithful editing examples are shown in red arrow and attention leakage are shown in green arrow. is 0.87, whereas our MIVE scores 0.5 even though our results look more like baseball. This observation suggests that the IA metric may not be well-suited for evaluating performance in the scenarios with small-sized instances. Quantitative results based on number of instances. In addition to the quantitative comparisons with respect to various instance sizes, we provide further analysis for the different numbers of edited instances in each video, as shown in Tab. 6. Videos are categorized into three groups: (i) easy video (EV) having 13 edited instances, (ii) medium video (MV) having 47 edited instances, and (iii) hard video (HV) having 7 or more edited instances. For the Leakage Scores, there is no significant deviation from the quantitative comIn most cases, our parisons in the main paper (Tab. 2). MIVE achieves the best Leakage Scores performance, while either GAV or FLATTEN secures the second-best perfor19 Method Venue Editing Scope Global Scores GTC GTF FA LTC Local Scores LTF Leakage Scores IA CIA (Ours) BP ControlVideo [70] FLATTEN [11] RAVE [26] TokenFlow [16] FreSCo [62] GAV [25] MIVE (Ours) ControlVideo [70] FLATTEN [11] RAVE [26] TokenFlow [16] FreSCo [62] GAV [25] MIVE (Ours) ControlVideo [70] FLATTEN [11] RAVE [26] TokenFlow [16] FreSCo [62] GAV [25] MIVE (Ours) ICLR24 ICLR24 CVPR24 ICLR24 CVPR24 ICLR24 - ICLR24 ICLR24 CVPR24 ICLR24 CVPR24 ICLR24 - ICLR24 ICLR24 CVPR24 ICLR24 CVPR24 ICLR24 - Editing on 1-3 Instances (Easy Video) - 116 Videos Global Global Global Global Global Local, Multiple Local, Multiple 0.9729 0.9661 0.9661 0.9686 0.9534 0.9643 0.9583 0.2724 0.2415 0.2698 0.2578 0.2491 0.2518 0.2738 0.8839 0.2924 0.5452 0.5710 0.4138 0.5583 0.8589 0.9513 0.9484 0.9533 0.9465 0.9327 0.9477 0.9441 0.2020 0.1893 0.1886 0.1880 0.1877 0.1915 0. Editing on 4-7 Instances (Medium Video) - 66 Videos Global Global Global Global Global Local, Multiple Local, Multiple 0.9757 0.9700 0.9689 0.9686 0.9555 0.9674 0.9614 0.2775 0.2373 0.2777 0.2559 0.2589 0.2648 0.2763 0.8834 0.2305 0.5819 0.5425 0.4276 0.5549 0.8397 0.9579 0.9526 0.9570 0.9486 0.9318 0.9549 0. 0.1875 0.1853 0.1842 0.1845 0.1835 0.1859 0.2060 Editing on >7 Instances (Hard Video) - 18 Videos Global Global Global Global Global Local, Multiple Local, Multiple 0.9781 0.9717 0.9711 0.9682 0.9538 0.9715 0.9697 0.2692 0.2274 0.2735 0.2552 0.2536 0.2580 0.2784 0.9051 0.2007 0.7714 0.5778 0.4339 0.4825 0. 0.9651 0.9584 0.9602 0.9529 0.9322 0.9628 0.9626 0.1885 0.1898 0.1857 0.1878 0.1844 0.1870 0.2002 0.5374 0.2568 0.3620 0.3480 0.2841 0.3851 0.6932 0.4543 0.2288 0.3396 0.3631 0.3177 0.3660 0.5872 0.3602 0.2499 0.3242 0.3162 0.2954 0.2911 0.5118 0.6192 0.6060 0.5964 0.6248 0.6085 0.6466 0. 0.3704 0.4199 0.3951 0.4498 0.4389 0.4676 0.6483 0.1698 0.2339 0.2026 0.2212 0.2156 0.2210 0.3669 79.3548 67.1713 71.2598 74.2420 92.0291 63.3588 58.4894 65.3667 56.7246 57.2210 62.3709 76.8818 57.8612 48.2502 58.5803 57.0569 51.7408 55.8445 71.5167 47.0554 49.9864 Table 6. Quantitative comparison for multi-instance video editing on various number of instances. We categorize 200 videos of MIVE Dataset depending on the number of edited instances: (i) Easy Video (EV): video that contains 1-3 edited instances, (ii) Medium Video (MV): video that contains 4-7 edited instances, and (iii) Hard Video (HV): video that contains > 7 edited instances. The best and second best scores are shown in red and blue, respectively. mance. This aligns with the preferences highlighted in our User Study from the main paper. For Local Scores, the results are consistent with the main paper: our MIVE achieves the best LTF and IA scores, as well as competitive LTC performance. Similarly, for Global Scores, the GTC and FA metrics are also in line with the main paper, with our method achieving competitive GTC and second-best FA results. One notable deviation from the main paper is in the GTF score. While our MIVE achieves the best GTF score in the main paper, this only holds for the EV and HV scenarios. In the MV scenario, RAVE achieves the best result, followed by ControlVideo, with our MIVE ranking third. The margin between our method and the top-performing method is small (0.0014). This deviation in GTF is likely due to CLIPs limitations in compositional reasoning [23, 36], which hinder its ability to capture complex relationships between instances in the global caption and frames. This further shows the importance of Local Scores in quantifying the performance in multi-instance video editing. In summary, while there are minor deviations in some scores compared to the quantitative results in the main paper, the performance of our MIVE remains consistent across various scenarios. This demonstrates the robustness of our approach against variations in instance sizes and the number of edited instances. D.3. User Study Details To conduct our user study, we select 50 videos from our dataset covering diverse scenarios (varying numbers of objects per clip, numbers of instances per object class, and instance sizes). We show the statistics of the videos we used in our user study in Tab. 4. We edit the videos using our MIVE framework and the other six SOTA video editing methods, namely, ControlVideo [70], FLATTEN [11], RAVE [26], TokenFlow [16], FreSCo [62], and GAV [25]. We task 31 participants to select the method with the: Best Temporal Consistency: Choose the video with the smoothest transitions; Best Textual Faithfulness: Choose the video with the most accurate text-object alignment. Make sure to check the videos alignment with the overall target caption and the individual instance captions; Least Editing Leakage: Choose the video with the least text leakage onto other objects and/or the background. Before starting the user study, we provided the participants with good and bad examples for each criterion for guidance. Fig. 15 shows our user study interface and the questionnaire form. 20 Figure 16. Video-P2P results on recursive multi-instance editing. The artifacts that accumulate when Video-P2P is used repeatedly for multi-instance editing is shown in red arrow. Our MIVE prevents this error accumulation since we do not edit the frames recursively. demonstrates better global editing faithfulness (GTF and FA) in addition to improved local faithfulness (LTF and IA). It also achieves competitive temporal consistency scores (GTC and LTC), consistent with the observations reported in the main paper. For the ablation study on IPR, the Global Scores show some deviation from the Local Scores, with Dense Diffusion achieving better global editing faithfulness performance. However, as shown in Fig. 7 of the main paper, Dense Diffusion may exhibit smoothing artifacts. These smoothing artifacts reduce the Local Scores but do not impact the Global Scores. In some cases, they may even improve the Global Scores, as Global Scores evaluate each frame with the whole word tokens of the caption. If each token is visible in any position of the frame, it can lead to higher Global Scores due to CLIPs limitations in compositional reasoning [23, 36]. E.1. DMS Ablations We conduct ablation studies on the hyperparameters of our Disentangled Multi-instance Sampling (DMS) technique. The quantitative results are summarized in Tab. 8, while the qualitative comparisons are shown in Figures 17, 18, 19, 20, and 21. First, we determine the optimal balance between alternating Latent Parallel Sampling (LPS) and Noise Parallel Figure 15. Our user study interface and questionnaire form. Participants are presented with an input video with source caption, an annotated video with target caption, and 7 randomly ordered videos edited using our MIVE and six other SOTA video editing methods. Each instance mask in the annotated video is colorcoded to correspond with its instance target caption. Participants are tasked to select the video with the Best Temporal Consistency, Best Textual Faithfulness, and Least Editing Leakage. D.4. Video-P2P Results While the recent work Video-P2P [32] is capable of local video editing, its design limits it to editing only one object at time. Attempting to edit multiple objects recursively with this method results in artifacts, as demonstrated in Fig. 16. Consequently, we exclude Video-P2P from our SOTA comparison. E. Additional Analysis and Ablation Studies Here, we present the Global Scores, which could not be included in the ablation study of the main paper due to space limitations. In the ablation study on DMS, our full method"
        },
        {
            "title": "GTF",
            "content": "FA LTC LTF IA CIA BP D I 0.9591 0.2667 0.7907 0.9460 0.2072 0.5587 0.6663 54.6597 Only NPS 0.9602 0.2645 0.7690 0.9483 0.2068 0.5716 0.6688 50.8549 Only LPS LPS + NPS w/o Re-Inversion 0.9615 0.2674 0.7810 0.9485 0.2080 0.5776 0.6783 52.3240 Ours, Full 54.3452 0.9604 0.2750 0.8557 0.9478 0.2138 0.6419 0.7100 No Modulation [44] Dense Diffusion [28] Ours, Full 0.9642 0.2642 0.7468 0.9535 0.2060 0.5225 0.6553 50.1319 0.9611 0.2760 0.9029 0.9482 0.2136 0.6215 0.6891 59.2100 0.9604 0.2750 0.8557 0.9478 0.2138 0.6419 0.7100 54.3452 Table 7. The full results (Global Scores and Local Scores) of our ablation study on Disentangled Multi-instance Sampling (DMS) and Instance-centric Probability Redistribution (IPR). LPS and NPS denotes Latent Parallel Sampling and Noise Parallel Sampling, respectively. The best and second best scores are shown in red and blue, respectively. 0.6851 0.6809 0.6713 0.6697 0.6727 0.6713 0.6792 0.6739 0.6734 0.6867 0.6970 0.6989 55.1253 52.4647 51.4300 51.1941 54.8790 51.4300 52.3441 52.9035 53.4412 52.8582 53.2689 53. Method (1) Alternate (50): LPS = 1 + NPS = 1 (2) Alternate (50): LPS = 4 + NPS = 1 (3) Alternate (50): LPS = 9 + NPS = 1 (4) Alternate (50): LPS = 14 + NPS = 1 (5) Alternate (50): NPS = 4 + LPS = 1 (1) Alternate (50): LPS = 9 + NPS = 1; NPS = 0 (2) Alternate (40): LPS = 9 + NPS = 1; NPS = 10 (3) Alternate (30): LPS = 9 + NPS = 1; NPS = 20 (4) Alternate (20): LPS = 9 + NPS = 1; NPS = 30 (a) Ablation on Alternating LPS and NPS on All Sampling Steps (50) Global Scores Local Scores Leakage Scores GTC GTF FA LTC LTF IA CIA (Ours) BP 0.9608 0.2691 0.8098 0.9472 0.2090 0.5883 0.9610 0.2667 0.7908 0.9481 0.2080 0.5914 0.9605 0.2654 0.7779 0.9483 0.2075 0.5748 0.9604 0.2649 0.7706 0.9483 0.2073 0.5743 0.9598 0.2690 0.8181 0.9465 0.2081 0.5804 (b) Ablation on Last NPS after Alternating Sampling 0.9605 0.2654 0.7779 0.9483 0.2075 0.5748 0.9613 0.2673 0.7824 0.9485 0.2081 0.5786 0.9614 0.2688 0.7890 0.9483 0.2083 0.5752 0.9614 0.2680 0.7990 0.9485 0.2084 0. (c) Ablation on Re-Inversion Step only on Alternating Sampling (1) Alternate (40): LPS = 9 + Re-Inv = 1 + NPS = 1; NPS = 10 (2) Alternate (40): LPS = 9 + Re-Inv = 2 + NPS = 1; NPS = 10 (3) Alternate (40): LPS = 9 + Re-Inv = 3 + NPS = 1; NPS = 10 0.9607 0.2697 0.8012 0.9479 0.2094 0.5930 0.9603 0.2712 0.8162 0.9475 0.2107 0.6180 0.9599 0.2724 0.8305 0.9471 0.2114 0.6233 (d) Ablation on Re-Inversion Step of Last NPS=10 with Alternating LPS=9 & NPS=1 & Re-Inversion L=3 (1) Alternate (40): LPS = 9 + Re-Inv = 3 + NPS = 1; NPS = 10 + Re-Inv = 1 (2) Alternate (40): LPS = 9 + Re-Inv = 3 + NPS = 1; NPS = 10 + Re-Inv = 2 (Ours, Full) (3) Alternate (40): LPS = 9 + Re-Inv = 3 + NPS = 1; NPS = 10 + Re-Inv = 3 0.9603 0.2740 0.8413 0.9477 0.2129 0.6315 0.9604 0.2750 0.8557 0.9478 0.2138 0.6419 0.9606 0.2751 0.8591 0.9476 0.2143 0. 0.7068 0.7100 0.7090 53.9953 54.3452 54.7422 0.9638 0.2749 0.8497 0.9505 0.2138 0.6385 (1) Alternate (40): LPS = 9 + Re-Inv = 3 + NPS = 1; NPS = 10 + Re-Inv (2D) = 2 (2) Alternate (40): LPS = 9 + Re-Inv = 3 + NPS = 1; NPS = 10 + Re-Inv (3D) = 2 (Ours, Full) 0.9604 0.2750 0.8557 0.9478 0.2138 0.6419 0.7078 0.7100 54.9499 54.3452 (e) Ablation on Re-Inversion Step using 2D vs 3D Model 0.9597 0.2775 0.8774 0.9467 0.2151 0.6626 (1) Alternate (40): LPS = 4 + Re-Inv = 3 + NPS = 1; NPS = 10 + Re-Inv (3D) = 2 (2) Alternate (40): LPS = 9 + Re-Inv = 3 + NPS = 1; NPS = 10 + Re-Inv (3D) = 2 (Ours, Full) 0.9604 0.2750 0.8557 0.9478 0.2138 0.6419 0.7159 0.7100 55.6459 54.3452 (f) Ablation on Another Alternative Configuration Table 8. Ablation study on various hyperparameter configurations for our Disentangled Multi-instance Sampling (DMS). The best and second best scores are shown in red and blue, respectively. Sampling (NPS) steps during sampling. Setting the number of NPS steps higher than the number of LPS steps results in cloudy artifacts and reduced details (Fig. 17-(5)). We observe improved details as we increase the number of LPS steps while keeping the number of NPS steps constant at 1 (Fig. 17-(1) to Fig. 17-(4)). Based on our findings, setting the LPS steps to 9 provides good tradeoff between qualitative and quantitative performance (Tab. 8-(a)). We also experimented on the final sampling steps of our DMS using only NPS. Increasing the number of NPS steps from 0 to 30 improves the quantitative performance of our method (Tab. 8-(b)). However, we observed that higher NPS values lead to objects losing their edges (Fig. 18-(3) and (4)). Consequently, we set NPS to 10 for the final steps of our DMS. We further conduct experiments by introducing reinversion in the alternating LPS-NPS steps, specifically after the LPS steps. Increasing the number of re-inversion steps after the LPS enhances the quantitative performance (Tab. 8-(c)). Moreover, as we increase L, the qualitative results demonstrate sharper details, such as the object shadows shown in Fig. 19-(3). We, thus, set the number of reinversion steps in this stage to = 3. We also incorporate re-inversion during the final NPS steps of our DMS sampling. Increasing the number of reinversion steps quantitatively improves the performance of 22 Figure 17. Ablation study on DMS: (a) Ablation on alternating LPS and NPS on all sampling steps (T = 50). Increasing the number of LPS steps while setting the number of NPS steps to 1 produces more details as shown in (1) - (4). Increasing the number of NPS steps while setting the number of LPS steps to 1 produces cloudy artifacts and less details as shown in (5). Setting the number of LPS steps to 9 is good tradeoff between qualitative and quantitative performance. See Tab. 8-(a) for quantitative results. our method (Tab. 8-(d)). However, higher number of re-inversion steps tends to introduce oversmoothing to the edited instances, giving them an animation-like appearance  (Fig. 20)  . Therefore, we limit the number of re-inversion steps in the final NPS stage to = 2. We further determine which between the 2D or 3D model used during the re-inversion steps yields better results. As shown in Tab. 8-(e), the 3D model enhances the quantitative performance of our framework. Further, using the 3D model produces sharper edges (Fig. 21-(2)). Therefore, we opt for the 3D model during the re-inversion steps. Finally, we experiment with another DMS configuration by setting the number of LPS steps in the alternating LPSNPS stage to 4. This results in slight increase in FA and IA, with negligible differences in other quantitative metrics (Tab. 8-(f)). Additionally, we observe that setting LPS steps 23 Figure 18. Ablation study on DMS: (b) Ablation on last NPS after alternating sampling. Reducing the number of alternating LPS-NPS steps during the initial sampling stage while increasing the number of NPS steps in the final stage enhances the quantitative performance of MIVE. Quantitative results are provided in Tab. 8-(b). However, we set the number of final NPS steps to only 10 to avoid degradation in object edges as shown in in (3) and (4). to 9 better preserves subtle details in the edited frames, such as object shadows (Fig. 22-(2)). Therefore, we adopt this configuration in the final model. E.2. Analysis on IPR Our Instance-centric Probability Redistribution (IPR) is (i) manipulatproposed based on two key observations: ing the attention probabilities of the padding tokens Ai,jP may lead to artifacts; therefore, we avoid altering them; and (ii) increasing the tokens probability decreases editing faithfulness, whereas decreasing it and reallocating those values to the and tokens improves editing faithfulness. Figure 23 illustrates the first observation, showing that altering the padding tokens tends to cause artifacts, e.g., both the alien face and the oven exhibit severe blurring and noise. In contrast, our proposed IPR avoids modifying the padding tokens Ai,jP and produces sharper and less noisy editing results, e.g., sharp alien face and less noisy oven. The second observation concerns the redistribution of attention probability values among the S, , and tokens. Figure 24 illustrates the results of different redistribution approaches. We observe the following: (i) Redistributing attention probability values from both the and tokens to (second row), as well as from the token to (third row), reduces editing faithfulness. For example, the right washing machine appears gray and does not transform into yellow washing machine, and the person remains human-like instead of transforming into an alien. Moreover, Figure 19. Ablation study on DMS: (c) Ablation on re-inversion step on alternating sampling. Increasing the number of re-inversion steps in the alternating LPS-NPS steps improves quantitative performance as shown in Tab. 8-(c). We also observe better details with higher values of L. Ultimately, we set = 3 for the re-inversion in the alternating LPS-NPS stage. while the structure of the left washing machine is starting to change, it still does not resemble an oven. (ii) Redistributing attention probability values from the token to (fourth row) causes the right washing machine to transform into yellow washing machine, but neither the left washing machine nor the person transforms into an oven or an alien, respectively. (iii) Redistributing attention probability values from the token to both the and tokens by setting λS = 0.2 (fifth row) begins to achieve more faithful editing. For instance, the humans eyes start to resemble alien eyes, the left washing machine begins to transform into an oven, and the right washing machine starts to turn into yellow washing machine. These first three observations highlight the importance of redistributing attention probabilities from to both and tokens, which forms the main motivation for our proposed IPR. (iv) Both increasing λS and utilizing our IPR to compute λS enhance editing faithfulness. Notably, our proposed IPR with dynamic λS achieves better editing faithfulness, e.g., improved faithfulness in both the aliens face and oven, with knobs instead of digital interface resembling the input. E.3. IPR Ablations In this subsection, we perform additional ablation studies on our IPR, focusing on the hyperparameters λ, λr, and the percentage of DDIM steps where IPR is applied. The quantitative comparison is presented in Tab. 9 and qualitative comparisons are presented in Fig. 25, Fig. 26, and Fig. 27. Figure 25 illustrates the results of varying the values of the hyperparameter λ. From these results, we observe that increasing λ enhances details and textures. For example, the metal beaker with rusty finish displays more intricate details as λ increases. Similarly, the robots head and hand exhibit improved textures, e.g., the texture of the shirt becomes more natural. However, excessively increasing λ may introduce artifacts, e.g., the wood carving becomes blurry. This occurs because significant increase in λ alters the spatial distribution of the initial attention values, leading to artifacts similar to DenseDiffusion [28]. In terms of quantitative performance, both λ = 0.4 and λ = 0.6 yield comparable results. However, due to the potential risk of artifacts, we use λ = 0.4 as our default, even though λ = 0.6 Figure 20. Ablation study on DMS: (d) Ablation on re-inversion step on last NPS=10 with alternating LPS=9 & NPS=1 & re-inversion L=3. Increasing the number of re-inversion steps in the last NPS steps of the sampling improves quantitative performance (see Tab. 8-(d)). Higher number of NPS steps, however, give the edited objects an animation-like appearance. Thus, we limit the number of re-inversion steps in the final NPS stage to = 2. Method λ = 0.2 λ = 0.4 (Ours) λ = 0.6 λr = 0.0 λr = 0.2 λr = 0.4 λr = 0.6 (Ours) λr = 0.8 λr = 1.0 Global Scores GTC GTF FA LTC Local Scores LTF Leakage Scores IA CIA (Ours) BP (a) Ablation on λ 0.9605 0.9604 0.9605 0.2745 0.2750 0.2753 0.8469 0.8557 0.8525 0.9477 0.9478 0. 0.2130 0.2138 0.2139 0.6299 0.6419 0.6459 (b) Ablation on λr 0.9599 0.9599 0.9610 0.9604 0.9600 0.9605 0.2757 0.2752 0.2759 0.2750 0.2746 0.2728 0.8770 0.8663 0.8589 0.8557 0.8457 0. 0.9478 0.9475 0.9477 0.9478 0.9479 0.9484 0.2137 0.2137 0.2140 0.2138 0.2130 0.2121 0.6439 0.6396 0.6386 0.6419 0.6438 0.6277 0.7072 0.7100 0.7111 0.7041 0.7023 0.7101 0.7100 0.7004 0.6999 0.7096 0.7100 0. 53.8473 54.3452 54.5119 54.8389 54.6357 54.5377 54.3452 54.2150 53.9180 54.3023 54.3452 54.0010 (c) Ablation on Percentage of Sampling Step that Utilizes IPR Applying IPR on The First 60% Sampling Steps Applying IPR on The First 80% Sampling Steps (Ours) Applying IPR on The First 100% Sampling Steps 0.9608 0.9604 0. 0.2750 0.2750 0.2740 0.8579 0.8557 0.8447 0.9485 0.9478 0.9452 0.2134 0.2138 0.2128 0.6350 0.6419 0.6363 Table 9. Ablation study on various hyperparameter configurations for our Instance-centric Probability Redistribution (IPR). The best and second best scores are shown in red and blue, respectively. performs slightly better quantitatively. For the second hyperparameter, λr, in our IPR, the qualitative results are shown in Fig. 26. We observe that increasing λr tends to decrease overall editing faithfulness 26 Figure 21. Ablation study on DMS: (e) Ablation on Re-inversion step using 2D vs 3D Model. Using the 3D model for re-inversion improves the quantitative performance of our framework (see Tab. 8-(e)). Further, using the 3D model renders sharper edges as shown above. We, thus, use the 3D model during the re-inversion steps. Figure 22. Ablation study on DMS: (f) ablation on another alternative configuration. Reducing the number of LPS steps during the alternating LPS-NPS stage to 4 improves FA and IA performance (see Tab. 8-(f)). However, setting the number of LPS steps to 9 better preserves subtle details, as shown in (2). 27 Figure 23. IPR analysis: Effect of altering the attention probability values of padding token Ai,jP. but preserves the original object shape. For example, the metal beaker begins to revert to its original shape with glass material, and the wood carving starts to lose its carvings. Conversely, decreasing λr tends to improve overall editing faithfulness but introduces more noise due to the enhanced details. For instance, the shirts exhibit more noise when λr is reduced. These observations are further supported by the quantitative comparison in Tab. 9, where reducing λr improves editing faithfulness scores (GTF, FA, LTF, and IA) but worsens Leakage Scores (CIA and BP), likely due to the emergence of artifacts. Based on these findings, we select λr = 0.6 to achieve balance between overall editing faithfulness and the minimization of artifacts. The final hyperparameter is the percentage of steps during which we apply our IPR (IPR steps percentage). Figure 27 illustrates the qualitative results of different configurations for the IPR steps percentage. Increasing the IPR steps percentage generally enhances editing faithfulness, e.g., better wood carvings on the table, improved rusty effects on the beaker, and more refined features on the robots head. However, excessively increasing the percentage may introduce artifacts, e.g., the appearance of noise on the glass table, due to the enhanced editing faithfulness, similar to the observations with λr. Our quantitative results in Tab. 9 align with these findings, showing that increasing the IPR steps percentage tends to improve editing faithfulness metrics (GTF, LTF, and IA). Interestingly, applying IPR to 100% of the sampling steps reduces editing faithfulness, likely due to the emergence of artifacts that disrupt details and negatively impact the editing faithfulness scores. Based on these observations, we choose to apply IPR to 80% of the sampling steps. Overall, the selection of these three hyperparameters in IPR is guided by the trade-off between editing faithfulness and the emergence of artifacts, which are the two key factors in cross-attention. F. Additional Details F.1. Details on Long Video Editing Our MIVE dataset comprises of several long videos, but directly editing them is constrained by GPU memory limitations. To address this, we leverage the keyframe selection algorithm proposed in FreSCo [62]. Using our MIVE framework, we edit the selected keyframes and employ the generative video interpolation method Ebsynth [24] to obtain the non-keyframes. F.2. Running Time Figure 28 illustrates the running time of our method on 512 512 video using single 48GB-NVIDIA RTX A6000 GPU. The running time of our method is influenced by two main factors: (i) the number of frames and (ii) the num28 ber of instances to edit, both of which impact the running time linearly. On average, our method takes approximately 16.61 seconds per instance per frame. Regarding memory consumption, it is largely independent of the number of instances to edit, as this factor has only minimal impact on memory usage. Instead, the memory consumption is primarily determined by the number of frames. G. Failure Case and Limitation G.1. Failure Case We present failure case in Fig. 29, where our MIVE struggles when faced with an excessive number of instances to edit. However, note that other SOTA methods, e.g., ControlVideo [70] and GAV [25], also struggle in this extreme scenario. One potential solution is to train an additional adapter on the LDM backbone that leverages the mask to joint modify features after the self-attention (similar to the approach in [55]) and after the cross-attention layers. This approach could better utilize the mask condition, thereby improving performance in scenarios involving large number of instances in multi-instance video editing. However, this solution may reduce the flexibility of our method, as it would require re-training the adapter each time we switch to different backbone. G.2. Limitations As shown in Fig. 30, our model struggles with reflection consistency due to mask limitations, covering only the instance, not its reflection. This causes discrepancies between the edited instance and its reflection in scenes with reflective surfaces. Future works can build on approaches like [13] to ensure consistency for both the instance and its reflection. Another limitation of our method is its running time. We introduce new factorthe number of instances to editwhich impacts the running time linearly. Although the design of our method significantly improves editing faithfulness and reduces attention leakage, it also increases the running time. Consequently, the more instances that need to be edited, the longer the process takes. One potential solution to this issue is to replace our current sampling method with faster approach, such as DPM-Solver [35] or DPM-Solver++ [34] and adjust our inversion process accordingly using DPM-Solver inversion [22]. 29 Figure 24. IPR analysis: Various scenarios of redistributing the attention probability values of tokens S, T, and E. Row 2-4: Redistributing the probability values from either or tokens to decreases the editing faithfulness. Row 5-6: Redistributing the probability values from token by constant factor λS to the and tokens improves the editing faithfulness. Ours: Redistributing the probability value of the token using our proposed dynamic approach achieves the best editing faithfulness. 30 Figure 25. Ablation study on IPR: (a) λ. Increasing λ enhances details and textures, but increasing it too much to λ = 0.6 may cause artifacts, e.g., the wood carvings get blurry. The best trade-off between details and emergence of artifacts is λ = 0.4. 31 Figure 26. Ablation study on IPR: (b) λr. Decreasing λr tends to improve overall editing faithfulness but introduces more noise due to the enhanced details, e.g., the shirt becomes more noisy when λr is reduced. The best trade-off between editing faithfulness and noise is λr = 0.6. 32 Figure 27. Ablation study on IPR: (c) percentage of sampling step where we apply our IPR. Increasing the IPR steps percentage generally enhances editing faithfulness, but excessively increasing the percentage (100%) may introduce artifacts, e.g., the appearance of noise on the glass table. The best trade-off between editing faithfulness and emergence of artifacts for percentage sampling step is 80%. Figure 28. Running time of our method. (a) Running time of our method vs number of video frames. (b) Running time of our method vs number of instances 33 Figure 29. Failure case. Our method is less faithful when there is large number of instances to edit. Note that other SOTA methods also fail to handle this large number of instances to edit. Figure 30. Limitation. Our method struggles in scenes with reflective surfaces."
        }
    ],
    "affiliations": [
        "Adobe Research",
        "Chung-Ang University",
        "KAIST"
    ]
}