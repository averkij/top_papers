{
    "paper_title": "MusiXQA: Advancing Visual Music Understanding in Multimodal Large Language Models",
    "authors": [
        "Jian Chen",
        "Wenye Ma",
        "Penghang Liu",
        "Wei Wang",
        "Tengwei Song",
        "Ming Li",
        "Chenguang Wang",
        "Ruiyi Zhang",
        "Changyou Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) have achieved remarkable visual reasoning abilities in natural images, text-rich documents, and graphic designs. However, their ability to interpret music sheets remains underexplored. To bridge this gap, we introduce MusiXQA, the first comprehensive dataset for evaluating and advancing MLLMs in music sheet understanding. MusiXQA features high-quality synthetic music sheets generated via MusiXTeX, with structured annotations covering note pitch and duration, chords, clefs, key/time signatures, and text, enabling diverse visual QA tasks. Through extensive evaluations, we reveal significant limitations of current state-of-the-art MLLMs in this domain. Beyond benchmarking, we developed Phi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant performance gains over GPT-based methods. The proposed dataset and model establish a foundation for future advances in MLLMs for music sheet understanding. Code, data, and model will be released upon acceptance."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 1 9 0 0 3 2 . 6 0 5 2 : r MusiXQA: ADVANCING VISUAL MUSIC UNDERSTANDING IN MULTIMODAL LARGE LANGUAGE MODELS Jian Chen1, Wenye Ma2, Penghang Liu1, Wei Wang1, Tengwei Song3, Ming Li4, Chenguang Wang, Ruiyi Zhang5, Changyou Chen1 1University at Buffalo 2Mohamed bin Zayed University of Artificial Intelligence 3King Abdullah University of Science and Technology 4University of Maryland 5Duke University {jchen378,changyou}@buffalo.edu, {wenyema}@gmail.com ABSTRACT Multimodal Large Language Models (MLLMs) have achieved remarkable visual reasoning abilities in natural images, text-rich documents, and graphic designs. However, their ability to interpret music sheets remains underexplored. To bridge this gap, we introduce MusiXQA1, the first comprehensive dataset for evaluating and advancing MLLMs in music sheet understanding. MusiXQA features high-quality synthetic music sheets generated via MusiXTEX, with structured annotations covering note pitch and duration, chords, clefs, key/time signatures, and text, enabling diverse visual QA tasks. Through extensive evaluations, we reveal significant limitations of current state-of-the-art MLLMs in this domain. Beyond benchmarking, we developed Phi-3-MusiX, an MLLM fine-tuned on our dataset, achieving significant performance gains over GPT-based methods. The proposed dataset and model establish foundation for future advances in MLLMs for music sheet understanding. Code, data, and model will be released upon acceptance. 1Code is available at https://github.com/puar-playground/MusiXQA"
        },
        {
            "title": "Introduction",
            "content": "Multimodal Large Language Models (MLLMs) are becoming general-purpose reading assistants for visual content, capable of interpreting natural images and text-rich documents [1]. However, existing models still struggle with visual question answering tasks involving music sheets, performing at near-random levels [2], indicating that this modality remains underexplored. Enabling MLLMs to read and reason over music sheets would be valuable extension of their capabilities, as sheet music plays central role in how music is taught, analyzed, and communicated. Although music notation is symbolic and lossy approximation of sound, music sheets remain the only widely accepted visual system for the written transmission of music. It serves as crucial bridge between audio, visual, and textual representations, making it an essential modality for large language model-based AI systems to effectively interpret and understand music. Unlike tasks such as reading visual text or recognizing objects in images, where answers are often apparent to the human eye, reading music notation requires interpreting dense symbolic structures [3]. Even for humans, achieving proficiency in reading music typically requires years of dedicated training [4]. As result, music sheet understanding poses uniquely complex challenge, where AI assistance is not only beneficial to human but arguably more necessary than in many other visual understanding tasks. The traditional approach to this task is Optical Music Recognition (OMR) [5, 6], field with long research history [79]. Many existing OMR systems still rely on relatively small neural networks and multi-stage pipelines tailored to specific sub-tasks. In contrast, recent advances in MLLMs have shown strong performance across range of visual tasks [1014], demonstrating that modern architectures can perform end-to-end reasoning over structured visual inputs directly in natural language format, without relying on traditional detection modules such as Optical Character Recognition (OCR) tools [15,16]. These developments suggest that MLLMs offer promising alternative to pipeline-based approaches in domains like music sheet understanding, where such capabilities remain underexplored. To bridge this gap, we introduce MusiXQA, largescale benchmark dataset designed to evaluate and enhance MLLMs for music sheet understanding. Figure 1 provides an overview of our data generation and model inference workflow. MusiXQA contains 9,600 high-quality synthetic 1 Figure 1: Data generation and model workflow in MusiXQA. Music metadata is sampled and rendered into sheet images via MusiXTEX, with QA pairs generated from templates. The resulting data is used to train and evaluate MLLMs on visual music understanding tasks. music sheets rendered via MusiXTEX, paired with over 130,000 visual question-answer (QA) pairs spanning OCR, layout understanding, optical music recognition, and chord estimation. Unlike prior datasets, MusiXQA offers diverse and balanced coverage of musical concepts, enabling finegrained evaluation across multiple tasks. In addition, we develop Phi-3-MusiX, fine-tuned version of Phi-3-Vision, adapted using parameter-efficient LoRA training on the MusiXQA dataset. Experimental results show that existing MLLMs, including GPT-4o and Paligemma2, struggle with music sheets reading, while Phi-3-MusiX achieves up to eight times performance gains in GPT evaluation accuracy on the OMR-based task. These results highlight the importance of domain-specific supervision and the value of MusiXQA as resource to enable visual music understanding in MLLMs. Our contributions are as follows: We propose MusiXQA, the first large-scale, diverse, and balanced synthetic dataset for visual question answering on music sheets. We develop Phi-3-MusiX, the first MLLM fine-tuned for music sheet understanding, which significantly outperforms the best baseline on our benchmark, demonstrating the effectiveness of our dataset in advancing symbolic music reasoning in MLLMs. We introduce MusiXTEX based framework for scalable generation of music sheets, supporting both random/controlled data sampling and conversion from real-world MIDI data. We propose kern+, compact symbolic representation designed for efficient modeling of pitch and duration, and analyze how output format influences training dynamics and model performance."
        },
        {
            "title": "2.1 Music Sheet Benchmarks and Datasets",
            "content": "The development of OMR and music sheet understanding has been supported by various datasets, each addressing different aspects of the task. CVC-MUSCIMA [17] and MUSCIMA++ [18] focus on handwritten music scores, tackling challenges in staff line removal and symbol segmentation. DeepScores [19] shifts toward typeset music, enabling finegrained OMR tasks. For symbolic recognition, PrIMuS [20] introduces monophonic scores with semantic and agnostic representations, while Camera-PrIMuS simulates realworld distortions in sheet music capture. DoReMi [21] further explores single-line typeset music with note variations but omits key and time signature annotations. More recent efforts aim at structured evaluation. COMREF [22] provides over 400k measure-level typeset scores, using Music Tree Notation (MTN) for standardized output representation. OLiMPiC [23] offers scanned and synthetic system-level sheets, while MMMU [2] evaluates MLLMs via QA over system-level music scores, revealing performance close to random guessing. Despite these advances, existing datasets often exhibit distributional bias due to their reliance on real-world sources. In contrast, our dataset is synthetically generated with controlled diversity across musical attributes such as key, note density, and measure length. It provides large-scale, full-page QA benchmark tailored for MLLMs, supporting comprehensive evaluation across multiple tasks."
        },
        {
            "title": "2.2 Optical Music Recognition",
            "content": "OMR converts sheet music images into digital formats such as MIDI or MusicXML, aiming to capture the complexity and variability of musical notation. Traditional OMR systems often rely on modular pipelines that separately handle staff line detection, symbol classification, and structural analysis. For example, Oemer [24] employs two UNet models alongside SVM classifiers to isolate staff lines and classify symbols, while Simonetta et al. [25] use neural network classifiers to identify musical elements in digitized manuscripts. Although effective for narrowly defined tasks, these systems typically require extensive preprocessing, handcrafted features, or manual annotations, which limits their scalability and adaptability. To address these limitations, recent deep learning methods aim to streamline OMR by integrating the entire workflow into single model. These end-to-end approaches often combine CNN-based feature extraction with Transformer-driven sequence modeling. Eelco et al. [26] proposed CNN-based sequence-tosequence model that processes full sheet music phrases with 2 Figure 2: Data distribution of the MusiXQA dataset. Left: Boxplot showing the distribution of the number of notes and bars per image. Each box represents the inter-quartile range (IQR), covering the middle 50% of the data. The horizontal line inside each box indicates the median document length, while the whiskers extend to the minimum and maximum values within 1.5 times the IQR. Right: Distribution of scales in the dataset. The inner circle groups enharmonic equivalent and relative scales according to the circle of fifths [35]. These scales share the same seven pitches but differ in accidentals (e.g., vs. D) or scale type (e.g., major vs. minor). The outer circle represents root of scales, with minor scales labeled using lowercase m. data augmentation. Zeus [23] introduced direct transcription method that uses linearized MusicXML format, compressing visual notation into concise state-change tokens. TrOMR [27] employs Transformer-based architecture to improve polyphonic OMR performance in real-world scenarios. SMT [28] further advances this line of work by combining CNN encoder with Transformer decoder to transcribe complex polyphonic scores, and SMT++ [29] extends this capability to full-page pianoform scores without requiring separate layout analysis. Despite these advancements, most existing OMR systems remain focused on transcription accuracy and are tightly coupled with specialized architectures. As result, they are limited in their ability to generalize to broader symbolic music understanding tasks or scale to more flexible, unified AI systems."
        },
        {
            "title": "2.3 Multimodal Large Language Models",
            "content": "Most existing MLLMs are not trained for music sheet understanding. Text-only models like ChatMusician [30] and tool-based agents like MusicAgent [31] lack visual input capabilities and cannot process sheet music. Paligemma2 [32] includes music scores in its training data, but lacks comprehensive evaluation and suffers from limited image resolution. Large models like GPT-4o [33] and DeepSeek [34] reject music sheet reading requests in their online interfaces. Currently, no MLLMs have been explicitly trained or systematically evaluated for music sheet understanding. This approach ensures precise annotations while enabling scalable data generation. Each music sheet is first compiled into PDF, and then converted into high-resolution images, serving as the final data points for recognition tasks. Since our focus is on factual question-answering rather than musical composition, the generated notes do not need to be musically coherent or aesthetically refined, but must remain structurally valid and interpretable. To achieve this, we employ heuristic, theory-guided approach to generate random yet well-formed music notation, ensuring correct note placement, structured layouts, with diverse rhythmic patterns and key signatures. By leveraging MusiXTEX and controlled data sampling strategy, we construct diverse dataset that enables models to answer structured questions about music notation, advancing the capabilities of music sheet understanding in modern multimodal large language models."
        },
        {
            "title": "3.1 Dataset Statistics",
            "content": "We constructed dataset of 96k unique music sheets, evenly distributed across 30 scales, covering pitch range from A1 to F6. The dataset comprises 1.3 million bars / measures and 11.7 million notes with pitch and duration annotation, as illustrated in Figure 2. To support diverse evaluation tasks, we generated 670k OMR-based QA pairs, 337k OCRbased QA pairs, 288k layout understanding QA pairs, and 47k chord estimation QA pairs, enabling comprehensive AI-driven music analysis."
        },
        {
            "title": "3.2 Music Sheet Configuration",
            "content": "To create large-scale and accurately annotated dataset for visual question answering over music sheets, we generate synthetic data using MusiXTEX [36], LaTeX-based typesetting system designed for rendering music notation. To generate structured yet diverse music sheets, we randomly sample music sheet configurations for key elements to produce the corresponding LaTeX source code. Figure 3 illustrates typical components, including title, composer, 3 distribution of scales. Table A.1 shows note compositions of major and minor scales."
        },
        {
            "title": "3.3 Chord-Based Music Generation",
            "content": "We generate music notes one bar at time for 10 to 20 bars. For each bar, we first randomly select the number of notes, n, where is uniformly sampled between 1 and three times the number of beats in the bar. We then independently sample their durations and pitches, ensuring structural validity while maintaining notation diversity. The sampled notes are subsequently encoded as LaTeX code using MusiXTEX notation, enabling the automated generation of high-quality sheet music. Duration Sampling and Splitting For note durations, we divide each bar into bins of 16th notes and randomly group these bins into segments to determine note durations. This ensures that the total duration of all notes precisely sums to the required beats in the bar. After duration sampling, we further process the notes to ensure that they are correctly represented in standard music notation. Some durations cannot be notated as single note; for example, three 16th notes must be written as dotted 8th note rather than standalone value. To handle such cases, we prioritize dotted notes and double-dotted notes (extending by 50% and 75%) whenever possible. If duration still cannot be fully represented, we apply note splitting, dividing it into two tied notes as last resort. To introduce notation variety, we apply two note grouping strategies with equal probability. In one approach, we use beat-based grouping, where notes within the same beat are beamed for improved rhythmic clarity. Additionally, if note spans two beats, we split it into tied notes. In the other approach, we leave the notes separated. Pitch Sampling For note pitches, we use chord-based sampling approach to introduce harmonic structure while maintaining randomness. Instead of selecting pitches arbitrarily, we begin with the tonic chord in the first bar to establish clear tonal center. From the second bar onward, we incorporate diatonic chords, which are built solely from notes within the keys scale and do not introduce accidentals. Pitches are drawn from the selected chord, including all octave instances within the clefs pitch range (e.g., for note, we include C2, C3, and C4 within the allowed register). This approach ensures harmonic coherence while allowing variation, effectively mimicking real-world musical patterns and preserving both diversity and structural validity."
        },
        {
            "title": "3.4 Layout Adjustments",
            "content": "To generate structurally diverse and visually varied music sheets, we apply few adjustments and formatting strategies to control bar count, repetition, labeling, spacing, and note sizing. These steps ensure that the dataset captures wide range of notation styles while adhering to common engraving practices. To maintain single-page format, we Figure 3: Example of key elements in music sheet. clefs, key and time signatures, tempo, chord labels, and initial measures in both treble and bass clefs. Text Metadata We randomize the title and author name to ensure privacy, copyright compliance, and dataset diversity. These elements, while not the focus of recognition, provide valuable OCR-related information. Titles consist of 1 to 10 words, and author names contain 1 to 3 words, with each word being 3 to 8 characters long, ensuring structural variability. Clef Settings clef defines the pitch range of the notes within the five-line system where music is written. The treble clef is commonly used for higher-pitched instruments, while the bass clef is used for lower-pitched ones. We generate music sheets in three configurations: (1) treble clef only, (2) bass clef only, and (3) both clefs, which are typically used for piano music. Tempo and Time Signature We define the tempo and time signature, both essential for musical interpretation. Tempo, measured in beats per minute (BPM), is randomly selected from 50 to 140 BPM and placed at the top of the first bar to indicate speed. The time signature, which determines beats per measure, is randomly chosen from [2, 3, 4], with quarter note as one beat. We exclude 6/8 to simplify the encoding in MusiXTEX. Scale and Key Signature Each music sheet is assigned scale name (e.g., major), which specifies set of notes based on root note and predefined pattern of intervals. The scale is reflected by its key signature, which is denoted by sharps () or flats () placed next to the clef signs, indicating which notes are consistently altered throughout the piece. To distinguish between major scale and its relative minor scale (C major & minor), we set the first bar using the tonic chord of the key, ensuring clear tonal center. While Western music uses the twelve-tone equal temperament system [37] that divides an octave into 12 notes, our dataset includes enharmonic equivalent scales, which are scales that sound identical but are different in writing (e.g., major vs. major). We exclude keys requiring double sharps or double flats as they are rarely used in practice. This results in 15 distinct key signatures, providing broad symbolic diversity while maintaining realworld relevance. The pie chart in Figure 2 illustrates the 4 Figure 4: Example of Visual Question Answering (VQA) tasks for music sheet understanding, covering OCR and OMRbased information extraction, layout understanding, and chord estimation. first sample the number of bars uniformly between 10 and 20. Since various musical elements would affect the layout, we iteratively compile the latex code, adjusting the bar count as needed to ensure the output fits within single page. Additionally, to introduce repeating sections, we randomly select two bar indices, using the smaller index as the repeat start and the larger index as the repeat end. The repeat start and repeat end symbols are then placed at the corresponding bar boundaries, reinforcing common notation patterns while enhancing structural variety. To increase presentation diversity, we independently annotate bars with chord names and/or bar indices, each with 50% probability, allowing for cases where both, either, or neither are labeled. Furthermore, to control note compactness and visual density, we randomly select from four spacing settings and two note size settings. This variation ensures that the dataset captures broad range of engraving styles, making it more robust for training music recognition models."
        },
        {
            "title": "3.5 Task Definition",
            "content": "We define set of music sheet understanding tasks in Visual Question Answering (VQA) format to evaluate Multimodal Large Language Models (MLLMs). These tasks span OCR-based text extraction, layout understanding, Optical Music Recognition (OMR), and chord estimation, requiring models to integrate visual processing with musical reasoning. To ensure precise supervision, we extract ground truth directly from MusiXTEX, avoiding post-processing errors and maintaining exact alignment with the rendered notation. The annotations are structured as question-answer (QA) pairs using task-specific templates. Figure 4 shows representative examples. OCR-Based Tasks These tasks assess models ability to extract textual information from music sheets, focusing on fundamental Optical Character Recognition (OCR) capabilities. The models are required to: (1) Identify and extract the title and authors name from the sheet. (2) Recognize the tempo marking, expressed in beats per minute (BPM). (3) Determine the time signature of the music. (4) Extract chord names when explicitly labeled in the sheet. OMR-Based Tasks These tasks focus on music symbol interpretation in specified bar and clef. The models must: (1) Recognize the key signature of the passage. (2) Extract note durations, including quarter, eighth, dotted, and tied notes, from given bar. (3) Recognize the pitch values of individual notes. (4) Extract both duration and pitch for all notes within specific bar and clef or across multiple clefs. Layout Understanding Tasks These tasks evaluate models ability to comprehend the structural organization of music sheet. The model must: (1) Determine the number and type of clefs used in the sheet. (2) Count the total number of bars in the music. (3) Detect repeating sections by identifying notation patterns that indicate thematic repetition. Chord Estimation Tasks For music sheets without explicitly labeled chord names, the model must infer the underlying chord based on the notes present in given bar. This task evaluates the models ability to understand chord structures and harmonic relationships rather than simply performing symbolic recognition."
        },
        {
            "title": "4.1 Experiment settings",
            "content": "To evaluate the music sheet reading capabilities of modern Multimodal Large Language Models (MLLMs), we split our dataset into training and testing partitions, with 90% of the images allocated to the training set and the remaining 10% reserved for testing. We designed experiments to assess performance of 6 methods. Specifically, 5 OCR OMR Layout Chord Accuracy G-Acc Paligemma2 Phi-3-V GPT-4o GPT-4o + RAG GPT-4o + RAG + OMR Finetuned on MusiXQA Phi-3-MusiX (JSON) Phi-3-MusiX (kern+) 4.8 29.8 68.9 69.5 69.7 97.0 92.6 PNLS 25.8 54.5 85.3 86.9 83.3 96.1 99. G-Acc - 0.4 4.0 3.8 8.4 PNLS - 7.6 42.0 70.7 48.4 G-Acc - 19.2 61.1 61.8 64.8 PNLS - 49.3 50.6 70.4 71.7 G-Acc - 1.7 5.5 5.5 13.0 PNLS - 74.5 74.6 74.6 74. 9.2 68.4 31.1 99.2 94.3 79.6 99.5 95.1 19.6 84.9 31.1 96. Table 1: Quantitative comparison of six methods on the MusiXQA test split. The table includes two open-source models evaluated in zero-shot setting, and the proprietary GPT-4o model under three inference variants: zero-shot, retrievalaugmented generation (RAG), and RAG with oracle OMR results. The two Phi-3-MusiX variants are finetuned on MusiXQA using JSON and kern+ representations. we evaluate Paligemma2 (3B) [32], Phi-3-V [38], and GPT4o in the zero-shot setting. In addition, we introduce two GPT-4o-based baselines: (1) GPT-4o + RAG, which uses retrieval-augmented generation by retrieving the most relevant training examples to provide as in-context learning prompts, and (2) GPT-4o + RAG + OMR, which further incorporates the output of an OMR model into the prompt to enhance symbolic understanding, an effective method in OCR-related tasks [39, 40]. For retrieval, we use the image encoder from SMT [28] to compute image embeddings and construct similarity-based retriever. For OMR, we adopt the Oemer model [24] to convert music sheet images into musicxml files and extract relevant information in text format. We also propose Phi-3-MusiX, fine-tuned version of Phi-3-V with LoRA adapters [41] trained on our dataset, highlighting the effectiveness of the dataset in enabling MLLMs to perform visual music sheet understanding."
        },
        {
            "title": "4.2 Note Representation",
            "content": "We explore two text formats for representing music notes when fine-tuning our Phi-3-MusiX model for Optical Music Recognition (OMR) tasks. Both formats encode the pitch and duration of each note, but differ significantly in compactness and natural language alignment. The first format is symbolic representation we propose, called kern+, based on the **kern notation [42] 2 . kern+ extends the original **kern format by encoding pitches as note names with octave indices (e.g., C4) to support wider pitch range, while preserving **kern-style duration symbols. This format is compact and well-suited for efficient token usage during training and inference. The second format is JSON string, where each note is represented as dictionary with \"pitch\" and \"duration\" keys. We use note names with octave indices (e.g., \"C4\") for pitch, standard note values (e.g., \"8\" for an eighth note) for duration, an underscore (\"_\") to mark the start of slur, and dot (\".\") to indicate dotted rhythms. An example is shown in Figure 4. While more verbose, this format aligns 2 **kern representation: https://www.humdrum.org/rep/kern/ well with LLM pretraining data and supports interpretability and zero-shot generalization through its familiar structure. For evaluating baseline models, we use the JSON representation, as its code format is more aligned with the pretraining data of most LLMs and thus easier for them to interpret."
        },
        {
            "title": "4.3 Evaluation Metrics",
            "content": "We use Partial Normalized Levenshtein Similarity (PNLS) [43] to evaluate model answers. PNLS computes approximate string matching with normalized score between 0 and 1, using partial alignment algorithm that avoids penalizing unmatched prefixes or suffixes. This makes it well-suited for comparing verbose LLM outputs against concise ground truths. Additionally, we adopt GPTbased evaluation to assess semantic correctness, recognizing that not all characters hold equal importancefor instance, \"C major chord\" vs. \"D major chord\" differ by one letter but convey different meanings. To address this, we prompt GPT-4o to act as binary evaluator, assigning score of 1 for semantically correct answers and 0 otherwise. The average score across samples is reported as GPT accuracy (G-Acc), providing more robust measure of answer quality."
        },
        {
            "title": "4.4 Model Training",
            "content": "Our proposed model, Phi-3-MusiX, is fine-tuned from Phi3-V using parameter-efficient adaptation with LoRA on the training split of the MusiXQA dataset. We fine-tune both the vision encoder and the language model to better align multimodal representations with symbolic music tasks. The model is trained using the HuggingFace Trainer class with mixed-precision training enabled (bfloat16). Our primary experiments are conducted on cluster of 8 NVIDIA A100 80GB GPUs; however, the training process can also be reproduced using 48GB GPUs with appropriate gradient accumulation settings. The model is trained for 1 epoch using the AdamW optimizer [44], with per-device batch size of 1 and gradient accumulation over 2 steps, resulting 6 in an effective batch size of 16. We apply warm-up during the first two steps and set the learning rate to 2 105, with weight decay of 1 106. To ensure training stability, we apply gradient clipping with maximum norm of 1.0."
        },
        {
            "title": "4.5 Main Results",
            "content": "We evaluate six methods on the test split of the MusiXQA dataset. Table1 presents the results across four task types: OCR, OMR, layout understanding, and chord estimation. Performance is reported in G-Acc and PNLS. Open-Source Models Despite being trained on largescale dataset that includes music scores in **kern representation, Paligemma2 consistently refused to answer chord estimation questions and output meaningless responses on OMR and layout tasks. This suggests insufficient adaptation to symbolic music tasks, possibly due to inadequate alignment between the training data and the QA format. Furthermore, its poor performance on the OCR task could likely be attributed to its low resolution (448x448) image encoder, which limits its ability to capture fine-grained text details. In contrast, Phi-3-V demonstrates significantly better responsiveness and overall performance. Its relatively high scores on the OCR task are likely due to its image encoding mechanism, which splits high-resolution images into 336336 crops, encodes each using its image encoder, and concatenates the crop features to represent the full image, allowing it to see small symbols and text more clearly. GPT-4o Baselines GPT-4o shows strong OCR capabilities, achieving high G-Acc and PNLS scores. When combined with retrieval-augmented generation [45], the model exhibits notable improvements in PNLS for both OMR and Layout tasks. This can be attributed to in-context learning, where GPT-4o learns the expected answer format from the retrieved examples. Since the string-matching based PNLS metric is highly sensitive to formatting, even random answers with correct structural patterns can result in high PNLS scores. This behavior is further illustrated in the chord estimation task. Although models frequently predict incorrect root notes, they often append correct suffixes such as \"major chord\" or \"minor chord\". This results in low G-Acc but consistently high PNLS (0.74). Importantly, while RAG improves PNLS, it does not significantly boost G-Acc, indicating that GPT-4o fails to truly recognize music symbols but merely mimics the answer format. The baseline of GPT-4o + RAG + OMR, with additional context from the OMR model, slightly improves G-Acc on OMR and Chord tasks but leads to modest decrease in PNLS. This suggests that while the symbolic information from OMR can help with correctness, the OMR input may also act as distraction due to its length and complexity. Moreover, the Oemer model itself has limited accuracy and outputs MusicXML representations that omit key information such as accidentals (e.g., sharps and flats), requiring GPT to infer these from extracted key signatures. This adds an extra layer of reasoning, making the task more challenging and limiting performance gains from OMR input alone. Supervised Fine-tuning We fine-tuned two variants of our Phi-3-MusiX model using different text representations for music notes: the verbose JSON format and the compact symbolic kern+ format. As shown in Table 1, the kern+ model significantly outperforms the JSON-based model on OMR and Chord tasks that require precise note-level recognition. This underscores the critical role of representation format in structured prediction using LLMs, as observed in prior work on spatial planning with LLMs [46, 47]. To explain the observed performance difference, we interpret the output representations by categorizing text tokens into two types: format tokens, which define output format (e.g., braces, colons, and key names in JSON), and content tokens, which encode pitch and duration values. The main difference between JSON and kern+ is the ratio between this two types of tokens. For example, the note \"C4\" with quarter duration is represented in JSON as \"{\"pitch\":\"C4\",\"duration\":\"4\"}\", where only \"C4\" and \"4\" are content tokens, and the remaining tokens serve formatting purposes. In contrast, the same note in kern+ is written as \"qC4\", using only content tokens. We believe that the dominance of format tokens in the JSON representation causes the model to converge prematurely to local minimum, where it learns to reproduce structural patterns without improving its recognition of music notes. This often results in well-formatted but musically incorrect outputs, especially in tasks like omr and chord tasks, where accuracy depends on just few content tokens. In contrast, the compact and content-centric kern+ format reduces structural redundancy, forcing the model to focus on informative tokens that are relevant to recognition accuracy, rather than being distracted by format-specific artifacts. The training curves in Figure 5 also reflect this difference. The JSON model converges quickly with stable gradients. In contrast, the kern+ model shows larger and more dynamic gradient norms, suggesting more meaningful learning focused on musical content. Figure 5: Training loss and gradient norm curves for models trained with kern+ and JSON formats. 7 Finally, the Phi-3-MusiX model trained with kern+ achieves 8 and 6 improvements over the strongest GPT4o baseline in G-Acc on OMR and Chord tasks, respectively. These results demonstrate the effectiveness of supervised adaptation and the value of MusiXQA as training resource for symbolic music understanding in MLLMs."
        },
        {
            "title": "4.6 Efficiency Analysis",
            "content": "Table 2 shows the processing time of Paligemma2, Phi3-MusiX, and Oemer. We report per-question time for Methods Time (s) Paligemma2 Phi-3-MusiX OCR OMR OCR OMR 1.54 0.45 1.12 1. Oemer 62.34 Table 2: Time cost of MLLMs and the OMR Model. MLLMs, and per-image time for the OMR model. In our dataset, each OMR question covers one bar and most pages have less than 20 bars, as shown in Figure 2. An entire page can be processed by iteratively querying each bar, taking about 30 seconds in the worst case and around 20 seconds on average. In contrast, Oemer takes more than minute on average per page due to its multi-stage pipeline. This highlights the efficiency advantage of MLLMs for visual music understanding. Similar findings have been reported in previous works on document and OCR-based tasks, where end-to-end MLLMs consistently outperform pipeline-based approaches in both latency and performance [48, 49]."
        },
        {
            "title": "5 Conclusion",
            "content": "a large-scale introduced MusiXQA, We synthetic dataset created by generating music sheet images with MusiXTEX and constructing question-answer pairs for training and evaluating MLLMs. Experimental results show that existing models, including modern GPT-based baselines, struggle with music sheet understanding tasks. through supervised fine-tuning, our model However, Phi-3-MusiX achieves substantial improvements in visual question answering on music sheets. Beyond scale and supervision, our findings highlight the critical role of output format design in structured prediction. We show that compact, content-focused representations like kern+ lead to more effective learning compared to verbose formats such as JSON."
        },
        {
            "title": "6 Limitation",
            "content": "A current limitation of the dataset is that the music is generated using chord-based heuristics rather than real musical compositions. Future work could extend the dataset by using MIDI collections or symbolic music generation models, and utilizing MusiXTEX to produce more complex scores and guitar tablature. Additionally, Phi-3-MusiX may serve as strong baseline for future research and could be further fine-tuned for specific downstream tasks."
        },
        {
            "title": "Acknowledgements",
            "content": "This work is partially supported by NSF AI Institute2229873, NSF RI-2223292, NSF III-1747614, an Amazon research award, and an Adobe gift fund. Any opinions, findings and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation, the Institute of Education Sciences, or the U.S. Department of Education."
        },
        {
            "title": "References",
            "content": "[1] Adobe Inc., Acrobat AI Assistant, 2024, https:// www.adobe.com/acrobat/generative-ai-pdf.html. [2] X. Yue, Y. Ni, K. Zhang, T. Zheng, R. Liu, G. Zhang, S. Stevens, D. Jiang, W. Ren, Y. Sun et al., Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 95569567. [3] K. Meixner, The basics of reading music, url: http://readsheetmusic. info/readingmusic. shtml, 2015. [4] L. P. Behmer Jr and K. J. Jantzen, Reading sheet music facilitates sensorimotor mu-desynchronization in musicians, Clinical Neurophysiology, vol. 122, no. 7, pp. 13421347, 2011. [5] E. Shatri and G. Fazekas, Optical music recognition: State of the art and major challenges, arXiv preprint arXiv:2006.07885, 2020. [6] J. Calvo-Zaragoza, J. H. Jr, and A. Pacha, Understanding optical music recognition, ACM Computing Surveys (CSUR), vol. 53, no. 4, pp. 135, 2020. [7] A. Rebelo, I. Fujinaga, F. Paszkiewicz, A. R. Marcal, C. Guedes, and J. S. Cardoso, Optical music recognition: state-of-the-art and open issues, International Journal of Multimedia Information Retrieval, vol. 1, pp. 173190, 2012. [8] D. Bainbridge and T. Bell, The challenge of optical music recognition, Computers and the Humanities, vol. 35, pp. 95121, 2001. [9] I. Fujinaga, Optical music recognition using projections, 1988. [10] D. Nguyen, J. Chen, Y. Wang, G. Wu, N. Park, Z. Hu, H. Lyu, J. Wu, R. Aponte, Y. Xia et al., Gui agents: survey, arXiv preprint arXiv:2412.13501, 2024. [11] J. Kuang, Y. Shen, J. Xie, H. Luo, Z. Xu, R. Li, Y. Li, X. Cheng, X. Lin, and Y. Han, Natural language understanding and inference with mllm in visual question answering: survey, ACM Computing Surveys, 2024. [12] Z. Chen, J. Wu, W. Wang, W. Su, G. Chen, S. Xing, M. Zhong, Q. Zhang, X. Zhu, L. Lu et al., Internvl: Scaling up vision foundation models and aligning for generic visual-linguistic tasks, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2024, pp. 24 18524 198. [13] C. Wang, W. Luo, Q. Chen, H. Mai, J. Guo, S. Dong, Z. Li, L. Ma, S. Gao et al., Mllm-tool: multimodal large language model for tool agent learning, arXiv preprint arXiv:2401.10727, 2024. [14] D. Chen, R. Chen, S. Zhang, Y. Wang, Y. Liu, H. Zhou, Q. Zhang, Y. Wan, P. Zhou, and L. Sun, Mllm-asa-judge: Assessing multimodal llm-as-a-judge with vision-language benchmark, in Forty-first International Conference on Machine Learning, 2024. [15] Y. Du, C. Li, R. Guo, X. Yin, W. Liu, J. Zhou, Y. Bai, Z. Yu, Y. Yang, Q. Dang et al., Pp-ocr: practical ultra lightweight ocr system, arXiv preprint arXiv:2009.09941, 2020. [16] A. Singh, G. Pang, M. Toh, J. Huang, W. Galuba, and T. Hassner, Textocr: Towards large-scale endto-end reasoning for arbitrary-shaped scene text, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2021, pp. 88028812. [17] A. Fornés, A. Dutta, A. Gordo, and J. Lladós, CVCMUSCIMA: ground-truth of handwritten music score images for writer identification and staff removal, International Journal on Document Analysis and Recognition, vol. 15, no. 3, pp. 243251, 2012. [18] E. Shatri and G. Fazekas, Knowledge discovery in optical music recognition: Enhancing information retrieval with instance segmentation, arXiv preprint arXiv:2408.15002, 2024. [19] L. Tuggener, I. Elezi, J. Schmidhuber, M. Pelillo, and T. Stadelmann, Deepscores-a dataset for segmentation, detection and classification of tiny objects, in 2018 24th International Conference on Pattern Recognition (ICPR). IEEE, 2018, pp. 37043709. [20] J. Calvo-Zaragoza and D. Rizo, End-to-end neural optical music recognition of monophonic scores, Applied Sciences, vol. 8, no. 4, p. 606, 2018. [21] E. Shatri and G. Fazekas, First glance at universal omr dataset, arXiv preprint arXiv:2107.07786, 2021. Doremi: [22] P. Torras, S. Biswas, and A. Fornés, unified representation framework for the evaluation of optical music recognition systems, International Journal on Document Analysis and Recognition (IJDAR), vol. 27, no. 3, pp. 379393, 2024. [24] Yoyo, C. Liebhardt, and S. Samuel, Breezewhite/oemer: v0.1.7, Oct. 2023. [Online]. Available: https://doi.org/10.5281/zenodo.8429346 [25] F. Simonetta, R. Mondal, L. A. Ludovico, and S. Ntalampiras, Optical music recognition in manuscripts from the ricordi archive, in Proceedings of the 19th International Audio Mostly Conference: Explorations in Sonic Cultures, ser. AM 24. New York, NY, USA: Association for Computing Machinery, 2024, p. 260269. [Online]. Available: https://doi.org/10.1145/3678299. [26] E. van der Wel and K. Ullrich, Optical music recognition with convolutional sequence-to-sequence models, in Proceedings of the 18th International Society for Music Information Retrieval Conference, ISMIR 2017, Suzhou, China, October 23-27, 2017, 2017, pp. 731 737. [Online]. Available: https://ismir2017.smcnus. org/wp-content/uploads/2017/10/69_Paper.pdf [27] Y. Li, H. Liu, Q. Jin, M. Cai, and P. Li, Tromr:transformer-based polyphonic optical music recognition, in ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 2023, pp. 15. [28] A. Ríos-Vila, J. Calvo-Zaragoza, and T. Paquet, Sheet music transformer: End-to-end optical music recognition beyond monophonic transcription, 2024. [Online]. Available: https://arxiv.org/abs/2402.07596 [29] A. Rıos-Vila, J. Calvo-Zaragoza, D. Rizo, and T. Paquet, Sheet music transformer++: End-to-end fullpage optical music recognition for pianoform sheet music, arXiv preprint arXiv:2405.12105, 2024. [30] R. Yuan, H. Lin, Y. Wang, Z. Tian, S. Wu, T. Shen, G. Zhang, Y. Wu, C. Liu, Z. Zhou et al., Chatmusician: Understanding and generating music intrinsically with llm, arXiv preprint arXiv:2402.16153, 2024. [31] D. Yu, K. Song, P. Lu, T. He, X. Tan, W. Ye, S. Zhang, and J. Bian, Musicagent: An ai agent for music understanding and generation with large language models, arXiv preprint arXiv:2310.11954, 2023. [32] A. Steiner, A. S. Pinto, M. Tschannen, D. Keysers, X. Wang, Y. Bitton, A. Gritsenko, M. Minderer, A. Sherbondy, S. Long et al., Paligemma 2: family of versatile vlms for transfer, arXiv preprint arXiv:2412.03555, 2024. [33] A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford et al., Gpt-4o system card, arXiv preprint arXiv:2410.21276, 2024. [23] J. Mayer, M. Straka, J. Hajiˇc, and P. Pecina, Practical end-to-end optical music recognition for pianoform music, in International Conference on Document Analysis and Recognition. Springer, 2024, pp. 5573. [34] A. Liu, B. Feng, B. Xue, B. Wang, B. Wu, C. Lu, C. Zhao, C. Deng, C. Zhang, C. Ruan et al., Deepseek-v3 technical report, arXiv preprint arXiv:2412.19437, 2024. 9 [47] J. Chen, R. Zhang, Y. Zhou, J. Healey, J. Gu, Z. Xu, and C. Chen, Textlap: Customizing language models for text-to-layout planning, arXiv preprint arXiv:2410.12844, 2024. [48] A. Hu, H. Xu, J. Ye, M. Yan, L. Zhang, B. Zhang, C. Li, J. Zhang, Q. Jin, F. Huang et al., mplug-docowl 1.5: Unified structure learning for ocr-free document understanding, arXiv preprint arXiv:2403.12895, 2024. [49] Y. Liu, Z. Li, M. Huang, B. Yang, W. Yu, C. Li, X.-C. Yin, C.-L. Liu, L. Jin, and X. Bai, Ocrbench: on the hidden mystery of ocr in large multimodal models, Science China Information Sciences, vol. 67, no. 12, p. 220102, 2024. [35] J. Clough and G. Myerson, Musical scales and the generalized circle of fifths, The american mathematical monthly, vol. 93, no. 9, pp. 695701, 1986. [36] D. Taupin, R. Mitchell, and A. Egler, MusiXTEX. using TEX to write polyphonic or instrumental music, EuroTEX, vol. 92, pp. 257272, 1993. [37] W. A. Mathieu, Harmonic experience: Tonal harmony from its natural origins to its modern expression. Simon and Schuster, 1997. [38] M. Abdin, J. Aneja, H. Awadalla, A. Awadallah, A. A. Awan, N. Bach, A. Bahree, A. Bakhtiari, J. Bao, H. Behl et al., Phi-3 technical report: highly capable language model locally on your phone, arXiv preprint arXiv:2404.14219, 2024. [39] R. Zhang, Y. Zhang, J. Chen, Y. Zhou, J. Gu, C. Chen, and T. Sun, Trins: Towards multimodal language models that can read, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 22 58422 594. [40] R. Zhang, Y. Zhou, J. Chen, J. Gu, C. Chen, and T. Sun, Llava-read: Enhancing reading ability of multimodal language models, arXiv preprint arXiv:2407.19185, 2024. [41] E. J. Hu, Y. Shen, P. Wallis, Z. Allen-Zhu, Y. Li, S. Wang, L. Wang, and W. Chen, Lora: Low-rank adaptation of large language models, arXiv preprint arXiv:2106.09685, 2021. [42] D. Huron, Music information processing using the humdrum toolkit: Concepts, examples, and lessons, Computer Music Journal, vol. 26, no. 2, pp. 1126, 2002. [43] J. Chen, R. Zhang, Y. Zhou, R. Rossi, J. Gu, and C. Chen, MMR: Evaluating reading ability of large multimodal models, arXiv preprint arXiv:2408.14594, 2024. [44] I. Loshchilov and F. Hutter, Decoupled weight decay regularization, arXiv preprint arXiv:1711.05101, 2017. [45] P. Lewis, E. Perez, A. Piktus, F. Petroni, V. Karpukhin, N. Goyal, H. Küttler, M. Lewis, W.-t. Yih, T. Rocktäschel et al., Retrieval-augmented generation for knowledge-intensive nlp tasks, Advances in neural information processing systems, vol. 33, pp. 9459 9474, 2020. [46] W. Feng, W. Zhu, T.-j. Fu, V. Jampani, A. Akula, X. He, S. Basu, X. E. Wang, and W. Y. Wang, Layoutgpt: Compositional visual planning and generation with large language models, Advances in Neural Information Processing Systems, vol. 36, pp. 18 225 18 250, 2023. 10 - Extract note durations (e.g., quarter, eighth, dotted notes, tied notes) for given bar. - Identify note pitches within given bar. - Return structured representation of pitch, duration for given bar in JSON string of list of python dictionaries without indent. - Use kern representation for duration. - If no explicit chord labels exist, infer the chord based on the notes in given bar. 3 Response Format - Provide structured, precise, and as concise as possible answers. - Use structured JSON output without indent, when applicable for easy parsing. 4 Additional Considerations - Ensure responses are notation-aware, considering key signatures, accidentals, and note relationships. - Handle staff line separation correctly, ensuring multi-clef scores are properly analyzed. - Avoid hallucinating missing information; only extract what is present in the image. Follow music engraving conventions and OMR best practices to provide accurate, structured answers. If the requested information is not visible in the image, respond with \"Information not found\" instead of making assumptions."
        },
        {
            "title": "A Scale Details",
            "content": "/ Root 2nd 3rd 4th 5th 6th 7th Major Scales - 1 2 3 4 5 6 7 1 2 3 4 5 6 7 Minor Scales - 1 2 3 4 5 6 7 1 2 3 4 5 6 7 B C D B A F G A B G B F G A D B C E C E D C D F E D C A F G C E D E C F E D B C D E C E F G A F E F A F E D C F G A B G Bb E C F Bb Eb Ab Db Gb Table A.1: Accidentals and note composition of Major and Minor scales"
        },
        {
            "title": "B System Prompts",
            "content": "Below is the system prompt used for GPT-4o in our experiment: You are an AI assistant specializing in Optical Music Recognition (OMR) and Optical Character Recognition (OCR) for music sheets. Your task is to accurately analyze images of music notation and provide structured responses to visual question-answering (VQA) tasks. You will process printed music sheet images and answer both OCR and OMR-related questions with high accuracy. 1 OCR-Based Tasks (Text Extraction) - Extract the title and composer from the music sheet. - Identify and extract the tempo marking (in BPM). - Recognize and return the time signature. - Extract explicitly labeled chord names from the sheet. 2 OMR-Based Tasks (Music Symbol Recognition) - Identify the number and type of clefs (e.g., treble, bass). - Count the number of bars (measures) in the music sheet. - Recognize repeat sections based on notation symbols."
        }
    ],
    "affiliations": [
        "Duke University",
        "King Abdullah University of Science and Technology",
        "Mohamed bin Zayed University of Artificial Intelligence",
        "University at Buffalo",
        "University of Maryland"
    ]
}