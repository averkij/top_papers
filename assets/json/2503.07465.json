{
    "paper_title": "YOLOE: Real-Time Seeing Anything",
    "authors": [
        "Ao Wang",
        "Lihao Liu",
        "Hui Chen",
        "Zijia Lin",
        "Jungong Han",
        "Guiguang Ding"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios. Recent open-set methods leverage text prompts, visual cues, or prompt-free paradigm to overcome this, but often compromise between performance and efficiency due to high computational demands or deployment complexity. In this work, we introduce YOLOE, which integrates detection and segmentation across diverse open prompt mechanisms within a single highly efficient model, achieving real-time seeing anything. For text prompts, we propose Re-parameterizable Region-Text Alignment (RepRTA) strategy. It refines pretrained textual embeddings via a re-parameterizable lightweight auxiliary network and enhances visual-textual alignment with zero inference and transferring overhead. For visual prompts, we present Semantic-Activated Visual Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches to bring improved visual embedding and accuracy with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. It utilizes a built-in large vocabulary and specialized embedding to identify all objects, avoiding costly language model dependency. Extensive experiments show YOLOE's exceptional zero-shot performance and transferability with high inference efficiency and low training cost. Notably, on LVIS, with 3$\\times$ less training cost and 1.4$\\times$ inference speedup, YOLOE-v8-S surpasses YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6 AP$^b$ and 0.4 AP$^m$ gains over closed-set YOLOv8-L with nearly 4$\\times$ less training time. Code and models are available at https://github.com/THU-MIG/yoloe."
        },
        {
            "title": "Start",
            "content": "YOLOE: Real-Time Seeing Anything Ao Wang1* Lihao Liu1* Hui Chen1 Zijia Lin1 Jungong Han1 Guiguang Ding1 1Tsinghua University 5 2 0 M 0 1 ] . [ 1 5 6 4 7 0 . 3 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Object detection and segmentation are widely employed in computer vision applications, yet conventional models like YOLO series, while efficient and accurate, are limited by predefined categories, hindering adaptability in open scenarios. Recent open-set methods leverage text prompts, visual cues, or prompt-free paradigm to overcome this, but often compromise between performance and efficiency due to high computational demands or deployment complexIn this work, we introduce YOLOE, which integrates ity. detection and segmentation across diverse open prompt mechanisms within single highly efficient model, achieving real-time seeing anything. For text prompts, we propose Re-parameterizable Region-Text Alignment (RepRTA) It refines pretrained textual embeddings via strategy. re-parameterizable lightweight auxiliary network and enhances visual-textual alignment with zero inference and transferring overhead. For visual prompts, we present Semantic-Activated Visual Prompt Encoder (SAVPE). It employs decoupled semantic and activation branches to bring improved visual embedding and accuracy with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. It utilizes builtin large vocabulary and specialized embedding to identify all objects, avoiding costly language model dependency. Extensive experiments show YOLOEs exceptional zero-shot performance and transferability with high inference efficiency and low training cost. Notably, on LVIS, with 3 less training cost and 1.4 inference speedup, YOLOEv8-S surpasses YOLO-Worldv2-S by 3.5 AP. When transferring to COCO, YOLOE-v8-L achieves 0.6 APb and 0.4 APm gains over closed-set YOLOv8-L with nearly 4 less training time. Code and models are available at https: //github.com/THU-MIG/yoloe. 1. Introduction Object detection and segmentation are foundational tasks in computer vision [15, 48], with widespread applications spanning autonomous driving [2], medical analyses [55], *Equal contribution. Figure 1. Comparison of performance, training cost, and inference efficiency between YOLOE (Ours) and advanced YOLO-Worldv2 in terms of open text prompts. LVIS AP is evaluated on minival set and FPS w/ TensorRT and w/ CoreML is measured on T4 GPU and iPhone 12, respectively. The results highlight our superiority. and robotics [8], etc. Traditional approaches like YOLO series [1, 3, 21, 47], have leveraged convolutional neural networks to achieve real-time remarkable performance. However, their dependence on predefined object categories constrains flexibility in practical open scenarios. Such scenarios increasingly demand models capable of detecting and segmenting arbitrary objects guided by diverse prompt mechanisms, such as texts, visual cues, or without prompt. Given this, recent efforts have shifted towards enabling models to generalize for open prompts [5, 20, 49, 80]. They target single prompt type, e.g., GLIP [32], or multiple prompt types in unified way, e.g., DINO-X [49]. Specifically, with region-level vision-language pretraining [32, 37, 65], text prompts are usually processed by text encoder to serve as contrastive objectives for region features [20, 49], achieving recognition for arbitrary categories, e.g., YOLOWorld [5]. For visual prompts, they are often encoded as class embeddings tied to specified regions for identifying similar objects, by the interaction with image features or language-aligned visual encoder [5, 19, 30, 49], e.g., TRex2 [20]. In prompt-free scenario, existing methods typically integrate language models, finding all objects and generating the corresponding category names conditioned on region features sequentially [49, 62], e.g., GenerateU [33]. Despite notable advancements, single model that supports diverse open prompts for arbitrary objects with high efficiency and accuracy is still lacking. For example, DINO1 [49] features unified architecture, which, however, incurs resource-intensive training and inference overhead. Additionally, individual designs for different prompts in separate works exhibit suboptimal trade-offs between performance and efficiency, making it difficult to directly combine them into one model. For example, text-prompted approaches often incur substantial computational overhead when incorporating large vocabularies, due to complexity of cross-modality fusion [5, 32, 37, 49]. Visual-prompted methods usually compromise deployability on edge devices owing to the transformer-heavy design or reliance on additional visual encoder [20, 30, 67]. Prompt-free ways, meanwhile, depend on large language models, introducing considerable memory and latency costs [33, 49]. In light of these, in this paper, we introduce YOLOE(ye), highly efficient, unified, and open object detection and segmentation model, like human eye, under different prompt mechanisms, like texts, visual inputs, and promptfree paradigm. We begin with YOLO models with widely proven efficacy. For text prompts, we propose Reparameterizable Region-Text Alignment (RepRTA) strategy, which employs lightweight auxiliary network to improve pretrained textual embeddings for better visualsemantic alignment. During training, pre-cached textual embeddings require only the auxiliary network to process text prompts, incurring low additional cost compared with closed-set training. At inference and transferring, auxiliary network is seamlessly re-parameterized into the classification head, yielding an architecture identical to YOLOs with zero overhead. For visual prompts, we design SemanticActivated Visual Prompt Encoder (SAVPE). By formalizing regions of interest as masks, SAVPE fuses them with multi-scale features from PAN to produce grouped promptaware weights in low dimension in an activation branch and extract prompt-agnostic semantic features in semantic branch. Prompt embeddings are derived through aggregation of them, resulting in favorable performance with minimal complexity. For prompt-free scenario, we introduce Lazy Region-Prompt Contrast (LRPC) strategy. Without relying on costly language models, LRPC leverages specialized prompt embedding to find all objects and built-in large vocabulary for category retrieval. By matching only anchor points with identified objects against the vocabulary, LRPC ensures high performance with low overhead. Thanks to them, YOLOE excels in detection and segmentation across diverse open prompt mechanisms within one model, enjoying high inference efficiency and low training cost. Notably, as shown in Fig. 1, under 3 less training cost, YOLOE-v8-S significantly outperforms YOLOWorldv2-S [5] by 3.5 AP on LVIS [14], with 1.4 and 1.3 inference speedups on T4 and iPhone 12, respectively. In visual-prompted and prompt-free settings, YOLOE-v8-L outperforms T-Rex2 by 3.3 APr and GenerateU by 0.4 AP with 2 less training data and 6.3 fewer parameters, respectively. For transferring to COCO [34], YOLOE-v8-M / outperforms YOLOv8-M / by 0.4 / 0.6 APb and 0.4 / 0.4 APm with nearly 4 less training time. We hope that YOLOE can establish strong baseline and inspire further advancements in real-time open prompt-driven vision tasks. 2. Related Work Traditional detection and segmentation. Traditional approaches for object detection and segmentation primarily operate under closed-set paradigms. Early two-stage frameworks [4, 12, 15, 48], exemplified by Faster RCNN [48], introduce region proposal networks (RPNs) followed by region-of-interest (ROI) classification and regression. Meanwhile, single-stage detectors [10, 35, 38, 56, 72] prioritizes speed through grid-based predictions within single network. The YOLO series [1, 21, 27, 47, 59, 60] plays significant role in this paradigm and are widely used in real world. Moreover, DETR [28] and its variants [28, 69, 77] mark major shift by removing heuristicdriven components with transformer-based architectures. To achieve finer-grained results, existing instance segmentation methods predict pixel-level masks rather than bounding box coordinates [15]. For this, YOLACT [3] facilitates real-time instance segmentation through integration of prototype masks and mask coefficients. Based on DINO [69], MaskDINO [29] utilizes query embeddings and highresolution pixel embedding map to produce binary masks. Text-prompted detection and segmentation. Recent advancements in open-vocabulary object detection [13, 25, 61, 68, 7476] have focused on detecting novel categories by aligning visual features with textual embeddings. Specifically, GLIP [32] unifies object detection and phrase grounding through grounded pre-training on largescale image-text pairs, demonstrating robust zero-shot performance. DetCLIP [65] facilitates open-vocabulary learning by enriching the concepts with descriptions. Besides, Grounding DINO [37] enhances this by integrating crossmodality fusion into DINO, improving alignment between text prompts and visual representations. YOLO-World [5] further shows the potential of pretraining small detectors with open recognition capabilities based on the YOLO architecture. YOLO-UniOW [36] builds upon YOLO-World by leveraging the adaptive decision-learning strategy. Similarly, several open-vocabulary instance segmentation models [11, 18, 26, 45, 63] learn rich visual-semantic knowledge from advanced foundation models to perform segmentation on novel object categories. For example, X-Decoder [79] and OpenSeeD [71] explore both the open-vocabulary detection and segmentation tasks. APE [54] introduces universal visual perception model that aligns and prompts all objects in image using various text prompts. Visual-prompted detection and segmentation. While 2 Figure 2. The overview of YOLOE, which supports detection and segmentation for diverse open prompt mechanisms. For text prompts, We design re-parameterizable region-text alignment strategy to improve performance with zero inference and transferring overhead. For visual prompts, SAVPE is employed to encode visual cues with enhanced prompt embedding under minimal cost. For prompt-free setting, we introduce lazy region-prompt contrast strategy to provide category names for all identified objects efficiently by retrieval. text prompts offer generic description, certain objects can be challenging to describe with language alone, such as In such those requiring specialized domain knowledge. cases, visual prompts can guide detection and segmentation more flexibly and specifically, complementing text prompts [19, 20]. OV-DETR [67] and OWL-ViT [41] leverage CLIP encoders to process text and image prompts. MQDet [64] augments text queries with class-specific visual information from query images. DINOv [30] explores visual prompts as in-context examples for generic and referring vision tasks. T-Rex2 [20] integrates visual and text prompts by region-level contrastive alignment. For segmentation, based on large-scale data, SAM [23] presents flexible and strong model that can be prompted interactively and iteratively. SEEM [80] further explores segmenting objects with more various prompt types. Semantic-SAM [31] excels in semantic comprehension and granularity detection, handling both panoptic and part segmentation tasks. Prompt-free detection and segmentation. Existing approaches still depend on explicit prompts during inference for open-set detection and segmentation. To address this limitation, several works [33, 40, 49, 62, 66] explore integrating with generative language models to produce object descriptions for all found objects. For instance, GRiT [62] employs text decoder for both dense captioning and object detection tasks. DetCLIPv3 [66] trains an object captioner on large-scale data, enabling model to generate rich label information. GenerateU [33] leverages the language model to generate object names in free-form way. Closing remarks. To the best of our knowledge, aside from DINO-X [49], few efforts have achieved object detection and segmentation across various open prompt mechanisms within single architecture. However, DINO-X entails extensive training cost and notable inference overhead, severely constraining the practicality for real-world edge deployments. In contrast, our YOLOE aims to deliver an efficient and unified model that enjoys real-time performance and efficiency with easy deployability. 3. Methodology In this section, we detail designs of YOLOE. Building upon YOLOs (Sec. 3.1), YOLOE supports text prompts through RepRTA (Sec. 3.2), visual prompts via SAVPE (Sec. 3.3), and prompt-free scenario with LRPC (Sec. 3.4). 3.1. Model architecture As shown in Fig. 2, YOLOE adopts the typical YOLOs architecture [1, 21, 47], consisting of backbone, PAN, regression head, segmentation head, and object embedding head. The backbone and PAN extracts multi-scale features for the image. For each anchor point, the regression head predicts the bounding box for detection, and the segmentation head produces the prototype and mask coefficients for segmentation [3]. The object embedding head follows the structure of classification head in YOLOs, except that the output channel number of last 1 convolution layer is changed from the class number in closed-set scenario to the embedding dimension. Meanwhile, given text and visual prompts, we employ RepRTA and SAVPE to encode them as normalized prompt embeddings P, respectively. They serve as the classification weights and contrast with the anchor points object embeddings to obtain category labels. The process can be formalized as Label = : RN RDC RN , (1) where denotes the number of anchor points, indicates the number of prompts, and means the feature dimension of embeddings, respectively. 3 3.2. Re-parameterizable region-text alignment In open-set scenarios, the alignment between textual and object embeddings determines the accuracy of identified categories. Prior works usually introduce complex crossmodality fusion to improve the visual-textual representation for better alignment [5, 37]. However, these ways incur notable computational overhead, especially with large number of texts. Given this, we present Re-parameterizable RegionText Alignment (RepRTA) strategy, which improves pretrained textual embeddings during training through the reparameterizable lightweight auxiliary network. The alignment between textual and anchor points object embeddings can be enhanced with zero inference and transferring cost. Specifically, with the text prompts of with length of C, we first employ the CLIP text encoder [44, 57] to obtain pretrained textual embedding = TextEncoder(T ). Before training, we cache all embeddings of texts in datasets in advance and the text encoder can be removed with no extra training cost. Meanwhile, as shown in Fig. 3.(a), we introduce lightweight auxiliary network fθ with only one feed forward block [53, 58], where θ indicates the trainable parameters and introduces low overhead compared with closed-set training. It derives the enhanced textual embedding = fθ(P ) RCD for contrasting with the anchor points object embedding during training, leading to improved visual-semantic alignment. Let RDD11 be the kernel parameters of last convolution layer with input features RDHW in the object embedding head, be the convolution operator, and be the reshape function, we have Label = RDHW HW D(I K) (fθ(P ))T . (2) Moreover, after training, the auxiliary network can be reparameterized with the object embedding head into the identical classification head of YOLOs. The new kernel parameters RCD11 for last convolution layer after re-parameterization can be derived by = RCDCD11(fθ(P )) . (3) The final predication can be obtained by Label = , which is identical to the original YOLO architecture, leading to zero overhead for deployment and transferring to downstream closed-set tasks. 3.3. Semantic-activated visual prompt encoder Visual prompts are designed to indicate the object category of interest through visual cues, e.g., box and mask. To produce the visual prompt embedding, prior works often employ transformer-heavy design [20, 30], e.g., deformable attention [78], or additional CLIP vision encoder [44, 67]. These ways, however, introduce challenges in deployment and efficiency due to complex operators or high computational demands. Considering this, we introFigure 3. (a) The structure of lightweight auxiliary network in RepRTA, which consists of one SwiGLU FFN block [53]. (b) The structure of SAVPE, which consists of semantic branch to generate prompt-agnostic semantic features and activation branch to provide grouped prompt-aware weights. Visual prompt embedding can thus be efficiently derived by their aggregation. duce Semantic-Activated Visual Prompt Encoder (SAVPE) It features two defor efficiently processing visual cues. coupled lightweight branches: (1) Semantic branch outputs prompt-agnostic semantic features in channels without overhead of fusing visual cues, and (2) Activation branch produces grouped prompt-aware weights by interacting visual cues with image features in much fewer channels under low costs. Their aggregation then leads to informative prompt embedding under minimal complexity. As shown in Fig. 3.(b), in the semantic branch, we adopt the similar structure as object embedding head. With multiscale features {P3, P4, P5} from PAN, we employ two 33 convs for each scale, respectively. After upsampling, features are concatenated and projected to derive semantic features RDHW . In the activation branch, we formalize visual prompt as mask with 1 for indicated region and 0 for others. We downsample it and leverage 33 conv to derive prompt feature FV RAHW . Besides, we obtain image features FI RAHW for fusion with it from {P3, P4, P5} by convs. FV and FI are then concatenated and utilized to output prompt-aware weights RAHW , which is normalized using softmax within prompt-indicated region. Moreover, we divide the channels of into groups with channels in each. The channels in the i-th group share the weight Wi:i+1 from the ith channel of W. With D, we can process visual cues with image features in low dimension, bringing minimal cost. Furthermore, prompt embedding can be derived with aggregation of two branches by = Concat(G1, ..., GA); Gi = Wi:i+1 ST i: A (i+1). (4) It can thus contrast with anchor points object embeddings to identify objects with category of interest. 3.4. Lazy region-prompt contrast In prompt-free scenario without explicit guidance, models are expected to identity all objects with names in the image. Prior works usually formulate such setting as generative problem, where language model is employed to generate categories for dense found objects [33, 49, 62]. However, this introduces notable overhead, where language models, e.g., FlanT5-base [6] with 250M parameters in GenerateU [33] and OPT-125M [73] in DINO-X [49], are far from meeting high efficiency requirement. Given this, we reformulate such setting as retrieval problem and present Lazy Region-Prompt Contrast (LRPC) strategy. It lazily retrieves category names from built-in large vocabulary for anchor points with objects in the cost-effective way. Such paradigm enjoys zero dependency on language models, meanwhile with favorable efficiency and performance. Specifically, with pretrained YOLOE, we introduce specialized prompt embedding and train it exclusively to find all objects, where objects are treated as one category. Meanwhile, we follow [16] to collect large vocabulary which covers various categories and serve as the built-in data source for retrieval. One may directly leverage the large vocabulary as text prompts for YOLOE to identify all objects, which, however, incurs notable computational cost by contrasting abundant anchor points object embeddings with numerous textual embeddings. Instead, we employ the specialized prompt embedding Ps to find the set of anchor points corresponding to objects by = {o > δ}, (5) where denotes all anchor points and δ is the threshold hyperparameter for filtering. Then, only anchor points in are lazily matched against the built-in vocabulary to retrieve category names, bypassing the cost for irrelevant anchor points. This further improves efficiency without performance drop, facilitating the real world application. 3.5. Training objective During training, we follow [5] to obtain an online vocabulary for each mosaic sample with the texts involved in the images as positive labels. Following [21], we leverage taskaligned label assignment to match predictions with ground truths. The binary cross entropy loss is employed for classification, with IoU loss and distributed focal loss adopted for regression. For segmentation, we follow [3] to utilize binary cross-entropy loss for optimizing masks. 4. Experiments 4.1. Implementation details Model. For fair comparison with [5], we employ the same YOLOv8 architecture [21] for YOLOE. Besides, to verify its good generalizability on other YOLOs, we also experiment with YOLO11 architecture [21]. For both of them, we provide three model scales, i.e., small (S), medium (M), and large (L), to suit various application needs. Text prompts are encoded using the pretrained MobileCLIP-B(LT) [57] text encoder. We empirically use = 16 in SAVPE, by default. Data. We follow [5] to utilize detection and grounding datasets, including Objects365 (V1) [52], GoldG [22] (includes GQA [17] and Flickr30k [43]), where images from COCO [34] are excluded. Beside, we leverage advanced SAM-2.1 [46] model to generate pseudo instance masks using ground truth bounding boxes from the detection and grounding datasets for segmentation data. These masks undergo filtering and simplification to eliminate noise [9]. For visual prompt data, we follow [20] to leverage ground truth bounding boxes for visual cues. In prompt-free tasks, we reuse the same datasets, but annotate all objects as single category to learn specialized prompt embedding. Training. Due to limited computational resource, unlike YOLO-Worlds training for 100 epochs, we first train YOLOE with text prompts for 30 epochs. Then, we only train the SAVPE for merely 2 epochs with visual prompts, which avoids additional significant training cost that comes with supporting visual prompts. At last, we train the specialized prompt embedding for only 1 epoch for promptfree scenarios. During the text prompt training stage, we adopt the same settings as [5]. Notably, YOLOE-v8-S / / can be trained on 8 Nvidia RTX4090 GPUs in 12.0 / 17.0 / 22.5 hours, with 3 less cost compared with YOLOWorld. For visual prompt training, we freeze all other parts and adopt the same setting as in text prompt training. To enable prompt-free capability, we leverage the same data to train specialized embedding. We can see that YOLOE not only enjoys low training costs but also show exceptional zero-shot performance. Besides, to verify YOLOEs good transferability on downstream tasks, we fine-tune our YOLOE on COCO [34] for closed-set detection and segmentation. We experiment with two distinct practical fine- (1) Linear probing: Only the classifituning strategies: cation head is learnable and (2) Full tuning: All parameters are trainable. For Linear probing, we train all models for only 10 epochs. For Full tuning, we train small scale models including YOLOE-v8-S / 11-S for 160 epochs, and medium and large scale models including YOLOE-v8-M / and YOLOE-11-M / for 80 epochs, respectively. Metric. For text prompt evaluation, we utilize all category names from the benchmark as inputs, adhering to the standard protocol for open-vocabulary object detection tasks. For visual prompt evaluation, following [20], for each category, we randomly sample training images (N =16 by default), extract visual embeddings using their ground truth bounding boxes, and compute the average prompt embedding. For prompt-free evaluation, we employ the same protocol as [33]. pretrained text encoder [57] is employed to map open-ended predictions to semantically similar category names within the benchmark. In contrast to [33], we streamline the mapping process by selecting the most confident prediction, and eliminating the need for top-k selection and beam search. We use the tag list from [16] as the 5 Table 1. Zero-shot detection evaluation on LVIS. For fair comparisons, Fixed AP is reported on LVIS minival set in zero-shot manner. The training time is for text prompts, based on 8 Nvidia V100 GPUs for [32, 65] and 8 RTX4090 GPUs for YOLO-World and YOLOE. The FPS is measured on Nvidia T4 GPU using TensorRT and on iPhone 12 using CoreML, respectively. Results are provided with text prompt (T) and visual prompt (V) type. For training data, OI, HT, and CH indicates OpenImages [24], HierText [39], and CrowdHuman [51], respectively. OG indicates Objects365 [52] and GoldG [22], and G-20M represents Grounding-20M [50]. Model Prompt Type Params Training Data Training Time FPS T4 / iPhone GLIP-T [32] GLIPv2-T [70] GDINO-T [37] DetCLIP-T [65] G-1.5 Edge [50] T-Rex2 [20] YWorldv2-S [5] YWorldv2-M [5] YWorldv2-L [5] T T 232M 232M 172M 155M - - 13M 29M 48M - - 250.0h - OG,Cap4M 1337.6h OG,Cap4M OG,Cap4M OG G-20M O365,OI,HT CH,SA-1B OG OG OG 41.7h 60.0h 80.0h - YOLOE-v8-S / 12M / 13M YOLOE-v8-M / 27M / 30M YOLOE-v8-L / 45M / 50M YOLOE-11-S / 10M / 12M YOLOE-11-M / 21M / 27M YOLOE-11-L / 26M / 32M"
        },
        {
            "title": "OG\nOG\nOG\nOG\nOG\nOG",
            "content": "12.0h 17.0h 22.5h 13.0h 18.5h 23.5h built-in large vocabulary with total 4585 category names, and empirically use δ = 0.001 for LRPC, by default. For all three prompt types, following [5, 20, 33], evaluations are conducted on LVIS [14] in zero-shot manner, which contains 1,203 categories. By default, Fixed AP [7] on LVIS minival subset is reported. For transferring to COCO, standard AP is evaluated, following [1, 21]. Besides, we measure the FPS for all models on Nvidia T4 GPU with TensorRT and mobile device iPhone 12 with CoreML. 4.2. Text and visual prompt evaluation As shown in Tab. 1, for detection on LVIS, YOLOE exhibits favorable trade-offs between efficiency and zero-shot performance across different model scales. We also note that such results are achieved under much less training time, e.g., 3 faster than YOLO-Worldv2. Specifically, YOLOEv8-S / / outperforms YOLOv8-Worldv2-S / / by 3.5 / 0.2 / 0.4 AP, along with 1.4 / 1.3 / 1.3 and 1.3 / 1.2 / 1.2 inference speedups on T4 and iPhone 12, respectively. Besides, for rare category which is challenging, our YOLOE-v8-S and YOLOE-v8-L obtains significant improvements of 5.2% and 7.6% APr. Besides, compared with YOLO-Worldv2, while YOLOE-v8-M / achieves lower APf , this performance gap primarily stems from YOLOEs integration of both detection and segmentation in one model. Such multi-task learning introduces trade-off that adversely impact detection performance on frequent categories, as shown in Tab. 5. Besides, YOLOE with YOLO11 architecture also exhibits favorable perforAP 26.0 29.0 27.4 34.4 33. 37.4 24.4 32.4 35.5 APr 20.8 - 18.1 26.9 28.0 29.9 17.1 28.4 25. APc 21.4 - 23.3 33.9 34.3 33.9 22.5 29.6 34.6 APf 31.0 - 32.7 36.3 33. 41.8 27.3 35.5 38.1 27.9 / 26.2 32.6 / 31.0 35.9 / 34.2 27.5 / 26.3 33.0 / 31.4 35.2 / 33.7 22.3 / 21.3 26.9 / 27.0 33.2 / 33.2 21.4 / 22.5 26.9 / 27.1 29.1 / 28.1 27.8 / 27.7 31.9 / 31.7 34.8 / 34.6 26.8 / 27.1 32.5 / 31.9 35.0 / 34.6 29.0 / 25.7 34.4 / 31.1 37.3 / 34.1 29.3 / 26.4 34.5 / 31.7 36.5 / 33. - / - - / - - / - - / - - / - - / - 216.4 / 48.9 117.9 / 34.2 80.0 / 22.1 305.8 / 64.3 156.7 / 41.7 102.5 / 27.2 301.2 / 73.3 168.3 / 39.2 130.5 / 35.1 mance and efficiency. For example, YOLOE-11-L achieves comparable AP with YOLO-Worldv2-L, but with notably 1.6 inference speedups on T4 and iPhone 12, highlighting the strong generalizability of our YOLOE. Moreover, the inclusion of visual prompts further amplifies YOLOEs versatility. Compared with T-Rex2, YOLOEv8-L yield the improvements of 3.3 APr and 0.9 APc, with 2 less training data (3.1 vs. Our: 1.4 M) and much lower training resource (16 Nvidia A100 GPUs vs. Our: 8 Nvidia RTX4090 GPUs). Besides, for visual prompts, while we only train SAVPE with other parts frozen for 2 epochs, we note that it can achieve comparable APr and APc with the text prompts for various model scales. This shows the efficacy of visual prompts in less frequent objects that text prompts often struggle to accurately describe, which is similar to the observation in [20]. Furthermore, for segmentation, we present the evaluation results on the LVIS val set with the standard APm reported in Tab. 2. It shows that YOLOE exhibits strong performance by leveraging both text prompts and visual prompts. Specifically, YOLOE-v8-M / achieves 20.8 and 23.5 APm in the zero-shot manner, significantly outperforming YOLO-Worldv2-M / that is fine-tuned on LVIS-Base dataset, by 3.0 and 3.7 APm, respectively. These results well show the superiority of YOLOE. 4.3. Prompt-free evaluation As shown in Tab. 3, for prompt-free scenario, YOLOE also exhibits superior performance and efficiency. Specifically, 6 Table 2. Segmentation evaluation on LVIS. We evaluate all models on LVIS val set with the standard APm reported. YOLOE supports both text (T) and visual cues (V) as inputs. indicates that the pretrained models are fine-tuned on LVIS-Base data for segmentation head. In contrast, we evaluate YOLOE in zero-shot manner without utilizing any images from LVIS during training. Model Prompt APm YWorld-M YWorld-L YWorldv2-M YWorldv2-L T 16.7 19.1 17.8 19.8 APm 12.6 14.2 13.9 17.2 APm 14.6 17.2 15.5 17.5 APm 20.8 23.5 22.0 23.6 YOLOE-v8-S / 17.7 / 16.8 15.5 / 13.5 16.3 / 16.7 20.3 / 18.2 YOLOE-v8-M / 20.8 / 20.3 17.2 / 17.0 19.2 / 20.1 24.2 / 22.0 YOLOE-v8-L / 23.5 / 22.0 21.9 / 16.5 21.6 / 22.1 26.4 / 24.3 YOLOE-11-S / 17.6 / 17.1 16.1 / 14.4 15.6 / 16.8 20.5 / 18.6 YOLOE-11-M / 21.1 / 21.0 17.2 / 18.3 19.6 / 20.6 24.4 / 22.6 YOLOE-11-L / 22.6 / 22.5 19.3 / 20.5 20.9 / 21.7 26.0 / 24. Table 3. Prompt-free evaluation on LVIS. Fixed AP is reported on the LVIS minival set, following the protocol in [33]. The FPS is measured on Nvidia T4 GPU with Pytorch [42]."
        },
        {
            "title": "Backbone Params AP APr APc APf FPS",
            "content": "GenerateU [33] GenerateU [33] Swin-T Swin-L 297M 26.8 20.0 24.9 29.8 0.48 467M 27.9 22.3 25.2 31.4 0.40 YOLOE-v8-S YOLOv8-S 13M 21.0 19.1 21.3 21.0 95.8 YOLOE-v8-M YOLOv8-M 29M 24.7 22.2 24.5 25.3 45.9 YOLOE-v8-L YOLOv8-L 47M 27.2 23.5 27.0 28.0 25.3 YOLOE-11-S YOLO11-S 11M 20.6 18.4 20.2 21.3 93.0 YOLOE-11-M YOLO11-M 24M 25.5 21.6 25.5 26.1 42.5 YOLOE-11-L YOLO11-L 29M 26.3 22.7 25.8 27.5 34.9 YOLO-v8-L achieves 27.2 AP and 23.5 APr, outperforming GenerateU with Swin-T backbone by 0.4 AP and 3.5 APr, along with 6.3 fewer parameters and 53 inference speedups. It shows the effectiveness of YOLOE by reformulating the open-ended problem as the retrieval task for built-in large vocabulary and underscores its potential in generalizing across wide range of categories without replying on explicit prompts. Such functionality also enhances YOLOEs practicality, enabling its application in broader range of real-world scenarios. 4.4. Downstream transferring As shown in Tab. 4, when transferring to COCO for downstream closed-set detection and segmentation, YOLOE exhibits favorable performance under limited training epochs in both two fine-tuning strategies. Specifically, for Linear probing, with less than 2% of the training time, YOLOE11-M / can achieve over 80% of the performance of YOLO11-M / L, respectively. This highlights the strong transferability of YOLOE. For Full tuning, YOLOE can further enhance the performance under limited training cost. For example, with nearly 4 less training epochs, YOLOETable 4. Downstream transfer on COCO. We fine-tune YOLOE on COCO and report the standard AP for both detection and segmentation. We experiment with two practical fine-tuning strategies, i.e., Linear probing and Full tuning. 50 APb 75 APm APm 50 APm 75 Model YOLOv8-S YOLOv8-M YOLOv8-L YOLO11-S YOLO11-M YOLO11-L Epochs APb APb Training from scratch 500 300 300 500 600 600 44.7 61.4 50.0 66.8 52.4 69.3 46.6 63.3 51.5 68.5 53.3 70.1 Linear probing 35.6 51.5 42.2 59.2 45.4 63.3 37.0 52.9 43.1 60.6 45.1 62."
        },
        {
            "title": "10\nYOLOE-v8-S\nYOLOE-v8-M 10\n10\nYOLOE-v8-L\nYOLOE-11-S\n10\nYOLOE-11-M 10\n10\nYOLOE-11-L",
            "content": "YOLOE-v8-S 160 YOLOE-v8-M 80 80 YOLOE-v8-L YOLOE-11-S 160 YOLOE-11-M 80 80 YOLOE-11-L"
        },
        {
            "title": "Full tuning",
            "content": "45.0 61.6 50.4 67.0 53.0 69.8 46.2 62.9 51.3 68.3 52.6 69.7 48.7 54.8 57.2 50.6 55.7 58.2 38.9 46.3 50.0 40.4 47.4 49.5 49.1 55.2 57.9 50.0 56.0 57.5 36.6 40.5 42.3 37.8 41.5 42.8 30.3 35.5 38.3 31.5 36.5 38. 36.7 40.9 42.7 37.6 41.5 42.4 58.0 63.4 66.0 59.7 65.0 66.8 48.2 55.6 59.6 49.7 56.9 59.2 58.3 63.7 66.5 59.3 64.8 66.2 38.6 43.3 44.9 40.0 43.9 45.5 32.0 37.7 40.8 33.5 39.0 40. 39.1 43.5 45.6 40.1 44.3 45.2 Table 5. Roadmap to YOLOE in terms of text prompts. The standard AP is reported on LVIS minival set in the zero-shot manner. The FPS is is measured on Nvidia T4 GPU and iPhone 12 with TensorRT (T) and CoreML (C), respectively."
        },
        {
            "title": "Model",
            "content": "Epochs AP APr APc APf FPS (T / C)"
        },
        {
            "title": "100\nYOLO-Worldv2-L\n+ Fewer train. epochs\n30\n+ Global negative dict. 30\n- Cross-modal. fusion\n30\n+ MobileCLIP encoder 30\n30\n+ RepRTA\n30\n+ Segment. (YOLOE)",
            "content": "33.0 22.6 32.0 35.8 80.0 / 22.1 31.0 22.6 28.8 34.2 80.0 / 22.1 31.9 22.8 31.0 34.4 80.0 / 22.1 30.0 19.1 28.0 33.9 102.5 / 27.2 31.5 20.2 30.5 34.4 102.5 / 27.2 33.5 29.5 32.0 35.5 102.5 / 27.2 33.3 30.8 32.2 34.6 102.5 / 27.2 v8-M / outperforms YOLOv8-M / by 0.4 APm and 0.6 APb, respectively. Under 3 less training time, YOLO-v8S also obtains better performance compared with YOLOv8S for both detection and segmentation. These results well demonstrate that YOLOE can serve as strong starting point for transferring to downstream task. 4.5. Ablation study We further provide extensive analyses for the effectiveness of designs in our YOLOE. Experiments are conducted on YOLOE-v8-L and standard AP is reported on LVIS minival set for zero-shot evaluation, by default. Roadmap to YOLOE. We outline the stepwise progression from the baseline model YOLOv8-Worldv2-L to our Figure 4. (a) Zero-shot inference on LVIS. (b) Results with customized text prompt, where white hat, red hat, white car, sunglasses, mustache, tie are provided as text prompts. (c) Results with visual prompt, where the red dashed bounding box serves as the visual cues. (d) Results in prompt-free scenario, where no explicit prompt is provided. Please refer to the supplementary for more examples. Table 6. Effective. of SAVPE. Table 7. Effective. of LRPC. Model AP APr APc APf Model Mask pool 30.4 27.6 31.3 30.2 31.9 29.4 32.5 31.7 SAVPE v8-S LRPC AP APr APc APf FPS 21.0 19.1 21.4 21.0 56.5 δ = 1e3 21.0 19.1 21.3 21.0 95.8 δ = 1e4 21.0 19.1 21.3 21.0 66.1 δ = 1e2 20.8 19.1 21.2 20.8 106 = 1 = 16 = 30.9 28.2 31.9 30.4 31.9 29.4 32.5 31.7 31.9 28.2 33.0 31.7 v8-L 27.2 23.5 27.0 28.0 19.9 δ = 1e3 27.2 23.5 27.0 28.0 25.3 YOLOE-v8-L in terms of text prompts in Tab. 5. With the initial baseline metric of 33.0% AP, due to limited computational resource, we first reduce the training epochs to 30, leading to 31.0% AP. Besides, instead of using empty string as negative texts for grounding data, we follow [65] by maintaining global dictionary to sample more diverse negative prompts. The global dictionary is constructed by selecting category names that appear more than 100 times in the training data. This leads to 0.9% AP improvement. Next, we remove the cross-modality fusion to avoid costly visual-textual feature interaction, which results in 1.9% AP degradation but with 1.28 and 1.23 inference speedups on T4 and iPhone 12, respectively. To address this drop, we utilize stronger MobileCLIP-B(LT) text encoder [57] to obtain better pretrained textual embeddings, which recovers AP to 31.5%. Furthermore, we employ RepRTA to enhance the alignment between anchor points object and textual embeddings, which leads to notable 2.3% AP enhancement with zero inference overhead, showing its effectiveness. At last, we introduce the segmentation head and train YOLOE for detection and segmentation simultaneously. Although this leads to 0.2% AP and 0.9 APf drop due to multi-task learning, YOLOE gains ability to segment arbitrary objects. Effectiveness of SAVPE. To verify the effectiveness of SAVPE for visual inputs, we remove the activation branch and simply leverage mask pooling to aggregate semantic features with the formulated visual prompt mask. As shown in Tab. 6, SAVPE significantly outperforms Mask pool by 1.5 AP. This is because Mask pool neglects the varying semantic importance at different positions within promptindicated region, while our activation branch effectively models such difference, leading to improved aggregation of semantic features and better prompt embedding for contrast. We also examine the impact of different group numbers, i.e., A, in the activation branch. As shown in Tab. 6, performance can also be enhanced with only group, i.e., = 1. Besides, we can achieve the strong performance of 31.9 AP under = 16, obtaining the favorable balance, where more groups lead to marginal performance difference. Effectiveness of LRPC. To verify the effectiveness of LRPC for prompt-free setting, we introduce the baseline that directly leverage the built-in large vocabulary as text prompts for YOLOE to identify all objects. Tab. 7 presents the comparison results. We observe that with the same performance, our LRPC obtains notably 1.7 / 1.3 inference speedups for YOLOE-v8-S / L, respectively, by lazily retrieving the categories for anchor points with found objects and skipping the numerous irrelevant ones. These results well highlight its efficacy and practicality. Besides, with different threshold δ for filtering, LRPC can achieve different performance and efficiency trade-offs, e.g., enabling 1.9 speedup for YOLOE-v8-S with only 0.2 AP drop. 4.6. Visualization analyses We conduct visualization analyses for YOLOE in four scenarios: (1) Zero-shot inference on LVIS in Fig. 4.(a), where its category names are text prompts, (2) Text prompts in Fig. 4.(b), where arbitrary texts can be input as prompts, (3) Visual prompts in Fig. 4.(c), where visual cues can be drawn as prompts, and (4) No explicit prompt in Fig. 4.(d), where model identifies all objects. We can see that YOLOE performs well and can accurately detect and segment various objects in these diverse scenarios, further showing its efficacy and practicality in various applications. 5. Conclusion In this paper, we present YOLOE, single highly efficient model that seamlessly integrates object detection and segmentation across diverse open prompt mechanisms. Specifically, we introduce RepRTA, SAVPE, and LRPC to enable YOLOs to process textual prompt, visual cues, and prompt-free paradigm with favorable performance and low cost. Thanks to them, YOLOE enjoys strong capabilities and high efficiency for various prompt ways, enabling realtime seeing anything. We hope that it can serve as strong baseline to inspire further advancements."
        },
        {
            "title": "References",
            "content": "[1] Alexey Bochkovskiy, Chien-Yao Wang, and HongYuan Mark Liao. Yolov4: Optimal speed and accuracy of object detection. arXiv preprint arXiv:2004.10934, 2020. 1, 2, 3, 6 [2] Daniel Bogdoll, Maximilian Nitsche, and Marius Zollner. Anomaly detection in autonomous driving: survey. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44884499, 2022. 1 [3] Daniel Bolya, Chong Zhou, Fanyi Xiao, and Yong Jae Lee. Yolact: Real-time instance segmentation. In Proceedings of the IEEE/CVF international conference on computer vision, pages 91579166, 2019. 1, 2, 3, 5 [4] Zhaowei Cai and Nuno Vasconcelos. Cascade r-cnn: Delving into high quality object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 61546162, 2018. 2 [5] Tianheng Cheng, Lin Song, Yixiao Ge, Wenyu Liu, XingYolo-world: Real-time gang Wang, and Ying Shan. In Proceedings of the open-vocabulary object detection. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1690116911, 2024. 1, 2, 4, 5, [6] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Yunxuan Li, Xuezhi Wang, Mostafa Dehghani, Siddhartha Brahma, et al. Scaling instructionfinetuned language models. Journal of Machine Learning Research, 25(70):153, 2024. 5 [7] Achal Dave, Piotr Dollar, Deva Ramanan, Alexander Kirillov, and Ross Girshick. Evaluating large-vocabulary object detectors: The devil is in the details. arXiv preprint arXiv:2102.01066, 2021. 6 [8] Douglas Henke Dos Reis, Daniel Welfer, Marco Antonio De Souza Leite Cuadros, and Daniel Fernando Tello Gamarra. Mobile robot navigation using an object recognition software with rgbd images and the yolo algorithm. Applied Artificial Intelligence, 33(14):12901305, 2019. 1 [9] David Douglas and Thomas Peucker. Algorithms for the reduction of the number of points required to represent digitized line or its caricature. Cartographica: the international journal for geographic information and geovisualization, 10 (2):112122, 1973. 5, 13 [10] Chengjian Feng, Yujie Zhong, Yu Gao, Matthew Scott, and Weilin Huang. Tood: Task-aligned one-stage object detection. In 2021 IEEE/CVF International Conference on Computer Vision (ICCV), pages 34903499. IEEE Computer Society, 2021. 2 [11] Golnaz Ghiasi, Xiuye Gu, Yin Cui, and Tsung-Yi Lin. Scaling open-vocabulary image segmentation with image-level In European conference on computer vision, pages labels. 540557. Springer, 2022. 2 [12] Ross Girshick. Fast r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 14401448, 2015. [13] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui. Open-vocabulary object detection via vision and language knowledge distillation. International Conference on Learning Representation, 2022. 2 [14] Agrim Gupta, Piotr Dollar, and Ross Girshick. Lvis: dataset for large vocabulary instance segmentation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 53565364, 2019. 2, 6 [15] Kaiming He, Georgia Gkioxari, Piotr Dollar, and Ross Girshick. Mask r-cnn. In Proceedings of the IEEE international conference on computer vision, pages 29612969, 2017. 1, 2 [16] Xinyu Huang, Yi-Jie Huang, Youcai Zhang, Weiwei Tian, Rui Feng, Yuejie Zhang, Yanchun Xie, Yaqian Li, and Lei Zhang. Open-set image tagging with multi-grained text supervision. arXiv preprint arXiv:2310.15200, 2023. 5 [17] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. 5, 13 [18] Dat Huynh, Jason Kuen, Zhe Lin, Jiuxiang Gu, and Ehsan Elhamifar. Open-vocabulary instance segmentation via roIn Proceedings of the bust cross-modal pseudo-labeling. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 70207031, 2022. 2 [19] Qing Jiang, Feng Li, Tianhe Ren, Shilong Liu, Zhaoyang Zeng, Kent Yu, and Lei Zhang. T-rex: Counting by visual prompting. arXiv preprint arXiv:2311.13596, 2023. 1, 3 [20] Qing Jiang, Feng Li, Zhaoyang Zeng, Tianhe Ren, Shilong Liu, and Lei Zhang. T-rex2: Towards generic object detection via text-visual prompt synergy. In European Conference on Computer Vision, pages 3857. Springer, 2024. 1, 2, 3, 4, 5, 6 [21] Glenn Jocher, Jing Qiu, and Ayush Chaurasia. Ultralytics YOLO, 2023. 1, 2, 3, 5, 6 [22] Aishwarya Kamath, Mannat Singh, Yann LeCun, Gabriel Synnaeve, Ishan Misra, and Nicolas Carion. Mdetrmodulated detection for end-to-end multi-modal understanding. In Proceedings of the IEEE/CVF international conference on computer vision, pages 17801790, 2021. 5, 6, 13 [23] Alexander Kirillov, Eric Mintun, Nikhila Ravi, Hanzi Mao, Chloe Rolland, Laura Gustafson, Tete Xiao, Spencer Whitehead, Alexander Berg, Wan-Yen Lo, et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, pages 40154026, 2023. 3 [24] Ivan Krasin, Tom Duerig, Neil Alldrin, Vittorio Ferrari, Sami Abu-El-Haija, Alina Kuznetsova, Hassan Rom, Jasper Uijlings, Stefan Popov, Andreas Veit, et al. Openimages: public dataset for large-scale multi-label and multi-class image classification. Dataset available from https://github. com/openimages, 2(3):18, 2017. 6 [25] Weicheng Kuo, Yin Cui, Xiuye Gu, AJ Piergiovanni, and Anelia Angelova. F-vlm: Open-vocabulary object detection upon frozen vision and language models. International Conference on Learning Representation, 2022. 2 [26] Boyi Li, Kilian Weinberger, Serge Belongie, Vladlen Koltun, and Rene Ranftl. Language-driven semantic segmentation. International Conference on Learning Representation, 2022. 2 [27] Chuyi Li, Lulu Li, Hongliang Jiang, Kaiheng Weng, Yifei Geng, Liang Li, Zaidan Ke, Qingyuan Li, Meng Cheng, 9 Weiqiang Nie, et al. Yolov6: single-stage object detection framework for industrial applications. arXiv preprint arXiv:2209.02976, 2022. 2 [28] Feng Li, Hao Zhang, Shilong Liu, Jian Guo, Lionel Ni, and Lei Zhang. Dn-detr: Accelerate detr training by introIn Proceedings of the IEEE/CVF ducing query denoising. conference on computer vision and pattern recognition, pages 1361913627, 2022. 2 [29] Feng Li, Hao Zhang, Huaizhe Xu, Shilong Liu, Lei Zhang, Lionel Ni, and Heung-Yeung Shum. Mask dino: Towards unified transformer-based framework for object detection In Proceedings of the IEEE/CVF conand segmentation. ference on computer vision and pattern recognition, pages 30413050, 2023. 2 [30] Feng Li, Qing Jiang, Hao Zhang, Tianhe Ren, Shilong Liu, Xueyan Zou, Huaizhe Xu, Hongyang Li, Jianwei Yang, In ProChunyuan Li, et al. Visual in-context prompting. ceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1286112871, 2024. 1, 2, 3, 4 [31] Feng Li, Hao Zhang, Peize Sun, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianwei Yang, Lei Zhang, and Jianfeng Gao. Segment and recognize anything at any granularity. In European Conference on Computer Vision, pages 467484. Springer, 2024. [32] Liunian Harold Li, Pengchuan Zhang, Haotian Zhang, Jianwei Yang, Chunyuan Li, Yiwu Zhong, Lijuan Wang, Lu Yuan, Lei Zhang, Jenq-Neng Hwang, et al. Grounded the language-image pre-training. IEEE/CVF conference on computer vision and pattern recognition, pages 1096510975, 2022. 1, 2,"
        },
        {
            "title": "In Proceedings of",
            "content": "[33] Chuang Lin, Yi Jiang, Lizhen Qu, Zehuan Yuan, and Jianfei Cai. Generative region-language pretraining for open-ended object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1395813968, 2024. 1, 2, 3, 5, 6, 7 [34] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. 2, 5 [35] Tsung-Yi Lin, Priya Goyal, Ross Girshick, Kaiming He, and Piotr Dollar. Focal loss for dense object detection. In Proceedings of the IEEE international conference on computer vision, pages 29802988, 2017. 2 [36] Lihao Liu, Juexiao Feng, Hui Chen, Ao Wang, Lin Song, Jungong Han, and Guiguang Ding. Yolo-uniow: Efficient universal open-world object detection. arXiv preprint arXiv:2412.20645, 2024. 2 [37] Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, pages 3855. Springer, 2024. 1, 2, 4, 6 [38] Wei Liu, Dragomir Anguelov, Dumitru Erhan, Christian Szegedy, Scott Reed, Cheng-Yang Fu, and Alexander In Computer Berg. Ssd: Single shot multibox detector. VisionECCV 2016: 14th European Conference, Amsterdam, The Netherlands, October 1114, 2016, Proceedings, Part 14, pages 2137. Springer, 2016. 2 [39] Shangbang Long, Siyang Qin, Dmitry Panteleev, Alessandro Bissacco, Yasuhisa Fujii, and Michalis Raptis. Towards end-to-end unified scene text detection and layout analysis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10491059, 2022. 6 [40] Yanxin Long, Youpeng Wen, Jianhua Han, Hang Xu, Pengzhen Ren, Wei Zhang, Shen Zhao, and Xiaodan Liang. Capdet: Unifying dense captioning and open-world detection pretraining. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 15233 15243, 2023. 3 [41] Matthias Minderer, Alexey Gritsenko, Austin Stone, Maxim Neumann, Dirk Weissenborn, Alexey Dosovitskiy, Aravindh Mahendran, Anurag Arnab, Mostafa Dehghani, Zhuoran In Shen, et al. Simple open-vocabulary object detection. European conference on computer vision, pages 728755. Springer, 2022. 3 [42] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. 7 [43] Bryan Plummer, Liwei Wang, Chris Cervantes, Juan Caicedo, Julia Hockenmaier, and Svetlana Lazebnik. Flickr30k entities: Collecting region-to-phrase correIn Prospondences for richer image-to-sentence models. ceedings of the IEEE international conference on computer vision, pages 26412649, 2015. 5, [44] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. 4 [45] Yongming Rao, Wenliang Zhao, Guangyi Chen, Yansong Tang, Zheng Zhu, Guan Huang, Jie Zhou, and Jiwen Lu. Denseclip: Language-guided dense prediction with contextIn Proceedings of the IEEE/CVF conaware prompting. ference on computer vision and pattern recognition, pages 1808218091, 2022. 2 [46] Nikhila Ravi, Valentin Gabeur, Yuan-Ting Hu, Ronghang Hu, Chaitanya Ryali, Tengyu Ma, Haitham Khedr, Roman Radle, Chloe Rolland, Laura Gustafson, et al. Sam 2: arXiv preprint Segment anything in images and videos. arXiv:2408.00714, 2024. 5, 13 [47] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779788, 2016. 1, 2, 3 [48] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region proposal networks. Advances in neural information processing systems, 28, 2015. 1, 2 10 [49] Tianhe Ren, Yihao Chen, Qing Jiang, Zhaoyang Zeng, Yuda Xiong, Wenlong Liu, Zhengyu Ma, Junyi Shen, Yuan Gao, Xiaoke Jiang, et al. Dino-x: unified vision model for openworld object detection and understanding. arXiv preprint arXiv:2411.14347, 2024. 1, 2, 3, [50] Tianhe Ren, Qing Jiang, Shilong Liu, Zhaoyang Zeng, Wenlong Liu, Han Gao, Hongjie Huang, Zhengyu Ma, Xiaoke Jiang, Yihao Chen, Yuda Xiong, Hao Zhang, Feng Li, Peijun Tang, Kent Yu, and Lei Zhang. Grounding dino 1.5: Advance the edge of open-set object detection, 2024. 6 [51] Shuai Shao, Zijian Zhao, Boxun Li, Tete Xiao, Gang Yu, Xiangyu Zhang, and Jian Sun. Crowdhuman: bencharXiv preprint mark for detecting human in crowd. arXiv:1805.00123, 2018. 6 [52] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 84308439, 2019. 5, 6, 13 [53] Noam Shazeer. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. 4 [54] Yunhang Shen, Chaoyou Fu, Peixian Chen, Mengdan Zhang, Ke Li, Xing Sun, Yunsheng Wu, Shaohui Lin, and Rongrong Ji. Aligning and prompting everything all at once for universal visual perception. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1319313203, 2024. 2 [55] Joseph Sobek, Jose Medina Inojosa, Betsy Medina Inojosa, SM Rassoulinejad-Mousavi, Gian Marco Conte, Francisco Lopez-Jimenez, and Bradley Erickson. Medyolo: medical image object detection framework. Journal of Imaging Informatics in Medicine, pages 19, 2024. [56] Zhi Tian, Chunhua Shen, Hao Chen, and Tong He. Fcos: Fully convolutional one-stage object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 96279636, 2019. 2 [57] Pavan Kumar Anasosalu Vasu, Hadi Pouransari, Fartash Faghri, Raviteja Vemulapalli, and Oncel Tuzel. Mobileclip: Fast image-text models through multi-modal reinforced In Proceedings of the IEEE/CVF Conference on training. Computer Vision and Pattern Recognition, pages 15963 15974, 2024. 4, 5, 8 [58] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. 4 [59] Ao Wang, Hui Chen, Lihao Liu, Kai Chen, Zijia Lin, Jungong Han, et al. Yolov10: Real-time end-to-end object detection. Advances in Neural Information Processing Systems, 37:107984108011, 2025. 2 [60] Chien-Yao Wang, Alexey Bochkovskiy, and HongYuan Mark Liao. Yolov7: Trainable bag-of-freebies sets In Pronew state-of-the-art for real-time object detectors. ceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 74647475, 2023. [61] Yu Wang, Xiangbo Su, Qiang Chen, Xinyu Zhang, Teng Xi, Kun Yao, Errui Ding, Gang Zhang, and Jingdong Wang. Ovlw-detr: Open-vocabulary light-weighted detection transformer. arXiv preprint arXiv:2407.10655, 2024. 2 [62] Jialian Wu, Jianfeng Wang, Zhengyuan Yang, Zhe Gan, Zicheng Liu, Junsong Yuan, and Lijuan Wang. Grit: generative region-to-text transformer for object understanding. In European Conference on Computer Vision, pages 207224. Springer, 2024. 1, 3, 5 [63] Jiarui Xu, Shalini De Mello, Sifei Liu, Wonmin Byeon, Thomas Breuel, Jan Kautz, and Xiaolong Wang. Groupvit: In Semantic segmentation emerges from text supervision. Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1813418144, 2022. 2 [64] Yifan Xu, Mengdan Zhang, Chaoyou Fu, Peixian Chen, Xiaoshan Yang, Ke Li, and Changsheng Xu. Multi-modal queried object detection in the wild. Advances in Neural Information Processing Systems, 36:44524469, 2023. 3 [65] Lewei Yao, Jianhua Han, Youpeng Wen, Xiaodan Liang, Dan Xu, Wei Zhang, Zhenguo Li, Chunjing Xu, and Hang Xu. Detclip: Dictionary-enriched visual-concept paralleled pretraining for open-world detection. Advances in Neural Information Processing Systems, 35:91259138, 2022. 1, 2, 6, 8 [66] Lewei Yao, Renjie Pi, Jianhua Han, Xiaodan Liang, Hang Xu, Wei Zhang, Zhenguo Li, and Dan Xu. Detclipv3: Towards versatile generative open-vocabulary object detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2739127401, 2024. [67] Yuhang Zang, Wei Li, Kaiyang Zhou, Chen Huang, and Chen Change Loy. Open-vocabulary detr with conditional In European conference on computer vision, matching. pages 106122. Springer, 2022. 2, 3, 4 [68] Alireza Zareian, Kevin Dela Rosa, Derek Hao Hu, and ShihFu Chang. Open-vocabulary object detection using captions. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1439314402, 2021. 2 [69] Hao Zhang, Feng Li, Shilong Liu, Lei Zhang, Hang Su, Jun Zhu, Lionel Ni, and Heung-Yeung Shum. Dino: Detr with improved denoising anchor boxes for end-to-end object detection. arXiv preprint arXiv:2203.03605, 2022. 2 [70] Haotian Zhang, Pengchuan Zhang, Xiaowei Hu, Yen-Chun Chen, Liunian Harold Li, Xiyang Dai, Lijuan Wang, Lu Yuan, Jenq-Neng Hwang, and Jianfeng Gao. Glipv2: Unifying localization and vision-language understanding, 2022. 6 [71] Hao Zhang, Feng Li, Xueyan Zou, Shilong Liu, Chunyuan Li, Jianwei Yang, and Lei Zhang. simple framework for In Proceedopen-vocabulary segmentation and detection. ings of the IEEE/CVF International Conference on Computer Vision, pages 10201031, 2023. 2 [72] Shifeng Zhang, Cheng Chi, Yongqiang Yao, Zhen Lei, and Stan Li. Bridging the gap between anchor-based and anchor-free detection via adaptive training sample selection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97599768, 2020. 2 [73] Susan Zhang, Stephen Roller, Naman Goyal, Mikel Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan, Mona Diab, Xian Li, Xi Victoria Lin, et al. Opt: Open pre-trained transformer language models. arXiv preprint arXiv:2205.01068, 2022. 5 [74] Tiancheng Zhao, Peng Liu, Xuan He, Lu Zhang, and Kyusong Lee. Real-time transformer-based open-vocabulary arXiv preprint detection with efficient arXiv:2403.06892, 2024. 2 fusion head. [75] Yiwu Zhong, Jianwei Yang, Pengchuan Zhang, Chunyuan Li, Noel Codella, Liunian Harold Li, Luowei Zhou, Xiyang Dai, Lu Yuan, Yin Li, et al. Regionclip: RegionIn Proceedings of the based language-image pretraining. IEEE/CVF conference on computer vision and pattern recognition, pages 1679316803, 2022. [76] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp Krahenbuhl, and Ishan Misra. Detecting twenty-thousand classes using image-level supervision. In European conference on computer vision, pages 350368. Springer, 2022. 2 [77] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection. International Conference on Learning Representation, 2021. 2 [78] Xizhou Zhu, Weijie Su, Lewei Lu, Bin Li, Xiaogang Wang, and Jifeng Dai. Deformable detr: Deformable transformers for end-to-end object detection, 2021. [79] Xueyan Zou, Zi-Yi Dou, Jianwei Yang, Zhe Gan, Linjie Li, Chunyuan Li, Xiyang Dai, Harkirat Behl, Jianfeng Wang, Lu Yuan, et al. Generalized decoding for pixel, image, and language. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1511615127, 2023. 2 [80] Xueyan Zou, Jianwei Yang, Hao Zhang, Feng Li, Linjie Li, Jianfeng Wang, Lijuan Wang, Jianfeng Gao, and Yong Jae Lee. Segment everything everywhere all at once. Advances in neural information processing systems, 36:1976919782, 2023. 1, 3 12 A. More Implementation Details Data. We employ Objects365[52], GoldG [22] (including GQA[17] and Flickr30k [43]) for training YOLOE. Tab. 8 present their details. We utilize SAM-2.1-Hiera-Large [46] to generate high-quality pseudo labeling of segmentation masks with ground truth bounding boxes as prompts. We filter out ones with too few areas. To enhance the smoothness of mask edges, we apply Gaussian kernel to masks, using 33 and 77 kernels for small and large ones, respectively. Besides, we refine the masks following [9], which iteratively simplifies the mask contours. This reduces noise pixels while preserving overall structure. Table 8. Data details for YOLOE training. Type Dataset Objects365 [52] Detection Grounding GQA [17] Grounding Flickr [43] Box Mask Images Anno. 609k 621k 149k 8,530k 3,662k 638k Training. For all models, we adopt AdamW optimizer with an initial learning rate of 0.002. The batch size and weight decay are set to 128 and 0.025, respectively. The data augmentation includes color jittering, random affine transformations, random horizontal flipping, and mosaic augmentation. During transferring to COCO, for both Linear probing and Full tuning, we utilize the AdamW optimizer with an initial learning rate of 0.001, setting the batch size and weight decay to 128 and 0.025, respectively. B. More Analyses for LRPC To qualitatively show the efficacy of LRPC strategy, we visualize the number of anchor points retained for category retrieval after filtering. We present their average count under varying filtering threshold δ on the LVIS minival set in Fig. 5. It reveals that as δ increases, the number of retained anchor points decreases substantially across different models. This reduction significantly lowers computational overhead compared with the baseline scenario, which employs total of 8400 anchor points. For example, for YOLOEv8-S, with δ = 0.001, the number of valid anchor points is reduced by 80%, enjoying 1.7 inference speedup with the same performance (see Tab. 7 in the paper). The results further confirm the notably redundancy of anchor points for category retrieval and verify the efficacy of LRPC. C. More Visualization Results To qualitatively show the efficacy of YOLOE, we present more visualization results for it in various scenarios. Zero-shot inference on LVIS. In Fig. 6, we present the zero-shot inference capabilities of YOLOE on the LVIS. By 13 Figure 5. The count of retained anchor points under different filtering thresholds in LRPC. The dashed line means the total number. leveraging the 1203 category names as text prompts, the model demonstrates its ability to detect and segment diverse objects across various images. Prompt with customized texts. Fig. 7 presents the results with customized text prompts. We can see that YOLOE can interpret both generic and specific textual inputs, enabling precise object detection and fine-grained segmentation. Such capability allows users to tailor the models behavior to meet specific requirements by defining input prompts at varying levels of granularity. Prompt with visual inputs. In Fig. 8, we present the results of YOLOE with visual inputs as prompt. The visual inputs can take various forms, such as bounding box, point, It can also be provided across the or handcrafted shape. images. We can see that with visual prompt indicating the target object, YOLOE can accurately find other instances of the same category. Beside, it performs well across different objects and images, exhibiting robust capability. Prompt-free inference. Fig. 9 shows the results of YOLOE with the prompt-free paradigm. We can see that in such setting, YOLOE achieves effective identification for diverse objects. This highlights its practicality in scenarios where predefined inputs are unavailable or impractical. Figure 6. Zero-Shot inference on LVIS. The categories of LVIS are provided as text prompts. Figure 7. Prompt with customized texts. YOLOE adapts to both generic and specific text prompts for flexible usage. Figure 8. Prompt with visual inputs. YOLOE demonstrates the ability to identify objects guided by various visual prompts, like bounding box (top left), point (top right), handcrafted shape (bottom left). The visual prompt can also be applied across images (bottom right). 14 Figure 9. Prompt-free inference (omitting segmentation masks for clearer visualization). YOLOE can find diverse objects without prompt."
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}