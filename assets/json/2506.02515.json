{
    "paper_title": "FinChain: A Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning",
    "authors": [
        "Zhuohan Xie",
        "Dhruv Sahnan",
        "Debopriyo Banerjee",
        "Georgi Georgiev",
        "Rushil Thareja",
        "Hachem Madmoun",
        "Jinyan Su",
        "Aaryamonvikram Singh",
        "Yuxia Wang",
        "Rui Xing",
        "Fajri Koto",
        "Haonan Li",
        "Ivan Koychev",
        "Tanmoy Chakraborty",
        "Salem Lahlou",
        "Veselin Stoyanov",
        "Preslav Nakov"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-step symbolic reasoning is critical for advancing downstream performance on financial tasks. Yet, benchmarks for systematically evaluating this capability are lacking. Existing datasets like FinQA and ConvFinQA supervise only final numerical answers, without assessing intermediate reasoning steps. To address this, we introduce FinChain, the first symbolic benchmark designed for verifiable Chain-of- Thought (CoT) financial reasoning. Spanning 54 topics across 12 financial domains, Fin- Chain offers five parameterized templates per topic, each varying in reasoning complexity and domain expertise required. Each dataset instance includes an executable Python trace, enabling automatic generation of extensive training data and easy adaptation to other domains. We also introduce ChainEval, a new metric for automatic evaluation of both final answers and intermediate reasoning. Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models have considerable room for improvement in multi-step financial reasoning. All templates and evaluation metrics for FinChain are available at https: //github.com/mbzuai-nlp/finchain."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . [ 1 5 1 5 2 0 . 6 0 5 2 : r FinChain: Symbolic Benchmark for Verifiable Chain-of-Thought Financial Reasoning Zhuohan Xie1 Dhruv Sahnan1 Debopriyo Banerjee1 Georgi Georgiev2 Rushil Thareja1 Hachem Madmoun3 Jinyan Su4 Aaryamonvikram Singh1 Yuxia Wang1 Rui Xing1 Fajri Koto1 Haonan Li1 Ivan Koychev2 Tanmoy Chakraborty5 Salem Lahlou1 Veselin Stoyanov1 Preslav Nakov1 1MBZUAI, UAE 2FMI, Sofia University, Bulgaria 3Quantsquare, France 4Cornell University, USA 5IIT Delhi, India {zhuohan.xie, preslav.nakov}@mbzuai.ac.ae"
        },
        {
            "title": "Abstract",
            "content": "FinChain Template (Compound Interest) Multi-step symbolic reasoning is critical for advancing downstream performance on financial tasks. Yet, benchmarks for systematically evaluating this capability are lacking. Existing datasets like FinQA and ConvFinQA supervise only final numerical answers, without assessing intermediate reasoning steps. To address this, we introduce FinChain, the first symbolic benchmark designed for verifiable Chain-ofThought (CoT) financial reasoning. Spanning 54 topics across 12 financial domains, FinChain offers five parameterized templates per topic, each varying in reasoning complexity and domain expertise required. Each dataset instance includes an executable Python trace, enabling automatic generation of extensive training data and easy adaptation to other domains. We also introduce ChainEval, new metric for automatic evaluation of both final answers and intermediate reasoning. Benchmarking 30 LLMs on our dataset, we find that even state-of-the-art models have considerable room for improvement in multi-step financial reasoning. All templates and evaluation metrics for FinChain are available at https: //github.com/mbzuai-nlp/finchain. #Question: investor_name invested principal in project_name . The investment grows at an annual interest rate of rate % compounded annually over time years. Calculate the compound interest (CI) . #Variables: - investor_name = sample(investors) - project_name = sample(projects) - principal = range(1000, 5000) - rate = uniform(2, 10) - time = range(1, 5)"
        },
        {
            "title": "Compute",
            "content": "#Chain-of-Thought Solution: Step compound the 1: amount = principal amount: (cid:19) time (cid:18) 1 + rate 100 Step 2: Compute the compound interest: CI = amount"
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated strong performance across wide range of tasks (Zhao et al., 2023; Xie et al., 2023b), including practical applications in fields such as finance, healthcare and law (Chen et al., 2024b). In the financial field, for instance, effective analysis often requires processing large volumes of text from diverse sources, including documents, news articles, and social media. These texts frequently reflect or shape financial phenomena, such as market sentiment (Nie et al., 2024). Figure 1: Symbolic template for generating compound interest problems in FinChain. traction (Shah et al., 2023), sentiment analysis (Pei et al., 2022), and text classification (Sy et al., 2023), where models are trained to output relatively simple targets such as entity spans, sentiment labels, or numerical scores. In contrast, our task addresses financial reasoning, which requires models to generate complete chain-of-thought (CoT) reasoning traces that justify each step in solving financial problem, as shown in Figure 1. Prior work in financial NLP has predominantly focused on adapting general-purpose tasks to financial text. These tasks include information exExisting benchmarks for financial reasoning, such as FinQA (Chen et al., 2021) and ConvFinQA (Chen et al., 2022), frame the task primarily as numerical question answering. These datasets focus on predicting the final answer and do not require models to explicitly generate or justify intermediate reasoning steps. While some examples may include traces of intermediate operations (loosely resembling reasoning steps), these are neither comprehensive nor rigorously structured to reflect the standards of financial analysis. In particular, they lack explicit, verifiable representations of full reasoning chain, which is critical for transparency and accountability in financial decision-making. As result, these benchmarks offer limited diagnostic value: they do not reliably reveal where models reasoning succeeds or fails, nor do they distinguish genuine multi-step reasoning from shallow pattern matching. In the field of mathematical reasoning, the symbolic template paradigm introduced by GSMSymbolic (Mirzadeh et al., 2024) has become widely adopted approach. This method re-templates 100 math problems from GSM8K (Cobbe et al., 2021), providing explicit intermediate steps alongside the final answer. We adopt similar templating strategy, but construct our dataset entirely from scratch for the financial field. As illustrated in Figure 1, each symbolic template encodes parameterized financial problem (e.g., compound interest) with variable named entities and numerical inputs. Each template is paired with executable Python code that computes both intermediate steps and the final result. This design enables scalable, contamination-free generation of consistent examples for both training and evaluation. The financial field comprises wide range of distinct domains, each involving different stakeholders and downstream objectives. Consequently, it is essential to design the dataset in templated manner that supports domain-specific evaluation. To address this, we organize our dataset using fine-grained financial taxonomy (Figure 2) covering 12 high-level domains (e.g., Corporate Finance, Sustainable Finance, Crypto), each comprising distinct topics, 54 topics in total. For each topic, we develop five parameterized templates (two easy, two intermediate, one advanced), varying in symbolic reasoning complexity and required domain knowledge. To the best of our knowledge, this hierarchical structure constitutes the most detailed classification of financial reasoning tasks among existing datasets in this area. Each templated instance includes: (1) scenario card describing the topic, difficulty tier, and sampled inputs (e.g., Discounted cash flow valuation, advanced), and (2) an executable chain of reasoning steps in Python, grounded in domainspecific equations and concepts. Because every operation in the chain is explicit and executable, our benchmark supports full machine-verifiability: any hallucinated, skipped, or incorrect step in models output can be automatically detected. This contrasts sharply with existing datasets like FinQA and ConvFinQA, which only supervise the final answer. To enable rigorous evaluation, we introduce ChainEval, which assesses both the correctness of the final answer and the alignment of intermediate reasoning steps. Our evaluation across proprietary and open-source LLMs reveals consistent pattern: while large models such as GPT-4.1 and LLaMA 3.3 (70B) achieve high overall accuracy, they still struggle with complex symbolic tasks and multi-step financial reasoning. In contrast, smaller models consistently underperform, even when finetuned on financial data. These findings indicate that domain-specific supervision alone is insufficient; robust reasoning performance also depends on adequate model capacity for handling symbolic and multi-hop inference. Our main contributions in this study are summarized below. We present the first from-scratch symbolic reasoning, conbenchmark for financial structed using custom designed financial taxonomy spanning 12 domains and 54 topics. For each topic, we provide five parameterized templates (two easy, two intermediate, one advanced) along with executable Python code that generates step-by-step reasoning traces for each instance. We propose an automatic alignment metric that evaluates models chain of thought solution by comparing it against gold-standard reasoning traces, assessing both the correctness of the final answer and the alignment of intermediate steps. We evaluate range of open-source and proprietary large language models and large reasoning models, and observe that even stateof-the-art models exhibit substantial difficulty in handling symbolic financial reasoningparticularly on advanced templates that 2 require longer and more complex reasoning chains."
        },
        {
            "title": "2.1 Financial NLP\nRecent advances in financial NLP have been driven\nby both task-specific models and large-scale do-\nmain adaptation. Early work focused on extrac-\ntion and classification tasks using models such\nas FinBERT (Araci, 2019), while more recent\nefforts have extended to personal finance (Hean\net al., 2025), credit scoring (Feng et al., 2023),\nand risk-awareness benchmarking (Yuan et al.,\n2024). Domain-specific datasets such as FiNER-\nORD, REFinD, FinARG, and ECTSum provide\nsupport for named entity recognition, relation ex-\ntraction, argument mining, and summarization, re-\nspectively (Shah et al., 2023; Kaur et al., 2023;\nMukherjee et al., 2022; Xie et al., 2024).",
            "content": "Apart from task-specific models, general-purpose financial language models have grown rapidly in both scale and specialization. BloombergGPT (Wu et al., 2023) demonstrated strong performance across wide range of financial NLP tasks, while FinGPT (Liu et al., 2023) emphasized open-source adaptability. FinMA (Xie et al., 2023a), compact model fine-tuned from LLaMA, achieved performance competitive with proprietary baselines. With the emergence of these general-purpose models, there has been corresponding increase in benchmarking efforts aimed at evaluating their performance across diverse financial tasks. FLANG (Shah et al., 2022) introduced early baselines, which were further extended by FinBen (Xie et al., 2024), covering 24 tasks, and FinMTEB (Tang and Yang, 2025), spanning 64 embedding datasets. BizBench (Koncel-Kedziorski et al., 2023) and PIXIU (Xie et al., 2023a) highlighted challenges in quantitative and multimodal reasoning. Despite this progress, limitations remain in multistep mathematical reasoning, long-context understanding, and cross-market generalization (Chen et al., 2024b). These challenges motivate the need for benchmarks that assess capabitity of models to perform faithful, auditable reasoning grounded in financial knowledge."
        },
        {
            "title": "2.2 Financial Reasoning\nReal-world financial problems typically require ar-\nriving at a precise numerical result. Consequently,",
            "content": "evaluating financial reasoning through numerical QA framework emerges as natural and intuitive choice. FinQA (Chen et al., 2021) and ConvFinQA (Chen et al., 2022) supervise models to generate arithmetic programs that compute final answers, but offer only weak supervision over intermediate steps. The resulting traces are neither fully explicit nor verifiable. FinTextQA (Chen et al., 2024a) introduces long-form financial questions derived from textbooks and regulatory documents. While it emphasizes explanatory, retrievalbased QA, it only assesses final answer correctness and lacks annotated reasoning steps. Outside finance, GSM-Symbolic (Mirzadeh et al., 2024) retemplates GSM8K (Cobbe et al., 2021) problems into symbolic programs with step-level supervision. However, these tasks focus on basic arithmetic and lack financial semantics. No equivalent benchmark currently exists in the financial domain. Model-centric efforts have also aimed to enhance financial reasoning. FinR1 (Liu et al., 2025) applies reinforcement learning over synthetic programs, and Fino1 (Qian et al., 2025), based on an 8B LLaMA model, outperform larger baselines on FinQA, ConvFinQA, and FinTextQA. Nonetheless, these models still underperform on long-horizon and cross-table reasoning. In summary, existing benchmarks either provide coarse program-level supervision (FinQA, ConvFinQA), fine-grained but domain-agnostic traces (GSM-Symbolic), or retrieval-based QA without traceability (FinTextQA). None support systematic, step-level, domain-aware supervision grounded in financial equations. FinChain addresses this gap by introducing symbolic, executable benchmark for financial reasoning. It supervises each intermediate step, supports both automatic alignment and human assessment, and covers 54 topics across 12 financial domains."
        },
        {
            "title": "3.1 Data Creation Process",
            "content": "The data creation process begins with the identification and definition of financial domains. This step is informed by established literature (Bodie et al., 2025) and guided by input from financial experts. Through this process, we identify 12 distinct financial domains. To generate topics within each domain, we extract relevant passages from the literature and use the domain name as prompt to 3 Figure 2: FinChain taxonomy of financial reasoning topics. Our benchmark spans 54 topics organized into 12 major domains, ranging from traditional areas like Corporate Finance and Financial Reporting to emerging fields such as Crypto Finance and Sustainable Finance. This hierarchical structure enables fine-grained evaluation of symbolic reasoning across diverse financial domains. ChatGPT1 to extract candidate financial topics. The resulting list is then curated by our financial experts, who filter and refine the outputs to yield total of 54 financial topics, averaging 4.5 topics per domain. This structured approach results in the comprehensive taxonomy illustrated in Figure 2. Inspired by Mirzadeh et al. (2024), we develop executable Python methods that generate both questions and solutions featuring chain-of-thought reasoning by instructing ChatGPT. The prompt is provided in Appendix A. For each topic, we prompt the model to produce ten template-based questions and explicitly guide it to vary the complexity levels, easy, intermediate, and advanced, based on the number of reasoning steps required. We then manually select five representative templates per topic, two easy, two intermediate, and one advanced, to ensure balanced coverage across difficulty levels in FinChain."
        },
        {
            "title": "3.2 Sanity Check\nAfter initial template generation through prompting,\nwe observed several recurring issues that we intend\nto filter out from our dataset creation methodology,\nto ensure utmost quality. Therefore, we imple-\nmented a dedicated sanity check phase focused on\nthe following problems: (1) Cross-national in-\nconsistencies: The automated prompting process,\noccasionally introduced hyper-localized financial\ncontexts (e.g., currencies, exchange rates, indices\nand terminology from local economies in varied\ncountries). To ensure consistency, we standardized",
            "content": "1We use GPT-4o, the latest version available at the time of all questions to U.S. based financial settings only. (2) Precision mismatch: Questions and solutions often used rounded values for readability, while full precision values were used in the underlying computation. We adjusted the computation outputs to match the displayed precision. (3) Incomplete input specification: Some questions omitted key variables needed for calculation. We manually edited these cases to include all required inputs."
        },
        {
            "title": "3.3 Expert Check",
            "content": "To address the known limitations of automated generation, we incorporate expert verification by qualified financial professionals as critical component of our quality control process. To reduce LLM specific artifacts and improve reliability, we employ two-step strategy. First, we introduce model diversity by using different LLM to assist in annotation, mitigating overfitting to the idiosyncrasies of single model. Second, when this auxiliary LLM identifies potential error or inconsistency, the case is escalated to human expert for validation. Specifically, we first conducted an automated screening using another high-capacity LLM, Claude 3.5 Sonnet (Claude). Claude was prompted to assess the soundness of each question-andsolution template produced by ChatGPT.2 Templates flagged by Claude as potentially flawed were escalated to qualified financial expert. The experts role was to adjudicate these cases by evaluating the correctness of the original ChatGPT template in light of Claudes feedback. Each template writing. 2The prompt is provided in Appendix B. 4 (a) was classified into one of three categories: Correct, (b) Incorrect, or (c) Correct but better expressed from an alternative financial perspective3. If template was deemed incorrect, we regenerated and re-validated it through the same pipeline until version was confirmed as correct by the expert. This iterative filtering process ensured that only high-quality, financially sound templates were retained in FinChain."
        },
        {
            "title": "4 ChainEval",
            "content": "We propose ChainEval, an evaluation framework that assesses model outputs along two axes: final answer correctness and reasoning step alignment. While our step-wise semantic alignment approach is inspired by prior work on reasoning consistency (Lyu et al., 2023; Golovneva et al., 2023), we extend it by explicitly modeling the verification of intermediate results through step-answer matching. Moreover, unlike prior work, which primarily evaluates textual consistency, our framework also verifies the final numeric answer, which ensures holistic view of model performance, on both reasoning faithfulness and end-task fidelity. To the best of our knowledge, this combination has not been previously explored, particularly in the context of financial reasoning tasks."
        },
        {
            "title": "4.1 Preliminaries",
            "content": "We define the gold solution and the predicted solution ˆS as sequence of and steps, respectively, represented as: = (s 1, . . . , m), ˆS = (ˆs1, . . . , ˆsn), (1) where and ˆsj represent individual steps in and ˆS, respectively. We also define function StepRes(), denoted as: StepRes(si) = ai, (2) which extracts the intermediate result ai, computed at the ith step of the corresponding solution."
        },
        {
            "title": "4.2 Reasoning Step Alignment",
            "content": "To measure reasoning faithfulness, we break the evaluation into two components: 3In some cases, the expert noted that ChatGPTs template was valid, but Claude proposed an equally sound formulation. The expert chose the most appropriate version for the dataset. Step Semantic-Similarity. We embed each step using sentence encoder Enc() and compute cosine similarity between gold and predicted step pairs (s , ˆsj), resulting in scores SS() [0, 1], represented as: SS(s , ˆsj) = CosSim (Enc(s ), Enc(ˆsj)) (3) denotes the ith step of the gold solution where and ˆsj denotes the jth step of the predicted solution. Step Answer-Match. We evaluate whether the and ˆsj are results computed at intermediate steps consistent with each other. To this end, we introduce function AM(.), and the answer matching process is denoted as follows: StepRes(s ) = , StepRes(ˆsj) = ˆaj, AM(s , ˆsj) = (cid:40)I( ˆaj a I(ˆaj == ) ϵ) R, if ˆaj, otherwise. (4) where I() is the indicator function, and ϵ is small tolerance threshold to account for errors propagating due to numerical rounding. We set ϵ = 0.05 for our experiments, allowing relative error of up to 5%. Now, we consider pair of steps to be aligned with each other if: (i) their semantic similarity is above threshold, and (ii) the corresponding intermediate results are consistent. Using Equations (3) and (4), we represent the process of measuring step-level alignment between predicted solution and ground truth as follows: , ˆsj) = SS(s A(s StepAlign(s , ˆsj) AM(s A(s , ˆS) = max j=1,...,n , ˆsj), , ˆsj), (5) and similarly, StepAlign(S, ˆsj) = max i=1,...,m A(s , ˆsj) (6) Finally, we compute step-level recall and precision using Equations (5) and (6): Rec = 1 (cid:88) (cid:16) i=1 StepAlign(s , ˆS) τ (cid:17) , (7) Prec = 1 (cid:88) j= (StepAlign(ˆsj, S) τ ) , (8) where τ is an alignment score threshold. 5 We also define the StepF1, which is the harmonic mean of step-level precision and recall: StepF1 = 2 Prec Rec Prec + Rec + γ , (9) where γ is small constant to prevent division by zero. We set τ = 0.7 and γ = 0.0001 for our experiments."
        },
        {
            "title": "4.3 Final Answer Correctness\nTo measure effectiveness of the models on overall\nend-task fidelity, we evaluate whether the final\nanswer in the predicted solution is consistent with\nthat of the ground truth. We define a function\nFAC(·) using AM(·) from Equation (4). We extract\nthe final answer from the last step of the solution,\nand formalize it as follows:",
            "content": "FAC(ˆS, S) = AM(s m, ˆsn) (10)"
        },
        {
            "title": "5 Experiments and Results",
            "content": "In this section, we evaluated the symbolic financial reasoning capabilities of 30 language models over 2,700 cases, and analyzed performance from overall, domain and difficulty-level perspectives."
        },
        {
            "title": "5.1 Evaluated Models\nTo comprehensively benchmark model capabili-\nties, we evaluate a diverse set of language models\ngrouped into six categories:",
            "content": "General-Purpose Models serve as strong domain-agnostic baselines. We include GPT-4.1, GPT-{4o, 4.1} mini (Hurst et al., 2024), LLaMA 3.1 {8, 70}B Instruct, LLaMA 3.3 70B Instruct (Grattafiori et al., 2024), Mistral 7B Instruct (Jiang et al., 2023), Mixtral (8x7B) Instruct (Jiang et al., 2024), 27}B Gemma Instruct (Riviere et al., 2024), Gemma 3 27B Instruct (Kamath et al., 2025), and Qwen2.5 7B (Yang et al., 2024a). {9, 2 General Reasoning Models are trained or fine-tuned for broad multi-step reasoning tasks. We include o{3, 4}-mini, DeepSeek-R1 Distill LLaMA {8, 70}B, DeepSeek-R1 Distill Qwen {7, 32}B (Guo et al., 2025), and Qwen3 {8, 30}B (Qwen, 2025). Financially Fine-Tuned Models are adapted via instruction tuning or continued pretraining on financial corpora. We evaluate Finance-LLM 13B (Cheng et al., 2024), 13B (CeADAR, 2023) FinanceConnect WiroAI Finance-LLaMA 8B, and WiroAI Finance-Qwen 7B (Abdullah Bezir, 2025). Financial Reasoning Models are specifically designed for multi-step reasoning in finance, often leveraging chain-of-thought (CoT) supervision. We include Fino1 8B (Qian et al., 2025) and FinR1 7B (Liu et al., 2025). Mathematical Models are fine-tuned on mathcentric corpora for symbolic computation. We include Mathstral 7B (Mistral, 2024), and Qwen2.5 Math 7B (Yang et al., 2024b). Mathematical Reasoning Models excel at step-by-step problem solving in mathematics. We include WizardMath 7B (Luo et al., 2023), and MetaMath 13B (Yu et al., 2023)."
        },
        {
            "title": "5.2 Experimental Setup",
            "content": "We generate 10 instances per symbolic template using different random seeds, resulting in total of 54 topics 5 templates 10 instances = 2,700 test cases. All models are evaluated under consistent decoding configuration: temperature = 0.7, top-p = 0.95, and maximum token limit of 4,096. For evaluation, we use our proposed metric, ChainEval, as the primary indicator for both final answer correctness and alignment of intermediate reasoning steps. Additionally, we include two widely used reference-based metrics (Xie et al., 2023c) to assess surface-level generation quality: ROUGE (Lin, 2004) for n-gram overlap and BERTScore (Zhang et al., 2020) for semantic similarity."
        },
        {
            "title": "5.3.1 Overall Model Performance\nTable 1 presents a comprehensive evaluation of\nmodel performance on FinChain, measured across\nfour components of ChainEval, including Final\nAnswer Correctness (FAC), Step Precision/Recal-\nl/F1, alongside ROUGE and BERTScore. Overall,\ngeneral-purpose models such as LLaMA 3.3 (70B)\nInstruct and GPT-4.1 lead in both accuracy and\nconsistency. These models also exhibit moderate\nvariance, indicating not only high performance\nbut also robust generalization across diverse finan-\ncial scenarios. Crucially, the results highlight a\nstrong correlation between model size and reason-\ning ability. Larger models consistently outperform\ntheir smaller counterparts, even when the latter are\nexplicitly trained on domain-specific data, which\nsuggests that domain exposure alone is insufficient\nwithout adequate model capacity.",
            "content": "Model FAC ChainEval Step-Precision Step-Recall Step-F R2 ROUGE RL RLsum BERTScore General-Purpose Mistral (7B) Instruct v0.3 Qwen2.5 (7B) Mixtral (8x7B) Instruct v0.1 Llama 3.1 (8B) Instruct Gemma 2 (9B) Instruct Gemma 2 (27B) Instruct Gemma 3 (27B) Instruct Llama 3.1 (70B) Instruct Llama 3.3 (70B) Instruct GPT-4o-mini GPT-4.1-mini GPT-4.1 General Reasoning DeepSeek-R1-Distill Qwen (7B) DeepSeek-R1-Distill Llama (8B) Qwen3 (8B) Qwen3 (30B) A3B DeepSeek-R1-Distill Qwen (32B) DeepSeek-R1-Distill Llama (70B) o3-mini o4-mini Finance Fine-Tuned WiroAI Finance Qwen (7B) WiroAI Finance Llama (8B) FinanceConnect (13B) Finance-LLM (13B) Finance Reasoning Fin-R1 (7B) Fino1 (8B) Math Fine-Tuned Mathstral (7B) Qwen2.5-Math (7B) Math Reasoning MetaMath (13B) WizardMath (7B) 0.320.24 0.40830.32 0.3570.28 0.42670.28 0.48380.21 0.53250.21 0.48790.18 0.57350.25 0.5840.22 0.5240.23 0.56680.24 0.56910.23 0.43830.31 0.43940.31 0.54020.27 0.50890.18 0.49910.30 0.49270.31 0.51830.25 0.38730. 0.44190.28 0.28530.28 0.07250.12 0.17090.21 0.33170.20 0.35540.24 0.33450.21 0.39350.21 0.46430.16 0.49370.15 0.50230.16 0.48110.21 0.51450.19 0.43520.19 0.48770.19 0.41970.21 0.35680.23 0.35830.22 0.39920.23 0.42450.21 0.40710.23 0.40130.23 0.38810.21 0.29760.19 0.40640.23 0.27290.23 0.08840.13 0.22470.19 0.210.14 0.20950.16 0.23540.15 0.22660.13 0.32430.11 0.34880.11 0.30640.09 0.30480.15 0.27240.11 0.23850.11 0.28820.12 0.25270.13 0.22390.15 0.22040.13 0.21450.13 0.19950.12 0.2620.15 0.2550.15 0.24320.12 0.17680. 0.24440.15 0.24860.17 0.26220.16 0.27290.15 0.3670.12 0.39260.12 0.3650.11 0.35580.16 0.33840.13 0.29230.13 0.34560.13 0.29930.15 0.26060.16 0.25760.15 0.26430.15 0.25090.13 0.30390.17 0.29650.17 0.28390.14 0.21260.13 0.11830.02 0.16150.04 0.15960.03 0.14540.03 0.19750.03 0.19620.03 0.17170.02 0.16910.04 0.14920.03 0.13370.02 0.17250.02 0.16570.02 0.18870.04 0.18410.04 0.19510.05 0.03730.01 0.18950.04 0.19610.04 0.20280.03 0.19450.03 0.18680.02 0.25230.06 0.25370.04 0.22730.04 0.30210.03 0.30020.03 0.25670.02 0.2590.05 0.22690.03 0.21770.02 0.26450.02 0.26310.02 0.31260.05 0.30040.05 0.30180.06 0.05380.01 0.31810.04 0.32390.05 0.30830.04 0.30930. 0.22390.02 0.30520.07 0.30450.05 0.27530.05 0.36670.04 0.36210.03 0.30450.03 0.31080.05 0.26940.04 0.25880.02 0.32010.03 0.31760.03 0.3810.05 0.36890.05 0.3640.06 0.06490.01 0.3840.05 0.39220.05 0.37140.04 0.38060.04 0.25310.15 0.16270.15 0.05610.08 0.14430.13 0.29330.17 0.19110.17 0.06460.09 0.16540.14 0.17820.04 0.16250.05 0.1660.04 0.16280.04 0.28660.05 0.26710.06 0.28370.05 0.26710. 0.34090.06 0.32540.08 0.34820.06 0.32630.06 0.83410.002 0.85640.01 0.85840.01 0.85650.01 0.86460.01 0.86390.006 0.85670.005 0.8580.01 0.85370.006 0.84580.005 0.85690.006 0.85050.007 0.8640.01 0.8650.01 0.86960.01 0.84530.01 0.85750.01 0.86180.01 0.85740.02 0.85630.02 0.86510.01 0.86110.02 0.85730.01 0.86310.01 0.5120.28 0.41910.26 0.36490.22 0.36210. 0.19780.12 0.16370.10 0.24280.15 0.21330.12 0.16850.04 0.17010.03 0.28060.04 0.27950.04 0.34290.05 0.34580.05 0.85850.01 0.8680. 0.43970.26 0.18910.26 0.39530.21 0.17630.21 0.27320.16 0.12980.15 0.30630.16 0.14120.16 0.16350.03 0.07050.06 0.25860.04 0.13370. 0.30980.05 0.14880.10 0.85870.01 0.80670.02 0.23130.22 0.34610.28 0.27040.20 0.33570.24 0.13570.10 0.19420.14 0.17170.13 0.23320. 0.18630.05 0.19330.06 0.30640.06 0.31420.07 0.37530.07 0.37370.08 0.87280.01 0.87230.01 Table 1: Overall performance of language models on FinChain. We report four components of our ChainEval metric (FAC, step precision, step recall, and step-level F1), alongside ROUGE (R2, RL, RLsum) and BERTScore. Models are grouped into six categories. Top-performing models under FAC within each category are bolded. Open-source models such as Qwen3 (8B) demonstrate competitive performance despite its smaller size, achieving strong results, which can be attributed to its enhanced reasoning capabilities in recent release. Financially reasoning models, such as Fin-R1 exhibit reasonable symbolic alignment but fall short in final answer accuracy. Overall, our findings highlight the critical role of model scale in symbolic financial reasoning. Larger generalpurpose models not only deliver higher performance but also generalize across tasks. Yet, smaller models tend to struggle to complete reasoning chains accurately despite domain-specific tuning. This underscores the advantage of large-scale instruction tuning for chain-of-thought reasoning in finance."
        },
        {
            "title": "5.3.2 Performance Over Domains\nWe present how model performance varies across\nfinancial domains in Figure 3, which shows domain-\nlevel FAC for representative models. For the non-\nreasoning models, structured and quantitative do-\nmains such as Personal Finance and Crypto Fi-\nnance show higher FAC. These areas tend to require\nstraightforward computations or familiar terminolo-\ngies, which general-purpose and math-tuned mod-",
            "content": "els can handle well. In contrast, domains like Risk Management and Sustainable Finance exhibit clear performance drops, likely due to their multi-step logic requirements, ambiguous scenarios, or regulatory nuances that are harder to encode symbolically. For reasoning models, we observe stronger performance in traditionally challenging domains like Financial Ratios and Corporate Finance, suggesting improved reasoning ability over multi-step symbolic structures. Nevertheless, even reasoning models still exhibit relatively weaker results in Sustainable Finance and Mergers & Acquisitions, which demands nuanced understanding and domain-specific reasoning strategies. These disparities underscore our domain-specific taxonomy enables granular diagnosis of model strengths and blind spots, offering roadmap for developing targeted financial reasoning capabilities. We report domain-wise performance across all evaluated models in Appendix C."
        },
        {
            "title": "5.3.3 Performance Over Difficulty Levels\nTo assess model robustness under increasing com-\nplexity of reasoning, we group the results into three\npredefined tiers (Basic, Intermediate, and Advanced",
            "content": "7 Figure 3: Domain-level performance across financial domains. Radar plots compare representative models from (a) Non-reasoning models and (b) Reasoning models. Each spoke corresponds to one of the 12 top-level financial domains. Models are selected as the top performers within each of the six categories presented in Table 1."
        },
        {
            "title": "Model",
            "content": "GPT-4.1 GPT-4.1-mini GPT-4o-mini o3-mini o4-mini Both 0.50800.4999 0.47200.4992 0.37600.4844 0.40400.4907 0.34000.4737 Name Only 0.49600.5000 0.46400.4987 0.36000.4800 0.39200.4882 0.36000.4800 Value Only 0.50400.5000 0.46400.4987 0.37200.4833 0.41600.4929 0.36800.4823 Table 2: Model robustness to input perturbation. FAC is reported for Both, Name Only, and Value Only. model reasoning performance under one domain: Investment Analysis. We consider three settings: (i) Name Only, where only names are varied; (ii) Value Only, where only numerical values are varied; and (iii) Both, where both names and values are varied simultaneously. Larger models such as GPT-4.1 maintain consistent performance across all settings, reflecting strong abstraction over both name and value inputs. However, smaller models exhibit divergent sensitivities. For instance, o3-mini achieves higher FAC when only values are varied compared to the names setting, GPT-4o-mini also shows mild instability, with modest performance shifts between conditions. These findings highlight that entity names and numerical values have different impact in model reasoning. The templated structure of FinChain allows precise manipulation of such variables, offering powerful diagnostic tool for stress-testing reasoning robustness."
        },
        {
            "title": "6 Conclusions and Future Work",
            "content": "We introduce FinChain, symbolic benchmark designed for verifiable chain-of-thought financial reasoning. Constructed from fine-grained taxonomy spanning 54 topics across 12 financial domains and three difficulty levels, it enables targeted evaluFigure 4: Model performance across difficulty levels. FAC is reported for Basic, Intermediate, and Advanced. ( 3). Each difficulty level reflects an increase in reasoning and computational depth. Figure 4 reports FAC across these tiers for six representative models (same subset as Figure 3). All models show performance degradation as complexity increases, underscoring the difficulty of executing longer financial reasoning chains. Among them, LLaMA 3.3 (70B) Instruct stands out for its strong absolute performance and gradual decline, indicating its better compositional generalization. In contrast, smaller models such as Fin-R1 (7B) suffer more severe drops, suggesting difficulty in maintaining reasoning coherence as reasoning depth grows. These findings validate the utility of difficulty levels of FinChain for fine-grained stress testing. It also expose models limitations in scaling their reasoning capabilities, particularly for longer step financial reasoning tasks."
        },
        {
            "title": "5.3.4\nTo evaluate model robustness under surface-level\nperturbations, we test how changes of entity names\nand numerical values in the templates affect the",
            "content": "8 ation across both reasoning complexity and domainspecific challenges. To assess model performance, we propose ChainEval, joint metric that evaluates both final answer correctness and alignment with intermediate reasoning steps. Experimental results reveal that while large general-purpose LLMs outperform specialized financial models, all models struggle with complex, multi-step symbolic reasoning. Looking ahead, we plan to expand FinChain to support multilingual and region-specific scenarios, and to investigate how step-by-step reasoning traces can enhance the trustworthiness and factuality of model-generated answers, especially in long-form financial question answering over realworld documents. This line of work may help bridge symbolic financial reasoning and factual verification (Xie et al., 2025), supporting more robust and interpretable financial AI systems."
        },
        {
            "title": "Limitations",
            "content": "We acknowledge several limitations in this work that we plan to address in future research. First, our dataset is entirely synthetic, generated from symbolic templates. While this design enables fine-grained control, contamination-free generation, and automatic verifiability, it may lack the linguistic diversity and contextual richness of real-world financial texts. In future work, we plan to incorporate real-world financial documents, such as earnings reports, investor communications, or financial news, as seed sources for semi-structured or templated generation, allowing us to better simulate naturally occurring language while preserving symbolic grounding. Second, the benchmark focuses narrowly on symbolic numerical reasoning and does not capture qualitative, contextual, or strategic aspects of financial decision-making, such as risk assessment, or market sentiment. To address this, we aim to complement our benchmark with additional tasks that evaluate models on these higher-level reasoning dimensions, potentially through multi-modal inputs (e.g., combining text with charts or scenarios) or interactive decision-making simulations. Third, FinChain is restricted to English and U.S. centric financial conventions, such as currency formats, and investment norms, which limits its generalizability to multilingual and regional financial contexts. In future extensions, we plan to expand coverage to other languages and financial systems, through collaboration with domain experts and native speakers to localize templates and ensure culturally grounded reasoning tasks."
        },
        {
            "title": "Ethical Statement and Broad Impact",
            "content": "This work uses only synthetic data generated through templated code and language model outputs. No private, sensitive, or copyrighted content was used. Our benchmark is designed for transparency and reproducibility in financial AI. However, caution should be taken when deploying LLMs in real-world financial decision-making, especially where symbolic correctness and regulatory compliance are critical. We believe FinChain will support research toward more interpretable, verifiable, and safe reasoning systems in high-stakes domains. Data License The FinChain dataset and accompanying code will be released under the MIT License. References Cengiz Asmazoğlu Abdullah Bezir, Furkan Burhan Türkay. 2025. Wiroai/wiroai-financeqwen-7b. Hugging Face Hub. Dogu Araci. 2019. Finbert: Financial sentiment analysis with pre-trained language models. ArXiv preprint, abs/1908.10063. Zvi Bodie, Robert C. Merton, and Richard T. Thakor. 2025. Principles of Finance. Cambridge University Press. CeADAR. 2023. Financeconnect-13b (revision 5f7841d). Jian Chen, Peilin Zhou, Yining Hua, Loh Xin, Kehui Chen, Ziyuan Li, Bing Zhu, and Junwei Liang. 2024a. FinTextQA: dataset for long-form financial question answering. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 60256047, Bangkok, Thailand. Association for Computational Linguistics. Zhiyu Chen, Wenhu Chen, Charese Smiley, Sameena Shah, Iana Borova, Dylan Langdon, Reema Moussa, Matt Beane, Ting-Hao Huang, Bryan Routledge, and William Yang Wang. 2021. FinQA: dataset of numerical reasoning over financial data. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 36973711, Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Zhiyu Chen, Shiyang Li, Charese Smiley, Zhiqiang Ma, Sameena Shah, and William Yang Wang. 2022. ConvFinQA: Exploring the chain of numerical reasoning in conversational finance question answering. 9 In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 62796292, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Zhiyu Zoey Chen, Jing Ma, Xinlu Zhang, Nan Hao, An Yan, Armineh Nourbakhsh, Xianjun Yang, Julian McAuley, Linda Petzold, and William Yang Wang. 2024b. survey on large language models for critical societal domains: Finance, healthcare, and law. ArXiv preprint, abs/2405.01769. Daixuan Cheng, Shaohan Huang, and Furu Wei. 2024. Adapting large language models via reading comprehension. In The Twelfth International Conference on Learning Representations. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. 2021. Training verifiers to solve math word problems. ArXiv preprint, abs/2110.14168. Duanyu Feng, Yongfu Dai, Jimin Huang, Yifang Zhang, Qianqian Xie, Weiguang Han, Zhengyu Chen, Alejandro Lopez-Lira, and Hao Wang. 2023. Empowering many, biasing few: Generalist credit scoring through large language models. ArXiv preprint, abs/2310.00566. Olga Golovneva, Moya Chen, Spencer Poff, Martin Corredor, Luke Zettlemoyer, Maryam Fazel-Zarandi, and Asli Celikyilmaz. 2023. ROSCOE: suite of metrics for scoring step-by-step reasoning. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. 2024. The llama 3 herd of models. ArXiv preprint, abs/2407.21783. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z.F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, Aixin Liu, Bing Xue, Bingxuan Wang, Bochao Wu, Bei Feng, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, Damai Dai, Deli Chen, Dongjie Ji, Erhang Li, Fangyun Lin, Fucong Dai, Fuli Luo, Guangbo Hao, Guanting Chen, Guowei Li, H. Zhang, Han Bao, Hanwei Xu, Haocheng Wang, Honghui Ding, Huajian Xin, et al. 2025. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. ArXiv preprint, abs/2501.12948. Oudom Hean, Utsha Saha, and Binita Saha. 2025. Can ai help with your personal finances? Applied Economics, pages 19. Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, et al. 2024. Gpt-4o system card. ArXiv preprint, abs/2410.21276. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de Las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023. Mistral 7b. ArXiv preprint, abs/2310.06825. Albert Q. Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, et al. 2024. Mixtral of experts. ArXiv preprint, abs/2401.04088. Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, et al. 2025. Gemma 3 technical report. ArXiv preprint, abs/2503.19786. Simerjot Kaur, Charese Smiley, Akshat Gupta, Joy Sain, Dongsheng Wang, Suchetha Siddagangappa, Toyin Aguda, and Sameena Shah. 2023. Refind: Relation extraction financial dataset. In Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 2023, Taipei, Taiwan, July 23-27, 2023, pages 30543063. ACM. Rik Koncel-Kedziorski, Michael Krumdick, Viet Lai, Varshini Reddy, Charles Lovering, and Chris Tanner. 2023. Bizbench: quantitative reasoning benchmark for business and finance. ArXiv preprint, abs/2311.06602. Chin-Yew Lin. 2004. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pages 7481, Barcelona, Spain. Association for Computational Linguistics. Xiao-Yang Liu, Guoxuan Wang, Hongyang Yang, and Daochen Zha. 2023. Fingpt: Democratizing internetscale data for financial large language models. ArXiv preprint, abs/2307.10485. Zhaowei Liu, Xin Guo, Fangqi Lou, Lingfeng Zeng, Jinyi Niu, Zixuan Wang, Jiajie Xu, Weige Cai, Ziwei Yang, Xueqian Zhao, et al. 2025. Fin-r1: large language model for financial reasoning through reinforcement learning. ArXiv preprint, abs/2503.16252. Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, and Dongmei Zhang. 2023. Wizardmath: Empowering mathematical reasoning for large language models via reinforced evol-instruct. ArXiv preprint, abs/2308.09583. Qing Lyu, Shreya Havaldar, Adam Stein, Li Zhang, Delip Rao, Eric Wong, Marianna Apidianaki, and Chris 10 Callison-Burch. 2023. Faithful chain-of-thought reasoning. In Proceedings of the 13th International Joint Conference on Natural Language Processing and the 3rd Conference of the Asia-Pacific Chapter of the Association for Computational Linguistics (Volume 1: Long Papers), pages 305329, Nusa Dua, Bali. Association for Computational Linguistics. Iman Mirzadeh, Keivan Alizadeh, Hooman Shahrokhi, Oncel Tuzel, Samy Bengio, and Mehrdad Farajtabar. 2024. Gsm-symbolic: Understanding the limitations of mathematical reasoning in large language models. ArXiv preprint, abs/2410.05229. AI Mistral. 2024. Mathstral. 7B parameter model for mathematical reasoning, released under Apache 2.0 license. Rajdeep Mukherjee, Abhinav Bohra, Akash Banerjee, Soumya Sharma, Manjunath Hegde, Afreen Shaikh, Shivani Shrivastava, Koustuv Dasgupta, Niloy Ganguly, Saptarshi Ghosh, and Pawan Goyal. 2022. ECTSum: new benchmark dataset for bullet point summarization of long earnings call transcripts. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1089310906, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Yuqi Nie, Yaxuan Kong, Xiaowen Dong, John Mulvey, Vincent Poor, Qingsong Wen, and Stefan Zohren. 2024. survey of large language models for financial applications: Progress, prospects and challenges. ArXiv preprint, abs/2406.11903. Yulong Pei, Amarachi Mbakwe, Akshat Gupta, Salwa Alamir, Hanxuan Lin, Xiaomo Liu, and Sameena Shah. 2022. TweetFinSent: dataset of stock senIn Proceedings of the Fourth timents on Twitter. Workshop on Financial Technology and Natural Language Processing (FinNLP), pages 3747, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics. Lingfei Qian, Weipeng Zhou, Yan Wang, Xueqing Peng, Jimin Huang, and Qianqian Xie. 2025. Fino1: On the transferability of reasoning enhanced llms to finance. ArXiv preprint, abs/2502.08127. Team Qwen. 2025. Qwen3. Morgane Riviere, Shreya Pathak, Pier Giuseppe Sessa, Cassidy Hardin, Surya Bhupatiraju, Léonard Hussenot, Thomas Mesnard, Bobak Shahriari, Alexandre Ramé, Johan Ferret, Peter Liu, Pouya Tafti, Abe Friesen, Michelle Casbon, Sabela Ramos, Ravin Kumar, Charline Le Lan, Sammy Jerome, Anton Tsitsulin, Nino Vieillard, Piotr Stanczyk, Sertan Girgin, Nikola Momchev, et al. 2024. Gemma 2: Improving open language models at practical size. ArXiv preprint, abs/2408.00118. Agam Shah, Ruchit Vithani, Abhinav Gullapalli, and Sudheer Chava. 2023. Finer: Financial named entity recognition dataset and weak-supervision model. ArXiv preprint, abs/2302.11157. Raj Shah, Kunal Chawla, Dheeraj Eidnani, Agam Shah, Wendi Du, Sudheer Chava, Natraj Raman, Charese Smiley, Jiaao Chen, and Diyi Yang. 2022. When FLUE meets FLANG: Benchmarks and large pretrained language model for financial domain. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 2322 2335, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Eugene Sy, Tzu-Cheng Peng, Shih-Hsuan Huang, HengYu Lin, and Yung-Chun Chang. 2023. Fine-grained argument understanding with BERT ensemble techniques: deep dive into financial sentiment analysis. In Proceedings of the 35th Conference on Computational Linguistics and Speech Processing (ROCLING 2023), pages 242249, Taipei City, Taiwan. The Association for Computational Linguistics and Chinese Language Processing (ACLCLP). Yixuan Tang and Yi Yang. 2025. Finmteb: Finance massive text embedding benchmark. ArXiv preprint, abs/2502.10990. Shĳie Wu, Ozan Irsoy, Steven Lu, Vadim Dabravolski, Mark Dredze, Sebastian Gehrmann, Prabhanjan Kambadur, David Rosenberg, and Gideon Mann. 2023. Bloomberggpt: large language model for finance. ArXiv preprint, abs/2303.17564. Qianqian Xie, Weiguang Han, Zhengyu Chen, Ruoyu Xiang, Xiao Zhang, Yueru He, Mengxi Xiao, Dong Li, Yongfu Dai, Duanyu Feng, et al. 2024. The finben: An holistic financial benchmark for large language models. ArXiv preprint, abs/2402.12659. Qianqian Xie, Weiguang Han, Xiao Zhang, Yanzhao Lai, Min Peng, Alejandro Lopez-Lira, and Jimin Huang. 2023a. Pixiu: large language model, instruction data and evaluation benchmark for finance. ArXiv preprint, abs/2306.05443. Zhuohan Xie, Trevor Cohn, and Jey Han Lau. 2023b. The next chapter: study of large language models in storytelling. In Proceedings of the 16th International Natural Language Generation Conference, pages 323 351, Prague, Czechia. Association for Computational Linguistics. Zhuohan Xie, Miao Li, Trevor Cohn, and Jey Lau. 2023c. DeltaScore: Fine-grained story evaluation with perturbations. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 53175331, Singapore. Association for Computational Linguistics. Zhuohan Xie, Rui Xing, Yuxia Wang, Jiahui Geng, Hasan Iqbal, Dhruv Sahnan, Iryna Gurevych, and Preslav Nakov. 2025. FIRE: Fact-checking with iterative retrieval and verification. In Findings of the Association for Computational Linguistics: NAACL 2025, pages 29012914, Albuquerque, New Mexico. Association for Computational Linguistics. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, 11 Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2024a. Qwen2.5 technical report. ArXiv preprint, abs/2412.15115. An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, Keming Lu, Mingfeng Xue, Runji Lin, Tianyu Liu, Xingzhang Ren, and Zhenru Zhang. 2024b. Qwen2.5-math technical report: Toward mathematical expert model via selfimprovement. ArXiv preprint, abs/2409.12122. Longhui Yu, Weisen Jiang, Han Shi, Jincheng Yu, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. 2023. Metamath: Bootstrap your own mathematical questions for large language models. ArXiv preprint, abs/2309.12284. Tongxin Yuan, Zhiwei He, Lingzhong Dong, Yiming Wang, Ruĳie Zhao, Tian Xia, Lizhen Xu, Binglin Zhou, Fangqi Li, Zhuosheng Zhang, Rui Wang, and Gongshen Liu. 2024. R-judge: Benchmarking safety risk awareness for LLM agents. In Findings of the Association for Computational Linguistics: EMNLP 2024, pages 14671490, Miami, Florida, USA. Association for Computational Linguistics. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie Zhang, Zican Dong, Yifan Du, Chen Yang, Yushuo Chen, Zhipeng Chen, Jinhao Jiang, Ruiyang Ren, Yifan Li, Xinyu Tang, Zikang Liu, Peiyu Liu, Jian-Yun Nie, and Ji-Rong Wen. 2023. survey of large language models. ArXiv preprint, abs/2303.18223. Financial-Symbolic: Template"
        },
        {
            "title": "Creation Prompt",
            "content": "To construct symbolic financial reasoning benchmarks analogous to GSM-Symbolic, we design structured prompt that guides the generation of executable financial templates. These templates support variable-based instantiation, symbolic stepwise supervision, and controlled perturbations for robustness evaluation. Below, we present the prompt used for template construction. System Instruction: You are financial NLP expert developing symbolic reasoning datasets. Your task is to construct symbolic templates for financial reasoning problems. Each template should support (i) controlled generation of diverse question instances, (ii) executable reasoning traces for automatic verification, and (iii) systematic variation in surface and logical complexity. Please follow the steps below: 1. Identify financial reasoning task: For example, compound interest, IRR, ROI, NPV, breakeven analysis, loan amortization, etc. 2. Write natural language question template: Formulate the question using variable placeholders instead of fixed values. For instance, use {principal}, {rate}, {years}, etc. 3. Define variables and constraints: Specify the domain (e.g., numerical range or categorical values) for each variable. Add algebraic constraints to ensure the question is solvable and the answer valid. 4. Write symbolic solution trace: Provide step-by-step solution using the variables. Ensure the reasoning chain is executable in Python for automatic evaluation. 5. Vary difficulty levels: For each task, generate 10 templates with increasing complexity. Longer and more compositional reasoning chains should correspond to harder levels."
        },
        {
            "title": "B Claude Verification Prompt",
            "content": "We use the following prompt to query Claude for template verification: have written several Python methods that automatically generate random questions and corresponding solutions about <topic> by varying key variables. need 12 assistance in verifying the correctness of the equations used, ensuring that the solutions are accurate and solid. Please respond by first indicating if the methods underlying logic and solution are correct or flawed."
        },
        {
            "title": "C Performance Over Topics for all models",
            "content": "We present domain-level performance comparisons across financial domains for four categories of models: general-purpose non-reasoning models (Figure 5), general-purpose reasoning models (Figure 6), financial fine-tuned and reasoning models (Figure 7), and mathematical fine-tuned and reasoning models (Figure 8). 13 Figure 5: Domain-level performance comparison across financial domains for all general-purpose nonreasoning models from Table 1. For clarity, we split the models into three subtypes: (a) Model Size 10B, (b) Model Size > 10B, and (c) Closed-Source Models. Figure 6: Domain-level performance comparison across financial domains for all general-purpose reasoning models from Table 1. For clarity, we split the models into two subtypes: (a) Model Size 30B, and (b) Model Size > 30B (along with all closed-source models). 14 Figure 7: Domain-level performance comparison across financial domains for all financial fine-tuned and reasoning models from Table 1. For clarity, we split the figure into two parts: (a) Finance Non-Reasoning, and (b) Finance Reasoning models. Figure 8: Domain-level performance comparison across financial domains for all mathematical fine-tuned and reasoning models from Table 1. For clarity, we split the figure into two parts: (a) Math Non-Reasoning, and (b) Math Reasoning models."
        }
    ],
    "affiliations": [
        "Cornell University, USA",
        "FMI, Sofia University, Bulgaria",
        "IIT Delhi, India",
        "MBZUAI, UAE",
        "Quantsquare, France"
    ]
}