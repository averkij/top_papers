{
    "paper_title": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework",
    "authors": [
        "Zhaorui Yang",
        "Bo Pan",
        "Han Wang",
        "Yiyao Wang",
        "Xingyu Liu",
        "Minfeng Zhu",
        "Bo Zhang",
        "Wei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visualizations play a crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), a structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82\\% overall win rate over the baseline method."
        },
        {
            "title": "Start",
            "content": "Multimodal DeepResearcher: Generating Text-Chart Interleaved Reports From Scratch with Agentic Framework Zhaorui Yang*, Bo Pan*, Han Wang*, Yiyao Wang, Xingyu Liu Minfeng Zhu(cid:66), Bo Zhang(cid:66), Wei Chen State Key Lab of CAD&CG, Zhejiang University Zhejiang University https://rickyang1114.github.io/multimodal-deepresearcher/ 5 2 0 2 3 ] . [ 1 4 5 4 2 0 . 6 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Visualizations play crucial part in effective communication of concepts and information. Recent advances in reasoning and retrieval augmented generation have enabled Large Language Models (LLMs) to perform deep research and generate comprehensive reports. Despite its progress, existing deep research frameworks primarily focus on generating text-only content, leaving the automated generation of interleaved texts and visualizations underexplored. This novel task poses key challenges in designing informative visualizations and effectively integrating them with text reports. To address these challenges, we propose Formal Description of Visualization (FDV), structured textual representation of charts that enables LLMs to learn from and generate diverse, high-quality visualizations. Building on this representation, we introduce Multimodal DeepResearcher, an agentic framework that decomposes the task into four stages: (1) researching, (2) exemplar report textualization, (3) planning, and (4) multimodal report generation. For the evaluation of generated multimodal reports, we develop MultimodalReportBench, which contains 100 diverse topics served as inputs along with 5 dedicated metrics. Extensive experiments across models and evaluation methods demonstrate the effectiveness of Multimodal DeepResearcher. Notably, utilizing the same Claude 3.7 Sonnet model, Multimodal DeepResearcher achieves an 82% overall win rate over the baseline method."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated broad capabilities in solving diverse tasks such as question answering, coding and math (Bai et al., 2022; Guo et al., 2025; Huang et al., 2024). Augmented with searching and reasoning capabilities (Xie et al., 2023; Nakano et al., 2021; Li et al., *Equal Contribution (cid:66)Corresponding Authors Under review. Code will be released upon acceptance. 1 Figure 1: text-chart interleaved snippet from the report generated by our Multimodal DeepResearcher. 2025a), LLMs can perform deep research and effectively leverage up-to-date external information beyond static parameters (Li et al., 2025a). Recently, this paradigm has garnered significant attention with its remarkable efficacy in generating grounded, comprehensive reports from scratch (Shao et al., 2024; Huot et al., 2025). However, existing deep research frameworks from both academia (Jin et al., 2025; Zheng et al., 2025b; Li et al., 2025b) and industry (OpenAI, 2025c; Google, 2024; xAI, 2025; David Zhang, 2025) predominantly focus on generating text-only content, neglecting the display beyond text modality. The text-heavy nature of these reports impedes effective communication of concepts and information (Ku et al., 2025; Zheng et al., 2025a), which limits their readability and practical utility. In real-world scenarios, visualization serves as crucial part of reports and presentations, offering remarkable capabilities for conveying data insights (Otten et al., 2015), facilitating the identification of implicit patterns (Yang et al., 2024), and enhancing audience engagement (Barrick et al., 2018; Zheng et al., 2025a). Human experts typically craft meticulously designed visualizations with consistent styles to effectively communicate ideas and insights. They then integrate these visualizations within appropriate textual context (He et al., 2024) to create coherent text-chart interleaved reports. However, the end-to-end generation of multimodal reports remains challenging. Although prompting LLMs to generate individual visualization charts is promising solution (Yang et al., 2024; Seo et al., 2025; Han et al., 2023), effectively representing and integrating these visualizations with textual content poses significant challenges. Although in-context learning appears to be promising approach for guiding such generation, the absence of standardized format for text-chart interleaved content impedes effective implementation of in-context learning strategies. To address this challenge, we introduce the Formal Description of Visualization (FDV), structured representation method inspired by the grammar of graphics theory (Wilkinson, 1999). FDV comprehensively captures visualization designs through four perspectives (i.e., overall layout, plotting scale, data, and marks). This representation provides universal and high-fidelity descriptions that enables in-context learning of human expert designs and produce charts of professional quality. Building upon FDV, we introduce Multimodal DeepResearcher, an agentic framework that generates text-chart interleaved reports from scratch. snippet of generated report is illustrated in Figure 1. The framework operates through four stages: (1) researching, which gathers comprehensive information through searching and reasoning; (2) exemplar report textualization, which textualizes multimodal reports from human experts using our proposed Formal Description of Visualization (FDV, Section 3.2) for in-context learning; (3) planning, which establishes content outline and visualization style guide to ensure consistency throughout the report; and (4) multimodal report generation, which produces the final interleaved report through drafting, coding and iterative chart refinement. We evaluate our framework with MultimodalBench (Section 4.1), which comprises 100 topics used as inputs and 5 dedicated evaluation metrics. Our experiments include both proprietary and opensource models with automatic and human evaluation. As baseline, we adapted DataNarrative (Islam et al., 2024), relevant framework that generates placeholders for charts from tabular inputs, to perform our task. Both automatic and human evaluations consistently demonstrate Multimodal DeepResearchers superior performance compared to the baseline. Notably, when using Claude 3.7 Sonnet as the generator, Multimodal DeepResearcher achieves an impressive 82% overall win rate. Our contributions can be summarized as follows: We propose novel task that generates text-chart interleaved multimodal report from scratch and corresponding dataset and evaluation metrics. We propose Formal Description of Visualization, structured textual representation of visualizations that enables the in-context learning and generation of multimodal reports. We introduce Multimodal DeepResearcher, an end-to-end agentic framework that generates high-quality multimodal reports, which largely outperform the baseline method."
        },
        {
            "title": "2 Related Work",
            "content": "Deep Research Recently, the combination of retrieval techniques (Li et al., 2025c; Zhao et al., 2024) and reasoning (Guo et al., 2025) has enabled LLMs to transcend their parametric constraints by leveraging external knowledge. Pioneering works have designed specialized prompts and workflows for complex research tasks, as exemplified by OpenResearcher (Zheng et al., 2024) and Search-o1 (Li et al., 2025a). Subsequent research has explored reinforcement learning for end-to-end reasoning and information retrieval (Jin et al., 2025; Zheng et al., 2025b). However, these approaches primarily focus on generating and evaluating text-only results, whereas our study advances the field by generating text-chart interleaved reports that significantly enhance information comprehension and communication with visualizations. LLM for Data Visualizations Current work has focused on enhancing individual chart quality through various approaches, including multi-stage pipelines (Dibia, 2023), iterative debugging with visual feedback (Yang et al., 2024), chain-of-thought prompted query reformulation (Seo et al., 2025), and models fine-tuned with domain-specific data for chart generation (Han et al., 2023; Tian et al., 2024). Other research has explored how to articulate generation intent, such as multimodal prompting with sketches and direct manipulations (Wen 2 It decomposes the task of multimodal report Figure 2: The framework of the Multimodal DeepResearcher. generation into four stages: Iterative researching about given topic; Exemplar textualization of human experts using proposed Formal Description of Visualization (FDV, Section 3.2); Planning; Report Generation, which generates the final report with crafting, coding and iterative refinement. et al., 2025), multilingual natural language interfaces (Maddigan and Susnjak, 2023), and conversational context management (Hong and Crisan, 2023). Corresponding evaluation methodologies have also been proposed (Li et al., 2024a; Chen et al., 2025). Unlike previous work that focuses predominantly on single chart or limited data and chart types, our work is the first to generate and evaluate text-chart interleaved reports, which contains multiple diverse visualizations based on inthe-wild complex and heterogeneous information. LLM for agentic generation LLMs have been widely applied to various generation tasks due to their ability to process complex textual information (Ku et al., 2024; Nijkamp et al., 2023b,a; Jimenez et al., 2024; Yang et al., 2025b). For more challenging tasks, researchers have designed LLM agents that decompose problems into reasoning, planning, and execution stages (Luo et al., 2025). These agents have demonstrated remarkable success across scientific research (Lu et al., 2024; Si et al., 2024; Li et al., 2024b; Bogin et al., 2024), video generation (He et al., 2025), and computer system interaction (Xie et al., 2024; Deng et al., 2023; Zhang et al., 2023). This paradigm extends effectively to the visualization domain as well. TheoremExplainAgent (Ku et al., 2025) uses agents to generate educational videos, and PPTAgent (Zheng et al., 2025a) automatically creates presentations in the form of slides with integrated text and visuals. Most relevant to our work, DataNarrative (Islam et al., 2024) explores generating simple specifications for data-driven visualizations and evaluating these specifications as proxies for visualization assessment. However, this approach remains limited to simple chart types (e.g., bar chart or line chart), which restricts their practical use."
        },
        {
            "title": "3 Method",
            "content": "We formulate the task of multimodal report generation as follows: given topic and set of multimodal exemplar reports containing interleaved textual content and charts, the system is expected to output multimodal report as in based on t. To solve this task, we introduce Multimodal DeepResearcher, an agentic framework which decomposes the task into four steps: (1) researching through iterative web search and reasoning (Section 3.1); (2) exemplar report textualization (Section 3.2), which textualizes multimodal exemplar reports from human experts using our proposed Formal Description of Visualization (FDV, Section 3.2); (3) planning (Section 3.3); and (4) Multimodal report generation (Section 3.4). An overview of Multimodal DeepResearcher is presented in Figure 2."
        },
        {
            "title": "3.1 Researching",
            "content": "To leverage up-to-date information beyond parametric knowledge, Multimodal DeepResearcher conducts iterative research on given topic t, generating comprehensive set of learnings L. These learnings encompass both information acquired through web sources and their corresponding references. The process involves iterative execution of two primary operations: (1) web search and (2) subsequent reasoning based on search results. Initially, the agent prompts the LLM to generate relevant keywords = k1, , knK based on the given topic t. The agent then conducts web 3 Figure 3: The illustration Formal Description of Visualization (FDV) for the exemplar textualization process. (A) Original traffic volume visualizations for UK and US cities; (B) The Formal Description of Visualization (FDV) that systematically captures the visualizations layout, scale, data, and marks using structured format; and (C) The reconstructed visualization based on the formal description. This process textualizes high-quality text-chart interleaved reports by transforming visual elements into structured textual representations that preserve the visualizations essential characteristics. searches using these keywords and retrieves webpages = p1, , pnP . Subsequently, the agent analyzes these webpages, synthesizes the information into learnings L, and formulates research question for the next iteration. Based on this research question and the original topic, the research agent performs the next research cycle. After nR rounds of iteration, the researcher produces final compilation of learnings. Further details of this process are provided in Appendix A.1."
        },
        {
            "title": "3.2 Exemplar Textualization",
            "content": "Human experts typically produce reports with both texts and visualizations to enhance communication and audience engagement (Zheng et al., 2025a; Yang et al., 2024). To generate high-quality multimodal content comparable to expert-created reports, we employ in-context learning with exemplar reports crafted by human experts. This approach necessitates an effective methodology for converting multimodal exemplar reports into textual exemplar reports R. To address this challenge, we propose Formal Description of Visualization (FDV), structured description method for visualization charts inspired by the grammar of graphics (GoG) theory (Wilkinson, 1999), which theoretically provides universal and high-fidelity descriptions for any visualization designs. As shown in Figure 3 (B), FDV characterizes each visualization chart from four perspectives: (1) Overall layout, detailing the constituent subplots and their spatial arrangements; (2) Plotting scale, describing the scaling logic behind each data to visual channel (e.g., position, color) mapping and their annotations; (3) Data, describing both the numeric data and text elements used to generate the visualization. (4) Marks, describing the design specifications of each visual element. The reverse process of textualization can be achieved via coding, which reconstructs the visualization from FDV, as shown in Figure 3 (C). In the practical implementation of textualization, Multimodal DeepResearcher first extracts all visualization charts from the report, then prompts multimodal large language model to textualize and replace each of them. This process is presented in Algorithm 1. The full prompt for the textualization process is provided in appendix B.2."
        },
        {
            "title": "3.3 Planning",
            "content": "After iterative researching about the topic through nR rounds, Multimodal DeepResearcher creates plan before the actual generation process. Specifically, it constructs an outline that establishes the narrative architecture of the report based on the learnings L, topic and textual exemplar report R. The outline comprises hierarchical structure of sections, each containing descriptive title and brief summary. To learn the style of visualizations present in exemplar reports and maintain consistent style in the generation of each visualization chart, Multimodal DeepResearcher also prompts the LLM to generate visualization style guide G. The visualization style guide provides guidelines 4 Algorithm 1 Textualization of multimodal reports 1: Inputs: Multimodal exemplar reports R. 2: Requires: Multimodal large language model Mv, replace function replace. 3: Outputs: Textualized exemplar reports R. 4: Initialize = 5: for in do Init. = 6: for each image in do 7: // Extract FDV from image FDVi = Mv(i) // Replace image with extracted FDV = r.replace(i, FDVi) 8: 9: 10: 11: 12: end for = {r} 13: 14: end for 15: Return: that control the overall style of visualizations in the report (e.g., color palette, font hierarchy). More details of this process can be found in appendix A."
        },
        {
            "title": "3.4 Generating the Final Report",
            "content": "The final stage of Multimodal DeepResearcher is responsible for the actual generation of the multimodal report with interleaved textual content and visualizations. The report is generated with outputs of previous stages, i.e., learnings L, exemplar textual reports R, outline and visualization style guide G. Multimodal DeepResearcher first prompts the LLM to generate textual report with Formal Description of Visualization (FDV) defined in Section 3.2 as placeholder for the underlying visualization chart to be generated. The format of this textual report is expected to be the same as those in textual exemplar reports used for in-context learning. Then, Multimodal DeepResearcher extracts all occurrences of FDVs, and prompts the LLM to implement the design via coding. Since visualizations represented by FDV have extensive flexibility, which may exceed the expressive capabilities of typical declarative visualization libraries (Heer and Bostock, 2010) (e.g., matplotlib), we directed the LLMs to utilize D3 , the most widely used imperative visualization programming library based on JavaScript and HTML, to implement the target visualization designs. To further improve the quality of visualizations generated, Multimodal DeepResearcher includes an actor-critic mechanism to revise the code for generating the visualization charts motivated by recent advancements of agents (Yang et al., 2024). In this scenario, the actor is the LLM Mt responsible for generating the code, and the critic feedback comes from both the console and critic model. Console feedback is collected using chrome developer tools provided as Python package to simulate browser. It first collects console messages that may contain errors during the loading of visualizations. After all elements are loaded, it takes screenshot to obtain the visualization chart. After getting the screenshot of the visualization chart, Multimodal DeepResearcher employs multimodal LLM Mv to serve as critic, which provides visual feedback. The multimodal LLM takes the chart rendered as input, examines the visual quality, and delivers corresponding feedback. It further determines whether the current chart needs improvement. If improvement is needed, the actor refines its code based on the feedback and console message. This iterative refinement continues until the critic is satisfied, or the predefined upper limit of retry times is reached, which we set as 3 to avoid infinite refinement cycles. When the refinement process finishes, the critic selects the final chart from the final two iterations. The algorithm for the refine process is presented in Algorithm 2. The prompts employed during this process are detailed in appendix B.5. comprehensive full report generated by Multimodal DeepResearcher is presented in Appendix E."
        },
        {
            "title": "4 Experiments",
            "content": "In this section, we present the MultimodalReportBench and corresponding evaluation criteria for evaluation, followed by the experimental results."
        },
        {
            "title": "4.1 Data Selection",
            "content": "To systematically evaluate the multimodal report generated by Multimodal DeepResearcher, we constructed dataset comprising 100 real-world topics curated from public websites that feature multimodal reports crafted by human experts, i.e., Pew Research (Pew, 2025), Our World in Data (OWID, 2025) and Open Knowledge Foundation (OKF, 2024). Pew Research informs the public about issues, attitudes and trends shaping the world through research report. Our World in Data presents empirical data and research on global development https://d3js.org https://developer.chrome.com/docs/devtools 5 Algorithm 2 Algorithm for refining charts 1: Inputs: chart represented as code. 2: Requires: Browser tool , LLM Mt, Multimodal LLM Mv. 3: Outputs: Refined chart c. 4: Hypars: Number of max retry times Nmax. 5: Initialize satisfied = False, c0 = c, = {c}. 6: for = 1 to Nmax do 7: 8: msg, = (c) 9: // Get console message and image // Critic Mv evaluates the chart satisfied, feedback = Mv(i) if satisfied == True then 10: 11: 12: 13: 14: 15: break end if // actor Mt refines previous chart ci = Mt(ci1, msg, feedback) = {ci} 16: 17: end for 18: = c0 19: if > 1 then 20: 21: 22: end if 23: Return: // Selects from the last two charts = Mv(C[1], C[2]) challenges through web publications. The Open Knowledge Foundation is dedicated to promoting open data and content across all domains, ensuring information accessibility. These sources contain exemplary multimodal reports, making their topics appropriate for our evaluation task. The topics are then used as inputs for multimodal report generation. To ensure that our dataset applies to the real-world scenario, we meticulously curated topics spanning 10 categories, such as travel, energy and education. Table 4 presents the distribution of topics. We also collected 6 multimodal reports with no overlapping in topics to serve as exemplar reports for in-context learning, as described in Section 3.2."
        },
        {
            "title": "4.2 Baseline Selection",
            "content": "Our task requires generating multimodal report from scratch, which is infeasible with direct prompting or existing deep research frameworks. Most similar to our work, DataNarrative (Islam et al., 2024) generates simple data-driven visualization specifications based on data tables as input, and evaluates the textual specification as proxy of chart. We incorporate our researching module and 6 adapt its framework accordingly to establish our baseline. For an apple-to-apple comparison, we utilize the learnings generated with our researching stage (Section 3.1) and plans (Section 3.3) instead of tables as the input. It then goes through generateverify-refine process, consistent with the original framework. Since the original framework lacks mechanisms for transforming design specifications into actual charts, we extract all design specifications and generate corresponding visualizations using the same pipeline as Multimodal Researcher does in Section 3.4. 4.3 Framework Implementation Multimodal DeepResearcher is an agentic framework with multiple stages. In this section, we describe the implementation details of each stage. In the researching stage (Section 3.1), we perform web search and scrape with Firecrawl API, and conduct reasoning with GPT-4o-mini (OpenAI, 2025a). GPT-4o-mini is also utilized for planning. Claude 3.7 Sonnet (Anthropic, 2025) is utilized as the MLLM for the textualization of exemplar reports (Section 3.2). The generation of the final multimodal report requires both large language model to craft textual report, and multimodal large language model to provide visual feedback for the chart. Our experiments encompasses two configurations: (1) State-of-the-art proprietary models, with Claude 3.7 Sonnet serving as both the LLM and multimodal LLM. (2) Open-source models, specifically Qwen3-235B-A22B (Yang et al., 2025a) and Qwen2.5-VL-72B-Instruct (Bai et al., 2025). To ensure fair comparison, all the settings are consistent in both Multimodal DeepResearcher and the DataNarrative baseline where applicable. All calls were made from OpenRouter with no GPU utilized in our experiments."
        },
        {
            "title": "4.4 Automatic Evaluation",
            "content": "Given the multimodal nature of the outputs in our task, evaluation necessitates assessment of both texts and visualizations. To accomplish this, we convert the visualizations generated into base64 encoding, and prompt Multimodal LLM to conduct head-to-head comparisons of two reports with the format of OpenAI messages. Specifically, we utilized GPT-4.1 (OpenAI, 2025b) as the evaluahttps://www.firecrawl.dev/ https://openrouter.ai/ https://platform.openai.com/docs/ api-reference/images tor in all automatic evaluation experiments. Since report generation constitutes an open-ended, objective task, reference-based metrics typically fail to align with human-perceived standards (Liu et al., 2023). Therefore, we established comprehensive criteria incorporating both texts and visualizations in reports, which primarily consists of five metrics: Informativeness and Depth. Evaluates whether the report delivers comprehensive, substantive and thorough information through both texts and accompany visualizations. Coherence and Organization. Evaluates whether the report is well-organized, and whether the visualizations connect meaningfully to the text. Verifiability. Evaluates whether the information of the reports can be verified with citations. Apart from textual links to references, we also prompt the evaluator to check the annotation present in visualizations that may contain source information. Visualization Quality. Evaluates the quality of visualization charts in the report, including visual clarity and textual labels and annotations. Visualization Consistency. Evaluates whether the visualizations in the report maintain consistent overall style. The style contains the color palettes, typography and information hierarchy in visualizations. During evaluation, we provide the evaluator with the topic, learnings which contain both knowledge acquired through web search and corresponding references and both reports. Specifically, we prompt the evaluator to rate both reports between on 1-5 scale with detailed guides, subsequently comparing scores to determine superiority or equivalence. To mitigate positional bias, we randomize the presentation order of reports. The complete evaluation prompt is provided at appendix B.6. Results. As illustrated in Table 1, Multimodal DeepResearcher consistently outperforms DataNarrativeacross across both proprietary and opensource model configurations. With Claude 3.7 Sonnet, it achieves an overall win rate of 82%. Specifically, Multimodal DeepResearcher outperforms with high win rate in Verifiability (86%), Visualization Quality (80%) and Visualization consistency (78%). similar pattern is observed with open-source models (Qwen3-235B-A22B and Qwen2.5-VL-72B-Instruct), where Multimodal DeepResearcher achieves win rate of 55%. The results demonstrate the efficacy of Multimodal DeepResearcher in generating multimodal reports. Table 1: Automatic evaluation results of the multimodal report: Multimodal DeepResearcher (Ours) vs. DataNarrative. Evaluation Metrics Ours Win Ours Lose Tie Ours vs DataNarrative w. Claude 3.7 Sonnet Informativeness and Depth Coherence and Organization Verifiability Visualization Quality Visualization Consistency Overall 75% 76% 86% 80% 78% 82% 25% 21% 5% 16% 17% 16% w. Qwen3-235B-A22B & Qwen2.5-VL-72B-Instruct Informativeness and Depth Coherence and Organization Verifiability Visualization Quality Visualization Consistency Overall 50% 41% 66% 48% 52% 55% 50% 51% 21% 46% 42% 40% 0% 3% 9% 4% 5% 2% 0% 8% 13% 6% 6% 5% Table 2: Human evaluation of the generated reports: Multimodal DeepResearcher (Ours) vs. DataNarrative. Evaluation Metrics Ours Win Ours Lose Tie Informativeness and Depth Coherence and Organization Verifiability Visualization Quality Visualization Consistency Overall 100% 100% 100% 80% 80% 100% 0% 0% 0% 10% 10% 0% 0% 0% 0% 10% 10% 0%"
        },
        {
            "title": "4.5 Human Evaluation",
            "content": "For human evaluation, we utilized the same set of metrics as in automatic evaluation. We selected random subset of 10 topics for evaluation. Specifically, 3 annotators performed pairwise comparison of reports generated by both Multimodal DeepResearcher and DataNarrative with Claude 3.7 Sonnet. As with automatic evaluation (Section 4.5), we randomized report presentation order to avoid positional bias. Results are presented in Table 2. Surprisingly, Multimodal DeepResearcher achieves an overall win rate of 100%. Specifically, two annotators preferred all 10 reports generated by Multimodal DeepResearcher, while the third annotator preferred 9 out of 10. The results further validate the effectiveness of Multimodal Deepresearcher."
        },
        {
            "title": "4.6 Ablation Studies",
            "content": "To assess the efficacy of individual components of Multimodal DeepResearcher, we conducted ablation experiments on random subset of 20 topics. Specifically, we compared 3 variants against Multi7 Table 3: Results of ablation studies across three different setups. We report the lose, win and tie rates for each setup against the complete framework. Claude 3.7 Sonnet serves as both the LLM and MLLM here. Ablated Components Lose Win Tie - w/o Exemplar Learning - w/o Planning - w/o Refinement of charts 70% 20% 10% 85% 15% 0% 80% 20% 0% modal DeepResearcher: (1) w/o in-context learning from exemplar reports (Section 3.2); (2) w/o planning (Section 3.3); (3) w/o iterative refinement of charts (Section 3.4). To ensure fair comparison, all other settings and hyperparameters remained consistent across variants. As shown in table 3, removing any component results in significant performance degradation. Specifically, eliminating exemplar learning from human reports yields 70% lose rate, direct generation without planning leads to 85% lose rate, and removing chart refinement process loses in 80% cases. These findings demonstrate the contribution of each component to the effectiveness of Multimodal DeepResearcher."
        },
        {
            "title": "5.1 Visualization Analysis",
            "content": "In this section, we analyze the characteristics of visualizations generated with Multimodal DeepResearcher and the baseline. While the average number of charts per report between our framework (9.3) and DataNarrative (9.4) is comparable, the visualizations generated by Multimodal DeepResearcher are notably more diverse. As illustrated in Figure 4, although both methods prioritized conventional chart types such as bar charts and line charts, Multimodal DeepResearcher demonstrates superior capability in generating sophisticated and complex visualizations. For instance, across the 100 selected topics, Multimodal DeepResearcher produces 15 flowcharts and 18 dashboards, while DataNarrative generates merely 2 flowcharts and 1 dashboard. Another example involves the Others category, which encompasses hard-to-categorize visualizations such as infographics and mind maps. Our framework generates 280 such charts, substantially exceeding the 96 produced by DataNarrative. This disparity underscores our approachs flexibility in accommodating to diverse real-world scenarios. We provide collection of examples for each type generated Figure 4: Distribution of visualization charts generated with DataNarrative and Multimodal DeepResearcher (Ours). The first column in the legend (denoted by red and yellow colors) represents conventional chart types. by Multimodal DeepResearcher in appendix C. 5.2 Error Analysis Despite the remarkable efficacy of Multimodal DeepResearcher, the integration of visualizations poses new challenges. In this section, we categorize the identified common errors into the following two categories. Overlapping Overlapping of elements is the most common error. It is generally attributed to two factors: (1) excessive information in FDV that complicates proper arrangement within limited space. (2) suboptimal placement of legends, labels and annotations. Illustrative examples of both scenarios are provided in Appendix D. Hallucination Hallucination persists as fundamental challenge for LLMs (Shao et al., 2024; Islam et al., 2024), which also extends to the generation of visualizations. Figure 17 exemplifies this issue through choropleth map example, where the model erroneously marked regions with inadequate data with hallucinated content using red color."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we investigate the challenge of generating multimodal reports from scratch. We introduce the Formal Description of Visualization, structured representation of charts that enables in-context learning from human-created exemplar reports. Based on this, we propose Multimodal DeepResearcher, an end-to-end framework for the generation of multimodal reports. While extensive experiments with both automatic evaluation and human evaluation confirm the efficacy of our 8 framework, challenges remain in improving visualization quality and reducing hallucination."
        },
        {
            "title": "Limitations",
            "content": "Although Multimodal DeepResearcher has demonstrated remarkable potential in end-to-end generation of multimodal reports from scratch, the framework contains limitations due to the complex nature of the task. First, several types of errors exist in the generated visualizations, as discussed in Section 5.2. Furthermore, in-context learning from exemplar reports imposes demands on context size and understanding capabilities of LLMs. Moreover, the considerable computational expenditure associated with state-of-the-art models, coupled with the extensive processing time required for visualization code generation, necessitated the utilization of relatively constrained dataset for experimental validation. The totality of these experiments incurred approximately 650 USD in API usage fees."
        },
        {
            "title": "Ethical Considerations",
            "content": "For human evaluation, we recruited three participants with extensive academic paper writing experience from local university. Prior to the experiment, participants were informed about the study duration and general procedures, and consent to participate. After human evaluation, all participants were compensated with rates above average wage. We also recognize and uphold the importance of intellectual property. The data used by our Multimodal DeepResearcher system was obtained solely from publicly accessible and legally permissible sources, where academic utilization is approved. All the selected topics are carefully checked to exclude potentially offensive content. We used large language models as auxiliary tools to facilitate the writing of the manuscript, with careful verification to ensure precision."
        },
        {
            "title": "References",
            "content": "Anthropic. 2025. Claude. https://www.anthropic. com/claude/sonnet. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, and 8 others. 2025. Qwen2.5-vl technical report. arXiv preprint arXiv:2502.13923. Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, and 1 others. 2022. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862. Andrea Barrick, Dana Davis, and Dana Winkler. 2018. Image versus text in powerpoint lectures: Who does it benefit? Journal of Baccalaureate Social Work, 23(1):91109. Ben Bogin, Kejuan Yang, Shashank Gupta, Kyle Richardson, Erin Bransom, Peter Clark, Ashish Sabharwal, and Tushar Khot. 2024. Super: Evaluating agents on setting up and executing tasks from research repositories. Preprint, arXiv:2409.07440. Nan Chen, Yuge Zhang, Jiahang Xu, Kan Ren, and Yuqing Yang. 2025. Viseval: benchmark for data visualization in the era of large language models. IEEE Transactions on Visualization and Computer Graphics, 31(1):13011311. David Zhang. 2025. https://github.com/dzhng/deephttps://github.com/dzhng/ research. deep-research. Xiang Deng, Yu Gu, Boyuan Zheng, Shijie Chen, Sam Stevens, Boshi Wang, Huan Sun, and Yu Su. 2023. Mind2web: Towards generalist agent for the web. In Advances in Neural Information Processing Systems, volume 36, pages 2809128114. Curran Associates, Inc. Victor Dibia. 2023. LIDA: tool for automatic generation of grammar-agnostic visualizations and infographics using large language models. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 3: System Demonstrations), pages 113126, Toronto, Canada. Association for Computational Linguistics. Google. 2024. Gemini deep research. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, and 1 others. 2025. Deepseek-r1: Incentivizing reasoning capability in arXiv preprint llms via reinforcement learning. arXiv:2501.12948. Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. 2023. Chartllama: multimodal llm for chart understanding and generation. arXiv preprint arXiv:2311.16483. Liu He, Yizhi Song, Hejun Huang, Pinxin Liu, Yunlong Tang, Daniel Aliaga, and Xin Zhou. 2025. Kubrick: Multimodal agent collaborations for synthetic video generation. Preprint, arXiv:2408.10453. Yi He, Ke Xu, Shixiong Cao, Yang Shi, Qing Chen, and Nan Cao. 2024. Leveraging foundation models for crafting narrative visualization: survey. IEEE transactions on visualization and computer graphics. 9 Jeffrey Heer and Michael Bostock. 2010. Declarative language design for interactive visualization. IEEE Transactions on Visualization and Computer Graphics, 16(6):11491156. Ruochen Li, Teerth Patel, Qingyun Wang, and Xinya Du. 2024b. Mlr-copilot: Autonomous machine learning research based on large language models agents. Preprint, arXiv:2408.14033. Matt-Heun Hong and Anamaria Crisan. 2023. Conversational ai threads for visualizing multidimensional datasets. Preprint, arXiv:2311.05590. Siming Huang, Tianhao Cheng, Jason Klein Liu, Jiaran Hao, Liuyihan Song, Yang Xu, Yang, JH Liu, Chenchen Zhang, Linzheng Chai, and 1 others. 2024. Opencoder: The open cookbook for toptier code large language models. arXiv preprint arXiv:2411.04905. Fantine Huot, Reinald Kim Amplayo, Jennimaria Palomaki, Alice Shoshana Jakobovits, Elizabeth Clark, and Mirella Lapata. 2025. Agents room: Narrative generation through multi-step collaboration. In International Conference on Learning Representations. Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Md Rizwan Parvez, Enamul Hoque, and Shafiq Joty. 2024. DataNarrative: Automated data-driven storytelling with visualizations and texts. In Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1925319286, Miami, Florida, USA. Association for Computational Linguistics. Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. 2024. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations. Bowen Jin, Hansi Zeng, Zhenrui Yue, Jinsung Yoon, Sercan Arik, Dong Wang, Hamed Zamani, and Jiawei Han. 2025. Search-r1: Training llms to reason and leverage search engines with reinforcement learning. arXiv preprint arXiv:2503.09516. Max Ku, Thomas Chong, Jonathan Leung, Krish Shah, Alvin Yu, and Wenhu Chen. 2025. Theoremexplainagent: Towards multimodal explanations for llm theorem understanding. arXiv preprint arXiv:2502.19400. Max Ku, Dongfu Jiang, Cong Wei, Xiang Yue, and Wenhu Chen. 2024. VIEScore: Towards explainable metrics for conditional image synthesis evaluation. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1226812290, Bangkok, Thailand. Association for Computational Linguistics. Xiaoxi Li, Guanting Dong, Jiajie Jin, Yuyao Zhang, Yujia Zhou, Yutao Zhu, Peitian Zhang, and Zhicheng Dou. 2025a. Search-o1: Agentic search-enhanced large reasoning models. CoRR, abs/2501.05366. Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, and Zhicheng Dou. 2025b. Webthinker: Empowering large reasoning models with deep research capability. CoRR, abs/2504.21776. Xiaoxi Li, Jiajie Jin, Yujia Zhou, Yuyao Zhang, Peitian Zhang, Yutao Zhu, and Zhicheng Dou. 2025c. From matching to generation: survey on generative information retrieval. ACM Transactions on Information Systems, 43(3):162. Yang Liu, Dan Iter, Yichong Xu, Shuohang Wang, Ruochen Xu, and Chenguang Zhu. 2023. G-eval: NLG evaluation using gpt-4 with better human alignIn Proceedings of the 2023 Conference on ment. Empirical Methods in Natural Language Processing, pages 25112522, Singapore. Association for Computational Linguistics. Chris Lu, Cong Lu, Robert Tjarko Lange, Jakob Foerster, Jeff Clune, and David Ha. 2024. The AI Scientist: Towards fully automated open-ended scientific discovery. arXiv preprint arXiv:2408.06292. Junyu Luo, Weizhi Zhang, Ye Yuan, Yusheng Zhao, Junwei Yang, Yiyang Gu, Bohan Wu, Binqi Chen, Ziyue Qiao, Qingqing Long, Rongcheng Tu, Xiao Luo, Wei Ju, Zhiping Xiao, Yifan Wang, Meng Xiao, Chenwu Liu, Jingyang Yuan, Shichang Zhang, and 7 others. 2025. Large language model agent: survey on methodology, applications and challenges. Preprint, arXiv:2503.21460. Paula Maddigan and Teo Susnjak. 2023. Chat2vis: Finetuning data visualisations using multilingual natural language text and pre-trained large language models. Preprint, arXiv:2303.14292. Reiichiro Nakano, Jacob Hilton, Suchir Balaji, Jeff Wu, Long Ouyang, Christina Kim, Christopher Hesse, Shantanu Jain, Vineet Kosaraju, William Saunders, and 1 others. 2021. Webgpt: Browser-assisted question-answering with human feedback. arXiv preprint arXiv:2112.09332. Erik Nijkamp, Hiroaki Hayashi, Caiming Xiong, Silvio Savarese, and Yingbo Zhou. 2023a. Codegen2: Lessons for training llms on programming and natural languages. ICLR. Guozheng Li, Xinyu Wang, Gerile Aodeng, Shunyuan Zheng, Yu Zhang, Chuangxin Ou, Song Wang, and Chi Harold Liu. 2024a. Visualization generation with large language models: An evaluation. Preprint, arXiv:2401.11255. Erik Nijkamp, Bo Pang, Hiroaki Hayashi, Lifu Tu, Huan Wang, Yingbo Zhou, Silvio Savarese, and Caiming Xiong. 2023b. Codegen: An open large language model for code with multi-turn program synthesis. ICLR. 10 OKF. 2024. Open knowledge foundation. OpenAI. 2025a. https://openai.com/index/ gpt-4o-mini-advancing-cost-efficient-intelligence. ChatGPT. Tianbao Xie, Fan Zhou, Zhoujun Cheng, Peng Shi, Luoxuan Weng, Yitao Liu, Toh Jing Hua, Junning Zhao, Qian Liu, Che Liu, and 1 others. 2023. Openagents: An open platform for language agents in the wild. arXiv preprint arXiv:2310.10634. OpenAI. 2025b. ChatGPT. https://openai.com/ index/gpt-4-1/. OpenAI. 2025c. Deep research system card. Technical report, OpenAI. Jennifer Otten, Karen Cheng, and Adam Drewnowski. 2015. Infographics and public policy: using data visualization to convey complex information. Health Affairs, 34(11):19011907. OWID. 2025. Our world in data. Pew. 2025. Pew research center. Wonduk Seo, Seungyong Lee, Daye Kang, Zonghao Yuan, and Seunghyun Lee. 2025. Vispath: Automated visualization code synthesis via multi-path reasoning and feedback-driven optimization. arXiv preprint arXiv:2502.11140. Yijia Shao, Yucheng Jiang, Theodore Kanell, Peter Xu, Omar Khattab, and Monica Lam. 2024. Assisting in writing Wikipedia-like articles from scratch with large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 62526278, Mexico City, Mexico. Association for Computational Linguistics. Chenglei Si, Diyi Yang, and Tatsunori Hashimoto. 2024. Can llms generate novel research ideas? large-scale human study with 100+ nlp researchers. Preprint, arXiv:2409.04109. Yuan Tian, Weiwei Cui, Dazhen Deng, Xinjing Yi, Yurun Yang, Haidong Zhang, and Yingcai Wu. 2024. Chartgpt: Leveraging llms to generate charts from abstract natural language. IEEE Transactions on Visualization and Computer Graphics. Zhen Wen, Luoxuan Weng, Yinghao Tang, Runjin Zhang, Yuxin Liu, Bo Pan, Minfeng Zhu, and Wei Chen. 2025. Exploring multimodal prompt for visualization authoring with large language models. arXiv preprint arXiv:2504.13700. Leland Wilkinson. 1999. The grammar of graphics. Springer-Verlag, Berlin, Heidelberg. xAI. 2025. Grok 3. Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, and 1 others. 2024. Osworld: Benchmarking multimodal agents for open-ended tasks in real computer environments. Advances in Neural Information Processing Systems, 37:5204052094. An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, Chujie Zheng, Dayiheng Liu, Fan Zhou, Fei Huang, Feng Hu, Hao Ge, Haoran Wei, Huan Lin, Jialong Tang, and 41 others. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388. John Yang, Carlos E. Jimenez, Alex L. Zhang, Kilian Lieret, Joyce Yang, Xindi Wu, Ori Press, Niklas Muennighoff, Gabriel Synnaeve, Karthik R. Narasimhan, Diyi Yang, Sida I. Wang, and Ofir Press. 2025b. SWE-bench multimodal: Do ai systems genIn The Thireralize to visual software domains? teenth International Conference on Learning Representations. Zhiyu Yang, Zihan Zhou, Shuo Wang, Xin Cong, Xu Han, Yukun Yan, Zhenghao Liu, Zhixing Tan, Pengyuan Liu, Dong Yu, Zhiyuan Liu, Xiaodong Shi, and Maosong Sun. 2024. MatPlotAgent: Method and evaluation for LLM-based agentic scientific data visualization. In Findings of the Association for Computational Linguistics: ACL 2024, pages 1178911804, Bangkok, Thailand. Association for Computational Linguistics. Chi Zhang, Zhao Yang, Jiaxuan Liu, Yucheng Han, Xin Chen, Zebiao Huang, Bin Fu, and Gang Yu. 2023. Appagent: Multimodal agents as smartphone users. Preprint, arXiv:2312.13771. Penghao Zhao, Hailin Zhang, Qinhan Yu, Zhengren Wang, Yunteng Geng, Fangcheng Fu, Ling Yang, Wentao Zhang, Jie Jiang, and Bin Cui. 2024. Retrieval-augmented generation for ai-generated content: survey. arXiv preprint arXiv:2402.19473. Hao Zheng, Xinyan Guan, Hao Kong, Jia Zheng, Weixiang Zhou, Hongyu Lin, Yaojie Lu, Ben He, Xianpei Han, and Le Sun. 2025a. Pptagent: Generating and evaluating presentations beyond text-to-slides. arXiv preprint arXiv:2501.03936. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. 2025b. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. Preprint, arXiv:2504.03160. Yuxiang Zheng, Shichao Sun, Lin Qiu, Dongyu Ru, Cheng Jiayang, Xuefeng Li, Jifan Lin, Binjie Wang, Yun Luo, Renjie Pan, Yang Xu, Qingkai Min, Zizhao Zhang, Yiwen Wang, Wenjie Li, and Pengfei Liu. 2024. OpenResearcher: Unleashing AI for accelIn Proceedings of the erated scientific research. 2024 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 209218, Miami, Florida, USA. Association for Computational Linguistics."
        },
        {
            "title": "A Implementation Details",
            "content": "Algorithm 3 Algorithm for the research process A.1 Researching Details Web search is implemented using Firecrawl API. In all of our experiments, we set the number of iterations nR to 2, the number of keywords generated nK to 3, the number of web pages retrieved for each keyword nP to 3, and the number of learnings generated from the research on each keyword nL to 3. Initially, large language model Mt generates nK semantically distinct keywords k1, ..., knK and next research goal from the research topic given by user and prior learnings. This step is guided by the prompt for SERP query generation in B.1. Prior learnings is incorporated as contextual constraints to avoid redundancy and ensure exploratory diversity. For each keyword ki, the agent Mt conducts web search to obtain nP webpage documents in Markdown format. The agent then filters duplicate contents through URL-based comparison and extract textual contents and semantic metadata from retained documents. The metadata is preserved as the reference. The agent analyzes documents and synthesizes it into nL learnings and nK questions as follow-up research directions for the next iteration. This step is guided by the prompt for learning generation in B.1. After completing these two steps, the agent integrates the obtained next research goal and followup research directions to serve as the new topic for initiating the next round of search cycle. In the next iteration, nK is reduced by half and rounded up, thereby reflecting the gradual concentration of the search breadth as the search depth increases. After nR rounds of iteration, the researcher finally returns list of final learnings and all the references. The workflow of the researching process is presented in Algorithm 3. A.2 Planning Details In the planning phase, we employ the prompts in B.3 to generate structured outline and visualization style guide based on the topic t, learnings and high-quality exemplar reports R. We have set comprehensive and detailed requirements for the generation of the outline, including the number of sections, the clarity of key points, the minimization of conceptual overlap between sections, and https://www.firecrawl.dev/ 1: Inputs: Topic t. 2: Requires: Search engine E, large language model Mt. 3: Outputs: Research Learnings L. 4: Hypars: Number of iteration nR, number of pages nP returned each search. 5: Initialize = , = , goal = ; 6: for = 0 to nR 1 do 7: Init. = // Generate keywords and research goal k1, ,nK , goal = Mt(t, q, L) for in do // Fetch result pages p1,nP = E(k) = {p1, ,nP } end for // Learn from page content to get learnings = Mt(P, goal) // Generate question for next iteration = Mt(P, L) = 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: end for 21: Return: the overall coherence of the report. We have also specified the format for each section. In addition to the outline, we also generate visualization style guide to ensure consistency while accommodating different concepts. We instruct the agent to use color coding and information hierarchy of professional industry reports that resembles the style of exemplar reports. With the help of the exemplar reports appended at the end of the prompt, the agent is able to generate higher-quality outlines and visualization style guides, thereby laying solid foundation for subsequent report generation. A.3 Data Details We have meticulously selected 100 topics from Pew Research (Pew, 2025), Our World in Data (OWID, 2025), and the Open Knowledge Foundation (OKF, 2024) to serve as inputs for the Multimodal DeepResearcher. These topics cover 10 different categories, including technology, population, education, travel, energy and so on. Investigating these topics holds great significance for addressing realworld problems. The distribution of topics is shown in Table 4."
        },
        {
            "title": "B Prompts",
            "content": "12 Table 4: The distribution of 100 topics across different categories."
        },
        {
            "title": "Number",
            "content": "Technology & Media Agriculture & Food Travel Population Healthcare Public Sector Energy Climate & Environment Education Economy & Work 15 13 4 8 15 3 9 14 6 13 13 B.1 Prompt for SERP Query and Learning Generation The first prompt below is used to guide the agent to generate keywords for web searches based on the topic provided by the user. The second prompt aims to guide the agent to extract relevant information from the Search Engine Results Page (SERP) and generate learnings. Prompt for SERP Query Generation System Prompt: You are an expert researcher. Follow these instructions when responding: - You may be asked to research subjects that is after your knowledge cutoff, assume the user is right when presented with news. - The user is highly experienced analyst, no need to simplify it, be as detailed as possible and make sure your response is correct. - Be highly organized. - Suggest solutions that didnt think about. - Be proactive and anticipate my needs. - Treat me as an expert in all subject matter. - Mistakes erode my trust, so be accurate and thorough. - Provide detailed explanations, Im comfortable with lots of detail. - Value good arguments over authorities, the source is irrelevant. - Consider new technologies and contrarian ideas, not just the conventional wisdom. - You may use high levels of speculation or prediction, just flag it for me. User Prompt: Given the following prompt from the user, generate list of SERP queries to research the topic. Return maximum of {queries_num} queries, but feel free to return less if the original prompt is clear. Make sure each query is unique and not similar to each other: <prompt>{query}</prompt> Here are some learnings from previous research: {learning_str}"
        },
        {
            "title": "Prompt for Learning Generation",
            "content": "User Prompt: Given the following contents from SERP search for the query <query>{query}</query>, generate list of learnings from the contents. Return maximum of {learning_num} learnings, but feel free to return less if the contents are clear. Make sure each learning is unique and not similar to each other. The learnings should be concise and to the point, as detailed and information dense as possible. Please seamlessly incorporate references to external sources using Markdown hyperlinks. Make sure to include any entities like people, places, companies, products, things, etc in the learnings, as well as any exact metrics, numbers, or dates. The learnings will be used to research the topic further. Extract all meaningful data available in the contents, including any tables or lists, and explictly contain them in the learnings. In addition, return list of follow-up questions to research the topic further, max of {question_num}. <contents> {contents} </contents> 14 B.2 Prompt for Chart Design Extraction Prompt for extracting formal discription of visualization from image System prompt: You are visualization design expert. You will be given visualization image, and your task is to extract the design document from the image. The design document should include the overall layout, plotting scale, data transform, and marks used in the visualization. Your description should be detailed enough that someone could accurately recreate the visualization based solely on your specifications. User prompt: Extract comprehensive and precise visualization design specification from the given image. Capture all visual elements, data representations, and design choices with exact measurements, positions, and relationships. Ignore branding elements like company logos or trademarks. ## Overall Format The format of the design document must strictly follow the following format: <visualization> { \"Part-A: Overall Layout\": { \"Part-A.1\": \"...\", \"Part-A.2\": \"...\", ... }, \"Part-B: Plotting Scale\": { \"Part-B.1\": \"...\", \"Part-B.2\": \"...\", ... }, \"Part-C: Data\": { \"Part-C.1\": \"...\", \"Part-C.2\": \"...\", ... }, \"Part-D: Marks\": { \"Part-D.1\": \"...\", \"Part-D.2\": \"...\", ... } } <visualization> ## Explanation for Each Part: ### Part-A: Overall Layout * Description of the overall figure dimensions, margins, and background * If there are multiple subplots, also describe the detailed breakdown of main component layout and positioning. * Description of title, subtitle, and caption placements with specific alignments * Analysis of whitespace usage and component spacing hierarchies ### Part-B: Plotting Scale Describe each scale used (such as x-axis scale, y-axis scale, color scale). Be specific in the 15 position, formatting, size and shape. ### Part-C: Data Comprehensive listing of **ALL** exact data represented in the visualization. This includes titles, subtitles, axis labels, legends, and any other text or numerical data. ### Part-D: Marks * Complete specification of all primary visual marks (bars, lines, points) with exact sizes. * Text label specifications (font, size, weight, positioning relative to marks) * Interaction between marks including overlaps, nestings, or connections * Annotations, highlights, or emphasis techniques * Color usage patterns and semantic meanings * Text alignment and spacing patterns B.3 Prompt for Outline Generation The following prompt generates report outline based on the topic and the learnings extracted from deep research."
        },
        {
            "title": "Prompt for Outline Generation",
            "content": "System Prompt: You an expert report-generation assistant specialized in creating professional documents that combine insightful analysis with diverse visualizations. Your purpose is to help users transform raw information into polished, presentation-ready reports. Below are list of professional reports for your reference. ## Example Reports {list_of_example_reports} User Prompt: Using the provided topic and previous learnings, please create structured outline for comprehensive report. The outline should present logical narrative flow that thoroughly explores the subject matter. Please do NOT include introduction or conclusion sections. ## Input **Topic** {topic} **Previous learnings** {learning_str} ## Requirements The outline should feature: * 4-6 distinct sections forming cohesive narrative progression * Clear identification of key insights and report points within each section * Minimal conceptual overlap between sections, with each section addressing unique aspects * clear and logical flow of ideas, ensuring that section are connected rather than isolated ## Deliverable Format 16 For each section, please provide: **Title:** concise, engaging heading that captures the sections essence **Summary:** brief narrative (3-5 sentences) synthesizing the key points and insights ## Visualization Style Guide Before detailing individual sections, please provide foundational style guide for visualizations that ensures consistency while accommodating different concepts, including: * **Base Design Elements:** Color palatte for common concepts across charts. Use color coding and information hierarchy of professional industry reports that resembles the style of example reports This style guide should offer flexible guidelines rather than rigid specifications, allowing each visualization to effectively represent its concept while maintaining overall visual cohesion. B.4 Prompt for Report Generation The following prompt is used to generate report. In the system prompt, the format of the visualization part in the report is elaborated, and the meaning of each part of the format is provided. The user prompt generates report with specified visualization format based on the topic, learnings, and the visualization style guide extracted from high-quality McKinsey reports."
        },
        {
            "title": "Prompt for Report Generation",
            "content": "System Prompt: You an expert report-generation assistant specialized in creating professional text-image interleaved documents that combine insightful analysis with diverse visualizations. When visualization is needed, generate comprehensive and precise visualization design specification. Include all visual elements, data representations, and design choices with exact measurements, positions, and relationships. ## Visualization format The format of the design document must strictly follow the following format: <visualization> {{ \"Part-A: Overall Layout\": {{ \"Part-A.1\": \"...\", \"Part-A.2\": \"...\", ... }}, \"Part-B: Plotting Scale\": {{ \"Part-B.1\": \"...\", \"Part-B.2\": \"...\", ... }}, \"Part-C: Data\": {{ \"Part-C.1\": \"...\", \"Part-C.2\": \"...\", ... }}, \"Part-D: Marks\": {{ \"Part-D.1\": \"...\", 17 \"Part-D.2\": \"...\", ... }} }} <visualization> ## Explanation for Each Part: ### Part-A: Overall Layout * Description of the overall figure dimensions, margins, and background * If there are multiple subplots, also describe the detailed breakdown of main component layout and positioning. * Description of title, subtitle, and caption placements with specific alignments * Analysis of whitespace usage and component spacing hierarchies * Consider creating composite visualizations where appropriate (for example, combining line and bar charts within single subplot to enhance data comparison and maximize visual space). ### Part-B: Plotting Scale Describe each scale used (such as x-axis scale, y-axis scale, color scale). Be specific in the position, formatting, size and shape. ### Part-C: Data * Comprehensive listing of **ALL** necessary data for visualization. **ALL** data should be present or can be derived from provided learnings. Do not create fake data or add placeholders. * Appropriate texts, including titles, subtitles, axis labels, legends and moderate amount of annotations. ### Part-D: Marks * Complete specification of all primary visual marks (bars, lines, points) with exact sizes. * Text label specifications (font, size, weight, positioning relative to marks) * Interaction between marks including overlaps, nestings, or connections * Annotations, highlights, or emphasis techniques * Color usage patterns and semantic meanings * Text alignment and spacing patterns Below are list of professional reports for your reference. Follow the style, including the layout, infomation hierarchy, stress of the visualization designs in these reports. ## Example Reports {list_of_example_reports} User Prompt: Please generate detailed report with interleaved texts and visualization based on the topic, outline and previous learnings. ## Input ### Topic of the report {topic} ### Outline for the report {outline} ### Previous learnings {learning_str} ### Visualization Style Guide {visualization_style_guide} ## Guidelines - When referencing the knowledge provided, include Markdown hyperlink at the appropriate position using the source URL provided - Maintain professional, academic tone throughout - Use second-level (##) headings for the section title, and third-level (###) headings for subsections - only utilize data available in the previous learnings part. Do not create fake data or add placeholders. B.5 Prompt for Chart Generation and Improvement Initially, the chart generation prompt generates the complete visualization code for the charts based on the visualization part of the report. Subsequently, the chart evaluation prompt renders the visualized charts, takes screenshots, and conducts an assessment, providing suggestions for modifications. The chart regeneration prompt then regenerates the charts based on the improvements. The chart selection prompt is employed to compare two sets of visualization code and select the implementation that better meets the design criteria."
        },
        {
            "title": "Prompt for Chart Generation",
            "content": "System prompt: You are HTML, D3.js V7 implementation expert who transforms visualization designs into working code. You write clean, efficient HTML and D3.js code to create data visualizations exactly as specified. You follow D3.js best practices, optimize for performance, and ensure responsive design across devices. User prompt: need professional HTML visualization to convey insight based on provided visualization design specification. Please implement with html and d3.js according to the specifications below. **Visualization Design Specification** {chart_design} ## Implementation Requirements - Ensure the visualization is located at the center and there is no large empty space - The top-level wrapper should have no box-shadow, no margin, and no visible borders - Use icons from font-awesome with <i> tag and corresponding class name when needed - Highlight key numbers with larger font size, font-family: Georgia, and deeper colors IMPORTANT: Deliver your solution as complete, self-contained HTML file enclosed in code block starting with \"html\" and ending with \"\" to ensure can extract it properly."
        },
        {
            "title": "Prompt for Chart Evaluation and Improvement",
            "content": "System prompt: You are HTML, D3.js V7 implementation expert who transforms visualization designs into working code. You write clean, efficient HTML and D3.js code to create data visualizations 19 exactly as specified. You follow D3.js best practices, optimize for performance, and ensure responsive design across devices. Chart evaluation prompt: Here is screenshot of the page rendered by the HTML code, along with any console messages that may contain errors. Please examine the image thoroughly and report any problems you find. Specifically check for these common rendering issues: 1. Placeholder content: Does the image contain placeholder text (e.g., \"Lorem ipsum\", \"Chart title\", \"Sample data\") instead of actual content? 2. Excessive annotations: Are there too many annotations or labels that clutter the visualization? 3. Overlapping elements: Do any text labels, legends, data points or other elements overlap, making content unreadable? 4. Sizing problems: Is the visualization too small to be readable or too large for its container? Does it have appropriate dimensions? 5. Excessive margins: Are there large empty spaces around the visualization? ## Console Message {console_message} For each issue found, provide: 1. clear description of the issue 2. The specific location in the image where it occurs 3. Relevent elements that cause the issue Focus on learning issues. found.\" If no issues are found, end your response with \"No issues Chart regeneration prompt: Based on the above evaluation, please regenerate the complete HTML code with all necessary fixes implemented. Ensure the new code: 1. Addresses all the issues you identified 2. Maintains the overall functionality and design intent 3. Is complete and ready to run without additional modifications Specifically: 1. Remove redundant or overlapping annotations that dont add critical information 2. Reposition remaining annotations to ensure clear visibility and logical placement 3. Adjust chart dimensions or add annotations to increase overall size and eliminate excessive margins 4. Reduce the size of specific elements to prevent overlapping between components 5. Expand container dimensions to fully display truncated content IMPORTANT: Deliver your solution as complete, self-contained HTML file enclosed in code block starting with \"html\" and ending with \"\" to ensure can extract it properly. 20 Prompt for Chart Selection System prompt: You are an expert in data visualization design. Your task is to evaluate the provided images based on the given design specification and select the most appropriate one. User prompt: Here are visualization design specification and two charts that implement the specification, please identify which one best meets the following criteria: * Most closely matches the design specification requirements * Offers optimal readability (e.g., has least isses regarding overlapping, elements are of appropriate size and margin) ## Visualization Design Specification {chart_design} ## Response Format Return your response in the following format: <evaluation> [Your evaluation of the charts] </evaluation> <selection> [first or second] </selection> B.6 Prompt for Multimodal Report Evaluation The following prompt is used to compare the quality of the reports generated by baseline and our Multimodal DeepResearcher through multi-dimensional scoring. Then the scores are compared to determine which one wins or they tie."
        },
        {
            "title": "Prompt for Report Evaluation",
            "content": "System prompt: You are an expert evaluator of AI-generated reports with advanced knowledge of data visualization and information analysis. Your role is to provide fair, impartial assessments of report quality based strictly on objective criteria. ## Evaluation Task You will evaluate two AI-generated reports based on: - The overarching topic - Research learnings from internet searches that are used as source of information for the reports For each criterion below, assign score from 1-5 (1=poor, 5=excellent) with half-point increments allowed (e.g., 3.5). Provide concise, evidence-based justification for each score, highlighting specific examples that demonstrate meaningful distinctions in quality between the reports. Your evaluation should clearly articulate why one report receives higher or lower score than another based on observable differences in content, structure, or analysis. Be cautious with extreme scores (1 and 5). ## Evaluation Criteria ### Informativeness and Depth: Does the report deliver comprehensive, substantive and thorough 21 information? Score 1: Extremely superficial content with minimal information. Contains only basic facts without context or explanation. Score 2: Limited content with some relevant information but significant gaps. Lacks necessary depth on key aspects. Score 3: Adequate information covering main points with some supporting details, but missing opportunities for deeper analysis. Score 4: Comprehensive information with substantive details, examples, and insights across most sections. Score 5: Exceptionally thorough coverage with rich, nuanced details, expert-level insights, and well-contextualized information throughout. Is the report well-organized with visualizations that ### Coherence and Organization: connect meaningfully to the text? Score 1: Disorganized; lacks logical structure and coherence. Visualizations appear random and unconnected to text. Score 2: Basic structure present but with awkward transitions. Visualizations loosely connected to surrounding content. Score 3: Clear overall organization with occasional flow issues. Visualizations generally support the text but integration could be improved. Score 4: Well-structured with smooth transitions between sections. Visualizations meaningfully integrated with text content. Score 5: Impeccable organization with seamless progression of sections. Visualizations perfectly complement and enhance textual narrative. ### Verifiability: Does the infomation of the reports can be verified with citations? Score 1: Rarely supported with evidence; many claims are unsubstantiated Score 2: Inconsistently verified; some claims are supported; evidence is occasionally provided Score 3: Generally verified; claims are usually supported with evidence; however, there might be few instances where verification is lacking Score 4: Well-supported; claims are very well supported with credible evidence, and instances of unsupported claims are rare. Score 5: Very well-supported; almost every claim is substantiated with credible evidence, showing high level of thorough verification. ### Visualization Quality: Do the visualizations in the report have excellent quality? Score 1: Poor visualizations that confuse rather than clarify. Inappropriate chart types, missing labels, or misleading representations. Score 2: Basic visualizations with few annotations or explanations; functional issues (e.g., unclear axes, poor color choices) hinder interpretation. Score 3: Adequate visualizations with labels and annotations that communicate data clearly but lack refinement or miss opportunities for improved insight. Score 4: Well-executed visualizations with great visual appeal, clear labeling and annotations, and thoughtful design choices. Score 5: Expert-level visualizations that reveal insights through masterful design, appropriate annotations, and careful attention to visual communication principles ### Visualization Consistency: Do the visualizations in the report maintain consistent style? Score 1: No visual consistency. Charts use different color palettes, conflicting typography, 22 inconsistent information hierarchy, and varying design treatments (such as different border styles, background treatments, or legend placements). Score 2: Minimal consistency with obvious style variations across visualizations. While some basic elements might align, there are clear discrepancies in color usage, information organization, axis formatting, or label treatments. Score 3: Moderate consistency with partially unified approach. Most visualizations share similar color schemes and basic formatting, but variations exist in how information hierarchy is presented, how emphasis is applied, or how supporting elements are styled. Score 4: Strong consistency with cohesive design elements. Visualizations share clear color system, consistent information hierarchy, and unified styling approach, with only minor variations that dont distract from the reports overall visual flow. Score 5: Perfect consistency across all visualizations with meticulously applied design system. Unified color palette used purposefully to highlight key information, consistent information hierarchy that guides the viewers attention appropriately, identical typography treatment, and harmonious spacing, scale, and proportion across all charts and graphics. ## Response Format: Please give your response in the following XML format: <evaluation> <report_a> <informativeness> <score>X</score> <justification> Provide brief justification here </justification> </informativeness> <coherence> <score>X</score> <justification> Provide brief justification here </justification> </coherence> <verifiability> <score>X</score> <justification> Provide brief justification here </justification> </verifiability> <visualization_quality> <score>X</score> <justification> Provide brief justification here </justification> </visualization_quality> <visualization_consistency> <score>X</score> <justification> Provide brief justification here </justification> 23 </visualization_consistency> <report_a> <report_b> <! The same as above > <report_b> <evaluation> User prompt: ## Topic: {topic} ## learnings: {learnings_str} <reportA> ... (base64 image into openai messages) ... </reportA> <reportB> ... (base64 image into openai messages) ... </reportB>"
        },
        {
            "title": "C Visualization examples",
            "content": "C.1 Regular types of charts Figure 5: Example Bar Chart generated by Multimodal DeepResearcher 24 Figure 6: Example Line Chart generated by Multimodal DeepResearcher 25 Figure 7: Example Pie Chart generated by Multimodal DeepResearcher 26 Figure 8: Example scatter chart generated by Multimodal DeepResearcher 27 Figure 9: Example bubble chart generated by Multimodal DeepResearcher 28 C.2 Sankey diagram Figure 10: Example sankey diagram generated by Multimodal DeepResearcher 29 C.3 Choropleth map Figure 11: Example of Choropleth map generated by Multimodal DeepResearcher 30 C.4 Flowchart Figure 12: Example of flowchart generated by Multimodal DeepResearcher 31 C.5 Dashboard Figure 13: Example of dashboard map generated by Multimodal DeepResearcher 32 C. Infographic Figure 14: Example of infographic map generated by Multimodal DeepResearcher"
        },
        {
            "title": "D Error examples",
            "content": "Figure 15: Overlapping caused by excessive information Figure 16: Overlapping caused by improper legend placement 34 Figure 17: Hallucination in visualization generation"
        },
        {
            "title": "E Report example",
            "content": "Below is comprehensive report generated by the Multimodal DeepResearcher from scratch. The input topic is: Investments in waste management are key to ending plastic pollution. For the sake of brevity, we have omitted the reference section of the report."
        }
    ],
    "affiliations": [
        "State Key Lab of CAD&CG, Zhejiang University",
        "Zhejiang University"
    ]
}