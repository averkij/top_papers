{
    "paper_title": "PlanGEN: A Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving",
    "authors": [
        "Mihir Parmar",
        "Xin Liu",
        "Palash Goyal",
        "Yanfei Chen",
        "Long Le",
        "Swaroop Mishra",
        "Hossein Mobahi",
        "Jindong Gu",
        "Zifeng Wang",
        "Hootan Nakhost",
        "Chitta Baral",
        "Chen-Yu Lee",
        "Tomas Pfister",
        "Hamid Palangi"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent agent frameworks and inference-time algorithms often struggle with complex planning problems due to limitations in verifying generated plans or reasoning and varying complexity of instances within a single task. Many existing methods for these tasks either perform task-level verification without considering constraints or apply inference-time algorithms without adapting to instance-level complexity. To address these limitations, we propose PlanGEN, a model-agnostic and easily scalable agent framework with three key components: constraint, verification, and selection agents. Specifically, our approach proposes constraint-guided iterative verification to enhance performance of inference-time algorithms--Best of N, Tree-of-Thought, and REBASE. In PlanGEN framework, the selection agent optimizes algorithm choice based on instance complexity, ensuring better adaptability to complex planning problems. Experimental results demonstrate significant improvements over the strongest baseline across multiple benchmarks, achieving state-of-the-art results on NATURAL PLAN ($\\sim$8%$\\uparrow$), OlympiadBench ($\\sim$4%$\\uparrow$), DocFinQA ($\\sim$7%$\\uparrow$), and GPQA ($\\sim$1%$\\uparrow$). Our key finding highlights that constraint-guided iterative verification improves inference-time algorithms, and adaptive selection further boosts performance on complex planning and reasoning problems."
        },
        {
            "title": "Start",
            "content": "PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Mihir Parmar1 2, Xin Liu1, Palash Goyal1, Yanfei Chen1, Long Le1, Swaroop Mishra1, Hossein Mobahi1, Jindong Gu1, Zifeng Wang1, Hootan Nakhost1, Chitta Baral2, Chen-Yu Lee1, Tomas Pfister1 and Hamid Palangi1 1Google, 2Arizona State University 5 2 0 2 2 2 ] . [ 1 1 1 1 6 1 . 2 0 5 2 : r Recent agent frameworks and inference-time algorithms often struggle with complex planning problems due to limitations in verifying generated plans or reasoning and varying complexity of instances within single task. Many existing methods for these tasks either perform task-level verification without considering constraints or apply inference-time algorithms without adapting to instance-level complexity. To address these limitations, we propose PlanGEN, model-agnostic and easily scalable agent framework with three key components: constraint, verification, and selection agents. Specifically, our approach proposes constraint-guided iterative verification to enhance performance of inferencetime algorithmsBest of , Tree-of-Thought, and REBASE. In PlanGEN framework, the selection agent optimizes algorithm choice based on instance complexity, ensuring better adaptability to complex planning problems. Experimental results demonstrate significant improvements over the strongest baseline across multiple benchmarks, achieving state-of-the-art results on NATURAL PLAN (8%), OlympiadBench (4%), DocFinQA (7%), and GPQA (1%). Our key finding highlights that constraintguided iterative verification improves inference-time algorithms, and adaptive selection further boosts performance on complex planning and reasoning problems. 1. Introduction Effective planning is crucial component for systems designed to solve complex real-world problems (Hao et al., 2023; Jiao et al., 2024; Wang et al., 2025, 2024c; Zhao et al., 2023). Traditional planning approaches, which rely heavily on template-based methods (Guan et al., 2023; Valmeekam et al., 2024; Wang et al., 2024b), often lack generalizability and fail to capture the nuances of real-world tasks. In contrast, natural planning with LLMs offers more promising direction, aligning better with real-world planning scenarios such as trip or meeting planning (Zheng et al., 2024). Furthermore, Wang et al. (2025) shows that planning in natural language helps solve practical problems such as code generation. Thus, we aim to enhance LLMs ability to generate effective natural plans and demonstrate their usefulness in solving downstream reasoning tasks within the scientific and financial domains. For the scope of this study, planning refers to the ability to decompose tasks and reason strategically to achieve solutions. In recent years, LLM agents have shown impressive abilities to solve complex reasoning problems (Wang et al., 2024a; Xiao et al., 2024; Yao et al., 2023). Orthogonal to this exploration, scaling search space during inference-time (i.e., test-time scaling) (Snell et al., 2025; Welleck et al., 2024) has gained popularity in tackling difficult problems such as mathematical reasoning (Zhang et al., 2024) and code generation (Wang et al., 2025). Despite the success of these frameworks, we hypothesize that they often struggle with complex planning problems due to the lack of better verification module, and failure to account for instance-level complexity across single-task. Furthermore, although some initial explorations exist (Bohnet et al., 2024; Lee et al., 2025), effectiveness of these frameworks for 1Please correspond with Mihir Parmar <mihirparmar@asu.edu>, and Hamid Palangi <hamidpalangi@google.com> Joint last authors PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Figure 1 Schematic representation of PlanGEN (Mixture of Algorithms). An initial plan and constraints guide iterative plan refinement. The verification agent provides reward scores for plan quality, and the selection agent chooses inference algorithms until the highest-reward plan is found and used for downstream reasoning (if needed). UCB: Upper Confidence Bound, BoN: Best of , ToT: Tree-of-Thought, RS: REBASE. natural planning is under-explored (extended related work is presented in App. A). Motivated by these, we proposed PlanGEN, model-agnostic, easily scalable, multi-agent framework for effective natural plan generation. PlanGEN consists of three specialized agents: constraint agent, verification agent, and selection agent. The constraint agent extracts instance-specific constraints (e.g., budget, concepts, rules, etc.); the verification agent evaluates plan quality and provides reward score considering the constraints; and the selection agent dynamically chooses the best inference algorithm using an improved Upper Confidence Bound (UCB) formula (Han et al., 2024) for instance of different complexity. We explore popular and widely used three inference algorithms within PlanGEN: Best of (Brown et al., 2024), Tree-of-Thought (ToT) (Yao et al., 2024), and REward-BAlanced SEarch (REBASE) (Wu et al., 2024a). We combine our agents with these algorithms, yielding four frameworks: (1) PlanGEN (Best of ), (2) PlanGEN (ToT), (3) PlanGEN (REBASE), and (4) PlanGEN (Mixture of Algorithms). In PlanGEN, we use Multi-Agent approach which signifies using the constraint and verification agents for the first three approaches, and all three agents for the Mixture of Algorithms (Figure 1). Figure 1 shows example from NATURAL PLAN (Calendar scheduling), and App. provides more examples. To evaluate PlanGEN, we perform all experiments using Gemini-1.5-Pro (Team et al., 2024) as underlying model. We further present case-study on Gemini-2.0-Flash, and GPT-4o (Hurst et al., 2024) to show the model-agnostic nature. We evaluate natural language planning ability on NATURAL PLAN (Zheng et al., 2024), scientific/mathematical reasoning on GPQA (Rein et al., 2024) and OlympiadBench (He et al., 2024), and financial reasoning on DocFinQA (Reddy et al., 2024). Performance is compared against Zero-shot Chain-of-Thought (CoT) and vanilla multi-agent baselines. We achieve PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving state-of-the-art results on NATURAL PLAN (8% average across all categories), OlympiadBench (text-only) (5% on MATH, 4% on PHYSICS), and DocFinQA (7%). On GPQA, we outperform Gemini-1.5-Pro (13%), GPT-4o (12%), and Claude-3.5-Opus (9%), while achieving competitive performance compared to the vanilla multi-agent baseline (1%). Further analysis reveals that the simplest method (i.e., PlanGEN (Best of )) achieves the best performance on NATURAL PLAN (Figure 5). PlanGEN (Mixture of Algorithms) achieves the best performance for complex problems (Figure 6) including GPQA, and OlympiadBench(MATH). We further conduct thorough analysis of the results which reveals several important findings. In summary, our contributions are: (1) PlanGEN, novel, model-agnostic, and scalable multi-agent framework for enhancing LLM natural planning; (2) SOTA results on several complex planning and reasoning benchmarks; and (3) novel approach to constraint-based verification and instance-level complexity-based inference algorithm selection. 2. PlanGEN 2.1. Proposed LLM Agents PlanGEN comprises three specialized LLM agents: constraint agent, verification agent, and selection agent. Each agent utilizes an off-the-shelf LLM (e.g., Gemini, GPT) which is equipped with task-specific prompts for efficient performance. 2.1.1. Constraint Agent We define constraints as the criteria necessary for verifying solutions to planning problems. These criteria are inherently instance-specific. For instance, in the calendar scheduling from NATURAL PLAN, relevant constraints include individual schedules, availabilities, and preferences. In scientific reasoning problems from GPQA, constraints might be the concepts used, calculation correctness, and formula selection. We argue that careful extraction of instance-specific constraints is critical for successful verification. The constraint agent serves as preprocessing component in the framework, designed to extract instance-specific constraints from the problem description. By analyzing the input problem, this agent identifies all possible critical constraints that are required for generated plan verification. The extracted constraints provide foundation for verifying plans to improve the overall relevance and quality of the planning process. The prompt used by the constraint agent enables it to systematically identify constraints by asking the underlying LLM to focus on specific aspects of the problem description. This ensures that no critical information is overlooked and that the resulting constraints are comprehensive. Prompts used by the constraint agent and examples of generated constraints are provided in App. and App. F, respectively. 2.1.2. Verification Agent The verification agent plays critical role in the framework by assessing the quality of generated plans based on constraints generated by the constraint agent. This agent ensures that plans are aligned with task objectives, adhere to constraints, and progress logically toward correct and complete solution. The verification agent has two key components: (i) feedback generation, and (ii) numerical reward score generation based on feedback. Verification prompts and examples of verification are provided in App. and App. F, respectively. Feedback Generation While verifying each generated plan against different constraints, the verification agent generates detailed natural language reasoning regarding plan quality. We consider this explanation as feedback, offering interpretability and actionable next step towards improvement. 3 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Numerical Reward Generation Motivated by Zhang et al. (2024), we instruct the agent to evaluate the plan against various constraints and assign reward score on scale of 100 to 100. The scoring mechanism is designed to enforce strict quality standards, with threshold (e.g., score of 95 or higher) indicating verified, high-quality plan. 2.1.3. Selection Agent The selection agent dynamically determines the most suitable inference algorithm for solving given problem instance based on its complexity. It leverages combination of historical performance; diversity, and recovery scores; and guidance from LLM to adaptively select the best algorithm for the current instance. To create the selection agent, we utilize modified Upper Confidence Bound (UCB) policy. The policy combines multiple factors, including normalized rewards, exploration bonuses, diversity adjustments, and recovery scores. Additionally, the agent incorporates LLM-guided priors, which provide algorithm suitability scores based on the problem statement, task requirements, and previous plan (if available). These priors enable the agent to align its selections with the input instance complexity and corresponding constraints, improving the relevance of the chosen algorithm. Modified UCB Policy equation combines several terms to balance exploitation, and exploration when selecting the best algorithm for given task instance. To modify UCB, we first conducted preliminary ablation study, presented in App. B. UCB(ùëé) = ùëÖ(ùëé) ùëÅ (ùëé) + 2 log(ùëá + 1) ùëÅ (ùëé) + ùúÜprior Prior(ùëé) + ùõºdiversity ùëÅ (ùëé) + 1 + ùõºrecovery ùëÜrecovery (ùëé) All terms in equation given above are calculated across one evaluation run. Here, the cost of calculation is negligible since it only utilizes reward values from previous runs, but only one LLM call require to get score for Prior(a). The first term, ùëÖ (ùëé) , represents the average reward for algorithm ùëé, ùëÅ (ùëé) where ùëÖ(ùëé) is the total reward accumulated by the algorithm, and ùëÅ (ùëé) is the number of times the algorithm has been selected. This term ensures that algorithms with higher historical performance 2 log(ùëá+1) ùëÅ (ùëé) are prioritized. The second term, , serves as the exploration component, encouraging the selection of algorithms with fewer trials, denoted as ùëá. This term ensures that under-explored options are adequately evaluated. Furthermore, ùúÜprior Prior(ùëé), which leverages LLM-guided priors to align algorithm selection with the instance-specific complexity. Here, ùúÜprior is dynamically decaying weight defined as ùúÜprior , where ùëá represents the total number of trials. This decay gradually shifts 1+ùëá the focus from initial priors to historical performance as trials progress. The diversity bonus, ùõºdiversity ùëÅ (ùëé)+1 , penalizes overused algorithms, ensuring balanced exploration across all options. Finally, the recovery term, ùõºrecovery ùëÜrecovery(ùëé), rewards algorithms that recover effectively from failures, with ùëÜrecovery(ùëé) representing the recovery score for algorithm ùëé. Selection Process This process begins by initializing algorithm-specific variables, such as accumulated rewards, selection counts, and failure counts. Further details on the algorithm can be found in Algorithm 1 (App. B). The agent then incorporates LLM-guided priors to generate suitability scores for the algorithms based on the problem statement and any provided feedback. These priors are derived from LLM (prompt for this given in App. B), and serve as initial estimates to adjust the UCB (Han et al., 2024) values. 4 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving 2.2. Proposed Frameworks Within PlanGEN, we propose four different frameworks: (1) PlanGEN (Best of ) (Figure 2), (2) PlanGEN (ToT) (Figure 3), and (3) PlanGEN (REBASE) (Figure 4), and (4) PlanGEN (Mixture of Algorithms) (Figure 1). 2.2.1. PlanGEN (Best of ) Motivated by Brown et al. (2024), we adapted the Best of algorithm and modified it using our constraint and verification agents as illustrated in Figure 2. The framework generates candidate plans (Plan 1, Plan 2, ..., Plan n), and each plan is assessed by verification agent based on set of constraints. Then, corresponding reward (Reward 1, Reward 2, ..., Reward n) gets assigned by the verification agent. Finally, the plan with the maximum reward is chosen, guaranteeing an optimal solution that best satisfies the problem constraints. 2.2.2. PlanGEN (ToT) Figure 2 Schematic representation of PlanGEN (Best of ) (BoN). ToT algorithm has been studied in detail for solving many complex problems (Yao et al., 2024). As shown in Figure 3, we modify the ToT algorithm with our constraint and verification agents. The method begins by initializing root node that represents the problem and generating multiple potential next steps, creating tree-like structure. The generated steps are verified using verification agent which assigns reward scores based on set of constraints. The iterative process involves evaluating all possible steps at given depth, selecting the most promising path based on reward scores, and expanding it further by generating new steps. This process continues until valid solution is identified or pre-defined limit on iterations is reached. Further details on various prompts for the ToT are presented in App. C. Figure 3 Schematic representation of PlanGEN (ToT). Highest-reward steps are highlighted in green. 2.2.3. PlanGEN (REBASE) The REBASE tree search method inherits the exploitation and pruning properties of tree search and is well-studied for mathematical reasoning (Wu et al., 2024a). As shown in Figure 4, the framework incorporates dynamic selection and expansion strategy to iteratively refine solutions. At each depth of the tree, candidate nodes are ranked based on their assigned reward scores (obtained using verification agent), ensuring that the most promising candidates are explored first. Even steps with lower rewards are considered but with reducing number of children, meaning that their exploration depth is limited. This hierarchical pruning helps maintain efficiency, thereby reducing unnecessary exploration of weaker nodes. This process continues until either valid, complete solution is found or predefined depth or width limit is reached. Also, there is completion check similar to ToT which identifies nodes that represent complete solutions, enabling REBASE to terminate early once satisfactory outcome is identified. App. provides further details on prompts for the REBASE. 5 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving 2.2.4. PlanGEN (Mixture of Algorithms) The Mixture of Algorithms framework (Figure 1) introduces selection agent (2.1.3) which dynamically selects the best possible inference-time algorithms proposed in the above sections based on instance-level complexity. The framework operates in modular and iterative manner, ensuring adaptability in addressing planning and reasoning problems with different complexity effectively. Figure 4 Schematic representation of PlanGEN (REBASE). Green shading indicates step reward (darker = higher). Darker steps prioritized for exploration. Orchestration The process begins with generating an initial plan using LLM based on the task description and problem statement. Along with this, the constraint agent (2.1.1) is employed to generate an instance-specific set of constraints. Based on the constraints, the verification agent (2.1.2) evaluates the quality of the initial plan and provides reward score (indicated as Score in Figure 1). If the initial plan meets the required threshold (denoted ùëá‚Ñé), it is acceptable as the Final Plan. Otherwise, the iterative refinement process begins. Iterative Refinement The refinement loop is driven by suite of inference algorithms as shown in Figure 1. During this iterative refinement, the selection agent (2.1.3) determines the most suitable algorithm based on the instance-specific complexity and historical UCB values. The selected algorithm produces an updated plan, which is then re-evaluated by the verification agent. To ensure continual improvement, the framework incorporates feedback generated by verification agent that provides guidance, and this feedback loop enables the system to refine the plan incrementally. 3. Experiments and Results 3.1. Experimental Setup Datasets To demonstrate improvement in natural planning abilities, we utilize the NATURAL PLAN (Zheng et al., 2024). After improving the planning, we show that this significantly enhances the reasoning capabilities of LLMs on two benchmarks: GPQA (Rein et al., 2024) and OlympiadBench (text-only) (He et al., 2024). Additionally, we show that PlanGEN improves performance on domain-specific dataset, DocFinQA (Reddy et al., 2024). Further details are presented in App. D. Baselines and Our Frameworks We develop two baselines for comparison with our frameworks: (i) Zero-shot CoT (Kojima et al., 2024) and (ii) Vanilla Multi-Agent Baseline. In the Zero-shot CoT, we provide an input prompt to the model, which generates outputs in the form of <CoT reasoning, Answer>. For the Multi-Agent Baseline, the same model is called iteratively across multiple iterations. The system repeatedly refines its outputs through feedback loops, where the feedback is generated based on self-reflective prompt (App. D) designed to improve reasoning. We evaluate all proposed frameworks (2.2) on all benchmarks. For reasoning tasks, we use two-stage approach: (1) generating an optimized plan using our frameworks, and (2) executing the plan to produce the final answer (Figure 1). App. presents further details on model selection, metrics, and experiment hyper-parameters including the hyper-parameter choices for inference-time algorithms. 6 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving (a) NATURAL PLAN (b) OlympiadBench (c) GPQA (d) DocFinQA Figure 5 Performance comparison of the proposed multi-agent frameworks against baselines across four benchmarks. All experiments are conducted using Gemini-1.5-Pro. Algo: Algorithms, MATH: Mathematics, PHY: Physics. 3.2. Main Results Figure 5 compares performance of multi-agent frameworks across various single-agent and multiagent baselines (varies across benchmarks - some single-agent baselines for GPQA are obtained from https://klu.ai/glossary/gpqa-eval). From the results, it is evident that the multi-agent frameworks are consistently outperforming the baselines. Performance on NATURAL PLAN From Figure 5a, PlanGEN (Best of ) achieves the highest EM scores across all tasks: 60.70 (Calendar), 43.80 (Meeting), and 41.63 (Trip). In calendar scheduling, all four frameworks surpass the strongest baseline (Multi-Agent Baseline) by 10%. For meeting and trip planning, all except ToT outperform the best baseline (Gemini-1.5-Pro) by 6% and 7%, respectively. PlanGEN (Mixture of Algo.) achieves the second-highest performance in meeting and trip planning while remains competitive in calendar scheduling. These results demonstrate the effectiveness of our frameworks in handling diverse natural language planning tasks and establishing SOTA for all three categories of NATURAL PLAN. 7 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Performance on OlympiadBench From Figure 5b, PlanGEN (Mixture of Algo.) achieves the highest accuracy in the MATH category (55.94%), outperforming the strongest Multi-Agent Baseline (50.68%) by 5%. Notably, the superior performance of the PlanGEN (Mixture of Algo.) in MATH highlights its effectiveness in complex mathematical reasoning, setting SOTA for the MATH. In the PHY category, all multi-agent frameworks surpass Gemini-1.5-Flash (strongest baseline), with PlanGEN (Best of ) achieving the highest accuracy (31.78%), setting SOTA for the PHY. Performance on GPQA From Figure 5c, the PlanGEN (Mixture of Algo.) achieves the highest accuracy (59.6%). The individual inference-time algorithms achieve lower performance, indicating the usefulness of selection. All proposed frameworks outperform Gemini-1.5-Pro (46.2%), GPT models ( 48%), and Claude-3-Opus (50.4%) by large margin. While Claude-3.5-Sonnet, and Multi-Agent Baseline perform competitively ( 59%) compared to PlanGEN (Mixture of Algo.). Performance on DocFinQA From Figure 5d, our frameworks significantly improve performance on DocFinQA, with PlanGEN (Best of ) achieving the highest accuracy (31.16%) and F1-Score (29.45%), setting SOTA for the task. All our frameworks outperform the Gemini-1.5-Pro (strongest baseline) by large margin ( 7%). These results highlight the effectiveness of multi-agent frameworks in financial document understanding, and performing reasoning over them. Performance of our frameworks w.r.t. different complexity As shown in Figure 6, we conduct case study on calendar scheduling task from NATURAL PLAN to analyze the impact of varying complexity levels on the performance of different frameworks. For the calendar scheduling, we observe that PlanGEN (ToT) performs best for simple problems, while PlanGEN (Best of ) is more effective for intermediate problems. As complexity increases, PlanGEN (Mixture of Algo.) proves to be the most effective approach. We further conduct similar analysis for meeting and trip planning from NATURAL PLAN presented in App. E. Figure 6 Performance comparison of inferencetime algorithms across different complexity levels for calendar scheduling from NATURAL PLAN. Main Findings Compared to single-agent systems, multi-agent frameworks consistently outperform in generating optimized planning trajectories (Figure 5). Furthermore, Multi-Agent (Baseline) is not always the strongest benchmark, as self-correction can introduce challenges as shown in Huang et al. (2024). Thus, different agents within the system require distinct handling strategies similar to our PlanGEN. Additionally, even in multi-agent frameworks for PlanGEN, relying on single inference-time algorithm proves insufficient for more complex problems (Figure 6). PlanGEN (Mixture of Algo.) approach offers substantial advantages for solving complex planning problems, highlighting the importance of algorithm selection based on instance-specific complexity (Figure 1). Given that our frameworks are multi-agent, we provide further discussion on # of LLM calls vs. their performance in subsequent section. 8 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving 4. Analysis and Discussion Here, we discuss detailed analysis over importance of our agents and model-agnostic nature of our frameworks. Additionally, we also present more analysis on results in App. E. Importance of Verification Agent Figure 7 demonstrates the verification agents crucial role in PlanGEN by showing strong correlation between assigned reward values and prediction correctness (1 for correct, 0 for incorrect). The plotted points represent the average correctness rate for data buckets of varying reward values, each bucket containing hundreds of samples. logistic regression model trained on DocFinQA and GPQA data ( 1100 total samples) reveals sigmoidal trend: higher rewards correlate with increased success probability, highlighting the agents effectiveness. This reinforces the importance of constraint-guided verification for improving inference-time algorithms (see App. for further details). Figure 7 Logistic regression plot showing verification agents positive performance impact. P(Successful Outcome) = probability of prediction being correct. Importance of Selection Agent Figure 8 illustrates the importance of the selection agent by comparing the performance on the NATURAL PLAN. Here, Multi-Agent (Ver.) includes only the verification agent, while Multi-Agent (Ver. + Selection) further includes selection agent. The results highlight the progressive impact of these components. For example, in calendar scheduling, MultiAgent (Ver.) improves performance to 56.1 EM compared to Multi-Agent (Baseline). However, Multi-Agent (Ver. + Selection) achieves 59.3 EM, demonstrating the additional benefit of algorithm selection. similar trend is observed in trip planning where Multi-Agent (Ver. + Selection) outperforms Multi-Agent (Ver.) (41.17 EM vs. 35.44 EM) and the Multi-Agent (Baseline). For meeting planning, Multi-Agent (Ver.) achieves 43.1 EM compared to 36.8 EM of MultiAgent (Baseline), whereas, Multi-Agent (Ver. + Selection) achieves competitive performance. Together, verification and selection agents drive significant improvements over single-agent and multi-agent baselines. Figure 8 Case study on NATURAL PLAN, showing the impact of selection agent. Ver.: Verification. Model-Agnostic Nature The results from Table 1 demonstrate the model-agnostic nature of our proposed multi-agent frameworks. While the primary experiments were conducted using 9 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving GPQA 46.20 56. Methods Gemini-1.5-Pro PlanGEN (BoN) Gemini-1.5-Pro PlanGEN (ToT) Gemini-1.5-Pro PlanGEN (REBASE) Gemini-1.5-Pro PlanGEN (Mixture of Algo.) Gemini-1.5-Pro NATURAL PLAN (Calendar) OlympiadBench MATH PHY Methods 48. 32.63 28.35 Gemini-2.0-Flash 60.70 53.85 31. 56.60 59.10 54.45 29.37 57.10 59. 54.90 31.36 PlanGEN (BoN) Gemini-2.0-Flash PlanGEN (ToT) Gemini-2.0-Flash PlanGEN (REBASE) Gemini-2.0-Flash GPQA 60.10 56.83 NATURAL PLAN (Calendar) OlympiadBench MATH PHY 61.10 51.13 27.54 68.90 59.90 35. 59.18 62.30 60.30 35.70 64.14 61. 60.98 36.02 Methods GPT-4o PlanGEN (BoN) GPT-4o PlanGEN (ToT) GPT-4o PlanGEN (REBASE) GPT-4o GPQA 47.98 40.40 46.70 41. 59.60 59.30 55.94 31.28 PlanGEN (Mixture of Algo.) Gemini-2.0-Flash 63. 66.55 64.10 37.29 PlanGEN (Mixture of Algo.) GPT-4o 49.40 Table 1 Performance comparison for model-agnostic nature of PlanGEN. We utilize Gemini-1.5-Pro, Gemini-2.0-Flash, and GPT-4o as baseline and underlying model in PlanGEN frameworks. Comparing methods that use the same base and underlying model for fair assessment Gemini-1.5-Pro, the frameworks effectiveness holds across different underlying models, such as Gemini-2.0-Flash and GPT-4o. For instance, in the NATURAL PLAN (calendar scheduling), the PlanGEN (Best of ) framework achieves significant improvement, reaching 68.90 EM, outperforming Gemini-2.0-Flash (61.10 EM). Similarly, in OlympiadBench, the PlanGEN (Mixture of Algo.) achieves the highest scores in MATH (64.10) and PHY (37.29), surpassing Gemini-2.0-Flash (52.13 MATH, 27.54 PHY). Note that, the Mixture of Algo. outperforms other three frameworks, showing effectiveness of selection agent. On GPQA, Mixture of Algo. (49.40) and PlanGEN (REBASE) (64.14) outperform GPT-4o (47.98) and Gemini-2.0-Flash (60.10), respectively. These results highlight that regardless of the underlying model, our frameworks consistently enhance performance by leveraging multi-agent collaboration, reinforcing their flexibility and robustness across models. Discussion on LLM calls vs. Performance (%) Figure 9 shows the relationship between the number of LLM calls and task performance across baselines (single-agent and multi-agent) and proposed frameworks, using OlympiadBench (MATH category). The single-agent system, zero-shot CoT, requires only one LLM call. The multi-agent baseline requires the same number of calls as PlanGEN (Best of ), but our framework outperforms the multi-agent baseline. For PlanGEN (ToT) and PlanGEN (REBASE), we focus on LLM calls during the tree expansion phase. PlanGEN (ToT) involves dynamic exploration, where each explored path requires three LLM calls: step generation, reward evaluation, and completion verification. The total cost is the per-path cost multiplied by the number of paths explored, constrained by either the number of steps generated for each problem or predefined iteration budget (i.e., 20). For PlanGEN (REBASE), the number of LLM calls depends on the search width (i.e., 10). Each solution path expansion involves three calls: step generation, quality evaluation, and completion verification, thus, giving maximum 30 LLM calls for single problem. For PlanGEN (Mixture of Algo.), we estimate the average LLM calls by summing the estimated calls for each selected algorithm per problem, then dividing by the total number of problems. As shown in Figure 9, the single-agent system exhibits the lowest performance despite requiring just one LLM call. Multi-agent approaches show improved performance, with PlanGEN (ToT) and PlanGEN (REBASE) balancing LLM call efficiency and accuracy. The PlanGEN (Mixture of Algo.) method achieves the highest performance, suggesting that combining diverse planning strategies enhances efficiency. Figure 9 Comparison of baselines and our frameworks, showing the trade-off between LLM calls and performance (%) for OlympiadBench (MATH). 10 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving 5. Conclusions In this work, we proposed PlanGEN, an easily scalable multi-agent approach incorporating three key components: constraint, verification, and selection agents. We leveraged these agents to improve the verification process of existing inference algorithms and proposed three frameworks: Multi-Agent Best of , ToT, and REBASE. Further, we introduced Mixture of Algorithms, an iterative framework that integrates the selection agent (Figure 1) to dynamically choose the best algorithm. We evaluated our frameworks on NATURAL PLAN, OlympiadBench, GPQA, and DocFinQA. Experimental results demonstrate that PlanGEN outperforms strong baselines, achieving SOTA results across datasets. Furthermore, our findings suggest that the proposed frameworks are scalable and generalizable to different LLMs, improving their natural language planning ability."
        },
        {
            "title": "Limitations",
            "content": "Despite the strong performance of our frameworks, an area of improvement is the reliance on predefined heuristics for selecting inference-time algorithms, which may not always generalize optimally across all tasks and domains. Additionally, while our frameworks demonstrate strong performance, their computational overhead could be further optimized for efficiency in real-world applications. We believe that our frameworks can be useful in further boosting the planning and reasoning capabilities of existing models such as o1 and Gemini-thinking. In addition, the use of reinforcement learning or meta-learning techniques to dynamically adapt agent strategies based on task complexity could be an interesting area to explore. Moreover, broadening the scope to multi-modal and multi-lingual reasoning would significantly expand the applicability of our approach, and exploring the use of generated planning trajectories for model training offers valuable direction."
        },
        {
            "title": "Ethics Statement",
            "content": "The use of proprietary LLMs such as GPT-4, Gemini, and Claude-3 in this study adheres to their policies of usage. We have used AI assistants (Grammarly and Gemini) to address the grammatical errors and rephrase the sentences."
        },
        {
            "title": "References",
            "content": "B. Bohnet, A. Nova, A. T. Parisi, K. Swersky, K. Goshvadi, H. Dai, D. Schuurmans, N. Fiedel, and H. Sedghi. Exploring and benchmarking the planning capabilities of large language models. arXiv preprint arXiv:2406.13094, 2024. B. Brown, J. Juravsky, R. Ehrlich, R. Clark, Q. V. Le, C. R√©, and A. Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling. arXiv preprint arXiv:2407.21787, 2024. W. Chen, S. Koenig, and B. Dilkina. RePrompt: Planning by Automatic Prompt Engineering for Large Language Models Agents. arXiv preprint arXiv:2406.11132, 2024. L. Guan, K. Valmeekam, S. Sreedharan, and S. Kambhampati. Leveraging pre-trained large language models to construct and utilize world models for model-based task planning. Advances in Neural Information Processing Systems, 36:7908179094, 2023. Q. Han, K. Khamaru, and C.-H. Zhang. UCB algorithms for multi-armed bandits: Precise regret and adaptive inference. arXiv preprint arXiv:2412.06126, 2024. 11 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving S. Hao, Y. Gu, H. Ma, J. Hong, Z. Wang, D. Wang, and Z. Hu. Reasoning with language model is planning with world model. In H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 81548173, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.507. URL https://aclanthology.org/2023.emnlp-main.507/. C. He, R. Luo, Y. Bai, S. Hu, Z. Thai, J. Shen, J. Hu, X. Han, Y. Huang, Y. Zhang, J. Liu, L. Qi, Z. Liu, and M. Sun. OlympiadBench: challenging benchmark for promoting AGI with olympiad-level bilingual multimodal scientific problems. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.211. URL https://aclanthology.org/2024.acl-long.211/. J. Huang, X. Chen, S. Mishra, H. S. Zheng, A. W. Yu, X. Song, and D. Zhou. Large language models cannot self-correct reasoning yet. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=IkmD3fKBPQ. A. Hurst, A. Lerer, A. P. Goucher, A. Perelman, A. Ramesh, A. Clark, A. Ostrow, A. Welihinda, A. Hayes, A. Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. F. Jiao, C. Qin, Z. Liu, N. F. Chen, and S. Joty. Learning planning-based reasoning by trajectories collection and process reward synthesizing. In Y. Al-Onaizan, M. Bansal, and Y.-N. Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 334350, Miami, Florida, USA, Nov. 2024. Association for Computational Linguistics. doi: 10. 18653/v1/2024.emnlp-main.20. URL https://aclanthology.org/2024.emnlp-main.20/. T. Kojima, S. S. Gu, M. Reid, Y. Matsuo, and Y. Iwasawa. Large language models are zero-shot reasoners. In Proceedings of the 36th International Conference on Neural Information Processing Systems, NIPS 22, Red Hook, NY, USA, 2024. Curran Associates Inc. ISBN 9781713871088. K.-H. Lee, I. Fischer, Y.-H. Wu, D. Marwood, S. Baluja, D. Schuurmans, and X. Chen. Evolving deeper llm thinking. arXiv preprint arXiv:2501.09891, 2025. Y. Liu, X. Peng, Y. Zhang, J. Cao, X. Zhang, S. Cheng, X. Wang, J. Yin, and T. Du. Tool-Planner: Dynamic Solution Tree Planning for Large Language Model with Tool Clustering. arXiv preprint arXiv:2406.03807, 2024. V. Reddy, R. Koncel-Kedziorski, V. D. Lai, M. Krumdick, C. Lovering, and C. Tanner. DocFinQA: longcontext financial reasoning dataset. In L.-W. Ku, A. Martins, and V. Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers), pages 445458, Bangkok, Thailand, Aug. 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-short.42. URL https://aclanthology.org/2024.acl-short.42/. D. Rein, B. L. Hou, A. C. Stickland, J. Petty, R. Y. Pang, J. Dirani, J. Michael, and S. R. Bowman. GPQA: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. URL https://openreview.net/forum?id=Ti67584b98. C. V. Snell, J. Lee, K. Xu, and A. Kumar. Scaling test-time compute optimally can be more effective than scaling LLM parameters. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/forum?id=4FWAwZtd2n. G. Team, P. Georgiev, V. I. Lei, R. Burnell, L. Bai, A. Gulati, G. Tanzer, D. Vincent, Z. Pan, S. Wang, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. 12 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving K. Valmeekam, M. Marquez, A. Olmo, S. Sreedharan, and S. Kambhampati. Planbench: An extensible benchmark for evaluating large language models on planning and reasoning about change. Advances in Neural Information Processing Systems, 36, 2024. E. Z. Wang, F. Cassano, C. Wu, Y. Bai, W. Song, V. Nath, Z. Han, S. M. Hendryx, S. Yue, and H. Zhang. Planning in natural language improves LLM search for code generation. In The Thirteenth International Conference on Learning Representations, 2025. URL https://openreview.net/ forum?id=48WAZhwHHw. L. Wang, C. Ma, X. Feng, Z. Zhang, H. Yang, J. Zhang, Z. Chen, J. Tang, X. Chen, Y. Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024a. X. Wang, C. Li, Z. Wang, F. Bai, H. Luo, J. Zhang, N. Jojic, E. Xing, and Z. Hu. Promptagent: Strategic planning with language models enables expert-level prompt optimization. In The Twelfth International Conference on Learning Representations, 2024b. URL https://openreview.net/ forum?id=22pyNMuIoa. Y. Wang, T. Shen, L. Liu, and J. Xie. Sibyl: Simple yet effective agent framework for complex real-world reasoning. CoRR, 2024c. S. Welleck, A. Bertsch, M. Finlayson, H. Schoelkopf, A. Xie, G. Neubig, I. Kulikov, and Z. HarInference-time algorithms for large language ISSN 2835-8856. URL https: chaoui. models. Transactions on Machine Learning Research, 2024. //openreview.net/forum?id=eskQMcIbMS. Survey Certification. From decoding to meta-generation: Y. Wu, Z. Sun, S. Li, S. Welleck, and Y. Yang. An empirical analysis of compute-optimal inference for problem-solving with language models. CoRR, 2024a. Z. Wu, C. Han, Z. Ding, Z. Weng, Z. Liu, S. Yao, T. Yu, and L. Kong. Os-copilot: Towards generalist computer agents with self-improvement. In ICLR 2024 Workshop on Large Language Model (LLM) Agents, 2024b. Z. Xiao, D. Zhang, Y. Wu, L. Xu, Y. J. Wang, X. Han, X. Fu, T. Zhong, J. Zeng, M. Song, and G. Chen. Chain-of-experts: When LLMs meet complex operations research problems. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview. net/forum?id=HobyL1B9CZ. C. Xie and D. Zou. human-like reasoning framework for multi-phases planning task with large language models. arXiv preprint arXiv:2405.18208, 2024. S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, and Y. Cao. React: Synergizing reasoning and acting in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=WE_vluYUL-X. S. Yao, D. Yu, J. Zhao, I. Shafran, T. Griffiths, Y. Cao, and K. Narasimhan. Tree of thoughts: Deliberate problem solving with large language models. Advances in Neural Information Processing Systems, 36, 2024. D. Zhang, X. Huang, D. Zhou, Y. Li, and W. Ouyang. Accessing gpt-4 level mathematical olympiad solutions via monte carlo tree self-refine with llama-3 8b. arXiv preprint arXiv:2406.07394, 2024. H. Zhao, K. Wang, M. Yu, and H. Mei. Explicit planning helps language models in logical reaIn H. Bouamor, J. Pino, and K. Bali, editors, Proceedings of the 2023 Conference on soning. 13 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Empirical Methods in Natural Language Processing, pages 1115511173, Singapore, Dec. 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.688. URL https://aclanthology.org/2023.emnlp-main.688/. H. S. Zheng, S. Mishra, H. Zhang, X. Chen, M. Chen, A. Nova, L. Hou, H.-T. Cheng, Q. V. Le, E. H. Chi, et al. Natural plan: Benchmarking llms on natural language planning. arXiv preprint arXiv:2406.04520, 2024. Y. Zhu, S. Qiao, Y. Ou, S. Deng, N. Zhang, S. Lyu, Y. Shen, L. Liang, J. Gu, and H. Chen. Knowagent: Knowledge-augmented planning for llm-based agents. arXiv preprint arXiv:2403.03101, 2024. 14 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving A. Related Works LLM Agents for Planning Agent-based frameworks for planning have gained interest, focusing on enhancing how LLMs decompose tasks and refine their outputs. The Sibyl framework (Wang et al., 2024c) effectively decomposes tasks into smaller subtasks, assigning each to specialized agents that iteratively collaborate until solution is reached. OS-Copilot (Wu et al., 2024b) introduces generalist computer agent that employs self-improvement through modularization and feedback loops. Another approach is KnowAgent (Zhu et al., 2024), which integrates knowledge-augmented planning to enhance the decision-making capabilities of LLM agents. Similarly, Tool-Planner (Liu et al., 2024) proposed grouping tools based on similar functionalities into toolkits, allowing LLMs to select the best tool for given task. Many agent-based works focusing on planning have been developed (Chen et al., 2024; Wang et al., 2024b; Xie and Zou, 2024). Despite the progress, these methods generally (i) focus on domain-specific tasks or limited benchmarks, reducing generalizability, and (ii) lack or under-explore mechanisms for verifying and refining plans iteratively. While some works explore natural language planning (Bohnet et al., 2024; Lee et al., 2025), they either single-agent frameworks or evaluate proposed framework on domain-specific benchmarks. Inference-time algorithms have recently shown significant improveInference-time Algorithms ment in LLMs performance during inference. For instance, Best of sampling (Brown et al., 2024) selects the most promising output from multiple generations performed using temperature sampling, while Tree-of-Thought (ToT) (Yao et al., 2024) models reasoning as an iterative tree search. REBASE (Wu et al., 2024a) optimizes search-space pruning using reward balancing. One very popular approach is Monte Carlo Tree Search (MCTS) (Zhang et al., 2024) which iteratively explores solution paths during inference. Applied to models such as LLaMa-3-8B, it enables self-refinement by revisiting and improving initial solutions. Test-time optimization (Snell et al., 2025), focuses on dynamically adjusting computational resources during inference (Wu et al., 2024a). Furthermore, Wang et al. (2025) uses the inference time algorithms to improve LLMs planning capabilities to solve code synthesis problems. In inference-time algorithms, verification is the key component. In contrast to these past works, here, we enhance performance of inference-time algorithms utilizing constraint-guided verification, and multi-agent collaboration for natural language planning, and its applications in downstream complex reasoning tasks. B. Further Details on LLM Agents In this section, we provide additional details about each specialized agent in PlanGEN. We present the prompts used for each agent, highlighting their roles in the framework. The prompt for the constraint agent includes task-specific parameters that can be adjusted to extract relevant constraints for different tasks. In contrast, the prompts for the verification agent and selection agent are entirely task-agnostic, ensuring generalizability and adaptability across various problem domains. Prompts for Constraint Agent The constraint agent is responsible for extracting problem-specific constraints that guide the planning process. To enable systematic extraction of constraints, we design task-specific prompt for the constraint agent:"
        },
        {
            "title": "Prompt",
            "content": "You are an expert in understanding an input problem and generating set of constraints. Analyze the input problem and extract all relevant instance-specific constraints and contextual details 15 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving necessary for accurate and feasible planning. (Optional) These constraints may include: <You may provide any specific type of constraints> <You may provide any formatting instruction> Input Problem: <problem statement> Based on the above prompts, we define the types of constraints used in the NATURAL PLAN benchmark for different planning tasks: calendar scheduling, meeting planning, and trip planning. For DocFinQA, we provide set of formatting instructions to ensure structured constraint generation. For GPQA and OlympiadBench, the constraint extraction follows the general prompt outlined above. Prompts for Verification Agent The prompt for the verification agent is designed to be task-agnostic, meaning it can be applied across different problem domains without modification. By enforcing strict evaluation criteria, this agent enhances the reliability of PlanGEN, making it robust for various planning and reasoning tasks. In this prompt, list of constraints are generated using constraint agent. Notably, the list of constraints used in the verification prompt is dynamically generated by the constraint agent. This ensures that the verification process is based on instance-specific constraints rather than relying on predefined, static rules."
        },
        {
            "title": "Prompt",
            "content": "Provide reward score between -100 and 100 for the quality of the provided plan steps, using strict evaluation standards. Ensure the reward reflects how effectively the plan contributes to progressing toward the correct solution. Problem Statement: {problem} Plan: {plan} Consider the following constraints while evaluating: - [Constraint 1] - [Constraint 2] - [Constraint 3] Provide feedback in the following format: [Step-by-step reasoning for the reward score] Score: [Strictly provide an integer reward score between -100 and 100] Prompts for Selection Agent The prompt for the Selection Agent is task-agnostic, allowing it to be applied across various domains without modification. It processes feedback from the verification agent and contextual information from the problem statement to assign suitability scores to different inference-time algorithms. 16 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving"
        },
        {
            "title": "Prompt",
            "content": "Analyze the following planning problem and explain your reasoning for assigning priority scores to the algorithms based on their suitability. Scores should be between 0 and 1, where 1 represents the most suitable algorithm for the given problem. Problem Statement: <problem statement> Requirements: <feedback> Context: <context if context else None provided> Start by providing brief reasoning for each algorithms suitability based on problem complexity. Then, ONLY output your response strictly as list with the exact format below: Reasoning: Best of N: [Explain why this algorithm is or isnt suitable] Rebase: [Explain why this algorithm is or isnt suitable] ToT: [Explain why this algorithm is or isnt suitable] Scores: [(\"Best of N\", float), (\"Rebase\", float), (\"ToT\", float)] Algorithm for Selection using UCB The algorithm (Algorithm 1) presented is modified UCB selection strategy that incorporates additional factors for exploration, diversity, and recovery. It initializes each algorithm with basic statistics like reward (ùëÖ(ùëé)), count of trials (ùê∂(ùëé)), and recovery score (ùëÖùëíùëê(ùëé)). The algorithm computes normalized reward ùëÖnorm(ùëé) for each option, balancing the reward with exploration (ùê∏(ùëé)), which encourages trying less-used algorithms. diversity bonus ùê∑(ùëé) penalizes overused algorithms, while recovery bonus ùëÖùëíùëêùêµ(ùëé) rewards algorithms that perform well after prior failures. LLM-guided priors (ùêøùêøùëÄ_ùëùùëüùëñùëúùëü) are used to influence the selection process based on prior knowledge. The final selection is made by maximizing the UCB score, which combines these factors to balance exploitation and exploration. Ablation Study on UCB Modifications To design our selection agent, we conducted an ablation study evaluating modifications to the UCB formula, shown in Figure 10. Initially, we replaced the selection agent with simple sequential strategy, termed Multi-Agent (Sequential), where algorithms execute in sequence, and the verification agent selects the highest-scoring plan. Next, we implemented UCB selection agent, but excluded the diversity bonus and recovery term introduced in our proposed formulation in the main paper, denoted as MultiAgent (UCB w/o div. and rec.). Finally, we implemented the complete selection agent incorporating our proposed UCB, labeled MultiAgent (UCB). As shown in Figure 10, the inclusion of the diversity bonus and recovery terms Figure 10 Ablation Study of UCB Modifications on Selection Agent and its impact on Multi-Agent Mixture of Algorithms framework. div.: diversity bonus, rec: recovery term. 17 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving in the UCB formula (\"Multi-Agent (UCB)\") resulted in 3.5% performance gain compared to the UCB variant without these terms, further enhancing overall results. Note that the LLM-guided priors are still the part of Multi-Agent (UCB w/o div. and rec.) and Multi-Agent (UCB). C. Prompts for Proposed Frameworks We provide further details in this section regarding the prompts used for PlanGEN (ToT) and PlanGEN (REBASE), as well as the specific algorithms used to execute these inference-time methods. Prompts used for ToT and REBASE PlanGEN (ToT) and PlanGEN (REBASE) employ three prompt types: (1) step prompt, (2) step reward prompt, and (3) completion prompt. Step prompt guide the model to generate subsequent steps based on the problem statement and previously generated steps. Step reward prompt evaluate each intermediate step against the problem statement and constraints, similar to the prompts used by verification agent. Completion prompt check for complete solution after each step. If solution is found, exploration terminates; otherwise, the process continues until solution is reached."
        },
        {
            "title": "Step Prompt",
            "content": "You are an expert assistant for generating step-by-step plan to solve given question using specified tools. Given the problem and any intermediate steps, output only the next step in the plan. Ensure that the next action helps in moving toward the correct plan to solve the given question. Do not provide the full plan. Keep responses concise, focusing solely on the immediate next step that is most effective in progressing toward the correct plan. <problem> {Add problem statement here} </problem> <intermediate_step> {Append previously generated steps} </intermediate_step>"
        },
        {
            "title": "Completion Prompt",
            "content": "You are an assistant tasked with verifying if the final, complete plan to solve the given question has been achieved within the intermediate steps. Output only 1 if the intermediate steps contain the full solution needed to solve the question. If the full plan has not yet been reached, output only 0. Provide no additional commentaryreturn exclusively 1 or 0. <problem> {Add problem statement here} </problem> <intermediate_step> {Append previously generated steps} </intermediate_step> 18 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving D. Details on Benchmarks and Experiments Statistics of Benchmarks For evaluation, we utilize evaluation sets of all four benchmarks. For NATURAL PLAN, we employed the provided evaluation sets, consisting of 1000 instances each for Calendar Scheduling and Meeting Planning, and 1600 instances for Trip Planning. The GPQA evaluation was conducted using the Diamond set, which comprises 198 highly challenging instances. From OlympiadBench, we selected the text-only problems, excluding those requiring theorem prover, resulting in 674 instances for the MATH category and 236 for the PHY category. We also used 922 instances from the DocFinQA evaluation set. Models Our primary evaluations use Gemini-1.5-Pro for all the experiments. We also present case study with Gemini-2.0-Flash and GPT-4o to showcase the model-agnostic nature of PlanGEN. Metrics We use task-specific metrics for all evaluations. Specifically, we use Exact Match (EM) for NATURAL PLAN similar to Zheng et al. (2024), micro-average accuracy for OlympiadBench similar to He et al. (2024), and accuracy for GPQA and DocFinQA (along with F1-Score for DocFinQA). Feedback prompt for Multi-Agent Baseline In the multi-agent baseline, we employ feedback prompt to iteratively generate improved and refined outputs. The prompt is provided below:"
        },
        {
            "title": "Feedback Prompt",
            "content": "Analyze the following planning problem and explain your reasoning for assigning priority scores You are an intelligent assistant capable of self-reflection and refinement. will provide you with your last response, and your task is to improve it, if necessary. Here is your previous response: {previous_response} Analyze and refine your response step-by-step: 1. Reflect on your reasoning process. Where might it be unclear or incorrect? Improve it. 2. Revise the explanation to address any identified issues and make it more logical and comprehensive. 3. Ensure the final answer is correct, supported by clear reasoning. Hyper-parameters for Experiments To ensure deterministic behavior, we set the temperature of all models to 0 for each agent. For the inference-time algorithms, we used the following settings: PlanGEN (Best of ) with five samples at temperature of 0.7; Tree of Thoughts (ToT) with three children per root node, generated at temperature of 0.7; and REBASE, initialized with width 10 at temperature of 0.7, decremented by 1 after each call to expand. E. Additional Analysis Importance of Verification Agent The kernel density estimation (KDE) plot visualizes the distribution of reward values assigned to two distinct outcomes: Success (green) and Failure (red). The plot reveals clear separation between the reward distributions, with Success outcomes strongly associated with high reward values (around 80-100) and Failure outcomes primarily associated with low reward values (around 20-40). The sharply peaked green curve suggests consistent and high rewards for successful outcomes, while the broader red curve reflects more variability in rewards assigned to 19 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Figure 12 Performance comparison of inference-time algorithms across different complexity levels for meeting and trip planning from NATURAL PLAN failures. However, small bump in the red curve at high reward values (around 80-90) suggests few instances where failures received unexpectedly high rewards, warranting further investigation. This observation is further supported by statistically significant difference between the reward distributions, Mann-Whitney test (ùëà = 116128.0, ùëù < 0.0001). The low p-value (3.42e09) provides evidence that the difference in reward distributions is statistically significant. Performance of our frameworks w.r.t. different complexity From Figure 12, in the meeting planning, PlanGEN (Best of ) excels in both simple and intermediate problems, whereas PlanGEN (Mixture of Algo.) performs better for complex problems. The trip planning presents different trend, where PlanGEN (Best of ) and PlanGEN (Mixture of Algo.) consistently outperform other approaches across all complexity levels. Nonetheless, in very complex problems for both meeting and trip planning, all algorithms exhibit poor performance. Figure 11 KDE plot illustrating relationship between reward value and outcome (success/failure)."
        },
        {
            "title": "PHY",
            "content": "PlanGEN (Best of ) (5) PlanGEN (Best of ) (10) PlanGEN (Best of ) (20) 53.26 54.90 53.22 32.63 31.36 29.38 PlanGEN (ToT) (3) PlanGEN (ToT) (5) PlanGEN (ToT) (10) Different hyper-parameters of inference-time algorithms vs. their performance We conduct case study on OlympiadBench, where we analyze the impact of varying hyper-parameters on the performance of different inference-time algorithms. The results  (Table 2)  indicate that while increasing the number of samples (Best of ), steps (ToT), or refinements (REBASE) lead to marginal improvements, the overall differences remain relatively small. Given this, we opted for lower hyper-parameter values across all inference-time algorithms to balance efficiency and performance. Table 2 Performance impact of hyper-parameters on inference-time algorithms in OlympiadBench PlanGEN (REBASE) (10) PlanGEN (REBASE) (20) PlanGEN (REBASE) (30) 31.78 29.37 30.28 31.36 32.05 32.52 54.45 54.45 55.04 52.97 55.20 55.79 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Frequency of inference-time algorithm selection across datasets For the PlanGEN (Mixture of Algo.) method, we analyze how frequently each inference-time algorithm (Best of , ToT, and REBASE) is selected across different datasets. The results (shown in Table 3) show that PlanGEN (ToT) is the most frequently chosen algorithm in NATURAL PLAN, OlympiadBench, and GPQA, indicating its effectiveness in these domains. In contrast, for DocFinQA, PlanGEN (Best of ) is the dominant choice, suggesting that its strategy aligns better with financial reasoning tasks. PlanGEN (REBASE) is selected the least across all datasets, implying that its refinements are less favored by the selection mechanism. These findings highlight the dataset-dependent nature of inference-time algorithm effectiveness and the adaptability of the mixture approach in dynamically choosing the most suitable method. F. Various Examples for Different Components of PlanGEN Examples for Constraint Agent To illustrate the output of our constraint agent, Table 4, Table 5, and Table 6 present representative examples of generated constraints. These tables highlight the diverse constraints generated for problem instances of different tasks. Example for Verification Agent To illustrate the output of our verification agent, Table 7 presents representative examples of verification process for NATURAL PLAN (calendar scheduling). This table highlights the how the verification agent verifies the generated plan using constraints. Frameworks NATURAL PLAN OlympiadBench GPQA DocFinQA PlanGEN (Best of ) PlanGEN (ToT) PlanGEN (REBASE) 19.55% 68.85% 11.6% 7.09% 90.09% 2.82% 8.56% 85.59% 5.86% 81.03% 12.5% 6.47% Table 3 Algorithm Selection Frequency by Dataset Examples of Generated Plans To demonstrate the plan generation process, Table 8, Table 9, Table 10, and Table 11 present example plans for NATURAL PLAN, GPQA, DocFinQA, and OlympiadBench. Generated using PlanGEN (Best of ), these tables highlight the varied nature of plans produced across different task types. For GPQA, DocFinQA, and OlympiadBench (i.e., downstream reasoning tasks), the examples additionally illustrate how generated plans are executed to derive final answer. 21 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Algorithm 1 Selection using Modified UCB with LLM-Guided Priors 1: Initialize: ùëÖ(ùëé) 0, ùê∂(ùëé) 1, ùëÖùëíùëê(ùëé) 0, ùêπ(ùëé) 0, ùê∑(ùëé) 1, ùëá 0 2: Set ùúÜprior, ùõºdiversity, ùõºrecovery 3: Load LLM-guided priors 4: procedure Select lgorithm(args) Compute prior decay: ùúÜprior 5: Set max exploration term ùëÄ 5 Obtain LLM prior scores: ùêøùêøùëÄ_ùëùùëüùëñùëúùëü LLM_Guided_Prior(ùëéùëüùëîùë†) Compute max reward: ùëÖmax max(ùëÖ(ùëé)) (set to 1 if all rewards are 0) for each algorithm ùëé do ùúÜprior 1+ùëá 6: 8: 7: 9: Reduces as trials increase 10: Compute normalized reward: ùëÖnorm(ùëé) ùëÖ(ùëé) ùê∂(ùëé)ùëÖmax 11: Compute exploration term: Scales rewards between 0 and 1 for comparability ùê∏(ùëé) min (cid:169) (cid:173) (cid:171) 2 log(ùëá + 1) ùê∂(ùëé) , ùëÄ(cid:170) (cid:174) (cid:172) Encourages trying less-used algorithms, capped at ùëÄ 12: Compute diversity bonus: ùê∑(ùëé) ùõºdiversity ùê∂(ùëé) + 1 13: Compute recovery bonus: Penalizes frequently used algorithms to encourage variety 14: Compute final UCB score: ùëÖùëíùëêùêµ(ùëé) ùõºrecovery ùëÖùëíùëê(ùëé) Rewards algorithms that perform well after failures ùëàùê∂ ùêµ(ùëé) ùëÖnorm(ùëé) + ùê∏(ùëé) + ùúÜpriorùêøùêøùëÄ_ùëùùëüùëñùëúùëü(ùëé) + ùê∑(ùëé) + ùëÖùëíùëêùêµ(ùëé) Balances exploitation, exploration, diversity, and recovery 15: 16: end for Select best algorithm: return (ùëé, ùëàùê∂ ùêµ(ùëé)) 17: 18: end procedure ùëé arg max ùëé ùëàùê∂ ùêµ(ùëé) 22 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Benchmark Problem Statement Generated Constraints Calendar Scheduling Meeting Planning You are an expert at scheduling meetings. You are given few constraints on the existing schedule of each participant, the meeting duration, and possibly some preferences on the meeting time. Note there exists solution that works with existing schedule of every participant. TASK: You need to schedule meeting for Michelle, Steven and Jerry for one hour between the work hours of 9:00 to 17:00 on Monday. Here are the existing schedules for everyone during the day: - Michelle has meetings on Monday during 11:00 to 12:00; - Steven has blocked their calendar on Monday during 9:00 to 9:30, 11:30 to 12:00, 13:30 to 14:00, 15:30 to 16:00; -Jerry has blocked their calendar on Monday during 9:00 to 9:30, 10:00 to 11:00, 11:30 to 12:30, 13:00 to 14:30, 15:30 to 16:00, 16:30 to 17:00; Find time that works for everyones schedule and constraints. You are visiting San Francisco for the day and want to meet as many friends as possible. Solve the problem by considering various different schedules and picking the best one to optimize your goals. Travel distances (in minutes): Bayview to Golden Gate Park: 22. Golden Gate Park to Bayview: 23. CONSTRAINTS: You arrive at Bayview at 9:00AM. Barbara will be at Golden Gate Park from 8:00AM to 11:30AM. Youd like to meet Barbara for minimum of 90 minutes. Your response should start with SOLUTION: Instance-Specific Constraints and Details: Participants: Michelle, Steven, Jerry - The meeting must include all three. Meeting Duration: 1 hour - The meeting requires contiguous 60-minute slot. Work Hours: 9:00 to 17:00 (Monday) - The meeting must start and end within these working hours. This acts as global constraint. Individual Schedules (Existing Meetings/Blocked Times): - Michelle: Busy from 11:00 to 12:00 on Monday. - Steven: Busy from 9:00 to 9:30, 11:30 to 12:00, 13:30 to 14:00, and 15:30 to 16:00 on Monday. - Jerry: Busy from 9:00 to 9:30, 10:00 to 11:00, 11:30 to 12:30, 13:00 to 14:30, and 15:30 to 17:00 on Monday (note this constraint ends at the end of the workday). While the problem states \"Note there exists solution,\" this isnt constraint in itself; its guarantee that feasible solution exists given the constraints. The actual constraints are the schedules, meeting duration, and working hours. Instance-Specific Constraints and Details: Travel Times: - Bayview to Golden Gate Park: 22 minutes. - Golden Gate Park to Bayview: 23 minutes (Note: This information is provided but not strictly necessary for this specific problem as the return trip isnt relevant to meeting Barbara). Locations: - Starting Location: Bayview - Meeting Location: Golden Gate Park Scheduled Meeting Times/Participant Availability: - Barbara is available at Golden Gate Park from 8:00 AM to 11:30 AM. Arrival Time: - You arrive at Bayview at 9:00 AM. Minimum Meeting Duration: - The meeting with Barbara must be at least 90 minutes long. Sequence Requirements: - Travel (Bayview to Golden Gate Park) Wait (if necessary, due to arriving before Barbaras availability ends) Meeting (with Barbara) Objective: Maximize the number of friends met (in this case, its simplified to just meeting Barbara, so the objective is to determine IF meeting is possible given the constraints). Table 4 Examples of constraints generated by the constraint agent given problem. These examples illustrate constraint generation for calendar scheduling and meeting planning from NATURAL PLAN 23 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Question: Two quantum states with energies E1 and E2 have lifetime of 109 sec and 108 sec, respectively. We want to clearly distinguish these two energy levels. Which one of the following options could be their energy difference so that they can be clearly resolved? (A) 1011 eV (B) 108 eV (C) 109 eV (D) 104 eV"
        },
        {
            "title": "Constraints for Verifying the Energy Difference Solution for Resolvable Quantum States",
            "content": "This problem revolves around the energy-time uncertainty principle, which dictates the resolvability of energy levels given their lifetimes. To verify proposed energy difference (Œîùê∏ = ùê∏2 ùê∏1) that allows clear resolution, we need constraints encompassing the following: 1. Uncertainty Principle Adherence: Primary Constraint: The product of the uncertainty in energy (Œîùê∏) and the uncertainty in time (Œîùë°) must satisfy the energy-time uncertainty relation: Œîùê∏Œîùë° ‚Ñè Specific Constraint: Since we have two states, we must consider the combined uncertainty in time. We can approximate this as the average of the individual lifetimes: Œîùë° ùúè1+ùúè2 Operational Constraint: For \"clear resolution,\" we typically require Œîùê∏ to be significantly greater than the uncertainty dictated by the uncertainty principle. common rule of thumb is Œîùê∏ ‚Ñè Œîùë° . stricter constraint might use larger factor (e.g., Œîùê∏ 2‚Ñè , where ùúè1 and ùúè2 are the lifetimes of the two states. 2 , where ‚Ñè is the reduced Planck constant. Œîùë° ) for even clearer resolution. 2. Units Consistency: Unit Constraint: All calculations must use consistent units. Ensure that energy (Œîùê∏), time (Œîùë°), and ‚Ñè are expressed in compatible units (e.g., Joules and seconds, or eV and seconds). Clearly state the chosen unit system. 3. Numerical Accuracy: Precision Constraint: Calculations should be performed with sufficient numerical precision to avoid rounding errors. Define the acceptable level of precision. Constant Accuracy: Use precise value for the reduced Planck constant (‚Ñè). 4. Solution Justification: Explanation Constraint: The solution must not only provide numerical value for Œîùê∏ but also clearly explain why this value satisfies the constraints and allows for clear resolution. This explanation should reference the uncertainty principle and the chosen resolution criterion. 5. Alternative Solutions Consideration (Optional): Exploration Constraint: Explore whether multiple values of Œîùê∏ could satisfy the constraints. If so, discuss the trade-offs between different solutions. Example Implementation of Constraints: Lets assume were working in SI units (Joules and seconds). 1. Uncertainty Principle: Œîùê∏ 2. Units: All calculations are in Joules and seconds. 3. Numerical Accuracy: Calculations will be performed to at least 4 significant figures. 4. Justification: The solution will explain how the calculated Œîùê∏ is significantly larger than the uncertainty, ensuring clear resolution. , where ‚Ñè 1.054 1034 s. ‚Ñè [ (109 s+108 s)/2] Table 5 Examples of constraints generated by the constraint agent given reasoning question. The example illustrates the constraint generation for solving physics question from GPQA 24 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Question: For positive integer ùëé, define sequence of integers ùë•1, ùë•2, . . . by letting ùë•1 = ùëé and ùë•ùëõ+1 = 2ùë•ùëõ + 1 for ùëõ 1. Let ùë¶ùëõ = 2ùë•ùëõ 1. Determine the largest possible ùëò such that, for some positive integer ùëé, the numbers ùë¶1, . . . , ùë¶ùëò are all prime."
        },
        {
            "title": "Constraints for Verifying the Solution Plan for Prime Sequence Problem",
            "content": "This problem requires solution plan that likely involves combination of mathematical reasoning and computational verification. Heres breakdown of constraints to verify and evaluate such plan: I. Mathematical Reasoning Constraints: 1. Correct Recurrence Relation: The plan must correctly interpret and utilize the given recurrence relations for ùë•ùëõ and ùë¶ùëõ. Verify that the plan doesnt misinterpret or modify these definitions. 2. Mersenne Primes Understanding: The plan should demonstrate an understanding that ùë¶ùëõ are Mersenne numbers (numbers of the form 2ùëù 1). It should acknowledge that for ùë¶ùëõ to be prime, ùë•ùëõ must itself be prime (though this is necessary but not sufficient condition). 3. Logical Flow and Justification: The plans reasoning should be clear, logical, and well-justified. Each step should follow from the previous ones, with clear explanations for any claims or deductions made. Avoid leaps of logic or unsubstantiated assertions. 4. Handling of Base Case (ùëò = 1): The plan should explicitly address the trivial case where ùëò = 1. Since ùë¶1 = 2ùëé 1, any prime value of will result in ùë¶1 being potential Mersenne prime. This confirms that ùëò 1. 5. Strategy for Finding Maximum ùëò: The plan must outline clear strategy for determining the largest possible ùëò. This likely involves exploring increasing values of ùëò and checking the primality of the resulting ùë¶ùëõ sequence. The plan should explain how it avoids infinite search or unnecessary computations. 6. Optimality Argument: The plan should include an argument (or at least strategy for developing an argument) for why the found value of ùëò is indeed the largest possible. This might involve demonstrating that for ùëò + 1, no suitable exists. II. Computational Verification Constraints: 1. Primality Testing Algorithm: If the plan involves computational primality testing, it should specify the algorithm used (e.g., probabilistic tests like Miller-Rabin or deterministic tests like AKS). The plan should justify the choice of algorithm based on the size of the numbers being tested and the desired level of certainty. 2. Correct Implementation: If code is involved, it must be correct and free of errors. The plan should include testing procedures to ensure the code accurately implements the chosen primality test and the recurrence relations. 3. Computational Resources: The plan should consider the computational resources required for the primality testing. For large values of ùë•ùëõ, testing the primality of ùë¶ùëõ can be computationally expensive. The plan should address how it manages these computational challenges. 4. Range of considered: The plan should specify the range of values for that will be explored. It should justify this range and explain why exploring values outside this range is unnecessary. 5. Termination Condition: The plan must have clear termination condition for its computational search. It should explain how it determines when to stop searching for larger values of k. III. Presentation and Clarity Constraints: 1. Clear and Concise Explanation: The plan should be presented in clear, concise, and easy-to-understand manner. Avoid unnecessary jargon or overly complex language. 2. Well-Organized Structure: The plan should be well-organized, with logical flow of ideas. Use headings, subheadings, and bullet points to improve readability. 3. Reproducibility: The plan should be sufficiently detailed to allow another person to reproduce the results. All assumptions, parameters, and procedures should be clearly stated. Table 6 Examples of constraints generated by the constraint agent given MATH reasoning problem. The example illustrates the constraint generation for solving MATH problem from OlympiadBench 25 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Problem Statement: You are an expert at scheduling meetings. You are given few constraints on the existing schedule of each participant, the meeting duration, and possibly some preferences on the meeting time. Note there exists solution that works with existing schedule of every participant. TASK: You need to schedule meeting for Michelle, Steven and Jerry for one hour between the work hours of 9:00 to 17:00 on Monday. Here are the existing schedules for everyone during the day: - Michelle has meetings on Monday during 11:00 to 12:00; - Steven has blocked their calendar on Monday during 9:00 to 9:30, 11:30 to 12:00, 13:30 to 14:00, 15:30 to 16:00; -Jerry has blocked their calendar on Monday during 9:00 to 9:30, 10:00 to 11:00, 11:30 to 12:30, 13:00 to 14:30, 15:30 to 16:00, 16:30 to 17:00; Find time that works for everyones schedule and constraints. Instance-Specific Constraints and Details: Participants: Michelle, Steven, Jerry - The meeting must include all three. Meeting Duration: 1 hour - The meeting requires contiguous 60-minute slot. Work Hours: 9:00 to 17:00 (Monday) - The meeting must start and end within these working hours. This acts as global constraint. Individual Schedules (Existing Meetings/Blocked Times): - Michelle: Busy from 11:00 to 12:00 on Monday. - Steven: Busy from 9:00 to 9:30, 11:30 to 12:00, 13:30 to 14:00, and 15:30 to 16:00 on Monday. - Jerry: Busy from 9:00 to 9:30, 10:00 to 11:00, 11:30 to 12:30, 13:00 to 14:30, and 15:30 to 17:00 on Monday (note this constraint ends at the end of the workday). While the problem states \"Note there exists solution,\" this isnt constraint in itself; its guarantee that feasible solution exists given the constraints. The actual constraints are the schedules, meeting duration, and working hours. Generated Plan: 1. Analyze Constraints: - Participants: Michelle, Steven, Jerry - Duration: 1 hour - Work hours: 9:00 - 17:00 - Day: Monday 2. Identify Individual Availability: - Michelle: 9:00-11:00, 12:00-17:00 - Steven: 9:30-11:30, 12:00-13:30, 14:00-15:30, 16:00-17:00 - Jerry: 9:30-10:00, 11:00-11:30, 12:30-13:00, 14:30-15:30, 16:00-16:30 3. Determine Common Availability: Looking for 1-hour slots: - Comparing Michelle and Steven: 9:30-11:00, 12:00-13:30, 14:00-15:30, 16:00-17:00 - Comparing the above with Jerry: 14:30-15: 4. Prioritize Earliest Time Slot: The earliest 1-hour slot is 14:30-15:30 5. Ensure Solution Exists: The slot 14:30-15:30 works for all three. 6. Strict Format Requirement: SOLUTION: Here is the proposed time: Monday, 14:30 - 15:30 Verification: The plan demonstrates significant improvement compared to the initial plan. It correctly identifies individual availabilities and successfully finds the two viable one-hour slots: 9:30-10:30 and 14:30-15:30. It avoids exploring irrelevant shorter slots. The plan is well-structured and clearly explains each step. Its on the verge of providing the correct solution. However, it has not yet explicitly stated the final answer (the prioritized earliest time slot) which is key part of the task. Therefore, while much improved, it doesnt quite merit top score due to this omission. Score: Table 7 Examples of verification generated by the verification agent given generated plan and constraints. This example illustrates verification process for calendar scheduling from NATURAL PLAN 26 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Category Problem Statement Generated Plan Calendar Scheduling You are an expert at scheduling meetings. You are given few constraints on the existing schedule of each participant, the meeting duration, and possibly some preferences on the meeting time. Note there exists solution that works with existing schedule of every participant. TASK: You need to schedule meeting for Michelle, Steven and Jerry for one hour between the work hours of 9:00 to 17:00 on Monday. Here are the existing schedules for everyone during the day: - Michelle has meetings on Monday during 11:00 to 12:00; - Steven has blocked their calendar on Monday during 9:00 to 9:30, 11:30 to 12:00, 13:30 to 14:00, 15:30 to 16:00; -Jerry has blocked their calendar on Monday during 9:00 to 9:30, 10:00 to 11:00, 11:30 to 12:30, 13:00 to 14:30, 15:30 to 16:00, 16:30 to 17:00; Find time that works for everyones schedule and constraints. Meeting Planning Trip Planning You are visiting San Francisco for the day and want to meet as many friends as possible. Solve the problem by considering various different schedules and picking the best one to optimize your goals. Travel distances (in minutes): Bayview to Golden Gate Park: 22. Golden Gate Park to Bayview: 23. CONSTRAINTS: You arrive at Bayview at 9:00AM. Barbara will be at Golden Gate Park from 8:00AM to 11:30AM. Youd like to meet Barbara for minimum of 90 minutes. You plan to visit 4 European cities for 12 days in total. You only take direct flights to commute between cities. You plan to stay in Budapest for 6 days. You plan to stay in Reykjavik for 5 days. You would like to meet your friends at Reykjavik between day 6 and day 10 to tour together. You want to spend 2 days in Milan. You are going to attend wedding in Milan between day 10 and day 11. You would like to visit Porto for 2 days. Here are the cities that have direct flights: Budapest and Reykjavik, Reykjavik and Milan, Budapest and Milan, Milan and Porto. Find trip plan of visiting the cities for 12 days by taking direct flights to commute between them. 1. Analyze Constraints: - Participants: Michelle, Steven, Jerry - Duration: 1 hour - Work hours: 9:00 - 17:00 - Day: Monday 2. Identify Individual Availability: - Michelle: 9:00-11:00, 12:00-17:00 - Steven: 9:30-11:30, 12:00-13:30, 14:00-15:30, 16:0017:00 - Jerry: 9:30-10:00, 11:00-11:30, 12:30-13:00, 14:3015:30, 16:00-16:30 3. Determine Common Availability: Looking for 1-hour slots: - Comparing Michelle and Steven: 9:30-11:00, 12:0013:30, 14:00-15:30, 16:00-17:00 - Comparing the above with Jerry: 14:30-15:30 4. Prioritize Earliest Time Slot: The earliest 1-hour slot is 14:30-15:30 5. Ensure Solution Exists: The slot 14:30-15:30 works for all three. 6. Strict Format Requirement: SOLUTION: Here is the proposed time: Monday, 14:30 - 15:30 You leave Bayview at 9:00 AM and travel 22 minutes to Golden Gate Park, arriving at 9:22 AM. You meet Barbara from 9:22 AM 10:52 AM, fulfilling the 90-minute requirement. Day 1-6: Arriving in Budapest and visit Budapest for 6 days. Day 6: Fly from Budapest to Reykjavik. Day 6-10: Visit Reykjavik for 5 days. Day 10: Fly from Reykjavik to Milan. Day 10-11: Visit Milan for 2 days. Day 11: Fly from Milan to Porto. Day 11-12: Visit Porto for 2 days. Table 8 Examples of generated plans by PlanGEN (Best of ) given problem statement. These examples illustrate the plan generation for all three task categories from NATURAL PLAN 27 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Question: Two quantum states with energies E1 and E2 have lifetime of 109 sec and 108 sec, respectively. We want to clearly distinguish these two energy levels. Which one of the following options could be their energy difference so that they can be clearly resolved? Plan to solve the problem: 1. Identify the relevant physics principle: This problem involves the energy-time uncertainty principle, which states that the product of the uncertainty in energy (Œîùê∏) and the uncertainty in time (Œîùë°) is greater than or equal to h-bar/2 (‚Ñè/2). The uncertainty in time is related to the lifetime of the states. To clearly distinguish the energy levels, the energy difference between them must be greater than the uncertainty in their energies. 2. Relate lifetimes to time uncertainty: The lifetimes given (109 and 108 s) represent the uncertainties in the time for each state (Œîùë°1 and Œîùë°2). We need to consider which lifetime is more restrictive for resolving the energy levels. Since we want to *clearly* distinguish them, we should use the *smaller* lifetime as the relevant Œîùë°. This is because the shorter lifetime leads to larger energy uncertainty, making the levels harder to resolve. 3. Calculate the minimum energy difference: Use the energy-time uncertainty principle (Œîùê∏ Œîùë° ‚Ñè/2) with the smaller lifetime to calculate the minimum resolvable energy difference (Œîùê∏). We can approximate this by setting Œîùê∏ Œîùë° ‚Ñè/2. Well need the value of ‚Ñè (Plancks constant divided by 2ùúã) in eVs. 4. Compare the calculated Œîùê∏ with the choices: Compare the calculated minimum resolvable energy difference (Œîùê∏) with the given options (A) through (D). The correct answer will be the option that is *greater* than the calculated Œîùê∏, ensuring that the energy difference is large enough to resolve the two levels. 5. Select the correct answer: Based on the comparison in step 4, choose the option that represents an energy difference greater than the minimum resolvable energy difference. Plan Execution: 1. Identify the relevant physics principle: The energy-time uncertainty principle (Œîùê∏ Œîùë° ‚Ñè/2) governs this situation. 2. Relate lifetimes to time uncertainty: The shorter lifetime, 109 s, is the relevant Œîùë° because it leads to larger energy uncertainty, making resolution more difficult. So, Œîùë° = 109 s. 3. Calculate the minimum energy difference: We have Œîùê∏ Œîùë° = ‚Ñè/2. We need ‚Ñè in eVs. ‚Ñè = 6.582 1016 eVs. Œîùê∏ = (‚Ñè/2)/Œîùë° Œîùê∏ = (6.582 1016 eVs/2)/(109 s) Œîùê∏ = 3.291 107 eV This is approximately 3 107 eV. 4. Compare the calculated Œîùê∏ with the choices: We need an energy difference greater than 3 107 eV to clearly resolve the levels. (A) 1011 eV - Too small (B) 108 eV - Too small (C) 109 eV - Too small (D) 104 eV - Large enough 5. Select the correct answer: Option (D) is the only one significantly larger than the calculated minimum energy difference. Answer: Table 9 Examples of generated plans by PlanGEN (Best of ) given physics problem. The example illustrates the plan generation and its execution for physics problem from GPQA 28 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving <Entire Document> Question: What is the percentage change in cash flow hedges in 2011 compare to the 2010? Plan to solve the problem: 1. Locate the information on cash flow hedges: Search the document for \"cash flow hedges\" or similar terms related to derivative instruments used for hedging cash flows. Focus on sections discussing derivative instruments, hedging activities, and related notional amounts. 2. Identify the notional amounts for 2011 and 2010: Once the relevant section is found (Note 2i), extract the notional amounts of the cash flow hedges specifically for fiscal year 2011 and fiscal year 2010. 3. Calculate the percentage change: Use the formula for percentage change: [(Notional Amount 2011 - Notional Amount 2010) / Notional Amount 2010] * 100. This will provide the percentage change in cash flow hedges from 2010 to 2011. Plan Execution: 1. Locate the information on cash flow hedges: The information on cash flow hedges was located in section discussing derivative instruments and hedging activities. Specifically, it was found within sentence detailing the notional amounts of various hedging instruments. Code (if needed): python # No code needed for this step. Result: Information on cash flow hedges located successfully. 2. Identify the notional amounts for 2011 and 2010: The notional amounts for cash flow hedges were identified as $153.7 million for 2011 and $139.9 million for 2010. Code (if needed): python # No code needed for this step. Result: 2011 notional amount: $153.7 million; 2010 notional amount: $139.9 million. 3. Calculate the percentage change: Code (if needed): python notional_2011 = 153.7 notional_2010 = 139.9 percentage_change = ((notional_2011 - notional_2010) / notional_2010) * 100 rounded_percentage_change = round(percentage_change, 1) print(f\"{rounded_percentage_change}%\") Answer: 9.9% Table 10 Examples of generated plans by PlanGEN (Best of ) given financial reasoning problem. The example illustrates the plan generation and its execution for problem from DocFinQA 29 PlanGEN: Multi-Agent Framework for Generating Planning and Reasoning Trajectories for Complex Problem Solving Question: For positive integer ùëé, define sequence of integers ùë•1, ùë•2, . . . by letting ùë•1 = ùëé and ùë•ùëõ+1 = 2ùë•ùëõ + 1 for ùëõ 1. Let ùë¶ùëõ = 2ùë•ùëõ 1. Determine the largest possible ùëò such that, for some positive integer ùëé, the numbers ùë¶1, . . . , ùë¶ùëò are all prime. Plan for Solving the IMO Problem: 1. Problem Understanding: The problem defines recursive sequence x_n based on an initial value a, and related sequence y_n based on x_n. The goal is to find the largest such that y_1, y_2, ..., y_k can all be prime numbers for some choice of a. 2. Key Information and Variables: a: positive integer, the starting value for the sequence x_n. This is the variable we need to choose optimally. x_n: Defined recursively as x_1 = and x_(n+1) = 2x_n + 1. y_n: Defined as y_n = 2^(x_n) - 1. This is the sequence we need to check for primality. k: The length of the subsequence of y_n consisting only of primes. We want to maximize this. 3. Problem Type and Method: This is number theory problem involving sequences, primality, and recursion. The method will involve combination of: Analyzing the structure of x_n and y_n: Finding closed-form expression for x_n in terms of and will be crucial. This will help us understand the structure of y_n. Primality testing/properties: Well need to leverage known properties of prime numbers, particularly those related to Mersenne primes (primes of the form 2^p - 1, where is prime). Case analysis/exploration: We might need to explore small values of and to identify patterns and formulate conjectures. Proof by contradiction: To establish the largest possible k, we may need to assume larger value is possible and derive contradiction. 4. Solution Process: Step 1: Derive closed-form expression for x_n. The recursive definition suggests linear recurrence relation. We can solve it to express x_n directly in terms of and n. Step 2: Substitute x_n into the definition of y_n. This gives us formula for y_n in terms of and n. Step 3: Analyze the primality conditions for y_n. Given that y_n has the form 2^m - 1, we know that for y_n to be prime, (which is x_n) must itself be prime. This suggests focusing on values of that lead to prime values of x_n. Step 4: Explore small values of and k. This helps to identify patterns and limitations on the possible values of k. Step 5: Investigate divisibility properties. Consider the factors of y_n and how they might relate to previous terms in the sequence. Look for patterns in the factors that might prevent multiple consecutive y_n from being prime. Step 6: Formulate conjecture for the maximum k. Based on the observations from the previous steps, propose value for the maximum k. Step 7: Prove the conjecture. Use proof by contradiction. Assume larger is possible and demonstrate that this leads to contradiction based on the divisibility properties of y_n. This will establish the proposed value of as the maximum. Plan Execution: 1. Closed-form for ùë•ùëõ: The recurrence relation ùë•ùëõ+1 = 2ùë•ùëõ + 1 with ùë•1 = ùëé can be solved as follows: ùë•2 = 2ùëé + 1 ùë•3 = 2(2ùëé + 1) + 1 = 4ùëé + 3 = 22ùëé + 22 1 ùë•4 = 2(4ùëé + 3) + 1 = 8ùëé + 7 = 23ùëé + 23 1 By induction, we can show that ùë•ùëõ = 2ùëõ1ùëé + 2ùëõ1 1 = 2ùëõ1 (ùëé + 1) 1. 2. ùë¶ùëõ in terms of ùëé and ùëõ: Substituting the expression for ùë•ùëõ into ùë¶ùëõ = 2ùë•ùëõ 1, we get ùë¶ùëõ = 22ùëõ1 (ùëé+1) 1 1. 3. Primality of ùë¶ùëõ: For ùë¶ùëõ to be prime, ùë•ùëõ must be prime. Let ùë•ùëõ = ùëùùëõ, where ùëùùëõ is prime number. 4. Exploring small values: If ùëé = 1, ùë•1 = 1, ùë¶1 = 21 1 = 1, which is not prime. If ùëé = 2, ùë•1 = 2, ùë¶1 = 22 1 = 3, which is prime. ùë•2 = 2(2) + 1 = 5, ùë¶2 = 25 1 = 31, which is prime. ùë•3 = 2(5) + 1 = 11, ùë¶3 = 211 1 = 2047 = 23 89, which is not prime. So, for ùëé = 2, ùëò = 2. 5. Divisibility: If ùë•1 = ùëé is prime, say ùëé = ùëù, then ùë•2 = 2ùëù + 1. If ùë•2 is also prime, then ùë¶1 and ùë¶2 are potentially Mersenne primes. However, ùë•3 = 2(2ùëù + 1) + 1 = 4ùëù + 3. If ùëù = 2, ùë•3 = 11, and ùë¶3 is not prime. If ùëù = 3, ùë•2 = 7, ùë•3 = 15, which is not prime. If ùëù = 5, ùë•2 = 11, ùë•3 = 23, ùë•4 = 47, ùë•5 = 95, which is not prime. If ùëò = 3, we need ùë•1 = ùëé, ùë•2 = 2ùëé + 1, and ùë•3 = 4ùëé + 3 to all be prime. If ùëé = 2, ùë•1 = 2, ùë•2 = 5, ùë•3 = 11. But ùë¶3 = 211 1 = 2047 = 23 89, which is not prime. 6. Conjecture: The largest possible ùëò is 2. 7. Proof: We have shown that for ùëé = 2, ùë¶1 and ùë¶2 are prime. We need to show that ùë¶3 cannot be prime. ùë•3 = 4ùëé + 3. If ùëé 2 (mod 3), then 4ùëé + 3 4(2) + 3 11 2 (mod 3). If ùëé 1 (mod 3), then 4ùëé + 3 4(1) + 3 7 1 (mod 3). If ùëé 0 (mod 3), then 4ùëé + 3 3 0 (mod 3). So, if ùëé > 3 and ùëé 0 (mod 3), then ùë•3 is divisible by 3 and greater than 3, so it is not prime. If ùëé = 3, ùë•3 = 15, not prime. If ùëé = 2, ùë•3 = 11, and ùë¶3 = 2047 = 23 89. So the final answer is 2 . Table 11 Examples of generated plans by PlanGEN (Best of ) given maths problem. The example illustrates the plan generation and its execution for problem from OlympiadBench (MATH)"
        }
    ],
    "affiliations": [
        "Arizona State University",
        "Google"
    ]
}