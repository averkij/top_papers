{
    "paper_title": "InfLLM-V2: Dense-Sparse Switchable Attention for Seamless Short-to-Long Adaptation",
    "authors": [
        "Weilin Zhao",
        "Zihan Zhou",
        "Zhou Su",
        "Chaojun Xiao",
        "Yuxuan Li",
        "Yanghao Li",
        "Yudi Zhang",
        "Weilun Zhao",
        "Zhen Li",
        "Yuxiang Huang",
        "Ao Sun",
        "Xu Han",
        "Zhiyuan Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Long-sequence processing is a critical capability for modern large language models. However, the self-attention mechanism in the standard Transformer architecture faces severe computational and memory bottlenecks when processing long sequences. While trainable sparse attention methods offer a promising solution, existing approaches such as NSA introduce excessive extra parameters and disrupt the conventional \\textit{pretrain-on-short, finetune-on-long} workflow, resulting in slow convergence and difficulty in acceleration. To overcome these limitations, we introduce dense-sparse switchable attention framework, termed as InfLLM-V2. InfLLM-V2 is a trainable sparse attention that seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2 reuses dense attention parameters through parameter-free architecture modification, maintaining consistency between short and long sequence processing. Additionally, InfLLM-V2 ensures computational efficiency across all sequence lengths, by using dense attention for short inputs and smoothly transitioning to sparse attention for long sequences. To achieve practical acceleration, we further introduce an efficient implementation of InfLLM-V2 that significantly reduces the computational overhead. Our experiments on long-context understanding and chain-of-thought reasoning demonstrate that InfLLM-V2 is 4$\\times$ faster than dense attention while retaining 98.1% and 99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we have trained and open-sourced MiniCPM4.1 (https://huggingface.co/openbmb/MiniCPM4.1-8B), a hybrid reasoning model, providing a reproducible implementation for the research community."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 3 6 6 4 2 . 9 0 5 2 : r Preprint. Under review INFLLM-V2: DENSE-SPARSE SWITCHABLE ATTENTION FOR SEAMLESS SHORT-TO-LONG ADAPTATION Weilin Zhao1, Zihan Zhou2, Zhou Su2, Chaojun Xiao1, Yuxuan Li2, Yanghao Li1, Yudi Zhang3, Weilun Zhao2, Zhen Li2, Yuxiang Huang1, Ao Sun2, Xu Han1, Zhiyuan Liu1 1Tsinghua University 2OpenBMB 3Harbin Institute of Technology zwl23@mails.tsinghua.edu.cn {xcj,han-xu,liuzy}@tsinghua.edu.cn"
        },
        {
            "title": "ABSTRACT",
            "content": "Long-sequence processing is critical capability for modern large language models. However, the self-attention mechanism in the standard Transformer architecture faces severe computational and memory bottlenecks when processing long sequences. While trainable sparse attention methods offer promising solution, existing approaches such as NSA introduce excessive extra parameters and disrupt the conventional pretrain-on-short, finetune-on-long workflow, resulting in slow convergence and difficulty in acceleration. To overcome these limitations, we introduce dense-sparse switchable attention framework, termed as InfLLMV2. InfLLM-V2 is trainable sparse attention that seamlessly adapts models from short to long sequences. Specifically, InfLLM-V2 reuses dense attention parameters through parameter-free architecture modification, maintaining consistency between short and long sequence processing. Additionally, InfLLM-V2 ensures computational efficiency across all sequence lengths, by using dense attention for short inputs and smoothly transitioning to sparse attention for long sequences. To achieve practical acceleration, we further introduce an efficient implementation of InfLLM-V2 that significantly reduces the computational overhead. Our experiments on long-context understanding and chain-of-thought reasoning demonstrate that InfLLM-V2 is 4 faster than dense attention while retaining 98.1% and 99.7% of the performance, respectively. Based on the InfLLM-V2 framework, we have trained and open-sourced MiniCPM4.11, hybrid reasoning model, providing reproducible implementation for the research community."
        },
        {
            "title": "INTRODUCTION",
            "content": "With the rapid development of large language models (LLMs) (Brown et al., 2020; Bommasani et al., 2021; Han et al., 2021; OpenAI, 2023), the demand for long-sequence processing capabilities has become increasingly critical. From long-input scenarios such as deep research (Zheng et al., 2025; Xu & Peng, 2025), chatbots with long-term memory, and software issue resolution (Jimenez et al., 2023; Yang et al., 2025), to long-output tasks including complex reasoning (OpenAI et al., 2024; DeepSeek et al., 2025) and LLM-driven agents (Wang et al., 2024), models capability to understand and generate long sequences directly determines its performance in real-world applications. However, the self-attention mechanism in the existing Transformer (Vaswani et al., 2017) architecture faces severe computational and memory bottlenecks when processing long sequences. To address the challenge of processing long sequences, efforts have been devoted to exploring sparse attention mechanisms (Beltagy et al., 2020; Zaheer et al., 2020; Tay et al., 2022), which restrict each token within the context to attend to only subset of tokens related to that token. Early research in this area focuses on the training-free setting, leveraging the sparsity naturally occurring in selfattention mechanisms to accelerate inference (Xiao et al., 2024a;b; Jiang et al., 2024). However, the training-free setting introduces fundamental trade-off between sparsity and model performance. To avoid significant performance degradation, the degree of sparsity that can be applied is often limited, which in turn restricts the potential efficiency gains. *Corresponding Authors. 1https://huggingface.co/openbmb/MiniCPM4.1-8B 1 Preprint. Under review Figure 1: The comparison of Vanilla Full Attention, NSA (Yuan et al., 2025), and our InfLLM-V2. Given the limitations of training-free attention mechanisms, trainable sparse attention mechanisms have garnered increasing attention from researchers (Lu et al., 2025; Gao et al., 2024). Among them, the natively trainable sparse attention (NSA) (Yuan et al., 2025) method adopts the widelyused block-sparse attention (Child et al., 2019) structure, designing three different sparse attention modules and developing corresponding CUDA kernels to accelerate model computation. Despite its effectiveness, we find misalignment between the sparse architecture of NSA and the standard pretrain-on-short, finetune-on-long workflow. widely used way to build long LLMs is to pretrain on short sequences and finetune on long sequences. The NSA creates an architectural mismatch with vanilla full attention, as it introduces three sets of key-value parameters and three attention modules, forcing the model to abruptly switch from single-output attention to multioutput attention architecture. As shown in Section 4, this mismatch destabilizes training, erases what the model has already learned, and introduces significant efficiency bottleneck for short sequences. To address all the above issues, we propose dense-sparse switchable attention framework (InfLLMV2). InfLLM-V2 is built on InfLLM (Xiao et al., 2024a), training-free block-sparse attention mechanism, and introduces three core innovations: 1. Seamless Short-to-Long Adaptation: As depicted in Figure 1, different from NSA, which requires additional parameters and multiple attention modules, InfLLM-V2 seamlessly transitions from dense to sparse attention by directly reusing existing dense attention parameters. This design naturally aligns with the standard pretrain-on-short, finetune-on-long workflow, eliminating architectural mismatches and training instability. 2. Efficiency for Both Short and Long Sequences: Because the transition from dense to sparse attention in InfLLM-V2 requires no additional parameters and introduces minimal distributional shifts, the model preserves its strong performance on short texts and can easily switch back to dense attention for short sequence efficiency. 3. Accelerated Block Selection Mechanism: The block selection step before sparse attention inherently undermines the efficiency gains of the sparse attention itself. We propose hardwareawared efficient implementation, effectively removing the bottleneck and unlocking the full potential of sparse attention. We evaluate our method on long-context understanding and long chain-of-thought (CoT) generation benchmarks. Our InfLLM-V2 is 4 faster than dense attention while maintaining 98.1% and 99.7% of the original performance on these tasks, respectively. We will release all associated implementations to facilitate future research on efficient attention."
        },
        {
            "title": "2 RELATED WORK",
            "content": "As the demand for LLMs to understand and generate long sequences continues to grow, research on improving attention efficiency has garnered increasing attention (Tay et al., 2022; Sun et al., 2025; Zhang et al., 2025a). In this section, we discuss the sparse attention paradigm from two perspectives: training-free and trainable sparse attention approaches."
        },
        {
            "title": "2.1 TRAINING-FREE SPARSE ATTENTION",
            "content": "Training-free sparse attention approaches aim to utilize the intrinsic sparsity of attention layers. These methods enable LLMs trained with dense attention to perform sparse attention between each token and small subset of relevant contexts. Based on the selection strategy for relevant contexts, these algorithms can be categorized into predefined sparse patterns and dynamic sparse patterns. Predefined Sparse Patterns. Sparse attention with predefined pattern employs manually defined heuristic rules to determine which contextual tokens should be selected for attention compu2 Preprint. Under review tation (Xiao et al., 2024b; Han et al., 2024; Child et al., 2019; Zaheer et al., 2020; Beltagy et al., 2020; Xiao et al., 2025). For instance, sliding window attention restricts each token to interact only with neighboring tokens (Beltagy et al., 2020). Building upon sliding windows, some works select special tokens such as initial tokens or segment separators, requiring all tokens to attend to these special tokens (Xiao et al., 2024b; Chen et al., 2024; Child et al., 2019). These approaches typically rely on human observations to formulate heuristic rules for selecting relevant contexts. Dynamic Sparse Patterns. Dynamic sparse patterns incorporate the semantic information of query tokens into the context selection process by computing the relevance between query tokens and candidate contexts. Early works primarily perform similarity computation at the token level (Kitaev et al., 2020; Roy et al., 2021; Wang et al., 2020). As sequence lengths increase, block sparse methods have gained widespread adoption, which partition contexts into contiguous block units and perform relevance computation and context selection at the block granularity (Xiao et al., 2024a; Jiang et al., 2024; Xu et al., 2025; Tang et al., 2024; Zhang et al., 2025b; Lai et al., 2025). Furthermore, research on attention sparsity has inspired the development of key-value (KV) eviction and compression methods, which reduce memory consumption by discarding or compressing KV caches with low attention probabilities (Zhang et al., 2023; Li et al., 2024; Huang et al., 2024; 2025). Training-free methods, while focusing on improving the inference efficiency of dense attention models, are often constrained by insufficient sparsity levels in order to avoid severe performance degradation and finally suffer from limited acceleration benefits."
        },
        {
            "title": "2.2 TRAINABLE SPARSE ATTENTION",
            "content": "To further enhance efficiency for long sequence processing, researchers incorporate sparse attention into the model training phase. SeerAttention (Gao et al., 2024) employs self-distillation posttraining algorithm to train router that selects relevant contexts for query blocks. MoBA (Lu et al., 2025) employs block sparse attention structure during the short-to-long adaptation phase, training routers between query blocks and KV blocks for context selection. These methods partition query tokens into blocks and can only accelerate the prefilling phase. NSA (Yuan et al., 2025) designs three attention components for token-level sparsity, effectively accelerating both prefilling and decoding processes. However, NSA introduces substantial additional parameters, making it unsuitable for efficient short-to-long adaptation and imposing significant computational overhead on short-sequence processing. In this paper, we focus on proposing sparse attention mechanism that effectively and efficiently processes both short and long sequences, supporting both prefilling and decoding."
        },
        {
            "title": "3.1 BACKGROUND",
            "content": "Grouped-Query Attention. Attention mechanisms enable models to selectively focus on relevant parts of the input sequence. Among various attention variants, grouped-query attention (GQA) (Ainslie et al., 2023) has emerged as popular method that strikes balance between model performance and computational efficiency. Given an input sequence of hidden states Rnd, where is the sequence length and is the model dimension, GQA computes the queries (Q), keys (K), and values (V) via linear projections as = XWQ, = XWK, = XWV . The projection matrices have the shapes WQ Rd(hqdh) and WK, WV Rd(hkvdh), with the head dimension dh. These tensors are then reshaped to form hq query heads {Qi}hq i=1, hkv KV heads {Kj, Vj}hkv j=1, with each head having the shape dh. The query heads are partitioned by group size = hq/hkv. The attention scores Si and the attention output Oi for the i-th query head are computed by attending to its corresponding KV heads with the index = (i 1)/G + 1: Si = Softmax (cid:33) (cid:32) QiK dh , Oi = SiVj. (1) The final output is obtained by concatenating the attention outputs and projecting them through final linear layer WO R(hqdh)d: Attention(X) = Concat(O1, . . . , Ohq )WO. 3 Preprint. Under review Figure 2: The overview of NSA and InfLLM-V2. InfLLM-V2 uses shared KV for both Sparse Attention and Dense Attention. InfLLM-V2 fuses Selected Attention and Sliding Attention and eliminates the output of Compressed Attention. InfLLM-V2 introduces no extra parameters. NSA. NSA (Yuan et al., 2025) is an enhancement of GQA designed for efficiency on long sequences. The key insight is that for long sequences, e.g., when > 32k, the attention score matrix exhibits strong sparsity. This allows for approximating the attention matrix by ignoring negligible values, leading to faster computation. As illustrated in Figure 2, NSA utilizes three distinct modules and combines them using gating module. Based on the observation that adjacent attention scores are similar (Jiang et al., 2024), NSA splits the sequences into blocks of size B. First, Compressed Attention employs compressed representation of the KV tensors to reduce the computational complexity. Second, Selected Attention leverages the attention scores from compressed attention to compute only the blocks with high attention scores. Finally, Sliding Attention is used to focus on local contextual information within the sequence. For these three attention modes, they introduce three sets of KV projection matrices: Wcmp . This final output can be mathematically represented as Output = gcmpOcmp +gslcOslc +gwinOwin, where Ocmp, Oslc, and Owin are the outputs of the three respective modules, and the gate scores gcmp, gslc, and gwin are derived from the input features via an MLP and sigmoid activation. They also train an MLP module for compressing the KV tensors. The three distinct KV projections, combined with an additional MLP and gating module, result in highly complex architecture. This complexity, in turn, makes the model poorly suited for training from scratch on short-sequence data and also complicates the process of converting pretrained dense models to sparse ones. , Wcmp , Wwin , Wslc , Wwin , Wslc"
        },
        {
            "title": "3.2 OVERALL FRAMEWORK",
            "content": "We propose InfLLM-V2, more concise framework with zero extra parameters that more closely aligns dense and sparse attention patterns. Shared Key-Value Projection. We find that using three separate sets of KV projection parameters in NSA (Yuan et al., 2025) is unnecessary, which not only complicates the adaptation from short to long sequences but also significantly slows down computation for short sequences. Therefore, we propose using single shared set of projection parameters, WK and WV , initialized with the pretrained dense attention parameters and used for finetuning on long sequences. Aligned Computation. In addition to ensuring that sparse and dense attention share the same parameters, their computational processes must also be closely aligned. In NSA, the three attention modules all generate outputs that are aggregated by an extra gating module. This forces the computation of all three modules even for short sequences, leading to substantial overhead. To mitigate this, we take union of the two sparse patterns in Selected Attention and Sliding Attention and eliminate the output of Compressed Attention, forming unified Sparse Attention module. Specifically, the original Selected Attention module identifies important token blocks based on the attention scores from the Compressed Attention module, Scmp. For query token with index i, located in the block bi = i1 + 1, attention is always granted to fixed set of initial blocks and set of local blocks: (2) Ilocal(i) = {bi Nlocal + 1, . . . , bi 1, bi}. Iinit = {1, 2, . . . , Ninit}, Preprint. Under review The top-k selection is then applied to Scmp over the set of remaining blocks, denoted as Itopk(i). The complete set of attended block indices for this query token is the union of these three sets: I(i) = Iinit Ilocal(i) Itopk(i). If we denote the set of token indices in the j-th block as Tj = {jB + 1, . . . , (j + 1)B}, the selected attention allows token in the block bi to attend to the union of blocks (cid:83) jI(i) Tj. The Sliding Attention, on the other hand, allows the i-th token to attend to range {i + 1, . . . , i} of window size w. Since the local blocks in Selected Attention and the window in Sliding Attention create overlapping, we merge them by expanding the number of local blocks within our unified Sparse Attention to strictly cover the region of the Sliding Attention, that is, Nlocal + 1, as illustrated in Figure 3. (3) Furthermore, we eliminate the output of the Compressed Attention module, only retaining its attention scores Scmp for block selection in Sparse Attention. This single-output design more closely mirrors dense attention and aids the training of the sparse attention model. InfLLM-V2 can thus dynamically switch between dense and sparse attention patterns based on the input sequence length. Simplified and Efficient Compression Module. Since we eliminate the output of the Compression Attention, using MLP for token compression would not receive gradients. We replace it with more intuitive parameter-free pooling function, which will be detailed in Section 3.3. Additionally, computing the attention scores Scmp introduces non-negligible overhead, and we will reduce this overhead in Section 3.4. Figure 3: The illustration of the union of Selected Attention and Sliding Attention."
        },
        {
            "title": "3.3 BLOCK REPRESENTATION",
            "content": "Simply compressing long sequence with large block size in 1-stage can lead to significant loss of granular information (Yuan et al., 2025). To address this, we implement 3-stage, coarse-grained to fine-grained compression process, as shown in Figure 4. In the first stage, we process the input key sequence to produce an intermediate and coarse-grained representation KC1. By denoting the initial compression block size as lC1 and the stride as sC1 , we achieve this by applying meanpooling operation over sequential blocks: KC1 = Mean(KisC1 :isC1 +lC1 Then, we compute the attention scores SC1 between the query and KC1: (4) ). Figure 4: The illustration of the 3-stage grouplevel compression, compared with the 1-stage token-level compression. SC1 = Softmax(Q(KC1 )). (5) In the second stage, we employ block-wise sparse attention rather than token-level approaches for the efficiency of Sparse Attention. In model utilizing GQA, we can achieve this by forcing the block selection pattern across all heads within group to be the same. We conduct summation within the head group to get the shared importance score Sshared: Sshared = (cid:88) SC1 (h). (6) h=1 In the third stage, we apply max-pooling operation, which can preserve the most salient features. The aggregated score Scmp are defined as follows and used for the Sparse Attention: Scmp = Max(Sshared is:is+l). (7) 5 Preprint. Under review Algorithm 1 Computation of Sshared (Suppose hkv = 1 without loss of generality.) Require: RnGdh , KC1 R(n/sC )dh , KC2 R(n/sC2 )dh in HBM. Block sizes Bq, Bk. Divide into Tq = n/Bq blocks Q1, . . . , QTq of size Bq dh each. Divide KC1 into T1 = n/sC1 /Bk blocks KC1 Divide KC2 into T2 = n/sC2 /Bk blocks KC2 Divide Sshared into Tq T1 blocks of size Bq Bk each. for = 1, . . . , Tq (parallel) do 1 , . . . , KC1 T1 1 , . . . , KC2 T2 of size Bk dh each. of size Bk dh each. Load Qi from HBM to on-chip SRAM. On chip, initialize online-softmax related statistic log-sum-exp lse. for = 1, . . . , T2 (sequential) do First pass (Coarse-grained) Load KC2 On chip, compute attention scores SC2 from HBM to on-chip SRAM. ij RGBq Bk as in Eq. (8) and update lse. for = 1, . . . , T1 (sequential) do Second pass (Fine-grained) from HBM to on-chip SRAM. Load KC1 On chip, compute attention scores SC1 On chip, compute the final block Sshared Write the block Sshared ij ij to its corresponding position in HBM. ij RGBq Bk as in Eq. (5) and normalize it using lse. RBq Bk by summing SC1 ij over the head group. return the output Sshared. In our method, we set lC1 = 4 , = 5, and = 4 so that it can achieve the same compression ratio as 1-stage compression of block size B. Intuitively, we compute the sparse scores of the entire block based on several sliding sub-blocks within the block. 2 , sC1 = B"
        },
        {
            "title": "3.4 EFFICIENT IMPLEMENTATION",
            "content": "For efficient Sparse Attention, we follow the techniques in NSA (Yuan et al., 2025) to set the group size of GQA to 16, configuration well-suited for block sparse attention. More details can be found in Appendix A. However, our profiling reveals that the computation of the compression score, Scmp, introduces significant performance bottleneck. primary source of this slowdown is the substantial I/O required to store the first-stage attention scores SC1 into the slow GPU HBM. The amount of data that needs to be written is hqn2/sC1 , where is the full sequence length. Given that sC1 n, materializing the full attention score matrix to GPU HBM incurs prohibitive cost. Drawing inspiration from FlashAttention (Dao, 2024), we aim to minimize this I/O by ensuring the attention scores remain within the fast GPU SRAM as much as possible. Our approach, Fused Head Group Summation, is to fuse the summation over the head group, required for the secondstage compression, directly into the SRAM-based computation loop of FlashAttention. After that, we can only store the reduced attention scores Sshared into GPU HBM, whose size is hqn2/(sC1 G). Another challenge arises from the fact that summing over the head group dimension and performing the online-softmax (Dao, 2024) along the sequence dimension are not commutative operations. This conflict prevents straightforward fusion. To overcome this, we implement two-pass approach. In the first pass, we compute the log-sum-exp (lse) term required for the softmax normalization within the SRAM. In the second pass, we leverage the lse to calculate the final attention scores, perform the summation across the head group within the SRAM, and write the reduced scores to the HBM. The trade-off of this two-pass method is that it doubles the computational workload. Therefore, we propose LSE Approximation to approximate the lse computation by using coarser-grained attention score SC2 . Following Eq. (4) and Eq. (5), we change them to KC2 = Mean(KisC2 :isC2 +lC2 ), SC2 = Softmax(Q(KC2)). (8) By setting sC2 = 4sC1 and lC2 = 4lC1, the computational overhead was reduced from 2 to 1.25. We summarize the procedure for computing Sshared in Algorithm 1. To further reduce memory I/O, the max-pooling and top-k operations related to Scmp could also be fused into the kernel; however, we leave this implementation for future work. 6 Preprint. Under review Figure 5: The training loss of models. We only show the last few iterations of the short pretraining."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "We evaluate InfLLM-V2 on tasks ranging from short to long contexts, and demonstrate its efficiency."
        },
        {
            "title": "4.1 EXPERIMENT SETUP",
            "content": "Pretraining Setup. We first use full attention to pretrain model on short-sequence data, marked as SHORT. We employ standard GQA (Ainslie et al., 2023) model backbone with 8B parameters, with the hidden size = 4096, the number of heads hq = 32, hkv = 2, and the head dimension dh = 128. The pretraining dataset consists of 8T tokens of 4k-length sequences, primarily comprising FineWeb-Edu (Penedo et al., 2024) and Stack-v2 (Lozhkov et al., 2024). We set 8M tokens per batch, and use WSD learning rate scheduler (Hu et al., 2024) with 2000 warmup steps to an initial learning rate of 7.5e-3, and 27000 decay steps to the final learning rate of 3e-4. Long-Context Adaptation. When transitioning to long-context finetuning, we switch to INFLLM-V2 (SPARSE). Following NSA (Yuan et al., 2025), we set the compression block size lC1 = 32, stride sC1 = 16, and attention block size = 64. For our efficient block selection implementation in Section 3.4, we additionally set the LSE Approximation block size lC2 = 128 and stride sC2 = 64. We set the selected block count = 96 (including Iinit = 1, Itopk = 63, and Ilocal = 32) for both training and inference. Therefore, the total number of visible tokens is = 6k. We conduct long-sequence finetuning on the pretrained model using 5B tokens, with an initial learning rate of 3e-4 and linear decay to 2.75e-4. The training batches contain sequences from four length intervals: 0-4k, 4-12k, 12-24k, and 24-32k, with token counts in 1:1:1:1 ratio. Baselines. We finetune baseline model with full attention, marked as FULLATTN, using the same training configuration as INFLLM-V2 (SPARSE). We then apply several typical training-free sparse attention methods on FULLATTN as baselines, including InfLLM (Xiao et al., 2024a) and MInference (Jiang et al., 2024). In addition, we present the results of SHORT with YaRN (Peng et al., 2023) to extend the context window size. In terms of trainable sparse attention, we compare with NSA (Yuan et al., 2025). By using the same training settings as in INFLLM-V2 (SPARSE), we finetune our pretrained model into an NSA version. We initialize NSAs three sets of KV parameters by replicating the original KV parameters in dense attention. As NSA does not publish their code, we adopt an open-source Triton implementation of NSA for experiments2. Table 1: Task Performance on RULER. Best results in sparse attention are bolded. Method FULLATTN SHORT+YARN INFLLM MINFERENCE NSA INFLLM-V2 (SPARSE) w/ LSE Approx w/o LSE Approx INFLLM-V2 (DENSE) SG1 SG2 SG3 MK1 MK2 MK3 MV MQ VT CWE FWE QA1 QA2 Avg. 100.00 100. 100.00 96.00 94.00 92.00 82.00 98. 93.20 44.40 91.33 48.00 56.00 84. 98.00 98.00 100.00 100.00 68.00 6.00 100.00 88.00 50.00 4.00 100.00 82.00 46.00 10.00 76.00 54.00 6.00 10.00 36.00 38.00 0.00 10.00 46.00 30. 32.00 9.00 79.50 59.00 31.50 7.50 93.50 61.50 36.00 70.00 88.00 56.00 21.40 16.00 64.20 34.40 87.33 80.67 92.67 86.00 26.00 18.00 32.00 56. 26.00 24.00 44.00 34.00 40.63 27.94 73.22 59.92 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 100.00 94.00 92.00 94. 82.00 80.00 98.00 62.00 64.00 98.00 98.50 98.50 99.00 94.50 95.50 98.00 98.00 98.00 98.40 50.40 47.80 52. 82.67 81.33 90.00 72.00 70.00 76.00 40.00 40.00 44.00 82.62 82.09 88.32 2https://github.com/XunhaoLai/native-sparse-attention-triton Preprint. Under review Table 2: Task Performance on LongBench and LongPPL. Best results in sparse attention are bolded."
        },
        {
            "title": "FULLATTN",
            "content": "SHORT+YARN INFLLM MINFERENCE NSA INFLLM-V2 (SPARSE) INFLLM-V2 (DENSE) LongBench LongPPL 42.30 2.06 37.86 5. 32.30 12.01 41.55 2.62 37.10 4.24 42.54 2.12 42.49 2.00 For all the above sparse attention methods, we maintain the same sparsity level to ensure fair comparison. We provide the training curve for trainable methods in Figure 5. NSA causes disruption in the loss, while INFLLM-V2 is closer to FULLATTN."
        },
        {
            "title": "4.2 TASK PERFORMANCE",
            "content": "In this section, we evaluate InfLLM-V2 and other baselines across various tasks. Notably, while the original NSA paper demonstrates performance comparable to full attention when training on long sequences from scratch, NSA fails to achieve satisfactory results in short-to-long adaptation settings. This indicates that the substantial parameter overhead introduced by NSA renders it unsuitable for the conventional pretraining-on-short, finetuning-on-long paradigm. Long-Context Understanding. To evaluate InfLLM-V2s performance on long-input tasks, we compare InfLLM-V2 and different baselines on RULER (Hsieh et al., 2024), LongBench (Bai et al., 2024) and LongPPL (Fang et al., 2025). RULER is synthetic dataset with configurable average length. LongBench is bilingual benchmark for long-context understanding. Compared to RULER, LongBench is primarily built from existing, real-world datasets. LongPPL is perplexity evaluation benchmark for long sequences. The experimental results of RULER when the length is 32k are shown in Table 1. The results on LongBench and LongPPL are shown in Table 2. Please refer to Appendix for detailed performance of the sub-tasks in LongBench. From the results, we can observe that: 1) INFLLM-V2 achieves the best performance compared to other sparse methods, with its results being highly competitive and closely matching the strong, FULLATTN baseline. Alternative approaches, whether applying training-free sparsity or training-based sparsity, result in substantial drop in performance. 2) Compared to NSA, INFLLM-V2 can achieve significant performance improvements through minimal finetuning on long-sequences. Although NSA has low training loss, its high perplexity on the LongPPL evaluations indicates that NSA has not adequately learned longrange dependencies. 3) unique advantage of INFLLM-V2 is the flexibility to seamlessly switch between dense mode and sparse mode. This flexibility not only provides an option for dense computation but can also lead to further improvement in performance, surpassing even the full attention baseline. 4) Furthermore, the INFLLM-V2 (SPARSE) variant with LSE Approximation does not lose any performance, confirming the effectiveness of our acceleration technique. 86."
        },
        {
            "title": "FULLATTN",
            "content": "MATH-500 AIME 24 AIME 25 LCB v5 LCB v6 Avg. Table 3: Task Performance on Long Reasoning Tasks. Long Reasoning. To evaluate the performance of InfLLM-V2 in long-output scenarios, we compared several major Long Reaincluding soning MATH-500 (Hendrycks et al., 2021b), AIME (MAA), and LiveCodeBench (LCB) (Jain et al., 2025). We finetune InfLLM-V2 and baselines on OpenMathReasoning (Moshkov et al., 2025) and OpenCodeReasoning (Ahmad et al., 2025). As InfLLM and MInference primarily accelerate long-input processing, we exclude them from this long-output evaluation. The experimental results are shown in Table 3. The results show that InfLLM-V2 attains performance on par with full attention, confirming its effectiveness for long-output scenarios. NSA InfLLM-V2 (Sparse) InfLLM-V2 (Dense) 25.15 29.94 29.94 28.75 38.33 36. 23.54 29.38 23.33 25.14 27.83 26.29 37.28 42.66 40.53 83.80 87.80 86.40 tasks, 37. 29.14 42.79 30.63 30.67 General Tasks. We verify that the InfLLM-V2 architecture can freely switch back to Dense mode without performance degradation on short-sequence tasks after long-sequence fine-tuning. Zeroshot evaluations on MMLU (Hendrycks et al., 2021a), MMLU-Redux (Gema et al., 2025), CEval (Huang et al., 2023), MATH-500 (Hendrycks et al., 2021b), HumanEval (Chen et al., 2021), MBPP (Austin et al., 2021) and BBH (Suzgun et al., 2023) are shown in Table 4. Experimental results show that InfLLM-V2 achieves performance comparable to full attention."
        },
        {
            "title": "4.3 EFFICIENCY",
            "content": "We first evaluate the efficiency of our kernel implementation on NVIDIA A100 and NVIDIA 4090. We evaluate InfLLM-V2s inference efficiency on the batch=1 setting. We select FlashAttention8 Preprint. Under review Method SHORT FULLATTN NSA InfLLM-V2 (Dense) Table 4: Task Performance on General Tasks. MMLU MMLU-Redux CEval MATH-500 HumanEval MBPP BBH Avg. 72.73 73.38 68.27 71. 72.71 70.24 66.39 69.73 76.17 78.11 74.33 77.70 54.40 54.60 44.40 54. 70.73 71.34 62.20 73.17 75.49 75.10 65.00 73.54 51.90 49.13 43.81 47. 67.73 67.41 60.63 66.76 Figure 6: Speed of the kernels on NVIDIA A100 and NVIDIA 4090. 2 (Dao, 2024) implementation for full attention. For fair efficiency comparison with NSA, we ignore its sliding attention component, and compare solely on the compression and sparse attention parts by selecting an equal number of blocks I. Experiment results are shown in Figure 6. When the number of selected blocks is 16, InfLLM-V2 achieves up to 7.4 over FlashAttention on A100 and 9.3 on 4090. In contrast, NSAs speedup is limited to 3.5 in the same setting. The breakdown of the execution time shows that the overhead from the Block Selection stage is greatly optimized by our efficient implementation in Section 3.4. We further conduct an ablation study on the Block Selection, as shown in Table 5, which shows the effectiveness of our proposed LSE Approximation. Table 5: Ablation study of Block Selection efficiency, with and without LSE Approximation. All measurements are in time (ms), and the number of selected blocks is set to 16."
        },
        {
            "title": "Sequence Length",
            "content": "w/o LSE Approximation w/ LSE Approximation A100 4090 32k 4.67 3.93 64k 96k 18.20 14.07 42.46 32.44 128k 75.36 56.59 32k 4.89 3.70 64k 96k 19.95 14.39 46.51 33.16 128k 83.26 59.04 The end-to-end inference speed (with = 96 and W4A16 quantization (Frantar et al., 2025)) is shown in Figure 7. InfLLM-V2 can achieve 2.13 prefilling speedup and 2.32 decoding speedup. Since InfLLM-V2 does not accelerate the Feed-Forward Network (FFN) layers, higher speedup ratio can be achieved by incorporating FFN-specific acceleration techniques in future work. Figure 7: End-to-end inference speed of our 8B model when the number of visible tokens is 6k. TTFT means time-to-first-token, and TPOT means time-per-output-token. 9 Preprint. Under review"
        },
        {
            "title": "5 CONCLUSION",
            "content": "In this paper, we introduced InfLLM-V2, dense-sparse switchable attention framework designed to overcome the limitations of existing trainable sparse attention mechanisms. By ensuring architectural alignment with the standard pretrain-on-short and finetune-on-long workflow, InfLLM-V2 facilitates seamless and efficient sparse adaptation to long contexts without requiring extra parameters or causing disruptive distributional shifts. We believe InfLLM-V2 offers practical and powerful solution for advancing the capabilities of large language models in the long-context era."
        },
        {
            "title": "REFERENCES",
            "content": "Wasi Uddin Ahmad, Sean Narenthiran, Somshubra Majumdar, Aleksander Ficek, Siddhartha Jain, Jocelyn Huang, Vahid Noroozi, and Boris Ginsburg. Opencodereasoning: Advancing data distillation for competitive coding. arXiv preprint arXiv:2504.01943, 2025. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 48954901, 2023. Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie Cai, Michael Terry, Quoc Le, et al. Program synthesis with large language models. arXiv preprint arXiv:2108.07732, 2021. Yushi Bai, Xin Lv, Jiajie Zhang, Hongchang Lyu, Jiankai Tang, Zhidian Huang, Zhengxiao Du, Xiao Liu, Aohan Zeng, Lei Hou, et al. Longbench: bilingual, multitask benchmark for long context understanding. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 31193137, 2024. Iz Beltagy, Matthew Peters, and Arman Cohan. Longformer: The long-document transformer. arXiv preprint arXiv:2004.05150, 2020. Rishi Bommasani, Drew Hudson, Ehsan Adeli, Russ Altman, Simran Arora, Sydney von Arx, Michael Bernstein, Jeannette Bohg, Antoine Bosselut, Emma Brunskill, et al. On the opportunities and risks of foundation models. Preprint, 2021. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pp. 1877 1901, 2020. Guoxuan Chen, Han Shi, Jiawei Li, Yihang Gao, Xiaozhe Ren, Yimeng Chen, Xin Jiang, Zhenguo Li, Weiyang Liu, and Chao Huang. Sepllm: Accelerate large language models by compressing one segment into one separator. In Forty-second International Conference on Machine Learning, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Rewon Child, Scott Gray, Alec Radford, and Ilya Sutskever. Generating long sequences with sparse transformers. arXiv preprint arXiv:1904.10509, 2019. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In The Twelfth International Conference on Learning Representations, 2024. Daya DeepSeek, Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, and Yisen Wang. What is wrong with perplexity for long-context language modeling? In The Thirteenth International Conference on Learning Representations, 2025. 10 Preprint. Under review Elias Frantar, Roberto Castro, Jiale Chen, Torsten Hoefler, and Dan Alistarh. Marlin: Mixedprecision auto-regressive parallel inference on large language models. In Proceedings of the 30th ACM SIGPLAN Annual Symposium on Principles and Practice of Parallel Programming, pp. 239251, 2025. Yizhao Gao, Zhichen Zeng, Dayou Du, Shijie Cao, Peiyuan Zhou, Jiaxing Qi, Junjie Lai, Hayden Kwok-Hay So, Ting Cao, Fan Yang, et al. Seerattention: Learning intrinsic sparse attention in your llms. arXiv preprint arXiv:2410.13276, 2024. Aryo Pradipta Gema, Joshua Ong Jun Leang, Giwon Hong, Alessio Devoto, Alberto Carlo Maria Mancino, Rohit Saxena, Xuanli He, Yu Zhao, Xiaotang Du, Mohammad Reza Ghasemi Madani, et al. Are we done with mmlu? In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 50695096, 2025. Chi Han, Qifan Wang, Hao Peng, Wenhan Xiong, Yu Chen, Heng Ji, and Sinong Wang. Lm-infinite: Zero-shot extreme length generalization for large language models. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pp. 39914008, 2024. Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao Liu, Yuqi Huo, Jiezhong Qiu, Yuan Yao, Ao Zhang, Liang Zhang, et al. Pre-trained models: Past, present and future. AI Open, 2:225250, 2021. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. In International Conference on Learning Representations, 2021a. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021b. Cheng-Ping Hsieh, Simeng Sun, Samuel Kriman, Shantanu Acharya, Dima Rekesh, Fei Jia, and Boris Ginsburg. Ruler: Whats the real context size of your long-context language models? In First Conference on Language Modeling, 2024. Shengding Hu, Yuge Tu, Xu Han, Ganqu Cui, Chaoqun He, Weilin Zhao, Xiang Long, Zhi Zheng, Yewei Fang, Yuxiang Huang, et al. Minicpm: Unveiling the potential of small language models with scalable training strategies. In First Conference on Language Modeling, 2024. Yuxiang Huang, Binhang Yuan, Xu Han, Chaojun Xiao, and Zhiyuan Liu. Locret: Enhancing eviction in long-context llm inference with trained retaining heads on consumer-grade devices. arXiv preprint arXiv:2410.01805, 2024. Yuxiang Huang, Mingye Li, Xu Han, Chaojun Xiao, Weilin Zhao, Ao Sun, Hao Zhou, Jie Zhou, Zhiyuan Liu, and Maosong Sun. APB: Accelerating distributed long-context inference by passing In Proceedings of the 63rd Annual Meeting of the compressed context blocks across GPUs. Association for Computational Linguistics (Volume 1: Long Papers), pp. 1070810727, 2025. Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Yao Fu, et al. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. Advances in Neural Information Processing Systems, 36: 6299163010, 2023. Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free In The Thirteenth International Conference on evaluation of large language models for code. Learning Representations, 2025. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. Advances in Neural Information Processing Systems, 37:5248152515, 2024. 11 Preprint. Under review Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2023. Nikita Kitaev, Lukasz Kaiser, and Anselm Levskaya. Reformer: The efficient transformer. In International Conference on Learning Representations, 2020. Xunhao Lai, Jianqiao Lu, Yao Luo, Yiyuan Ma, and Xun Zhou. Flexprefill: context-aware sparse attention mechanism for efficient long-sequence inference. In The Thirteenth International Conference on Learning Representations, 2025. Yuhong Li, Yingbing Huang, Bowen Yang, Bharat Venkitesh, Acyr Locatelli, Hanchen Ye, Tianle Cai, Patrick Lewis, and Deming Chen. Snapkv: Llm knows what you are looking for before generation. Advances in Neural Information Processing Systems, 37:2294722970, 2024. Anton Lozhkov, Raymond Li, Loubna Ben Allal, Federico Cassano, Joel Lamy-Poirier, Nouamane Tazi, Ao Tang, Dmytro Pykhtar, Jiawei Liu, Yuxiang Wei, et al. Starcoder 2 and the stack v2: The next generation. arXiv preprint arXiv:2402.19173, 2024. Enzhe Lu, Zhejun Jiang, Jingyuan Liu, Yulun Du, Tao Jiang, Chao Hong, Shaowei Liu, Weiran He, Enming Yuan, Yuzhi Wang, et al. Moba: Mixture of block attention for long-context llms. arXiv preprint arXiv:2502.13189, 2025. MAA. American invitational mathematics examination-aime. URL https://maa.org/ maa-invitational-competitions/. Ivan Moshkov, Darragh Hanley, Ivan Sorokin, Shubham Toshniwal, Christof Henkel, Benedikt Schifferer, Wei Du, and Igor Gitman. Aimo-2 winning solution: Building state-of-the-art mathematical reasoning models with openmathreasoning dataset. arXiv preprint arXiv:2504.16891, 2025. OpenAI. GPT-4 technical report. Preprint, 2023. Aaron OpenAI, Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. Guilherme Penedo, Hynek Kydlıˇcek, Anton Lozhkov, Margaret Mitchell, Colin Raffel, Leandro Von Werra, Thomas Wolf, et al. The fineweb datasets: Decanting the web for the finest text data at scale. Advances in Neural Information Processing Systems, 37:3081130849, 2024. Bowen Peng, Jeffrey Quesnelle, Honglu Fan, and Enrico Shippole. Yarn: Efficient context window extension of large language models. In The Twelfth International Conference on Learning Representations, 2023. Aurko Roy, Mohammad Saffar, Ashish Vaswani, and David Grangier. Efficient content-based sparse attention with routing transformers. Transactions of the Association for Computational Linguistics, 9:5368, 2021. Yutao Sun, Zhenyu Li, Yike Zhang, Tengyu Pan, Bowen Dong, Yuyi Guo, and Jianyong Wang. Efficient attention mechanisms for large language models: survey. arXiv preprint arXiv:2507.19595, 2025. Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc Le, Ed Chi, Denny Zhou, et al. Challenging big-bench tasks and whether chain-of-thought can solve them. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 1300313051, 2023. Jiaming Tang, Yilong Zhao, Kan Zhu, Guangxuan Xiao, Baris Kasikci, and Song Han. Quest: Query-aware sparsity for efficient long-context llm inference. In Forty-first International Conference on Machine Learning, 2024. Yi Tay, Mostafa Dehghani, Dara Bahri, and Donald Metzler. Efficient transformers: survey. ACM Comput. Surv., 55(6), 2022. ISSN 0360-0300. doi: 10.1145/3530811. 12 Preprint. Under review Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. Lei Wang, Chen Ma, Xueyang Feng, Zeyu Zhang, Hao Yang, Jingsen Zhang, Zhiyuan Chen, Jiakai Tang, Xu Chen, Yankai Lin, et al. survey on large language model based autonomous agents. Frontiers of Computer Science, 18(6):186345, 2024. Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. Infllm: Training-free long-context extrapolation for llms with an efficient context memory. Advances in Neural Information Processing Systems, 37:119638119661, 2024a. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. In The Twelfth International Conference on Learning Representations, 2024b. Guangxuan Xiao, Jiaming Tang, Jingwei Zuo, Shang Yang, Haotian Tang, Yao Fu, Song Han, et al. In The Duoattention: Efficient long-context llm inference with retrieval and streaming heads. Thirteenth International Conference on Learning Representations, 2025. Renjun Xu and Jingwen Peng. comprehensive survey of deep research: Systems, methodologies, and applications. arXiv preprint arXiv:2506.12594, 2025. Ruyi Xu, Guangxuan Xiao, Haofeng Huang, Junxian Guo, and Song Han. Xattention: Block sparse attention with antidiagonal scoring. In Forty-second International Conference on Machine Learning, 2025. John Yang, Kilian Lieret, Carlos Jimenez, Alexander Wettig, Kabir Khandpur, Yanzhe Zhang, Binyuan Hui, Ofir Press, Ludwig Schmidt, and Diyi Yang. Swe-smith: Scaling data for software engineering agents. arXiv preprint arXiv:2504.21798, 2025. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. Manzil Zaheer, Guru Guruganesh, Kumar Avinava Dubey, Joshua Ainslie, Chris Alberti, Santiago Ontanon, Philip Pham, Anirudh Ravula, Qifan Wang, Li Yang, et al. Big bird: Transformers for longer sequences. Advances in neural information processing systems, 33:1728317297, 2020. Jintao Zhang, Rundong Su, Chunyu Liu, Jia Wei, Ziteng Wang, Pengle Zhang, Haoxu Wang, Huiqiang Jiang, Haofeng Huang, Chendong Xiang, et al. survey of efficient attention methods: Hardware-efficient, sparse, compact, and linear attention. 2025a. Jintao Zhang, Chendong Xiang, Haofeng Huang, Haocheng Xi, Jun Zhu, Jianfei Chen, et al. Spargeattention: Accurate and training-free sparse attention accelerating any model inference. In Forty-second International Conference on Machine Learning, 2025b. Zhenyu Zhang, Ying Sheng, Tianyi Zhou, Tianlong Chen, Lianmin Zheng, Ruisi Cai, Zhao Song, Yuandong Tian, Christopher Re, Clark Barrett, et al. H2o: Heavy-hitter oracle for efficient generative inference of large language models. Advances in Neural Information Processing Systems, 36:3466134710, 2023. Yuxiang Zheng, Dayuan Fu, Xiangkun Hu, Xiaojie Cai, Lyumanshan Ye, Pengrui Lu, and Pengfei Liu. Deepresearcher: Scaling deep research via reinforcement learning in real-world environments. arXiv preprint arXiv:2504.03160, 2025. 13 Preprint. Under review"
        },
        {
            "title": "A IMPLEMENTATION DETAIL",
            "content": "We have shown the implementation of Block Selection in Section 3.4. We show the implementation detail of Sparse Attention here in Algorithm 2. Algorithm 2 Computation of Sparse Attention. (Suppose hkv = 1 without loss of generality.) Require: RnGdh , K, Rndh . Block sizes Bk. Divide into blocks Q1, . . . , Qn of size dh each. Divide K, into Tk = n/Bk blocks K1, . . . , KTk and V1, . . . , VTk of size Bk dh each. Divide RnGdh into blocks of size dh each. Divide the log-sum-exp lse into blocks of size each. for = 1, . . . , (parallel) do Load Qi from HBM to on-chip SRAM. On chip, initialize O(0) , ℓ(0) = (0)Gdh for = 1, . . . , Tk (sequential) do = (0)G, m(0) = ()G. if Kj in visible tokens (determined by the I(i) in Eq. 3) then RGBk . = max(m(j1) Load Kj, Vj from HBM to on-chip SRAM. On chip, compute attention scores Sij = QiK On chip, compute m(j) On chip, compute Pij = exp(Sij m(j) On chip, compute ℓ(j) On chip, compute O(j) On chip, compute Oi = diag(ℓ(Tk) On chip, compute lsei = m(Tk) Write Oi to HBM as the i-th block of O. Write lsei to HBM as the i-th block of lse. return the output and the log-sum-exp lse. , rowmax(Sij)) RG. ) RGBk . )ℓ(j1) = exp(m(j1) m(j) m(j) = diag(exp(m(j1) )1O(Tk) . + log(ℓ(Tk) ). i + rowsum( Pij) RG. ))1O(j1) + PijVj. Algorithm 3 Computation of Dense Attention. (Suppose hkv = 1 without loss of generality.) Require: RnGdh , K, Rndh . Block sizes Bq, Bk. Divide into Tq = n/Bq blocks Q1, . . . , QTq of size Bq dh each. Divide K, into Tk = n/Bk blocks K1, . . . , KTk and V1, . . . , VTk of size Bk dh each. Divide RnGdh into Tq blocks of size Bq dh each. Divide the log-sum-exp lse into Tq blocks of size Bq each. for = 1, . . . , Tq (parallel) do Load Qi from HBM to on-chip SRAM. On chip, initialize O(0) for = 1, . . . , Tk (sequential) do = (0)Bq dh , ℓ(0) = (0)Bq , m(0) = ()Bq . Load Kj, Vj from HBM to on-chip SRAM. On chip, compute attention scores Sij = QiK On chip, compute m(j) On chip, compute Pij = exp(Sij m(j) On chip, compute ℓ(j) On chip, compute O(j) , rowmax(Sij)) RBq . ) RBq Bk . = exp(m(j1) m(j) )ℓ(j1) m(j) = diag(exp(m(j1) = max(m(j1) RBq Bk . ))1O(j1) i + rowsum( Pij) RBq . + PijVj. On chip, compute Oi = diag(ℓ(Tk) On chip, compute lsei = m(Tk) Write Oi to HBM as the i-th block of O. Write lsei to HBM as the i-th block of lse. return the output and the log-sum-exp lse. )1O(Tk) . + log(ℓ(Tk) ). Similar to FlashAttention (Dao, 2024), the algorithm divides the input into blocks. The differences are: 1) The FlashAttention block size Bk of K, should divide the sparse attention block size B. That is, should be multiple of Bk. 2) The FlashAttention block of typically contains single attention head and multiple tokens. We follow NSA (Yuan et al., 2025) to make it contain group of attention heads of single token, so that they can share the same sparse pattern. 3) The inner loop of the FlashAttention iterates over all blocks of K, whereas our methods loop only covers the visible 14 Preprint. Under review blocks of the sparse attention. We also show the FlashAttention implementation of Dense Attention to Algorithm 3 for reference."
        },
        {
            "title": "B BENCHMARK DETAILS",
            "content": "We provide the detailed results of the LongBench benchmark, mentioned in Table 2, in Table 6. Following LongBench (Bai et al., 2024), the Overall score is computed by the macro-average over the six task categories. Table 6: Task Performance on LongBench. Best results in sparse attention are bolded. Category FULLATTN SHORT + YARN INFLLM MINFERENCE NSA INFLLM-V2 (SPARSE) INFLLM-V2 (DENSE) Single-Doc QA Multi-Doc QA Summary Few-shot Learning Synthetic Task NarQA Qasper MFQA-en MFQA-zh HotpotQA 2WikiQA MuSiQue Dureader GovReport QMSum MultiNews VCSUM TREC TriviaQA SAMSum LSHT PsgCount PsgRe-en PsgRe-zh Code LCC RepoBen-P Overall 21.38 43.80 55.07 57.26 50.13 39.54 24.68 33. 32.17 24.35 26.70 16.37 45.00 84.35 40.26 37.75 4.00 86.50 90.50 35.72 35.00 42.30 18.17 30.98 43.81 54. 48.49 32.71 23.22 33.00 31.93 22.45 26.46 16.55 65.50 85.67 42.92 38.00 4.06 20.75 42.00 58.65 43.93 37. 21.02 34.92 49.39 51.75 44.03 30.58 17.85 33.01 21.40 20.96 22.90 17.81 61.00 75.78 37.46 24.57 3.00 19.00 43.00 31.35 30. 32.30 20.16 44.51 54.83 57.00 48.00 36.22 22.87 33.94 32.21 25.05 26.50 16.17 43.50 81.93 39.81 35.75 3.50 85.00 90. 35.91 34.17 41.55 18.34 39.96 51.35 59.06 46.78 35.33 16.97 33.62 28.72 23.81 25.02 19.12 23.50 83.95 38.47 25. 3.50 66.00 68.00 33.83 34.95 37.10 20.75 45.29 53.53 59.33 54.11 37.86 21.74 33.39 30.33 24.58 25.71 16. 22.50 84.22 40.69 22.01 5.00 92.00 90.50 44.73 44.62 42.54 21.03 45.29 53.54 59.64 54.07 37.86 21.24 33. 30.38 24.35 25.75 16.20 24.00 84.22 40.51 21.47 4.50 91.00 90.50 44.73 44.76 42."
        }
    ],
    "affiliations": [
        "Harbin Institute of Technology",
        "OpenBMB",
        "Tsinghua University"
    ]
}