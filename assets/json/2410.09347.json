{
    "paper_title": "Toward Guidance-Free AR Visual Generation via Condition Contrastive Alignment",
    "authors": [
        "Huayu Chen",
        "Hang Su",
        "Peize Sun",
        "Jun Zhu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Classifier-Free Guidance (CFG) is a critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying different modalities for visual AR. Motivated by language model alignment methods, we propose \\textit{Condition Contrastive Alignment} (CCA) to facilitate guidance-free AR visual generation with high performance and analyze its theoretical connection with guided sampling methods. Unlike guidance methods that alter the sampling process to achieve the ideal sampling distribution, CCA directly fine-tunes pretrained models to fit the same distribution target. Experimental results show that CCA can significantly enhance the guidance-free performance of all tested models with just one epoch of fine-tuning ($\\sim$ 1\\% of pretraining epochs) on the pretraining dataset, on par with guided sampling methods. This largely removes the need for guided sampling in AR visual generation and cuts the sampling cost by half. Moreover, by adjusting training parameters, CCA can achieve trade-offs between sample diversity and fidelity similar to CFG. This experimentally confirms the strong theoretical connection between language-targeted alignment and visual-targeted guidance methods, unifying two previously independent research fields. Code and model weights: https://github.com/thu-ml/CCA."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 2 1 ] . [ 1 7 4 3 9 0 . 0 1 4 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "TOWARD GUIDANCE-FREE AR VISUAL GENERATION VIA CONDITION CONTRASTIVE ALIGNMENT Huayu Chen1, Hang Su1, Peize Sun2, Jun Zhu1 1Tsinghua University chenhuay17@gmail.com 2The University of Hong Kong"
        },
        {
            "title": "ABSTRACT",
            "content": "Classifier-Free Guidance (CFG) is critical technique for enhancing the sample quality of visual generative models. However, in autoregressive (AR) multi-modal generation, CFG introduces design inconsistencies between language and visual content, contradicting the design philosophy of unifying different modalities for visual AR. Motivated by language model alignment methods, we propose Condition Contrastive Alignment (CCA) to facilitate guidance-free AR visual generation with high performance and analyze its theoretical connection with guided sampling methods. Unlike guidance methods that alter the sampling process to achieve the ideal sampling distribution, CCA directly fine-tunes pretrained models to fit the same distribution target. Experimental results show that CCA can significantly enhance the guidance-free performance of all tested models with just one epoch of fine-tuning (1% of pretraining epochs) on the pretraining dataset, on par with guided sampling methods. This largely removes the need for guided sampling in AR visual generation and cuts the sampling cost by half. Moreover, by adjusting training parameters, CCA can achieve trade-offs between sample diversity and fidelity similar to CFG. This experimentally confirms the strong theoretical connection between language-targeted alignment and visual-targeted guidance methods, unifying two previously independent research fields. Code and model weights: https://github.com/thu-ml/CCA. (a) LlamaGen (b) VAR Figure 1: CCA significantly improves guidance-free sample quality for AR visual generative models with just one epoch of fine-tuning on the pretraining dataset."
        },
        {
            "title": "INTRODUCTION",
            "content": "Witnessing the scalability and generalizability of autoregressive (AR) models in language domains, recent works have been striving to replicate similar success for visual generation (Esser et al., 2021; Lee et al., 2022). By quantizing images into discrete tokens, AR visual models can process images using the same next-token prediction approach as Large Language Models (LLMs). This approach is attractive because it provides potentially unified framework for vision and language, promoting consistency in reasoning and generation across modalities (Team, 2024; Xie et al., 2024). Despite the design philosophy of maximally aligning visual modeling with language modeling methods, AR visual generation still differs from language generation in notable aspect. AR visual"
        },
        {
            "title": "Preprint",
            "content": "generation relies heavily on Classifier-Free Guidance (CFG) (Ho & Salimans, 2022), sampling technique unnecessary for language generation, which has caused design inconsistencies between the two types of content. During sampling, while CFG helps improve sample quality by contrasting conditional and unconditional models, it requires two model inferences per visual token, which doubles the sampling cost. During training, CFG requires randomly masking text conditions to learn the unconditional distribution, preventing the simultaneous training of text tokens (Team, 2024). In contrast to visual generation, LLMs rarely rely on guided sampling. Instead, the surge of LLMs instruction-following abilities has largely benefited from fine-tuning-based alignment methods (Schulman et al., 2022). Motivated by this observation, we seek to study: Can we avoid guided sampling in AR visual generation, but attain similar effects by directly fine-tuning pretrained models? In this paper, we derive Condition Contrastive Alignment (CCA) for enhancing visual AR performance without guided sampling. Unlike CFG which necessitates altering the sampling process to achieve more desirable sampling distribution, CCA directly fine-tunes pretrained AR models to fit the same distribution target, leaving the sampling scheme untouched. CCA is quite convenient to use since it does not rely on any additional datasets beyond the pretraining data. Our method functions by contrasting positive and negative conditions for given image, which can be easily created from the existing pretraining dataset as matched or mismatched image-condition pairs. CCA is also highly efficient given its fine-tuning nature. We observe that our method achieves ideal performance within just one training epoch, indicating negligible computational overhead (1% of pretraining). In Sec. 4, we highlight theoretical connection between CCA and guided sampling techniques (Dhariwal & Nichol, 2021; Ho & Salimans, 2022). Essentially these methods all target at the same sampling distribution. The distributional gap between this target distribution and pretrained models is related to physical quantity termed conditional residual (log p(xc) p(x) ). Guidance methods typically train an additional model (e.g., unconditional model or classifier) to estimate this quantity and enhance pretrained models by altering their sampling process. Contrastively, CCA follows LLM alignment techniques (Rafailov et al., 2023; Chen et al., 2024a) and parameterizes the conditional residual with the difference between our target model and the pretrained model, thereby directly training sampling model. This analysis unifies language-targeted alignment and visual-targeted guidance methods, bridging the gap between the two previously independent research fields. We apply CCA to two state-of-the-art autoregressive (AR) visual models, LLamaGen (Sun et al., 2024) and VAR (Tian et al., 2024), which feature distinctly different visual tokenization designs. Both quantitative and qualitative results show that CCA significantly and consistently enhances the guidance-free sampling quality across all tested models, achieving performance levels comparable to CFG (Figure 1). We further show that by varying training hyperparameters, CCA can realize controllable trade-off between image diversity and fidelity similar to CFG. This further confirms their theoretical connections. We also compare our method with some existing LLM alignment methods (Welleck et al., 2019; Rafailov et al., 2023) to justify its algorithm design. Finally, we demonstrate that CCA can be combined with CFG to further improve performance. Our contributions: 1. We take big step toward guidance-free visual generation by significantly improving the visual quality of AR models. 2. We reveal theoretical connection between alignment and guidance methods. This shows that language-targeted alignment can be similarly applied to visual generation and effectively replace guided sampling, closing the gap between these two fields."
        },
        {
            "title": "2 BACKGROUND",
            "content": "2.1 AUTOREGRESSIVE (AR) VISUAL MODELS Autoregressive models. Consider data represented by sequence of discrete tokens x1:N := {x1, x2, ..., xN }, where each token xn is an integer. Data probability p(x) can be decomposed as: p(x) = p(x1) (cid:89) n=2 p(xnx<n). (1) AR models thus aim to learn pϕ(xnx<n) p(xnx<n), where each token xn is conditioned only on its previous input x<n. This is known as next-token prediction (Radford et al., 2018)."
        },
        {
            "title": "Preprint",
            "content": "Visual tokenization. Image pixels are continuous values, making it necessary to use vectorquantized tokenizers for applying discrete AR models to visual data (Van Den Oord et al., 2017; Esser et al., 2021). These tokenizers are trained to encode images into discrete token sequences x1:N and decode them back by minimizing reconstruction losses. In our work, we utilize pretrained and frozen visual tokenizers, allowing AR models to process images similarly to text."
        },
        {
            "title": "2.2 GUIDED SAMPLING FOR VISUAL GENERATION",
            "content": "Despite the core motivation of developing unified model for language and vision, the AR sampling strategies for visual and text contents differ in one key aspect: AR visual generation necessitates sampling technique named Classifier-Free Guidance (CFG) (Ho & Salimans, 2022). During inference, CFG adjusts the sampling logits ℓsample for each token as: ℓsample = ℓc + s(ℓc ℓu), (2) where ℓc and ℓu are the conditional and unconditional logits provided by two separate AR models, pϕ(xc) and pϕ(x). The condition can be class labels or text captions, formalized as prompt tokens. The scalar is termed guidance scale. Since token logits represent the (unnormalized) log-likelihood in AR models, Ho & Salimans (2022) prove that the sampling distribution satisfies: psample(xc) pϕ(xc) (cid:20) pϕ(xc) pϕ(x) (cid:21)s . (3) At = 0, the sampling model becomes exactly the pretrained conditional model pϕ. However, previous works (Ho & Salimans, 2022; Podell et al., 2023; Chang et al., 2023; Sun et al., 2024) have widely observed that an appropriate > 0 is critical for an ideal trade-off between visual fidelity and diversity, making training another unconditional model pϕ necessary. In practice, the unconditional model usually shares parameters with the conditional one, and can be trained concurrently by randomly dropping condition prompts during training. Other guidance methods, such as Classifier Guidance (Ho & Salimans, 2022) and Energy Guidance (Lu et al., 2023) have similar effects of CFG. The target sampling distribution of these methods can all be unified under Eq. 3. 2.3 DIRECT PREFERENCE OPTIMIZATION FOR LANGUAGE MODEL ALIGNMENT Reinforcement Learning from Human Feedback (RLHF) is crucial for enhancing the instructionfollowing ability of pretrained Language Models (LMs) (Schulman et al., 2022; OpenAI, 2023). Performing RL typically requires reward model, which can be learned from human preference data. Formally, the Bradley-Terry preference model (Bradley & Terry, 1952) assumes. p(xw xlc) := er(c,xw) er(c,xl) + er(c,xw) = σ(r(c, xw) r(c, xl)), (4) where xw and xl are respectively the winning and losing response for an instruction c, evaluated by human. r() represents an implicit reward for each response. The target LM πθ should satisfy πθ(xc) µϕ(xc)er(c,x)/β to attain higher implicit reward compared with the pretrained LM µϕ. Direct Preference Optimization (Rafailov et al., 2023) allows us to directly optimize pretrained LMs on preference data, by formalizing rθ(c, x) := β log πθ(xc) β log µϕ(xc): θ = E{c,xwxl} log σ LDPO (cid:18) β log πθ(xwc) µϕ(xwc) β log πθ(xlc) µϕ(xlc) (cid:19) . (5) DPO is more streamlined and thus often more favorable compared with traditional two-stage RLHF pipelines: first training reward models, then aligning LMs with reward models using RL."
        },
        {
            "title": "3 CONDITION CONTRASTIVE ALIGNMENT",
            "content": "Autoregressive visual models are essentially learning parameterized model pϕ(xc) to approximate the standard conditional image distribution p(xc). Guidance algorithms shift the sampling policy"
        },
        {
            "title": "Preprint",
            "content": "psample(xc) away from p(xc) according to Sec. 2.2: psample(xc) p(xc) (cid:20) p(xc) p(x) (cid:21)s . (6) At guidance scale = 0, sampling from psample(xc) = p(xc) pϕ(xc) is most straightforward. However, it is widely observed that an appropriate > 0 usually leads to significantly enhanced sample quality. The cost is that we rely on an extra unconditional model pϕ(x) p(x) for sampling. This doubles the sampling cost and causes an inconsistent training paradigm with language. In this section, we derive simple approach to directly model the same target distribution psample by fine-tuning pretrained models. Specifically, our methods leverage singular loss function for optimizing pϕ(xc) p(xc) to become psample (xc) psample(xc). Despite having similar effects as guided sampling, our approach does not require altering the sampling process. We theoretically derive our method in Sec. 3.1 and discuss its practical implementation in Sec. 3.2. θ"
        },
        {
            "title": "3.1 ALGORITHM DERIVATION",
            "content": "The core difficulty of directly learning psample is that we cannot access datasets under the distribution of psample. However, we observe the distributional difference between psample(xc) and p(xc) is related to simple quantity that can be potentially learned from existing datasets. Specifically, by taking the logarithm of both sides in Eq. 6 and applying some algebra, we have1: θ 1 log psample(xc) p(xc) = log p(xc) p(x) , (7) of which the right-hand side (i.e., log p(xc) probability and unconditional probability for an image x, which we term as conditional residual. p(x) ) corresponds to the log gap between the conditional Our key insight here is that the conditional residual can be directly learned through contrastive learning approaches (Gutmann & Hyvärinen, 2012), as sated below: Theorem 3.1 (Noise Contrastive Estimation, proof in Appendix A). Let rθ be parameterized model which takes in an image-condition pair (x, c) and outputs scalar value rθ(x, c). Consider the loss function: (x, c) = Ep(x,c) log σ(rθ(x, c)) Ep(x)p(c) log σ(rθ(x, c)). Given unlimited model expressivity for rθ, the optimal solution for minimizing LNCE LNCE θ satisfies θ (8) (9) θ (x, c) = log p(xc) p(x) . Now that we have tractable way of learning rθ(x, c) log p(xc) p(x) , the target distribution psample can be jointly defined by rθ(x, c) and the pretrained model pϕ. However, we would still lack an explicitly parameterized model psample if rθ(x, c) is another independent network. To address this problem, we draw inspiration from the widely studied alignment techniques in language models (Rafailov et al., 2023) and parameterize rθ(x, c) with our target model psample (xc) and pϕ(xc) according to Eq. 7: θ Then, the loss function becomes rθ(x, c) := 1 log θ psample θ pϕ(xc) (xc) . θ = Ep(x,c) log σ LCCA During training, psample θ log (xc) (cid:104) 1 psample θ pϕ(xc) 1 is learnable while pretrained pϕ is frozen. psample Ep(x)p(c) log σ (cid:105) (cid:104) θ (10) (11) log (xc) psample θ pϕ(xc) (cid:105) . can be initialized from pϕ. This way we can fit psample with single AR model psample unconditional model for guided sampling. Sampling strategies for psample language model decoding methods, which unifies decoding systems for multi-modal generation. , eliminating the need for training separate are consistent with standard θ θ 1We ignore normalizing constant in Eq. 7 for brevity. more detailed discussion is in Appendix B."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: An overview of the CCA method."
        },
        {
            "title": "3.2 PRACTICAL ALGORITHM",
            "content": "Figure 2 illustrates the CCA method. Specifically, implementing Eq. 11 requires approximating two expectations: one under the joint distribution p(x, c) and the other under the product of its two marginals p(x)p(c). The key difference between these distributions is that in p(x, c), images and conditions are correctly paired. In contrast, and are sampled independently from p(x)p(c), meaning they are most likely mismatched. In practice, we rely solely on the pretraining dataset to estimate LCCA . Consider batch of data pairs {x, c}1:K. We randomly shuffle the condition batch c1:K to become cneg 1:K, where each cneg represents negative condition of image xk, while the original ck is positive one. This results in our training batch {x, c, cneg}1:K. The loss function is θ LCCA θ (xk, ck, cneg ) = log σ (cid:104) β log psample θ pϕ(xkck) (cid:125) (cid:123)(cid:122) (cid:124) relative likelihood for positive conditions (xkck) (cid:105) λ log σ (cid:104) β log (xkcneg psample ) θ pϕ(xkcneg ) (cid:105) , (12) (cid:125) (cid:123)(cid:122) (cid:124) relative likelihood for negative conditions where β and λ are two hyperparameters that can be adjusted. β replaces the guidance scale parameter s, while λ is for controlling the loss weight assigned to negative conditions. The learnable psample is initialized from the pretrained conditional model pϕ, making LCCA fine-tuning loss. θ θ We give an intuitive understanding of Eq. 12. Note that log σ() is monotonically increasing. The first term of Eq. 12 aims to increase the likelihood of an image given positive condition, with similar effect to maximum likelihood training. For mismatched image-condition data, the second term explicitly minimizes its relative model likelihood compared with the pretrained pϕ. We name the above training technique Condition Contrastive Alignment (CCA) due to its contrastive nature in comparing positive and negative conditions with respect to single image. This naming also reflects its theoretical connection with Noise Contrastive Estimation (Theorem 3.1)."
        },
        {
            "title": "4 CONNECTION BETWEEN CCA AND GUIDANCE METHODS",
            "content": "As summarized in Table 1, the key distinction between CCA and guidance methods is how to model log p(xc) p(x) , which defines the distributional gap between the target psample(xc) and p(xc) (Eq. 7). In particular, Classifier Guidance (Dhariwal & Nichol, 2021) leverages Bayes Rule and turn log p(xc) p(x) into conditional posterior: log p(xc) p(x) = log p(cx) log p(c) log pθ(cx) log p(c), where p(cx) is explicitly modeled by classifier pθ(cx), which is trained by standard classification loss. p(c) is regarded as uniform distribution. CFG trains an extra unconditional model pθ(x) to"
        },
        {
            "title": "Preprint",
            "content": "Method Classifier Guidance Classifier-Free Guidance Condition Contrastive Alignment Modeling of log p(xc) p(x) Training loss log pθ(cx) log p(c) maxθ Ep(x,c) log pθ(cx) log pϕ(xc) log pθ(x) maxθ Ep(x) log pθ(x) Sampling policy log pϕ(xc) + log pθ(cx) (1 + s) log pϕ(xc) log pθ(x) β[log psample (xc) log pϕ(xc)] θ minθ LCCA θ log psample θ in Eq. 11 (xc) Extra training cost Sampling cost Applicable area 9% of learning pϕ 1.3 10% of learning pϕ 2 1% of pretraining pϕ 1 Diffusion Diffusion & Autoregressive Autoregressive Table 1: Comparison of CCA (ours) and guidance methods in visual generative models. Computational costs are estimated according to Dhariwal & Nichol (2021) and Ho & Salimans (2022). estimate the unknown part of log p(xc) p(x) : p(xc) p(x) log log pϕ(xc) log pθ(x). Despite their effectiveness, guidance methods all require learning separate model and modified sampling process compared with standard autoregressive decoding. In comparison, CCA leverages Eq. 7 and models log p(xc) p(x) as log p(xc) p(x) β[log psample θ (xc) log pϕ(xc)], which allows us to directly learn psample θ instead of another guidance network. Although CCA and conventional guidance techniques have distinct modeling methods, they all target at the same sampling distribution and thus have similar effects in visual generation. For instance, we show in Sec. 5.2 that CCA offers similar trade-off between sample diversity and fidelity to CFG."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We seek to answer the following questions through our experiments: 1. How effective is CCA in enhancing the guidance-free generation quality of pretrained AR visual models, quantitatively and qualitatively? (Sec. 5.1) 2. Does CCA allow controllable trade-offs between sample diversity (FID) and fidelity (IS) similar to CFG? (Sec. 5.2) 3. How does CCA perform in comparison to alignment algorithms for LLMs? (Sec. 5.3) 4. Can CCA be combined with CFG to further improve performance? (Sec. 5.4) 5.1 TOWARD GUIDANCE-FREE AR VISUAL GENERATION Base model. We experiment on two families of publicly accessible AR visual models, LlamaGen (Sun et al., 2024) and VAR (Tian et al., 2024). Though both are class-conditioned models pretrained on ImageNet, LlamaGen and VAR feature distinctively different tokenizer and architecture designs. LlamaGen focuses on reducing the inductive biases on visual signals. It tokenizes images in the classic raster order and adopts almost the same LLM architecture as Llama (Touvron et al., 2023a). VAR, on the other hand, leverages the hierarchical structure of images and tokenizes them in multiscale, coarse-to-fine manner. VAR adopts GPT-2 architecture but tailors the attention mechanism specifically for visual content. For both works, CFG is default and critical technique. Training setup. We leverage CCA to finetune multiple LlamaGen and VAR models of various sizes on the standard ImageNet dataset. The training scheme and hyperparameters are mostly consistent with the pretraining phase. We report performance numbers after only one training epoch and find this to be sufficient for ideal performance. We fix β = 0.02 in Eq. 12 and select suitable λ for each model. Image resolutions are 384 384 for LlamaGen and 256 256 for VAR. Following the original work, we resize LlamaGen samples to 256 256 whenever required for evaluation."
        },
        {
            "title": "Model",
            "content": "n u D ADM (Dhariwal & Nichol, 2021) LDM-4 (Rombach et al., 2022) U-ViT-H/2 (Bao et al., 2023) DiT-XL/2 (Peebles & Xie, 2023) MDTv2-XL/2 (Gao et al., 2023) MaskGIT (Chang et al., 2022) MAGVIT-v2 (Yu et al., 2023) MAGE (Li et al., 2023) e s e u VQGAN (Esser et al., 2021) ViT-VQGAN (Yu et al., 2021) RQ-Transformer (Lee et al., 2022) LlamaGen-3B (Sun et al., 2024) +CCA (Ours) VAR-d30 (Tian et al., 2024) +CCA (Ours) w/o Guidance w/ Guidance IS Precision Recall FID IS 127.5 103.5 121.5 155. 182.1 200.5 195.8 74.3 175.1 134.0 112.9 276.8 175.6 264.2 0.72 0.71 0.67 0.72 0.80 0.69 0.80 0.75 0.83 0.63 0.62 0.67 0. 0.51 0.67 0.59 0.62 0.56 3.94 3.60 2.29 2.27 1.58 1.78 5.20 3.04 3.80 2.18 1.92 215.8 247.7 263.9 278.2 314. 319.4 280.3 227.4 323.7 263.3 323.1 FID 7.49 10.56 9.62 5.06 6.18 3.65 6.93 15.78 4.17 7.55 9.38 2.69 5.25 2. Table 2: Model comparisons on class-conditional ImageNet 256 256 benchmark. LlamaGen (w/o Guidance) IS=64.7 LlamaGen + CCA (w/o G.) IS=384.6 LlamaGen (w/ CFG) IS=404.0 VAR (w/o Guidance) IS=154.3 VAR + CCA (w/o G.) IS=350. VAR (w/ CFGv2) IS=390.8 Figure 3: CCA and CFG can similarly enhance the sample fidelity of AR visual models. The base models are LlamaGen-L (343M) and VAR-d24 (1.0B). We use = 3.0 for CFG, and β = 0.02, λ = 104 for CCA. Figure 7 and Figure 8 contain more examples. Experimental results. We find CCA significantly improves the guidance-free performance of all tested models (Figure 1), evaluated by metrics like FID (Heusel et al., 2017) and IS (Salimans et al., 2016). For instance, after one epoch of CCA fine-tuning, the FID score of LlamaGen-L (343M) improves from 19.07 to 3.41, and the IS score from 64.3 to 288.2, achieving performance levels"
        },
        {
            "title": "Preprint",
            "content": "Figure 4: CCA can achieve similar FID-IS trade-offs to CFG by adjusting training parameter λ. FID Model LlamaGen-L 19.00 61.69 12.22 3.43 +DPO +Unlearn +CCA IS 64.7 30.8 111.6 288.2 sFID Precision Recall 0.67 0.61 8.78 0.40 0.36 44.98 0.64 0.66 7.99 0.81 7.44 0.52 Model VAR-d24 +DPO +Unlearn +CCA FID 6.20 7.53 5.55 2.63 IS 154.3 232.6 165.9 298.8 sFID Precision Recall 0.62 0.74 8.50 0.85 0.34 19.10 0.61 0.75 8.41 7.63 0.55 0.84 Table 3: Comparision of CCA and LLM alignment algorithms in visual generation. comparable to CFG. This result is compelling, considering that CCA has negligible fine-tunning overhead compared with model pretraining and only half of sampling costs compared with CFG. Figure 3 presents qualitative comparison of image samples before and after CCA fine-tuning. The results clearly demonstrate that CCA can vastly improve image fidelity, as well as class-image alignment of guidance-free samples. Table 2 compares our best-performing models with several state-of-the-art visual generative models. With the help of CCA, we achieve record-breaking FID score of 2.54 and an IS score of 276.8 for guidance-free samples of AR visual models. Although these numbers still somewhat lag behind CFG-enhanced performance, they demonstrate the significant potential of alignment algorithms to enhance visual generation and indicate the future possibility of replacing guided sampling. 5.2 CONTROLLABLE TRADE-OFFS BETWEEN DIVERSITY AND FIDELITY distinctive feature of CFG is its ability to balance diversity and fidelity by adjusting the guidance scale. It is reasonable to expect that CCA can achieve similar trade-off since it essentially targets the same sampling distribution as CFG. Figure 4 confirms this expectation: by adjusting the λ parameter for fine-tuning, CCA can achieve similar FID-IS trade-offs to CFG. The key difference is that CCA enhances guidance-free models through training, while CFG mainly improves the sampling process. It is worth noting that VAR employs slightly different guidance technique from standard CFG, which we refer to as CFGv2. CFGv2 involves linearly increasing the guidance scale during sampling, which was first proposed by Chang et al. (2023) and found beneficial for certain models. The FID-IS curve of CCA more closely resembles that of standard CFG. Additionally, the hyperparameter β also affects CCA performance. Although our algorithm derivation shows that β is directly related to the CFG scale s, we empirically find adjusting β is less effective and less predictable compared with adjusting λ. In practice, we typically fix β and adjust λ. We ablate β in Appendix C. 5.3 CAN LLM ALIGNMENT ALGORITHMS ALSO ENHANCE VISUAL AR? Intuitively, existing preference-based LLM alignment algorithms such as DPO and Unlearning (Welleck et al., 2019) should also offer improvement for AR visual models given their similarity to CCA. However, Table 3 shows that naive applications of these methods fail significantly. DPO. As is described in Eq. 5, one can treat negative image-condition pairs as dispreferred data and positive ones as preferred data to apply the DPO loss. We ablate βd {0.01, 0.1, 1.0, 10.0} and report the best performance in Table 3. Results indicate that DPO fails to enhance pretrained"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: The impact of training parameter λ on the performance of CCA+CFG. Figure 6: Integration of CCA+CFG yields improved performance over CFG alone. models, even causing performance collapse for LlamaGen-L. By inspecting training logs, we find that the likelihood of the positive data continuously decreases during fine-tuning, which may explain the collapse. This phenomenon is well-observed problem of DPO (Chen et al., 2024a; Pal et al., 2024), stemming from its focus on optimizing only the relative likelihood between preferred and dispreferred data, rather than controlling likelihood for positive and negative image-condition pairs separately. We refer interested readers to Chen et al. (2024a) for detailed discussion. Unlearning. Also known as unlikelihood training, this method maximizes log pθ(xc) through standard maximum likelihood training on positive data, while minimizing log pθ(xcneg) to unlearn negative data. training parameter λu controls the weight of the unlearning loss. We find that with small 0.01 λu 0.1, Unlearning provides some benefit, but it is far less effective than CCA. This suggests the necessity of including frozen reference model. 5.4 INTEGRATION OF CCA AND CFG If extra sampling costs and design inconsistencies of CFG are not concerns, could CCA still be helpful? takeaway conclusion is yes: CCA+CFG generally outperforms CFG (Figure 6), but it requires distinct hyperparameter choices compared with CCA-only training. Implementation. After pretraining the unconditional AR visual model by randomly dropping conditions, CFG requires us to also fine-tune the unconditional model during alignment. To achieve this, we follow previous approaches by also randomly replacing data conditions with [MASK] tokens at probability of 10%. These unconditional samples are treated as positive data during CCA training. Comparison of CCA-only and CCA+CFG. They require different hyperparameters. As shown in Figure 5, larger λ is typically needed for optimal FID scores in guidance-free generation. For models optimized for guidance-free sampling, adding CFG guidance does not further reduce the FID score. However, with smaller λ, CCA+CFG could outperform the CFG method."
        },
        {
            "title": "6 RELATED WORKS",
            "content": "Visual generative models. Generative adversarial networks (GANs) (Goodfellow et al., 2014; Brock et al., 2018; Karras et al., 2019; Kang et al., 2023) and diffusion models (Ho et al., 2020;"
        },
        {
            "title": "Preprint",
            "content": "Song & Ermon, 2019; Song et al., 2020; Dhariwal & Nichol, 2021; Kingma & Gao, 2024) are representative modeling methods for visual content generation, widely recognized for their ability to produce realistic and artistic images (Sauer et al., 2022; Ho et al., 2022; Ramesh et al., 2022; Podell et al., 2023). However, because these methods are designed for continuous data like images, they struggle to effectively model discrete data such as text, limiting the development of unified multimodal models for both vision and language. Recent approaches seek to address this by integrating diffusion models with language models (Team, 2024; Li et al., 2024; Zhou et al., 2024). Another line of works (Chang et al., 2022; 2023; Yu et al., 2023; Xie et al., 2024; Ramesh et al., 2021; Yu et al., 2022) explores discretizing images (Van Den Oord et al., 2017; Esser et al., 2021) and directly applying language models such as BERT-style (Devlin et al., 2018) masked-prediction models and GPT-style (Radford et al., 2018) autoregressive models for image generation. Language model alignment. Different from visual generative models which generally enhance sample quality through sampling-based methods (Dhariwal & Nichol, 2021; Ho & Salimans, 2022; Zhao et al., 2022; Lu et al., 2023), LLMs primarily employ training-based alignment techniques to improve instruction-following abilities (Touvron et al., 2023b; OpenAI, 2023). Reinforcement Learning (RL) is well-suited for aligning LLMs with human feedback (Schulman et al., 2017; Ouyang et al., 2022). However, this method requires learning reward model before optimizing LLMs, leading to an indirect two-stage optimization process. Recent developments in alignment techniques (Rafailov et al., 2023; Cai et al., 2023; Azar et al., 2024; Ethayarajh et al., 2024; Chen et al., 2024a; Ji et al., 2024) have streamlined this process. They enable direct alignment of LMs through singular loss. Among all LLM alignment algorithms, our method is perhaps most similar to NCA (Chen et al., 2024a). Both NCA and CCA are theoretically grounded in the NCE framework (Gutmann & Hyvärinen, 2012). Their differences are mainly empirical regarding loss implementations, particularly in how to estimate expectations under the product of two marginal distributions. Visual alignment. Motivated by the success of alignment techniques in LLMs, several studies have also investigated aligning visual generative models with human preferences using RL (Black et al., 2023a; Xu et al., 2024) or DPO (Black et al., 2023b; Wallace et al., 2023). For diffusion models, the application is not straightforward and must rely on some theoretical approximations, as diffusion models do not allow direct likelihood calculation, which is required by most LLM alignment algorithms (Chen et al., 2024b). Moreover, previous attempts at visual alignment have primarily focused on enhancing the aesthetic quality of generated images and necessitate different dataset from the pretrained one. Our work distinguishes itself from prior research by having fundamentally different optimization objective (replacing CFG) and does not rely on any additional data input."
        },
        {
            "title": "7 CONCLUSION",
            "content": "In this paper, we propose Condition Contrastive Alignment (CCA) as fine-tuning algorithm for AR visual generation models. CCA can significantly enhance the guidance-free sample quality of pretrained models without any modification of the sampling process. This paves the way for further development in multimodal generative models and cuts the cost of AR visual generation by half in comparison to CFG. Our research also highlights the strong theoretical connection between language-targeted alignment and visual-targeted guidance methods, facilitating future research of unifying visual modeling and language modeling. ACKNOWLEDGMENTS We thank Fan Bao, Kai Jiang, Xiang Li, and Min Zhao for providing valuable suggestions. We thank Keyu Tian and Kaiwen Zheng for the discussion."
        },
        {
            "title": "REFERENCES",
            "content": "Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele Calandriello. general theoretical paradigm to understand learning from In International Conference on Artificial Intelligence and Statistics, pp. human preferences. 44474455. PMLR, 2024. Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 2266922679, 2023. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023a. Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. Training diffusion models with reinforcement learning. arXiv preprint arXiv:2305.13301, 2023b. Ralph Allan Bradley and Milton Terry. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Andrew Brock, Jeff Donahue, and Karen Simonyan. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Tianchi Cai, Xierui Song, Jiyan Jiang, Fei Teng, Jinjie Gu, and Guannan Zhang. Ulma: Unified language model alignment with demonstration and point-wise human preference. arXiv preprint arXiv:2312.02554, 2023. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming-Hsuan Yang, Kevin Murphy, William Freeman, Michael Rubinstein, et al. Muse: Text-to-image generation via masked generative transformers. arXiv preprint arXiv:2301.00704, 2023. Huayu Chen, Guande He, Hang Su, and Jun Zhu. Noise contrastive alignment of language models with explicit rewards. Advances in neural information processing systems, 2024a. Huayu Chen, Kaiwen Zheng, Hang Su, and Jun Zhu. Aligning diffusion behaviors with q-functions for efficient continuous control. Advances in neural information processing systems, 2024b. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. arXiv preprint arXiv:1810.04805, 2018. Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. Kto: Model alignment as prospect theoretic optimization. arXiv preprint arXiv:2402.01306, 2024. Shanghua Gao, Pan Zhou, Ming-Ming Cheng, and Shuicheng Yan. Masked diffusion transformer is strong image synthesizer. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2316423173, 2023. Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. Advances in neural information processing systems, 27, 2014. Michael Gutmann and Aapo Hyvärinen. Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. Journal of machine learning research, 13(2), 2012."
        },
        {
            "title": "Preprint",
            "content": "Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Jonathan Ho, Chitwan Saharia, William Chan, David Fleet, Mohammad Norouzi, and Tim Salimans. Cascaded diffusion models for high fidelity image generation. The Journal of Machine Learning Research, 23(1):22492281, 2022. Haozhe Ji, Cheng Lu, Yilin Niu, Pei Ke, Hongning Wang, Jun Zhu, Jie Tang, and Minlie Huang. Towards efficient and exact optimization of language model alignment. arXiv preprint arXiv:2402.00856, 2024. Minguk Kang, Jun-Yan Zhu, Richard Zhang, Jaesik Park, Eli Shechtman, Sylvain Paris, and Taesung Park. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1012410134, 2023. Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 44014410, 2019. Diederik Kingma and Ruiqi Gao. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36, 2024. Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1152311532, 2022. Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 21422152, 2023. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. Cheng Lu, Huayu Chen, Jianfei Chen, Hang Su, Chongxuan Li, and Jun Zhu. Contrastive energy prediction for exact energy-guided diffusion sampling in offline reinforcement learning. In International Conference on Machine Learning, pp. 2282522855. PMLR, 2023. OpenAI. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:27730 27744, 2022. Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024. William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. Improving language understanding by generative pre-training. article, 2018."
        },
        {
            "title": "Preprint",
            "content": "Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher Manning, Stefano Ermon, and Chelsea Finn. Direct preference optimization: Your language model is secretly reward model. In Thirty-seventh Conference on Neural Information Processing Systems, 2023. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International Conference on Machine Learning, pp. 88218831. PMLR, 2021. Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen. Hierarchical textconditional image generation with clip latents. arXiv preprint arXiv:2204.06125, 1(2):3, 2022. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1068410695, 2022. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 110, 2022. John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. John Schulman, Barret Zoph, Christina Kim, Jacob Hilton, Jacob Menick, Jiayi Weng, Juan Felipe Ceron Uribe, Liam Fedus, Luke Metz, Michael Pokorny, et al. Chatgpt: Optimizing language models for dialogue. OpenAI blog, 2022. Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. Yang Song and Stefano Ermon. Generative modeling by estimating gradients of the data distribution. Advances in neural information processing systems, 32, 2019. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023a. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023b. Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Bram Wallace, Meihua Dang, Rafael Rafailov, Linqi Zhou, Aaron Lou, Senthil Purushwalkam, Stefano Ermon, Caiming Xiong, Shafiq Joty, and Nikhil Naik. Diffusion model alignment using direct preference optimization. arXiv preprint arXiv:2311.12908, 2023. Sean Welleck, Ilia Kulikov, Stephen Roller, Emily Dinan, Kyunghyun Cho, and Jason Weston. Neural text generation with unlikelihood training. arXiv preprint arXiv:1908.04319, 2019."
        },
        {
            "title": "Preprint",
            "content": "Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. Jiahui Yu, Yuanzhong Xu, Jing Yu Koh, Thang Luong, Gunjan Baid, Zirui Wang, Vijay Vasudevan, Alexander Ku, Yinfei Yang, Burcu Karagol Ayan, et al. Scaling autoregressive models for contentrich text-to-image generation. arXiv preprint arXiv:2206.10789, 2(3):5, 2022. Lijun Yu, José Lezama, Nitesh Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, et al. Language model beats diffusion tokenizer is key to visual generation. arXiv preprint arXiv:2310.05737, 2023. Min Zhao, Fan Bao, Chongxuan Li, and Jun Zhu. Egsde: Unpaired image-to-image translation via energy-guided stochastic differential equations. Advances in Neural Information Processing Systems, 35:36093623, 2022. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024."
        },
        {
            "title": "Preprint",
            "content": "w/o Guidance +CCA (w/o Guidance) w/ CFG Guidance Figure 7: Comparison of LlamaGen-L samples generated with CCA or CFG."
        },
        {
            "title": "Preprint",
            "content": "w/o Guidance +CCA (w/o Guidance) w/ CFG Guidance Figure 8: Comparison of VAR-d24 samples generated with CCA or CFG."
        },
        {
            "title": "A THEORETICAL PROOFS",
            "content": "In this section, we provide the proof of Theorem 3.1. Theorem A.1 (Noise Contrastive Estimation ). Let rθ be parameterized model which takes in an image-condition pair (x, c) and outputs scalar value rθ(x, c). Consider the loss function: LNCE θ (x, c) = Ep(x,c) log σ(rθ(x, c)) Ep(x)p(c) log σ(rθ(x, c)). Given unlimited model expressivity for rθ, the optimal solution for minimizing LNCE θ satisfies θ (x, c) = log p(xc) p(x) . (13) (14) Proof. First, we construct two binary (Bernoulli) distributions: Qx,c := { p(x, c) p(x, c) + p(x)p(c) , p(x)p(c) p(x, c) + p(x)p(c) } = { p(xc) p(xc) + p(x) , p(x) p(xc) + p(x) } θ x,c := { erθ(x,c) erθ(x,c) + , 1 erθ(x,c) + 1 } = {σ(rθ(x, c)), 1 σ(rθ(x, c))} Then we rewrite LNCE θ (x, c) as LNCE θ (x, c) = Ep(x,c) log σ(rθ(x, c)) Ep(x)p(c) log σ(rθ(x, c)) (cid:90) (cid:104) (cid:105) p(x, c) log σ(rθ(x, c)) + p(x)p(c) log σ(rθ(x, c)) dxdc (cid:90) (cid:104) (p(x, c) + p(x)p(c)) (cid:105) = = (cid:104) p(x, c) p(x, c) + p(x)p(c) log σ(rθ(x, c)) + p(x)p(c) p(x, c) + p(x)p(c) log (cid:2)1 σ(rθ(x, c))(cid:3)(cid:105) dxdc (cid:90) (cid:104) (cid:90) (cid:104) = = (p(x, c) + p(x)p(c)) (cid:105) H(Qx,c, θ x,c)dxdc (p(x, c) + p(x)p(c)) (cid:105)(cid:104) DKL(Qx,cP θ (cid:105) x,c) + H(Qx,c) dxdc x,c) represents the cross-entropy between distributions Qx,c and θ Here H(Qx,c, θ x,c. H(Qx,c) is the entropy of Qx,c, which can be regarded as constant number with respect to parameter θ. Due to the theoretical properties of KL-divergence, we have (cid:90) (cid:104) (cid:105)(cid:104) LNCE θ (x, c) = (p(x, c) + p(x)p(c)) DKL(Qx,cP θ x,c) + H(Qx,c) (cid:105) dxdc (cid:90) (cid:104) (p(x, c) + p(x)p(c)) (cid:105) H(Qx,c)dxdc constantly hold. The equality holds if and only if Qx,c = θ x,c, such that σ(rθ(x, c)) = erθ(x,c) erθ(x,c) + 1 = p(x, c) p(x, c) + p(x)p(c) rθ(x, c) = log p(x, c) p(x)p(c) = log p(xc) p(x)"
        },
        {
            "title": "B THEORETICAL ANALYSES OF THE NORMALIZING CONSTANT",
            "content": "We omit normalizing constant in Eq. 7 for brevity when deriving CCA. Strictly speaking, the target sampling distribution should be: psample(xc) ="
        },
        {
            "title": "1\nZ(c)",
            "content": "p(xc)[ p(xc) p(x) ]s, such that 1 log psample(xc) p(xc) = log p(xc) p(x) that psample(xc) 1 log Z(c). The normalizing constant Z(c) ensures (cid:82) psample(xc)dx = 1. We have Z(c) = (cid:82) p(xc)[ p(xc) p(x) ]sdx = Ep(xc)[ p(xc) p(x) ]s. is properly normalized, i.e., To mitigate the additional effects introduced by Z(c), in our practical algorithm, we introduce new training parameter λ to bias the optimal solution for Noise Contrastive Estimation. Below, we present result that is stronger than Theorem 3.1. Theorem B.1. Let λc > 0 be scalar function conditioned only on c. Consider the loss function: LNCE θ (x, c) = Ep(x,c) log σ(rθ(x, c)) λcEp(x)p(c) log σ(rθ(x, c)). Given unlimited model expressivity for rθ, the optimal solution for minimizing LNCE θ satisfies θ (x, c) = log p(xc) p(x) log λc. (15) (16) Proof. We omit the full proof here, as it requires only redefinition of the distributions Qx,c from the proof of Theorem A.1: Qx,c := { p(x, c) p(x, c) + λcp(x)p(c) , λcp(x)p(c) p(x, c) + λcp(x)p(c) } = { p(xc) p(xc) + λcp(x) , λcp(x) p(xc) + λcp(x) } Then we can follow the steps in the proof of Theorem A.1 to arrive at the result. = (cid:2)Ep(xc)[ p(xc) If let λc := Z(c) 1 to psample. However, in practice estimating Z(c) could be intricately difficult, so we formalize λc as training parameter, resulting in our practical algorithm in Eq. 12. , we could guarantee the convergence of psample p(x) ]s(cid:3) 1 θ"
        },
        {
            "title": "C ADDITIONAL EXPERIMENTAL RESULTS",
            "content": "We provide more image samples to compare CCA and CFG in Figure 7 and Figure 8. We illustrate the effect of training parameter β on the FID-IS trade-off in Figure 9. Overall, β affects the fidelity-diversity trade-off similar to CCA λ and the CFG method. Figure 9: Effect of varying β of CCA for the LlamaGen-L model. In our CCA experiments, we either fix λ = 1e3 and ablate β [2, 5e 3] (from left to right) or fix β = 0.02 and ablate λ [0, 1e4]. In our CFG experiments, we ablate [0, 3]."
        },
        {
            "title": "D TRAINING HYPERPARAMETERS",
            "content": "Table 4 reports hyperparameters for chosen models in Figure 1 and Figure 6. Other unmentioned design choices and hyperparameters are consistent with the default setting for LlamaGen https://github.com/FoundationVision/LlamaGen and VAR https://github. com/FoundationVision/VAR repo. All models are fine-tuned for 1 epoch on the ImageNet dataset. We use mix of NVIDIA-H100, NVIDIA A100, and NVIDIA A40 GPU cards for training."
        },
        {
            "title": "VAR",
            "content": "B d30 3B 111M 343M 775M 1.4B 3.1B 310M 600M 1.0B 2.0B"
        },
        {
            "title": "XXL",
            "content": "d16 d20 d24 XL CCA β CCA λ CCA+CFG β CCA+CFG λ Learning rate Dropout? Batch size 0.02 1000 0.1 1 1e-5 Yes 256 0.02 300 0.02 1 1e-5 Yes 256 0.02 1000 0.1 1 1e-5 Yes 256 0.02 1000 0.1 1 1e-5 Yes 256 0.02 500 0.1 1 1e-5 Yes 256 0.02 50 - - 2e-5 None 0.02 50 - - 2e-5 Yes 256 0.02 100 - - 2e-5 Yes 256 0.02 1000 - - 2e-5 Yes 256 Table 4: Hyperparameter table."
        }
    ],
    "affiliations": [
        "The University of Hong Kong",
        "Tsinghua University"
    ]
}