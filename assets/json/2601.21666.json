{
    "paper_title": "SONIC-O1: A Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding",
    "authors": [
        "Ahmed Y. Radwan",
        "Christos Emmanouilidis",
        "Hina Tabassum",
        "Deval Pandya",
        "Shaina Raza"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) are a major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audio-video data remains underexplored. This gap highlights the need for a high-quality benchmark to systematically evaluate MLLM performance in a real-world setting. We introduce SONIC-O1, a comprehensive, fully human-verified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closed- and open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe a substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: Project page: https://vectorinstitute.github.io/sonic-o1/ Dataset: https://huggingface.co/datasets/vector-institute/sonic-o1 Github: https://github.com/vectorinstitute/sonic-o1 Leaderboard: https://huggingface.co/spaces/vector-institute/sonic-o1-leaderboard"
        },
        {
            "title": "Start",
            "content": "SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models on Audio-Video Understanding Ahmed Y. Radwan 1 Christos Emmanouildis 2 Hina Tabassum 3 Deval Pandya 1 Shaina Raza 1 6 2 0 2 9 2 ] A . [ 1 6 6 6 1 2 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multimodal Large Language Models (MLLMs) are major focus of recent AI research. However, most prior work focuses on static image understanding, while their ability to process sequential audiovideo data remains underexplored. This gap highlights the need for high-quality benchmark to systematically evaluate MLLM performance in real-world setting. We introduce SONIC-O1, comprehensive, fully humanverified benchmark spanning 13 real-world conversational domains with 4,958 annotations and demographic metadata. SONIC-O1 evaluates MLLMs on key tasks, including open-ended summarization, multiple-choice question (MCQ) answering, and temporal localization with supporting rationales (reasoning). Experiments on closedand open-source models reveal limitations. While the performance gap in MCQ accuracy between two model families is relatively small, we observe substantial 22.6% performance difference in temporal localization between the best performing closed-source and open-source models. Performance further degrades across demographic groups, indicating persistent disparities in model behavior. Overall, SONIC-O1 provides an open evaluation suite for temporally grounded and socially robust multimodal understanding. We release SONIC-O1 for reproducibility and research: (cid:140) Project page 3 Leaderboard"
        },
        {
            "title": "Github",
            "content": "1Vector Institute Intelligence, MaRS for Artificial Centre, Toronto, ON M5G 1L7, Canada 2University of Groningen, Nijenborgh 4, 9747 AG Groningen, Netherlands 3York University, 4700 Keele Street, Toronto, ON Correspondence to: Ahmed Y. RadM3J 1P3, Canada. Raza <ahmed.radwan@vectorinstitute.ai, wan, shaina.raza@vectorinstitute.ai>. Shaina Preprint. January 30, 2026. Figure 1. Performance comparison across 13 conversational domains. We compare closed-source and open-source MLLMs across 13 conversational domains using LLM-judge scores (010) for video summarization task. Gemini 3.0 Pro consistently outperforms open-source models, and high-stakes domains (e.g., Emergency Response, Mental Health) remain more challenging. 1. Introduction MLLMs have advanced from static image captioning to general-purpose perception-and-reasoning systems capable of processing video and audio inputs (Fu et al., 2024b). These models are increasingly deployed to assist real-world decisions through conversational interactions. As MLLMs move into high-stakes such as healthcare, education, and public safety capability alone is insufficient. We must also evaluate whether these systems behave accurately, fairly across demographics, and transparently across diverse users and situations. Real-world interactions rely fundamentally on both audio and video modalities. For example, speech conveys affect, emphasis, hesitation, and social intent that are essential for understanding communication. However, existing audiovideo benchmarks exhibit two critical gaps. First, audio is frequently treated as optional or replaced with transcripts and subtitles (Ataallah et al., 2025; Wang et al., 2025), SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Figure 2. Overview of SONIC-O1 tasks and evaluation format: (Top) video summarization, (Middle) evidence-grounded MCQ, (Bottom) temporal localization (event timing). Each example shows the input clip (frames) and the expected output format; demographic attributes shown beneath each clip represent associated metadata, enabling group-wise evaluation across 13 domains. under-exploring paralinguistic cues that shape interpretation of speaker meaning and their emotional state. Second, even when native audio-video inputs are evaluated, groupwise analysis across demographic groups remains largely absent (Chen et al., 2024; Li et al., 2025a), precluding assessment of whether model performance varies systematically with user characteristics. Together, these limitations obscure whether current MLLMs can operate accurately and fairly in real-world deployment scenarios. We present comparison of seminal audio-video benchmarks in Table 1. To this end, we introduce SONIC-O1, SOcial Natural Interaction Corpus (Omnimodal, v1), the first open-source, fully human-verified benchmark of 4,958 audio-video question-answer (QA) instances derived from 60 hours of real-world audio-video interactions. As shown in Figure 3, the dataset spans 13 high-impact topics across 5 domains (including legal/civic, educational, and public health), covering real-world scenarios. SONIC-O1 covers videos that range from 30 seconds to 60 minuts (covering both short, medium, long duration) and tagged with metadata covering 6 racial groups, genders, and age groups. SONIC-O1 evaluates three important evaluation tasks on state-of-the-art MLLMs: (i) summarization for global comprehension, (ii) multiple-choice QA for fine-grained reasoning, and (iii) temporal localization for identifying when events occur. Unlike prior work that treats audio as optional (as shown in Table 1) , SONIC-O1 requires native audiovideo reasoning in an omnimodal (audio, video, and text) setting, where models use the audiovideo input together with text to evaluate MLLMs responses. Our goal with this work is to reveal where omnimodal MLLMs succeed, fail, and diverge across demographic groups in conversational contexts. Contributions. (1) We introduce SONIC-O1, an opensource benchmark with human-verified domain-expert annotations and demographic metadata for evaluating MLLM performance and group-wise analysis on real-world interactions. (2) We benchmark omnimodal MLLMs, revealing substantial performance gaps and systematic disparities across demographic groups. (3) We release the full evaluation suite (dataset, scripts, leaderboard) under research license. Our evaluation reveals several key findings. (1) Closedsource models consistently outperform open-source alternatives, (2) temporal localization remains the most challenging task, and (3) systematic demographic disparities persist across models. 2. Related Work Growth in MLLMs. MLLMs have rapidly evolved from static image understanding (Radford et al., 2021; Li et al., 2022) to video comprehension (Sun et al., 2019; Tong et al., 2 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Table 1. Comparison of video-QA benchmarks. SONIC-O1 uniquely combines comprehensive temporal understanding, audio-visual reasoning, social cue analysis, and open-source availability with both automatic and human annotations across diverse video lengths. Legend: = Yes, = No, δ = Partial temporal, AVQA = Audio-Visual Question Answering, MCQs = Multiple Choice Questions. Benchmark Data Size Video Lengths QA Type Summ. Temp. Reas. AVQA Social* Annotation Open (min) Cues Auto Human Src Video-MME (Fu et al., 2025) LongVideoBench (Wu et al., 2024) LVBench (Wang et al., 2025) CinePile (Rawal et al., 2024) Sports-QA (Li et al., 2024b) InfiniBench (Ataallah et al., 2025) VAST (Chen et al., 2023) Daily-Omni (Zhou et al., 2025b) MAVERIX (Xie et al., 2025) CG-Bench (Chen et al., 2024) OmniVideoBench (Li et al., 2025a) AURA (Galougah et al., 2025) 2,700 6,678 1,549 305K 94,000 87,700 27M 1,197 2,556 12,129 1,000 1,600 Video-QA Benchmarks (AVQA: ) <2; 415; 3060 8 68 3 <1 53 MCQs MCQs MCQs MCQs Open-ended MCQs + Open δ δ δ δ δ δ Audio-Visual Benchmarks (AVQA: ) <1 <1 6 1060 130 < Captioning MCQs MCQs + Open MCQs + Open MCQs + Open MCQs δ δ δ SONIC-O 4,958 <5; 520; 2060 MCQs + Open *Social Cues: Demographic metadata (race, gender, age) annotated from observable characteristics in videos via AI-assisted human review (see Appendix A.7), enabling fairness evaluation across demographic groups. Temporal: Temporal localization questions requiring localizing start and end of an event. Reasoning: Open-ended explanation of the given answer from the model. 2022). Recent vision-language models (Bai et al., 2023; Li et al., 2024a; Maaz et al., 2024) extend these capabilities to video, but they often operate over sampled frames and text, treating audio as auxiliary rather than integral to multimodal reasoning. In contrast, new generation of omnimodal MLLMs, including VITA 1.5 (Fu et al., 2024a), Qwen3-Omni (Xu et al., 2025), Gemini 3.0 Pro (DeepMind, 2025), MiniCPM-o-2.6 (Yao et al., 2024), and UniMoE2.0 (Li et al., 2025b), jointly process audio, video, and text within unified framework. Omnimodal MLLMs thus refer to models designed for integrated reasoning across multiple modalities (Jiang et al., 2025). However, evaluation benchmarks have not kept pace. Existing datasets often omit audio entirely, replace it with text transcripts, or lack the demographic annotations necessary to assess group-wise performance across diverse user populations. Video Benchmarks. Recent benchmarks have begun to evaluate temporal reasoning across diverse video lengths, including short-form tasks such as MVBench (Li et al., 2025a) and TempCompass (Liu et al., 2024), as well as longform understanding in Video-MME (Fu et al., 2025) (up to one hour) and LongVideoBench (Wu et al., 2024).However, two critical gaps remain. First, audio is underutilized. Even when available, evaluations often default to frameor subtitle-based setups due to limited model support. Second, group-wise analysis is largely absent, preventing systematic measurement of performance disparities across demographic groups. As shown in Table 1, SONIC-O1 addresses these limitations by combining native audio-video evaluation, temporal localization, and demographic metadata to enable group-wise analysis assessment. 3. SONIC-O1: Dataset and Benchmark Design In this section, we describe the design of the SONIC-O1 dataset and benchmark. 3.1. Data Collection The construction of SONIC-O1 is grounded in real-world data. To ensure broad coverage, we collected videos from YouTube (Google Developers, 2025) restricted to CC BY 4.0 licenses. We target five high-stakes domains, such as Professional, Educational, Legal/Civic, Service-Oriented, and Public Health, subdivided into 13 topics (e.g., job interviews, courtroom proceedings) as shown in Figure 3. To benchmark long-context capabilities, we stratified sampling across three duration ranges: short (<5 minutes), medium (5-20 minutes), and long (20-60 minutes). Our search strategy combined topic keywords with demographic descriptors (race, gender, age) to maximize representation. From an initial pool of 2,237 candidates, 1,794 met licensing constraints, and subsequent manual quality filtering yielded 231 final videos totaling approximately 60 hours. SONICO1 provides fully human-verified, real-world audio-video content in sensitive domains with perceived demographic annotations, enabling fairness-oriented slice analysis that is typically not reported in movie/TV-based benchmarks (Song et al., 2024; Wang et al., 2025) or in many judge-based / synthetically-evaluated evaluation suites (Fang et al., 2024; Zhou et al., 2025a). Full details are provided in Appendix A. 3.2. Tasks Annotation We define three evaluation tasks over the same video pool and discuss them below. Figure 4b shows the annotation SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models (a) Video duration distribution over topics Figure 3. Video categories. Our benchmark covers 5 key domains and 13 video topics. (b) Question type distribution over topics counts per task across the 13 topics; Appendix A.7 describes the AI-assisted annotation workflow and prompts. Figure 4. Video duration and question type distributions. SONICO1 spans full spectrum of video lengths and evaluates core abilities of MLLMs. Table 2. SONIC-O1 task statistics. Task #Inst. Unit / Ground Truth Summarization 231 Full video reference summary MCQ 1,335 3-minutes overlap video segment answer + reasoning Temporal loc. 3,392 3-minutes overlap video segment relation + (ts, te) + reasoning Total 4,958 231 videos (60 hours) Task 1: Video Summarization. This task requires models to summarize videos by capturing key events, actions, and outcomes. Following established video summarization benchmarks (Ning et al., 2025), models generate detailed narrative summary up to 300 words. During annotation, to preserve coverage for long videos, we apply hierarchical procedure, where videos longer than 10 minutes are segmented into 10-minute chunks to context limits, each chunk is summarized, and the chunk-level summaries are merged into final reference summary. The task contains 231 summarization instances, each consisting of an audio-video input paired with human-verified reference summary. Task 2: MCQs with Reasoning. This task evaluates finegrained understanding of local video segments using controlled answer space, common design for objective comparison in video benchmarks (Ye et al., 2024; Lei et al., 2018; Patraucean et al., 2023). We operate on segments of up to 3 minutes; longer videos are split into 3-minute windows with 30-second overlap. For each segment, we construct an MCQ with five choices, consisting of four candidate answers plus Not enough evidence option. During evaluation, MLLMs must choose the correct answer and provide rationale grounded in visual and auditory evidence. Task 3: Temporal Localization. This task evaluates temporal reasoning by grounding events with timestamps (e.g., whether diagnosis occurs before or after symptoms). Temporal localization is standard in audio-video benchmarks for fine-grained event alignment (Zhou et al., 2025b; Geng et al., 2025). We extend this by requiring models to predict timestamps and provide rationales grounded in audio-visual evidence. We use an anchor-target formulation that given an anchor event, models must (1) predict the targets start and end times, and (2) provide reasoning with salient evidence. Domain experts label times relative to each segments start (0.0s), which are converted to absolute timestamps by adding the segments offset. The dataset contains 3,392 instances (up to three per segment), each with target timestamps and supporting reasoning. 3.3. Quality Assurance Each evaluation task is manually reviewed and verified by domain experts; team details and guidelines are provided in Appendix B.1. Experts watched each full video before labeling, with the ability to revisit any timestamp as needed. 4 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Table 3. Performance of MLLMs on SONIC-O1. We report results on all dataset across three tasks: summarization (LLM-judge score, Recall-Oriented Understudy for Gisting EvaluationL (ROUGE-L), cosine similarity), multiple-choice questions (MCQs; accuracy and rationale quality), and temporal localization (mean Intersection-over-Union (mIoU), Recall@0.5 (R@0.5), and rationale quality). Accuracy, mIoU, R@0.5, and ROUGE-L are reported as percentages; cosine similarity is in [0,1]. All metrics are macro-averaged across all topics in the benchmark. Frames per video (FPV) and frames per second (FPS) denote input settings. Higher is better. Bold denotes best performance, underline denotes second-best per task-metric. represents closed-source model. Detailed results in Appendix A4. Model LLM Params Summarization MCQ Temporal Localization Score ROUGE-L Sim Acc. Score ROUGE-L Sim mIoU R@0.5 Score ROUGE-L Sim Gemini 3.0 Pro (1 FPS) Qwen3-Omni (256 FPV) UniMoE-2.0 (256 FPV) MiniCPM-o-2.6 (256 FPV) VITA 1.5 (64 FPV) VideoLLaMA 2 (128 FPV) - 30B 33B 9B 8B 7B 7.07 5.72 4.71 3.34 2.77 1.53 27.2 22.8 20.8 14.7 16.3 12.7 0.81 0.71 0.70 0.56 0.49 0. 96.4 93.6 88.2 87.4 81.5 54.3 9.38 8.88 8.12 8.01 7.72 5.11 36.9 33.8 26.5 24.4 25.2 18.9 0.80 0.78 0.73 0.73 0.71 0.58 26.6 3.7 1.8 1.8 1.8 3.5 25.4 2.8 1.0 0.7 1.2 0. 5.38 2.58 2.11 3.65 3.91 2.22 28.3 28.0 23.7 19.3 22.1 22.5 0.67 0.70 0.55 0.41 0.43 0.49 To support scalable annotation, we used Gemini 2.5 Flash to draft perceived demographic metadata (race, gender, age) and task annotations across all tasks, inspired by prior benchmark efforts (Ataallah et al., 2025). Full prompts are given in Appendix A.7.1. All outputs were then verified, corrected, and revised via our internal review interface (Appendix B.3). We removed any items with ambiguity, insufficient evidence, or annotation errors and resolved disagreements through consensus adjudication. This process results in the construction of SONIC-O1, with dataset statistics reported in Table 2 and further details provided in Appendix C. 3.4. Evaluation Suite We release full evaluation suite. While existing audiovideo benchmarks  (Table 1)  often provide limited datasets and evaluation scripts, they are not always designed to scale as new MLLMs, interaction formats, and task variants emerge. To address this, we release an extensible SONIC-O1 evaluation suite that offers unified framework for models operating on direct audiovideo inputs, with consistent segmenting and sampling policies for long videos. The suite modularizes the pipeline into data loading, inference, and metric computation, supporting both commercial and open-source MLLMs across heterogeneous API formats. This design lowers the barrier to adoption and provides practical foundation for reproducible evaluation and future community extensions. 4. Experiments In this section, we define our experimental setup. 4.1. Settings Evaluated model suite We conduct the evaluation on commercial and open-source MLLMs. For commercial models, we used Gemini-3.0-Pro (DeepMind, 2025) and GPT-4o (OpenAI, 2024) (ablation only). Representative open-source omni MLLMs including Qwen3-Omni-30BA3B (Xu et al., 2025), Uni-MoE-2.0-Omni (Li et al., 2025b), MiniCPM-o-2.6 (Yao et al., 2024), VITA-1.5 (Fu et al., 2024a), and VideoLLaMA2 (Cheng et al., 2024) are evaluated as well. We prioritize those models that natively support audio-video inputs. GPT-4o supports vision-text but not native audio, so we include it for modality ablation. We follow best configurations and practices for open-source MLLMs, setting maximum frames at 256 with adaptive fallback for models with context limitation. Model details and settings appear in Table A3 and Appendix Metrics We employ both statistical and LLM-based metrics, following evaluation protocols as in related works (Fang et al., 2024; Ataallah et al., 2025). For summarization, we report ROUGE-L, cosine similarity, and an LLM judge score (GPT-5-mini with 010 scale). For MCQs, we report accuracy (%); we additionally report the LLM judge score on open-ended reasoning for this task. For temporal localization, we report mean intersection over union (mIoU) and recall at intersection over union (R@0.5). Full definitions and prompts are in Appendix E. 4.2. Results and Analysis In this section, we report the performance of MLLMs on SONIC-O1. Performance of MLLMs models on SONIC-O1. We report comprehensive evaluation results across all three tasks in Table 3. Overall, we observe that the closed-source model Gemini 3.0 Pro achieves the highest performance among all models, obtaining 96.4% MCQ accuracy, 7.07 judge score on summarization, and 25.4% R@0.5 on temporal localization. In the temporal localization task, we observe large performance gap of about 23% between Gemini 3.0 and the next-best open-source Qwen3-Omni model. Among open-source models, Qwen3-Omni demonstrates the strongest overall performance, achieving 93.6% MCQ 5 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models accuracy and 5.72 judge score on summarization. UniMoE2.0 attains 88.2% MCQ accuracy, but its summarization capabilities remain weaker (4.71 judge score). The results further show that model scale plays key role, with smaller models showing some performance limitations. MiniCPMo-2.6 and VideoLLaMA2 are built on smaller Qwen backbones. They show noticeable drops compared to Qwen3Omni (30B), with 41.6% lower judge score on summarization, 6.62% lower MCQ accuracy, and 51.4% lower R@0.5 on temporal localization. Key Finding: Closed-source models remain consistently ahead, with temporal localization is the most challenging task for current open-source models. The summarization and MCQ tasks are relatively easier for MLLMs. Larger open-source models provide measurable gains, however, they do not fully close the performance gap. Table 4. Performance of MLLMs on summarization task across demographic groups. Results are on all dataset reported using LLM judge Score (010), higher scores indicate better performance. Bold: best, highlighted : worst within each group per column. Detailed results in Appendix Table A15 Group Race Arab Indigenous Asian White Hispanic Black Gender Male Female Age 1824 2539 40+ Gemini 3.0 Pro Qwen3 Omni UniMoE 2.0 MiniCPM o-2. VITA 1.5 Video LLaMA2 6.90 6.70 7.05 6.68 6.41 6.02 6.36 7.02 6.28 6.52 6.91 5.95 4.13 5.71 5.28 4.99 4. 4.97 5.47 4.93 5.01 5.47 5.00 4.35 4.62 4.29 3.70 3.45 3.93 4.55 4.02 4.01 4.45 3.57 3.61 3.26 3.26 3.04 2. 2.98 3.53 3.30 3.14 3.21 2.76 1.65 2.65 2.50 2.21 2.31 2.31 2.65 2.41 2.45 2.47 1.00 1.04 1.63 1.45 1.23 1. 1.36 1.53 1.38 1.50 1.36 Table 5. MCQ performance across demographic groups. Accuracy (%), with higher values indicating better performance. Detailed results in Appendix Table A16. Group Race Arab Indigenous Asian White Hispanic Black Gender Male Female Age 1824 2539 40+ Gemini 3.0 Pro Qwen3 Omni UniMoE 2. MiniCPM o-2.6 VITA 1.5 Video LLaMA2 98.4 94.3 97.8 96.9 95.8 96.4 96.3 97.6 97.4 95.6 98. 96.9 77.1 96.1 93.3 92.8 92.0 93.5 93.4 91.5 92.7 94.6 95.3 80.0 89.2 89.0 86.4 87.7 88.4 89.3 83.3 87.7 91. 92.7 82.9 88.7 87.6 82.2 86.9 86.5 88.3 84.5 86.8 88.4 93.2 62.9 84.6 82.3 79.9 82.7 82.2 83.8 83.0 81.7 84. 67.0 65.7 58.1 55.0 51.5 55.4 53.0 60.0 54.8 55.8 56.3 Table 6. Temporal localization performance across demographic groups. Results are reported as R@0.5 (%), where higher values indicate better temporal grounding. Detailed results in Appendix Tables A17 and A18. Group Race Arab Indigenous Asian White Hispanic Black Gender Male Female Age 1824 2539 40+ Gemini 3.0 Pro Qwen3 Omni UniMoE 2.0 MiniCPM o-2.6 VITA 1.5 Video LLaMA2 21.1 40.9 30.7 23.0 23.8 19.5 23.5 24. 27.3 25.2 21.9 1.6 0.0 2.9 2.6 2.3 1.8 2.6 2.1 2.4 2.5 2.3 0.2 1.3 0.6 1.2 0.1 0.6 0.9 0. 1.2 1.2 0.4 2.6 0.0 0.8 0.9 0.2 0.3 0.7 0.9 1.1 0.7 0.8 1.2 1.3 1.4 1.4 0.8 1.4 1.2 1. 1.1 1.2 1.5 0.0 1.3 0.4 0.5 0.0 0.3 0.4 0.4 0.3 0.3 0.4 Overall, we observe that the closed-source Gemini 3.0 Pro achieves the strongest performance across nearly all demographic groups, while Qwen3-Omni is the most competitive open-source model. MCQ accuracy is relatively stable across groups, whereas summarization scores show larger variation. The largest gaps emerge in temporal localization (R@0.5), particularly across racial groups (e.g., Indigenous: 40.9% vs. Black: 19.5%). Race shows the most pronounced disparities, with smaller gender effects and generally higher performance for participants aged 40+. Female participants achieve slightly higher scores, one possible contributor is that safety alignment can shift response tone and refusal tendencies, which may interact with our scoring. Finally, smaller models appear less robust across demographic slices. Key Finding: While aggregate results  (Table 3)  provide overall task rankings, however, group-wise demographic analysis reveals that temporal localization shows substantially larger disparities across race and age than averages suggest, while MCQ remains stable across groups. Performance across topics. We also observe consistent topic-level variation. As can be seen in Figure 1 Gemini 3.0 Pro performs best across nearly all domains, Qwen3Omni and UniMoE-2.0 (30B) form competitive middle tier, while smaller 7-9B models drop sharply. High-stakes scenarios (e.g., emergency response, mental health) are most challenging to solve by MLLMs, whereas some topics in professional settings (e.g., consultations, interviews) are comparatively easier (Appendix G). Key Finding: Robustness remains uneven across topics, with the largest gaps in emotionally sensitive contexts. Performance across demographic groups. We report MLLM performance stratified by race, gender, and age in Tables 4, 5, and 6 to quantify demographic disparities. Performance across short, medium, and long durations. Table 7 reports MLLM performance stratified by video duration (short < 5 minutes, medium 520 minutes, long 20 6 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Table 7. Duration robustness of MLLMs across video lengths. Performance is reported on all dataset for short, medium, and long videos. Summarization is measured using LLM-judge scores, MCQ using accuracy (%), and temporal localization using R@0.5 (%). Higher values indicate better performance. Bold and underline denote the best and second-best results within each duration bin and task. Model Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2.6 VITA 1.5 VideoLLaMA 2 LLM Params - 30B 33B 9B 8B 7B Summarization (Score) MCQ (Accuracy) Temporal Localization (R@0.5) Short Medium Long Short Medium Long Short Medium Long 8.16 6.64 5.49 4.08 3.39 1.89 6.11 4.95 4.37 2.87 2.40 1.46 6.63 4.87 3.87 2.87 2.46 1.27 98.9 91.5 89.4 92.6 85.1 61.7 95.1 93.6 86.5 84.8 78.4 55.5 96.9 94.5 90.9 89.2 82.4 52. 50.6 9.2 6.9 0.9 3.0 0.9 26.0 2.6 0.3 1.1 1.2 0.3 18.6 2.5 0.6 0.6 1.0 0.4 Table 8. Effect of input modalities. Performance of representative MLLMs on representative topics under different input modalities (video-only vs. video+audio). Summarization is reported using LLM-judge scores (task-specific scale); MCQ is reported using accuracy (%) and rationale score; Temporal localization is reported using R@0.5 (%) and rationale score. Bold denotes the best within each model. Model Video Audio Sum. (Score) MCQ (Acc./Score) Temporal (R@0.5/Score) Qwen3-Omni UniMoE-2. VideoLLaMA2 GPT4o 4.60 6.82 3.96 5. 2.02 1.88 5.02 5.82 86.1 / 7.98 96.8 / 9.42 83.9 / 8.09 92.7 / 8.89 54.6 / 5.61 57.1 / 5.23 91.2 / 8.83 97.5 / 9. 1.6 / 2.55 4.2 / 2.69 0.7 / 2.42 1.2 / 2.06 0.8 / 2.89 0.6 / 2.13 4.3 / 2.87 3.0 / 2.93 Note: For GPT4o, the Audio column corresponds to Text input (i.e., GPT4o uses Video+Text instead of Video+Audio). 60 minutes) to assess the impact of temporal extent. Overall, we observe that Gemini 3.0 Pro consistently achieves the highest performance across all video lengths (short, medium, long) on all three tasks. Smaller models like VITA 1.5, VideoLLaMA2 shows limited robustness to longer and more complex videos. We also observe that performance degrades as video length increases, with temporal localization most affected, while MCQ remains relatively stable. Key Finding: Performance declines on longer videos, with temporal localization most affected by the challenge of sustained event tracking. 4.3. Ablation studies The impact of input modality and frame sampling rate is examined through ablation studies on three representative topics, including patientdoctor consultations (T1), job interviews (T2), and courtroom proceedings (T5). Effect of input modalities. Table 8 presents modality ablation study comparing video-only input (frames without audio) to combined video+audio input. The results show that adding audio improves performance across most models and tasks. For example, Qwen3-Omni improves from 86.1% to 96.8% MCQ accuracy and from 4.60 to 6.82 summarization score when audio is added. GPT-4o (using text transcripts instead of native audio) gains 6.3% in MCQ accuracy with text modality. However, the magnitude of improvement varies by model and task. Qwen3Omni benefits most consistently, with adding audio yielding +10.7% MCQ accuracy, +2.22 summarization score, and +2.6% R@0.5 in temporal localization. Smaller models show weaker gains, e.g., VideoLLaMA2 exhibits mixed results, with slight degradation in summarization (-0.14 score) despite small MCQ gains (+2.5%). Key Finding: Multimodal inputs improve performance across tasks, with larger models benefiting more consistently than smaller models, suggesting effective audio-visual fusion requires sufficient model capacity. Effect of frame sampling rate. MLLMs typically operate over limited set of uniformly sampled frames (e.g., 16 or 32), raising the question of whether denser visual coverage improves performance. Figure 5 shows that increasing the number of input frames does not consistently yield gains. For summarization and MCQ (Figures 5a and 5b), accuracy remains largely unchanged, suggesting that additional frames provide limited benefit for high-level semantic understanding. In contrast, temporal localization (Figure 5c) improves more noticeably with higher frame counts for stronger models (e.g., Qwen3-Omni), while weaker models (e.g., VideoLLaMA2) exhibit inconsistent trends. Key Finding: Higher frames primarily benefit temporal localization, but yield little gain for summarization or MCQ. 4.4. Qualitative Analysis To characterize the baseline emotional language profiles of different models, we analyze generated audio-video empathic summaries using linguistic inquiry and word count (LIWC-22), validated dictionary-based tool (Tausczik & Pennebaker, 2010; Boyd et al., 2022) (details in Appendix J). We examine total emotional expression, negative emotion, and overall tone, which capture differences in affective style and warmth in unconstrained model outputs. Results in Table 9 reveal distinct default emotional profiles 7 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models (a) Summarization Score (b) MCQ Accuracy (c) Temporal R@0. Figure 5. Frame-count sensitivity across metrics. Performance of three representative MLLMs (Qwen3-Omni, UniMoE-2, VideoLLaMA2) on representative topics across varying frame counts (16, 32, 64, 128) for three tasks: (a) Summarization quality measured by LLM judge scores, (b) Multiple-choice question accuracy, and (c) Temporal localization R@0.5. Models exhibit different saturation patterns across tasks. Table 9. Emotional language analysis. Analysis of MLLM empathic summary outputs on the all dataset (Task 1: summarization). Total Emotion (%) denotes the percentage of words expressing any emotion; Neg. Emotion (%) denotes the percentage of words expressing negative affect; Tone reflects overall warmth and positivity (higher values indicate warmer, more informal tone). Bold denotes best performance and highlighted denotes worst. Model Total Emotion (%) Neg. Emotion (%) Tone Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2.6 VITA-1.5 VideoLLaMA 4.35 3.88 3.54 1.41 2.65 2.02 2.03 1.39 0.93 0.38 0.59 0.49 46.16 56.99 57.94 46.10 55.45 49.83 across MLLMs. Gemini 3.0 Pro adopts neutral, clinical style with higher emotional acknowledgment but lower tone scores. In contrast, Qwen3-Omni and UniMoE-2.0 exhibit warmer, more supportive profile, characterized by higher tone scores and reduced negative emotion. Smaller models (e.g., MiniCPM-o-2.6, VideoLLaMA2) show limited emotional modulation, indicating reduced capacity for affective language adjustment. Key Finding: MLLMs demonstrate distinct baseline emotional language styles, with larger models showing richer affective expression. These profiles reflect default tendencies rather than controlled emotional modulation capabilities. 5. Discussion and Limitations Our results reveal clear gap in current MLLMs. They excel at content understanding but struggle with temporal grounding. Video inputs offer limited gains, and temporal localization remains especially challenging for opensource models. Frequent hallucinated time references point to deeper weaknesses in internal temporal reasoning. Limitations. We acknowledge some limitations of this study. SONIC-O1 focuses on English-language interactions, which may limit cultural and linguistic coverage. Although annotations are human-verified, some subjectivity remains, particularly for open-ended tasks. The benchmark is also restricted in scale and task scope, and findings may not fully generalize to other domains, modalities, or future model generations. From an evaluation perspective, some constraints are observed. First, many MLLMs rely on similar Qwen-based backbones but we observe that they differ substantially in audio processing. In practice, long audio streams are often truncated to fixed durations rather than adaptively aligned with sampled video frames. Second, models frequently struggle with absolute temporal localization. When clips do not begin at zero (e.g., 500800s), predictions are often reported relative to the segment start (e.g., 5080s instead of 550580s), suggesting reliance on relative playback position rather than robust temporal representations (see error taxonomy in Appendix H). Third, demographic groups have unequal sample sizes (Figure A7), which may increase variance for underrepresented groups. Finally, context-length limitations require video segmentation for most models, which can further amplify temporal errors. In contrast, Geminis larger context window allows processing full videos at 1 FPS, reducing the need for segmentation. 6. Conclusion We introduced SONIC-O1, real-world audio-video benchmark for evaluating MLLMs performance through the lens of perceived demographic fairness. SONIC-O1 includes human-verified annotations and three tasks. These are summarization, MCQs, and temporal localization with reasonings, to assess both semantic understanding and temporal grounding. Our findings show that audio or transcripts often provide the strongest cues for comprehension, while temporal localization remains the most challenging setting. We also observe persistent demographic disparities across modSONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models els, emphasizing the need for more equitable multimodal evaluation. Overall, SONIC-O1 offers practical testbed for evaluating MLLMs in realistic audio-video scenarios, and we hope it will guide future work on stronger temporal reasoning and broader benchmark coverage."
        },
        {
            "title": "Impact Statement",
            "content": "SONIC-O1 is designed to support the responsible development of audiovideo models. It is intended to help researchers, practitioners, and organizations better understand how these systems behave, especially in sensitive areas such as media analysis, education, and public information. Like other public benchmarks, the results could be taken out of context or selectively used in adversarial, reputational, or legal situations if the limitations are ignored. While SONIC-O1 examines gaps affecting marginalized demographic groups, this analysis is meant to identify and reduce potential harms, not to reinforce stereotypes, offend communities, or be used against any group."
        },
        {
            "title": "Acknowledgement",
            "content": "Resources used in preparing this research were provided, in part, by the Province of Ontario and the Government of Canada through CIFAR, as well as companies sponsoring the Vector Institute (http://www. vectorinstitute.ai/#partners). This research was funded by the European Unions Horizon Europe research and innovation programme under the AIXPERT project (Grant Agreement No. 101214389), which aims to develop an agentic, multi-layered, GenAI-powered framework for creating explainable, accountable, and transparent AI systems."
        },
        {
            "title": "References",
            "content": "Ataallah, K., Bakr, E. M., Ahmed, M., Gou, C., Pahwa, K., Ding, J., and Elhoseiny, M. Infinibench: benchmark for large multi-modal models in long-form movies and In Proceedings of the 2025 Conference on tv shows. Empirical Methods in Natural Language Processing, pp. 1949619523, 2025. Bai, J., Bai, S., Yang, S., Wang, S., Tan, S., Wang, P., Lin, J., Zhou, C., and Zhou, J. Qwen-vl: versatile visionlanguage model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. Bain, M., Huh, J., Han, T., and Zisserman, A. Whisperx: Time-accurate speech transcription of long-form audio. INTERSPEECH 2023, 2023. Boyd, R. L., Ashokkumar, A., Seraj, S., and Pennebaker, J. W. LIWC-22: Linguistic inquiry and word count. https://www.liwc.app/, 2022. Computer software. Chen, G., Liu, Y., Huang, Y., He, Y., Pei, B., Xu, J., Wang, Y., Lu, T., and Wang, L. Cg-bench: Clue-grounded question answering benchmark for long video understanding. arXiv preprint arXiv:2412.12075, 2024. Chen, S., Li, H., Wang, Q., Zhao, Z., Sun, M., Zhu, X., and Liu, J. Vast: vision-audio-subtitle-text omni-modality foundation model and dataset. Advances in Neural Information Processing Systems, 36:7284272866, 2023. Cheng, Z., Leng, S., Zhang, H., Xin, Y., Li, X., Chen, G., Zhu, Y., Zhang, W., Luo, Z., Zhao, D., et al. Videollama 2: Advancing spatial-temporal modeling and audio understanding in video-llms. arXiv preprint arXiv:2406.07476, 2024. Creative Commons. Attribution 4.0 international (cc https://creativecommons.org/ by 4.0). licenses/by/4.0/, 2013. Accessed: 2025-11-10. DeepMind. Gemini 3 pro model card. Technical report, Google DeepMind, 2025. URL https://storage. googleapis.com/deepmind-media/ Model-Cards/Gemini-3-Pro-Model-Card. pdf. Technical Report. Fang, X., Mao, K., Duan, H., Zhao, X., Li, Y., Lin, D., and Chen, K. Mmbench-video: long-form multi-shot benchmark for holistic video understanding. Advances in Neural Information Processing Systems, 37:89098 89124, 2024. Fu, C., Lin, H., Long, Z., Shen, Y., Dai, Y., Zhao, M., Zhang, Y.-F., Dong, S., Li, Y., Wang, X., et al. Vita: Towards open-source interactive omni multimodal llm. arXiv preprint arXiv:2408.05211, 2024a. Fu, C., Zhang, Y.-F., Yin, S., Li, B., Fang, X., Zhao, S., Duan, H., Sun, X., Liu, Z., Wang, L., et al. Mme-survey: comprehensive survey on evaluation of multimodal llms. arXiv preprint arXiv:2411.15296, 2024b. Fu, C., Dai, Y., Luo, Y., Li, L., Ren, S., Zhang, R., Wang, Z., Zhou, C., Shen, Y., Zhang, M., et al. Video-mme: The first-ever comprehensive evaluation benchmark of multi-modal llms in video analysis. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 2410824118, 2025. Galougah, S. S., Raj, R., Chowdhury, S., Nag, S., and Duraiswami, R. Aura: fine-grained benchmark and decomposed metric for audio-visual reasoning. arXiv preprint arXiv:2508.07470, 2025. 9 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Geng, T., Zhang, J., Wang, Q., Wang, T., Duan, J., and Zheng, F. Longvale: Vision-audio-language-event benchmark towards time-aware omni-modal perception of long videos. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1895918969, 2025. Maaz, M., Rasheed, H., Khan, S., and Khan, F. S. Videochatgpt: Towards detailed video understanding via large vision and language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL 2024), 2024. Google Developers. YouTube Data API v3. Google, 2025. URL https://developers.google.com/ youtube/v3. Accessed: 2025-11-10. Jiang, S., Liang, J., Wang, J., Dong, X., Chang, H., Yu, W., Du, J., Liu, M., and Qin, B. From specific-mllms to omnimllms: survey on mllms aligned with multi-modalities. In Findings of the Association for Computational Linguistics: ACL 2025, pp. 86178652, 2025. Lei, J., Yu, L., Bansal, M., and Berg, T. Tvqa: Localized, compositional video question answering. In Proceedings of the 2018 conference on empirical methods in natural language processing, pp. 13691379, 2018. Li, C., Chen, Y., Ji, Y., Xu, J., Cui, Z., Li, S., Zhang, Y., Tang, J., Song, Z., Zhang, D., et al. Omnivideobench: Towards audio-visual understanding evaluation for omni mllms. arXiv preprint arXiv:2510.10689, 2025a. Li, F., Zhang, R., Zhang, H., Zhang, Y., Li, B., Li, W., Ma, Z., and Li, C. Llava-next-interleave: Tackling multiimage, video, and 3d in large multimodal models. arXiv preprint arXiv:2407.07895, 2024a. Li, H., Deng, A., Ke, Q., Liu, J., Rahmani, H., Guo, Y., Schiele, B., and Chen, C. Sports-qa: large-scale video question answering benchmark for complex and professional sports. arXiv preprint arXiv:2401.01505, 2024b. Li, J., Li, D., Xiong, C., and Hoi, S. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International conference on machine learning, pp. 1288812900. PMLR, 2022. Li, Y., Chen, X., Jiang, S., Shi, H., Liu, Z., et al. Uni-moe2.0-omni: Scaling language-centric omnimodal large model with advanced moe, training and data. arXiv preprint arXiv:2511.12609, 2025b. URL https:// arxiv.org/abs/2511.12609. Lin, C.-Y. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481. Association for Computational Linguistics, 2004. URL https://aclanthology.org/W04-1013. Ning, M., Zhu, B., Xie, Y., Lin, B., Cui, J., Yuan, L., Chen, D., and Yuan, L. Video-bench: comprehensive benchmark and toolkit for evaluating video-based large language models. Computational Visual Media, 2025. Nissenbaum, H. Privacy as contextual integrity. Wash. L. Rev., 79:119, 2004. OpenAI. Gpt-4o system card. org/abs/2410.21276, 2024. arXiv:2410.21276. https://arxiv. arXiv preprint OpenAI. Gpt-5 mini model documentation. https://platform.openai.com/docs/ models/gpt-5-mini, 2026. Accessed January 2026. Patraucean, V., Smaira, L., Gupta, A., Recasens, A., Markeeva, L., Banarse, D., Koppula, S., Malinowski, M., Yang, Y., Doersch, C., et al. Perception test: diagnostic benchmark for multimodal video models. Advances in Neural Information Processing Systems, 36:42748 42761, 2023. Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., et al. Learning transferable visual models from natural In International conference on language supervision. machine learning, pp. 87488763. PmLR, 2021. Rawal, R., Saifullah, K., Farre, M., Basri, R., Jacobs, D., Somepalli, G., and Goldstein, T. Cinepile: long video question answering dataset and benchmark. arXiv preprint arXiv:2405.08813, 2024. Song, E., Chai, W., Wang, G., Zhang, Y., Zhou, H., Wu, F., Chi, H., Guo, X., Ye, T., Zhang, Y., et al. Moviechat: From dense token to sparse memory for long video understanding. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 18221 18232, 2024. Sun, C., Myers, A., Vondrick, C., Murphy, K., and Schmid, C. Videobert: joint model for video and language representation learning. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 7464 7473, 2019. Liu, Y., Li, S., Liu, Y., Wang, Y., Ren, S., Li, L., Chen, S., Sun, X., and Hou, L. Tempcompass: Do video llms really understand videos? arXiv preprint arXiv:2403.00476, 2024. Tausczik, Y. R. and Pennebaker, J. W. The psychological meaning of words: Liwc and computerized text analysis methods. Journal of language and social psychology, 29 (1):2454, 2010. SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Zimmer, M. but the data is already public: on the ethics of research in facebook. In The ethics of information technologies, pp. 229241. Routledge, 2020. Tong, Z., Song, Y., Wang, J., and Wang, L. Videomae: Masked autoencoders are data-efficient learners for selfsupervised video pre-training. Advances in neural information processing systems, 35:1007810093, 2022. U.S. Equal Employment Opportunity Commission. Age discrimination, a. URL https://www.eeoc.gov/ age-discrimination. Accessed: 2025-01-27. U.S. Equal Employment Opportunity Commission. Sexbased discrimination, b. URL https://www.eeoc. gov/sex-based-discrimination. Accessed: 2025-01-27. U.S. Equal Employment Opportunity Commission. Section 15: Race and color discrimination, 2006. URL https://www.eeoc.gov/laws/guidance/ section-15-race-and-color-discrimination. EEOC Compliance Manual. Accessed: 2025-01-27. Wang, W., He, Z., Hong, W., Cheng, Y., Zhang, X., Qi, J., Ding, M., Gu, X., Huang, S., Xu, B., et al. Lvbench: An extreme long video understanding benchmark. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2295822967, 2025. Wu, H., Li, D., Chen, B., and Li, J. Longvideobench: benchmark for long-context interleaved video-language understanding. Advances in Neural Information Processing Systems, 37:2882828857, 2024. Xie, L., Wei, G. Z., Kuthiala, A., Zheng, C., Bal, A., Dabhi, M., Wen, L., Rustagi, T., Lai, E., Khyalia, S., et al. Maverix: Multimodal audio-visual evaluation reasoning index. arXiv preprint arXiv:2503.21699, 2025. Xu, J., Guo, Z., Hu, H., Chu, Y., Wang, X., He, J., et al. Qwen3-omni technical report. arXiv preprint arXiv:2509.17765, 2025. URL https://arxiv. org/abs/2509.17765. Yao, Y., Yu, T., Zhang, A., Wang, C., Cui, J., Zhu, H., Cai, T., Li, H., Zhao, W., He, Z., et al. Minicpm-v: gpt-4v level mllm on your phone, 2024. Ye, H., Zhang, H., Daxberger, E., Chen, L., Lin, Z., Li, Y., Zhang, B., You, H., Xu, D., Gan, Z., et al. Mm-ego: Towards building egocentric multimodal llms for video qa. arXiv preprint arXiv:2410.07177, 2024. Zhou, J., Shu, Y., Zhao, B., Wu, B., Liang, Z., Xiao, S., Qin, M., Yang, X., Xiong, Y., Zhang, B., et al. Mlvu: Benchmarking multi-task long video understanding. In Proceedings of the Computer Vision and Pattern Recognition Conference, pp. 1369113701, 2025a. Zhou, Z., Wang, R., and Wu, Z. Daily-omni: Towards audio-visual reasoning with temporal alignment across modalities. arXiv preprint arXiv:2505.17862, 2025b. 11 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models A. Data Collection Details As illustrated in the Retrieval & Metadata stage of Figure A1, we retrieve real-world scenario videos using the YouTube Data API v3 (Google Developers, 2025), following established ethical frameworks for web-based data collection that respect contextual integrity and avoid privacy violations (Zimmer, 2020; Nissenbaum, 2004). We restrict candidates to videos published under the Creative Commons Attribution 4.0 (CC BY 4.0) license (Creative Commons, 2013), which explicitly permits redistribution for research purposes. Videos under YouTubes Standard License are excluded due to redistribution restrictions. Our search strategy uses topic-specific base queries that capture real-world interactions (e.g., consultations, interviews, hearings), and expands them with demographic descriptors (e.g., young adult, woman, Black, Middle Eastern, First Nations), following federal standards for demographic categorization (U.S. Equal Employment Opportunity Commission, 2006), to improve coverage. These demographic keywords are used only for query expansion and are not treated as labels; final demographic annotations are produced later through human-verified review process (Appendix A.7). This approach ensures that our data collection respects the original context and licensing terms under which content was shared (Zimmer, 2020), avoiding privacy violations that can occur when data is extracted from contexts with restricted access expectations. Figure A1. SONIC-O1 data curation and annotation pipeline. We first retrieve candidate real-world scenario videos via topicand demographic-augmented search queries and filter by licensing constraints (CC BY 4.0), then download video/audio and any available captions. Human reviewers screen candidates for quality and coverage, after which we parse metadata and perform balanced sampling (e.g., by topic and duration). For benchmark construction, videos are segmented into 10-minute chunks for summarization (T1) and into overlapping 3-minute segments for MCQ and temporal localization (T2T3). If captions are unavailable, we obtain speech transcripts via ASR; otherwise we use downloaded captions. Draft demographic metadata and task annotations can be bootstrapped with model-assisted tools (e.g., Gemini/ASR) and are subsequently corrected and verified by domain experts, with ambiguous or low-evidence items removed. Arrows indicate the flow from raw retrieval to finalized, human-verified instances. A.1. Curation Strategy and Licensing Compliance To construct SONIC-O1, we followed structured video curation pipeline combining keyword-based API retrieval with strict license filtering. We restricted candidates to CC BY 4.0 videos and performed additional manual verification to ensure redistribution and research-use compliance. 12 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Example Search Query Construction (4 of 13 Topics Shown) Base Query Examples Across Topics: Topic 2: Job Interviews (Professional Domain) panel interview full interview technical interview session full candidate interview recording Topic 1: Patient-Doctor Consultations (Community/Public Health) doctor patient conversation full session clinic consultation recording telehealth visit recording Topic 5: Courtroom Proceedings (Legal/Civic) oral argument sentencing hearing full recording courtroom small claims court full hearing official recording Topic 7: Public Transportation Conflicts (Community/Public Health) bus passenger fight driver -news -compilation train passenger confrontation cctv airport security passenger meltdown bodycam Demographic Variations Generated (Example: Job Interviews): Race: Black panel interview full interview, Asian technical interview session full, Hispanic candidate interview recording, Middle Eastern panel interview full interview, White technical interview session full Gender: woman panel interview full interview, man technical interview session full Age: young adult panel interview full interview, middle aged technical interview session full, older adult candidate interview recording Special Handling for Underrepresented Groups: Arab: Multiple variations used: Arab, Middle Eastern, Arabic, MENA, Arab American Indigenous: Multiple variations used: Indigenous, Native American, First Nations, Aboriginal, tribal This approach generated approximately 4050 search queries per topic (46 base queries 11 demographic variations), yielding diverse representation across racial, gender, and age groups. Total queries across 13 topics: 600 searches. 13 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models A.2. Inclusion Criteria Candidate videos must satisfy the following requirements: Time of curation: Only videos published from January 2020 to October 2025 to ensure topical coverage, availability, and consistency in retrieval conditions. Keywords: Topic-specific base queries that capture authentic, real-world interactions, further expanded with demographic descriptors for race, gender, and age. Video lengths: Recordings sufficient to capture complete interactions, typically ranging from 30 seconds-60 minutes. Language: Primarily English-language recordings with clear, intelligible speech to support multimodal transcription and reasoning tasks. Topics & categories: Videos covering 13 topics across five domains, selected to reflect common, high-impact real-world interactions relevant for multimodal reasoning. Demographic diversity: Videos featuring speakers across diverse race (White, Black, Asian, Indigenous, Arab, Hispanic), gender (male, female), and age groups (1824, 2539, 40+), following federal standards for demographic categorization (U.S. Equal Employment Opportunity Commission, 2006; b;a). A.3. Exclusion Criteria Our upstream retrieval produces many candidates that are off-topic, low quality, or unsuitable for ethical/legal use. We therefore apply two-stage filtering procedure: (i) lightweight metadata-only heuristics to remove obvious spam and outliers, followed by (ii) domain-expert human screening pass (Figure A2) in which candidates are manually viewed and labeled as Good or Bad. We retain only videos labeled Good and licensed under CC BY 4.0. video is rejected if it meets any of the following criteria: Low media quality: poor audio or visual quality that hinders transcription, comprehension, or annotation. Non-continuous content: heavily edited or montage-style videos that do not preserve continuous interactions. Irrelevant topic: content not aligned with the target domains or study scope. Sensitive or unsafe content: videos containing sensitive personal data or content that cannot be used in an ethical manner. Language constraint: non-English, audio-dominant content that prevents reliable English annotation. License constraint: videos under YouTubes Standard License (i.e., not CC BY 4.0). A.4. Filtering and Media Processing Stage 1: Metadata-only pre-filtering. Before human review, we apply lightweight heuristics using only public metadata to remove obviously low-quality uploads while preserving licensing and safety constraints. We drop candidates with missing basic signals (e.g., missing duration), enforce duration window, and require minimum view count (500 by default; 100 for scarce topics such as emergency response). We further filter clickbait/spam via simple title-based signals (e.g., excessive capitalization with caps ratio > 0.55, heavy punctuation/repetition, emoji spam with > 5 emojis, and common spam phrases), suspicious engagement (e.g., > 10k views with 0 likes and 0 comments, or > 5k views with engagement rate < 104), and invalid channels (e.g., extremely short or numeric-only channel identifiers). Remaining candidates receive simple metadata quality score that rewards suitable durations and healthy engagement and penalizes generic/clickbait titles. Stage 2: Human Review & Selection. During Human Review & Selection (Figure A1), reviewers label each candidate video as Good or Bad based on topic relevance, audiovisual quality, safety, and licensing. 14 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Balanced sampling and media pre-processing. From the human-accepted pool, we construct the final set using topic-capped, balanced sampling procedure designed to maximize coverage under licensing and quality constraints. Concretely, we cap each topic at up to 25 videos when sufficient candidates exist and stratify selections across short, medium, and long durations to avoid over-representing any single length regime. When topic contains fewer than the target after filtering, we include all eligible videos for that topic (i.e., no downsampling). For each selected video, we download the MP4 (capped at 1080p to balance quality and storage), extract stereo audio (48 kHz, 192 kbps), and retrieve English captions via the YouTube caption API when available. Videos without native captions are flagged for automatic transcription. The output is three-modality package per videovideo (MP4), audio (M4A), and text (SRT)plus JSON metadata record containing IDs, timestamps, and processing artifacts for reproducibility. A.5. Video Filtering Summary Table A1 documents the filtering process from initial search to final dataset. Table A1. Video filtering summary. Counts of candidate videos at each curation stage: initial API retrieval during time 2020-2025, CC BY 4.0 license filtering, and final acceptance after metadata pre-filtering and human Good/Bad screening. The sharp reduction reflects deliberate prioritization of licensing compliance, annotation quality, and balanced coverage over scale"
        },
        {
            "title": "Stage",
            "content": "Initial search results After license filtering (CC BY 4.0 only) Final dataset (post quality assurance)"
        },
        {
            "title": "Count",
            "content": "2,237 1,794 231 A.6. Topical Coverage Table A2 summarizes SONIC-O1s topical coverage across five conversational domains and 13 topics, including video counts and duration statistics for the curated set. Table A2. Topical coverage of SONIC-O1. Breakdown of the 5 domains and 13 topics, reporting the number of curated videos (Samples) and their aggregate/average durations after the full selection pipeline. Category Topics Samples Total Duration (min) Avg Duration (min) Professional Educational Legal / Civic ServiceOriented Community Public Health / Job interviews; workplace meetings Parentteacher conferences Courtroom proceedings; community town halls Customer service; restaurant service; housing/apartment tours Medical (patientdoctor); emergency response; public transportation conflicts; mental-health counseling; Olympics/sports 49 18 32 63 69 708.8 651.5 843.8 726.3 681.2 14.5 36.2 26.4 11.3 9. A.7. AI-Assisted Annotation Process In the Demographics Annotation and Question Generation stages (Figure A1), we use Gemini 2.5 Model-assisted drafting. Flash to produce initial drafts from joint multimodal signals (video, audio, and captions), where captions are always available (downloaded captions when present, otherwise generated via automatic speech recognition (ASR) using Whisper (Bain et al., 2023)). This approach follows established practices in recent benchmarking efforts (Galougah et al., 2025; Ataallah et al., 2025). Importantly, all drafts are then reviewed and corrected by domain expert annotators (Appendix B.1) using our review interface (Appendix B.3). Only human-verified annotations are retained for dataset construction and benchmarking. Demographic annotation workflow. We first draft demographics at the video level as set of tags (e.g., race, gender, age, language) using Gemini 2.5 Flash, and annotators review and correct these tags. During audio-video question-answering (AVQA) instance construction, we propagate the verified video-level tags to each segment by expanding them into persegment counts of unique individuals per demographic combination. This expansion step is model-assisted but constrained 15 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models to the human-verified tag set (i.e., it cannot introduce new demographic values), and is again reviewed and corrected by the domain expert annotators. For each segment, we store human-verified demographic tags, optional confidence scores, and per-segment counts. For multi-person segments, we store counts of individuals grouped by unique demographic combinations (e.g., 2 White males aged 40+, 1 Asian female aged 25-39). Task-specific annotation workflow. For all three tasks (summarization, MCQ, temporal localization), we use Gemini 2.5 Flash to draft questions, answers, and task-specific annotations. All model outputs undergo mandatory human verification, correction, and revision. The full prompts are provided in Appendix A.7.1. Annotation schema. Each item includes the question or summary and reference answer; for Tasks 23, we additionally store short reasoning rationale. Each segment also includes human-verified demographic metadata (and optional confidence scores) with counts for each unique demographic combination when multiple individuals appear. A.7.1. AI GENERATION PROMPTS Below are the complete prompts used to generate preliminary annotation drafts for each task and the initial demographics prompt. All AI-generated outputs underwent mandatory human verification as described in Section A.8. Demographic Generation Prompt. Demographics: AI Annotation Generation You are demographics annotation specialist for academic research. Analyze the provided multimodal media (video, audio, and captions/transcripts) and identify the demographic characteristics of all individuals who appear visually or speak. Be objective and avoi stereotype-based assumptions. Return ONLY valid JSON that can be directly parsed (no extra text). Analyze this MULTIMODAL media to identify demographics of ALL people who appear visually or speak. MEDIA INFORMATION: - Title: {title} - Duration: {duration_seconds} seconds - Topic: {topic_name} INPUTS PROVIDED: You have access to multiple modalities for this analysis: 1. VIDEO: Visual content showing individuals (if available) 2. AUDIO: Sound/speech from individuals (may be embedded in video or separate) 3. TRANSCRIPT/CAPTIONS: Text representation of spoken content {transcript_preview} IMPORTANT: Use ALL available modalities together for the most accurate analysis. Cross-reference visual, audio, and text cues to identify and confirm demographics. --- ANALYSIS GUIDELINES BY MODALITY: **VIDEO ANALYSIS (when available):** - Race/Ethnicity: Primary visual assessment of facial features, skin tone - Gender: Visual presentation (appearance, clothing, mannerisms) - Age: Visual appearance (facial features, hair, physical characteristics) - Language: Can support audio analysis with lip movements **AUDIO ANALYSIS (always available in video or as separate audio):** - Gender: Vocal pitch and timbre (deep pitch -> Male, high pitch -> Female) - Age: Vocal characteristics (youthful energy vs. mature tone vs. older/quavering voice) - Language: Primary method for identifying spoken languages and accents 16 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models - Race/Ethnicity: MAY provide supporting evidence via accent, but use LOW confidence **TRANSCRIPT/CAPTION ANALYSIS (when available):** - Language: Confirms which languages are spoken - Speaker identification: Helps count unique individuals - Context: Provides semantic understanding of the interaction - Names/References: May help distinguish between speakers **CROSS-MODAL VERIFICATION:** - When you have both video and audio, verify gender and age across both modalities - Use transcript to confirm language identification from audio - Count unique individuals by combining visual appearances with distinct voices - Higher confidence when multiple modalities agree --- DEMOGRAPHIC CATEGORIES TO IDENTIFY: 1. RACE/ETHNICITY (select all that apply): - White: European descent appearance - Black: African descent appearance - Asian: East/Southeast/South Asian appearance - Indigenous: Native American/Aboriginal appearance - Arab: Middle Eastern/North African appearance - Hispanic: Latin American appearance - Note: Primarily visual assessment. Audio-only inference should have LOW confidence unless very strong accent indicators. 2. GENDER (select all that apply): - Male: Masculine presenting individuals OR deep vocal pitch - Female: Feminine presenting individuals OR high vocal pitch - Use visual cues first, audio cues second 3. AGE GROUPS (select all that apply): - Young (18-24): Visual appearance OR youthful voice - Middle (25-39): Visual appearance OR mature voice - Older adults (40+): Visual appearance OR older voice characteristics - Combine visual and audio cues for best accuracy 4. LANGUAGE (select all spoken): - Identify all languages and distinct accents heard - Use audio AND transcript to confirm - Default to [\"English\"] if only English is spoken --- ANALYSIS METHODOLOGY (MULTIMODAL): **Step 1: IDENTIFY INDIVIDUALS** - Count unique people visible in video - Count unique voices in audio (use transcript speaker labels if available) - Total = unique individuals across both modalities **Step 2: ASSESS EACH INDIVIDUAL** For each person: - If visible: Use video for race, gender, age (HIGH confidence) - If speaking: Use audio for gender, age, language (MEDIUM-HIGH confidence) - If both: Cross-verify and use HIGHEST confidence - Use transcript to confirm language and count speakers **Step 3: ASSIGN CONFIDENCE** - 0.9-1.0: Clear visual evidence OR audio + visual agreement - 0.7-0.89: Clear audio evidence OR single modality with good clarity SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models - 0.5-0.69: Uncertain (e.g., accent-based inference, unclear visuals) - Below 0.5: Do not include **Step 4: LIST UNIQUE DEMOGRAPHICS** - Aggregate all unique demographics across all individuals - Include only those meeting minimum confidence threshold --- OUTPUT FORMAT: Return ONLY this JSON structure with no additional text: { \"demographics_detailed\": { \"race\": [list unique races observed with sufficient confidence], \"gender\": [list unique genders observed with sufficient confidence], \"age\": [list unique age groups observed with sufficient confidence], \"language\": [list languages/accents actually spoken] }, \"demographics_confidence\": { \"race\": {\"race1\": confidence1, \"race2\": confidence2}, \"gender\": {\"gender1\": confidence1, \"gender2\": confidence2}, \"age\": {\"age1\": confidence1, \"age2\": confidence2}, \"language\": {\"language1\": confidence1} }, \"demographics_annotation\": { \"model\": \"{model_name}\", \"annotated_at\": \"{timestamp}\", \"individuals_count\": total_number_of_unique_individuals, \"modalities_used\": [list of \"video\", \"audio\", \"transcript\" that were available], \"explanation\": \"Brief factual description combining visual, audio, and transcript observations.\" } } CRITICAL REMINDERS: - Use ALL available modalities (video, audio, transcript) together - Cross-verify demographics across modalities for higher confidence - Return ONLY valid JSON - No text before or after the JSON - Empty arrays are acceptable if no confident matches found Task 1: Summarization Generation Prompt. Task 1: AI Annotation Generation (Map Phase) You are precise video segment summarizer. SEGMENT INFORMATION: - Segment time: {start_time}s to {end_time}s ({duration}s duration) - Video title: {title} - Topic: {topic_name} TRANSCRIPT/CAPTIONS: {transcript_text} YOUR TASK: Summarize this video segment concisely and accurately. RULES: - Maximum {max_words} words 18 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models - Keep strict chronology - describe events in the order they occur - Prefer facts that are audible in the transcript or visible in the video - If nothing meaningful happens, state \"No salient events in this segment\" - Be specific: include names, actions, objects, and key details - Do NOT speculate beyond what you see/hear OUTPUT FORMAT (JSON): { \"segment_start\": \"{start_time}\", \"segment_end\": \"{end_time}\", \"summary\": \"120 words maximum describing what happens in this segment...\", \"mini_timeline\": [ {\"time\": \"MM:SS\", \"title\": \"Event name\", \"note\": \"One line description\"}, {\"time\": \"MM:SS\", \"title\": \"Another event\", \"note\": \"Brief detail\"} ], \"entities\": [\"names\", \"objects\", \"places\", \"key terms mentioned\"], \"confidence\": 0.85 } CRITICAL: - Return ONLY valid JSON - No markdown, no extra text - Timestamps in mini_timeline should be relative to SEGMENT start time - Include 2-5 timeline items for this segment Begin analysis: Task 1: AI Annotation Generation (Reduce Phase) You are an editor merging ordered segment summaries into comprehensive video summary. VIDEO INFORMATION: - Video ID: {video_id} - Title: {title} - Topic: {topic_name} - Total duration: {duration}s - Number of segments: {num_segments} SEGMENT SUMMARIES: {segment_summaries_json} YOUR TASK: Merge these segment summaries into single comprehensive video summary. PRODUCE: 1. TL;DR: {num_bullets} concise bullet points 2. Detailed summary: {max_words_detailed} words (structure: Purpose -> Key Points -> Outcomes) 3. Global timeline: {timeline_min}-{timeline_max} items covering the entire video 4. Glossary: {glossary_min}-{glossary_max} key entities/terms from the full video RULES: - Remove duplicates across segments - Keep strict chronological order - Be concise and factual - Prefer events that appear in multiple segments or are clearly important - Timeline should span from video start to end with actual timestamps (HH:MM:SS format) - Glossary should include: peoples names, important objects, locations, technical terms OUTPUT FORMAT (JSON): 19 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models \"summary_short\": [ \" First key point...\", \" Second key point...\", \" Third key point...\", \" Fourth key point...\", \" Fifth key point...\" ], \"summary_detailed\": \"200-300 word comprehensive summary. Start with the videos purpose, cover key points in chronological order, and conclude with outcomes or main takeaways...\", \"timeline\": [ { }, { }, { } \"start\": \"00:00:12\", \"end\": \"00:00:45\", \"title\": \"Introduction\", \"note\": \"Brief description\" \"start\": \"00:05:30\", \"end\": \"00:06:15\", \"title\": \"Main topic\", \"note\": \"Key details\" \"start\": \"00:12:00\", \"end\": \"00:13:30\", \"title\": \"Conclusion\", \"note\": \"Final points\" ], \"glossary\": [\"Entity 1\", \"Entity 2\", \"Important Term\", \"Key Person\", \"Location\"], \"confidence\": 0.88 { } CRITICAL: - Return ONLY valid JSON - No markdown, no extra text - Timeline timestamps must be in HH:MM:SS format relative to VIDEO start - Confidence should reflect how well segments agreed and information quality Begin merging Task 2: Multiple-Choice Question Generation Prompt. Task 2: AI Annotation Generation You are meticulous multimodal annotator creating challenging multiple-choice questions that test deep understanding of the content. SEGMENT INFORMATION: - Video ID: {video_id} - Topic: {topic_name} - Time range: {start_time}s to {end_time}s - Duration: {duration}s TRANSCRIPT/CAPTIONS: {transcript_text} YOUR TASK: Create challenging, thought-provoking multiple-choice questions that require reasoning and inference to answer correctly. 20 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models MULTIMODAL UNDERSTANDING FOCUS: This task tests models ability to comprehend and reason about: - Visual information: Actions, objects, settings, body language, demonstrations, equipment, text on screen - Auditory information: Spoken dialogue, explanations, tone, background sounds, verbal instructions - Integrated understanding: Connecting what is shown with what is said to form complete understanding CRITICAL: Questions should naturally require multiple sources of information when possible - Prioritize questions where explanations clarify what is demonstrated (or vice versa) - Create questions about relationships between actions and explanations - Test understanding that requires integrating multiple cues QUESTION DIFFICULTY REQUIREMENTS: AVOID simple recall questions - Dont ask questions that can be answered by simply remembering one fact. PREFER questions that require: - Integration: \"Based on the procedure shown and the safety warnings mentioned, what complication is being prevented?\" - Correlation: \"What is the key difference between the described technique and the demonstrated approach?\" - Inference: \"What condition is most likely being assessed based on the complaints and examination techniques?\" - Guided Analysis: \"According to the explanation, what is the purpose of the specific hand positioning observed?\" - Application: \"If patient presented with the symptoms described and signs shown, which intervention would be most appropriate?\" - Cause-and-effect: \"What is the physiological reason mentioned for the clinical finding observed?\" Question complexity guidelines: - Require connecting 2-3 pieces of information - Ask \"why\" or \"how\" questions that need comprehensive understanding - Test comprehension of relationships between different elements - Require understanding of underlying principles from available evidence - Questions should be 15-30 words to allow for complexity OPTIONS REQUIREMENTS: - Provide EXACTLY {num_options} options labeled (A) through ({last_option_letter}) - The LAST option ({last_option_letter}) must ALWAYS be: \"({last_option_letter}) Not enough evidence\" - IMPORTANT: \"Not enough evidence\" IS valid correct answer when: * The content doesnt provide sufficient information to answer confidently * Multiple interpretations are equally plausible from the evidence * Key information needed for reasoning is missing * The question requires inference beyond what can be reasonably concluded - Use \"Not enough evidence\" as the correct answer approximately 10-15% of the time - For content-based answers (positions through {second_to_last_letter}), create principled distractors: * One near-miss (plausible but incorrect reasoning) * One salient decoy (addresses part of the question but misses integration) * One partial trap (correct if considering incomplete information) * The correct answer (requires proper comprehensive understanding) ANSWER POSITION DISTRIBUTION: - Distribute correct answers evenly across ALL {num_options} positions ({distribution_percentage}% each) - DO NOT favor middle positions - Consciously vary answer placement - DO NOT avoid position ({last_option_letter}) - Use it when genuinely appropriate SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models EVIDENCE TAGS: Choose from this controlled vocabulary ONLY: {evidence_tags_list} Use tags that are actually present in the content. requires_audio: - Set to true when transcript/audio is NECESSARY to answer correctly - Set to false ONLY if visual information alone is sufficient - Default to true for questions requiring comprehensive understanding OUTPUT FORMAT: CRITICAL: Return SINGLE JSON object (not an array): { \"question\": \"string\", \"options\": [\"(A) ...\", \"(B) ...\", \"(C) ...\", \"(D) ...\", \"({last_option_letter}) Not enough evidence\"], \"answer_index\": integer (0-{max_index}), \"answer_letter\": \"string (A-{last_option_letter})\", \"rationale\": \"string\", \"evidence_tags\": [\"tag1\", \"tag2\"], \"requires_audio\": boolean, \"confidence\": float (0.0-1.0) } CRITICAL RULES: - Return ONLY ONE JSON object (not an array) - Return ONLY valid JSON with no markdown, no extra text - DO NOT wrap in json markers - DO NOT include \"Question:\" prefix - Each option MUST include its letter: \"(A)\", \"(B)\", etc. - Include BOTH \"answer_index\" AND \"answer_letter\" - The last option must ALWAYS be \"({last_option_letter}) Not enough evidence\" - USE \"Not enough evidence\" as correct answer 10-15% of the time - Distribute correct answers evenly across ALL positions ({distribution_percentage}% each) - Only use evidence_tags from the provided list - Questions must require reasoning, inference, or application (15-30 words) Now generate ONE MCQ question as single JSON object for this segment: Task 3: Temporal Localization Generation Prompt. Task 3: AI Annotation Generation You are careful video annotator creating temporal reasoning questions. VIDEO CONTEXT: - Video ID: {video_id} - Duration: {duration} seconds (you are watching from 0.0s to {duration}s) - Target: Generate {num_questions} questions - Time unit: SECONDS (all timestamps must be decimal seconds) TRANSCRIPT (if available): {transcript_text} =========================================================================== STEP 1: WATCH AND UNDERSTAND THE VIDEO =========================================================================== First, watch the entire video carefully from start to finish. - Note major events, actions, and scene changes - Pay attention to both audio (speech, sounds) and visual (actions, objects) cues - Observe the temporal flow and relationships between events 22 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models - Prefer anchors tied to sharp audio/speech events (because they are easier to re-locate) While watching, internally number events in the order they occur: E1, E2, E3, ... You will refer to these IDs in your rationale to make the temporal chain unambiguous. =========================================================================== STEP 2: IDENTIFY KEY EVENTS WITH PRECISE TIMESTAMPS =========================================================================== As you watch, note down significant events with their exact timing in DECIMAL SECONDS: CRITICAL TIMESTAMP FORMAT: [OK] ALWAYS use decimal seconds: 5.2, 45.0, 78.5, 125.0 [X] NEVER use MM:SS format: 1:18, 2:30, 5:45 [X] NEVER concatenate minutes+seconds: \"1 minute 18 seconds\" is 78.0, NOT 118 CONVERSION RULE (important!): - If you think \"1 minute 18 seconds\" -> calculate 1 60 + 18 = 78.0 seconds - If you think \"2 minutes 30 seconds\" -> calculate 2 60 + 30 = 150.0 seconds - If you think \"5 seconds\" -> write 5.0 seconds Example event list (create your own): Person says \"hello\" at 5.2 seconds Door closes at 23.8 seconds Phone rings at 67.5 seconds Person picks up phone at 71.0 seconds All times must be between 0.0 and {duration}. =========================================================================== STEP 3: PLAN YOUR QUESTIONS =========================================================================== Now select {num_questions} event pairs that have clear temporal relationships. For each question, identify: 1. ANCHOR event (the reference point) ideally something sharp like speech or distinct visual change 2. TARGET event (what were searching for) 3. TEMPORAL RELATION between them TEMPORAL RELATIONS TO USE (mix these): after - Target occurs sometime after anchor completes Example: \"After the speaker says lets begin, when does the camera show the desk?\" once_finished - Target occurs immediately after anchor completes Example: \"Once the woman finishes writing, when does she turn around?\" next - Target is the next occurrence of similar action/person/event Example: \"When is the next time the teacher speaks after the student asks question?\" during - Target happens while anchor is ongoing Example: \"While the blue slide is displayed, when does the speaker point to the chart?\" before - Target happens before anchor (use sparingly) Example: \"Before the host introduces the guest, when does the music start?\" SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models QUALITY CHECKS FOR EACH QUESTION: - Both anchor and target clearly exist in the video - Theres genuine temporal relationship (not random) - The question tests temporal reasoning, not just recognition - Timestamps are verifiable by watching the video - Question is specific and unambiguous - CRITICAL: end_s captures the COMPLETE event, not just when it begins - For speech: end when speaker finishes the complete thought/sentence - For actions: end when action fully completes - For visual elements: end when element disappears or transitions away =========================================================================== STEP 4: VERIFY YOUR TIMESTAMPS =========================================================================== For each question you plan to generate: VERIFICATION CHECKLIST: 1. [OK] Locate anchor event -> Note time in decimal seconds 2. [OK] Locate target event START -> Note time in decimal seconds 3. [OK] Locate target event END -> Watch until event FULLY COMPLETES - Dont stop at first appearance - watch the entire event unfold - For speech: wait until the speaker finishes the complete sentence/explanation - For actions: wait until action concludes (not just begins) - For visual elements: note when they disappear or transition 4. [OK] Verify temporal relationship is correct 5. [OK] Double-check: converted MM:SS to pure seconds correctly? 6. [OK] Confirm both times are between 0.0 and {duration} 7. If two plausible target moments are <0.4 seconds apart AND the video frame rate is unknown treat this as ambiguous and abstain EXAMPLE VERIFICATION: - observe an event that appears to be \"1 minute 23 seconds\" into video - CALCULATION: 1 60 + 23 = 83 seconds - [OK] CORRECT: Write \"start_s\": 83.0 - [X] WRONG: Write \"start_s\": 123 (this is concatenation error!) - [X] WRONG: Write \"start_s\": \"1:23\" (wrong format!) =========================================================================== STEP 5: GENERATE JSON OUTPUT =========================================================================== Now output EXACTLY {num_questions} questions in JSON array format: [ { \"question_index\": 0, \"question\": \"After [anchor description], when does [target description] happen?\", \"temporal_relation\": \"afteronce_finishednextduringbefore\", \"anchor_event\": \"Brief description of anchor\", \"target_event\": \"Brief description of target\", \"answer\": { \"start_s\": 78.5, \"end_s\": 82.0 }, \"requires_audio\": true, \"confidence\": 0.9, \"abstained\": false, \"rationale_model\": \"E1 (anchor) at 65.0s -> E2 (target) at 78.5s, relation=after, target spans 78.5-82.0s. All times in seconds.\" }, ... 24 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models ] REQUIRED FIELDS (do not add or remove fields): - question_index: 0, 1, 2, ... (integers starting from 0) - question: Natural language question in English - temporal_relation: Must be one of: after, once_finished, next, during, before - anchor_event: One sentence describing the anchor - target_event: One sentence describing the target - answer.start_s: Decimal seconds when target BEGINS (or null if abstained) - answer.end_s: Decimal seconds when target COMPLETES/ENDS (or null if abstained) CRITICAL: end_s must capture when the event FINISHES, not just when it starts For speech events: end when the complete sentence/thought finishes For visual events: end when element disappears or transitions away For actions: end when the action fully completes - requires_audio: true if audio is needed to answer, false if purely visual - confidence: Float 0.0-1.0 indicating your certainty - abstained: true only if target event does not exist in video OR events are too temporally close to disambiguate - rationale_model: Detailed explanation with timestamps in decimal seconds - Keep rationale concise (<= 80 words) - Refer to event IDs E1, E2, ... to make the sequence clear - Always restate anchor time and target time (both start AND end) ABSTENTION RULES: - If anchor exists but target does NOT exist Set abstained=true, answer times=null - If temporal relationship cannot be determined Set abstained=true, answer times=null - If two plausible target moments are closer than 0.4 seconds Set abstained=true, answer times=null - Provide clear explanation in rationale_model CRITICAL REQUIREMENTS: [OK] Generate EXACTLY {num_questions} question objects [OK] Use ONLY decimal seconds (5.2, 78.0, 125.5) - NEVER MM:SS format [OK] Convert any MM:SS thinking to seconds: (M 60 + S) [OK] All times must be between 0.0 and {duration} [OK] Return ONLY the JSON array (no markdown, no extra text) [OK] Do NOT fabricate events that dont exist in the video [OK] Include detailed rationale with specific timestamps [OK] Keep rationale concise (<= 80 words) and refer to E1, E2, ... [X] Do NOT use MM:SS format in answer fields (like 1:18 or 2:30) [X] Do NOT concatenate minutes and seconds (78 is NOT \"one eighteen\") [X] Do NOT output markdown code blocks or explanations [X] Do NOT create questions where anchor=target [X] Do NOT skip required fields Now begin your annotation process following ALL five steps above. Think carefully about timestamps - convert any MM:SS to decimal seconds! Output your JSON array: A.8. Task-Specific Annotation Procedures Task 1: Video summarization. This task evaluates comprehension of the full recording. For videos under 10 minutes, we summarize the full audiovisual input and captions directly. Longer videos are split into 10-minute segments (with shorter final segment); each segment is summarized independently and merged during the reducing phase into (i) narrative summary and (ii) bullet-point summary. Summaries emphasize causal structure, key actions, and outcomes. Any model-generated drafts are edited or rewritten by humans for correctness and style. Task 2: Multiple-Choice Questions (MCQs). For closed-ended evaluation, videos longer than 3 minutes are segmented with 30-second overlaps to preserve context. For each segment, we generate one MCQ with four answer choices plus fifth option (Not enough evidence) for cases where the segment does not provide sufficient information. Each MCQ is 25 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models accompanied by brief rationale that explicitly references visual and auditory cues supporting the correct answer. Task 3: Temporal localization. This task evaluates temporal grounding and event ordering. Using the same segmentation as the MCQ task, we construct multiple questions per segment based on an anchor event and target event occurring before, after, during, or immediately following the anchor. During annotation, human annotators watch each segment independently and provide timestamps relative to the segment start (0.0s to segment duration), which then converted to absolute time by adding the segment start time to the relative predictions giving the ground truth annotations. The dataset contains 3,392 instances, with up to three questions per segment, each annotated with the correct relation, temporal interval, and supporting rationale. This enables evaluation of both localization accuracy and temporal reasoning quality. Ethical and licensing compliance. All datasets comply with their original licenses (e.g., Creative Commons Attribution 4.0 International (CC BY 4.0), research-only use) and respect data provenance and API terms. Personally identifiable or sensitive content is excluded wherever it cannot be ethically anonymized. Our process aligns with responsible AI and data governance standards (e.g., EU AI Act and NIST AI RMF principles). B. Team and Annotation / Review Guidelines B.1. Team Formation The annotation team consisted of 5 domain experts with strong English proficiency and research experience in visionlanguage learning: 2 PhD holders, 2 Masters students, and 1 research scientist. The team composition was gender-balanced (2 male, 3 female) and geographically diverse, with members from East Asia, Middle East, North America, and South Asia. All annotators completed task-specific training before beginning annotation work. The annotation workflow followed hierarchical review structure: Masters students conducted initial annotations, which were reviewed by PhD holders, with the research scientist providing final oversight and quality control. The team held weekly calibration meetings to discuss edge cases, resolve annotation disagreements, and ensure consistency across the dataset. When disagreements arose, they were escalated to the weekly calibration meetings and resolved synchronously through discussion among domain experts. Annotators compared their rationales against the labeling guidelines and the available evidence from the multimodal inputs (video, audio, and captions). We explicitly solicited perspectives from multiple domain experts to reduce individual bias and clarify ambiguous cases. Decisions were made by consensus when possible; otherwise we adopted majority vote among the reviewing domain experts, with the research scientist serving as the final tie-breaker and recording the final decision to maintain consistency. B.2. Guidelines To ensure annotation quality and consistency, annotators followed these core principles:"
        },
        {
            "title": "Annotation Quality Checklist",
            "content": "Before Starting Annotation: Specify the topic being annotated (e.g., Topic 2: Job Interviews) Specify the task being reviewed (Task 1: Summarization / Task 2: MCQ / Task 3: Temporal Localization) Watch the entire video fully before annotation Review any available captions or transcripts for context During Annotation: Ground all questions, answers, and rationales in observable audio-visual content Flag cases with insufficient evidence rather than making assumptions Assign demographic attributes based on visual and auditory presentation in the video Revisit any timestamp as needed to verify accuracy Discuss ambiguous cases with team leads before finalizing Quality Control: Ensure all required fields are completed (question, answer, rationale, timestamps where applicable) Verify that rationales explicitly reference audio-visual evidence Check for internal consistency within annotations you reviewed After Review: Address reviewer feedback and revise annotations as needed Participate in weekly calibration meetings to discuss edge cases Update annotations based on refined guidelines from team discussions 26 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models For task-specific protocols (segmentation, timestamps, rationale structure), see Section A.8. B.3. Review Interface We developed custom web-based review interface to facilitate efficient human verification and correction of AI-generated annotations. The interface supports all three annotation tasks and provides real-time video playback, synchronized captions, and structured editing fields. Quality filtering interface. Figure A2 shows the initial quality filtering interface used during video selection. Reviewers assess each candidate video for topic relevance, audio-visual quality, and demographic coverage, marking videos as either Good or Bad for inclusion in the dataset. Task-specific review interfaces. Figures A3, A4, and A5 illustrate the task-specific interfaces for editing and validating annotations across the three tasks. Each interface provides: Video player: Embedded YouTube player with timestamp controls and caption display Edit fields: Structured input fields for questions, answers, rationales, and timestamps Demographics panel: JSON editor for verifying and correcting demographic labels with per-person annotations Navigation controls: Previous/Next buttons, jump functionality, and session management Auto-save: Changes persist automatically when navigating between items The interface ensures high annotation quality by requiring annotators to watch full videos, providing easy access to AI-generated drafts for comparison, and supporting rapid iteration across large annotation batches. Figure A2. Quality filtering interface for initial video selection. Reviewers watch candidate videos and label them as Good or Bad based on inclusion criteria (audio-visual quality, topic relevance, demographic coverage). The interface displays video metadata (duration, topic category, licensing) and allows batch processing with session management. 27 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Figure A3. Review interface for Task 1 (Video Summarization). Annotators edit AI-generated summaries (both detailed and bullet-point formats) while watching the full video with synchronized captions. The demographics panel allows verification and correction of demographic labels in JSON format. Changes auto-save when navigating between items. 28 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Figure A4. Review interface for Task 2 (Multiple-Choice Questions). Annotators review and edit MCQ questions, answer options (AE), correct answer labels, and rationales that reference visual and auditory evidence. The segment indicator shows the current 3-minute video segment being annotated. 29 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Figure A5. Review interface for Task 3 (Temporal Localization). Annotators edit anchor-target event pairs, temporal relations (before/after/during/immediately), answer start/end timestamps, rationales explaining the temporal reasoning, and audio requirements. The segment timeline helps annotators identify precise event boundaries within 3-minute video segments. 30 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models (a) Video count by duration category per topic (b) Average video duration per topic (minutes) Figure A6. Video duration analysis by topic. (a) Distribution showing the count of short (orange), medium (teal), and long (dark teal) videos within each of the 13 topics. Housing/Apartment Tours contains the most videos (24), while Workplace Team Meetings has the fewest (12). (b) Average duration per topic, with Parent-Teacher Conferences having the longest average (36.2 min) and Customer Service Interactions the shortest (6.0 min). C. Extended Dataset Statistics C.1. Demographic Distribution Figure A7 reports the demographic distribution at the benchmark instance level. Counts reflect how often demographic groups appear across all benchmark instances (Task 1 chunks and Task 23 segments), using per-instance demographic counts/mappings, rather than the number of unique individuals or unique videos. As result, the same individuals may be counted multiple times across different segments or tasks when the underlying content is reused. This aggregated view should be interpreted as demographic exposure frequency in the benchmark. It is appropriate for evaluating model behavior, since models are evaluated on these instances, but it does not represent census of unique participants. We observe moderate skew toward male-presenting speakers and participants aged 40+, which likely reflects the underlying distribution of English-language CC BY 4.0 YouTube content in our target domains. Topics with large groups (e.g., community town halls and courtroom proceedings) further increase exposure counts because many individuals co-occur and persist across multiple segments. C.2. Video Duration Statistics Figures A6a, and A6b illustrate the distribution of video durations across the three length categories (Short: <5 minutes, Medium: 5-20 minutes, Long: 20-60 minutes) and by topic. D. Preprocessing and Inference Configuration D.1. Hardware-Aware Inference Constraints To rigorously assess open-source MLLMs, we utilize standardized compute environment of 4 NVIDIA A40 GPUs (40GB VRAM each). This configuration allows for substantial parallelization but imposes hard memory limits on longcontext processing. notable exception is MiniCPM-o-2.6, which we evaluate on single NVIDIA A100 (80GB) GPU, as the official implementation does not currently support distributed inference. These hardware constraints necessitate strict input capping and fallback policy to prevent Out-Of-Memory (OOM) failures while maximizing the multimodal context available to each model. 31 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models (a) Distribution by race (b) Distribution by age group (c) Distribution by gender Figure A7. Demographic distribution of benchmark instance level (a) race/ethnicity, (b) age group, and (c) gender. Race distribution shows: White (10,997), Black (3,494), Asian (1,831), Hispanic (1,443), Indigenous (511), Arab (438). Age distribution shows: 18-24 (2,124), 25-39 (5,317), 40+ (11,173). Gender distribution shows: Male (12,672), Female (5,942). D.2. Visual and Audio Input Strategy We adopt unified prompting strategy, concatenating the textual prompt, audio input, and sampled video frames, strictly adhering to each models official implementation. Modality Fusion: Most models ingest separated modalities (frames + audio). However, VideoLLaMA 2 is an exception, requiring the raw video file with embedded audio rather than discrete inputs. Visual Capping: We cap the initial maximum visual input at Fmax = 256 frames. This limit was empirically determined to fit within the memory constraints of our 4A40 setup for the majority of architectures. D.3. Robustness and Adaptive Fallback Protocols To ensure evaluation fairness across diverse architectures, we enforce consistent, dynamic fallback protocol for models that encounter Out-Of-Memory (OOM) errors or context overflow. We initiate inference with each models designated maximum frame capacity: 256 frames for Qwen3-Omni, UniMoE-2.0, and MiniCPM-o-2.6; 128 frames for VideoLLaMA2; and 64 frames for VITA 1.5. These represent upper bounds; shorter videos naturally use fewer frames. All models initially receive full unchunked audio. Lower frame caps for VITA and VideoLLaMA2 reflect their smaller context windows and more limited memory footprints under our hardware constraints. Upon failure, we apply synchronized reduction strategy: 1. Visual Reduction: The frame budget is iteratively halved from each models maximum. For example, Qwen3-Omni reduces through the sequence {256, 128, 64, 32}, allowing up to four retry attempts. 2. Audio Reduction: Simultaneously, we adjust audio fidelity through uniform chunk sampling rather than truncation. If the full audio track causes failure, we transition to chunked representation with maximum limit of = 64 chunks. Each chunk spans 10 seconds of non-overlapping audio. In subsequent retries, this chunk limit halves in lockstep with visual frames through the sequence {64, 32, 16}. Given audio of duration seconds, the natural chunk count is = D/10. When , all chunks are retained; when > , we uniformly sample chunks using indices = linspace(0, 1, ) to preserve temporal coverage across the full recording. We adopted this audio chunking because not all MLLMs handle long audio tracks, as such they employ naive audio handling strategies such as end-truncation or middle-cropping, introducing uncontrolled confounds in evaluation. Our uniform chunk sampling approach is inspired by MiniCPM-o-2.6 (Yao et al., 2024). This start-high, fall-back approach ensures that reported performance reflects each models maximum capability under standard hardware constraints (4 NVIDIA A40 GPUs, 40GB VRAM each or 1 NVIDIA A100, 80GB VRAM). D.4. Task-Specific Prompts We report the shared task instruction text (T1T3) used across models. For open-source MLLMs, we rely on each models default chat template and required modality tokens/placeholders (e.g., Uni-MoE-2.0s <video>/<audio> markers; VITA 32 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models 1.5s image/audio special tokens) as implemented in their official codebases. Task 1: Video Summarization. Task 1: Standard Prompt You are analyzing video that is {video_duration} seconds long. Please provide comprehensive analysis with the following components: 1. DETAILED SUMMARY: Write detailed paragraph (150-250 words) that captures the main content, key points, and flow of the video. 2. SHORT SUMMARY: Provide 3-5 concise bullet points highlighting the most important takeaways. 3. TIMELINE: Create timeline of major sections/topics in the video with: - start: timestamp in \"HH:MM:SS\" format - end: timestamp in \"HH:MM:SS\" format - title: brief section title - note: one sentence description 4. GLOSSARY: List 5-15 key terms, acronyms, or important concepts mentioned in the video. 5. CONFIDENCE: Rate your confidence in this analysis from 0.0 to 1.0. Return your response as JSON object with this exact structure: { \"summary_detailed\": \"detailed paragraph here\", \"summary_short\": [\"bullet 1\", \"bullet 2\", \"bullet 3\"], \"timeline\": [ { } \"start\": \"00:00:00\", \"end\": \"00:02:30\", \"title\": \"Section Title\", \"note\": \"Brief description\" ], \"glossary\": [\"term1\", \"term2\", \"term3\"], \"confidence\": 0.95 } Only return the JSON object, no additional text. SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Task 1: Empathy Prompt You are analyzing video that is {video_duration} seconds long. Please provide an empathetic and emotionally aware analysis. Focus on understanding the emotional context, interpersonal dynamics, and human elements in the content. Provide: 1. DETAILED SUMMARY: Write detailed, empathetic paragraph (150-250 words) that captures not just what happens, but the emotional tone, interpersonal dynamics, and human aspects. Consider how participants might feel and what underlying concerns or needs are being expressed. 2. CONFIDENCE: Rate your confidence in this analysis from 0.0 to 1.0. Return your response as JSON object: { \"summary_detailed_empathic\": \"empathetic detailed paragraph here\", \"confidence\": 0.95 } Only return the JSON object, no additional text. Task 2: Multiple-Choice Question Answering. Task 2 Prompt You are analyzing video segment to answer multiple choice question. QUESTION: {question} OPTIONS: {options_text} Please analyze the video content carefully and: 1. Select the best answer from the options 2. Provide clear rationale explaining why this answer is correct based on what you observed in the video 3. Rate your confidence in this answer from 0.0 to 1.0 Return your response as JSON object with this exact structure: { \"answer_letter\": \"B\", \"answer_index\": 1, \"rationale\": \"Explanation of why this answer is correct based on video \"confidence\": 0.90 content\", } Notes: - answer_letter should be \"A\", \"B\", \"C\", \"D\", or \"E\" - answer_index should be 0, 1, 2, 3, or 4 (corresponding to A, B, C, D, E) - rationale should reference specific details from the video - CRITICAL: include all fields in the JSON and ensure proper formatting especially answer_letter Only return the JSON object, no additional text. Task 3: Temporal Localization. SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models For temporal localization inference, models receive absolute segment boundaries and must predict timestamps in absolute video time. For example, when evaluating 3-minute segment spanning 500680 seconds of 30-minute video, the model is told You are analyzing video segment from 500s to 680s and must return predictions within [500s, 680s], not [0s, 180s]. This design ensures models maintain temporal awareness of their position within the full video, which is critical for long-form understanding. In contrast, during annotation (Section A.8), human annotators worked with segment-relative timestamps (0.0s to segment duration) for cognitive ease when watching isolated clips. These segment-relative annotations were automatically converted to absolute video time by adding the segment start offset, producing ground truth in the same coordinate system used during inference. This alignment between annotation conversion and inference format ensures fair evaluation: models are tested on their ability to reason about events in absolute video time, matching real-world deployment scenarios where systems must index events within full-length recordings. The inference prompt below specifies the absolute segment range and constraints: Task 3 Prompt You are analyzing video segment from {segment_start}s to {segment_end}s (duration: {segment_duration}s). QUESTIONS ({num_questions} total): {questions_text} REQUIRED OUTPUT FORMAT: { \"questions\": [ \"question_id\": \"001\", \"start_s\": <timestamp_float>, \"end_s\": <timestamp_float>, \"confidence\": <float_between_0_and_1>, \"rationale_model\": \"<your explanation>\" { }, { \"question_id\": \"002\", \"start_s\": <timestamp_float>, \"end_s\": <timestamp_float>, \"confidence\": <float_between_0_and_1>, \"rationale_model\": \"<your explanation>\" } ... (continue for all {num_questions} questions) ] } RATIONALE REQUIREMENTS: Explain: When E1 (anchor) occurs -> When E2 (target) starts/ends -> Temporal relationship -> Visual/audio cues Example: \"E1 (anchor) starts at 5.2s with speakers introduction. E2 (target) starts at 35.0s when he says am final year medical student, ends at 36.6s. Relationship: after.\" CONSTRAINTS: - All timestamps within [{segment_start}s, {segment_end}s] - start_s < end_s for each question - Include ALL {num_questions} questions with their question_id field - CRITICAL: include all fields in the JSON and ensure proper formatting - CRITICAL: Your response MUST include these exact question_ids: {question_ids_list} and include the question_id field - Return ONLY valid JSON (no markdown, no code blocks, no extra text) 35 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Table A3. Effective inference settings and preprocessing strategies. Checkpoint refers to the specific HuggingFace model ID used. Native indicates the models default preprocessing; Adaptive denotes non-native audio chunking applied as fallback mechanism. Model / Checkpoint Backbone Frames Audio Hardware Notes Qwen2-7B VideoLLaMA2.1-7B-AV Qwen3-Omni-30B-A3B-Instruct Qwen3-MoE Uni-MoE-2.0-Omni Qwen2.5-7B MiniCPM-o-2.6 Qwen2.5-7B VITA-1.5 Qwen2-7B Gemini 3.0 Pro Proprietary GPT-4o Proprietary 128 256 256 256 64 1 FPS Embedded Adaptive Full Full Adaptive Full Captions 4A40 4A40 4A40 1A100 4A40 API API Integrated video+audio input. Chunking triggered by length overflow. Native processing. Native processing. Chunking triggered by length overflow. Native processing. No native audio support; ASR captions used. Audio Chunking triggered Fallback to text modality. All models use temp=0.7, top-p=0.95, max tokens=8192. D.5. Per-Model Effective Settings Table A3 details the effective frame budget, audio handling, and specific hardware allocation for each evaluated model. E. Evaluation Metrics This section provides formal definitions of all evaluation metrics used in our benchmark. Metrics are computed internally in the range [0, 1] but are reported as percentages (scaled by 100) where indicated. Text similarity metrics remain in the [0, 1] range following standard conventions. E.1. Task 1: Video Summarization Metrics For Task 1 (Video Summarization), we evaluate both detailed summaries and short bullet-point summaries using the following metrics: ROUGE-L. We compute ROUGE-L (Longest Common Subsequence) F1-score (Lin, 2004) to measure n-gram overlap between generated and reference summaries. For reference summary and predicted summary : ROUGE-L(R, ) = 2 PrecLCS RecLCS PrecLCS + RecLCS 100 (1) where PrecLCS and RecLCS are precision and recall based on the longest common subsequence. The score is reported as percentage in [0, 100]. Text Similarity. We measure semantic similarity using cosine similarity between sentence embeddings. Given embeddings eR and eP from pre-trained model (all-MiniLM-L6-v2): TextSim(R, ) = eR eP eReP This metric is reported in [0, 1], following standard cosine similarity conventions. Aggregation. For dataset with videos, we report the mean metric across all samples: ROUGE-L ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 ROUGE-L(Ri, Pi) We compute these metrics separately for detailed summaries and short summaries. E.2. Task 2: Multiple-Choice Question Answering Metrics For Task 2 (MCQ Question Answering), we evaluate both answer accuracy and rationale quality. 36 (2) (3) SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Answer Accuracy. For each question qi with ground truth answer agt and predicted answer apred Correct(qi) = (cid:40) 1 0 The overall accuracy across questions is: = agt if apred otherwise Accuracy ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 Correct(qi) 100 : (4) (5) reported as percentage in [0, 100]. Rationale Metrics. For each question, we evaluate the quality of the models rationale (explanation) using: ROUGE-L: Measures lexical overlap with reference rationale (reported as percentage, [0, 100]) Text Similarity: Semantic similarity via cosine distance (reported in [0, 1]) We aggregate rationale metrics by computing the mean across all questions with valid rationales. E.3. Task 3: Temporal Localization Metrics For Task 3 (Temporal Localization), we evaluate the models ability to identify temporal intervals that answer questions about video segments. Temporal Intersection over Union (tIoU). For ground truth temporal interval = [sgt, egt] and predicted interval = [spred, epred], where and denote start and end times in seconds: tIoU(G, ) = max(cid:0)0, min(egt, epred) max(sgt, spred)(cid:1) max(egt, epred) min(sgt, spred) 100 (6) The tIoU measures temporal overlap and is reported as percentage in [0, 100]. Mean Intersection over Union (mIoU). The mean IoU aggregates tIoU scores across all temporal localization questions: mIoU ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 tIoU(Gi, Pi) (7) This metric is reported as percentage in [0, 100]. Recall at IoU Threshold (Recall@θ). We measure the percentage of predictions whose tIoU meets or exceeds threshold θ: Recall@θ ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 [tIoU(Gi, Pi) 100, θ] 100 (8) where [] is the indicator function. We report Recall@0.3, Recall@0.5, and Recall@0.7 as percentages in [0, 100]. We abbreviate Recall@θ as R@θ in tables. SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Mean Absolute Error (MAE). We compute the temporal localization error for start and end times: MAEstart = MAEend ="
        },
        {
            "title": "1\nN",
            "content": "N (cid:88) i=1 (cid:88) i=1 sgt spred egt epred MAE is reported in seconds. MAEavg = MAEstart + MAEend 2 (9) (10) (11) Rationale Metrics. Similar to Task 2, we evaluate rationale quality using ROUGE-L (percentage) and Text Similarity ([0, 1]). E.4. LLM-as-Judge Evaluation For tasks involving free-form text generation (summaries, rationales), we employ an LLM-as-Judge evaluation protocol adopted from previous work to assess semantic correctness and quality beyond n-gram overlap metrics (Ataallah et al., 2025). We use GPT-5-mini (OpenAI, 2026) as our primary judge. Evaluation Protocol. For each generated output, the judge model: 1. Receives the original question and both the reference and predicted answers 2. Evaluates semantic similarity, factual correctness, and completeness 3. Assigns numerical score from 0 (completely incorrect) to 10 (perfect match) 4. Provides concise justification (1-3 sentences) for the assigned score The judge is instructed to accept paraphrases, synonyms, or rephrasings as valid when they preserve the original meaning, and to penalize omissions of key factual elements, hallucinated content, or contradictions. The evaluation focuses on semantic alignment rather than surface-level textual similarity. Scoring Scale and Output Format. Judges assign integer scores from 0 to 10 and return responses in JSON format: { } \"score\": <integer 0-10>, \"justification\": \"<brief explanation>\" We aggregate judge scores by computing the mean across all evaluated samples. This provides complementary assessment to automatic metrics (ROUGE-L, cosine similarity), particularly for capturing semantic equivalence that lexical overlap metrics may miss. Judge Prompt. The complete system prompt provided to the judge model is shown in Figure A8. The prompt emphasizes fair evaluation, human-like judgment, and consistent scoring criteria across all evaluated outputs. E.5. Aggregation Across Topics Our benchmark spans multiple topics (content domains). For each task, we first compute metrics within each topic, then aggregate across topics by computing the mean metric value. This approach provides single aggregate score while accounting for performance variation across different content types. Due to space constraints in tables, we report only mean values; per-topic breakdowns are available in our supplementary materials. 38 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models System Prompt: You are an intelligent and fair evaluator AI that specializes in assessing the correctness and semantic alignment between ground truth answers and predicted responses for question-answering tasks, including those based on video content. Evaluation Instructions: Focus on semantic similarity, factual correctness, and completeness Accept paraphrases, synonyms, or rephrasings as valid, as long as they preserve the original meaning Do not penalize for stylistic differences or changes in tone, unless they impact factual accuracy Penalize if: The predicted answer omits key factual elements present in the correct answer The prediction includes hallucinated content or unfounded details The prediction contradicts the correct answer Use human-like judgment: apply reasoning beyond surface text similarity When uncertain, provide conservative but fair score Use scoring scale from 0 (completely incorrect) to 10 (perfect match) Output Format: Return JSON object with two fields: { \"score\": <integer from 0 to 10>, \"justification\": \"<concise explanation (1-3 sentences)>\" } User Prompt Template: Please evaluate the following video-based question-answer pair: Question: {question} Correct Answer: {correct answer} Predicted Answer: {predicted answer} Please return your evaluation in the specified JSON format with both score and justification. Figure A8. LLM-as-Judge evaluation prompt. The system prompt instructs the judge to evaluate semantic correctness, factual completeness, and relevance on 0-10 scale, with specific guidelines for handling paraphrases, hallucinations, and contradictions. E.6. Implementation Details Libraries. We use the following libraries for metric computation: ROUGE: rouge-score package with Porter stemming Text Embeddings: sentence-transformers with all-MiniLM-L6-v2 model LLM Judge: OpenAI API (GPT-5-mini) Handling Failed Predictions. sample is excluded from metric computation. We report the number of successfully evaluated samples for each task. If model fails to generate prediction for sample (e.g., due to timeout or error), that Reproducibility. All metric computation code and detailed per-topic results are available in our code repository. For statistical comparison between models, we recommend paired bootstrap tests with = 10, 000 resamples and α = 0.05 significance level. F. Detailed Results Table A4 presents the complete benchmark results across all three tasks, including all automatic metrics and LLM-as-judge rationale scores. We report detailed summarization performance, MCQ accuracy with rationale quality, and temporal localization metrics including mIoU, recall at multiple thresholds, mean absolute error, and rationale quality. 39 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Table A4. Comprehensive benchmark results across all tasks and metrics. Results are reported for MLLMs on all dataset. Summarization reports judge scores (010), ROUGE-L (%), and similarity (01). MCQ reports accuracy (%) and rationale quality (score 010, ROUGE-L %, similarity 01). Temporal localization reports mIoU (%), R@0.3/R@0.5 (%), MAE in seconds, and rationale quality. Bold indicates best performance, underline indicates second-best."
        },
        {
            "title": "Model",
            "content": "Score RG-L Sim Acc. Score RG-L Sim mIoU R@0.3 R@0.5 MAE Score RG-L Sim Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2.6 VITA 1.5 VideoLLaMA2 7.07 5.72 4.71 3.34 2.77 1.53 27.2 22.8 20.8 14.7 16.3 12.7 0.813 96.4 0.713 93.6 0.700 88.2 0.563 87.4 0.491 81.6 0.383 54.4 9.38 8.88 8.12 8.01 7.72 5. 36.9 33.8 26.5 24.4 25.2 18.9 0.802 0.777 0.727 0.729 0.714 0.577 26.6 3.70 1.81 1.81 1.81 3.50 39.3 5.14 2.40 2.38 2.55 1.68 25.4 2.81 1.04 0.73 1.15 0.42 12.0 60.0 685.8 367.1 116.1 234. 5.38 2.58 2.11 3.65 3.91 2.22 28.3 28.0 23.7 19.3 22.1 22.5 0.673 0.698 0.549 0.406 0.433 0.493 Key findings. Gemini 3.0 Pro demonstrates strong performance across all tasks, achieving the best scores in summarization (7.07), MCQ accuracy (96.4%), and temporal localization (26.6% mIoU, 25.4% R@0.5). Qwen3-Omni achieves second-best performance on most metrics, with particularly strong rationale quality (28.0% ROUGE-L, 0.698 similarity on temporal task). Among open-source models, UniMoE-2.0 shows the best summarization capabilities, while VITA 1.5 produces the highest-quality temporal rationales (3.91 judge score). The temporal localization task reveals critical challenge for open-source models: all models except Gemini exhibit large MAE values (60.0685.8 seconds), indicating systematic temporal reference frame hallucination as discussed in Appendix G. This failure mode occurs when models process long videos in segments but fail to ground predictions in absolute timestamps, instead treating each segment as starting at time 0.0 seconds. Despite this localization failure, models can still produce reasonable rationales (scores 2.113.91), demonstrating disconnect between semantic understanding and temporal grounding. Rationale quality metrics show interesting patterns: while Gemini produces the best overall rationales for summarization and MCQ tasks, Qwen3-Omni achieves competitive or superior similarity scores on temporal localization (0.698), and VITA 1.5 receives the highest judge scores for temporal rationales (3.91). This suggests that rationale quality does not perfectly correlate with task performance, and that different models may excel at explanation versus execution. G. Detailed Results Per-Topic This appendix provides comprehensive per-topic performance breakdowns for all three tasks. Each table reports metrics for individual topics (T1T13), allowing detailed analysis of model capabilities across different conversational domains. All metrics follow the same conventions as the main paper: percentages for ROUGE-L, Accuracy, mIoU, and Recall; 01 scale for cosine similarity; 010 scale for LLM judge scores; seconds for MAE. Bold denotes best performance, underline denotes second-best within each topic. denotes closed-source model. G.1. Key Observations from Per-Topic Analysis Topic-specific performance variation. Models exhibit substantial performance variation across topics. In summarization (Tables A5A7), Gemini achieves 8.33 on T2: Job Interviews but drops to 5.04 on T13: Olympics. MCQ accuracy (Tables A8A10) shows similar variation: Qwen3-Omni achieves 98.0% on T2 but only 82.4% on T7: Transportation. Temporal hallucination in open-source models. Temporal localization (Tables A11A13) reveals stark differences between models. Gemini maintains consistent grounding (MAE: 7.823.5s, R@0.5: 15.933.5%), while open-source models exhibit systematic temporal reference frame hallucination. Despite explicit prompting with absolute timestamps, most open-source models treat each 3-minute segment as starting at 0s. For example, UniMoE-2.0 achieves MAE of 1049.41532.0s on T1, T3, and T5 despite reasonable rationale scores (2.002.22), indicating models understand what happens but fail to ground when in absolute video time. Model hierarchy. Results show clear performance tiers: Gemini (MAE: 7.823.5s), Qwen3-Omni (38.595.7s), VITA 1.5 (44.6209.0s), and UniMoE-2.0/MiniCPM-o-2.6/VideoLLaMA2 (often 3001500s). This exposes fundamental limitation: most open-source models lack mechanisms to maintain temporal coherence across segments, treating each chunk 40 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models independently rather than as part of continuous timeline. Rationale-accuracy disconnect. High rationale scores do not guarantee temporal accuracy. VITA 1.5 achieves rationale scores of 3.374.45 but R@0.5 of 0.02.3% (MAE: 44.6209.0s). UniMoE-2.0 and MiniCPM-o-2.6 similarly produce coherent explanations (scores 1.744.10) while exhibiting severe localization errors (MAE >1000s on multiple topics). Professional vs. dynamic scenes. Models consistently perform better on structured interactions (T1: Patient-Doctor, T2: Job Interviews, T3: Parent-Teacher) than dynamic scenarios (T6: Emergency, T7: Transportation, T13: Olympics). Even Geminis MAE increases from 7.89.4s on structured topics to 13.423.5s on dynamic ones, though it remains substantially better than open-source alternatives. G.2. Task 1: Video Summarization Per-Topic Table A5. Task 1: Detailed Summarization Performance of MLLMs on Topics 14. Judge Score (010), RG-L: ROUGE-L (%), and Sim: Similarity (01) shown for detailed summaries. Bold = best, underline = second-best per topic. Model T1: Patient-Dr. T2: Job Int. T3: Parent-Tch. T4: Customer Score RG-L Sim Score RG-L Sim Score RG-L Sim Score RG-L Sim Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2.6 VITA 1.5 VideoLLaMA 8.12 7.38 6.12 4.94 4.44 2.31 29.7 24.6 23.2 15.8 18.3 13.6 0.86 0.78 0.76 0.70 0.63 0.49 8.33 7.14 6.76 4.19 3.71 2.19 26.5 25.5 21.0 15.5 17.2 11.9 0.82 0.76 0.75 0.59 0.53 0. 7.94 6.72 5.72 4.28 4.22 2.28 27.6 22.2 20.1 13.8 16.1 11.9 0.82 0.74 0.73 0.60 0.54 0.38 7.67 6.13 4.33 3.20 1.73 1.27 29.1 25.6 20.6 14.7 17.2 12.3 0.81 0.74 0.69 0.59 0.51 0. Table A6. Task 1: Detailed Summarization Performance of MLLMs on Topics 58. Judge Score (010), RG-L: ROUGE-L (%), and Sim: Similarity (01) shown for detailed summaries. Bold = best, underline = second-best per topic. Model T5: Courtroom T6: Emergency T7: Transport T8: Workplace Score RG-L Sim Score RG-L Sim Score RG-L Sim Score RG-L Sim Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2.6 VITA 1.5 VideoLLaMA 6.54 6.00 4.08 3.15 1.77 0.85 25.1 22.8 21.1 14.2 14.5 11.7 0.79 0.70 0.67 0.49 0.41 0.27 6.25 5.10 3.90 3.35 2.70 1.65 27.6 22.8 20.4 16.0 16.8 15.4 0.77 0.69 0.64 0.58 0.52 0. 7.43 4.86 3.29 2.21 2.00 1.00 28.1 21.2 19.5 14.2 15.6 13.6 0.85 0.70 0.69 0.52 0.53 0.45 7.00 6.08 4.75 3.17 2.50 1.17 26.7 20.8 19.2 13.4 14.2 11.0 0.83 0.72 0.69 0.51 0.43 0. Table A7. Task 1: Detailed Summarization Performance of MLLMs on Topics 913. Score: Judge Score (010), RG-L: ROUGE-L (%), and Sim: Similarity (01) shown for detailed summaries. Bold = best, underline = second-best per topic. Model T9: Housing T10: Restaurant T11: Mental Hlth T12: Town Halls T13: Olympics Score RG-L Sim Score RG-L Sim Score RG-L Sim Score RG-L Sim Score RG-L Sim Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2.6 VITA 1.5 VideoLLaMA2 6.58 4.58 4.38 2.88 2.58 1.50 27.7 23.0 22.0 14.9 17.0 14.1 0.79 0.67 0.67 0.50 0.43 0.35 5.91 5.00 4.91 3.13 2.26 1. 27.1 23.2 19.8 15.3 17.2 13.8 0.79 0.72 0.67 0.60 0.46 0.42 8.08 7.23 6.69 4.46 3.69 2.08 28.4 26.3 25.2 15.6 17.6 13.3 0.83 0.77 0.79 0.63 0.55 0.42 7.00 4.72 4.33 2.06 2.28 0. 26.0 18.8 19.8 11.7 14.1 9.8 0.79 0.60 0.65 0.39 0.34 0.18 5.04 3.39 1.96 2.43 2.09 1.13 23.7 19.2 18.7 16.5 16.6 12.9 0.82 0.67 0.70 0.63 0.50 0.39 G.3. Task 2: Multiple-Choice Questions Per-Topic 41 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Table A8. Task 2: MCQ Performance of MLLMs on Topics 14. Acc.: Accuracy (%), Score: Rationale Judge Score (010), RG-L: ROUGE-L (%), and Sim: Similarity (01) shown. Bold = best, underline = second-best per topic. Model T1: Patient-Dr. T2: Job Int. T3: Parent-Tch. T4: Customer Acc. Score RG-L Sim Acc. Score RG-L Sim Acc. Score RG-L Sim Acc. Score RG-L Sim Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2.6 VITA 1.5 VideoLLaMA 99.0 94.1 92.2 92.2 80.4 56.9 9.82 9.42 8.97 8.55 7.60 5.52 42.6 40.0 29.6 27.0 26.7 21.5 0.82 0.79 0.76 0.74 0.71 0.61 97.0 98.0 94.0 95.0 92.0 64.0 9.64 9.57 9.21 8.94 8.86 6. 40.0 37.6 28.4 25.8 26.8 21.1 0.80 0.77 0.73 0.72 0.70 0.61 98.7 95.2 92.6 92.6 89.6 61.7 9.63 9.25 8.92 8.52 8.42 5.51 38.9 35.8 27.7 25.3 25.8 19.3 0.80 0.79 0.75 0.73 0.72 0. 94.9 92.3 76.9 84.6 74.4 59.0 9.13 8.62 7.03 7.62 6.92 5.54 31.9 28.5 26.7 24.7 25.7 18.6 0.78 0.77 0.74 0.73 0.73 0.60 Table A9. Task 2: MCQ Performance of MLLMs on Topics 58. Acc.: Accuracy (%), Score: Rationale Judge Score (010), RG-L: ROUGE-L (%), and Sim: Similarity (01) shown. Bold = best, underline = second-best per topic. Model T5: Courtroom T6: Emergency T7: Transport T8: Workplace Acc. Score RG-L Sim Acc. Score RG-L Sim Acc. Score RG-L Sim Acc. Score RG-L Sim Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2.6 VITA 1.5 VideoLLaMA2 96.5 98.3 92.2 87.0 87.0 51.3 9.28 9.28 8.53 8.37 7.97 4.16 39.6 36.8 27.3 25.7 26.1 18.3 0.80 0.79 0.74 0.73 0.71 0. 95.4 87.4 85.1 85.1 79.3 55.2 8.99 8.39 7.53 7.24 7.61 4.94 32.0 28.6 24.0 23.7 25.2 17.5 0.79 0.76 0.69 0.72 0.71 0.55 96.1 82.4 88.2 82.4 84.3 49.0 9.29 7.73 7.51 7.33 7.33 5. 34.7 26.6 24.2 22.6 24.0 18.3 0.80 0.74 0.70 0.71 0.72 0.59 94.0 100.0 91.0 95.5 85.1 53.7 9.45 9.57 8.70 8.61 8.45 5.16 37.1 36.7 25.8 23.6 24.8 19.4 0.79 0.78 0.73 0.76 0.73 0. Table A10. Task 2: MCQ Performance of MLLMs on Topics 913. Acc.: Accuracy (%), Score: Rationale Judge Score (010), RG-L: ROUGE-L (%), and Sim: Similarity (01) shown. Bold = best, underline = second-best per topic. Model T9: Housing T10: Restaurant T11: Mental Hlth T12: Town Halls T13: Olympics Acc. Score RG-L Sim Acc. Score RG-L Sim Acc. Score RG-L Sim Acc. Score RG-L Sim Acc. Score RG-L Sim Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2.6 VITA 1.5 VideoLLaMA2 95.3 93.8 85.3 80.6 76.0 51.9 9.29 8.79 7.82 7.26 7.03 4.38 38.6 36.8 27.5 24.3 25.7 19.4 0.80 0.78 0.73 0.70 0.68 0.56 97.8 94.4 80.0 82.4 64.4 45. 9.33 8.77 7.50 7.79 6.49 4.50 36.3 34.2 27.5 24.8 24.4 17.8 0.82 0.79 0.75 0.74 0.69 0.55 98.5 100.0 93.8 89.2 87.7 50.8 9.63 9.57 8.71 8.48 8.54 4.83 36.3 34.3 24.6 23.2 24.0 16. 0.79 0.80 0.73 0.74 0.74 0.52 96.8 94.2 93.7 92.1 84.7 58.2 9.49 9.13 8.53 8.25 8.02 5.59 40.2 36.1 28.4 24.3 25.0 20.5 0.81 0.78 0.73 0.71 0.70 0.59 92.8 87.0 81.2 78.3 75.4 49. 8.96 7.39 6.64 7.13 7.10 4.94 31.8 27.0 22.3 22.3 23.5 18.0 0.82 0.77 0.69 0.73 0.72 0.61 42 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models G.4. Task 3: Temporal Localization Per-Topic Table A11. Task 3: Temporal Localization Performance of MLLMs on Topics 14. mIoU (%), R@0.5 (%), MAE (seconds), and Score: Rationale Judge Score (010) shown. Bold = best, underline = second-best per topic. indicates lower is better. Model T1: Patient-Dr. T2: Job Int. T3: Parent-Tch. T4: Customer mIoU R@0.5 MAE Score mIoU R@0.5 MAE Score mIoU R@0.5 MAE Score mIoU R@0.5 MAE Score Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2.6 VITA 1.5 VideoLLaMA 20.2 5.0 2.0 1.4 3.5 4.0 18.6 4.5 1.9 0.4 3.8 1.1 9.2 57.4 1049.4 657.9 170.1 357.0 5.01 2.73 2.05 3.37 3.77 2.05 32.1 6.3 2.5 3.3 2.6 4.2 31.0 5.5 1.2 1.6 1.2 0. 8.9 58.3 618.8 372.0 122.2 219.2 5.47 2.78 2.17 3.50 3.79 2.05 22.7 3.1 0.9 2.1 2.3 3.1 21.0 1.5 1.0 1.5 1.3 0.2 12.5 49.9 1423.0 1041.5 176.2 510.7 5.12 2.72 2.22 3.41 3.58 2. 30.1 3.7 3.1 2.1 1.2 4.7 31.9 3.4 0.9 0.9 0.9 0.0 13.4 62.7 315.8 123.3 86.2 126.4 5.82 2.40 1.98 4.58 4.04 2.00 Table A12. Task 3: Temporal Localization Performance of MLLMs on Topics 58. mIoU (%), R@0.5 (%), MAE (seconds), and Score: Rationale Judge Score (010) shown. Bold = best, underline = second-best per topic. indicates lower is better. Model T5: Courtroom T6: Emergency T7: Transport T8: Workplace mIoU R@0.5 MAE Score mIoU R@0.5 MAE Score mIoU R@0.5 MAE Score mIoU R@0.5 MAE Score Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2.6 VITA 1.5 VideoLLaMA2 32.7 4.0 1.1 2.3 1.8 3.2 33.5 2.9 0.6 1.5 1.5 0. 15.1 38.5 1532.0 652.0 128.4 480.8 5.38 2.59 2.00 3.28 4.08 2.25 20.0 3.0 1.4 1.1 0.7 1.8 17.8 2.4 0.4 0.0 0.0 0.4 14.1 42.3 477.4 238.1 63.9 215.6 5.44 2.46 2.03 4.10 4.45 2. 27.0 1.4 1.6 0.3 0.6 3.4 22.6 0.0 1.0 0.0 0.0 0.0 7.8 44.8 550.3 263.0 137.7 179.7 6.09 2.57 2.64 3.91 4.31 3.40 28.8 4.1 4.8 1.6 1.5 4.9 29.6 3.2 3.3 0.7 0.0 1. 10.2 59.0 391.5 302.7 166.2 176.0 5.31 2.50 1.94 3.29 3.59 2.33 Table A13. Task 3: Temporal Localization Performance of MLLMs on Topics 913. mIoU (%), R@0.5 (%), MAE (seconds), and Score: Rationale Judge Score (010) shown. Bold = best, underline = second-best per topic. indicates lower is better. Model T9: Housing T10: Restaurant T11: Mental Hlth T12: Town Halls T13: Olympics mIoU R@0.5 MAE Score mIoU R@0.5 MAE Score mIoU R@0.5 MAE Score mIoU R@0.5 MAE Score mIoU R@0.5 MAE Score Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2.6 VITA 1.5 VideoLLaMA2 29.5 3.3 1.3 2.2 2.4 3.6 27.6 3.1 0.8 1.3 1.6 0. 8.5 73.9 401.3 170.9 52.5 134.2 5.16 2.20 2.08 3.42 4.36 2.37 27.7 5.8 2.2 1.8 1.7 3.1 26.6 4.3 0.5 0.5 0.5 0.0 9.4 82.7 429.0 201.9 96.9 129.4 5.52 2.65 2.45 3.97 4.39 2. 29.5 3.2 0.9 2.8 1.1 3.2 29.6 2.6 0.7 0.7 0.7 0.0 9.4 67.1 429.7 208.2 55.6 168.3 5.45 2.66 1.74 3.68 3.37 1.83 20.5 4.2 0.4 2.2 2.3 3.2 15.9 3.0 0.2 0.5 1.3 0. 23.5 95.7 1111.6 469.7 209.0 253.1 5.07 2.97 2.18 3.62 3.39 1.77 25.2 1.3 1.3 0.3 1.9 3.0 24.8 0.0 1.2 0.0 2.3 1.1 13.6 47.2 185.3 71.4 44.6 98.3 5.11 2.29 1.95 3.38 3.66 2. H. Error Taxonomy Analysis To better understand failure modes in Task 3 (Temporal Localization), we categorize each prediction into small set of error types. In this task, models are given absolute segment boundaries and are required to output absolute timestamps within the provided segment. Ground-truth timestamps are obtained by converting segment-relative annotations to absolute video time by adding the segment start offset. Notation. Let the ground-truth interval be = [sgt, egt] and the predicted interval be = [spred, epred], both in seconds. We use the same IoU and MAE definitions as in the main evaluation. H.1. Category assignment rules Each prediction is assigned to exactly one category using the following rules (evaluated in order). We use two fixed thresholds: reference-frame tolerance τrelabs = 10s and timing-shift tolerance τshift = 5s. Relative-to-absolute mismatch captures cases where the model outputs timestamps as if time starts at 0 at the beginning of the segment (i.e., segment-relative), even though the task requires absolute video time. We detect this by shifting the prediction by the segment start time: = [spred + S, epred + S], where is the segment start. If this shifted interval is close to the ground truth (within τrelabs = 10s) while the original is not, we label the prediction as relative-to-absolute mismatch. Hallucination covers violations of output validity or prompt constraints: (i) malformed intervals (non-numeric timestamps or epred spred), (ii) timestamps outside the provided segment bounds, or (iii) timestamps outside the full video duration 43 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models when available. This category also includes extreme teleport errors where the predicted interval is far from the provided segment window. Timing shift (Too Early / Too Late) is assigned when overlap with the ground truth is negligible (IoU(P, G) < 0.1) and the prediction is clearly before or after the ground truth by more than τshift seconds: Too Early if epred < sgt τshift; Too Late if spred > egt + τshift. Wrong event, right time is semantic proxy: we assign it when temporal overlap is high (IoU(P, G) > 0.5) but the rationale is judged poor (LLM-judge score < 5), suggesting the model localized different event occurring at similar time. Other includes all remaining errors not covered above. H.2. Key Observations Figure A9 provides qualitative examples of UniMoE-2.0 predictions and Gemini 3.0 Pro. Table A14 shows clear, modeldependent failure patterns. Uni-MoE-2.0 exhibits large fraction of relative-to-absolute mismatches (39.4%), indicating that reference-frame confusion is primary source of error for this model. MiniCPM-o-2.6 also shows substantial hallucination (26.8%) and strong tendency toward late timing shifts (37.5%), suggesting frequent violations of segment constraints and delayed localization. By contrast, Qwen3-Omni and VITA-1.5 are dominated by timing shifts (Too Early + Too Late = 67.4% and 74.6%, respectively), consistent with models often selecting the correct neighborhood but misplacing the interval. Gemini 3.0 Pro has very low hallucination and reference-frame errors, but large residual Other bucket (81.5%), indicating that most of its mistakes do not follow single dominant structural pattern under this taxonomy. VideoLLaMA2 shows more balanced profile, with moderate reference-frame errors (10.4%) and hallucination (10.0%), and the remainder largely falling into Other (50.2%). Table A14. Error taxonomy distribution (%) for Task 3 temporal localization. Error breakdown of MLLMs predictions on all dataset. Hallucination aggregates invalid outputs, out-of-bounds timestamps, and extreme teleport errors. Each prediction is assigned to exactly one category."
        },
        {
            "title": "Model",
            "content": "RelativeAbsolute Hallucination Too Early Too Late Wrong Event / Right Time Other Gemini 3.0 Pro Qwen3-Omni Uni-MoE-2.0 VideoLLaMA2 MiniCPM-o-2.6 VITA-1.5 0.2 0.5 39.4 10.4 7.3 0.8 0.4 7.0 27.1 10.0 26.8 7.7 5.6 30.5 13.0 17.6 14.8 38.0 9.2 36.9 9.6 11.6 37.5 36. 3.1 1.3 0.7 0.3 0.6 0.5 81.5 23.8 10.1 50.2 12.9 16.4 I. Detailed Results Across Demographic Groups Tables A15A18 report performance stratified by race, gender, and age. Demographic disparities are systematic: the same groups consistently underperform across models and tasks, indicating persistent robustness gaps. Race disparities. Models most consistently underperform on Black participants. In summarization, Gemini drops from 7.05 (Asian) to 6.02 (Black; 1.03), and Qwen from 5.95 (Arab) to 4.39 (Black; 1.56). Indigenous participants show inconsistent treatment: some models remain competitive, while others degrade sharply (Qwen: 4.13, VITA: 1.65), suggesting limited coverage of Indigenous speech patterns in training corpora. Gender disparities. Female-participant videos consistently yield higher performance across all models and tasks. Female advantages in summarization range from +0.17 to +0.66 across models. The uniform directionality suggests systematic differences in processing male vs. female voices and interaction styles. Age disparities. Models favor older speakers. For summarization, the 40+ group outperforms 1824 (Gemini: 6.91 vs. 6.28), consistent with training data skewed toward formal, professional contexts aligned with older-participant scenarios. Temporal localization failures. Temporal localization exhibits the most severe disparities. For Indigenous participants, open-source models collapse to 0.0% R@0.5 while Gemini achieves 40.9%, indicating binary failure where systems fail to register critical temporal grounding cues. This contrasts with Black participants, where performance degrades but does not completely collapse (Gemini: 19.5% R@0.5), implying different underlying error mechanisms across subgroups. 44 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Figure A9. Qualitative examples of temporal localization errors. Three common error types are illustrated: Too Early (top)predictions preceding the ground truth event; Relative-to-Absolute mismatch (middle)model outputs segment-relative rather than absolute timestamps; Too Late (bottom)predictions following the ground truth event. Each example shows the question, ground truth timestamps, and predictions from UniMoE-2.0 and Gemini-3.0-Pro. Implications. These disparities likely reflect upstream biases in pre-training corpora rather than annotation artifacts. Demographic slice sizes are uneven and can inflate variance for smaller groups. Generation-oriented summarization shows relative stability, while reasoning-heavy MCQ and especially temporal localization expose larger robustness failures across demographic groups. Table A15. Performance of MLLMs on the summarization task across demographics. Results are on all dataset. Score: Judge score (010), ROUGE-L (%), and Sim: similarity (01). Bold = best, highlighted = worst within each demographic group (Race/Gender/Age) per column. Group Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2. VITA 1.5 VideoLLaMA2 Score ROUGE-L Sim Score ROUGE-L Sim Score ROUGE-L Sim Score ROUGE-L Sim Score ROUGE-L Sim Score ROUGE-L Sim Race Arab Indigenous Asian White Hispanic Black Gender Male Female Age 1824 2539 40+ 6.90 6.70 7.05 6.68 6.41 6.02 6.36 7.02 6.28 6.52 6.91 27.8 24.3 27.9 25.9 26.3 25.2 25.8 26.9 25.4 26.2 26. 0.811 0.853 0.818 0.809 0.809 0.806 5.95 4.13 5.71 5.28 4.99 4.39 0.809 0.816 4.97 5.47 0.818 0.811 0.809 4.93 5.01 5. 22.2 19.0 23.2 21.6 21.8 20.8 21.4 22.2 21.5 21.5 22.1 0.695 0.745 0.740 0.701 0.704 0.663 5.00 4.35 4.62 4.29 3.70 3.45 0.698 0. 3.93 4.55 0.713 0.703 0.700 4.02 4.01 4.45 21.9 18.9 20.2 20.2 20.2 19.0 19.8 20.3 20.0 19.7 20. 0.706 0.749 0.695 0.699 0.691 0.683 3.57 3.61 3.26 3.26 3.04 2.92 0.690 0.707 2.98 3.53 0.710 0.689 0.699 3.30 3.14 3. 14.1 14.6 14.5 14.7 15.0 14.9 14.6 14.8 15.3 14.7 14.4 0.576 0.689 0.579 0.572 0.558 0.559 2.76 1.65 2.65 2.50 2.21 2.31 0.567 0. 2.31 2.65 0.623 0.570 0.552 2.41 2.45 2.47 16.4 15.2 15.8 16.0 15.1 15.7 15.8 15.9 16.7 15.7 15. 0.465 0.440 0.488 0.476 0.466 0.438 1.00 1.04 1.63 1.45 1.23 1.38 0.470 0.467 1.36 1.53 0.493 0.466 0.461 1.38 1.50 1. 11.5 8.9 13.0 12.7 12.6 13.1 12.8 12.4 12.5 12.8 12.6 0.289 0.227 0.412 0.367 0.352 0.363 0.370 0.361 0.387 0.377 0. 45 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Table A16. Performance of MLLMs on the MCQ task across demographics. Results on all dataset. Acc.: Accuracy (%), and rationale quality (Score: Judge score (010), RG-L: ROUGE-L (%), Sim: similarity (01)). Bold = best, highlighted = worst within each demographic group per column. Group Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2.6 VITA 1.5 VideoLLaMA2 Acc. Score RG-L Sim Acc. Score RG-L Sim Acc. Score RG-L Sim Acc. Score RG-L Sim Acc. Score RG-L Sim Acc. Score RG-L Sim Race 98.4 Arab Indigenous 94.3 97.8 Asian 96.9 White 95.8 Hispanic 96.4 Black 9.44 9.03 9.43 9.41 9.19 9.27 37.4 31.8 37.7 37.6 35.7 36.6 0.802 0.825 0.807 0.802 0.796 0.805 96.9 77.1 96.1 93.3 92.8 92.0 9.09 6.86 9.12 8.90 8.53 8.66 35.1 29.4 34.2 34.3 31.7 32. 0.790 0.771 0.781 0.779 0.772 0.784 95.3 80.0 89.2 88.9 86.4 87.4 8.95 6.74 7.90 8.13 7.79 7.84 27.9 20.3 26.4 26.3 27.0 25.9 0.753 0.631 0.723 0.723 0.734 0.723 92.7 82.9 88.7 87.5 82.2 86. 8.21 7.09 8.28 8.08 7.39 7.66 24.4 22.6 24.2 24.4 23.8 23.3 0.738 0.758 0.729 0.729 0.723 0.723 93.2 62.9 84.6 82.0 79.9 82.2 8.47 6.17 7.83 7.81 7.46 7.70 25.5 24.0 24.8 24.8 25.0 24. 0.729 0.726 0.705 0.712 0.720 0.715 67.0 65.7 57.9 55.1 51.5 55.0 5.59 6.17 5.21 5.12 4.64 5.02 19.4 19.5 19.0 18.6 18.5 19.0 0.561 0.597 0.564 0.576 0.581 0.584 Gender Male Female Age 1824 2539 40+ 96.4 97.7 9.29 9.48 36.8 37.6 0.804 0.801 93.5 93. 8.77 8.93 33.4 34.1 0.779 0.781 88.0 89.3 7.93 8.21 26.2 26. 0.720 0.733 86.1 88.4 7.89 8.05 23.9 24.3 0.728 0.729 81.6 83. 7.71 7.87 24.7 24.9 0.713 0.714 52.6 60.0 4.91 5.36 18.7 19. 0.575 0.578 97.4 95.6 98.0 9.31 9.27 9.48 34.3 36.4 38.4 0.802 0.802 0.804 91.5 92.7 94. 8.36 8.76 9.01 30.0 33.4 34.9 0.773 0.780 0.781 82.4 87.5 91.0 7.33 7.90 8.33 23.2 26.2 27. 0.690 0.724 0.734 83.6 86.7 88.2 7.67 7.89 8.08 22.9 23.9 24.5 0.730 0.727 0.728 82.1 81.4 83. 7.80 7.67 7.87 24.1 24.7 25.1 0.723 0.712 0.712 54.8 55.6 56.4 5.06 5.09 5.12 17.4 18.7 19. 0.565 0.574 0.579 Table A17. Performance of MLLMs on the temporal localization task across demographics (Models 13). Results on all dataset. mIoU (%), R@0.3/R@0.5 (%), and rationale quality (Score: Judge score (010), RG-L: ROUGE-L (%), Sim: similarity (01)). Bold = best, highlighted = worst within each demographic group per column. Group Gemini 3.0 Pro Qwen3-Omni UniMoE-2. mIoU R@0.3 R@0.5 Score RG-L Sim mIoU R@0.3 R@0.5 Score RG-L Sim mIoU R@0.3 R@0.5 Score RG-L Sim Race Arab Indigenous Asian White Hispanic Black Gender Male Female Age 1824 2539 40+ 23.4 36.7 31.3 24.4 25.1 22.4 32.6 55.7 46.6 35.3 36.6 33.5 21.1 40.9 30.7 23.0 23.8 19.5 5.23 6.41 5.69 5.22 5.19 5.20 26.9 28.0 27.6 28.0 27.6 28. 0.638 0.694 0.658 0.666 0.669 0.675 25.1 25.7 37.1 37.0 23.5 24.2 5.21 5.42 27.8 28. 0.665 0.666 27.9 26.1 24.2 42.8 38.2 34.9 27.3 25.2 21.9 5.64 5.35 5.20 29.4 27.7 27. 0.696 0.668 0.658 2.3 4.0 4.5 3.6 3.4 3.3 3.7 3.5 3.9 3.4 3.7 2.9 8.1 6.1 5.2 5.4 4.8 5.3 5. 6.5 4.7 5.4 1.6 0.0 2.9 2.6 2.3 1.8 2.6 2.1 2.4 2.5 2.3 2.59 2.48 2.67 2.62 2.58 2.60 27.0 27.5 27.6 27.7 27.1 28. 0.686 0.677 0.690 0.695 0.684 0.696 2.59 2.66 27.8 27.5 0.695 0.689 2.57 2.51 2.72 29.7 27.5 27. 0.712 0.694 0.687 0.7 1.2 1.6 1.6 0.7 0.9 1.5 1.2 1.8 1.7 0.9 0.2 1.3 1.8 2.0 0.8 0.8 1.7 1. 1.8 2.1 0.9 0.2 1.3 0.6 1.2 0.1 0.6 0.9 0.7 1.2 1.2 0.4 2.33 2.51 2.26 2.10 2.08 2.15 22.6 28.3 22.8 23.9 23.7 24. 0.530 0.594 0.521 0.557 0.566 0.566 2.14 2.16 23.9 23.6 0.556 0.548 2.34 2.15 2.11 26.5 23.8 23. 0.589 0.555 0.544 Table A18. Performance of MLLMs on the temporal localization task across demographics (Models 46). Results on all dataset. mIoU (%), R@0.3/R@0.5 (%), and rationale quality (Score: Judge score (010), RG-L: ROUGE-L (%), Sim: similarity (01)). Bold = best, highlighted = worst within each demographic group per column. Group MiniCPM-o-2.6 VITA 1.5 VideoLLaMA mIoU R@0.3 R@0.5 Score RG-L Sim mIoU R@0.3 R@0.5 Score RG-L Sim mIoU R@0.3 R@0.5 Score RG-L Sim Race Arab Indigenous Asian White Hispanic Black Gender Male Female Age 1824 2539 40+ 3.0 0.0 2.1 2.0 1.9 2.1 1.9 2.3 2.2 1.9 2.2 3.6 0.0 3.0 2.5 2.5 2.7 2.5 2. 3.6 2.4 2.7 2.6 0.0 0.8 0.9 0.2 0.3 0.7 0.9 1.1 0.7 0.8 3.46 4.23 3.60 3.56 3.65 3.77 18.6 21.1 18.1 19.3 18.6 19. 0.412 0.484 0.377 0.405 0.399 0.416 3.62 3.60 19.3 18.7 0.405 0.400 3.97 3.65 3.52 21.0 18.8 18. 0.432 0.404 0.396 2.0 1.2 2.3 1.9 1.7 2.3 1.9 2.2 1.6 2.0 2.2 2.6 1.3 3.5 2.9 3.0 4.0 3.0 3. 2.0 3.2 3.3 1.2 1.3 1.4 1.4 0.8 1.4 1.2 1.4 1.1 1.2 1.5 3.62 4.56 4.03 3.88 3.70 4.08 22.0 23.9 20.8 22.0 21.8 22. 0.433 0.501 0.403 0.434 0.431 0.445 3.90 3.93 21.9 21.6 0.432 0.429 4.40 3.96 3.79 23.3 21.5 21. 0.455 0.432 0.426 2.5 4.8 3.8 3.4 3.0 2.8 3.3 3.4 3.1 3.2 3.4 0.5 1.3 1.4 1.8 1.5 1.1 1.5 1. 0.8 1.2 1.9 0.0 1.3 0.4 0.5 0.0 0.3 0.4 0.4 0.3 0.3 0.4 2.19 2.46 1.99 2.14 2.27 2.02 22.7 22.0 22.9 22.2 21.3 23. 0.496 0.516 0.512 0.493 0.494 0.525 2.06 2.19 22.6 22.3 0.507 0.497 2.29 2.15 2.05 24.7 22.4 22. 0.545 0.503 0.494 46 SONIC-O1: Real-World Benchmark for Evaluating Multimodal Large Language Models Table A19. Emotional language analysis. Analysis of MLLM empathic summary outputs on the all dataset (Task 1: summarization) using LIWC-22 categories. Bold denotes best performance, underline denotes second-best. Model Gemini 3.0 Pro Qwen3-Omni UniMoE-2.0 MiniCPM-o-2.6 VITA-1.5 VideoLLaMA Prosocial (%) Affiliation (%) Insight (%) Total Emotion (%) Neg. Emotion (%) Tone (percentile) 1.82 1.97 2.07 1.06 1.60 0.62 2.68 2.78 2.69 1.42 2.08 0.93 3.07 3.00 3.39 2.51 3.51 2.16 4.35 3.88 3.54 1.41 2.65 2.02 2.03 1.39 0.93 0.38 0.59 0.49 46.16 56.99 57.94 46.10 55.45 49. J. Emotional Language Analysis In Section 4.4, we discussed empathic capabilities of models. Table A19 reports the psycholinguistic profile of model empathic summaries across six LIWC-22 dimensions: Prosocial (helping or caring language such as support, help), Affiliation (references to social connections like friend, together), Insight (cognitive understanding words like realize, understand), Total Emotion (any emotional vocabulary), Negative Emotion (distress-specific terms like anxious, worried), and Tone (overall warmth and positivity on 0-100 percentile scale, where higher values indicate warmer, more informal language). J.1. Key Observations Divergent empathy strategies. Models adopt fundamentally different empathic approaches. Gemini prioritizes emotional validation (4.35% Total Emotion, 2.03% Negative Emotion) but maintains formal tone (46.16, below neutral 50th percentile). UniMoE-2.0 shows the opposite profile: highest Prosocial language (2.07%) and warmest Tone (57.94) but lower emotional intensity (3.54%, 0.93%). Qwen3-Omni falls between these extremes (1.97% Prosocial, 3.88% Emotion, 56.99 Tone), demonstrating balanced empathic expression. Cognitive vs. affective empathy. VITA-1.5 achieves the highest Insight score (3.51%), reflecting strong cognitive perspective-taking, yet minimal emotional validation (0.59% Negative Emotion). This dissociation reveals that VITA 1.5 can articulate understanding of mental states but fails to express emotional responses, showing understanding without feeling. Small model limitations. Smaller models (MiniCPM-o-2.6, VideoLLaMA2) show consistently weak empathic performance. VideoLLaMA2 produces only 0.62% Prosocial language and 2.02% Total Emotion with near-neutral Tone (49.83), reading as detached clinical descriptions. These results show that empathic modulation requires model scale since smaller models default to factual reporting without adapting linguistic style to emotional contexts. Tone-emotion independence. Tone and emotional vocabulary do not correlate: Geminis high emotion (4.35%, 2.03%) pairs with low Tone (46.16), while UniMoEs high Tone (57.94) pairs with lower emotion (3.54%, 0.93%). This pattern reveals that warmth and emotional validation are orthogonal dimensions. The optimal balance depends on context: medical consultations may benefit from Geminis approach (serious tone, high emotional validation), while customer service may favor UniMoEs profile (warm tone, supportive language). Implications. Current models lack nuanced empathic control. While commercial models demonstrate some capacity for empathic modulation when prompted, they adopt fixed strategies rather than adapting to situational demands. Future work should explore fine-grained empathy prompting that separately controls emotional validation, warmth, and perspectivetaking; context-adaptive empathy that adjusts based on interaction type; and empathy-focused training objectives beyond standard language modeling."
        },
        {
            "title": "Data Release and License",
            "content": "transparency and reproducibility, we To support at https://huggingface.co/datasets/vector-institute/sonic-o1. All personally identifiable information (PII) was removed during preprocessing. The dataset is distributed under the Creative Commons Attribution 4.0 International (CC BY 4.0) license, which permits reuse, redistribution, and adaptation with appropriate attribution to the original authors. We also release the scripts used to process the data and reproduce the experiments in the same repository. The dataset page includes persistent download link and detailed documentation (e.g., dataset structure, fields, and usage instructions). this preprint full dataset alongside release the"
        }
    ],
    "affiliations": [
        "University of Groningen, Nijenborgh 4, 9747 AG Groningen, Netherlands",
        "Vector Institute Intelligence, MaRS for Artificial Centre, Toronto, ON M5G 1L7, Canada",
        "York University, 4700 Keele Street, Toronto, ON M3J 1P3, Canada"
    ]
}