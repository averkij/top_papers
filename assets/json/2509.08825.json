{
    "paper_title": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation",
    "authors": [
        "Joachim Baumann",
        "Paul Röttger",
        "Aleksandra Urman",
        "Albert Wendsjö",
        "Flor Miriam Plaza-del-Arco",
        "Johannes B. Gruber",
        "Dirk Hovy"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are rapidly transforming social science research by enabling the automation of labor-intensive tasks like data annotation and text analysis. However, LLM outputs vary significantly depending on the implementation choices made by researchers (e.g., model selection, prompting strategy, or temperature settings). Such variation can introduce systematic biases and random errors, which propagate to downstream analyses and cause Type I, Type II, Type S, or Type M errors. We call this LLM hacking. We quantify the risk of LLM hacking by replicating 37 data annotation tasks from 21 published social science research studies with 18 different models. Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure how plausible researcher choices affect statistical conclusions. We find incorrect conclusions based on LLM-annotated data in approximately one in three hypotheses for state-of-the-art models, and in half the hypotheses for small language models. While our findings show that higher task performance and better general model capabilities reduce LLM hacking risk, even highly accurate models do not completely eliminate it. The risk of LLM hacking decreases as effect sizes increase, indicating the need for more rigorous verification of findings near significance thresholds. Our extensive analysis of LLM hacking mitigation techniques emphasizes the importance of human annotations in reducing false positive findings and improving model selection. Surprisingly, common regression estimator correction techniques are largely ineffective in reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors. Beyond accidental errors, we find that intentional LLM hacking is unacceptably simple. With few LLMs and just a handful of prompt paraphrases, anything can be presented as statistically significant."
        },
        {
            "title": "Start",
            "content": "Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation 1 Joachim Baumann , Paul Röttger 1 2 , Aleksandra Urman 4 Plaza-del-Arco 5 , Johannes B. Gruber , Albert Wendsjö 1 , and Dirk Hovy 3 , Flor Miriam 5 2 0 S 0 1 ] . [ 1 5 2 8 8 0 . 9 0 5 2 : r 1Bocconi University 2University of Zurich 3University of Gothenburg 4LIACS, Leiden University 5GESIS, Leibniz Institute for the Social Sciences Abstract Large language models (LLMs) are rapidly transforming social science research by enabling the automation of labor-intensive tasks like data annotation and text analysis. However, LLM outputs vary significantly depending on the implementation choices made by researchers (e.g., model selection, prompting strategy, or temperature settings). Such variation can introduce systematic biases and random errors, which propagate to downstream analyses and cause Type (false positive), Type II (false negative), Type (wrong sign for significant effect), or Type (correct but exaggerated effect) errors. We call this LLM hacking. We quantify the risk of LLM hacking by replicating 37 data annotation tasks from 21 published social science research studies with 18 different models. Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure how plausible researcher choices affect statistical conclusions. We find incorrect conclusions based on LLM-annotated data in approximately one in three hypotheses for state-of-the-art (SOTA) models, and in half the hypotheses for small language models. While our findings show that higher task performance and better general model capabilities reduce LLM hacking risk, even highly accurate models do not completely eliminate it. The risk of LLM hacking decreases as effect sizes increase, indicating the need for more rigorous verification of findings near significance thresholds. Our extensive analysis of LLM hacking mitigation techniques emphasizes the importance of human annotations in reducing false positive findings and improving model selection. Surprisingly, common regression estimator correction techniques are largely ineffective in reducing LLM hacking risk, as they heavily trade off Type vs. Type II errors. Beyond accidental errors, we find that intentional LLM hacking is unacceptably simple. With few LLMs and just handful of prompt paraphrases, anything can be presented as statistically significant. Overall, our findings advocate for fundamental shift in LLM-assisted research practices, from viewing LLMs as convenient black-box annotators to seeing them as complex instruments that require rigorous validation. Based on our findings, we publish list of practical recommendations to limit accidental and deliberate LLM hacking for various common tasks. 1 Figure 1: We quantify LLM hacking risk through systematic replication of 37 diverse computational social science annotation tasks. For these tasks, we use combined set of 2,361 realistic hypotheses that researchers might test using these annotations. Then, we collect 13 million LLM annotations across plausible LLM configurations. These annotations feed into 1.4 million regressions testing the hypotheses. For hypothesis with no true effect (ground truth > 0.05), different LLM configurations yield conflicting conclusions. Checkmarks indicate correct statistical conclusions matching ground truth; crosses indicate LLM hacking incorrect conclusions due to annotation errors. Across all experiments, LLM hacking occurs in 31-50% of cases even with highly capable models. Since minor configuration changes can flip scientific conclusions, from correct to incorrect, LLM hacking can be exploited to present anything as statistically significant."
        },
        {
            "title": "1 Introduction",
            "content": "Large language models (LLMs) can easily be instructed to process unstructured data for virtually any analytical purpose [125, 93]. It is thus tempting for researchers to outsource time-consuming tasks like data annotation or text analysis to these systems. For data-driven analyses in particular, the ability to extract insights from vast amounts of unstructured text through automatic annotation enables research at unprecedented scales [26, 138]. We tend to forget that high-performing models paired with careful prompt engineering can still produce bad science. The integration of LLMs into scientific research workflows represents one of the most significant recent methodological shifts in the social sciences and other scientific disciplines. Unfortunately, beneath this excitement for using LLMs in research lies fundamental threat to scientific validity that has remained largely unexamined. Traditionally, computational social scientists used trained student assistants or domain experts to annotate unstructured data to feed their quantitative downstream analyses of interest [73, 123, 57]. As the volume of available text data grew exponentially, researchers increasingly turned to computational methods that could scale analysis, either through unsupervised techniques or by training supervised machine learning models on human-labeled data [39, 87, 10, 62]. In recent years, large part of the community has enthusiastically embraced the use of LLMs for automated scientific analyses, with many reporting that LLMs can match or even exceed human performance on various annotation tasks [37, 138, 114]. Yet this same literature almost entirely overlooks how annotation errors propagate through statistical analyses [27, 38]. This gap between adoption and validation threatens to undermine the credibility of an entire era of computational social science (CSS) research. Every LLM-based annotation requires researchers to make numerous configuration choices: 2 which model to use, how to formulate the prompt, which decoding parameters to set, and how to map outputs to categories, among others. These configuration choices become garden of forking paths [34], where each decision point branches out into different analytical outcomes. This effect is magnified by LLMs sensitivity to seemingly minor changes to the input [105, 104]. And when these LLM outputs determine scientific conclusions about human behavior, political discourse, or social phenomena, even minor input variations become consequential [27, 95, 5, 6]. For example, when researcher tests whether conservative political discourse contains more economic framing than progressive discourse [8, 92] or whether social media posts mentioning certain topics exhibit different sentiment patterns [37], the conclusion can flip from significant to non-significant based solely on arbitrary researcher decisions (or lack thereof) about the LLM configuration earlier in the workflow. Insidiously, LLM configuration choices can systematically bias results while maintaining apparent methodological rigor. In this paper, we introduce the concept of LLM hacking1 , describing the phenomenon where researchers LLM configuration choices result in incorrect downstream scientific conclusions. This issue extends beyond simple measurement error. LLM hacking often emerges from ambiguity about how to configure LLMs, but researchers can also deliberately exploit multiple configuration comparisons to achieve desirable, statistically significant results. Concerns about the reliability of hypothesis testing [12] and the risk of manufactured results [43, 110] are as old as scientific data analysis itself. However, while LLM hacking is related to p-hacking in its susceptibility to researcher degrees of freedom [107], it operates at the data generation stage rather than during the analysis stage. This creates an entirely new layer of vulnerability. Our work demonstrates that the scientific risk of LLM hacking is not merely theoretical but pressing challenge for reproducibility in CSS. Our findings demand fundamental reconsideration of how LLMs should be integrated into scientific workflows. We provide practical recommendations, which we hope will help researchers navigate these challenges and harness the transformative potential of LLMs for CSS research, without falling prey to LLM hacking. Research questions. Through systematic literature review and an extensive CSS replication study, we address four fundamental research questions (RQs): RQ1: What are current practices for using LLMs as automated annotators in CSS research? RQ2: What is the risk of accidental LLM hacking across different tasks and models? RQ3: How can accidental LLM hacking risk be reduced through mitigation strategies and validation practices? RQ4: How feasible is intentional LLM hacking manipulation across different tasks?"
        },
        {
            "title": "1.1 Key contributions",
            "content": "We introduce the concept of Large Language Model (LLM) hacking (2). We then quantify the risk of LLM hacking through large-scale empirical assessment of over 13 million annotations across 37 diverse CSS tasks (see Table 1), 18 LLMs, and 2,361 realistic hypothesis tests. This includes tasks like stance detection or political ideology classification, with hypotheses testing 1 We use hacking in the statistical sense (analogous to p-hacking). This is unrelated to cybersecurity or unauthorized system intrusion. 3 group differences, such as whether Conservative manifestos express more right-wing economic positions than Labour manifestos. Our experimental setup (4) is informed by systematic literature analysis of 103 papers, which revealed enthusiastic advocacy for LLM usage alongside insufficient validation and limited attention to potential risks (3)."
        },
        {
            "title": "1.2 Summary of findings",
            "content": "In more detail, we report the following key findings: SOTA models have on average one-in-three chance of LLM Hacking (5.1). LLM hacking affects every model and task tested (see Table 3), from small 1B parameter models with 50% error rates to state-of-the-art 70B+ parameter models still exhibiting 31% risk. The risk varies strongly across the annotation tasks we evaluated, ranging from 5% for humor detection to over 65% for ideology and frame classification (for some model configurations), showing that no annotation task is immune. Type II errors dominate, with models more frequently missing true effects than fabricating false ones. Type II errors occur in 31-59% of cases (depending on model size). Even when models correctly identify significant effects, the estimated effect sizes deviate from true values by 40-77% on average. This systematic bias means that researchers cannot simply rely on larger, more capable models to escape the problem. Beyond accidental errors, our experiments demonstrate the feasibility of intentional result manipulation through model and prompt selection. It is shockingly easy to present any finding as statistically significant (5.2). By simply selecting one of the tested models with handful of prompt paraphrases, malicious actors can arrive at any desired downstream conclusion. Using just the models and prompts we tested, false positives can be manufactured for 94.4% of null hypotheses, while true effects can be hidden in 98.1% of cases. Most alarmingly, statistically significant effects can be reversed entirely in 68.3% of cases with true differences (Type error). These rates remain high even among the top-performing models that reviewers might consider methodologically sound. To make matters worse, the distinction between legitimate and manipulated analyses becomes virtually undetectable post hoc. This vulnerability means that researcher seeking to support predetermined conclusion is almost guaranteed to succeed while maintaining apparent scientific credibility. Neither high annotation performance nor careful prompting prevent LLM hacking (5.3). Through large regression analysis of all hypothesis-model-prompt combinations  (Table 9)  , we establish clear hierarchy of risk factors  (Table 4)  . Proximity to statistical significance thresholds emerges as the strongest predictor, with values near 0.05 showing error rates approaching 70%. Task characteristics account for 21% of the explained variance, while model performance contributes only 8%. Surprisingly, prompt engineering choices contribute less than 1% of explained variance, challenging the common belief that careful prompt design can eliminate these risks. Surprisingly, we find no correlation between human inter-annotator agreement and LLM hacking risk, meaning that even tasks where human experts perfectly agree can yield unreliable LLM-based conclusions. 100 human annotations outperform 100K LLM annotations (5.4). Testing 21 different approaches combining human annotation sampling strategies with statistical corrections, we discover an LLM data scale paradox: Using human annotations alone often provides the strongest 4 protection against false positives, achieving error rates of 10% with just 100 human labels, compared to 10-40% for hybrid approaches with acceptable false negative rates below 80% (using >100K annotations from various LLMs for each task). Regression estimator correction techniques are largely ineffective because they trade off Type vs. Type II errors. Rather than solving the underlying problem, statistical correction methods like Design-based Supervised Learning (DSL) and Confidence-Driven Inference (CDI) face unavoidable trade-offs between error types. Both methods reduce Type errors but increases Type II errors by up to 60 percentage points. Even optimal mitigation strategies achieve only modest risk reductions, with 1,000 human annotations reducing overall LLM hacking risk to about 20%, compared to 31-50% baseline risk. Researchers should use LLMs wisely, not widely  (Table 5)  . Our systematic experiments allow us to develop evidence-based guidelines for researchers navigating the trade-offs between annotation efficiency and scientific validity. These concrete recommendations acknowledge both the appeal of LLM automation and its fundamental limitations. For research contexts where false positives (Type errors) pose the greatest risk, such as novel discovery claims, we show that ground-truth-only approaches provide the only reliable protection. If, instead, Type II errors are the primary concern, or when human annotation is truly infeasible, our results suggest that researchers use specific model selection strategies and correction techniques. We recommend transparency standards, including pre-registration of LLM configuration choices and comprehensive reporting of all tested combinations, as essential safeguards against both accidental and deliberate manipulation. Taken together, our results show that, while LLMs offer unprecedented scalability for data annotation, their use in hypothesis testing requires fundamental changes to current practices. We need to shift from treating them as convenient replacements for human annotators to recognizing them as complex instruments requiring careful calibration and validation."
        },
        {
            "title": "2 Large Language Model (LLM) Hacking",
            "content": "We formalize LLM hacking as phenomenon occurring when researchers using LLMs for data annotation draw incorrect scientific conclusions. Depending on the researchers outcome of interest, wrong conclusions can be the false (non)discovery of an effect or wrong statistical estimate, for example. In this study, we focus specifically on LLM-generated text annotations used in regression analyses. More precisely, the scientific outcome of interest is whether regression coefficients reach statistical significance. However, the concept of LLM hacking is more general and encompasses any plausible scientific conclusion downstream of LLM usage. LLM hacking is consequence of the researchers vast degrees of freedom in LLM configuration choices. In the course of using LLMs to prepare data for downstream statistical analysis, researchers have many decisions to make. Each choice is often plausible but questionable at the same time, since the downstream impact on result validity is hard to quantify. This issue is exacerbated by the fact that many decisions are not explicitly made by models and model API services, but rather default to predetermined values that researchers are sometimes unaware of. Definition 1 (LLM Hacking). Let θ denote the true outcome of interest, Φ the space of possible reasonable LLM configurations, and ˆθ(ϕ) the outcome obtained using configuration ϕ Φ. LLM 5 hacking occurs when: L(ϕ) = 1[ ˆθ(ϕ) = θ] (1) Here, θ and ˆθ(ϕ) can, for example, represent conclusions from statistical hypothesis tests. The configuration space Φ encompasses all choices affecting LLM outputs, such as model selection, prompt formulation, temperature and other decoding parameters, and output mapping strategies (e.g., whether generated tokens are mapped to labels). Φ is context-dependent, but must be plausible, excluding configurations that are clearly unreasonable or produce nonsensical outputs (e.g., due to completely mis-specified prompts). Even seemingly minor variations like prompt paraphrases or small temperature adjustments can produce dramatically different outcomes that propagate to downstream analyses. When studying phenomena without true effects, LLM hacking produces Type errors (false positives). For real effects, it causes Type II errors (false negatives), Type errors (wrong direction), or Type errors (magnitude distortions) [77, 33]. Definition 2 (LLM Hacking Risk). The LLM hacking risk quantifies the probability of obtaining false conclusions when randomly selecting from reasonable configurations: EϕΦ[L(ϕ)] (2) Importantly, this risk exists even for well-intentioned researchers who must necessarily make configuration choices (or are subject to undocumented default settings) without clear guidance on optimal settings. Intentional LLM hacking. Beyond accidental errors, malicious actors could deliberately exploit the configuration flexibility to manufacture desired outcomes. We define LLM hacking feasibility as the existence of at least one configuration producing specific incorrect conclusion. Definition 3 (LLM Hacking Feasibility). For specific hypothesis and desired incorrect outcome ˆθ = θ, LLM hacking is feasible if: F( ˆθ) = 1[ϕ Φ : ˆθ(ϕ) = ˆθ] (3)"
        },
        {
            "title": "2.1 Relationship to p-hacking",
            "content": "LLM hacking differs fundamentally from p-hacking [107, 110], though both produce the same outcome: false statistical conclusions arising from researcher degrees of freedom [34]. The key distinction lies in their strategies. p-hacking manipulates analytical choices (e.g., variable selection, outlier removal, subgroup analysis), while LLM hacking manipulates data generation through configuration choices. Both practices can yield significant values where none should exist, but LLM hacking achieves this by shaping the annotated data itself rather than how that data is analyzed. Risks stemming from LLM hacking and p-hacking are cumulative. Hence, studies using LLMannotated data face both configuration-induced biases and traditional analytical flexibility issues, including selective reporting [128, 44], HARKing [53, 46], and publication bias [111, 102, 48, 108]."
        },
        {
            "title": "3 Literature review: Using LLMs for automated data annotation",
            "content": "To understand the extent of LLM use for data annotation in CSS research, we conducted comprehensive literature review. Our systematic search across five major databases yielded 1,592 initial papers, refined to 103 relevant studies through rigorous screening (see Appendix for full methodology). The selected papers from 2022-2025 benchmark LLM performance or provide guidance for CSS annotation tasks. We manually extracted relevant information including the CSS tasks covered, LLM usage recommendations made, and validation approaches discussed (see Appendix B). The risk of LLM hacking is widespread. Our systematic review reveals strong consensus toward adopting LLMs for automated data annotation across CSS. 88% of reviewed papers recommend using LLMs for data annotation tasks (at least under certain conditions), while only 12% advise against their use. This overwhelming endorsement spans diverse range of CSS annotation tasks, including sentiment analysis, stance detection, hate speech identification and political discourse analysis. The reviewed studies evaluate LLM performance across numerous CSS tasks, with 84% using empirical datasets to benchmark annotation quality. Despite the limitations of using proprietary models for research [90], the GPT family of models dominates the evaluation landscape, appearing in over 70% of studies, while open-weight alternatives like Llama and Qwen are tested less frequently. Current validation practices for LLM annotation are insufficient. 43% of papers disregard model validation (despite recommending the use of LLMs). Papers that include validations predominantly focus on simple performance metrics. Only four papers explicitly mention the risk of false conclusions in downstream statistical analyses when using LLM-generated annotations [27, 95, 5, 6, 38]. Two of those [27, 38] propose techniques to correct regression estimates based on LLM annotation bias to reduce Type risk. We test both techniques in our experiments, see Section 4.5. Despite the widespread adoption of LLMs in research, we still lack any principled guidance on when their use might compromise scientific validity. We found no empirical evidence identifying which factors determine whether LLM errors will distort downstream hypothesis testing, regression analyses, or other statistical procedures commonly used in CSS research. It thus remains unclear how well-intentioned researchers can avoid erroneous conclusions and how easily these configuration choices could be exploited to manufacture invalid results. This omission represents fundamental gap in our understanding of LLM reliability for scientific inference. The validation gap in LLM usage for CSS is particularly troubling, given that these same papers advocate for replacing human annotators with automated systems whose reliability remains unverified for specific research contexts. Despite well-established evidence that LLMs are extremely brittle annotation tools [27], producing different outputs even with minor prompt modifications [105, 104], the prevailing sentiment appears to be that LLMs can seamlessly automate numerous annotation tasks [37, 114]."
        },
        {
            "title": "4.1 Data",
            "content": "Table 1 lists the 37 annotation tasks that we compiled from 21 datasets into unified format for systematic evaluation. We selected tasks that represent range of typical CSS annotation scenarios, including stance detection, topic classification, sentiment analysis, and many more. Table 1 lists the 37 annotation tasks that we compiled from 21 datasets into unified format for systematic evaluation. We selected tasks that represent range of typical CSS annotation scenarios, including stance detection, topic classification, sentiment analysis, and many more. These tasks span diverse domains from political science (party manifestos, election tweets) to public health (COVID-19 misinformation) and social psychology (humor detection, politeness classification). The annotation complexity ranges from straightforward binary classifications (relevant/irrelevant) to nuanced multi-class categorizations requiring domain expertise (hatespeech, issue framing). Starting from all 1,592 papers identified by our systematic literature review (Appendix A), we considered the subset of relevant CSS annotation tasks and checked those for available datasets. We sampled our final list of annotation tasks for diversity in data types (e.g., Tweets, news articles, Reddit posts, open-ended survey responses, and party manifestos) and prioritized datasets with publicly available ground truth labels (to be able to test LLM annotation reliability) and additional metadata (to formulate realistic hypotheses). Our selection includes datasets from early research on automated LLM annotation work Gilardi et al. [37] alongside more specialized resources such as the British Election Study open-ended responses [31], ensuring broad coverage of contemporary CSS research applications. Ground truth annotations. The ground truth labels come from expert annotators (trained research assistants, domain experts) and crowdworkers, with an average of 2.3 annotators per datapoint (see Table 1 and Appendix for details on annotator types and aggregation methods). Data preprocessing. To maximize comparability, we replicated the preprocessing from prior work, with two exceptions: we used universal deduplication (final sizes in Table 11) and stratified random sampling (considering class and grouping variables) for datasets over 10,000 instances to preserve statistical properties while keeping computational costs manageable. Furthermore, we decompose multiclass annotation tasks into binary comparisons (e.g., class vs. rest) to make hypotheses comparable across tasks in the downstream statistical analysis."
        },
        {
            "title": "4.2 Downstream statistical analysis",
            "content": "We use the LLM-generated annotations as inputs to downstream statistical analyses. Their conclusions constitute our primary outcome of interest. Consider, for example, the manifestos_econ_ideology task that classifies sentences from UK party manifestos as economically left or right-wing [8]. researcher might test whether Conservative party manifestos contain more right-wing economic positions than Labour manifestos, assuming the null hypothesis H0 that there are no differences in class proportions between parties, versus the alternative HA: significant difference exists between the parties. Similarly, for the framesI_tweets task from Gilardi 8 Table 1: Overview of annotation tasks used in experiments. See Appendix for detailed task descriptions. Dataset Name Task Name of DatNr. apoints: Used/Total of Nr. Ground Truth Classes of Prompts Nr. (% paraphrases) (% zero-shot) Nr. of Groupings Total (% original) Nr. of Ground Truth Annotators Per Datapoint essay [116] ideology_news [5] manifestos_uk [8] 489/489 essay_domestic 489/489 essay_housewife 489/489 essay_living 489/489 essay_location 489/489 essay_narrative 489/489 essay_worksocial 9968/37554 ideology_news manifestos_econ_ideology 2463/2463 9983/11620 manifestos_issue manifestos_social_ideology 2091/2091 tone tone [14] politeness_stack [22] politeness_stack politeness_wiki [22] politeness_wiki emotion [25] topic [27] hatespeech [29] issue_survey [31] misinfo [32] news [37] tweets17 [37] tweets23 [37] tweets [37] emotion topic hatespeech_explicit hatespeech_implicit hatespeech_target issue_survey misinfo framesI_news relevance_news framesII_tweets17 relevance_tweets17 framesI_tweets23 relevance_tweets23 framesII_tweets framesI_tweets relevance_tweets stance_tweets topic_tweets stance_climate 859/859 3298/3298 2171/2171 7276/7276 10000/10000 9998/21476 6135/6135 5475/5475 9922/101930 9991/25164 326/326 1599/1599 569/569 1856/1856 150/150 411/411 311/311 593/593 1934/1934 557/557 611/611 1793/ stance_climate [75] manifestos [81, 63] manifestos_issues_detailed 9986/1239149 factuality [83] fakenews 131] ideology_tweets [114] ideology_tweets humor [127] 9965/15225 7954/7954 4099/4099 9999/15817 factuality fakenews humor [106, 8 2 9 5 3 2 3 2 2 2 3 2 2 26 2 3 6 26 48 2 3 2 15 2 3 2 10 3 2 3 5 3 46 3 2 2 2 5 (80%) (100%) 5 (80%) (100%) 5 (80%) (100%) 5 (80%) (100%) 5 (80%) (100%) 5 (80%) (100%) 5 (60%) (100%) 5 (40%) (100%) 5 (40%) (80%) 5 (20%) (80%) 5 (60%) (80%) 5 (60%) (100%) 5 (60%) (100%) 5 (60%) (100%) 5 (20%) (60%) 5 (40%) (80%) 5 (40%) (80%) 5 (40%) (80%) 5 (20%) (80%) 5 (80%) (100%) 7 (0%) (85.7%) 7 (0%) (85.7%) 5 (0%) (80%) 5 (0%) (80%) 7 (0%) (85.7%) 7 (0%) (85.7%) 5 (0%) (80%) 7 (0%) (85.7%) 7 (0%) (85.7%) 7 (0%) (85.7%) 5 (0%) (80%) 5 (60%) (100%) 5 (40%) (100%) 5 (20%) (80%) 5 (80%) (100%) 5 (60%) (100%) 5 (60%) (100%) 128 (57.0%) 92 (79.3%) 131 (55.7%) 110 (66.4%) 98 (74.5%) 92 (79.3%) 58 (56.9%) 28 (32.1%) 28 (32.1%) 28 (32.1%) 25 (0%) 49 (61.2%) 44 (56.8%) 123 (37.4%) 33 (42.4%) 25 (0%) 43 (0%) 94 (21.3%) 364 (78.8%) 35 (45.7%) 25 (0%) 19 (0%) 74 (0%) 19 (0%) 25 (0%) 19 (0%) 64 (0%) 25 (0%) 19 (0%) 25 (0%) 37 (0%) 25 (0%) 212 (64.6%) 48 (47.9%) 34 (44.1%) 41 (53.7%) 22 (13.6%) 1 1 1 1 1 1 1 6.11 7.69 4.99 1 5 5 3.64 1 3 1-3 2 1-2 1 2 2 2 2 2 2 2 2 2 2 2 8 1 1-2 1 1 1 21 datasets 37 tasks 145277/1533400 266 199 (36.2%) (89.4%) 2361 (48.1%) 2.3 on average 9 et al. [37], researcher might test whether Tweets containing the keyword Trump are more likely to frame content moderation as problem than those without this keyword. We evaluate these hypotheses using logistic regression with binary dependent and independent variables. The dependent variable represents either the ground truth annotation or the LLMgenerated annotation, and the independent variable indicates group membership in one of two data subsets (e.g., Conservative vs. Labour manifestos, or texts containing Trump vs. not). Outcome of interest. For each hypothesis h, we run two logistic regressions logit(P(y = 1)) = α + βx: Ground truth regression: yGT xh, yielding coefficient βGT . LLM-informed regression (using configuration ϕ): yLLM h,ϕ xh, yielding coefficient βLLM h,ϕ We test for each coefficient whether it reaches statistical significance at α = 0.05, effectively asking: 2 Does the proportion of positive class labels differ significantly between the two groups? and h,ϕ denote the significance of the coefficients βGT SLLM h,ϕ and sgn() denote the sign function. and βLLM"
        },
        {
            "title": "Let SGT",
            "content": "In this study, we assume that βGT reflects the true effect. For the purpose of our experiments, this assumes no noise in human ground truth annotations, which means that the significance level of the ground truth regression estimate SGT is the same as the alternative hypothesis HA. If we now replace the ground truth with LLM annotations, four types of statistical errors can occur [77, 33]: Type error: detecting difference when none exists (SGT Type II error: missing true difference (SGT = 1 and SLLM Type error: wrong direction (SGT h,ϕ = 1 but sgn(βGT = SLLM Type error: correctly detecting difference with correct direction but with incorrect = 0 and SLLM h,ϕ = 0) ) = sgn(βLLM h,ϕ = 1) h,ϕ )) magnitude (SGT = SLLM h,ϕ = 1 and sgn(βGT ) = sgn(βLLM h,ϕ ) but βGT = βLLM h,ϕ ) Hypotheses. We evaluate multiple hypotheses for each annotation task. We generate these hypotheses through different data groupings, i.e., criteria that split dataset into two parts. We consider 1) keyword-based splits (e.g., texts containing economy vs. not) and 2) splits based on original metadata (e.g., male vs. female authors). This process yields 2,361 hypotheses across all tasks, of which 48.1% are based on original metadata. See Appendix C.2 for more details."
        },
        {
            "title": "4.3 LLM configuration space",
            "content": "We rely on models and prompts used in prior work, wherever possible, to ensure plausible configuration space Φ. Models. We evaluate 18 models from 4 model providers representing the current state of the art. For each model family, we test different scales: 1. Llama 3 family (1B-70B) [72], 2 This question could also be answered with two-proportion z-test. We verified that both methods yield identical results. 10 2. Qwen 2.5 [98] and 3 [99] families (1.5B-72B), 3. Gemma family (1B-27B) [35], and 4. GPT-4o variants [91]. We use the instruction-tuned versions for all models, but omit -instruct suffices for brevity. We test both open-weight and proprietary models to evaluate the full spectrum of LLM capabilities typically used in CSS research. All models and exact versions used are listed in Appendix C.1. Prompts, decoding, and output mapping. We use the original prompts from prior work where available. We formulate new prompts based on the original data annotation guidelines when prior prompts are unavailable. We create few-shot prompt versions when these guidelines include specific examples. To test sensitivity to prompt formulation, we generated additional semantically equivalent prompt paraphrases to have at least five plausible prompt versions per task (details in Appendix C.3) Table 1 shows that this process yielded 5-7 prompts per task. The full set contains 199 prompts total. 72 (36.2)% are our generated paraphrases, 178 (89.4)% are zero-shot prompts, and 21 (10.6)% are few-shot prompts. We set the model temperature to 0 for reproducibility and the maximum number of generated tokens to 20. We match generated tokens to class labels using regular expressions to ensure valid category selection. We account for cases where models fail to follow instructions or produce invalid outputs (see Table E.2), finding that larger models demonstrate substantially better instruction adherence."
        },
        {
            "title": "4.4 Metrics",
            "content": "We use weighted F1 as performance metric throughout. If not mentioned otherwise, we average metrics across annotation tasks with hypotheses Ht for each task and for set of LLM configuration choices Φ (consisting of 18 models and up to seven prompts per task). Let H0 = 0} denote hypotheses where ground truth is non-significant, and H1 = 1} denote hypotheses where ground truth is significant. This gives us the following empirical risk measures: = {h Ht : SGT = {h Ht : SGT The Type risk quantifies the false positive rate: the probability that LLM annotations detect significant effect when none exists in the ground truth. Type Risk = 1 tT 1 H0 hH0 1 Φ ϕΦ 1[SLLM h,ϕ = 1] (4) The Type II risk quantifies the false negative rate: the probability that LLM annotations fail to detect true effects present in the ground truth data. Type II Risk = 1 tT 1 H1 hH1 1 Φ ϕΦ 1[SLLM h,ϕ = 0] (5) The Type risk captures the sign error rate: among true effects, the probability that LLM annotations detect significant effect in the opposite direction. This is particularly concerning form of error that fundamentally mischaracterizes the relationship between variables. Type Risk = 1 tT 1 H1 hH1 1 Φ ϕΦ 11 1[SLLM h,ϕ = 1, sgn(βGT ) = sgn(βLLM h,ϕ )] (6) Finally, the empirical LLM hacking risk measures the overall probability of reaching incorrect statistical conclusions: LLM Hacking Risk = Type Risk + Type II Risk + Type Risk 2 (7) This measure averages errors on null hypotheses (Type I) and errors on alternative hypotheses (Type II and S), providing balanced assessment of the overall risk of reaching incorrect statistical conclusions when using LLM annotations. We use this balanced measure since we do not know the expected base rates of true effects in the studied research domains. Even in the absence of LLM hacking, effect sizes of the downstream conclusion may still be overor underestimated. To empirically quantify Type errors, we calculate the ratio of effect sizes, measured as differences in group proportions, for cases where both ground truth and LLM annotations yield significant results with matching signs. Let pGT h,ϕ denote the differences in class proportions between groups for ground truth and LLM annotations, respectively. We define: and pLLM Type Risk = hH1 ϕΦ (cid:12) (cid:12) (cid:12) (cid:12) hH1 1 tT 1 pLLM h,ϕ pGT ϕΦ 1[SLLM 1[SLLM (cid:12) (cid:12) (cid:12) (cid:12) h,ϕ = 1, sgn(βGT h,ϕ = 1, sgn(βGT ) = sgn(βLLM h,ϕ )] ) = sgn(βLLM h,ϕ )] (8) The Type risk quantifies the average relative magnitude error for correctly identified effects. value of 0 indicates perfect magnitude estimation, while value of 0.5 means that LLM-based effect size estimates deviate from true effect sizes by an average of 50%."
        },
        {
            "title": "4.4.1 Feasibility of intentional LLM hacking",
            "content": "The LLM hacking feasibility rates quantify how often it is possible to obtain wrong scientific conclusion through intentional LLM hacking. Empirically, it is measured as the proportion of hypotheses for which at least one configuration exists that results in Type I, Type II, or Type error. More precisely: Type Error Feasibility Rate = Type II Error Feasibility Rate = 1 tT 1 H0 hH0 1 tT 1 H1 hH1 1[ϕ Φ : SLLM h,ϕ = 1] 1[ϕ Φ : SLLM h,ϕ = 0] (9) (10) Type Error Feasibility Rate = 1 tT 1 H1 hH1 1[ϕ Φ : SLLM h,ϕ = 1, sgn(βGT ) = sgn(βLLM h,ϕ )] (11) Here, the indicator functions denote the LLM hacking feasibility for hypothesis h. These rates quantify the potential for deliberate manipulation while maintaining apparent methodological credibility, particularly when constraining Φ to methodologically defensible choices (e.g., using only top-performing models). 12 Conversely, we measure the feasibility of correct scientific conclusions as the proportion of hypotheses for which at least one configuration exists that yields the correct outcome. We measure this for null and alternative hypotheses separately: H0 Correctness Feasibility Rate = 1 tT 1 H0 hH0 1[ϕ Φ : SLLM h,ϕ = 0] (12) HA Correctness Feasibility Rate = 1 tT 1 H1 hH1 1[ϕ Φ : SLLM h,ϕ = 1, sgn(βGT ) = sgn(βLLM h,ϕ )] (13) low correctness feasibility rate would suggest that LLMs may not be suitable for the given annotation task."
        },
        {
            "title": "4.5 Mitigation techniques using human-annotated samples",
            "content": "When researchers have access to limited number of human expert annotations (nhuman), these ground truth (GT) samples can become valuable resource to mitigate LLM hacking risk. Table 2 summarizes our mitigation strategies (M1-M9) along three dimensions: (1) how datapoints are sampled to be annotated by humans, (2) how the collected human annotations are used for downstream statistical analyses, and (3) how the LLM is selected (provided that LLM annotations are used for estimation). For each dimension, we test three strategies we identified in the literature. This gives an extensive testbed of 21 alternative techniques to mitigate LLM hacking. In the following, we describe each of the approaches across these three dimensions in more detail. Table 2: LLM hacking mitigation strategies (abbreviated with M1-M9) using human-annotated samples. The third dimension (how the model is selected) only applies to the strategies using LLM annotations, i.e., M2, M3, M5, M6, M8, and M9. l S t S ) 1 ( Data Usage Strategy (2) GT Only GT + LLM GT + LLM + Correction Random Low Confidence Active M1 M4 M2 M5 M8 M3 (DSL) M6 (DSL) M9 (CDI) (cid:124) (cid:123)(cid:122) Model Selection Strategy (3) Random, GPT-4o, Best-performing (cid:125) 4.5.1 Sampling strategies for human annotations Random sampling. Selects ground truth samples uniformly at random with probability nhuman for each instance. This is the sampling strategy used by Egami et al. [27]. Low confidence sampling. Several papers mention that researchers may over-sample data that is difficult to annotate automatically, for example, by increasing the sampling probability for 13 texts that LLMs are more uncertain about [65, 28]. We implement this strategy by sampling the nhuman instances with the lowest LLM prediction confidence. Following Lin et al. [67], Li et al. [65], and Gligoric et al. [38], we consider verbalized model confidence probabilities as proxy for the models expected performance. This approach is promising since LLMs are mostly able to provide information about their uncertainty themselves [120, 51]. See Appendix C.4 for more details on the verbalized confidence score elicitation. Active sampling. Active learning has emerged as technique to strategically annotate the most informative samples [117, 11]. Active learning typically uses supervised machine learning model to iteratively select the next batch of texts to be annotated. In Natural Language Processing, active learning has long history of improving data annotation by sampling datapoints that are either difficult or most representative of the full data [76, 136]. We use the implementation provided by Gligoric et al. [38], which uses an XGBoost model that predicts LLM annotation errors from its annotations and verbalized confidence scores. Data is then sampled proportionally to the predicted error, mixed with 10% uniform sampling for increased stability. This approach shares the same objective as low confidence sampling but is more sophisticated, as it learns the relationship between verbalized confidence and actual annotation error rather than simply using verbalized confidence as proxy. See Appendix C.5 for more implementation details."
        },
        {
            "title": "4.5.2 Using human annotations for downstream statistical analyses",
            "content": "Ground truth only (Table 2 cells M1, M4, and M7). Using only the small human-annotated sample instead of any LLM annotations is the simplest approach. This provides unbiased estimates but with high variance due to the limited sample size. Egami et al. [27] and Gligoric et al. [38] call this Gold-Standard Only and Human only, respectively. Ground truth + LLM annotations (M2, M5, M8). This second approach treats sampled ground truth labels as perfect replacements for LLM annotations on those instances, using LLM annotations for all other instances. This approach reduces the variance but potentially introduces bias from incorrect LLM predictions. Ground truth + LLM + Corrected estimator (M3, M6, M9). Finally, we combine human and LLM annotations using bias-corrected estimators that account for LLM annotation errors while leveraging the full dataset. We consider the following estimator correction techniques: Design-based Supervised Learning (DSL). Egami et al. [27] introduced DSL to correct downstream estimators for LLM prediction bias. DSL uses doubly-robust procedure that combines LLM predictions with human annotations to create bias-corrected pseudooutcomes. It maintains valid statistical inference even when LLM predictions are arbitrarily biased, requiring only that the sampling probability for human annotations is known and bounded away from zero. See Appendix C.6 for implementation details. Confidence-Driven Inference (CDI). Introduced by Gligoric et al. [38], CDI is built on top of the active sampling strategy described above. To produce an unbiased downstream estimate, CDI takes the active sampling probabilities into account and combines human annotations (where available) with LLM annotations. The final regression estimate additionally uses learned tuning parameter that controls the trust placed in LLM annotations to minimize the estimator variance while maintaining unbiasedness. See Appendix C.7 for implementation details. DSL and CDI are both optimized for producing confidence intervals that cover the true effect. Thus, they essentially reduce the risk for Type errors. However, Type II errors are not explicitly controlled for. 4.5.3 Model selection for automated data annotation When LLM annotations are incorporated into downstream analyses (M2, M3, M5, M6, M8, M9), researchers must decide which model to use. This choice can significantly impact the quality and reliability of downstream results. The predominant model validation approach identified in the literature involves ranking candidate models based on their performance on small subset of available ground truth data, then selecting the best-performing model for all subsequent annotations [69, 47, 2]. To evaluate the efficacy of this strategy, we compare it against two simple baselines (random and GPT-4o model selection) that do not require any ground truth data: Random: As baseline, we randomly select one model from the set of available LLMs with equal probability. This strategy serves as lower bound for model selection performance and requires no ground truth data. GPT-4o: Researchers often default to the best-performing/most well-known model within their budget. We implement this heuristic by consistently selecting GPT-4o, which represents one of the most widely used models at the time of this study. Best-performing: When ground truth annotations are available, we can select LLMs based on their performance on the annotated subset. This data-driven approach uses the humanannotated sample as test set to identify the most suitable model for the specific task at hand."
        },
        {
            "title": "5.1 Empirical LLM hacking risk",
            "content": "Even state-of-the-art LLMs produce incorrect scientific conclusions in substantial fraction of cases. Table 3 and Figure 2 show that empirical LLM hacking risk ranges from 31% for the best-performing 70B parameter models to 50% for 1B parameter models. Even when LLM annotations allow for the correct identification of significant effect, the estimated effect sizes are heavily biased. As shown in Table 3, the Type risk reveals that LLMbased effect size estimates deviate from true effect sizes by 40%-70% on average across models. This systematic inaccuracy makes precise effect size estimation particularly challenging with LLM annotations. The right panel of Figure 3 illustrates this limitation: even for the best-performing model (GPT-4o), only 2.7% of correctly identified significant effects have magnitude estimates within 10% of the true effect size. 15 Figure 2: Scaling relationships for LLM hacking risk and annotation performance. Left panel shows task-averaged LLM hacking risk decreasing with model size across all model families, with larger models consistently outperforming smaller ones. Right panel shows corresponding improvements in weighted F1 annotation performance. Both metrics exhibit clear scaling trends, though substantial risk remains even for the largest models. Type risk is substantial, though Type II errors dominate the error landscape across all model families. So on average, LLMs more often fail to detect true differences than fabricate false ones. While larger models within each family consistently show lower risk, the improvements follow pattern of diminishing returns. Figure 2 illustrates this scaling relationship. Going from Llama model with 8B to 70B parameters reduces the risk by only 5 percentage points (from 36.6% to 31.6%), whereas going from 1B to 8B parameters yields 13.6 percentage point improvement (from 50.3% to 36.6%). Similarly, going from Qwen2.5 with 7B to 72B parameters moderately improves the risk (35.0% to 31.8% risk), while scaling smaller models shows larger improvements. This correlation suggests fundamental limitations that additional scaling alone may not overcome, particularly beyond 30-70B parameters. Figures 2 (right panel) and 4 show similar patterns for annotation performance scaling. Performance metrics tell only part of the story. Figure 5 shows large variation in task difficulty, with weighted F1 scores ranging from about 0.2 to above 0.8 for some tasks. Yet even high performance does not prevent LLM hacking, as several tasks with F1 scores exceeding 0.93 still exhibit hacking risks above 50% (see Figure 7). This disconnect between annotation performance and downstream validity underscores that traditional performance metrics inadequately capture the risks to scientific inference. Practical Recommendation Use the most capable available models for annotation tasks (where model size is proxy for capability). Models with 70B+ parameters show approximately 31% LLM hacking risk compared to about 50% for 1-2B parameter models. However, expect diminishing returns with model scale. 16 Table 3: Empirical LLM Hacking Risk and Error Types by Model (lower is better). Highlights indicate best and worst risk values. Model Llama-3.2-1B Llama-3.2-3B Llama-3.1-8B Llama-3.1-70B Qwen2.5-1.5B Qwen2.5-3B Qwen2.5-7B Qwen2.5-32B Qwen2.5-72B Qwen3-1.7B Qwen3-4B Qwen3-8B Qwen3-32B Gemma-3-1b-it Gemma-3-4b-it Gemma-3-27b-it GPT-4o-mini GPT-4o LLM Hacking Risk Type Risk Type II Risk Type Risk Type Risk 0.771 0.572 0.482 0.415 0.695 0.601 0.488 0.447 0.422 0.585 0.612 0.502 0.473 0.725 0.566 0.449 0.494 0. 0.258 0.310 0.293 0.257 0.327 0.308 0.299 0.263 0.269 0.267 0.293 0.314 0.303 0.314 0.314 0.268 0.287 0.263 0.591 0.438 0.370 0.324 0.458 0.409 0.344 0.324 0.324 0.499 0.378 0.349 0.333 0.533 0.376 0.345 0.321 0.317 0.157 0.095 0.069 0.050 0.132 0.094 0.056 0.043 0.042 0.091 0.081 0.076 0.056 0.157 0.081 0.050 0.053 0.043 0.503 0.422 0.366 0.316 0.459 0.405 0.350 0.315 0.318 0.429 0.376 0.369 0.346 0.502 0.385 0.332 0.331 0.312 5."
        },
        {
            "title": "Intentional LLM hacking",
            "content": "Deliberate model selection and prompt formulation can make almost anything be presented as significant. This manipulation potential is far worse than accidental errors, as it can be exploited by what appear to be negligible configuration differences. Even when restricting analyses to topperforming models which reviewer might consider methodologically sound the deliberate hacking feasibility remains alarmingly high. Figure 6 shows that for hypotheses without true differences, Type errors can be manufactured in 94.4% of cases by using at least one model-prompt combination we tested. Similarly, fabricating Type II error (i.e., hiding true effect) is feasible in 98.1% of the cases with true differences. Even more surprisingly, Type errors, i.e., finding significant effects in the opposite direction, are still feasible in 68.3% of cases with true effects, enabling the complete reversal of scientific conclusions while maintaining and appearance of methodological rigor. Feasibility rates are consistently high across all annotation tasks (see Figure 14 in the Appendix). Given that correct conclusions are also feasible with LLM-generated annotations (in 99.2% and 96.2% of cases, respectively), these exorbitantly high feasibility rates underscore the fundamental unreliability of LLMs as data annotators. Even more concerning: our high reported hacking rates are actually even conservative estimates, as we considered only few models and prompts in our experiments. malicious actor could easily test more models and prompts to achieve 100% success rate. Even when considering only the seven best-performing models, Type and Type II error feasibility rates remain unacceptably high (69.7% and 68.1%, respectively). This rate suggests that 17 Figure 3: Type error analysis for seven top models: Relative magnitude errors indicate to what extent estimated effect sizes derived from LLM annotations deviate from true effect sizes in the absence of LLM hacking. Left: The histogram visualizes the distribution of Type errors. Right: The Cumulative probability shows the chance of landing below certain error level, as indicated on the x-axis. the vulnerability to manipulation is not merely consequence of poor-quality models but rather fundamental characteristic of LLM-based annotation approaches, where even methodologically defensible choices can produce incorrect outcomes. In general, constraining the configuration space Φ reduces LLM hacking feasibility. Preregistration of all LLM configuration choices, combined with reporting results from all preregistered configurations, thus makes intentional LLM hacking harder by eliminating post-hoc selection opportunities."
        },
        {
            "title": "Practical Recommendation",
            "content": "Document all tested model-prompt combinations, not just the final choice. As reviewer, be suspicious of studies reporting results from single LLM configuration without justification. Pre-register all LLM configuration choices, including model selection (criteria), prompt formulations, and decoding parameters, to make this standard practice in the field."
        },
        {
            "title": "5.3 Predictors of LLM hacking",
            "content": "If we understand what drives LLM hacking risk, we can devise targeted mitigation strategies, informing adequate research practice. To this end, we run an OLS regression analysis  (Table 9)  to 18 Figure 4: Average weighted F1 scores by model across all 37 annotation tasks. identify key predictors. In particular, we fit the following OLS regression model: LLM Hacking weighted F1 score (Task + Model) + significant difference found + normalized distance from significance threshold + prompt type + prompt detail (14) We include fixed effects to control for baseline differences across tasks and models. The inclusion of interaction terms allows the influence of model performance (F1 score) to vary across tasks and models, while the significance-related predictors capture whether statistical difference was detected and how far results lie from the = 0.05 threshold. Table 4 reveals clear hierarchy in the predictors of LLM hacking risk. The dominant factor is whether significant difference was found in the original analysis, accounting for 56.6% of the explained variance. This makes sense, as Type II risk is generally higher than Type risk. Furthermore, the normalized distance from the significance threshold adds another 10.2%. This shows that LLM hacking is more likely to occur near decision boundaries, which is when results hover close to significance thresholds. Task-specific characteristics contribute the secondlargest share at 20.8%, indicating that certain annotation tasks are inherently more vulnerable to configuration-dependent outcomes. Model performance (F1 score) explains only 7.7% of the variance. All else equal, prompt engineering choices alone often emphasized in practice cannot prevent LLM hacking (prioritizing fewover zero-shot prompts: 0.1%, and providing detailed over concise prompt formulations: 0.01%). This distribution challenges conventional wisdom about LLM reliability, suggesting that even high-performing models and careful prompt engineering cannot eliminate hacking risk, especially when results lie near statistical boundaries. In the following subsections, we examine each predictor to understand the mechanisms underlying LLM hacking vulnerability. 19 Figure 5: Average weighted F1 scores and LLM hacking risk across all 37 annotation tasks (sorted by decreasing LLM hacking risk). Error bars show 95% confidence intervals."
        },
        {
            "title": "5.3.1 Model capability and performance",
            "content": "Higher annotation performance strongly predicts lower LLM hacking risk, in general. The negative slopes in Figure 7 display this relationship across all tasks, with correlations ranging from -0.02 (essentially no relationship) to -0.46 (strong negative correlation). We list exact correlation scores in Table 10 in the Appendix. The variation in correlation strength across tasks suggests that for some annotation scenarios, improving performance directly translates to more reliable downstream inferences. However, even highly accurate annotations can lead to false conclusions. For example, we find instances with 93% weighted F1 score and 50% LLM hacking risk in the essay_housewife task. The relevance_tweets17 task even shows 96% weighted F1 score and an LLM hacking risk of 16%. General model capability, measured through MMLU scores, shows consistent negative relationship with hacking risk (see Figure 13 in Appendix E.5). Models scoring above 50% on MMLU exhibit lower LLM hacking risk (30%, on average), while those below 20% show risks approaching 50%. This relationship holds across model families, suggesting that broad capabilities transfer to more reliable annotation behavior even for specialized tasks. This is in line with research suggesting selecting LLMs based on public benchmarks [69, 47]. However, minor prompt reformulations can drastically change the outputs, which results in large model performance variance across prompt paraphrases (see Appendix E.5 for more details). 20 Figure 6: Average feasibility rates of LLM hacking (red bars) and correct conclusions (green bars) across annotation tasks. Red bars show the proportion of hypotheses where at least one configuration yields an incorrect conclusion (lower is better, indicating more robust results). Green bars show the proportion where at least one configuration yields the correct conclusion (higher is better, indicating greater potential for accurate conclusions). The top panel shows feasibility rates for hypotheses without significant differences, while the bottom panel shows feasibility rates for hypotheses with significant differences. Analysis restricted to top models includes: Llama-3.1-70B, Qwen2.5-32B, Qwen2.5-72B, Qwen3-32B, Gemma-3-27b, GPT-4o-mini, and GPT-4o."
        },
        {
            "title": "Practical Recommendation",
            "content": "Use models and prompts with high annotation performance for hypothesis testing. When F1 scores fall below 0.5, LLM hacking risk typically exceeds 40%. However, be aware that high performance alone does not guarantee the correctness of downstream conclusions. Results near conventional significance thresholds prove extraordinarily unreliable when using LLM-generated annotations. Figure 8 reveals how proximity to the = 0.05 threshold creates fundamental instability in research conclusions. The left panel of Figure 8a shows error risks conditional on ground truth values. This knowledge is of course only available in controlled experiments where the true labels are known. We observe spikes in Type risk (false positives) as ground truth values approach 0.05, reaching 21 Figure 7: LLM hacking risk versus model performance by task. Each dot represents model-prompt combination, where LLM hacking risk is calculated across all hypotheses within that task. Lines represent linear regression fits. Table 4: Variable Importance in Predicting LLM Hacking Predictor Significant Difference Found Task (Fixed Effects) Normalized Distance from Significance Threshold Weighted F1 score Model (Fixed Effects) Weighted F1 score Task Weighted F1 score Model Shot Type Prompt Detail Level Note: Relative importance calculated using the LMG method by [68] with the relaimpo package [40]. Values represent the proportion of total explained variance (R2 = 0.154) attributable to each predictor after accounting for correlations among predictors. Relative Importance (in %) 56.6% 20.8% 10.2% 7.7% 2.5% 1.9% 0.1% 0.1% 0.01% (a) Risk by values derived from ground truth annotations. (b) Error rates by values derived from LLM annotation. Figure 8: LLM hacking by values. Both panels show elevated LLM hacking near the significance threshold (p = 0.05). close to 70% Type risk probability. Type II and Type error risks are also elevated near the significance threshold. In other words: when the true effect is borderline, LLM annotations will likely yield contradicting conclusions. What is more concerning, though, is that Type I, Type II, and Type Risks all remain high even as values are distant from 0.05. Thus, LLM annotation errors can obscure or reverse even robust phenomena; that is, even if true effects are strong, LLM annotations may still produce wrong downstream conclusions. These findings suggest that the unreliability stems not merely from noise around decision boundaries but from systematic LLM limitations. The right panel (Figure 8b) presents the more practically relevant scenario: error rates as function of values derived from the LLM annotations themselves. Since researchers using LLM annotations typically lack access to ground truth, this represents the information actually available when making research decisions. Here we use discovery rates rather than risk metrics, since condition on ground truth outcomes (as we do in Equations (4.4)-(7)) does not make sense when also conditioning on LLM-derived values (x-axis in the right panel of Figure 8b). We provide exact metric definitions in Appendix D. Intuitively, the False Discovery Rate quantifies the fraction 23 of false positives among all discoveries. And the False Nondiscovery Rate quantifies the fraction of false negatives among all non-discoveries. Both False Discovery and False Nondiscovery rates surge dramatically for LLM-derived values around 0.05. This shows that LLM annotations that produce borderline significant results are systematically unreliable. The critical insight is that while ground truth values remain unknown in practice, researchers can observe the values resulting from their LLM-based analyses as form of reliability. In other words, the proximity to decision boundaries serves as an observable warning signal in practice. The persistence of elevated error rates even for strong effects (p values near 0 and 1) reveals fundamental limitation of LLM-based annotation. Unlike random measurement error, which primarily affects borderline cases, even highly significant effects can be false positives when relying on annotations produced by LLMs. Practical Recommendation Exercise extreme caution when interpreting any LLM-based statistical results, not just borderline cases. Our findings reveal that: Even when LLM-derived values are far from 0.05, error rates are substantial. Proximity to significance thresholds serves as warning signal, with False (Non)Discovery Rates peaking near = 0.05. LLM annotation errors can fundamentally reverse conclusions even for robust phenomena. Therefore, never rely on single LLM configuration for hypothesis testing, but always conduct sensitivity analyses across multiple models and prompts. Furthermore, treat all LLM-derived statistical conclusions as preliminary, requiring validation through alternative methods or data sources. Last but not least, report the full distribution of findings (including values and effect sizes) across different LLM configurations rather than single point estimate."
        },
        {
            "title": "5.3.2 Prompt engineering effects",
            "content": "Table 9 shows that zero-shot prompting increases risk relative to few-shot prompting, while brief prompts underperform detailed ones. Though the variable importance analysis  (Table 4)  shows that prompt engineering influence is modest compared to model and task characteristics, accounting for less than 1% of explained variance. However, these are not causal effects, and prompt detail effects are likely hidden in performance as predictor. Despite limited overall impact, all else equal, prompt engineering remains one of the few factors directly under researcher control. good prompt can increase performance scores and thereby reduce LLM hacking risk. However, researchers should not expect prompt optimization alone to be strong countermeasure to LLM hacking risk. Practical Recommendation Prefer few-shot over zero-shot prompting and use detailed task descriptions over brief instructions. While prompt engineering provides limited protection against LLM hacking per se, testing various different prompt formulations can help improve robustness. 24 5.3.3 Task characteristics and human annotator agreement LLM hacking risk varies across annotation tasks, from as low as 5% for humor detection to over 65% for ideology, factuality, and frame classification tasks (factuality, ideology_tweets, framesII_tweets, and manifestos_issues_detailed), for some models (see Table 4, as well as Table 7 in Appendix E.1). However, this variation shows no significant correlation with human inter-annotator agreement, as visualized in Figure 9. We measured human agreement as Krippendorffs alpha to allow for different data types and annotation scales across all annotation tasks [57]. This independence suggests that LLM errors follow different patterns than human disagreement. This means that even tasks with perfect human agreement can exhibit high LLM hacking risk, while tasks with human disagreement sometimes show lower risk. The absence of correlation with human agreement carries important implications for research practice. Researchers cannot use high human agreement as justification for automated annotation without validation. Even seemingly objective tasks with near-perfect human consensus can yield unreliable downstream inferences when annotated by LLMs."
        },
        {
            "title": "Practical Recommendation",
            "content": "Human annotations and high human annotator agreement are important aspects of the general data annotation pipeline. However, do not rely on human annotator agreement to determine whether LLM annotation is appropriate. High human agreement does not predict low LLM hacking risk. Validate LLM performance on your specific task and downstream analysis, regardless of human agreement rates."
        },
        {
            "title": "5.4 Mitigating unintentional LLM hacking risk",
            "content": "Access to even small numbers of human annotations enables multiple mitigation strategies, though each involves fundamental trade-offs. Figure 10 shows that no single strategy dominates across different error types. The most sophisticated statistical corrections often perform worse on average than simpler alternatives. Our findings suggest that human expert annotation plays crucial role in mitigating LLM hacking risks. Of the nine LLM hacking mitigation techniques described in Section 4.5, we here only focus on M1, M2, M3, and M9. Other techniques (M4, M5, M6, M7, M8) only differ in the sampling method used for the expert annotations and are shown in the full Figure 15 in the appendix. Differences across sampling methods are minor compared to the ground truth data usage approach we analyze and the fundamental trade-offs between error types we observe. To account for different annotation budgets, we consider nhuman = {25, 50, 100, 250, 500, 1000} known ground truth annotations, including tasks where specific number of known annotations amounts to at most 70% of the entire dataset. 5.4.1 Ground truth annotations outperform hybrid approaches for Type error risk Simply using human annotations alone (M1) achieves the lowest Type risk on average (see left panel of Figure 15 in the Appendix). With just 100 human annotations, this approach reduces Type risk to approximately 10% (see gray circles in Figure 10). This is much lower than approaches 25 Figure 9: LLM hacking risk versus inter-annotator agreement measured as the Krippendorffs alpha of the human annotations used as the ground truth. Krippendorffs alpha can only be calculated for tasks with more than one available annotation per datapoint (see Table 1 for an overview and Appendix for more details on the exact agreement calculations). Each point represents task with error bars indicating 95% CI. that combine the 100 GT samples with LLM annotations, which have Type risk of 30-38%. The advantage stems from avoiding LLM biases entirely rather than attempting correction. While limited sample sizes increase variance, producing wider confidence intervals and higher Type II risk, the estimates remain unbiased. For research contexts where false positives pose greater risks than false negatives, such as claiming novel discoveries, ground-truth-only approaches provide the most conservative and defensible option. Practical Recommendation Collect as many expert annotations as feasible. Using randomly sampled human annotations alone provides strong protection against Type errors, outperforming LLM-based techniques. 26 Figure 10: Trade-off between Type errors and combined Type II+S errors across mitigation strategies. Marker types represent different mitigation techniques with varying numbers of ground truth (GT) samples (100 or 1,000)."
        },
        {
            "title": "5.4.2 LLM bias correction methods create error trade-offs",
            "content": "Statistical correction techniques promise to combine the efficiency of LLM annotation with the reliability of human labels, but our results reveal fundamental tradeoffs, as visualized in Figure 10. Design-based Supervised Learning (M3) and confidence-Driven Inference (M9) successfully reduce Type errors compared to naive combination of human and LLM annotations (M2), achieving rates comparable to ground-truth-only approaches (M1). However, this improvement comes at substantial cost: Type II risk increases by up to 60 percentage points, meaning researchers miss more true effects. Relying only on small sample of human annotations provides the best of both worlds, with low Type and moderate Type II risk."
        },
        {
            "title": "Practical Recommendation",
            "content": "Choose bias correction methods based on research priorities. Use collected ground truth samples only or CDI-corrected LLM annotations when Type errors are most concerning (e.g., novel discovery claims). Use combination of GT samples + LLM annotations (M2) when Type II errors are most problematic (e.g., replication studies). Understand that all corrections involve trade-offs. 5.4.3 Human annotations are crucial The baseline risk without any mitigation ranges from about 31-50% across models, while solely relying on 1,000 human annotations reduces this to about 20%. Surprisingly, abandoning the use 27 Figure 11: Model selection strategy comparison across ground truth sample sizes and mitigation techniques. of LLMs and just collecting human samples turns out to be the most effective mitigation strategy, outperforming even sophisticated estimator correction techniques, on average. Figure 11 shows that increasing human annotations consistently reduces LLM hacking risk across all mitigation techniques. However, if researchers can only collect few human samples (< 1, 00), additional annotations from strong LLMs can reduce Type II risk. If LLM annotations are used, model selection based on performance with human-annotated subsamples provides additional protection against LLM hacking. For our 18 models tested, selecting the best performer for each hypothesis consistently reduces risk by about 4 percentage points compared to using GPT-4o for all tasks (see green lines in Figure 11). In this case, GPT-4o is selected as optimal in 49% of cases, making default selection of GPT-4o strong baseline (see Figure 17 in Appendix E.10). Practical Recommendation Prioritize human annotation collection over LLM sophistication. Even 100 human annotations outperform LLM-based approach in terms of Type risk. 1,000 human annotations outperform any LLM-based approach in terms of LLM hacking risk generally, on average. When relying on LLM annotations (e.g., because Type II risk is relevant or because even moderate-scale human annotation is truly infeasible), use human-annotated validation sets to select the best-performing model for your specific task. When using regression estimate correction techniques like CDI or DSL, it is crucial to consider their inherent Type vs. Type II error trade-off."
        },
        {
            "title": "6 Limitations",
            "content": "Our LLM hacking risk quantification assumes noise-free ground truth annotations and effective significance thresholds (p = 0.05) for identifying true effects. Real human annotations contain their own biases and errors. We may overestimate LLM hacking risk by treating imperfect human annotations as ground truth. However, detailed bias analyses for our datasets are unavailable, making this limitation difficult to address quantitatively. LLM hacking feasibility rates are unaffected by ground truth quality. Even if true effects differ from our estimates, the ability to achieve different downstream conclusions through configuration choices remains. This fundamental vulnerability exists regardless of annotation quality. We find no significant correlation between inter-annotator agreement and LLM hacking risk (Section 5.3.3). Low agreement may stem from multiple factors, such as task difficulty, subjectivity, instruction quality, and annotator selection biases. While some tasks lack single correct answers, we follow label aggregation methods suggested by prior work and exclude ambiguous examples in highly subjective tasks. Future work should disentangle how task characteristics affect LLM hacking risk. Our risk estimates depend on the configuration space considered. Misspecified prompts could inflate LLM hacking risk. We ensure plausibility of configurations by replicating models and prompts from prior research, and by excluding prompts that do not yield valid LLM outputs. Our extensive analyses across models and performance levels show that LLM hacking risk persists even with model scaling and high annotation performance. The problem is not whether risk exists, but that we find no evidence that it will disappear."
        },
        {
            "title": "7 Discussion",
            "content": "LLMs are rapidly becoming standard tools for data annotation across scientific disciplines. Researchers advocate for their cost-effectiveness and claim superior performance to crowdworkers [37, 114]. Our findings challenge enthusiasm for using LLMs as automated research assistants. With large-scale CSS replication study, we demonstrate that LLM-generated annotations lead to incorrect downstream statistical conclusions in 31-50% of cases. The problem extends beyond simple inaccuracy. Researchers cannot predict when, how, or which LLMs will fail. This unpredictability undermines the validity of any LLM-based hypothesis testing. Without rigorous safeguards, LLM adoption poses an emerging threat to scientific integrity. Our work aligns with emerging concerns about LLM use in behavioral science [30]. We provide the first systematic risk quantification, detailed predictor analysis, and evidence-based recommendations for the use LLMs as automated research assistants for text annotation tasks. While our guidelines cannot eliminate LLM hacking risk, they establish minimal standards for methodological rigor. Reviewers should use these criteria to assess LLM-based conclusions. Researchers must follow them to distinguish valid findings from configuration artifacts. Human-LLM annotation collaboration. Our results suggest that human annotations and taskspecific validation remain indispensable. Pure LLM approaches underperform hybrid strategies that leverage human expertise. Even 100 human annotations provide better Type error control 29 than sophisticated LLM-based corrections. Future systems should prioritize human-in-the-loop designs [79, 42, 134] rather than full automation. However, there are open questions regarding optimal collaboration methodologies [65, 56]. Multiverse LLM annotation approaches. Multiverse analysis [109] offers one path forward: reporting coefficient distributions across all reasonable LLM configurations rather than single point estimates. This transparency reveals the fragility of the robustness of findings. Future work should explore the practical feasibility of multiverse approaches and possible insights from analyzing p-curves from LLM annotations [108]. Annotator hacking. Human annotators introduce their own biases [1, 36, 23]. Guideline changes alone can shift annotations substantially [101]. Theoretically, annotator hacking through selective recruitment or instruction manipulation is possible. However, the cost and logistics of human annotation make intentional large-scale manipulation impractical. Besides, decades of experience have established reliable practices for human annotation. Our study aims to develop equivalent standards for the LLM era, defining questionable research practices when using automated annotators."
        },
        {
            "title": "7.1 Evidence-based recommendations for practice",
            "content": "Table 5 synthesizes our empirical findings into actionable guidelines. These recommendations acknowledge both the appeal of LLM annotation and its fundamental limitations. Researchers must choose strategies based on their specific error tolerances, for example, prioritizing Type error control for novel discoveries. Similarly, reviewers can use these guidelines when assessing the robustness of LLM-based annotation procedures reported. Additionally, transparency requirements provide essential safeguards against both accidental and deliberate manipulation. While these practices cannot eliminate LLM hacking risk, they establish minimum standards for scientific rigour with increasing research automation. 30 Table 5: Evidence-Based Guidelines for LLM-Assisted Data Annotation in Research Category Practical Recommendations A. LLM Hacking Risk Mitigation Task-Unspecific Mitigation (no human samples required) vs 47% for 1B) Use largest available models (70B+ show 27% risk Exercise extreme caution when values are close to significance threshold (risk >70% near = 0.05) Prefer few-shot over zero-shot prompting and use detailed task descriptions over brief instructions. Human-Annotated Sample-Enabled (human-annotated samples required) Collect as many expert annotations as possible. The lowest Type error rates are achieved by simply using randomly sampled human expert annotations, outperforming all LLM-based annotation techniques. Regression estimator correction techniques (e.g., DSL or CDI) can help in some cases but have practical limitations (e.g., they trade-off Type vs. Type II errors and require enough expert annotations). Low annotation accuracy is strong predictor of LLM hacking risk. Thus, avoid using low-quality annotations. But testing multiple models and selecting based on sample performance only yields minor improvements Do not rely on human annotator agreement to decide whether annotations should be automated or not. High human agreement rates are not associated with low LLM hacking risk. B. Transparency & Reproducibility Documentation Report all models, versions, prompts, and parameters Pre-registration tested Document selection criteria and decision process Release both LLM and human annotations with analysis code Specify criteria for model selection, prompts, and parameters before analysis Declare hypotheses and statistical tests in advance Document planned sensitivity analyses"
        },
        {
            "title": "References",
            "content": "[1] Hala Al Kuwatly, Maximilian Wich, and Georg Groh. Identifying and measuring annotator bias based on annotators demographic characteristics. In Seyi Akiwowo, Bertie Vidgen, Vinodkumar Prabhakaran, and Zeerak Waseem, editors, Proceedings of the Fourth Workshop on Online Abuse and Harms, pages 184190, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.alw-1.21. URL https://aclanthology.org/2020.alw1.21/. [2] Meysam Alizadeh, Maël Kubli, Zeynab Samei, Shirin Dehghani, Mohammadmasiha Zahedivafa, Juan Bermeo, Maria Korobeynikova, and Fabrizio Gilardi. Open-source llms for text annotation: practical guide for model setting and fine-tuning. Journal of Computational Social Science, 8(1), 2025. doi: 10.1007/s42001-024-00345-9. [3] David Alonso del Barrio and Daniel Gatica-Perez. Framing the news: From human perception to large language model inferences. In Proceedings of the 2023 ACM International Conference on Multimedia Retrieval, ICMR 23, page 627635, New York, NY, USA, 2023. Association for Computing Machinery. ISBN 9798400701788. doi: 10.1145/3591106.3592278. URL https://doi.org/10.1145/3591106.3592278. [4] Julian Ashwin, Aditya Chhabra, and Vijayendra Rao. Using large language models for qualitative analysis can introduce serious bias. Sociological Methods & Research, 2025. doi: 10.1177/00491241251338246. URL https://doi.org/10.1177/00491241251338246. [5] Ramy Baly, Giovanni Da San Martino, James Glass, and Preslav Nakov. We can detect your bias: Predicting the political ideology of news articles. In Bonnie Webber, Trevor Cohn, Yulan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP), pages 49824991, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.emnlp-main.404. URL https://aclanthology.org/2020.emnlp-main.404/. [6] Christopher Barrie, Alexis Palmer, and Arthur Spirling. Replication for language mod2024. URL https: els problems, principles, and best practice for political science. //arthurspirling.org/documents/BarriePalmerSpirling_TrustMeBro.pdf. [7] Yoav Benjamini and Yosef Hochberg. Controlling the false discovery rate: practical and powerful approach to multiple testing. Journal of the Royal Statistical Society: Series (Methodological), 57(1):289300, 1995. doi: https://doi.org/10.1111/j.2517-6161.1995.tb02031.x. URL https://rss.onlinelibrary.wiley.com/doi/abs/10.1111/j.2517-6161.1995.tb02031.x. [8] Kenneth Benoit, Drew Conway, Benjamin Lauderdale, Michael Laver, and Slava Mikhaylov. Crowd-sourced text analysis: Reproducible and agile production of political data. American Political Science Review, 110(2):278295, 2016. [9] Aanisha Bhattacharyya, Yaman Singla, Balaji Krishnamurthy, Rajiv Ratn Shah, and Changyou Chen. video is worth 4096 tokens: Verbalize videos to understand them in zero shot. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 98229839, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.608. URL https://aclanthology.org/2023.emnlp-main.608/. 32 [10] David M. Blei, Andrew Y. Ng, and Michael I. Jordan. Latent dirichlet allocation. J. Mach. Learn. Res., 3(null):9931022, March 2003. ISSN 1532-4435. [11] Mitchell Bosley, Saki Kuzushima, Ted Enamorado, and Yuki Shiraito. Improving probabilistic models in text classification via active learning. American Political Science Review, 119(2): 9851002, 2025. doi: 10.1017/S0003055424000716. [12] Nate Breznau, Eike Mark Rinke, Alexander Wuttke, Hung H. V. Nguyen, Muna Adem, Jule Adriaans, Amalia Alvarez-Benjumea, Henrik K. Andersen, Daniel Auer, Flavio Azevedo, Oke Bahnsen, Dave Balzer, Gerrit Bauer, Paul C. Bauer, Markus Baumann, Sharon Baute, Verena Benoit, Julian Bernauer, Carl Berning, Anna Berthold, Felix S. Bethke, Thomas Biegert, Katharina Blinzler, Johannes N. Blumenberg, Licia Bobzien, Andrea Bohman, Thijs Bol, Amie Bostic, Zuzanna Brzozowska, Katharina Burgdorf, Kaspar Burger, Kathrin B. Busch, Juan Carlos-Castillo, Nathan Chan, Pablo Christmann, Roxanne Connelly, Christian S. Czymara, Elena Damian, Alejandro Ecker, Achim Edelmann, Maureen A. Eger, Simon Ellerbrock, Anna Forke, Andrea Forster, Chris Gaasendam, Konstantin Gavras, Vernon Gayle, Theresa Gessler, Timo Gnambs, Amélie Godefroidt, Max Grömping, Martin Groß, Stefan Gruber, Tobias Gummer, Andreas Hadjar, Jan Paul Heisig, Sebastian Hellmeier, Stefanie Heyne, Magdalena Hirsch, Mikael Hjerm, Oshrat Hochman, Andreas Hövermann, Sophia Hunger, Christian Hunkler, Nora Huth, Zsófia S. Ignácz, Laura Jacobs, Jannes Jacobsen, Bastian Jaeger, Sebastian Jungkunz, Nils Jungmann, Mathias Kauff, Manuel Kleinert, Julia Klinger, Jan-Philipp Kolb, Marta Kołczy nska, John Kuk, Katharina Kunißen, Dafina Kurti Sinatra, Alexander Langenkamp, Philipp M. Lersch, Lea-Maria Löbel, Philipp Lutscher, Matthias Mader, Joan E. Madia, Natalia Malancu, Luis Maldonado, Helge Marahrens, Nicole Martin, Paul Martinez, Jochen Mayerl, Oscar J. Mayorga, Patricia McManus, Kyle McWagner, Cecil Meeusen, Daniel Meierrieks, Jonathan Mellon, Friedolin Merhout, Samuel Merk, Daniel Meyer, Leticia Micheli, Jonathan Mijs, Cristóbal Moya, Marcel Neunhoeffer, Daniel Nüst, Olav Nygård, Fabian Ochsenfeld, Gunnar Otte, Anna O. Pechenkina, Christopher Prosser, Louis Raes, Kevin Ralston, Miguel R. Ramos, Arne Roets, Jonathan Rogers, Guido Ropers, Robin Samuel, Gregor Sand, Ariela Schachter, Merlin Schaeffer, David Schieferdecker, Elmar Schlueter, Regine Schmidt, Katja M. Schmidt, Alexander Schmidt-Catran, Claudia Schmiedeberg, Jürgen Schneider, Martijn Schoonvelde, Julia Schulte-Cloos, Sandy Schumann, Reinhard Schunck, Jürgen Schupp, Julian Seuring, Henning Silber, Willem Sleegers, Nico Sonntag, Alexander Staudt, Nadia Steiber, Nils Steiner, Sebastian Sternberg, Dieter Stiers, Dragana Stojmenovska, Nora Storz, Erich Striessnig, Anne-Kathrin Stroppe, Janna Teltemann, Andrey Tibajev, Brian Tung, Giacomo Vagni, Jasper Van Assche, Meta van der Linden, Jolanda van der Noll, Arno Van Hootegem, Stefan Vogtenhuber, Bogdan Voicu, Fieke Wagemans, Nadja Wehl, Hannah Werner, Brenton M. Wiernik, Fabian Winter, Christof Wolf, Yuki Yamada, Nan Zhang, Conrad Ziller, Stefan Zins, and Tomasz Zółtak. Observing many researchers using the same data and hypothesis reveals hidden universe of uncertainty. Proceedings of the National Academy of Sciences, 119(44):e2203150119, 2022. doi: 10.1073/pnas.2203150119. URL https://www.pnas.org/doi/abs/10.1073/pnas.2203150119. [13] Martin Juan José Bucher and Marco Martini. Fine-tunedsmallllms (still) significantly outperform zero-shot generative ai models in text classification. arXiv preprint arXiv:2406.08660, 2024. 33 [14] David Carlson and Jacob Montgomery. pairwise comparison framework for fast, flexible, and reliable human coding of political texts. American Political Science Review, 111 (4):835843, 2017. [15] Youngjin Chae and Thomas Davidson. Large language models for text classification: From zero-shot learning to fine-tuning. Open Science Foundation, 10, 2023. [16] Chi-Min Chan, Weize Chen, Yusheng Su, Jianxuan Yu, Wei Xue, Shanghang Zhang, Jie Fu, and Zhiyuan Liu. Chateval: Towards better LLM-based evaluators through multiagent debate. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=FQepisCUWu. [17] shiqi chen, Yiran Zhao, Jinghan Zhang, I-Chun Chern, Siyang Gao, Pengfei Liu, and Junxian He. Felm: Benchmarking factuality evaluation of large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 44502 44523, 2023. URL https://proceedings.neurips.cc/paper_files/paper/2023/file/ 8b8a7960d343e023a6a0afe37eee6022-Paper-Datasets_and_Benchmarks.pdf. [18] Robert Chew, John Bollenbacher, Michael Wenger, Jessica Speer, and Annice Kim. Llmassisted content analysis: Using large language models to support deductive coding. arXiv preprint arXiv:2306.14924, 2023. [19] Cyril Chhun, Pierre Colombo, Fabian M. Suchanek, and Chloé Clavel. Of human criteria and automatic metrics: benchmark of the evaluation of story generation. In Nicoletta Calzolari, Chu-Ren Huang, Hansaem Kim, James Pustejovsky, Leo Wanner, Key-Sun Choi, Pum-Mo Ryu, Hsin-Hsi Chen, Lucia Donatelli, Heng Ji, Sadao Kurohashi, Patrizia Paggio, Nianwen Xue, Seokhwan Kim, Younggyun Hahm, Zhong He, Tony Kyungil Lee, Enrico Santus, Francis Bond, and Seung-Hoon Na, editors, Proceedings of the 29th International Conference on Computational Linguistics, pages 57945836, Gyeongju, Republic of Korea, October 2022. International Committee on Computational Linguistics. URL https://aclanthology.org/ 2022.coling-1.509/. [20] Cheng-Han Chiang and Hung-yi Lee. Can large language models be an alternative to human evaluations? In Anna Rogers, Jordan Boyd-Graber, and Naoaki Okazaki, editors, Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1560715631, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.acl-long.870. URL https://aclanthology.org/2023. acl-long.870/. [21] Eun Cheol Choi and Emilio Ferrara. Automated claim matching with large language models: Empowering fact-checkers in the fight against misinformation. In Companion Proceedings of the ACM Web Conference 2024, WWW 24, page 14411449, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400701726. doi: 10.1145/3589335.3651910. URL https://doi.org/10.1145/3589335.3651910. [22] Cristian Danescu-Niculescu-Mizil, Moritz Sudhof, Dan Jurafsky, Jure Leskovec, and Christopher Potts. computational approach to politeness with application to social factors. In Hinrich Schuetze, Pascale Fung, and Massimo Poesio, editors, Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 250259, Sofia, Bulgaria, August 2013. Association for Computational Linguistics. URL https://aclanthology.org/P13-1025/. [23] Aida Davani, Mark Díaz, Dylan Baker, and Vinodkumar Prabhakaran. Disentangling perceptions of offensiveness: Cultural and moral correlates. In Proceedings of the 2024 ACM Conference on Fairness, Accountability, and Transparency, FAccT 24, page 20072021, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704505. doi: 10.1145/3630106.3659021. URL https://doi.org/10.1145/3630106.3659021. [24] Thomas Davidson. Start generating: Harnessing generative artificial intelligence for sociological research. Socius, 10, 2024. [25] Dorottya Demszky, Dana Movshovitz-Attias, Jeongwoo Ko, Alan Cowen, Gaurav Nemade, and Sujith Ravi. GoEmotions: dataset of fine-grained emotions. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and Joel Tetreault, editors, Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, pages 40404054, Online, July 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.acl-main.372. URL https://aclanthology.org/2020.acl-main.372/. [26] Dorottya Demszky, Diyi Yang, David Yeager, Christopher Bryan, Margarett Clapper, Susannah Chandhok, Johannes Eichstaedt, Cameron Hecht, Jeremy Jamieson, Meghann Johnson, et al. Using large language models in psychology. Nature Reviews Psychology, 2(11): 688701, 2023. [27] Naoki Egami, Musashi Hinck, Brandon Stewart, and Hanying Wei. Using imperfect surrogates for downstream inference: Design-based supervised learning for social science applications of large language models. In A. Oh, T. Naumann, A. Globerson, K. Saenko, M. Hardt, and S. Levine, editors, Advances in Neural Information Processing Systems, volume 36, pages 6858968601. Curran Associates, Inc., 2023. URL https://proceedings.neurips.cc/paper_ files/paper/2023/file/d862f7f5445255090de13b825b880d59-Paper-Conference.pdf. [28] Naoki Egami, Musashi Hinck, Brandon Stewart, and Hanying Wei. Using large language model annotations for the social sciences: general framework of using predicted variables in downstream analyses. 2024. URL https://naokiegami.com/paper/dsl_ss.pdf. [29] Mai ElSherief, Caleb Ziems, David Muchlinski, Vaishnavi Anupindi, Jordyn Seybolt, Munmun De Choudhury, and Diyi Yang. Latent hatred: benchmark for understanding implicit hate speech. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 345363, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlp-main.29. URL https://aclanthology.org/2021.emnlp-main.29/. [30] Stefan Feuerriegel, Abdurahman Maarouf, Dominik Bär, Dominique Geissler, Jonas Schweisthal, Nicolas Pröllochs, Claire Robertson, Steve Rathje, Jochen Hartmann, Saif Mohammad, et al. Using natural language processing to analyse text data in behavioural science. Nature Reviews Psychology, 4(2):96111, 2025. 35 [31] Edward Fieldhouse, Jane Green, Geoffrey Evans, Jonathan Mellon, Christopher Prosser, Jack Bailey, James Griffiths, and Stuart Perrett. British election study internet panel waves 1-29. The University of Manchester, Manchester, 2024. doi: 10.5255/UKDA-SN-8202-2. [32] Saadia Gabriel, Skyler Hallinan, Maarten Sap, Pemi Nguyen, Franziska Roesner, Eunsol Choi, and Yejin Choi. Misinfo reaction frames: Reasoning about readers reactions to news headlines. In Smaranda Muresan, Preslav Nakov, and Aline Villavicencio, editors, Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 31083127, Dublin, Ireland, 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.acl-long.222. URL https://aclanthology.org/2022. acl-long.222/. [33] Andrew Gelman and John Carlin. Beyond power calculations: Assessing type (sign) and type (magnitude) errors. Perspectives on Psychological Science, 9(6):641651, 2014. doi: 10.1177/1745691614551642. URL https://doi.org/10.1177/1745691614551642. [34] Andrew Gelman and Eric Loken. The garden of forking paths: Why multiple comparisons can be problem, even when there is no fishing expedition or p-hacking and the research hypothesis was posited ahead of time. Department of Statistics, Columbia University, 348(1-17):3, 2013. [35] Gemma Team, Google DeepMind. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [36] Mor Geva, Yoav Goldberg, and Jonathan Berant. Are we modeling the task or the annotator? an investigation of annotator bias in natural language understanding datasets. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 11611166, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D19-1107. URL https://aclanthology.org/D19-1107/. [37] Fabrizio Gilardi, Meysam Alizadeh, and Maël Kubli. Chatgpt outperforms crowd workers for text-annotation tasks. Proceedings of the National Academy of Sciences, 120(30):e2305016120, 2023. doi: 10.1073/pnas.2305016120. URL https://www.pnas.org/doi/abs/10.1073/pnas. 2305016120. [38] Kristina Gligoric, Tijana Zrnic, Cinoo Lee, Emmanuel Candes, and Dan Jurafsky. Can unconfident LLM annotations be used for confident conclusions? In Luis Chiruzzo, Alan Ritter, and Lu Wang, editors, Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 35143533, Albuquerque, New Mexico, April 2025. Association for Computational Linguistics. ISBN 979-8-89176-189-6. URL https://aclanthology.org/2025. naacl-long.179/. [39] Justin Grimmer and Brandon Stewart. Text as data: The promise and pitfalls of automatic content analysis methods for political texts. Political analysis, 21(3):267297, 2013. 36 [40] Ulrike Groemping. Relative importance for linear regression in r: The package relaimpo. Journal of Statistical Software, 17(1):127, 2006. [41] Zhouhong Gu, Xiaoxuan Zhu, Haoning Ye, Lin Zhang, Jianchen Wang, Yixin Zhu, Sihang Jiang, Zhuozhi Xiong, Zihan Li, Weijie Wu, Qianyu He, Rui Xu, Wenhao Huang, Jingping Liu, Zili Wang, Shusen Wang, Weiguo Zheng, Hongwei Feng, and Yanghua Xiao. Xiezhi: An ever-updating benchmark for holistic domain knowledge evaluation. Proceedings of the AAAI Conference on Artificial Intelligence, 38(16):1809918107, Mar. 2024. doi: 10.1609/aaai. v38i16.29767. URL https://ojs.aaai.org/index.php/AAAI/article/view/29767. [42] Xingwei He, Zhenghao Lin, Yeyun Gong, A-Long Jin, Hang Zhang, Chen Lin, Jian Jiao, Siu Ming Yiu, Nan Duan, and Weizhu Chen. AnnoLLM: Making large language models to be better crowdsourced annotators. In Yi Yang, Aida Davani, Avi Sil, and Anoop Kumar, editors, Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 6: Industry Track), pages 165190, Mexico City, Mexico, June 2024. Association for Computational Linguistics. doi: 10. 18653/v1/2024.naacl-industry.15. URL https://aclanthology.org/2024.naacl-industry. 15/. [43] Megan L. Head, Luke Holman, Rob Lanfear, Andrew T. Kahn, and Michael D. Jennions. The extent and consequences of p-hacking in science. PLOS Biology, 13(3):115, 03 2015. doi: 10.1371/journal.pbio.1002106. URL https://doi.org/10.1371/journal.pbio.1002106. [44] Megan Head, Luke Holman, Rob Lanfear, Andrew Kahn, and Michael Jennions. The extent and consequences of p-hacking in science. PLoS biology, 13(3):e1002106, 2015. [45] Michael Heseltine and Bernhard Clemm von Hohenberg. Large language models as substitute for human experts in annotating political text. Research & Politics, 11(1): 20531680241236239, 2024. doi: 10.1177/20531680241236239. URL https://doi.org/10. 1177/20531680241236239. [46] Christopher Hitchcock and Elliott Sober. Prediction versus accommodation and the risk of overfitting. The British Journal for the Philosophy of Science, 55(1):134, 2004. doi: 10.1093/ bjps/55.1.1. URL https://doi.org/10.1093/bjps/55.1.1. [47] Zak Hussain, Marcel Binz, Rui Mata, and Dirk Wulff. tutorial on open-source large language models for behavioral science. Behavior Research Methods, 56(8):82148237, 2024. [48] John PA Ioannidis. Why most discovered true associations are inflated. Epidemiology, 19(5): 640648, 2008. [49] Bohan Jiang, Zhen Tan, Ayushi Nirmal, and Huan Liu. Disinformation Detection: An Evolving Challenge in the Age of LLMs, pages 427435. 2024. doi: 10.1137/1.9781611978032.50. URL https://epubs.siam.org/doi/abs/10.1137/1.9781611978032.50. [50] Yiqiao Jin, Minje Choi, Gaurav Verma, Jindong Wang, and Srijan Kumar. MM-SOC: Benchmarking multimodal large language models in social media platforms. In LunWei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 61926210, Bangkok, Thailand, August 2024. As37 sociation for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.370. URL https://aclanthology.org/2024.findings-acl.370/. [51] Saurav Kadavath, Tom Conerly, Amanda Askell, Tom Henighan, Dawn Drain, Ethan Perez, Nicholas Schiefer, Zac Hatfield-Dodds, Nova DasSarma, Eli Tran-Johnson, et al. Language models (mostly) know what they know. arXiv preprint arXiv:2207.05221, 2022. [52] Andres Karjus. Machine-assisted quantitizing designs: augmenting humanities and social sciences with artificial intelligence. Humanities and Social Sciences Communications, 12(1):118, 2025. [53] Norbert L. Kerr. Harking: Hypothesizing after the results are known. Personality and Social Psychology Review, 2(3):196217, 1998. doi: 10.1207/s15327957pspr0203_4. URL https://doi.org/10.1207/s15327957pspr0203_4. PMID: 15647155. [54] Nataliia Kholodna, Sahib Julka, Mohammad Khodadadi, Muhammed Nurullah Gumus, and Michael Granitzer. Llms in the loop: Leveraging large language model annotations for active learning in low-resource languages. In Machine Learning and Knowledge Discovery in Databases. Applied Data Science Track, pages 397412, Cham, 2024. Springer Nature Switzerland. ISBN 978-3-031-70381-2. [55] Md Tawkat Islam Khondaker, Abdul Waheed, El Moatez Billah Nagoudi, and Muhammad Abdul-Mageed. GPTAraEval: comprehensive evaluation of ChatGPT on Arabic NLP. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 220247, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.16. URL https://aclanthology.org/2023.emnlp-main.16/. [56] Hannah Kim, Kushan Mitra, Rafael Li Chen, Sajjadur Rahman, and Dan Zhang. MEGAnno+: human-LLM collaborative annotation system. In Nikolaos Aletras and Orphee De Clercq, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics: System Demonstrations, pages 168176, St. Julians, Malta, March 2024. Association for Computational Linguistics. URL https://aclanthology.org/2024.eacldemo.18/. [57] Klaus Krippendorff. Content analysis: An introduction to its methodology. Sage publications, 2018. [58] Jan Ole Krugmann and Jochen Hartmann. Sentiment analysis in the age of generative ai. Customer Needs and Solutions, 11(1):3, 2024. [59] Deepak Kumar, Yousef Anees AbuHashem, and Zakir Durumeric. Watch your language: Investigating content moderation with large language models. Proceedings of the International AAAI Conference on Web and Social Media, 18(1):865878, May 2024. doi: 10.1609/icwsm.v18i1. 31358. URL https://ojs.aaai.org/index.php/ICWSM/article/view/31358. [60] Md Tahmid Rahman Laskar, Saiful Bari, Mizanur Rahman, Md Amran Hossen Bhuiyan, Shafiq Joty, and Jimmy Huang. systematic study and comprehensive evaluation of In Anna Rogers, Jordan Boyd-Graber, and Naoaki ChatGPT on benchmark datasets. 38 Okazaki, editors, Findings of the Association for Computational Linguistics: ACL 2023, pages 431469, Toronto, Canada, July 2023. Association for Computational Linguistics. doi: 10. 18653/v1/2023.findings-acl.29. URL https://aclanthology.org/2023.findings-acl.29/. [61] Md Tahmid Rahman Laskar, Mizanur Rahman, Israt Jahan, Enamul Hoque, and Jimmy Huang. Cqsumdp: chatgpt-annotated resource for query-focused abstractive summarization based on debatepedia. arXiv preprint arXiv:2305.06147, 2023. [62] Michael Laver, Kenneth Benoit, and John Garry. Extracting policy positions from political texts using words as data. American political science review, 97(2):311331, 2003. doi: 10.1017/ S0003055403000698. [63] Pola Lehmann, Simon Franzmann, Denise Al-Gaddooa, Tobias Burst, Christoph Ivanusch, Sven Regel, Felicia Riethmüller, Andrea Volkens, Bernhard Weßels, and Lisa Zehnter. The manifesto data collection. manifesto project (mrg/cmp/marpor). version 2024a, 2024. URL https://doi.org/10.25522/manifesto.mpds.2024a. [64] Lingyao Li, Lizhou Fan, Shubham Atreja, and Libby Hemphill. hot chatgpt: The promise of chatgpt in detecting and discriminating hateful, offensive, and toxic comments on social media. ACM Trans. Web, 18(2), March 2024. ISSN 1559-1131. doi: 10.1145/3643829. URL https://doi.org/10.1145/3643829. [65] Minzhi Li, Taiwei Shi, Caleb Ziems, Min-Yen Kan, Nancy Chen, Zhengyuan Liu, and Diyi Yang. CoAnnotating: Uncertainty-guided work allocation between human and large In Houda Bouamor, Juan Pino, and Kalika Bali, language models for data annotation. editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 14871505, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.92. URL https://aclanthology.org/2023.emnlp-main.92/. [66] Percy Liang, Rishi Bommasani, Tony Lee, Dimitris Tsipras, Dilara Soylu, Michihiro Yasunaga, Yian Zhang, Deepak Narayanan, Yuhuai Wu, Ananya Kumar, Benjamin Newman, Binhang Yuan, Bobby Yan, Ce Zhang, Christian Cosgrove, Christopher Manning, Christopher Re, Diana Acosta-Navas, Drew A. Hudson, Eric Zelikman, Esin Durmus, Faisal Ladhak, Frieda Rong, Hongyu Ren, Huaxiu Yao, Jue WANG, Keshav Santhanam, Laurel Orr, Lucia Zheng, Mert Yuksekgonul, Mirac Suzgun, Nathan Kim, Neel Guha, Niladri S. Chatterji, Omar Khattab, Peter Henderson, Qian Huang, Ryan Andrew Chi, Sang Michael Xie, Shibani Santurkar, Surya Ganguli, Tatsunori Hashimoto, Thomas Icard, Tianyi Zhang, Vishrav Chaudhary, William Wang, Xuechen Li, Yifan Mai, Yuhui Zhang, and Yuta Koreeda. Holistic evaluation of language models. Transactions on Machine Learning Research, 2023. ISSN 28358856. URL https://openreview.net/forum?id=iO4LZibEqW. Featured Certification, Expert Certification, Outstanding Certification. [67] Stephanie Lin, Jacob Hilton, and Owain Evans. Teaching models to express their uncertainty in words. Transactions on Machine Learning Research, 2022. ISSN 2835-8856. URL https: //openreview.net/forum?id=8s8K2UZGTZ. [68] Richard Harold Lindeman, Peter Francis Merenda, Ruth Gold, et al. Introduction to bivariate and multivariate analysis, volume 4. Scott, Foresman Glenview, IL, 1980. 39 [69] Mitchell Linegar, Rafal Kocielnik, and R. Michael Alvarez. Large language models and political science. Frontiers in Political Science, Volume 5 - 2023, 2023. ISSN 2673-3145. doi: 10.3389/fpos.2023.1257092. URL https://www.frontiersin.org/journals/politicalscience/articles/10.3389/fpos.2023.1257092. [70] Yang Liu, Yuanshun Yao, Jean-Francois Ton, Xiaoying Zhang, Ruocheng Guo, Hao Cheng, Yegor Klochkov, Muhammad Faaiz Taufiq, and Hang Li. Trustworthy llms: survey and guideline for evaluating large language models alignment. arXiv preprint arXiv:2308.05374, 2023. [71] Chandreen R. Liyanage, Ravi Gokani, and Vijay Mago. Gpt-4 as an data annotator: Unraveling its performance on stance classification task. PLOS ONE, 19(8):121, 08 2024. doi: 10.1371/journal.pone.0307741. URL https://doi.org/10.1371/journal.pone. 0307741. [72] Llama Team, AI @ Meta. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [73] Matthew Lombard, Jennifer Snyder-Duch, and Cheryl Campanella Bracken. Content analysis in mass communication: Assessment and reporting of intercoder reliability. Human Communication Research, 28(4):587604, 2002. doi: https://doi.org/10.1111/j.1468-2958.2002. tb00826.x. URL https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1468-2958.2002. tb00826.x. [74] Tianxuan Lu, Jin Hu, and Pingping Chen. Benchmarking llama 3 for chinese news June 2024. doi: summation: Accuracy, cultural nuance, and societal value alignment. 10.36227/techrxiv.171742386.68305769/v1. URL http://dx.doi.org/10.36227/techrxiv. 171742386.68305769/v1. [75] Yiwei Luo, Dallas Card, and Dan Jurafsky. Detecting stance in media on global warmIn Trevor Cohn, Yulan He, and Yang Liu, editors, Findings of the Association for ing. Computational Linguistics: EMNLP 2020, pages 32963315, Online, November 2020. Association for Computational Linguistics. doi: 10.18653/v1/2020.findings-emnlp.296. URL https://aclanthology.org/2020.findings-emnlp.296/. [76] Katerina Margatina, Giorgos Vernikos, Loïc Barrault, and Nikolaos Aletras. Active learning by acquiring contrastive examples. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pages 650663, Online and Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.emnlpmain.51. URL https://aclanthology.org/2021.emnlp-main.51/. [77] Paul E. Meehl. Theory-testing in psychology and physics: methodological paradox. Philosophy of Science, 34(2):103115, 1967. doi: 10.1086/288135. [78] Jonathan Mellon, Jack Bailey, Ralph Scott, James Breckwoldt, Marta Miori, and Phillip Schmedeman. Do ais know what the most important issue is? using language models to code open-text social survey responses at scale. Research & Politics, 11(1):20531680241231468, 2024. doi: 10.1177/20531680241231468. URL https://doi.org/10.1177/20531680241231468. 40 [79] Xiao-Li Meng. Statistical paradises and paradoxes in big data (I): Law of large populations, big data paradox, and the 2016 US presidential election. The Annals of Applied Statistics, 12(2):685 726, 2018. doi: 10.1214/18-AOAS1161SF. URL https://doi.org/10.1214/18AOAS1161SF. [80] Gaël Le Mens, Balázs Kovács, Michael T. Hannan, and Guillem Pros. Uncovering the semantics of concepts using gpt-4. Proceedings of the National Academy of Sciences, 120(49): e2309350120, 2023. doi: 10.1073/pnas.2309350120. URL https://www.pnas.org/doi/abs/ 10.1073/pnas.2309350120. [81] Nicolas Merz, Sven Regel, and Jirka Lewandowski. The manifesto corpus: new resource for research on political parties and quantitative text analysis. Research & Politics, 3(2): 2053168016643346, 2016. [82] Mark Mets, Andres Karjus, Indrek Ibrus, and Maximilian Schich. Automated stance detection in complex topics and small languages: The challenging case of immigration in polarizing news media. PLOS ONE, 19(4):116, 04 2024. doi: 10.1371/journal.pone.0302380. URL https://doi.org/10.1371/journal.pone.0302380. [83] Sewon Min, Kalpesh Krishna, Xinxi Lyu, Mike Lewis, Wen-tau Yih, Pang Koh, Mohit Iyyer, Luke Zettlemoyer, and Hannaneh Hajishirzi. FActScore: Fine-grained atomic evaluation of factual precision in long form text generation. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1207612100, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.741. URL https://aclanthology.org/ 2023.emnlp-main.741/. [84] Shyamal Mishra and Preetha Chatterjee. Exploring chatgpt for toxicity detection in github. In Proceedings of the 2024 ACM/IEEE 44th International Conference on Software Engineering: New Ideas and Emerging Results, ICSE-NIER24, page 610, New York, NY, USA, 2024. Association ISBN 9798400705007. doi: 10.1145/3639476.3639777. URL for Computing Machinery. https://doi.org/10.1145/3639476.3639777. [85] Atsushi Mizumoto and Masaki Eguchi. Exploring the potential of using an ai language model for automated essay scoring. Research Methods in Applied Linguistics, 2(2):100050, ISSN 2772-7661. doi: https://doi.org/10.1016/j.rmal.2023.100050. URL https: 2023. //www.sciencedirect.com/science/article/pii/S2772766123000101. [86] Anders Giovanni Møller, Arianna Pera, Jacob Dalsgaard, and Luca Aiello. The parrot dilemma: Human-labeled vs. LLM-augmented data in classification tasks. In Yvette Graham and Matthew Purver, editors, Proceedings of the 18th Conference of the European Chapter of the Association for Computational Linguistics (Volume 2: Short Papers), pages 179192, St. Julians, Malta, March 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.eaclshort.17. URL https://aclanthology.org/2024.eacl-short.17/. [87] Franco Moretti. Distant reading. Verso Books, 2013. [88] Yida Mu, Ben P. Wu, William Thorne, Ambrose Robinson, Nikolaos Aletras, Carolina Scarton, Kalina Bontcheva, and Xingyi Song. Navigating prompt complexity for zero-shot 41 classification: study of large language models in computational social science. In Nicoletta Calzolari, Min-Yen Kan, Veronique Hoste, Alessandro Lenci, Sakriani Sakti, and Nianwen Xue, editors, Proceedings of the 2024 Joint International Conference on Computational Linguistics, Language Resources and Evaluation (LREC-COLING 2024), pages 1207412086, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024.lrec-main.1055/. [89] Jingwei Ni, Minjing Shi, Dominik Stammbach, Mrinmaya Sachan, Elliott Ash, and Markus Leippold. AFaCTA: Assisting the annotation of factual claim detection with reliable LLM annotators. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 18901912, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.104. URL https://aclanthology.org/2024. acl-long.104/. [90] Étienne Ollion, Rubing Shen, Ana Macanovic, and Arnault Chatelain. The dangers of using proprietary llms for research. Nature Machine Intelligence, 6(1):45, 2024. [91] OpenAI. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [92] Joseph T. Ornstein, Elise N. Blasingame, and Jake S. Truscott. How to train your stochastic parrot: large language models for political texts. Political Science Research and Methods, 13(2): 264281, 2025. doi: 10.1017/psrm.2024.64. [93] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In Advances in Neural Information Processing Systems, volume 35, pages 2773027744, 2022. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf. [94] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. In S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, Advances in Neural Information Processing Systems, volume 35, pages 2773027744. Curran Associates, Inc., 2022. URL https://proceedings.neurips.cc/paper_files/paper/ 2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf. [95] Nicholas Pangakis, Samuel Wolken, and Neil Fasching. Automated annotation with generative ai requires validation. arXiv preprint arXiv:2306.00176, 2023. [96] Maja Pavlovic and Massimo Poesio. The effectiveness of LLMs as annotators: comparative overview and empirical analysis of direct representation. In Gavin Abercrombie, Valerio Basile, Davide Bernadi, Shiran Dudy, Simona Frenda, Lucy Havens, and Sara Tonelli, editors, Proceedings of the 3rd Workshop on Perspectivist Approaches to NLP (NLPerspectives) @ LREC-COLING 2024, pages 100110, Torino, Italia, May 2024. ELRA and ICCL. URL https://aclanthology.org/2024.nlperspectives-1.11/. 42 [97] Youri Peskine, Damir Korenˇcic, Ivan Grubisic, Paolo Papotti, Raphael Troncy, and Paolo Rosso. Definitions matter: Guiding GPT for multi-label classification. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 40544063, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.267. URL https://aclanthology.org/ 2023.findings-emnlp.267/. [98] Qwen Team. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2025. [99] Qwen Team. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [100] Steve Rathje, Dan-Mircea Mirea, Ilia Sucholutsky, Raja Marjieh, Claire E. Robertson, and Jay J. Van Bavel. Gpt is an effective tool for multilingual psychological text analysis. Proceedings of the National Academy of Sciences, 121(34):e2308950121, 2024. doi: 10.1073/pnas.2308950121. URL https://www.pnas.org/doi/abs/10.1073/pnas.2308950121. [101] Björn Roß, Michael Rist, Guillermo Carbonell, Benjamin Cabrera, Nils Kurowsky, Michael Wojatzki, and NLP4CMC III: 3rdWorkshop on Natural Language Processing for ComputerMediated Communication 22 September 2016. Measuring the reliability of hate speech annotations: the case of the european refugee crisis. Sep 2016. doi: 10.17185/duepublico/ 42132. URL https://doi.org/10.17185/duepublico/42132. [102] Hannah Rothstein, Alexander Sutton, and Michael Borenstein. Publication bias in meta-analysis. Publication bias in meta-analysis: Prevention, assessment and adjustments, pages 17, 2005. [103] Sarthak Roy, Ashish Harshvardhan, Animesh Mukherjee, and Punyajoy Saha. Probing LLMs for hate speech detection: strengths and vulnerabilities. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 61166128, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findings-emnlp.407. URL https://aclanthology.org/2023.findingsemnlp.407/. [104] Abel Salinas and Fred Morstatter. The butterfly effect of altering prompts: How small changes and jailbreaks affect large language model performance. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Findings of the Association for Computational Linguistics: ACL 2024, pages 46294651, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.findings-acl.275. URL https://aclanthology.org/2024. findings-acl.275/. [105] Melanie Sclar, Yejin Choi, Yulia Tsvetkov, and Alane Suhr. Quantifying language models sensitivity to spurious features in prompt design or: How learned to start worrying about prompt formatting. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=RIu5lyNXjT. [106] Kai Shu, Deepak Mahudeswaran, Suhang Wang, Dongwon Lee, and Huan Liu. Fakenewsnet: data repository with news content, social context, and spatiotemporal information for studying fake news on social media. Big Data, 8(3):171188, 2020. doi: 10.1089/big.2020.0062. URL https://doi.org/10.1089/big.2020.0062. PMID: 32491943. [107] Joseph P. Simmons, Leif D. Nelson, and Uri Simonsohn. False-positive psychology: Undisclosed flexibility in data collection and analysis allows presenting anything as significant. Psychological Science, 22(11):13591366, 2011. doi: 10.1177/0956797611417632. URL https://doi.org/10.1177/0956797611417632. PMID: 22006061. [108] Uri Simonsohn, Leif Nelson, and Joseph Simmons. P-curve: key to the file-drawer. Journal of experimental psychology: General, 143(2):534, 2014. doi: 10.1037/a0033242. [109] Sara Steegen, Francis Tuerlinckx, Andrew Gelman, and Wolf Vanpaemel. Increasing transparency through multiverse analysis. Perspectives on Psychological Science, 11(5):702712, 2016. doi: 10.1177/1745691616658637. URL https://doi.org/10.1177/1745691616658637. PMID: 27694465. [110] Angelika M. Stefan and Felix D. Schönbrodt. Big little lies: compendium and simulation of <i>p</i>-hacking strategies. Royal Society Open Science, 10(2):220346, 2023. doi: 10.1098/ rsos.220346. URL https://royalsocietypublishing.org/doi/abs/10.1098/rsos.220346. [111] Theodore Sterling. Publication decisions and their possible effects on inferences drawn from tests of significanceor vice versa. Journal of the American statistical association, 54(285): 3034, 1959. [112] Petter Törnberg. How to use llms for text analysis. arXiv preprint arXiv:2307.13106, 2023. [113] Petter Törnberg. Best practices for text annotation with large language models. arXiv preprint arXiv:2402.05129, 2024. [114] Petter Törnberg. Large language models outperform expert coders and supervised classifiers at annotating political social media messages. Social Science Computer Review, page 08944393241286471, 2024. [115] Petter Törnberg. Large language models outperform expert coders and supervised classifiers at annotating political social media messages. Social Science Computer Review, 0(0): 08944393241286471, 2024. doi: 10.1177/08944393241286471. URL https://doi.org/10. 1177/08944393241286471. [116] Centre for Longitudinal Studies University of London, Institute of Education. National child development study: Age 11, sweep 2, sample of essays, 1969. SN 5790, 2024. doi: 10.5255/UKDA-SN-5790-2. URL http://doi.org/10.5255/UKDA-SN-5790-2. [117] Michiel van der Meer, Neele Falk, Pradeep K. Murukannaiah, and Enrico Liscio. Annotatorcentric active learning for subjective NLP tasks. In Yaser Al-Onaizan, Mohit Bansal, and Yun-Nung Chen, editors, Proceedings of the 2024 Conference on Empirical Methods in Natural Language Processing, pages 1853718555, Miami, Florida, USA, November 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.emnlp-main.1031. URL https: //aclanthology.org/2024.emnlp-main.1031/. [118] Kamil Wais. Gender Prediction Methods Based on First Names with genderizeR. The Journal, 8(1):1737, 2006. doi: 10.32614/RJ-2016-002. URL https://doi.org/10.32614/RJ2016-002. 44 [119] Mengting Wan, Tara Safavi, Sujay Kumar Jauhar, Yujin Kim, Scott Counts, Jennifer Neville, Siddharth Suri, Chirag Shah, Ryen W. White, Longqi Yang, Reid Andersen, Georg Buscher, Dhruv Joshi, and Nagu Rangan. Tnt-llm: Text mining at scale with large language models. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 24, page 58365847, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704901. doi: 10.1145/3637528.3671647. URL https://doi.org/10.1145/3637528. 3671647. [120] Shuohang Wang, Yang Liu, Yichong Xu, Chenguang Zhu, and Michael Zeng. Want to reduce labeling cost? GPT-3 can help. In Marie-Francine Moens, Xuanjing Huang, Lucia Specia, and Scott Wen-tau Yih, editors, Findings of the Association for Computational Linguistics: EMNLP 2021, pages 41954205, Punta Cana, Dominican Republic, November 2021. Association for Computational Linguistics. doi: 10.18653/v1/2021.findings-emnlp.354. URL https: //aclanthology.org/2021.findings-emnlp.354/. [121] Xinru Wang, Hannah Kim, Sajjadur Rahman, Kushan Mitra, and Zhengjie Miao. Human-llm collaborative annotation through effective verification of llm labels. In Proceedings of the 2024 CHI Conference on Human Factors in Computing Systems, CHI 24, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400703300. doi: 10.1145/3613904.3641960. URL https://doi.org/10.1145/3613904.3641960. [122] Maximilian Weber and Merle Reichardt. Evaluation is all you need. prompting generative large language models for annotation tasks in the social sciences. primer using open models. arXiv preprint arXiv:2401.00284, 2023. [123] Robert Philip Weber. Basic content analysis, volume 49. Sage, 1990. [124] Tharindu Weerasooriya, Sujan Dutta, Tharindu Ranasinghe, Marcos Zampieri, Christopher Homan, and Ashiqur KhudaBukhsh. Vicarious offense and noise audit of offensive speech classifiers: Unifying human and machine disagreement on what is offensive. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pages 1164811668, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.emnlp-main.713. URL https://aclanthology.org/2023.emnlp-main.713/. [125] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, and Quoc Le. Finetuned language models are zero-shot learners. In International Conference on Learning Representations, 2022. URL https://openreview.net/ forum?id=gEZrGCozdqR. [126] Jerry Wei, Chengrun Yang, Xinying Song, Yifeng Lu, Nathan Hu, Jie Huang, Dustin Tran, Daiyi Peng, Ruibo Liu, Da Huang, Cosmo Du, and Quoc V. Le. Long-form factuality in large language models. In A. Globerson, L. Mackey, D. Belgrave, A. Fan, U. Paquet, J. Tomczak, and C. Zhang, editors, Advances in Neural Information Processing Systems, volume 37, pages 8075680827. Curran Associates, Inc., 2024. URL https://proceedings.neurips.cc/paper_ files/paper/2024/file/937ae0e83eb08d2cb8627fe1def8c751-Paper-Conference.pdf. [127] Orion Weller and Kevin Seppi. Humor detection: transformer gets the last laugh. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 45 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 36213625, Hong Kong, China, November 2019. Association for Computational Linguistics. doi: 10.18653/v1/D191372. URL https://aclanthology.org/D19-1372/. [128] Halbert White. reality check for data snooping. Econometrica, 68(5):10971126, 2000. doi: https://doi.org/10.1111/1468-0262.00152. URL https://onlinelibrary.wiley.com/doi/ abs/10.1111/1468-0262.00152. [129] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Rémi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander M. Rush. Transformers: State-ofthe-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations, pages 3845, Online, October 2020. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/2020. emnlp-demos.6. [130] Jiaying Wu, Jiafeng Guo, and Bryan Hooi. Fake news in sheeps clothing: Robust fake news detection against llm-empowered style attacks. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 24, page 33673378, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704901. doi: 10.1145/ 3637528.3671977. URL https://doi.org/10.1145/3637528.3671977. [131] Jiaying Wu, Jiafeng Guo, and Bryan Hooi. Fake news in sheeps clothing: Robust fake news detection against llm-empowered style attacks. In Proceedings of the 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, KDD 24, page 33673378, New York, NY, USA, 2024. Association for Computing Machinery. ISBN 9798400704901. doi: 10.1145/ 3637528.3671977. URL https://doi.org/10.1145/3637528.3671977. [132] Patrick Wu, Jonathan Nagler, Joshua Tucker, and Solomon Messing. Large language models can be used to estimate the latent positions of politicians. arXiv preprint arXiv:2303.12057, 2023. [133] Ruoxi Xu, Yingfei Sun, Mengjie Ren, Shiguang Guo, Ruotong Pan, Hongyu Lin, Le Sun, and Xianpei Han. Ai for social science and social science of ai: survey. Information Processing & Management, 61(3):103665, 2024. ISSN 0306-4573. doi: https://doi.org/10.1016/j.ipm.2024. 103665. URL https://www.sciencedirect.com/science/article/pii/S0306457324000256. [134] Bingsheng Yao, Ishan Jindal, Lucian Popa, Yannis Katsis, Sayan Ghosh, Lihong He, Yuxuan Lu, Shashank Srivastava, Yunyao Li, James Hendler, and Dakuo Wang. Beyond labels: Empowering human annotators with natural language explanations through novel activelearning architecture. In Houda Bouamor, Juan Pino, and Kalika Bali, editors, Findings of the Association for Computational Linguistics: EMNLP 2023, pages 1162911643, Singapore, December 2023. Association for Computational Linguistics. doi: 10.18653/v1/2023.findingsemnlp.778. URL https://aclanthology.org/2023.findings-emnlp.778/. 46 [135] Hao Yu, Zachary Yang, Kellin Pelrine, Jean Francois Godbout, and Reihaneh Rabbany. Open, closed, or small language models for text classification? arXiv preprint arXiv:2308.10092, 2023. [136] Zhisong Zhang, Emma Strubell, and Eduard Hovy. survey of active learning for natural language processing. In Yoav Goldberg, Zornitsa Kozareva, and Yue Zhang, editors, Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 61666190, Abu Dhabi, United Arab Emirates, December 2022. Association for Computational Linguistics. doi: 10.18653/v1/2022.emnlp-main.414. URL https://aclanthology.org/2022.emnlp-main.414/. [137] Ruiqi Zhong, Charlie Snell, Dan Klein, and Jacob Steinhardt. Describing differences between text distributions with natural language. In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba Szepesvari, Gang Niu, and Sivan Sabato, editors, Proceedings of the 39th International Conference on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pages 2709927116. PMLR, 1723 Jul 2022. URL https://proceedings.mlr.press/v162/zhong22a. html. [138] Caleb Ziems, William Held, Omar Shaikh, Jiaao Chen, Zhehao Zhang, and Diyi Yang. Can large language models transform computational social science? Computational Linguistics, 50 (1):237291, March 2024. doi: 10.1162/coli_a_00502. URL https://aclanthology.org/2024. cl-1.8/."
        },
        {
            "title": "A Literature review methodology",
            "content": "We conducted systematic literature review, which included the following four stages. Stage 0: Database search. in CSS research using the following search criteria: In March 2025, we searched five databases for papers on LLM usage Search period: January 1, 2022 March 5, 2025 Databases: Scopus (362 papers), Web of Science (133), Semantic Scholar (17), ACL Anthology (57), Google Scholar (999), manually added (24) Keywords: combinations of annotation terms (human/expert/crowdsourced annotation, text classification) AND LLM terms (GPT, large language model, LLM) AND performance terms (benchmark, outperform) AND domain terms (social/communication/political science, sociology) This initial search yielded 1,592 papers. We removed 102 duplicates and 1,030 papers with fewer than 3 citations per year, resulting in 453 papers for screening. Stage 1: Title and abstract screening. Next, we reviewed the title and abstract of all 453 papers, to select all that are relevant for our research. We used two inclusion criteria: Papers benchmarking LLM performance for social science tasks Papers making recommendations for LLM usage in social science research We excluded non-social science domains (e.g., legal reasoning). This screening identified 103 relevant papers. Stage 2: Data extraction. For each of the 103 relevant papers, we then extracted: Task characteristics (type, data format, language) Models tested and prompting techniques Validation approaches mentioned LLM usage recommendations Ground truth construction Performance metrics reported Stage 3: Dataset identification. From all of the 103 relevant papers, we identified the ones including dataset of CSS data annotation task. We then selected subset of 21 datasets according to the following criteria: Inclusion criterion: Public availability Inclusion criterion: CSS annotation tasks Exclusion criterion: Tasks cannot be formulated as multiple choice question answering task Sampled for: Available metadata for hypothesis testing Sampled for: data and task diversity Sampled for: Wide usage/citation in the field This process informed our selection of 21 datasets  (Table 11)  , covering 37 annotation tasks  (Table 1)  , for the experiments. Database Search Query Papers Found ALL((\"human annotation OR \"expert annotation OR \"crowdsourced annotation OR \"human coder* OR \"human coding OR \"text classification OR \"text categorization OR \"manual"
        },
        {
            "title": "Scopus",
            "content": "annotation) AND (\"gpt OR \"large language model OR \"LLM*) AND (\"benchmark OR \"outperform) AND (\"social science OR \"communication science OR \"sociology OR \"political science)) AND PUBYEAR AFT 2021 (human annotation OR expert annotation OR crowdsourced Web of Science annotation OR human coder OR human coding OR text classification OR text categorization OR manual annotation) AND (gpt OR large language model OR LLM) AND (benchmark OR"
        },
        {
            "title": "Semantic Scholar",
            "content": "outperform) AND (social science OR communication science OR sociology OR political science)"
        },
        {
            "title": "ACL Anthology",
            "content": "annotation* OR \"human coder* OR \"human coding* OR \"text (\"human annotation* OR \"expert annotation* OR \"crowdsourced classification* OR \"text categorization* OR \"manual annotation*) AND (\"gpt OR \"large language model* OR \"LLM*) AND"
        },
        {
            "title": "Google Scholar",
            "content": "(\"benchmark* OR \"outperform*) AND (\"social science* OR Manually added \"communication science* OR \"sociology OR \"political science*) - 362 133 57 999 24 Total: 1,592 Table 6: Exact search query and number of papers found for each database considered in the literature search. Structured results of literature review for in-scope articles This section provides detailed results from our systematic literature review on LLM usage for data annotation in CSS. We analyzed 103 papers to understand current practices, recommendations, and gaps in the field as discussed in Section 3. large fraction of the papers we reviewed recommend using LLMs for automated data annotation without rigorous model validation [37, 132, 58, 74, 126, 41, 19, 137, 9, 45, 82, 64, 61, 97, 21]. Research often suggests that LLMs should be used as data annotators on the grounds of measured model performance for given task, while highlighting the need for rigorous model validation [69, 47, 112, 85, 20, 121, 15, 18, 100, 80, 115, 119, 2, 78]. Most reviewed papers argue that there is [60, 49, 135, 122, 103, 84, 88, 55, 52, 59, 86, 95, 66, 138, 50, 92, 24, 113, 27, 71, 89, 96, 133, 3, 54]. minority of identified research papers argue against the use of LLMs for automated 49 data annotations [70, 17, 130, 16, 4, 124, 13]. Only four of the reviewed papers raise concerns of erroneous LLM annotations affecting downstream statistical conclusions [27, 95, 5, 6, 38]. Notice also that about 73% of studies focused exclusively on English text."
        },
        {
            "title": "C Additional experimental details",
            "content": "C.1 Models We test 18 models: meta-llama/Llama-3.2-1B-Instruct meta-llama/Llama-3.2-3B-Instruct meta-llama/Llama-3.1-8B-Instruct meta-llama/Llama-3.1-70B-Instruct Qwen/Qwen2.5-1.5B-Instruct Qwen/Qwen2.5-3B-Instruct Qwen/Qwen2.5-7B-Instruct Qwen/Qwen2.5-32B-Instruct Qwen/Qwen2.5-72B-Instruct Qwen/Qwen3-1.7B Qwen/Qwen3-4B Qwen/Qwen3-8B Qwen/Qwen3-32B google/gemma-3-1b-it google/gemma-3-4b-it google/gemma-3-27b-it gpt-4o-mini-2024-07-18 gpt-4o-2024-08-06 All 18 models are instruction-tuned model variants. We used the model snapshots gpt-4o-202408-06 and gpt-4o-mini-2024-07-18 that the OpenAI API points to for the model families gpt-4o and gpt-4o-mini at the time we ran our experiments. Open-weight model inference used the HuggingFace Transformers library [129] with full precision (no quantization) to ensure maximum annotation quality. C.2 Dataset groupings Each dataset is split into binary groups using both metadata-based and text-based approaches to generate realistic hypothesis tests. For datasets with original metadata (e.g., author gender, 50 political party affiliation), we create groupings based on these attributes. Additionally, we generate default applicable to any text dataset based on: Keyword presence: Binary splits based on whether text contains specific keywords, selected through: The 3 most frequent words in the dataset (excluding stopwords) The 3 most frequent words per ground truth class (up to 15 classes) Words that maximize the absolute difference in class proportions between groups Text length: Splitting at the median character count (short vs. long texts) Random splits: Multiple random 50/50 splits and unbalanced splits (20/80, 40/60) using different random seeds This automated approach generates diverse, data-driven hypotheses while ensuring that each grouping creates meaningful contrast that could plausibly be of research interest. C.3 Prompt paraphrases Most annotation tasks in this study have been used in prior work to benchmark LLM performance. Where available, we use prompts from prior work. To test sensitivity to prompt formulation, we generated paraphrased versions to ensure at least five prompt variations per task. Paraphrases were generated using OpenAIs o3-2025-04-16 reasoning model with parameters {max_completion_tokens=20000, reasoning_effort=high}. The paraphrasing prompt template used was: Prompt used to instruct o3 model to paraphrase data annotation prompt. Paraphrase the following prompt that will be used for automated data annotation. Key requirements: - Preserve the exact same annotation task - Keep all placeholders (e.g., {text}) unchanged - Ensure the paraphrased prompt guides the model to output ONLY the keys from this {prompt_output_mapping_placeholder} mapping dictionary: - Base your paraphrase on the content in the original prompt without adding new information - Improve clarity and structure while maintaining the original meaning Original prompt to paraphrase: {prompt_placeholder} Provide only the paraphrased prompt. Here, {prompt_output_mapping_placeholder} is filled with the list of valid class labels, and {prompt_placeholder} is filled with the base prompt to be paraphrased. Generated paraphrases were manually reviewed to be appropriate for the task and adjusted only when necessary (e.g., to correctly escape special characters). This systematic paraphrasing approach allowed us to test whether minor variations in prompt formulation affect downstream statistical conclusions. All original prompts, paraphrases, and their compatible output mappings are available in our code repository for full reproducibility. C.4 Low confidence sampling with verbalized confidence scores We elicit verbalized LLM annotation confidence using similar prompts as Gligoric et al. [38], though slightly adapted to match each annotation task. For example, for the tone task, we use the following prompt, where {previous_answer_placeholder} corresponds to the LLMs annotation of {text}: How likely is it that the tone of the following political advertisement is {previous_answer_placeholder}? Output only single number between 0 and 1, without any context or explanation. Political advertisement: {text} Probability: C.5 Active sampling implementation details We use the same prediction model and training parameters as Gligoric et al. [38]: an XGBoost model with 2,000 boosting rounds, step size of 0.001, maximum depth of 3, and squared-error objective. Since we use human annotation budgets of nhuman = [25, 50, 100, 250, 500, 1000] in our experiments, the first 25 burn-in samples (with at least one example of both classes) are used for initialization, and we retrain the XGBoost model after every batch. C.6 DSL implementation details We implement DSL with default parameters of 5-fold cross-fitting and 10 sample splits. When these fail due to insufficient labeled data, we adaptively try alternative combinations: cross-fit values {5, 4, 3, 2, 6, 7, 8, 9, 10} and sample-split values {10, 8, 6, 4, 3, 2, 12, 15} in order until successful estimation is achieved. For low confidence sampling, we set sampling probabilities inversely proportional to the LLMs verbalized confidence scores (mixed with 10% uniform sampling for stability): πi = 0.1 πbase + 0.9 (1 ci), where ci is the LLMs verbalized confidence and πbase = nhuman/n, ensuring higher sampling probability for instances with lower confidence. C.7 CDI implementation details The Confidence-Driven Inference (CDI) method introduced by Gligoric et al. [38] combines human annotations with LLM predictions using an optimized trust parameter λ [0, 1] that minimizes estimator variance while maintaining unbiasedness. To handle numerical instabilities with sparse labeled data, we implement adaptive initialization by attempting λinit {1.0, 0.9, 0.7, 0.4, 0.1, 0.0} in descending order until stable optimization is 52 achieved. In case of quasi-complete separation or extreme sparsity (resulting in negative variance), we fall back to λ = 0 (human annotations only)."
        },
        {
            "title": "D Additional Metrics",
            "content": "The False Discovery Rate quantifies the fraction of false positives among all discoveries: among all cases where LLM annotations detect significant effect, what proportion are incorrect discoveries? False Discovery Rate = tT hHt ϕΦ 1[SGT = 0, SLLM h,ϕ = 1] ϕΦ 1[SLLM h,ϕ = 1] tT hHt (15) The False Nondiscovery Rate quantifies the fraction of false negatives among all non-discoveries: among all cases where LLM annotations fail to detect significant effect, what proportion incorrectly misses true effects? False Nondiscovery Rate = tT hHt ϕΦ 1[SGT = 1, SLLM h,ϕ = 0] ϕΦ 1[SLLM h,ϕ = 0] tT hHt (16) The Type Error Rate captures the fraction of sign errors among true discoveries: among all cases where both ground truth and LLM annotations detect significant effects, what proportion have incorrect signs? Type Error Rate = tT hHt ϕΦ 1[SGT tT hHt = 1, SLLM h,ϕ = 1, sgn(βGT = 1, SLLM h,ϕ = 1] ϕΦ 1[SGT ) = sgn(βLLM h,ϕ )] (17) These metrics provide complementary perspective to the Type risk and Type II risk metrics by conditioning on LLM decisions rather than ground truth outcomes. In line with the false discovery rate framework of Benjamini and Hochberg [7], these rates focus on the reliability of discoveries made by the LLM testing procedure, providing insight into the practical trustworthiness of LLM-based findings."
        },
        {
            "title": "E Additional results",
            "content": "E.1 Complete LLM hacking risk breakdown by task and model Table 7 provides the complete breakdown of LLM hacking risk across all 37 annotation tasks and 18 models evaluated. There are large variations both across tasks (ranging from 5% for humor detection with larger models to over 75% for some relevance classification tasks) and within tasks across different models. E.2 Instruction-following failure rates Table 8 reports the fraction of cases where models produced outputs that could not be mapped to valid annotation categories. These instruction-following failures occur when models generate responses outside the specified format or provide explanatory text instead of categorical labels. Table 7: LLM Hacking Risk by Task and Model 1 - 2 3 . - l 0.39 0.46 0.58 0.58 0.52 0.57 0.46 0.51 0.51 0.60 0.52 0.53 0.57 0.53 0.54 0.51 0.50 0.37 0.51 - 0.35 0.25 0.41 0.56 0.31 0.58 0.60 0.58 0.50 0.52 0.49 0.58 0.50 0.36 0.55 0.57 0.60 3 - 2 3 . - l 0.25 0.54 0.39 0.56 0.46 0.61 0.44 0.46 0.47 0.38 0.46 0.47 0.36 0.50 0.45 0.38 0.35 0.33 0.29 0.58 0.35 0.20 0.37 0.38 0.34 0.55 0.55 0.40 0.52 0.38 0.41 0.42 0.54 0.32 0.53 0.16 0.45 8 - 1 3 . - l 0.18 0.36 0.33 0.54 0.42 0.66 0.30 0.40 0.40 0.36 0.35 0.43 0.48 0.47 0.45 0.41 0.29 0.12 0.19 - 0.39 0.23 0.43 0.17 0.30 0.47 0.43 0.38 0.59 0.30 0.29 0.32 0.45 0.27 0.47 0.11 0.44 0 7 - 1 3 . - l 0.22 0.28 0.30 0.47 0.42 0.62 0.33 0.36 0.38 0.30 0.37 0.31 0.26 0.53 0.30 0.41 0.23 0.04 0.08 0.39 0.35 0.24 0.36 0.07 0.21 0.40 0.35 0.34 0.53 0.22 0.27 0.18 0.48 0.18 0.40 0.07 0.41 5 . 1 - . 2 Q 0.27 0.48 0.39 0.53 0.37 0.66 0.43 0.54 0.48 0.47 0.54 0.53 0.47 0.53 0.49 0.47 0.45 0.13 0.49 0.58 0.38 0.18 0.37 0.42 0.47 0.60 0.55 0.53 0.55 0.52 0.33 0.57 0.51 0.34 0.62 0.26 0.50 3 - . 2 Q 0.28 0.37 0.32 0.53 0.37 0.57 0.42 0.43 0.50 0.40 0.44 0.38 0.41 0.43 0.50 0.38 0.42 0.14 0.30 0.63 0.37 0.20 0.33 0.45 0.39 0.54 0.31 0.46 0.59 0.42 0.45 0.24 0.49 0.32 0.63 0.20 0.39 7 - . 2 Q 0.30 0.35 0.22 0.52 0.47 0.60 0.38 0.40 0.36 0.34 0.39 0.36 0.25 0.50 0.38 0.41 0.28 0.10 0.20 0.44 0.40 0.21 0.42 0.20 0.27 0.48 0.36 0.28 0.57 0.31 0.23 0.31 0.45 0.35 0.45 0.08 0.34 2 3 - . 2 Q 0.26 0.33 0.24 0.48 0.46 0.59 0.30 0.39 0.37 0.32 0.29 0.35 0.32 0.55 0.28 0.35 0.23 0.03 0.04 0.28 0.40 0.25 0.39 0.25 0.23 0.42 0.33 0.31 0.54 0.26 0.12 0.19 0.48 0.17 0.36 0.09 0.39 2 7 - . 2 Q 0.18 0.29 0.22 0.50 0.46 0.56 0.33 0.35 0.39 0.33 0.29 0.31 0.35 0.50 0.27 0.38 0.22 0.15 0.12 0.50 0.35 0.21 0.37 0.24 0.21 0.41 0.27 0.32 0.52 0.28 0.20 0.16 0.54 0.19 0.37 0.07 0.36 7 . - 3 Q 0.35 0.44 0.39 0.59 0.44 0.59 0.40 0.50 0.44 0.39 0.50 0.57 0.43 0.45 0.44 0.41 0.36 0.21 0.42 0.53 0.36 0.21 0.34 0.48 0.34 0.58 0.45 0.52 0.56 0.31 0.39 0.42 0.53 0.31 0.57 0.18 0.47 4 - 3 Q 0.21 0.34 0.30 0.55 0.49 0.65 0.38 0.41 0.42 0.35 0.44 0.33 0.32 0.44 0.42 0.39 0.30 0.10 0.30 0.64 0.36 0.19 0.40 0.26 0.39 0.49 0.32 0.48 0.55 0.42 0.33 0.22 0.54 0.14 0.56 0.12 0.39 8 - 3 Q 0.29 0.32 0.33 0.51 0.46 0.63 0.43 0.39 0.42 0.33 0.36 0.33 0.30 0.37 0.50 0.44 0.30 0.09 0.21 0.54 0.38 0.21 0.38 0.10 0.26 0.45 0.39 0.40 0.64 0.32 - 0.35 0.52 0.28 0.50 0.10 0.48 2 - 3 Q 0.25 0.32 0.31 0.54 0.51 0.63 0.33 0.34 0.38 0.32 0.32 0.36 0.33 0.47 0.32 0.38 0.23 0.16 0.20 0.66 0.34 0.28 0.38 0.22 0.21 0.42 0.36 0.32 0.52 0.30 0.19 0.32 0.51 0.30 0.36 0.07 0.35 - 1 - - e 0.28 0.40 0.46 0.59 - 0.62 0.56 0.57 0.55 0.59 0.61 0.55 0.46 0.59 0.58 0.50 0.60 0.35 0.43 0.66 0.40 0.31 0.43 0.35 0.29 0.65 0.55 0.57 0.59 0.55 0.54 0.47 0.54 0.31 0.56 0.48 0.54 - 4 - 3 - e 0.21 0.43 0.30 0.57 0.28 0.62 0.41 0.38 0.40 0.35 0.44 0.44 0.31 0.49 0.43 0.44 0.26 0.19 0.26 0.50 0.37 0.28 0.44 0.29 0.30 0.45 0.34 0.46 0.67 0.36 0.37 0.24 0.46 0.36 0.59 0.10 0.45 - 7 2 - 3 - e 0.17 0.22 0.33 0.44 0.41 0.60 0.33 0.34 0.42 0.32 0.33 0.38 0.30 0.49 0.31 0.50 0.21 0.05 0.11 0.44 0.35 0.23 0.39 0.26 0.28 0.42 0.40 0.39 0.56 0.32 0.21 0.14 0.48 0.34 0.33 0.07 0. i - 4 - 0.16 0.27 0.17 0.57 0.45 0.59 0.37 0.36 0.38 0.32 0.40 0.33 0.33 0.47 0.36 0.37 0.24 0.07 0.12 0.49 0.36 0.21 0.37 0.10 0.27 0.43 0.38 0.34 0.57 0.28 0.28 0.22 0.48 0.27 0.39 0.08 0.38 4 - 0.18 0.32 0.23 0.49 0.47 0.44 0.26 0.34 0.39 0.31 0.35 0.35 0.36 0.52 0.28 0.40 0.21 0.02 0.07 0.30 0.39 0.25 0.38 0.14 0.30 0.41 0.47 0.27 0.48 0.25 0.25 0.24 0.51 0.26 0.32 0.07 0. Task politeness_wiki politeness_stack stance_climate ideology_news fakenews factuality hatespeech_explicit hatespeech_implicit hatespeech_target emotion essay_living essay_housewife essay_worksocial essay_location essay_domestic essay_narrative issue_survey humor topic ideology_tweets misinfo tone manifestos_issue manifestos_econ_ideology manifestos_social_ideology manifestos_issues_detailed relevance_tweets framesI_tweets framesII_tweets stance_tweets topic_tweets relevance_tweets23 framesI_tweets23 relevance_tweets17 framesII_tweets17 relevance_news framesI_news Higher failure rates occur for smaller models. We exclude model-task-prompt combinations with failure rates exceeding 1% from our analyses to ensure robust statistical comparisons, though this conservative threshold means our reported hacking risks may underestimate the true extent of the problem for models with poor instruction-following capabilities. E.3 LLM hacking predictors Table 9 presents the OLS regression results predicting binary LLM hacking outcomes using Linear Probability Model. While the R2 of 0.154 appears modest, this is typical for binary outcomes where explained variance is inherently constrained. The model achieves 83.0% classification accuracy, demonstrating strong predictive performance. The highly significant F-statistic (F = 2,352.722, < 0.001) confirms that the predictors jointly have significant explanatory power. With over 1.4 million observations, the analysis provides substantial statistical power to detect meaningful relationships between prompt characteristics, model performance, and hacking susceptibility. Table 8: Average fraction of non-responses (NA) by task and model, indicating instruction-following failure rates where models produced outputs that could not be mapped to valid annotation categories. 1 - 2 . - l 3 - 2 . - l 8 - 1 . - l Task 0 politeness_wiki 0.004 politeness_stack 0.019 stance_climate 0.091 ideology_news 0.005 fakenews 0 factuality 0.092 hatespeech_explicit 0.184 hatespeech_implicit 0 hatespeech_target 0 emotion 0.005 essay_living 0 essay_housewife 0 essay_worksocial 0.033 essay_location 0.027 essay_domestic 0.002 essay_narrative 0.062 issue_survey 0 humor 0 topic 0.055 ideology_tweets 0 misinfo 0.036 tone 0.101 manifestos_issue 0.022 manifestos_econ_ideology manifestos_social_ideology 0.028 0.046 manifestos_issues_detailed 0.341 relevance_tweets 0.144 framesI_tweets 0.161 framesII_tweets 0.084 stance_tweets 0.371 topic_tweets 0.041 relevance_tweets23 0.089 framesI_tweets23 0.011 relevance_tweets17 0.096 framesII_tweets17 0.063 relevance_news 0.134 framesI_news 0.014 0.029 0.002 0 0 0 0.102 0.242 0 0 0 0 0 0 0 0 0.019 0.001 0 0.137 0 0.001 0.027 0.002 0.061 0.095 0.002 0 0.003 0.004 0.064 0.002 0.002 0 0.001 0 0.003 0.003 0.007 0 0 0 0 0 0.006 0 0 0 0 0 0.010 0 0 0.191 0 0.002 0.131 0 0.013 0.046 0.038 0.096 0.072 0.286 0 0.022 0.002 0.032 0 0 0 0.008 0 0.003 0 - 1 . 3 - l 0.002 0.003 0 0 0 0.001 0 0.008 0 0 0 0 0 0.002 0 0 0 0 0 0.085 0 0.015 0.035 0.063 0.032 0.011 0.144 0 0.052 0.001 0.012 0 0 0 0.007 0 5 . 1 - 5 . 2 Q 0 0 0 0.001 0 0 0 0.090 0 0 0 0 0 0 0 0 0.106 0 0.072 0.085 0 0 0.010 0.156 0 0.042 0.286 0.119 0.035 0.136 0.155 0.041 0.165 0.076 0.032 0.004 0.057 3 - 5 . 2 Q 7 - 5 . 2 Q 2 3 - 5 . 2 Q 2 - 5 . 2 Q 0 0 0 0 0 0 0 0.009 0 0 0 0 0 0 0.013 0 0.057 0 0.014 0.068 0 0 0.012 0.153 0.049 0.029 0.287 0.206 0.023 0.124 0.047 0.018 0.236 0.003 0.009 0.039 0.241 0 0 0 0.001 0 0 0 0.001 0 0 0.001 0 0 0 0 0 0.027 0 0 0.075 0 0.007 0.028 0.139 0.031 0.012 0.287 0.079 0.044 0.057 0.078 0.014 0.132 0.004 0.016 0.002 0. 0 0 0 0 0.008 0 0 0.003 0 0 0 0 0 0 0 0 0.001 0 0 0.146 0 0.005 0.042 0.159 0.095 0.005 0.286 0.124 0.074 0.064 0.306 0.043 0.144 0.007 0.022 0.065 0.098 0 0 0 0 0 0 0 0.002 0 0 0 0 0 0 0 0 0.013 0 0 0.091 0 0.014 0.010 0.148 0.068 0.009 0.144 0.051 0.082 0.027 0.031 0.013 0.065 0.007 0.035 0 0.025 7 . 1 - 3 Q 0 0 0 0 0 0 0 0.081 0 0 0 0 0 0 0 0 0.093 0 0 0.226 0 0.001 0.019 0.115 0.023 0.056 0.143 0.267 0.003 0.053 0.218 0.026 0.271 0.021 0.051 0.014 0.285 4 - 3 Q - 3 Q 0 0 0.001 0 0 0 0 0 0 0 0 0 0 0.014 0 0 0.004 0 0 0.089 0 0.001 0.004 0.127 0.018 0.017 0.143 0.043 0.008 0.051 0.117 0.024 0.038 0.007 0 0.001 0.052 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.008 0 0 0.115 0 0.001 0.007 0.166 0.023 0.016 0.144 0.033 0.030 0.011 0.074 0.010 0.027 0.001 0.003 0.001 0.003 2 3 - 3 Q 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0.006 0 0 0.078 0 0.002 0.010 0.028 0.005 0.018 0.286 0.066 0.060 0.060 0.109 0.035 0.070 0.029 0.022 0 0.001 - 1 - 3 - e t - 4 - 3 - e 0 0.001 0 0 0.344 0.009 0.001 0.100 0 0 0.002 0.006 0.093 0 0 0.005 0.289 0.319 0.001 0.144 0 0 0.193 0.023 0.062 0.217 0.286 0 0.021 0.018 0.002 0.015 0.004 0.001 0.034 0.001 0 0.003 0.028 0 0 0.001 0 0 0.005 0 0 0 0 0 0 0 0 0.024 0 0 0.057 0.003 0.002 0.049 0.007 0.013 0.008 0.286 0 0.044 0 0.005 0.001 0 0 0.005 0 0. - 7 2 - 3 - e 0.070 0.076 0 0.001 0.001 0 0 0.003 0 0 0 0 0 0 0 0 0.007 0 0 0.101 0 0.011 0.015 0.031 0.016 0.001 0.287 0 0.059 0 0.001 0.002 0 0.002 0.017 0 0.001 m - 4 - 0.001 0 0 0 0 0 0 0.001 0 0 0 0 0 0 0 0 0.001 0 0 0.093 0 0 0.003 0.100 0.017 0.006 0.286 0.006 0.083 0.092 0.062 0.007 0.011 0.002 0.018 0.004 0.015 4 - 6 0.003 0.002 0 0 0 0 0 0.001 0 0 0 0 0 0 0.001 0 0 0 0 0.098 0 0 0.009 0.082 0.030 0.005 0.286 0.001 0.161 0.001 0.008 0.002 0.001 0.001 0.047 0 0 Table 9: OLS Regression Results: LLM hacking risk by prompt type, prompt detail, weighted F1 score with task (T) and model (M) interactions, and significance-related predictors (shot_type)zero_shot (detail_level)short Weighted F1 Score (T)essay_domestic (T)essay_housewife (T)essay_living (T)essay_location (T)essay_narrative (T)essay_worksocial (T)factuality (T)fakenews (T)framesII_tweets (T)framesII_tweets17 (T)framesI_news (T)framesI_tweets Dependent variable: LLM Hacking 0.003 (0.001) 0.002 (0.001) 0.102 (0.017) 0.030 (0.007) 0.059 (0.013) 0.001 (0.007) 0.009 (0.007) 0.063 (0.010) 0.088 (0.011) 0.364 (0.016) 0.264 (0.066) 0.067 (0.011) 0.018 (0.009) 0.302 (0.014) 0.463 (0.014) Continued on next page 55 Table 9 continued from previous page (T)framesI_tweets23 (T)hatespeech_explicit (T)hatespeech_implicit (T)hatespeech_target (T)humor (T)ideology_news (T)ideology_tweets (T)issue_survey (T)manifestos_econ_ideology (T)manifestos_issue (T)manifestos_issues_detailed (T)manifestos_social_ideology (T)misinfo (T)politeness_stack (T)politeness_wiki (T)relevance_news (T)relevance_tweets (T)relevance_tweets17 (T)relevance_tweets23 (T)stance_climate (T)stance_tweets (T)tone (T)topic (T)topic_tweets (M)Qwen2.5-32B (M)Qwen2.5-3B (M)Qwen2.5-72B (M)Qwen2.5-7B (M)Qwen3-1.7B (M)Qwen3-32B (M)Qwen3-4B (M)Qwen3-8B (M)Gemma-3-1b-it (M)Gemma-3-27b-it (M)Gemma-3-4b-it (M)GOT-4o (M)GOT-4o-mini (M)Llama-3.1-70B (M)Llama-3.1-8B (M)Llama-3.2-1B (M)Llama-3.2-3B significant_difference_found normalized_distance_from_significance_treshold f1_score_weighted:(T)essay_domestic f1_score_weighted:(T)essay_housewife f1_score_weighted:(T)essay_living f1_score_weighted:(T)essay_location f1_score_weighted:(T)essay_narrative f1_score_weighted:(T)essay_worksocial f1_score_weighted:(T)factuality f1_score_weighted:(T)fakenews f1_score_weighted:(T)framesII_tweets f1_score_weighted:(T)framesII_tweets17 f1_score_weighted:(T)framesI_news f1_score_weighted:(T)framesI_tweets f1_score_weighted:(T)framesI_tweets23 f1_score_weighted:(T)hatespeech_explicit f1_score_weighted:(T)hatespeech_implicit f1_score_weighted:(T)hatespeech_target f1_score_weighted:(T)humor f1_score_weighted:(T)ideology_news 56 Dependent variable: LLM Hacking 0.080 (0.015) 0.269 (0.014) 0.342 (0.010) 0.076 (0.007) 0.041 (0.027) 0.317 (0.014) 0.478 (0.047) 0.019 (0.005) 1.022 (0.044) 0.422 (0.053) 0.052 (0.005) 0.467 (0.041) 0.0004 (0.073) 0.554 (0.042) 0.876 (0.040) 0.603 (0.028) 0.829 (0.031) 0.431 (0.208) 1.070 (0.047) 0.407 (0.020) 0.383 (0.021) 0.355 (0.045) 0.709 (0.026) 0.299 (0.020) 0.002 (0.007) 0.011 (0.006) 0.003 (0.007) 0.008 (0.006) 0.001 (0.005) 0.003 (0.007) 0.001 (0.005) 0.00001 (0.007) 0.027 (0.005) 0.001 (0.006) 0.005 (0.006) 0.009 (0.007) 0.007 (0.007) 0.00000 (0.007) 0.005 (0.005) 0.039 (0.005) 0.004 (0.005) 0.324 (0.001) 0.220 (0.001) 0.042 (0.014) 0.159 (0.019) 0.009 (0.013) 0.097 (0.015) 0.012 (0.024) 0.044 (0.018) 0.064 (0.035) 0.177 (0.108) 0.062 (0.033) 0.086 (0.024) 0.268 (0.029) 0.576 (0.029) 0.217 (0.031) 0.351 (0.027) 0.646 (0.028) 0.128 (0.016) 0.129 (0.043) 0.135 (0.033) Continued on next page Table 9 continued from previous page f1_score_weighted:(T)ideology_tweets f1_score_weighted:(T)issue_survey f1_score_weighted:(T)manifestos_econ_ideology f1_score_weighted:(T)manifestos_issue f1_score_weighted:(T)manifestos_issues_detailed f1_score_weighted:(T)manifestos_social_ideology f1_score_weighted:(T)misinfo f1_score_weighted:(T)politeness_stack f1_score_weighted:(T)politeness_wiki f1_score_weighted:(T)relevance_news f1_score_weighted:(T)relevance_tweets f1_score_weighted:(T)relevance_tweets17 f1_score_weighted:(T)relevance_tweets23 f1_score_weighted:(T)stance_climate f1_score_weighted:(T)stance_tweets f1_score_weighted:(T)tone f1_score_weighted:(T)topic f1_score_weighted:(T)topic_tweets f1_score_weighted:(M)Qwen2.5-32B f1_score_weighted:(M)Qwen2.5-3B f1_score_weighted:(M)Qwen2.5-72B f1_score_weighted:(M)Qwen2.5-7B f1_score_weighted:(M)Qwen3-1.7B f1_score_weighted:(M)Qwen3-32B f1_score_weighted:(M)Qwen3-4B f1_score_weighted:(M)Qwen3-8B f1_score_weighted:(M)Gemma-3-1b-it f1_score_weighted:(M)Gemma-3-27b-it f1_score_weighted:(M)Gemma-3-4b-it f1_score_weighted:(M)GPT-4o f1_score_weighted:(M)GPT-4o-mini f1_score_weighted:(M)Llama-3.1-70B f1_score_weighted:(M)Llama-3.1-8B f1_score_weighted:(M)Llama-3.2-1B f1_score_weighted:(M)Llama-3.2-3B Constant Observations Accuracy (on binarized model predictions) R2 Adjusted R2 Residual Std. Error Statistic Note: Dependent variable: LLM Hacking 0.328 (0.074) 0.007 (0.011) 1.522 (0.066) 0.517 (0.076) 0.107 (0.013) 0.581 (0.059) 0.033 (0.100) 0.754 (0.070) 1.217 (0.054) 0.780 (0.036) 0.891 (0.044) 0.386 (0.229) 1.388 (0.068) 0.598 (0.037) 0.541 (0.039) 0.523 (0.056) 0.943 (0.038) 0.380 (0.028) 0.044 (0.015) 0.044 (0.016) 0.046 (0.016) 0.024 (0.015) 0.027 (0.016) 0.040 (0.016) 0.041 (0.014) 0.039 (0.016) 0.040 (0.019) 0.048 (0.015) 0.031 (0.015) 0.060 (0.016) 0.056 (0.016) 0.047 (0.015) 0.031 (0.014) 0.041 (0.019) 0.025 (0.015) 0.296 (0.006) 1,429,925 0.830 0.154 0.154 0.352 (df = 1429813) 2,352.722 (df = 111; 1429813) p<0.1; p<0.05; p<0.01 E.4 Correlations between LLM hacking and F1 score by task E.5 LLM annotation reliability Figure 12shows that LLMs are unreliable annotators. Performance varies by >10 percentage points on average when comparing the bestwith the worst-performing prompt. These results are in line with the findings by Sclar et al. [105]. Smaller models show wider distributions of performance differences across prompts, indicating less stable annotation behavior compared to larger models. E.5.1 Using larger, more capable models reduced LLM hacking risk Figure 13 demonstrates negative correlation between LLM hacking risk and general model capability as measured by MMLU benchmark scores. The analysis includes eight models from the 57 Figure 12: Distribution of performance spread, measured as the absolute differences in weighted F1 across multiple prompt variations for each model. The large y-axis values indicate low output consistency across different prompting strategies. Llama and Qwen families with publicly available MMLU scores. While this suggests that model capability improvements can reduce LLM hacking risk, the fact that even the highest-scoring models retain considerable risk (up to 52%) highlights the limitations of relying solely on model scaling. E.6 LLM hacking feasibility by annotation task Expanding the results of Section 5.2, we provide the full overview of LLM hacking feasibility by annotation task in Figure 14. E.7 Mitigating LLM hacking risk Here, we extend the results from Section 5.4, providing complete overview of all mitigation techniques, as well as additional details on model selection strategies. E.8 Sampling strategy comparison While Section 5.4 focuses on the most distinct mitigation strategies, Figure 15 reveals that the choice of sampling strategy (random vs. low confidence vs. active) has minimal impact on overall effectiveness. The differences between M1, M4, and M7 (all using ground truth only) are negligible, as are the differences between M2, M5, and M8 (all using ground truth + LLM). Hence, Figure 13: Negative correlation between LLM hacking risk and general model performance (MMLU scores) for eight Llama and Qwen models with known MMLU scores. sophisticated sampling strategies provide limited benefit over simple random sampling when the number of human annotations is fixed. E.9 Prompt performance variation The best prompt results shown throughout our analysis represent an optimistic scenario where researchers have access to enough ground truth labels to select the optimal prompt. This provides an upper bound on what careful prompt engineering might achieve. In practice, the baseline results (averaged across all reasonable prompts) better represent expected performance when researchers lack extensive validation data. Some tasks show big gap between best-prompt and baseline performance of up to 5 percentage points and more. Hence, prompt sensitivity is itself task characteristic that researchers should consider when considering using LLM annotations. E.10 Model selection Figure 17 reveals the gains from sophisticated model selection (see Figure 11 in Section 5.4): GPT-4o is only the best selected in about 49% of cases. Figure 16 shows that the negative relationship between performance and LLM hacking risk is significant even for performance proxy measured only on the small human annotation sample. 59 Figure 14: LLM hacking and correct conclusion feasibility by annotation task. Each task is labeled with the total number of unique hypotheses tested (in parentheses). Feasibility represents the fraction of hypotheses within each task where at least one model-prompt combination achieved the specified outcome. 60 Table 10: Point-biserial correlations between LLM hacking occurrence and model F1 score weighted across different tasks. Significance levels: * < 0.05, ** < 0.01, *** < 0.001."
        },
        {
            "title": "Task",
            "content": "politeness_wiki politeness_stack stance_climate ideology_news fakenews factuality hatespeech_explicit hatespeech_implicit hatespeech_target emotion essay_living essay_housewife essay_worksocial essay_location essay_domestic essay_narrative issue_survey humor topic ideology_tweets misinfo tone manifestos_issue manifestos_econ_ideology manifestos_social_ideology manifestos_issues_detailed relevance_tweets framesI_tweets framesII_tweets stance_tweets topic_tweets relevance_tweets23 framesI_tweets23 relevance_tweets17 framesII_tweets17 relevance_news framesI_news Correlation p-value 0.000 0.000 0.000 0.000 0.318 0.012 0.000 0.000 0.000 0.000 0.000 0.000 0.173 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.028 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.000 0.238 0.005 0.000 0.000 0.000 -0.292 -0.136 -0.198 -0.063 -0.020 -0.023 -0.175 -0.152 -0.068 -0.137 -0.115 -0.298 -0.017 -0.088 -0.081 -0.082 -0.126 -0.123 -0.399 -0.146 -0.043 -0.108 -0.117 -0.460 -0.228 -0.101 -0.313 -0.193 -0.085 -0.147 -0.257 -0.398 -0.019 -0.076 -0.138 -0.425 -0."
        },
        {
            "title": "Significance N samples",
            "content": "*** *** *** *** * *** *** *** *** *** *** *** *** *** *** *** *** *** * *** *** *** *** *** *** *** *** *** *** *** ** *** *** *** 3,741 3,936 6,525 11,968 2,604 12,015 6,525 18,525 118,170 154,440 62,745 7,565 6,786 35,856 69,872 22,704 751,502 1,914 2,871 1,120 2,670 6,142 1,680 1,680 1,428 445,224 1,786 7,100 10,206 6,674 4,794 1,568 4,059 1,406 21,634 2,185 7,125 Figure 15: Effectiveness of mitigation strategies across models and error types. Each subplot shows different risk metric with models on the y-axis and risk values on the x-axis. Points are vertically offset by the number of ground truth samples, as indicated by the numbers on the right (0, 100, or 1,000 samples). 62 Figure 16: Significant negative correlation between LLM hacking risk and model accuracy derived from small samples of known ground truth data (nhuman {100, 1000}). Each dot and square corresponds to one task-model-nhuman run. Figure 17: Frequency of model selection when using best-performing strategy. In case of ties, models are selected in order of increasing LLM hacking risk, i.e., GPT-4o first, see Table 3. 63 Table 11: Overview of datasets used in experiments Dataset Name Data Type Dataset Description of DataNr. points Nr. of Unique Datapoints Essays written by eleven year old cohort members from the National Child Development Study (NCDS) in 1969 with the following instructions: \"Imagine you are now 25 years old. Write about the life you are leading, your interests, your home life and your work at the age of 25. (You have 30 minutes to do this).\" Additionally, they were given short questionnaire to complete at school about their interests outside school, the school subjects they enjoyed most, and what they thought they were most likely to do when they left secondary school. set of articles from media sources covering the United States of America. Data was scraped from https://www. allsides.com/. Sentences from manifestos from UK parties, 1987 to 2010. The data contains 935 political ads from US Senate Candidates. The data was constructed by Carlson and Montgomery [14] (accessed via Ornstein et al. [92]). Requests from the StackExchange question-answering communities. These are often users commenting on existing posts requesting further information or proposing edits. Requests from Wikipedia editors on user talk pages. Reddit comments from 2005-2019 selected from subreddits with 10k+ comments, filtered to remove deleted, non-English, and toxic content. balanced random sample of congressional bills. They are relating to economy (5k), or three other topics (5k). Filtered set of tweets, retweets, and replies from most prominent U.S hate groups between January 1, 2015 and December 31, 2017. The British Election Study (BES) dataset contains responses from 118,597 respondents to survey questions from 29 Internet Panel waves, conducted between February 2014 and July 2024. The Misinfo Reaction Frames corpus is an dataset of 25k news headlines to articles that have been factchecked. The articles can be about Covid-19, Cancer or Climate Change. random sample of 1,606 articles newspaper articles on content moderation published from January 2020 to April 2021, drawn from dataset of 980k articles collected via LexisNexis. random sample of 1,856 tweets posted by members of the US Congress from 2017 to 2022, drawn from dataset of 20 million tweets. This dataset is similar to the tweets dataset, but for January 2023. It includes random sample of 500 tweets (of which 339 were in English) drawn from dataset of 1.3 million tweets. random sample of 2,382 tweets drawn from dataset of 2.6 million tweets on content moderation posted from January 2020 to April 2021. Opinion spans extracted from global warming news articles, published from Jan. 1, 2000 to April 12, 2020 by various U.S. news sources. Political manifesto sentences from the manifestoproject corpus, 2000 to 2022. LLM prompt-response pairs. The outputs were generated by three different models (InstructGPT [94], Chat3 GPT ), using the prompt \"Tell me bio of <entity>\", where entity corresponds to one of 183 people entities from Wikidata who have corresponding Wikipedia page (randomly sampled over 20 categories based on pageview frequency and nationality). News articles that have been fact-checked by PolitiFact or GossipCop. Tweets made by politicians before elections. 15910 posts from reddit/r/jokes. 4 , and search augmented PerplexityAI 489 489 37554 859 3302 2178 7347 10000 37554 11620 859 3298 2171 10000 21476 562514 101930 25164 1606 1856 411 2383 1996 1856 411 1934 1793 1305323 16040 15225 8366 4100 15910 7954 4099 2040566 1511674 University of London [116] essay Essays Baly et al. [5] Benoit et al. [8] ideology_news News articles manifestos_uk Party Manifestos from the UK Carlson and Montgomery [14] tone Political ads Danescu-Niculescu-Mizil et al. [22] politeness_stack StackExchange requests Danescu-Niculescu-Mizil et al. [22] politeness_wiki Wikipedia requests Demszky et al. [25] emotion Reddit comments Egami et al. [27] topic Congressional bills ElSherief et al. [29] hatespeech Tweets Fieldhouse et al. [31] issue_survey Open ended survey responses Gabriel et al. [32] misinfo News headlines Gilardi et al. [37] news News articles Gilardi et al. [37] tweets17 Tweets Gilardi et al. [37] tweets23 Tweets Gilardi et al. [37] tweets Tweets Luo et al. [75] stance_climate News article spans Merz et al. [81], Lehmann et al. [63] manifestos Political Party Manifestos Min et al. [83] factuality LLM outputs Shu et al. [106], Wu et al. [131] Törnberg [114] Weller and Seppi [127] fakenews News articles ideology_tweets humor Tweets Reddit posts Total 21 datasets 3 4 https://chatgpt.com/ https://www.perplexity.ai/"
        },
        {
            "title": "F Dataset and Task Overview",
            "content": "Here, we provide detailed description of all datasets (ı) and annotation tasks ((cid:30)) used in the experiments. Table 11 concisely lists all datasets. F.1 ı politeness_wiki dataset Data Type: Wikipedia requests Dataset description: Requests from Wikipedia editors on user talk pages. Dataset Links: http://www.cs.cornell.edu/cristian/Politeness_files/Stanford_politeness_ corpus.zip Dataset citation: Danescu-Niculescu-Mizil et al. [22] Dataset notes: Following Gligoric et al. [38], we consider perceived politeness as binary classification task and omit the neutral class. This reduces the number of datapoints from 4338 to 2171. F.1.1 (cid:30) politeness_wiki task Task description: Indicate perceived politeness of wikipedia request. Ground truth annotators: Amazon Mechanical Turk workers (US-based) Annotator agreement notes: Krippendorffs alpha was calculated on the ternary classification task (polite/neutral/impolite) using all 5 annotators scores. F.2 ı politeness_stack dataset Data Type: StackExchange requests Dataset description: Requests from the StackExchange question-answering communities. These are often users commenting on existing posts requesting further information or proposing edits. Dataset Links: http://www.cs.cornell.edu/cristian/Politeness_files/Stanford_politeness_ corpus.zip Dataset citation: Danescu-Niculescu-Mizil et al. [22] Dataset notes: Following Gligoric et al. [38], we consider perceived politeness as binary classification task and omit the neutral class. This reduces the number of datapoints from 6594 to 3298. F.2.1 (cid:30) politeness_stack task Task description: Indicate perceived politeness of StackExchange request. Ground truth annotators: Amazon Mechanical Turk workers (US-based) 65 Annotator agreement notes: Krippendorffs alpha was calculated on the ternary classification task (polite/neutral/impolite) using all 5 annotators scores. F.3 ı stance_climate dataset Data Type: News article spans Dataset description: Opinion spans extracted from global warming news articles, published from Jan. 1, 2000 to April 12, 2020 by various U.S. news sources. Dataset Links: https://github.com/yiweiluo/GWStance Dataset citation: Luo et al. [75] Dataset notes: We only include data where at least 5 out of 8 crowdworkers agree on the stance. F.3.1 (cid:30) stance_climate task Task description: Annotate the stance of sentence in terms of agreeing, disagreeing, or being neutral with respect to the target opinion, \"Climate change/-global warming is serious concern.\" Ground truth annotators: Amazon Mechanical Turk workers (US-based) Annotator agreement notes: Luo et al. [75] report that the average inter-annotator agreement measured as Krippendorffs alpha ranged from 0.54 to 0.64 over the 5 rounds of annotation, we use the average of 0.59. F.4 ı ideology_news dataset Data Type: News articles Dataset description: set of articles from media sources covering the United States of America. Data was scraped from https://www.allsides.com/. Dataset Links: https://github.com/ramybaly/Article-Bias-Prediction Dataset citation: Baly et al. [5] F.4.1 (cid:30) ideology_news task Task description: Labelling news articles according to Left, Right, and Centrist political bias. This task provides an entire news article as context. Ground truth annotators: Ratings provided by AllSides. F.5 ı fakenews dataset Data Type: News articles Dataset description: News articles that have been fact-checked by PolitiFact or GossipCop. 66 Dataset Links: https://github.com/KaiDMML/FakeNewsNet, https://github.com/jiayingwu19/ SheepDog Dataset citation: Shu et al. [106], Wu et al. [131] Dataset notes: The dataset was originally released by Shu et al. [106] with ground truth annotations and metadata. Wu et al. [131] subsequently provided the news article contents. F.5.1 (cid:30) fakenews task Task description: Labelling news articles as fake or real depending on whether they contain fake claims or not. Ground truth annotators: Journalists and domain experts F.6 ı factuality dataset Data Type: LLM outputs Dataset description: LLM prompt-response pairs. The outputs were generated by three 5 different models (InstructGPT [94], ChatGPT ), using the prompt \"Tell me bio of <entity>\", where entity corresponds to one of 183 people entities from Wikidata who have corresponding Wikipedia page (randomly sampled over 20 categories based on pageview frequency and nationality). , and search augmented PerplexityAI 6 Dataset Links: https://drive.google.com/drive/folders/1kFey69z8hGXScln01mVxrOhrqgM62X7I? usp=sharing Dataset citation: Min et al. [83] F.6.1 (cid:30) factuality task Task description: LLM responses that have been manually split into more than 16K individual facts, and each individual fact has been manually labeled as either supported, irrelevant, or not supported. Ground truth annotators: Crowdworkers (fact-checking experts through Upwork) Annotator agreement notes: Min et al. [83] reported an annotator agreement of 91%, measured on 10% of the data (1,604 atomic facts) using two additional freelance annotators. Since the individual freelancer annotations were not available in the released dataset, we estimated Krippendorffs alpha as follows. We generated 10,000 synthetic annotation pairs with 91% agreement by sampling from the observed label distribution in the full dataset (Supported: 63.0%, Not-supported: 27.5%, Irrelevant: 9.4%) and randomly flipping 9% of labels to alternative classes to match the reported disagreement rate. This simulation yielded an estimated Krippendorffs alpha of 0.8329. 5 https://chatgpt.com/ https://www.perplexity.ai/ 67 F.7 ı hatespeech dataset Data Type: Tweets Dataset description: Filtered set of tweets, retweets, and replies from most prominent U.S hate groups between January 1, 2015 and December 31, 2017. Dataset Links: https://www.dropbox.com/s/24meryhqi1oo0xk/implicit-hate-corpus.zip? dl=0 Dataset citation: ElSherief et al. [29] F.7.1 (cid:30) hatespeech_explicit task Task description: Classify tweets as explicit hate speech, implicit hate speech, or not hate speech based on whether they attack people based on protected characteristics. Ground truth annotators: Amazon Mechanical Turk annotators Annotator agreement notes: ElSherief et al. [29] reported majority agreement for 95.3% of tweets, with perfect agreement on 45.6% of the data. Since the individual annotations were not available in the released dataset, we estimated Krippendorffs alpha as follows. We generated 10,000 synthetic annotation matrices where each tweet received 3 annotations following the reported agreement patterns: 45.6% perfect agreement (all annotators agree), 49.7% majority agreement (2 out of 3 agree), and 4.7% no majority (all disagree). Labels were sampled according to the observed distribution in the full dataset. This simulation yielded an estimated Krippendorffs alpha of 0.3084. F.7.2 (cid:30) hatespeech_implicit task Task description: Categorize implicit hate tweets into six specific types: white grievance, incitement, inferiority, irony, stereotypical, or threatening. This task is only done on subset of the dataset, considers tweets that have been categorized as \"implicit hate speech\" in prior annotation stage (hatespeech_explicit task). Ground truth annotators: Expert annotators (trained research assistants) Annotator agreement notes: ElSherief et al. [29] reported Fleiss Kappa of 0.61 and 0.55 for two samples of 150 tweets. Since the individual annotations were not available in the released dataset, we cannot estimate Krippendorffs alpha. F.7.3 (cid:30) hatespeech_target task Task description: Identify the target group of implicit hate tweets from 26 possible demographic and political groups. This task is only done on subset of the dataset, considers tweets that have been categorized as \"implicit hate speech\" in prior annotation stage (hatespeech_explicit task). Ground truth annotators: Expert annotators (trained research assistants) 68 F.8 ı emotion dataset Data Type: Reddit comments Dataset description: Reddit comments from 2005-2019 selected from subreddits with 10k+ comments, filtered to remove deleted, non-English, and toxic content. Dataset Links: https://github.com/google-research/google-research/tree/master/goemotions Dataset citation: Demszky et al. [25] Dataset notes: We only include reddit comments with at least three annotators and where all annotators agree on single label. F.8.1 (cid:30) emotion task Task description: Classify the emotion expressed in Reddit comments using 25 emotion categories plus neutral. Ground truth annotators: Native English speakers from India Annotator agreement notes: The annotator agreement of 0.272 corresponds to the average Krippendorffs alpha across all 26 emotion categories. F.9 ı essay dataset Data Type: Essays Dataset description: Essays written by eleven year old cohort members from the National Child Development Study (NCDS) in 1969 with the following instructions: \"Imagine you are now 25 years old. Write about the life you are leading, your interests, your home life and your work at the age of 25. (You have 30 minutes to do this).\" Additionally, they were given short questionnaire to complete at school about their interests outside school, the school subjects they enjoyed most, and what they thought they were most likely to do when they left secondary school. Dataset Links: https://beta.ukdataservice.ac.uk/datacatalogue/studies/study?id= Dataset citation: University of London [116] F.9.1 (cid:30) essay_living task Task description: Description of the anticipated living situation at age 25. Ground truth annotators: Human experts F.9.2 (cid:30) essay_housewife task Task description: Whether being housewife is mentioned as an occupation or role. Ground truth annotators: Human experts 69 F.9.3 (cid:30) essay_worksocial task Task description: Whether social aspects of work are mentioned. Ground truth annotators: Human experts F.9.4 (cid:30) essay_location task Task description: Geographic location mentioned for living at age 25. Ground truth annotators: Human experts F.9.5 (cid:30) essay_domestic task Task description: Type of domestic labor discussed. Ground truth annotators: Human experts F.9.6 (cid:30) essay_narrative task Task description: Level of narrative structure in the essay. Ground truth annotators: Human experts F.10 ı issue_survey dataset Data Type: Open ended survey responses Dataset description: The British Election Study (BES) dataset contains responses from 118,597 respondents to survey questions from 29 Internet Panel waves, conducted between February 2014 and July 2024. Dataset Links: https://www.britishelectionstudy.com/data-object/british-electionstudy-combined-wave-1-26-internet-panel-open-ended-response-data/, https://www.britishelectionstudy.com/data-object/british-election-study-combinedwave-1-29-internet-panel/ Dataset citation: Fieldhouse et al. [31] Dataset notes: Following the study of Mellon et al. [78], the open text responses to the MII question from wave 26 onwards of the BES of the were coded using GPT-4o and not by BES research associates [31]. In our experiments, we only consider waves 1-25, which were coded by human experts. We also exclude trivial samples where the open text response is equivalent to the coded category. We also exlude MII ground truth code 46, which is used for responses that cannot be coded as they are unintelligible, are excessively vague as to what aspect of politics they are getting at (e.g., \"So many issues, not sure where to start\", \"attitudes\", \"our future\"), do not relate to politics or society in any meaningful way or when it is impossible to choose between multiple possible codes. 70 F.10.1 (cid:30) issue_survey task Task description: In each wave of the British Election Study Internet Panel, respondents are asked the following question, with the option to provide an open text response: \"As far as youre concerned, what is the SINGLE MOST important issue facing the country at the present time?\". The task is to classify the response to this most important issue (MII) question into one of 49 categories. Prompts taken from: [78] Ground truth annotators: Expert annotators (British election study research associates) Annotator agreement notes: Mellon et al. [78] reported an annotator agreement of 86.6%, measured on 1,000 randomly sampled open-text responses annotated by two trained experts. Since the individual expert annotations were not available in the released dataset, we estimated Krippendorffs alpha as follows. We generated 10,000 synthetic annotation pairs with 86.6% agreement by sampling from the observed label distribution in the full dataset and randomly flipping 13.4% of labels to alternative classes to match the reported disagreement rate. This simulation yielded an estimated Krippendorffs alpha of 0.8425. F.11 ı humor dataset Data Type: Reddit posts Dataset description: 15910 posts from reddit/r/jokes. Dataset Links: https://github.com/orionw/RedditHumorDetection Dataset citation: Weller and Seppi [127] Dataset notes: On average, each tweet received 2918.39 votes. F.11.1 (cid:30) humor task Task description: The task is to predict whether the text is funny to lot of people. The ground truth is operationalized as binary variable indicating if the post received more than 200 upvotes [127]. Ground truth annotators: Reddit users. Annotator agreement notes: Since the ground truth is operationalized based on the number of upvotes but the voters are unknown, Krippendorffs Alpha cannot be calculated. F.12 ı topic dataset Data Type: Congressional bills Dataset description: balanced random sample of congressional bills. They are relating to economy (5k), or three other topics (5k). Dataset Links: https://osf.io/gjt87/files/osfstorage Dataset citation: Egami et al. [27] 71 Dataset notes: We only use the random sample of 10K provided by the authors, consisting of one balanced random sample from the full data that they use in the paper (5k texts about economy, 5k about 3 other topics). The full dataset contains 400k bills as is available at https://comparativeagendas.s3.amazonaws.com/datasetfiles/US-Legislativecongressional_bills_19.3_3_2.csv. F.12.1 (cid:30) topic task Task description: Binary topic classification, more precisely, identify whether congressional bill is relating to the economy or not. Ground truth annotators: Trained human annotators F.13 ı ideology_tweets dataset Data Type: Tweets Dataset description: Tweets made by politicians before elections. Dataset Links: https://github.com/cssmodels/llm/tree/main/Data/countries Dataset citation: Törnberg [114] Dataset notes: We use the genderizeR package to predict gender from first names of tweet authors [118]. Sampling: We used all data. F.13.1 (cid:30) ideology_tweets task Task description: Classify tweet authors party affiliation from tweet content. Ground truth annotators: Ground truth values correspond to the actual political party affiliation of the tweets author. Annotator agreement notes: Krippendorfs Alpha is computed based on the annotations of three researchers with PhDs in political science or related disciplines who classified 500 tweets. F.14 ı misinfo dataset Data Type: News headlines Dataset description: The Misinfo Reaction Frames corpus is an dataset of 25k news headlines to articles that have been factchecked. The articles can be about Covid-19, Cancer or Climate Change. Dataset Links: https://github.com/skgabriel/mrf-modeling Dataset citation: Gabriel et al. [32] F.14.1 (cid:30) misinfo task Task description: The annotation task is to predict whether the news article is likely to contain misinformation. Ground truth annotators: Amazon Mechanical Turk workers (US-based) Annotator agreement notes: Gabriel et al. [32] report pairwise agreement of 79% and Cohens Kappa of 0.51 (indicating moderate agreement) for the binary classification of headlines as real news or misinformation. However, since the authors only provide aggregate pairwise agreement statistics rather than the complete annotation matrix or information about which specific annotators labeled which headlines, Krippendorffs Alpha cannot be calculated. F.15 ı tone dataset Data Type: Political ads Dataset description: The data contains 935 political ads from US Senate Candidates. The data was constructed by Carlson and Montgomery [14] (accessed via Ornstein et al. [92]). Dataset Links: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/ DVN/DZZ0OM Dataset citation: Carlson and Montgomery [14] F.15.1 (cid:30) tone task Task description: Classifying the tone of political ad as positive, neutral, or negative. Ground truth annotators: Expert coders F.16 ı manifestos_uk dataset Data Type: Party Manifestos from the UK Dataset description: Sentences from manifestos from UK parties, 1987 to 2010. Dataset Links: https://github.com/kbenoit/CSTA-APSR/, https://dataverse.harvard. edu/file.xhtml?fileId=10595729&version=1.0 Dataset citation: Benoit et al. [8] Dataset notes: We only include data where at least 80 F.16.1 (cid:30) manifestos_issue task Task description: Identify two widely used policy dimensions: \"economic\" policy and \"social\" policy issue dimension. Ground truth annotators: Crowd workers from 26 CrowdFlower channels 73 Annotator agreement notes: Krippendorffs alpha was calculated on the full dataset before filtering, measuring agreement on the economic vs. social policy dimension classification. F.16.2 (cid:30) manifestos_econ_ideology task Task description: Classify sentences as left or right (for sentences on economic policy). Ground truth annotators: Crowd workers from 26 CrowdFlower channels Annotator agreement notes: Krippendorffs alpha was calculated on the full dataset before filtering. F.16.3 (cid:30) manifestos_social_ideology task Task description: Classify sentences as liberal or conservative (for sentences on social policy). Ground truth annotators: Crowd workers from 26 CrowdFlower channels Annotator agreement notes: Krippendorffs alpha was calculated on the full dataset before filtering. F.17 ı manifestos dataset Data Type: Political Party Manifestos Dataset description: Political manifesto sentences from the manifestoproject corpus, 2000 to 2022. Dataset Links: https://manifesto-project.wzb.eu/, https://manifesto-project.wzb. eu/down/data/2024a/datasets/MPDataset_MPDS2024a.csv Dataset citation: Merz et al. [81], Lehmann et al. [63] Dataset notes: We used all data available for parties since 2001, with English translation and sentence level annotations. F.17.1 (cid:30) manifestos_issues_detailed task Task description: The task is to classify quasi-sentences in party manifestos according to the manifesto project coding scheme. There are 46 categories in total (56 in the original task, where categories that differed only in stance have been aggregated into their main issues). Ground truth annotators: Trained country experts (mostly political scientist or political science students and native speakers) Annotator agreement notes: Mean Krippendorffs Alpha calculated from available testresult values in the dataset [63]. The testresult represents Krippendorffs Alpha coder reliability scores from entry tests as given in the coding instructions, comparing individual coder performance to master copies. 74 F.18 ı tweets dataset Data Type: Tweets Dataset description: random sample of 2,382 tweets drawn from dataset of 2.6 million tweets on content moderation posted from January 2020 to April 2021. Dataset Links: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/ DVN/PQYF6M Dataset citation: Gilardi et al. [37] Dataset notes: Gilardi et al. [37] refer to this as the Relevance task for the Tweets (2020 2021) dataset. We employ identical data preprocessing as Gilardi et al. [37], which means that we consider only tweets for which all annotators agree on the ground truth label. Additionally, we filter out duplicates. F.18.1 (cid:30) relevance_tweets task Task description: Binary annotation of whether tweet is about content moderation (relevant/irrelevant). Prompts taken from: [37, 2] Ground truth annotators: Trained research assistants (political science students) F.18.2 (cid:30) framesI_tweets task Task description: General frame detection: whether tweet contains set of two opposing frames (\"problem\" and \"solution\"). The solution frame describes tweets framing content moderation as solution to other issues (e.g., hate speech). The problem frame describes tweets framing content moderation as problem on its own as well as to other issues (e.g., free speech). Prompts taken from: [37, 2] Ground truth annotators: Trained research assistants (political science students) F.18.3 (cid:30) framesII_tweets task Task description: Policy frame detection: whether tweet contains set of fourteen policy frames or other (i.e., Economy, Capacity and resources, Morality, Fairness and Equality, Constitutionality and Jurisprudence, Policy Prescription and Evaluation, Law and Order, Crime and Justice, Security and Defense, Health and Safety, Quality of Life, Cultural Identity, Public Opinion, Political, External Regulation and Reputation, or Other). Prompts taken from: [37, 2] Ground truth annotators: Trained research assistants (political science students) F.18.4 (cid:30) stance_tweets task Task description: Stance detection: whether tweet is in favor of, against, or neutral about repealing Section 230 (a piece of US legislation central to content moderation). Prompts taken from: [37, 2] Ground truth annotators: Trained research assistants (political science students) F.18.5 (cid:30) topic_tweets task Task description: Topic detection: whether tweet is about set of six predefined topics (i.e., Section 230, Trump Ban, Complaint, Platform Policies, Twitter Support, and others). Prompts taken from: [37, 2] Ground truth annotators: Trained research assistants (political science students) F.19 ı tweets23 dataset Data Type: Tweets Dataset description: This dataset is similar to the tweets dataset, but for January 2023. It includes random sample of 500 tweets (of which 339 were in English) drawn from dataset of 1.3 million tweets. Dataset Links: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/ DVN/PQYF6M Dataset citation: Gilardi et al. [37] Dataset notes: Gilardi et al. [37] refer to this as the Relevance task for the Tweets (2023) dataset. We employ identical data preprocessing as Gilardi et al. [37], which means that we consider only tweets for which all annotators agree on the ground truth label. Additionally, we filter out duplicates. F.19.1 (cid:30) relevance_tweets23 task Task description: Binary annotation of whether tweet is about content moderation (relevant/irrelevant). Prompts taken from: [37, 2] Ground truth annotators: Trained research assistants (political science students) F.19.2 (cid:30) framesI_tweets23 task Task description: General frame detection: whether tweet contains set of two opposing frames (\"problem\" and \"solution\"). The solution frame describes tweets framing content moderation as solution to other issues (e.g., hate speech). The problem frame describes tweets framing content moderation as problem on its own as well as to other issues (e.g., free speech). 76 Prompts taken from: [37, 2] Ground truth annotators: Trained research assistants (political science students) F.20 ı tweets17 dataset Data Type: Tweets Dataset description: random sample of 1,856 tweets posted by members of the US Congress from 2017 to 2022, drawn from dataset of 20 million tweets. Dataset Links: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/ DVN/PQYF6M Dataset citation: Gilardi et al. [37] Dataset notes: Gilardi et al. [37] refer to this as the Relevance task for the Tweets (2017 2023) dataset. We employ identical data preprocessing as Gilardi et al. [37], which means that we consider only tweets for which all annotators agree on the ground truth label. Additionally, we filter out duplicates. F.20.1 (cid:30) relevance_tweets17 task Task description: Binary annotation of whether tweet is relevant to political content (relevant/irrelevant). Prompts taken from: [37, 2] Ground truth annotators: Trained research assistants (political science students) F.20.2 (cid:30) framesII_tweets17 task Task description: Policy frame detection: whether tweet contains set of fourteen policy frames or other (i.e., Economy, Capacity and resources, Morality, Fairness and Equality, Constitutionality and Jurisprudence, Policy Prescription and Evaluation, Law and Order, Crime and Justice, Security and Defense, Health and Safety, Quality of Life, Cultural Identity, Public Opinion, Political, External Regulation and Reputation, or Other). Prompts taken from: [37, 2] Ground truth annotators: Trained research assistants (political science students) F.21 ı news dataset Data Type: News articles Dataset description: random sample of 1,606 articles newspaper articles on content moderation published from January 2020 to April 2021, drawn from dataset of 980k articles collected via LexisNexis. Dataset Links: https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/ DVN/PQYF6M 77 Dataset citation: Gilardi et al. [37] Dataset notes: Gilardi et al. [37] refer to this as the Relevance task for the News Articles (20202021) dataset. We employ identical data preprocessing as Gilardi et al. [37], which means that we consider only tweets for which all annotators agree on the ground truth label. Additionally, we filter out duplicates. F.21.1 (cid:30) relevance_news task Task description: Binary annotation of whether news article is about content moderation (relevant/irrelevant). Prompts taken from: [37, 2] Ground truth annotators: Trained research assistants (political science students) F.21.2 (cid:30) framesI_news task Task description: General frame detection: whether news article contains set of two opposing frames (\"problem\" and \"solution\"). The solution frame describes news articles framing content moderation as solution to other issues (e.g., hate speech). The problem frame describes news articles framing content moderation as problem on its own as well as to other issues (e.g., free speech). Prompts taken from: [37, 2] Ground truth annotators: Trained research assistants (political science students)"
        }
    ],
    "affiliations": [
        "Bocconi University",
        "GESIS, Leibniz Institute for the Social Sciences",
        "LIACS, Leiden University",
        "University of Gothenburg",
        "University of Zurich"
    ]
}