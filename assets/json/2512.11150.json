{
    "paper_title": "Causal Judge Evaluation: Calibrated Surrogate Metrics for LLM Systems",
    "authors": [
        "Eddie Landesberg"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "LLM-as-judge evaluation has become the de facto standard for scaling model assessment, but the practice is statistically unsound: uncalibrated scores can invert preferences, naive confidence intervals on uncalibrated scores achieve near-0% coverage, and importance-weighted estimators collapse under limited overlap despite high effective sample size (ESS). We introduce Causal Judge Evaluation (CJE), a framework that fixes all three failures. On n=4,961 Chatbot Arena prompts (after filtering from 5k), CJE achieves 99% pairwise ranking accuracy at full sample size (94% averaged across configurations), matching oracle quality, at 14x lower cost (for ranking 5 policies) by calibrating a 16x cheaper judge on just 5% oracle labels (~250 labels). CJE combines three components: (i) AutoCal-R, reward calibration via mean-preserving isotonic regression; (ii) SIMCal-W, weight stabilization via stacking of S-monotone candidates; and (iii) Oracle-Uncertainty Aware (OUA) inference that propagates calibration uncertainty into confidence intervals. We formalize the Coverage-Limited Efficiency (CLE) diagnostic, which explains why IPS-style estimators fail even when ESS exceeds 90%: the logger rarely visits regions where target policies concentrate. Key findings: SNIPS inverts rankings even with reward calibration (38% pairwise, negative Kendall's tau) due to weight instability; calibrated IPS remains near-random (47%) despite weight stabilization, consistent with CLE; OUA improves coverage from near-0% to ~86% (Direct) and ~96% (stacked-DR), where naive intervals severely under-cover."
        },
        {
            "title": "Start",
            "content": "EVALUATION:"
        },
        {
            "title": "CALIBRATED",
            "content": "5 2 0 2 1 1 ] . s [ 1 0 5 1 1 1 . 2 1 5 2 : r Eddie Landesberg CIMO Labs eddie@cimolabs.com"
        },
        {
            "title": "ABSTRACT",
            "content": "LLM-as-judge evaluation has become the de facto standard for scaling model assessment, but the practice is statistically unsound: uncalibrated scores can invert preferences, naive confidence intervals on uncalibrated scores achieve near-0% coverage, and importance-weighted estimators collapse under limited overlap despite high effective sample size (ESS). We introduce Causal Judge Evaluation (CJE), framework that fixes all three failures. On n=4,961 Chatbot Arena prompts (after filtering from 5k), CJE achieves 99% pairwise ranking accuracy at full sample size (94% averaged across configurations), matching oracle quality, at 14 lower cost (for ranking 5 policies) by calibrating 16 cheaper judge on just 5% oracle labels (250 labels). CJE combines three components: (i) AutoCal-R, reward calibration via mean-preserving isotonic regression; (ii) SIMCal-W, weight stabilization via stacking of S-monotone candidates; and (iii) Oracle-UncertaintyAware (OUA) inference that propagates calibration uncertainty into confidence intervals. We formalize the Coverage-Limited Efficiency (CLE) diagnostic, which explains why IPS-style estimators fail even when ESS exceeds 90%: the logger rarely visits regions where target policies concentrate. Key findings: SNIPS inverts rankings even with reward calibration (38% pairwise, negative Kendalls τ ) due to weight instability; calibrated IPS remains near-random (47%) despite weight stabilization, consistent with CLE; OUA improves coverage from near-0% to 86% (Direct) and 96% (stacked-DR) where naive intervals severely under-cover."
        },
        {
            "title": "INTRODUCTION",
            "content": "Evaluating large language models against expensive oracle labels (human preferences, expert audits, downstream KPIs) is prohibitively costly at scale. The standard workaround is to use cheap LLM judges as proxies: collect judge scores on every sample, reserve oracle labels for small validation slice, and hope the correlation is strong enough. This approach is ubiquitous in practice but statistically unsound. Three systematic failures. Uncalibrated LLM-as-judge evaluation exhibits three failure modes that compound in production: 1. Preference inversion. Judge scores are treated as if they were rewards on the same scale. They are not. Without calibration, higher can predict lower , meaning the optimization target is fundamentally misaligned with the evaluation target. The Youre absolutely right! phenomenon (sycophantic responses scoring high on early metrics but low on actual user value) is preference inversion in production. 2. Invalid confidence intervals. Naive CIs on uncalibrated judge scores achieve near-0% coverage: their 95% CIs capture the true value in 0/50 seeds. Calibration with Oracle-Uncertainty-Aware (OUA) inference dramatically improves coverage. (Figure 5: at 5% oracle fraction, calibration uncertainty contributes over half of total variance.) 3. Catastrophic OPE failure. When evaluating target policies π using logged data from π0, importance-weighted estimators (IPS, SNIPS) fail even after weight stabilization boosts effective sample size (ESS) from degenerate (< 1%) to healthy (> 80%) regimes. The problem is not weight concentration but coverage: the logger rarely visits regions where the target policy concentrates. We formalize this via the Coverage-Limited Efficiency (CLE) diagnostic, which explains why IPS-style estimation hits precision floor under limited overlap. (Table 3: TTC ranges 1949% with CLE factors of 2461.) Related work. Recent work recognizes that LLM judges require calibration (Lee et al., 2025), but existing methods address only the simplest setting: single-model accuracy with binary outcomes. Guerdan et al. (2025) use doubly robust methods to address external validity when LLM judges are prompted with demographic personas, problem complementary to our focus on judge calibration for policy ranking. The harder problem of ranking multiple policies with continuous rewards under distribution shift remains inadequately addressed: no existing method provides calibration, overlap diagnostics, and valid uncertainty quantification in unified framework. Without such solution, practitioners either pay for full oracle labeling or accept invalid comparisons. CJE fills this gap. Causal Judge Evaluation (CJE). We introduce CJE, framework that addresses all three failures. CJE casts offline evaluation as calibrated surrogate estimation with three core components: AutoCal-R (reward calibration): Mean-preserving isotonic regression from judge score to oracle labels , with an automatic two-stage fallback for covariate-dependent bias (e.g., response length). SIMCal-W (Surrogate-Indexed Monotone Calibration for Importance Weights): Variance-optimal stacking of S-monotone, unit-mean weight candidates, stabilizing weights for improved ESS. OUA inference: Delete-one-fold jackknife that propagates calibration uncertainty into confidence intervals, dramatically improving coverage. These components instantiate single design rule, Design-by-Projection (DbP), that encodes justified knowledge as model restrictions. Building on established semiparametric theory, we show that when justified knowledge defines restricted statistical model, the efficiency bound in the restricted model is at most the bound in the baseline model (Theorem 2); with cross-fitting, our estimators attain the surrogate information bound. Beyond variance control, CJE includes mean transport test: for each target policy π or time period, we test whether the mean residual E[Y (S, X)] is zero. This is necessary condition for reusing single calibration function across policies or over time; if it fails, surrogate-only evaluation is systematically biased. In our Arena benchmark, the base-trained calibration transports cleanly to clone, premium, and prompt-engineered policies, but fails for an adversarial unhelpful policy, where mean residuals reveal 0.31 level shift. The Arena benchmark. We validate CJE on large-scale benchmark: 4,961 Chatbot Arena prompts, five LLM policies (including an adversarial unhelpful policy), 13 estimators, and GPT-5 as oracle. Key findings: Calibration works. The Direct Method with two-stage calibration achieves 94% pairwise ranking accuracy on average (vs. 38% for SNIPS). Naive CIs on uncalibrated scores achieve 0% coverage; calibration with OUA improves coverage to 8587% (Direct) and 9596% (stacked-DR). OPE fails unexpectedly. We expected weight stabilization to enable OPE; it did not. Despite SIMCal-W boosting ESS from < 1% to > 80%, calibrated IPS remains near-random (47% pairwise). CLE explains why: high ESS is necessary but not sufficient when the logger rarely visits target-typical regions. DR does not dominate. We expected DR to outperform Direct by combining logged data with fresh draws. Instead, Direct slightly outperforms DR (94.3% vs. 94.1%): under low overlap, DRs IPS component adds noise rather than information. Contributions. 1. Framework: CJE unifies calibration, weight stabilization, and uncertainty quantification for surrogate-based LLM evaluation. 2. Methods: AutoCal-R (mean-preserving calibration), SIMCal-W (weight stabilization for improved ESS), and OUA inference (calibration-aware CIs). 3. Theory: The CLE diagnostic provides novel explanation for why high ESS is insufficient for IPSstyle estimation, formalizing precision floor under limited overlap. The Design-by-Projection framework applies established semiparametric principles to justify CJEs methods. 4. Validation: 13-estimator benchmark on 5k Arena prompts demonstrating 99% ranking accuracy at full sample size (94% averaged across all configurations) with 14 cost reduction (for 5 policies), and near-nominal CI coverage for stacked-DR (9596%). 5. Operations: closed-form Square Root Law for optimal budget allocation (Section F), enabling practitioners to minimize variance subject to cost constraints by balancing evaluation and calibration uncertainty. Readers primarily interested in deployment can focus on Section 3 (pipeline overview), Section 5.3 (six practical rules), and Table 5 (estimator selection guide)."
        },
        {
            "title": "2 BACKGROUND AND SETUP",
            "content": "Setup & notation. We observe i.i.d. logs (Xi, Ai, Si) under fixed logger π0( X); = s(X, A) is scalar judge score on every row, and small i.i.d. oracle slice provides labels . For candidate policy π, the sequence-level importance ratio is Wπ,i = π(Ai Xi) π0(Ai Xi) = exp(cid:8) log pπ(Ai Xi) log pπ0 (Ai Xi)(cid:9), computed via teacher forcing (TF). The target is the counterfactual value (π) = E[Y (π)]. We use the sample-mean-one normalization (SNIPS) when helpful. The LLM evaluation regime. Standard off-policy evaluation (OPE) assumes the target policy cannot be executed. LLM evaluation operates in different regime: generating text is cheap, but grading it with oracle labels (human experts, downstream KPIs) is expensive. This creates three evaluation modes: (i) logs-only (IPS/SNIPS), when we cannot re-run the model; (ii) fresh draws (Direct), when we generate new responses but score them with calibrated surrogate; and (iii) hybrid (DR), combining both. All three share the core challenge: learning calibration from small oracle slice collected under π0, then applying it to evaluate π. The calibration is always off-policy; the actions may or may not be. IPS/SNIPS estimate (π) by reweighting logged outcomes (Horvitz & Thompson, OPE basics. 1952; Hajek, 1964; Li et al., 2011; Swaminathan & Joachims, 2015). The direct method (DM) plugs in g(x) = (cid:80) π(a x) ˆm(x, a). Doubly robust (DR) estimators combine IPS and DM (Bang & inference under the standard Robins, 2005) and, with samplesplitting and crossfitting, admit oneoftwo n1/4 productrate condition (Bickel et al., 1993; van der Vaart & Wellner, 2000; Kosorok, 2008; Jiang & Li, 2016; Chernozhukov et al., 2018; van der Laan & Rose, 2011; Kallus & Uehara, 2020). Teacher forcing (TF) provides sequencelevel propensities/ratios, so these forms apply to sequence policies in principle; however, TF reliability depends on API implementation details (see Limitations). Variance, overlap, and stabilization. limited overlap (Crump et al., 2009). We monitor stability with the effective sample size (ESS), π] and deteriorates under IPS variance scales with E[W 2 ESS(W ) = (cid:1)2 (cid:0) (cid:80) (cid:80) Wi 2 , ESS(W ) = 1 1 + CV2(W ) when = 1 (global meanone/SNIPS). We also track tail behavior via diagnostics (Hill, 1975; Liu, 2001; Owen, 2013). Common stabilizers include truncation/clipping (Ionides, 2008), overlap weighting (Li et al., 2018), balancing objectives (Kallus, 2018), and covariateshift reweighting (Shimodaira, 2000; Sugiyama et al., 2007). The Coverage-Limited Efficiency (CLE) problem. High ESS indicates that weights are not dominated by few extreme observations, but it does not guarantee that the logger has meaningful coverage in regions where the target policy concentrates. Let be target-relevant region with α = Pπ(T ) (target mass) and β = Pπ0 (T ) (logger mass). For any IPS-style estimator based 3 on importance-weighted outcomes, SE( ˆΨIPS) (cid:113) σT α β 1 + χ2(cid:0)π π0,T (cid:1), (1) where σ2 := ess inf (x,a)T Var(Y X=x, A=a) is the minimal outcome noise in , and χ2(π π0,T ) measures shape mismatch inside (proof in Appendix C.12). The diagnostic has three multiplicative factors: (i) the coverage penalty α/ β, which explodes when the logger rarely visits target-typical regions; (ii) the shape mismatch (cid:112)1 + χ2, inflating the floor even with good coverage; and (iii) the standard noise term σT / n. Implication. When β is small (poor logger coverage), the CLE floor is prohibitive regardless of weight stabilization. This explains our empirical finding that calibrated IPS fails despite ESS > 90% (Section 5): high ESS indicates no weight concentration, but low β (poor coverage in target-typical regions) sets hard precision floor that logs-only methods cannot beat. Diagnostics for CLE. The CLE bound decomposes into two factors, each with corresponding diagnostic: Coverage penalty (α/ β): Measured by TTC (Target-Typicality Coverage) = ˆβ, the estimated logger mass on . We define the target-typical region via surprisal threshold: let ℓπ(x, a) = log pπ(a x) be the total negative log-likelihood under the target policy (from teacher forcing), and let ℓπ(x, a) = ℓπ(x, a)/a be the mean per-token surprisal. Set = {(x, a) : ℓπ(x, a) q0.9}, where q0.9 is the 90th percentile of ℓπ under π. Then α = Pπ(T ) = 0.9 by construction, and TTC = ˆβ = ˆPπ0(T ) is the empirical fraction of logged samples falling in . If TTC < 0.7, logs-only IPS will fail, meaning the CLE lower bound implies standard errors too large to distinguish realistic policy differences, even under ideal calibration and weight stabilization; prefer Direct or DR methods. (Threshold calibrated for MDE 0.02 at n5k; see App. D.7.) Applicability: TTC requires sampling from or computing surprisal under π (as we do via teacher forcing). When π-access is unavailable, fall back to conservative rules (e.g., always prefer Direct over IPS) or use proxy distributions to approximate TTC. Shape mismatch ((cid:112)1 + χ2): = (cid:82) (cid:112)pSπ(s) pSπ0(s) ds in the surrogate space. Low affinity (AB < 0.5) indicates severe shape mismatch; the gate threshold (AB 0.85; Table 6) ensures adequate overlap for reliable IPS. by Bhattacharyya affinity AB Measured Both diagnostics are complementary: TTC checks if the logger visits target-typical regions (action space); Bhattacharyya affinity checks overall alignment of judge score distributions across policies (surrogate space). Calibration for OPE. Calibration enforces identities under π0: outcome calibration de-biases g(X); ratio calibration enforces Eπ0 [Wπ] = 1 and Eπ0[Wπh] = Eπ[h] for test class h; orthogonal moments enable honest inference (Chernozhukov et al., 2018). Recent work gives projection-based IPS/DR with stability guarantees (van der Laan et al., 2025; 2024). For DR, IF orthogonality renders small calibration error second order (Bickel et al., 1993; Chernozhukov et al., 2018; van der Laan & Rose, 2011). Shape constraints (isotonic). Isotonic regression is the Euclidean projection onto the cone of monotone functions (PAVA) (Ayer et al., 1955; Barlow et al., 1972); it avoids extrapolation and weakly reduces dispersion by majorization (Hardy et al., 1952; Marshall et al., 2011). CJE exploits this via mean-preserving isotonic projections for both reward calibration and weight stabilization (Section 3). Judges as surrogates. Automatic judges (LLM-as-judge or preference models) provide scalable scoring (Ouyang et al., 2022; Bai et al., 2022; Zheng et al., 2023; Kim et al., 2024; Kocmi & Federmann, 2023) but are correlational and may drift (Dietz et al., 2025). Viewing as surrogate connects to surrogate endpoints and mediation (Prentice, 1989; Robins & Greenland, 1992; Frangakis & Rubin, 2002; Pearl, 2012; VanderWeele, 2015). The surrogate paradox, wherein policy improves and correlates with yet the policy harms , can arise even with high correlation (VanderWeele, 4 Figure 1: CJE pipeline overview. small oracle slice (525%) provides expensive oracle labels to train calibration model (S ). The learned mapping is then applied to bulk evaluation data where oracle labels are unavailable, enabling policy evaluation at fraction of the cost. (In experiments, the oracle is GPT-5; in production this would typically be human raters or downstream KPIs.) 2013); our transport test (Proposition 3) provides necessary check. Under mean sufficiency (E[Y X, A, S] = µ(S)), calibrating = (S) preserves (π) = E[f (Sπ )] and supplies one-dimensional index that stabilizes weights. OUA uncertainty & IF stacking. Treating learned = ˆf (S) as fixed understates uncertainty when the oracle slice is small. CJE addresses this via Oracle-Uncertainty-Aware (OUA) inference, which propagates calibration uncertainty into confidence intervals using delete-one-fold jackknife (Efron & Stein, 1981; Bickel et al., 1993). For ensemble estimation, CJE stacks multiple estimators by minimizing IF covariance over the simplex (Wolpert, 1992; van der Laan et al., 2007). Details in Section 3."
        },
        {
            "title": "3 METHODS",
            "content": "The CJE pipeline has three components that directly address the three failures identified above: reward calibration to prevent preference inversion, weight stabilization to control IPS variance, and uncertainty-aware inference that dramatically improves CI coverage. Figure 1 provides an overview. CJE follows one design rule, Design-by-Projection (DbP), applied to each object in the pipeline: (i) calibrate the reward (projection onto monotone cone), (ii) stabilize ratios (projection onto unit-mean, S-monotone cone), (iii) compute an orthogonalized estimator (projection onto nuisanceorthogonal subspace), and (iv) optionally hedge variance by stacking (projection onto simplex). All learners are cross-fitted; by the Efficiency via Model Restriction theorem, these projections encode justified knowledge that lowers the efficiency bound (see Section 4). 3.1 REWARD CALIBRATION (AUTOCAL-R: ISOTONIC IN WITH AN AUTOMATIC TWO-STAGE FALLBACK) On the oracle slice {(Si, Yi)}, fit mean-preserving calibrator = (Z(S)) with K-fold crossfitting: Figure 2: AutoCal-R: monotone vs. two-stage calibration. Left: standard isotonic regression enforces monotonicity but cannot capture non-monotonic patterns in E[Y S]. Right: two-stage calibration (spline index isotonic) can fit flexible patterns while preserving the mean. AutoCal-R automatically selects the mode via cross-validation. Monotone mode (default). Isotonic regression on S: ˆf arg minf PAVA preserves the slice mean exactly. (cid:80) iO (cid:0)Yi (Si)(cid:1) . Two-stage mode (automatic fallback). Fit smooth index Z(S, X) = g(S, X) (splines+ridge), map to mid-ranks = ECDF{Z(S, X)}, then fit isotonic ˆh(U ). Predictions are = ˆh(ECDF{g(S, X)}). Remark (covariate flexibility). The terminal isotonic step preserves mean-honesty regardless of the first-stage model, so can be any cross-fitted learner (splines, random forests, gradient boosting) and can incorporate covariates beyond S. In our experiments, including response length as first-stage covariate significantly improves ranking accuracy: LLM judges often favor longer responses independent of quality, and conditioning on length removes this confounder from the mapping. Select the mode by OOF RMSE with one-standard-error (1-SE) preference for monotone. Let ROOF denote OOF predictions used along the IF path; the point estimate may use the pooled fit. The terminal isotonic step makes AutoCal-R mean-honest in either mode. We treat ˆf as learned and propagate its uncertainty via OUA inference (Section 3.5). 3.2 WEIGHT CALIBRATION (SIMCAL-W: UNIT-MEAN RATIOS VIA OOF STACKING OF S-MONOTONE CANDIDATES) Let π be the sample-mean-one baseline (SNIPS). For each fold k: 1. Monotone projections (train on Ik). Fit increasing and decreasing isotonic maps on (the latter by running PAVA on S), rescale each to mean one on Ik, and predict OOF candidates on Ik: OOF ; optionally include an identity candidate OOF base 1. , OOF 2. OOF stacking (variance-aware). Define residuals used by the downstream estimator: i=Ri for {base, for IPS and i=Ri ˆm(Xi, Ai) for DR (with ˆm=ˆq below). Let Uc = OOF , } and compute ˆΣcd = cov(Uc, Ud) (tiny ridge if needed). Choose simplex weights ˆβ arg min β3 β ˆΣ β, stack = ˆβc OOF , (cid:88) then renormalize stack to mean one. 3. Light variance guard (optional; ρ=1 by default). Cap variance at absolute threshold ρ: α = min (cid:114) (cid:110) 1, ρ Var(W stack) (cid:111) , blend = 1 + α (W stack 1), ˆWπ = blend/ blend. 6 Figure 3: SIMCal-W weight stabilization across Arena policies (n=4,961 samples with complete logprobs for all four target policies). Raw importance weights (blue dots) span 10130 to 102; Smonotone projection (green line) stabilizes weights while preserving unit-mean. ESS improvements range from 4.6 (clone) to >3000 (parallel universe prompt). The premium policy shows weights spanning 130 orders of magnitude before stabilization. Note: Table 3 reports ablationaveraged ESS across experimental conditions. Each step preserves (or restores) the sample mean. Note: The theoretical guarantee (Lemma 5) uses relative bound Var( ˆW ) ρ Var(W m1); the implementations absolute cap is more conservative when Var(W m1)>1. Remark (transport view). In the continuous case the ideal component is m(s) = pSπ(s)/pSπ0(s); SIMCal-W stacks increasing and decreasing S-monotone candidates to minimize IF variance, yielding approximately monotone weights when one direction dominates. 3.3 ESTIMATORS: CALIBRATED IPS AND DR-CPO Let ˆq(z) E[R Z=z] be the cross-fitted calibrator (in experiments, ˆq = ˆf (k)(Z); we do not fit separate text-conditioned reward model). Define ˆgπ(x) = Eπ[ˆq(Z) X=x]; all nuisances are cross-fitted and OOF predictions are used inside IFs. Calibrated IPS. (cid:98)VIPS(π) = 1 (cid:88) i=1 ˆWπ,i Ri, = ˆWπ,i ROOF ϕIPS (cid:98)VIPS. DR-CPO (Doubly Robust Calibrated Policy Outcomes). (cid:98)VDR(π) = 1 n (cid:88) (cid:110) i=1 ˆgπ(Xi)+ ˆWπ,i (cid:0)Riˆq(Zi)(cid:1)(cid:111) , ϕDR = ˆgπ(Xi)+ ˆWπ,i (cid:0)ROOF ˆqOOF (cid:1) (cid:98)VDR. 3. IF-SPACE STACKING (VARIANCE-OPTIMAL CONVEX ENSEMBLING) For small library of regular estimators (e.g., DR/TMLE/MRDR variants, capped IPS), form the matrix of centered IF columns Φ = [ϕ(e)]eE (computed OOF on the same folds), estimate 7 ˆΣ = 1 ΦΦ + λI, and solve the simplex QP ˆα arg min α α ˆΣ α, (cid:98)Vstack = ˆαe (cid:98)V (e), ϕstack = (cid:88) eE ˆαe ϕ(e). (cid:88) eE"
        },
        {
            "title": "3.5 ORACLE-UNCERTAINTY-AWARE (OUA) INFERENCE",
            "content": "Standard CIs treat the calibration function ˆf as fixed, ignoring first-stage estimation error. When the oracle slice is small, this understates total variance and produces dramatically narrow intervals. In our experiments, naive CIs on uncalibrated scores achieve 0% coverage; calibration with OUA improves coverage to 8596%. OUA variance decomposition. Total variance decomposes into two components: Vartotal( (cid:98)V ) = Vareval( (cid:98)V ˆf ) + Varcal( ˆf ), where Vareval is the standard evaluation variance (estimated via the IF) and Varcal is the variance due to calibration uncertainty. Delete-one-fold jackknife. We estimate Varcal via delete-one-oracle-fold jackknife: 1. Partition the oracle slice into folds. 2. For each fold {1, . . . , K}, refit calibration omitting fold k: ˆf (k). 3. Compute the leave-one-fold-out estimate: (cid:98)V (k) = (cid:98)V ( ˆf (k)). 4. Estimate calibration variance: (cid:100)Varcal = 1 (cid:88) k=1 (cid:0) (cid:98)V (k) (cid:1)2 , = 1 (cid:88) (cid:98)V (k). OUA confidence intervals. (cid:100)Vartotal = (cid:100)Vareval + (cid:100)Varcal, CI95% = (cid:98)V 1.96 (cid:100)Vartotal. (cid:113) The OUA share (cid:100)Varcal/(cid:100)Vartotal ranges from 0% (100% oracle fraction, where no calibration is needed) to over 50% (5% oracle fraction), explaining why ignoring it produces catastrophically narrow CIs. Computational cost. OUA adds refits of AutoCal-R (isotonic regression) and re-evaluations of the estimator. Since both are O(n log n), the overhead is modest relative to the cost of teacher forcing and response generation."
        },
        {
            "title": "4 THEORY: EIF, DESIGN-BY-PROJECTION, AND EFFICIENCY",
            "content": "This section explains why the projections in CJE are not heuristics but information-optimal. We show: (i) using judge as surrogate can only reduce variance, (ii) encoding justified restrictions like monotonicity and mean-one weights tightens efficiency bounds, and (iii) our estimators attain these bounds under standard conditions. Proofs and technical lemmas are deferred to the appendix. Surrogate model and EIF. Let = E[Y S] and m(S) = E[Wπ S]. Under mean sufficiency E[Y X, A, S] = R(S), (π) = E(cid:2)m(S) R(S)(cid:3), π,R(x) = (cid:80) with Theorem 1 (Surrogate EIF and variance reduction). Let ϕuncon be the canonical gradient in the nonparametric model that does not use S. Then ϕsur is the canonical gradient in the surrogate model, and Var(ϕsur) Var(ϕuncon), with strict inequality unless Wπ is σ(S)-measurable and is degenerate. π,R(X) + m(S)(cid:0)R π(a x) R(x, a) = E[R X=x, A=a] and R(X, A)(cid:1) (π), ϕsur(O; π) = R(x, a). In plain terms: If the judge carries any information about the oracle, using it can only help variance and never hurt it. Coarser judges (garblings) are strictly worse. (This variance reduction from conditioning is standard result; we state it to establish the foundation for subsequent results.) 8 Efficiency via Model Restriction. Let L2 0 be the mean-zero Hilbert space with inner product f, = E[f g]. Let be baseline semiparametric model with tangent space (P ) and canonical gradient (EIF) ϕ. When justified knowledge defines restricted model MC (e.g., mean sufficiency in S, monotonicity of conditional expectations), the restricted tangent space TC(P ) (P ) yields new canonical gradient ϕ C. Theorem 2 (Efficiency via Model Restriction). (i) Efficiency improvement. The canonical gradient in the restricted model satisfies ϕ 2, with equality iff TC(P ) = (P ). If TC1 (P ) TC2(P ) 2 (i.e., C2 imposes stronger restrictions), then ϕ 2. (ii) Attainability. One-step/TMLE C2 estimators with cross-fitting attain the efficiency bound Var(ϕ C) in the restricted model MC. 2 ϕ 2 C1 2 ϕ C2 In plain terms: Every justified assumption we encode as model restriction is free variance reduction: we shrink the tangent space without changing the estimand. Remark (prior art). Theorem 2 formalizes well-known principle in semiparametric efficiency theory (Newey, 1990; Bickel et al., 1993); our contribution is naming it and systematically applying it to surrogate-based LLM evaluation via the Design-by-Projection framework. Corollary 1 (Blackwellefficiency monotonicity). If S2 is garbling of S1 (i.e., σ(S2) σ(S1)), then Var(cid:0)ϕsur(S1)(cid:1) Var(cid:0)ϕsur(S2)(cid:1), with strict inequality unless Wπ is already σ(S2)- measurable and is degenerate. Consequences for CJE (i) Conditioning: taking = {f : = E[f S]} recovers Theorem 1. (ii) Mean-one monotone weights: restricting the weight component to {w : E[w] = 1, S} motivates SIMCal-W; the exact IsoMeanOneS projection weakly reduces dispersion in finite samples by majorization. (iii) Stacking: restricting to the convex hull of candidate IF columns gives the variance-optimal convex ensemble. Proposition 1 (Cal-IPS: mean correctness and dispersion control). Let = ˆf (Z(S)) be AutoCal-R π Wπ/ Wn denote the sample- (monotone in or two-stage index; cross-fitted), and let m1 mean-one (SNIPS) ratios, where Wn = n1 (cid:80) Wπ,i. Let ˆWπ be SIMCal-W weights (OOF stack + ˆWπ,iRi (π). Under the exact IsoMeanOneS provariance guard ρ 1). Then (cid:98)VIPS = 1 jection (see App. A.3), Varn( ˆWπ) ρ Varn(W m1 π ); the reference implementation uses simpler normalization that empirically yields similar ESS gains. Theorem 3 (DR-CPO: and cross-fitted nuisances satisfying the one-of-two rate condition ˆq op(n1/2) (e.g., either factor = op(n1/4)). Then limits and efficiency). Assume mean sufficiency, suitable tails/moments, R2 ˆWπ m2 = π ) and ESS( ˆWπ) ESS(W (cid:80) n(cid:0) (cid:98)VDR (π)(cid:1) N(cid:0)0, Var(ϕsur)(cid:1), i.e., DR-CPO attains the surrogate efficiency bound. (Efficiency attainment under the product-rate condition is standard property of DR estimators (Bang & Robins, 2005; Chernozhukov et al., 2018); our contribution is verifying that the CJE construction satisfies these conditions.) Budgeted bound (variance cap). For ρ 1, define Wρ = : E[m] = 1, S, E(cid:2)(m 1)2(cid:3) ρ E(cid:2)(m 1)2(cid:3)(cid:111) (cid:110) , ρ(S)(cid:0)R R(X, A)(cid:1) (π). ρ be the L2(PS) projection of onto Wρ. Define the budgeted gradient ϕ(ρ) = and let m Theorem 4 (Budgeted information bound). The optimal asymptotic variance under the cap ρ equals Var(ϕ(ρ)), which is nonincreasing in ρ and satisfies limρ Var(ϕ(ρ)) = Var(ϕsur). If SIMCal-W converges to ρ (with the guard), then DR-CPO attains Var(ϕ(ρ)). π,R(X)+ In plain terms: This formalizes dont let weights explode. Limiting weight variance (ρ) trades bit of asymptotic efficiency for much better finite-sample behavior. Theorem 5 (IF-space stacking). Let { (cid:98)V (e)}eE be regular, asymptotically linear estimators with centered IFs {ϕ(e)}, and let ˆα arg minα α ˆΣ α with ˆΣ the empirical IF covariance. If ˆΣ Σ uniformly on , then the stacked estimator (cid:98)V ( ˆα) is asymptotically linear with IF ϕ(α) and variance 9 minα αΣ α mine Σee. An outer split leaves the limit unchanged. (This extends ensemble learning from prediction, where Super Learner (van der Laan et al., 2007) combines algorithms to minimize cross-validated risk, to asymptotic efficiency: we combine estimators by minimizing the variance of their combined influence function.) Corollary 2 (Caratheodory sparsity). If Φ = [ϕ(e)] has empirical rank r, the variance-optimal convex combination uses at most r+1 base estimators. Proposition 2 (OUA jackknife variance estimation). Let (cid:98)V ( ˆf ) be any CJE estimator that uses = ˆf (S) (cross-fitted along the IF path). Under L2(PS)-consistency of ˆf and non-overlapping folds, the delete-one-oracle-fold jackknife provides variance estimate for the calibration-induced component, so that (cid:100)Vartotal = (cid:100)Varmain + (cid:100)Varcal estimates total variance. Theoretical consistency requires regularity conditions on the functional (cid:55) (cid:98)V (f ); isotonic regressions non-smoothness complicates formal guarantees. Empirically, we verify valid coverage across 50 seeds (Section 5). := Eπ[Y ] and sur π Policy-wise mean transport test. Let (S, X) be calibration function learned on the oracle slice under the logging policy π0. For target policy π, define the oracle and surrogate values oracle π π = Eπ[επ]. Therefore, the null Proposition 3 (Mean transport equivalence). oracle π hypothesis H0,π : Eπ[επ] = 0 is equivalent to mean-unbiasedness of the surrogate for policy π: H0,π oracle := Eπ[f (S, X)], and the residual επ := (S, X). sur . = sur π π Remark. The mean residual decomposes into transportability gap (failure of S2: Eπ[µπ(S, X) µπ0(S, X)]) and calibration estimation error (Eπ[µπ0(S, X) (S, X)]). The latter vanishes as the oracle slice grows, so the test asymptotically isolates transportability failures. This test is strictly weaker than full transportability (S2): it guarantees mean-unbiasedness for policy-wise values but does not rule out conditional miscalibration within subgroups or at tails. Data requirement. Applying this test requires small oracle slice under each target policy π (or per evaluation environment, e.g., time window or subgroup). We recommend collecting modest audit sample (50200 labels per policy) to validate that the base-trained calibrator can be safely reused; if policy fails the test, either recalibrate with policy-specific data or fall back to oracle-only evaluation for that cell. Discussion. Theorem 2 formalizes Design-by-Projection: when justified knowledge defines restricted statistical model (smaller tangent space), the efficiency bound in the restricted model is at most the bound in the baseline model. The key instantiations are: Theorem 1 (conditioning on S), Theorem 4 (budgeted monotone weights), and Theorem 5 (convex hull of candidate IFs). AutoCal-R, SIMCal-W, and DR/TMLE with cross-fitting attain the corresponding bounds in their respective restricted models. In finite samples, the exact IsoMeanOneS projection additionally majorizes dispersion (Lemma 2), explaining the ESS gains delivered by SIMCal-W. (i) Mean sufficiency generalization. Theorems 1 and 3 assume mean Scope and limitations. sufficiency in S: E[Y X, A, S] = R(S). AutoCal-Rs two-stage mode uses learned index Z(S, X); the theory generalizes by replacing with Z(S, X) throughout, provided is consistently estimated. (ii) Double robustness and weight misspecification. Theorem 3 assumes at least one nuisance converges at rate n1/4. When teacher-forced weights are unreliable (as in high-overlapfailure regimes), DR remains consistent via the outcome model alone; however, the efficiency bound Var(ϕsur) is attained only when both nuisances are well-specified. In practice, when IPS fails due to poor coverage, DR effectively reduces to the Direct Method, which achieves the same consistency without the variance penalty from unstable weights (see Section 5, Q3). (iii) SIMCal-W convergence. Theorem 4 assumes SIMCal-W converges to ρ; this holds under standard regularity conditions for isotonic regression and cross-validated stacking (see, e.g., van der Laan et al. (2007))."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "We evaluate CJE on large-scale benchmark. We initially expected doubly robust methods to dominate by combining logged data with fresh draws. Instead, we found two surprises: (i) OPE fails catastrophically even after weight stabilization boosts ESS above 90%, and (ii) DR provides no 10 advantage over Direct under low overlap, effectively reducing to the outcome model alone. These findings motivated the CLE diagnostic, which explains why high ESS is insufficient. Our experiments address three questions: 1. Does calibration work? AutoCal-R should reduce RMSE, improve ranking accuracy, and with OUA dramatically improve CI coverage. 2. Why does OPE fail despite high ESS? We expected IPS to work after stabilization; CLE predicts failure when TTC is low. 3. Why doesnt DR dominate? Under low overlap, DRs IPS component contributes noise rather than information."
        },
        {
            "title": "5.1 EXPERIMENTAL DESIGN",
            "content": "Data. We started with 5,000 prompts from Chatbot Arena (Zheng et al., 2023), crowdsourced platform where users chat with anonymous LLMs. Prompts were randomly selected from first-turn English conversations; after filtering for teacher-forcing reliability, n=4,961 samples remain with complete data for all policies (see Limitations, Section 6). Policies. Five LLM configurations, each generating responses to all prompts: base: Llama 3.3 70B with standard system prompt (logging policy π0) clone: Same model and prompt as base, different seed (TF reliability test) premium: Llama 3.1 405B (model size effect) parallel universe prompt: Llama 3.3 70B with modified system prompt (prompt engineering test) unhelpful: Deliberately low-quality, confusing responses (adversarial stress test) Total: 25k responses (5k prompts 5 policies; n=4,961 after filtering). Oracle and judge. Oracle : GPT-5 (OpenAI, 2025) quality scores (0100, normalized to [0, 1]) collected for every response. Judge S: GPT-4.1-nano scores on all responses (temperature 0); approximately 16 cheaper than oracle. Note: GPT-5 operates at temperature 1.0 due to API constraints, introducing stochasticity in oracle labels; we did not repeat-score responses, so coverage results may be conservative. All estimators except naive-direct calibrate judge scores using AutoCal-R. Base variants use monotone-only calibration (Z=S); +cov variants use twostage calibration with response length as covariate (Z=g(S, length)). For DR methods, rewards = ˆfall(Z) use the full calibrator while the outcome model ˆq = ˆf (k)(Z) uses cross-fitted (leaveone-fold-out) predictions; the difference ˆq captures residual information for the IPS correction. Experimental grid. We cross 13 estimators 5 oracle fractions (5%, 10%, 25%, 50%, 100%) 5 sample sizes (500, 1k, 2k, 3k, 5k) 50 random seeds = 16,250 total runs (1,250 per estimator). Reported metrics are means across seeds within each (oracle fraction, sample size) cell. 2 (cid:1)1 (cid:80) Metrics. Ranking: Pairwise accuracy (PairAcc = (cid:0)P ) > 0}, the fraction of correctly ordered policy pairs), Top-1 accuracy, Kendalls τ . Magnitude: RMSEd max(0, MSE (cid:100)Var( )), where (cid:100)Var( ) is the variance of (oracle-noise-debiased): RMSEd = the oracle mean estimate (conservatively bounded by 0.25/n per policy). This isolates estimator error from irreducible oracle sampling noise. Uncertainty: Coverage (95% CI capture rate). Note: Accuracy and uncertainty metrics exclude the unhelpful policy (which intentionally violates transportability); ranking metrics include all five policies to test whether estimators correctly identify the adversarial policy as worst. j<k 1{( ˆVj ˆVk)(V (cid:113) 5.2 RESULTS Q1: Does calibration work? Yes, for both accuracy and inference. Table 1 shows calibration improves accuracy across method families. For Direct: uncalibrated naive-direct achieves 11 Table 1: Estimator comparison across all regimes. Metrics averaged over 5 sample sizes 5 oracle fractions 50 seeds. RMSEd = oracle-noise-debiased; CI coverage target = 95%. RMSEd Coverage % SE Pairwise % Top-1 % Estimator τ Time (s) Direct Methods (on-policy, no importance weights) naive-direct direct direct+cov 0.083 0.023 0.026 0.0 0.004 85.7 0.007 87.0 0.008 IPS Methods (off-policy via importance weighting) SNIPS SNIPS+cov calibrated-ips calibrated-ips+cov 0.160 0.184 0.025 0.026 98.3 0.182 97.8 0.184 99.1 0.096 99.2 0.103 DR Methods (doubly robust, combines IPS + outcome model) dr-cpo dr-cpo+cov calibrated-dr-cpo calibrated-dr-cpo+cov stacked-dr stacked-dr+cov 0.046 0.057 0.023 0.026 0.023 0. 99.2 0.037 99.4 0.045 99.5 0.023 99.3 0.024 95.5 0.013 96.1 0.013 90.9 91.8 94.3 38.3 37.4 47.1 46.5 78.4 79.1 91.0 94.1 91.9 92.1 79.6 84.1 89.4 0.82 0.84 0. 8.7 -0.24 6.4 -0.25 19.1 -0.06 20.6 -0.07 46.3 50.1 81.0 87.5 83.1 84.7 0.57 0.58 0.82 0.88 0.84 0.84 0.5 0.9 2.9 0.3 0.5 0.5 0.7 6.5 20.3 6.6 20.2 14.6 62. : lower is better; : higher is better. Bold: best; underline: second-best. Coverage: closest to 95% is best. Naming: All estimators except naive-direct use reward calibration (AutoCal-R). The calibratedprefix adds weight stabilization (SIMCal-W). The +cov suffix uses two-stage calibration with response length. 90.9% pairwise vs. calibrated direct+cov at 94.3%. For DR: raw-weight dr-cpo achieves 78.4% pairwise (τ =0.57) vs. weight-stabilized calibrated-dr-cpo at 91.0% (τ =0.82), 12.6 percentage point gain (both use AutoCal-R; the gain is from SIMCal-W). For inference: uncalibrated CIs (naive-direct) achieve 0% coveragenominal 95% CIs capture the true value in 0/50 seeds. Calibration with OUA dramatically improves coverage: Direct achieves 8587%, stacked-DR achieves near-nominal 9596%. The OUA share of total variance ranges from 0% (100% oracle) to over 50% (5% oracle), explaining why ignoring calibration uncertainty produces catastrophically narrow CIs (Figure 5). Q2: Why does OPE fail despite high ESS? This was our main surprise. Despite SIMCal-W boosting ESS from < 1% to > 80% for target policies, calibrated IPS ranking performance remains near-random: 47% pairwise accuracy vs. 50% chance baseline. We expected weight stabilization to be sufficient; it was not. CLE explains why: TTC ranges from 0.19 to 0.49 for non-clone policies  (Table 3)  , all below the 0.7 threshold, indicating the logger rarely visits target-typical regions. The resulting CLE factors (2461) set hard precision floor that logs-only methods cannot beat regardless of ESS. Q3: Why doesnt DR dominate? We expected DR to outperform Direct by combining logged data with fresh draws. Instead, direct+cov slightly outperforms calibrated-dr-cpo+cov (94.3% vs. 94.1%). The explanation: under low overlap, DRs IPS component contributes noise rather than information. DR estimators are consistent if either the propensity model or the outcome model is correct. Given IPSs failure, DR effectively relies entirely on the outcome model ˆq(Z), not the propensities. This is exactly the regime CLE identifies: TTC is low, so logged data contributes little; DR Direct + extra variance from unstable weights. Implication: When teacher forcing is unreliable, prefer Direct over DR; it achieves the same accuracy without requiring logprobs. Component ablation. Table 2 isolates the marginal contribution of each CJE component. AutoCalR (reward calibration) reduces RMSE by 72% and is necessary to avoid catastrophic undercoverage: without it, 0% of CIs contain the true value; with it, coverage improves to 86%. SIMCal-W (weight stabilization) reduces IPS RMSE by 84% while maintaining coverage, validating the S-monotone projection approach. For ranking accuracy, we recommend direct+cov (Direct + AutoCal-R with response-length covariate); for CI-critical applications, stacked-DR provides near-nominal coverage. Table 2: Ablation: marginal contribution of CJE components. Metrics averaged over 5 sample sizes 5 oracle fractions 50 seeds. CI coverage = % of 95% CIs containing truth. Configuration RMSEd Coverage RMSEd Effect of AutoCal-R (Reward Calibration) Direct, no calibration Direct + AutoCal-R 0.083 0.023 Effect of SIMCal-W (Weight Stabilization) SNIPS (raw weights) Calibrated-IPS (+ SIMCal) 0.160 0.025 Combined Effect on DR DR-CPO, raw weights DR-CPO + SIMCal DR-CPO + SIMCal + cov 0.046 0.023 0.026 0% 86% 98% 99% 99% 100% 99% 72% 84% 50% 43% Recommended Configuration Direct + AutoCal-R + cov 0.026 87% 69% Key findings: AutoCal-R reduces RMSE by 72% and restores coverage (0%86%). SIMCal-W reduces IPS RMSE by 84%. Both contribute additively. Table 3: Weight stabilization effect and CLE diagnostics by policy. SIMCal-W dramatically improves ESS while TTC reveals why IPS still fails: the logger rarely visits target-typical regions. ESS (%) Policy SNIPS SIMCal Uplift CLE Diagnostics TTC ( ˆβ) CLE Factor Clone Parallel Universe Premium Unhelpful 26.2 0.6 0.7 0.4 99.0 95.4 82.1 84.6 3.8 79.6% 159 49.4% 117 19.0% 212 25.6% 1.9 61 24 38 TTC = Target-Typicality Coverage; CLE Factor = precision floor inflation. TTC < 70%: logs-only IPS expected to fail regardless of ESS. Bold: passes TTC threshold. A/A note: Clones raw ESS (26%) is below the theoretical 100%, reflecting API-level non-determinism and floating-point divergence in log-probability summation; SIMCal-W corrects this measurement artifact, restoring ESS to 99%. The 26% is still substantially higher than other policies (0.40.7%), confirming TF captures real distributional similarity. The dominant issue is CLE: LLM response trajectories are high-dimensional enough that even moderate policy differences yield poor coverage, making IPS unreliable regardless of TF quality. TTC (79.6% for clone vs. 1949% for others) provides more robust overlap diagnostic. Weight diagnostics. Table 3 quantifies SIMCal-Ws effect and CLE diagnostics. The mean-one isotonic projection reduces dispersion, yielding ESS uplifts from < 1% to > 80%, CV shrinkage from > 100 to < 2, and tail relief (Hill α > 2). Yet ranking performance does not improve. The rightmost columns explain why: TTC ( ˆβ) measures logger coverage in target-typical regions, and the CLE factor quantifies the resulting SE floor inflation. For non-clone policies, TTC ranges from 1949% with CLE factors of 2461, validating the CLE explanation: high ESS is necessary but not sufficient. Calibration transportability. Figure 6 applies the policy-wise mean transport test (Proposition 3) to validate whether the base-trained calibration can be reused across target policies. We test H0,π : Eπ[Y (S, X)] = 0 for each policy with Bonferroni correction (α=0.0125). Clone, Parallel Universe, and Premium all pass: their residual distributions are centered at zero, implying that Direct + calibrated surrogate remains mean-unbiased for these policies. Unhelpful fails dramatically (mean residual 0.31, < 0.001). By Proposition 3, this implies oracle π 0.31: the surrogate systematically overestimates oracle quality for adversarial responses. This flags failure of mean transportability: absolute value estimates for this policy are biased, triggering the REFUSE-LEVEL gate (App. D.6). Practical implication: This test decides whether single calibration can be safely reused across policies; if policy fails, either recalibrate with policy-specific oracle data or fall back to oracle-only evaluation for that cell. sur π 13 Figure 4: CJE output: policy value estimates at n=1000, 25% oracle. Red diamonds show oracle ground truth; blue circles show CJE estimates with 95% CIs. CIs capture the true value for policies satisfying transportability; unhelpful (which violates transportability) shows slight miscoverage, failure mode flagged by the transportability test (Figure 6). This is representative of what practitioner would see when applying CJE to their own evaluation data. Figure 5: Uncertainty decomposition at n=1000 for direct+cov. Yellow: base sampling uncertainty (constant 0.014). Orange: oracle uncertainty (dominates at 5% oracle fraction, vanishes at 100%). At 5% oracle fraction, OUA contributes 55% of total variance; ignoring it produces invalid CIs. Sample size planning. Figure 7 shows MDE contours for the Direct Method: the smallest true policy difference detectable at 80% power, 5% two-sided significance. Key observation: for fixed label budget = oracle fraction, larger with lower oracle fraction outperforms smaller with higher oracle fraction, suggesting spread labels across more samples as practical rule. Practical guidance: For new domains, oversample the first batch (e.g., n=2,000 with 25% oracle fraction) to generate domain-specific MDE plot, then use it to select the minimal viable (n, oracle fraction) for future experiments. 14 Figure 6: Policy-wise mean transport test at 25% oracle fraction. Testing H0,π : Eπ[Y (S, X)] = 0 per policy (Bonferroni-corrected α=0.0125). Clone, Parallel Universe, and Premium pass (residuals centered at zero). Unhelpful fails (mean residual = 0.31): the surrogate overestimates oracle scores for adversarial responses by +0.31 in absolute level. Figure 7: Sample size planning for direct+cov. (A) RMSE by sample size and oracle fraction. (B) Standard errors including OUA. (C) MDE contours showing the smallest effect detectable at 80% power. Dashed lines show common detection thresholds; cells with MDE < 0.02 enable detecting 2point quality differences. Example: To detect 3% improvement with 80% power requires n1,000 with 10% oracle fraction. Cost analysis. Table 4 quantifies the cost-accuracy tradeoff. CJE with 5% oracle calibration achieves 99% pairwise ranking accuracy at n=5,000 (98.7% averaged over 50 seeds) at 14 lower total cost than pure oracle labeling (for all 5 policies). Oracle labels cost approximately 16 more than judge scores, so total cost scales primarily with oracle fraction. At 5% oracle fraction, this yields an 8.8 cost reduction for single policy. With amortization across 5 policies, this becomes 14 (see below), enabling scaling to production workloads where pure oracle labeling is prohibitive. Amortization improves further: calibrating once on shared oracle slice enables evaluating multiple policies at marginal judge-only cost, approaching the 16 cost ratio as grows. Table 4: Cost-accuracy tradeoff for n=5,000 prompts 5 policies (rounded for cost arithmetic; experiments use n=4,961 after filtering). CJE achieves near-oracle accuracy at 14 lower cost. Method Oracle Cost Judge Cost Total Pairwise % Pure oracle (100%) CJE (5% oracle) $55.00 $0.55 $3.50 $55.00 $4. 100% 99% 5.3 PRACTICAL TAKEAWAYS 1. Default to Direct + two-stage calibration. If you can generate fresh responses on shared prompt set, use the Direct Method. It requires no overlap assumption and achieves the best ranking accuracy  (Table 5)  . 15 Table 5: Estimator Selection Guide. Requirements and recommended use cases for each estimator family. Estimator Direct IPS/SNIPS Calibrated IPS DR Stacked-DR Fresh Logprobs Overlap When to Use Required Required Helpful Helpful Default for LLM eval Small policy changes only + SIMCal-W stabilization Logs + fresh draws available Production ensemble 2. Check TTC before using OPE. If TTC < 0.7, logs-only IPS will fail. Use DR only when fresh draws are available to complement logged data. 3. Always use OUA inference. Standard CIs severely under-cover. The jackknife cost is negligible. 4. Use OUA share to guide resource allocation. The fraction (cid:100)Varcal/(cid:100)Vartotal identifies bottlenecks: if OUA share > 50%, collect more oracle labels; if OUA share < 20%, collect more evaluation prompts (Figure 5). 5. Covariates matter. Response length as calibration covariate (not reweighting covariate) improves ranking across all methods. 6. Optimize the budget ratio. How many oracle labels do you need? The optimal ratio of labels (m) to scores (n) follows square-root law based on costs and variances (Section F). Check the OUA 1ω > Spendoracle share ω to equalize marginal utility: if ω , the experiment is under-investing in Spendsurrogate oracle labels."
        },
        {
            "title": "6 LIMITATIONS",
            "content": "Oracle alignment (scope). We assume the operational oracle aligns with stakeholder values; oracle selection itself is governance question outside the scope of this work (Assumption A0 in App. C.13). IPS requires support overlap between π0 and each π; DR benefits from Overlap (positivity). overlap but remains consistent via its outcome model when overlap is poor; Direct requires no overlap (it generates fresh responses under π). When overlap is poor, raw ratios are heavy-tailed and uncertainty inflates. Mitigations: SIMCalW reduces dispersion and raises ESS; if tails persist we (i) gate on ESS and Hill indices, (ii) use overlap weighting or cohort restriction, and (iii) run an online check when (cid:98)αHill<1 or singlerow dominance persists (App. D). Judge assumptions (surrogate validity). AutoCalR assumes mean sufficiency and monotonicity in (or learned index). If strained, the twostage fallback preserves mean honesty but targets E[f (Sπ )]. Mitigations: surface reliability curves and regional residuals; when evidence is weak, label as surrogatetarget, widen/refresh the oracle slice, and target labels where error concentrates. Calibration failure modes. AutoCal-R can fail in three distinct ways, each with observable signatures: 1. Policy-specific calibration: The mapping differs across policies. Signature: Withinpolicy rankings accurate, cross-policy comparisons fail. Test: Policy-wise mean transport test (Proposition 3). If Eπ[Y (S, X)] = 0, surrogate values are biased for that policy. 2. Extrapolation beyond range: Target policy produces values outside the calibration range. Signature: Boundary flatness; OutOfRange > 5%. Test: Coverage diagnostic (App. D). 3. Temporal drift: The mapping changes over time. Signature: Recent residuals larger than historical; calibration accuracy degrades. Test: Periodic mean transport test on fresh oracle batches. Scope of the mean transport test. The policy-wise mean transport test (Proposition 3) is necessary but not sufficient condition for full surrogate transportability (S2): it guarantees that the 16 surrogate is unbiased for policy-wise mean values, but does not rule out conditional miscalibration within subgroups or at the tails of the score distribution. Mitigations: perform subgroup-specific transport tests when fairness concerns apply; monitor reliability diagrams for regional deviations; use richer moment conditions for stronger tests (see remark after Proposition 3). If π pushes outside the labeled range, isotonic calCalibration coverage (identification). ibration flattens at the boundary and levels are not pointidentified. Mitigations: flag LIMITED CALIBRATION SUPPORT and set REFUSE-LEVEL (report rankings and partialID bounds) until targeted labels cover the uncovered region (App. D). Approximate sufficiency (bias modulus). When E[Y X, A, S] = µ(S), the residual (X, A, S) induces bias proportional to calibration error. Mitigations: by CauchySchwarz, Bias Wπ2 2; DbP shrinks Wπ2, so bias is second order when either calibration is tight or the violation small; we surface this via diagnostics and invoke REFUSE-LEVEL when unbounded. Temporal dependence and logger drift. Nonstationarity (launches, safety updates) can bias or widen intervals. Mitigations: report dependencerobust SEs (block/stationary bootstrap), shorten analysis windows, and monitor judge drift via rankbased/residual change detection with FDR control. Oracle independence and leakage. OUA assumes the oracle slice is i.i.d. with non-overlapping folds. Mitigations: reuse deterministic folds across modules, deduplicate the slice, and periodically refresh it. Selection and multiplicity. Scanning many π inflates winners curse. Mitigations: use FDR control (BH/BY), optionally an outer split for IFStack to reduce selection optimism, and emphasize prespecified contrasts. Teacher forcing and API drift. Accurate propensities require deterministic, chatnative TF (stable tokenizer/template) with additivity/conditionality invariants; missing/invalid TF corrupts ratios. In our experiments, 39 of 5,000 prompts (0.8%) were filtered: 11 due to base-policy TF conformance failures, and 28 due to missing target-policy logprobs. While small, we report this explicitly: practitioners should expect some data loss from TF gaps, and filtering should always be logged for reproducibility. This also highlights the current state of TF support from LLM providers: major closed-source providers (OpenAI, Anthropic, Google) do not currently support teacher forcing for chat models, and among open-weight hosting providers, Fireworks AI was the only service we found with reliable TF support. Even for our TF reliability test (clone = base with different seed), raw ESS is 26% rather than the theoretical 100%, reflecting API-level non-determinism and floating-point divergence in log-probability summation; SIMCal-W restores this to 99%  (Table 3)  . This 26% is still substantially higher than other policies (0.40.7%), confirming TF captures real distributional similarity. The dominant issue is CLE, not TF quality: LLM trajectories are high-dimensional enough that even moderate policy differences yield poor coverage (see Table 3). Mitigations: enforce schema/conformance checks, ledger failures, and treat results as conditional on TF quality (App. E). Subgroups and fairness. Calibration quality and ESS gains may differ across subgroups. Mitigations: provide subgroup diagnostics (ESS, reliability) and, when feasible, use subgroupspecific calibration/weights or constrained pooling. Judge informativeness (garbling). Coarser judges raise the surrogate information bound and widen CIs. Mitigations: prefer richer rubrics (multidimensional with stable aggregation) and validate with coarsening ablations (empirical Blackwell monotonicity). Compute. DR-CPO adds one rollout + judge per (X, π); AutoCalR refits for OUA add modest overhead. We amortize via TF caches, shared folds, and small stacking library. Ethics Statement. We analyze retrospective logs that may include sensitive content. Diagnostics/gates prevent overconfident claims under poor overlap/coverage and surface judge drift. When identification fails, we report rankings only (REFUSE-LEVEL) and recommend targeted labeling or 17 online checks. Any deployment should assess subgroup reliability and adopt privacy safeguards for logs. Reproducibility Statement. Exact prompts, schema, TF contract, pseudocode, and numerics appear in the appendices. Code and data are available at https://github.com/cimo-labs/ cje-arena-experiments; the CJE library is at https://github.com/cimo-labs/ cje."
        },
        {
            "title": "7 CONCLUSION",
            "content": "Uncalibrated LLM-as-judge evaluation is the default practice, and it produces rankings that can be inverted, confidence intervals with near-zero coverage, and OPE estimates that fail silently. These are not edge cases; they are the norm when calibration is ignored. We introduced CJE, an audit-ready framework that fixes all three failures, built around single rule: Design-by-Projection (DbP). The principle is simple: encode justified assumptions as closed convex sets and project valid objects onto them. Projections onto subspaces (nuisance-orthogonal scores), monotone cones (mean-preserving reward/weight calibration), and simplices (variancehedged stacking) preserve the estimand while weakly reducing variance. Concretely, AutoCal-R learns mean-preserving surrogate = (Z(S)); SIMCal-W stacks Smonotone candidates to produce unit-mean ratios with empirical ESS gains via OOF stacking and variance guard; DR-CPO delivers inference under cross-fitting; IF-Stack minimizes plug-in IF variance; and OUA adds calibration uncertainty to dramatically improve CI coverage. Theoretically, the Efficiency via Model Restriction result, applying established semiparametric principles to surrogate-based evaluation, explains why encoding justified knowledge as model restrictions lowers the efficiency bound, which is attainable with projection-designed estimators and cross-fitting. Two design corollaries follow: (i) Blackwellefficiency monotonicity: richer judges (finer σ-fields) strictly help; (ii) under the exact IsoMeanOneS projection, isotonic mean-one calibration weakly reduces weight dispersion (the reference implementation uses simpler normalization that empirically yields similar ESS gains). Empirically, on Arena-derived logs, SIMCal-W turns neardegenerate ratios into stable weights (large ESS gains), stacked-DR achieves near-nominal coverage scaling, and stacking improves ordering; when calibration support is limited, CJE flags and nearthe issue and reports robust rankings with conservative uncertainty (REFUSE-LEVEL). (i) Default to Direct: OPE fails under low overlap even with high ESS; calibrated Takeaways. judge scores suffice when transportability holds. (ii) Always use OUA: naive CIs achieve 0% coverage; OUA improves to 8596%. (iii) Validate transportability: the mean transport test catches real failures (e.g., adversarial policies). Future work. Selection-aware inference over large policy sets; robust/DP isotonic calibration (mirror/Bregman DbP) for heavy tails; active oracle budgeting via shadow prices; sequential/agent evaluations with prefix-aware SIMCal and stepwise DR; and subgroup-aware constraints with fairness diagnostics."
        },
        {
            "title": "8 RELATED WORK",
            "content": "CJE draws on four research threads: surrogate-assisted causal inference, LLM evaluation, probability calibration, and off-policy evaluation. We position our contributions by first clarifying three regimes of surrogacy that organize the literature. Three regimes of surrogacy. Surrogate-assisted estimation admits three regimes with increasing assumptions and decreasing oracle burden: 1. Surrogates for efficiency only (Kallus & Mao, 2020): The surrogate improves estimation efficiency (e.g., as features in outcome models) but does not identify treatment effects. Oracle labels are required in every evaluation context. This is the fallback when stronger surrogacy assumptions fail. 18 2. Local surrogacy (Athey et al., 2019; Prentice, 1989): Surrogate sufficiency (S1) holds within single environment g: E[Y X, A, S] = (S, X) in g. Calibrate once per environment; evaluate many policies within that environment using only S. 3. Global surrogacy with transport (CJE): Surrogate sufficiency (S1) plus transportability (S2): the same calibration applies across environments. Calibrate once; evaluate policies across environments using only S, subject to transport diagnostics. CJE operates in Regime 3. This requires stronger assumptions than Kallus & Mao (who use surrogates for efficiency but not identification) and differs in scope from Athey et al. (who build surrogate index for binary-treatment ATEs in single environment). The benefit is amortization: one oracle slice enables evaluation across policies and deployment contexts. The cost is that S2 must hold; our diagnostics (TTC, Bhattacharyya affinity, transport tests) validate when it does. LLM-as-judge evaluation. Automatic judges (LLMs scoring other LLMs) are now standard for scalable evaluation (Zheng et al., 2023; Kim et al., 2024; Kocmi & Federmann, 2023). Benchmarks like AlpacaEval (Dubois et al., 2024) and RewardBench (Lambert et al., 2024) systematically compare judge quality, revealing biases (verbosity, position, self-preference) and drift under distribution shift (Dietz et al., 2025). These works focus on ranking judges by correlation with human labels; CJE addresses the orthogonal problem of calibrating fixed judge to produce unbiased point estimates and calibration-aware confidence intervals for policy value. Recent work like CalibraEval (Li et al., 2025) addresses position and token bias via label-free calibration; CJE is complementary, providing oracle-grounded calibration with transportability guarantees and uncertainty quantification. Probability calibration. Platt scaling (Platt, 1999), isotonic regression (Zadrozny & Elkan, 2002; Niculescu-Mizil & Caruana, 2005), and temperature scaling (Guo et al., 2017) are standard post-hoc calibration methods for classification. Our AutoCal-R applies isotonic regression not for classification confidence but for reward calibration: mapping continuous judge score to an expected oracle outcome E[Y S]. The mean-preserving property (Lemma 1) ensures the expected value of calibrated rewards matches the oracle under the logging distribution. Off-policy evaluation (OPE). IPS (Horvitz & Thompson, 1952; Hajek, 1964) and doubly robust estimators (Bang & Robins, 2005; Jiang & Li, 2016; Kallus & Uehara, 2020) are foundational. Weight stabilization via clipping (Ionides, 2008), balancing (Kallus, 2018), and overlap weighting (Li et al., 2018; Crump et al., 2009) address variance inflation. Most relevant is recent work on isotoniccalibrated IPW (van der Laan et al., 2025) and DR inference via calibration (van der Laan et al., 2024). Our SIMCal-W extends this by projecting importance weights onto the cone of judge-score-monotone functions, exploiting surrogate structure absent in generic OPE. Ensemble methods for estimation. Super Learner (van der Laan et al., 2007) provides powerful framework for combining candidate algorithms by minimizing cross-validated prediction risk. Recent work extends ensemble ideas to semiparametric estimation (Shin et al., 2020), combining efficient estimators derived from factored likelihoods. Our IF-space stacking (Theorem 5) takes complementary approach: instead of combining predictions to minimize MSE, we combine estimators by minimizing the asymptotic variance of their combined influence function. This provides general formulation that directly targets efficiency, is not tied to specific likelihood factorizations, and is explicitly framed as optimization in the Hilbert space of influence functions. Transportability. Generalizing from source to target populations is studied in transportability (Pearl & Bareinboim, 2014) and covariate shift (Shimodaira, 2000; Sugiyama et al., 2007). Our S2 assumption is transportability condition: calibration learned on the oracle slice applies to the full dataset (and across policies) when no selection mechanism points into given (X, A, S). The CLE bound (Section 2) formalizes when logs-only estimation fails even with valid transportability, motivating the shift to Direct or DR methods under poor coverage."
        },
        {
            "title": "ACKNOWLEDGEMENTS",
            "content": "We thank Kevin Zielnicki, Molly Davies, Sven Schmit, Brad Klingenberg, Chris Rinaldi, Viridiana Lourdes, Izzy Farley, Sudhish Kasaba Ramesh, Sean Kelly, and Adith Swaminathan for helpful discussions and feedback on earlier drafts of this work."
        },
        {
            "title": "REFERENCES",
            "content": "Susan Athey, Raj Chetty, Guido W. Imbens, and Hyunseung Kang. The surrogate index: Combining short-term proxies to estimate long-term treatment effects more rapidly and precisely. Working Paper 26463, National Bureau of Economic Research, 2019. URL https://www.nber.org/ papers/w26463. Miriam Ayer, H. D. Brunk, G. M. Ewing, W. T. Reid, and Edward Silverman. An empirical distribution function for sampling with incomplete information. The Annals of Mathematical Statistics, 26(4): 641647, 1955. Yuntao Bai, Saurav Kadavath, Sandipan Kundu, Amanda Askell, Jason Kernion, Andy Jones, Anna Chen, Anna Goldie, Azalia Mirhoseini, Brian McKinnon, Carol Chen, Catherine Olsson, Daniel Brown, El Mahdi El-Mhamdi, Ethan Perez, Ilya Tolstikhin, Ishita Ganguli, Tom Henighan, Jared Carter, Shauna Kravec, Scott Johnston, Tim Shlegeris, Kamal Ndousse, Chitwan Saharia, Elizabeth Barnes, Ellie Soros, Hai Tieu, Jared Kaplan, Jan Leike, Geoffrey Irving, and Dario Amodei. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv, 2022. URL https://arxiv.org/abs/2204.05862. Moulinath Banerjee. Likelihood ratio inference in regular models with convex parameter space. Annals of Statistics, 29(1):169200, 2001. Heejung Bang and James M. Robins. Doubly robust estimation in missing data and causal inference models. Biometrics, 61(4):962973, 2005. doi: 10.1111/j.1541-0420.2005.00377.x. Richard E. Barlow, David J. Bartholomew, John M. Bremner, and Herman D. Brunk. Statistical Inference under Order Restrictions. Wiley, 1972. Heinz H. Bauschke and Patrick L. Combettes. Convex Analysis and Monotone Operator Theory in Hilbert Spaces. Springer, 2 edition, 2017. Peter J. Bickel, Chris A. J. Klaassen, Yaacov Ritov, and Jon A. Wellner. Efficient and Adaptive Estimation for Semiparametric Models. Johns Hopkins University Press, 1993. Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney K. Newey, and James M. Robins. Double/debiased machine learning for treatment and structural parameters. The Econometrics Journal, 21(1):C1C68, 2018. doi: 10.1111/ectj.12097. Richard K. Crump, V. Joseph Hotz, Guido W. Imbens, and Oscar A. Mitnik. Dealing with limited overlap in estimation of average treatment effects. Biometrika, 96(1):187199, 2009. Laura Dietz, Oleg Zendel, Peter Bailey, Charles L. A. Clarke, Ellese Cotterill, Jeff Dalton, Faegheh Hasibi, Mark Sanderson, and Nick Craswell. Principles and guidelines for the use of llm judges. In Proceedings of the 2025 International ACM SIGIR Conference on Innovative Concepts and Theories in Information Retrieval (ICTIR), ICTIR 25, pp. 218229, New York, NY, USA, 2025. Association for Computing Machinery. ISBN 9798400718618. doi: 10.1145/3731120.3744588. URL https://doi.org/10.1145/3731120.3744588. Yann Dubois, Xuechen Li, Rohan Taori, Tianyi Zhang, Ishaan Gulrajani, Jimmy Ba, Carlos Guestrin, Percy Liang, and Tatsunori B. Hashimoto. Length-controlled AlpacaEval: simple way to debias automatic evaluators. In First Conference on Language Modeling (COLM), 2024. URL https://openreview.net/forum?id=lEzXIFQG8c. Bradley Efron and Charles Stein. The jackknife estimate of variance. The Annals of Statistics, 9(3): 586596, 1981. doi: 10.1214/aos/1176345462. 20 Constantine E. Frangakis and Donald B. Rubin. Principal stratification in causal inference. Biometrics, 58(1):2129, 2002. Luke Guerdan, Justin Whitehouse, Kimberly Truong, Kenneth Holstein, and Zhiwei Steven Wu. Doubly-robust llm-as-a-judge: Externally valid estimation with imperfect personas, 2025. URL https://arxiv.org/abs/2509.22957. Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning (ICML), pp. 13211330, 2017. Jaroslav Hajek. Asymptotic theory of rejective sampling with varying probabilities from finite population. Annals of Mathematical Statistics, 35(4):14911523, 1964. doi: 10.1214/aoms/ 1177700375. G. H. Hardy, J. E. Littlewood, and G. Polya. Inequalities. Cambridge University Press, 2 edition, 1952. Bruce M. Hill. simple general approach to inference about the tail of distribution. Annals of Statistics, 3(5):11631174, 1975. doi: 10.1214/aos/1176343247. Daniel G. Horvitz and Donovan J. Thompson. generalization of sampling without replacement from finite universe. Journal of the American Statistical Association, 47(260):663685, 1952. doi: 10.1080/01621459.1952.10483446. Edward L. Ionides. Truncated importance sampling. Journal of Computational and Graphical Statistics, 17(2):295311, 2008. doi: 10.1198/106186008X320456. Nan Jiang and Lihong Li. Doubly robust off-policy value evaluation for reinforcement learning. In Maria Florina Balcan and Kilian Q. Weinberger (eds.), Proceedings of the 33rd International Conference on Machine Learning (ICML), volume 48 of Proceedings of Machine Learning Research, pp. 652661. PMLR, 2016. URL https://proceedings.mlr.press/v48/ jiang16.html. Nathan Kallus. Balanced policy evaluation and learning. In Advances in Neural Information Processing Systems (NeurIPS), 2018. Nathan Kallus and Xiaojie Mao. On the role of surrogates in the efficient estimation of treatment effects with limited outcome data. arXiv, 2020. Nathan Kallus and Masatoshi Uehara. Double reinforcement learning for efficient off-policy evaluation in markov decision processes. Journal of Machine Learning Research, 21(167):163, 2020. URL http://jmlr.org/papers/v21/19-827.html. Hyung Won Kim et al. Prometheus: Towards trainable LLM-as-a-judge for evaluating LLMs. In Proceedings of the Twelfth International Conference on Learning Representations (ICLR), 2024. Tom Kocmi and Christian Federmann. Large language models are state-of-the-art evaluators of translation quality. arXiv, 2023. URL https://arxiv.org/abs/2302.14520. WMT 2023. Michael R. Kosorok. Introduction to Empirical Processes and Semiparametric Inference. Springer, 2008. Hans R. Kunsch. The jackknife and the bootstrap for general stationary observations. The Annals of Statistics, 17(3):12171241, 1989. Nathan Lambert, Valentina Pyatkin, Jacob Morrison, LJ Miranda, Bill Yuchen Lin, Khyathi Chandu, Nouha Dziri, Sachin Kumar, Tom Zick, Yejin Choi, Noah A. Smith, and Hannaneh Hajishirzi. RewardBench: Evaluating reward models for language modeling. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (ACL), 2024. URL https: //arxiv.org/abs/2403.13787. Chungpa Lee, Thomas Zeng, Jongwon Jeong, Jy-yong Sohn, and Kangwook Lee. How to correctly report LLM-as-a-judge evaluations. arXiv, 2025. URL https://arxiv.org/abs/2511. 21140. Fan Li, Kari Lock Morgan, and Alan M. Zaslavsky. Balancing covariates via propensity score weighting. Journal of the American Statistical Association, 113(521):390400, 2018. Haitao Li, Junjie Chen, Qingyao Ai, Zhumin Chu, Yujia Zhou, Qian Dong, and Yiqun Liu. CalibraEval: Calibrating prediction distribution to mitigate selection bias in LLMs-as-judges. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (ACL), 2025. Lihong Li, Wei Chu, John Langford, and Robert E. Schapire. Unbiased offline evaluation of contextual-bandit performance. In Proceedings of the Fourth ACM International Conference on Web Search and Data Mining (WSDM), 2011. Jun S. Liu. Monte Carlo Strategies in Scientific Computing. Springer, 2001. Albert W. Marshall, Ingram Olkin, and Barry C. Arnold. Inequalities: Theory of Majorization and Its Applications. Springer, 2 edition, 2011. Whitney K. Newey. Semiparametric efficiency bounds. Journal of Applied Econometrics, 5(2): 99135, 1990. Alexandru Niculescu-Mizil and Rich Caruana. Predicting good probabilities with supervised learning. In Proceedings of the 22nd International Conference on Machine Learning (ICML), 2005. OpenAI. GPT-5 system card. https://openai.com/index/gpt-5-system-card/, 2025. Accessed: 2025-12-01. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Luke Kelton, Fraser Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. Training language models to follow instructions with human feedback. arXiv, 2022. URL https://arxiv.org/abs/2203.02155. Art B. Owen. Monte Carlo Theory, Methods and Examples. 2013. Monograph, available at https://artowen.su.domains/mc/. Judea Pearl. The causal mediation formulaa guide to the assessment of pathways and mechanisms. Prevention Science, 13(4):426436, 2012. doi: 10.1007/s11121-011-0270-1. Judea Pearl and Elias Bareinboim. External validity: From do-calculus to transportability across populations. Statistical Science, 29(4):579595, 2014. doi: 10.1214/14-STS486. John C. Platt. Probabilistic outputs for support vector machines and comparisons to regularized likelihood methods. In Advances in Large Margin Classifiers, 1999. Dimitris N. Politis and Joseph P. Romano. The stationary bootstrap. Journal of the American Statistical Association, 89(428):13031313, 1994. Ross L. Prentice. Surrogate endpoints in clinical trials: Definition and operational criteria. Statistics in Medicine, 8(4):431440, 1989. Tim Robertson, F. T. Wright, and R. L. Dykstra. Order Restricted Statistical Inference. Wiley, 1988. James M. Robins and Sander Greenland. Identifiability and exchangeability for direct and indirect effects. Epidemiology, 3(2):143155, 1992. Hidetoshi Shimodaira. Improving predictive inference under covariate shift by weighting the loglikelihood function. Journal of Statistical Planning and Inference, 90(2):227244, 2000. Sangwook Shin, Yufeng Liu, Stephen R. Cole, and Jason P. Fine. Ensemble estimation and variable selection with semiparametric regression models. Biometrika, 107(2):433448, 2020. doi: 10. 1093/biomet/asz073. 22 Masashi Sugiyama, Matthias Krauledat, and Klaus-Robert Muller. Covariate shift adaptation by importance weighted cross validation. Journal of Machine Learning Research, 8:9851005, 2007. Adith Swaminathan and Thorsten Joachims. Counterfactual risk minimization: Learning from logged bandit feedback. In Proceedings of the 32nd International Conference on Machine Learning (ICML), pp. 814823, 2015. Anastasios A. Tsiatis. Semiparametric Theory and Missing Data. Springer, 2006. Lars van der Laan, Alex Luedtke, and Marco Carone. Doubly robust inference via calibration. arXiv, 2024. doi: 10.48550/arXiv.2411.02771. URL https://arxiv.org/abs/2411.02771. Lars van der Laan, Ziming Lin, Marco Carone, and Alex Luedtke. Stabilized inverse probability weighting via isotonic calibration. In Proceedings of the Fourth Conference on Causal Learning and Reasoning (CLeaR), volume 275 of Proceedings of Machine Learning Research, pp. 139173. PMLR, 2025. URL https://proceedings.mlr.press/v275/laan25a.html. Mark J. van der Laan and Sherri Rose. Targeted Learning: Causal Inference for Observational and Experimental Data. Springer, 2011. Mark J. van der Laan, Eric C. Polley, and Alan E. Hubbard. Super learner. Statistical Applications in Genetics and Molecular Biology, 6(1):Article 25, 2007. Aad W. van der Vaart and Jon A. Wellner. Weak Convergence and Empirical Processes. Springer, 2000. Tyler J. VanderWeele. Surrogate measures and consistent surrogates. Biometrics, 69(3):561581, 2013. Tyler J. VanderWeele. Explanation in Causal Inference: Methods for Mediation and Interaction. Oxford University Press, 2015. David H. Wolpert. Stacked generalization. Neural Networks, 5(2):241259, 1992. Bianca Zadrozny and Charles Elkan. Transforming classifier scores into accurate multiclass probability estimates. In Proceedings of the Eighth ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2002. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Joseph E. Gonzalez, Matei Zaharia, and Ion Stoica. Judging LLM-as-a-judge with MT-Bench and Chatbot Arena. In Advances in Neural Information Processing Systems (NeurIPS), volume 36, pp. 4659546623, 2023."
        },
        {
            "title": "A NOTATION AND FORMAL SETUP",
            "content": "Observed data and policies. We observe i.i.d. logs Oi = (Xi, Ai, Si, obs , Li), = 1, . . . , n, generated under fixed logger Ai π0( Xi). scalar judge Si = s(Xi, Ai) is available on all rows. The label indicator Li {0, 1} marks inclusion in the oracle slice; when Li = 1 we observe obs is missing. For candidate policy π, define the sequence-level importance ratio = Yi, otherwise obs Wπ,i = π(Ai Xi) π0(Ai Xi) (cid:110) = exp log pπ(Ai Xi) log pπ0 (Ai Xi) (cid:111) , computed via teacher forcing (TF) with the models own tokenizer/rendering. Write m1 π,i = Wπ,i j=1 Wπ,j (cid:80)n 1 for the samplemeanone (SNIPS) baseline (global normalization over the evaluation cohort). 23 Estimand. Let (π) denote the outcome under the counterfactual draw π( X). The target is (π) = E(cid:2)Y (π)(cid:3). A.1 ASSUMPTIONS (COMPACT) (D1) Fixed logger & i.i.d. (Xi, Ai, Si) are i.i.d. under π0; TF log-likelihoods are stable and welldefined. (D2) Overlap (positivity). π0(a x) > 0 whenever π(a x) > 0, and Eπ0[W 2 (D3) Judge coverage & stability. is well-defined under both π0 and π; the RadonNikodym derivative on σ(S) exists; the judge/rubric is stable on the analysis window. π] < . (J1) Oracle slice. There exists an i.i.d. subsample = {i : Li = 1} with = on which is observed. (J2-M) Mean sufficiency (monotone). E[Y X, A, S] = µ(S) with µ weakly nondecreasing. (J2-SI) Single-index fallback. There exist : and nondecreasing µ such that E[Y X, A, S] = µ(cid:0)g(S, X)(cid:1). When depends only on S, this reduces to J2-M; when covariates (e.g., response length) are included in g, the target remains (π) and the estimand is preserved provided the index is consistently estimated. (R1) Tails/moments. E[Y 2] < , E[S2] < , and Eπ0[W 2 projection with relative variance bound ρ 1, Varn( ˆWπ) ρ Varn(W m1 times the SNIPS baseline). (R2) Calibration consistency. AutoCal-R satisfies ˆf (Z) E[Y Z]L2(PZ ) = op(1), where = in monotone mode and = g(S, X) (learned index) in two-stage mode. SIMCal-W satisfies ˆWπ ρ is the S-monotone, mean-one, variance-capped projection of Wπ (see Section A.3); when E[Wπ S] is itself monotone and satisfies the variance constraint, π] < . Under the exact IsoMeanOneS π ) (i.e., variance at most ρ ρL2(P ) = op(1), where ρ = E[Wπ S]. (R3) One-of-two rates with cross-fitting. With nuisances ˆq(x, a) E[R x, a] and ˆWπ, ˆq RL2(P ) ˆWπ mL2(P ) = op(n1/2), e.g., either factor is op(n1/4) with the other consistent. A.2 CROSS-FITTING AND FOLDS Let : {1, . . . , n} {1, . . . , K} be deterministic fold map (e.g., hash of id). For any learner = ˆη(F (i))(Oi) in L, train ˆη(k) = on {i : (i) = k} and use out-of-fold predictions ˆηOOF influence-function (IF) calculations. The same folds are reused across AutoCal-R, SIMCal-W, and DR nuisances. A.3 PROJECTION OPERATORS USED BY CJE Monotone cone. = {f : nondecreasing}. The isotonic projector (PAVA) ΠM enjoys: (i) L2 optimality; (ii) mean preservation on the training sample; (iii) dispersion reduction by majorization. Mean-one cone for weights. For Rn ordered by S, define the mean-one isotonic projection IsoMeanOneS(w) = arg min (cid:88) (ui wi)2 s.t. M(S), 1 (cid:88) ui = 1, where M(S) denotes vectors nondecreasing in the S-order. This preserves the sample mean and weakly reduces empirical variance; hence ESS weakly increases (deterministically, by majorization). Simplex hull. For centered IF columns {ϕ(e)}eE , let Φ = [ϕ(e)] and = {α : αe 0, (cid:80) 1}. IF-space stacking solves minα α ˆΣ α with ˆΣ = (1/n)ΦΦ + λI. αe = 24 Figure 8: Two-stage calibration with covariates. Left: first-stage spline on judge score (response length fixed at mean). Center: first-stage effect of response length (judge score fixed). Right: secondstage isotonic mapping on the risk index (Stage 1 output) enforces monotonicity while preserving flexibility from the first stage. A.4 AUTOCAL-R AND SIMCAL-W PRIMITIVES AutoCal-R. On = {i : Li = 1}, fit = ˆf (Z(S)) by either: (i) monotone (Z(S) = S), or (ii) two-stage (Z(S, X) = ECDF{g(S, X)} with spline g), selecting by OOF RMSE (1-SE rule). Use OOF predictions ROOF along IF paths; the point estimate may use pooled fit. terminal isotonic step enforces slice-mean preservation. SIMCal-W. (Per fold) Fit up/down isotonic maps on to obtain OOF OOF where ˆΣ is the covariance of Uc = OOF one (global, over the evaluation cohort), optionally apply the absolute variance guard ; optionally include base 1. Define residuals (IPS: Ri; DR: Ri ˆq(Xi, Ai)). Choose ˆβ arg minβ3 β ˆΣ β . Form stack = (cid:80) , renormalize to mean ˆβcW OOF , OOF α = min (cid:114) (cid:110) 1, ρ Var(W stack) (cid:111) , blend = 1 + α (W stack 1), and normalize to mean one: ˆWπ = blend/ blend. (The theoretical Lemma 5 uses relative bound.) A.5 DR NUISANCES AND SEQUENCE VALUE Let ˆq(x, a) E[R x, a] and define ˆgπ(x) = (cid:80) π(a x) ˆq(x, a). For sequences, approximate ˆgπ(x) with one (default) rollout π( x) and = ˆf(cid:0)s(x, A)(cid:1); light smoother (e.g., ridge over (x, z) features) can reduce Monte Carlo noise. Cross-fitting is used throughout. A.6 INFLUENCE FUNCTIONS AND VARIANCE Let {ϕi}n with crossfitted/OOF nuisances and ROOF along the IF path, so that 1 standard regularity conditions, i=1 denote the (approximately) centered influencefunction contributions of ˆψ, computed i=1 ϕi 0. Under (cid:80)n (cid:0) ˆψ ψ(cid:1) (0, Var(ϕ)) , hence Var( ˆψ) Var(ϕ) . (cid:80)n Since E[ϕ] = 0, we have Var(ϕ) 1 variance (including the oracle addition) by i=1 ϕ2 . We estimate the variance of ˆψ (not ϕ) and the total (cid:100)Varmain = 1 n2 (cid:88) i=1 ϕ2 , (cid:100)Vartotal = (cid:100)Varmain + (cid:100)Varcal, and report the (1 α) Wald interval CI1α : ˆψ z1α/2 (cid:113) (cid:100)Vartotal . 25 When serial or cluster dependence is concern, we additionally report dependencerobust SEs (e.g., clusterrobust sandwich or block/stationary bootstrap) as sensitivity analysis. A.7 ORACLEUNCERTAINTYAWARE (OUA) JACKKNIFE Partition into oracle folds {Ok}K and rerun the full pipeline to obtain ˆψ(k). Then k=1. For each k, refit AutoCal-R on Ok, recompute R(k), ψ = 1 (cid:88) k=1 ˆψ(k), (cid:100)Varcal = 1 (cid:88) k=1 (cid:0) ˆψ(k) ψ(cid:1)2 . (With unequal fold sizes, use the standard weighted delete-one-group formula.) A.8 DIAGNOSTICS (DEFINITIONS) ESS. ESS(W ) = (cid:0) (cid:80) Wi normalization (SNIPS), (cid:80) (cid:1)2 / (cid:80) 2 Wi = and ; we report the fraction ESS/n. Under global mean-one ESS(W ) = 1 1 + CV2(W ) with CV2(W ) = Var(W ) (since E[W ] = 1). Unless stated otherwise, diagnostics use the global (not per-fold) mean-one scaling. Max-weight share. maxi Wi (cid:14) (cid:80) Wj. Tail index (Hill). For top-k order statistics W(1) W(k), ˆα1 = 1 (we sweep over stability grid and report the plateau). Bhattacharyya affinity in S. AB = (cid:82) (cid:112)pSπ(s) pSπ0(s) ds (discrete: sum over bins); DB = log AB. DR orthogonality score. n1 (cid:80) (cid:0)ROOF (cid:0)S / [Sorc ˆqOOF(Xi, Ai)(cid:1) with Wald CI. min, Sorc max](cid:1); large out-of-range mass with near-flat Coverage badge. Plug-in estimate of Prπ boundaries triggers LIMITED CALIBRATION SUPPORT and REFUSE-LEVEL. ˆWπ,i j=1 log(cid:0)W(j)/W(k) (cid:80)k (cid:1) A.9 SYMBOL GLOSSARY Symbol Meaning Context, action (sequence) Judge score (scalar) Ground-truth outcome (on oracle slice) Logger and candidate policies Importance ratio π(A X)/π0(A X) Mean-one (SNIPS) baseline X, = s(X, A) π0, π Wπ m1 π = ˆf (Z(S)) Calibrated reward (AutoCal-R; monotone or two-stage) ˆWπ ˆq, ˆgπ ϕ (cid:100)Varmain (cid:100)Varcal ESS(W ) Calibrated, unit-mean, S-monotone weights (SIMCal-W) Outcome and policyvalue nuisances for DR Per-row centered influence-function contribution Estimator variance n2 (cid:80) Calibration variance (oracle jackknife) Effective sample size ϕ2 ALGORITHMS (EXTENDED) This appendix gives compact, cross-fitted pseudocode for CJE modules: reward calibration (AUTOCAL-R), surrogate-indexed weight calibration (SIMCAL-W), estimators (CAL-IPS, DRCPO), IF-space stacking, and oracleuncertaintyaware variance (OUA). We reuse the same K-fold map (i) {1:K} across all modules. OOF denotes out-of-fold predictions used along the IF path. 26 Algorithm 1 AUTOCAL-R: mean-preserving reward calibration (cross-fitted; automatic two-stage fallback) 1: Inputs: Oracle pairs {(Si, Yi) : Li=1}; optional covariates {Xi}; folds (); smooth index class g() 2: Outputs: Calibrated rewards Ri and OOF ROOF 3: for = 1 to do 4: 5: Monotone candidate: ˆf (k) Train set Ok = {i : Li=1, (i) = k}; test set Ok = {i : Li=1, (i) = k} arg minf (Yi (Si))2; set ROOF (cid:80) iOk ,i = ˆf (k) (Si) 6: for Ok Two-stage candidate: fit g(k)(S, X) on Ok; ranks Ui = ECDFOk (g(k)(Si, Xi)); fit ˆh(k) 7: end for 8: Compute OOF risks (overall and by tertile); select mode via 1-SE rule (prefer monotone unless (Yi h(Ui))2; set ROOF 2s,i = ˆh(k) arg minhM (Ui) for Ok iOk (cid:80) two-stage is significantly better) 9: Refit the selected mode on the full oracle slice to obtain global Ri for all {1:n}; retain ROOF per fold for IFs 10: Note: The terminal isotonic step preserves the oracle-slice mean exactly. Algorithm 2 SIMCAL-W: surrogate-indexed, unit-mean monotone weight calibration (OOF project stack cap normalize) 1: Inputs: Baseline mean-one ratios m1 folds (); variance cap ρ 1 (default 1) π ; scores S; residuals (IPS: =R; DR: =R ˆq); 2: Output: Calibrated weights ˆWπ (mean-one, approximately S-monotone) 3: for = 1 to do 4: Train Ik = {i : (i) = k}; test Ik = {i : (i) = k} {OOF candidate projections} Fit isotonic regression of m1 (via S); rescale each to mean one on Ik (S), OOF Predict on Ik: OOF π on over Ik: increasing m(k) (S), and optionally include OOF (S) and decreasing m(k) 5: = m(k) = m(k) base 1 (S) 6: 7: end for 8: OOF stacking (variance-aware). Form Uc = OOF for {base, , }; compute ˆΣcd = cov(Uc, Ud) + λ1c=d 9: Choose ˆβ arg minβ3 β ˆΣ β; set stack = (cid:80) (cid:110) 1, (cid:112)ρ/ Var(W stack) 10: Light variance guard (optional). α = min ˆβc OOF mean one ; renormalize stack to sample (cid:111) ; set blend = 1 + α (W stack 1) 11: Mean normalization. ˆWπ blend/ blend (restores sample mean one) Complexity notes. PAVA is O(n) after shared sort by S. SIMCAL-W is linear-time per fold; the stacking QP is 33 (weights) or small EE system. DR-CPO adds one rollout + judge per (X, π). The OUA jackknife refits the calibrator times and reruns the pipeline; TF caches and precomputed features amortize cost."
        },
        {
            "title": "C PROOFS AND TECHNICAL LEMMAS",
            "content": "We collect standing identities, shape-constrained facts, and proofs for the results in Section 4. Unless stated otherwise, expectations are under the logging law Pπ0; L2 norms are with respect to the relevant marginal (e.g., L2(P ) or L2(PS)). We reuse the fold map () from Section A.2 and the projection operators from Section A.3. C.1 STANDING IDENTITIES AND TOOLS Change of measure. For any integrable h(X, A, S, ) and any candidate π, E(cid:2)Wπ h(X, A, S, )(cid:3) = Eπ (cid:2)h(X, A, S, )(cid:3), E[Wπ] = 1. (2) 27 Algorithm 3 CAL-IPS: calibrated importance sampling 1: Inputs: Calibrated rewards R, ROOF (Alg. 1); calibrated weights ˆWπ (Alg. 2) 2: Outputs: (cid:98)VIPS and IF ϕIPS (cid:88) ˆWπ,i Ri, = ˆWπ,i ROOF ϕIPS (cid:98)VIPS 3: (cid:98)VIPS = 1 i=1 Algorithm 4 DR-CPO: sequence-aware doubly robust estimator (cross-fitted) 1: Inputs: ˆWπ; and ROOF from Alg. 1; folds () 2: Outputs: (cid:98)VDR and IF ϕDR 3: for = 1 to do 4: Train ˆq(k)(x, a) E[R x, a] on {i : (i) = k}; predict OOF ˆqOOF Ik Approximate ˆg(k) π smoothing; obtain OOF gOOF π,i for Ik 5: 6: end for 7: (cid:98)VDR = 1 i=1 (cid:88) (cid:110) ˆgπ(Xi) + ˆWπ,i (cid:0)Ri ˆq(Xi, Ai)(cid:1)(cid:111) 8: ϕDR = ˆgπ(Xi) + ˆWπ,i (cid:0)ROOF ˆqOOF (cid:1) (cid:98)VDR (x) = EAπ(x)[ˆq(k)(x, A)] via one rollout π( X) and optional = ˆq(k)(Xi, Ai) for DoobDynkin / conditional expectation as L2 projection. Let = σ(S). Then m(S) := E[Wπ G] is the L2 projection of Wπ onto the closed subspace L2(G) L2(P ), i.e., E(cid:2)(Wπ (S))2(cid:3) = E(cid:2)(Wπ m(S))2(cid:3) + E(cid:2)(m(S) (S))2(cid:3), (3) for all (S) L2(G). In particular, Var(WπU ) Var(m(S)U ) for any (S) L2(G). Pythagoras in Hilbert spaces. Let L2 f, = E[f g]. For nonempty closed convex set L2 metric projection. If C1 C2 then dist(z, C2) dist(z, C1). 0(P ) be the mean-zero Hilbert space with inner product 0, denote by ΠC(z) the 0 and any L2 C.2 ISOTONIC REGRESSION: MEAN PRESERVATION AND MAJORIZATION Lemma 1 (Mean preservation; PAVA). Let ˆf arg minf (cid:80) (PAVA) on indices I. Then 1 ˆf (si) = 1 iI yi. (cid:80) iI (cid:80) iI (yi (si))2 be the isotonic fit Lemma 2 (Dispersion reduction by majorization). After sorting by s, the isotonic fitted vector ˆu is mean-preserving adjacent pooling of y; hence for any convex ϕ, (cid:80) ϕ(yi) (Hardy et al., 1952; Marshall et al., 2011). In particular, with sample mean one, Varn(ˆu) Varn(y) and ESS(ˆu) ESS(y). ϕ(ˆui) (cid:80) Proofs are standard; see Ayer et al. (1955); Barlow et al. (1972); Robertson et al. (1988); Banerjee (2001). C.3 PROOF OF THEOREM 1 (SURROGATE EIF & VARIANCE DROP) Let = E[Y S] and m(S) = E[Wπ S]. Under mean-sufficiency E[Y X, A, S] = R(S), (π) = E(cid:2)WπR(cid:3) = E(cid:2)m(S)R(S)(cid:3). Standard semiparametric calculations (projecting the unconstrained score onto the tangent space of the surrogate model) yield (4) ϕsur(O; π) = R(X, A)(cid:1) (π), π,R(X) + m(S)(cid:0)R π,R(x) = EAπ(x)[q with R(x, A)] (Bickel et al., 1993; van der Vaart & Wellner, 2000). Since is the L2 projection of Wπ onto L2(σ(S)), Pythagoras (or (3)) implies Var(ϕsur) Var(ϕuncon), strictly unless Wπ L2(σ(S)) and is degenerate. R(x, a) = E[R x, a] and (5) 28 Algorithm 5 IF-STACK: variance-optimal convex ensembling in IF space 1: Inputs: Candidates { (cid:98)V (e), ϕ(e)}eE (centered IFs; same folds); ridge λ 2: Outputs: (cid:98)V ( ˆα) and ϕ( ˆα) 3: Form Φ = [ϕ(e)]eE ; ˆΣ = (1/n)ΦΦ + λI 4: Solve ˆα arg minα α ˆΣ α 5: (cid:98)V ( ˆα) = (cid:80) eE ˆαe ϕ(e) 6: Optional outer split: learn ˆα on one half; apply to the other to reduce selection optimism 7: Support note (Caratheodory). If rank(Φ) = r, the variance-optimal stack uses at most r+1 base eE ˆαe (cid:98)V (e), ϕ( ˆα) = (cid:80) estimators. Algorithm 6 OUA jackknife: oracleuncertaintyaware variance addition 1: Inputs: Oracle folds {Ok}K 2: Outputs: (cid:100)Varcal and (cid:100)Vartotal = (cid:100)Varmain + (cid:100)Varcal 3: for = 1 to do 4: k=1; end-to-end estimator (cid:98)V () Refit AUTOCAL-R on Ok; recompute R(k) and all downstream nuisances & weights; run the full pipeline to get (cid:98)V (k) 5: end for 6: = 1 (cid:98)V (k), (cid:100)Varcal = K1 (cid:80) (cid:0) (cid:98)V (k) (cid:1)2 (cid:88) 7: Return: (cid:100)Vartotal = (cid:100)Varmain + (cid:100)Varcal C.4 PROOF OF THEOREM 2 (EFFICIENCY VIA MODEL RESTRICTION) Let be baseline semiparametric model with tangent space (P ) and canonical gradient ϕ. Let MC be restricted model with tangent space TC(P ) (P ) and canonical gradient ϕ C. The canonical gradient is defined as the projection of the pathwise derivative onto the tangent space. Since TC(P ) (P ), we can decompose: where ϕ (P ) TC(P ) (the orthogonal complement of TC(P ) within (P )). ϕ = ϕ + ϕ, By the Pythagorean theorem in Hilbert spaces: C2 C2 ϕ2 2, with equality iff ϕ = 0, i.e., TC(P ) = (P ) (no actual restriction). 2 + ϕ2 2 = ϕ 2 ϕ Monotonicity for nested restrictions follows: if TC1(P ) TC2(P ) (i.e., C2 imposes stronger restric2 ϕ 2 tions, yielding smaller tangent space), then ϕ C2 C1 For attainability, standard semiparametric arguments apply: one-step corrections or TMLE with cross-fitting yield regular estimators achieving the restricted efficiency bound (Bickel et al., 1993; van der Vaart & Wellner, 2000; van der Laan & Rose, 2011). 2 2. C.5 PROOF OF COROLLARY 1 If σ(S2) σ(S1), then L2(σ(S2)) L2(σ(S1)). The surrogate model using S1 has tangent space (S1), and the model using S2 has (S2) (S1) when σ(S2) σ(S1). By Theorem 1 and the nested-tangent-space argument in Theorem 2, the canonical gradient in the smaller tangent space has variance no larger than in the larger one: Var(ϕsur(S1)) Var(ϕsur(S2)). Strictness fails only if the finer knowledge already holds, i.e., if Wπ is σ(S2)-measurable and is degenerate. C.6 PROOF OF PROPOSITION 1 (CAL-IPS) Write (cid:98)VIPS (π) = (Pn )(cid:2)m(S)R(cid:3) + P(cid:2)( ˆWπ m)R(cid:3) + P(cid:2)m( ˆf (Z(S)) R)(cid:3) + rem, where rem collects second-order sample-splitting terms. The empirical process term is Op(n1/2); the second and third vanish by L2-consistency of SIMCal-W and AutoCal-R (monotone or two-stage) and CauchySchwarz; the remainder is op(1) by cross-fitting. Finite-sample dispersion control follows from Lemma 2 and, if used, the blend cap ρ 1; under the exact IsoMeanOneS projection, ESS( ˆWπ) ESS(W m1 π ). C.7 PROOF OF THEOREM 3 (DR-CPO LIMITS) With cross-fitted nuisances and ROOF along the IF path, (cid:98)VDR (π) = (Pn )(cid:2)ϕsur (cid:3) + P(cid:2)( ˆWπ m){q ˆq}(cid:3) + op(n1/2). The second term is op(n1/2) by the one-of-two product-rate condition ˆWπ m2 ˆq op(n1/2) and cross-fitting; the central limit theorem yields the limit variance Var(ϕsur). R2 = C.8 PROOF OF THEOREM 4 (BUDGETED BOUND) Let Wρ = {m : E[m] = 1, S, E[(m 1)2] ρ E[(m 1)2]}. Intersecting the surrogate tangent space with the linear span induced by Wρ replaces by its L2(PS) projection ρ = ΠWρ (m) in ϕsur, giving ϕ(ρ). Monotonicity in ρ follows from nested convex sets Wρ1 Wρ2 for ρ1 ρ2, and limρ ϕ(ρ) = ϕsur. If SIMCal-W converges to ρ (with the same cap), DR-CPO attains Var(ϕ(ρ)) by the same one-of-two rate argument. C.9 PROOF OF THEOREM 5 (IF-SPACE STACKING) AND COROLLARY 2 Let { (cid:98)V (e)} be regular and asymptotically linear with centered IFs {ϕ(e)}. Set Φ = [ϕ(e)] and ˆΣ = (1/n)ΦΦ + λI. uniform law of large numbers on the simplex yields ˆΣ Σ uniformly; by argmin continuity, ˆα α arg minα αΣ α. Hence (cid:98)V ( ˆα) is asymptotically linear with IF ϕ(α) = (cid:80) eϕ(e) and variance minα αΣ α mine Σee. For Corollary 2: if rank(Φ) = r, then the feasible IF combinations lie in an r-dimensional affine subspace; by Caratheodorys theorem, any point in conv{ϕ(e)} admits representation using at most r+1 extreme points. α C.10 PROOF OF PROPOSITION 2 (OUA JACKKNIFE) Let (cid:98)V ( ˆf ) be regular estimator that depends on only through = ˆf (T (S)), with (cid:55) (cid:98)V (f ) Hadamard-differentiable at in L2(PS). Using delta-method expansion and cross-fitting (so that oracle folds are asymptotically independent of the IF path), the delete-one-oracle-fold jackknife (Bickel et al., 1993; Kunsch, 1989; Politis & Romano, 1994) consistently estimates the variance contribution from first-stage calibration. Therefore (cid:100)Vartotal = (cid:100)Varmain + (cid:100)Varoracle is consistent for Var( (cid:98)V ). C.11 AUXILIARY LEMMAS USED IN THE MAIN PROOFS = ˆf (F (i))(Z(Si)) Lemma 3 (OOF mean preservation for AutoCal-R). Let be fixed and let ROOF be OOF predictions from AutoCal-R (either mode). Then Pn[ROOF]Pn[Y ] = op(1) and [ROOF R] = op(1) under L2(PS)-consistency of ˆf . Lemma 4 (Second-order remainder for DR with cross-fitting). Let (cid:98)VDR be DR-CPO with cross-fitted (ˆq, ˆgπ) and calibrated ˆWπ. Then ˆq)(cid:3) + op(n1/2), (cid:98)VDR (π) (Pn )ϕsur = P(cid:2)( ˆWπ m)(q and the bracketed term is op(n1/2) under ˆWπ m2 ˆq R2 = op(n1/2). Lemma 5 (Guard stability). Let stack be the OOF-stacked candidate and m1 π baseline. For ρ 1, define α = min(cid:8)1, (cid:112)ρ Var(W m1 α(W stack 1). Then Var(W blend) = α2 Var(W stack) ρ Var(W m1 one isotonic projection cannot increase empirical variance by Lemma 2. the mean-one π )/ Var(W stack)(cid:9) and blend = 1 + π ), and the subsequent mean30 Implementation note. The reference implementation uses simpler absolute-cap formula α = min{1, (cid:112)ρ/ Var(W stack)} and omits the final isotonic re-projection (normalizing to mean one instead). This relaxation empirically yields similar ESS improvements but does not carry the deterministic guarantee of Lemma 5. Lemma 6 (Firm non-expansiveness of projections). Metric projections onto closed convex sets in Hilbert spaces are firmly non-expansive: ΠC(x) ΠC(y)2 ΠC(x) ΠC(y), y. Hence compositions of DbP modules (reward, weight, IF-space projections) are non-expansive. See, e.g., Bauschke & Combettes (2017, Prop. 4.16). Remarks on dependence. If logs exhibit serial or cluster dependence, the IF CLTs can be replaced by block/stationary bootstrap arguments; our reported intervals can include dependence-robust alternative (App. A.8). C.12 COVERAGE-LIMITED EFFICIENCY (CLE) DIAGNOSTIC We derive the CLE diagnostic stated in Equation (1). Let be target-relevant region with α = Pπ(T ) and β = Pπ0(T ). Lemma 7 (Weight second moment on ). For the normalized conditional distributions π and π0,T , Eπ0 (cid:2)W π 1T (cid:3) = α2 β (cid:0)1 + χ2(π π0,T )(cid:1). Proof. By definition of χ2 divergence for the normalized conditionals: χ2(π 1 = β α π1T ] 1. Rearranging gives the stated identity. α2 Eπ0[W 2 1 = β (dπ)2 dπ0 (cid:82) π0,T ) = (cid:82) (dπ )2 dπ0,T Proposition 4 (CLE diagnostic for IPS-style estimators). For any IPS-style estimator ˆΨIPS of (π) = Eπ[Y ] based on importance-weighted outcomes, SE( ˆΨIPS) σT α β (cid:113) 1 + χ2(π π0,T ), where σ2 := ess inf (x,a)T Var(Y X=x, A=a) is the minimal outcome noise in . Proof. We lower bound Varπ0 (WπY ) using the Law of Total Variance, conditioning on (X, A): Varπ0(WπY ) = Eπ0[Var(WπY X, A)] + Varπ0 (E[WπY X, A]). Since Wπ is (X, A)-measurable: Var(WπY X, A) = 2 π Var(Y X, A), E[WπY X, A] = WπE[Y X, A]. Dropping the non-negative second term: Restricting to (non-negative integrand) and applying Var(Y X, A) σ on : Varπ0(WπY ) Eπ0[W 2 π Var(Y X, A)]. Eπ0[W 2 π Var(Y X, A)] σ2 Eπ0[W π1T ]. Applying Lemma 7: Varπ0(WπY ) σ2 α2 β (1 + χ2). The asymptotic variance of IPS is Varπ0(WπY )/n; taking square roots yields the stated bound. Interpretation. The CLE diagnostic has three multiplicative factors: (i) the coverage penalty α/ β, which explodes when the logger rarely visits target-typical regions (β α); (ii) the shape mismatch (cid:112)1 + χ2, which inflates the floor even with good coverage if the target concentrates differently within ; (iii) the standard noise term σT / n. 31 Corollary (ESS form). Using ESS(Wπ) = n/(1 + CV2(Wπ)) and the relationship between χ2 and ESS on : SE( ˆΨ) σT"
        },
        {
            "title": "ESST",
            "content": ", where ESST is the effective sample size restricted to . This shows that high global ESS is insufficient: what matters is ESS in target-relevant regions. C.13 ASSUMPTIONS LEDGER For any CJE deployment, record and validate the following assumptions: A0 (Bridge Assumption). E[Y X, A] = E[Y X, A]. The operational oracle is sufficiently aligned with the idealized deliberation oracle such that optimizing for approximates optimizing for . Validation: Bridge Validation Protocol (BVP) with predictive transportability experiments, construct validity audits, and stability monitoring. S1 (Prentice Sufficiency). E[Y X, A, S] = (S, X) a.s. on supp(π0 Πeval). The surrogate captures all relevant information about the oracle given covariates X. Testable implication: Residuals ˆf (S, X) should be mean-zero and uncorrelated with conditional on (X, S). S2 (Transportability). The calibration function transports across environments G: Eg[Y X, A, S] = (S, X) for all (Pearl & Bareinboim, 2014). Graphical test: S-admissibility requires that no selection node points into given in the selection diagram with incoming arrows to removed. Testable implication: Calibration curves should be stable across (i) policy subgroups, (ii) time periods, and (iii) covariate strata. S3 (Overlap/Positivity). π(a x) > 0 π0(a x) > 0 a.s. Required for off-policy estimation. Diagnostic: Target-Typicality Coverage (TTC). If TTC < 0.7, expect logs-only IPS to fail. L1 (Oracle MAR). (X, A, S), where {0, 1} indicates oracle labeling. Oracle labeling is ignorable conditional on observed surrogates. Testable implication: Oracle-labeled and unlabeled samples should have similar (X, A, S) distributions. L2 (Oracle Positivity). (L = 1 X, A, S) > 0 on the support where calibration will be applied. Diagnostic: Check for gaps in oracle labeling across the (X, A, S) space. C.14 WHAT THE BOUNDS DO NOT INCLUDE The surrogate and budgeted information bounds describe model limits: they do not include (i) finitesample dispersion control from the sample cap (we encode it population-wise via ρ) or (ii) the oracle first-stage uncertainty (added separately by OUA). C.15 CITATIONS FOR TECHNICAL INGREDIENTS Semiparametric efficiency and one-step/TMLE: Bickel et al. (1993); van der Vaart & Wellner (2000); van der Laan & Rose (2011); Tsiatis (2006). Isotonic regression / PAVA and order-restricted inference: Ayer et al. (1955); Barlow et al. (1972); Robertson et al. (1988); Banerjee (2001). Majorization theory: Hardy et al. (1952); Marshall et al. (2011). Calibration for OPE: Kallus & Mao (2020); van der Laan et al. (2025; 2024). 32 Jackknife/bootstraps for dependence: Kunsch (1989); Politis & Romano (1994). Projections and non-expansive maps: Bauschke & Combettes (2017). DIAGNOSTICS, GATES, AND REPORTING (DETAILS) This appendix formalizes the diagnostics used in CJE, the associated ship/stop gates, and the reporting ledger. The main text shows compact panel per policy; here we provide precise formulas, defaults, and recommended thresholds. Unless stated otherwise, expectations and variances are empirical over the evaluation cohort (global SNIPS normalization). D.1 WEIGHT BEHAVIOR & OVERLAP Effective sample size (ESS). For nonnegative weights, Under global mean-one normalization (SNIPS), (cid:80) ESS(W ) = (cid:1)2 (cid:0) (cid:80) (cid:80) . Wi 2 Wi = n, so ESS(W ) = 1 1 + CV2(W ) , CV2(W ) = Var(W ) when E[W ] = 1. Report the ESS fraction ESS(W )/n and the multiplicative uplift ESS( ˆWπ)/ESS(W m1 π ). Max-weight share. maxi Wi 99.5th percentile of . (cid:14) (cid:80) Wj flags single-row dominance; display alongside the empirical Tail index (Hill) and CCDF. For the top-k order statistics W(1) W(k), ˆα1(k) = 1 (cid:88) j=1 log (cid:18) W(j) W(k) (cid:19) , K, swept over stability grid (e.g., 15% of n). Plot ˆα(k) with band over the plateau region (median and IQR over K), and the empirical CCDF of on loglog scale. Overlap in judge space. Let pSπ0 and pSπ denote the (binned) densities of under π0 and π: AB = (cid:88) (cid:112)pb(π0) pb(π), DB = log AB. Overlay an S-binned heatmap of log to localize regions of poor overlap. D.2 JUDGE CALIBRATION, COVERAGE, AND DRIFT Reliability diagram. Partition into bins; for bin b, plot the bin mean of = ˆf (Z(S)) against the oracle mean of , with 95% binomial intervals. Report Brier-style reliability term and the OOF RMSE, with 1-SE model-selection overlay (monotone vs. two-stage). Coverage badge. Estimate the fraction of evaluation mass outside the oracle range: OutOfRange = (cid:99)Prπ (cid:0)S < Sorc min or > Sorc max (cid:1). Also report boundary flatness (slope of ˆf in the lowest/highest oracle decile). Large OutOfRange together with flat boundaries triggers LIMITED CALIBRATION SUPPORT and the REFUSE-LEVEL gate. Rank drift (optional anchor). Given fixed anchor set of (X, A) pairs scored over time, compute Kendalls τ between historical and current judge rankings with permutation p-value. Change detection on residuals can be monitored via CUSUM/EWMA with FDR control across anchors. 33 D.3 DR ORTHOGONALITY AND DECOMPOSITION Orthogonality score. Let Ui = ˆWπ,i (cid:113) Wald CI for using the standard error and its CI; near-zero indicates successful orthogonality. ˆqOOF(Xi, Ai)(cid:1) and = n1 (cid:80) (cid:0)ROOF Ui. Form (cid:100)Var(U )/n (or cluster-/block-robust analogue). Report DMIPS decomposition. Display (cid:98)VDM = n1 (cid:80) the empirical correlation between their per-row contributions. ˆgπ(Xi) and (cid:98)VAug = n1 (cid:80) Ui, with CIs and D.4 UNCERTAINTY: IF VARIANCE AND OUA ADDITION For any estimator with centered IF contributions {ϕi}n i=1, (cid:100)Varmain = 1 (cid:100)Var(ϕi), (cid:100)Vartotal = (cid:100)Varmain + (cid:100)Varcal, with (cid:100)Varcal from the oracle jackknife (App. A.7). Report the OUA share (cid:100)Varcal/(cid:100)Vartotal and, optionally, dependence-robust alternative (below). Dependence-robust SEs. When time/cluster dependence is suspected, also report: (i) clusterrobust sandwich SEs when cluster id (e.g., session/user) is available; and (ii) block/stationary bootstrap intervals (block length chosen by simple variance-stability sweep) (Kunsch, 1989; Politis & Romano, 1994). D.5 MULTIPLICITY FOR MANY-POLICY COMPARISONS For contrasts = (cid:98)V (π p) (cid:98)V (π), compute Wald p-values and apply BH at level [0.05, 0.2]; BY can be used under strong dependence. Provide pairwise win matrix with FDR marks and Kendalls τ over policy means (all policies). D.6 POLICY-WISE MEAN TRANSPORT DIAGNOSTIC For each target policy π (or more generally, each evaluation environment such as time period or subgroup), we test whether the calibration function (S, X) learned on the base policy remains mean-unbiased. Test procedure. Given an oracle slice under policy π, compute the residual επ = (S, X) and test H0,π : Eπ[επ] = 0 vs H1,π : Eπ[επ] = 0 using one-sample t-test. When testing multiple policies, apply Bonferroni or BH correction across environments. Interpretation (Proposition 3). By Proposition 3, H0,π holds if and only if the surrogate-estimated policy value equals the oracle value: Eπ[f (S, X)] = Eπ[Y ]. Rejecting H0,π implies that surrogateonly evaluation is systematically biased for policy π: the calibration does not transport. OUA-corrected inference. To account for uncertainty in estimating , the confidence interval for the mean residual should incorporate OUA variance (Section 3.5). In practice, we refit (k) omitting each oracle fold and compute residuals under each refit, propagating calibration uncertainty into the test statistic. Actions on failure. If policy fails the mean transport test: (i) flag surrogate-only value estimates for that policy as biased; (ii) report the estimated bias ( ˆ = επ) and its direction; (iii) either recalibrate using policy-specific oracle data or fall back to oracle-only evaluation for that cell; (iv) rankings across policies that all pass may still be valid even if absolute levels are not. 34 Gate OVERLAP JUDGE DR MULTIPLICITY CAP Table 6: Default gates (suggested; tighten for high-stakes launches). Default Action if failed Reliability band covers diagonal at knots; no persistent drift alarms ESS/n 0.30 (3.3 var); Hill ˆα 2 (finite var); AB 0.85 (H0.39); TTC 0.70 Use overlap weights or cohort restriction; if TTC < 0.70, prefer Direct over IPS; report with warning if ˆα [1, 2); do not ship offline conclusions if ˆα < 1 Refresh/extend oracle slice; switch to two-stage index; re-validate Flag LIMITED CALIBRATION SUPPORT; set REFUSELEVEL: report rankings + partial-ID only Strengthen nuisances/cross-fitting; fall back to stabilized IPS as diagnostic Report adjusted p-values; avoid uncorrected winner claims If guard active on > 50% folds or sensitivity high, show cap curve and prefer overlap weights/restriction Orthogonality CI includes 0; no NaNs; residual tails acceptable Guard rarely engaged; CI width not sensitive to ρ FDR control applied when Π > 5 IDENTIFICATION OutOfRange η (default η=5%; worst-case bias η) or non-flat boundaries Table 7: Assumptions Ledger: Quick Reference. For each assumption, we list the estimators it applies to, how to test it, and recommended mitigations if violated. Assumption"
        },
        {
            "title": "If Violated",
            "content": "suffiOverlap (positivity) Mean ciency Monotonicity in Calibration coverage"
        },
        {
            "title": "Direct",
            "content": "IPS, DR"
        },
        {
            "title": "All",
            "content": "ESS/n 0.30; Hill ˆα 2; AB 0.85; TTC 0.70 Reliability diagram; residuals centered at zero regional AutoCal-R OOF model selection: monotone vs. two-stage; 1-SE rule OutOfRange 5% (worstcase bias 5%); non-flat boundaries Paired design check (same prompt set across policies) Residual test: E[Y (S, X)] = 0 per policy (Bonferroni) Overlap weights; cohort restriction; if TTC < 0.70, use Direct Two-stage fallback with covariates; refresh oracle slice Use two-stage with response length covariate REFUSE-LEVEL; targeted labeling in uncovered regions Cannot use Direct; must use OPE methods Recalibrate per policy; collect policy-specific oracle data D.7 GATES: THRESHOLDS AND ACTIONS Threshold calibration. The defaults in Table 6 are operationally calibrated for typical LLM evaluation regimes (n 5,000, MDE 0.02). Practitioners may derive context-specific values: (i) ESS/n bounds variance inflation to 1/f (e.g., =0.30 3.3); (ii) Hill α > 2 ensures finite weight variance, CLT precondition; (iii) AB maps to Hellinger distance via = 1 AB; (iv) OutOfRangeη bounds worst-case bias by η under bounded outcomes. For high-stakes applications, tighten thresholds or derive them from target MDE using the CLE bound (Equation (1)). REFUSE-LEVEL procedure. When IDENTIFICATION fails: (i) gray-out level estimates; (ii) highlight OutOfRange and boundary flatness; (iii) report rank-only conclusions with conservative relative CIs; (iv) recommend targeted labeling in uncovered regions. D.8 PLANNER: MDE AND LABEL/LOG BUDGETS Given two independent estimates with equal SE (cid:99)SE, the two-sided 95% test at 80% power has MDE80% = (z0.8 + z0.975) 2 (cid:99)SE. We tabulate (cid:99)SE versus (n, m/n) (labels per log) using Stacked-DR with OUA and annotate iso-cost lines for the label budget. D.9 REPORTING LEDGER (PER POLICY/COHORT) Persist: (i) calibrator mode, OOF risk by tertiles, knots/levels (hash); (ii) SIMCalW maps, stacking weights ˆβ, guard ρ and blend α; (iii) ESS fraction, max-weight share, Hill index band, AB; (iv) DR orthogonality score and CI; DMIPS split; (v) OUA trace { (cid:98)V (k)} and variance breakdown; (vi) 35 Algorithm 7 Gate logic (per policy) 1: Compute weight/tail metrics: ESS fraction, max-share, Hill band; compute AB; judge reliability/coverage; orthogonality score; OUA share REFUSE-LEVEL TRUE 2: if ESS/n < 0.30 or median Hill< 2 or AB < 0.85 then Flag OVERLAP (warn; restrict or use overlap weights) 3: 4: end if 5: if OutOfRange> η and boundary slopes 0 then 6: 7: end if 8: if Orthogonality CI excludes 0 then 9: 10: end if 11: if Cap engaged on > 50% folds or CI sensitivity to ρ high then Show capsensitivity; prefer overlap weights/restriction 12: 13: end if 14: Apply multiplicity control (BH/BY) when Π > 5 Flag DR; strengthen nuisances/cross-fitting filter counts (e.g., TF gaps) and an inclusion manifest of x-ids; (vii) multiplicity control (family, q, adjusted p). D.10 VISUALIZATION PRIMITIVES (FOR REPRODUCIBLE PANELS) ESS/tails strip: bars for ESS fraction (baseline vs. SIMCalW); dot for max-weight share; Hill band. S-overlap heatmap: density of under π0 vs. π with overlaid log ; annotate AB. Reliability panel: bin means of (R, ) with 95% CIs; mode card (monotone vs. two-stage; OOF RMSE). Orthogonality panel: point/CI for ; DMIPS bars with CIs and correlation. Uncertainty ring: pie of (cid:100)Varcal/(cid:100)Vartotal (OUA share). D.11 OPTIONAL: DEPENDENCEROBUST IMPLEMENTATION DETAILS Cluster-robust SEs: if cluster id c(i) is available, (cid:100)VarCR = n2 (cid:80) , with finite-sample correction. Stationary bootstrap: sample blocks of geometric length ℓ Geom(p) glued to length n; form the bootstrap distribution of ˆψ (or of n1/2 (cid:80) ϕi) and report percentile or t-based bands. ic ϕi ic ϕi (cid:1)(cid:0) (cid:80) (cid:0) (cid:80) (cid:1) D.12 COMPACT GATE PSEUDO-LOGIC IMPLEMENTATION, ENGINEERING, AND REPRODUCIBILITY This appendix enumerates the concrete artifacts needed to reproduce CJE endtoend: minimal logging schema, teacherforcing (TF) contract with conformance checks, fold construction, numerics, persisted outputs, and lightweight resource model. No packages beyond the ICLR style file are required. E.1 MINIMAL LOGGING SCHEMA (STORAGEAGNOSTIC) Each row corresponds to one promptcontinuation pair under the fixed logger π0. We persist only what is necessary to reconstruct SNIPS/IPS weights and judge scores. 36 Table 8: Columns required for CJE. Columnar formats (Parquet) are convenient but not required. Field Type Description Stable identifier (hash of normalized prompt + cohort) string bytes/string Canonicalized (tokenizer + normalization recorded) id prompt continuation bytes/string Realized under π0 (full sequence) tokens logp pi0 judge judge cfg run cfg fold id cohort Token ids for under each models TF tokenizer Pertoken log pπ0(at ht) across Scalar judge score = s(X, A) (or struct of subscores) Judge rubric, decoding params, model snapshot hash π0 engine tag, decoding params, checkpoint hash, seed Deterministic fold (F (x id); see E.3) Optional slice label (time window, traffic source, etc.) int[] float[] float/json json json int string TF cache (per target π). separate table stores, for each (x id, π): logp pi prime, logW= log pπ log pπ0, and m1 π = exp (cid:16) logW logsumexp(logW) + log (cid:17) , i.e., single global denominator that enforces samplemeanone. Rows with missing/invalid TF are filtered and recorded in ledger. E.2 TEACHER FORCING: CONTRACT AND CONFORMANCE We require singlecall, chatnative TF API that returns pertoken and summed log pπ(A X) under fixed template, tokenizer, and snapshot. Clientside checks: Determinism. identical calls for the same (X, A, π@SNAPSHOT, template) must be bitidentical (tolerance < 107). Additivity. Also return log pπ(X) and log pπ(X+A) and verify log pπ(X+A) log pπ(X) + log pπ(A X) log pπ(X). Violation discard row (and log it). Template/tokenizer provenance. Return immutable hashes; reject moving aliases. Masking. If safety masks are applied, return mask bits and renormalized flag; prefer an evaluationonly path without hidden renormalization. Conformance snippet (pseudo). lp = TF(model_id, template_id, X, A) assert same_bits(lp.sum, sum(lp.per_token), tol) lpX, lpXA = TF_logp(X), TF_logp(X+A) assert abs(lpXA - (lpX + lp.sum)) < eps and lp.sum <= 0 E.3 FOLDS AND CROSSFITTING We use K=5 folds by default. The fold map (i) is stable hash of id modulo K, ensuring that all modules (AutoCalR, SIMCalW, DR nuisances) share identical OOF boundaries. Oracle folds are derived by intersecting (i) with Li=1. The hash rule is (x id)=hash(x id) mod 5. E.4 NUMERICS AND STABILITY Ratios in logspace. Keep log until forming the global meanone normalization; use single logsumexp for the denominator. Center residuals. For stacking objectives and covariances, center and drop NaNs/inf at ingestion. Variance estimates. Use Welfords online formulas for high dynamic range; add tiny ridge (λ [1010, 106]) to covariance matrices. 37 PAVA. Run once per fold after sorting by S. The reference implementation enforces mean-one via multiplicative rescaling (W/ ), which preserves nonnegativity but may slightly distort isotonicity at extremes. Note: The exact constrained projection IsoMeanOneS defined in Section A.3 preserves both monotonicity and mean-one simultaneously; the deterministic majorization guarantee strictly applies only to that exact operator. We empirically observe similar ESS improvements with multiplicative rescaling. Guard (absolute variance cap). Default ρ=1; compute α = min(cid:8)1, (cid:112)ρ/ Var(W stack)(cid:9); blend and normalize to meanone. Persist whether the guard engaged. Note: This is an absolute cap (Var ρ); the theoretical Lemma 5 uses relative bound (Var ρ Var(W m1)). Reference code note. minimal reference implementation uses global isotonic fits and computes the stacking covariance on those same in-sample fits (no OOF); production code should use cross-fitting as described above. E.5 PERSISTED ARTIFACTS (PER POLICY/COHORT) Calibrator: mode (monotone vs. twostage), OOF RMSE (overall + tertiles), knots/levels (hash), OOF vs. pooled predictions. Weights: isotonic merge metadata, orientation (up/down), stacking weights ˆβ including the identity/baseline candidate, guard ρ and blend α, final meanone check. Estimators: point estimates, centered IF vectors hashes, (cid:100)Varmain, orthogonality score and CI, dependencerobust SEs (if used). OUA: { (cid:98)V (k)}K Diagnostics: ESS fraction, maxweight share, Hill band, Soverlap (AB), coverage badge, gate k=1, (cid:100)Varcal, (cid:100)Vartotal. statuses. Ledger: counts by filter reason (TF gaps, moderation, timeouts), id inclusion manifest. E.6 REFERENCE RUN ORDER (PSEUDOCODE) # 0) Build TF cache for each pi (one pass per policy) build_tf_cache --policies <list> --dataset logs.parquet --out tf_cache.parquet # 1) Reward calibration (cross-fitted; auto monotone vs two-stage) autocal_r --oracle oracle.parquet --folds 5 --out rewards.parquet # 2) SIMCal-W per policy (OOF project->stack->cap->translate-to-mean-one) simcal_w --tf-cache tf_cache.parquet --scores S.parquet --rho 1.0 --folds 5 --out weights.parquet # 3) Estimation + IFs (Cal-IPS / DR-CPO) estimate --rewards rewards.parquet --weights weights.parquet --folds 5 --out estimates.parquet # 4) IF-Stack (optional) stack --estimates estimates.parquet --out stacked.parquet # 5) OUA jackknife oua --oracle-folds 5 --pipeline-config cfg.yaml --out variance.parquet # 6) Report (diagnostics, gates, CIs) report --inputs *.parquet --figs figs/ --out report.html E.7 COMPUTE AND RESOURCE MODEL Let be prompts, the mean continuation length, and Π the number of candidate policies. 38 TF cache. O(Π ) forward tokens; microbatch by length; nearlinear scaling across GPUs. SIMCalW. O(n log n) for sort + O(n) for PAVA per fold; covariance/stacking are tiny (33). DR-CPO. If ˆgπ uses one rollout per (X, π), add O(Π ) tokens once; light smoother amortizes Monte Carlo noise. OUA. refits of AutoCalR and reruns of the pipeline; cache features to avoid recomputation. E.8 DETERMINISM, VERSIONING, AND PRIVACY Determinism: fix seeds at engine, dataloader, and sampler; record random states in run cfg; serialize fold maps (internal). Versioning: record immutable hashes for model weights, tokenizer, and template; pin checkpoints. Privacy: encrypt prompts/continuations at rest; hash id with salt; public artifacts include only aggregates/diagnostics and redacted IDs. E.9 EXACT PROMPTS USED IN EXPERIMENTS Policy system prompts. All policies use Llama 3.3 70B except premium (Llama 3.1 405B), served via Fireworks AI (https://fireworks.ai). Temperature = 0.7 for all; no top-p/top-k truncation, so TF logprobs represent the exact sampling distribution. base, clone, premium: You are helpful assistant. parallel universe prompt: Imagine parallel universes where you vary your responses and can observe which one improves the users life the most. Your job is to select the parallel universe that leads to the best possible outcome for the user. Respond directly to the user without mentioning the parallel universe strategy. unhelpful: You are an unhelpful assistant that deliberately confuses and misleads the user. Judge and oracle evaluation prompts. Both the judge (GPT-4.1-nano) and oracle (GPT-5) use identical prompts; the only difference is the underlying model. Temperature = 0.0 for judge, 1.0 for oracle (model constraint). System prompt: You are an AI evaluator. Rate responses from 0 to 100. Always provide score, even if the response is incomplete or truncated. User prompt template: <task> Evaluate the quality of the AI assistants response to the users question. Score from 0-100 based on relevance, helpfulness, clarity, and completeness. If the response is truncated, score based on what is provided. </task> <exchange> <question>{prompt}</question> <answer>{response}</answer> </exchange> <instruction> Provide your evaluation score (0-100): </instruction>"
        },
        {
            "title": "F EFFICIENT BUDGET ALLOCATION",
            "content": "While CJE provides calibration-aware confidence intervals for any sample configuration, practitioners often face constrained optimization problem: minimizing estimation error subject to fixed financial or latency budget. Here we derive the optimal allocation between cheap surrogate scores (n) and expensive oracle labels (m). 39 Proposition 5 (Square Root Allocation Law). Let cS and cY be the marginal costs of surrogate scoring and oracle labeling, respectively. Assume the variance decomposes as Vtotal(n, m) σ2 eval + σ2 cal , = cSn + cY m. The allocation (n, m) minimizing Vtotal subject to budget satisfies: n = (cid:114) cS cY (cid:115) σ2 cal σ2 eval with the feasibility constraint n (the oracle slice is subset of scored examples). If the unconstrained optimum exceeds n, set = n. Note: This first-order approximation assumes additive variance components with O(1/m) calibration variance. While isotonic regression has nonparametric rates for function estimation, the mean functional E[f (S)] typically achieves cal (cid:100)Varcal and σ eval (cid:100)Vareval from the OUA decomposition, then apply the formula. rates under smoothness. In practice, estimate σ2 Closed-form solution. Solving the constrained optimization yields: (cid:112)σ2 cal/cY eval + (cid:112)cY σ2 eval/cS eval + (cid:112)cY σ2 (cid:112)cSσ (cid:112)cSσ2 (cid:112)σ2 = = cal , cal . Proof. We form the Lagrangian for minimizing variance subject to cost B: L(n, m, λ) = σ2 eval + σ2 cal + λ(cSn + cY B) Taking partial derivatives with respect to and yields the first-order conditions: n = m Equating the expressions for λ: = eval σ2 n2 + λcS = 0 = λ = σ2 cal m2 + λcY = 0 = λ = σ2 eval cSn2 σ2 cal cY m2 eval σ2 cSn2 = σ2 cal cY m2 = m2 n2 = σ2 cal σ2 eval cS cY Taking the square root yields the ratio result. Substituting into the budget constraint and solving gives the closed-form expressions. The OUA Marginal Utility Diagnostic. The optimality condition implies that resources are allocated efficiently when the marginal variance reduction per dollar is equal across both channels. We can express this in terms of the observed OUA Share ω = Varcal / Vartotal. At the optimum, the Spend-Balance Rule holds: ω = cY cSn + cY = Spendoracle Spendtotal . That is, the variance share from calibration should equal the budget share spent on oracle labels. Deviations diagnose inefficiency: Under-labeled (Invest in m): If ω > cY cS n+cY , calibration uncertainty dominates relative to its share of the budget. Action: Shift budget to acquire more oracle labels. Over-labeled (Invest in n): If ω < cY cS n+cY , evaluation sampling noise dominates. Action: Shift budget to evaluate more prompts with the surrogate. 40 Worked Example (Arena Benchmark). Using costs from our experiments: the GPT-4.1-nano judge is approximately 16 cheaper than the GPT-5 oracle, giving cost ratio cS/cY 0.064. At = 1000 prompts with = 50 oracle labels (5% oracle fraction), the observed OUA share is ω 55%. To find the intrinsic variance ratio, note that ω = σ2 cal/m cal/m+σ2 σ eval/n , which implies: σ2 cal σ2 eval = ω (1 ω) = 0.55 50 0.45 1000 = 0.061 Applying the Square Root Law: n = 0.064 0.061 0.25 0.25 = 6.2% The empirical 5% oracle allocation is close to the variance-optimal 6.2%, confirming that the studys cost-effective design did not substantially sacrifice precision. Extension: Multi-Policy Amortization. When calibrating once to evaluate policies on the same prompt set, surrogate costs scale as cS while oracle costs remain cY m. The optimal ratio becomes m/n = (cid:112)P cS σ2 cal/(cY σ2 eval), favoring larger oracle slices as grows (calibration cost amortizes across policies, making it relatively cheaper per evaluation)."
        }
    ],
    "affiliations": [
        "CIMO Labs"
    ]
}