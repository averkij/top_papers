{
    "paper_title": "NVIDIA Nemotron Parse 1.1",
    "authors": [
        "Kateryna Chumachenko",
        "Amala Sanjay Deshmukh",
        "Jarno Seppanen",
        "Ilia Karmanov",
        "Chia-Chih Chen",
        "Lukas Voegtle",
        "Philipp Fischer",
        "Marek Wawrzos",
        "Saeid Motiian",
        "Roman Ageev",
        "Kedi Wu",
        "Alexandre Milesi",
        "Maryam Moosaei",
        "Krzysztof Pawelec",
        "Padmavathy Subramanian",
        "Mehrzad Samadi",
        "Xin Yu",
        "Celina Dear",
        "Sarah Stoddard",
        "Jenna Diamond",
        "Jesse Oliver",
        "Leanna Chraghchian",
        "Patrick Skelly",
        "Tom Balough",
        "Yao Xu",
        "Jane Polak Scowcroft",
        "Daniel Korzekwa",
        "Darragh Hanley",
        "Sandip Bhaskar",
        "Timo Roman",
        "Karan Sapra",
        "Andrew Tao",
        "Bryan Catanzaro"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "We introduce Nemotron-Parse-1.1, a lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0. Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports a longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including a compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it a strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with a subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on a reduced vision token length, offering a 20% speed improvement with minimal quality degradation."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 2 ] . [ 1 8 7 4 0 2 . 1 1 5 2 : r 2025-11NVIDIA Nemotron-Parse 1."
        },
        {
            "title": "NVIDIA",
            "content": "Abstract. We introduce Nemotron-Parse-1.1, lightweight document parsing and OCR model that advances the capabilities of its predecessor, Nemoretriever-Parse-1.0 (Karmanov et al., 2025). Nemotron-Parse-1.1 delivers improved capabilities across general OCR, markdown formatting, structured table parsing, and text extraction from pictures, charts, and diagrams. It also supports longer output sequence length for visually dense documents. As with its predecessor, it extracts bounding boxes of text segments, as well as corresponding semantic classes. Nemotron-Parse-1.1 follows an encoder-decoder architecture with 885M parameters, including compact 256M-parameter language decoder. It achieves competitive accuracy on public benchmarks making it strong lightweight OCR solution. We release the model weights publicly on Huggingface, as well as an optimized NIM container, along with subset of the training data as part of the broader Nemotron-VLM-v2 dataset. Additionally, we release Nemotron-Parse-1.1-TC which operates on reduced vision token length, offering 20% speed improvement with minimal quality degradation. 1. Introduction In the recent years, document-level OCR has developed beyond simple extraction of plain text characters from an image. Modern applications such as Large Language Models, retrieval systems, question-answering solutions, demand richer representation, which includes layout, reading order, semantic classes (such as captions or footnotes), formulas, tables, and understanding of multicolumn/multi-page structure. number of models were introduced in recent years, ranging from pipeline-based solutions to end-to-end Vision-Language models (Feng et al., 2025; Marker, 2025; Mathpix, 2025; Wang et al., 2024; Li et al., 2025; Cui et al., 2025; Blecher et al., 2024; Nassar et al., 2022, 2025; Poznanski et al., 2025; Wei et al., 2024; ocr, 2025; Rednote). Pipeline solutions often rely on brittle multi-stage pipelines with each stage responsible for subtask of the overall document extraction process, hence achieving versatility at the cost of the lower throughput. At the same time, end-to-end models benefit from fast inference speeds while often not performing equally well on all subtasks associated with document extraction simultaneously, i.e., general OCR, structured text formatting, mathematical equations, extraction of complex tables, prediction of bounding boxes and semantic classes of text blocks. To close this gap, we introduce Nemotron Parse 1.1, successor to Nemoretriever-Parse 1.0 and Eclair (Karmanov et al., 2025), which is an end-to-end vision-language model capable of extracting formatted text (Markdown/LaTeX), bounding boxes of text blocks, and semantic classes for each block while preserving the reading order. Beyond the full-capability Nemotron Parse 1.1 model, we also introduce Nemotron Parse 1.1 with token compression (TC), streamlined variant designed for applications that prioritize speed without majorly sacrificing output quality. Nemotron-ParseTC maintains nearly the same level of accuracy as the full model, while offering reduced latency and lower computational overhead. This makes it particularly well-suited for large-scale batch processing, edge deployments, or interactive systems where rapid response times are critical. Despite 2025 NVIDIA. All rights reserved. NVIDIA Nemotron-Parse 1.1 its lightweight architecture, Nemotron-Parse-TC preserves support for core document understanding featuresincluding layout-aware text extraction, semantic block classification, and consistent readingorder generationproviding an efficient alternative for production environments that demand both performance and precision. Additionally, Nemotron-Parse-TC offers an improved reading order, with all page elements, including floating ones, following the page ordering. 2. Nemotron-Parse 2.1. Model Architecture Nemotron-parse follows an encoder-decoder transformer architecture. The vision encoder, denoted as ‚Ñ∞, is initialized from RADIO (Ranzinger et al., 2024; Heinrich et al., 2025) which follows ViT-H /16 (Dosovitskiy et al., 2021) architecture (657M parameters), and maps an image R3ùêªùëä to latent representation RùëÅ ùëë, where ùëë is the hidden dimension and ùëÅ is the sequence length. The vision neck ùí© consisting of horizontal convolutional kernels of size 1 4 and stride 1 4 then reduces the dimensionality of the latent space as well as the sequence length. For an input image of 1648 2048 this reduces the sequence length to 3200. We additionally concatenate the summary token of RADIO to the sequence. For Nemotron-Parse-TC, we additionally apply pixel-shuffle on top of the compressed sequence, further reducing the sequence length to 833 tokens, hence achieving total of 16 reduction. The decoder, denoted as ùíü, uses mBART (Liu et al., 2020) architecture reduced to 10 layers and with tied wights, and predicts text tokens = {ùë°ùëÉ +1, ùë°ùëÉ +2, . . . , ùë°ùêø} by conditioning on the latent encoder representation, ùí© (Z), and the context ùë°<ùëñ, ùëÉ (ùë°ùëñùí© (Z), ùë°<ùëñ), where = ‚Ñ∞(I) and {ùë°1, ùë°2, . . . , ùë°ùëÉ } are the prompt tokens and where ùêø is the prompt-augmented sequence length. The model has 885M parameters in total. 2.1.1. Positional embeddings To enable large-context inference, we train and evaluate the model without positional embeddings in the decoder. We find that the network achieves comparable accuracy to models trained with positional embeddings, while allowing inference with significantly longer context lengths. In the design of Vision-Language Models (VLMs) for OCR, our decision to omit positional embeddings in the LLM decoder is motivated by the ability of decoder-only transformer architectures to implicitly encode position. While transformers are often described as permutation-invariant and thus dependent on positional embeddings, in causal decoder-only models the attention mask already provides positional cues: each token can only attend to preceding elements, which enables the model to infer its location in the sequence (Kazemnejad et al., 2023; Zuo et al., 2025). This simplifies the architecture by removing additional positional parameters and aligns with the multimodal nature of OCR, where visual tokens from the image encoder carry spatial structure relative to the document layout. Without an extra 1D positional signal, the decoder avoids possible interference between sequence-based embeddings and the 2D spatial information already present in the visual features. For OCR, this reduced reliance on explicit positional encoding can improve generalization and computational efficiency across different document lengths, from short receipts to long academic pages with complex tables. No Positional Encoding (NoPE) approaches have been reported to generalize better to longer sequences, since they avoid interpolation issues that arise with explicit embeddings (Kazemnejad et al., 2023; Zuo et al., 2025). 2 NVIDIA Nemotron-Parse 1.1 2.1.2. Multi-token inference Autoregressive models, including those used for OCR, operate by decoding one token at time, leading to slow inferenceespecially for text-dense images. To address this, we repurpose our solution from Nemotron-Parse for multi-token generation, predicting ùëõ tokens simultaneously (Gloeckle et al., 2024). During training, for predicting ùëö tokens we add ùëö 1 2 additional linear layers. We adopt simple architecture, where given the context of size ùëõ, the logits for ùëõ + 1ùë†ùë° token are obtained following standard architecture from hùëõ, i.e., the final hidden state of the ùëõùë°‚Ñé token, and for subsequent ùëõ + 2..ùëö tokens is obtained as lhead(l1 (hn + l2 (en+1))), where en+1 is the embedding of ùëõ + 1ùë°‚Ñé token predicted by the preceding head, l1 and l2 are Linear layers, and lhead refers to the decoder head. During training, we use teacher forcing for token embeddings of additional ùëõ + 2..ùëö tokens. At inference, decoding proceeds greedily without token verification. We find that adoption of the multi-token training strategy additionally allows to achieve improved accuracy in the default single-token inference setup, compared to the models trained with standard protocol. 2.2. Prompts and Output Format 2.2.1. Input prompts Similarly to Eclair (Karmanov et al., 2025), we train the model jointly on heterogeneous datasets that provide different supervision signals (plain or formatted text, bounding boxes, and semantic classes). To unify these sources, we use fixed prompt interface and assign, for each training sample, the prompt that exactly matches the annotations available in its dataset. This lets the model learn single conditional interface while leveraging all datasets efficiently. At the core are three independent prompt tokens that define the requested outputs, yielding the eight valid combinations used in training and inference: Text formatting prompts <output_markdown>: text is formatted as Markdown, and formulas and Tables are formatted as LaTeX. Inline formulas that do not require any LaTeX syntax so be represented (e.g., consisting only of characters and subscripts/superscripts) remain in markdown format for versatility. <output_plain>: emit unformatted text; inline formulas are plain text. <output_no_text>: output no text. Bounding Box prompts <predict_bbox>: return bounding boxes for detected elements. <no_bbox>: output no bounding boxes. Class prompts <predict_classes>: return semantic class labels for each box. This option is used only together with <bbox>. <no_classes>: suppress class labels. During training, we map each datasets label set to compatible prompt, ensuring only valid combinations are used (we exclude the trivial no output case and any request for classes without boxes). During inference, we define the maximal-information prompt (MIP) as: 3 NVIDIA Nemotron-Parse 1. < output_markdown > < predict_bbox > < predict_classes > , i.e., prediction of formatted text, bounding boxes, and semantic classes. 2.2.2. Output format Nemotron-Parse predicts bounding boxes of the semantic blocks in the form of relative coordinates, in scale of 1024 1280. These bounding boxes are predicted in canonical reading order, that includes Page-Header elements at the start of the page, followed by Text, Section-Header, List-Item, Title and Formula elements in the order as they would be read by person looking at the given page, followed by Footnotes, Page-Footers, Tables, Pictures, and Captions in the end. Nemotron-Parse-TC improves upon this canonical reading order, and also includes non-reading-order (floating) elements, i.e., Footnotes, Page-Footers, Tables, Pictures, and Captions within the natural ordering of the page. In the maximal information setting the output is in the following format: <x_(d+)><y_(d+)>(.*?)<x_(d+)><y_(d+)> <class_([^>]+)> where the the first group denotes the coordinates of the top-left corner, the second group denotes the text contained within the bounding box, the third group denotes the coordinates of the bottom-right corner, and the final group represents the semantic class. For example, <x_0.1152><y_0.2586># NVIDIA Nemotron-Parse 1.1<x_0.8799><y_0.2797> <class_Title>. 3. Training and Data 3.1. Datasets Nemotron-Parse is trained on combination of synthetic, public, and human-annotated data. The overview of the data used for training is provided in Table 1 and we provide brief descriptions of internally curated and synthetic data below. NVpdftex pipeline At the core of the data utilized for training the Nemotron-Parse, we build data generation pipeline inspired by Nougat (Blecher et al., 2024). Unlike their pipeline which builds supervision by turning arXiv papers in LATEX format into HTML with LaTeXML and then into markdown, our pipeline couples LATEX compilation with structured-output extraction in single pass, preserving tight alignment between the rendered page and the text down to character-level bounding boxes and enabling per-box semantic labels. We extend the open-source TEX Live toolchain which allows us to intercept node and character creation, hbox/vbox allocations, token reads, and page output events, leading to preservation of accurate bounding boxes, semantic classes, and reading order. Using this integrated method, we produced high-quality large-scale ground-truth corpus of documents referred to as NVpdftex. Additionally, we open-source the generation pipeline for the community https://github.com/NVIDIA-NeMo/Curator/tree/experimental/experimental/nvpdftex. To improve multilingual capabilities of Nemotron-Parse we generate additional data by applying machine translation to NVpdftex dataset in 6 languages. We also employ LaTeX-level augmentations of fonts, color, and layout to increase the diversity of the datasets. 4 NVIDIA Nemotron-Parse 1.1 Dataset/source Size Modality Languages Multilingual arXiv 8.3M Structured, Boxes, Classes SynthTabNet (Nassar et al., 2022) DocLayNet (Pfitzmann et al., 2022) Common Crawl samples Synthetic tables Multilingual Synthetic OCR data Multilingual Wikipedia OCR data Pubtables (Smock et al., 2022) Fintabnet (FinTabNet-dataset, 2024) TabRecSet (Yang et al., 2023) 480K Structured, Boxes, Classes 56K Plain+Structured, Boxes, Classes 255k 26K Plain+Structured, Boxes, Classes Structured, Boxes, Classes 3.5M Plain+Structured, Boxes, Classes 9.5M Structured, Boxes, Classes English, Chinese, German Spanish, French, Italian Japanese English English Various English English, Chinese, Japanese, Korean, Latin, Greek English, French, German, Spanish, Italian, Dutch, Portugese, Japanese, Korean, Chinese 585K Structured, Boxes, Classes 91.5K Structured, Boxes, Classes English English 38.2K Structured, Boxes, Classes English, Chinese Table 1 Summary of the datasets used to train Nemotron-Parse, including description of the maximum information available in the annotations of each dataset. DocLayNet DocLayNet (Pfitzmann et al., 2022) is public dataset used for layout analysis. On top of the existing annotations, we additionally autolabel the reading order of the text inside the DocLayNet samples, text inside images, as well as markdown formatting, and formatting of Table objects and Formulas. The resulting blend includes combination of variants of different output formats, including both plaintext and markdown/LaTeX. Common Crawl Data We utilize set of diverse data samples from Common Crawl which are annotated in plaintext format along with bounding boxes and semantic class labels by human experts. We further autolabel text inside images and markdown formatting to increase the format diversity of the data. In case of autolabeling, structured formatting labels are derived by treating each individual bounding box crop as an individual image and running inference with stage-1 trained Nemotron-Parse. The low-quality predictions are filtered out and blanked on the image on the basis of edit distance of the formatting-stripped output to the plaintext labels. Page-level formatting, such as multi-level headers, are obtained by running full-page inference and aligning global header formatting with individual text blocks heuristically. Synthetic tables To ensure strong table extraction performance, significant portion of table data is synthetically generated with various styles and layouts. The data is generated in HTML format, and further converted to LaTeX, and rendered on page to produce corresponding image. We capture various layouts, text formatting, sparsity levels, presence of checkboxes, etc. Multilingual dense OCR data Upon observation that oftentimes models struggle with dense OCR, we synthetically generate dense text of multiple languages and render it on the image. This includes random words, characters, and symbols, in 6 different languages. 5 NVIDIA Nemotron-Parse 1.1 Method Kosmos-2.5 (Lv et al., 2023) (ocr-mode) Kosmos-2.5 (Lv et al., 2023) (md-mode) GOT (Wei et al., 2024) (ocr-mode) GOT (Wei et al., 2024) (md-mode) Nemotron-Parse-MIP Nemotron-Parse-MIP Nemotron-Parse-TC-MIP Nemotron-Parse-TC-MIP Mask out WER F1 0.937 0.195 + 0.890 0.249 + 0.818 0.302 + 0.879 0.259 + - + - + 0.109 0.102 0.111 0.121 0.958 0.957 0.953 0.949 Table 2 Evaluation results on an internal test set. Multilingual Wikipedia OCR data An additional source of multilingual data is obtained from Wikipedia text in multiple languages that is converted to LaTeX formatting and augmented with font, background, and color augmentations. Pubtables, Fintabnet, TabRecSet We additionally utilize publicly available table extraction datasets (Smock et al., 2022; FinTabNet-dataset, 2024; Yang et al., 2023) where we convert the tables from HTML to LaTeX format and autolabel the remaining elements (if present) in the pages to follow the Nemotron-Parse format. large portion of the synthetic and human-labeled datasets have been released as part of the Nemotron-VLM-Dataset-V2 release. 4. Experimental results 4.1. OCR and reading order evaluation We assess reading-order accuracy of Nemotron-Parse on an internally curated, human-labeled set of 789 PDF pages (Karmanov et al., 2025) drawn from magazines, books, and the Common Crawl corpus (Sebastian Spiegler, 2013). This test set reflects the layout diversity seen in DocLayNet (Pfitzmann et al., 2022), an human annotators followed DocLayNets labeling scheme, with key addition of the explicit reading-order annotations. Using this benchmark, we compare Nemotron-Parse against baselines Kosmos-2.5 (Lv et al., 2023) and GOT (Wei et al., 2024) and report the results in Table 2. We normalize the outputs of all the methods and exclude tables, equations, and TEX commands from evaluation in this benchmark. Additionally, for GOT (md), we also mask-out headers and footers from the images, as their model seems to ignore these elements. Because both baselines offer two output modesplain OCR and markdownwe evaluate Nemotron-Parse against each of these modalities. Next, we compare Nemotron-Parse-1.1 to other popular OCR solutions on the GOT benchmark (Wei et al., 2024). The results are reported in Table 3. As can be seen, Nemotron-Parse shows strong performance, being outperformed only by Gemini Flash 2.0. Further, we evaluate Nemotron-Parse on widely-adopted OmniDocBench (v1.0) benchmark and present results in Table 4. We report the results on English subset. For the formula metrics, we should note that since Nemotron-Parse outputs text in markdown format, simple mathematical equations not requiring specialized LaTeX commands (e.g., consisting of purely subscripts and 6 NVIDIA Nemotron-Parse 1.1 Extractor OCR/F1 Score Text-Only RO/ Edit Dist. Text-Only RO/ METEOR Text-Only RO/ BLEU Pdfium Docling Gemini Flash 2.0 Mistral LandingAI Document Agent Marker SmolDocling Nemotron-Parse-1.1 Nemotron-Parse-1.1-TC 0.0036 0.6744 0.9915 0.9729 0.9524 0.9696 0.9588 0.9785 0.9755 0.9993 0.4300 0.0125 0.0189 0.0699 0.0322 0.0352 0.014 0.014 0.0083 0.6331 0.9934 0.9831 0.9428 0.9795 0.9728 0.9858 0. 0.0000 0.4651 0.9828 0.9560 0.8945 0.9451 0.9343 0.9623 0.9582 Table 3 OCR metrics on GOT (Wei et al., 2024) benchmark superscripts as additional formatting) would be represented by markdown text rather than enclosed in LaTeX math environment delimiters, resulting in their penalization in formula category of OmniDocBench, even in cases of correct representation via markdown. As can be seen, NemotronParse and Nemotron-Parse-TC achieve competitive performance, showing strong overall accuracy, and in particular accuracy on tables and reading order metrics, outperforming competing methods in the same size/vision token count category. We note that thanks to vastly improved reading order of Nemotron-Parse-TC, it outperforms the base Nemotron-Parse on this benchmark overall, while having only minor losses in other sub-categories. 4.2. Table extraction Nemotron-Parse and Nemotron-Parse-TC exhibit strong table performance compared to similarlysized and even larger models, as seen from the OmniDocBench results. We additionally report TEDS and S-TEDS on several public table benchmarks in Table 5. Since Nemotron-Parse predicts tables in LaTeX format, we convert the predicted tables to HTML or Markdown where necessary. Further, we compare Nemotron-Parse to other competing methods on public RD-TableBench (Reducto, 2024) benchmark which consists of collection of diverse in-the-wild tables. We report the results in Table 6. As can be seen, Nemotron-Parse achieves competitive performance, being outperformed only by Reducto. 4.3. Multilingual OCR Nemotron-Parse supports variety of European languages, as well as other languages in limited domains (due to the nature of the data sources in the training blend for Chinese, Japanese, and Korean we find Nemotron-Parse to perform well in the scientific domain as well as standard pdf documents, with limited support for in-the-wild images/documents in these languages). To assess the multilingual capability of Nemotron-Parse, we evaluate it on set of multilingual documents obtained via NVpdftex pipeline (Nvidia, 2025). Test set of each language consists of 10,000 dense documents from scientific domain, with font and color augmentations ensuring the document diversity, and we report plaintext WER and F1 in Table 7. As can be seen, Nemotron-Parse achieves competitive F1 > 0.96 on all languages, with accuracy on English achieving 0.98. 7 NVIDIA Nemotron-Parse 1.1 Model Tokens overall text English formula table order Dolphin (Feng et al., 2025) Marker (Marker, 2025) Mathpix (Mathpix, 2025) MinerU-2.1.1 (Wang et al., 2024) MonkeyOCR-1.2B (Li et al., 2025) PPstructure-v3 (Cui et al., 2025) Pipeline Models - - - - - - End-to-end Models Nougat (Blecher et al., 2024) SmolDocling (Nassar et al., 2025) InternVL2-76B (Chen et al., 2024) Qwen2.5-VL-7B (Bai et al., 2025) OLMOCR (Poznanski et al., 2025) GOT-OCR2.0 (Wei et al., 2024) OCRFlux-3B (ocr, 2025) GPT4o (OpenAI, 2023) InternVL3-78B (Zhu et al., 2025) Qwen2.5-VL-72B (Bai et al., 2025) dots.ocr (Rednote) Gemini2.5-Pro (AI, 2025) MinerU2.0 (Wang et al., 2024) dots.ocr200dpi (Rednote) DeepSeek-OCR-Tiny (Wei et al., 2025) DeepSeek-OCR-Small (Wei et al., 2025) DeepSeek-OCR-Base (Wei et al., 2025) DeepSeek-OCR-Large (Wei et al., 2025) DeepSeek-OCR-Gundam (Wei et al., 2025) DeepSeek-OCR-Gundam-M200dpi (Wei et al., 2025) Nemotron-Parse Nemotron-Parse-TC 2352 392 6790 3949 3949 256 3949 - 6790 3949 3949 - 6790 5545 64 100 256(182) 400(285) 795 1853 3201 833 0.356 0.296 0.191 0.162 0.154 0.152 0.452 0.493 0.44 0.316 0.326 0.287 0.238 0.233 0.218 0.214 0.182 0.148 0.133 0.125 0.386 0.221 0.137 0.138 0.127 0.123 0.131 0.129 0.352 0.085 0.105 0.072 0.062 0. 0.365 0.262 0.353 0.151 0.097 0.189 0.112 0.144 0.117 0.092 0.137 0.055 0.045 0.032 0.373 0.142 0.054 0.054 0.043 0.049 0.052 0.055 0.465 0.374 0.306 0.313 0.295 0.295 0.488 0.753 0.543 0.376 0.455 0.360 0.447 0.425 0.38 0.315 0.320 0.356 0.273 0.329 0.469 0.373 0.267 0.277 0.269 0.242 0.288 0.295 0.258 0.609 0.243 0.166 0.164 0. 0.572 0.729 0.547 0.598 0.608 0.459 0.269 0.234 0.279 0.341 0.166 0.13 0.15 0.099 0.422 0.242 0.163 0.152 0.134 0.147 0.118 0.121 0.35 0.116 0.108 0.097 0.094 0.077 0.382 0.227 0.317 0.138 0.145 0.141 0.126 0.128 0.095 0.106 0.182 0.049 0.066 0.04 0.283 0.125 0.064 0.067 0.062 0.056 0.066 0.048 Table 4 Accuracy of Nemotron-Parse on OmniDocBench. Metrics for competing methods are obtained from DeepSeek-OCR paper (Wei et al., 2025). 8 NVIDIA Nemotron-Parse 1.1 Nemotron-Parse Nemotron-Parse-TC TEDS S-TEDS S-TEDS TEDS RD-TableBench PubTabNet OmniDocBench 1.0 en 86.2 81.3 82.68 79.9 93.99 89.06 85.3 80.9 84.73 79.6 93.6 91.44 Table 5 TEDS and S-TEDS of Nemotron-Parse on public table extraction benchmarks. Method Reducto Azure Textract Sonnet 3.5 GPT-4o Llamaparse Gcloud Unstructured Nemotron-Parse Nemotron-Parse-TC Table similarity 90.2 82.7 80.9 80.7 76.0 74.6 64.6 60.2 85.8 85. Table 6 Table extraction accuracy on RD-TableBench benchmark. Metrics for competing methods are obtained from public Reducto results. (Reducto, 2024) 5. Implementation We release both Nemotron-Parse-v1.1 and Nemotron-Parse-v1.1-TC on Huggingface in fp32/bf16 formats at https://huggingface.co/nvidia/NVIDIA-Nemotron-Parse-v1.1 and https://huggingface. co/nvidia/NVIDIA-Nemotron-Parse-v1.1-TC along with VLLM support. Nemotron-Parse is additionally released as an optimized NIM container at https://build.nvidia.com/nvidia/ nemotron-parse. We report the speed on single H100 GPU of bf16 model in the Table 8. Inference speed is measured as average tokens/seconds end-to-end over 10,000 pages of length 1000 tokens each. This roughly translates to 5 pages/second for Nemotron-Parse-TC and 4 pages/second for Nemotron-Parse for an average page. 6. Contributors Core Model Development Kateryna Chumachenko, Amala Sanjay Deshmukh, Jarno Sepp√§nen, Ilia Karmanov, Chia-Chih Chen, Lukas V√∂gtle, Philipp Fischer, Marek Wawrzos, Timo Roman, Karan Sapra, Andrew Tao, Bryan Catanzaro Infrastructure, Data, Benchmarking, and Product Saeid Motiian, Roman Ageev, Kedi Wu, Alexandre Milesi, Maryam Moosaei, Krzysztof Pawelec, Padmavathy Subramanian, Mehrzad Samadi, Xin Yu, Celina Dear, Sarah Stoddard, Jenna Diamond, Jesse Oliver, Leanna Chraghchian, Patrick Skelly, Tom Balough, Yao Xu, Jane Polak Scowcroft, Daniel Korzekwa, Darragh Hanley, Sandip Bhaskar 9 NVIDIA Nemotron-Parse 1.1 Language WER F1 0.98 English 0.96 German 0.97 French Italian 0.97 0.97 Spanish 0.98 Chinese 0.98 Japanese 0.03 0.06 0.05 0.05 0.04 0.03 0. Table 7 OCR results on multilingual NVpdftex dataset. Method Nemotron-Parse Nemotron-Parse-TC Tok/sec 3800 4500 Table 8 Inference throughput (tokens per second) showing that Nemotron-Parse-TC achieves higher generation speed than the standard Nemotron-Parse mode 7. Acknowledgement We would like to thank Mike Ranzinger, Collin McCarthy, Bo Liu, Jean-Francois Puget, Sean Sodha, Nave Algarici, Randy Gelhausen for helpful discussion and feedback. 8. Examples In the following section, we provide several output examples showcasing the capabilities of Nemotron Parse v1.1 in layout understanding, table extraction, OCR, and formula extraction. Figure 1 Layout analysis: bounding box detection and prediction of semantic classes 10 NVIDIA Nemotron-Parse 1.1 Figure 2 OCR, extraction of text formatting and mathematical equations in LaTeX and markdown. Figure 3 Extraction of complex tables to LaTeX format."
        },
        {
            "title": "References",
            "content": "Ocrflux, 2025. URL https://github.com/chatdoc-com/OCRFlux. G. AI. Gemini 2.5-pro, 2025. URL https://gemini.google.com/. Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, et al. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923, 2025. Lukas Blecher, Guillem Cucurull, Thomas Scialom, and Robert Stojnic. Nougat: Neural optical 11 NVIDIA Nemotron-Parse 1. understanding for academic documents. In International Conference on Learning Representations, 2024. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. Science China Information Sciences, 67(12):220101, 2024. Cheng Cui, Ting Sun, Manhui Lin, Tingquan Gao, Yubo Zhang, Jiaxuan Liu, Xueqing Wang, Zelun Zhang, Changda Zhou, Hongen Liu, et al. Paddleocr 3.0 technical report. arXiv preprint arXiv:2507.05595, 2025. Alexey Dosovitskiy, Lukas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. In International Conference on Learning Representations, 2021. URL https://openreview.net/ forum?id=YicbFdNTTy. Hao Feng, Shu Wei, Xiang Fei, Wei Shi, Yingdong Han, Lei Liao, Jinghui Lu, Binghong Wu, Qi Liu, Chunhui Lin, et al. Dolphin: Document image parsing via heterogeneous anchor prompting. arXiv preprint arXiv:2505.14059, 2025. FinTabNet-dataset. Fintabnet dataset. https://huggingface.co/datasets/bsmock/FinTabNet.c, 2024. Fabian Gloeckle, Badr Youbi Idrissi, Baptiste Rozi√®re, David Lopez-Paz, and Gabriel Synnaeve. Better & faster large language models via multi-token prediction. arXiv preprint arXiv:2404.19737, 2024. Greg Heinrich, Mike Ranzinger, Hongxu Yin, Yao Lu, Jan Kautz, Andrew Tao, Bryan Catanzaro, and Pavlo Molchanov. Radiov2.5: Improved baselines for agglomerative vision foundation models. In Proceedings of the Computer Vision and Pattern Recognition Conference (CVPR), pp. 2248722497, June 2025. Ilia Karmanov, Amala Sanjay Deshmukh, Lukas Voegtle, Philipp Fischer, Kateryna Chumachenko, Timo Roman, Jarno Sepp√§nen, Jupinder Parmar, Joseph Jennings, Andrew Tao, and Karan Sapra. √âclair Extracting Content and Layout with Integrated Reading Order for Documents, 2025. URL https://arxiv.org/abs/2502.04223. Amirhossein Kazemnejad, Inkit Padhi, Karthikeyan Natesan Ramamurthy, Payel Das, and Siva Reddy. The impact of positional encoding on length generalization in transformers. Advances in Neural Information Processing Systems, 36:2489224928, 2023. Zhang Li, Yuliang Liu, Qiang Liu, Zhiyin Ma, Ziyang Zhang, Shuo Zhang, Zidun Guo, Jiarui Zhang, Xinyu Wang, and Xiang Bai. Monkeyocr: Document parsing with structure-recognition-relation triplet paradigm. arXiv preprint arXiv:2506.05218, 2025. Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, 8:726742, 2020. Tengchao Lv, Yupan Huang, Jingye Chen, Lei Cui, Shuming Ma, Yaoyao Chang, Shaohan Huang, Wenhui Wang, Li Dong, Weiyao Luo, et al. Kosmos-2.5: multimodal literate model. arXiv preprint arXiv:2309.11419, 2023. 12 NVIDIA Nemotron-Parse 1.1 Marker. Marker, 2025. URL https://github.com/datalab-to/marker. Mathpix. Mathpix, 2025. URL https://mathpix.com/. Ahmed Nassar, Nikolaos Livathinos, Maksym Lysak, and Peter Staar. Tableformer: Table structure understanding with transformers. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 46144623, 2022. Ahmed Nassar, Andres Marafioti, Matteo Omenetti, Maksym Lysak, Nikolaos Livathinos, Christoph Auer, Lucas Morin, Rafael Teixeira de Lima, Yusik Kim, Said Gurbuz, et al. Smoldocling: An ultra-compact vision-language model for end-to-end multi-modal document conversion. arXiv preprint arXiv:2503.11576, 2025. Nvidia. Nvpdftex. experimental/nvpdftex, 2025. https://github.com/NVIDIA-NeMo/Curator/tree/experimental/ OpenAI. Gpt-4 technical report. arxiv 2303.08774. View in Article, 2(5):1, 2023. Birgit Pfitzmann, Christoph Auer, Michele Dolfi, Ahmed Nassar, and Peter Staar. DocLayNet: large human-annotated dataset for document-layout segmentation. In Proceedings of the 28th ACM SIGKDD conference on knowledge discovery and data mining, pp. 37433751, 2022. Jake Poznanski, Aman Rangapur, Jon Borchardt, Jason Dunkelberger, Regan Huff, Daniel Lin, Christopher Wilhelm, Kyle Lo, and Luca Soldaini. olmocr: Unlocking trillions of tokens in pdfs with vision language models. arXiv preprint arXiv:2502.18443, 2025. Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. AM-RADIO: Agglomerative vision foundation model reduce all domains into one. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1249012500, 2024. Rednote. dots.ocr. URL https://github.com/rednote-hilab/dots.ocr. Reducto. Rd-tablebench. https://github.com/reductoai/rd-tablebench, 2024. Sebastian Spiegler. Statistics of the Common Crawl Corpus 2012, 2013. URL https://docs.google. com/file/d/1_9698uglerxB9nAglvaHkEgU-iZNm1TvVGuCW7245-WGvZq47teNpb_uL5N9. Brandon Smock, Rohith Pesala, and Robin Abraham. Pubtables-1m: Towards comprehensive table extraction from unstructured documents. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 46344642, 2022. Bin Wang, Chao Xu, Xiaomeng Zhao, Linke Ouyang, Fan Wu, Zhiyuan Zhao, Rui Xu, Kaiwen Liu, Yuan Qu, Fukai Shang, et al. Mineru: An open-source solution for precise document content extraction. URL https://arxiv. org/abs/2409.18839, 2024. Haoran Wei, Chenglong Liu, Jinyue Chen, Jia Wang, Lingyu Kong, Yanming Xu, Zheng Ge, Liang Zhao, Jianjian Sun, Yuang Peng, et al. General OCR theory: Towards OCR-2.0 via unified end-to-end model. arXiv preprint arXiv:2409.01704, 2024. Haoran Wei, Yaofeng Sun, and Yukun Li. Deepseek-ocr: Contexts optical compression. arXiv preprint arXiv:2510.18234, 2025. Fan Yang, Lei Hu, Xinwu Liu, Shuangping Huang, and Zhenghui Gu. large-scale dataset for end-to-end table recognition in the wild. Scientific Data, 10(1):110, 2023. 13 NVIDIA Nemotron-Parse 1.1 Jinguo Zhu, Weiyun Wang, Zhe Chen, Zhaoyang Liu, Shenglong Ye, Lixin Gu, Hao Tian, Yuchen Duan, Weijie Su, Jie Shao, et al. Internvl3: Exploring advanced training and test-time recipes for open-source multimodal models. arXiv preprint arXiv:2504.10479, 2025. Chunsheng Zuo, Pavel Guerzhoy, and Michael Guerzhoy. Position information emerges in causal transformers without positional encodings via similarity of nearby embeddings. In Proceedings of the 31st International Conference on Computational Linguistics, pp. 94189430, 2025."
        }
    ],
    "affiliations": [
        "NVIDIA"
    ]
}