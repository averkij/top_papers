{
    "paper_title": "CrossOver: 3D Scene Cross-Modal Alignment",
    "authors": [
        "Sayan Deb Sarkar",
        "Ondrej Miksik",
        "Marc Pollefeys",
        "Daniel Barath",
        "Iro Armeni"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Multi-modal 3D object understanding has gained significant attention, yet current approaches often assume complete data availability and rigid alignment across all modalities. We present CrossOver, a novel framework for cross-modal 3D scene understanding via flexible, scene-level modality alignment. Unlike traditional methods that require aligned modality data for every object instance, CrossOver learns a unified, modality-agnostic embedding space for scenes by aligning modalities - RGB images, point clouds, CAD models, floorplans, and text descriptions - with relaxed constraints and without explicit object semantics. Leveraging dimensionality-specific encoders, a multi-stage training pipeline, and emergent cross-modal behaviors, CrossOver supports robust scene retrieval and object localization, even with missing modalities. Evaluations on ScanNet and 3RScan datasets show its superior performance across diverse metrics, highlighting adaptability for real-world applications in 3D scene understanding."
        },
        {
            "title": "Start",
            "content": "CrossOver: 3D Scene Cross-Modal Alignment Sayan Deb Sarkar1 Ondrej Miksik3 Marc Pollefeys2, 3 Daniel Barath2 Iro Armeni 1 1Stanford University 2ETH Zurich 3Microsoft Spatial AI Lab, Zurich sayands.github.io/crossover 5 2 0 2 0 2 ] . [ 1 1 1 0 5 1 . 2 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Multi-modal 3D object understanding has gained significant attention, yet current approaches often assume complete data availability and rigid alignment across all modalities. We present CrossOver, novel framework for crossmodal 3D scene understanding via flexible, scene-level modality alignment. Unlike traditional methods that require aligned modality data for every object instance, CrossOver learns unified, modality-agnostic embedding space for scenes by aligning modalities RGB images, point clouds, CAD models, floorplans, and text descriptions with relaxed constraints and without explicit object semantics. Leveraging dimensionality-specific encoders, multi-stage training pipeline, and emergent cross-modal behaviors, CrossOver supports robust scene retrieval and object localization, even with missing modalities. Evaluations on ScanNet and 3RScan datasets show its superior performance across diverse metrics, highlighting CrossOvers adaptability for real-world applications in 3D scene understanding. 1. Introduction In recent years, the need to align and transfer information across modalities has grown substantially, especially for tasks involving complex 3D environments. Such capability enables knowledge and experience transfer across modalities. For example, knowing the layout of kitchens in computer-aided design (CAD) format will provide guidance on how to build new kitchen, such that it follows the layout of the most similar CAD floorplan. Current multi-modal approaches tackle 3D data alignment of individual objects across modalities [18, 42, 43, 45], without including and considering scene context, making them challenging to extend effectively for scene-level understanding. These methods typically assume fully aligned, consistent datasets, where each modality is perfectly corresponding to all others for each object. However, real-world scenarios rarely provide such complete modality pairings. For example, video of room and its CAD model might share some spatial alignment but differ in Figure 1. CrossOver is cross-modal alignment method for 3D scenes that learns unified, modality-agnostic embedding space, enabling range of tasks. For example, given the 3D CAD model of query scene and database of reconstructed point clouds, CrossOver can retrieve the closest matching point cloud and, if object instances are known, it can identify the individual locations of furniture CAD models with matched instances in the retrieved point cloud, using brute-force alignment. This capability has direct applications in virtual and augmented reality. data characteristics and object instances (hereby referred to as instances) represented in the data (e.g., some instances could be missing in one modality, which is common between real-world scenes and their CAD models). Also, achieving consistent instance segmentation across modalities is nearly impossible in practice. Thus, these approaches struggle when certain modalities are missing or incomplete, limiting their flexibility in practical applications [5]. We address the inherent limitations of strict object-level modality alignment by introducing flexible scene-level modality alignment approach that operates without prior information during inference (e.g., semantic instance segmentation), unlike the current methods [34, 41]. Our method, namely CrossOver  (Fig. 1)  , enables the learning of crossmodal behaviors and relationships, such as identifying similar objects or scenes across different modalities, like the virtual CAD scene based on video of real room. This capability extends beyond instance-level matching towards unified, modality-agnostic understanding that supports seamless cross-modal interactions at the scene level. CrossOver modalitiesRGB images, focuses on aligning five key scene clouds, real-world point CAD models, floorplan images, and text descriptions, in the feature spacegoing beyond the RGB-PC-Text triplets of prior work. Importantly, it is designed with the assumption that not all modalities are available for every data point. By employing flexible training strategy, we allow CrossOver to leverage any available modality during training, without requiring fully aligned data across all modalities. This approach enables our encoders to learn emergent modality alignments, supporting cross-modal traversals even in cases with missing data. Our work is grounded in three key contributions: Dimensionality-Specific Encoders: We introduce 1D, 2D, and 3D encoders tailored to each modalitys dimensionality, removing the need for explicit 3D scene graphs or semantic labels during inference. This optimizes feature extraction for each modality and avoids reliance on consistent semantics, which is often hard to obtain. Three-Stage Training Pipeline: Our pipeline progressively builds modality-agnostic embedding space. First, object-level embeddings capture fine-grained modality relationships. Next, scene-level training develops unified scene representations without requiring all object pairs to align. Finally, dimensionality-specific encoders create semantic-free cross-modal embeddings. Emergent Cross-Modal Behavior: CrossOver learns emergent modality behavior, despite not being explicitly trained on all pairwise modalities. It recognizes, e.g., that Scenei in the image modality corresponds to Scenei in the floorplan modality or its point cloud to the text one, without these modality pairs being present in training. This unified, modality-agnostic embedding space enables diverse tasks such as object localization and crossmodal scene retrieval, offering flexible, scalable solution for real-world data that may lack complete pairings. 2. Related Work Multi-modal Representation Learning aims to bridge data modalities by learning shared embeddings for crossmodal understanding and retrieval. seminal work in this area is CLIP [32], which popularized the contrastive training objective to learn joint image-text embedding space. This framework has been extended to various tasks, such as video retrieval [27], unified vision-language modeling [26], and cross-modal alignment [16, 28]. In the 3D domain, PointCLIP [45] applied CLIP to point clouds by projecting them into multi-view depth maps, leveraging pretrained 2D knowledge. Subsequent research has focused on multimodality alignment, e.g. ImageBind [17] aligns six modalities in the 2D domain and shows the power of such representation for generative tasks. In 3D, ULIP [42] and its successor ULIP-2 [43] aim to learn unified representations among images, texts, and point clouds. Point-Bind [18] extends ImageBind [17] to 3D by aligning specific pairs of modalities using an InfoNCE loss [30]. While these methods effectively capture object-level data, they struggle to differentiate similar instances within scene, primarily focusing on isolated objects rather than complex scenes. Experiments in Section 4 demonstrate this limitation. common limitation of these approaches is the assumption of perfect modality alignments and complete data for each instance, often relying on datasets like ShapeNet55 [6]. This assumption is impractical for realworld scenarios where data is often incomplete or not wellmatched due to occlusions, dynamic changes, sensor limitations, or capture errors, such as in construction sites or robot navigation. Our work, CrossOver, addresses these challenges using real-world datasets consisting of incomplete point clouds and noisy images captured using affordable sensors. Unlike prior methods, we do not require perfect modality alignments or complete data (e.g., point clouds). 3D Scene Understanding has driven extensive work on text-to-image and point cloud based instance localization and alignment within large maps [2, 14, 22]. Techniques like NetVLAD [2] and CamNet [14] enable place recognition and image-based localization by extracting global image descriptors. Recent work has leveraged 3D scene graphs for enhanced scene understanding [3, 21, 33], with methods like SGAligner [34] and SG-PGM [41] facilitating scene alignment through 3D scene graph matching. For dynamic instance matching across long-term sparse environments, LivingScenes [47] parses an evolving 3D environment with an object-centric formulation. For cross-modal retrieval, approaches like ScanRefer [7] and ReferIt3D [1] localize objects in 3D scenes via natural language but rely on detailed annotations and fixed modality pairs. Methods like 3DSSG [39] and Where Am [8] extend scene retrieval across images and natural language using 3D scene graphs, yet they depend heavily on semantic annotations. SceneGraphLoc [29] performs image-to-scene-graph matching, using semantic information. Our approach diverges from these by removing the need for semantics or explicit scene graphs, instead leveraging dimensionalityspecific encoders and modality-agnostic embeddings for scene understanding without prior semantic knowledge. Handling Missing Modalities and noisy data is key challenge in multi-modal learning [5]. Traditional approaches often assume full data availability, limiting their real-world applicability. Some methods address missing data through modality imputation or robust models [37, 40]. Baltrusaitis et al. [5] highlight that many methods lack flexibility for incomplete or noisy data. Our framework tackles this by allowing independent mapping of each modality into shared embedding space, enabling flexible cross-modal interactions in unstructured environments with sparse or unaligned data. Furthermore, emergent behavior in multi-modal models, such as generalizing and inferring relationships beyond Figure 2. Overview of CrossOver. Given scene and its instances Oi represented across different modalities I, P, M, R, F, the goal is to align all modalities within shared embedding space. The Instance-Level Multimodal Interaction module captures modality interactions at the instance level within the context of scene. This is further enhanced by the Scene-Level Multimodal Interaction module, which jointly processes all instances to represent the scene with single feature vector FS . The Unified Dimensionality Encoders eliminate dependency on precise semantic instance information by learning to process each scene modality independently while interacting with FS . training data [17, 32], are promoted by structuring training around image embeddings as common representation. By mapping other modalities into this shared space, CrossOver fosters organic cross-modal relationships, enabling unified understanding across diverse data types. 3. Method Given 3D scene represented by various modalities, denoted as = {I, P, M, R, F}, our objective is to develop unified, modality-agnostic representation that maps independent modalities capturing the same 3D scene to common point in the embedding space. Here, is set of RGB images, is real-world reconstruction as point cloud, is digital mesh representation from computer aided design (CAD), is textual data describing within its surroundings, and is rasterized floorplan. Our proposed framework facilitates robust interactions across different modalities at both the comprising instances and scene levels, enhancing the multi-modal (e.g., pointcloud and floorplan F) and same modal (e.g., textual data R) understanding of 3D environments. We structure the development of the embedding space progressively, beginning with instance-level multi-modal interactions and culminating in scene-level multi-modal interactions without requiring prior knowledge, such as semantic information about constituent instances. An overview of CrossOver is shown in Fig. 2. To demonstrate the capabilities of this unified, modality-agnostic embedding space, we evaluate: 1. Cross-modal instance retrieval: Given an observed modality Qj of query instance Oi in scene (e.g., mesh or pointcloud P), we aim to retrieve any other modality Qk representing Oi within S. 2. Cross-modal scene retrieval: Given scene Si represented by modality Qj (e.g., image or floorplan F), we aim to retrieve another modality Qk representing Si. 3.1. Instance-Level Multi-Modal Interactions First, we describe the pipeline used for learning multimodal embedding space for independent instances. This will provide basis for the scene-level embeddings. We process each of the 1D (R), 2D (I), and 3D (P and M) instance modalities with corresponding encoders1: 1D Encoder. An instance Oi can be represented by its textual context in scene S, using descriptions like The chair is in front of the lamp and The chair is left of the table. We term these descriptions as object referrals [20] and encode each referral as ij using the pre-trained text encoder 1The modality is not used when learning an instance-level embedding since there is no notion of floorplan in this scenario. . i1 , . . . , BLIP [23], where is the instance of interest (e.g., chair) and represents another instance in the scene (e.g., lamp, table, or another chair). Practically, we collect object referrals per instance, resulting in = {f ik }. To create single feature vector representing the instances context, we apply average pooling over 2D Encoder. Given collection IS of images capturing scene S, we integrate multi-view and per-view multi-level visual embeddings for each Oi to encode Inspired . by [29], for each Oi, we select the top Kview defined by largest visibility of Oi among IS and calculate multi-level bounding boxes around Oi {bv,l [0, L)} within each view v. pre-trained DinoV2 [12, 31] encoder processes the image crops defined by bv,l to give us the [CLS] tokens per crop [44]. Subsequent average pooling operations aggregate these tokens into singular feature vector . In contrast to [29], we do not assume available camera poses. 3D Encoder. Given instance Oi and its corresponding realworld point cloud Pi and shape mesh Mi, we extract instance features using pretrained I2PMAE [46] point cloud encoder. Importantly, we do not utilize the semantic class [20, 48] of Oi in these operations. We concatenate the 3D location of Pi and Mi to and , respectively, to form the instance tokens ˆf . To introduce partial scene-level reasoning, we incorporate interactions between instances by integrating the instance tokens and encoding the pairwise spatial relationships of an instance with all others in within transformer network. Similar to [20], we employ spatial-attention-based transformers, following [9, 48], to generate . Details about the 3D location and spatial relationships are in Supp. For the mesh modality M, we sample points on the mesh surface to enable input to point cloud encoder. We encode neither the 3D location nor the spatial pairwise relation among instances, as we do not assume that the meshes are aligned with the scene geometry. and ˆf and i and i i All pre-trained encoders, which remain frozen during training, are followed by trainable projection layers. During training, after encoding each modality, we apply contrastive loss to enforce alignment of modality features within joint embedding space. Unlike prior work which requires full data modality alignment [18, 43] or semantic scene graph information [29, 34], CrossOver accommodates the practical challenge that not all modalities may always be available by not requiring the presence of Instead, it aligns all other all modalities simultaneously. modality embeddings with image space I. The loss function can be defined as: LOi = Lf ,f + Lf ,f + Lf ,f . (1) During training, CrossOver requires base modality for every instance, to align other modalities with its feature space. We choose images as the base modality due to their availability and strong encoder priors, though any supported modality can serve this role. Crucially, no modality availability assumptions are made during inference, allowing any query-target modality pair. Our experiments (see Supp.) show that aligning to single reference modality, rather than using all pairwise combinations as in prior work, improves performance. 3.2. Scene-Level Multi-Modal Interactions We distill knowledge from instance-level modality encoders to scene-level encoders, allowing us to leverage instancebased insights during training and enabling scene-level retrieval at inference without relying on 3D scene graphs or semantic instance information across modalities. Multi-modal Scene Fusion. Given the instance features , for each instance Oi in scene S, we compute each of the scene level features R, I, , and by first applying average pooling per modality to the features of all instances in S. We then perform weighted fusion of these pooled features to learn fixed-size multimodal embedding FS : , and , i FS = (cid:34) (cid:88) qQ exp(wq) jQq exp(wj) (cid:80) (cid:35) , (2) where j, Q, wq and wj are modality-wise trainable attention weights. We use an MLP head to project the dimensionality to our final representation space, resulting in an embedding that serves as unified scene representation, capturing interactions across all modalities. In practice, this representation is flexible, adapting to data availability and specifically to any missing modalities. 3.3. Unified Dimensionality Encoders The above scene-level encoder provides unified, modalityagnostic embedding space; however, it requires semantic instance information that is consistent across modalities during inference, which is challenging to obtain in practice. To eliminate this requirement, we design single encoder per modality dimensionality (i.e. 1D, 2D, and 3D) that directly processes raw data without needing additional information. Moreover, our experiments (Supp.) show that the scenelevel encoder needs all modalities at inference to perform reasonably. 1D Encoder. Similar to Sec. 3.1, we use object referrals to describe scene context [48]. We randomly sample = 10 referrals per scene and use text encoder to form F1D. 2D Encoder. Here, we consider both RGB and floorplan images. The floorplan is represented as top-view orthographic projection image of the 3D layout with geometrically aligned shape meshes for furniture instances. Since scene can be captured with multiple RGB images IS , we employ naive key-frame selection strategy to sample = 10 multi-view images (see Supp.). We process the 2D, . We pass each Fi images using DinoV2 [31] encoder and concatenate the output [CLS] token and aggregated patch embeddings to form Fi 2D via an MLP projection head and apply average pooling to generate F2D. In practice, we use the same encoder with shared weights for both RGB images IS and floorplan F; i.e., inputs are not distinguished between RGB and floorplan during training. This is the first use of the floorplan modality in CrossOver and there is no pairwise modality interaction during training between it and the image modality, unlike other modalities. 3D Encoder. We utilize sparse convolutional architecture with residual network as the encoder, built with the Minkowski Engine [10]. Given an input point cloud RN 3 containing points, it is first quantized into M0 voxels represented as RM03. The model then produces full-resolution output feature map F3D RM0D. The goal is to align each of the unified dimensionality encoders with the scene-level multi-modal encoder. The loss function for unified training becomes: Ls = αLFS ,F1D + βLFS ,F2D + γLFS ,F3D , where, α, β, and γ are learnable hyper-parameters. Thus, our combined loss is as follows: = Ls + (cid:88) OiS LOi (3) (4) . (5) exp(qT Lq,k = log i) and = H(Qn 3.4. Loss Definition Given = G(Qm i), B, two different encoder outputs for modalities Qm and Qn in minibatch B, we use contrastive loss similar to [17]: ki/τ ) j=i exp(qT exp(qT ki/τ ) + (cid:80) Here, τ is learnable temperature parameter, to modulate similarity between positive pairs. We consider every example = in minibatch as negative example. In practice, we use symmetric loss for better convergence: Lq,k + Lk,q. Although we pair each modality with the most prevalent one (i.e., I) to avoid the need for fully aligned modalities per data point during training, there are cases where not all modality pairs are available for given data point. To enhance CrossOvers flexibility, we account for these scenarios by masking the corresponding loss term for any unavailable modality pairs. kj/τ ) 3.5. Inference After training CrossOver with the loss objective defined in Eq. 4, we use the embedding feature vectors for retrieval tasks. Given scene containing = {Oi} instances each represented by one or more modalities from Q, we use our instance-level multi-modal encoders to perform crossmodal retrieval. Given Oi in query modality Qj and all other instances in target modality Qk, the goal is to retrieve the Oi in Qk. For scene retrieval, we apply simiFigure 3. Cross-modal Scene Retrieval Inference Pipeline. Given query modality (P) that represents scene, we obtain with the corresponding dimensionality encoder its feature vector (F3D) in the shared cross-modal embedding space. We identify the closest feature vector (F2D) in the target modality (F) and retrieve the corresponding scene from database of scenes in F. lar approach using our unified dimensionality encoders, except that instead of instances, we retrieve entire scenes. schematic diagram for one modality pair is shown in Fig. 3. 4. Experiments 4.1. Datasets We train and evaluate CrossOver on the ScanNet [11] and 3RScan [38] datasets. We choose ScanNet for providing comprehensive coverage of all modalities, and 3RScan for including more data on temporal scenes. For both datasets, we use the object referrals from SceneVerse [20], which is million-scale 3D vision-language dataset with 68K 3D indoor scenes comprising major indoor scene understanding datasets and 2.5M vision-language pairs. In all evaluations, we use model trained across all datasets (details in Supp.). ScanNet [11] is an RGB-D video dataset containing 2.5 million views in more than 1500 scenes, annotated with 3D camera poses, surface reconstructions, and instancelevel semantic segmentation; we obtain images and 3D point clouds. For mesh and floorplan F, we use the Scan2CAD [4] dataset, which provides annotated keypoint pairs between CAD models from ShapeNet [6] and their counterpart objects in the scans. 3RScan [38] benchmarks instance relocalization, featuring 1428 RGB-D sequences across 478 indoor scenes, including rescans of the latter after object relocation. It provides annotated 2D and 3D instance segmentation, camera trajectories, and reconstructed scan meshes. We obtain images and point clouds. 4.2. Evaluation Metrics We assess the quality of our representation space by quantifying its ability to identify the same instance Oi or scene Si across different modalities, Qj and Qk. Extending image feature matching evaluation [25, 35], we compute the instance matching recall as the ratio of correctly identified Scene-level Recall R@25% R@50% R@75% R@25% R@50% R@75% Scannet [11] 3RScan [38] ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours 1.28 6.73 88.46 98.08 98.12 98.22 99.31 99.66 37.24 54.83 98.63 99.31 0.64 0.96 37.82 76. 96.21 95.17 97.59 98.28 16.90 27.93 83.85 96.56 0.24 0.32 1.92 23.40 60.34 62.07 71.13 76.29 8.62 11.72 46.74 70.10 1.91 3.18 93.63 99. 98.66 100 100 100 16.78 21.48 92.62 100 0.40 0.64 35.03 79.62 85.91 87.25 92.62 97.32 6.04 6.04 60.40 89.26 0.28 0.01 3.82 22. 36.91 41.61 55.03 67.79 1.34 2.01 20.81 50.34 (a) Instance Matching Recall on ScanNet (b) Scene-Level Matching Recall on ScanNet and 3RScan Figure 4. Cross-Modal Instance Retrieval on ScanNet and 3RScan. (a) Even though CrossOver does not explicitly train all modality combinations, it achieves emergent behavior within the embedding space. The same applies to our Instance Baseline (Ours). CrossOver performs better than our self-baseline since it incorporates more scene context in the fusion of modalities. (b) Our method outperforms all baselines in all datasets, showcasing the robustness of learned cross-modal interactions. Oi matches, given database of instances. Additionally, we evaluate scene-level (instance) matching recall at thresholds of 25%, 50%, and 75%, indicating how many objects from scene in modality Qj out of the total objects in the same scene we can match in modality Qk. This combined measure shows instance matching failure within scene. We further evaluate the challenging task of cross-modal scene retrieval within database. For example, given query point cloud of scene, we aim to retrieve its corresponding 2D floorplan. This analysis includes multiple levels: (i) scene matching recall, or the models ability to retrieve the exact scene Si; (ii) scene category recall to test retrieval of scene from the same category (e.g., retrieving any kitchen when given kitchen query in multicategory database); (iii) temporal recall to evaluate whether the model can recover the same scene captured at different time, accounting for potential object movement or removal; and (iv) intra-category recall, which assesses retrieval of specific scene within single-category database (e.g., retrieving particular kitchen from only kitchen scenes). This last metric uniquely requires different database. 4.3. Instance Retrieval Cross-Modal Instance Matching. Our goal is instance matching within the same scene where multiple instances of the same furniture (e.g., two identical chairs) are commonly present. We showcase our results on ScanNet and 3RScan datasets in Fig. 4. We compare CrossOver with pretrained multi-modal methods ULIP-2 [43] and PointBind [18] and our instance-level multi-modal encoder to highlight the importance of scene-level understanding in cross-modal embedding space. As shown in Fig. 4a, our performance on ScanNet is robust across modalities, while baselines exhibit varying results. Current multi-modal methods are large pretrained models with strong text encoders that boost performance for referral-based retrieval. While prior work trains on all pairwise modalities, we selectively train only in reference to the image modality (I). Yet, we still achieve robust performance across all modalities, even without direct interactions during training. Emergent interactions are in green. Similar trends appear in Fig. 4b for scene-level matching. Temporal Instance Matching. Although not part of the learning objective, we evaluate CrossOvers effectiveness on temporal point cloud-based instance retrieval (samemodal) using scans acquired at different time intervals, with scene changes like object displacement and rearrangement. Tab. 1 presents comparative analysis against multiple baselines on the 3RScan dataset, highlighting our methods superior performance. This is large gain, lying in the strong representational power of our multi-modal embedding space, which allows the encoder to efficiently extract each instances spatial and geometric features in dynamic scenes. Moreover, our method, while primarily evaluated in the same-modal setting, also demonstrates superior performance in the cross-modal scenario, shown in the second half of Tab. 1, further underlining the importance of scenelevel multi-modal alignment to handle temporal variations in indoor scene understanding. Figure 5. Cross-Modal Scene Retrieval Qualitative Results on ScanNet. Given scene in query modality F, we aim to retrieve the same scene in target modality P. While PointBind and the Instance Baseline do not retrieve the correct scene within the top-4 matches, CrossOver identifies it as the top-1 match. Notably, temporal scenes appear close together in CrossOvers embedding space (e.g., = 2, = 3), with retrieved scenes featuring similar object layouts to the query scene, such as the red couch in = 4. Figure 6. Cross-Modal Scene Retrieval on ScanNet (Scene Matching Recall). Plots showcase the top 1, 5, 10, 20 scene matching recall of different methods on three modality pairs: P, R, R. Ours and the Instance Baseline have not been explicitly trained on R. Results are computed on database of 306 scenes and showcase the superior performance of our approach. Once again, the difference between Ours and our self-baseline is attributed to the enhanced cross-modal scene-level interactions achieved with the unified encoders. Scene-level Recall Method Scene Category Recall Temporal Recall Intra-Category Recall top-1 top-5 top-10 top-1 top-5 toptop-1 top-3 top-5 Method R@25% R@50% R@75% same-modal (P P) MendNet [15] VN-DGCNNcls [13] VN-ONetrecon [13] LivingScenes [47] Ours cross-modal (ours) P 80.68 72.32 86.36 87.50 92.31 89.74 62.33 68.83 64.77 53.41 71.59 78.41 84.62 73.08 38.96 40.26 37.50 29.55 44.32 50.00 57.69 42.31 18.18 22. Table 1. Temporal Instance Matching on 3RScan [38]. Our method exhibits better performance in the same-modal task compared to baselines, despite not being specifically trained on this. It also performs well on cross-modal tasks. Lower performance when is involved is expected, as descriptions are contextualized within the scenes layout and may lose validity if objects rearrange. Method Scene Matching Recall Temporal Recall top-1 toptop-10 top-20 top-1 top-5 top-10 ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours 1.27 1.27 8.92 14.01 2.01 1.34 8.72 6.04 0.67 0.67 0.76 6.71 5.10 4.46 30.57 49.04 4.70 4.77 40.94 26.85 3.36 3.36 14.09 19. 7.01 9.55 43.31 66.88 7.38 6.71 57.05 42.28 6.71 6.71 24.83 32.31 12.74 17.20 64.33 83.44 14.77 13.42 69.80 62.42 12.75 13.42 36.24 51. 0.04 2.13 0.04 12.77 2.13 2.13 6.38 2.13 2.13 2.13 0.04 8.51 4.26 4.26 19.15 36.17 6.38 6.38 38.30 34.04 6.38 6.38 14.89 27. 12.77 8.51 42.55 70.21 12.77 14.89 63.83 63.83 6.38 6.38 27.66 51.06 Table 2. Cross-Modal Scene Retrieval on 3RScan. Similar performance to the ScanNet results in Tab. 3 is observed. 4.4. Cross-Modal Scene Retrieval We compare our cross-modal scene retrieval results across multiple metrics with [18, 43] and our instance-level baseline. Since prior work does not address this task, we adapt their methods by averaging object embeddings per modality to create scene representations, treating our baseline similarly. Unlike CrossOver, these methods rely on semantic instance segmentation. Scene matching recall results on the ScanNet dataset  (Fig. 6)  , show that our unified dimensionality encoders, which do not rely on semantics, consistently outperform prior methods in all pairwise modalities and surpass our baseline in most cases. Detailed results on ScanNet and 3RScan are in Tabs. 3 and 2. Our method captures overall scene understanding, even with small-scale object reconfigurations, as shown by its superior temporal recall. The lower performance of pretrained methods may stem from training biases that limit their robustness with real-world ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours ULIP-2 [43] PointBind [18] Ours ULIP-2 [43] PointBind [18] Ours ULIP-2 [43] PointBind [18] Ours 7.37 13.78 42.95 64.74 41.92 49.48 49.14 57.39 11.34 18.21 28.87 57.73 38.46 35.58 58. 13.14 14.10 55.77 8.25 14.43 54.64 25.96 24.36 70.19 89.42 57.73 70.45 71.48 82.82 15.12 26.46 50.86 79.04 55.77 62.82 81. 26.28 48.72 78.53 29.21 27.15 74.91 43.27 42.95 81.09 94.23 61.86 80.07 80.07 87.63 23.27 31.96 66.67 85.57 64.42 72.76 89. 33.65 59.62 86.54 40.21 48.45 80.41 0.04 2.00 13.00 13.00 1.00 2.00 8.00 3.00 1.00 1.00 5.00 5.00 1.00 1.00 8. 1.00 0.50 10.00 1.00 1.00 6.00 1.00 5.00 35.00 41.00 2.00 6.00 28.00 25.00 2.00 2.00 13.00 20.00 2.00 11.00 32. 1.00 5.00 30.00 2.00 5.00 17.00 3.00 7.00 60.00 84.00 8.00 12.00 46.00 51.00 4.00 6.00 23.00 46.00 10.00 21.00 61. 6.00 7.00 57.00 5.00 8.00 35.00 16.77 20.03 46.37 38.98 19.48 19.19 28.00 29.04 18.12 18.25 29.41 26.79 18.48 20.03 28. 17.46 23.17 31.34 18.24 13.64 23.00 41.53 40.68 79.68 73.28 42.18 41.54 62.33 57.85 41.15 40.05 50.84 56.67 39.09 43.08 55. 38.74 39.23 63.42 41.80 38.32 51.37 55.54 57.01 88.43 85.00 56.69 55.85 72.62 70.75 54.93 54.84 65.65 68.63 55.96 58.62 71. 53.99 57.08 74.15 55.35 54.20 66.84 Table 3. Cross-Modal Scene Retrieval on ScanNet. We consistently outperform state-of-the-art methods and our self-baseline in most cases. The latter performs better in certain modality pairs on intra-category, with the biggest gap observed in R; this can be attributed to our less powerful text encoder. data, such as incomplete point clouds and blurry images. Qualitative results are in Fig. 5. 4.5. Missing Modalities To further demonstrate CrossOvers ability to capture emergent modality behavior with non-overlapping training data points, we train CrossOver using different data repositories for each modality pair. Specifically, we use the ScanNet dataset and split the image repository into two chunks of varying sizes. Training on image-point cloud (I P) and image-mesh (I M) using each chunk respectively, we expect to see an emergent behavior between point cloud and mesh (P M). The results (Tab. 4) include top-1 and top3 instance matching recall, as well as same and diff recall for evaluating intra- (e.g., identical chairs) and inter- (e.g., chair and table) object category performance within scene. Although partial data availability decreases recall, our matching only decreases by 3% even when using 25% P. This scenario is common in real-world applications, where certain modalities might be scarce. 5. Conclusion this work introduces CrossOver, frameIn summary, work for flexible, scene-level cross-modal alignment without the need for semantic annotations or perfectly aligned data. CrossOver leverages unified embedding space centered on image features, allowing it to generalize across unpaired modalities and outperform existing methods in crossmodal scene retrieval and instance matching on real-world Available Data Instance Matching Recall (%) (%) same diff top-1 top-3 25 50 75 100 75 50 25 86.32 87.46 87.35 87.44 73.38 70.02 67.65 72.46 55.46 57.49 54.99 59.88 79.73 79.94 79.45 80.81 Table 4. Ablation on instance matching on ScanNet with non-overlapping data per modality pair. Despite modality pairs not sharing the same image repository, our method retains high performance even when pair is underrepresented in the data. datasets. This approach addresses the limitations of traditional multi-modal models and holds promise for practical applications in areas like robotics, AR/VR, and construction monitoring. Although CrossOver excels in crossmodal instance matching, its scene retrieval generalizability could benefit from training on diverse indoor and outdoor datasets. CrossOver assumes base modality per dataset, advancing prior work requiring perfect modality alignment. Further relaxation is promising direction. Finally, exploring its embedding space for downstream scene understanding remains key area. Future research can explore how our approach can be applied to dynamic scene reconstruction and real-time navigation, thus leading to interactive and immersive mixed-reality experiences. 6. Acknowledgements This work was partially funded by the ETH RobotX research grant."
        },
        {
            "title": "References",
            "content": "[1] Panos Achlioptas, Ahmed Abdelreheem, Fei Xia, Mohamed Elhoseiny, and Leonidas J. Guibas. ReferIt3D: Neural listeners for fine-grained 3d object identification in real-world scenes. In 16th European Conference on Computer Vision (ECCV), 2020. 2 [2] Relja Arandjelovic, Petr Gronat, Akihiko Torii, Tomas Pajdla, and Josef Sivic. Netvlad: Cnn architecture for weakly supervised place recognition. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages 52975307, 2015. 2 [3] Iro Armeni, Zhi-Yang He, JunYoung Gwak, Amir Zamir, Martin Fischer, Jitendra Malik, and Silvio Savarese. 3d scene graph: structure for unified semantics, 3d space, and camera. In Proceedings of the IEEE International Conference on Computer Vision, pages 56645673, 2019. 2 [4] Armen Avetisyan, Manuel Dahnert, Angela Dai, Manolis Savva, Angel X. Chang, and Matthias Niessner. Scan2cad: Learning cad model alignment in rgb-d scans. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2019. 5 [5] Tadas Baltruˇsaitis, Chaitanya Ahuja, and Louis-Philippe Morency. Multimodal machine learning: survey and taxonomy. IEEE transactions on pattern analysis and machine intelligence, 41(2):423443, 2018. 1, [6] Angel X. Chang, Thomas A. Funkhouser, Leonidas J. Guibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong Xiao, L. Yi, and Fisher Yu. Shapenet: An information-rich 3d model repository. ArXiv, abs/1512.03012, 2015. 2, 5 [7] Dave Zhenyu Chen, Angel Chang, and Matthias Nießner. Scanrefer: 3d object localization in rgb-d scans using natural language. In European conference on computer vision, pages 202221. Springer, 2020. 2 [8] Jiaqi Chen, Daniel Barath, Iro Armeni, Marc Pollefeys, and where am i? scene retrieval with lanHermann Blum. guage. ArXiv, abs/2404.14565, 2024. 2 [9] Shizhe Chen, Pierre-Louis Guhur, Makarand Tapaswi, Cordelia Schmid, and Ivan Laptev. Language conditioned spatial relation reasoning for 3d object grounding. In Proceedings of the 36th International Conference on Neural Information Processing Systems, Red Hook, NY, USA, 2024. Curran Associates Inc. 4 [10] Christopher Choy, JunYoung Gwak, and Silvio Savarese. 4d spatio-temporal convnets: Minkowski convolutional neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 30753084, 2019. 5 [11] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. 5, 6, [12] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers, 2023. 4, 15 [13] Congyue Deng, Or Litany, Yueqi Duan, Adrien Poulenard, Andrea Tagliasacchi, and Leonidas Guibas. Vector neurons: general framework for so (3)-equivariant networks. In CVPR, 2021. 8 [14] Mingyu Ding, Zhe Wang, Jiankai Sun, Jianping Shi, and Ping Luo. Camnet: Coarse-to-fine retrieval for camera reIn 2019 IEEE/CVF International Conference localization. on Computer Vision (ICCV), pages 28712880, 2019. 2 [15] Shivam Duggal, Zihao Wang, Wei-Chiu Ma, Sivabalan Manivasagam, Justin Liang, Shenlong Wang, and Raquel Urtasun. Mending neural implicit modeling for 3d vehicle reconstruction in the wild. In WACV, 2022. 8 [16] Peng Gao, Shijie Geng, Renrui Zhang, Teli Ma, Rongyao Fang, Yongfeng Zhang, Hongsheng Li, and Yu Qiao. Clip-adapter: Better vision-language models with feature adapters. arXiv preprint arXiv:2110.04544, 2021. 2 [17] Rohit Girdhar, Alaaeldin El-Nouby, Zhuang Liu, Mannat Singh, Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra. Imagebind: One embedding space to bind them all. In CVPR, 2023. 2, 3, 5, 16 [18] Ziyu Guo, Renrui Zhang, Xiangyang Zhu, Yiwen Tang, Xianzheng Ma, Jiaming Han, Kexin Chen, Peng Gao, Xianzhi Li, Hongsheng Li, and Pheng-Ann Heng. Point-bind & point-llm: Aligning point cloud with multi-modality for 3d understanding, generation, and instruction following, 2023. 1, 2, 4, 6, 8, 12, 14, [19] Georg Hess, Adam Tonderski, Christoffer Petersson, Kalle Astrom, and Lennart Svensson. Lidarclip or: How learned to talk to point clouds. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024. 13, 17 [20] Baoxiong Jia, Yixin Chen, Huangyue Yu, Yan Wang, and Siyuan Sceneverse: Scaling 3d vision-language learnarXiv preprint Xuesong Niu, Tengyu Liu, Qing Li, Huang. ing for grounded scene understanding. arXiv:2401.09340, 2024. 3, 4, 5, 15, 16 [21] Ue-Hwan Kim, Jin-Man Park, Taek-Jin Song, and JongHwan Kim. 3-d scene graph: sparse and semantic representation of physical environments for intelligent agents. IEEE transactions on cybernetics, 50(12):49214933, 2019. 2 [22] Manuel Kolmet, Qunjie Zhou, Aljosa Osep, and Laura LealText2Pos: Text-to-Point-Cloud Cross-Modal LoTaixe. In 2022 IEEE/CVF Conference on Computer calization . Vision and Pattern Recognition (CVPR), pages 66776686, Los Alamitos, CA, USA, 2022. IEEE Computer Society. 2 [23] Junnan Li, Dongxu Li, Caiming Xiong, and Steven C. H. Hoi. Blip: Bootstrapping language-image pre-training for unified vision-language understanding and generation. In International Conference on Machine Learning, 2022. 4, [24] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In International Conference on Learning Representations, 2017. 15 [25] David G. Lowe. Distinctive image features from scaleInt. J. Comput. Vision, 60(2):91110, invariant keypoints. 2004. 5 [26] Huaishao Luo, Lei Ji, Botian Shi, Haoyang Huang, Nan Duan, Tianrui Li, Jason Li, Taroon Bharti, and Ming Zhou. Univl: unified video and language pre-training model for multimodal understanding and generation. arXiv preprint arXiv:2002.06353, 2020. 2 [27] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei, Nan Duan, and Tianrui Li. CLIP4Clip: An empirical study of clip for end to end video clip retrieval. arXiv preprint arXiv:2104.08860, 2021. 2 [28] Yiwei Ma, Guohai Xu, Xiaoshuai Sun, Ming Yan, Ji Zhang, and Rongrong Ji. X-CLIP:: End-to-end multi-grained conarXiv preprint trastive learning for video-text retrieval. arXiv:2207.07285, 2022. [29] Yang Miao, Francis Engelmann, Olga Vysotska, Federico Tombari, Marc Pollefeys, and Daniel Bela Barath. SceneGraphLoc: Cross-Modal Coarse Visual Localization on 3D Scene Graphs. In EuropeanConference on Computer Vision (ECCV), 2024. 2, 4, 13, 15, 16, 17 [30] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv preprint arXiv:1807.03748, 2018. 2 [31] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 4, 5, 15 [32] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In International Conference on Machine Learning, 2021. 2, 3, 15 [33] Antoni Rosinol, Arjun Gupta, Marcus Abate, Jingnan Shi, and Luca Carlone. 3d dynamic scene graphs: Actionable spatial perception with places, objects, and humans. arXiv preprint arXiv:2002.06289, 2020. [34] Sayan Deb Sarkar, Ondrej Miksik, Marc Pollefeys, Daniel Barath, and Iro Armeni. Sgaligner : 3d scene alignment with scene graphs. Proceedings of the IEEE International Conference on Computer Vision (ICCV), 2023. 1, 2, 4, 13 [35] Paul-Edouard Sarlin, Daniel DeTone, Tomasz Malisiewicz, SuperGlue: Learning feature and Andrew Rabinovich. matching with graph neural networks. In CVPR, 2020. 5 [36] Sai Shubodh, Mohammad Omama, Husain Zaidi, Udit Singh Parihar, and Madhava Krishna. Lip-loc: Lidar image pretraining for cross-modal localization. 2024. 13, 17 [37] Yao-Hung Hubert Tsai, Paul Pu Liang, Amir Zadeh, LouisPhilippe Morency, and Ruslan Salakhutdinov. Learning factorized multimodal representations. arXiv preprint arXiv:1806.06176, 2018. 2 [38] Johanna Wald, Armen Avetisyan, Nassir Navab, Federico Tombari, and Matthias Nießner. Rio: 3d object instance relocalization in changing indoor environments. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 76587667, 2019. 5, 6, 8, 15 [39] Johanna Wald, Helisa Dhamo, Nassir Navab, and Federico Tombari. Learning 3d semantic scene graphs from 3d indoor reconstructions. In Conference on Computer Vision and Pattern Recognition (CVPR), 2020. 2 [40] Renjie Wu, Hu Wang, and Hsiang-Ting Chen. comprehensive survey on deep multimodal learning with missing modality. arXiv preprint arXiv:2409.07825, 2024. 2 [41] Yaxu Xie, Alain Pagani, and Didier Stricker. Sg-pgm: Partial graph matching network with semantic geometric fusion for 3d scene graph alignment and its downstream tasks. arXiv preprint arXiv:2403.19474, 2024. 1, [42] Le Xue, Mingfei Gao, Chen Xing, Roberto Martın-Martın, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip: Learning unified representation of language, image and point cloud for 3d understanding. arXiv preprint arXiv:2212.05171, 2022. 1, 2 [43] Le Xue, Ning Yu, Shu Zhang, Junnan Li, Roberto MartınMartın, Jiajun Wu, Caiming Xiong, Ran Xu, Juan Carlos Niebles, and Silvio Savarese. Ulip-2: Towards scalable multimodal pre-training for 3d understanding, 2023. 1, 2, 4, 6, 8, 12, 14, 15 [44] Jiawei Yang, Katie Luo, Jiefeng Li, Congyue Deng, Leonidas J. Guibas, Dilip Krishnan, Kilian Weinberger, Yonglong Tian, and Yue Wang. Dvt: Denoising vision transformers. arXiv preprint arXiv:2401.02957, 2024. 4 [45] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xupeng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng Li. Pointclip: Point cloud understanding by clip. arXiv preprint arXiv:2112.02413, 2021. 1, 2 [46] Renrui Zhang, Liuhui Wang, Yu Jiao Qiao, Peng Gao, and Hongsheng Li. Learning 3d representations from 2d pre-trained models via image-to-point masked autoencoders. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 2176921780, 2022. 4, 15 [47] Liyuan Zhu, Shengyu Huang, and Iro Armeni Konrad Schindler. Living scenes: Multi-object relocalization In The and reconstruction in changing 3d environments. IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2024. 2, [48] Zhu Ziyu, Ma Xiaojian, Chen Yixin, Deng Zhidong, Huang Siyuan, and Li Qing. 3d-vista: Pre-trained transformer for 3d vision and text alignment. In ICCV, 2023. 4, 15 CrossOver: 3D Scene Cross-Modal Alignment"
        },
        {
            "title": "Abstract",
            "content": "In the supplementary material, we provide: 1. Impact of scaling up data (Sec. A) 2. Results on training with all pairwise modalities (Sec. B) 3. Results on same modality scene retrieval (Sec. C) 4. Results on scene retrieval with one modality input to the scene-level encoder (Sec. D) 5. Results on cross-modal coarse visual localization (Sec. E) 6. Additional qualitative results on scene retrieval (Sec. F) 7. Details on the camera view sampling algorithm (Sec. G) 8. Analysis of inference runtime (Sec. H) 9. Further details on the experimental setup (Sec. I) A. Data Scale-up Improvements We investigate the impact of scaling up training data by merging different datasets and its effect on CrossOvers performance, particularly for instanceand scene-level matching recall. Figure 7 demonstrates the advantages of joint training on the ScanNet and 3RScan datasets compared to training on each dataset individually. Please note that 3RScan includes only the I, P, and modalities. Joint training significantly enhances scene-level recall performance and also improves instance-level recall. These results highlight CrossOvers ability to effectively leverage diverse data sources, enabling better generalization across varying scenes and object arrangements, ultimately boosting overall performance. B. All Pairwise Modality Training As mentioned in Sec. 3.1 of the main paper, training with all pairwise modality combinations, as in prior work [18, 43], directly aligns all modality pairs in shared embedding space. However, this approach underperforms compared to alignment with single reference modality, as evidenced by the results in Tabs. B.1 and B.2. Note that Ours results are copied from Fig. 4 of the main paper. The key limitation of aligning all modality pairs lies in its added complexity, which dilutes focus and leads to lower scenelevel recall metrics. In contrast, intra-modal alignment enhances robustness, particularly in cases of missing modality inputs, by concentrating learning on specific modality relationships. This focused alignment not only improves performance but also facilitates emergent modality behavior. Similar insight is also noticed when training the unified encoders with the raw scene data using all pairwise modaliScene-level Recall R@25% R@50% R@75% All Pairs Ours All Pairs Ours All Pairs Ours All Pairs Ours (emergent) All Pairs Ours (emergent) All Pairs Ours (emergent) 97.12 98.08 100 99.66 87.82 86.54 99.66 99.31 89.42 87.50 100 99. 75.00 76.92 98.08 98.28 63.14 63.46 97.25 96.56 65.71 61.54 98.08 97. 15.06 23.40 75.95 76.29 33.97 34.29 75.26 70.10 35.26 30.77 83.52 83. Table B.1. Scene-level matching results on ScanNet. All Pairs refers to training our instance-level encoder with all pairwise modality combinations. As shown, training on all pairwise combinations does not provide drastically improved performance, as one would expect, even in the modality pairs that are not directly aligned in Ours (emergent). ties, namely F1D, F2D, F3D and FS . This is shown as All Pairs in Tabs. D.1 and D.2. C. Same-Modal Scene Retrieval We present results for same-modality scene retrieval in Tabs. C.1 and C.2, evaluated on the ScanNet and 3RScan datasets. Metrics include scene category recall, temporal recall, and intra-category recall. Our method is compared to ULIP-2 [43], PointBind [18], and our instance baseline. The instance baseline is not evaluated on the floorplan modality due to the lack of floorplan representation at the instance level. Additionally, the scene-level encoder combines all instance modalities to generate the FS encoding, utilizing ground truth instance segmentation that is consistent across all modalities. This can serve as an upper bound of performance for our method. Results indicate that individual modalities in our method are closely aligned within the embedding space, despite the cross-modal training objective. Consistent with cross-modal results, our Scene-level Recall Trained on R@25% R@50% R@75% 3RScan Scannet 3RScan + Scannet 3RScan Scannet 3RScan + Scannet 3RScan Scannet 3RScan + Scannet 22.44 86.54 86.54 84.54 99.31 99.31 68.97 99.62 99.23 8.01 64.42 63. 48.80 96.22 97.25 48.28 98.47 97.70 2.24 33.97 34.29 24.74 68.38 70.10 22.22 82.38 83.91 (a) Instance Matching Recall (b) Scene-Level Matching Recall Figure 7. Scaled-up training performance on ScanNet. When training on both ScanNet and 3RScan datasets together, results improve from any individual dataset training. As expected, training on 3RScan and evaluating on ScanNet will have limited performance. Note that the 3RScan includes only the I, P, and modalities. Scene-level Recall R@25% R@50% R@75% All Pair loss Ours All Pair Loss Ours All Pair Loss Ours (emergent) 99.36 99. 100 100 100 100 77.71 79.62 97.32 97.32 93.96 89.26 17.20 22. 62.42 67.79 54.36 50.34 Table B.2. Scene-level matching results on 3RScan. All Pairs refers to training our instance-level encoder with all pairwise modality combinations. Similar to ScanNet, training on all pairwise combinations does not provide improved performance, as one would expect, even in the modality pairs that are not directly aligned in Ours (emergent). method performs better than the instance baseline in most cases, highlighting the importance of scene-level understanding. Moreover, it achieves significantly better or comparable performance to ULIP-2 and PointBind. Notably, our method achieves 100% accuracy on the intra-category recall metric in all modalities, consistently distinguishing the same, e.g., kitchen among database of kitchens, with ULIP-2 following closely. ULIP-2 and PointBind show decreased performance on the text referral modality, likely due to training on simple object descriptions (e.g., point cloud of chair) without scene context. Finally, while our scene-level encoder excels when all modalities are available, challenges arise with missing modalities, as discussed in Sec. D. D. Uni-modal Scene-Level Encoder Inference In Sec. 3.3 of the main paper, we highlighted two key advantages of unified dimensionality encoders over the scenelevel encoder: (i) they eliminate the need for instance-level modalities or instance information, and (ii) the scene-level encoder struggles when provided with only single modality (uni-modal) instead of all. To validate the latter, crossmodal scene retrieval results are presented in Tabs. D.1 and D.2. Our method significantly outperforms the uni-modal scene-level encoder in most cases, underscoring the effectiveness and value of the unified modality encoders. E. Cross-Modal Coarse Visual Localization We evaluate our method on the task of cross-modal coarse visual localization of single image against database of multi-modal reference maps, comparing it to SceneGraphLoc [29] and its baselines LipLoc [36] and LidarCLIP [19] on the 3RScan dataset. SceneGraphLoc uses 3D scene graphs during inference as the multi-modal reference maps, incorporating object instance point clouds, their attributes and relationships, and the scenes structure (for formal definition of these modalities we point the reader to [29, 34]). For fair comparison, we use the 2D unified dimensionality encoder to process the input image into an Method Scene Category Recall Temporal Recall Intra-Category Recall top-1 top-5 top-10 top-1 toptop-10 top-1 top-3 top-5 ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours ULIP-2 [43] PointBind [18] Ours FS FS Ours 35.9 93.59 89.74 91.67 11.34 11.34 69.42 76.98 13.14 17.63 38.14 86.54 44.23 96.79 95.19 97.76 18.56 18.56 91.75 91. 13.14 58.33 75.00 95.51 13.78 63.78 59.95 24.36 82.37 83.65 56.73 98.08 97.12 98.08 24.05 24.05 94.16 94.85 23.72 71.47 85.38 96. 41.03 89.10 90.38 1.00 22.00 22.00 11.00 1.00 1.00 13.00 14.00 1.00 7.00 14.00 19.00 2.00 59.00 58.00 59.00 2.00 2.00 51.00 40. 2.00 23.00 42.00 57.00 1.00 7.00 14.00 2.00 37.00 43.00 30.00 99.00 99.00 98.00 4.00 4.00 83.00 79.00 3.00 45.00 73.00 96. 5.00 67.00 74.00 89.75 90.21 80.22 100 36.63 36.63 86.56 100 21.52 59.54 86.31 100 96.91 100 98.84 100 57.12 57.12 97.65 42.12 90.36 97.14 100 99.27 100 100 99.89 100 100 96.91 100 99.87 100 66.17 66.17 99.20 100 57.25 96.46 99.81 99.89 100 100 94.23 97.44 98.08 17.00 57. 99.00 100 100 100 Table C.1. Same-Modality Scene Retrieval on ScanNet. Our method performs on par with or better than baselines in same-modality scene retrieval across most metrics, indicating that individual modalities in our method are closely aligned within the embedding space, despite the cross-modal training objective. Figure 8. Cross-Modal Scene Retrieval on ScanNet. Plots showcase scene matching recall (Recall), category recall, temporal recall, and intra-category recall for different number of camera views evaluated on several Top-k matches. Note that maximum differs per recall since the amount of eligible matches depends on the criteria for each recall type: scene similarity, category, temporal changes. F2D feature vector, which is then compared to the FS feature vectors of all scenes in the database, extracted by our scene-level encoder. As shown in Tab. E.1, despite encoding less information in our multi-modal maps, our method performs competitively with SceneGraphLoc. F. Qualitative Results We present additional qualitative results in Figs. 11 and 12 for cross-modal scene retrieval of the pairwise modalities P. Fig. 11 illustrates success case for our method, where the correct scene is retrieved in the first match. In contrast, PointBind [18] and our instance baseline fail to Method Temporal Recall ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours ULIP-2 [43] PointBind [18] Inst. Baseline (Ours) Ours FS FS Ours top-1 top-5 top-10 2.13 10.64 4.26 17.02 2.13 2.13 19.15 12. 0.04 2.13 6.38 19.15 8.51 51.06 65.96 61.70 6.38 6.38 46.81 51.06 4.26 17.02 29.79 65.96 29.79 93.62 100 100 8.51 8.51 91.49 87. 6.38 36.17 3.83 97.87 17.02 59.57 97.87 Table C.2. Same-Modality Scene Retrieval on 3RScan. Our method performs on par with or better than baselines in samemodality scene retrieval across most metrics. The lower performance on this dataset is likely due to limited training data availability. Figure 9. Cross-Modal Scene Retrieval on 3RScan. Plots showcase scene matching recall (Recall) and temporal recall for different number of camera views. retrieve the correct scene within the first four matches. Notably, our instance baseline does not retrieve any bedrooms. Fig. 12 illustrates failure case of our method. Despite this, it successfully retrieves all office scenes with layout similar to the query one. In comparison, the baselines also fail to retrieve the correct scene but instead find matches in bedrooms and meeting rooms. Fig. 13 shows success and failure cases on 3RScan dataset for cross-modal scene retrieval of the pairwise modalities P. G. Camera View Sampling To sample camera views for the unified 2D encoder (Sec. 3.3 of the main paper), we represent each camera pose as 7D grid, combining its 3D translation and quaternionbased rotation (4 quaternion + 3 translation components). Our method selects camera poses to maximize 3D spatial separation in rotation and translation. Starting with random pose, we iteratively select the pose farthest from all previously chosen ones. This method ensures an even and diverse sampling of camera viewpoints across the scene. We analyze the impact of the number of selected cameras and present results for values of 1, 5, 10, and 20) in Figs. 8 and 9. The results show that performance stabilizes after = 10, with additional frames providing only slight improvements, indicating full scene coverage is not necessary for training CrossOver. Consequently, we use = 10 for all reported results in our method. H. Runtime Analysis Our scene retrieval model consists of 1.5B-parameter. On an NVIDIA 4090 GPU, our model takes 1.01s 0.26s for single modality and 1.98s for all modalities in 1D, 2D and 3D. It can be adapted to lightweight encoders for faster inference in compute-limited scenarios, with potential performance trade-off. I. Experimental Setup Details Location Encoding & Instance Spatial Relationships. Given Pi, we compose features and the location li (ie, 3D position, length, width and height) to form instance tokens ˆf [48]. similar process is followed for Mi. Since we do not use scene graph representations, for instance modality P, we embed the pairwise spatial relationships between objects in spatial transformer [20, 48] to encode the scene layout and context. For any two objects Oi and Oj present in scene, we define relationship sij = [dij, sin(θh), cos(θh), sin(θv), cos(θv)], where dij is the Euclidean distance between the centroids of objects and j, and θh and θv are the horizontal and vertical angles of the line connecting the centers of objects and j. The pairwise spatial feature matrix = {sij} is used to update the attention weights in the self-attention layers of the transformer. Evaluation Setup. Our results are reported on the validation sets of ScanNet [11] and 3RScan [38], as their corresponding test sets lack public annotations or is unavailable. For the experiments in Sec. E, we follow the dataset split provided by SceneGraphLoc [29] to ensure fairness. Implementation. Inspired by CLIP [32], we adopt an embedding space of size 768, consistent across instance-level, scene-level, and unified training stages. Each model is trained for 300 epochs on an NVIDIA GeForce RTX 4090 Ti GPU using the AdamW optimizer [24] with learning rate of 1e 3, and cosine annealing scheduler with warm restarts. To fine-tune the pre-trained encoders (BLIP [23], DinoV2 [12, 31], and I2PMAE [46]), we employ 2-layer MLP projection head with dropout and Layer NormalizaMethod Scene Matching Recall Scene Category Recall Temporal Recall Intra-Category Recall top-1 top-5 top-10 toptop-1 top-5 top-10 top-1 top-5 toptop-1 top-3 top-5 Uni-modal All Pairs Ours Uni-modal All Pairs Ours Uni-modal All Pairs Ours 16.67 16.35 21.15 2.75 7.56 8. 2.06 6.87 7.22 51.92 54.17 57.05 11.00 33.68 31.27 5.15 24.05 27.49 66.67 75.32 77.56 18.21 50.17 45. 12.03 37.46 44.33 85.26 91.35 89.10 29.90 65.64 59.79 21.31 58.42 57.73 36.22 65.71 64.74 19.59 65.98 57. 11.68 56.70 57.73 73.72 86.54 89.42 46.74 83.16 82.82 39.86 74.57 79.04 85.26 93.91 94.23 62.89 88.66 87. 57.04 82.82 85.57 14.00 11.00 13.00 2.00 8.00 3.00 3.00 3.00 5.00 36.00 42.00 41.00 14.00 28.00 25. 6.00 22.00 20.00 67.00 77.00 84.00 19.00 52.00 51.00 11.00 41.00 46.00 49.05 41.51 38.98 26.12 29.99 29. 25.82 31.94 26.79 85.15 71.38 73.28 55.80 58.42 57.85 53.52 56.12 56.57 91.91 84.85 85.00 66.71 72.64 70. 68.08 70.22 68.63 Table D.1. Uni-modal & All pair-wise modality training on Scene-Level Encoder Inference on Cross-Modal Scene Retrieval on ScanNet. All Pairs refers to training our unified encoder with all pairwise modality combinations. Uni-modal refers to the scene-level encoder with single-modality input. As shown in the Table, our approach outperforms the scene-level encoder and All Pairs in most cases. Unlike the unified dimensionality encoders, the scene-level encoder relies on instance-level data, even when operating on single modality. Method Scene Matching Recall Temporal Recall toptop-5 top-10 top-20 top-1 top-5 topI Uni-modal All Pairs Ours Uni-modal All Pairs Ours Uni-modal All Pairs Ours 11.46 12.74 14.01 3.36 8.05 6.04 1.34 7.38 6.71 42.68 43.31 49.04 14.77 30.20 26. 12.08 21.48 19.46 64.33 64.97 66.88 28.86 46.98 42.28 19.46 37.58 32.21 84.71 80.89 83.44 51.01 60.40 62. 36.91 59.73 51.01 12.77 8.51 12.77 8.51 8.51 2.13 4.26 4.26 8.51 31.91 44.68 36.17 21.28 31.91 34. 14.89 29.79 27.66 68.09 74.47 70.21 42.55 59.57 63.83 29.79 55.32 51.06 Table D.2. Uni-modal & All pair-wise modality training on Scene-Level Encoder Inference on Cross-Modal Scene Retrieval on 3RScan. All Pairs refers to training our unified encoder with all pairwise modality combinations. Uni-modal refers to the scene-level encoder with single-modality input. As shown in the Table, our approach outperforms the scene-level encoder in all but one case. Unlike the unified dimensionality encoders, the scene-level encoder relies on instance-level data, even when operating with single modality. tion [17, 29]. The τ parameter in the contrastive loss formulation is treated as learnable parameter. Consistent with practices in [20], we pre-train object-level and scene-level encoders and freeze them during unified dimensionality encoder training. Method Static Scenario out of 10 out of 50 top-1 toptop-10 top-1 top-5 top-10 LidarCLIP [19] LipLoc [36] SceneGraphLoc [29] Ours 16.30 14.00 53.60 46. 41.40 35.80 81.90 77.97 60.60 57.90 92.80 90.58 4.70 2.00 30.20 18.69 11.00 8.60 50.20 39.16 16.30 15.20 61.20 51.62 Table E.1. Cross-Modal Coarse Visual Localization on 3RScan. Given single image of scene, the goal is to retrieve the corresponding scene from database of multi-modal maps. We evaluate following the experimental setup in SceneGraphLoc [29] and compare our method to it and its baselines. Despite encoding less information in our multi-modal maps, our method performs competitively with SceneGraphLoc. Figure 10. Camera View Sampling Visualisation on ScaNnet dataset. Here, we visualise the = 20 selected views (in purple projected onto the ground truth scene mesh) using our camera sampling method. Although, the selected cameras may not cover the entire scene, they are spatially well-separated. Figure 11. Cross-Modal Scene Retrieval Success Qualitative Results on ScanNet. Given scene in query modality F, we aim to retrieve the same scene in target modality P. While PointBind and the Instance Baseline do not retrieve the correct scene within the top-4 matches, CrossOver identifies it as the top-1 match. Figure 12. Cross-Modal Scene Retrieval Failure Qualitative Results on ScanNet. Given scene in query modality F, we aim to retrieve the same scene in target modality P. While the baselines also fail to retrieve the same scene, CrossOver (k = 2) and PointBind (k = 3) retrieve temporal scan as match. Figure 13. Cross-Modal Scene Retrieval Qualitative Results on 3RScan. Top row - Success, Bottom row - Failure. Given scene in query modality R, we aim to retrieve the same scene in target modality P. Temporal scenes cluster naturally in the embedding space. However, query referrals may retrieve scans with similar objects across different scenes, especially when not discriminative enough (bottom)."
        }
    ],
    "affiliations": [
        "ETH Zurich",
        "Microsoft Spatial AI Lab, Zurich",
        "Stanford University"
    ]
}