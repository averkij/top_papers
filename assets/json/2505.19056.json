{
    "paper_title": "An Embarrassingly Simple Defense Against LLM Abliteration Attacks",
    "authors": [
        "Harethah Abu Shairah",
        "Hasan Abed Al Kader Hammoud",
        "Bernard Ghanem",
        "George Turkiyyah"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are typically aligned to comply with safety guidelines by refusing harmful instructions. A recent attack, termed abliteration, isolates and suppresses the single latent direction most responsible for refusal behavior, enabling the model to generate unethical content. We propose a defense that modifies how models generate refusals. We construct an extended-refusal dataset that contains harmful prompts with a full response that justifies the reason for refusal. We then fine-tune Llama-2-7B-Chat and Qwen2.5-Instruct (1.5B and 3B parameters) on our extended-refusal dataset, and evaluate the resulting systems on a set of harmful prompts. In our experiments, extended-refusal models maintain high refusal rates, dropping at most by 10%, whereas baseline models' refusal rates drop by 70-80% after abliteration. A broad evaluation of safety and utility shows that extended-refusal fine-tuning neutralizes the abliteration attack while preserving general performance."
        },
        {
            "title": "Start",
            "content": "Harethah Abu Shairah Hasan Abed Al Kader Hammoud Bernard Ghanem George Turkiyyah King Abdullah University of Science and Technology (KAUST) 5 2 0 2 5 2 ] . [ 1 6 5 0 9 1 . 5 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are typically aligned to comply with safety guidelines by refusing harmful instructions. recent attack, termed abliteration, isolates and suppresses the single latent direction most responsible for refusal behavior, enabling the model to generate unethical content. We propose defense that modifies how models generate refusals. We construct an extended-refusal dataset that contains harmful prompts with full response that justifies the reason for refusal. We then fine-tune LLAMA-2-7B-CHAT and QWEN2.5-INSTRUCT (1.5B and 3B parameters) on our extended-refusal dataset, and evaluate the resulting systems on set of harmful prompts. In our experiments, extended-refusal models maintain high refusal rates, dropping at most by 10%, whereas baseline models refusal rates drop by 7080% after abliteration. broad evaluation of safety and utility shows that extended-refusal fine-tuning neutralizes the abliteration attack while preserving general performance."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated remarkable capabilities across diverse tasks (Brown et al., 2020; Naveed et al., 2024; Minaee et al., 2025), from question answering (Kamalloo et al., 2023) to creative writing (Gómez-Rodríguez and Williams, 2023). However, these advances come with significant safety concerns (Wu et al., 2024). Models trained on vast, uncurated internet corpora inevitably encounter harmful content (Mendu et al., 2025), potentially leading to unsafe outputs when deployed. To mitigate these risks, developers employ post-training alignment techniques that teach models to refuse harmful instructions while remaining helpful for benign tasks. This alignment typically occurs through supervised fine-tuning (SFT) with carefully crafted demonstrations (Liu et al., 1 2023) or through reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022). Despite these safeguards, recent research has exposed vulnerabilities in aligned models (Lin et al., 2024; Chu et al., 2024a; Wei et al., 2023). While prompt-based jailbreaks have received substantial attention, more concerning development is refusal direction abliterationan efficient weight-surgery attack that neutralizes safety guardrails by removing single direction in the models residual stream (Arditi et al., 2024). This surgical intervention dramatically reduces the refusal rate of LLAMA-27B-CHAT from 100% to approximately 20% while preserving general utility, representing significant breach of safety mechanisms. The effectiveness of abliteration suggests an important insight: conventional safety alignment creates distinct, isolated neural pathways for refusal behavior rather than integrating safety throughout the models representation space. We hypothesize that this vulnerability stems from the brief, formulaic nature of typical refusal responses, which creates concentrated activation signature that can be easily identified and neutralized. Building on this insight, we propose simple yet effective defense: changing how models express refusal. We construct an extended-refusal dataset where harmful prompts are paired with comprehensive responses containing three components: (i) neutral topic overview, (ii) explicit refusal, and (iii) ethical rationale. By teaching models to generate these semantically rich refusals, we aim to disperse the safety signal across multiple dimensions in the representation space, making it substantially harder to isolate and remove. We validate our approach by fine-tuning three open-weight LLMs (LLAMA-2-7B-CHAT, QWEN2.5-3B-INSTRUCT, and QWEN2.5-1.5BINSTRUCT) on our extended-refusal dataset and subjecting them to abliteration attacks. Our experiments demonstrate that extended-refusal Figure 1: Base vs. Extended Refusal. Standard LLMs issue an immediate refusal without providing context or explanation. In contrast, the extended refusal first explains the nature of the request before refusing to assist with it. models maintain high refusal rates (>90%) after abliteration, compared to dramatic drops (to 13-21%) in conventional models. Importantly, this enhanced robustness comes with minimal impact on general model performance. This work makes the following contributions: We introduce extended-refusal fine-tuning as practical defense against direction-based safety attacks We demonstrate empirically that modifying refusal expression substantially increases robustness across multiple model architectures and sizes We provide insights into how safety alignment is represented within neural networks and how it can be more effectively integrated with general capabilities Our findings highlight that the manner in which models express safety-critical behaviors significantly impacts their vulnerability to targeted attacks, which suggests promising directions for developing more robust alignment techniques."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 LLMs Alignment Alignment techniques for LLMs aim to ensure that the models outputs adhere to human values, ethical norms, and task-relevant instructions. There are many different approaches to align LLMs (Ouyang et al., 2022; Rafailov et al., 2024; Ethayarajh et al., 2024; Shao et al., 2024; Meng et al., 2024; Zhou et al., 2023; Wang et al., 2024). These methods typically rely on fine-tuning models using curated instruction-response datasets (Shi et al., 2024; Ji et al., 2023; Xu et al., 2021) that contain examples of helpful and harmful prompts. In most cases, these techniques have resulted in models that produce shallow and direct refusals (Qi et al., 2024), as illustrated in Figures 1 and 2."
        },
        {
            "title": "2.2 Attacks Against Alignment",
            "content": "Despite advances in alignment training, the safety of LLMs is still quite brittle (Qi et al., 2023; Wei et al., 2023), these models also remain susceptible to variety of adversarial techniques known as jailbreaks (Lin et al., 2024; Chu et al., 2024a). These attacks include adversarial Supervised Fine-tuning (SFT) on harmful datasets (He et al., 2024), roleplaying attacks (Tang et al., 2025), gradient-based attacks (Zou et al., 2023), logits-based attacks (Guo et al., 2024; Zhang et al., 2023), prompt injection and context-based attacks (Wei et al., 2024; Liu et al., 2024), static weights modification attacks (Arditi et al., 2024; Hammoud et al., 2024), and many more (Yi et al., 2024; Chu et al., 2024b). 2 Figure 2: Base LLM Refusal Completions. LLAMA-2-7B-CHAT consistently produces monotone and repetitive refusal templates across different categories of unethical requests."
        },
        {
            "title": "2.3 Defenses Against Jailbreaks",
            "content": "Breaking the alignment of LLMs has always been cat-and-mouse game, with defenses that get developed only to face newer and more effective attacks, which in turn lead to better protections against jailbreaks. Rosati et al. (2024) introduced framework that validates the protection against harmful fine-tuning, while Bhardwaj et al. (2024) demonstrated that lost safety from benign fine-tuning can be restored via simple addition of safety vector to the fine-tuned model. Qi et al. (2024) shows how alignment is concentrated in only the very few first token positions in transformer based LLMs, and presents new training objective that minimizes the harm from safe fine-tuning, and dataset of safety recovery examples to protect against prompts prefix injection attacks. One way to protect against role-playing attacks and adversarial prompts is by adding an extra layer of protection to the model, this could be in the form of classifier that verifies the safety of the request before passing it to the LLM (Sharma et al., 2025; Inan et al., 2023), or different forms of prompt manipulation as seen in Sharma et al. (2024); Cao et al. (2024); Robey et al. (2024). Another approach to make alignment more robust is via deliberation, Guan et al. (2024) described how deliberative alignment makes many jailbreaks less affective. Hammoud et al. (2024) showed how the incorporation of synthetic safety data into dataaware model merging techniques can result in an expert model with maintained alignment. However, to the best of our knowledge, no prior work has addressed refusal direction ablation, as introduced by Arditi et al. (2024) and discussed in Section 3.1."
        },
        {
            "title": "3 Preliminaries",
            "content": "Consider transformer language model with layers and hidden size d. For an input token sequence = (x1, . . . , xT ) the residual-stream activation at layer ℓ {1, . . . , L} and position {1, . . . , } is denoted by hℓ,p(x) Rd. Two disjoint instruction sets are used throughout the analysis: := harmful instructions, := benign instructions. Cardinalities are = and = m. 3.1 Refusal Direction Abliteration For every layerposition pair (ℓ, p) we compute the mean activations µℓ,p := 1 (cid:88) xH hℓ,p(x), νℓ,p := 1 (cid:88) xB hℓ,p(x)."
        },
        {
            "title": "The difference vector",
            "content": "rℓ,p := µℓ,p νℓ,p Rd (1) (2) 3 Figure 3: Experiment pipeline overview. We begin with base chat LLM and fine-tune it using the extended refusal dataset. Refusal direction vectors are then computed from the resulting model, and each vector is used to perform Abliteration, yielding an abliterated model (cid:102)MER (i,j). Each abliterated model is subsequently evaluated for safety (refusal behavior) and general utility (coherence, MMLU, and perplexity). serves as candidate refusal direction. Let = {rℓ,p}ℓ,p be the family of all LT candidates. Following Arditi et al. (2024), we select the single vector ˆr = arg max rR refusal(M, r), (3) where refusal measures the drop in refusal accuracy when is removed. We normalise ˆr so that ˆr2 = 1. For each output projection matrix (ℓ) out Rdd we eliminate the component parallel to ˆr using the orthogonal projector Pˆr := Id ˆrˆr, Id Rdd. (4) latent signature, we construct an Extended Refusal (ER) dataset DER. Starting from 4,289 harmful prompts merged from Beavertails (Ji et al., 2023), AdvBench (Zou et al., 2023), StrongReject (Souly et al., 2024), and TDC-2023 (Mazeika et al., 2023), we generate structured refusals using GPT4O (OpenAI et al., 2024). Each response comprises: (i) neutral topic overview, (ii) an explicit refusal, and (iii) short ethical rationale. To maintain the utility of the model on benign tasks, we augment DER with 5,711 benign instruction-response pairs from Alpaca-GPT4-en (Peng et al., 2023), creating our complete finetuning dataset: The abliterated weight is (cid:102)W (ℓ) out := Pˆr (ℓ) out . DFT = DER DAlpaca (6) This combined dataset contains 10,000 examples (5) in total. Applying (5) to every layer yields an abliterated model (cid:102)M whose ability to refuse is greatly diminished while general perplexity is not affected. 3.2 Extended-Refusal Data Abliteration exploits the fact that standard refusals are brief and stylistically uniform. To diffuse their Fine-tuning. Let θ0 be the original model weights. Fine-tuning on DFT yields parameters θER. We denote the corresponding model by MER and its abliterated variant by (cid:102)MER (obtained via Equation 5). The subsequent sections evaluate to what extent Equation 5 degrades refusal accuracy for versus MER and how each manipulation 4 Table 1: Overall Performance Before and After Abliteration. Comparison of refusal score, coherence, MMLU, and perplexity for base and Extended-Refusal models. Extended-Refusal models sustain high refusal rates after abliteration, whereas baseline models suffer large safety drops. Model Refusal Score () Coherence () MMLU () Perplexity () Llama-2-7b-chat Llama-2-7b-abliterated Llama-2-7b-extended-refusal Llama-2-7b-extended-refusal-abliterated Qwen2.5-3B-instruct Qwen2.5-3B-instruct-abliterated Qwen2.5-3B-extended-refusal Qwen2.5-3B-extended-refusal-abliterated Qwen2.5-1.5B-instruct Qwen2.5-1.5B-instruct-abliterated Qwen2.5-1.5B-extended-refusal Qwen2.5-1.5B-extended-refusal-abliterated 100.0 20.7 100.0 92.7 93.1 15.1 100.0 90.9 93.8 13.6 100.0 96.7 100.0 100.0 100.0 89. 100.0 100.0 100.0 98.7 100.0 100.0 100.0 96.2 47.5 46.8 46.4 41.8 65.6 64.0 65.0 64.5 60.9 59.6 59.6 54.4 14.1 18.5 16.7 20. 21.6 22.4 23.7 25.4 24.1 24.7 26.7 49.5 affects utility metrics."
        },
        {
            "title": "4 Experiments",
            "content": "This section describes our experimental methodology and analyzes the effectiveness of extendedrefusal fine-tuning as defense against abliteration attacks. We present comprehensive evaluations across multiple models, assessing both safety alignment and general utility."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "Models We evaluate our approach on three openweight language models: LLAMA-2-7B-CHAT (Touvron et al., 2023), QWEN2.5-3B-INSTRUCT, and QWEN2.5-1.5B-INSTRUCT (Qwen et al., 2025). For each model, we create an extendedrefusal variant through fine-tuning on our constructed dataset DFT. Datasets Our experimental pipeline uses several datasets: Fine-tuning: We use our combined dataset DFT consisting of 4,289 harmful prompts with extended refusals (DER) and 5,711 benign instruction-response pairs from Alpaca-GPT4en (DAlpaca). Refusal Evaluation: We use CatQA (Bhardwaj et al., 2024), categorized set of 550 harmful questions spanning various categories of harmful content. Utility Evaluation: We use MMLU (Hendrycks et al., 2021) for benchmarking general knowledge and reasoning, and subset of 1,000 randomly sampled passages from C4 (Raffel et al., 2023) for perplexity evaluation. Evaluation Metrics We assess model performance using four key metrics: Refusal Score (%): The percentage of harmful instructions that the model successfully refuses. We use QWEN-2.5-14B as an LLM judge (Zheng et al., 2023) to classify responses as safe/unsafe. In Appendix 3 we also report results using Llama-Guard-3-8B (Inan et al., 2023) as an automatic safety classifier to determine refusal status. Coherence (%): The percentage of model responses that remain lexically and semantically consistent and well-formed, evaluated using an LLM-based judge (QWEN-2.5-14B). MMLU Score (%): Performance on the Massive Multitask Language Understanding benchmark, measuring knowledge and reasoning capabilities. Perplexity: Measured on C4 passages, with lower values indicating better next-token prediction and linguistic fluency. Abliteration Process For each model (base and extended-refusal), we identify candidate refusal directions rℓ,p for each layer ℓ and position using the methodology described in Section 3. We then perform abliteration by projecting out the identified 5 LLAMA-2-7B-EXTENDED-REFUSAL QWEN2.5-3B-EXTENDED-REFUSAL QWEN2.5-1.5B-EXTENDED-REFUSAL Figure 4: RefusalUtility Trade-off. Refusal and MMLU scores for each model (cid:102)MER (ℓ,p) after abliteration along direction rℓ,p. Directions that yield coherence below 70% are filtered out. refusal direction from output matrices as defined in Equation 5. We select the refusal direction that maximizes refusal degradation while maintaining coherence above 70%. 4.2 Main Results Table 1 presents the primary results of our work, comparing base models and their extended-refusal variants before and after abliteration. The results demonstrate that extended-refusal fine-tuning provides substantial protection against refusal direction abliteration. Our key findings include: Robustness to Abliteration Standard abliteration dramatically reduces refusal capabilities in base models, with refusal rates dropping by 7080 percentage points (to as low as 13.63% for QWEN2.5-1.5B). In contrast, extended-refusal models maintain strong safety alignment after abliteration, with refusal rates remaining above 90% across all tested models. This represents reduction of at most 9.1%, confirming our hypothesis that extended refusals distribute the safety signal across multiple dimensions. Performance Trade-offs Extended-refusal finetuning introduces minimal performance costs before abliteration, with slight reductions in MMLU scores (0.5-1.3%) and moderate increases in perplexity. After abliteration, extended-refusal models show greater degradation in utility metrics compared to abliterated base models, suggesting that refusal capabilities have become more intertwined with general model functionality. 6 LLAMA-2-7B-EXTENDED-REFUSAL QWEN2.5-3B-EXTENDED-REFUSAL QWEN2.5-1.5B-EXTENDED-REFUSAL Figure 5: Effect of Abliteration on Perplexity. Perplexity scores for each model (cid:102)MER(ℓ,p) after abliteration along direction rℓ,p. Only directions with coherence 70% are shown. Coherence Impact notable observation is that no extended-refusal model maintains perfect coherence after abliteration, unlike base models which remain fully coherent. This supports our hypothesis that extended refusals create more diffused set of representations that cannot be eliminated without affecting other aspects of model behavior. 4.3 Abliterated Models Analysis To have complete idea of how abliterating model using different refusal vectors impacts the performance of the model, both in terms of refusal and utility, we evaluate all models on every candidate refusal direction rℓ,p. Figure 4 shows the refusal and MMLU scores, while Figure 5 shows the perplexity of models that maintain coherence score above 70% after abliteration. The directions of the QWEN2.5-1.5B-EXTENDED-REFUSAL token positions {2, 5} do not produce coherent models. We see clear trend in the plots, with directions that result in lower refusal score, also leading to drop in utility (lower MMLU score and higher perplexity), which demonstrates the impact of abliterating more diffused refusal vectors. 4.4 Feature Space Analysis To better understand why abliteration works effectively on standard models but fails on models trained with extended refusals, we analyzed the feature spaces of harmful versus benign prompts before and after abliteration. Specifically, we examined how abliteration affects the separation between harmful and benign prompt representations 7 Table 2: Centroid Distances Under Abliteration. Euclidean distance between harmful and benign prompt centroids before and after abliteration. Model Distance Before Distance After QWEN2.5-1.5B QWEN2.5-1.5B-EXTENDED QWEN2.5-3B QWEN2.5-3B-EXTENDED LLAMA2-7B LLAMA2-7B-EXTENDED 76.25 75.02 100.88 79.59 112.01 74.64 47.46 64.98 66.99 71. 83.28 60.99 nal in single latent direction that attackers can easily target. Our insight is that how model refuses directly impacts how secure that refusal is. By training models on semantically richer refusals, with topic overviews, explicit refusals, and ethical rationales, we successfully dispersed this safety signal across multiple latent dimensions, making it substantially more resilient. Unlike reinforcement-based methods and adversarial training that primarily address robustness through iterative fine-tuning and prompt-level defenses, our method proactively restructures the internal representation space. Furthermore, while external moderation approaches such as Llama Guard provide effective post-generation safety checks, our method inherently strengthens internal alignment, reducing dependency on external safeguards. Empirical evaluations demonstrate that extended-refusal fine-tuning maintains refusal integrity (>90%) under attack conditions, with minimal degradation of overall utility, compared to drastic drops (7080%) observed in conventionally aligned models."
        },
        {
            "title": "Limitations",
            "content": "While our extended-refusal fine-tuning method significantly improves robustness against directional attacks such as abliteration, it is not guaranteed to resist all jailbreak or prompt-engineering attacks. Our approach focuses primarily on restructuring the internal representation of refusal behaviors, and thus might remain vulnerable to adversarial inputs that exploit nuanced reasoning or sophisticated context manipulations. Additionally, the increased verbosity of extended refusals could affect user experience by producing longer-than-necessary explanations. Future work should explore balancing refusal clarity and conciseness, as well as integrating our method with complementary defenses such as external moderation systems to achieve comprehensive robustness. Figure 6: Latent-Space Separation. Euclidean distance between hidden representations of safe and unsafe prompts, before and after abliteration, for both base and Extended-Refusal models. in the models hidden states. Using principal component analysis (PCA) on the final hidden state representations of both harmful and benign prompts, we measured the Euclidean distance between the centroids of these two categories. Table 2 shows the distances before and after abliteration across our tested models. Our analysis shows clear pattern: while abliteration substantially reduces the distance between harmful and benign representations in standard models (with reductions of 28.8, 33.9, and 28.7 points respectively), extended-refusal models maintain much more distinct representations (with smaller reductions of 10.0, 7.7, and 13.7 points). For instance, in QWEN2.5-1.5B, abliteration reduces the distance by 37.8% in the standard model but only by 13.4% in the extended-refusal variant. This preservation of feature space separation, as can be seen in Figure 6, explains why extendedrefusal models maintain their ability to distinguish between harmful and benign requests even after abliteration. The more distributed nature of refusal representations in these models prevents the abliteration attack from collapsing the distinction between content categories, thereby preserving safety alignment even when the primary refusal direction is removed."
        },
        {
            "title": "5 Conclusions",
            "content": "In this paper, we introduced extended-refusal finetuning, simple and practical alignment technique that significantly improves the robustness of language models against representation-level attacks, such as abliteration. Traditional alignment methods (SFT, RLHF, DPO) typically produce concise refusals, inadvertently concentrating the safety sig-"
        },
        {
            "title": "References",
            "content": "Andy Arditi, Oscar Obeso, Aaquib Syed, Daniel Paleka, Nina Panickssery, Wes Gurnee, and Neel Nanda. 2024. Refusal in Language Models Is Mediated by Single Direction. arXiv. Rishabh Bhardwaj, Do Duc Anh, and Soujanya Poria. 2024. Language Models are Homer Simpson! Safety Re-Alignment of Fine-tuned Language Models through Task Arithmetic. arXiv. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, LanClemens Winter, and 12 others. 2020. Preprint, guage models are few-shot learners. arXiv:2005.14165. Bochuan Cao, Yuanpu Cao, Lu Lin, and Jinghui Chen. 2024. Defending against alignment-breaking attacks In Proceedings of the via robustly aligned LLM. 62nd Annual Meeting of ACL (Volume 1: Long Papers), pages 1054210560, Bangkok, Thailand. Association for Computational Linguistics. Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, and Yang Zhang. 2024a. Comprehensive assessment of jailbreak attacks against llms. Preprint, arXiv:2402.05668. Junjie Chu, Yugeng Liu, Ziqing Yang, Xinyue Shen, Michael Backes, and Yang Zhang. 2024b. Comprehensive assessment of jailbreak attacks against llms. Preprint, arXiv:2402.05668. Kawin Ethayarajh, Winnie Xu, Niklas Muennighoff, Dan Jurafsky, and Douwe Kiela. 2024. Kto: Model alignment as prospect theoretic optimization. Preprint, arXiv:2402.01306. Melody Guan, Manas Joglekar, Eric Wallace, Saachi Jain, Boaz Barak, Alec Helyar, Rachel Dias, Andrea Vallone, Hongyu Ren, Jason Wei, Hyung Won Chung, Sam Toyer, Johannes Heidecke, Alex Beutel, and Amelia Glaese. 2024. Deliberative Alignment: Reasoning Enables Safer Language Models. arXiv. Xingang Guo, Fangxu Yu, Huan Zhang, Lianhui Qin, Jailbreaking and Bin Hu. 2024. llms with stealthiness and controllability. Preprint, arXiv:2402.08679. Cold-attack: Carlos Gómez-Rodríguez and Paul Williams. 2023. confederacy of models: comprehensive evalPreprint, uation of llms on creative writing. arXiv:2310.08433. Hasan Abed Al Kader Hammoud, Umberto Michieli, Fabio Pizzati, Philip Torr, Adel Bibi, Bernard Ghanem, and Mete Ozay. 2024. Model merging and safety alignment: One bad model spoils the bunch. Preprint, arXiv:2406.14563. Luxi He, Mengzhou Xia, and Peter Henderson. 2024. What is in your safe data? identifying benign data that breaks safety. Preprint, arXiv:2404.01099. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring massive multitask language understanding. Preprint, arXiv:2009.03300. Hakan Inan, Kartikeya Upasani, Jianfeng Chi, Rashi Rungta, Krithika Iyer, Yuning Mao, Michael Tontchev, Qing Hu, Brian Fuller, Davide Testuggine, and Madian Khabsa. 2023. Llama Guard: LLMbased Input-Output Safeguard for Human-AI Conversations. arXiv. Jiaming Ji, Mickel Liu, Juntao Dai, Xuehai Pan, Chi Zhang, Ce Bian, Chi Zhang, Ruiyang Sun, Yizhou Wang, and Yaodong Yang. 2023. Beavertails: Towards improved safety alignment of llm via humanpreference dataset. Preprint, arXiv:2307.04657. Ehsan Kamalloo, Nouha Dziri, Charles L. A. Clarke, and Davood Rafiei. 2023. Evaluating open-domain question answering in the era of large language models. Preprint, arXiv:2305.06984. Yuping Lin, Pengfei He, Han Xu, Yue Xing, Makoto Yamada, Hui Liu, and Jiliang Tang. 2024. Towards understanding jailbreak attacks in LLMs: repreIn EMNLP 2024, pages sentation space analysis. 70677085, Miami, Florida, USA. Association for Computational Linguistics. Ruibo Liu, Ruixin Yang, Chenyan Jia, Ge Zhang, Denny Zhou, Andrew M. Dai, Diyi Yang, and Soroush Vosoughi. 2023. Training socially aligned language models on simulated social interactions. Preprint, arXiv:2305.16960. Yi Liu, Gelei Deng, Yuekang Li, Kailong Wang, Zihao Wang, Xiaofeng Wang, Tianwei Zhang, Yepang Liu, Haoyu Wang, Yan Zheng, and Yang Liu. 2024. Prompt injection attack against llm-integrated applications. Preprint, arXiv:2306.05499. Mantas Mazeika, Andy Zou, Norman Mu, Long Phan, Zifan Wang, Chunru Yu, Adam Khoja, Fengqing Jiang, Aidan OGara, Ellie Sakhaee, Zhen Xiang, Arezoo Rajabi, Dan Hendrycks, Radha Poovendran, Bo Li, and David Forsyth. 2023. Tdc 2023 (llm edition): The trojan detection challenge. In NeurIPS Competition Track. Sai Krishna Mendu, Harish Yenala, Aditi Gulati, Shanu Kumar, and Parag Agrawal. 2025. Towards safer pretraining: Analyzing and filtering harmful content in webscale datasets for responsible llms. Preprint, arXiv:2505.02009. Yu Meng, Mengzhou Xia, and Danqi Chen. 2024. Simpo: Simple preference optimization with reference-free reward. Preprint, arXiv:2405.14734. Shervin Minaee, Tomas Mikolov, Narjes Nikzad, Meysam Chenaghlu, Richard Socher, Xavier Amatriain, and Jianfeng Gao. 2025. Large language models: survey. Preprint, arXiv:2402.06196. Humza Naveed, Asad Ullah Khan, Shi Qiu, Muhammad Saqib, Saeed Anwar, Muhammad Usman, Naveed Akhtar, Nick Barnes, and Ajmal Mian. 2024. comprehensive overview of large language models. Preprint, arXiv:2307.06435. OpenAI, :, Aaron Hurst, Adam Lerer, Adam P. Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, Aleksander adry, Alex Baker-Whitcomb, Alex Beutel, Alex Borzunov, Alex Carney, Alex Chow, Alex Kirillov, and 401 others. 2024. Gpt-4o system card. Preprint, arXiv:2410.21276. Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Training language models to follow instructions with human feedback. Preprint, arXiv:2203.02155. Baolin Peng, Chunyuan Li, Pengcheng He, Michel Galley, and Jianfeng Gao. 2023. Instruction tuning with gpt-4. arXiv preprint arXiv:2304.03277. Xiangyu Qi, Ashwinee Panda, Kaifeng Lyu, Xiao Ma, Subhrajit Roy, Ahmad Beirami, Prateek Mittal, and Peter Henderson. 2024. Safety alignment should be made more than just few tokens deep. Preprint, arXiv:2406.05946. Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023. Fine-tuning aligned language models compromises safety, even when users do not intend to! Preprint, arXiv:2310.03693. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, and 25 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Rafael Rafailov, Archit Sharma, Eric Mitchell, Stefano Ermon, Christopher D. Manning, and Chelsea Finn. 2024. Direct preference optimization: Your language model is secretly reward model. Preprint, arXiv:2305.18290. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter J. Liu. 2023. Exploring the limits of transfer learning with unified text-to-text transformer. Preprint, arXiv:1910.10683. Alexander Robey, Eric Wong, Hamed Hassani, and George J. Pappas. 2024. Smoothllm: Defending large language models against jailbreaking attacks. Preprint, arXiv:2310.03684. Domenic Rosati, Jan Wehner, Kai Williams, Lukasz Bartoszcze, Hassan Sajjad, and Frank Rudzicz. 2024. Immunization against harmful fine-tuning attacks. In Findings of EMNLP 2024, pages 52345247, Miami, Florida, USA. Association for Computational Linguistics. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, Y. K. Li, Y. Wu, and Daya Guo. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. Preprint, arXiv:2402.03300. Mrinank Sharma, Meg Tong, Jesse Mu, Jerry Wei, Jorrit Kruthoff, Scott Goodfriend, Euan Ong, Alwin Peng, Raj Agarwal, Cem Anil, Amanda Askell, Nathan Bailey, Joe Benton, Emma Bluemke, Samuel R. Bowman, Eric Christiansen, Hoagy Cunningham, Andy Dau, Anjali Gopal, and 24 others. 2025. Constitutional classifiers: Defending against universal jailbreaks across thousands of hours of red teaming. Preprint, arXiv:2501.18837. Reshabh Sharma, Vinayak Gupta, and Dan Grossman. 2024. SPML: DSL for defending language models against prompt attacks. Preprint, arXiv:2402.11755. Taiwei Shi, Kai Chen, and Jieyu Zhao. 2024. Saferinstruct: Aligning language models with automated preference data. Preprint, arXiv:2311.08685. Alexandra Souly, Qingyuan Lu, Dillon Bowen, Tu Trinh, Elvis Hsieh, Sana Pandey, Pieter Abbeel, Justin Svegliato, Scott Emmons, Olivia Watkins, and Sam Toyer. 2024. strongREJECT for empty jailbreaks. In NeurIPS. Yihong Tang, Bo Wang, Xu Wang, Dongming Zhao, Jing Liu, Ruifang He, and Yuexian Hou. 2025. RoleBreak: Character hallucination as jailbreak attack in role-playing systems. In Proceedings of the 31st COLING, pages 73867402, Abu Dhabi, UAE. Association for Computational Linguistics. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, Dan Bikel, Lukas Blecher, Cristian Canton Ferrer, Moya Chen, Guillem Cucurull, David Esiobu, Jude Fernandes, Jeremy Fu, Wenyin Fu, and 49 others. 2023. Llama 2: Open foundation and fine-tuned chat models. Preprint, arXiv:2307.09288. Zhichao Wang, Bin Bi, Shiva Kumar Pentyala, Kiran Ramnath, Sougata Chaudhuri, Shubham Mehrotra, Zixu, Zhu, Xiang-Bo Mao, Sitaram Asur, Na, and Cheng. 2024. comprehensive survey of LLM alignment techniques: RLHF, RLAIF, PPO, DPO and more. Preprint, arXiv:2407.16216. Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2023. Jailbroken: How does llm safety training fail? Preprint, arXiv:2307.02483. 10 Zeming Wei, Yifei Wang, Ang Li, Yichuan Mo, and Yisen Wang. 2024. Jailbreak and guard aligned language models with only few in-context demonstrations. Preprint, arXiv:2310.06387. Fangzhou Wu, Ning Zhang, Somesh Jha, Patrick McDaniel, and Chaowei Xiao. 2024. new era in llm security: Exploring security concerns in real-world llm-based systems. Preprint, arXiv:2402.18649. Jing Xu, Da Ju, Margaret Li, Y-Lan Boureau, Jason Weston, and Emily Dinan. 2021. Bot-adversarial dialogue for safe conversational agents. In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pages 29502968, Online. ACL. Sibo Yi, Yule Liu, Zhen Sun, Tianshuo Cong, Xinlei He, Jiaxing Song, Ke Xu, and Qi Li. 2024. Jailbreak attacks and defenses against large language models: survey. Preprint, arXiv:2407.04295. Zhuo Zhang, Guangyu Shen, Guanhong Tao, Siyuan Cheng, and Xiangyu Zhang. 2023. Make them spill the beans! coercive knowledge extraction from (production) llms. Preprint, arXiv:2312.04782. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. 2023. Judging llm-as-a-judge with mt-bench and chatbot arena. Preprint, arXiv:2306.05685. Chunting Zhou, Pengfei Liu, Puxin Xu, Srini Iyer, Jiao Sun, Yuning Mao, Xuezhe Ma, Avia Efrat, Ping Yu, Lili Yu, Susan Zhang, Gargi Ghosh, Mike Lewis, Luke Zettlemoyer, and Omer Levy. 2023. LIMA: Less Is More for Alignment. arXiv. Andy Zou, Zifan Wang, Nicholas Carlini, Milad Nasr, J. Zico Kolter, and Matt Fredrikson. 2023. Universal and transferable adversarial attacks on aligned language models. Preprint, arXiv:2307.15043. 11 LLM as Judge vs Llama-Guard In addition to using QWEN-2.5-14B as our primary LLM-based judge of refusal behavior, we also evaluated the ablated extended-refusal models with LLAMA-GUARD-3-8B, an external safety classifier. Table 3 reports refusal rates under both evaluators. Overall, the two methods are in strong agreement, with differences of at most 4 percentage points. For LLAMA-2-7B-EXTENDED-REFUSAL-ABLITERATED, Qwen-2.5-14B judges 92.7% refusal rate versus 90.4% under Llama-Guard. Similarly, QWEN2.5-3B-EXTENDED-REFUSAL-ABLITERATED shows 90.9% (LLM judge) vs. 94.6% (Llama-Guard), and QWEN2.5-1.5B-EXTENDED-REFUSAL-ABLITERATED yields 96.7% vs. 89.3%. These results confirm that our extended-refusal defense remains robust regardless of the safety classification method, and any minor discrepancies likely stem from subtle differences in each classifiers decision boundary. Table 3: LLM-Judge vs. Llama-Guard. Refusal scores computed with QWEN-2.5-14B (LLM Judge) versus LLAMA-GUARD-3-8B. Model Refusal Scores LLM as Judge Llama-Guard LLAMA-2-7B-EXTENDED-REFUSAL-ABLITERATED QWEN2.5-3B-EXTENDED-REFUSAL-ABLITERATED QWEN2.5-1.5B-EXTENDED-REFUSAL-ABLITERATED 92.7 90.9 96.7 90.4 94. 89."
        }
    ],
    "affiliations": [
        "King Abdullah University of Science and Technology (KAUST)"
    ]
}