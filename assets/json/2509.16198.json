{
    "paper_title": "RPG: A Repository Planning Graph for Unified and Scalable Codebase Generation",
    "authors": [
        "Jane Luo",
        "Xin Zhang",
        "Steven Liu",
        "Jie Wu",
        "Yiming Huang",
        "Yangyu Huang",
        "Chengyu Yin",
        "Ying Xin",
        "Jianfeng Liu",
        "Yuefeng Zhan",
        "Hao Sun",
        "Qi Chen",
        "Scarlett Li",
        "Mao Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models excel at function- and file-level code generation, yet generating complete repositories from scratch remains a fundamental challenge. This process demands coherent and reliable planning across proposal- and implementation-level stages, while natural language, due to its ambiguity and verbosity, is ill-suited for faithfully representing complex software structures. To address this, we introduce the Repository Planning Graph (RPG), a persistent representation that unifies proposal- and implementation-level planning by encoding capabilities, file structures, data flows, and functions in one graph. RPG replaces ambiguous natural language with an explicit blueprint, enabling long-horizon planning and scalable repository generation. Building on RPG, we develop ZeroRepo, a graph-driven framework for repository generation from scratch. It operates in three stages: proposal-level planning and implementation-level refinement to construct the graph, followed by graph-guided code generation with test validation. To evaluate this setting, we construct RepoCraft, a benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo produces repositories averaging nearly 36K LOC, roughly 3.9$\\times$ the strongest baseline (Claude Code) and about 64$\\times$ other baselines. It attains 81.5% functional coverage and a 69.7% pass rate, exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further analysis shows that RPG models complex dependencies, enables progressively more sophisticated planning through near-linear scaling, and enhances LLM understanding of repositories, thereby accelerating agent localization."
        },
        {
            "title": "Start",
            "content": "RPG: REPOSITORY PLANNING GRAPH FOR UNIFIED AND SCALABLE CODEBASE GENERATION 5 2 0 2 9 1 ] . [ 1 8 9 1 6 1 . 9 0 5 2 : r Jane Luo1 Xin Zhang1 Steven Liu1 Jie Wu1 2 Yiming Huang3 Yangyu Huang1 Chengyu Yin1 Ying Xin1 Jianfeng Liu1 Yuefeng Zhan1 Hao Sun1 Qi Chen1 Scarlett Li1 Mao Yang1 1 Microsoft 2 Tsinghua University 3 University of California, San Diego September 22,"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models excel at functionand file-level code generation, yet generating complete repositories from scratch remains fundamental challenge. This process demands coherent and reliable planning across proposaland implementation-level stages, while natural language, due to its ambiguity and verbosity, is ill-suited for faithfully representing complex software structures. To address this, we introduce the Repository Planning Graph (RPG), persistent representation that unifies proposaland implementation-level planning by encoding capabilities, file structures, data flows, and functions in one graph. RPG replaces ambiguous natural language with an explicit blueprint, enabling long-horizon planning and scalable repository generation. Building on RPG, we develop ZeroRepo, graph-driven framework for repository generation from scratch. It operates in three stages: proposal-level planning and implementation-level refinement to construct the graph, followed by graph-guided code generation with test validation. To evaluate this setting, we construct RepoCraft, benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo generates repositories averaging nearly 36K LOC, roughly 3.9 the strongest baseline (Claude Code) and about 64 other baselines. It attains 81.5% functional coverage and 69.7% pass rate, exceeding Claude Code by 27.3 and 35.8 percentage points, respectively. Further analysis shows that RPG models complex dependencies, enables progressively more sophisticated planning through near-linear scaling, and enhances LLM understanding of repositories, thereby accelerating agent localization."
        },
        {
            "title": "Introduction",
            "content": "Recent large language models (LLMs) have shown strong performance on function-level and file-level code generation, reliably producing functions and files from natural language descriptions [14]. However, scaling this capability from isolated functions to generate large-scale software repositories from scratch remains fundamental challenge. The core difficulty is bridging the gap between high-level user intent and the repositorys intricate network of files, classes, and dependencies[5, 6]. Successfully navigating this gap necessitates process of progressive planning, which naturally decomposes into two complementary phases: proposal-level planning, which determines what to build by defining the functional scope and key capabilities, and implementation-level planning, which determines how to build it by specifying the file structure, interfaces, dependencies, and data flows. Prior work has approached this repository-level planning challenge through three paradigms. Distributed planning leverages multi-agent coordination [7, 8], where specialized roles (e.g., manager, architect, engineer) negotiate between high-level requirements and low-level implementation under predefined Standard Operating Procedures (SOPs). Staged planning follows fixed workflows [9, 10], decomposing development into sequential phases that first establish architectural skeletons and then expand into detailed code. Externalized iterative planning adopts general-purpose agent Equal contribution Corresponding author: xinzhang3@microsoft.com PREPRINT - SEPTEMBER 22, 2025 frameworks [1114], where intermediate plans are externalized (e.g., in markdown files) and refined across iterations to connect intent with execution. While these paradigms differ in execution, they share critical vulnerability: reliance on natural language as the intermediate representation for planning. We argue this is fundamental limitation. Natural language, while ideal for human-computer interaction, is inherently ambiguous (making intent/constraints hard to disambiguate in code search [15]), unstructured (lacking explicit hierarchy and dependency tracking for complex reasoning [16]), and fragile over long horizons (static plans degrade across iterations without adaptive adjustment [17]). When extended to automatic repository generation, this limitation yields two failure modes: (1) unstable proposal-level planning, where generated functionalities are incomplete, overlapping, or unevenly scoped, making it difficult to ensure systematic coverage of requirements [18]; and (2) fragmented implementation-level planning, where function implementations and architectural plans drift across iterations, causing inconsistencies in dependencies, data flows, and modular boundaries [19, 20]. To address these limitations, we introduce the Repository Planning Graph (RPG), persistent and evolvable representation that unifies planning and implementation for repository generation. RPG encodes both functional goals and implementation design in single graph: nodes specify hierarchical capabilities and their corresponding files, classes, and functions, while edges encode semantic relations and data flows. By replacing free-form natural language with structured medium, RPG provides compact, interpretable basis for consistent long-horizon planning. Building on RPG, we develop ZeroRepo, graph-driven framework for controllable repository generation. Given user specification, ZeroRepo proceeds in three stages. (1) Proposal-Level Construction organizes and refines requirements into functional graph by retrieving nodes from large-scale feature tree. (2) Implementation-Level Construction expands this graph into the full RPG by encoding file skeletons, interfaces, and data flows. (3) Graph-Guided Code Generation traverses the RPG in topological order, applying test-driven development with graph-guided localization and editing to iteratively implement and validate components. To evaluate agents ability in end-to-end repository generation, we introduce RepoCraft, benchmark of six real-world projects with 1,052 tasks. On RepoCraft, ZeroRepo attains 81.5% functional coverage and 69.7% pass rate, exceeding the strongest baseline (Claude Code) by 27.3 and 35.8 points, while producing repositories averaging 36K Code Lines, about 3.9 larger than Claude Code and 64 larger than others. Further analysis shows that the Repository Planning Graph (RPG) captures complex dependencies, including inter-module data flows and class-level relations. It enables near-linear scaling of functionality and code size, supporting more complex planning and providing robust foundation for large-scale repositories and long-horizon development. As global, functionality-oriented representation, RPG also improves agents repository interpretation and localization. Our Contributions are list below: 1. We introduce the Repository Planning Graph (RPG), unified representation that integrates proposaland implementation-level planning, encoding functional goals, file structures, data flows, and function designs. 2. Building on RPG, we develop ZeroRepo, graph-driven framework that constructs the graph via proposaland implementation-level planning, followed by graph-guided code generation with test validation. 3. To evaluate planning ability for repository generation from scratch, we construct RepoCraft, benchmark of six real-world projects with 1,052 tasks measuring functional coverage, accuracy, and code scale. 4. On RepoCraft, ZeroRepo attains 81.5% functional coverage and 69.7% pass rate, and produces repositories averaging 36K Lines of Code, which is about 3.9 larger than the strongest baseline (Claude Code) and nearly 64 larger than others. Further analysis shows that RPG captures complex dependencies, supports near linear scaling, and improves agents repository understanding, thereby accelerating localization."
        },
        {
            "title": "2 Related Work",
            "content": "LLM-based Code Generation SOTA models (e.g., GPT-4o [21], Claude 4 [22], Gemini [23], DeepSeek-R1 [24]) excel at diverse SWE tasks, including code completion, test generation [25, 26], refactoring [27], and program repair [28]. Instruction-tuned variants (e.g., Qwen-Coder [29], EpiCoder [2]) further improve reliability. These advances establish strong function-level performance, laying the foundation for progress toward broader software engineering tasks. Agents for Repository-Level Generation Agent frameworks embed LLMs in planningcoding loops [30, 31]. Multi-agent systems (e.g., ChatDev [8], MetaGPT [7]) assign roles under fixed procedures, while workflow systems (e.g., Paper2Code [9], AutoP2C [10]) decompose synthesis into phases. Industrial systems (e.g., Codex [12], Claude Code [32]) extend these ideas to multi-file projects. However, most rely on ad-hoc natural language plans without persistent structural representations, often leading to fragmented implementations. By contrast, ZeroRepo employs graph-guided abstraction that enforces structured planning and implementation guide. 2 PREPRINT - SEPTEMBER 22, 2025 Figure 1: The ZeroRepo pipeline for repository generation. (A) Proposal-level construction translates specifications into functionality graph. (B) Implementation-level construction refines it via (B1) file-structure encoding into file-augmented graph and (B2) data-flow/function encoding into the final Repository Planning Graph (RPG). (C) Graph-guided code generation traverses RPG in topological order to produce stable repository."
        },
        {
            "title": "3 Repository Planning Graph Construction",
            "content": "To address the ambiguity of natural language plans, we propose the Repository Planning Graph (RPG), structured representation that encodes repository functionality and implementation logic as nodes and edges. Building on RPG, we develop ZeroRepo, framework for repository generation from scratch. This section first introduces the structure of RPG (3.1), and then explains how ZeroRepo constructs it through proposal-level planning (3.2) and implementation-level refinement (3.3). The overall pipeline is shown in Figure 1(A-B)."
        },
        {
            "title": "3.1 Repository Planning Graph Structure",
            "content": "As shown in Figure 2, RPG provides unified representation for repository planning by encoding functionality and implementation in structured graph, rather than unstable natural language descriptions. Its nodes carry dual semantics. At the functional level, they represent progressively refined capabilities: high-level modules (e.g., algorithms, evaluation) decompose into mid-level components and ultimately into leaf nodes corresponding to concrete algorithms. At the structural level, this hierarchy naturally mirrors repository organization: root nodes typically align with file regions, intermediate nodes with files, and leaf nodes with functions or classes, thereby unifying functional decomposition with code structure. Beyond the hierarchical nodes, edges in RPG capture execution dependencies across granularity. Inter-module edges (black arrows in Figure 2) encode data flows between mod3 Figure 2: Example of Repository Planning Graph. Solid black lines show hierarchical containment, black arrows depict inter-module data flows (AB), and dashed gray arrows represent intra-module file ordering. PREPRINT - SEPTEMBER 22, 2025 ules, such as outputs from Data Loading feeding into ML Algorithms and then into Evaluation. Intra-module edges (gray dashed arrows) capture file-level orderings; for instance, load_data.py precedes preprocess.py, with outputs propagated to preprocessing. Collectively, these edges impose topological order that aligns functional decomposition with code organization, ensuring coherence between global execution semantics and local implementation."
        },
        {
            "title": "3.2 Proposal-Level Construction",
            "content": "At the proposal level, the aim is to translate high-level user specifications into coherent functionality graph. This involves three steps: grounding functionalities in large-scale Feature Tree, selecting repository-aligned subtree via exploreexploit search, and refactoring it into the graph. The full algorithm is detailed in Appendix A.1. Global Tree as Knowledge Base LLMs alone provide unstable and biased capability enumeration, often yielding incomplete coverage [33, 34]. To stabilize planning, we ground functionality selection in the EpiCoder Feature Tree [2], large-scale ontology of over 1.5M software capabilities. Its broad coverage and hierarchy serve as structured prior, mitigating randomness and bias while ensuring repository functionalities are systematically captured and diverse. For efficient retrieval, each feature node is embedded into vector representation, with its full hierarchical path stored as metadata in vector database. This design preserves both semantic similarity and structural context, enabling precise and scalable functionality grounding. Detailed statistics of the Feature Tree are provided in Appendix A.2. ExploreExploit Subtree Selection Using the Feature Tree as structured knowledge base, the first step is to construct repo-aligned subtree tailored to the users goal. Exhaustive enumeration is infeasible at the 1.5M scale, so ZeroRepo incrementally expands the subtree via an exploreexploit strategy. (1) Exploitation ensures precision: we retrieve top-k feature paths most aligned with the user goal and augment them with keywords suggested by LLM queries. (2) Exploration ensures diversity: we deliberately expand into unvisited regions of the ontology to capture less obvious but relevant functionalities. Candidates from both strategies are filtered by the LLM and integrated into the evolving subtree, yielding balanced and comprehensive foundation for downstream planning. Refactoring by Goal Alignment The repo-aligned subtree, though capturing relevant functionalities, still inherits the generic organization of the global ontology. To align it with the users repository goal, we refactor feature placements into modular functionality graph. The LLM partitions functionalities into cohesive modules following software engineering principles of cohesion and coupling. For instance, in machine learning library, metrics such as silhouette_score are reorganized under an evaluation module rather than within clustering algorithms. The resulting graph establishes clear functional boundaries, encoding proposal-level planning directly into the representation. 3.3 Implementation-Level Construction After proposal-level construction establishes the multi-level functional plan, the graph is further enriched with implementation details, culminating in the complete Repository Planning Graph (RPG) at this stage. The process includes encoding the repositorys file structure, modeling inter-module data flows and intra-module orderings, and specifying concrete functions and interfaces. 3.3.1 File Structure Encoding While proposal-level planning defines modular functionalities, it remains abstract and detached from implementation. To bridge this gap, the graph is extended with folder and file layouts, instantiating repository skeleton that maps functional modules into executable structures, resulting in file-augmented graph. Folder-Level Encoding Proposal-level planning partitions functionalities into modular subgraphs, but this abstraction does not yet define the repositorys structural layout. To bridge the gap, we enrich root nodes with folder-level specifications, assigning each subgraph dedicated directory namespace (e.g., algoritms/, evaluation/). This encoding couples semantic modularity with explicit structural separation, ensuring that descendant functionalities inherit consistent namespace and that the repository skeleton align with high-level capability decomposition. File-Level Encoding Once folder regions are encoded at the root nodes, the graph is enriched by assigning files to intermediate nodes. This step specifies how functionalities within module are grouped into executable files. For example, preprocessing utilities are consolidated into preprocess.py, while models such as linear regression and its variants are grouped into linear_models.py. By embedding file structure in the graph, we preserve semantic cohesion, reduce cross-file coupling, and obtain file-augmented graph that anchors downstream design."
        },
        {
            "title": "3.3.2 Data Flow and Functions Encoding",
            "content": "After obtaining the file-augmented graph, this stage finalizes the full Repository Planning Graph (RPG) by assigning executable roles to leaf nodes. To ensure coherence across modules and functions, we first incorporate interand 4 PREPRINT - SEPTEMBER 22, 2025 intra-module data flows as inputoutput constraints, then abstract shared structures as design anchors, and finally refine leaf nodes into concrete functions or classes. Data-Flow Encoding To ground interface design in execution semantics, the graph is augmented with data-flow edges that capture interand intra-module relations. At the global level, as shown in Figure 2, typed inputoutput flows connect subgraph roots; for example, data-loading module may provide an array of training data to downstream algorithms. At the local level, files within module are planned in specific order, ensuring coherent and dependency-aware implementation. These flows impose hierarchical topological order that constrains and organizes interface design. Abstracting Global Interfaces To improve scalability and maintainability, recurring inputoutput patterns across modules are abstracted into common data structures or base classes, providing design anchors that enforce interface consistency and reduce redundancy. For example, algorithms can be unified under BaseEstimator class to ensure standardized interaction with preprocessing and evaluation modules. Adaptive Interface Design Within each file-level subgraph, leaf features are clustered into executable interfaces according to semantic relatedness. Independent features are implemented as standalone functions, while interdependent features are consolidated into shared classes with individual methods. For example, in Figure 2, load_json and load_csv are grouped into DataLoader class, while regression variants are unified under LinearModels. This adaptive mapping balances granularity with cohesion, yielding complete Repository Planning Graph (RPG) that preserves modularity and semantic consistency at the repository scale."
        },
        {
            "title": "4 Graph-Guided Code Generation",
            "content": "As shown in Figure 1(D), given user query and the completed RPG, ZeroRepo generates repositories by traversing the graph in topological order, ensuring dependencies are implemented before dependents. At each leaf node, test-driven development (TDD) is applied: test is derived from the task specification, after which the corresponding functions or classes are implemented and validated against it; failing cases trigger revisions until the test passes. Only functions that pass all tests are committed to the repository, enabling incremental expansion while preserving stability. Additional details are provided in Appendix C. Graph-Guided Localization and Editing To handle implementation and debugging requests, we adopt two-stage workflow: first localizing the target in the RPG, then editing the associated code. Localization leverages the graphs global structure and three complementary tools: (1) RPG-Guided search, which uses functionality-based fuzzy matching to identify candidate definitions; (2) repository code view, retrieving full interface bodies for inspection or modification; and (3) dependency exploration, tracing edges to reveal related modules and interactions. Once localized, the agent revises or generates the corresponding code to complete the requested implementation or repair. Graph-Guided Test Validation To ensure correctness and contain errors early, validation follows staged workflow aligned with the graph. Each function or class is first verified in isolation through unit tests automatically derived from its docstring. Validated components trigger regression tests upon modification, while completed subgraphs undergo integration tests to ensure consistent data flows and contracts across modules. lightweight majority-vote diagnosis distinguishes genuine implementation errors from environment or test issues, automatically handling the latter and returning the former for repair through the localizationediting workflow."
        },
        {
            "title": "5 Experiment Setup",
            "content": "Table 1: Overview of the six reference repositories and their paraphrased counterparts (Para. Name) in RepoCraft. #F. Cate. denotes functional categories, #Files the total source files, LOC the effective lines of code, and Task Counts the evaluation tasks for measuring code accuracy."
        },
        {
            "title": "Real Repo",
            "content": "Para. Name #F. Cate. #Files"
        },
        {
            "title": "LOC Code Tokens Task Counts",
            "content": "scikit-learn MLKit-Py pandas sympy statsmodels requests django"
        },
        {
            "title": "TableKit\nSymbolicMath\nStatModeler\nHttpEasy\nPyWebEngine",
            "content": "47 81 40 88 22 42 185 217 699 271 17 681 65,972 106,447 218,924 83,325 2,793 109,457 592,187 943,873 2,339,881 893,824 22,297 917,622 236 175 192 234"
        },
        {
            "title": "5.1 RepoCraft Benchmark",
            "content": "A key challenge in evaluating repository-level generation is the absence of benchmarks that assess end-to-end reasoning and planning from scratch. Existing work either focuses on incremental development (editing, refactoring, or bug 5 PREPRINT - SEPTEMBER 22, 2025 fixing in existing codebases [27, 28, 3537]) or repo generation but provides detailed skeletons and specifications that reduce the need for autonomous planning [38, 39]. RepoCraft addresses this gap by requiring agents to build complete repositories from high-level natural language descriptions and evaluating them against real-world projects in terms of scale, functionality, and correctness, with final statistics shown in Table 1."
        },
        {
            "title": "5.1.1 Reference Repository Selection",
            "content": "To provide strong reference for evaluating reasoning and planning, RepoCraft grounds assessment in six widely used Python projects: scikit-learn, pandas, sympy, statsmodels, requests, and django. These repositories exemplify high-quality engineering practice as they have been developed by active communities, exhibit modular structures, and include comprehensive test suites. Their diversity across scientific computing, data analysis, symbolic reasoning, web services, and full-stack frameworks ensures that the benchmark captures breadth and realism. To mitigate pretraining leakage, we paraphrase their names and descriptions into lexically distinct forms before providing them to agents."
        },
        {
            "title": "5.1.2 Metrics",
            "content": "RepoCraft evaluates generated repositories along four complementary dimensions, with detailed definitions and formulas provided in Appendix D.3.1: Functionality Coverage Coverage is measured as the proportion of functional categories, drawn from official documentation, that are represented by the generated functionalities. category is counted as covered if at least one generated functionality corresponds to it. This metric reflects only the breadth of functionality, without assessing correctness. Reference taxonomies are provided in Appendix D.4. Functionality Novelty Novelty is defined as the proportion of generated functionalities that fall outside the reference taxonomy, i.e., those assigned to the new features category. It captures the systems ability to propose coherent but unseen capabilities beyond the ground specification. Functionality Accuracy Accuracy evaluates correctness at the task level using two metrics: (1) Pass Rate, the fraction of ground-truth tests passed; and (2) Voting Rate, the fraction validated by majority-vote semantic checks. Unlike coverage, accuracy measures whether implementations faithfully realize the intended algorithms. Code-Level Statistics We also report repository scale indicators, including file count, normalized LOC, and token count, measured after excluding test files and non-core implementation artifacts."
        },
        {
            "title": "5.1.3 Functional Task Construction and Evaluation",
            "content": "Figure 3: Pipeline for Evaluation Task Collection. It comprises test file filtering, hierarchical parsing into test trees, sampling and filtering, and final task generation. To assess models planning ability on constructed repositories, we evaluate whether they (i) implement the intended algorithms and (ii) realize them correctly. Simple measures of repository size or coverage are insufficient for this purpose, so RepoCraft introduces task-level evaluations that capture both functional fidelity and implementation accuracy (see Appendix D.2 for details). To enable such fine-grained evaluation, RepoCraft derives tasks from reference repositories. As shown in Figure 3, we collect all available test functions and classes, organize them hierarchically following each projects modular structure, and apply stratified sampling to ensure representative coverage. Trivial or non-algorithmic tests are filtered out, resulting in diverse and computationally meaningful set of 1,052 tasks that closely mirror practical software evaluation. Each task consists of natural language description of the target algorithm, ground-truth test, and necessary auxiliary materials. Evaluation then proceeds in three steps: (1) Localization, where the requirement is mapped to candidate functions or classes in the generated repository; (2) Semantic Validation, where majority-vote checking is applied over two rounds to confirm fidelity to the task specification; and (3) Execution Testing, where the ground-truth test is adapted and executed to verify interface correctness under realistic inputs and outputs. This design aligns with real-world development practices while reducing sensitivity to spurious model errors. In our experiments, we use o3-mini as the base model for automated evaluation. 6 PREPRINT - SEPTEMBER 22, 2025 Table 2: Performance of agent frameworks and model backbones on RepoCraft. \"Nov.\" denotes the novelty rate; the number in parentheses is Novel/Total, where Novel is the number of novel functionalities and Total is the total number of planned functionalities. Gold Projects are used as confidence ablation for the automatic evaluation pipeline, and per-repository detailed results are reported in Appendix E.2."
        },
        {
            "title": "Model",
            "content": "Cov. (%) Nov. (%) (Novel/Total) Pass. / Vot. (%) Files LOC Tokens"
        },
        {
            "title": "OpenHands",
            "content": "Paper2Code o3-mini Qwen3-Coder o3-mini Qwen3-Coder o3-mini Qwen3-Coder o3-mini Qwen3-Coder"
        },
        {
            "title": "Codex CLI\nGemini CLI\nClaude Code CLI",
            "content": "o3 pro gemini 2.5 pro claude 4 sonnet"
        },
        {
            "title": "ZeroRepo",
            "content": "o3-mini Qwen3-Coder 16.6 17.1 18.3 22.1 22.0 21.7 21.7 30.2 28.4 42.0 54. - 81.5 75.1 0.0 (0.0/24.8) 0.0 (0.0/32.7) 9.2 (3.0/32.8) 3.9 (1.5/38.3) 0.3 (0.1/36.5) 0.0 (0.0/33.7) 5.2 (2.1/40.0) 5.5 (4.0/73.8) 0.0 (0.0/48.5) 0.6 (0.8/132.7) 6.7 (41.6/621.0) - 13.6 (151.5/1114.2) 9.2 (108.3/1173.3) 4.5 / 10.2 3.2 / 9.4 2.6 / 10.5 6.9 / 11.6 5.1 / 16.9 5.8 / 11. 6.0 / 15.8 4.9 / 15.9 11.0 / 20.0 14.5 / 37.9 33.9 / 52.5 81.0 / 92.0 69.7 / 75.0 57.3 / 68.0 2.3 8.5 5.8 6. 9.8 8.3 7.2 8.8 5.3 15.2 33.3 345 225.3 326.5 410.3 540. 292.2 458.0 2180.3 3369.3 4458 5422.2 2712.8 4778.3 547.7 1365.2 611.5 1484.8 10,586. 5920.8 14,555.0 6248.5 14,922.2 105,236.2 97,819.7 951,614 271.5 389.0 23,977.3 36,941. 260,761.2 445,511."
        },
        {
            "title": "5.2 Baselines",
            "content": "We evaluate against three baseline paradigms:(1) Multi-agent frameworks (MetaGPT [7], ChatDev [8]) that assign specialized roles for end-to-end development;(2) Workflow-based system (Paper2Code [9]) that follows fixed three-stage pipeline;(3) Terminal agents (Codex CLI [40], Claude Code CLI [13], Gemini CLI [41], OpenHands [11]) that perform natural-language editing, debugging, and testing with interactive multi-file reasoning. For comparability, MetaGPT, ChatDev, and Paper2Code are run with two backbones: o3-mini [42] and Qwen3-Coder480B-A35B-Instruct (Qwen3-Coder) [43]. Terminal agents are evaluated under their official configurations and granted access to web search for external retrieval. To ensure fairness, we extend their runs to 30 planninggeneration iterations by interactively prompting at each step whether additional functionalities should be proposed or implemented. 5."
        },
        {
            "title": "Implementation Details",
            "content": "We conduct 30 iterations for feature selection in Proposal-Level Graph Construction. Each function in Code Generation Stage undergoes up to 8 debugging iterations, with 20 localization attempts per iteration. For test failures, we use 5-round majority voting to attribute the cause and allow up to 20 remediation attempts for testor environment errors."
        },
        {
            "title": "6 Main Results\nRPG enables richer functionality and larger repositories. ZeroRepo demonstrates that RPG–guided planning\nyields repositories of substantially greater scale, diversity, and novelty than existing approaches. On RepoCraft, it\nachieves up to 81.5% coverage with o3-mini, representing a 27.3% absolute improvement over the strongest baseline\n(Claude Code at 54.2%). Beyond covering the required functionality, ZeroRepo also exhibits strong innovation, attaining\nnovelty rates of 11–13% with over 100 new functionalities, whereas most baselines contribute fewer than 10. In terms\nof repository size, ZeroRepo with Qwen3-Coder generates 36K LOC and 445K tokens, corresponding to 3.9× the\ncode scale of Claude Code and about 64× that of other baselines. Among these approaches, ZeroRepo is the closest\nto human-developed Gold Projects, underscoring that RPG serves as the key structured representation for building\nrepositories that are larger, more diverse, and closer to real-world software complexity.",
            "content": "RPG enhances reasoning consistency and structural fidelity. Beyond scale, ZeroRepo delivers substantially higher correctness and stability. To ensure reliability, we first validate the automatic localization and validation pipeline on human-developed Gold Projects, where it achieves 81.0% pass rate and 92.0% voting agreement, establishing the ceiling under our test harness. Under the same protocol, ZeroRepo attains 69.7% pass rate with o3-mini, an absolute improvement of 35.8% compared to the strongest baseline (Claude Code at 33.9%). These results indicate that RPG serves as structured reasoning representation that enforces modular boundaries and functional contracts, thereby supporting coherent planning and yielding repositories that more faithfully realize intended specifications. 7 PREPRINT - SEPTEMBER 22, 2025 Figure 4: Illustration of dependencies in the repository generated by Qwen3-Coder on MLKit-Py, showing (1) the repository skeleton at the folder/file level, (2) inter-module data flows, and (3) class and function dependencies. RPG induces complex data flows and dependencies. To illustrate the capacity of RPGguided planning for generating complex repositories, we visualize ZeroRepo with Qwen3-Coder on the MLKit-Py task. At the file level, RPG organizes coherent folder hierarchy; at the module level, inter-module flows define execution pipelines from data_lifecycle through clustering and models to evaluation; and at the function level, inheritance and invocation edges capture intricate class interactions. These results show that RPG induces layered dependencies and coordinated execution, enabling repositories with both structural complexity and internal coherence."
        },
        {
            "title": "7.1 Analysis of the RPG’s Scalability",
            "content": "RPG enables near-linear growth of repository functionalities. key question in repository-level generation is whether functionalities can continue to expand with iterative planning, or whether growth quickly stagnates. To examine this, we compute the number of planned features at each iteration on RepoCraft, averaging across 30 rounds for strong baselines (Claude Code, Gemini CLI, Codex CLI) and for ZeroRepo. As shown in Figure 5, ZeroRepo exhibits near-linear growth, surpassing 1,100 leaf features with o3-mini, while natural-languagebased baselines display limited scalability: Claude Code grows steadily but with diminishing rates, Gemini CLI increases only slowly before converging by round 30, and Codex ceases to add features after just 45 iterations. These results demonstrate that the RPG provides persistent and extensible planning substrate, enabling high-level goals to be refined into progressively richer functionalities. In contrast to natural-language representations, which degrade in coherence and stagnate, RPG sustains structural consistency and extensibility, establishing it as stronger representational foundation for modeling repositories with increasingly complex functionalities and architectures. 8 Figure 5: Feature comparison of ZeroRepo (o3-mini) against strong baselines (Codex, Gemini CLI, Claude Code) across iterations. RPG ensures near-linear growth in repository size. Functional scalability is only meaningful if it translates into executable code. To examine this, we analyze how repository size evolves across iterations, measured in lines of code (LOC). As shown in Figure 6, ZeroRepo sustains near-linear growth, surpassing 30K LOC within 30 iterations. In contrast, natural-languagebased baselines stagnate: Claude Code and Gemini CLI plateau around 34K LOC, while Codex remains below 1K LOC. This divergence reflects fundamental representational gap. Natural language planning rapidly accumulates inconsistencies across iterations, leading to fragmented specifications that fail to translate into coherent code. In contrast, the RPG maintains persistent, extensible structure in which proposed functionalities are grounded in explicit modules, interfaces, and data flows. This grounding ensures that planned expansions are consistently realized as executable code, producing repositories that grow not only in size but also in organizational coherence. These results highlight the RPGs ability to sustain repository scaling in both volume and integrity, positioning it as robust representational basis for long-horizon code generation."
        },
        {
            "title": "7.2 Analysis of RPG’s Stability and Innovation Potential",
            "content": "A PREPRINT - SEPTEMBER 22, 2025 Figure 6: Scaling behavior of total lines of code across iteration steps on MLKit-Py Table 3: Coverage and Novelty of the Constructed RPG over Iterations on MLKit-Py (o3-mini) RPG supports comprehensive and extensible functionality. central challenge in repository-level generation is ensuring that repositories not only satisfy user-specified requirements but also extend beyond them in coherent way. As shown in Table 3, ZeroRepo steadily increases coverage from 70.2% at 5 iterations to nearly 96% at 30, far surpassing baselines that remain below 60%  (Table 2)  . Simultaneously, it maintains meaningful novelty, reaching 8% with over 100 additional features, whereas baselines contribute fewer than 50. These results suggest that RPG functions as persistent planning substrate, enabling repositories to achieve comprehensive coverage while supporting principled functional growth beyond reference implementations. Representative examples in Appendix E.3 further validates that RPG sustains coherence in both coverage allocation and novel feature proposals. 5 10 15 20 25 30 (ours) 70.2 80.9 83.0 85.1 87.2 95. Iteration Cov. (%) Nov. (%) 4.6 (15.3/336.1) 5.4 (29.01/542.0) 4.9 (39.0/796.0) 5.2 (51.0/981.0) 7.0 (73.5/1043.0) 7.9 (99.4/1258.0)"
        },
        {
            "title": "7.3 Analysis of Graph-Guided Localization",
            "content": "Graph guidance accelerates agent localization. We evaluate the impact of RPG guidance by comparing localization steps with and without graph support  (Table 4)  . Across Integration Testing (Int. Test.), Incremental Development (Incr. Dev.), and Debugging (Debug.), graph-guided search consistently reduces effort by 3050%. This demonstrates that RPG equips agents with principled navigation mechanism, enabling faster dependency tracing, more accurate bug localization, and smoother module integration, thereby improving overall repository development efficiency. Compared to natural-language, RPG offers global structural representation of the repository, enabling agents to localize targets from functionality-wide perspective and accelerating the entire development cycle. Table 4: Ablation Results for the Graph-Guided Localization on MLKit-Py Using o3-mini: Localization steps for three different tasks reported as mean SD. 8.5 2.9 10.8 2.6 6.22 2.10 6.75 1.81 5.83 2.75 ZeroRepo wo Graph 13.3 11.1 ZeroRepo Incr. Dev. Category Int. Test. Debug."
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we presented the Repository Planning Graph (RPG), structured representation that unifies proposaland implementation-level planning for repository generation. Built on RPG, we introduced ZeroRepo, graph-driven framework that achieves state-of-the-art coverage, correctness, and scalability on the RepoCraft benchmark. Our analyses show that RPG models complex dependencies, enables progressively more sophisticated planning through near-linear scaling of functionality and code size, and improves agents repository understanding, thereby accelerating localization. These results highlight the potential of graph-based representations as foundation for advancing long-horizon and large-scale repository generation. 9 PREPRINT - SEPTEMBER 22,"
        },
        {
            "title": "References",
            "content": "[1] Qihao Zhu, Daya Guo, Zhihong Shao, Dejian Yang, Peiyi Wang, Runxin Xu, Wu, Yukun Li, Huazuo Gao, Shirong Ma, et al. Deepseek-coder-v2: Breaking the barrier of closed-source models in code intelligence. arXiv preprint arXiv:2406.11931, 2024. [2] Yaoxiang Wang, Haoling Li, Xin Zhang, Jie Wu, Xiao Liu, Wenxiang Hu, Zhongxin Guo, Yangyu Huang, Ying Xin, Yujiu Yang, et al. Epicoder: Encompassing diversity and complexity in code generation. arXiv preprint arXiv:2501.04694, 2025. [3] Yifei Liu, Li Lyna Zhang, Yi Zhu, Bingcheng Dong, Xudong Zhou, Ning Shang, Fan Yang, and Mao Yang. rstarcoder: Scaling competitive code reasoning with large-scale verified dataset. arXiv preprint arXiv:2505.21297, 2025. [4] Huaye Zeng, Dongfu Jiang, Haozhe Wang, Ping Nie, Xiaotong Chen, and Wenhu Chen. Acecoder: Acing coder rl via automated test-case synthesis. arXiv preprint arXiv:2502.01718, 2025. [5] Hongyuan Tao, Ying Zhang, Zhenhao Tang, Hongen Peng, Xukun Zhu, Bingchang Liu, Yingguang Yang, Ziyin Zhang, Zhaogui Xu, Haipeng Zhang, et al. Code graph model (cgm): graph-integrated large language model for repository-level software engineering tasks. arXiv preprint arXiv:2505.16901, 2025. [6] Haiyang Li. Mrg-bench: Evaluating and exploring the requirements of context for repository-level code generation. arXiv preprint arXiv:2508.02998, 2025. [7] Sirui Hong, Xiawu Zheng, Jonathan Chen, Yuheng Cheng, Jinlin Wang, Ceyao Zhang, Zili Wang, Steven Ka Shing Yau, Zijuan Lin, Liyang Zhou, et al. Metagpt: Meta programming for multi-agent collaborative framework. arXiv preprint arXiv:2308.00352, 3(4):6, 2023. [8] Chen Qian, Wei Liu, Hongzhang Liu, Nuo Chen, Yufan Dang, Jiahao Li, Cheng Yang, Weize Chen, Yusheng Su, Xin Cong, et al. Chatdev: Communicative agents for software development. arXiv preprint arXiv:2307.07924, 2023. [9] Minju Seo, Jinheon Baek, Seongyun Lee, and Sung Ju Hwang. Paper2code: Automating code generation from scientific papers in machine learning. arXiv preprint arXiv:2504.17192, 2025. [10] Zijie Lin, Yiqing Shen, Qilin Cai, He Sun, Jinrui Zhou, and Mingjun Xiao. Autop2c: An llm-based agent framework for code repository generation from multimodal content in academic papers. arXiv preprint arXiv:2504.20115, 2025. [11] Xingyao Wang, Boxuan Li, Yufan Song, Frank Xu, Xiangru Tang, Mingchen Zhuge, Jiayi Pan, Yueqi Song, Bowen Li, Jaskirat Singh, et al. Openhands: An open platform for ai software developers as generalist agents. arXiv preprint arXiv:2407.16741, 2024. [12] OpenAI. Introducing codex: cloud-based software engineering agent, May 2025. URL https://openai.com/ index/introducing-codex/. Accessed: 2025-07-22. [13] Anthropic. Claude code cli: Anthropics agentic terminal-based coding tool. https://www.anthropic.com/ news/claudeâĂŚ4, February 2025. Initial research preview Feb 24, 2025 (Claude 3.7 Sonnet), general availability launched with Claude 4 on May 22, 2025 :contentReference[oaicite:2]index=2. [14] Taylor Mullen and Ryan J. Salva. Gemini cli: your open-source ai agent, June 2025. URL https://blog. google/technology/developers/introducing-gemini-cli-open-source-ai-agent/. [15] Evan Wang, Federico Cassano, Catherine Wu, Yunfeng Bai, Will Song, Vaskar Nath, Ziwen Han, Sean Hendryx, Summer Yue, and Hugh Zhang. Planning in natural language improves llm search for code generation. arXiv preprint arXiv:2409.03733, 2024. [16] Maciej Besta, Nils Blach, Ales Kubicek, Robert Gerstenberger, Michal Podstawski, Lukas Gianinazzi, Joanna Gajda, Tomasz Lehmann, Hubert Niewiadomski, Piotr Nyczyk, et al. Graph of thoughts: Solving elaborate problems with large language models. In Proceedings of the AAAI conference on artificial intelligence, volume 38, pages 1768217690, 2024. [17] Haotian Sun, Yuchen Zhuang, Lingkai Kong, Bo Dai, and Chao Zhang. Adaplanner: Adaptive planning from feedback with language models. Advances in neural information processing systems, 36:5820258245, 2023. [18] Yueheng Zhu, Chao Liu, Xuan He, Xiaoxue Ren, Zhongxin Liu, Ruwei Pan, and Hongyu Zhang. Adacoder: An adaptive planning and multi-agent framework for function-level code generation. arXiv preprint arXiv:2504.04220, 2025. [19] Amr Almorsi, Mohanned Ahmed, and Walid Gomaa. Guided code generation with llms: multi-agent framework for complex code tasks. In 2024 12th International Japan-Africa Conference on Electronics, Communications, and Computations (JAC-ECC), pages 215218. IEEE, 2024. 10 PREPRINT - SEPTEMBER 22, 2025 [20] Nazmus Ashrafi, Salah Bouktif, and Mohammed Mediani. Enhancing llm code generation: systematic evaluation of multi-agent collaboration and runtime debugging for improved accuracy, reliability, and latency. arXiv preprint arXiv:2505.02133, 2025. [21] OpenAI. Gpt-4o. https://openai.com/index/gpt-4o, 2024. [22] Anthropic. Claude 4 sonnet. https://www.anthropic.com/news/introducing-claude-4, May 2025. Released May 22, 2025; hybrid reasoning model with enhanced coding, multimodal support, and extended-thinking capabilities. Accessed: 2025-07-18. [23] Google DeepMind. Gemini 2.5 pro: Advanced reasoning multimodal model. https://blog.google/ technology/google-deepmind/gemini-model-thinking-updates-march-2025/, March 2025. Announced March 25, 2025; general-availability release June 17, 2025; supports advanced reasoning, coding, multimodal inputs. Accessed: 2025-07-18. [24] Daya Guo, Dejian Yang, Haowei Zhang, ..., et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https://arxiv.org/abs/2501.12948. Released January 20, 2025; open-source under MIT license. [25] Muhammad Nouman Zafar, Wasif Afzal, and Eduard Enoiu. Evaluating system-level test generation for industrial software: comparison between manual, combinatorial and model-based testing. In Proceedings of the 3rd ACM/IEEE international conference on automation of software test, pages 148159, 2022. [26] Arghavan Moradi Dakhel, Amin Nikanjam, Vahid Majdinasab, Foutse Khomh, and Michel Desmarais. Effective test generation using pre-trained large language models and mutation testing. Information and Software Technology, 171:107468, 2024. [27] Dhruv Gautam, Spandan Garg, Jinu Jang, Neel Sundaresan, and Roshanak Zilouchian Moghaddam. Refactorbench: Evaluating stateful reasoning in language agents through code. arXiv preprint arXiv:2503.07832, 2025. [28] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. Swe-bench: Can language models resolve real-world github issues? arXiv preprint arXiv:2310.06770, 2023. [29] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [30] Jianwen Luo, Yiming Huang, Jinxiang Meng, Fangyu Lei, Shizhu He, Xiao Liu, Shanshan Jiang, Bin Dong, Jun Zhao, and Kang Liu. Gate: Graph-based adaptive tool evolution across diverse tasks. arXiv preprint arXiv:2502.14848, 2025. [31] Qingyun Wu, Gagan Bansal, Jieyu Zhang, Yiran Wu, Beibin Li, Erkang Zhu, Li Jiang, Xiaoyun Zhang, Shaokun Zhang, Jiale Liu, et al. Autogen: Enabling next-gen llm applications via multi-agent conversations. In First Conference on Language Modeling, 2024. [32] Anthropic. Claude code: agentic coding command-line tool, 2025. URL https://www.anthropic.com/ claude-code. Retrieved July 22, 2025, from https://www.anthropic.com/claude-code. [33] Karthik Valmeekam, Sarath Sreedharan, Matthew Marquez, Alberto Olmo, and Subbarao Kambhampati. On the planning abilities of large language models (a critical investigation with proposed benchmark). arXiv preprint arXiv:2302.06706, 2023. [34] Maayan Armony, Albert Meroño-Peñuela, and Gerard Canal. How far are llms from symbolic planners? an nlp-based perspective. arXiv preprint arXiv:2508.01300, 2025. [35] Linghao Zhang, Shilin He, Chaoyun Zhang, Yu Kang, Bowen Li, Chengxing Xie, Junhao Wang, Maoquan Wang, Yufan Huang, Shengyu Fu, et al. Swe-bench goes live! arXiv preprint arXiv:2505.23419, 2025. [36] Wei Li, Xin Zhang, Zhongxin Guo, Shaoguang Mao, Wen Luo, Guangyue Peng, Yangyu Huang, Houfeng Wang, and Scarlett Li. Fea-bench: benchmark for evaluating repository-level code generation for feature implementation. arXiv preprint arXiv:2503.06680, 2025. [37] Yiming Huang, Jianwen Luo, Yan Yu, Yitong Zhang, Fangyu Lei, Yifan Wei, Shizhu He, Lifu Huang, Xiao Liu, Jun Zhao, et al. Da-code: Agent data science code generation benchmark for large language models. arXiv preprint arXiv:2410.07331, 2024. [38] Wenting Zhao, Nan Jiang, Celine Lee, Justin Chiu, Claire Cardie, Matthias Gallé, and Alexander Rush. Commit0: Library generation from scratch. arXiv preprint arXiv:2412.01769, 2024. [39] Giulio Starace, Oliver Jaffe, Dane Sherburn, James Aung, Jun Shern Chan, Leon Maksin, Rachel Dias, Evan Mays, Benjamin Kinsella, Wyatt Thompson, et al. Paperbench: Evaluating ais ability to replicate ai research. arXiv preprint arXiv:2504.01848, 2025. 11 PREPRINT - SEPTEMBER 22, 2025 [40] OpenAI. Codex cli: Local-first terminal ai coding agent. https://openai.com/introducing-codex, April 2025. Launched April 15, 2025 as an open-source CLI coding agent that executes commands locally :contentReference[oaicite:1]index=1. [41] Google. Gemini cli: Open-source ai agent for developer terminals. https://blog.google/technology/ developers/introducing-gemini-cli-open-source-ai-agent, June 2025. Announced June 25, 2025 as Apache-2.0 licensed CLI using Gemini 2.5 Pro model :contentReference[oaicite:3]index=3. [42] OpenAI. Openai o3-mini reasoning model. System Card and official release (OpenAI), 2025. URL https: //openai.com/index/openai-o3-mini/. Released January 31, 2025. Accessed: 2025-07-28. [43] Qwen Team. Qwen3 technical report, 2025. URL https://arxiv.org/abs/2505.09388. 12 PREPRINT - SEPTEMBER 22, 2025 Appendix of Proposal-Level Graph Construction The construction of the RPG is central to our framework, as it transforms high-level repository goals into structured and persistent representation. The process starts with carefully designed prompting strategies for selecting repositoryrelevant functionalities from the global feature ontology, followed by iterative refinement to ensure both semantic coverage and modular coherence. A.1 Algorithms of Functionality Graph Construction Rejection Sampling Algorithm We extend the base sampling strategy introduced in EPICODER [2] by incorporating diversity-aware rejection mechanism, as shown in Algorithm 1. At each step, candidate tree is accepted only if its overlap with previously sampled nodes is below specified threshold; otherwise, the tree with the minimal overlap is returned. This encourages broader feature space exploration under limited number of sampling iterations. Algorithm 1 Feature Sampling with Diversity-Aware Rejection Sampling Require: Root node R; frequency library ; temperature t; per-tree sample size S; overlap threshold ρ; maximum number of retries Tmax selected_set for = 1 to do GET_CHILDREN(R) if = then break 1: function BASESAMPLE(R, F, t, S) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end function end if fi [i] for all pi fi/ (cid:80) jC fj for all qi TEMPTRANSFORM(pi, t) for all cur_node SAMPLE_NODE(C, [q1, q2, . . . ]) selected_set.ADD(cur_node) cur_node end for return selected_set best_ovl + best_T ; for τ = 1 to Tmax do Tcand BASESAMPLE(R, F, t, S) ovl OVERLAP(Tcand, Sseen) if ovl < best_ovl then best_ovl ovl; 17: function REJECTSAMPLE(R, F, t, S, ρ, Tmax) 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: end if 33: return 34: 35: end function end if if ovl ρ then Tcand break end for if = then best_T Tcand best_T end if move root downward for next step retry up to Tmax times sample candidate tree compute overlap with seen nodes update best candidate so far accept immediately if overlap threshold fallback: choose least-overlap candidate return the final selected tree Repository-Aligned Subtree Selection Algorithm 2 outlines the procedure for constructing repository-specific feature subtree from global feature tree. The algorithm iteratively selects candidate features based on combination of exploitation (retrieving top-scoring nodes) and exploration (sampling unvisited nodes). At each iteration, an LLM agent filters and ranks candidates, proposes missing but relevant features, and performs batch-level self-checks to ensure internal consistency. Accepted candidates are incorporated into the current subtree, and the process continues until 13 PREPRINT - SEPTEMBER 22, 2025 fixed iteration budget is reached. The resulting subtree captures features most relevant to the target repository while balancing coverage and quality. Algorithm 2 Repository-Specific Subtree Selection Require: Global Feature Tree ; Repo description R; iteration budget K; top-k; termination threshold τ ; batch size B; LLM Ensure: Repository-specific subtree 1: Initialize current repo tree ; missing features Cmissing ; visited set 2: for = 1 . . . do 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: end for 17: return Eexploit RETRIEVETOPK(T , R, = top-k) Eexplore SAMPLEUNVISITED(T , V) // Candidate selection via LLM Cexploit LLM.SELECTEXPLOITCANDIDATES(Eexploit, , R) Cexplore LLM.SELECTEXPLORECANDIDATES(Eexplore, , R) Cmissing Cmissing LLM.PROPOSEMISSING(T , R) Craw Cexploit Cexplore Cmissing // Batch self-check (filter useful paths within each batch) for all batch Craw with do LLM.SELFCHECK(T , B) INSERTPATHS(T , B) end for iterate with given budget select promising nodes (exploit) sample unexplored nodes (explore) filter exploit candidates filter explore candidates generate novel candidates not in tree merge all candidate sources process in small batches accept only consistent/relevant paths expand repo-specific tree mark all evaluated paths as visited return final subtree Repo Subtree Reorganization into the functionality graph The algorithm operates in three stages to refactor subtree. In the first stage, an LLM agent iteratively extracts meaningful subtrees from the input hierarchy, organizing them into initial subgraphs until sufficient coverage of leaf nodes is reached. In the second stage, the agent reorganizes the subgraphs by merging semantically related components or moving branches across groups to improve structure. Finally, each subgraph is refined to ensure naming consistency and hierarchical coherence, yielding clean, interpretable functionality graph. A.2 Detailed Construction Process"
        },
        {
            "title": "Level",
            "content": "#Elements Examples 17 1,527 21,739 113,348 613,311 33,801 min with inclusion, multiple with keyword functionality, data structures, data processing text processing, process monitoring, create flowchart heap allocation, dayjs, affine transformation update record by ID, automated versioning, M5 Model Tree add vertices, angular velocity, find minimum by group, mark outlier data 1 2 3 4 5 6 7 Table 5: Statistics of the global feature tree across hierarchical levels, with representative examples from each level. validate against thesaurus, swipe event detection Global Feature Tree The global feature tree consists of more than one million nodes across seven hierarchical levels  (Table 5)  , reflecting broad and diverse functional knowledge base. Nevertheless, the distribution of features across Level-1 categories is highly skewed (Figure 7). In particular, the data processing branch dominates the tree, while many other categories contain only small number of nodes, resulting in pronounced long-tail distribution. Such bias is inherent to real-world software ecosystems, where data processing utilities are disproportionately prevalent compared to specialized functionalities. As consequence, constructing repository-specific RPG requires large-scale filtering and reorganization in order to extract the most relevant features and mitigate the imbalance of the global distribution. 14 Figure 7: Distribution of feature counts under Level-1 categories in the global feature tree. PREPRINT - SEPTEMBER 22, 2025 Figure 8: Evolution of Feature Tree Leaf Counts over Iterations Across Repositories, Highlighting the Differences Between qwen3 coder and o3-mini Model-Specific Growth Patterns Beyond the two traces in Fig. 8. Concretely, qwen3-coder exhibits the most open expansion, with an approximately linear increase in leaf counts per iterationmaximizing coverage early but with higher risk of admitting loosely related features. o3-mini follows with moderately aggressive trajectory, striking balance between breadth and relevance. Together, these curves delineate different points on the recallprecision spectrum of subtree selection strategies that can be matched to repository needs. From Global to Repository-Specific Distributions The comparison between the global feature tree  (Fig.7)  and the final repository-specific profiles (Figs.10) highlights the transformative effect of model-guided reorganization. While the global tree is dominated by generic categories such as data processing and user interaction, the restructured graphs consistently downweight these high-frequency but less discriminative categories and elevate domain-relevant branches to the foreground. This shift effectively counteracts the inherent long-tail bias of the global ontology, redistributing feature density toward categories that better capture repository semantics. As result, the constructed graphs are not only semantically sharper but also more functionally coherent with respect to the target domain. Between models, qwen3-coder favors broad coverage with slower convergence and higher variance, whereas o3-mini achieves more balanced trade-off between generality and specificity. Together, these contrasting tendencies illustrate complementary strategies along the recallprecision spectrum, offering flexibility in matching feature selection to downstream repository needs. Final Graph Structures The final RPGs (Figure 11a, 11b) reveal how repository-specific functionalities are consolidated into coherent modular organizations. Compared to the more diffuse subtree distributions, the resulting graphs exhibit markedly skewed allocation of functionalities across subgraphs: small number of core subgraphs absorb the majority of features, while peripheral subgraphs remain lightweight. This reflects natural modularization process, where dominant clusters correspond to central repository capabilities and minor clusters capture auxiliary or specialized functions. Between models, the partitioning strategies diverge: qwen3-coder produces larger number of medium-sized subgraphs, favoring breadth and parallel coverage; whereas o3-mini yields more balanced distribution, with several subgraphs of comparable size anchoring distinct semantic roles. These differences indicate that model-driven reorganization not only mitigates the global ontologys long-tail bias but also shapes the granularity of modular decomposition, thereby influencing how functional responsibilities are distributed within the generated graph. 15 PREPRINT - SEPTEMBER 22, 2025 Figure 9: Final distribution of feature counts across subtrees for all repositories under o3-mini. The figure shows how features are reorganized after the iterative construction process, reflecting the models preference in balancing breadth and precision. A.3 Prompt Template Parts of Prompt Templates for ExploitExplore Strategy in Subtree Selection Prompt for Exploitation Paths You are GitHub project assistant responsible for expanding repositorys feature tree through path-based modifications to ensure alignment with the projects goals. In each response, you will be given: - An Exploit Feature Tree: curated subset of high-relevance feature paths. - The Current Repository Feature Tree. When returning selected paths, always use \"path/to/feature\" format with / as the separator. Objective (excerpt) Expand the Repository Feature Tree so it: 1. Aligns with the repositorys purpose and scope. 2. Achieves broad coverage across functional areas. 3. Ensures essential capabilities are represented. 4. Identify and fill critical gaps. . . . Selection Principles (excerpt) - Select exclusively from the Exploit Feature Tree. - Include all non-duplicated, useful paths. 16 PREPRINT - SEPTEMBER 22, 2025 Figure 10: Final distribution of feature counts across subtrees for all repositories under qwen3-coder. The figure shows how features are reorganized after the iterative construction process, reflecting the models preference in balancing breadth and precision. - Maintain structural balance by covering underrepresented modules. . . . Exclusions (excerpt) Skip generic infra (e.g., logging, configuration) and abstract goals (e.g., optimize CPU usage). Response Format Respond only with Thought and an Action. <think> Reason about relevance and gaps in the Exploit Tree. </think> <action> { \"all_selected_feature_paths\": [ \"path/to/feature\", ... ] } </action>"
        },
        {
            "title": "Prompt for Exploration Paths",
            "content": "17 PREPRINT - SEPTEMBER 22, 2025 (a) o3-mini Figure 11: Leaf node counts distribution across feature subgraphs in each repository RPG, reorganized by different models. (b) qwen3-coder You are GitHub project assistant responsible for expanding repositorys feature tree through path-based modifications to ensure alignment with the projects goals. In each response, you will be given: - Sampled Feature Tree (Exploration Tree). - The Current Repository Feature Tree. When returning selected paths, always use \"path/to/feature\" format with / as the separator. Objective (excerpt) Improve and expand the Repository Feature Tree so that it: - Aligns with the repositorys purpose and usage scenarios. 18 PREPRINT - SEPTEMBER 22, 2025 - Achieves comprehensive coverage of core and supporting areas. . . . Selection Principles (excerpt) - Select only from the Exploration Tree. - Include actionable, domain-relevant features. - Skip paths already present in the current Repository Tree. - Slight over-inclusion is acceptable. . . . Exclusions (excerpt) Do not select generic infra (e.g., logging, config) or large-scale features (e.g., cloud integrations). Response Format Respond only with single <think> and <action> block. <think> Explain how each Exploration Tree path was evaluated and why it was included or excluded. </think> <action> { \"all_selected_feature_paths\": [ \"path/to/feature\", ... ] } </action>"
        },
        {
            "title": "Parts for Prompt Template for Retrieving Missing Features",
            "content": "Instruction You are GitHub project assistant tasked with designing functionally complete, production-grade repository. Your goal is to identify and recommend missing functional capabilities or algorithms that the project should include, based on its real-world purpose, scope, and domain expectations. Focus on intended functionality not the existing Feature Tree, which may be incomplete. Objective (excerpt) Identify groups of functionally concrete features that: 1. Align with the repositorys domain and purpose. 2. Are entirely missing or only superficially represented. 3. Are specific and implementable (e.g., functions, classes, modules, algorithms). Inclusion Criteria (excerpt) - Must be code-level operations (computation, transformation, algorithm, evaluation). - Realistically implementable within the repositorys scope. - Both standard and advanced algorithms are allowed. Exclusion Criteria (excerpt) Do not include abstract intentions (e.g., improve accuracy), generic infra (e.g., logging, connectors), placeholders, or duplicates. Naming Rules (excerpt) - Use 35 lowercase words, separated by spaces. - Each leaf node must describe concrete algorithm or behavior. - Avoid vague terms, camelCase, or snake_case. Structure Guidelines (excerpt) - Organize into logical hierarchies (up to 45 levels). - Reflect computational architecture, not documentation taxonomy. Response Format Respond with ONLY <think> and <action> block: <think> Reason about functional domains, workflows, and algorithms that are missing from the current Feature Tree but expected in real-world use. </think> <action> { \"missing_features\": { \"root node\": { \"child node 1\": [ \"leaf feature 1\", \"leaf feature 2\" ], \"child node 2\": [ \"leaf feature 3\", \"leaf feature 4\" ] } } } </action> 19 PREPRINT - SEPTEMBER 22, Appendix of Implementation-Level Graph Construction This section illustrates how the RPG is enriched with file organization and function design to form concrete code structures. B.1 Prompt Template for Implementation-Level Graph Construction We provide the prompt templates that guide the transformation from graph subtrees into modular code skeletons."
        },
        {
            "title": "Prompt",
            "content": "You are system architect tasked with designing the inter-subtree data flow for Python software repository. Your goal is to define how data moves between functional modules (subtrees) including who produces it, who consumes it, and how it is transformed and express this as structured, directed graph. ## Data Flow ### Format [ { \"from\": \"<source subtree name>\", \"to\": \"<target subtree name>\", \"data_id\": \"<unique name or description of the data>\", \"data_type\": \"<type or structure of the data>\", \"transformation\": \"<summary of what happens to the data, if anything>\" }, ... ] ### Validity & Structural Constraints 2. Full Connectivity Required - Every subtree listed in {trees_names} must appear in at least one edge. - No subtree should be isolated or unused. 3. Acyclic Structure - The data flow must form Directed Acyclic Graph (DAG): 4. Field Guidelines - data_id: Use unique, descriptive names to identify each data exchange. - data_type: Use precise and interpretable types to describe the structure, format, or abstract role of the data being passed. - transformation: Describe how the data is modified, filtered, enriched, or combined. If unchanged, say \"none\". ... ## Output Format <solution> [ { \"from\": \"...\", \"to\": \"...\", \"data_id\": \"...\", \"data_type\": \"...\", \"transformation\": \"...\" }, ... ] </solution>"
        },
        {
            "title": "Parts of Prompt Templates for Raw Skeleton Mapping",
            "content": "You are repository architect responsible for designing the initial project structure of software repository in its early development stage. Your task is to design clean, modular file system skeleton that organizes the repository into appropriate top-level folders based on these subtrees. ## Requirements 1. The folder structure must clearly separate each functional subtree and reflect logical domain boundaries. 2. Folder names must be concise, meaningful, and follow Python conventions (e.g., snake_case). Names should feel natural and developer-friendly. 3. Folder names do not need to match subtree names exactly. - Treat subtree names as functional labels. - Rename folders as needed for clarity and convention, while preserving the correct mapping. - When assigning subtree to folder, include the exact subtree name in the mapping (e.g., \"ml_models\": [\"Machine Learning\"]). 4. You may choose flat structure (all folders at root level) or nested structure (e.g., under src), depending on what best supports clarity, organization, and practical use. 5. Include commonly used auxiliary folders as appropriate. 6. The proposed structure should balance clarity, scalability, and maintainability. Avoid unnecessary complexity 20 PREPRINT - SEPTEMBER 22, 2025 or excessive nesting. ... ## Output Format Return single JSON-style nested object representing the repository folder structure: - \"folder_name\": [\"Subtree Name\"] means this folder is assigned to specific subtree. The name in the list must match exactly what appears in the given list of subtrees. - \"folder_name\": [] means the folder exists but does not correspond to specific subtree (e.g., utility or support folders). - \"file_name.ext\": null indicates the presence of file. File content is not required."
        },
        {
            "title": "Prompt for Mapping Feature Paths to Python Skeleton Files",
            "content": "You are repository architect tasked with incrementally assigning all remaining leaf-level features from functional subtree into the repositorys file structure. This is an iterative process, You are not expected to assign all features at once each round should make clear, meaningful progress. Your ultimate goal is production-grade file organization that evolves cleanly and logically over time. ## Context In each iteration, you will receive: - list of unassigned leaf features (each is full path like \"a/b/c\"). - designated functional folder under which all new paths must begin. - partial skeleton showing the current structure (existing assignments are hidden). Assign the remaining features to .py file paths that: - Begin with the designated folder. - Group semantically related features together. - Reflect how real developers would modularize logic in production Python codebase. - Prefer organizing major functional categories into subfolders when appropriate. ### File & Folder Structure - Group features by functionality into logically meaningful modules that reflect real-world development practice. - Avoid bundling many unrelated features into single file - If folder contains 10 or more files, introduce subfolders based on semantic structure (e.g., format/, client/, csv/) to keep directories manageable. ### Naming & Organization Guidelines ... ### Examples ... ## Output Format You must structure your response in two clearly separated blocks, each wrapped with the appropriate tags: <think> Explain how you grouped the features into logically coherent modules with clean file and folder structure. Describe how your choices improve clarity, minimize clutter, and reflect good design principles. </think> <solution> { \"<path to file1.py>\": [ \"feature1\", \"feature2\" ], \"<path to file2.py>\": [ \"feature3\" ] } </solution>"
        },
        {
            "title": "Prompt for Converting Subgraphs into Base Classes",
            "content": "You are an expert software engineer tasked with designing reusable abstractions and shared data structures for Python codebase. ... ## Base Class Output Format You must return your design as set of code blocks grouped by target subtree and file: 21 PREPRINT - SEPTEMBER 22, 2025 ## General ### path/to/file.py python ... ... ## <Subtree Name> ### path/to/file.py python ... </solution> ## Design Strategy Abstractions must follow system structure and dataflow analysis, not mechanical repetition. - Shared Data Structures: define for nodes with high out-degree (outputs consumed widely). Good candidates are feature batches, inference results, or training containers. Create global type only when field names, data types, and usage context are stable and consistent. - Functional Base Classes: define for nodes with high in-degree (consuming many inputs). Use when multiple modules share roles (e.g., cleaning, predicting), follow common lifecycles (run(), build(), validate()), or rely on similar hooks. - Principles: - Avoid speculative abstractions. - Prefer fewer, well-justified classes (typically 13 globally). - Capture structural commonality that aids extensibility and coordination. ... ## Output Formate Wrap your entire output in two blocks: <think> ... </think> <solution> ## SubtreeA ### path/to/file.py python ... ... </solution>"
        },
        {
            "title": "Prompt for Mapping Feature Paths to Interfaces",
            "content": "You are designing modular interfaces for large-scale Python system. You are given repository context: overview, tree structure, skeleton, data flow, base classes, upstream interfaces, target subtree, and target file. ### Objective - For each feature, define exactly one interface (function or class). - Provide imports, signature, and detailed docstring (purpose, args, returns, assumptions). - No implementation: use pass. - One interface per block. ### Design Guidelines - Function: simple, atomic, stateless. - Class: stateful, multiple methods, inherits base class, or extensible. - Prefer fewer, well-justified abstractions. - Group only tightly related features. - Use global abstractions sparingly. ### Output Format 22 PREPRINT - SEPTEMBER 22, 2025 Use two blocks: <think> reasoning </think> <solution> design_itfs_for_feature(features=[\"feature/path\", ...]): python # One interface (function or class) with docstring and pass </solution> B.2 Case of Built Skeleton and Designed Interfaces We present the skeleton generated by o3-mini, together with the mapping between the generated skeleton and the nodes of machine learning algorithms. In addition, we illustrate one or two designed base classes as well as concrete functions or classes."
        },
        {
            "title": "Repository Skeleton",
            "content": "src / algorithms / advanced / enhancements / e _ a m s / __init__ . py i _ r g _ a i . py s i i _ s i _ a m s . py c _ e _ a m s . py i a n _ _ a _ r g . py r i _ a m s . py __init__ . py t e _t n e / extended_methods / __init__ . py e l o _ _ e _ r g . py i i _ _ s i . py new_models / __init__ . py _ e _ h u . py _ e _ h u _ i n . py __init__ . py baselines . py p e l _ o h / __init__ . py a d _ s i _ _ e o i _ h . py a d _ e a n _ _ c r . py s i i _ _ t _ o n _ h . py e _ o h _ h . py e e _ l i _ _ m _ e o . py a _ i a n _ h . py e _ i a n _ h . py e a _ e l o _ h . py r i _ _ _ i a n _ h . py i _ e l o _ _ u d _ s i . py _ e e _ _ i a n _ h . py e a n _ h . py __init__ . py ensemble_tree / oo sting_bagging / boosting / __init__ . py s g _ a d _ t s . py s g _a r m . py s g _ a e _ i . py tacking_voting / __init__ . py primary . py secondary . py __init__ . py bagging . py adi ent _b oos tin . py 23 PREPRINT - SEPTEMBER 22, 2025 decision_trees / __init__ . py d t _ s g _ e . py id3 . py post_pruning . py random_forest . py egression_tree . py __init__ . py regression / linear_models / __init__ . py lasso . py ultiple_linear . py polynomial . py simple_linear . py __init__ . py s _ _ r i . py id ge_classifier . py supervised / classification / logistic / __init__ . py cost . py optimization . py sigmoid . py __init__ . py decision_tree . py knearest . py naive_bayes . py support_vector . py __init__ . py unsupervised / clustering / __init__ . py a d _ cl e g . py kmeans . py p e l _ s i . py e o i _ u o / __init__ . py extended_dr . py kernel_pca . py pca . py __init__ . py __init__ . py core / data_conversion / __init__ . py api_requests . py feature_encoding . py ea ur _e tr ct on . py fo rma t_c onv er sio . py sql_queries . py data_transform / __init__ . py filter_advanced . py filter_basic . py join_operations . py ca ling_advanced . py scaling_basic . py sorting . py splitting . py numerics / __init__ . py as ic_statistics . py c _ t i i . py decompositions . py dot_products . py e t _ _ t e . py inversions . py r _ t z o . py r _ r g n . py r i _ t i . py sparse_lu . py preprocessing / __init__ . py csv_io . py data_cleaning . py e o i _ l s . py e _ n r i . py PREPRINT - SEPTEMBER 22, 2025 json_io . py _ n sf a n . py oi e_ ug en at on . py __init__ . py ata _processing / ly sis _p ipe lin / analytical / __init__ . py r t _ o h . py ta_ per tu rba tio . py data_query . py oin_operations . py st_ man ip ula tio . py ample_partition . py aso nal _a nal ysi . py ip li e_ ti it es / __init__ . py data_streaming . py learning_setup . py odel_validation . py f a ce _ r . py __init__ . py a g _ p t / advanced / __init__ . py up ic te _ ha dl ng . py mp ta io _m th ds . py ou tli er_ de tec tio . py preparation / __init__ . py data_splitting . py u i n_ e g . py validation . py __init__ . py type_conversion . py in teg rat io n_m erg / __init__ . py r t _ r a . py merge_operations . py merge_search . py e t on _ r / __init__ . py api_operations . py conversion_auth . py di cti ona ry_ co nfi . py xp rt _ in eg at on . py io_logging . py manipulation / __init__ . py da ta_ man ipu la tio . py shuffling . py string_pivot / __init__ . py pivoting . py st rin g_o per at ion . py n r i _ t _ / __init__ . py t _ r i _ o g . py file_io . py t_ enh anc em ent . py n r i _ m z o . py utilities / __init__ . py metadata . py metrics . py parallel . py sparse_storage . py ext_processing . py validation / inspection / __init__ . py overview . py sorting . py statistics . py __init__ . py data_integrity . py __init__ . py extended_eval / diagnostics / PREPRINT - SEPTEMBER 22, 2025 __init__ . py model_quality . py t i _ g t . py po ral _an al ysi . py __init__ . py explainability . py d i _ e e . py robustness . py math_utils / algorithms / ore_techniques / __init__ . py s i _ _ e o . py r s _ _ t _ l s . py tri x_o pe rat ion . py i a n _ _ e o . py t i al _ h . py __init__ . py auxiliary / __init__ . py a_ man ipu la tio . py m i _ r o . py d t _ _ g . py h_ com put at ion . py ml_utilities . py i a n _ h . py ut ie _v li at on . py ra ndo m_o per at ion . py s _ _ e o . py text_processing . py time_processing . py at _p ep oc ss ng / __init__ . py mo del _pe rsi st enc . py sampling . py text_tools . py performance / __init__ . py drift_detection . py st ati sti cal _t est . py sy ste m_m oni to rin . py e _ i _ l s . py e e _ ev a n / __init__ . py ad van ced _an al ysi . py data_resampling . py va ua io _m tr cs . py e r t _ i . py e _ po _ _ . py i _ r g _ p . py f a _ c r g . py pipeline_creator . py simulation / __init__ . py e r t _ i . py r _ g r _ r . py random_sampling . py simulation . py im _i te po at on . py t i _ l s / __init__ . py cr ipt ive _s tat . py e t ia _ h . py t r e _ l s . py b l i _ e . py vi val _an al ysi . py im _s ri s_ od ls . py ar iance_metrics . py __init__ . py ml_compute / ml_methods / __init__ . py lu te in _m th ds . py e o i _ u o . py svm_validation . py optimization / __init__ . py s i _ p _ h u . py i a n _ o h . py 26 PREPRINT - SEPTEMBER 22, 2025 ra ining_control . py stat_inference / __init__ . py o s _ t _ a d . py o s _ t _ i . py e _ l i _ r . py e _ e o _ r . py a e _ i i . py t i _ t _ r . py time_series / distribution / __init__ . py e n _ li i d . py t u n _ i i . py e t _ o s . py e t _ i e . py __init__ . py forecast_plots . py e t _ o h . py __init__ . py model_eval / deployment / __init__ . py deployment_ops . py ti ng_ res ou rce . py evaluation / diagnostics / __init__ . py a d _ g t . py ba sic _di ag nos tic . py f a ce _ r . py __init__ . py i n al _ l s . py error_display . py r i _ g t . py training_ops . py management / __init__ . py counterfactual . py t _ ex n o . py hyperparameter . py persistence_ops . py e e _ e t . py uncertainty . py visualization / __init__ . py dashboard . py visual_reports . py __init__ . py pipeline / data_cleaning / __init__ . py dimensionality . py encoding . py filtering . py imputation . py knn_methods . py merging . py deployment / __init__ . py export . py e t io _ t . py interactive . py monitoring . py online . py or tin g_f or mat . py standard_plots . py trend_analysis . py evaluation / __init__ . py ro ss_validation . py d_ optimization . py metrics_plots . py is c_evaluation . py monitoring . py t _ ng e g / __init__ . py i _ n r i . py extraction . py 27 PREPRINT - SEPTEMBER 22, 2025 interaction . py selection . py synthesis . py orchestration / __init__ . py configuration . py startup . py transformers . py workflow . py preprocessing / __init__ . py dv anced_parsing . py imputation . py np ut_validation . py training / __init__ . py i g _ u e . py i g _s t e . py tuning / __init__ . py calibration . py evolution . py gaussian . py meta . py parzen . py __init__ . py ui / interactivity / __init__ . py domain_commands . py general_commands . py help_support . py av ga io _a ti ns . py visualization / __init__ . py dashboard . py standard_charts . py __init__ . py __init__ . py main . py setup . py SubGraph-to-Skeleton"
        },
        {
            "title": "Machine Learning Algorithms",
            "content": "A dv nc dE te si ns [ - > dir : src / algorithms / advanced ] Miscellaneous [ - > dir : src / algorithms / advanced / p e l _ o h ] OtherAlgorithms [ - > file : src / algorithms / advanced / p e l _ o h / s i i _ _ t _ o n _ h . py , file : src / algorithms / advanced / p e l _ o h / r i _ _ _ i a n _ h . py , file : src / algorithms / advanced / p e l _ o h / a d _ s i _ _ e o i _ h . py , file : src / algorithms / advanced / p e l _ o h / a _ i a n _ h . py , file : src / algorithms / advanced / p e l _ o h / _ e e _ _ i a n _ h . py , file : src / algorithms / advanced / p e l _ o h / i _ e l o _ _ u d _ s i . py , file : src / algorithms / advanced / p e l _ o h / e a _ e l o _ h . py , file : src / algorithms / advanced / p e l _ o h / e _ o h _ h . py , file : src / algorithms / advanced / p e l _ o h / a d _ e a n _ _ c r . py , file : src / algorithms / advanced / p e l _ o h / e e _ l i _ _ m _ e o . py , file : src / algorithms / advanced / p e l _ o h / e a n _ h . py , file : src / algorithms / advanced / p e l _ o h / e _ i a n _ h . py ] xt nd dT ch iq es [ - > dir : src / algorithms / advanced / e nd _ h u ] ExtendedMethods [ - > file : src / algorithms / advanced / e d _ h ni s / extended_method / e l o _ _ e _ r g . py , file : src / algorithms / advanced / e d_ h u / extended_methods / i i _ _ s i . py ] NewModels [ - > file : src / algorithms / advanced / e nd _ h u / new_models / w _ e _ h u . py , file : src / algorithms / advanced / e ed _ h u / new_models / _ e _ h u _ i n . py ] Baselines [ - > file : src / algorithms / advanced / e nd _ h u / baselines . py ] a m s G r [ - > dir : src / algorithms / advanced / enhancements / e _ a m s ] e E ha m s [ - > file : src / algorithms / advanced / enhancements / e _ a m s / i a n _ _ a _ r g . py , file : src / algorithms / advanced / enhancements / e _ a m s / r i _ a m s . py , file : src / algorithms / advanced / enhancements / e _ a m s / i _ r g _ a i . py , file : src / 28 PREPRINT - SEPTEMBER 22, algorithms / advanced / enhancements / e _ a m s / c _ e _ a m s . py , file : src / algorithms / advanced / enhancements / e _ a m s / s i i _ s i _ a m s . py ] Regression [ - > dir : src / algorithms / regression / linear_models ] LinearModels [ - > dir : src / algorithms / regression / linear_models ] MultipleLinear [ - > file : src / algorithms / regression / linear_models / multiple_linear . py , file : src / algorithms / regression / linear_models / lasso . py ] y i e s n [ - > file : src / algorithms / regression / linear_models / polynomial . py ] Lasso [ - > file : src / algorithms / regression / linear_models / multiple_linear . py , file : src / algorithms / regression / linear_models / lasso . py ] SimpleLinear [ - > file : src / algorithms / regression / linear_models / simple_linear . py ] therRegression [ - > dir : src / algorithms / regression ] g g s C s c o [ - > file : src / algorithms / regression / ridge_classifier . py ] ElasticNet [ - > file : src / algorithms / regression / s _ _ r i . py ] u v d r g [ - > dir : src / algorithms / unsupervised ] e o i e t [ - > dir : src / algorithms / unsupervised / e o i _ u o ] KernelMethods [ - > file : src / algorithms / unsupervised / e o i _ u o / kernel_pca . py ] OtherDR [ - > file : src / algorithms / unsupervised / e o i _ u o / extended_dr . py ] PCA [ - > file : src / algorithms / unsupervised / e o i _ u o / pca . py ] Clustering [ - > dir : src / algorithms / unsupervised / clustering ] KMeans [ - > file : src / algorithms / unsupervised / clustering / kmeans . py ] OtherClustering [ - > file : src / algorithms / unsupervised / clustering / p e l _ s i . py ] dv nc dC us er ng [ - > file : src / algorithms / unsupervised / clustering / a nc _ s i . py ] e e T M o [ - > dir : src / algorithms / ensemble_tree / boosting_bagging ] BoostingBagging [ - > dir : src / algorithms / ensemble_tree / boosting_bagging ] StackingVoting [ - > file : src / algorithms / ensemble_tree / boosting_bagging / stacking_voting / secondary . py , file : src / algorithms / ensemble_tree / boosting_bagging / stacking_voting / primary . py ] Bagging [ - > file : src / algorithms / ensemble_tree / boosting_bagging / bagging . py ] GradientBoosting [ - > file : src / algorithms / ensemble_tree / boosting_bagging / gr adi ent _bo ost in . py ] Boosting [ - > file : src / algorithms / ensemble_tree / boosting_bagging / boosting / s g_ o h . py , file : src / algorithms / ensemble_tree / boosting_bagging / boosting / s g _ a d _ t s . py , file : src / algorithms / ensemble_tree / decision_trees / d t _ s g _ e . py , file : src / algorithms / ensemble_tree / boosting_bagging / boosting / s g _ a e _ i . py ] DecisionTrees [ - > dir : src / algorithms / ensemble_tree / decision_trees ] ID3 [ - > file : src / algorithms / ensemble_tree / decision_trees / id3 . py ] RegressionTree [ - > file : src / algorithms / ensemble_tree / decision_trees / regression_tree . py ] d t s g e [ - > file : src / algorithms / ensemble_tree / boosting_bagging / boosting / s g _ a d _ t s . py , file : src / algorithms / ensemble_tree / decision_trees / d t _ s g _ e . py ] PostPruning [ - > file : src / algorithms / ensemble_tree / decision_trees / post_pruning . py ] RandomForest [ - > file : src / algorithms / ensemble_tree / decision_trees / random_forest . py ] up rv se Le rn ng [ - > dir : src / algorithms / supervised / classification / logistic ] Classification [ - > dir : src / algorithms / supervised / classification / logistic ] og st cR gr ss on [ - > file : src / algorithms / supervised / classification / logistic / sigmoid . py , file : src / algorithms / supervised / classification / logistic / optimization . py , file : src / algorithms / supervised / classification / logistic / cost . py ] SupportAndTree [ - > file : src / algorithms / supervised / classification / decision_tree . py , file : src / algorithms / supervised / classification / support_vector . py ] NaiveBayes [ - > file : src / algorithms / supervised / classification / naive_bayes . py ] KNearest [ - > file : src / algorithms / supervised / classification / knearest . py ]"
        },
        {
            "title": "Base Classes Example",
            "content": "## General ### src / general / base_components . py python from abc import ABC , abstractmethod class BaseComponent ( ABC ) : \"\"\" Abstract base class for core pipeline components . Provides standard lifecycle : - initialize () : Prepare the component . - process ( data ) : Process input data . - finalize () : Clean up resources . Concrete subclasses must implement process () . \"\"\" 29 PREPRINT - SEPTEMBER 22, 2025 def __init__ ( self ) -> None : self . initialized : bool = False def initialize ( self ) -> None : \"\"\" Set up the component before processing starts . \"\"\" self . initialized = True @ ab stractmethod def process ( self , data ) : \"\"\" Process the input data . Args : data : Input data in predefined schema . Returns : Output data after processing . \"\"\" pass def finalize ( self ) -> None : \"\"\" Tear down or clean up the component after processing . \"\"\" self . initialized = False class Es im to Co po en ( BaseComponent ) : \"\"\" Abstract base class for estimator components ( . . , models ) . Defines the contract for model training and prediction . \"\"\" @abstractmethod def fit ( self , training_data ) -> None : \"\"\" Train the estimator using the provided training data . Args : training_data : Data batch conforming to shared TrainingBatch schema . \"\"\" pass @abstractmethod def predict ( self , input_data ) : \"\"\" Generate predictions based on the input data . Args : input_data : Data in format specified by the pipeline . Returns : Predictions corresponding to the input features . \"\"\" pass def process ( self , data ) : \"\"\" For an estimator , process () defaults to prediction . \"\"\" return self . predict ( data )"
        },
        {
            "title": "Designed Interfaces",
            "content": "## Feature Paths : \" Regression / LinearModels / y i e s n / Cubic Regression with regularization \" , \" Regression / LinearModels / y i e s n / polynomial model fitting \" , \" Regression / LinearModels / y i e s n / cubic regression \" , \" Regression / LinearModels / y i e s n / quadratic regression \" , \" Regression / LinearModels / y i e s n / Quadratic Regression with regularization \" ### src / algorithms / regression / linear_models / polynomial . py from src . general . base_components import st ma or om on nt class y i al r o ( Es im to Co po en ) : 30 PREPRINT - SEPTEMBER 22, 2025 def __init__ ( self , degree : int , u i i _ b : float =0.0) -> None : pass def fit ( self , : list [ float ] , : list [ float ]) -> None : \"\"\" Fit the polynomial regression model to the provided data . Constructs the polynomial features based on the specified degree and applies optional regularization if u i i _ b is provided ( > 0) . Args : ( list [ float ]) : list of feature values . ( list [ float ]) : list of target values corresponding to the features . Returns : None Raises : ValueError : If the degree is not supported or if input lists are empty or mismatched . \"\"\" pass def predict ( self , : list [ float ]) -> list [ float ]: \"\"\" Generate predictions using the fitted polynomial regression model . Transforms the input features into polynomial features and computes the output via the fitted model coefficients . Applies regularization adjustments if the model was fitted with regularization term . Args : ( list [ float ]) : list of feature values for prediction . Returns : list [ float ]: list of predicted values . \"\"\" pass B.3 Patterns in Implementation-Level Graph Construction The mapping from RPGs to code structures exhibits strong isomorphic relationship: each subgraph corresponds to coherent code region, with files, classes, and functions serving as structural anchors. Table 7 illustrates this correspondence for the case of o3-mini during sklearn generation, where algorithmic subgraphs (e.g., ML Algorithms, Data Processing, ML Pipeline) map to larger number of files and functions, while auxiliary subgraphs (e.g., Diagnostics, Visualization) remain compact yet feature-dense. This pattern reflects the semantic granularity of different subgraphs: core computational domains require broader structural scaffolding, whereas specialized domains concentrate more features per unit. Extending to the cross-repository view in Table 6, we observe that both models preserve this structural isomorphism but with distinct emphases: o3-mini tends to distribute features more evenly across units, while qwen3-coder consistently produces the highest feature densities, especially at the class level. Together, these results demonstrate that the graph-to-code translation process not only preserves the hierarchical semantics of the RPG but also manifests in distinct structural footprints that vary with model choice. Table 6: Per-repository structural statistics across o3-mini and qwen-coder. Count = number of entities (Files/Classes/Functions) per repository; Avg Feat. = mean number of features per entity (features per file/class/function)."
        },
        {
            "title": "Files",
            "content": "o3-mini"
        },
        {
            "title": "Files",
            "content": "qwen-coder"
        },
        {
            "title": "Functions",
            "content": "Count Avg Feat. Count Avg Feat. Count Avg Feat. Count Avg Feat. Count Avg Feat. Count Avg Feat. TableKit MLKit-Py StatModeler SymbolicMath PyWebEngine HttpEasy 475 266 219 126"
        },
        {
            "title": "Average",
            "content": "271.7 3.64 4.74 4.71 4.70 3.89 4.17 4.31 252 321 117 95 576 79 240 2.28 1.64 2.47 2.17 1.74 2. 2.12 1092 708 726 370 689 235 636.7 271 566 330 89 482 178 319.3 6.69 2.44 3.48 8.98 3.52 4. 4.90 496 815 573 71 890 239 514 1.62 1.30 1.22 1.73 1.26 1.52 1.44 587 281 411 786 501 488.7 1.50 1.13 1.07 0.86 1.13 1.06 1.12 1.05 1.04 1.02 1.04 1.02 1.03 1.03 PREPRINT - SEPTEMBER 22, 2025 Table 7: Structural distribution of files, classes, functions, and feature densities corresponding to each subgraph in the feature graph of o3-mini during sklearn synthesis. Here, Files/Classes/Functions denote the number of code units mapped from each subgraph; File/Class/Function Features are the total extracted features; and Avg Features/... indicates the average number of features per unit type."
        },
        {
            "title": "File Features Class Features",
            "content": "Function Features Avg Feat./File Avg Feat./Class Avg Feat./Func"
        },
        {
            "title": "ML Algorithms\nMath Utilities\nData Processing\nML Pipeline\nCore Operations\nML Computation\nML Evaluation\nML Diagnostics\nVisualization",
            "content": "58 47 45 38 30 19 17 6 6 171 26 30 23 15 39 12 3 2 67 102 169 149 76 34 51 44 16 323 143 231 202 124 88 77 50 24 256 40 59 42 33 54 25 6 8 67 103 172 160 91 34 52 44 5.57 3.04 5.13 5.32 4.13 4.63 4.53 8.33 4.00 1.50 1.54 1.97 1.83 2.20 1.38 2.08 2.00 4.00 1.00 1.01 1.02 1.07 1.20 1.00 1.02 1.00 1.00 Appendix of Graph-Guided Repository Generation C.1 Details on Localization To facilitate the localization stage in graph-guided repository generation, we designed graph-guided toolset that allows agents to systematically explore and map design-level features onto concrete code artifacts. The tools support both fine-grained inspection of files and interfaces, as well as feature-driven exploration across the repository. Specifically, view_file_interface_feature_map and get_interface_content enable inspection of code structures and retrieval of their implementations, while expand_leaf_node_info and search_interface_by_functionality allow navigation of the RPG and fuzzy semantic search. Finally, the Terminate command ensures that the localization process produces ranked and standardized output. Together, these tools provide structured workflow that balances automation with flexibility, ensuring both accuracy and interpretability in the localization process."
        },
        {
            "title": "Localization Tools",
            "content": "### Interface Inspection Tools - w _ e _ e c _ t _ ( file_path ) Inspects single Python file to list the interface structures ( functions , classes , methods ) it contains , along with the feature mappings they support . * Usage *: Useful for quickly understanding which interfaces exist in given file and the feature tags associated with them . * Example *: begin { verbatim } w _ e _ e c _ t _ ( src / algorithms / classifier . py ) end { verbatim } - _ e c _ t ( target_specs ) Retrieves the full implementation code of specific function , class , or method , given its fully qualified name ( file path + entity name ) . * Usage *: Applied when particular interface has been located and its source code needs to be examined in detail . * Example *: begin { verbatim } _ e c _ t ([ src / core / data_loader . py : DataLoader . load_data ]) _ e c _ t ([ src / core / utils . py : clean_text ]) end { verbatim } ### Feature - Driven Exploration Tools - a _ f _ e _ o ( feature_path ) Given feature path from the implemented feature tree , this tool expands and lists all associated interfaces ( functions or classes ) in structural summary . * Usage *: Applied when analyzing how specific functional leaf node in the design tree maps to repository interfaces . * Example *: begin { verbatim } a _ f _ e _ o ( Algorithm / Supervised Learning / Classification Algorithms / Naive Bayes ) end { verbatim } - r _ e c _ _ c n t ( keywords ) Performs fuzzy semantic search for interfaces based on given keywords and returns the top -5 most relevant interface implementations . 32 PREPRINT - SEPTEMBER 22, 2025 * Usage *: Useful when the exact file or interface name is unknown , but functionality - related keywords are available . * Example *: begin { verbatim } r _ e c _ _ c n t ([ optimize , initialize ]) end { verbatim } ### Termination Tool - Terminate ( result ) Terminates the localization exploration and returns the final ranked list of located interfaces . The result must follow the specified JSON - style format , including file path and interface type ( function , class , or method ) . * Usage *: Invoked after completing exploration to deliver the final interface localization results . * Example *: begin { verbatim } Terminate ( result =[ {\" file_path \": \" to 1_ il _f ll at . py \" , \" interface \": \" method : Class1 . function1 \"} , {\" file_path \": \" to 2_ il _f ll at . py \" , \" interface \": \" function : function2 \"} , {\" file_path \": \" to 3_ il _f ll at . py \" , \" interface \": \" class : Class3 \"} , ]) end { verbatim } C.2 Tools for Coding To support systematic repository modification, we introduce suite of editing tools that operate at multiple levels of code granularity."
        },
        {
            "title": "Editing Coding Tools",
            "content": "### t _ l _ s _ _ e ( file_path , class_name ) Use when : An entire class ( including all methods and its docstring ) requires editing or replacement . Output must : Provide the full class definition , with all methods and docstring . ### t _ h _ _ s _ _ e ( file_path , class_name , method_name ) Use when : single method inside class needs to be edited or replaced . Output must : - Return the full class ClassName : block containing only the target method . - Exclude all unrelated methods . - Do not output the method alone ; it must appear within its class block . ### t _ c n _ _ e ( file_path , function_name ) Use when : top - level function requires editing or replacement . Output must : Provide the full function , including signature , logic , and docstring . ### t _ o _ _ i e _ _ e ( file_path ) Use when : Import statements or top - level assignments in the file need to be added or corrected . Output must : - Contain only import statements and top - level assignments ( no functions or classes ) . - Follow strict import order : (1) standard library , (2) third - party packages , (3) local modules . Important : - Do not remove existing imports unless they are demonstrably incorrect ( . . , invalid path , typo , or non - existent module ) . - Retain imports even if they appear unused , to preserve runtime dependencies . ### Terminate () Use when : All required edits are complete and no further tool invocations are necessary . Important : Do not call prematurely ; ensure the issue is fully resolved before termination . C.3 Graph-Guided Localization and Editing Logs To illustrate how the proposed system operates in practice, we include representative logs that demonstrate the end-toend process: (i) locating relevant interfaces in the repository through RPGguided exploration, and (ii) applying code editing operations to implement the required functionality. These logs capture the step-by-step decision process, tool invocations, and final termination results. PREPRINT - SEPTEMBER 22,"
        },
        {
            "title": "Task Example Trajectory",
            "content": "Task : This issue is to implement the a d f n l n r class in src / symbolic / enhanced / ca lc lu _s mm ti / differential . py . Contributors should aim to encapsulate the necessary transformation logic and ensure the methods integrate well with the rest of the symbolic algebra framework . Clear error handling and input validation are expected in order to gracefully manage undefined behaviors . ============ Localization ================ Agent : < think > will begin by exploring the file \" src / symbolic / enhanced / ca cu us _ su ma io / differential . py \" since the a d f n l n r class should be implemented here . will use the e _ e _ e c _ t _ tool on that file to inspect its structure and get an idea of what is already provided , and any hints or TODOs might be present . </ think > < solution > w _ e _ e c _ t _ ( src / symbolic / enhanced / ca cu us _ su ma ti / differential . py ) </ solution > Env : python ## src / symbolic / enhanced / lc lu _s mm ti on / differential . py from typing import Any , Dict ### Features : Calculus & Advanced Summation / Differential & Asymptotic Methods / Advanced Transformation Methods / asymptotic series expansion , Calculus & Advanced Summation / Differential & Asymptotic Methods / Advanced Transformation Methods / differentiate special functions , Calculus & Advanced Summation / Differential & Asymptotic Methods / Advanced Transformation Methods / simplify inequality expressions class a d f n l n r : def m t _ i _ a o ( self , expression : Any , expansion_order : int ) -> Any : ... def f n t _ c _ c n ( self , expression : Any , function_name : str , ad dit ion al_ par : Dict [ str , Any ]= None ) -> Any : ... def p y _ q al ( self , inequality_expr : Any , tolerance : float =1 -05) -> Any : ... Agent : < think > ll now check the file \" src / symbolic / enhanced / simplification / algebraic . py \" since it is part of the Enhanced Symbolic Algorithms simplification area , and it might contain functions or methods relevant to simplifying inequalities . </ think > < solution > w _ e _ e c _ t _ ( src / symbolic / enhanced / simplification / algebraic . py ) </ solution > Env : python ## src / symbolic / enhanced / simplification / algebraic . py from typing import Any from src . expr_transform . simplification . techniques import l _ m t _ o t _ r n , order_terms from src . symbolic . core . r i _ i a n . tree_utils import ll ct _ li e_ er from src . symbolic . core . r i _ i a n . rewriting import o a e _ r i _ e ### Features : Simplification / Algebraic Simplification / Basic Techniques / simplify expressions def p y _ r ss ( expr : Any ) -> Any : ... Agent : < think > will now search for interfaces related to \" differentiate special function \" or advanced symbolic iffe rentiation since our method f n t _ c _ c n might rely on those routines . This might reveal supporting functions or logic that can be reused . </ think > < solution > r _ e c _ _ c n t ([ differentiate special function , advanced symbolic differentiation ]) </ solution > 34 PREPRINT - SEPTEMBER 22, 2025 Env : Keyword \" differentiate special function \" matched interfaces : class AdvancedExpressionAnalyzer , class AutomaticDifferentiator , class t r l f e a . Keyword \" advanced symbolic differentiation \" matched interfaces : class AutomaticDifferentiator , class IntegerAdditionOperations , class t r l f e a . ## File : src / calculus / differentiation / t _d v v . py from typing import Any , List , Callable ### Features : Differentiation / Specialized / Techniques / validate partial derivatives , Differentiatio / Specialized / Techniques / compute partial derivatives , Differentiation / Specialized / Techniques / symbolic gradient computation , Differentiation / Specialized / Techniques / Jacobian matrix , iffe rentiation / Specialized / Techniques / symbolic hessian computation , Differentiation / Specialized / Techniques / multivariable differentiation class t r l f e a : def i e _ t _ i i ( self , function : Callable [... , Any ] , variables : List [ str ] , computed : Any ) -> bool : ... def p _ t _ i i ( self , function : Any , variable : str ) -> Any : \"\"\" ... def p _ b c _ d t ( self , function : Any , variables : List [ str ]) -> Any : ... def p _ o n _ r ( self , functions : List [ Any ] , variables : List [ str ]) -> Any : ... def p _ b c _ s ( self , function : Any , variables : List [ str ]) -> Any : ... def f n t _ t r l ( self , function : Any , variables : List [ str ]) -> Any : ... ... Agent : < think > Next , ll search for interfaces that mention \" asymptotic series expansion \" because that functionality is required in a d f n l n r . m t _ i _ a o . This might reveal candidate functions that can be reused to generate series expansions . ll also search for interfaces related to \" simplify inequality \" to see if there any existing logic for inequality simplification that we can leverage . Let perform both searches . </ think > < solution > r _ e c _ _ c n t ([ asymptotic series expansion ]) r _ e c _ _ c n t ([ simplify inequality ]) </ solution > Env : Keyword \" asymptotic series expansion \" matched interfaces : class AdvancedExpressionAnalyzer , class MultivariableDifferentiator , class e a lI g o . ## File : src / calculus / differentiation / t l_ i i . py from typing import Any , List , Callable ### Features : Differentiation / Specialized / Techniques / validate partial derivatives , Differentiatio / Specialized / Techniques / compute partial derivatives , Differentiation / Specialized / Techniques / symbolic gradient computation , Differentiation / Specialized / Techniques / Jacobian matrix , iffe rentiation / Specialized / Techniques / symbolic hessian computation , Differentiation / Specialized / Techniques / multivariable differentiation class t r l f e a : ... Agent : < think > We have gathered several candidate supporting interfaces needed for our implementation of a d f n l n r . Our analysis indicates that the following components are very likely to be relevant dependencies : ... These are our top -5 interface - level code units that are most likely to serve as essential dependencies for implementing the class a d f n l n r with its three key methods . 35 PREPRINT - SEPTEMBER 22, </ think > < solution > Terminate ( result =[ {\" file_path \": \" src / symbolic / core / calculus / differentiation . py \" , \" interface \": \" function : f n t _ r i _ e \"} , {\" file_path \": \" src / symbolic / enhanced / simplification / algebraic . py \" , \" interface \": \" function : p y _e e o \"} , {\" file_path \": \" src / expr_transform / transformation / an s _ r i on . py \" , \" interface \": \" class : v e p s A y \"} , {\" file_path \": \" src / arith_algebra / differentiation . py \" , \" interface \": \" class : o i f e a \"} , {\" file_path \": \" src / arith_algebra / equation_solving / no li ne r_ ym ol . py \" , \" interface \": \" class : b c a n v \"} ]) =========== Coding ============ Agent : < think > We need to implement the a d f n l n r class in the file src / symbolic / enhanced / al ul s_ um at on / differential . py . The class should provide three methods . For asymptotic_series_expansion , we will check that expansion_order is positive integer and that the expression is provided . Our implementation will simulate an asymptotic expansion by returning new symbolic expression with type marker ( . . , \" asy mpt oti c_ ser ies \") including the original expression and the specified order . Similarly , f n t _ c _ c n should validate that the provided function name is among set of allowed special functions ( for example , erf , gamma , beta , bessel , hypergeometric ) and then simulate specialized differentiation by returning new symbolic expression that indicates that the transformation has been applied . Finally , p y _ q it will check that the input is structured ( . . dictionary ) and that tolerance is positive float reasonably bounded ( we can enforce tolerance > 0 and maybe below 1 to avoid very high tolerance ) and then simulate simplification by deep - copying the input expression with additional metadata fields . We also include proper error handling and input validation . </ think > < solution > t _ l _ s _ _ e (\" src / symbolic / enhanced / ca cu us _ su ma io / differential . py \" , \" a d f n l n r \") python from typing import Any , Dict import copy class a d f n l n r : \"\"\" Provides advanced differential transformation methods for asymptotic series expansion , differentiating special functions , and simplifying inequality expressions within symbolic mathematics . This class encapsulates transformation techniques that are applied to symbolic expressions in the context of differential and asymptotic methods , enabling enhanced symbolic analyses and computations . Methods : m t _ i _ a o ( expression : Any , expansion_order : int ) -> Any :"
        },
        {
            "title": "Expand the provided expression into its asymptotic series representation up to the",
            "content": "given order . f n t _ c _ c n ( expression : Any , function_name : str , ad di tio nal _pa ram : Dict [ str , Any ] = None ) -> Any : Differentiate symbolic expression that contains special function using advanced rules . p y _ in a y ( inequality_ expr : Any , tolerance : float = 1e -5) -> Any : Simplify symbolic inequality expression using transformation techniques to reduce complexity . \"\"\" ... </ solution > Env : # Code Patch Diff : --- src / symbolic / enhanced / lc lu _s mm ti / differential . py +++ src / symbolic / enhanced / ca cu us _ su ma io / differential . py @@ -1 ,4 +1 ,5 @@ from typing import Any , Dict + import copy class a d f n l n r : \"\"\" 36 PREPRINT - SEPTEMBER 22, 2025 @@ -18 ,6 +19 ,7 @@ p y _i u t ( inequality _expr : Any , tolerance : float = 1e -5) -> Any : Simplify symbolic inequality expression using transformation techniques to reduce \"\"\" O _ C _ C N = { erf , gamma , beta , bessel , hypergeometric } + complexity . def m t _ i _ a o ( self , expression : Any , expansion_order : int ) -> Any : \"\"\" @@ -26 ,15 +28 ,23 @@ Args : expression ( Any ) : The symbolic expression to be expanded . expansion_order ( int ) : The order up to which the series expansion should be computed . Returns : Any : symbolic expression representing the asymptotic series expansion . Edge Cases : - If expansion_order is non - positive , the behavior is undefined . - The function expects valid symbolic expression input . Raises : ValueError : If expansion_order is not positive integer . TypeError : If the expression is not in valid symbolic format ( . . , dictionary ) . \"\"\" pass if not isinstance ( expansion_order , int ) or expansion_order <= 0: - + - - - + + + - + ... Agent : < solution > Terminate () </ solution > C.4 Testing Framework As illustrated in Algorithm 3, the testing framework combines unit tests, regression tests, and integration tests into unified process. Unit tests validate each new patch at fine-grained level, regression tests re-execute existing nodes to ensure backward compatibility, and integration tests verify that multiple patched components interact correctly when combined. As illustrated in Algorithm 4, the testing pipeline proceeds in sequence of stages: branch planning, test generation, execution, judgment, and repair. First, candidate test branch is created for the given code unit(s). Then, test code is generated and wrapped into TestNode or IntegrationTestNode, which is executed inside controlled Docker environment. The execution results are judged by an LLM; if failures are detected, the framework automatically generates fix queries and iteratively repairs the test until validated version is obtained. C.5 Statistics of Three Stage Table 8 demonstrates that graph-guided localization provides reasonable efficiency across repositories, with Incremental Development generally easier to localize than Integration Testing or Debugging. In terms of models, o3-mini achieves higher localization efficiency but with larger variance, whereas qwen3-coder shows more stable yet overall lower efficiency. These results suggest that while graph guidance is effective, model capacity and stability jointly influence localization performance. Table 8: Localization results across six open-source repositories under three task categories: Integration Testing, Incremental Development, and Debugging. Each entry reports the mean performance with standard deviation (meanstd) of the corresponding modeltask pair."
        },
        {
            "title": "TableKit",
            "content": "MLKit-Py"
        },
        {
            "title": "SymbolicMath",
            "content": "o3-mini qwen3-coder"
        },
        {
            "title": "Integration Testing\nIncremental Development\nDebugging",
            "content": "13.332.92 12.305.19 11.595."
        },
        {
            "title": "Integration Testing\nIncremental Development\nDebugging",
            "content": "6.162.37 6.811.87 6.752.21 8.754.32 9.834.00 8.244.40 6.622.12 7.101.98 6.012.16 10.943.44 11.605.09 9.155.55 7.892.42 7.481.85 6.251.82 6.651.98 12.516.67 10.288. 5.932.06 6.981.92 6.622.47 9.243.65 12.626.15 13.027.01 9.243.65 6.491.79 5.942.19 7.883.30 9.935.13 8.906.21 7.883.30 7.121.77 6.421.94 As shown in Table 9, o3-mini achieves relatively high code success rates across repositories, often exceeding 75% and in some cases approaching 90%, whereas qwen3-coder lags behind with rates around 5055%. In contrast, the 37 Algorithm 3 Patch-Oriented Testing with Unit, Regression, and Integration Stages Require: Patch set P; repo skeleton R; dependency code D; existing unit nodes Nu; existing integration nodes Ni; task description Θ PREPRINT - SEPTEMBER 22, 2025 nold FINDEXISTINGUNITNODE(Nu, p) if nold = and SAMESIGNATUREORLOGIC(nold, p) then Regression test: reuse existing node if signature/logic unchanged Extend patch set with dependency patches else Tinte [] nnew nold Tunit []; Ttraj {unit : {}, inte : {}} FINDDEPPATCHES(P) for patch do 1: function TESTPATCHES(P, R, D, Θ) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: end function end if res nnew.EXECUTETEST() Tinte.APPEND(res) end for return Tunit Tinte, Ttraj nnew nold else nnew, traj CREATEORUPDATEUNITNODE(p, D, Θ, nold) Ttraj[unit][p.key] traj end if R.INSERTFILE(nnew.test_file, nnew.test_code) res nnew.EXECUTETEST() Tunit.APPEND(res) end for for patch group clustered by integration-node do nold FINDEXISTINGINTEGRATIONNODE(Ni, G) if nold = and ALLEQUAL(nold, G) then nnew, traj CREATEINTEGRATIONNODE(G, Θ) Ttraj[inte][G] traj R.INSERTFILE(nnew.test_file, nnew.test_code) Regression integration test: reuse existing node corresponding test coverage remains moderate, typically within the 6070% range. Figure 12 further illustrates that coverage fluctuates and tends to decline as code length increases: shorter implementations reach high class-level coverage, but both function-level and overall coverage drop significantly with greater complexity. These results suggest that while current models are increasingly effective at generating functional code, their ability to produce comprehensive and high-quality test cases remains limited, highlighting test generation as key bottleneck for practical deployment. Table 9: Average success rate and test coverage (%) for six repositories across two models."
        },
        {
            "title": "TableKit",
            "content": "MLKit-Py"
        },
        {
            "title": "Success Coverage",
            "content": "o3-mini qwen3-coder 81.8% 55.0% 65.0% 48.0% 82.8% 52.0% 61.0% 46.0% 88.9% 50.0% 64.0% 45.0% 74.7% 53.0% 60.0% 47.0% 71.0% 54.0% 62.0% 48.0% 84.8% 51.0% 59.0% 46.0%"
        },
        {
            "title": "D Details about RepoCraft Benchmark",
            "content": "In this section, we describe the construction of the REPOCRAFT benchmark, covering four key aspects: the choice of repositories, the preparation of test data, the evaluation methodology, and the configuration of agent systems. D.1 Repositories Selection For the benchmark, we curated six representative open-source repositories: scikit-learn, pandas, Django, statsmodels, SymPy, and requests. These projects span diverse functional domains including machine learning, data analysis, web frameworks, statistical modeling, symbolic computation, and HTTP communication, thereby ensuring broad coverage of typical software development tasks. To prevent models from simply memorizing or retrieving solutions from training 38 PREPRINT - SEPTEMBER 22, 2025 Main entry point for testing workflow else UNITTESTNODE(U, C, test_code) INTEGRATIONTESTNODE(U, test_code) // Step 1: Plan test branches branch GENERATECODEBRANCH(C, nold, Tmax) // Step 2: Generate candidate test code test_code GENERATETEST(branch, C, U, nold) // Step 3: Build TestNode if represents integration of multiple units then Algorithm 4 End-to-End Test Generation, Execution, and Repair Require: Repo skeleton R; tested unit(s) ; source code C; optional prior test node nold; maximum retries Tmax 1: function RUNTESTINGPIPELINE(R, U, C, nold) 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: end if 22: // Step 7: Return final validated test node 23: return 24: 25: end function end if // Step 4: Execute test code in Docker result n.EXECUTETEST() output result.stdout result.stderr // Step 5: LLM judge outcome if result contains errors then query GENERATEFIXQUERY(C, test_code, output, branch, reviews) FIXTESTANDENV(query, U, C, output, n) (err_type, reviews) LLMJUDGE(C, test_code, output, branch) if err_type {test_code, environment} then end if data, we deliberately anonymized the repositories by modifying their names and descriptions. Furthermore, the task instructions prohibit directly reusing the original implementations, requiring models to generate solutions guided only by feature specifications. This setup enforces fairer evaluation, focusing on the models capacity for feature-grounded reasoning and code generation rather than exploitation of prior exposure. D.2 Evaluation Tasks Collection To construct diverse and reliable evaluation set, we developed an automated pipeline that extends and systematizes the collection of test functions from the official repositories. Our design leverages the fact that mature open-source projects typically include comprehensive test suites with robust inputs and ground-truth outputs, ranging from unit-level checks to integration-level workflows. These tests provide principled source of evaluation data, ensuring that generated repositories are assessed on both algorithmic diversity and functional correctness. Test Function Harvesting. For each repository, we first gathered all available test functions and classes. These serve as the raw pool of evaluation candidates, capturing the behaviors developers themselves deemed important to verify. Hierarchical Categorization. Next, we organized the collected tests into hierarchical taxonomy. At the top level, categories follow the natural modular structure used by human developers (e.g., metrics, linear_model, decomposition). Within each category, we grouped related test classes and functions by algorithmic target. For example: { \" metrics \": { \" te st_ regression \": { \" functions \": { \" reg_targets \": [ \" t _ _ c _ _ g \" , \" t _ _ c _ _ g _ e o \" ] , \" re si n_ et ic \": [ \" t _ r i _ r \" , \" t _ t _ n _ a _ o _ t t _ _ u \" , ... 39 PREPRINT - SEPTEMBER 22, Figure 12: Test coverage of o3-mini on MLKit-Py during generation. The figure shows how the coverage of generated test functions varies as code length increases. ] , \" pinball_loss \": [ \" t _ n _ b _ s _ _ s t _ d i \" , \" t _ m _ n e _ a e _ i \" , \" t _ b _ s _ a n _ h _ \" ] } } } } This taxonomy mirrors repository semantics: higher levels correspond to broad functional modules, while deeper levels capture fine-grained algorithmic tests. Sampling and Filtering. To ensure balanced coverage, we applied the sampling algorithm(Alg 1) to draw representative subsets of test categories. Each sampled test was then refined into task description that models could follow during generation. Finally, we filtered out cases irrelevant to core algorithmic behavior (e.g., string formatting checks, version consistency tests), retaining only tests that probe substantive computational functionality. Example Task Instance. To illustrate the outcome of the pipeline, consider the following task specification extracted from the Django repository: { } \" category \": \" gis_migrations \" , \" file \": \" tests / gis_tests / gis_migration / test_operations . py \" , \" module \": \" class OperationTests \" , \" cap \": \" spatial_index \" , \" functions \": [ \" t _ a _ e _ t _ e \" , \" t _ e _ l _ _ t _ e \" , \" t _ e _ l _ o _ t _ e \" , \" t _ e _ l _ l e _ h _ t _ e \" , \" t _ e _ l _ h _ t _ e \" ] , \" task_query \": \" You are testing an algorithm that applies migration operations to GIS models , ensuring that spatial indexes on spatial fields are properly created , enabled , disabled , or removed as dictated by the migration specifications .\" , \" id \": \" django -0109\" 40 PREPRINT - SEPTEMBER 22, 2025 Table 10: Overview of the six benchmark repositories in REPOCRAFT. Each repository is anonymized by renaming to prevent direct memorization or retrieval by models. We list both the anonymized names and their original counterparts, together with category, purpose, and scope."
        },
        {
            "title": "Anonymized\nName",
            "content": "scikit-learn MLKit-Py sympy"
        },
        {
            "title": "SymbolicMath",
            "content": "pandas"
        },
        {
            "title": "TableKit",
            "content": "django"
        },
        {
            "title": "PyWebEngine",
            "content": "requests"
        },
        {
            "title": "HttpEasy",
            "content": "statsmodels"
        },
        {
            "title": "Data Analysis\nLibrary",
            "content": "Web Framework Provides efficient tools for data mining and analysis, supporting classical supervised and unsupervised learning algorithms. Enables symbolic mathematics including algebraic manipulation, calculus, simplification, and equation solving. Provides flexible data structures (e.g., DataFrame, Series) for manipulating and analyzing tabular data. High-level framework for rapid development with ORM, routing, templating, and admin support."
        },
        {
            "title": "HTTP Client\nLibrary",
            "content": "Simple, human-friendly library for sending HTTP requests and handling responses."
        },
        {
            "title": "Statistical\nModeling\nLibrary",
            "content": "Provides econometric and statistical modeling tools, including regression, time series, and hypothesis testing. Focuses on model training, evaluation, and selection for standard ML tasks; excludes deep learning and distributed systems. Pure Python implementation of symbolic and algebraic computation, lightweight and extensible, without external dependencies. Supports efficient single-machine structured data processing; excludes distributed data frameworks. Offers an all-in-one toolkit for building web applications on small-scale/single-server deployments. Covers API refor sponses, cookies, sessions, headers; excludes advanced networking and async features. Focuses on classical statistical analysis and diagnostics; excludes modern machine learning and deep learning. requests, Each task is represented by (i) its repository category and file location, (ii) the associated test class and functions, and (iii) natural-language query summarizing the algorithm under test. Given such task, the benchmark provides the algorithm description, its corresponding inputoutput ground truth, and the test method. Evaluation is then conducted along two dimensions: (1) Algorithm Presence whether the generated repository contains an implementation that matches the target algorithm, and (2) Algorithm Correctness whether the adapted tests pass against the generated implementation, reflecting functional accuracy. This dual perspective allows us to measure both coverage of algorithmic functionality and the reliability of generated implementations. D.3 Agent Pipeline The evaluation employs three-stage agent pipeline to connect task descriptions with generated repositories and derive executable judgments of success. Stage 1: Localization. Given task and its algorithmic description, the agent first explores the generated repository to locate candidate functions or classes that may implement the target algorithm. This step uses the exploration tools detailed in Appendix C.1, and produces set of potentially relevant code anchors. Stage 2: Majority-Vote Validation. To verify whether the localized candidates truly correspond to the target algorithm, we employ majority-voting mechanism with large language model (LLM). Each candidate is evaluated five times; the majority outcome is taken as the decision. If the validation fails, the pipeline triggers re-localization attempt. The localizationvalidation loop is retried up to three times; if all attempts fail, the repository is judged to lack an implementation of the algorithm. Stage 3: Test Adaptation and Execution. For validated candidates, the agent then adapts the tasks reference test code. Concretely, the provided ground-truth test (including inputs, outputs, and checking methods) is rewritten to match the naming and structural conventions of the localized function or class. The adapted test is executed, and its outcome determines whether the generated implementation is functionally correct. 41 PREPRINT - SEPTEMBER 22, This pipeline ensures that evaluation captures both coverage (whether an algorithm is present in the generated repository) and correctness (whether its implementation passes the adapted tests). D.3.1 Metrics To comprehensively evaluate the generated repositories, we adopt multi-dimensional set of metrics that capture four complementary aspects: functionality alignment, novelty, execution accuracy, and code scale. The motivation is to move beyond single success/failure judgment and instead characterize (i) whether the right algorithms are generated, (ii) whether new functionalities are introduced, (iii) whether these implementations actually work, and (iv) at what level of scale and complexity they are realized. Together, these metrics provide holistic view of the strengths and limitations of different models. Functionality Coverage. The first question is whether model can reproduce the expected range of functionalities in target repository. We extract feature descriptions from both ground-truth repositories and generated repositories, and define reference set of categories = {c1, . . . , cK} based on official documentation and developer guidelines. Generated functionalities = {g1, . . . , gN } are obtained either from structured intermediate outputs (for agent-based methods) or directly from raw code (for baseline models). To align generated features with reference categories, we perform K-Means clustering with as fixed centroids, plus an additional centroid cOOD for out-of-distribution features. Each generated feature gi is mapped to (gi) {cOOD}, with assignments further refined by an LLM-as-Judge to reduce semantic drift. Coverage is then defined as the fraction of reference categories that are hit by at least one generated feature: Coverage = 1 K (cid:88) j=1 1 [gi G, (gi) = cj] . (1) This metric quantifies how well the generated repository aligns with the intended functionality footprint. Functionality Novelty. Coverage alone cannot distinguish between model that simply memorizes existing categories and one that proposes extensions. To capture creativity and diversity, we measure the proportion of generated functionalities that fall outside the reference taxonomy. Specifically, novelty is the fraction of generated nodes assigned to the out-of-distribution centroid cOOD: Novelty = 1 (cid:88) i=1 1 [f (gi) = cOOD] . (2) High novelty indicates tendency to introduce new capabilities, though such capabilities may or may not be useful. This metric is therefore best interpreted jointly with accuracy (below). Functionality Accuracy. Even if repository covers the right categories, the implementations must be correct. We therefore evaluate repository-specific tasks by checking whether generated code passes adapted test cases. Two complementary statistics are reported: Voting Rate the fraction of tasks where the localizationvalidation pipeline confirms that an implementation of the target algorithm is present. This measures algorithm presence. Success Rate the fraction of tasks where the adapted tests execute successfully. This measures algorithm correctness. Together, these metrics disentangle whether errors stem from missing functionality versus incorrect implementation. Code-Level Statistics. Finally, we report statistics on the scale and complexity of generated codebases. This helps distinguish minimal solutions from more realistic, full-fledged repositories. We compute these metrics over filtered Python source files, excluding directories unrelated to core functionality (e.g., tests, examples, benchmarks). The reported quantities are: File Count: number of valid source files, reflecting modular spread; Normalized LOC: effective lines of code after removing comments, docstrings, and blank lines, capturing implementation size; Code Token Count: number of tokens in normalized code, measured with standard tokenizer, reflecting lexical complexity. 42 PREPRINT - SEPTEMBER 22, 2025 By jointly considering these four dimensions (coverage, novelty, accuracy in terms of presence and correctness, and scale), we obtain nuanced evaluation of generated repositories. This design ensures that models are rewarded not only for producing functional code, but also for producing diverse, accurate, and realistically sized repositories. D.4 Ground-Truth Taxonomy for Coverage and Novelty Calculation { \" Supervised learning \": { \" Linear Models \": [] , \" Kernel ridge regression \": [] , \" Support Vector Machines \": { \" SVC \": [] , \" SVR \": [] , \" OneClassSVM \": [] } , \" Nearest Neighbors \": [] , \" Gaussian Processes \": [] , \" Cross decomposition \": [] , \" Naive Bayes \": [] , \" Decision Trees \": [] , \" Ensembles \": [] , \" Multiclass and multioutput algorithms \": [] , \" Feature selection \": [] , \" Semi - supervised learning \": [] , \" Isotonic regression \": [] , \" Probability calibration \": [] , \" Neural network models ( supervised )\": [] } , \" Unsupervised learning \": { \" Gaussian mixture models \": [] , \" Manifold learning \": [] , \" Clustering \": [] , \" Biclustering \": [] , \" matrix factorization problems \": [] , \" Covariance estimation \": [] , \" Novelty and Outlier Detection \": [] , \" Density Estimation \": [] , \" Neural network models ( unsupervised )\": [] } , \" Model selection and evaluation \": { \" Cross - validation \": [] , \" Tuning the hyper - parameters of an estimator \": [] , \" Validation curves \": { \" Classification Metrics \": [] , \" Regression Metrics \": [] , \" Clustering Metrics \": [] } } , \" Inspection \": { \" Permutation feature importance \": [] } , \" Dataset transformations \": { \" Feature extraction \": { \" Feature hashing \": [] , \" Text feature extraction \": [] , \" Image feature extraction \": [] } , \" Preprocessing data \": { \" Scaling and Normalization \": [] , \" Discretization and Binarization \": [] , \" Polynomial and Non - linear Feature Engineering \": [] , \" Categorical Feature Encoding \": [] , 43 PREPRINT - SEPTEMBER 22, 2025 \" Missing Value Imputation \": [] , \" Kernel and Matrix Centering \": [] } , \" Unsupervised dimensionality reduction \": [] , \" Random Projection \": [] , \" Kernel Approximation \": [] , \" Pairwise metrics , Affinities and Kernels \": [] } , \" Dataset loading utilities \": {} , \" Model persistence \": [] } { \" requests \": { \" Core Request Features \": { \" HTTP Method Support \": [] , \" URL and Query Handling \": [] , \" Request Body Construction \": [] , \" Custom Headers \": [] } , \" Response Handling \": { \" Response Body Access \": [] , \" JSON Processing \": [] , \" Response Metadata \": [] , \" Cookie Handling \": [] } , \" Session Management \": { \" Session Persistence \": [] , \" Session Customization \": [] } , \" Advanced Configuration \": { \" Timeouts and Retries \": [] , \" Redirect Control \": [] , \" Streaming and Chunking \": [] , \" Authentication Support \": [] , \" Event Hooks \": [] } , \" Security and Transport \": { \" SSL Verification \": [] , \" Client Certificates \": [] , \" Transport Control \": [] , \" Proxy Support \": [] } , \" Compliance and Encoding \": { \" Encoding Handling \": [] , \" Standards Compliance \": [] , \" Blocking Behavior \": [] } } }"
        },
        {
            "title": "E Experiment Results",
            "content": "E.1 Baseline Configurations To ensure fair comparison, we evaluate three representative systems for repository synthesis: MetaGPT, ChatDev, and Paper2Code, together with several single-agent LLM baselines. All methods are run with their official or default configurations. 44 PREPRINT - SEPTEMBER 22, 2025 MetaGPT. MetaGPT is multi-agent framework that simulates software company by assigning roles such as Product Manager, Architect, Project Manager, Engineer, and Tester. The agents collaborate following predefined Standard Operating Procedures to complete planning, design, implementation, and debugging. ChatDev. ChatDev also follows company-style organization, where agents take charge of requirement analysis, coding, testing, and review. It uses chat-based interaction mechanism to coordinate between stages. We run ChatDev with its default settings. Paper2Code. Paper2Code is fixed workflow system designed to convert machine learning papers into executable repositories. It follows three-stage pipeline of planning, analysis, and generation, which we execute sequentially using the default setup. Vibe-Coding Agent (OpenHands, Codex, Claude Code, Gemini CLI). For comparison with standalone LLM systems, we configure each model with maximum of 30 iterations. The first round is initialized with the repository description. In each subsequent round, the model receives fixed self-reflection prompt: Please check whether the current repository still has any features that could be enhanced or any missing functionality that needs to be added . If there are no further improvements , or if you consider the task complete , please reply with \" yes \" only . If there are still potential enhancements or improvements to be made , please continue working on them , and do not reply with \" yes \" just because you are concerned about complexity . E.2 Detailed Experiment Results We report the results of different methods on six repositories. For each repository, the methods are evaluated under the same settings to enable direct comparison. Table 11: Performance on the MLKit-Py \"Nov.\" denotes the novelty rate; the number in parentheses is Novel/Total, where Novel is the number of novel functionalities and Total is the total number of planned functionalities."
        },
        {
            "title": "Model",
            "content": "Cov. (%) Nov. (%) (Novel/Total) Pass. / Vot. (%) Files LOC Tokens"
        },
        {
            "title": "OpenHands",
            "content": "Paper2Code o3-mini Qwen3-Coder o3-mini Qwen3-Coder o3-mini Qwen3-Coder o3-mini Qwen3-Coder"
        },
        {
            "title": "Codex CLI\nGemini CLI\nClaude Code CLI",
            "content": "o3 pro gemini 2.5 pro claude 4 sonnet"
        },
        {
            "title": "ZeroRepo",
            "content": "o3-mini Qwen3-Coder 14.9 19.2 8.5 12.8 31.9 34.0 25.5 31.9 31.9 59.6 59. - 97.9 85.1 0.0 (0.0/13.0) 0.0 (0.0/23.0) 14.3 (2/14) 0.0 (0/49) 0.0 (0/39) 0.0 (0/48) 0.0 (0/41) 0.0 (0/118) 0.0 (0/59) 0.0 (0/141) 0.0 (0/163) - 4.7 (54/1258) 15.0 (176/1233) 6.3 / 7.3 9.9 / 12.0 6.3 / 7.3 10.5 / 11.5 11.5 / 13.6 11.0 / 14. 17.8 / 19.9 18.8 / 24.6 11.0 / 16.9 0.0 / 33.5 27.5 / 42.4 85.1 / 98.3 73.5 / 78.7 63.6 / 74.6 3.0 8.0 6 14 26 5 12 14 19 31 185 266 642 95.0 170. 163 280 272 1020 564 1710 829 2316 3559 65972 31596 928.0 1718 2064 3100 2499 10213 6346 20821 8344 24782 37056 351554 741634 E.3 Examples of Coverage Calculation and Novelty Assessment In this subsection, we provide examples of how coverage and novelty are computed from the constructed RPG, illustrating category alignment for coverage and out-of-distribution detection for novelty. Analysis of Coverage Examples. These examples demonstrate that our coverage metric provides reasonable allocation of generated functionalities to reference categories. Core areas such as regression, classification, clustering, and preprocessing are consistently captured, while supporting utilities (e.g., normalization, imputation) are distributed into their respective modules without overlap or misplacement. This validates the soundness of our metric design for assessing functional completeness. Moreover, the RPG ensures that functionalities are not only well aligned with reference categories but also diversified across them, highlighting its effectiveness as planning substrate for repository-level generation. 45 PREPRINT - SEPTEMBER 22, Table 12: Performance on the HttpEasy repo. \"Nov.\" denotes the novelty rate; the number in parentheses is Novel/Total, where Novel is the number of novel functionalities and Total is the total number of planned functionalities."
        },
        {
            "title": "Model",
            "content": "Cov. (%) Nov. (%) (Novel/Total) Pass. / Vot. (%) Files LOC Tokens"
        },
        {
            "title": "OpenHands",
            "content": "Paper2Code o3-mini Qwen3-Coder o3-mini Qwen3-Coder o3-mini Qwen3-Coder o3-mini Qwen3-Coder"
        },
        {
            "title": "Codex CLI\nGemini CLI\nClaude Code CLI",
            "content": "o3 pro gemini 2.5 pro claude 4 sonnet"
        },
        {
            "title": "Human Developers",
            "content": "22.7 31.8 36.4 40.9 22.7 31.8 27.3 50.0 45.5 59.1 50.0 - 0.0 (0/12) 0.0 (0/17) 18.2 (2/11) 3.5 (1/31) 0.0 (0/5) 0.0 (0/20) 0.0 (0/18) 2.7 (1/39) 0.0 (0/19) 3.1 (1/33) 0.0 (0/21) -"
        },
        {
            "title": "ZeroRepo",
            "content": "o3-mini Qwen3-Coder 100.0 95.5 2.05 (7/433) 0.3 (2/854) 5.0 / 15.0 20.0 / 25.0 15.0 / 15.0 20.0 / 30.0 20.5 / 28.2 20.0 / 30. 0.0 / 24.2 0.0 / 45.5 14.0 / 28.0 40.0 / 56.0 36.0 / 42.0 72.3 / 87.2 64.0 / 72.0 54.0 / 64.0 1 4 3 3 2 5 5 1 1 2 167 175 177 323 72 192 377 197 420 436 1802 2023 2055 3151 669 1960 1856 1879 5407 4931 17 109 245 2793 6192 15559 61922 165051 Table 13: Performance on the PyWebEngine repo. \"Nov.\" denotes the novelty rate; the number in parentheses is Novel/Total, where Novel is the number of novel functionalities and Total is the total number of planned functionalities."
        },
        {
            "title": "Model",
            "content": "Cov. (%) Nov. (%) (Novel/Total) Pass. / Vot. (%) Files LOC Tokens"
        },
        {
            "title": "OpenHands",
            "content": "Paper2Code o3-mini Qwen3-Coder o3-mini Qwen3-Coder o3-mini Qwen3-Coder o3-mini Qwen3-Coder"
        },
        {
            "title": "Codex CLI\nGemini CLI\nClaude Code CLI",
            "content": "o3 pro gemini 2.5 pro claude 4 sonnet"
        },
        {
            "title": "ZeroRepo",
            "content": "o3-mini Qwen3-Coder 27.1 18.8 25.0 27.1 31.3 25.0 27.1 43.8 39.6 45.8 64. - 79.2 68.8 0.0 (0/52) 0.0 (0/52) 0.0 (0/40) 0.0 (0/49) 2.0 (1/55) 0.0 (0/52) 0.0 (0/46) 0.0 (0/103) 0.0 (0/88) 0.3 (1/318) 38.1 (669/2165) - 38.2 (566/1680) 18.1 (244/1561) 0.0 / 13.5 0.0 / 9.2 0.0 / 14.2 0.0 / 12.1 0.0 / 14.2 0.0 / 19. 0.0 / 15.6 0.0 / 19.9 12.1 / 26.7 7.6 / 48.1 33.9 / 66.1 81.6 / 86.5 74.1 / 84.4 56.4 / 64.8 2 9 8 18 13 11 10 2 45 80 421 238 372 679 304 619 1761 769 2975 34302 3733 1928 3185 5950 2628 3996 6342 7751 27655 317883 681 430 521 109457 917622 27647 275782 539052 { \" SVR \": [ \" NuSVR \" ] , \" Gaussian mixture models \": [ \" gmm expectation maximization \" , \" dp gaussian mixture \" ] , \" Scaling and Normalization \": [ \" quantile scaling \" , \" scale to [0 , 1]\" , \"z - score scaling \" , \" IQR scaling \" ] , \" Missing Value Imputation \": [ \" mean imputation \" , \" matrix completion imputation \" , \" impute using - nearest neighbors \" , 46 PREPRINT - SEPTEMBER 22, Table 14: Performance on the TableKit repo. \"Nov.\" denotes the novelty rate; the number in parentheses is Novel/Total, where Novel is the number of novel functionalities and Total is the total number of planned functionalities."
        },
        {
            "title": "Model",
            "content": "Cov. (%) Nov. (%) (Novel/Total) Pass. / Vot. (%) Files LOC Tokens"
        },
        {
            "title": "OpenHands",
            "content": "Paper2Code o3-mini Qwen3-Coder o3-mini Qwen3-Coder o3-mini Qwen3-Coder o3-mini Qwen3-Coder"
        },
        {
            "title": "Codex CLI\nGemini CLI\nClaude Code CLI",
            "content": "o3 pro gemini 2.5 pro claude 4 sonnet"
        },
        {
            "title": "ZeroRepo",
            "content": "o3-mini Qwen3-Coder 13.2 6.6 21.1 19.7 11.8 11.8 17.1 17.1 11.8 44.7 52. - 72.4 65.8 0.0 (0/21) 0.0 (0/17) 0.0 (0/36) 0.0 (0/54) 0.0 (0/26) 0.0 (0/23) 9.4 (5/53) 0.0 (0/61) 0.0 (0/23) 0.0 (0/117) 0.0 (0/191) - 21.1 (306/1701) 13.9 (178/1500) 0.0 / 11.5 0.0 / 6.4 0.0 / 15.0 0.0 / 0.0 0.0 / 18.1 0.0 / 12. 0.0 / 23.5 6.2 / 20.4 21.1 / 30.9 38.6 / 48.5 53.1 / 77.7 90.6 / 94.0 81.4 / 88.3 48.0 / 64.8 1 3 2 6 2 7 9 2 15 11 217 477 347 186 332 918 193 174 529 1886 552 1249 8509 1814 1453 3517 1753 1914 5325 19337 6299 12242 83834 106447 943873 37331 395536 389886 Table 15: Performance on the StatModeler repo. \"Nov.\" denotes the novelty rate; the number in parentheses is Novel/Total, where Novel is the number of novel functionalities and Total is the total number of planned functionalities."
        },
        {
            "title": "Model",
            "content": "Cov. (%) Nov. (%) (Novel/Total) Pass. / Vot. (%) Files LOC Tokens"
        },
        {
            "title": "OpenHands",
            "content": "Paper2Code o3-mini Qwen3-Coder o3-mini Qwen3-Coder o3-mini Qwen3-Coder o3-mini Qwen3-Coder"
        },
        {
            "title": "Codex CLI\nGemini CLI\nClaude Code CLI",
            "content": "o3 pro gemini 2.5 pro claude 4 sonnet"
        },
        {
            "title": "ZeroRepo",
            "content": "o3-mini Qwen3-Coder 11.4 5.7 10.2 11.4 13.6 14.8 12.5 13.6 20.5 23.7 34. - 77.3 77.3 0.0 (0/19) 0.0 (0/10) 21.1 (8/38) 0.0 (0/18) 0.0 (0/32) 0.0 (0/27) 21.6 (8/29) 30.0 (12/50) 0.0 (0/23) 0.0 (0/55) 0.0 (0/191) - 15.6 (143/1021) 8.2 (83/1113) 5.6 / 6.1 0.0 / 2.8 1.1 / 9.5 3.2 / 7.7 7.9 / 9.0 9.5 / 12. 0.0 / 10.7 3.2 / 14.0 8.2 / 9.9 13.5 / 23.2 18.4 / 27.8 87.2 / 96.2 76.4 / 81.1 66.2 / 73.9 6 13 9 9 5 9 8 9 6 28 271 220 436 228 726 320 335 670 813 1179 709 736 4043 83325 24141 2330 5435 9644 3797 3338 8476 9793 13519 8473 8063 46182 294292 598058 \" impute with global median \" , \" impute missing data \" , ] , \" Ensembles \": [ \" light gradient boosting \" , \" HistG rad ien tB oo st in \" , \" bagging classification trees \" , \" random forest \" , \" LightGBM \" , \" CatBoost \" , \" XGBoost \" ] , \" Clustering Metrics \": [ \" density peak clustering \" , \" gap statistic \" , \" silhouette score calculation \" , \" inertia calculation \" ] , \" Naive Bayes \": [ 47 PREPRINT - SEPTEMBER 22, 2025 \" multinomial naive bayes \" , \" bernoulli naive bayes \" , \" gaussian naive bayes \" ] , \" Linear Models \": [ \" ridge regression \" , \" lasso regression \" , \" huber regression \" , \" ransac regression \" ] , \" SVC \": [ \" soft margin SVM \" , \" hard margin SVM \" , \" SVM with precomputed kernel \" ] } { \" Proxy Support \": [ \" rotate proxy list \" , \" auto detect system proxy \" , \" custom dns resolver integration \" ] , \" HTTP Method Support \": [ \" send POST request \" , \" GET request with cookies \" , \" send DELETE request \" , \" PUT with JSON payload \" ] , \" URL and Query Handling \": [ \" encode path segments \" , \" parse query string \" , \" normalize request url \" ] , \" Redirect Control \": [ \" auto follow redirects \" , \" limit redirect chain \" ] , \" Authentication Support \": [ \" send basic auth \" , \" include oauth2 bearer token \" , \" refresh auth token \" ] , \" Timeouts and Retries \": [ \" set request timeout \" , \" apply exponential backoff \" , \" custom retry hook \" ] , \" JSON Processing \": [ \" auto deserialize json \" , \" validate json schema \" , \" serialize dict to JSON \" ] , \" SSL Verification \": [ \" ssl hostname verification \" , \" load custom certificates \" ] , \" Streaming and Chunking \": [ \" process chunked response \" , \" resume file download support \" ] 48 PREPRINT - SEPTEMBER 22, 2025 } Analysis of Novelty Examples The novelty cases illustrate two key observations. First, novelty captures meaningful extensions rather than random noise: in MLKit-Py, we see coherent additions such as Prophet forecasting, STL decomposition, and genetic programming feature synthesis, while in StatModeler new capabilities include vector autoregression and Cox proportional hazards models. Second, the new functionalities proposed by the RPG remain reasonable within the target domain: they extend statistical modeling, optimization, or robustness analysis in ways that align with real-world software evolution. Together, these examples confirm that the RPG supports not only stable replication of reference repositories but also the introduction of coherent and domain-consistent innovations. { \" new_features \": [ \" vector autoregression model \" , \" forecasting with Prophet \" , \" genetic programming feature synthesis \" , \" multi - objective bayesian optimization \" , \" online learning \" , \" apriori association mining \" , \" Cox proportional hazards model \" , \" STL decomposition \" , \" temporal drift detection \" , \" fuzz testing \" , \" interactive dashboards \" , \" NoSQL queries \" , \" pareto optimization \" , \" demographic parity test \" , \" secure argument parsing \" , ... ] } { \" new_features \": [ \" vector autoregression model \" , \" forecasting with Prophet \" , \" genetic programming feature synthesis \" , \" multi - objective bayesian optimization \" , \" online learning \" , \" apriori association mining \" , \" Cox proportional hazards model \" , \" STL decomposition \" , \" temporal drift detection \" , \" fuzz testing \" , \" interactive dashboards \" , \" NoSQL queries \" , \" pareto optimization \" , \" demographic parity test \" , \" secure argument parsing \" , ... ] } E.4 Examples of Localization Behavior Graph guidance structures localization into systematic search. Figure 13 shows that with graph guidance, localization behavior follows structured CCG pattern (Coarse Search Content Inspection Global Graph 49 PREPRINT - SEPTEMBER 22, 2025 Figure 13: Aggregated function call frequency distribution across localization steps in all repositories using o3-mini. Exploration). The agent begins by traversing the RPG at coarse level to identify high-level candidates, then inspects content-rich nodes for detailed signals, and finally explores semantically related structures across the graph. Termination calls rise as the search converges. This progression indicates that the RPG reshapes the agents behavior into systematic, relation-aware search process, replacing ad hoc or repetitive probing."
        }
    ],
    "affiliations": [
        "Microsoft",
        "Tsinghua University",
        "University of California, San Diego"
    ]
}