{
    "paper_title": "Generalizable Origin Identification for Text-Guided Image-to-Image Diffusion Models",
    "authors": [
        "Wenhao Wang",
        "Yifan Sun",
        "Zongxin Yang",
        "Zhentao Tan",
        "Zhengdong Hu",
        "Yi Yang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Text-guided image-to-image diffusion models excel in translating images based on textual prompts, allowing for precise and creative visual modifications. However, such a powerful technique can be misused for spreading misinformation, infringing on copyrights, and evading content tracing. This motivates us to introduce the task of origin IDentification for text-guided Image-to-image Diffusion models (ID$^2$), aiming to retrieve the original image of a given translated query. A straightforward solution to ID$^2$ involves training a specialized deep embedding model to extract and compare features from both query and reference images. However, due to visual discrepancy across generations produced by different diffusion models, this similarity-based approach fails when training on images from one model and testing on those from another, limiting its effectiveness in real-world applications. To solve this challenge of the proposed ID$^2$ task, we contribute the first dataset and a theoretically guaranteed method, both emphasizing generalizability. The curated dataset, OriPID, contains abundant Origins and guided Prompts, which can be used to train and test potential IDentification models across various diffusion models. In the method section, we first prove the existence of a linear transformation that minimizes the distance between the pre-trained Variational Autoencoder (VAE) embeddings of generated samples and their origins. Subsequently, it is demonstrated that such a simple linear transformation can be generalized across different diffusion models. Experimental results show that the proposed method achieves satisfying generalization performance, significantly surpassing similarity-based methods ($+31.6\\%$ mAP), even those with generalization designs."
        },
        {
            "title": "Start",
            "content": "Generalizable Origin Identification for Text-Guided Image-to-Image Diffusion Models Wenhao Wang1, Yifan Sun2, Zongxin Yang3, Zhentao Tan2, Zhengdong Hu1, Yi Yang3 1University of Technology Sydney, 2Baidu Inc., 3Zhejiang University 5 2 0 2 4 ] . [ 1 6 7 3 2 0 . 1 0 5 2 : r Figure 1. The illustration for misusing text-guided image-to-image diffusion models in several scenarios: misinformation, copyright infringement, and evading content tracing. Specifically: (a) An altered image originally showing Donald Trump post-assassination is edited to depict Joe Biden instead; (b) The removal of watermark from copyrighted beach image, followed by modifications, could assist in escaping copyright checks; (c) An image of Norwegian government building after an explosion is altered to bypass restrictions, which limit the spread of disturbing images."
        },
        {
            "title": "Abstract",
            "content": "Text-guided image-to-image diffusion models excel in translating images based on textual prompts, allowing for precise and creative visual modifications. However, such powerful technique can be misused for spreading misinformation, infringing on copyrights, and evading content tracing. This motivates us to introduce the task of origin IDentification for text-guided Image-to-image Diffusion models (ID2), aiming to retrieve the original image of given translated query. straightforward solution to ID2 involves training specialized deep embedding model to extract and compare features from both query and reference images. However, due to visual discrepancy across generations produced by different diffusion models, this similarity-based approach fails when training on images from one model and testing on those from another, limiting its effectiveness in real-world applications. To solve this challenge of the proposed ID2 task, we contribute the first dataset and theoretically guaranteed method, both emphasizing generalizability. The curated dataset, OriPID, contains abundant Origins and guided Prompts, which can be used to train and test potential IDentification models across various diffusion models. In the method section, we first prove the existence of linear transformation that minimizes the distance between the pre-trained Variational Autoencoder (VAE) embeddings of generated samples and their origins. Subsequently, it is demonstrated that such simple linear transformation can be generalized across different diffusion models. Experimental results show that the proposed method achieves satisfying generalization performance, significantly surpassing similarity-based methods (+31.6% mAP), even those with generalization designs. 1. Introduction Text-guided image-to-image diffusion models are notable for their ability to transform images based on textual descriptions, allowing for detailed and highly customizable modification. While they are increasingly used in creative industries for tasks such as digital art re-creation, customizing visual content, and personalized virtual try-ons, there are growing security concerns associated with their misuse. As illustrated in Fig. 1, for instance, they could be misused for misinformation, copyright infringement, and evading content tracing. To help combat these misuses, this paper introduces the task of origin IDentification for text-guided Image-to-image Diffusion models (ID2), which aims to identify the original image of generated query from large-scale reference set. When the origin is identified, subsequent compensations include deploying factual corrections for misinformation, enforcing copyright compliance, and keeping the tracing of target content. straightforward solution for the proposed ID2 task is to employ similarity-based retrieval approach. Specifically, Figure 2. The demonstration for visual discrepancy between generated images by different diffusion models. The images generated by various models exhibit distinctive visual features such as realistic textures, complex architectures, life-like details, vibrant colors, abstract expression, magical ambiance, and photorealistic elements. this approach (1) fine-tunes pre-trained network by minimizing the distances between generated images and their origins, and (2) uses the trained network to extract and compare feature vectors from the queries and references. However, this approach is impractical in real-world scenarios. This is because: for most current popular diffusion models, such as Stable Diffusion 2 [36], Stable Diffusion XL [33], OpenDalle [15], ColorfulXL [35], Kandinsky-3 [1], Stable Diffusion 3 [9], and Kolors [17], in training-free manner, text-guided image-to-image translation can be easily achieved by using an input image with added noise as the starting point (instead of starting from randomly distributed noise). Further, as shown in Fig. 2, there exists visual discrepancy across images generated by different diffusion models, i.e., different diffusion models exhibit distinct visual features. An experimental evidence for such discrepancy is that we can train lightweight classification model, such as Swin-S [23], to achieve top-1 accuracy of 95.9% when classifying images generated by these seven diffusion models. The visual discrepancy presents an inherent challenge of our ID2, i.e., the approach mentioned above fails when trained on images generated by one diffusion model and tested on queries from another. For instance, when trained on images generated by Stable Diffusion 2, this approach achieves 87.1% mAP on queries from Stable Diffusion 2, while only achieving 30.5% mAP on queries from ColorfulXL. To address the generalization challenge in the proposed task, our efforts focus primarily on constructing the first ID2 dataset and proposing theoretically guaranteed method. new dataset emphasizing generalization. To verify the generalizability, we construct the first ID2 dataset, OriPID, which includes abundant Origins with guided Prompts for training and testing potential IDentification models. Specifically, the training set contains 100, 000 origins. For each origin, we use GPT-4o [26] to generate 20 different prompts, each of which implies plausible translation direction. By inputting these origins and prompts into Stable Diffusion 2, we generate 2, 000, 000 training images. For testing, we randomly select 5, 000 images as origins from reference set containing 1, 000, 000 images, and ask GPT-4o to generate guided prompt for each origin. Subsequently, we generate 5, 000 queries using the origins, corresponding prompts, and each of the following models: Stable Diffusion 2, Stable Diffusion XL, OpenDalle, ColorfulXL, Kandinsky-3, Stable Diffusion 3, and Kolors. The design of using different diffusion models to generate training images and queries is particularly practical because, in the real world, where numerous diffusion models are publicly available, we cannot predict which ones might be misused. simple, generalizable, and theoretically guaranteed solution. To solve the generalization problem, we first theoretically prove that, after specific linear transformations, the embeddings of an original image and its translation, encoded by the diffusion models Variational Autoencoder (VAE), will be sufficiently close. This suggests that we can use these linearly transformed query embeddings to match against the reference embeddings. Furthermore, we demonstrate that these kinds of feature vectors are generalizable across diffusion models. Specifically, by using trained linear transformation and the encoder of VAE from one diffusion model, we can also effectively embed the generated images from another diffusion model, even if their VAEs have different parameters or architectures (see Section 5.3 for more details). The effectiveness means the similar performance of origin identification for both diffu2 sion models. Finally, we implement this theory (obtain the expected linear transformation) by gradient descending metric learning loss and experimentally show the effectiveness and generalizability of the proposed solution. In summary, we make the following contributions: 1. This paper proposes novel task, origin identification for text-guided image-to-image diffusion models (ID2), which aims to identify the origin of generated query. This task tries to alleviate an important and timely security concern, i.e., the misuse of text-guided image-toimage diffusion models. To support this task, we build the first ID2 dataset. 2. We highlight an inherent challenge of ID2, i.e., the existing visual discrepancy prevents similarity-based methods from generalizing to queries from unknown diffusion models. Therefore, we propose simple but generalizable method by utilizing linear-transformed embeddings encoded by the VAE. Theoretically, we prove the existence and generalizability of the required linear transformation. 3. Extensive experimental results demonstrate (1) the challenge of the proposed ID2 task: all pre-trained deep embedding models, fine-tuned similarity-based methods, and specialized domain generalization methods fail to achieve satisfying performance; and (2) the effectiveness of our proposed method: it achieves 88.8%, 81.5%, 87.3%, 89.3%, 85.7%, 85.7%, and 90.3% mAP, respectively, for seven different diffusion models. 2. Related Works Diffusion Models. Recent diffusion models, including Stable Diffusion 2 [36], Stable Diffusion XL [33], OpenDalle [15], ColorfulXL [35], Kandinsky-3 [1], Stable Diffusion 3 [9], and Kolors [17], have brought significant improvements in visual generation. This paper considers using these popular diffusion models for text-guided imageto-image translation in training-free manner, which is common and cost-effective approach in the real world. Security Issues with AI-Generated Content. Recently, generative models have gained significant attention due to their impressive capabilities. However, alongside their advancements, several security concerns have been identified. Prior research has explored various dimensions of these security issues. For instance, [22] focuses on detecting AIgenerated multimedia to prevent its associated societal disruption. Additionally, [10] and [3] explore the ethical implications and technical challenges in ensuring the integrity and trustworthiness of AI-generated content. In contrast, while our work also aims to help address the security issues, we specifically focus on novel perspective: identifying the origin of given translated image. Image Copy Detection. The task most similar to our ID2 is Image Copy Detection (ICD) [29], which identifies whether query replicates the content of any reference. Various works focus on different aspects: PE-ICD [44] and AnyPattern [43] build benchmarks and propose solutions emphasizing novel patterns in realistic scenarios; ASL [42] addresses the hard negative challenge; Active Image Indexing [11] explores improving the robustness of ICD; and SSCD [31] leverages self-supervised contrastive learning for ICD. Unlike ICD, which focuses on manually-designed transformations, our ID2 aims to find the origin of query translated by the diffusion model with prompt-guidance. 3. Dataset To advance research in ID2, this section introduces OriPID, the first dataset specifically designed for the proposed task. The source images in OriPID are derived from the DISC21 dataset [29], which is subset of the real-world multimedia dataset YFCC100M [39]. As result, OriPID is diverse and comprehensive, encompassing wide range of subjects found in real-world scenarios. An illustration of the proposed dataset is shown in Fig. 3. Training Set. The training set comprises (1) 100, 000 origins randomly selected from the 1, 000, 000 original images in DISC21, (2) 2, 000, 000 guided prompts (20 for each origin) generated by GPT-4o (for details on how these prompts were generated, see Supplementary (Section 9)), and (3) 2, 000, 000 images generated by inputting the origins and prompts into Stable Diffusion 2 [36]. Test Set. We design the test set with focus on realworld/practical settings. On one hand, we use seven popular diffusion models, namely, Stable Diffusion 2 [36], Stable Diffusion XL [33], OpenDalle [15], ColorfulXL [35], Kandinsky-3 [1], Stable Diffusion 3 [9], and Kolors [17], to generate queries. This setting well simulates real-world scenarios where new diffusion models continuously appear, and we do not know which one is being misused. On the other hand, for each diffusion model, we generate 5, 000 queries to match 1, 000, 000 references inherited from DISC21. This setting mimics the real world, where many distractors are not translated by any diffusion models. Scalability. Currently, we only use Stable Diffusion 2 to generate training images. However, our OriPID can be easily scaled by incorporating more diffusion models for training, which may result in better generalizability. Furthermore, we only use 100, 000 origins and generate 20 prompts for each origin. Researchers can scale up our dataset by using the entire 1, 000, 000 original images and generating more prompts with the script in Supplementary (Section 9). 4. Method To solve the proposed ID2, we introduce simple yet effective method, which is theoretically guaranteed and emphasizes generalizability. This section first presents two 3 Figure 3. The images in our dataset, which is diverse and comprehensive. Specifically, it encompasses variety of subjects commonly found in real-world scenarios where issues such as misinformation, copyright infringement, and content tracing evasion occur. For instance, our dataset includes images of nature, architecture, animals, planes, art, and indoor. Note that for simplicity, we omit the prompts here. Please refer to Supplementary (Section 8) for examples of prompts and generations. theorems regarding existence and generalizability, respectively. Existence means that we can linearly transform the VAE embeddings of an origin and its translation such that their distance is close enough. Generalizability means that the linear transformation trained on the images generated by one diffusion model can be effectively applied to another different diffusion model. Finally, we show how to train the required linear transformation in practice. 4.1. Existence Theorem 1. Consider well-trained diffusion model F1 with an encoder E1 from its VAE and its text-guided image-to-image functionability achieved by denoising noised images. There exists linear transformation matrix W, for any original image and its translation g1, such that: E1(g1) = E1(o) W. (1) Note that we omit the flattening operation that transforms multi-dimensional matrix, E1(g1) or E1(o), into one-dimensional vector. Proof. The proof of Theorem 1 is based on the below lemmas. Please refer to Supplementary (Section 7) for the proofs of lemmas. We prove the Theorem 1 here. Lemma 1. Consider the diffusion model as defined in Theorem 1. Define αt as the key coefficient regulating the noise level. Let ϵ denote the noise vector introduced during the diffusion process, and let ϵθ(zt, t, c) represent the noise estimated by the diffusion model, where: θ denotes the parameters of the model, zt represents the state of the system at time t, and encapsulates the textconditioning information. Under these conditions, the following identity holds: E1 (g1) E1(o) = (ϵ ϵθ (zt, t, c)) . (2) 1 αt αt Lemma 2. Consider the equation AX = 0, where is matrix. If approximately equals to zero matrix, i.e., O, then there exists an approximate full-rank solution to the equation. Because well-trained diffusion model learns robust features and associations from diverse data, it generalizes well to inference prompts that are semantically similar to the training prompts. Moreover, the inference prompts here are generated by GPT-4o based on its understanding of the images, thus sharing semantic overlap with the training prompts. As result, the estimated noise ϵθ (zt, t, c) closely approximates the true noise ϵ. This means the dif4 Table 1. The cos (φ) gained by compared Stable Diffusion 2 against different diffusion models. The experiments are repeated for ten times to calculate mean and standard deviation. cos (φ) SD2 SDXL 0.995790 0.000037 OpenDalle 0.996532 0.000016 ColorfulXL 0.998436 0.000015 Kandinsky-3 0.999788 0. SD3 0.993256 0.000035 Kolors 0.991808 0.000042 ference between them is approximately equals to zero, i.e., ϵ ϵθ (zt, t, c) 0. According to Lemma 1, this results in E1 (g1) E1(o) 0. Denote T1 as the matrix, in which each column is E1 (g1) E1(o) from training pair. According to Lemma 2 and T1 O, we have T1X = 0 has an approximate full-rank solution. That means the matrix satisfying Eq. 1 exists. Note: here we do not show that E1(g1) = E1(o) (in this case, there would be no need of W); instead, we prove that there exists that can further minimize the distance between E1(g1) and E1(o), despite the distance already being small. Please see Table 4 and Fig. 7 for experiments. 4.2. Generalizability Theorem 2. Following Theorem 1, consider different well-trained diffusion model F2 and its text-guided image-to-image functionability achieved by denoising noised images. The matrix can be generalized such that for any original image and its translation g2, we have: E1(g2) = E1(o) W. (3) Proof. The proof of Theorem 2 is based on the below observation and lemmas. Please refer to Supplementary (Section 7) for the proofs of lemmas. We prove the Theorem 2 here. Observation 1. Consider two distinct matrices, W1 and W2, satisfying Eq. 1 and Eq. 3, respectively. Let vi denote the vector of all singular values of Wi, where {1, 2}. Specifically, define vi = (σ1 ), with each σj representing an singular value of Wi. Despite the inequality W1 = W2, as shown in Table 1, it is observed that: , . . . , σk , σ cos (φ) = v1 v2 v1v2 1. (4) Lemma 3 (Singular Value Decomposition). Any matrix can be decomposed into the product of three matrices: = UΣV, where and are orthogonal matrices, Σ is diagonal matrix with non-negative singular values of on the diagonal, and is the conjugate transpose of V. Lemma 4. matrix has left inverse if and only if it has full rank. Figure 4. The implementation of learning theoretical-expected matrix W. Specifically, in practice, we use gradient descent to optimize metric loss function in order to learn W. Consider T1 in the proof of Theorem 1, and denote T2 as the matrix, in which each column is E1 (g2) E1(o) from training pair. Therefore, we have T1W1 = 0 and T2W2 = 0. To prove Theorem 2, we only need to prove T2W1 = 0. According to Lemma 3, there exists orthogonal matrices, U1, U2, V1, and V2, with diagonal matrices, Σ1 and Σ2, satisfying W1 = U1Σ1V 1 and W2 = U2Σ2V 2. According to Observation 1, there exists α > 0 such that Σ1 = α Σ2. Therefore, we have: = αU1 (U W1 = U1Σ1V 2W2V2) 1 = αU1Σ2V 1 1 = α (U1U 2) W2 (V2V 1) . (5) Let U3 = U1U 2 and V3 = V2V thus orthogonal matrices. Therefore: 1, where U3 and V3 are T2W1 = α T2 (U1U 2) W2 (V2V 1) = α T2U3W2V3 α T2U3W2 V3 = α T2U3W2 . (6) According to Lemma 2 and 4, there exists matrix K, such that KW2 = I. That means there exists M, such that U3W2 = W2M. This results in: T2W1 α T2U3W2 = α T2W2M α T2W2 (7) Considering T2W2 = 0, we have T2W1 = 0. 4.3. Implementation As illustrated in Fig. 4, we show how to learn the theoretical-expected matrix in practice. Consider triplet (g, o, n), where is the generated image, is the origin used to generate g, and is negative sample relative to g. We have: = (g) , zo = (o) , and, zn = (n) , (8) 5 Table 2. Publicly available models fail on the test set of OriPID. Method Venue mAP Acc Supervised Pre-trained Models Selfsupervised Learning Models Visionlanguage Models Image Copy Detection Models Swin-B [23] ResNet-50 [12] ConvNeXt [24] EfficientNet [38] ViT-B [8] SimSiam [6] MoCov3 [13] DINOv2 [27] MAE [14] SimCLR [5] CLIP [34] SLIP [25] ZeroVL [7] BLIP [19] ICCV CVPR CVPR ICML ICLR CVPR CVPR TMLR CVPR ICML ICML ECCV ECCV ICML ASL [42] AAAI PMLR PMLR CVPR AnyPattern [43] Arxiv CNNCL [47] BoT [41] SSCD [31] 3.9 4.5 4.5 4.6 6. 1.8 2.1 4.3 11.6 11.3 2.9 5.4 5.6 8.3 5.2 6.3 10.5 14.8 29.1 2.7 3.0 3.1 3.3 4.6 1.0 1.2 2.9 9.2 9.7 1.8 3.7 3.8 5. 4.1 5.0 8.2 12.5 25.7 where is the encoder of VAE. Therefore, the final loss is defined as: = Lmtr (z W, zo W, zn W) , (9) where Lmtr is metric learning loss function that aims to bring positive data points closer together in the embedding space while pushing negative data points further apart. We use CosFace [40] here as Lmtr for its simplicity and effectiveness. Using gradient descent, we can optimize the loss function to obtain the theoretically expected matrix W. 5. Experiments 5.1. Evaluation Protocols and Training Details Evaluation protocols. We adopt two commonly used evaluation metrics for our ID2 task: i.e., Mean Average Precision (mAP) and Top-1 Accuracy (Acc). mAP evaluates models precision at various recall levels, while Acc measures the proportion of instances where the models top prediction exactly matches the original image. Acc is stricter as it only counts when the first guess is correct. Training details. We distribute the optimization of the theoretically expected matrix across 8 NVIDIA A100 GPUs using PyTorch [30]. The images are resized to resolution of 256 256 before being embedded by the VAE encoder. The peak learning rate is set to 3.5 104, and the Adam optimizer [16] is used. 5.2. The Challenge from ID2 This section benchmarks popular public deep embedding models on the OriPID test dataset. As shown in TaFigure 5. Examples of failure cases for each kind of model. Table 3. VAE differs between seen and unseen diffusion models. Sim. SDXL OpDa CoXL Kan3 SD3 Kolor 2 Conv. Embed. 0.169 0.169 0.169 0.002 0.120 0.121 0.120 0.023 - - 0.169 0.120 ble 2 and Fig. 5, we extensively experiment on supervised pre-trained models, self-supervised learning models, vision-language models, and image copy detection models. We use these models as feature extractors, matching query features against references. The mAP and Acc are calculated by averaging the results of 7 diffusion models. Please refer to Table 7 in Supplementary for the complete results. We observe that: (1) All existing methods fail on the OriPID test dataset, highlighting the importance of constructing specialized training datasets and developing new methods. Specifically, supervised pre-trained models overly focus on category-level similarity and thus achieve maximum mAP of 6.2%; self-supervised learning models handle only subtle changes and thus achieve maximum mAP of 11.6%; vision-language models return matches with overall semantic consistency, achieving maximum mAP of 8.3%; and image copy detection models are trained with translation patterns different from those of the ID2 task, thus achieving maximum mAP of 29.1%. (2) AnyPattern [43] achieves significantly higher mAP (29.1%) and accuracy (25.7%) compared to other methods. This is reasonable because it is designed for pattern generalization. Although the translation patterns generated by diffusion models in our ID2 differ from the manually designed ones in AnyPattern, there remains some generalizability. 5.3. VAE differs between Seen and Unseen Models common misunderstanding is that the generalizability of our method comes from different diffusion models sharing the same or similar VAE. In Table 3, we demonstrate that the VAE encoders used in our method differ between the diffusion models for generating training and testing images: (1) The parameters of VAE encoders are different. For instance, the cosine similarity of the last convolutional layer weights of the VAE encoder between Stable Diffusion 2 and Stable Diffusion XL is only 0.169. Furthermore, 6 Table 4. Our method excels in performance while keeping efficiency. mAP and Acc are in percentage; Train, Extract, and Match are in h, 104 s/img, and 1010 s/pair, respectively. Similarity -based Models Generalizable Models Ours Method Venue Circle loss [37] SoftMax [18] CosFace [40] CVPR NC CVPR IBN-Net [28] ECCV TransMatcher [20] NeurIPS CVPR QAConv-GS [21] Embeddings of VAE With Linear Transformation Upper: Train&Test Same Domain - - - Seen Unseen Efficiency mAP 70.4 82.7 87. 88.6 65.6 78.8 51.0 88.8 88.8 Acc 64.3 78.3 83.2 85.1 60.3 74.9 47.0 86.6 86. mAP 53.9 55.0 52.2 54.6 65.3 75.8 46.9 86.6 92.0 Acc 48.5 49.4 46. 49.0 60.7 72.3 43.0 84.5 90.4 Train Extract Match 1.79 2.25 2.43 2.03 1.84 1. - 0.17 0.17 2.81 2.81 2.81 3.42 2.30 2.30 1.59 1.59 1.59 0.80 0.80 0.80 2.14 941 4.25 0.53 0.53 Figure 6. Our method demonstrates certain level of robustness against different types and intensities of attacks. the number of channels in the last convolutional layer differs between Stable Diffusion 2 and Stable Diffusion 3. (2) The embeddings encoded by VAEs from different diffusion models vary. For instance, the average cosine similarity of VAE embeddings for 100,000 original images between Stable Diffusion 2 and Kandinsky-3 is close to 0. Additionally, the dimension of the VAE embedding for Stable Diffusion 2 is 4, 096, whereas for Stable Diffusion 3, it is 16, 384. 5.4. The Effectiveness of our Method This section shows the effectiveness of our method in terms of (1) generalizability, (2) efficiency, (3) robustness, and (4) the consistency between theory and experiments. The experimental results for Unseen are obtained by averaging the results from six different unseen diffusion models. Our method is much more generalizable than others. In Table 4, we compare our method with common similarity-based methods (incorporating domain generalization designs), all trained on the OriPID training dataset. The mAP and Acc for Unseen are calculated by averaging the results of 6 unseen diffusion models. Please refer to Table 8 and Section 11 in Supplementary for the complete results and failure cases, respectively. We make three observations: (1) On unseen data, our method demonstrates significant performance superiority over common similaritybased models. Specifically, compared against the best one, Figure 7. As expected by the theory, the cosine similarities increase w.r.t. epochs. we achieve superiority of +31.6% mAP and +35.1% Acc. (2) Although domain generalization methods alleviate the generalization problem, they are still not satisfactory compared to ours (with at least 10.8% mAP and 9.1% Acc). Moreover, those with the best performance suffer from severe efficiency issues, as detailed in the next section. (3) On the seen data, we achieve comparable performance with others. Specifically, there is 0.2% mAP and 1.5% Acc superiority compared to the best one. Our method outperforms others in terms of efficiency. Efficiency is crucial for the proposed task, as it often involves matching query against large-scale database in real-world scenarios. In Table 4, we compare the efficiency of our method with others regarding (1) training, (2) feature extraction, and (3) matching. We draw three observations: (1) Training: Learning matrix based on VAE embeddings is more efficient compared to training deep models on raw images. Specifically, our method is 8.6 times faster than the nearest competitor. (2) Feature extraction: Compared to other models that use deep networks, such as ViT [8], the VAE encoder we use is relatively lightweight, resulting in faster feature extraction. (3) Matching: Compared to the best domain generalization models, QAConv-GS [21], which use feature maps for matching, our method still relies on feature vectors. This leads to an 875 superiority in matching speed. 7 Table 5. Ablation for choices of VAE encoders. Table 6. Ablation for different supervision losses. VAE Open-Sora Open-Sora-Plan Stable Diffusion 2 Seen Unseen mAP 86.3 88.8 88.8 Acc 83.5 86.4 86.6 mAP 86.5 86.1 86.6 Acc 84.2 84.0 84.5 Supervision SoftMax Circle loss CosFace Seen Unseen mAP 76.1 84.9 88.8 Acc 72.6 82.0 86.6 mAP 62.4 82.5 86.6 Acc 59.0 80.4 84.5 Figure 8. The change in performance w.r.t the rank of W. Figure 9. The change in performance w.r.t the number of layers. Our method is relatively robust against different attacks. In the real world, the quality of an image may deteriorate during transmission. As shown in Fig. 6, we apply varying intensities of Gaussian blur and JPEG compression, following previous works such as [4, 45], to evaluate the robustness of our method. It is observed that the side effects of these attacks are relatively minor. For instance, for the unseen diffusion models, the strongest Gaussian blur (σ = 3) reduces the mAP by only 3.7%, while the strongest compression (30%) decreases the mAP by just 0.3%. Note that our models are not trained with these attacks. Our training scheme successfully learns the theoryanticipated matrix W. In the theorems, we have proven that E1 (g1/g2) = E1(o) holds ideally. In Fig. 7, we experimentally show this phenomenon. Specifically, we first calculate two cosine similarities of < E1 (g1) W, E1(o)W > (seen) and < E1 (g2)W, E1(o)W > (unseen), and then plot their changes with respect to the epochs. We observe that: (1) as expected, the two cosine similarities increase during training; and (2) the cosine similarities of the seen models are higher than those of the unseen ones, which is reasonable due to certain degree of overfitting. 5.5. Ablation Study In this section, we ablate the proposed method by (1) using different VAE encoders, (2) supervising the training with different loss functions, (3) exploring the minimum rank of W, and (4) experimentally exploring beyond the theoretical guarantees. As shown in Table 5, Table 6, Fig. 8, and Fig. 9, we observe that: (1) Our method is insensitive to the choice of VAE encoder; (2) In practice, selecting an appropriate supervision for learning is essential; (3) To improve efficiency, the rank of can be relatively low; and (4) Using multilayer perceptron (MLP) with activation function inFigure 10. The image-to-image paradigm beyond our theorems. stead of the theoretically expected leads to overfitting. Please see the details in Supplementary (Section 12). 6. Conclusion This paper explores popular text-guided image-to-image diffusion models from novel perspective: retrieving the original image of query translated by these models. The proposed task, ID2, is both important and timely, especially as awareness of security concerns posed by diffusion models grows. To support this task, we introduce the first ID2 dataset, OriPID, designed with focus on addressing generalization challenges. Specifically, the training set is generated by one diffusion model, while the test set is generated by seven different models. Furthermore, we propose simple, generalizable solution with theoretical guarantees: First, we theoretically prove the existence of linear transformations that minimize the distance between the VAE embeddings of query and its original image. Then, we demonstrate that the learned linear transformations generalize across different diffusion models, i.e., the VAE encoder and the learned transformations can effectively embed images generated by new diffusion models. Limitation. We note that certain methods, such as InstructPix2Pix [2] and IP-Adapter [46] (see Fig. 10), perform text-guided image-to-image tasks in paradigms that go beyond the scope of our theorems. For more detailed discussion, please refer to Supplementary (Section 13)."
        },
        {
            "title": "References",
            "content": "[1] Vladimir Arkhipkin, Andrei Filatov, Viacheslav Vasilev, Anastasia Maltseva, Said Azizov, Igor Pavlov, Julia Agafonova, Andrey Kuznetsov, and Denis Dimitrov. Kandinsky 3.0 technical report. arXiv preprint arXiv:2312.03511, 2023. 2, 3 [2] Tim Brooks, Aleksander Holynski, and Alexei Efros. Instructpix2pix: Learning to follow image editing instructions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1839218402, 2023. 8, 5 [3] Chuan Chen, Zhenpeng Wu, Yanyi Lai, Wenlin Ou, Tianchi Liao, and Zibin Zheng. Challenges and remedies to privacy and security in aigc: Exploring the potential of privacy computing, blockchain, and beyond. arXiv preprint arXiv:2306.00419, 2023. 3 [4] Jiaxuan Chen, Jieteng Yao, and Li Niu. single simple patch is all you need for ai-generated image detection. arXiv preprint arXiv:2402.01123, 2024. 8 [5] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PMLR, 2020. 6 [6] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1575015758, 2021. [7] Quan Cui, Boyan Zhou, Yu Guo, Weidong Yin, Hao Wu, Osamu Yoshie, and Yubo Chen. Contrastive vision-language pre-training with limited resources. In European Conference on Computer Vision, pages 236253. Springer, 2022. 6 [8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale. ICLR, 2021. 6, 7 [9] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. 2, 3 [10] Mingyuan Fan, Cen Chen, Chengyu Wang, and Jun Huang. On the trustworthiness landscape of state-of-the-art generarXiv preprint ative models: comprehensive survey. arXiv:2307.16680, 2023. 3 [11] Pierre Fernandez, Matthijs Douze, Herve Jegou, and Teddy Furon. Active image indexing. In International Conference on Learning Representations (ICLR), 2023. 3 [12] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 6 [13] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 97299738, 2020. 6 [14] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross Girshick. Masked autoencoders are scalable vision learners. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 16000 16009, 2022. 6 [15] Alexander Izquierdo. Opendallev1.1, 2023. 2, 3 [16] Diederik Kingma. Adam: method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014. 6 [17] KolorsTeam. Kolors: Effective training of diffusion model for photorealistic text-to-image synthesis. 2024. 2, 3 [18] Yann LeCun, Bernhard Boser, John Denker, Donnie Henderson, Richard Howard, Wayne Hubbard, and Lawrence Jackel. Backpropagation applied to handwritten zip code recognition. Neural computation, 1(4):541551, 1989. 7, 3 [19] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training for uniIn Infied vision-language understanding and generation. ternational Conference on Machine Learning, pages 12888 12900. PMLR, 2022. [20] Shengcai Liao and Ling Shao. Transmatcher: Deep image matching through transformers for generalizable person reidentification. Advances in Neural Information Processing Systems, 34:19922003, 2021. 7 [21] Shengcai Liao and Ling Shao. Graph sampling based deep metric learning for generalizable person re-identification. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 73597368, 2022. 7 [22] Li Lin, Neeraj Gupta, Yue Zhang, Hainan Ren, Chun-Hao Liu, Feng Ding, Xin Wang, Xin Li, Luisa Verdoliva, and Shu Hu. Detecting multimedia generated by large ai models: survey. arXiv preprint arXiv:2402.00045, 2024. 3 [23] Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: In Hierarchical vision transformer using shifted windows. Proceedings of the IEEE/CVF international conference on computer vision, pages 1001210022, 2021. 2, 6 [24] Zhuang Liu, Hanzi Mao, Chao-Yuan Wu, Christoph Feichtenhofer, Trevor Darrell, and Saining Xie. convnet for the 2020s. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1197611986, 2022. 6 [25] Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-image pretraining. In European Conference on Computer Vision, pages 529544. Springer, 2022. 6 [26] OpenAI. Hello gpt-4o, 2024. 2 [27] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 6 [41] Wenhao Wang, Weipu Zhang, Yifan Sun, and Yi Yang. Bag of tricks and strong baseline for image copy detection. arXiv preprint arXiv:2111.08004, 2021. 6 [42] Wenhao Wang, Yifan Sun, and Yi Yang. benchmark and asymmetrical-similarity learning for practical image copy In Proceedings of the AAAI Conference on Ardetection. tificial Intelligence, pages 26722679, 2023. 3, 6 [43] Wenhao Wang, Yifan Sun, Zhentao Tan, and Yi Yang. Anypattern: Towards in-context image copy detection. In arXiv preprint arXiv:2404.13788, 2024. 3, 6 [44] Wenhao Wang, Yifan Sun, and Yi Yang. Pattern-expandable International Journal of Computer image copy detection. Vision, pages 117, 2024. 3 [45] Zhendong Wang, Jianmin Bao, Wengang Zhou, Weilun Wang, Hezhen Hu, Hong Chen, and Houqiang Li. Dire for diffusion-generated image detection. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 2244522455, 2023. [46] Hu Ye, Jun Zhang, Sibo Liu, Xiao Han, and Wei Yang. Ipadapter: Text compatible image prompt adapter for text-toimage diffusion models. 2023. 8, 5 [47] Shuhei Yokoo. Contrastive learning with large memory bank and negative embedding subtraction for accurate copy detection. arXiv preprint arXiv:2112.04323, 2021. 6 [48] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, 2024. 3 [28] Xingang Pan, Ping Luo, Jianping Shi, and Xiaoou Tang. Two at once: Enhancing learning and generalization capacities via ibn-net. In Proceedings of the european conference on computer vision (ECCV), pages 464479, 2018. 7 [29] Zoe Papakipos, Giorgos Tolias, Tomas Jenicek, Ed Pizzi, Shuhei Yokoo, Wenhao Wang, Yifan Sun, Weipu Zhang, Yi Yang, Sanjay Addicam, et al. Results and findings of the 2021 image similarity challenge. In NeurIPS 2021 Competitions and Demonstrations Track, pages 112. PMLR, 2022. 3 [30] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. Pytorch: An imperative style, high-performance deep learning library. Advances in neural information processing systems, 32, 2019. [31] Ed Pizzi, Sreya Dutta Roy, Sugosh Nagavara Ravindra, Priya Goyal, and Matthijs Douze. self-supervised descriptor for image copy detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1453214542, 2022. 3, 6 [32] Lab PKU-Yuan and Tuzhan AI etc. Open-sora-plan, 2024. 3 [33] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. SDXL: Improving latent diffusion models In The Twelfth Interfor high-resolution image synthesis. national Conference on Learning Representations, 2024. 2, 3 [34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 6 [35] Recoilme. Colorfulxl-lightning, 2023. 2, 3 [36] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. High-resolution image In Proceedings of synthesis with latent diffusion models. the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1068410695, 2022. 2, 3 [37] Yifan Sun, Changmao Cheng, Yuhan Zhang, Chi Zhang, Liang Zheng, Zhongdao Wang, and Yichen Wei. Circle loss: unified perspective of pair similarity optimization. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 63986407, 2020. 7, 3 [38] Mingxing Tan and Quoc Le. Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning, pages 61056114. PMLR, 2019. 6 [39] Bart Thomee, David Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):6473, 2016. 3 [40] Hao Wang, Yitong Wang, Zheng Zhou, Xing Ji, Dihong Gong, Jingchao Zhou, Zhifeng Li, and Wei Liu. Cosface: Large margin cosine loss for deep face recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 52655274, 2018. 6, 7, 10 Generalizable Origin Identification for Text-Guided Image-to-Image Diffusion Models"
        },
        {
            "title": "Supplementary Material",
            "content": "7. Proofs of Lemmas Lemma 1. Consider the diffusion model as defined in Theorem 1. Define αt as the key coefficient regulating the noise level. Let ϵ denote the noise vector introduced during the diffusion process, and let ϵθ(zt, t, c) represent the noise estimated by the diffusion model, where: θ denotes the parameters of the model, zt represents the state of the system at time t, and encapsulates the textconditioning information. Under these conditions, the following identity holds: E1 (g1) E1(o) = (ϵ ϵθ (zt, t, c)) . (10) 1 αt αt Proof. Denote z0 = E1(o) and and denoising. Therefore, we have E1 (g1) E1(o) = E1 (D1 (z where D1 is the decoder of VAE. 0 as z0 after adding noise 0)) z0 = 0 z0, (11) Given an initial data point z0, the forward process in diffusion model adds noise to the data step by step. The expression for zt at specific timestep can be written as: zt = αtz0 + 1 αtϵ. (12) To denoise zt and recover an estimate of the original data z0, the reverse process is used. neural network θ is trained to predict the noise ϵ added to z0. The denoised data 0 can be expressed as: 0 = 1 αt Therefore, we have: (cid:0)zt 1 αtϵθ (zt, t, c)(cid:1) . (13) = (cid:0) = 1 αt E1 (g1) E1(o) = (cid:0)zt 0 z0 1 αtϵθ (zt, t, c)(cid:1) z0 1 αt αtz0 + = 1 αtϵ 1 αtϵθ (zt, t, c)(cid:1) z0 1 αt αt (ϵ ϵθ (zt, t, c)) . (14) The Eq. 10 is proved. Lemma 2. Consider the equation AX = 0, where is matrix. If approximately equals to zero matrix, i.e., O, then there exists an approximate full-rank solution to the equation. 1 Proof. Consider matrix Rmn. According to Lemma 3, there exists orthogonal matrices Rmm and Rnn, and diagonal matrix Σ Rmn with nonnegative singular values, such that, = UΣV. Therefore, the linear equation can be transformed as: UΣVX = 0. (15) Considering UU = and denoting = VX, we have ΣX = 0. Because O, all of its singular values approximately equals to 0. Considering the floating-point precision we need, ΣX = 0 could be regarded as: σ0 0 σ1 . . . 0 . . . 0 0 . . . σr 0 0 0 . . . 0 0 = 0, (16) where is the number of non-zero singular values. Therefore, there exists an Rnk with rank = min(m, n)r. When min(m, n) r, is full rank, i.e, = VZ is an approximate full-rank solution to the linear equation AX = 0. Lemma 3 (Singular Value Decomposition). Any matrix can be decomposed into the product of three matrices: = UΣV, where and are orthogonal matrices, Σ is diagonal matrix with non-negative singular values of on the diagonal, and is the conjugate transpose of V. Proof. Consider matrix Rmn. The matrix AA is therefore symmetric and positive semi-definite, which means the matrix is diagonalizable with an eigendecomposition of the form: AA = VΛV = (cid:88) i=1 λiviv = (cid:88) i= (σi)2 viv , (17) where is an orthonormal matrix whose columns are the eigenvectors of AA. We have defined the singular value σi as the square root of the i-th eigenvalue; we know we can take the square Figure 11. Illustration of prompts and corresponding generated images for 6 different subjects in our dataset. Our dataset comprehensively includes various subjects found in the real world. root of our eigenvalues because positive semi-definite matrices can be equivalently characterized as matrices with non-negative eigenvalues. For the i-th eigenvector-eigenvalue pair, we have AAvi = (σi)2 vi. Define new vector ui, such that, ui = Avi σi . (18) (19) This construction enables ui as unit eigenvector of AA. Now let be an matrix because AA is where the i-th column is vi; let be an matrix because Avi is an m-vector where the i-th column is ui; and let Σ be diagonal matrix whose i-th element is σi. Then we can express the relationships we have so far in matrix form as: = AVΣ1, UΣ = AV, (20) = UΣV, where we use the fact that VV = and Σ1 is diagonal matrix where the i-th value is the reciprocal of σi. Lemma 4. matrix has left inverse if and only if it has full rank. Proof. To prove Lemma 4, we must demonstrate two directions: if matrix has left inverse, then it must have full rank, and conversely, if matrix has full rank, then it has left inverse. (1) Suppose Rmn has left inverse Rnm such that BA = In. Because the In is of rank n, the matrix AB must have rank n. Considering the inequality: = rank(BA) min(rank(A), rank(B)) rank(A) min(m, n) (21) we have rank(A) = n, i.e., has full rank. (2) Suppose Rmn has full rank, i.e., rank(A) = min(m, n). We have the rows of are linearly independent, and thus there exists an matrix such that CA = In. That means is left inverse of A. 8. Prompt and Generation Examples In Fig. 11, we present several prompts with their corresponding generated images from our dataset, OriPID. The dataset comprehensively covers wide range of subjects commonly found in real-world scenarios, such as natural sceneries, cultural architectures, lively animals, luxuriant plants, artistic paintings, and indoor items. It is important to note that in the training set, for each original image, OriPID contains 20 prompts with corresponding generated images, and for illustration, we only show 4 of them in Fig. 11. 2 + am doing image-to-image translation. Could you think of creative prompts to translate this image to different ones? Keep them creative, and only return 20 different prompts. Figure 12. The script for requesting GPT-4o to generate 20 different prompts for each original image. Figure 13. This illustration shows failure cases predicted by our method. We have identified that our model may fail when encountering hard negative samples. 9. Implementation of GPT-4o As shown in Fig. 12, we request GPT-4o to generate 20 different prompts for each original image. 10. Complete Experiments for 7 Models We provide two types of complete experiments for seven different diffusion models: (1) Table 7 presents the results from directly testing publicly available models on the OriPID test dataset; and (2) Table 8 shows the results from testing models that we trained on the OriPID training dataset, which contains only images generated by Stable Diffusion 2. 11. Failure Cases and Potential Directions Failure cases. As shown in Fig. 13, we observe that our model may fail when negative samples are too visually similar to the queries. This hard negative problem is reasonable because our method relies on the VAE embeddings, which capture high-level representations and is insensitive to subtle changes. As result, visually similar negative samples can produce embeddings that are close to those of the queries, leading to inaccurate matchings. Potential directions. The hard negative problem has been studied in the Image Copy Detection (ICD) community, as exemplified by ASL [42]. It learns to assign larger norm to the deep features of images that contain more content or information. However, this method cannot be directly used in our scenario because the query and reference here do not have simple relationship in terms of information amount. Nevertheless, it offers promising research direction from the perspective of information. Specifically, on one hand, the noise-adding and denoising processes result in loss of information, while on the other hand, the guided text introduces new information into the output. 12. Details of Ablation Studies Our method is insensitive to the choice of VAE encoder. In Table 5, we replace the VAE encoder from Stable Diffusion 2 with two different encoders from OpenSora [48] and Open-Sora-Plan [32]. It is observed that, despite using significantly different well-trained VAEs, such as ones for videos, the performance drop is minimal (less than 1%). This observation experimentally extends the Eq. 1 from E1 (g1)W = E1(o)W to E2 (g1)W = E2(o)W, where E2 is an encoder from totally different VAE. In practice, selecting an appropriate supervision for learning is essential. In Table 6, we replace the used supervision CosFace [40] with two weaker supervisions, i.e., SoftMax [18] and Circle loss [37]. We observe that switching to Circle loss results in drop in mAP for seen and unseen categories by 3.9% and 4.1%, respectively. Furthermore, using SoftMax leads to mAP drops of 12.7% and 24.2% for the two categories, respectively. We infer this is because: while our theorems guarantee the distance between translation and its origin, many negative samples serve as distractors during retrieval. Without appropriate hard negative solutions, these distractors compromise the final performance. To improve efficiency, the rank of can be relatively low. Assume the matrix has shape of nm, where is the dimension of the VAE embedding and is hyperparameter. We show that is approximately full-rank in the proof of existence, and expect that in the proof of generalization. Therefore, the rank of is m. Experimentally, = 4, 096, and we explore the minimum rank of from 4, 096 as shown in Fig. 8. It is observed that: (1) From 4, 096 to 512, the performance remains nearly unchanged. This suggests that we can train relatively low-rank (2) It is to improve efficiency in real-world applications. expected to see performance decrease when reducing the rank from 512 to 64. This is because matrix with too low rank cannot carry enough information to effectively linearly transform the VAE embeddings. Using multilayer perceptron (MLP) with activation function instead of the theoretically expected leads to overfitting. In the theoretical section, we proved the existence and generalization of using concepts from diffuTable 7. The performance of publicly available models on 7 different diffusion models. Method Swin-B ResNet-50 ConvNeXt EfficientNet ViT-B SimSiam MoCov3 DINOv2 MAE SimCLR CLIP SLIP ZeroVL BLIP ASL CNNCL BoT SSCD AnyPattern Supervised Pre-trained Models Selfsupervised Learning Models Visionlanguage Models Image Copy Detection Models SD2 CoXL mAP Acc mAP Acc mAP Acc mAP Acc mAP Acc mAP Acc mAP Acc SDXL OpDa Kolor Kan3 SD3 3.1 3.8 3.5 2.9 4. 1.5 1.4 2.6 14.9 6.0 2.6 5.6 5.2 6.8 2.3 4.0 6.6 9.7 17.6 2.0 2.6 2.1 1.9 2.8 1.0 0.8 1.6 11.4 4.2 1.7 3.8 3.5 4. 1.7 2.9 4.9 7.7 14.3 2.9 3.1 3.3 2.9 4.5 1.2 1.5 2.7 10.0 7.0 2.1 3.5 4.4 6.5 3.0 4.2 6.1 8.7 18.5 1.9 2.0 2.2 2.0 3. 0.7 0.9 1.7 8.0 5.2 1.4 2.3 2.9 4.5 2.3 3.2 4.4 6.8 15.7 4.1 5.3 4.7 4.9 7.2 1.8 2.4 4.6 13.1 13.5 3.1 5.8 6.4 9. 5.6 8.3 10.4 16.4 33.0 2.9 3.7 3.3 3.4 5.5 0.9 1.3 3.0 10.5 10.6 2.1 4.0 4.4 7.0 4.4 6.7 8.2 14.0 29.2 4.2 4.5 5.0 5.4 6. 1.7 2.2 5.5 8.1 13.0 3.2 4.9 4.5 8.7 5.7 5.7 12.5 18.1 37.8 3.1 3.2 3.5 3.9 5.0 1.0 1.3 3.6 6.4 10.1 2.0 3.3 3.2 6. 4.6 4.5 10.2 15.6 34.0 6.8 8.1 8.4 8.7 11.2 3.1 3.8 8.4 17.6 23.7 4.2 9.1 9.8 13.8 10.3 12.2 20.6 28.1 48.0 4.7 5.7 6.2 6.5 8. 1.9 2.4 5.9 14.3 19.3 2.7 6.7 6.9 10.2 8.7 9.9 16.8 24.6 43.9 2.9 3.4 3.5 3.3 4.1 1.5 1.9 2.9 11.2 7.3 2.5 5.4 4.7 6. 2.7 3.7 7.4 9.0 18.2 1.9 2.0 2.4 2.2 2.8 0.8 1.1 1.9 8.5 12.0 1.6 3.5 3.0 3.9 2.1 2.7 5.4 6.8 15.0 3.0 3.4 3.6 4.1 5. 1.4 1.6 3.6 6.5 8.8 2.1 3.8 4.3 6.4 2.0 2.2 2.6 3.0 4.3 0.8 1.0 2.6 5.1 6.7 0.7 2.5 3.0 4.5 3.7 6.3 9.3 14.1 30. 2.9 5.0 7.3 11.9 27.5 Table 8. The performance of our trained models on 7 different diffusion models. Note that these models are trained on images generated by SD2 and tested on images from multiple models. Similaritybased Models Generalizable Models Ours Method Circle loss SoftMax CosFace IBN-Net TransMatcher QAConv-GS VAE Embed. Linear Trans. Upper SD2 CoXL mAP Acc mAP Acc mAP Acc mAP Acc mAP Acc mAP Acc mAP Acc SDXL OpDa Kolor Kan3 SD3 70.4 82.7 87.1 88.6 65.6 78. 64.3 78.3 83.2 85.1 60.3 74.9 56.2 62.4 63.7 65.7 60.6 71.6 50.1 56.5 58.2 60.1 55.8 67. 56.5 58.3 56.7 59.4 67.9 77.4 51.8 53.0 51.7 54.2 63.6 74.3 41.6 37.3 30.5 33.3 61.7 73. 37.0 32.2 25.2 28.3 57.4 70.5 60.0 52.5 47.5 49.8 68.9 75.2 53.8 46.0 40.6 42.8 64.2 71. 65.6 75.9 71.5 74.0 64.7 77.3 59.3 70.2 65.5 68.3 59.2 73.6 43.5 43.6 43.0 45.4 67.9 79. 39.2 38.7 38.0 40.5 63.9 76.9 47.0 38.3 51.0 43.6 88.8 86.6 81.5 78.8 87.3 85.3 89.3 87.7 85.7 83.3 85.7 82.9 90.3 88.8 92.8 88.8 38. 47.7 93.7 89.2 51.6 42.9 92. 93.1 84.9 86.6 82.4 33.8 42. 48.8 50.4 54.7 90.8 91.9 95. 94.3 46.9 94.0 sion models and linear algebra. natural experimental extension of this is to use an MLP with activation functions to replace the simple linear transformation (W). Although linear algebra theory cannot guarantee these cases, we can still explore them experimentally. Experimentally, we increase the number of layers from 1 to 7, all using ReLU activation and residual connections. As shown in Fig. 9, we observe overfitting in one type of diffusion model. Specifically, on one hand, the performance on seen diffusion models improves. For example, with 2 layers, the mAP increases to 91.4% (+2.6%), and Acc rises to 89.4% (+2.8%). However, on the other hand, significant performance drop is observed on unseen diffusion models: with 2 layers, the mAP decreases from 86.6% to 80.3% (6.3%), and Acc drops from 84.5% to 77.3% (7.2%). The performance drop becomes even more severe when using more layers. 13. Limitations and Future Works Limitations. Although the paradigm analyzed in the main paper (Fig. 14 (a)) is the simplest approach for text-guided image-to-image translation and serves as the default mode in the AutoPipelineForImage2Image of diffusers, we also observe the existence of an alternative paradigm, as shown in Fig. 14 (b). While this paradigm lies beyond our theoretical guarantees, we can still analyze it experimentally, as demonstrated in Table 9. Interestingly, we find that (1) our method generalizes well to InstructP2P, which still uses VAE encoder to embed the original images; and (2) all methods, including ours, fail on IP-Adapter, which uses CLIP for encoding. We also try the linear transformed CLIP embedding, but it still fails to generalize (36.6% mAP and 27.8% Acc). Based on these experiments, we conclude with hypothesis about the up4 Figure 14. Different paradigms used by text-guided image-toimage translations. Table 9. The generalization results on InstructPix2Pix [2] and IPAdapter [46]. No method succeeds in generalizing to IP-Adapter. Method Circle loss SoftMax CosFace IBN-Net TransMatcher QAConv-GS Similarity -based Models Generalizable Models Ours VAE Embed. Linear Trans. VAE per limit of our method: InstructP2P Acc mAP 42.7 44.9 19.2 21.5 17.7 20.1 18.9 21.4 54.5 56.6 53.0 55.1 67.1 68.2 79.2 80.7 IP-Adapter Acc mAP 4.0 6.2 3.5 5.8 0.9 1.5 0.8 1.5 1.4 2.7 0.6 1.2 0.1 0.2 0.2 0. Hypothesis 1. Following Theorem 1, consider different well-trained diffusion model F3 and its textguided image-to-image functionability achieved with VAE-encoded original images. The matrix can be generalized such that for any original image and its translation g3, we have: E1(g3) = E1(o) W. (22) Future Works. Future works may focus on (1) providing theoretical proof for Hypothesis 1, and (2) developing new generalization methods for text-guided image-to-image based on CLIP encodings."
        }
    ],
    "affiliations": [
        "Baidu Inc.",
        "University of Technology Sydney",
        "Zhejiang University"
    ]
}