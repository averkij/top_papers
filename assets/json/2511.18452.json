{
    "paper_title": "NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering",
    "authors": [
        "Loick Chambon",
        "Paul Couairon",
        "Eloi Zablocki",
        "Alexandre Boulch",
        "Nicolas Thome",
        "Matthieu Cord"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixel-level tasks. Existing upsampling approaches face a fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFM-specific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/valeoai/NAF."
        },
        {
            "title": "Start",
            "content": "NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering Loıck Chambon1,2 Paul Couairon2 Alexandre Boulch1 Nicolas Thome2,3 Matthieu Cord 1,2 Eloi Zablocki1 5 2 0 2 3 ] . [ 1 2 5 4 8 1 . 1 1 5 2 : r 1Valeo.ai, Paris, France 2Sorbonne Universite, CNRS, ISIR, F-75005 Paris, France 3Institut Universitaire de France (IUF) Figure 1. Neighborhood Attention Filtering (NAF) as Zero-Shot Feature Upsampler: train once, apply efficiently to any Vision Foundation Model (including 7B models) to any scale, achieving state-of-the-art results across multiple downstream tasks."
        },
        {
            "title": "Abstract",
            "content": "Vision Foundation Models (VFMs) extract spatially downsampled representations, posing challenges for pixellevel tasks. Existing upsampling approaches face fundamental trade-off: classical filters are fast and broadly applicable but rely on fixed forms, while modern upsamplers achieve superior accuracy through learnable, VFMspecific forms at the cost of retraining for each VFM. We introduce Neighborhood Attention Filtering (NAF), which bridges this gap by learning adaptive spatial-and-content weights through Cross-Scale Neighborhood Attention and Rotary Position Embeddings (RoPE), guided solely by the high-resolution input image. NAF operates zero-shot: it upsamples features from any VFM without retraining, making it the first VFM-agnostic architecture to outperform VFM-specific upsamplers and achieve state-of-the-art performance across multiple downstream tasks. It maintains high efficiency, scaling to 2K feature maps and reconstructing intermediate-resolution maps at 18 FPS. Beyond feature upsampling, NAF demonstrates strong performance on image restoration, highlighting its versatility. Code and checkpoints are available at https://github.com/ valeoai/NAF. 1. Introduction Vision Foundation Models (VFMs) extract rich semantic representations from images. However, these features are generated at reduced spatial resolutions due to computational constraints and architectural choices, limiting their effectiveness for fine-grained tasks. Increasing input image size is straightforward approach, but only few VFMs exhibit scale invariance and handle non-standard resolutions effectively [19, 35, 41, 46]. For most VFMs, this degrades performance [6], and computational cost scales quadratically with resolution, causing prohibitive inference times and memory constraints. more promising approach directly upsamples VFM output features rather than enlarging input images [6, 11, 24, 36, 43, 51]. Early approaches employ filtering techniques to upsample features, relying on spatial proximity [9, 23, 33] or incorporating guidance from the input image [16, 17, 25, 45]. However, traditional filters reliance on fixed forms (e.g., Gaussian) limits their adaptability, often leading to suboptimal results. To overcome this, learning-based feature upsamplers [6, 11, 21, 36, 43, 51] have been introduced. They optimize their parameters to recover high-resolution features from low-resolution ones. While achieving higherquality upsampling, they lack of interpretability and sacrifice efficiency of classical filters, introducing complex pipelines with low throughput, high memory consumption, and large parameter counts (see. Tab. 1), resulting in relatively modest maximum upscaling, or worse, fixed ratios [11, 36, 43]. Crucially, existing upsamplers depend on VFM-specific guidance, which forces retraining whenever the underlying VFM changes. We introduce Neighborhood Attention Filtering (NAF), VFM-agnostic upsampling module that generalizes in zero-shot manner to features from any VFM. NAF reweights features using image-based guidance and relative spatial proximity (see Fig. 1) via Cross-Scale Neighborhood Attention for local feature similarity, combined with Rotary Position Embeddings (RoPE) to encode relative spatial relationships. We show that this design implicitly learns an Inverse Discrete Fourier Transform (IDFT) of the aggregation: NAF predicts spectral coefficients that reconstruct an adaptive, data-dependent upsampling filter. This formulation preserves the interpretability of classical filters while allowing the model to learn flexible, spatial-and-contentaware aggregation (see. App. A). Overall, our contributions are: We present NAF, VFM-agnostic feature upsampler guided solely by high-resolution images. It leverages Cross-Scale Neighborhood Attention mechanism for content-aware local interpolation, exhibiting strong similarities with traditional filtering methods and IDFT. We show that NAF achieves state-of-the-art performance across diverse vision tasks and datasets, for many VFM families and wide range of model sizes. We implement an efficient upsampler that performs zeroshot upsampling at high throughput, to high resolutions (up to 2K) and works on very large VFMs (7B parameters), which previous methods cannot handle, while still achieving notable gains. We demonstrate NAFs generalization beyond upsampling to tasks such as image denoising, using the same architecture and opening new avenues for cross-task feature filtering. 2. Background and Related Work Background: Filtering. Filtering is fundamental operation in computer vision for recovering spatial details. In feature upsampling, we consider pair of low-resolution (LR) and high-resolution (HR) feature maps (FLR, FHR), both of dimension N. The objective is to reconstruct, for each location of the high-resolution map, the corresponding feature representation FHR . Spatial-and-content-aware filters. While spatial-aware filters [9, 23, 38, 48, 55] reweighting the input based on spatial proximity remain widely used, spatial-and-content-aware filters improve upon them by leveraging an auxiliary guidance signal (e.g., an image) to preserve edges and fine structures. The high-resolution feature at location is computed as FHR = 1 Z(p) (cid:88) qN (p) w(p, G)FLR , (1) where (p) is local neighborhood, Z(p) is normalization factor, and w(p, G) are content-adaptive weights reflecting similarity in the guidance signal. Classic examples include the bilateral filter [45], joint bilateral filter [25] (JBF) and others [16, 17], where weights depend on both spatial proximity and intensity similarity: w(p, G) = exp , with σs and σr controlling spatial and range intensity respectively. While effective at preserving edges, traditional formulations are limited by fixed kernel shapes and handcrafted similarity functions, motivating learning-based adaptive filters that learn expressive, content-dependent weights directly from data. GpGq2 2σ2 pq2 2σ2 (cid:17) (cid:16) Feature Upsampling. Deep learning naturally extends classical filtering by allowing the aggregation operation to be learned end-to-end through parameterized kernels. These deep methods optimize the combination of lowresolution features FLR using guidance G, which typically includes an encoding of the input image and the lowresolution features themselves. This leads to the generic formulation: FHR = 1 Z(p) (cid:88) qN (p) wθ(p, Encθ(I), FLR )FLR , (2) with θ learnable parameters of the kernel and Encθ trainable image encoder. The development of feature upsampling methods is largely driven by the specific context in Method FeatUp [11] LiFT [43] JBU [24] LoftUp [21] AnyUp [51] JAFAR [6] NAF VFM # Params GFLOPs FPS Maximum Agnostic (Million) computed at Ratio 0.17 1.19 0.03 4.30 0.88 0.63 0.66 83 512 4.88 1971 329 366 265 17 30 4 4 5 11 18 16 16 28 32 32 32 72 Table 1. Comparison of upsampling methods for 16 upsampling of input features (384, 28, 28). The maximum ratio indicates the largest upscaling factor that fits on single A100 40GB GPU. Methods are sorted by maximum ratio and FPS. which they are deployed, particularly whether they are designed for specific tasks or general vision backbones. Task-Designed Upsamplers. These methods serve as integrated components within downstream deep learning pipelines, such as semantic segmentation and depth estimation, which require pixel-level precision. Early techniques rely on standard methods like transposed convolutions or pixel unshuffling [27, 30, 37, 39, 56]. More sophisticated approaches use adaptive reweighting: CARAFE [50] and DySample [29] predict content-aware kernels or sampling points, while SAPA [31] exploits local similarity between high-resolution guidance and low-resolution features. While effective, these task-specific upsamplers require retraining for every new downstream task and are not primarily designed for general VFM features. VFM-Specific Upsamplers. These upsamplers are explicitly aligned with the feature distribution of particular VFM and are trained to generalize across different vision tasks without task-specific supervision. FeatUp [11] and LiFT [43] introduced early dedicated pipelines: FeatUp relies on parameterized variant of Joint Bilateral Upsampling (JBU) module [24], and LiFT uses CNN-based encoderdecoder architecture. FeatSharp [36] builds on JBU by adding tiling and debiasing strategies. More recently, JAFAR [6] and LoftUp [21] introduced attention-based architectures which allows for continuous upsampling to arbitrary resolutions. Crucially, all these methods rely heavily on the semantic content of the VFMs low-resolution features to compute the upsampling guidance. VFM-Agnostic Upsamplers. The most challenging goal is to create upsamplers that can be applied in zero-shot manner to features from any VFM without the need for retraining. The Joint Bilateral Upsampling (JBU) module [11] is inherently VFM-agnostic, but its performance as standalone upsampler is limited compared to modern methods. recent concurrent work, AnyUp [51], builds on attention mechanisms [6, 21] and removes the dependency on feature dimensionality allowing to be used to multiple VFMs. However, AnyUps guidance computation still relies on the VFMs low-resolution features, meaning the upsampling remains priori entangled with the target feature distribution. Furthermore, it is computationally heavier than the current best VFM-specific upsamplers Tab. 1. In contrast, NAF achieves full VFM-agnosticism by deriving guidance solely from lightweight encoder applied to the input image I, eliminating any dependency on VFM low-resolution features. This simple yet effective design makes NAF roughly 4 faster than AnyUp with 25% fewer parameters, while achieving superior reconstruction quality. 3. Method: NAF 3.1. Architecture We introduce Neighborhood Attention Filtering (NAF), learnable upsampling framework that generalizes classical filtering through an attention formulation. NAF interprets upsampling as spatialand content-aware aggregation of low-resolution features, guided solely by the input image. Instead of predicting the spatial aggregation kernel directly, NAF learns its representation in the frequency domain. In practice, it predicts the spectral coefficients whose inverse discrete Fourier transform gives the spatial kernel, as we show in App. A. General formulation. Starting from an input image RHHRWHR3, vision foundation model (VFM) produces low-resolution feature map FLR RHLRWLRd. The goal of NAF is to reconstruct high-resolution feature map FHR RHHRWHRd that aligns with the fine spatial details of the image I, where HHR = HLR and WHR = WLR for an upsampling factor s. At its core, NAF computes the high-resolution feature at position as an attention-weighted combination of lowresolution features in its spatial neighborhood (p): FHR = 1 Z(p) (cid:88) exp qN (p) (cid:19) (cid:18) Qp, Kq FLR , (3) where Q, denote queries and keys, ., . defines the dot product, and Z(p) is normalization factor. The attention values correspond directly to the VFM features LR. The key design question is therefore: how to define Q, so that the attention captures cross-scale similarity while remaining independent to the VFM? The overall NAF architecture is illustrated in Fig. 2 and detailed in the following. Dual-Branch Guidance Encoder. Queries and keys are both derived from the input image through learnable Dual-Branch Guidance Encoder that extracts highresolution guidance map: Encθ(I) RHHRWHRC, where Figure 2. NAF architecture allows to upsample low-resolution VFM features to any resolution, guided solely by the original highresolution image. θ is set of learned parameters and denotes the number of guidance channels. Inspired by Inception design [44], the encoder is composed of two complementary branches designed to capture both fine-grained pixel details and local contextual information (see Fig. 3). The pixel-encoding branch applies series of 11 convolutional blocks to extract pixel-wise features, while the contextual-encoding branch employs 33 convolutions to aggregate local neighborhood information. Each branch consists of stacked convolutional blocks and outputs C/2 channels. The resulting feature maps from both branches are concatenated along the channel dimension. RoPE and Pooling. To encode relative positional information, we apply 2D Rotary Positional Embeddings (RoPE) [20] to the guidance features, yielding position-aware features RoPE(Encθ(I)). Attention queries correspond to the high-resolution RoPE-encoded features: Qp := RoPE(Encθ(I))p. (4) Attention keys are obtained by average pooling the same features to the low-resolution grid, ensuring geometric alignment with the low-resolution features FLR: (cid:2) RoPE(Encθ(I))q Kq := AvgPool (5) (cid:3), qq where the pooling is taken over all high-resolution pixel falling within the low-resolution position q. Cross-Scale Neighborhood Attention. Dense visual features exhibit strong spatial autocorrelation, with the most informative cues for upsampling pixel lying within its local vicinity. Prior work [6] shows that even global attention mechanisms learn to focus on nearby positions. Building Figure 3. Details of the dual-branch image encoder. NAF encoder considers both pixel-wise branch and local-contextual branch. on this, NAF employs Cross-Scale Neighborhood Attention mechanism where each high-resolution query attends only to compact neighborhood around its corresponding low-resolution location, aligning receptive fields across resolutions while keeping attention localized and efficient. This design brings two key benefits. First, by constraining attention to depend only on the input image I, NAF eliminates the need for semantic low-resolution features when computing attention keys. This decoupling enables direct transfer across different VFMs without retraining. Second, restricting attention to local neighborhoods significantly reduces keyquery interactions, achieving about 40% fewer GFLOPs compared to JAFAR [6] while maintaining high reconstruction quality and supporting larger upsampling ratios with smaller memory footprint (Tab. 1). Analogy with Classical Filters Our architecture parallels classical joint filtering methods, such as Joint Bilateral Filtering and its upsampling variant (JBU) [24] formulated in Eq. 1. Indeed, the Cross-Scale Neighborhood Attention formulation can be directly interpreted as two-component filtering process, with (1) Spatial Kernel: product of RoPE encode relative positional information, serving as the localized spatial kernel, and (2) Content Kernel: The dotproduct attention weights between high-resolution queries and low-resolution keys define an adaptive content kernel, using similarity information derived solely from the input image. Unlike approaches that use separate branches for query and key encoding [6, 51], NAF derives keys directly from pooled, position-aware queries. This design guarantees that the guidance mechanism is independent of VFM features. In addition the local mechanism allows for each pixel to attend only to its nearby region, capturing spatial proximity and appearance similarity from the encoded guidance image. Consequently, NAF is fully VFM-agnostic during inference and remains substantially faster and more memory-efficient than full attention mechanisms. Segmentation mIoU (Pascal VOC12) [10] Depth δ1(%) (NYUv2) [40] Method V.A DINOv2-R RADIOv2.5 Franca DINOv3 Mean DINOv3-7B Nearest FeatUp [11] Bilinear AnyUp [51] JAFAR [6] NAF (ours) 85.37 OOM 87.96 87.55 OOM 88.84 +3.29 +3.34 +4.09 +5.12 +5.58 81.71 84.47 84.46 85.51 85.93 86.60 84.99 84.43 86.99 86.62 87.10 87.85 79.75 83.91 83.07 85.49 86.31 86. 78.70 81.36 81.30 81.98 82.29 83.07 DINOv2-R RADIOv2.5 Franca DINOv3 Mean DINOv3-7B 79.47 82.34 81.58 83.96 83.88 84.75 82.49 85.31 83.90 84.89 84.40 85.47 79.89 80.67 81.20 82.12 81.93 82.67 85.25 87.11 86.10 86.36 86.37 86. +2.09 +1.78 +2.52 +2.39 +3.16 89.92 OOM 90.69 91.24 OOM 91.74 Table 2. Semantic segmentation (mIoU ) and depth estimation (δ1 ): Results on Pascal VOC [10] and NYUv2 [40] using features from different VFMs: DINOv2-R [7], RADIOv2.5-B [19], Franca-B [49], DINOv3-B [41]. Mean is computed against Nearest. We highlight best and second best scores, and best gain. V.A indicates VFM-agnostic models. OOM indicates training Out-of-Memory. 3.2. Training NAF is trained following procedure similar to that of Couairon et al. [6]. Given high-resolution input image IHR, we generate corresponding low-resolution image ILR by applying bilinear downsampling with factor of 2. Features extracted from IHR using VFM serve as the ground-truth high-resolution features FHR. Similarly, features extracted from ILR with the same VFM define the low-resolution inputs FLR. NAF then upsamples FLR into (cid:98)FHR := NAF(IHR, FLR), using IHR as the guidance image. For supervision, we employ simple ℓ2 reconstruction loss between the predicted and ground-truth features: Ltrain = (cid:98)FHR FHR2 2. Unlike previous works such as FeatUp [11], LoftUp [21], or AnyUp [51], we do not rely on additional regularization terms such as total variation, segmentation masks, or cropping consistency losses. This minimalist training setup highlights the robustness of NAF, which achieves strong performance despite its simplicity. More details about the training are provided in Sec. B.1. 4. Experiments We evaluate the effectiveness of NAF across multiple VFMs, tasks, and datasets. First, in Sec. 4.1, we assess its upsampling quality through linear probing on semantic segmentation and depth estimation. Then, in Sec. 4.2, we study the consistency and usefulness of the upsampled features for downstream applications such as open-vocabulary and video object segmentation. More details about the datasets and experiments are provided in Sec. B.2. 4.1. Linear probing of upsampled features To assess the quality of our VFM-agnostic upsampler, we conduct linear probing experiments on semantic segmentation and depth estimation. All VFMs take as input 448448 images normalized according to their corresponding preprocessing. The extracted representations are upsampled to the original image resolution, corresponding to 14 as in [32, 49] or 16 as in [19, 41] spatial scaling depending on the VFM. linear layer is then trained on top of the upsampled features to predict per-pixel semantic labels or depth values, depending on the task. For VFM-specific upsamplers [6, 11], models are trained on each corresponding VFM following official training codes before being frozen during probing. 4.1.1. Semantic segmentation. Across VFMs. We evaluate NAF against both VFMspecific upsamplers [6, 11] and VFM-agnostic approaches such as bilinear interpolation and AnyUp [51]. To assess VFM generalization, we test all methods across diverse set of strong vision foundation models: DINOv2-R-B [7], RADIOv2.5-B [19], Franca-B [49], DINOv3-B [41], and the large-scale DINOv3-7B [41]. Experiments are conducted on Pascal VOC [10] and results are reported in Tab. 2 (left). NAF achieves the best results across all VFMs, with an average +5.58 mIoU improvement over the Nearest baseline. The next-best upsampler, JAFAR [6] reaches +5.12 mIoU average gain, but it has to be retrained for each VFM. Notably, NAF is the first VFM-agnostic approach to surpass VFM-specific models such as JAFAR, whereas AnyUp [51] fails to do so. Moreover, VFM-dependent upsamplers cannot be trained on large models like DINOv3-7B due to memory constraints (even with batch size 1 on an A100 40GB-GPU), while NAF remains applicable and continues to improve performance on linear probing (see. Tab. 2) and downstream tasks (see. Tab. 5). Across datasets. We next evaluate NAF on multiple semantic segmentation benchmarks: COCO [28], Pascal VOC [10], ADE20K [57], and Cityscapes [5], while fixing the VFM to DINOv3-B [41]. Results are reported in Tab. 3. In this setting, surprisingly, recent upsamplers such as JAFAR [6], FeatUp [11], and AnyUp [51] fail to outperform the bicubic baseline [23] while more classical methods such as Joint Bilateral Filtering or Joint Bilateral Upsampling lead to better scores. NAF leads to the best results and consistently improves performance across all datasets, demonstrating strong robustness and generalization, with an average +4.23 mIoU gain over the nearest-neighbor upsampling baseline, and Method V.A COCO VOC ADE20K Cityscapes Mean Nearest FeatUp [11] AnyUp [51] Bilinear JAFAR [6] Bicubic JBF [25] JBU [24] NAF (ours) +1.57 +2.19 +2.59 +2.60 +2.80 +2.99 +2.57 +4.23 63.78 64.24 65.49 65.65 65.85 65.65 65.91 65.64 66.37 84.99 87.07 86.63 86.99 87.10 87.20 87.24 87.00 87.86 44.23 46.10 46.00 46.37 46.72 46.45 46.68 46.36 47.41 58.73 62.58 60.35 63.08 62.46 63.62 63.85 63.02 64.98 Table 3. Semantic segmentation (mIoU ), using Dinov3-B [41] features, on various datasets: COCO [28], Pascal VOC [10], ADE20K [57], and Cityscapes [5]. Mean is computed against Nearest. We highlight best and second best scores, and best gain. V.A indicates VFM-agnostic models. substantial improvement on Cityscapes for fine-grained segmentation. Across model sizes. Finally, we study how upsampling methods scale with model size using the DINOv2-R [7] family, including Small (S), Base (B), and Large (L) variants. Results are reported in Tab. 4. NAF again delivers consistent improvements across all sizes, achieving an average +5.27 mIoU over the nearest baseline and setting new state-of-the-art scores at every size. To verify the generality of our findings, we further evaluate NAF on broader set of randomly selected VFMdataset pairs (e.g., PE-Core [2], CAPI [8], DINO [3], PE-Spatial [2], SigLIP2 [47], etc.) including model size from Tiny to Large. The detailed results are provided in Sec. B.4. The observed trends and performance gains remain consistent with those reported here, confirming the robustness of our conclusions. We also report scores for other and weaker VFM-specific upsamplers [36, 43]. 4.1.2. Depth estimation We evaluate the quality of upsampled feature maps following the Probe3D [1] protocol, for depth estimation on the NYUv2 dataset [40]. Experiments are conducted across multiple base VFMs, and results are summarized in Tab. 2 (right). As opposed to semantic segmentation, depth estimation is regression task which requires fine-grain prediction. Again, NAF consistently achieves the best performance across all tested VFMs, with an average +3.16 δ1 improvement over the Nearest interpolation baseline and +0.57 gain over AnyUp [51]. Notably, NAF also enhances the performance of the large-scale DINOv3-7B model [41], yielding substantial +12.69 δ1 increase compared to Nearestneighbor interpolation. Method V.A DINOv2-R-S DINOv2-R-B DINOv2-R-L Mean Nearest Bilinear FeatUp [11] AnyUp [51] JAFAR [6] NAF (ours) +2.74 +3.77 +4.13 +4.82 +5.27 40.73 43.54 44.28 44.91 45.64 46.14 41.00 44.30 46.01 46.39 47.04 47.31 38.61 40.70 41.35 41.42 42.13 42. Table 4. Semantic segmentation (mIoU ) on ADE20K [57], using features from DINOv2-R [7] models of different sizes: DINOv2-R-S, DINOv2-R-B, and DINOv2-R-L. Mean is computed against Nearest. We highlight best and second best scores, and best gain. V.A indicates VFM-agnostic models. 4.2. Downstream transfer We extend our evaluation by analyzing how upsampling affects the transferability and consistency of feature representations across different downstream tasks. Specifically, we investigate two complementary aspects: (i) the transferability of upsampled features to open-vocabulary segmentation, and (ii) their temporal consistency across video frames through video label propagation. Transfer to open-vocabulary segmentation. We first improvements from upsampling study whether spatial translate into better semantic transfer on open-vocabulary segmentation. We use ProxyCLIP [26] to evaluate upsampled representations, replacing its default bilinear upsampling with different upsamplers including NAF as drop-in replacement without additional training. As reported in Tab. 5 (left), NAF achieves the highest average performance across evaluated VFMs, confirming its compatibility with many tasks. Moreover, while some upsamplers perform slightly better on specific VFM, NAF consistently yields the best overall results, reaching +1.04 mIoU improvement over the baseline, compared to +0.63 mIoU for the second-best method, JAFAR [6]. Transfer to video segmentation. We next evaluate the temporal consistency of upsampled features through video segmentation propagation on the DAVIS dataset [34]. Following the protocol of Suri et al. [43], we extract dense features for each frame, upsample them, and propagate segmentation masks across frames using feature-space similarity matching. As in ProxyCLIP, we replace the default bilinear upsampling with different upsampler choices; our method can again be used as drop-in replacement. As reported in Tab. 5 (right), NAF yields the best overall performance, achieving an average +3.37 mIoU improvement over the baseline, highlighting the effectiveness of our approach to keep feature consistency through frames. Open Vocabulary Segmentation Pascal VOC [10], mIoU (%) Video Object Segmentation DAVIS [34], &F Mean Method V.A DINOv2-R RADIO Franca DINOv3 Mean DINOv3-7B DINOv2-R RADIO Franca DINOv3 Mean DINOv3-7B Bilinear (default) AnyUp [51] FeatUp [11] JAFAR [6] NAF (ours) 62.00 62.00 59.53 63.04 60.69 47.07 46.49 51.08 44.32 48.96 60.95 62.72 61.55 63.67 62.88 62.21 63.41 62.54 63.72 63. 0.00 +0.60 +0.62 +0.63 +1.04 61.88 62.88 OOM OOM 63.06 64.36 69.56 64.17 72.94 69.49 63.75 68.16 65.24 69.72 69.25 70.52 68.90 71.17 70.89 72.82 70.00 65.90 69.53 69.24 70. 0.00 +0.97 +0.37 +3.54 +3.37 69.50 69.03 OOM OOM 71.39 Table 5. Comparison of upsampling methods across downstream tasks. using features from different VFMs: DINOv2-R [7], RADIOv2.5-B [19], Franca-B [49], DINOv3-B [41]. Left: Open vocabulary segmentation using ProxyCLIP on Pascal VOC [10] (mIoU, %). Right: Video object segmentation propagation on DAVIS (J &F Mean). All VFMs use Base (B) variants. Mean is computed against Nearest. We highlight best and second best scores, and best gain. V.A indicates VFM-agnostic models. OOM indicates training Out-of-Memory. 5. Ablations Design of the Dual-Branch Encoder. We first analyze the design of the dual-branch guidance encoder Encθ. The ablation studies isolate the impact of four factors: (1) the presence of the context branch (33 conv) in addition to the pixel branch (11 conv), (2) the block design (our separate dual-branch blocks vs. ResNet [18] and Inception-style blocks [44]), (3) the output guidance dimension C, and (4) the number of stacked blocks L. Results are reported in Tab. 6, where the configuration used in all other experiments is highlighted in light blue. Key observations are as follows: Adding the context branch is crucial for capturing local structure beyond per-pixel information. Our dual-branch encoder outperforms Inception-style blocks while remaining simpler and more efficient. Increasing and improves accuracy but with diminishing returns and higher computational cost. For all main experiments, we adopt lightweight conDINOv2-R RADIOv2.5 Franca DINOv3 Mean Params (M) FPS Encoders Pixel Enc. + Context Enc. 62.87 63.82 Block Type Inception [44] ResNet Dual-Branch Guidance dim. = 64 = 128 = 256 = 512 = 768 = 1024 # conv. blocks = 1 = 2 = 3 = 4 = 5 NAF++ 62.73 62.99 63.82 62.90 63.49 63.82 63.98 64.23 64.45 62.97 63.82 64.04 64.54 64.55 69. 58.13 58.76 57.97 58.15 58.76 57.89 58.48 58.76 58.86 59.11 59.41 58.14 58.76 58.92 59.27 59.34 61.51 57.46 58. 60.96 61.01 59.86 60.41 57.17 57.52 58.03 57.04 57.46 58.03 58.33 58.49 58.63 57.55 58.03 58.24 58.61 58.81 61. 60.37 60.49 61.01 60.14 60.59 61.01 61.10 61.47 61.73 60.41 61.01 61.15 61.88 61.82 59.56 59.79 60.41 59.49 60.01 60.41 60.57 60.82 61.05 59.77 60.41 60.59 61.08 61. 65.69 64.56 0.26 0.66 0.15 0.66 0.66 0.04 0.16 0.66 2.63 5.92 10.5 0.33 0.66 0.99 1.32 1. 14.8 19 18 15 18 18 40 30 18 9 6 4 27 18 14 11 9 Table 6. Ablation of the dual-branch image encoder: mIoU (% ) on Cityscapes. Blue and orange rows highlight the base configuration and the best one. VFMs are of the Base (B) variant. figuration with = 256 and = 2, that has roughly the same number of parameters of previous state-of-the art VFM-specific upsampler [6]. It offers good trade-off between performance and efficiency. larger variant, NAF++ shown in Tab. 6, uses higher = 768 and = 5 for improved accuracy at the cost of additional parameters and lower FPS. Design of the attention keys K. We evaluate alternative designs for the attention keys used in the filtering process. Our default choice, AvgPool defined in Eq. 5, averages the guidance features within each low-resolution region. We compare this to three variants: (1) MaxPool: replaces average pooling with max operation, (2) AvgPool + Conv.: applies 1 1 convolution after average pooling to mix channel information, similar to the design used in JAFAR [6] and AnyUp [51], (3) Bilinear: removes pooling entirely and computes Kq by bilinear interpolation. Results in Tab. 7 (left) show that pooling is essential: both AvgPool and MaxPool outperform the Bilinear variant, confirming the need for local aggregation. However, adding convolution on top of pooled features, mixing independently channels from queries and keys as done in previous works [6, 51], significantly degrades performance, likely by breaking the channel alignment between queries and keys. We therefore adopt the simple AvgPool design in all experiments. Spatial positional encoding. To enable spatial relationship reasoning in the attention formulation, NAF uses positional embeddings on its guidance features. Our default strategy is RoPE, applied to both queries and keys (Eq. 4Eq. 5), which encodes relative spatial offsets directly into the attention computation. Beyond the following ablation, more in depth mathematical analysis motivating the choice of RoPE can be found in App. A. Yet we compare this to explicit multiplicative spatial kernels: Qp, Kq = exp (cid:16) pq 2σ2 (cid:17)(cid:10) Encθ(I)p, Encθ(I)q (cid:11), (6) DINOv2-R RADIOv2.5 Franca DINOv3 Mean DINOv2-R RADIOv2.5 Franca DINOv3 Mean Pooling design for attention keys AvgPool + Conv. Bilinear MaxPool AvgPool 61.08 62.79 63.35 63.82 56.63 57.76 58.45 58.76 56.08 57.27 57.68 58.03 59.71 60.80 60.25 61. Spatial encoding design 58.38 59.66 Manhattan 59.93 Gaussian 60.41 60.32 61.47 61.65 63.82 RoPE 54.91 55.68 55.87 58.76 55.52 56.48 56.52 58.03 57.52 58.47 58.71 61. 57.07 58.03 58.19 60.41 Table 7. Ablations on the design of the attention keys and spatial encoding. We report linear probing semantic segmentation results on Cityscapes [5] (mIoU, %, ). The setting used by NAF is in blue. All VFMs are of the Base (B) variant. where is 2 2 (Gaussian) or 1 (Manhattan) and σ is learnable. We also test variant without positional encoding (). As shown in Tab. 7 (right), positional encoding is crucial for spatial awareness. RoPE achieves the best performance, efficiently capturing relative geometry without additional parameters. 6. Extension: Image Restoration We extend NAF beyond feature upsampling to demonstrate its broader applicability. This generalization follows naturally from its filter-based formulation: by design, NAF imposes no constraints on the dimensionality or structure of its input and guidance signals. As result, the same architecture can process either low-resolution feature maps (for upsampling) or standard RGB images (for restoration). To illustrate this flexibility, we evaluate NAF on image restoration tasks, where both the guidance and input correspond to the same image. Since there is no downsampling in this setting, the average pooling operation becomes the identity, and the query and key representations simplify to: = = RoPE(Encθ(I)). (7) Task and setup. We focus on image denoising, where the goal is to recover clean RGB image of size 448 448 from its corrupted version. Input images are generated by Method # Params (M) σ = 0.1 σ = 0. σ [0.1, 0.5] Gaussian Denoising (PSNR / SSIM (%)) DnCNN [53] IRCNN [54] REDNet [22] Restormer [52] NAF (ours) 0.56 0.19 1.04 26.13 0.66 30.86 31.39 31.99 32.51 32.12 87.1 88.9 89.8 90.5 90. 22.82 23.87 24.46 25.51 24.52 57.3 65.0 67.4 73.2 68.8 20.23 25.86 26.52 27.77 27.11 40.5 73.6 74.6 79.9 77.2 Method # Params (M) = 0. = 0.5 [0.1, 0.5] Channel-Wise Salt & Pepper Denoising (PSNR / SSIM (%)) DnCNN [53] IRCNN [54] REDNet [22] Restormer [52] NAF (ours) 0.56 0.19 1.04 26.13 0.66 35.62 35.09 35.14 47.76 47. 98.2 97.1 95.7 99.6 99.7 28.44 22.75 25.11 36.90 32.91 81.0 62.3 73.9 97.0 94.3 27.72 25.07 26.46 40.41 39.62 87.3 71.7 75.3 98.2 98.9 Table 8. Gaussian and Channel-Wise Salt & Pepper Denoising (PSNR / SSIM ) on ImageNet. The standard deviation for gaussian noise σ, and the channel-wise salt-and-pepper noise corruption probability are either fixed to 0.1 or 0.5, or randomly sampled from [0.1, 0.5]. adding artificial noise to ground-truth samples. We consider two types of corruption: (1) additive Gaussian noise with standard deviation σ, and (2) channel-wise salt-and-pepper noise, meaning we randomly activate or desactivate some channels with corruption probability p. And we consider both fixed and dynamic noises. We use the same architecture as in the feature upsampling experiments, except that we enlarge the neighborhood attention kernel to better capture spatial dependencies (from 9 to 15). The network is trained end-to-end with combination of L1, L2, and SSIM losses as done in classical training pipelines. More details can be found in Sec. C.1. Baselines. For fair comparison, we retrain classical denoising networks [22, 53, 54] and state-of-the-art transformer-based model, Restormer [52], using the same data, losses, and training schedule. Results. As shown in Tab. 8, NAF achieves strong results across all noise levels and types, despite not being specifically designed for denoising. While Restormer remains the top performer, it relies on roughly 40 more parameters and an architecture specifically designed for image restoration. Notably, conventional models perform well on Gaussian noise but degrade significantly under salt-and-pepper corruption, whereas NAF effectively recovers structural details in both cases. These results confirm that the neighborhood attention mechanism, guided only by the image itself, generalizes well beyond feature upsampling to broader image restoration tasks."
        },
        {
            "title": "Conclusion",
            "content": "We introduced NAF, VFM-agnostic upsampling module capable of scaling any feature to any resolution, including very large 7B-VFMs, achieving state-of-the-art results on many downstream tasks. At its core lies Cross-Scale Neighborhood Attention mechanism that eliminates dependency on the target feature distribution, drawing strong analogies to classical Joint Bilateral Filtering. Its local interpolation design allows it to be fast and lightweight compared to previous VFM upsamplers, enabling previously unseen scaling ratios. Finally, its simple yet powerful approach serves as versatile module which can be used in many different tasks paving the way to broader applications."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Amaia and Nicolas for proofreading the paper, and Yihong for constant support throughout the project."
        },
        {
            "title": "References",
            "content": "[1] Mohamed El Banani, Amit Raj, Kevis-Kokitsi Maninis, Abhishek Kar, Yuanzhen Li, Michael Rubinstein, Deqing Sun, Leonidas J. Guibas, Justin Johnson, and Varun Jampani. In Probing the 3d awareness of visual foundation models. CVPR, 2024. 6 [2] Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, Junke Wang, Marco Monteiro, Hu Xu, Shiyu Dong, Nikhila Ravi, Daniel Li, Piotr Dollar, and Christoph Feichtenhofer. Perception encoder: The best visual embeddings are not at the output of the network. arXiv:2504.13181, 2025. 6, 4 [3] Mathilde Caron, Hugo Touvron, Ishan Misra, Herve Jegou, Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerging properties in self-supervised vision transformers. In ICCV, 2021. 6, 4 [4] Jang Hyun Cho, Andrea Madotto, Effrosyni Mavroudi, Triantafyllos Afouras, Tushar Nagarajan, Muhammad Maaz, Yale Song, Tengyu Ma, Shuming Hu, Hanoona Rasheed, Peize Sun, Po-Yao Huang, Daniel Bolya, Suyog Jain, Miguel Martin, Huiyu Wang, Nikhila Ravi, Shashank Jain, Temmy Stark, Shane Moon, Babak Damavandi, Vivian Lee, Andrew Westbury, Salman Khan, Philipp Krahenbuhl, Piotr Dollar, Lorenzo Torresani, Kristen Grauman, and Christoph Feichtenhofer. Perceptionlm: Open-access data and models for detailed visual understanding. arXiv:2504.13180, 2025. 8 [5] Marius Cordts, Mohamed Omran, Sebastian Ramos, Timo Rehfeld, Markus Enzweiler, Rodrigo Benenson, Uwe Franke, Stefan Roth, and Bernt Schiele. The cityscapes dataset for semantic urban scene understanding. In CVPR, 2016. 5, 6, 8, 4 [6] Paul Couairon, Loick Chambon, Louis Serrano, JeanEmmanuel Haugeard, Matthieu Cord, and Nicolas Thome. In NeurIPS, Jafar: Jack up any feature at any resolution. 2025. 2, 3, 4, 5, 6, 7, 8 [7] Timothee Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers. In ICLR, 2024. 5, 6, [8] Timothee Darcet, Federico Baldassarre, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Cluster and predict latents patches for improved masked image modeling. TMLR, 2025. 6, 4 [9] Claude Duchon. Lanczos filtering in one and two dimensions. Journal of Applied Meteorology (1962-1982), 1979. 2 [10] Mark Everingham, SM Ali Eslami, Luc Van Gool, Christopher KI Williams, John Winn, and Andrew Zisserman. The pascal visual object classes challenge: retrospective. IJCV, 2015. 5, 6, 7, 3, 4 [11] Stephanie Fu, Mark Hamilton, Laura E. Brandt, Axel Feldmann, Zhoutong Zhang, and William T. Freeman. Featup: model-agnostic framework for features at any resolution. In ICLR, 2024. 2, 3, 5, 6, 7, 9 [12] Ali Hassani and Humphrey Shi. Dilated neighborhood attention transformer. arXiv preprint arXiv:2209.15001, 2022. [13] Ali Hassani, Steven Walton, Jiachen Li, Shen Li, and Humphrey Shi. Neighborhood attention transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2023. [14] Ali Hassani, Wen-Mei Hwu, and Humphrey Shi. Faster neighborhood attention: Reducing the o(n2) cost of self attention at the threadblock level. In Advances in Neural Information Processing Systems, 2024. [15] Ali Hassani, Fengzhe Zhou, Aditya Kane, Jiannan Huang, Chieh-Yun Chen, Min Shi, Steven Walton, Markus Hoehnerbach, Vijay Thakkar, Michael Isaev, et al. Generalized neighborhood attention: Multi-dimensional sparse attention at the speed of light. arXiv preprint arXiv:2504.16922, 2025. 4 [16] Kaiming He and Jian Sun. Fast guided filter. arXiv preprint arXiv:1505.00996, 2015. 2 [17] Kaiming He, Jian Sun, and Xiaoou Tang. Guided image filtering. TPAMI, 2012. 2 [18] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. In CVPR, Deep residual learning for image recognition. pages 770778, 2016. 7 [19] Greg Heinrich, Mike Ranzinger, Hongxu Yin, Yao Lu, Jan Kautz, Andrew Tao, Bryan Catanzaro, and Pavlo Molchanov. Improved baselines for agglomerative vision Radiov2.5: foundation models. In CVPR, 2025. 2, 5, 7, 8 [20] Byeongho Heo, Song Park, Dongyoon Han, and Sangdoo Yun. Rotary position embedding for vision transformer. In European Conference on Computer Vision, pages 289305. Springer, 2024. 4 [21] Haiwen Huang, Anpei Chen, Volodymyr Havrylov, Andreas Geiger, and Dan Zhang. Loftup: Learning coordinatebased feature upsampler for vision foundation models. In ICCV, 2025. 2, 3, [22] Jindong Jiang, Lunan Zheng, Fei Luo, and Zhijun Zhang. Rednet: Residual encoder-decoder network for indoor rgbd semantic segmentation. arXiv preprint arXiv:1806.01054, 2018. 8, 7 [23] Robert Keys. Cubic convolution interpolation for digital image processing. IEEE transactions on acoustics, speech, and signal processing, 2003. 2, 5 [24] Johannes Kopf, Michael F. Cohen, Dani Lischinski, and Joint bilateral upsampling. ACM Matthew Uyttendaele. Trans. Graph., 26(3):96, 2007. 2, 3, 4, 6 [25] Johannes Kopf, Michael Cohen, Dani Lischinski, and Matt Uyttendaele. Joint bilateral upsampling. ACM Transactions on Graphics (ToG), 2007. 2, 6 [26] Mengcheng Lan, Chaofeng Chen, Yiping Ke, Xinjiang Wang, Litong Feng, and Wayne Zhang. Proxyclip: Proxy attention improves clip for open-vocabulary segmentation. In ECCV, 2024. 6, [27] Hanchao Li, Pengfei Xiong, Jie An, and Lingxue Wang. Pyramid attention network for semantic segmentation. arXiv preprint arXiv:1805.10180, 2018. 3 [28] Tsung-Yi Lin, Michael Maire, Serge J. Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and C. Lawrence Zitnick. Microsoft COCO: common objects in context. In ECCV, 2014. 5, 6, 4 [29] Wenze Liu, Hao Lu, Hongtao Fu, and Zhiguo Cao. Learning to upsample by learning to sample. ICCV, 2023. 3 [30] Jonathan Long, Evan Shelhamer, and Trevor Darrell. Fully convolutional networks for semantic segmentation. In CVPR, 2015. 3 [31] Hao Lu, Wenze Liu, Zixuan Ye, Hongtao Fu, Yuliang Liu, and Zhiguo Cao. Sapa: Similarity-aware point affiliation for feature upsampling. NeurIPS, 2022. [32] Maxime Oquab, Timothee Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, ShangWen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 5 [33] Anthony Parker, Robert Kenyon, and Donald Troxel. Comparison of interpolating methods for image resampling. IEEE Transactions on medical imaging, 2007. 2 [34] Jordi Pont-Tuset, Federico Perazzi, Sergi Caelles, Pablo Arbelaez, Alexander Sorkine-Hornung, and Luc Van Gool. The 2017 DAVIS challenge on video object segmentation. CoRR, abs/1704.00675, 2017. 6, 7 [35] Mike Ranzinger, Greg Heinrich, Jan Kautz, and Pavlo Molchanov. Am-radio: Agglomerative model reduce all domains into one, 2023. 2 [36] Mike Ranzinger, Greg Heinrich, Pavlo Molchanov, Bryan Catanzaro, and Andrew Tao. Featsharp: Your vision model features, sharper. In ICML, 2025. 2, 3, 6, 5 [37] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. Unet: Convolutional networks for biomedical image segmentation. In International Conference on Medical image computing and computer-assisted intervention, pages 234241. Springer, 2015. [38] Daniel Ruijters and Philippe Thevenaz. Gpu prefilter for accurate cubic b-spline interpolation. The Computer Journal, 2012. 2 [39] Wenzhe Shi, Jose Caballero, Ferenc Huszar, Johannes Totz, Andrew Aitken, Rob Bishop, Daniel Rueckert, and Zehan Wang. Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network. In CVPR, 2016. 3 [40] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Indoor segmentation and support inference from Fergus. RGBD images. In ECCV, 2012. 5, 6, 4, 7 [41] Oriane Simeoni, Huy Vo, Maximilian Seitzer, Federico Baldassarre, Maxime Oquab, Cijo Jose, Vasil Khalidov, Marc Szafraniec, Seungeun Yi, Michael Ramamonjisoa, et al. Dinov3. arXiv preprint arXiv:2508.10104, 2025. 2, 5, 6, 7, 4, 8 [42] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. 1, 2 [43] Saksham Suri, Matthew Walmer, Kamal Gupta, and Abhinav Shrivastava. Lift: surprisingly simple lightweight feature transform for dense vit descriptors. In ECCV, 2024. 2, 3, 6, 4, 5 [44] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott E. Reed, Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich. Going deeper with convolutions. In CVPR, 2015. 4, 7 [45] Carlo Tomasi and Roberto Manduchi. Bilateral filtering for gray and color images. In ICCV, 1998. 2 [46] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herve Jegou. Fixing the train-test resolution discrepancy: Fixefficientnet. CoRR, 2020. [47] Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. 6, 4 [48] Michael Unser, Akram Aldroubi, Murray Eden, et al. Fast b-spline transforms for continuous image representation and interpolation. In PAMI, pages 277285, 1991. 2 [49] Shashanka Venkataramanan, Valentinos Pariza, Mohammadreza Salehi, Lukas Knobel, Spyros Gidaris, Elias Ramzi, Andrei Bursuc, and Yuki Asano. Franca: Nested matryoshka clustering for scalable visual representation learning. arXiv preprint arXiv:2507.14137, 2025. 5, 7 [50] Jiaqi Wang, Kai Chen, Rui Xu, Ziwei Liu, Chen Change Loy, and Dahua Lin. Carafe: Content-aware reassembly of features. In ICCV, 2019. 3 [51] Thomas Wimmer, Prune Truong, Marie-Julie Rakotosaona, Michael Oechsle, Federico Tombari, Bernt Schiele, and Jan Eric Lenssen. Anyup: Universal feature upsampling. arXiv preprint arXiv:2510.12764, 2025. 2, 3, 4, 5, 6, 7, 8 [52] Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang. Restormer: Efficient transformer for high-resolution image restoration. In CVPR, 2022. 8, 7 [53] Kai Zhang, Wangmeng Zuo, Yunjin Chen, Deyu Meng, and Lei Zhang. Beyond Gaussian denoiser: Residual learning of deep CNN for image denoising. TIP, 2017. 8, [54] Kai Zhang, Wangmeng Zuo, Shuhang Gu, and Lei Zhang. Learning deep cnn denoiser prior for image restoration. In CVPR, 2017. 8, 7 [55] Qi Zhang, Li Xu, and Jiaya Jia. 100+ times faster weighted median filter (wmf). In CVPR, 2014. 2 [56] Hengshuang Zhao, Jianping Shi, Xiaojuan Qi, Xiaogang Wang, and Jiaya Jia. Pyramid scene parsing network. In CVPR, 2017. 3 [57] Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Scene parsing through Barriuso, and Antonio Torralba. ade20k dataset. In CVPR, 2017. 5, 6, NAF: Zero-Shot Feature Upsampling via Neighborhood Attention Filtering"
        },
        {
            "title": "Table of contents",
            "content": "A. Mathematical Discussions A.1. RoPE Expansion . . . . . . A.2. Representation with magnitudes and angles. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B. Feature Upsampling Experiments . . . . . . . . . . B.1. Training Details . . . . B.2. Task setups . . . B.3. Visualizations . . . B.4. Generalization results . B.5. Learning representation . . B.6. VFM upsamplers as filters . . B.7. Scaling ratio . . . . . . . . . . . . . . . . C. Image Restoration C.1. Evaluation Setup . . C.2. Visualizations . . . . . . . . . . . . . . D. Computation footprint E. Limitations and Perspectives A. Mathematical Discussions 1 1 2 3 3 4 4 4 5 6 7 7 7 8 9 We analyze the mathematical foundations of NAFs upscaling mechanism, focusing on the interaction between RoPE [42] and neighborhood attention. Our key finding is that NAF does not merely reweight the inputs using distance over the image encoder; instead, it learns the Inverse Discrete Fourier Transform (IDFT) of the upsampling aggregation kernel. In other words, NAF dynamically constructs an optimal upsampling filter by predicting spectral coefficients of the learned image encoder. Preliminaries To recall, NAF shows analogies with Joint Bilateral Filtering due to the spatial-and-content aware kernel. It allows to obtain high-resolution features via the following attention-weighted interpolation: FHR = 1 Z(p) (cid:88) qN (p) exp (cid:16) 1 S(p, q) (cid:17) FLR , (8) qN (p) exp (cid:0) 1 S(p, q)(cid:1) is the normalizawhere Z(p) = (cid:80) tion constant and S(p, q) := Qp, Kq is an attention-score for pair of points (p, q) with queries and keys defined as Qp := RoPE(G)p, Kq := AvgPool qq (cid:2) RoPE(G)q (cid:3), and = Encθ(I) Rd denotes the image encoder output having channels."
        },
        {
            "title": "Substituting the pooled key yields",
            "content": "the following attention-score: S(p, q) = 1 {q q} (cid:88) qq A.1. RoPE Expansion RoPE(G)p, RoPE(G)q. (10) RoPE introduction. To develop the last equation we discuss about 2D-RoPE [42]. To do so, we consider channel pairs (2c, 2c + 1) where {0, ..., d/2} and define the 2D feature vector for the c-th pair as: Gc(p) := (cid:18) G2c G2c+1 (cid:19) , (11) where G2c and in channel 2c. is the value of the encoded image G, at position By definition of RoPE we have for each c: RoPE( Gc(p)) = Rc(p) Gc(p), (12) where the rotation matrix is Rc(p) := (cid:18)cos Φc(p) sin Φc(p) (cid:19) cos Φc(p) sin Φc(p) . (13) with the rotation angle Φc(p) encoding the axial positional information for channel pair c. It is defined by: Φc(p) = (cid:40) 2π py/λc if 0 < d/4 (Height) 2π px/λc if d/4 < d/2 (Width) , (14) with λc the wavelength of the c-th frequency band, and px, py [1, 1] are normalized coordinates along each axis. Inner product after rotation. We can reinject the definition of RoPE [42] in the attention-score between two positions and q. It becomes: RoPE( Gc(p)), RoPE( Gc(q)) = Gc(p)Rc(p)Rc(q) Gc(q). (15) Using properties of 2D rotation matrices, the product Rc(p)Rc(q) is itself rotation by the relative angle (9) Φc(p, q) := Φc(q) Φc(p). (16) Figure 4. Illustration of the mean and channel-specific cosine and sine of Φc. We compute the mean across all channels and select single random channel to illustrate its individual behavior. For the cosine, we observe an overall decreasing pattern as the distance from the center increases. Hence, we can write Rc(p)Rc(q) = (cid:18)cos Φc(p, q) sin Φc(p, q) cos Φc(p, q) sin Φc(p, q) (cid:19) . (17) To better visualize Φc, we plot in Fig. 4 the mean of cosine and sine over all channels and their values at specific channel given neighborhood window of size 9 9. We see that in average the cosine decreases when the points become far, while the sine has diagonal shape due to the axial-nature of Φc(p). Dot and cross product decomposition. Expanding the inner product of 2D vectors under rotation gives the perchannel contribution: Ac(p, q) = Gc(p)Rc(p)Rc(q) Gc(q) = ( Gc(p) Gc(q)) cos Φc(p, q) (18) ( Gc(p) Gc(q)) sin Φc(p, q), with the standard dot and cross products. Interpretation. The dot product Gc(p) Gc(q) measures feature coherence (alignment), while the cross product Gc(p) Gc(q) captures content orthogonality (perpendicularity). RoPE modulates these content interactions based strictly on the relative axial distance: vertical distance for d/2 channels and horizontal distance for the remaining d/2 channels. As we can see in Fig. 5, the model learns to discriminate regions based on their encoding. While querying the dog, we recognize its shape and the model learns to aggregate inside values while discriminating outside ones. A.2. Representation with magnitudes and angles. Let Ψc(p, q) be the angle from Gc(p) to Gc(q), and let r(c) := Gc(p), r(c) := Gc(q) (19) Figure 5. Dot and cross products for specific channel given query point on an image. We highlight the neighborhood around using dashed red square. On the feature side, after VFM-downsampling, we observe that implicitly NAF discriminates boundaries. Then the dot and cross products can be expressed as: Gc(p) Gc(q) = r(c) Gc(p) Gc(q) = r(c) r(c) r(c) cos Ψc(p, q), sin Ψc(p, q), (20) so that the per-channel contribution becomes Ac(p, q) = ( Gc(p) Gc(q)) cos Φc(p, q) ( Gc(p) Gc(q)) sin Φc(p, q) = r(c) r(c) (cid:2) cos Ψc(p, q) cos Φc(p, q) sin Ψc(p, q) sin Φc(p, q)(cid:3) = r(c) r(c) cos (cid:0)Ψc(p, q) + Φc(p, q)(cid:1), (21) where the last equality follows from the cosine angle addition formula. Finally, the pooled attention-score aggregates pairwise interactions over the pooling neighborhood as: S(p, q) = 1 {q q} (cid:88) qq RoPE(G)p, RoPE(G)q = 1 {q q} (cid:88) d/21 (cid:88) qq c=0 r(c) r(c) cos(cid:0)Ψc(p, q) + Φc(p, q)(cid:1). (22) This decomposition clarifies the geometric mechanism of RoPE [42]. Rather than linearly adding positional bias to content score, Eq. 10 shows that position and content are coupled via phase addition. The magnitude term r(c) r(c) represents the raw signal strength (feature confiq dence). The cosine term indicates that the spatial phase difference Φc acts as rotation applied to the semantic phase alignment Ψc. Constructive interference (a high score) occurs only when the semantic relationship compensates for the spatial offset, effectively implementing spatially-varying matched filter. denote the magnitudes of the feature vectors for channel pair c. Fourier-inspired Interpretation The derivation of the pairwise attention-score in Eq. (10) reveals structural 3. Shift Demodulation. The RoPE term ωcx explicitly encodes the spatial shift theorem. By decomposing the interaction into spectral bands, the attention mechanism can align features that are semantically coherent but spatially phase-shifted. The summation over channels then reconstructs the spatial impulse response required to interpolate the feature at the exact sub-pixel position required by the target resolution. Figure 7. Attention maps: Qp, Kq between query point and its 9 9 patch neighborhood (q). We take 896 896 input images to visualize finer details. In the first row, we see that NAF learns to discriminate between the sky and the tree, i.e., borders. On the second row, it learns to discriminate more complex shapes (dog). On the third row, in uniform region, it shows decreasing attention pattern, akin to the gaussian filter used in classical JBF. B. Feature Upsampling Experiments B.1. Training Details NAF is initially trained for 25k iterations with batch size of 2, using input and target features extracted from 256 256 and 512 512 images, respectively, corresponding to 2 upsampling. Subsequently, the module undergoes an additional 2.5k iterations (10% of the initial training) using 1024 1024 images for the target features, while input features are drawn from images of varying resolutions between 256 256 and 896 896. Quantitatively, we observe an average of +0.4 mIoU gains on linear probing semantic segmentation evaluated on VOC [10]. Ablation studies in Sec. 5 are performed without this second training stage. Figure 6. Illustration of radial wavelets induced by NAF for 9 9 neighborhood. We plot cos(ωc x) for different λ where is defined as the ℓ1 distance over x-axis or y-axis between two coordinates over grid map. In this plot we set the periods as: λi = 100i/c. equivalence to the Inverse Discrete Fourier Transform (IDFT). To see this, consider the standard reconstruction of 1D spatial signal (x) from its frequency components: (x) = (cid:88) ω (ω) (cid:124) (cid:123)(cid:122) (cid:125) Amplitude cos( ψ (cid:124)(cid:123)(cid:122)(cid:125) Content Phase + ωx (cid:124) (cid:123)(cid:122) (cid:125) Spatial phase ). (23) By viewing the channel dimension through the lens of Rotary Embeddings, where each channel corresponds to specific wavelength λc, we can identify as spectral frequency index ωc 1/λc. We illustrate the resulting cosine and sine in Fig. 6. Consequently, our derived attentionscore S(p, q) represented as 1D score: S(x) acts for each axis as kernel synthesized via IDFT: r(c) r(c) (cid:124) (cid:123)(cid:122) (cid:125) Amplitude + ωcx (cid:124) (cid:123)(cid:122) (cid:125) Spatial Phase Ψc (cid:124)(cid:123)(cid:122)(cid:125) Content Phase S(x) cos(cid:0) (24) (cid:88) (cid:1). This mapping offers three fundamental insights into the mechanism of NAF: 1. Learning Fourier Coefficients. The network does not Instead, it predicts directly predict spatial filter weights. the Fourier series coefficients of the optimal upsampling kernel. The product of feature magnitudes r(c) acts as the spectral power for frequency band c. By modulating these magnitudes, the encoder determines how much contribution each frequencylow (global structure) or high (fine detail)makes to the final interpolation kernel. r(c) 2. Spatially-Varying Filter Synthesis. This formulation allows NAF to function as spatially-varying bandpass filter. In smooth image regions, the encoder can suppress high-frequency channels (reducing Ac for large c), effectively synthesizing broad, low-pass smoothing kernel. Conversely, at sharp boundaries, the encoder can boost high-frequency amplitudes to synthesize peaked, detailpreserving kernel (see Fig. 7). The full training procedure requires approximately 1 hour and 9 GB of memory on an A100 GPU, resulting in 5 speedup compared to the concurrent AnyUp method [51]. For the final model, the dual-branch encoder employs = 2 blocks with = 256 channels and neighborhood kernel size of 9. This configuration ensures fair comparison with other upsamplers, maintaining parameter count similar to JAFAR [6] and an equivalent number of encoder blocks. For the neighborhood attention module, we adopt an efficient implementation [1215]. B.2. Task setups Semantic Segmentation. To evaluate the upsamplers, we freeze their parameters and train linear classifiers on the extracted features following Couairon et al. [6]. We train for 20 epochs on Cityscapes [5], Pascal VOC [10], and ADE20K [57], and for 5 epochs on COCO [28]. We employ the AdamW optimizer with learning rate of 5 104 for most datasets; however unlike Couairon et al. [6], for Cityscapes, we reduce the learning rate to 1104 to ensure stability. one-cycle cosine annealing scheduler is applied, and all input and target images are resized to 448448. The classifiers are optimized using the cross-entropy loss. Each dataset has different number of classes: Cityscapes has 19 classes, Pascal VOC has 21 classes, ADE20K has 151 classes and COCO has 27 classes. Depth Estimation. For monocular depth estimation, we train linear regressors on NYUv2 [40] for 20 epochs, with learning rate of 5 104 and one-cycle cosine annealing scheduler following depth estimation protocol of Couairon et al. [6]. Consistent with the segmentation task, input and target images are resized to 448 448. Ground-truth depth values are clipped to the range [dmin, dmax], with dmin = 103 and dmax = 10.0. We optimize the model using combination of scaleinvariant and gradient-based losses. Let ˆD denote the predicted depth map and the target depth map, where values > dmax are set to zero. The total depth loss is defined as: Ldepth = λσ LSI( ˆD, D) + λ Lgrad( ˆD, D), (25) where LSI is the scale-invariant loss and Lgrad is the gradient loss. We set the weighting terms to λσ = 10.0 and λ = 0.5 to encourage both accurate depth prediction and spatial smoothness. Video Propagation. To evaluate the upsamplers in the context of video propagation, we follow the protocol of Suri et al. [43]. We use 448448 input images and extracted features are extracted and subsequently upscaled by factor of 2 using the proposed upsamplers, followed by bilinear interpolation. Then we propagate labels using Suri et al. [43] protocol. Regarding the sparsity constraints, we set = 20, retaining only the 20 strongest source pixels for each target pixel based on affinity scores; all lower affinity values are zeroed, and we apply binary locality constraint with neighborhood radius of = 24. This restricts the potential source pixels to spatial window of size (2r + 1)2 centered around the target pixel before the top-k selection is applied. Open Vocabulary. We follow ProxyCLIP [26] protocol using 4 upsampling factor with 448 448 input images. Since it is direct inference, we did not change anything from the pipeline. B.3. Visualizations In Fig. 8, we present PCA visualizations of the upsampled feature maps produced by the evaluated methods. The feature maps generated by NAF exhibit smoother spatial variations compared to those of JAFAR [6] and AnyUp [51], which display sharper transitions, while also avoiding the halo artifacts observed in FeatUp. These smoother feature maps are reflected in the downstream linear probing results for both semantic segmentation  (Fig. 9)  and depth estimation  (Fig. 10)  . For segmentation, NAF produces masks with substantially fewer sparse or fragmented artifacts compared to JAFAR and AnyUp. Likewise, the depth maps obtained with NAF exhibit markedly smoother predictions in flat regions while still preserving sharp object boundaries, outperforming Bilinear, JAFAR and AnyUp in this regard. B.4. Generalization results To better assess the quality of upsamplers across different state-of-the-art VFMs, we evaluate wide range of models of various sizes (TSBL) from different families (PECore [2], CAPI [8], DINO [3], PE-Spatial [2], SigLIP2 [47]), using both VFM-specific and VFM-agnostic upsamplers. on various datasets. To mitigate the computational cost of full factorial evaluation, we adopt randomized two distinct VFMs are randomly assampling strategy: signed to each dataset. The resulting performance metrics are summarized in Tab. 9. From this analysis, we draw the following conclusions: Increasing the VFM input image size by 2 factor leads to slightly higher average gains than using bilinear on standard images: +4.65 mIoU vs +4.13 mIoU. The larger the scaling factor, the lower the results. Using 4 factor instead of 2 leads to lower results from +4.65 mIoU gain to -6.25 mIoU drop. Only DINOv3-L [41] on COCO continue to have higher results when increasing image size highlighting that increasing image size can increase scores depending on models and datasets. NAF leads the average gains by +7.46 mIoU resulting in +1.23 mIoU gain over next previous state of the art. Figure 8. PCA plots of different upsamplers for random images. The first colum represents RGB images, the second one, the low resolution features, the others the PCA after upsampling. We use the same basis decomposition for plotting. Only JAFAR [6], AnyUp [51] and NAF produce sharp PCAs while preserving input feature representation. COCO [28] VOC [10] ADE20K [57] Cityscapes [5] Method Large (4) LiFT [43] Nearest Bicubic [23] Bilinear Large (2) FeatUp [11] FeatSharp [36] JAFAR [6] AnyUp [51] NAF V.A DINOv3-L DINOv2-R-S CAPI-L PE-Core-S DINOv2-B PE-Spatial-T SigLIP2-B DINOv3-C-S 39.93 38.75 39.82 41.93 42.85 43.58 44.28 42.90 44.94 44.44 45. 56.21 57.66 57.34 58.98 59.77 60.20 60.91 59.80 61.92 61.58 62.17 24.06 78.82 76.99 79.75 81.37 70.87 82.20 84.10 84.03 84.00 84.64 20.74 17.31 22.24 23.35 23.89 25.29 24.55 23.61 23.85 24.68 25.08 50.29 52.02 45.05 49.61 51.29 56.98 51.73 52.75 52.25 53.58 56.98 67.59 61.45 64.47 66.55 66.61 67.48 66.93 66.85 66.81 66.54 67.35 69.21 48.02 47.90 54.70 54.14 63.58 51.57 56.84 50.70 54.40 56. 35.06 55.36 59.25 62.76 66.20 62.99 68.10 67.35 74.85 73.66 74.47 Mean -6.25 -0.46 0.00 +3.07 +4.13 +4.65 +4.65 +5.14 +5.79 +6.23 +7.46 Table 9. Semantic Segmentation (mIoU ) on Random Combinations of VFMs and datasets. VFMs come from different sizes and families and are evaluated on many datasets: COCO [28], Pascal VOC [10], ADE20K [57]. Mean is computed against Nearest. We highlight best and second best scores, and best gain. V.A indicates VFM-agnostic models. B.5. Learning representation We investigate the following question for our framework: how does the choice of the VFM used during training affect the final upsampling performance? Although NAF is designed for zero-shot use with any VFM at inference, its Dual-Branch Encoder is optimized using features from one specific VFM during training. To analyze this, we trained NAF using features from DINOv3-B [41], DINOv2-R-B [7], and DINOv2-S [32]. The evaluation on other VFMs at inference time is detailed in Tab. 10. We find counterintuitive result: VFM with strong general performance such as DINOv3-B does not necessarily yield the best reFigure 9. Segmentation predictions using different upsamplers. The first two rows are on Cityscapes [5], the last two rows on VOC [10]. We see that while being VFM-agnostic NAF better preserves structure compared to JAFAR [6] and AnyUp [51] that tend to focus too much on colors leading to noisy semantics. sults for training NAF, and we often achieve higher upsampling scores when training with smaller or less abstract representations such as DINOv2-R-S. Conversely, using raw RGB pixels (treating image upsampling as feature upsampling) caused significant performance drop (RGB), confirming the need for encoded features. Furthermore, training with multiple VFMs simultaneously (Mixture) did not improve scores (see. DINOv2-R-B), indicating that single, appropriate representation is sufficient to efficiently guide the attention-based upsampling process. Backbone DINOv2-R-B RADIOv2.5-B Franca-B DINOv3-B Mean RGB No training DINOv3-B DINOv2-S Mixture DINOv2-R-B 58.36 60.46 63.82 64.02 64.16 65.25 54.79 56.53 58.76 58.98 59.18 59.86 54.52 56.26 58.03 58.24 58.29 59.24 60.40 61.92 61.01 61.06 61.73 62.54 57.02 58.79 60.41 60.58 60.84 61.72 Table 10. Segmentation (mIoU ) on Cityscapes [5], training NAF with different backbones. The best average score is highlighted in orange, and the standard training configuration used for NAF is indicated in blue. B.6. VFM upsamplers as filters Instead of performing upsampling, we investigate how well learned upsamplers can function as feature filters. To do so, we apply the upsamplers using the target output resolution equal to that of the input features. In this setup, no upsampling is performed; the upsamplers effectively act as feature filters. The filtered features are subsequently upsampled using bilinear interpolation, and linear classifier is trained on top of them. As shown in Tab. 11, NAF achieves the Figure 10. Detph Estimation using different upsamplers on NYUv2 [40]. Compared to AnyUp [51], NAF outputs smoother predictions without the noisy effect we observe on some regions using AnyUp. best average improvements compared to other filtering approaches with +0.75 mIoU. Although the gains are modest, they suggest free lunch: applying lightweight filters on top of VFMs yields additional mIoU without modifying the underlying model. Method JAFAR [6] AnyUp [51] Bilinear NAF (ours) V.A DINOv2-R RADIO Franca DINOv3 Mean +0.32 +0.29 0.00 +0.75 81.63 81.69 81.30 82.01 84.76 84.59 84.47 85.05 84.47 84.10 83.07 84. 86.26 86.62 86.99 86.88 Table 11. Semantic Segmentation (mIoU ) on VOC [10] using VFM-upsamplers as feature filters. We use base VFMs: DINOv2-R-B [7], RADIOv2.5-B [19], Franca-B [49], and DINOv3-B [41]. Mean is computed relative to Bilinear. Bold and underline indicate the best and second-best scores, respectively, while highlighted indicates the largest gain. V.A denotes VFM-agnostic models. B.7. Scaling ratio We evaluate the robustness of NAF to different upsampling ratio in Tab. 12. We take as input different image resolution (224 224, 448 448 and 896 896), feed them to the VFM and upscale the features to 448448 resolution before learning the linear probing classifier. NAF always leads to the best or second best score regardless of the scaling ratio, proving that it can be used across wide range of resolutions. Interestingly, as already mentioned, for some VFM (e.g., PE-Core-B) feeding larger images does not lead to higher scores. Nonetheless, NAF still improves the mIoU of degraded representations. C. Image Restoration C.1. Evaluation Setup We evaluate several image denoising models DNCNN [53], IRCNN [54], RedNet [22], and Restormer [52] on ImageNet for 25k steps using input images of size 448448. We select the largest batch size that fits on single A100 Method AnyUp Bilinear Nearest JAFAR NAF 14 448 Radiov2.5-B 28 56 448 14 448 PE-Core-B 28 448 56 448 53.85 55.85 49.10 54.51 56.31 62.04 66.63 61.34 62.33 67. OOM 68.74 66.42 OOM 69.04 54.92 53.54 45.12 51.02 54.47 58.64 56.83 49.98 51.95 61.06 OOM 43.81 39.03 OOM 48.44 Table 12. Semantic Segmentation (mIoU ) on Cityscapes [5] for different Upsampling-ratio. We compare different upsamplers for generating 448 448 features from various feature input resolutions. The first three columns correspond to RADIOv2.5-B [19], and the last three columns to PE-Core-B [4]. Bold indicates the best score, and underline the second-best. OOM indicates linear probing-training Out-of-Memory. 40GB GPU: = 32 for the convolutional models and = 1 for Restormer [52]."
        },
        {
            "title": "The denoisers are trained with a combination of three",
            "content": "loss terms: L1, L2, and SSIM. Denoting the total loss as = λ1 LL1 + λ2 LL2 + λ3 LSSIM, (26) we set the weights to λ1 = 1.0, λ2 = 5.0, and λ3 = 0.2. To train NAF we keep the same architecture than for feature upsampling but we increase the neighborhood kernel size from 9 to 15 to take into account the receptive field difference. C.2. Visualizations To evaluate the denoisers performance, we apply noise to set of clean 448 448 images and feed them to NAF. In Fig. 11, we test models trained on dynamic ranges of Gaussian noise σ [0.1, 0.5] and salt-and-pepper noise [0.1, 0.5]. We observe that the model can effectively denoise channel-wise salt-and-pepper noise even beyond the maximal training range (rightmost image uses 0.6, while the model has been trained up to 0.5 noise intensity), while achieving high-quality reconstructions for other noise levels as well. D. Computation footprint We previously provided initial insights into the efficiency of different upsamplers in Tab. 1, where NAF is the only approach capable of achieving 72 upsampling ratio, producing features at resolution of 2048 2048. We now investigate in greater detail the behavior of these upsamplers when targeting different scaling factors or when processing higher-dimensional feature maps (Fig. 12, Fig. 13). To this end, we initialize dummy feature tensor of size (28, 28, 384), corresponding to the output of standard model processing 448 448 input images. We then conduct two controlled studies: (i) varying the embedding dimension while keeping fixed 16 upsampling ratio  (Fig. 12)  , and (ii) varying the upsampling ratio while maintaining 384 Figure 11. Image restoration using NAF. On the left two images we apply gaussian noise. On the right we apply channel-wise salt and pepper noise. NAF allows to restore very noisy images even on unseen noise range (rightmost image). Method Large NAF GFLOPs Time (ms) 2 537 4 4 8 2148 8591 66 2 110 39 4 8 1035 6555 42 Table 13. Comparison of NAF and the Large Image baseline. We measure GFLOPs and inference time required to produce features at 2, 4, and 8 the original resolution of DINOv3-B [41] outputs. For the Large Image baseline, this corresponds to feeding images scaled by the same factors, relative to the standard 448 448 resolution. channels  (Fig. 13)  . For each configuration, we evaluate the total number of parameters (in millions, M), the computational cost (GFLOPs), the time required for the forward pass (relevant for inference) and backward pass (relevant for training), as well as the peak memory consumption during both forward and backward passes. Although JAFAR [6] and AnyUp [51] are attentionbased models like NAF, NAF achieves substantially higher In the embedding memory and computational efficiency. it provides an approximately 23 speedup and study, memory reduction compared to AnyUp. For low upsampling ratios, AnyUp is slightly more efficient; however, its efficiency degrades rapidly with larger upsampling factors, resulting in an approximately 3 advantage for NAF at high upscaling levels. Furthermore, processing larger input images with standard state-of-the-art VFM leads to substantial increase in GFLOPs and runtime, whereas NAF maintains significantly lower computational cost, as shown in Tab. 13. Across many configurations, the efficiency of NAF is comparable to that of FeatUp [11], which relies on convolutions but is intrinsically constrained to fixed output resolutions. In contrast, NAF combines the flexibility of an any-scale upsampler with the computational efficiency characteristic of convolution-based designs, while consistently outperforming attention-based alternatives. E. Limitations and Perspectives Compared to both VFM-specific and VFM-agnostic upsamplers, NAF achieves state-of-the-art performance across multiple datasets and tasks. Nevertheless, several avenues remain for improvement. By design, NAF employs neighborhood-attention mechanism with fixed kernel size. We observe that the attention maps vary depending on the query point (see. Fig. 7). Introducing dynamic kernel adaptationsimilar to approaches in Deformable Attention or Deformable Convolutionscould, in principle, reduce the computational cost per interpolation step while potentially enhancing reconstruction accuracy by introducing sampling flexibility. Although our method is VFM-agnostic, we currently lack principled framework for identifying which VFMs provide the most informative patch representations for learning the upsampling. Closing this gap requires deeper understanding of the representational properties that support zero-shot consistent upsampling. Empirically, we observe that neither combining multiple VFMs nor using larger or stronger VFMs leads to clear gains (see. Tab. 10). In terms of applications, looking ahead, the ability to preserve high-resolution spatial representations is especially valuable in medical imaging and remote sensing, highlighting promising avenues for future research. In addition, we have demonstrated that NAFs architecture is versatile and can be adapted across domains, particularly within the denoising context, paving the way to other applications such as in image restoration. (a) Number of Parameters (b) Computational Cost (GFLOPs) (c) Avg. Forward Pass Time (d) Avg. Backward Pass Time (e) Peak Memory (Forward) (f) Peak Memory (Backward) Figure 12. Benchmarking analysis across embedding dimensions. We study 4 different standard embedding sizes: 128, 384, 768 and 1024. (a)-(b) show model complexity, (c)-(d) compare execution time, and (e)-(f) analyze memory consumption. (a) Number of Parameters (b) Computational Cost (GFLOPs) (c) Avg. Forward Pass Time (d) Avg. Backward Pass Time (e) Peak Memory (Forward) (f) Peak Memory (Backward) Figure 13. Benchmarking analysis across upscaling ratio. We studied different upscaling ratio: 2, 4, 8, 16. (a)-(b) show model complexity, (c)-(d) compare execution time, and (e)-(f) analyze memory consumption."
        }
    ],
    "affiliations": [
        "Institut Universitaire de France (IUF)",
        "Sorbonne Universite, CNRS, ISIR, F-75005 Paris, France",
        "Valeo.ai, Paris, France"
    ]
}