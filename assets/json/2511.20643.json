{
    "paper_title": "Concept-Aware Batch Sampling Improves Language-Image Pretraining",
    "authors": [
        "Adhiraj Ghosh",
        "Vishaal Udandarao",
        "Thao Nguyen",
        "Matteo Farina",
        "Mehdi Cherti",
        "Jenia Jitsev",
        "Sewoong Oh",
        "Elisa Ricci",
        "Ludwig Schmidt",
        "Matthias Bethge"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "What data should a vision-language model be trained on? To answer this question, many data curation efforts center on the quality of a dataset. However, most of these existing methods are (i) offline, i.e. they produce a static dataset from a set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use model-based filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, a collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), a simple yet effective batch sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with a broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents a strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks."
        },
        {
            "title": "Start",
            "content": "Concept-Aware Batch Sampling Improves Language-Image Pretraining Adhiraj Ghosh1 Vishaal Udandarao1,2rThao Nguyen3 Matteo Farina1,4 Mehdi Cherti5 Jenia Jitsev5 Sewoong Oh3 Elisa Ricci4 Ludwig Schmidt6 Matthias Bethge1 1Tubingen AI Center, University of Tubingen 4University of Trento 2University of Cambridge 5LAION 6Stanford University 3University of Washington (cid:128) ProjectPage Code ı DataConcept"
        },
        {
            "title": "Abstract",
            "content": "What data should vision-language model be trained on? To answer this question, many data curation efforts center on the quality of dataset. However, most of these existing methods are (i) offline, i.e. they produce static dataset from set of predetermined filtering criteria, and (ii) concept-agnostic, i.e. they use modelbased filters which induce additional data biases. In this work, we go beyond such offline, concept-agnostic methods and advocate for more flexible, task-adaptive online concept-based curation. Our first contribution is DataConcept, collection of 128M web-crawled image-text pairs annotated with fine-grained details about their concept composition. Building on DataConcept, we introduce Concept-Aware Batch Sampling (CABS), simple yet effective batch-sampling framework that flexibly constructs batches on-the-fly based on specific target distributions. We propose two variants: (i) Diversity Maximization (CABS-DM) to curate batches with broad coverage of available concepts, and (ii) Frequency Maximization (CABS-FM) to curate batches with high object multiplicity. Through extensive evaluations across 28 benchmarks, we demonstrate that our CABS method significantly benefits CLIP/SigLIP model classes and yields highly performant models. Overall, CABS represents strong open-source alternative to proprietary online data curation algorithms, enabling practitioners to define custom concept distributions that optimize for specific downstream tasks. 5 2 0 2 5 2 ] . [ 1 3 4 6 0 2 . 1 1 5 2 : r Figure 1: Task-adaptive, steerable, Concept-Aware Batch Sampling (CABS). The per-sample concept multiplicities (left) of MSCOCO retrieval and ImageNet classification train sets depict their divergent distributional properties. By only modifying simple scoring function, CABS can flexibly adapt to different target tasks (details in Sec. 3). Both our classification-optimized (CABS-DM, see Sec. 4) and retrieval-optimized (CABS-FM, see Sec. 5) variants outperform IID sampling by large margins, across several experimental configurations. Equal contribution. Order decided by increasing performance on GSM8K."
        },
        {
            "title": "1 Introduction",
            "content": "Web-scale pretraining datasets underlie the impressive generalization capabilities of vision-language models (VLMs). The advent of CLIP (Radford et al., 2021), trained on 400M image-text pairs, motivated the open development of billion-scale datasets like LAION-5B (Schuhmann et al., 2022) or DataComp-12.8B (Gadre et al., 2023). Although dataset size is an influential factor, their quality is equally important, if not more (Nguyen et al., 2022; Gadre et al., 2023; Goyal et al., 2024). To improve quality, current curation methods range from filtering according to well-defined metrics (e.g., CLIP score) (Gadre et al., 2023) to synthetically augmenting the captions to be more descriptive (Nguyen et al., 2023; Li et al., 2024). However, most of the widely adopted curation strategies (e.g., those benchmarked by DataComp (Gadre et al., 2023)), focus on quality only at the level of individual samples, overlooking the finer, concept-level distribution 1 within web-scale datasets. In other words, existing curation methods tend to be concept-agnostic (MetaCLIP (Xu et al., 2024) is notable exception). Additionally, these methods operate in an offline manner, filtering out large portions of data, thus enforcing fixed design choice: once data is discarded, it is difficult, if not impossible to repurpose the resulting subset for other curation strategies. The offline filtering regime also accelerates the depletion of available training samples, creating data scarcity that ultimately imposes data wall on pretraining (Nguyen et al., 2025). Finally, concept-agnostic filtering methods often rely on state-of-the-art, but black-box, models to guide curation. This not only reduces transparency in selection criteria but also risks propagating the models biases into the curated dataset (Hong et al., 2024; Girrbach et al., 2025). In contrast, concept-aware curation provides transparency and direct control over the composition of the final dataset. In this work, we depart from such offline sample-level curation protocols, and instead advocate for more flexible online concept-based curation. Our rationale is simple: there is no universal notion of quality (Gururangan et al., 2022; Longpre et al., 2024), and importantly, as shown in Fig. 1 (left), different downstream evaluations might bias what the optimal concept distribution should look like (Mizrahi et al., 2025; Abbas et al., 2024b). Therefore, we aim to show that incorporating concept-level information during pretraining, without discarding any data priori, provides complementary and effective avenue for multimodal data curation. This aligns with recent works advocating for data reuse over filtering (Nguyen et al., 2024; Pouget et al., 2024). To achieve this goal, we introduce DataConcept: multimodal pretraining dataset with 128M image-text pairs fully annotated with grounded concept information. In DataConcept, each sample comes with ① semantic concepts, ② bounding boxes, ③ per-concept confidence scores, and ④ concept-driven synthetic captions. With DataConcept, we ask: How can we effectively modulate different visual concepts during vision-language pretraining? In order to answer this question, we introduce new training framework: Concept-Aware Batch-Sampling (CABS). In contrast to offline, static curation, we do not impose fixed, predetermined data distribution, but rather enable flexible, task-adaptive control over online concept-based batch creation. Our classification-optimized variant, CABS-Diversity Maximization (CABS-DM), selects samples based on concept-diversity. This scheme is in line with MetaCLIPs approach and significantly benefits zero-shot classification (see Fig. 1 (middle)), especially over long-tailed evaluations. Our second variant, specifically tailored to benefit image-text retrieval tasks (see Fig. 1 (right)), is CABS-Frequency Maximization (CABS-FM). It optimizes for concept-multiplicityselecting samples that encompass higher number of objects. To our best knowledge, these CABS-variants represent the first reproducible demonstration of task-adaptive online batch sampling. Taken together, our contributions are: 1. DataConcept: new, concept-centric pretraining dataset for VLMs comprising 128M samples. Each sample comes with fine-grained concept annotations and concept-grounded synthetic caption. This helps enable further exploration of concept-centric data curation, relatively underexplored avenue. 2. CABS: new framework for vision-language pretraining that involves online data curation through conceptaware batch sampling. Paired with DataConcept, CABS enables flexible control over the concept distribution of the data used throughout training. 3. Extensive experiments with 28 tasks, 4 visual backbones, and 2 training objectives (CLIP vs SigLIP), demonstrate that CABS variants are highly effective for vision-language pretraining (up to 7% gain on ImageNet zero-shot classification and up to 9.1% gain on image-text retrieval, over strong baselines), while being complementary to existing offline data curation recipes. 1We adopt the definition of concepts from (Udandarao et al., 2024), i.e. objects that can be found in the wild, that we can identify and locate in image samples. 2 Figure 2: DataConcept. We start with images from DataComp (Gadre et al., 2023) and build concept bank by merging, deduplicating, and filtering various concept sources. In ① First-order tagging, we assign preliminary list of concepts (from V) to each sample. ② We then ground each concept in the image, removing noise in the initial candidates. ③ Lastly, we use model to transform alt-texts into concept-aware captions."
        },
        {
            "title": "2 Concept-Aware Dataset Augmentation",
            "content": "We introduce DataConcept, our large-scale, concept-annotated pool of 128M image-text pairs. We will demonstrate the utility of our annotations by describing how they fit into CABS framework in the next Sec. 3. Initial pool. We start with DataComps unfiltered medium pool consisting of 128M image-text pairs (Gadre et al., 2023). We denote each sample as (Ii, Ti). The standard protocol for downloading the dataset suffers from significant link-rot.2 Hence, we opt for randomly sampling 128M subset from Datacomps XLarge pool (which consists of 12.8B samples). Building concept bank. The first step for annotating our pool is determining concept bank, i.e., the set of concepts that we seek to detect and tag. Previous work (Udandarao et al., 2024) curated concept bank but it is rather limited (4, 029) due to being constructed from 27 evaluation datasets. For broader coverage, we further source concepts from the class labels used in RAM++ (Huang et al., 2025), V3Det (Wang et al., 2023), and OpenImages (Kuznetsova et al., 2020), resulting in 19, 261 concepts, after de-duplication and safety removal (specific details and methods are provided in Sec. A.1). Concept tagging. Equipped with an expansive concept bank, following Udandarao et al. (2024), we employ the RAM++ model to provide multiple concept tags for each sample in our data pool (more details in Sec. A.2). Concept grounding. While RAM++ annotations provide fine-grained concept annotations per sample, we find that (i) RAM++ can be miscalibrated in its confidence predictions due to the extreme diversity of our concept bank, and (ii) RAM++ only provides list of concept tags, without localising them in the image, which can lead to incorrect grounding. Thus, we use GroundingDINO (Liu et al., 2024) to additionally provide concept-specific bounding boxes (see Sec. A.3). To enable precise localization of concepts, we propose two methods: (i) Confidence seeding: we feed RAM++ concept tags per sample (only those with at least 0.75 confidence) as seed prompts to GroundingDINO, and (ii) Resolution ensembling: we use Weighted Box Fusion (Solovyev et al., 2021) (see Secs. A.4 and A.5) to ensemble GroundingDINO predictions over multiple image resolutions of {384, 512, 800, 1000}. This helps reduce hallucinations without significantly affecting latency. With the two aforementioned steps, we obtain list of concepts and their corresponding bounding boxes and confidence scores for each sample. Across all samples in the pool, we end up with 12, 253 concepts, i.e. V, the final concept vocabulary for CABS. Each sample is now tagged with concept set Ci. Concept-aware recaptioning. Lastly, we augment each sample with concept-aware synthetic caption. Synthetic re-captions have been shown to improve training data quality by reducing noise in alt-texts (Nguyen et al., 2We successfully downloaded only 79% of the medium-scale pool, as of September 2024. 3 Algorithm 1 PyTorch-style code for CABS # D=(I,T,C)=super-batch(images,texts,concepts) # f=filter-ratio # h=concept-aware heuristic gain function # theta=parameter for heuristic gain function def cabs(D, f, h): I, T, = # unpack super-batch = I.size(0) = (1-f)*B # Step1: compute heuristic scores scores = [] for in range(B): s_i = h(C[i], D, theta) # scoring scores.append(s_i) # Step2: select top-k samples by score selected_indices = topk(scores, k=b) # Step3: construct target batch I_target = I[selected_indices] T_target = T[selected_indices] return (I_target, T_target) 2023; Faghri et al., 2025; Fan et al., 2023). We use Qwen2-VL-7B (Wang et al., 2024) to recaption each image in concept-aware manner : for each sample i, we provide the list of detected concepts Ci and the original alt-text caption Ti in the prompt to the model for recaptioning. The resulting generated caption is denoted as Ri. More insights into recaption distribution and sample visualizations can be found in Sec. DataConcept. Our multi-stage pipeline, fully summarised in Fig. 2, yields our final dataset. Each image-text sample in our dataset consists of concept metadata, including concept tags with confidence scores, localised bounding-boxes, and concept-aware synthetic captions. For ease of notation, we denote each sample as (Ii, Ti, Ri, Ci)."
        },
        {
            "title": "3 Concept-Aware Batch Sampling",
            "content": "Having described DataConcept, we now discuss how to leverage its concept-centric annotations to improve language-image pretraining in task-adaptive manner."
        },
        {
            "title": "3.1 Formulation",
            "content": "We formalize CABS as parameterized sampling framework. Given superbatch of size drawn IID from the data-pool, we define target batch size < controlled by filter ratio [0, 1), such that = (1 )B. For each sample with concept annotations Ci, CABS computes score si = h(Ci; B, θh), where h(.) is concept-aware heuristic gain function parameterized by θh(a set of parameters relevant to the sampling strategy), and selects sub-batch BsubB of size based on these scores. The target sub-batch is constructed as Bsub=TopKiB(si, k=b). For example, if the target is IID sampling, h(i) would be set to 1 for all samples in and θh=. Sampling the top-k in this way would be equivalent to IID sampling. By allowing h(.) and θh to be flexible, practitioners can flexibly instantiate different batch sampling strategies and induce different concept distributions in Bsub on-the-fly during training. This flexibility is powerful as it enables task-adaptive batch curation. We provide PyTorch-style pseudocode for CABS in Alg. 1."
        },
        {
            "title": "3.2 Task-Adaptivity of CABS",
            "content": "We now demonstrate two specific cases, zero-shot classification and image-text retrieval, where the flexibility of our CABS framework enables modifying the concept distributions to be task-aware. Prior work (Abbas et al., 2024b) argues that classification and retrieval benefit from distinct data curation strategies. However, they only perform offline curation and do not disclose details of their methods. This motivates us to develop concrete instantiations of CABS that can accommodate different capabilities tested by classification and retrieval benchmarks. 4 Table 1: Parameters of CABS variants. We indicate the heuristic function h() and parameters θh used for CABS-DM (see Sec. 4) and CABS-FM (see Sec. 5), and if the score is dependent on current state of sub-batch. Zero-shot classification assesses whether model has learned discriminative features for different classes. Under IID sampling and concept-imbalanced training batches, common concepts are over-represented, resulting in under-optimization for rare concepts, and consequently, poor long-tailed performance (He and Garcia, 2009; Zhao and Zhang, 2014). By constructing batches with more uniform distributions, model would learn stronger representations for rare concepts, yielding improved generalization on long-tailed classification. In contrast, retrieval benchmarks test multi-object compositional understanding, requiring models to align rich textual descriptions to complex visual scenes (images with multiple concepts). By constructing batches enriched with similarly complex samples, each encompassing multiple concepts, models would generalize better to the compositional nature of retrieval. Given this, we develop two CABS algorithms (Tab. 1): Dependent? CABS-DM Eq. (1) CABS-FM"
        },
        {
            "title": "Method\nIID",
            "content": "h(.) 1 θh tc Ci Diversity Maximization: balance the concept distribution, focusing on uniform concept coverage (Sec. 4). Frequency Maximization: prioritize samples with the highest concept counts (Sec. 5). Empirical Justification. To validate that classification and retrieval tasks exhibit substantially different concept distributions, we collect 4,096 random samples from MSCOCO (retrieval) and ImageNet (classification) and visualize their per-sample concept counts, following the same protocol used to construct DataConcept, by generating sample-level tags from RAM++ and then prompting GroundingDINO with them to obtain the final annotations. From Fig. 1 (left), we observe that ImageNet images tend to contain single objects, while MSCOCO naturally exhibits multi-object scenes. These characteristics are then approximated by the samples selected by our two CABS variants, further demonstrating the power and flexibility of task-adaptive batch curation. This also highlights the potential of analyzing salient task characteristics and shaping training distributions accordingly, as shown in Mizrahi et al. (2025)."
        },
        {
            "title": "3.3 Experimental Setup",
            "content": "Models. We train ViT-B-32 (Dosovitskiy et al., 2020) CLIP using 224 image-resolution and ViT-B-16 SigLIP (Zhai et al., 2023) at 256 resolution. We further test CABS by training ViT-S-16 CLIP and ViT-SO400M-14 SigLIP (Alabdulmohsin et al., 2023) models in Sec. D.2. Data. We experiment with two variants of DataConcept: one using noisy alt-texts (Ti) and another with our concept-aware re-captions (Ri). Note that IID sampling with alt-text captions corresponds to DataComps default setup (Gadre et al., 2023). Evaluation Benchmarks. Following Udandarao et al. (2025), we consider diverse pool of 25 classification and 2 image-text retrieval benchmarks, spanning fine-grained, object-centric, and scene-centric categories. Additionally, to assess the effectiveness of our models in long-tailed settings, we evaluate on the Let-It-Wag! test set from (Udandarao et al., 2024). Training. We fix the training budget to be 128M samples seen; additional findings for higher budgets (1.28B samples seen) are described in Sec. 6. Note that we closely follow the hyperparameters set by DataComp for fair comparison, including batch-size of 4096. The sample-level concepts Ci are used only for batch curation and do not contribute to the contrastive objective. Unless specified, we set the filter ratio to =0.8, sampling from superbatches of size B=20, 480. We show performance for other filter ratios in Sec. F. Baselines. We compare CABS performance with two popular online batch sampling methods, GRIT-VLP (Byun et al., 2022) and MAFA (Byun et al., 2024). Both GRIT-VLP and MAFA sample hard negatives based on embedding similarity. The key difference lies in how these similarities are computed: GRIT uses the current models embeddings, while MAFA relies on those from pretrained model. MAFA used BLIP for this purpose, but its smaller training budget makes comparisons unfair. To ensure parity, we instead pretrain CLIP and SigLIP on 128M samples and use their embeddings to compute MAFA similarities. Additionally, we note that, although JEST (Evans et al., 2024a) and ACID (Udandarao et al., 2025) are also relevant baselines, they are proprietary algorithms with no public implementation."
        },
        {
            "title": "4.1 Formulation",
            "content": "As motivated in Sec. 3.1, zero-shot classification tasks benefit from balanced concept-level supervision across batches. Given the general formulation detailed previously, we instantiate CABS with diversity maximization (CABS-DM) and its corresponding heuristic function hDM , which scores samples iteratively such that the top-k-filtered batch approximates uniform concept distribution. For superbatch B, CABS-DM assigns higher scores to samples containing under-represented concepts in Bsub and selects the top = (1 )B samples until the frequency of each concept reaches an upper bound tc, tunable hyperparameter. CABS-DM constructs sub-batch by iteratively selecting samples that maximize gain function hDM (i) and updating the sub-batch concept count nc (how many times concept has been selected) for all Ci. This process continues until the desired batch size for training is obtained, which is vastly different from an IID-sampled batch, as illustrated in Fig. 3. An average CABS-DM sub-batch contains 1.5 more concepts than an IID-sampled batch, in addition to exhibiting mostly flat concept distribution. This helps increase diversity at the batch level. CABS-DM includes the following components: Figure 3: Sub-batch compositions. CABS-DM induces near-uniform concept frequency distribution, de-biasing the distributional skew induced by IID-sampling. Unique indicates total unique concepts in the sub-batch: CABSDM incorporates nearly double the concepts in the curated sub-batch, compared to IID. Pooling Concepts and Target Count. For each B, we first compute the global frequency Fc of each concept. We next fix the target count tc for concept c, i.e. the maximum number of times should appear in the sub-batch, to enforce approximate uniformity (following our prior notation, θh consists of tc in this case). In simplified setting, if each sample comprises 1 concept, (cid:80) tc = (1 )B. Gain Function. The sample-level gain function is based on the current state of the sub-batchs concept distribution. Given sample with concept set Ci, we define the gain as hDM(i) = 1 Ci tc nc tc +"
        },
        {
            "title": "1\nFc",
            "content": ", (cid:88) cCi 0, if nc < tc, if nc tc. (1) Each concept contributes two components: balance gain ((tcnc)/tc) that prioritises under-represented concepts and rarity bonus (1/Fc) that upweights long-tailed concepts. At each step, we sort all remaining super-batch samples by this score, deterministically select sample = arg maxi hDM (i), append to the sub-batch, and update nc nc + 1 for all Ci . If concept exceeds tc, all remaining samples containing are rendered invalid. Scores si are then generated for all relevant samples remaining in using hDM and the sample with the highest score is incorporated into the sub-batch for the next iteration. Sample Selection. CABS-DM proceeds through sequence of greedy maximizations to yield balanced and diverse sub-batch. At every iteration, it deterministically selects the sample with the highest gain, conditioned on the current sub-batch composition, without randomness, akin to an Expectation-Maximization alternating optimization between sample selection and score update. Benefits of hDM include (i) reproducibility across runs for the same super-batch due to deterministic selection, and (ii) gain terms jointly enforce uniform concept coverage and higher batch diversity. We provide PyTorch-style pseudocode in Alg. 2. 6 Table 2: CABS-DM improves over IID. Our method substantially outperforms IID sampling, across settings. Importantly, gains from CABS-DM extend to the long-tailed Let-It-Wag! test set too. Method Caption Zero-shot Classification Let-it-Wag! Avg (Clf ) IID (Gadre et al., 2023) CABS-DM IID (Gadre et al., 2023) CABS-DM IID (Gadre et al., 2023) CABS-DM IID (Gadre et al., 2023) CABS-DM alt alt recap recap alt alt recap recap IN-Val IN-shift Obj Scene ViT-B-32-CLIP 17.3 21.9 21. 26.7 15.2 18.6 20.8 25.4 32. 34.5 36.4 39.6 ViT-B-16-SigLIP-256 17.2 24. 28.8 34.7 15.3 20.8 27.4 32. 29.6 33.5 41.5 43.2 36.4 38. 43.1 42.8 35.9 39.6 48.9 50. 5.1 7.5 5.9 7.1 5.2 7. 6.6 7.6 28.2 30.7 33.0 35. 26.4 30.9 38.6 41.1 Table 3: CABS-DM beats MetaCLIP-style curation. Despite having similar curation objectives, we show our online concept-balanced batch sampling significantly outperforms offline curation. Method Zero-shot Classification Let-it-Wag! Avg (Clf ) IN-Val IN-shift Obj Scene IID (Gadre et al., 2023) MetaCLIP (Xu et al., 2024) CABS-DM IID (Gadre et al., 2023) MetaCLIP (Xu et al., 2024) CABS-DM 17. 18.2 21.9 17.2 20.3 24.1 ViT-B-32-CLIP 15.2 16.9 18.6 32.3 30.3 34. ViT-B-16-SigLIP-256 15.3 18.9 20.8 29.6 30. 33.5 36.4 32.9 38.0 35.9 35. 39.6 5.1 5.3 7.5 5.2 5. 7.0 28.2 26.9 30.7 26.4 28. 30."
        },
        {
            "title": "4.2 Improvements on Zero-shot Classification",
            "content": "We now comprehensively evaluate the effectiveness of CABS-DM against standard IID sampling for multimodal pretraining. As shown in Tab. 2, CABS-DM consistently delivers improvements across four different test settings. On ImageNet, CABS-DM yields substantial gains over IID sampling, with an absolute improvement of +5.0% for CLIP ViT-B-32 and +6.9% for SigLIP B-16-256. Similar trends are observed across the broader suite of benchmarks and model variants (refer to Sec. D.2), where CABS-DM boosts average accuracy. Beyond standard benchmarks, CABS-DM also enhances long-tailed recognition on Let-It-Wag! (Udandarao et al., 2024), with boosts of 1.0 2.4%. This demonstrates CABS-DMs ability to improve both general and long-tailed performance. Notably, we also observe consistent improvements from using our concept-aware re-captions compared to alt-texts, even with standard IID sampling. With CLIP-ViT-B/32, our re-captions lead to +4.3% boost on ImageNet, and +4.8% for zero-shot classification. For SigLIP-ViT-B/16, the accuracy gains are as large as +11.6% and +12.2%. These results quantify the benefits of both DataConcept and CABS, showcasing that concept-aware recaptions and task-aware online curation provide the strongest gains."
        },
        {
            "title": "4.3 Improvements over State-of-the-art Methods",
            "content": "MetaCLIP. We compare CABS-DM with MetaCLIP (Xu et al., 2024), an offline curation method that aims at concept-balanced curation by first collecting 500,000 queries from WordNet synsets and Wikipedia titles, followed by matching these queries to pool of imagetext pairs via substring search in alt-texts, capping each query at 20,000 samples. To provide fair baseline, we re-implement MetaCLIP curation based on image content, using our concept 7 Table 4: CABS-DM outperforms SOTA open-source batch sampling methods. With both CLIP-ViT-B/32 and SigLIP-ViT-B/16, CABS-DM provides significant benefits to LIP compared to GRIT-VLP and MAFA, making it more suitable for modern LIP. Method Zero-shot Classification Let-it-Wag! Avg (Clf ) IID (Gadre et al., 2023) GRIT-VLP (Byun et al., 2022) MAFA (Byun et al., 2024) CABS-DM IID (Gadre et al., 2023) GRIT-VLP (Byun et al., 2022) MAFA (Byun et al., 2024) CABS-DM IN-Val IN-shift Obj Scene ViT-B-32-CLIP 15.2 15.0 15.0 18.6 32.3 31. 32.2 34.5 ViT-B-16-SigLIP-256 15.3 15.1 15. 20.8 29.6 30.7 30.7 33.5 17. 17.6 17.0 21.9 17.2 17.3 17. 24.1 36.4 35.6 35.9 38.0 35. 37.3 36.2 39.6 5.1 6.3 5. 7.5 5.2 5.0 5.1 7.0 28. 27.5 27.9 30.7 26.4 27.2 27. 30.9 vocabulary as the query pool and approximating the concept threshold based on the desired curated dataset size. To align with CABS-DM at = 0.8 (where the full dataset is repeated 5 to match our 128M samples-seen regime), we construct 25.6M MetaCLIP-subset and train with 5 repeats. This is achieved by using per-concept threshold of 70,000. Tab. 3 shows comparisons with CABS-DM, MetaCLIP and IID sampling. CABS-DM substantially outperforms MetaCLIP in zero-shot classification (+3.8%/ +2.9% gains on ImageNet and the average classification set respectively) as well as long-tailed evaluations, highlighting the performance boosts achieved with online batch curation. Online Batch Sampling. After demonstrating benefits of online sampling compared to offline curation, we next compare CABS-DM to other online approaches such as GRIT-VLP (Byun et al., 2022) and MAFA (Byun et al., 2024). Tab. 4 highlights that both methods lag behind CABS-DM. We note GRIT and MAFA also struggle to outperform the IID baseline (with CLIP), but offer modest improvements with SigLIP. These observations are in line with recent works suggesting that SigLIP models benefit more from active batch sampling (Evans et al., 2024a; Udandarao et al., 2025). With SigLIP-ViT-B/16, CABS-DM improvements are up to +6.8% on ImageNet and +3.7% on average."
        },
        {
            "title": "5.1 Formulation",
            "content": "As described previously in Sec. 3.1, we next focus on retrieval. Retrieval benchmarks like MSCOCO and Flickr30k often consist of images with multiple objects and complex scenes  (Fig. 1)  , necessitating changes to the design of scoring function compared to CABS-DM. This leads us to instantiate CABS with frequency maximization (CABS-FM) and its corresponding heuristic function hF , which scores samples based on concept count. As result, filtered sub-batches contain samples from super-batch with maximal object multiplicity, exhibiting higher scene complexity overall. Gain Function. We define simple sample-level gain function hF (i) = Ci, which denotes the number of annotated classes present in sample in DataConcept. CABS-FM scores every by hF (i), sorts samples by this value, constructs top-k sub-batch Bsub = TopKiB(Ci, = b) (PyTorch-style pseudocode can be found in Alg. 3). hF thus provides the model with the most concept-dense sub-batch."
        },
        {
            "title": "5.2 Experiment Results",
            "content": "Improvements on Image-Text Retrieval. Following our previous CABS-DM evaluation protocol, we test CABS-FM across the full model suite using alt-text and concept-aware re-captions. As shown in Tab. 5a, CABS-FM 8 Table 5: CABS-FM improves over IID and outperforms state-of-the-art online batch sampling methods. Performance on both Flickr and MSCOCO significantly improved, demonstrating that concept multiplicity curation indeed benefits retrieval. We show significant benefits in using CABS-FM compared to other online batch sampling methods such as GRIT-VLP (Byun et al., 2022) and MAFA (Byun et al., 2024). Method Captions COCO Flickr Avg(Ret) Method COCO Flickr Avg(Ret) IID CABS-FM IID CABS-FM IID CABS-FM IID CABS-FM ViT-B-32-CLIP alt alt recap recap 9.7 11.0 24.0 30.4 ViT-B-16-SigLIP-256 alt alt recap recap 11.1 12.3 37.1 39.7 16.2 21.9 41.3 52.9 18.9 23.9 57.0 63. 12.9 16.4 32.6 41.6 15.0 18.1 47.0 51.6 IID GRIT-VLP MAFA CABS-FM IID GRIT-VLP MAFA CABS-FM ViT-B-32-CLIP 9.7 9.6 9.6 11.0 16.2 15.6 15.5 21.9 ViT-B-16-SigLIP-256 11.1 11.6 10.5 12.3 18.9 19.6 19.4 23.9 12.9 12.6 12.5 16. 15.0 15.6 14.9 18.1 (a) Comparison with IID sampling. (b) Comparison with SoTA batch sampling algorithms. consistently outperforms IID sampling across all configurations, yielding gains of +3.5% and +3.1% for ViT-B-32CLIP and ViT-B-16-SigLIP-256 (alt-text), averaged over MSCOCO and Flickr30k. These improvements further widen to +9.0% and +4.6% when training on the re-captions. Online Batch Sampling Methods. In Tab. 5b, we find that CABS-FM outperforms GRIT-VLP and MAFA. Similar to the classification case, both baselines fail to surpass IID sampling for ViT-B-32-CLIP and offer only modest improvements for ViT-B-16-SigLIP-256. In contrast, CABS-FM offers large boosts, improving over GRIT-VLP by +3.9% (ViT-B-32-CLIP) and +2.5% (ViT-B-16-SigLIP-256)."
        },
        {
            "title": "6 Data- & Compute-Constrained Experiments",
            "content": "Having explored the efficacy of our CABS variants across both classification and image-text retrieval tasks, in this section, we study the benefits of CABS along another axis: datavs compute-constrained pretraining. Definition. Let denote the target compute (FLOPs), the pretraining dataset, and CD the required compute for one epoch over D. If CCD, then training is compute-constrained, i.e., the compute budget is insufficient to consume all the data. If C>CD, then training is data-constrained, i.e., samples must be repeated. Experimental Design. Due to the sampling mechanism of CABS, going from larger superbatch to training sub-batch, all the experiments in Secs. 4.2, 4.3 and 5.2 operate under data-constrained setting for both CABS variants. This occurs since fraction =0.8 of samples are filtered out online during training, making the effective samples-seen-per-epoch for CABS 5 less than IID, which instead operates with C=CD. Table 6: CABS-FM is also compatible with CLIPScore filtering. Despite the same repeat protocol as CABS-DM, we show unanimous performance gains across all benchmarks and models tested."
        },
        {
            "title": "MSCOCO",
            "content": "Flickr30k Avg(Ret) IID CABS-FM Following common practices in pretraining, we increase the constraints further with two experiments: ① less data, but higher quality, where we keep the 128M sample budget, but filter DataConcept via CLIP Score (Schuhmann et al., 2022; Hessel et al., 2021). We keep the top 30% samples as in Gadre et al. (2023), thereby reducing the starting dataset to 38M samples3. To prevent high repeat rates, we set =0.5, yielding 6.67 worst-case repeats for CABS, which are comparable to the 5 worst-case repeats induced by =0.8 in Sec. 4. Note that IID sampling yields 3.33 worst-case repeats after CLIP-Score filtering. ② long training, where we do not filter, but rather increase the training budget to 1.28B samples seen, matching the large scale of DataComp. This ViT-B-16-SigLIP-256 34.7 36.3 IID CABS-FM 18.9 21.2 24.1 26.5 18.7 20.1 26.7 28.2 ViT-B-32-CLIP 13.8 15.9 3We use OpenAIs CLIP ViT-L/14 model for scoring cosine similarities. 9 Table 7: CABS-DM is compatible with CLIPScore filtering. Although CABS-DM leads to more repeats, which yield diminishing returns on already curated data (Goyal et al., 2024), we generally improve over IID even with 2 more repeats across model architectures. Method Zero-shot Classification Let-it-Wag! Avg (Clf ) IID (Gadre et al., 2023) CABS-DM IID (Gadre et al., 2023) CABS-DM IN-Val IN-shift Obj Scene 27.3 30.1 34.7 37.5 ViT-B-32-CLIP 23. 25.6 39.8 41.8 ViT-B-16-SigLIP-256 29.5 32. 46.2 46.2 43.1 44.8 48.9 48. 10.7 12.7 11.9 12.6 35.7 37. 42.0 42.7 training regime corresponds to 10 repeats for IID training and 50 worst-case repeats for CABS, given filter ratio of = 0.8. Less data, but higher quality. In this regime, both CABS variants remain effective even with CLIP-score-filtered data (see Tab. 7 for CABS-DM and Tab. 6 for CABS-FM). Notably, while repeating curated data has been shown to yield diminishing returns (Goyal et al., 2024), CABS still trumps IID sampling despite using 2 more data repeat rate. Long Training. Next, we study the training dynamics under the regime where we train both IID and CABS variants with CLIP ViT-B/32 backbone for 1.28B samples seen. As illustrated in Fig. 4, we find that as long as IID training is compute-constrained (dashed gray line), CABS significantly outperforms the vanilla IID recipe, displaying impressive 3.2 and 2 compute multipliers, which means that IID training requires 3.2 more training steps to reach CABS-DMs ImageNet performance and 2 more training steps to reach CABS-FMs average retrieval performance. The performance gains only slightly diminish when training is far into the data-constrained regime, with CABS yielding worst-case of 50 repeats (over 25.6M samples) and IID yielding only 10. We hypothesize this is due to the combination of large number of repeats (50) over comparatively small sample pool: the original CLIP model (Radford et al., 2021), in comparison, used 32 repeats over 400M samples. However, the overall performance is still competitive to the IID baseline, even under this extreme repeat regime. These experiments confirm that our CABS method is fully compatible with ① smaller, highly curated datasets, and ② pre-training on web-crawled corpora for multiple epochs."
        },
        {
            "title": "7 Related Work",
            "content": "Sampling Approaches for Training Multimodal Models. Training web-scale foundation models typically uses uniform, IID mini-batch sampling, which assigns equal weights to each sample in the training set. However, in multimodal corpora, examples differ drastically in quality (Gadre et al., 2023; Schuhmann et al., 2022; Xu et al., 2023), are possibly redundant (Abbas et al., 2023; Elazar et al., 2023; Abbas et al., 2024a; Sorscher et al., 2022; Webster et al., 2023), and exhibit skewed, long-tailed distributions across concepts (Udandarao et al., 2024; Parashar et al., 2024). Moreover, for contrastive objectives like CLIP (Radford et al., 2021), batch composition heavily shapes the learning process. In this context, uniform sampling is not neutral: it can overexpose trivial or spurious correlations and under-represent rare but informative cases. Hence, several recent approaches try to apply better batch sampling schemes to ensure more effective cross-modal learning. Early works like RHO-Loss (Mindermann et al., 2022) and Bad-Students (Evans et al., 2024b) move away from IID sampling, but they select data samples independently without considering the overall batch composition. This issue is then addressed by methods such as GRIT-VLP (Byun et al., 2022), MAFA (Byun et al., 2024), JEST (Evans et al., 2024a), B3 (Thirukovalluru et al., 2025), Falcon (Kim et al., 2025) and ACID (Udandarao et al., 2025). Our paper builds on this line of work by incorporating concept diversity into the training batch construction, an aspect missing from previous methods. Analyzing Concepts in Multimodal Datasets. Understanding the composition of multimodal datasets is important for building better batch sampling methods. Early image-text datasets like CC-3M (Sharma et al., 2018), 10 Figure 4: CABS with longer training (1.28B samples seen). Both CABS-DM and CABS-FM show significant boost over IID for ViT-B-32-CLIP in both compute-constrained and data-constrained regimes, the grey dashed line being the point where compute-constraint shift to data-constraint in an IID sampling regime. CC-12M (Changpinyo et al., 2021) and YFCC-100M (Thomee et al., 2016) partially characterize their inherent concept distributions using metadata from the web sources where images are scraped from. The WebLI (Chen et al., 2022) dataset (used for training models like PaliGemma (Beyer et al., 2024) and SigLIP (Zhai et al., 2023)) was annotated using OCR models to detect objects in images. However, due to the scale of compute required for annotating recent open datasets like LAION-5B (Schuhmann et al., 2022) and DataComp-1B (Gadre et al., 2023), very few works have studied their concept distribution. Udandarao et al. (2024) tag each sample in LAION-400M with its constituent concepts by using pretrained image-tagging model (Huang et al., 2025) and text search. Other works have proposed improving concept coverage in various ways, e.g. considering multilingual data (Nguyen et al., 2024) or recaptioning Yu et al. (2024). Our DataConcept also augments samples with fine-grained concept annotations and is designed specifically to enable explicit control over online, concept-based batch construction."
        },
        {
            "title": "8 Conclusion",
            "content": "We investigate the role of incorporating concept-level information during vision-language pretraining, which is relatively underexplored by prior data-centric work. To this end, we introduce DataConcept, large-scale, fully annotated pretraining dataset designed to expose concept-level annotations, and CABS, flexible framework leveraging this information to perform online, concept-aware batch sampling during pretraining. Our extensive evaluations demonstrate the benefits of CABS over IID and other curation strategies (including existing batch sampling algorithms) across both classification and retrieval tasks, highlighting its versatility. By making DataConcept and CABS publicly available, we hope to motivate future work to incorporate concept-awareness into their data pipelines for building better VLMs. Limitations. One disadvantage of CABS is the cost of concept annotations. However, this cost is amortizable as the annotated data can be re-used for training different models to do well on different tasks. It is also worth noting that the runtime of CABS increases as we increase the filtering ratio from the superbatch. Our experiments show that CABS can still offer performance benefits at low filtering ratios, where the runtime overhead is more manageable. Besides, we have not experimented with more complex multimodal architectures or large-scale training runs that mirror current state-of-the-art training setups. Future Work. Our proposed framework motivates several directions to study concept-centric data curation further. One avenue could be applying CABS to fine-tuning data. In addition, future work could look into other score functions that will work well with wide range of tasks, balancing both retrieval and classification performance. This balance could potentially be achieved through curriculum learning as well. In our experiments, we pick score function at the start and apply it to all samples across all superbatches. One could study how to best update the score function throughout the course of training, e.g. by first prioritizing single-object images and then moving on to selecting complex scenes."
        },
        {
            "title": "Acknowledgements",
            "content": "The authors thank (in alphabetical order): Sebastian Dziadzio, Andreas Hochlehnert, Benno Krojer, Hilde Kuehne, Ameya Prabhu, Thaddaus Wiedemer, Konstantin Wilkin, Jiajun Zhang, for helpful feedback at different stages of this project. AG gratefully acknowledges LAION and the Gauss Centre for Supercomputing e.V. for funding this work by providing computing time on the JUWELS Booster at Julich Supercomputing Centre (JSC). AG receives funding from the European Unions Horizon Europe research and innovation program under ELLIOT - Grant Agreement No 101214398. AG and VU thank the International Max Planck Research School for Intelligent Systems (IMPRS-IS) for support. VU was supported by Google PhD fellowship in Machine Intelligence. MF acknowledges travel support from ELIAS (GA no 101120237). MB acknowledge financial support by the Federal Ministry of Education and Research (BMBF), FKZ: 011524085B and Open Philanthropy Foundation funded by the Good Ventures Foundation."
        },
        {
            "title": "References",
            "content": "Amro Abbas, Kushal Tirumala, Daniel Simig, Surya Ganguli, and Ari Morcos. Semdedup: Data-efficient learning at web-scale through semantic deduplication. arXiv preprint arXiv:2303.09540, 2023. 10 Amro Abbas, Evgenia Rusak, Kushal Tirumala, Wieland Brendel, Kamalika Chaudhuri, and Ari Morcos. Effective pruning of web-scale datasets based on complexity of concept clusters. arXiv preprint arXiv:2401.04578, 2024a. 10 Amro Abbas, Josh Wills, Haoli Yin, Paul Burstein, Ning Cao, Aldo Carranza, Alvin Deng, Priya Goyal, Pratyush Maini, Joshua McGrath, Fan Pan, Jack Urbanek, Vineeth Kada, Muhammed Razzak, Vishwa Shah, Vishruth Veerendranath, Bogdan Gaza, Ari Morcos, and Matthew Leavitt. DatologyAI Technical Deep-Dive: Image-Text Data Curation at the Billion-Sample Scale. Technical report, DatologyAI, 2024b. 2, 4, 40 Ibrahim Alabdulmohsin, Xiaohua Zhai, Alexander Kolesnikov, and Lucas Beyer. Getting vit in shape: Scaling laws for compute-optimal model design. Advances in Neural Information Processing Systems, 36:1640616425, 2023. 5 Andrei Barbu, David Mayo, Julian Alverio, William Luo, Christopher Wang, Dan Gutfreund, Josh Tenenbaum, and Boris Katz. Objectnet: large-scale bias-controlled dataset for pushing the limits of object recognition models. Advances in neural information processing systems, 32, 2019. 40 Yoshua Bengio, Jerˆome Louradour, Ronan Collobert, and Jason Weston. Curriculum learning. In Proceedings of the 26th annual international conference on machine learning, pages 4148, 2009. 43 Lucas Beyer, Andreas Steiner, Andre Susano Pinto, Alexander Kolesnikov, Xiao Wang, Daniel Salz, Maxim Neumann, Ibrahim Alabdulmohsin, Michael Tschannen, Emanuele Bugliarello, et al. Paligemma: versatile 3b vlm for transfer. arXiv preprint arXiv:2407.07726, 2024. 11 Cody Blakeney, Mansheej Paul, Brett Larsen, Sean Owen, and Jonathan Frankle. Does your data spark joy? performance gains from domain upsampling at the end of training. arXiv preprint arXiv:2406.03476, 2024. 43 Lukas Bossard, Matthieu Guillaumin, and Luc Van Gool. Food-101mining discriminative components with random forests. In European conference on computer vision, pages 446461. Springer, 2014. 40 Jaeseok Byun, Taebaek Hwang, Jianlong Fu, and Taesup Moon. Grit-vlp: Grouped mini-batch sampling for efficient vision and language pre-training. In European Conference on Computer Vision, pages 395412. Springer, 2022. 5, 8, 9, 10 Jaeseok Byun, Dohoon Kim, and Taesup Moon. Mafa: Managing false negatives for vision-language pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2731427324, 2024. 5, 8, 9, 10 Soravit Changpinyo, Piyush Sharma, Nan Ding, and Radu Soricut. Conceptual 12m: Pushing web-scale image-text pretraining to recognize long-tail visual concepts. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 35583568, 2021. 11 Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollar, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. 40 Xi Chen, Xiao Wang, Soravit Changpinyo, Anthony Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, et al. Pali: jointly-scaled multilingual language-image model. arXiv preprint arXiv:2209.06794, 2022. 11 Gong Cheng, Junwei Han, and Xiaoqiang Lu. Remote sensing image scene classification: Benchmark and state of the art. Proceedings of the IEEE, 105(10):18651883, 2017. 40 Gordon Christie, Neil Fendley, James Wilson, and Ryan Mukherjee. Functional map of the world. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 61726180, 2018. 40 Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi. Describing textures in the wild. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 36063613, 2014. Adam Coates, Andrew Ng, and Honglak Lee. An analysis of single-layer networks in unsupervised feature learning. In Proceedings of the fourteenth international conference on artificial intelligence and statistics, pages 215223. JMLR Workshop and Conference Proceedings, 2011. 40 Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, et al. Molmo and pixmo: Open weights and open data for state-of-the-art visionlanguage models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 91104, 2025. 31 Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. 40 Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. 5 Yanai Elazar, Akshita Bhagia, Ian Magnusson, Abhilasha Ravichander, Dustin Schwenk, Alane Suhr, Pete Walsh, Dirk Groeneveld, Luca Soldaini, Sameer Singh, et al. Whats in my big data? arXiv preprint arXiv:2310.20707, 2023. Talfan Evans, Nikhil Parthasarathy, Hamza Merzic, and Olivier Henaff. Data curation via joint example selection further accelerates multimodal learning. Advances in Neural Information Processing Systems, 37:141240141260, 2024a. 5, 8, 10 Talfan Evans, Shreya Pathak, Hamza Merzic, Jonathan Schwarz, Ryutaro Tanno, and Olivier Henaff. Bad students make great teachers: Active learning accelerates large-scale visual understanding. In European Conference on Computer Vision, pages 264280. Springer, 2024b. 10 Mark Everingham. The pascal visual object classes challenge 2007. In http://www. pascal-network. org/challenges/VOC/voc2007/workshop/index. html, 2009. 40 Fartash Faghri, Pavan Kumar Anasosalu Vasu, Cem Koc, Vaishaal Shankar, Alexander Toshev, Oncel Tuzel, and Hadi Pouransari. Mobileclip2: Improving multi-modal reinforced training. arXiv preprint arXiv:2508.20691, 2025. 4 Lijie Fan, Dilip Krishnan, Phillip Isola, Dina Katabi, and Yonglong Tian. Improving clip training with language rewrites. Advances in Neural Information Processing Systems, 36:3554435575, 2023. 4 Li Fei-Fei, Rob Fergus, and Pietro Perona. Learning generative visual models from few training examples: An incremental bayesian approach tested on 101 object categories. In 2004 conference on computer vision and pattern recognition workshop, pages 178178. IEEE, 2004. 40 Steven Feng, Shrimai Prabhumoye, Kezhi Kong, Dan Su, Mostofa Patwary, Mohammad Shoeybi, and Bryan Catanzaro. Maximize your datas potential: Enhancing llm accuracy with two-phase pretraining. arXiv preprint arXiv:2412.15285, 2024. 43 Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, et al. Datacomp: In search of the next generation of multimodal datasets. In Conference on Neural Information Processing Systems (NeurIPS), 2023. 2, 3, 5, 7, 8, 9, 10, 11, 39, William Gaviria Rojas, Sudnya Diamos, Keertan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world. Advances in Neural Information Processing Systems, 35:1297912990, 2022. 40 Adhiraj Ghosh, Sebastian Dziadzio, Ameya Prabhu, Vishaal Udandarao, Samuel Albanie, and Matthias Bethge. Onebench to test them all: Sample-level benchmarking over open-ended capabilities. In Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 3244532481, 2025. 26, 45 13 Leander Girrbach, Stephan Alaniz, Genevieve Smith, Trevor Darrell, and Zeynep Akata. Person-centric annotations of laion-400m: Auditing bias and its transfer to models. arXiv preprint arXiv:2510.03721, 2025. 2 Sachin Goyal, Pratyush Maini, Zachary Lipton, Aditi Raghunathan, and Zico Kolter. Scaling laws for data filteringdata curation cannot be compute agnostic. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2270222711, 2024. 2, Suchin Gururangan, Dallas Card, Sarah Dreier, Emily Gade, Leroy Wang, Zeyu Wang, Luke Zettlemoyer, and Noah Smith. Whose language counts as high quality? measuring language ideologies in text data selection. In Proceedings of the 2022 conference on empirical methods in natural language processing, pages 25622580, 2022. 2 Haibo He and Edwardo Garcia. Learning from imbalanced data. IEEE Transactions on knowledge and data engineering, 21(9):12631284, 2009. 5 Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, et al. The many faces of robustness: critical analysis of out-of-distribution generalization. In Proceedings of the IEEE/CVF international conference on computer vision, pages 83408349, 2021a. 40 Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Steinhardt, and Dawn Song. Natural adversarial examples. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1526215271, 2021b. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 conference on empirical methods in natural language processing, pages 75147528, 2021. 9 Rachel Hong, William Agnew, Tadayoshi Kohno, and Jamie Morgenstern. Whos in and whos out? case study of multimodal clip-filtering in datacomp. In Proceedings of the 4th ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization, pages 117, 2024. 2 Xinyu Huang, Yi-Jie Huang, Youcai Zhang, Weiwei Tian, Rui Feng, Yuejie Zhang, Yanchun Xie, Yaqian Li, and Lei Zhang. Open-set image tagging with multi-grained text supervision. In Proceedings of the 33rd ACM International Conference on Multimedia, pages 41174126, 2025. 3, 11, 19, 20 Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, 2021. If you use this software, please cite it as below. 39 Myunsoo Kim, Seong-Woong Shim, and Byung-Jun Lee. Falcon: False-negative aware learning of contrastive negatives in vision-language pretraining. arXiv preprint arXiv:2505.11192, 2025. Pang Wei Koh, Shiori Sagawa, Henrik Marklund, Sang Michael Xie, Marvin Zhang, Akshay Balsubramani, Weihua Hu, Michihiro Yasunaga, Richard Lanas Phillips, Irena Gao, et al. Wilds: benchmark of in-the-wild distribution shifts. In International conference on machine learning, pages 56375664. PMLR, 2021. 40 Jonathan Krause, Michael Stark, Jia Deng, and Li Fei-Fei. 3d object representations for fine-grained categorization. In Proceedings of the IEEE international conference on computer vision workshops, pages 554561, 2013. 40 Alex Krizhevsky, Geoffrey Hinton, et al. Learning multiple layers of features from tiny images. 2009. 40 Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. 3, 19 Chunyuan Li, Haotian Liu, Liunian Li, Pengchuan Zhang, Jyoti Aneja, Jianwei Yang, Ping Jin, Houdong Hu, Zicheng Liu, Yong Jae Lee, et al. Elevater: benchmark and toolkit for evaluating language-augmented visual models. Advances in Neural Information Processing Systems, 35:92879301, 2022. Xianhang Li, Haoqin Tu, Mude Hui, Zeyu Wang, Bingchen Zhao, Junfei Xiao, Sucheng Ren, Jieru Mei, Qing Liu, Huangjie Zheng, et al. What if we recaption billions of web images with llama-3? arXiv preprint arXiv:2406.08478, 2024. 2, 22 Shilong Liu, Zhaoyang Zeng, Tianhe Ren, Feng Li, Hao Zhang, Jie Yang, Qing Jiang, Chunyuan Li, Jianwei Yang, Hang Su, et al. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European conference on computer vision, pages 3855. Springer, 2024. 3, 22 14 Shayne Longpre, Gregory Yauney, Emily Reif, Katherine Lee, Adam Roberts, Barret Zoph, Denny Zhou, Jason Wei, Kevin Robinson, David Mimno, et al. pretrainers guide to training data: Measuring the effects of data age, domain coverage, quality, & toxicity. In Proceedings of the 2024 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 32453276, 2024. 2 Subhransu Maji, Esa Rahtu, Juho Kannala, Matthew Blaschko, and Andrea Vedaldi. Fine-grained visual classification of aircraft. arXiv preprint arXiv:1306.5151, 2013. Soren Mindermann, Jan Brauner, Muhammed Razzak, Mrinank Sharma, Andreas Kirsch, Winnie Xu, Benedikt Holtgen, Aidan Gomez, Adrien Morisot, Sebastian Farquhar, et al. Prioritized training on points that are learnable, worth learning, and not yet learnt. In International Conference on Machine Learning, pages 1563015649. PMLR, 2022. 10 David Mizrahi, Anders Boesen Lindbo Larsen, Jesse Allardice, Suzie Petryk, Yuri Gorokhov, Jeffrey Li, Alex Fang, Josh Gardner, Tom Gunter, and Afshin Dehghan. Language models improve when pretraining data matches target tasks. arXiv preprint arXiv:2507.12466, 2025. 2, 5 Alexander Neubeck and Luc Van Gool. Efficient non-maximum suppression. In 18th international conference on pattern recognition (ICPR06), pages 850855. IEEE, 2006. 25 Thao Nguyen, Gabriel Ilharco, Mitchell Wortsman, Sewoong Oh, and Ludwig Schmidt. Quality not quantity: On the interaction between dataset design and robustness of clip. Advances in Neural Information Processing Systems, 35: 2145521469, 2022. 2 Thao Nguyen, Samir Yitzhak Gadre, Gabriel Ilharco, Sewoong Oh, and Ludwig Schmidt. Improving multimodal datasets with image captioning. Advances in neural information processing systems, 36:2204722069, 2023. 2, 3, 22, 32 Thao Nguyen, Matthew Wallingford, Sebastin Santy, Wei-Chiu Ma, Sewoong Oh, Ludwig Schmidt, Pang Wei Koh, and Ranjay Krishna. Multilingual diversity improves vision-language representations. Advances in Neural Information Processing Systems, 37:9143091459, 2024. 2, 11 Thao Nguyen, Yang Li, Olga Golovneva, Luke Zettlemoyer, Sewoong Oh, Ludwig Schmidt, and Xian Li. Recycling the web: method to enhance pre-training data quality and quantity for language models. arXiv preprint arXiv:2506.04689, 2025. 2 Maria-Elena Nilsback and Andrew Zisserman. Automated flower classification over large number of classes. In 2008 Sixth Indian conference on computer vision, graphics & image processing, pages 722729. IEEE, 2008. 40 Chengcheng Ning, Huajun Zhou, Yan Song, and Jinhui Tang. Inception single shot multibox detector for object detection. In"
        },
        {
            "title": "2017 IEEE International Conference on Multimedia & Expo Workshops (ICMEW), pages 549–554. IEEE, 2017. 25",
            "content": "Shubham Parashar, Zhiqiu Lin, Tian Liu, Xiangjue Dong, Yanan Li, Deva Ramanan, James Caverlee, and Shu Kong. The neglected tails in vision-language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1298812997, 2024. 10, 28 Omkar Parkhi, Andrea Vedaldi, Andrew Zisserman, and CV Jawahar. Cats and dogs. In 2012 IEEE conference on computer vision and pattern recognition, pages 34983505. IEEE, 2012. 40 Angeline Pouget, Lucas Beyer, Emanuele Bugliarello, Xiao Wang, Andreas Steiner, Xiaohua Zhai, and Ibrahim Alabdulmohsin. No filter: Cultural and socioeconomic diversity in contrastive vision-language models. Advances in Neural Information Processing Systems, 37:106474106496, 2024. 2 Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. 2, 10, 40 Vikram Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron Adcock, Laurens van der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. Geode: geographically diverse evaluation dataset for object recognition. Advances in Neural Information Processing Systems, 36:6612766137, 2023. Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and Vaishaal Shankar. Do imagenet classifiers generalize to imagenet? In International conference on machine learning, pages 53895400. PMLR, 2019. 40 Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, 2019. 19 15 Karsten Roth, Vishaal Udandarao, Sebastian Dziadzio, Ameya Prabhu, Mehdi Cherti, Oriol Vinyals, Olivier Henaff, Samuel Albanie, Matthias Bethge, and Zeynep Akata. practitioners guide to continual multimodal pretraining. arXiv preprint arXiv:2408.14471, 2024. 43 Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. 2, 9, 10, Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of ACL, 2018. 10 Roman Solovyev, Weimin Wang, and Tatiana Gabruseva. Weighted boxes fusion: Ensembling boxes from different object detection models. Image and Vision Computing, 107:104117, 2021. 3, 22, 24, 25 Ben Sorscher, Robert Geirhos, Shashank Shekhar, Surya Ganguli, and Ari Morcos. Beyond neural scaling laws: beating power law scaling via data pruning. Advances in Neural Information Processing Systems, 35:1952319536, 2022. Raghuveer Thirukovalluru, Rui Meng, Ye Liu, Mingyi Su, Ping Nie, Semih Yavuz, Yingbo Zhou, Wenhu Chen, Bhuwan Dhingra, et al. Breaking the batch barrier (b3) of contrastive learning via smart batch mining. arXiv preprint arXiv:2505.11293, 2025. 10 Bart Thomee, David Shamma, Gerald Friedland, Benjamin Elizalde, Karl Ni, Douglas Poland, Damian Borth, and Li-Jia Li. Yfcc100m: The new data in multimedia research. Communications of the ACM, 59(2):6473, 2016. 11, 40 Lukas Tuggener, Raphael Emberger, Adhiraj Ghosh, Pascal Sager, Yvan Putra Satyawan, Javier Montoya, Simon Goldschagg, Florian Seibold, Urs Gut, Philipp Ackermann, et al. Real world music object recognition. Transactions of the International Society for Music Information Retrieval, 7(1):114, 2024. 22 Vishaal Udandarao, Ameya Prabhu, Adhiraj Ghosh, Yash Sharma, Philip Torr, Adel Bibi, Samuel Albanie, and Matthias Bethge. No zero-shot without exponential data: Pretraining concept frequency determines multimodal model performance. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024. 2, 3, 5, 7, 10, 11, 19, 20, 21, 28, 30, 40 Vishaal Udandarao, Nikhil Parthasarathy, Muhammad Ferjad Naeem, Talfan Evans, Samuel Albanie, Federico Tombari, Yongqin Xian, Alessio Tonioni, and Olivier Henaff. Active data curation effectively distills large-scale multimodal models. In Proceedings of the Computer Vision and Pattern Recognition Conference, pages 1442214437, 2025. 5, 8, 10, Haohan Wang, Songwei Ge, Zachary Lipton, and Eric Xing. Learning robust global representations by penalizing local predictive power. Advances in neural information processing systems, 32, 2019. 40 Jiaqi Wang, Pan Zhang, Tao Chu, Yuhang Cao, Yujie Zhou, Tong Wu, Bin Wang, Conghui He, and Dahua Lin. V3det: Vast vocabulary visual detection dataset. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1984419854, 2023. 3, 19 Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. 4, 31, 32 Ryan Webster, Julien Rabin, Loic Simon, and Frederic Jurie. On the de-duplication of laion-2b. arXiv preprint arXiv:2303.12733, 2023. 10 Hao Wu, Jiayuan Mao, Yufeng Zhang, Yuning Jiang, Lei Li, Weiwei Sun, and Wei-Ying Ma. Unified visual-semantic embeddings: Bridging vision and language with structured meaning representations. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 66096618, 2019. 19 Jianxiong Xiao, Krista Ehinger, James Hays, Antonio Torralba, and Aude Oliva. Sun database: Exploring large collection of scene categories. International Journal of Computer Vision, 119(1):322, 2016. 40 Hu Xu, Saining Xie, Po-Yao Huang, Licheng Yu, Russell Howes, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Cit: Curation in training for effective vision-language data. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 1518015189, 2023. 10 Hu Xu, Saining Xie, Xiaoqing Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying clip data. In The Twelfth International Conference on Learning Representations, 2024. 2, 7, 16 Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the association for computational linguistics, 2: 6778, 2014. 40 Qiying Yu, Quan Sun, Xiaosong Zhang, Yufeng Cui, Fan Zhang, Yue Cao, Xinlong Wang, and Jingjing Liu. Capsfusion: Rethinking image-text data at scale. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1402214032, 2024. 11 Xiaohua Zhai, Joan Puigcerver, Alexander Kolesnikov, Pierre Ruyssen, Carlos Riquelme, Mario Lucic, Josip Djolonga, Andre Susano Pinto, Maxim Neumann, Alexey Dosovitskiy, et al. The visual task adaptation benchmark. 2019. 40 Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023. 5, 11 Youcai Zhang, Xinyu Huang, Jinyu Ma, Zhaoyang Li, Zhaochuan Luo, Yanchun Xie, Yuzhuo Qin, Tong Luo, Yaqian Li, Shilong Liu, et al. Recognize anything: strong image tagging model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 17241732, 2024. 19, 20 Yang Zhang, Amr Mohamed, Hadi Abdine, Guokan Shang, and Michalis Vazirgiannis. Beyond random sampling: Efficient language model pretraining via curriculum learning. arXiv preprint arXiv:2506.11300, 2025. 43 Peilin Zhao and Tong Zhang. Accelerating minibatch stochastic gradient descent using stratified sampling. arXiv preprint arXiv:1405.3080, 2014. Xiangyu Zhao, Yicheng Chen, Shilin Xu, Xiangtai Li, Xinjiang Wang, Yining Li, and Haian Huang. An open and comprehensive pipeline for unified object grounding and detection. arXiv preprint arXiv:2401.02361, 2024. 26 17 Concept-Aware Batch Sampling Improves Language-Image Pretraining"
        },
        {
            "title": "Supplementary Material",
            "content": "A DataConcept Curation: Further Details"
        },
        {
            "title": "19\nA.1 Vocabulary Construction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19\nA.2 Object Tagging . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20\nA.3 Object Detection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22\nA.4 Weighted Box Fusion for Ensembling Bounding Boxes\n. . . . . . . . . . . . . . . . . . . . . . . . . . 24\nA.5 Ensembling: Quantitative Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26\nA.6 Concept Distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28",
            "content": "B Concept-aware Recaptioning"
        },
        {
            "title": "31\nB.1 Selecting the Recaptioning VLM . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31\nB.2 Caption Quality . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32\nB.3 Qualitative Evaluation: Visualization Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33",
            "content": "C CABS: More Details"
        },
        {
            "title": "F Ablation on Filter Ratios",
            "content": "43 44 Fine-grained Benchmark Performance"
        },
        {
            "title": "45\nG.1 Expanded Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45\n. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50\nG.2 MetaCLIP: Further Details",
            "content": "18 DataConcept Curation: Further Details A.1 Vocabulary Construction Scaling Concept Vocabulary: We scale up the tag generation pipeline of RAM++ (Recognize Anything) (Zhang et al., 2024; Huang et al., 2025) by incorporating more long-tailed concepts. In the original work, RAM++ extracts the top 4, 585 concepts by parsing 14 million sentences from their pool of pretraining datasets and then extracting tags using SceneGraph Parser (Wu et al., 2019), hence attempting to focus on more common concepts. However, our work focuses more on open-vocabulary recognition and localization, hence we scale up the concept vocabulary to include objects that may be found in-the-wild in image-text pretraining datasets. We include the concepts collected in Udandarao et al. (2024) as well as 200 classes from the rare classes subset of OpenImages (Kuznetsova et al., 2020). Finally, we also adopt and filter the vocabulary pool from V3Det (Wang et al., 2023), state-of-the-art open-vocabulary dataset which observes and encodes the relationship between categories by defining hierarchy tree of concepts. Systematic Concept Curation and Redundancy Resolution: Curating this concept pool comes with redundancies, which need to be systematically resolved. We first establish set of pre-defined heuristics that comprise grounds for removing concepts from the vocabulary. Then we first automate the concept removal process, followed by manual inspection of the collected vocabulary to remove concepts that violate these heuristics. This ensures very thorough curation, which we detail below: 1. Morphological Redundancies. We perform normalization step to remove morphological variants of the same concept (e.g singular and plural forms) into single entity using lemmatization. In practise, we canonicalize noun such that entries like dogs, dog, and dogs are collapsed to the same lemma. Addressing morphological redundancies early in the pipeline reduced spurious multiplicity caused by simple variations. 2. Syntactical Redundancies. We identify spelling/spacing artifacts and remove them if they are duplicated (\" cat\" and the correct \"cat\"). This normalization is deterministic and involves collapsing repeated whitespace, lowercasing capital letters, and replacing underscores with spaces. This step reduces accidental duplicates which were caused by formatting differences, occuring due to the collection of concepts from different sources, as highlighted above. Since the following heuristics involve embedding computations, this step prevents unnecessary computations. 3. Semantic Redundancies. We remove semantic redundancies using WordNet (formalized through synsets) to detect synonyms in addition to semantic embeddings of concepts using pretrained SentenceTransformer model (Reimers and Gurevych, 2019). This phase is conservative, we only want to remove near identical concepts (such as tv and television) rather than loosely related terms. This design choice is particularly important as we deal with lot of concepts that could be considered similar in relaxed definition (such as different editions of car models). WordNet synsets serve as an initial lightweight signal for detecting synonyms and the SentenceTransformer embeddings are used for more robust coverage. We use all-MiniLM-L6-v2 to compute vector embeddings, followed by comparing concept pairs using the cosine similarity and only merging/removing concepts if the similarity is higher than 0.95. This ensures only near-identical concepts are collapsed (for example, British and US English spellings of the same concept) and separate but related concepts (for example hedgehog/porcupine and crayfish/spiny lobster are preserved. 4. Unsafe Concepts. We identify unsafe concepts (e.g. racially motivated concepts like white man and black man) through thorough manual inspection and remove them. Additionally, we build lightweight safety classifier by encoding curated list of race-related and NSFW terms using the SentenceTransformer model from before. concept is flagged as unsafe if the cosine similarity between the concept and the encoded list of unsafe terms exceeds 0.7 for race-related terms and 0.65 for NSFW terms. These thresholds were determined iteratively to prevent false positives (for example black cat). With these steps, we obtain our final concept vocabulary of 19, 261. 19 A.2 Object Tagging Motivation. Previous attempts to annotate pretraining datasets have used object tagging to return list of probable objects in sample, above specified threshold. For example, Udandarao et al. (2024) used RAM++ Zhang et al. (2024); Huang et al. (2025) to annotate visual concepts in many large image-text datasets. However, as discussed in Sec. 2, the expanded vocabulary (from 4, 029 to 19, 261) introduces miscalibrations and overestimations in the model predictions. For example, abiding by the confidence threshold of 0.7 image resolution of (384,384) from Udandarao et al. (2024), we note that RAM++ tends to overestimate classes when the vocabulary is expanded. This arises from the increased semantic similarity among real-world concepts in the visual space, as factor of large vocabulary. An increase in the hierarchy for common and long-tailed classes (there are several sub-species of snakes in the vocabulary as we see in Fig. 5) is to be expected with an increase in the vocabulary of visual concepts, which leads to inherent uncertainty of making predicting for images that induce visual uncertainty. Optimal RAM++ Threshold. One simple solution is to increase the threhsold, which highlights the flexibility of open-set image tagging - the RAM++ model easily adapts to larger vocabulary despite being trained on 4, 000 concepts.As sanity check, we apply RAM++ under three different confidence thresholds: 0.7, 0.75, and 0.8, still processing each image at resolution of (384, 384). We choose this resolution as it is the default chosen by RAM++. This multi-threshold setup allows us to explicitly study how sensitive the predicted tag set is to the choice of threshold, and to quantify the extent to which miscalibration persists even under stricter filtering regimes. Note that the tags generated at threshold of 0.75 is strict subset of 0.7 and tags generated at threshold of 0.8 is strict subset of 0.75 and 0.7. Increasing the confidence threshold to 0.75 still results miscalibrations in some form (see Fig. 5), although some low-confidence noise seems to be removed. It is to be noted that increasing the threshold to 0.8 significantly increased the proportion of samples with no generated tags. Hence, we opt for using 0.75 as our final threshold for object tagging using RAM++. Why Object Detection? Simply generating concept tags can lead to mistakes as highlighted above, especially for images with high levels of visual uncertainty. Tagging lacks spatial grounding and cannot differentiate between multiple instances or object-level relationships. Additionally, concept tags injects only one form of added metadata: other tasks like object detection can add richer and more valuable fine-grained information into these large datasets. Hence, we advocate for the conducting an additional step to annotate image-text pretraining datasets. 20 Figure 5: Qualitative Results with different RAM++ thresholds. While Udandarao et al. (2024) found 0.7 to be the suitable RAM++ threshold, we show qualitative examples across three different thresholds: 0.7, 0.75, 0.8 on much larger concept bank. We find the most suitable pool of concepts at the 0.75 confidence threshold. 21 A.3 Object Detection Benefits of Localized Annotations. Object tagging using RAM++ provides great insights into the object composition of images in image-text datasets. However, relevant factors for the holistic understanding of pretraining data such as the number of instances of the same concept in an image(count) and the localization of these concepts(spatial awareness) are confounded away by simply tagging an image with objects. To mitigate this, we incorporate bounding box information into the pipeline, which resolves both the issues identified. GroundingDINO. Given an image, our model of choice, GroundingDINO (Liu et al., 2024) returns localized concept information, such as bounding boxes, detected concepts, confidence scores of each box, etc. Since, we use detection model grounded in natural language, GroundingDINO can effectively detect objects from an image when provided an input text and each detection is tagged with similarity score across the individual input text tokens. How to provide text for an image is design choice. Since Datacomp is an image-text dataset, one approach could be to provide the caption for the image as the input text. However, the alt-text captions are of low quality and do not always correspond to the visual concepts in the image. This artifact of web-scale image-text datasets have been well-studied and works such as (Nguyen et al., 2023; Li et al., 2024) have proposed methods to improve the text distribution. Another potential input involves providing the entire pool of concepts as the text input. Doing so leads to over-representation of objects being detected which are not visually present in the image, thus leading to some form of hallucination. This is especially true since we have 19, 261 concepts in our pool, significantly increasing the probability of hallucinations and reducing the processing speed of the model. Our Approach. Our solution involves providing RAM++ object tags at 0.75 confidence threshold as prompts to GroundingDINO. By reducing the vocabulary pool, we mitigate hallucinations and errors while also improving the detection models processing speed. Through manual inspection, to remove low-confidence predictions to prevent second degree of over-representation, we set text threshold by only extracting concepts with box-concept similarity score higher than 0.27. We set the same threshold for bounding box confidence scores too. With this configuration, we can now annotate each image of pretraining dataset with the concept tags, per-concept logit scores from RAM++ and the set of bounding boxes, detected classes and their corresponding confidence scores. Ensembling: An Introduction. An additional confounder is that DataComp-128M is available in multiple resolutions. To leverage this and increase the trustworthiness of DataConcept, we apply Weighted Box Fusion (WBF) Solovyev et al. (2021) for bounding box ensembling. WBF generates the final set of bounding boxes by using the confidence scores of the proposed bounding boxes of multiple object detection models/various configurations of the same object detection model. This approach is in contrast to Non-maximum suppression(NMS) which just removes part of the predictions instead of aggregating them. Ensembling has proven to be an effective strategy in complex object detection tasks (Tuggener et al., 2024). Specifically, we ensemble across image resolutions {384, 512, 800, 1000} to obtain more robust final detection results, refer to Fig. 6 for visual inspection. We provide more details in Sec. A.4. Final Annotations. As we have demonstrated, DataConcept has been curated using high confidence thresholds and stricter annotation protocols, with localization requiring bounding boxes to be generated for the precise regions of objects. This added difficulty has led to extremely rare concepts being underrepresented in the annotations. Nevertheless, DataConcept-M contains 12, 253 unique concepts, which we define as C, the concept pool for CABS. Figure 6: Ensembling bounding boxes provides the best detection predictions for DataConcept. Using Weighted Box Fusion, we are able to detect single instances(no overlap of bounding boxes) of all relevant objects in an image. 23 A.4 Weighted Box Fusion for Ensembling Bounding Boxes Weighted Box Fusion(WBF) (Solovyev et al., 2021) is post-processing measure within object detection, generally used when there are multiple bounding boxes predicted by different models, or the same model performed on image with different augmentations. Our approach involves the latter, with one GroundingDINO model producing 4 bounding box predictions for one sample across different image resolutions. While other approaches, such as in traditional Non-Maximum Suppression (NMS), may remove detection with lower score when multiple boxes overlap, WBF forms clusters of overlapping boxes, as long as it belongs to the same class, and produces single box by taking confidence-weighted average of coordinates. This preserves geometric evidence from different resolutions and often yields tighter, better-centered localization. Notation. We start with set of bounding boxes across the image resolutions {384, 512, 800, 1000} and their associated confidence scores = {bi = (x(i) = {si}n 1 , y(i) i=1, 1 , x(i) 2 , y(i) si [0, 1] 2 )}n i= Each box is also assigned class label (concept) predicted by the model at that resolution: = {ci}n i=1, ci V, where is our concept vocabulary (e.g., person, car, flower). We define resolution weights where m(i) is resolution for box i. The fusion weight for each box is defined as αm(i), = 1, . . . , wi = αm(i) si. In our setup, we do not upweight any specific resolution, hence αm(i) is always 1 and we do not use αm(i) in future definitions and formulae. Note that the set of bounding boxes at each resolution are first sorted in decreasing order of confidence scores before the following steps are implemented. Clustering. Bounding boxes need to be grouped into clusters to implement WBF. The heuristic is simple, two bounding boxes belong to the same cluster iff there is significant overlap spatially and the classes of the two boxes are the same."
        },
        {
            "title": "A cluster K associated with a reference box j is defined as",
            "content": "K(j) = { {1, . . . , n} IoU(bi, bj) > T, ci = cj } where is predefined IoU threshold. The IoU threshold is used as the metric for spatial overlap. In our experiments is set to 0.29. IoU Definition. For two boxes and B, the Intersection-over-Union (IoU) is defined as IoU(A, B) = Ensembling. For cluster containing bounding boxes corresponding to the same class, the final coordinates are computed as follows: ˆx1 = ˆx2 = (cid:80) (cid:80) iK wi x(i) 1 (cid:80) iK wi iK wi x(i) 2 (cid:80) iK wi (cid:80) (cid:80) iK wi y(i) 1 (cid:80) iK wi iK wi y(i) 2 (cid:80) iK wi , . , , ˆy1 = ˆy2 ="
        },
        {
            "title": "The fused confidence score for the fused box as the average confidence of all boxes that form the cluster as is",
            "content": "denotes as follows: (cid:80) iK wi si (cid:80) iK wi This is in stark contrast with other bounding box selection methods like NMS (Neubeck and Van Gool, 2006), Non-Maximum Weighted (NMW) method (Ning et al., 2017), etc. NMS completly exclude boxes that have lower IoU than the threshold, while NMW does not change confidence scores. On the other hand, WBF uses all boxes provided and determines the final coordinates by means of confidence scores of the specific prediction. ˆs = Two-stage post-filtering. Following closely the original WBF formulation (Solovyev et al., 2021), the fused confidence scores are rescaled to reflect model agreement: ˆs ˆs min(T, n) or ˆs ˆs"
        },
        {
            "title": "T\nn",
            "content": ", where is the number of boxes in the cluster and is the number or resolutions. This reduces the score of boxes supported by only small subset of resolutions. Essentially, if any of fails to predict bounding box belonging to cluster, we reduce the score of the fused box as opposed to cluster with predictions from all n. After WBF, we apply an optional second-stage filter to remove near-duplicate boxes of the same class. We do this for an added level of rigor to the final annotations. For each class, boxes with IoU above stricter threshold Tpost (e.g., 0.5) are re-clustered, and only the highest-confidence box in each cluster is retained. Summary. We adopt rigorous approach to ensemble bounding boxes across variety of resolutions and in this section we demonstrate why WBF is the most robust method to achieve this. Ensembling results in list of bounding boxes, concepts and confidence scores which have been re-calibrated via weighted averaging (producing smoother, more meaningful scores). We provide all of these annotations in DataConcept. 25 A.5 Ensembling: Quantitative Results Motivation. In this section, we ask: How do we quantify ensemble quality? Since we do not have ground-truth information when dealing with DataComp, we refer to evaluations on benchmarks aligned with our task: obtaining proxy for open-vocabulary object localization and detection. This is aligned with the takeaways from recent benchmarking works such as Ghosh et al. (2025), which proposes granular evaluations into semantically related domains to determine the quality of machine learning models. With this motivation, we test our ensembling approach using ODinW (Li et al., 2022), rigorous benchmark of 13 and 35 class variants comprisinng several varieties of image resolutions designed to assess model performance within real-world contexts (Zhao et al., 2024). GroundingDINO obtains an mAP of 26.1% on the 35 class variant of ODinW while more recent works using GroundingDINO as base model obtain an mAP of 28% (Zhao et al., 2024). This difficulty of the task (ODinW approximates the long-tail, open-vocabulary distribution of internet-scale pre-training data) and the multitude of image resolutions align with DataComp and demonstrates that ODinW is suitable benchmark to test our ensembling approach for bounding box annotations. Evaluation Protocol. Given an image from the ODinW test set, we generate bounding box predictions for single resolutions (among {384, 512, 800, 1000}), as well as all combinations of ensembling (two resolutions, three resolutions and all resolutions). Taking from the ODinW test classes, we report average precision results of 10 classes, chosen which provide variance in performance across our resolutions and ensembles, as this provides the most insight into which method should be adopted. Results with single resolutions are shown in Tab. 8 and combinations of resolutions in Tab. 9. We show consistently that ensembling across all 4 resolutions provides the best bounding boxes for annotating DataConcept. Table 8: Performance across resolutions and WBF ensembling on ODinW datasets.We show that ensembling across all 4 resolutions gives the best detection predictions."
        },
        {
            "title": "Dataset",
            "content": "AerialMaritimeDrone large AerialMaritimeDrone tiled ChessPieces DroneControl EgoHands generic MountainDewCommercial North American Mushrooms PKLot brackishUnderwater Self-driving car mAP 384 0.19 0.44 0.07 0.43 0.95 0.06 0.73 0.45 0.17 0.29 0.39 0.23 0.47 0.16 0.42 0.95 0.07 0.63 0.45 0.25 0.37 0."
        },
        {
            "title": "Resolution",
            "content": ""
        },
        {
            "title": "1000 Ensembled (All)",
            "content": "Image Size (WH) 0.39 0.35 0.18 0.45 0.97 0.07 0.63 0.46 0.33 0.36 0.31 0.23 0.17 0.47 0.97 0.09 0.63 0.44 0.39 0.37 0.42 0.41 0.41 0.55 0.17 0.46 1.00 0.11 0.70 0.62 0.59 0. 0.49 1000750 800600 20481732 300300 1280720 1290896 416416 640640 960540 19201200 26 Table 9: Performance across various WBF ensembling combinations on ODinW datasets. Ensembling across all 4 resolutions yields the best overall detection accuracy."
        },
        {
            "title": "Dataset",
            "content": "384 + 512 512 + 800 800 + 1000 384 + 512 + 800 512 + 800 +"
        },
        {
            "title": "Resolution",
            "content": "AerialMaritimeDrone large AerialMaritimeDrone tiled ChessPieces DroneControl EgoHands generic MountainDewCommercial North American Mushrooms PKLot brackishUnderwater Self-driving car mAP 0.29 0.48 0.12 0.33 1.00 0.06 0.70 0.60 0.41 0.29 0.43 0.40 0.48 0.16 0.38 1.00 0.07 0.64 0.63 0.55 0.35 0. 0.40 0.52 0.16 0.33 1.00 0.07 0.66 0.62 0.55 0.34 0.46 0.41 0.49 0.17 0.38 1.00 0.10 0.60 0.61 0.58 0.38 0.47 0.41 0.55 0.17 0.46 1.00 0.11 0.70 0.62 0.59 0.36 0. 1000750 800600 20481732 300300 1280720 1290896 416416 640640 960540 19201200 0.41 0.40 0.17 0.41 1.00 0.10 0.60 0.62 0.56 0.38 0.46 27 A.6 Concept Distribution Having created DataConcept, we run few analyses into the concept distribution of the dataset. We are particularly interested in two axes of inspection, ① DataConcept-wide concept count distribution (Sec. A.6.1) and ② Sample-level concept count distribution (Sec. A.6.2. Both these inspections inform different CABSvariants while curating online batches. A.6.1 Dataset-wide Concept Count As mentioned above, the final vocabulary of DataConceptcomprises 12, 253 unique concepts after GroundingDINO bounding box annotations, from the 19, 261 concepts in the concept bank. This means that in the complete 128M sample pool of DataConcept, 12, 253 concepts occur at least once. We ask: how are these concepts represented in the dataset? Fig. 7 demonstrates the extreme long-tailed nature of DataConcept, by-product of web-scaled distributions captured in DataComp. There is total of 486, 303, 998 annotations in DataConcept, the lowest number of annotations being 1 and the highest being 20, 974, 722 for man. We also find the median concept count to be 489. The figure shows an immense long-tail in the concept distribution, which is aligned with the findings in Udandarao et al. (2024); Parashar et al. (2024). Given this extreme long-tailed nature, it is easy to estimate the biased concept distribution of an IID sampled batch during training and why concept-balancing as done in CABS-DM is critical to address this bias. For better understanding of the concept distribution, we also provide the top 100 concepts with their respective counts as well as release the counts of all concepts as an artifact. Figure 7: What is the distribution of concepts in web-scale pretraining datasets? We demonstrate the distribution of concept counts in DataConceptafter annotations using GroundingDINO. Indeed, DataConceptis strongly long-tailed with 86 concepts having more than 1 million annotations, 685 concepts having more than 100,000 annotations, 2670 concepts having more than 10,000 annotations and 5326 concepts having more than 1,000 annotations. 1. man: 20,974,722 2. woman: 13,264,330 3. flower: 9,397,706 4. chair: 7,770,596 5. wall: 6,939,361 6. hand: 6,760, 7. car: 6,260,366 8. white: 6,212,499 9. poster: 5,647,604 10. shirt: 5,393,204 11. house: 4,308,600 12. floor: 4,250, 13. tree: 4,136,099 14. smile: 3,943,812 15. brand: 3,608,960 16. sign: 3,597,012 17. water: 3,497,612 18. text: 3,375, 19. picture: 3,318,294 20. building: 3,054,316 21. plate: 2,994,394 22. grass: 2,955,611 23. window: 2,727,034 24. dress: 2,712, 25. box: 2,535,939 26. drawer: 2,506,757 27. cup: 2,401,762 28. plant: 2,376,292 29. child: 2,355,394 30. blue: 2,321, 31. bottle: 2,268,065 32. girl: 2,215,856 33. road: 2,181,933 34. door: 2,149,296 35. light: 2,096,164 36. room: 1,991, 37. paper: 1,981,447 38. eye: 1,884,913 39. smartphone: 1,882,529 40. table: 1,779,574 41. flag: 1,759,935 42. blanket: 1,699, 43. circle: 1,682,581 44. sky: 1,659,255 45. bed: 1,637,075 46. crowd: 1,635,165 47. wheel: 1,634,712 48. hair: 1,634, 49. guy: 1,608,147 50. dog: 1,606,644 51. pillow: 1,555,904 52. bowl: 1,550,462 69. necklace: 1,238,552 70. logo: 1,214, 71. shoe: 1,162,959 72. counter: 1,159,415 73. illustration: 1,143,293 74. vase: 1,134,215 75. bathroom: 1,102,494 76. living room: 1,095, 77. fruit: 1,081,347 78. arm: 1,061,739 79. jacket: 1,056,604 80. truck: 1,026,752 81. image: 1,020,059 82. beard: 1,014, 83. mirror: 1,013,644 84. fence: 1,005,264 85. stone: 1,003,367 53. cocktail table: 1,542,248 86. goggles: 1,001,454 54. suit: 1,539, 55. palm tree: 1,531,113 56. head: 1,504,124 57. necktie: 1,501,698 58. couch: 1,493,784 59. screenshot: 1,396,054 60. microphone: 1,395, 61. document: 1,381,569 62. boat: 1,379,477 63. bag: 1,362,313 64. pillar: 1,356,866 65. cabinet: 1,310,379 66. number: 1,267, 67. bird: 1,267,290 68. kitchen: 1,239,892 87. map: 995,526 88. faucet: 956,533 89. ball: 948,458 90. star: 945, 91. carrot: 920,049 92. sink: 909,876 93. armchair: 899,612 94. bench: 899,012 95. face: 888,038 96. apple: 879, 97. cartoon: 870,921 98. tower: 867,593 99. furniture: 865,619 100. skyscraper: 855,711 29 A.6.2 Sample-level Concept Count To the best of our knowledge, previous works have not quantified image complexity using visual concepts in web-scale image-text pretraining datasets. Our GroundingDINO annotations are particularly useful here as we can leverage sample-level annotations to measure concept-multiplicity, i.e, how many concepts are there in sample? Object detection annotations are more advantageous than the object tagging approach from Udandarao et al. (2024) as RAM++ only tags specific concept once to sample, not taking into consideration if that concept is present multiple times in the image. Hence, our approach is the only publicly available resource to conduct study of this scale. Fig. 8 demonstrates that samples in DataConceptgenerally have few concepts in them, reflection of web-scale data, with median of 3 concepts per-sample. We can infer that the bias towards lower concept counts or lower image complexity is rampant in IID batches during training and that models trained this way do not generalize to complex scenes that are common in retrieval datasets. This bias necessitates the need for CABS-FM and curation with sample complexity in mind. Figure 8: What is the complexity of DataConceptsamples based on visual concepts? We demonstrate the distribution of concept counts per sample after annotations using GroundingDINO. Note that GroundingDINO can predict concept many times, hence these numbers reflect the total number of concepts detected in an image, not unique concepts, hence acting as suitable measure of image complexity. 30 Concept-aware Recaptioning B.1 Selecting the Recaptioning VLM Approach. Open-source VLMs have recently caught up with proprietary models in quality text generation given prompt and an image. Hence, we opt for choosing VLM that is optimal for both fidelity (adherence to the prompt and quality of output) and processing speed (we are annotating 128 million image-text pairs). Our initial model pool includes Molmo-7B-D-0924 Deitke et al. (2025), moondream2 and Qwen2-VL-7B (Wang et al., 2024). We test these models on random subset of 10, 000 samples to check both fidelity and processing speed, providing all of them the following prompt: Generate brief and concise image caption using relevant details from alt-text and classes present in the image. Alt-Text: {alt-text} Classes: {classes}. We incorporate the raw caption from the sample as well as the list of detected classes for richer and concept-aware captions. Qualitatively, we find that simply prompting performant open-weight VLMs with alt-text results in relevant information getting incorporated into the synthetic caption. Additionally, VLMs such as Molmo and Qwen2-VL also discard low quality alt-text, which suits our requirements. We observe that moondream2 has the fastest processing speed but returns low fidelity captions. Molmo-7B-D-0924 returns high quality captions but is often quite verbose and prone to hallucinations, on top of being the slowest VLM of the three. Hence, we choose Qwen2-VL-7B due to its ability to adhere to the prompt, generate high quality captions with relatively low hallucinations and moderate processing speed. We admit that these models are not the current state-of-the-art: they were at the time of experimentation and annotation. Please refer to Fig. 9 for more qualitative comparisons between the 3 models. Figure 9: Comparing 3 state-of-the-art open-weight VLMs on concept-aware captioning for pimage-text pretraining datasets. We compare Moondream2, Molmo-7B and Qwen2-VL-7B across random subset of DataComp-128M and select Qwen2-VL for combination of its higher quality captions and appropriate processing speed. B.2 Caption Quality To understand the richness of information in the synthetic captions generated by Qwen2-VL-7B (Wang et al., 2024), we adopt similar analysis as Nguyen et al. (2023) and measure ① the number of words and ② the concept adherence of our new captions compared to the original raw captions. Number of Words In Fig. 10, we observe the distributional difference between the raw captions used in DataComp and our synthetically generated captions. While the raw captions have median word count of 6 with standard deviation of 9.51, Qwen2-VL-7B recaptions have median word count of 33.56 with standard deviation of 16.45. Please note that the raw captions, though much shorter generally, contain 214, 787 samples with word count higher than 80 which are included in the mean and standard deviation measurement but are not presented in this plot. Figure 10: Word Count Distribution. Comparison of DataComp alt-text captions and Qwen2-VL-7B recaptions. Alt-text remains short-form, while recaptions are substantially longer. Extremely long alt-text outliers are excluded from the plot for clarity. Concept Adherence. We sample 1M random subset from DataConceptto estimate how frequently the alt-text or the synthetic caption contains the concepts the sample has been annotated with. Firstly, since the raw captions are multilingual and our concepts are in English, we translate our raw captions to English. Then we measure the exact match percentage which measures if the exact concept word is found in the text. We then do partial match with search over various forms of concept (lemmatized, plurals, gerunds, synonyms). The concept is found in the text if the best fuzzy match between any concept form and any token in the caption exceeds similarity threshold τ . We show our results in Tab. 10, By sweeping τ {0.6, 0.7, 0.8}, we quantify how robust the alignment is under progressively more difficult thresholds of semantic similarity. We show the staggering improvements in concept adherence when using our synthetic recaptions. Table 10: Exact and partial concept adherence between alt-text and Qwen2-VL recaptions."
        },
        {
            "title": "Caption",
            "content": "Exact Match (%) Partial Match(%) τ = 0.6 τ = 0.7 τ = 0.8 Alt-text Qwen2-VL Recaptions 3.89 51.17 32.65 86.69 15.63 79.15 9.82 67.46 32 B.3 Qualitative Evaluation: Visualization Results Figure 11: Some examples of concept-aware synthetic captions. We observe good concept-adherence and multilingual understanding. 33 Figure 12: Some more examples of concept-aware synthetic captions. We observe good concept-adherence and multilingual understanding. 34 Figure 13: Some more examples of concept-aware synthetic captions. We observe good concept-adherence and multilingual understanding. Figure 14: Some more examples of concept-aware synthetic captions. We observe good concept-adherence and multilingual understanding. 36 CABS: More Details C.1 CABS-DM We provide the full PyTorch style code for the heuristic function used in CABS-DM below. Algorithm 2 PyTorch-style code for CABS-DM heuristic function # h_DM: CABS for Diversity-Maximization # C_i = concept set for sample # = (I, T, C) = full super-batch # theta = (b, F, heap_state) where: # = target batch size # = maximum frequency per concept in batch # heap_state = (selected, n_c, heap) for iterative selection def h_DM(C_i, D, theta): b, F, heap_state = theta # unpack parameters I, T, = # unpack super-batch # Step1: initialize on first call if heap_state is None: global_freqs = gather_all_concept_frequencies(C) t_c = concept_balancing_targets(global_freqs, b, F) selected, n_c = [], zeros(global_freqs.size) heap = init_max_heap() # Step2: compute initial gains for all samples for in range(len(C)): gain_i = compute_marginal_gain(C[i], t_c, n_c, global_freqs, F) heap.push((gain_i, i)) heap_state = (selected, n_c, heap, t_c, global_freqs) selected, n_c, heap, t_c, global_freqs = heap_state # Step3: select top sample from heap. Greedy selection: pop best, update counts, refresh heap if len(selected) < and heap: idx = heap.pop() selected.append(idx) update_counts(n_c, C[idx]) refresh_heap(heap, idx, C, n_c, t_c, global_freqs, F) return selected if len(selected) == else None # Helper: compute gain from adding concepts in C_i def compute_marginal_gain(C_i, t_c, n_c, global_freqs, F): gain = 0 for in C_i: if n_c[c] < F: # respect frequency cap deficit = max(0, t_c[c] - n_c[c]) gain += deficit / (global_freqs[c] + 1e-8) return gain 37 C.2 CABS-FM We provide the full PyTorch style code for the heuristic function used in CABS-FM below. Algorithm 3 PyTorch-style code for CABS-FM heuristic function # h_FM: CABS for Frequency-Maximization # C_i = concept set for sample # = (I, T, C) = full super-batch # theta = [] where: def h_FM(C_i, D, theta): I, T, = # unpack super-batch # Step 1: count number of concepts in sample concepts = C_i num_concepts = len(concepts) # Step 2: compute frequency-maximization score # Higher score = more diverse concepts score = num_concepts return score 38 C.3 Hyperparameters We adopt the open clip (Ilharco et al., 2021) codebase to train CLIP and SigLIP models and incorporate CABSdirectly into the codebase, thus making it easily reproducible for practioners accustomed to the code. We also consider the hyperparameters fixed by Datacomp (Gadre et al., 2023) to ensure that IID results are easily reproducible and that all the performance boosts occur due to CABS. Tab. 11 shows the general hyperparameters used for training as well as CABS-specific hyperparameters. Table 11: General pretraining and CABS-specific hyperparameters."
        },
        {
            "title": "IID",
            "content": "CABS-DM CABS-FM batch size beta1 beta2 epochs eps force quick gelu gather with grad lr lr scheduler opt precision warmup wd 1024 0.9 0.98 1 1e-06 False True 0.0005 cosine adamw amp 500 0.2 5120 0.9 0.98 5 1e-06 False True 0.0005 cosine adamw amp 500 0.2 CABS-specific filter ratio max concept frequency min samples concept 0.8 40 1 5120 0.9 0.98 5 1e-06 False True 0.0005 cosine adamw amp 500 0.2 0."
        },
        {
            "title": "D Extended Benchmark Performance",
            "content": "D.1 Evaluation Suite: Further Details Testing contrastively trained VLMs on diverse set of benchmarks, such as the set of evaluation test sets suggested by (Gadre et al., 2023) is critical to understand their zero-shot generalization properties. However, recent probes into the reliability of these benchmarks such as (Abbas et al., 2024b; Udandarao et al., 2025) have exposed several noisy, error-prone and high variability test sets in this set. We decide to omit these benchmarks, resulting in final pool of 28 benchmarks, spanning 26 zero-shot classification and 2 image-text retrieval detailed below: Table 12: Datasets used in Zero-Shot Classification and Image-Text Retrieval Tasks"
        },
        {
            "title": "Classification",
            "content": "Caltech-101 (Fei-Fei et al., 2004) Camelyon17 CIFAR-10 (Krizhevsky et al., 2009) CIFAR-100 (Krizhevsky et al., 2009) Country211 (Radford et al., 2021; Thomee et al., 2016) Dollar Street (Gaviria Rojas et al., 2022) DTD (Cimpoi et al., 2014) FGVC Aircraft (Maji et al., 2013) Food-101 (Bossard et al., 2014) FMoW (Christie et al., 2018; Koh et al., 2021) GeoDE (Ramaswamy et al., 2023) ImageNet (Deng et al., 2009) ImageNet-A (Hendrycks et al., 2021b) ImageNet-O (Hendrycks et al., 2021b) ImageNet-R (Hendrycks et al., 2021a) ImageNet-Sketch (Wang et al., 2019) ImageNet-V2 (Recht et al., 2019) Let-it-Wag! (Udandarao et al., 2024) ObjectNet (Barbu et al., 2019) Oxford Flowers-102 (Nilsback and Zisserman, 2008) Oxford-IIIT Pets (Parkhi et al., 2012; Zhai et al., 2019) Pascal VOC 2007 (Everingham, 2009) RESISCS45 (Cheng et al., 2017; Zhai et al., 2019) Stanford Cars Krause et al. (2013) STL-10 (Coates et al., 2011) SUN-397 (Xiao et al., 2016)"
        },
        {
            "title": "Retrieval",
            "content": "Flickr30k (Young et al., 2014) MSCOCO (Chen et al., 2015) 6,085 85,054 10,000 10,000 21,100 3,503 1,880 3,333 25,250 22,108 12,488 50,000 7,500 2,000 30,000 50,889 10,000 130,000 18,574 6,149 3,669 14,976 6,300 8,041 8,000 108,754 31,014 5,000 102 2 10 100 211 58 47 100 101 62 40 1,000 200 200 200 1,000 1,000 290 113 102 37 20 45 196 10 397 N/A N/A We make several categories of datasets while presenting them such as IN-shift which comprises imagenet-a, imagenet-r, imagenet sketch, imagenetv2, imagenet-o and objectnet, Scene which comprises vtab-resisc45, sun397 and geode and Obj which comprises the remaining classification datasets. 40 D.2 Full Model Suite To provide more in-depth analysis of the trends seen when comparing IID sampling and CABS-DM and CABS-FM, we conduct experiments on two additional models, CLIP ViT-S-16 and SigLIP ViT-SO400M. We arrive at the same conclusions as discussed in Sec. 4.2, we see the strong performance boosts with CLIP ViT-S-16 and SigLIP ViT-SO400M as we see with CLIP ViT-B-32 and SigLIP ViT-B-16/256. Please refer to Tab. 13 for CABS-DM performance and Tab. 14 for CABS-FM performance. We make the conclusion that CABS is effective and provides state-of-the-art performance across varied model architectures and varied model sizes and may be adopted as the de-facto online batch sampling algorithm for contrastive pretraining. Table 13: Extended Classification Results including CLIP ViT-S-16 and SigLIP ViT-SO400M. CABS-DM delivers consistent improvements with these variants as well. Method Captions Zero-shot Classification Let-it-Wag! Avg (Clf ) IN-Val IN-shift ViT-SIID CABS-DM IID CABS-DM IID CABS-DM IID CABS-DM IID CABS-DM IID CABS-DM IID CABS-DM IID CABS-DM alt alt recap recap alt alt recap recap alt alt recap recap alt alt recap recap 16.9 24.6 24.8 30. 17.3 21.9 21.7 26.7 17.2 24.1 28.8 34.7 15.5 22.6 34.1 39. Scene 35.4 39.0 44.4 45.0 36.4 38.0 43.1 42.8 35.9 39. 48.9 50.6 34.7 40.0 55.9 57.5 6.1 8.3 6.3 8.0 5.1 7. 5.9 7.1 5.2 7.0 6.6 7.6 4.7 6.2 7.6 9.4 26.6 31. 35.4 37.8 28.2 30.7 33.0 35.5 26.4 30.9 38.6 41.1 24.5 30. 42.2 44.2 Obj 30.3 34.8 39.4 40.6 32.3 34.5 36.4 39. 15.0 20.6 22.8 27.4 15.2 18.6 20.8 25.4 ViT-B-32 ViT-B-16-SigLIP-256 29.6 33. 15.3 20.8 27.4 32.3 41.5 43.2 ViT-SO400M-14-SigLIP 13.7 18.8 31.8 36. 27.5 33.4 46.3 45.1 41 Table 14: Retrieval Results (COCO and Flickr30K) with averaged retrieval score. Method Captions COCO Flickr Avg(Ret) IID CABS-FM IID CABS-FM IID CABS-FM IID CABS-FM IID CABS-FM IID CABS-FM IID CABS-FM IID CABS-FM alt alt recap recap alt alt recap recap alt alt recap recap alt alt recap recap ViT-S-16 9.6 11.3 28.7 32.4 ViT-B-32 9.7 11.0 24.0 30.4 ViT-B-16-SigLIP11.1 12.3 37.1 39.7 ViT-SO400M-14-SigLIP 8.8 11.3 37.7 39.2 17.4 23. 47.2 56.2 16.2 21.9 41.3 52.9 18.9 23.9 57.0 63.5 13.7 15. 53.8 57.9 13.5 17.6 38.0 44.3 12.9 16.5 32.6 41.6 15.0 18. 47.0 51.6 11.2 13.6 45.7 48."
        },
        {
            "title": "E Continual Pretraining",
            "content": "All the experiments we conducted so far in the main paper and previous supplementary sections were operating in the pretraining from scratch regime. Now, we wish to see if CABS is strong batch sampling algorithm on other pretraining regimes as well, beyond standard pretraining. To this end, we adopt continual pretraining paradigm (Roth et al., 2024), where checkpoints trained at the same scale (128M samples seen) are used to initialize the model that we wish to train. Concretely, we initialize from CLIP ViT-B/32 model trained using IID-sampling on DataComp-128M. We then conduct continued pretraining for 128M more samples (so in total, we the final checkpoint is trained for 256M samples seen) using IID sampling, CABS-DM and CABS-FM. Our results are presented in Tabs. 15 and 16. Across both alt-text and concept-aware synthetic re-captions, CABS-DM and CABS-FM continues to outperform IID sampling on all benchmarks, even in the continual pretraining regime. Our results hence demonstrate that CABS variants can also be utilized as strong continual pretraining method that can utilize strong pretrained vision encoders. This has connections to similar results observed in mid-training and annealing of language models (Feng et al., 2024; Blakeney et al., 2024). We can further draw faint connection to data curriculums (Bengio et al., 2009; Zhang et al., 2025)where we first start with standard data-mixture (as induced by IID sampling), followed by more targeted mid-training mixture (as induced by CABS variants). In the future, we can more closely explore finer-grained curriculums using different CABS variants. Table 15: Continual Pretraining: Zero-shot Classification Performance. We isolate the zero-shot classification benchmarks from the continual-pretraining experiment to more clearly highlight the impact of CABS-DM. We observe that CABS-DM consistently outperforms IID sampling when continually pretraining from the same IID initialization, demonstrating stronger concept coverage and more robust generalization under distribution shift. Method Captions Zero-shot Classification Let-it-Wag! Avg (Clf ) IN-Val IN-shift ViT-B-32 IID CABS-DM IID CABS-DM alt alt recap recap 23.7 27.8 27.7 31.7 20.0 23. 25.8 29.1 Obj 37.7 37.4 41.7 43.4 Scene 42.3 42. 47.7 46.8 7.9 8.9 7.7 8.9 33.4 34.4 38.1 40.0 Table 16: Continual Pretraining: Cross-modal Retrieval Performance. This table isolates retrieval metrics to examine how CABS-FM performs in the continual pretraining setting. We report COCO and Flickr30K retrieval scores along with their mean. Similar to CABS-DM on classification, we observe significant performance boosts when comparing CABS-FM to IID sampling. Method Captions COCO Flickr Avg (Ret) IID CABS-FM IID CABS-DM ViT-B-32 13.7 14.9 30.5 32.7 alt alt recap recap 24.5 28. 49.0 54.2 19.1 21.8 39.8 43."
        },
        {
            "title": "F Ablation on Filter Ratios",
            "content": "In this section, we show how the filter ratio , defined as the parameter that determines the size of sub-batch given super-batch of size B. For example, filter ratio of = 0.5 would correspond to super-batch of size 8192 for sub-batch of size 4096. In most of our experiments, we fix the filter ratio to 0.8. Fig. 15 provides an ablation over various other filter ratios for ViT-B/32 CLIP model, tested on ImageNet across filter ratios {0.5,0.75,0.8,0.9}. Performance trends over the set of filter ratios indicate that 0.8 is indeed the optimal filter ratio at the 128M sample scale. Figure 15: CABS-DM filtering ratio ablation. We choose = 0.8 based on ImageNet validation performance. For simplicity, we maintain this filter ratio for CABS-FM as well, and still see strong performance gains on image-text retrieval benchmarks. 44 Fine-grained Benchmark Performance Motivation While it is common practice to report the aggregated performance across multiple benchmarks to demonstrate the capabilities of machine learning models, deeper probe into the benchmarks that comprise the complete suite of evaluation is often necessary to have deeper understanding of the true capabilities of the model. This is studied in Ghosh et al. (2025) for language models and autoregressive vision-language models but the principle may be applied to CLIP as well. G.1 Expanded Analysis To that end, we provide an expanded probe into the specific benchmarks where CABS-DM outperforms IID sampling (it is relatively straightforward to observe dataset-specific performance gains for CABS-FM as models are evaluated on 2 benchmarks, MSCOCO and Flickr30k). For example, in Fig. 18, we specifically show performance boosts for CABS-DM over IID-sampling in 23 out of 26 benchmarks. With this, we can ascertain that despite maximizing for concept diversity, CABS-DM shows strong gains on datasets that test for long-tailed concepts as well as for more common concepts. This confirms that CABS-DM is an all-round performant batch sampling algorithm for classification tasks. The per-benchmark breakdown of Tab. 13 is shown below. 45 Figure 16: Dataset-wise comparisons for all benchmarks for CLIP ViT-S/16 between CABS-DM and IID sampling for alt-text. positive performance difference indicates benchmark where CABS-DM outperforms IID sampling. Figure 17: Dataset-wise comparisons for all benchmarks for CLIP ViT-S/16 between CABS-DM and IID sampling for synthetic recaptions. positive performance difference indicates benchmark where CABS-DM outperforms IID sampling. 46 Figure 18: Dataset-wise comparisons for all benchmarks for CLIP ViT-B/32 between CABS-DM and IID sampling for alt-text. positive performance difference indicates benchmark where CABS-DM outperforms IID sampling. Figure 19: Dataset-wise comparisons for all benchmarks for CLIP ViT-B/32 between CABS-DM and IID sampling for synthetic recaptions. positive performance difference indicates benchmark where CABS-DM outperforms IID sampling. 47 Figure 20: Dataset-wise comparisons for all benchmarks for SigLIP ViT-B-16 between CABS-DM and IID sampling for alt-text. positive performance difference indicates benchmark where CABS-DM outperforms IID sampling. Figure 21: Dataset-wise comparisons for all benchmarks for SigLIP ViT-B-16 between CABS-DM and IID sampling for synthetic recaptions. positive performance difference indicates benchmark where CABS-DM outperforms IID sampling. 48 Figure 22: Dataset-wise comparisons for all benchmarks for SigLIP ViT-SO400M-14 between CABS-DM and IID sampling for alt-text. positive performance difference indicates benchmark where CABS-DM outperforms IID sampling. Figure 23: Dataset-wise comparisons for all benchmarks for SigLIP ViT-SO400M-14 between CABS-DM and IID sampling for synthetic recaptions. positive performance difference indicates benchmark where CABS-DM outperforms IID sampling. 49 G.2 MetaCLIP: Further Details In this section, we extend our analysis on MetaCLIP offline data curation from Sec. 4.3. We first adopt the concept balancing threshold of 20, 000 from (Xu et al., 2024) and filter DataConcept accordingly. Note, again, that we do not adopt the concepts curated by the original work, instead we use the 12, 253 concept vocabulary V. This results in 14M filtered dataset. Training ViT-B/32 CLIP model with IID sampling on this filtered pool for total of 128M samples seen results in an ImageNet accuracy of 15.1%, which underperforms standard IID training over the unfiltered pool. Thus, we adopt modified curation strategy to match the filtered dataset size of worst-case repeats of CABS over various filter ratios. Using this strategy, for filter ratio = {0.5, 0.75, 0.8}, we obtain an effective per-epoch samples-seen count of Dfilter = {64M, 32M, 25.6M}. We obtain the above datasets based on concept balancing using thresholds of τMetaCLIP = {600K, 110K, 70K}. We compare CLIP ViT-B/32 models trained using these filtered datasets with CABS-DM, with an additional probe into SigLIP ViT-B-16/256 at = 0.8 (25.6M samples). Finally, even though MetaCLIP is the appropriate baseline to compare CABS-DM with, we also show that CABS-FM outperforms MetaCLIP for both CLIP ViT-B/32 and SigLIP ViT-B-16/256 at = 0.8 (25.6M samples). Table 17: Retrieval Results. Comparing IID sampling, MetaCLIP curation and CABS-FM on MSCOCO and Flickr30k with averaged retrieval score. Method Captions MSCOCO Flickr30k Avg(Ret) IID MetaCLIP CABS-FM IID MetaCLIP CABS-FM alt alt alt alt alt alt ViT-B-32 9.7 8.7 11.0 ViT-B-16-SigLIP11.1 8.1 12.3 16.2 11.6 21.9 18.9 12.3 23.9 12.9 9.7 16.5 15.0 10.2 18.1 Figure 24: Dataset-wise comparisons for all benchmarks for CLIP ViT-B-32 between CABS-DM (f = 0.5) and MetaCLIP curation on alt-text. Figure 25: Dataset-wise comparisons for all benchmarks for CLIP ViT-B-32 between CABS-DM (f = 0.75) and MetaCLIP curation on alt-text. 51 Figure 26: Dataset-wise comparisons for all benchmarks for CLIP ViT-B-32 between CABS-DM (f = 0.8) and MetaCLIP curation on alt-text. Figure 27: Dataset-wise comparisons for all benchmarks for SigLIP ViT-B-16 between CABS-DM (f = 0.8) and MetaCLIP curation on alt-text."
        }
    ],
    "affiliations": [
        "LAION",
        "Stanford University",
        "Tubingen AI Center, University of Tubingen",
        "University of Cambridge",
        "University of Trento",
        "University of Washington"
    ]
}