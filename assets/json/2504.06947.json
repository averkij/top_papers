{
    "paper_title": "RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts",
    "authors": [
        "Natalia Loukachevitch",
        "Natalia Tkachenko",
        "Anna Lapanitsyna",
        "Mikhail Tikhomirov",
        "Nicolay Rusnachenko"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In this paper, we introduce the Dialogue Evaluation shared task on extraction of structured opinions from Russian news texts. The task of the contest is to extract opinion tuples for a given sentence; the tuples are composed of a sentiment holder, its target, an expression and sentiment from the holder to the target. In total, the task received more than 100 submissions. The participants experimented mainly with large language models in zero-shot, few-shot and fine-tuning formats. The best result on the test set was obtained with fine-tuning of a large language model. We also compared 30 prompts and 11 open source language models with 3-32 billion parameters in the 1-shot and 10-shot settings and found the best models and prompts."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 ] . [ 1 7 4 9 6 0 . 4 0 5 2 : r RuOpinionNE-2024: Extraction of Opinion Tuples from Russian News Texts Natalia Loukachevitch Lomonosov Moscow State University louk_nat@mail.ru Natalia Tkachenko Lomonosov Moscow State University nataliya.m.tkachenko@gmail.com Anna Lapanitsyna Lomonosov Moscow State University anna.lapachka@gmail.com Mikhail Tikhomirov Lomonosov Moscow State University tikhomirov.mm@gmail.com Nicolay Rusnachenko Bauman Moscow State Technical University rusnicolay@gmail.com Abstract In this paper, we introduce the Dialogue Evaluation shared task on extraction of structured opinions from Russian news texts. The task of the contest is to extract opinion tuples for given sentence; the tuples are composed of sentiment holder, its target, an expression and sentiment from the holder to the target. In total, the task received more than 100 submissions. The participants experimented mainly with large language models in zero-shot, fewshot and fine-tuning formats. The best result on the test set was obtained with fine-tuning of large language model. We also compared 30 prompts and 11 open source language models with 3-32 billion parameters in the 1-shot and 10-shot settings and found the best models and prompts. Keywords: Structured Sentiment Analysis, Opinion Tuples, Named Entity, News Texts DOI: 10.28995/2075-7182-2022-20-XX-XX RuOpinionNE-2024: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ –∫–æ—Ä—Ç–µ–∂–µ–π –º–Ω–µ–Ω–∏–π –∏–∑ –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤ –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ –õ—É–∫–∞—à–µ–≤–∏—á –ù.–í. –ú–ì–£ –∏–º. –õ–æ–º–æ–Ω–æ—Å–æ–≤–∞ –ú–æ—Å–∫–≤–∞, –†–æ—Å—Å–∏—è louk_nat@mail.ru –¢–∫–∞—á–µ–Ω–∫–æ –ù.–ú. –ú–ì–£ –∏–º. –õ–æ–º–æ–Ω–æ—Å–æ–≤–∞ –ú–æ—Å–∫–≤–∞, –†–æ—Å—Å–∏—è nataliya.m.tkahenko@gmail.com –õ–∞–ø–∞–Ω–∏—Ü—ã–Ω–∞ –ê.–ú. –ú–ì–£ –∏–º. –õ–æ–º–æ–Ω–æ—Å–æ–≤–∞ –ú–æ—Å–∫–≤–∞, –†–æ—Å—Å–∏—è –¢–∏—Ö–æ–º–∏—Ä–æ–≤ –ú.M. –ú–ì–£ –∏–º. –õ–æ–º–æ–Ω–æ—Å–æ–≤–∞ –ú–æ—Å–∫–≤–∞, –†–æ—Å—Å–∏—è anna.lapachka@gmail.com tikhomirov.mm@gmail.com –†—É—Å–Ω–∞—á–µ–Ω–∫–æ –ù.–õ. –ú–ì–¢–£ –∏–º. –ù.–≠. –ë–∞—É–º–∞–Ω–∞ –ú–æ—Å–∫–≤–∞, –†–æ—Å—Å–∏—è rusnicolay@gmail.com –ê–Ω–Ω–æ—Ç–∞—Ü–∏—è –í —ç—Ç–æ–π —Å—Ç–∞—Ç—å–µ –æ–ø–∏—Å–∞–Ω–æ –Ω–æ–≤–æ–µ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ –∏–∑–≤–ª–µ—á–µ–Ω–∏—é –º–Ω–µ–Ω–∏–π –∏–∑ —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–Ω—ã—Ö —Ç–µ–∫—Å—Ç–æ–≤, –æ—Ä–≥–∞–Ω–∏–∑–æ–≤–∞–Ω–Ω–æ–µ –≤ —Ä–∞–º–∫–∞—Ö —Å–µ—Ä–∏–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–π Dialogue Evaluation. –ó–∞–¥–∞—á–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è —Å–æ—Å—Ç–æ–∏—Ç –≤ –∏–∑–≤–ª–µ—á–µ–Ω–∏–∏ –∫–æ—Ä—Ç–µ–∂–µ–π –º–Ω–µ–Ω–∏–π –¥–ª—è –∑–∞–¥–∞–Ω–Ω–æ–≥–æ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è; –∫–æ—Ä—Ç–µ–∂–∏ —Å–æ—Å—Ç–æ—è—Ç –∏–∑ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –º–Ω–µ–Ω–∏—è, –æ–±—ä–µ–∫—Ç–∞ –º–Ω–µ–Ω–∏—è, –æ—Ü–µ–Ω–æ—á–Ω–æ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –∏ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–∞ –ø–æ –æ—Ç–Ω–æ—à–µ–Ω–∏—é –∫ –æ–±—ä–µ–∫—Ç—É. –í—Å–µ–≥–æ –Ω–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –±—ã–ª–æ –ø–æ–¥–∞–Ω–æ –±–æ–ª–µ–µ 100 –ø—Ä–æ–≥–æ–Ω–æ–≤. –£—á–∞—Å—Ç–Ω–∏–∫–∏ —ç–∫—Å–ø–µ—Ä–∏–º–µ–Ω—Ç–∏—Ä–æ–≤–∞–ª–∏ –≤ –æ—Å–Ω–æ–≤–Ω–æ–º —Å –±–æ–ª—å—à–∏–º–∏ —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏ –≤ —Ñ–æ—Ä–º–∞—Ç–∞—Ö zero-shot, few-shot –∏ fine-tuning. –õ—É—á—à–∏–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç –Ω–∞ —Ç–µ—Å—Ç–æ–≤–æ–º –Ω–∞–±–æ—Ä–µ –±—ã–ª –ø–æ–ª—É—á–µ–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ –¥–æ–æ–±—É—á–µ–Ω–∏—è (fine-tuning) –±–æ–ª—å—à–æ–π —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏. –¢–∞–∫–∂–µ –±—ã–ª–æ –ø—Ä–æ–≤–µ–¥–µ–Ω–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ 30 –ø—Ä–æ–º–ø—Ç–æ–≤ –∏ 11 —è–∑—ã–∫–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π —Å –æ—Ç–∫—Ä—ã—Ç—ã–º –∏—Å—Ö–æ–¥–Ω—ã–º –∫–æ–¥–æ–º —Ä–∞–∑–º–µ—Ä–æ–º 3-32 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –≤ 1-shot –∏ 10-shot —Ä–µ–∂–∏–º–∞—Ö –∏ –≤—ã—è–≤–ª–µ–Ω—ã –ª—É—á—à–∏–µ –º–æ–¥–µ–ª–∏ –∏ –ø—Ä–æ–º–ø—Ç—ã. –ö–ª—é—á–µ–≤—ã–µ —Å–ª–æ–≤–∞: —Ç–∞—Ä–≥–µ—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –∞–Ω–∞–ª–∏–∑ —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç–∏, –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å, –Ω–æ–≤–æ—Å—Ç–Ω—ã–µ —Ç–µ–∫—Å—Ç—ã Figure 1: Example of opinion extraction with explanations according to the RuOpinionNE-2024 task. Two opinion tuples are shown: (Italy, Bersany, beat Florence Mayor, positive) and (AUTHOR, Matteo Renzi, prominent, positive)"
        },
        {
            "title": "Introduction",
            "content": "Sentiment analysis is one of the most actively developing areas in natural language processing. At the beginning of studies in this area, the task was to extract the overall sentiment of user review or message in social network. Currently, there is wide variety of problem statements, including the identification of sentiment toward specific entity, an aspect (part or characteristic) of this entity, and related task of extracting position (stance) on certain discussion topic. Modern methods allow us to get closer to more complete opinion recognition (Liu, 2012), extracting opinion tuples, which include source of the opinion, an object of the opinion, sentiment, and other opinion components (Barnes et al., 2022). In this paper, we present the RuOpinionNE shared task on extraction of opinions from Russian news texts organized in the framework of the Dialogue evaluation competitions. The aim of the task is to extract opinion tuples for given sentence; the tuples are composed of sentiment holder, its target, an expression and sentiment from the holder to the target. News texts are very interesting source for extracting opinions because such texts contain numerous entities, which can be the holder or the target of opinion or both. In the same sentence, positive and negative attitudes can be met. At the same time, most entities are mentioned in neutral contexts. The extraction of such tuples from news texts allows better understanding of attitudes between different subjects discussed in the current flow of news. Figure 1 depicts an example of opinion tuples extracted from Russian text (right) along with the translated version of the related example (left). In the example, the author is positive to Matteo Renzi, because he is mentioned as prominent politician. In addition, Italy is positive to Bersani because Italians voted for him."
        },
        {
            "title": "2 Related Work",
            "content": "The extraction of opinions from texts should aim to identify tuples that comprise several components such as the source of the opinion, the target, maybe the aspect (part, characteristics) of the target and the sentiment (Liu, 2012). Currently, in numerous sentiment-oriented works, the ultimate task is often reduced to simpler subtasks such as general sentiment of text (sentence), target-oriented sentiment (the identification of the opinion source is not included in the task), and others (Barnes et al., 2021). Barnes et al. (Barnes et al., 2021) suggest extracting opinions as tuples (‚Ñé, ùë°, ùëí, ùëù) where ‚Ñé is holder who expresses polarity ùëù towards target ùë° through sentiment expression ùëí. Extracted tuples can be represented as graph comprising set of labeled nodes and set of unlabeled edges connecting pairs of nodes. Empty holders and targets are allowed. The authors used reimplementation of the neural syntactic parser by Dozat and Manning (Dozat and Manning, 2022) to extract such tuples from texts. The base of the network model is bidirectional LSTM (Hochreiter, 1997; Schuster and Paliwal, 1997) (BiLSTM), which creates contextualized representations [ùëê1, . . . , ùëêùëõ] = ùêµùëñùêøùëÜùëá ùëÄ (ùë§1, ..., ùë§ùëõ), where ùë§ùëñ is the concatenation of word embedding, POS tag embedding, lemma embedding, character embedding created by character-based LSTM for the ùëñth token, and also BERT (Devlin et al., 2019) contextualized embeddings. The obtained embeddings are then processed by two feedforward neural networks (FNN), creating specialized representations for potential heads and dependents as ‚Ñéùëñ = ùêπ ùëÅ ùëÅ ‚Ñéùëíùëéùëë(ùëêùëñ) and ùëëùëñ = ùêπ ùëÅ ùëÅ ùëëùëíùëù(ùëêùëñ) respectively. The scores for each possible relation between found items are computed by final bilinear transformation. In 2022, the multilingual contest on the extraction of opinion tuples was organized at the SemEval 2022 conference (Barnes et al., 2022). The evaluation collection comprises mainly review datasets and single news collection MPQA. The organizers provided two baselines in the contest: 1) dependency graph prediction model, and 2) sequence-labeling pipeline based on three separate BiLSTM models to extract holders, targets, and expressions. The outputs of the models were used to train relation prediction model that determines sentiment relations between found entities. The best results in the competition were achieved by applying modified version of the above mentioned sentiment graph analysis model with contextualized embeddings obtained with RoBERTaLarge (Liu et al., 2019). In aspect-oriented sentiment analysis, several tuple configurations are used to structure the content. Such configurations are: Aspect-Sentiment Triplet Extraction (ASTE), which should produce {aspect, opinion, sentiment category} triplets, such as menu, great, positive; and Aspect-Sentiment Quadruplet Extraction/Prediction (ASQE/ASQP) that outputs {aspect, opinion, aspect category, sentiment category} quadruplets, such as menu, great, general, positive (Hua et al., 2024). The authors of (Zhang et al., 2024) tested zero-shot large language models (LLM) in various sentiment analysis tasks, including aspect tuple extraction: Aspect-Sentiment Triplet Extraction (ASTE) and Aspect-Sentiment Quadruplet Extraction/Prediction (ASQE/ASQP). They found that zero-shot prompting of larger LLMs has very low results in these tasks, fine-tuned T5large model (Raffel et al., 2020) (small LLM) achieved much higher results: about 24 percent points for ASTE on average and 30 percent points for ASQE/ASQP. For Russian, there were some studies on targeted sentiment analysis, such as aspect-based sentiment analysis (Loukachevitch et al., 2015; Chumakov et al., 2023) or entity-oriented sentiment analysis (Loukachevitch and Rubtsova, 2015; Golubev and Loukachevitch, 2020; Pronoza et al., 2021; Golubev et al., 2023). In 2023, the RuSentNE competition devoted to extraction of sentiment tovards named entities in news texts was organized. The best approaches on the RuSentNE evaluation were BERT-based ensembles (Devlin et al., 2019; Golubev et al., 2023). Currently, the best result in this formulation of the problem has been achieved with fine-tuning of the Flan-T5 model (Rusnachenko et al., 2024). In (Rusnachenko et al., 2019), the authors studied the extraction of triples {holder, target, sentiment} at the document level in the RuSentRel dataset, which comprised 73 analytical texts on international relations. The current work is the first study on extracting sentiment tuples at the sentence level for the Russian language."
        },
        {
            "title": "3 RuSentNE Corpus",
            "content": "The RuOpinionNE-2024 dataset is based on the annotated RuSentNE corpus, which includes news texts written in Russian. In the first stage of the annotation, the RuSentNE texts were annotated with named entities, including such types as PERSON, ORGANIZATION, PROFESSION, COUNTRY, CITY, NATIONALITY and so on. In the next stage, positive or negative relations between entities (including the authors position towards mentioned entities) were established. Finally, for each annotated attitude, the evidence (that is, an expression that serves as key element for extracting the opinion) was carefully annotated and checked (Figure 2). The annotations allowed us to create dataset, in which sets of opinion tuples (holder, target, sentiment, expression) are assigned to each text fragment. Figure 2: Example of opinion annotation for the RuOpinionNE-2024 task The source of the opinion (Opinion holder) can be: (i) the author of the text, (ii) the author of the quote, or (iii) another mentioned entity. The opinion can be implicit, i.e. expressed through the actions of the entity. For example, in the sentence fired Y, it is assumed that there is an implicit negative opinion of towards Y. The first version of the created corpus became basis for the RuSentNE-2023 open evaluation, which required the recognition of sentiment in relation to given named entity (Golubev et al., 2023). The task was to predict whether the content of given sentence is positive or negative towards given entity (Golubev et al., 2023)."
        },
        {
            "title": "4 RuOpinionNE-2024 Dataset and Task Description",
            "content": "The task of the RuOpinionNE-2024 evaluation is as follows: for given fragments of news articles (mainly sentences), it is necessary to extract all tuples: (ùêª, ùëá, ùëÉ, ùê∏), where ùêª is an opinion holder who expresses polarity ùëÉ towards target ùëá through sentiment expression ùê∏. There can be sentences without any tuple. Holders and Targets can be entities of the following types: PERSON, ORGANIZATION, COUNTRY, CITY, REGION, PROFESSION, NATIONALITY, IDEOLOGY. Pronouns are not included in tuples. Holders can also be empty (NULL) for general opinion or AUTHOR for the authors opinion. Targets and expressions are never empty. Fragmented entities are possible. The polarity represents sentiment label which could be positive or negative. Table 1 shows examples of tuples extracted from the example sentence (Figure 2)."
        },
        {
            "title": "NEG\nNEG",
            "content": "violated patents violated patents Table 1: Examples of sentiment opinions in RuOpinionNE-2024 dataset (Figure 2) Table 2 contains the distribution of opinion tuples in train, validation and test sets of the RuOpinionNE2024 dataset. It can be seen that about 40% of sentences do not contain opinion tuples. Sentences with opinions contain two opinion tuples on average. In about 20% of annotated tuples, the holder is absent, and about 10% of tuples are annotated as the authors opinion."
        },
        {
            "title": "Stage",
            "content": "#Sent."
        },
        {
            "title": "Train",
            "content": "train validation Development test Total"
        },
        {
            "title": "Final",
            "content": "2556 1316 804 4676 #Sent. w/o sentiment 1062 547 216 1825 #Tuples 2904 1612 1172 5688 #Tuples w/o holder 484 436 334 1254 #Tuples with author as holder 173 196 158 Table 2: Distribution of sentiment scores in training, validation and test sets. The data was prepared in the JSON lines format1, where line corresponds to sentence with found opinion tuples or with an empty set of opinion tuples. 1https://jsonlines.org/ The main metric for the task is the F1 measure. When evaluating recall, each reference tuple was compared with the entire list of predicted tuples for the corresponding text fragment. The reference tuple was considered found if among the predictions at least one tuple detected, for which: 1) holder, target, expression intersect with the reference holder, target, expression by at least one token, respectively (it is necessary that each component is at least partially detected and all of them are combined into single tuple); 2) the polarity of the same tuple coincides with the polarity of the reference. The cases of holder = AUTHOR or NULL were processed as special tokens that required complete match. For example, for holder overlap: holder_overlap = (number of common tokens of the reference and the prediction)/(number of reference tokens). Target overlap and expression overlap were calculated in similar way. If there were several options, the best tuple was selected. If the tuple was found, weighted score was calculated for it, which reflected the coverage of the reference tuple by the predicted one, averaged over the three components. Weighted score and recall were calculated as follows: weighted_score = (holder_overlap + target_overlap + expression_overlap)/3, if each overlap >0 and the polarity matches, and 0 otherwise; recall = (sum of weighted_score over all reference tuples)/(number of reference tuples). Precision was calculated in the same way, but for it, the coverage of the set of predicted tuples by the set of reference tuples was estimated. Both metrics were calculated not for sentences separately, but for the entire dataset, and then combined into the F1 metric."
        },
        {
            "title": "5 Results of Evaluation",
            "content": "As baseline (baseline_model), we adopt Qwen2.5-32B-Instruct model (Qwen et al., 2025) (32 billion parameters) in the few-shot format (10 examples). The complete textual prompt utilized for inferring results is shown in Appendix A. In our competition, we received more than 100 submissions. Table 3 shows the results of the models at the development stage, table 4 presents the results of the models at the final stage of the competition. In the following, we describe the approaches of participants. Participant Zholtikov_Michail msuai iarnv baseline_model F1 0.34 0.28 0.21 0.17 Table 3: Results of the Development evaluation stage of RuOpinionNE-2024 Participant VatolinAlexey RefalMachine msuai iarnv Zholtikov_Michail baseline_model vitalymegabyte utmn F1 0.41 0.35 0.33 0.28 0.24 0.24 0.20 0.11 Table 4: Results of the Final evaluation stage of RuOpinionNE-2024 VatolinAlexey. The participant experimented with unsupervised and supervised approaches. In an unsupervised approach, the LlaMA-3.3-70B2 (et. al., 2024) model was used. The model could not calculate exact indices of the items for opinion tuples in the substring, therefore, the final prompt did not contain the task to determine the indices. Instead, the algorithm was written for fuzzy search of substring in given sentence. The best result of the unsupervised experiments was F1 = 0.22. As supervised approach, the QLora (Dettmers et al., 2023) adapter for the LLaMA-3.3-70B model with FP4 precision was trained on the training data. Only text was fed as input without any prompts. The output was similar to what is described in the task, i.e. JSON with list of opinions. The model could produce different results after each run due to randomization, so the model was run several times and the results were aggregated. Several aggregation strategies were tested: (1) most frequent answer (most_common), (2) ùëÅ most frequent answers (most_common_n), (3) combining all answers (combine). The best result on the test data was obtained with ùëÅ = 5, strategy most_common_n with the result F1 = 0.41. RefalMachine. The participant experimented with zero-shot, few-shot (Brown et al., 2020) and finetuning approaches. The best results were achieved by fine-tuning the Qwen2.5-32B-Instruct model and prompt in the instruction format, with an F1 score of 0.35 at the Final evaluation stage. msuai. The participant experimented with unsupervised and supervised formats of the LLM application: Mistral Large 23, GPT4-o(OpenAI et al., 2024). Models were used with prompts containing 12-15 examples. The examples were chosen according to semantic similarity with target sentence calculated using BERTlarge-uncased model for sentence embeddings in Russian language4 or Sentence RuBERT5 models. This approach achieved the highest participant F1 score of 0.33 at the Final evaluation stage. Zholtikov_Michail. The participant experimented with the instruction tuned versions of Mistral7B-v0.36, MetaLlama-3-8B7 and Vikhr-7B-0.38. The format of experiments: zero-shot and supervised learning. The best results F1=0.34 were obtained by instruction tuned version of Mistral-7B-v0.3 at the Development stage, and F1 = 0.24 at the Final evaluation stage. vitalymegabyte. The participant applied traditional NER, splitting polar expressions to separate tags: Polar_expression_POS and Polar_expression_NEG. To extract relationships between extracted items, self-attention was used to build the relationship matrix between items. Thus, we can see that almost all participants experimented with generative models in zero-shot, fewshot or fine-tuning regimes. The best results were achieved by parameter-efficient fine-tuning of quite large language model LlaMA-3.3-70B-Instruct with 70 billion parameters. Compared with the results of the Semeval 2022 competition (Barnes et al., 2021), it could be noted that the Semeval 2022 results for the review datasets are much higher, achieving 0.76 F-measure on an English review dataset, but the best results on the MPQA news dataset is approximately the same: about 0.44 F-measure."
        },
        {
            "title": "6 Experiments with RuOpinionNE Data in the Framework of Student Course",
            "content": "RuOpinionNE data were used as experimental data in the student course \"Practical aspects of LLM training\", which was taught by Mikhail Tikhomirov at the Faculty of Computational Mathematics and Cybernetics of Lomonosov Moscow State University. The students constructed 30 prompts and experimented with them in zero-shot or few-shot regimes on the RuOpinionNE data. The collected prompts were fed into eleven language models in 1-shot or 10-shot formats. The following language models were used in experiments: LLaMA-3.1-8B-Instruct 9, LLaMA-3-8B-Instruct 10; 2https://huggingface.co/meta-llama/Llama-3.3-70B-Instruct 3https://huggingface.co/mistralai/Mistral-Large-Instruct-2407 4https://huggingface.co/ai-forever/sbert_large_nlu_ru 5https://huggingface.co/DeepPavlov/rubert-base-cased-sentence 6https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3 7https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct 8https://huggingface.co/Vikhrmodels/Vikhr-7B-instruct_0.3 9https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct 10https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct Mistral-Nemo-Instruct-2407 (12.2B parameters) 11; Qwen2.5-32B-Instruct, Qwen2.5-14B-Instruct, Qwen2.5-7B-Instruct, Qwen2.5-3B-Instruct 12; T-lite-it-1.0 (7.6B parameters)13; OpenChat-3.5-0106 (7B parameters) 14; RuAdapt-LLaMA3 (8B parameters) 15 (Tikhomirov and Chernyshov, 2024); Saiga-LLaMA3-8B (8B parameters) 16 model_name Qwen2.5-32B-Instruct Mistral-Nemo-Instruct-2407 Qwen2.5-7B-Instruct Saiga-LLaMA3-8B T-lite-it-1.0 LLaMA-3-8b-Instruct Qwen2.5-14B-Instruct Meta-LlaMA-3.1-8B-Instruct RuAdapt-LLaMA3 OpenChat-3.5-0106 Qwen2.5-3B-Instruct k=10 0.195 0.190 0.184 0.179 0.157 0.153 0.145 0.141 0.123 0.113 0.091 k=1 0.158 0.112 0.139 0.091 0.096 0.119 0.121 0.090 0.073 0.087 0.088 Table 5: Average model quality in 10-shot and 1-shot settings for the RuOpinionNE test set. The best results are in bold, the second best results are underlined model_name Qwen2.5-32B-Instruct Mistral-Nemo-Instruct-2407 Qwen2.5-7B-Instruct Saiga-LLaMA3-8B LLaMA-3.1-8B-Instruct T-lite-it-1.0 LLaMA-3-8B-Instruct Qwen2.5-14B-Instruct RuAdapt-LLaMA3 OpenChat-3.5-0106 Qwen2.5-3B-Instruct k=10 0.229 0.211 0.199 0.193 0.173 0.171 0.169 0.169 0.134 0.132 0.120 k=1 0.204 0.157 0.168 0.118 0.110 0.119 0.154 0.144 0.104 0.108 0. Table 6: Maximum model quality in 10-shot and 1-shot settings for the RuOpinionNE test set. The best results are in bold, the second best results are underlined Thus, the set of models in experiments comprises larger model Qwen2.5-32B-Instruct (32 billion parameters), average-sized models with 12-13 billion parameters (Qwen2.5-14B-Instruct, MistralNemo-Instruct-2407), smaller models with 7-8 billion parameters, and the smallest model with 3 billion parameters (Qwen2.5-3B-Instruct). The models in experiments include three models adapted to the Russian language with 7-8 billion parameters: T-lite-it-1.0, Saiga-LLaMA3-8B and RuAdapt-LLaMA3. Table 5 shows the performance of each model averaged on all prompts. It can be seen that the averaged best results are obtained on 10-shot prompts by larger models Qwen2.5-32B-Instruct and Mistral-NemoInstruct-2407; the averaged best results on 1-shot prompts are achieved by Qwen2.5-32B-Instruct. The 11https://huggingface.co/mistralai/Mistral-Nemo-Instruct-2407 12https://huggingface.co/collections/Qwen/qwen25-66e81a666513e518adb90d9e 13https://huggingface.co/t-tech/T-lite-it-1.0 14https://huggingface.co/openchat/openchat-3.5-0106 15https://huggingface.co/RefalMachine/ruadapt_llama3_8b_instruct_extended_lep_ft 16https://huggingface.co/IlyaGusev/saiga_llama3_8b Qwen2.5-7B-Instruct model obtained the best averaged results among all the smaller models. Among the Russian-oriented models, the best results for the 10-shot setting were achieved by SaigaLLaMA3-8B. For 1-shot prompts, similar results were obtained by T-lite-it-1.0 and Saiga-LLaMA3-8B. In both cases, the results of the Russian models are worse than the results of the similar-sized model Qwen2.5-7B-Instruct. Table 6 shows the best results for each model. The best results are obtained by larger models: Qwen2.5-32B-Instruct and Mistral-Nemo-Instruct-2407. Among the smaller models, the Qwen2.5-7BInstruct model achieved the highest results. Among Russian models, the Saiga-LLaMA3-8B model obtained better results in the 10-shot setting and similar results with T-lite-it-1.0 in the 1-shot setting. Table 7 presents the results of all instructions averaged on the models. It can be seen that the obtained results are very close to each other in average quality. This means that there does not exist single prompt, which is best for all or most models. instruction_id 28 19 17 5 0 7 23 13 12 16 k=10 0.162 0.159 0.158 0.157 0.157 0.156 0.155 0.154 0.154 0.154 k=1 0.113 0.112 0.120 0.103 0.105 0.099 0.108 0.081 0.112 0.118 Table 7: Results of all instructions averaged on the models in 10-shot and 1-shot settings for the RuOpinionNE test set."
        },
        {
            "title": "7 Analysis of Errors",
            "content": "In this section, we provide an analysis of the submitted results. To explore the cases that arise from disagreement between gold standard annotation and tuples automatically extracted by participating models, we use two different modes for analysis: Extracted opinions mentioned in manual annotation: absence of annotated tuples in the submitted results. Extracted opinions not mentioned in the gold standard: analysis of tuples that were found in the submitted results but were not indicated in the gold annotation. We consider examples from the final stage of the competition. We limit the scope of methods by overviewing LLM-based submissions, according to the Section 5, The analysis includes all submissions that exceed the baseline, as well as the baseline itself (6 models). We analyze examples of annotated opinion tuples that were not identified by all models under consideration, or tuples, which are absent in the annotated data but recognized by at least three models. We found the following main causes of the differences between annotated tuples and the tuples extracted by the models. First, the context of the sentence is sometimes not sufficient to reveal the sentiment or the holder of the opinion. For example, the sentence \"By the way, Moscow occupies fourth place in the list of leaders for 2010\" looks very positive towards Moscow but in fact leaders in road congestion (jams) are discussed, which means negative information about Moscow. Second, in the task, we required participants to distinguish between the authors opinion and the opinion with the unknown holder. We thought that this is important for extracting the authors position. But this requirement was too difficult for models. In some cases, the short context of the sentence did not provide the necessary information. For example, from the sentence \"Russians started the Dakar rally as favorites\", it is difficult to understand if \"favorites\" is general opinion or an authors opinion. Next, we considered opinions between mention subjects that were not found by all the models. In the following sentence, all models did not find that the attitude of specialists towards UKRSPIRT is negative possibly because of the quite long distance between the mentions: \"In addition, the very fact of the possibility of alternative supplies will undermine the [monopoly] of the state concern UKRSPIRT, which most often does not agree to increase prices for its products, specialists note.\" In the next sentence, the positive attitude from Syria to HAMAS, Hezbollah and Iran is presupposed in the sentence but was not revealed by all models: In Tel Aviv, they believe that the price of the withdrawal of Israeli troops from the Golan Heights should be political concessions from Syria - weakening its ties with its strategic ally Iran and ending support for the Lebanese resistance movement Hezbollah and the Palestinian Hamas. In the following sentence, the models did not reveal the negative attitude from V. But to the United States because of the long distance between entities and non-typical expression of sentiment: V. But suggested that \"the court will not examine the factual objective side of the case, since the practice of hearing cases on charges of conspiracy in the United States is such that such charges automatically mean guilt.\""
        },
        {
            "title": "8 Conclusion",
            "content": "In this paper, we presented the shared task of extracting structured opinions from Russian news texts. The task of the RuOpinionNE competition was to extract opinion tuples for given sentence; the tuples are composed of sentiment holder, its target, an expression, and the holders sentiment towards the target. We prepared new dataset of annotated opinion tuples. In total, the task received more than 100 submissions. The participants experimented mainly with large language models in zero-shot, few-shot and fine-tuning formats. The best result on the test set was obtained with fine-tuning of large language model. We also compared 30 prompts and 11 open-source language models with 3-32 billion parameters in the 1-shot and 10-shot settings and found the best model and prompt."
        },
        {
            "title": "Acknowledgements",
            "content": "The study was conducted under the state assignment of Lomonosov Moscow State University."
        },
        {
            "title": "References",
            "content": "Jeremy Barnes, Robin Kurtz, Stephan Oepen, Lilja √òvrelid, and Erik Velldal. 2021. Structured sentiment analysis as dependency graph parsing. // Chengqing Zong, Fei Xia, Wenjie Li, and Roberto Navigli, Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics and the 11th International Joint Conference on Natural Language Processing (Volume 1: Long Papers), 33873402, Online, August. Association for Computational Linguistics. Jeremy Barnes, Laura Oberl√§nder, Enrica Troiano, Andrey Kutuzov, Jan Buchmann, Rodrigo Agerri, Lilja √òvrelid, // Proceedings of the 16th and Erik Velldal. 2022. Semeval 2022 task 10: Structured sentiment analysis. International Workshop on Semantic Evaluation (SemEval-2022), 12801295. Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. 2020. Language models are fewshot learners. Stanislav Chumakov, Anton Kovantsev, and Anatoliy Surikov. 2023. Generative approach to aspect based sentiment analysis with gpt language models. Procedia Computer Science, 229:284293. Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. 2023. Qlora: efficient finetuning of quantized llms. // Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA. Curran Associates Inc. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. // Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), 41714186, Minneapolis, Minnesota, June. Association for Computational Linguistics. Timothy Dozat and Christopher Manning. 2022. Deep biaffine attention for neural dependency parsing. // International Conference on Learning Representations. Aaron Grattafiori et. al. 2024. The llama 3 herd of models. Anton Golubev and Natalia Loukachevitch. 2020. // Artificial Intelligence and Natural Language: 9th Conference, AINL 2020, Helsinki, Finland, October 79, 2020, Proceedings 9, 109121. Springer. Improving results on russian sentiment datasets. Anton Golubev, Nicolay Rusnachenko, and Natalia Loukachevitch. 2023. Rusentne-2023: evaluating entityoriented sentiment analysis on russian news texts. // Computational Linguistics and Intellectual Technologies: papers from the Annual conference Dialogue. Hochreiter. 1997. Long short-term memory. Neural Computation MIT-Press. Yan Cathy Hua, Paul Denny, J√∂rg Wicker, and Katerina Taskova. 2024. systematic review of aspect-based sentiment analysis: domains, methods, and trends. Artificial Intelligence Review, 57(11):296. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: robustly optimized bert pretraining approach. Bing Liu. 2012. Sentiment Analysis and Opinion Mining. Morgan & Claypool Publishers. Natalia Loukachevitch and Yuliya Rubtsova. 2015. Entity-oriented sentiment analysis of tweets: results and // Text, Speech, and Dialogue: 18th International Conference, TSD 2015, Pilsen, Czech Republic, problems. September 14-17, 2015, Proceedings 18, 551559. Springer. Natalia Loukachevitch, Pavel Blinov, Evgeny Kotelnikov, Yulia Rubtsova, Vladimir Ivanov, and Elena Tutubalina. 2015. Sentirueval: testing object-oriented sentiment analysis systems in russian. // Proceedings of International Conference Dialog, volume 2, 313. OpenAI, :, Aaron Hurst, Adam Lerer, and Adam P. Goucher et. al. 2024. Gpt-4o system card. Ekaterina Pronoza, Polina Panicheva, Olessia Koltsova, and Paolo Rosso. 2021. Detecting ethnicity-targeted hate speech in russian social media texts. Information Processing & Management, 58(6):102674. Qwen, :, An Yang, Baosong Yang, and Beichen Zhang et. al. 2025. Qwen2.5 technical report. Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. 2020. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167. Nicolay Rusnachenko, Natalia Loukachevitch, and Elena Tutubalina. 2019. Distant supervision for sentiment // Proceedings of the International Conference on Recent Advances in Natural Language attitude extraction. Processing (RANLP 2019), 10221030. Nicolay Rusnachenko, Anton Golubev, and Natalia Loukachevitch. 2024. Large language models in targeted sentiment analysis for russian. Lobachevskii Journal of Mathematics, 45(7):31483158. Mike Schuster and Kuldip Paliwal. 1997. Bidirectional recurrent neural networks. IEEE transactions on Signal Processing, 45(11):26732681. Mikhail Tikhomirov and Daniil Chernyshov. 2024. Facilitating large language model russian adaptation with learned embedding propagation. Journal of Language and Education, 10(4):130145. Wenxuan Zhang, Yue Deng, Bing Liu, Sinno Pan, and Lidong Bing. 2024. Sentiment analysis in the era of large language models: reality check. // Findings of the Association for Computational Linguistics: NAACL 2024, 38813906. Appendix A: Prompt for Baseline Method Original Text of the prompt (Russian) –ø–æ–ª–æ–∂–∏—Ç–µ–ª—å–Ω–∞—è/–Ω–µ–≥–∞—Ç–∏–≤–Ω–∞—è –¢–≤–æ—è –∑–∞–¥–∞—á–∞ —Å–æ—Å—Ç–æ–∏—Ç –≤ —Ç–æ–º, —á—Ç–æ–±—ã –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å —Ç–µ–∫—Å—Ç –∏ –∏–∑–≤–ª–µ—á—å –∏–∑ –Ω–µ–≥–æ –≤—ã—Ä–∞–∂–µ–Ω–∏—è –º–Ω–µ–Ω–∏–π, –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–µ –≤ –≤–∏–¥–µ –∫–æ—Ä—Ç–µ–∂–∞ –º–Ω–µ–Ω–∏–π, —Å–æ—Å—Ç–æ—è—â–∏—Ö –∏–∑ 4 –æ—Å–Ω–æ–≤–Ω—ã—Ö —Å–æ—Å—Ç–∞–≤–ª—è—é—â–∏—Ö: 1. –ò—Å—Ç–æ—á–Ω–∏–∫ –º–Ω–µ–Ω–∏—è: –∞–≤—Ç–æ—Ä, –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å —Ç–µ–∫—Å—Ç–∞ (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞), –ª–∏–±–æ NULL. Key = Source; 2. –û–±—ä–µ–∫—Ç –º–Ω–µ–Ω–∏—è: –∏–º–µ–Ω–æ–≤–∞–Ω–Ω–∞—è —Å—É—â–Ω–æ—Å—Ç—å –≤ —Ç–µ–∫—Å—Ç–µ (–ø–æ–¥—Å—Ç—Ä–æ–∫–∞ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞). Key = Target; 3. –¢–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å: (POS/NEG). Key = Polarity; 4. –Ø–∑—ã–∫–æ–≤–æ–µ –≤—ã—Ä–∞–∂–µ–Ω–∏–µ: –∞—Ä–≥—É–º–µ–Ω—Ç, –Ω–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –∫–æ—Ç–æ—Ä–æ–≥–æ –ø—Ä–∏–Ω—è—Ç–∞ —Ä–µ–∑—É–ª—å—Ç–∏—Ä—É—é—â–∞—è —Ç–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å (–æ–¥–Ω–∞ –∏–ª–∏ –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø–æ–¥—Å—Ç—Ä–æ–∫ –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞). Key = Expression; –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫ –º–Ω–µ–Ω–∏—è –æ—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç, —Ç–æ Source = NULL. –ï—Å–ª–∏ –∏—Å—Ç–æ—á–Ω–∏–∫–æ–º –º–Ω–µ–Ω–∏—è —è–≤–ª—è–µ—Ç—Å—è –∞–≤—Ç–æ—Ä, —Ç–æ Source = AUTHOR. –í –ø—Ä–æ—á–∏—Ö —Å–ª—É—á–∞—è—Ö –ø–æ–ª–µ Source –¥–æ–ª–∂–Ω–æ –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–æ–≤–ø–∞–¥–∞—Ç—å —Å –ø–æ–¥—Å—Ç—Ä–æ–∫–æ–π –∏—Å—Ö–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞. –ü–æ–ª—è Target, Expression –≤—Å–µ–≥–¥–∞ —Å–æ–≤–ø–∞–¥–∞—é—Ç —Å –ø–æ–¥—Å—Ç—Ä–æ–∫–∞–º–∏ —Ç–µ–∫—Å—Ç–∞. –û—Ç–≤–µ—Ç –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–¥—Å—Ç–∞–≤–∏—Ç—å –≤ –≤–∏–¥–µ json —Å–ø–∏—Å–∫–∞, –∫–∞–∂–¥—ã–π —ç–ª–µ–º–µ–Ω—Ç –∫–æ—Ç–æ—Ä–æ–≥–æ —è–≤–ª—è–µ—Ç—Å—è –∫–æ—Ä—Ç–µ–∂–µ–º –º–Ω–µ–Ω–∏–π. –ö–∞–∂–¥—ã–π –∫–æ—Ä—Ç–µ–∂ –º–Ω–µ–Ω–∏–π —ç—Ç–æ —Å–ª–æ–≤–∞—Ä—å, —Å–æ—Å—Ç–æ—è—â–∏–π –∏–∑ —á–µ—Ç—ã—Ä–µ—Ö –∑–Ω–∞—á–µ–Ω–∏–π: Source, Target, Polarity, Expression. –î–ª—è –∏–∑–≤–ª–µ—á—ë–Ω–Ω—ã—Ö Source, Target, Polarity, Expression –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å —Å–ø—Ä–∞–≤–µ–¥–ª–∏–≤–æ —É—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ: –ù–∞ –æ—Å–Ω–æ–≤–∞–Ω–∏–∏ –≤—ã—Ä–∞–∂–µ–Ω–∏—è Expression –º–æ–∂–Ω–æ —Å–∫–∞–∑–∞—Ç—å, —á—Ç–æ Source –∏–º–µ–µ—Ç Polarity –æ—Ç–Ω–æ—à–µ–Ω–∏–µ –∫ Target. –ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã –ø—Ä–∏–º–µ—Ä—ã –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è –∑–∞–¥–∞—á–∏: ***–¢–µ–∫—Å—Ç*** –ü—Ä–µ–º—å–µ—Ä-–º–∏–Ω–∏—Å—Ç—Ä –ú–æ–ª–¥–æ–≤—ã –æ—Å—É–¥–∏–ª —Ç–µ—Ä—Ä–æ—Ä–∏—Å—Ç–∞ –∑–∞ –±–µ—Å—á–ª–æ–≤–µ—á–Ω—ã–µ –∏ –∂–µ—Å—Ç–æ–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è. Source: –ü—Ä–µ–º—å–µ—Ä-–º–∏–Ω–∏—Å—Ç—Ä –ú–æ–ª–¥–æ–≤—ã, Target: —Ç–µ—Ä—Ä–æ—Ä–∏—Å—Ç–∞, Polarity: NEG, Expression: –±–µ—Å—á–ª–æ–≤–µ—á–Ω—ã–µ –∏ –∂–µ—Å—Ç–æ–∫–∏–µ –¥–µ–π—Å—Ç–≤–∏—è ***–¢–µ–∫—Å—Ç*** –ó–Ω–∞–º–µ–Ω–∏—Ç–∞—è –∞–∫—Ç—Ä–∏—Å–∞ –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ —á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ—Å—Ç–æ—Ç—É, –¥–æ—Å—Ç–æ–π–Ω—É—é —É–≤–∞–∂–µ–Ω–∏—è –ø—É–±–ª–∏–∫–∏. ***–û—Ç–≤–µ—Ç*** Source: AUTHOR, Target: –∞–∫—Ç—Ä–∏—Å–∞, Polarity: POS, Expression: –ø—Ä–æ–¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä–æ–≤–∞–ª–∞ —á–µ–ª–æ–≤–µ—á–Ω–æ—Å—Ç—å –∏ –ø—Ä–æ—Å—Ç–æ—Ç—É, –¥–æ—Å—Ç–æ–π–Ω—É—é —É–≤–∞–∂–µ–Ω–∏—è –ø—É–±–ª–∏–∫–∏ –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–∞–∫–∏–º –∂–µ –æ–±—Ä–∞–∑–æ–º —Å–ª–µ–¥—É—é—â–∏–π —Ç–µ–∫—Å—Ç. ***–¢–µ–∫—Å—Ç*** {text} the author, named entity in the text (a Translated version of the prompt (English) Your task is to analyze the text and extract from it expressions of opinion, presented as tuple of opinions consisting of 4 main components: 1. Opinion source: substring of the source text), or NULL. Key = Source; 2. Opinion object: named entity in the text (a substring of the source text). Key = Target; 3. Sentiment: positive/negative (POS/NEG). Key = Polarity; 4. Language expression: the argument on the basis of which the resulting sentiment is accepted (one or more substrings of the source text). Key = Expression; If the opinion source is missing, then Source = NULL. If the opinion source is the author, then Source = AUTHOR. In other cases, the Source field must completely match the substring of the source text. The Target, Expression fields always match the substrings text. The answer must be presented as json list, each element of which is tuple of opinions. Each tuple of opinions is dictionary consisting of four values: Source, Target, Polarity, Expression. For the extracted Source, Target, Polarity, Expression, the statement \"\"Based on the Expression, it can be said that Source has Polarity relation to Target\"\" must be true. Below are examples of completing the task: ***Text*** The Prime Minister of Moldova condemned the terrorist for inhumane and cruel actions. Source: Prime Minister of Moldova, Target: terrorist, Polarity: NEG, Expression: inhumane and cruel actions ***Text*** The famous actress demonstrated humanity and simplicity worthy of public respect. ***Answer*** Source: AUTHOR, Target: actress, Polarity: POS, Expression: demonstrated humanity and simplicity worthy of public respect Analyze the following text in the same way. ***Text*** {text} Table 8: Textual prompt of the baseline submission; {text} refers to the input parameter of the sentence"
        }
    ],
    "affiliations": [
        "Bauman Moscow State Technical University",
        "Lomonosov Moscow State University"
    ]
}