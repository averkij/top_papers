{
    "paper_title": "EgoTwin: Dreaming Body and View in First Person",
    "authors": [
        "Jingqiao Xiu",
        "Fangzhou Hong",
        "Yicong Li",
        "Mengze Li",
        "Wentao Wang",
        "Sirui Han",
        "Liang Pan",
        "Ziwei Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While exocentric video synthesis has achieved great progress, egocentric video generation remains largely underexplored, which requires modeling first-person view content along with camera motion patterns induced by the wearer's body movements. To bridge this gap, we introduce a novel task of joint egocentric video and human motion generation, characterized by two key challenges: 1) Viewpoint Alignment: the camera trajectory in the generated video must accurately align with the head trajectory derived from human motion; 2) Causal Interplay: the synthesized human motion must causally align with the observed visual dynamics across adjacent video frames. To address these challenges, we propose EgoTwin, a joint video-motion generation framework built on the diffusion transformer architecture. Specifically, EgoTwin introduces a head-centric motion representation that anchors the human motion to the head joint and incorporates a cybernetics-inspired interaction mechanism that explicitly captures the causal interplay between video and motion within attention operations. For comprehensive evaluation, we curate a large-scale real-world dataset of synchronized text-video-motion triplets and design novel metrics to assess video-motion consistency. Extensive experiments demonstrate the effectiveness of the EgoTwin framework."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 1 ] . [ 1 3 1 0 3 1 . 8 0 5 2 : r EgoTwin: Dreaming Body and View in First Person Jingqiao Xiu1 Wentao Wang4 Fangzhou Hong2 Yicong Li1 Mengze Li3 Sirui Han3 Liang Pan4 Ziwei Liu2 1National University of Singapore 2Nanyang Technological University 3Hong Kong University of Science and Technology 4Shanghai AI Laboratory https://egotwin.pages.dev/ Figure 1: We propose EgoTwin, diffusion-based framework that jointly generates egocentric video and human motion in viewpoint consistent and causally coherent manner. Generated videos can be lifted into 3D scenes using camera poses derived from human motion via 3D Gaussian Splatting [28]."
        },
        {
            "title": "Abstract",
            "content": "While exocentric video synthesis has achieved great progress, egocentric video generation remains largely underexplored, which requires modeling first-person view content along with camera motion patterns induced by the wearers body movements. To bridge this gap, we introduce novel task of joint egocentric video and human motion generation, characterized by two key challenges: 1) Viewpoint Alignment: the camera trajectory in the generated video must accurately align with the head trajectory derived from human motion; 2) Causal Interplay: the synthesized human motion must causally align with the observed visual dynamics across adjacent video frames. To address these challenges, we propose EgoTwin, joint video-motion generation framework built on the diffusion transformer architecture. Specifically, EgoTwin introduces head-centric motion representation that anchors the human motion to the head joint and incorporates cybernetics-inspired interaction mechanism that explicitly captures the causal interplay between video and motion within attention operations. For comprehensive evaluation, we curate large-scale real-world dataset of synchronized text-video-motion triplets and design novel metrics to assess video-motion consistency. Extensive experiments demonstrate the effectiveness of the EgoTwin framework. Preprint. Under review."
        },
        {
            "title": "1\nRecent advances in deep generative models have delivered remarkable progress in exocentric\n(third-person) video generation [5, 58, 7, 60], demonstrating the ability to produce photorealis-\ntic and temporally consistent videos from natural language or other conditioning signals. However,\negocentric (first-person) video synthesis remains largely underexplored, despite its increasing im-\nportance for wearable computing [12], augmented reality [3], and embodied agents [34]. In contrast\nto exocentric setups, where the camera is static or externally controlled [56, 19], egocentric video\ncaptures the perspective of a moving individual, with the footage inherently entangled with the camera\nwearer’s motion. In particular, head movements influence the camera’s position and orientation, while\nfull-body actions affect the wearer’s body pose and the surrounding scene, collectively shaping the\negocentric recording. Therefore, to model body-driven dynamics in egocentric views, we argue that\nthe visual stream must be generated in lockstep with the motion stream that drives it.",
            "content": "In this paper, we introduce novel task of joint video-motion generation that explicitly models egocentric video together with the motion of the camera wearer. As illustrated in Figure 1, given static human pose and an initial scene observation, our goal is to generate synchronized sequences of egocentric video and human motion, guided by the textual description. This task introduces two fundamental challenges beyond prior works: (1) Viewpoint Alignment. Throughout the sequence, the camera trajectory captured in egocentric video must precisely align with the head trajectory derived from human motion. This requirement naturally stems from the fact that the camera is rigidly mounted on the wearers head [10, 2], causing head movement and camera motion to be tightly coupled. However, existing exocentric video generation methods typically employ unidirectional viewpoint-conditioning strategy that synthesizes video based on predefined camera poses [56, 19]. Such approaches are unsuitable for our setting, as the camera poses in egocentric video are not externally provided but are inherently determined by the wearers head motion. As result, the camera poses must be generated concurrently with the human motion, necessitating bidirectional interaction to ensure viewpoint alignment. (2) Causal Interplay. At each time step, the current visual frame provides spatial context that shapes human motion synthesis; conversely, the newly generated motion influences subsequent video frames. Take the opening door scenario in Figure 1 as an example: egocentric observation informs the wearer of the doors location, which guides the wearers action. In turn, the performed action can alter the body pose (e.g., reaching for the doorknob), the camera pose (e.g., orienting toward the door), and the surrounding scene (e.g., the door gradually opening). These changes must be accurately reflected in subsequent video frames, thereby affecting future motion generation. This recursive dependency forms closed observationaction loop between video and motion, highlighting the necessity of modeling their causal interplay over time. To address these challenges, we propose EgoTwin, joint video-motion generation framework that generates egocentric videos with body-induced camera motion patterns while capturing the causal interplay between visual observations and human actions. Specifically, EgoTwin adopts diffusion transformer backbone [37, 11], with three modality-specific branches for text, video, and motion, respectively. To model the joint distribution, EgoTwin employs asynchronous diffusion [4] in video and motion branches, which allows each modality to evolve on its timestep while maintaining crossmodal interaction. To facilitate accurate viewpoint alignment, we depart from the commonly used root-centric motion representation [14], which obscures head pose within full-body motion and thus fails to expose the egocentric perspective to the video branch. Instead, we introduce head-centric motion representation that anchors the human motion to the head joint, allowing for direct alignment between the camera viewpoint of the generated video and the head pose in the synthesized motion. To faithfully capture the causal interplay, we draw inspiration from the observation-action feedback loop in cybernetic systems [1, 35], where observations shape actions and actions alter future observations. We implement this principle through structured interaction mechanism: each video token attends to preceding motion tokens, capturing how current observations arise from past actions, while each motion token attends to current and upcoming video tokens, enabling the inference of actions based on perceived scene transitions. This bidirectional design allows motion-driven video synthesis and video-informed motion synthesis to evolve in synchrony. To foster research in this field, we curate large-scale dataset of real-world egocentric videos with human pose annotations from Nymeria [32]. For evaluation, we extend beyond assessing the individual quality of video and motion, and propose video-motion consistency metrics that quantify their cross-modal alignment. Extensive experiments demonstrate the effectiveness of EgoTwin. In summary, our contributions are fourfold: 2 To the best of our knowledge, we are the first to explore the joint generation of egocentric video and human motion in viewpoint consistent and causally coherent manner. We identify the limitations of conventional root-centric motion representations in egocentric contexts and reformulate head-centric approach that facilitates video-motion alignment. We design triple-branch diffusion transformer featuring video-motion interaction mechanism, supported by an efficient three-stage training paradigm and versatile sampling strategies. We propose video-motion consistency metrics and build benchmark for evaluating joint videomotion generation, where our EgoTwin demonstrates strong performance."
        },
        {
            "title": "2 Related Work",
            "content": "Video Generation. Video generation has witnessed significant advancements with the emergence of video diffusion models [22, 27, 24]. central research focus has been on text-to-video (T2V) generation and image-to-video (I2V) generation, where models synthesize coherent video sequences from textual prompts or static images. Early approaches [5, 6] augment UNet-based text-to-image (T2I) models [44] with temporal modeling layers to efficiently transform them to video generation models. Recent works [7, 60] adopt transformer-based architectures [37], achieving improved temporal consistency and generation quality. To incorporate camera control, representative methods [56, 19] inject camera parameters (e.g., extrinsic matrices or Plücker embeddings [46]) into pretrained video diffusion models [58, 17]. These approaches rely on known camera trajectories and encode them as input conditions. In contrast, our work considers fundamentally different setting where the camera trajectory is not available beforehand, yet the generated video must maintain consistency with other synthesized content that is strongly correlated to the underlying camera motion. This key distinction renders existing methods inapplicable, necessitating framework for controllable video generation that operates without predefined camera guidance. Motion Generation. Generating realistic and diverse human motions from text remains longstanding challenge in computer vision and graphics, offering intuitive control of motion synthesis through natural language. Early works [16, 39, 14, 40] employ temporal VAEs [29] to capture temporal dependencies and learn probabilistic mappings between language and motion. Recent advances have introduced powerful generative modeling techniques to this field, including diffusion models [49, 64, 9], autoregressive models [15, 63, 26], and generative masked models [13, 41, 33]. To comply with these frameworks, motion data is represented in different forms. Diffusion-based methods typically operate on continuous vectors, either in the latent space of VAE or directly from raw motion sequences. Autoregressive models, by contrast, often discretize motion into tokens using vector quantization techniques such as VQ-VAE [51] or RVQ-VAE [30]. Generative masked models are flexible in this regard, accommodating both discrete and continuous representations depending on the loss function and model architecture. Furthermore, several researchers [18, 53, 25, 65] have investigated human motion generation within 3D scenes represented as RGB point clouds. Others combine the above two tasks by simultaneously incorporating textual and scene information [55, 8, 54, 61]. Our work differs from this line of research in how scene information is provided: instead of granting full scene access during motion synthesis, we observe the scene only once from the initial human pose and rely on generative model to hallucinate scene observations as the human moves. Multimodal Generation. Recent advances have expanded generative models from unimodal to multimodal generation. Specifically for diffusion models, [45] introduces the first multimodal diffusion framework for synchronized audio-video generation. Other studies [59, 4] design unified models capable of jointly generating text and images. In the domain of human motion, [31] pioneers the simultaneous generation of motion and frame-level language descriptions that explain the generated motions. Despite these developments, the joint modeling of human motion and its corresponding egocentric views remains largely unexplored. To the best of our knowledge, we take the first step in this direction, uncovering the tight coupling between these two modalities."
        },
        {
            "title": "3 Methodology",
            "content": "Problem Definition. Given an initial human pose 0 RJ3 in scene, an egocentric observation 0 RHW 3 from that pose, and textual description of intended human actions in the scene, our goal is to generate two synchronized sequences: (1) human pose sequence 1:Nm RNmJ3 and 3 Figure 2: Overview. EgoTwin features triple-branch architecture (left), where motion branch spans only the lower half of the layers used by text and video branches. Each branch has its own tokenizer and transformer blocks (right), with shared weights across branches indicated by matching colors. (2) an egocentric view sequence 1:Nv RNvHW 3 spanning the same duration. Here, is the number of human joints, and are the image height and width, Nm and Nv are the number of frames in the pose and view sequences, respectively. This forms closed-loop generation paradigm where video and motion mutually and continuously influence each other throughout the sequence. Framework Overview. An overview of our EgoTwin framework is shown in Figure 2. Text, video, and motion inputs are first encoded using text encoder, video VAE encoder, and motion VAE encoder, respectively. These embeddings are then processed through the corresponding branches of diffusion transformer. Finally, the video and motion outputs are decoded by respective VAE decoders."
        },
        {
            "title": "3.1 Modality Tokenization",
            "content": "For the text and video modalities, we adopt T5-XXL [43] as the text tokenizer and encoder, and 3D causal VAE [60] as the video tokenizer. Specifically, the input text is first tokenized and adjusted to fixed length Lt via truncation or padding, then encoded into text embeddings RLtDt. The video frames are temporally and spatially compressed into latent representations zv R( Nv 8 Cv with compression ratio of 4 8 8 and Cv latent channels, which are subsequently patchified and unfolded into video embeddings Xv RLvDv of sequence length Lv. Dt and Dv denote the embedding dimension of text and video, respectively. 4 +1) 8 Motion Representation. Unlike the uniform representation for text and video, motion representation exhibits great degree of diversity. Currently, the most widely adopted format in human motion generation is the overparameterized canonical pose representation [14], which has become the default standard for popular datasets, including KIT-ML [42] and HumanML3D [14]. Formally, the human pose at each frame is defined as tuple of ( ra, rxz, ry, jp, jv, jr, cf ), comprising seven groups of features: root angular velocity along Y-axis ra, root linear velocities on XZ-plane rxz, root height ry, local joint positions jp R3(J1) and velocities jv R3(J1) in root space, joint rotations jr R6(J1) in local space, and binary foot-ground contacts cf R4. Motions are retargeted to default human skeletal template and initially rotated to face the positive Z-axis. However, the above root-centric representation is not suitable for our task, as the critical information for alignment with egocentric video, such as the pose of the head joint, is deeply buried in an intricate multi-step kinematic calculation. Mathematically, recovering the head joint pose requires integrating root velocities to obtain the root pose, then applying forward kinematics (FK) to propagate transformations through the kinematic chain to the head joint. Intuitively, this computation is too complex to be precisely modeled by neural networks. To validate our insights, we train GRU-based regression model that takes motion representation sequences as input, supervised by an MSE loss against ground-truth head pose sequences. As shown in Figure 3, both translation and rotation errors (TransErr and RotErr, see Section 4.1 for details) plateau at high levels due to insufficient explicit cues for accurately modeling head pose. Figure 3: Head pose regression errors over epochs. Figure 4: Interaction mechanism. To address this issue, we propose head-centric motion representation that explicitly exposes egocentric information. Specifically, we define the representation as tuple (hr, hr, hp, hp, jp, jv, jr), where hr R6 and hr R6 are the absolute and relative rotation of the head joint, hp R3 and hp R3 are the absolute and relative position of the head joint. The terms jp and jv are now expressed in head space, while jr retains its original meaning. Additionally, we normalize the initial head pose to zero translation and identity rotation, and set all first-order kinematic features to zero in the initial frame. Our representation naturally resonates with egocentric video in at least two novel ways: 1) It offers more accurate access to the head trajectory, which closely correlates with camera movement; 2) It more clearly informs the egocentric video how the body is observed egocentrically. Motion Tokenization. Inspired by the Causal 3D CNN [62], we build the motion VAE using 1D causal convolutions, where all padding is applied at the beginning of the convolutional axis. The encoder and decoder are symmetrically structured, each comprising two stages of 2 downsampling or upsampling, interleaved with ResNet blocks [20]. The motion VAE is trained using combination of reconstruction loss Lrec and KullbackLeibler (KL) divergence regularization LKL weighted by λKL. To ensure that loss contributions are balanced across different groups regardless of their dimensions, we compute the VAE loss LVAE separately for the 3D head (hp, hp), 6D head (hr, hr), 3D joint (jp, jv), and 6D joint (jr) components. The final loss averages these four items: LVAE = 1 (cid:88) (cid:16) rec + λKLL(c) L(c) KL (cid:17) , where {head3D, head6D, joint3D, joint6D} . (1) Using the trained VAE, motion representations are tokenized into latents Zm R( Nm 4 +1)Cm with 4 downsampling rate and Cm channels, and subsequently transformed into motion embeddings Xm RLmDm, with Lm as the sequence length and Dm as the embedding dimension."
        },
        {
            "title": "3.2 Diffusion Transformer",
            "content": "Our diffusion transformer extends MM-DiT [11], initially designed for text-to-image generation, to support text, video, and motion modalities. As illustrated in Figure 2, each branch consists of sequence of MLPs and applies adaptive layer normalization (AdaLN) in conjunction with gating mechanism [37] to incorporate timestep information. The text and video branches are initialized from CogVideoX [60], with shared weights except for the AdaLNs. The motion branch corresponds to only the lower half of the layers in other branches, as essential visual cues for video-motion interaction, such as camera pose and scene structure, are primarily captured in the early layers of the video diffusion backbone. In contrast, the higher layers specialize in appearance details, which are less relevant to motion. To further improve efficiency, the motion branch employs reduced channel dimensions, consistent with the lower representational complexity of motion relative to video. The embedding sequences from different modalities are projected to common dimensionality and concatenated for joint attention operations [52]. This triple-branch architecture allows each modality to work in its own representational space while still attending to and interacting with the others. Interaction Mechanism. The original MM-DiT framework includes only text and image modalities, where cross-modal consistency is enforced only at the global level, i.e., matching the entire image with 5 the entire text suffices. However, our task demands fine-grained temporal synchronization between video and motion: each video frame must be temporally aligned with the corresponding motion frame. Although we incorporate sinusoidal positional encodings [52] for both video and motion tokens, along with 3D rotary position embeddings (RoPE) [47] for video tokens to provide absolute and relative position information, these mechanisms primarily capture intra-modal structure. Consequently, the inter-modal correspondence at each time step remains implicit to the diffusion transformer, which may lead to globally consistent outputs that nevertheless lack frame-wise synchronization. To address this challenge, we explicitly encode the causal interplay between video and motion by introducing structured joint attention mask to the diffusion transformer. Given that human motion is typically captured at higher temporal resolution than egocentric video, we set the number of motion tokens to be twice the number of video tokens (i.e., Nm = 2Nv), without loss of generality. Formally, we follow the notations in Cybernetics [1, 35] to rewrite as the observation Oi, and (P 2i+1, 2i+2) as the (chunked) action Ai, where [0, Nv 1]. According to the principles of forward dynamics: {Oi, Ai} Oi+1 and inverse dynamics: {Oi, Oi+1} Ai, video tokens corresponding to Oi can attend to motion tokens that correspond to Ai1, capturing how Oi arise from Ai1, while motion tokens corresponding to Ai can attend to video tokens that correspond to both Oi and Oi+1, enabling the inference of Ai based on scene transitions from Oi to Oi+1. special case is given to 0, which is allowed bilateral attention with 0. As demonstrated in Figure 4, apart from the aforementioned relationship, the remaining attention between video and motion is blocked, while all intra-modal attention, as well as inter-modal attention related to text, are preserved. Asynchronous Diffusion. We independently sample two timesteps, tv and tm, between 0 and (maximum timestep), and add Gaussian noises ϵv and ϵm associated with these timesteps to the latents zv and zm, respectively. Each timestep is first encoded via sinusoidal embedding, and an MLP then processes two concatenated embeddings to produce unified timestep embedding y, which serves as input to the AdaLN layers. Our model consists of video denoiser ϵv , c, tv, tm) and motion denoiser ϵm , c, tm, tv), which are jointly optimized to simultaneously predict the noises added to the video and motion latents using the following objective: , ztv , ztm LDiT = Eϵv,ϵm,c,tv,tm 2 + (cid:13) , c, tv, tm)(cid:13) 2 (cid:13) , c, tm, tv)(cid:13) 2 (cid:13) 2 (cid:104)(cid:13) (cid:13)ϵv ϵv (cid:13)ϵm ϵm , ztm , ztv θ (ztm θ (ztm θ(ztv θ(ztv (2) (cid:105) ."
        },
        {
            "title": "3.3 Training and Sampling",
            "content": "Training Paradigm. Our training schema comprises three stages: 1) Motion VAE Training, as described in Equation (1). 2) Text-to-Motion Pretraining. Since the motion branch lacks pretrained weights for initialization, we pretrain it on the text-to-motion task using only text and motion embeddings as input, while keeping the text branch frozen. Following classifier-free guidance (CFG) [23], we randomly discard the text embeddings with probability of 10% to model unconditional motion generation. By omitting the much longer video embeddings at this stage, we can leverage greater parallelism, which accelerates the training process. Critically, freezing the text branch not only preserves the pretrained text-to-video weights but also facilitates the integration of motion embeddings into the pretrained text-video embedding space. 3) Joint Text-Video-Motion Training, as formulated in Equation (2). Video embeddings are incorporated in this final stage, and the model learns the joint distribution of video and motion conditioned on text. Again, text embeddings are randomly dropped with probability of 10% to model the unconditional video-motion generation. Sampling Strategy. Benefiting from the joint distribution modeling, our framework supports not only joint video-motion generation conditioned on text (T2VM), but also unimodal generation, including video generation conditioned on text and motion (TM2V), and motion generation conditioned on text and video (TV2M). The CFG for TM2V sampling is defined as follows: θ(zt ˆϵv v, z0 m, c, t, 0) = ϵv θ(zt v, zT m, ϕ, t, )+wt +wm (cid:0)ϵv θ(zt (cid:0)ϵv θ(zt v, zT v, z0 m, c, t, ) ϵv m, c, t, 0) ϵv θ(zt θ(zt v, zT v, zT m, ϕ, t, )(cid:1) m, c, t, )(cid:1) . (3) The CFG formula for TV2M sampling can be derived by exchanging the roles of and in Equation (3). Here, wt, wv, and wm denote the guidance scales for text, video, and motion conditions, respectively. For T2VM sampling, taking the motion branch as an example (with the video branch being analogous), its CFG formula is expressed as: , ϕ, t, )+wt +wv , c, t, ) ϵm v, c, t, t) ϵm , ϕ, t, )(cid:1) , c, t, )(cid:1) . v, c, t, t) = ϵm m, zT m, zT θ (zt θ (zt m, zT m, zt θ (zt θ (zt (cid:0)ϵm (cid:0)ϵm θ (zt ˆϵm m, zT m, zt θ (zt (4) 6 After sampling, latents from the video branch are unpatchified to recover their original shape and then decoded by the 3D causal VAE decoder [60] to reconstruct the video, while latents from the motion branch are passed through the learned motion VAE decoder to reconstruct the motion."
        },
        {
            "title": "4.1 Evaluation Metrics",
            "content": "Video Quality. We adopt Image Fréchet Inception Distance (I-FID) [21] to evaluate the visual fidelity and realism of individual frames by measuring the distributional distance between the features of generated frames and those of real images. At the video level, we employ Fréchet Video Distance (FVD) [50] to quantify temporal coherence and consistency across generated video sequences compared to real ones. Additionally, CLIP Similarity (CLIP-SIM) [57] is utilized to assess the semantic alignment and contextual relevance between generated video clips and textual prompts. Motion Quality. We choose Motion Fréchet Inception Distance (M-FID) [21] to assess the statistical similarity between the high-level features of generated motions and real motions. To evaluate the alignment between text and motion, we train GRU-based text feature extractor and GRU-based motion feature extractor, both sharing the same architecture as the evaluator in [14]. These models are optimized using contrastive loss on GloVe [38] text embeddings and our motion representation described in Section 3.1, ensuring that matched text-motion pairs yield geometrically close feature vectors. Within this learned feature space, the text-to-motion Retrieval Precision (R-Prec) is measured in terms of Top-3 retrieval accuracy. Meanwhile, the Multimodal Distance (MM-Dist) captures the average Euclidean distance between corresponding motion and text features. Video-Motion Consistency. We propose to evaluate the consistency between generated egocentric videos and human motions from two aspects: 1) View Consistency: We first estimate the frame-wise camera poses of the generated egocentric videos using DROID-SLAM [48] and extract the head joint poses from the generated human motions. Then, we align both trajectories at the first frame and apply Procrustes Analysis to determine the optimal scale factor that aligns the estimated camera trajectory with the extracted head trajectory. Finally, we compute the Translation Error (TransErr) as the average Euclidean distance between the corresponding camera and head positions, and the Rotation Error (RotErr) as the average angular difference between the corresponding camera and head orientations, using the same formulas as [19]. 2) Hand Consistency: We detect the presence of the left and right hands, equipped with the motion capture device, in the generated egocentric videos. For the generated human motions, we compute the hand visibility from the perspective of virtual camera mounted on the corresponding head joint with known intrinsics. Based on the presence and visibility analysis, we define the Hand F-Score (HandScore) as the average F-Score of left and right hands, where True Positive means the hand is present in the video and visible from the head in motion, False Positive means the hand is present in the video but invisible from the head in motion, and False Negative means the hand is absent in the video but visible from the head in motion."
        },
        {
            "title": "4.2 Experimental Setup",
            "content": "Dataset. To overcome the limitations of using synthetic or small-scale real-world datasets for evaluation, we train and evaluate our model on Nymeria [32], large-scale, real-device dataset that captures diverse people engaged in wide range of daily activities across various indoor and outdoor locations. The dataset provides paired text-video-motion data, including egocentric videos recorded with Project Aria glasses [10], full-body motions captured using the Xsens inertial motion capture system [36], and motion narrations written by human annotators. All data are segmented into 5-second clips, yielding approximately 170K samples after filtering, which are split into training, validation, and test sets for the joint training stage. We ensure that both the individuals and environments in the test split remain unseen during joint training. Baseline. Since no prior methods are capable of addressing our task, we propose simple yet effective baseline, VidMLD, that retains the architecture of EgoTwin while removing all dedicated designs introduced in Section 3.1 and Section 3.2. In other words, VidMLD combines the state-of-the-art video diffusion model CogVideoX [60] and the latent-space motion diffusion model MLD [9], both of which excel in unimodal generation, and connects them through the multimodal diffusion architecture MM-DiT [11] to enable joint generation. We adopt the same three-stage training recipe described in Section 3.3, and employ the original classifier-free guidance [23] for sampling. 7 Table 1: Quantitative results of joint video and motion generation, evaluated by metrics covering video quality, motion quality, and video-motion consistency. Method Video Quality Motion Quality Video-Motion Consistency I-FID FVD CLIP-SIM M-FID R-Prec MM-Dist TransErr RotErr HandScore VidMLD 157.86 1547.28 EgoTwin 98.17 1033.52 25.58 27.34 45.09 41. 0.47 0.62 19.12 15.05 1.28 0.67 1.53 0.46 0.36 0.81 Figure 5: Qualitative results of joint video and motion generation, based on textual prompt and initial frames of both video and motion. Implementation Details. In our experiments, videos are undistorted and resized to resolution of = = 480, with each segment containing Nv + 1 = 41 frames at 8 FPS. The motion data adopts the Xsens skeleton with = 23 joints and consists of Nm + 1 = 81 frames per segment at 16 FPS. The video and motion latents have Cv = 16 and Cm = 64 channels, respectively. The embedding lengths for text, video, and motion are Lt = 226, Lv = 9900, and Lm = 21, with corresponding dimensions Dt = Dv = = 3072, and Dm = 768. The hyperparameter λKL in Equation (1) is set to 1e-4. CFG scales are set to wt = 6 for text and wv = wm = 4 for video and motion. The text and video branches have 42 layers, totaling approximately 5B parameters, with most shared across both branches. The motion branch comprises 21 layers, corresponding to the lower halves of the other two branches, and contains roughly 300M parameters."
        },
        {
            "title": "4.3 Main Results",
            "content": "Quantitative Results. As shown in Table 1, EgoTwin significantly outperforms the baseline method across all evaluation metrics, with especially pronounced improvements in video-motion consistency scores. The brute-force joint training of the video and motion generation models leads to poor alignment between the two modalities, resulting in notably lower video-motion consistency performance. In contrast, EgoTwin effectively captures the intrinsic correlation between the two modalities, achieving not only excellent cross-modal consistency but also enhanced single-modal generation quality through the mutually beneficial interaction between video and motion modalities. Qualitative Results. We also visualize several examples generated by EgoTwin in Figure 5. These samples illustrate that both the video and motion streams not only adhere to the textual descriptions for single-modal generation but also evolve in strict cross-modal synchrony, particularly in terms of camera viewpoint and head pose, as well as in scene content and human action. We encourage readers to visit our project page1 for richer generation examples."
        },
        {
            "title": "4.4 Ablation Studies",
            "content": "We present the results of extensive ablation studies in Table 2, where each row corresponds to specific ablation setting. All variants exhibit consistent performance decline across all metrics compared to our full model (listed at the bottom), confirming the effectiveness of each design. First, we replace our Motion Reformulation with the standard representation [14] commonly used in human motion generation research (w/o MR). The resulting performance drop highlights the importance of our reformulation in exposing egocentric motion cues to the video, which fundamentally facilitates the 1https://egotwin.pages.dev/ 8 Table 2: Ablation results on three designs: Motion Reformulation (MR), Interaction Mechanism (IM), and Asynchronous Diffusion (AD). Variant Video Quality Motion Quality Video-Motion Consistency I-FID FVD CLIP-SIM M-FID R-Prec MM-Dist TransErr RotErr HandScore w/o MR 134.27 1356.81 w/o IM 117.54 1237.58 w/o AD 109.73 1124.19 EgoTwin 98.17 1033.52 26.36 27.10 26.91 27.34 43.65 44.01 42.58 41. 0.56 0.59 0.53 0.62 17.31 15.87 16.48 15.05 0.96 0.85 0.74 0.67 1.22 0.89 0.62 0.46 0.44 0.57 0.73 0.81 Figure 6: Results of conditional generation. Left: motion generation conditioned on text and video; Right: video generation conditioned on text and motion. alignment between egocentric video and human motion. Next, we remove the Interaction Mechanism from the joint attention operations and instead apply full attention without masking (w/o IM). The observed degradation underscores its critical role in capturing causal relationships between video and motion, as well as ensuring fine-grained temporal synchronization. Finally, we substitute the Asynchronous Diffusion with synchronous counterpart for video and motion latents, and accordingly simplify the sampling algorithm to vanilla CFG (w/o AD). The performance decline validates its value for modeling comprehensive and diverse dependencies between video and motion modalities, and enabling precise textual control over the joint generation process."
        },
        {
            "title": "4.5 Broader Applications",
            "content": "Conditional Generation. Our learned joint distribution enables conditional sampling of one modality given another, using the CFG algorithm described in Equation (3). As shown in Figure 6, we can generate human motion conditioned on text and egocentric video (left), as well as generate egocentric video conditioned on text and human motion (right). Interestingly, textual descriptions are often ambiguous (e.g., they may refer to cabinets on the left or right side of the scene in Figure 6), the ability to additionally condition on either motion or video provides greater control over the generation process, which further substantiates the strong coupling between video and motion in our model. Scene Reconstruction. With jointly generated video and motion, we can effortlessly extract camera poses from human motion and directly integrate both modalities into 3D Gaussian Splatting [28] pipeline. As illustrated in Figure 1, we reconstruct the 3D scene from the generated video and seamlessly position the synthesized human into it by aligning head poses with camera trajectories. The realistic spatial interactions exhibited, such as the feet on the ground and the right hand near the door handle, demonstrate strong spatiotemporal coherence between the generated video and motion."
        },
        {
            "title": "5 Conclusion",
            "content": "We propose EgoTwin, diffusion-based framework that jointly generates egocentric video and human motion in viewpoint consistent and causally coherent manner. Our method introduces head-centric motion representation and cybernetics-inspired interaction mechanism, supported by an efficient three-stage training paradigm and versatile sampling strategies. To evaluate our approach, we establish comprehensive benchmark that includes large-scale dataset of text-video-motion triplets and novel videomotion consistency metrics. Experiments demonstrate that EgoTwin delivers promising results. We hope our work encourages further exploration of joint generative modeling for egocentric video and human motion, and lays solid foundation for future research in this area."
        },
        {
            "title": "References",
            "content": "[1] P. Agrawal, A. V. Nair, P. Abbeel, J. Malik, and S. Levine. Learning to poke by poking: Experiential learning of intuitive physics. Advances in Neural Information Processing Systems, 29, 2016. [2] Apple Inc. Apple vision pro, 2023. [3] N. Ashtari, A. Bunt, J. McGrenere, M. Nebeling, and P. K. Chilana. Creating augmented and virtual reality applications: Current practices, challenges, and opportunities. In ACM CHI Conference on Human Factors in Computing Systems, pages 113, 2020. [4] F. Bao, S. Nie, K. Xue, C. Li, S. Pu, Y. Wang, G. Yue, Y. Cao, H. Su, and J. Zhu. One transformer fits all distributions in multi-modal diffusion at scale. In International Conference on Machine Learning, pages 16921717, 2023. [5] A. Blattmann, T. Dockhorn, S. Kulal, D. Mendelevitch, M. Kilian, D. Lorenz, Y. Levi, Z. English, V. Voleti, A. Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [6] A. Blattmann, R. Rombach, H. Ling, T. Dockhorn, S. W. Kim, S. Fidler, and K. Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2256322575, 2023. [7] T. Brooks, B. Peebles, C. Holmes, W. DePue, Y. Guo, L. Jing, D. Schnurr, J. Taylor, T. Luhman, E. Luhman, C. Ng, R. Wang, and A. Ramesh. Video generation models as world simulators. 2024. [8] Z. Cen, H. Pi, S. Peng, Z. Shen, M. Yang, S. Zhu, H. Bao, and X. Zhou. Generating human motion in 3d scenes from text descriptions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 18551866, 2024. [9] X. Chen, B. Jiang, W. Liu, Z. Huang, B. Fu, T. Chen, and G. Yu. Executing your commands via motion diffusion in latent space. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1800018010, 2023. [10] J. Engel, K. Somasundaram, M. Goesele, A. Sun, A. Gamino, A. Turner, A. Talattof, A. Yuan, B. Souti, B. Meredith, et al. Project aria: new tool for egocentric multi-modal ai research. arXiv preprint arXiv:2308.13561, 2023. [11] P. Esser, S. Kulal, A. Blattmann, R. Entezari, J. Müller, H. Saini, Y. Levi, D. Lorenz, A. Sauer, F. Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In International Conference on Machine Learning, 2024. [12] A. Fiannaca, I. Apostolopoulous, and E. Folmer. Headlock: wearable navigation aid that helps blind cane users traverse large open spaces. In International ACM SIGACCESS Conference on Computers and Accessibility, pages 1926, 2014. [13] C. Guo, Y. Mu, M. G. Javed, S. Wang, and L. Cheng. Momask: Generative masked modeling of 3d human motions. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19001910, 2024. [14] C. Guo, S. Zou, X. Zuo, S. Wang, W. Ji, X. Li, and L. Cheng. Generating diverse and natural 3d human motions from text. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 51525161, 2022. [15] C. Guo, X. Zuo, S. Wang, and L. Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generation of 3d human motions and texts. In European Conference on Computer Vision, pages 580597. Springer, 2022. [16] C. Guo, X. Zuo, S. Wang, S. Zou, Q. Sun, A. Deng, M. Gong, and L. Cheng. Action2motion: Conditioned generation of 3d human motions. In ACM International Conference on Multimedia, pages 20212029, 2020. [17] Y. Guo, C. Yang, A. Rao, Z. Liang, Y. Wang, Y. Qiao, M. Agrawala, D. Lin, and B. Dai. Animatediff: In International Animate your personalized text-to-image diffusion models without specific tuning. Conference on Learning Representations, 2024. [18] M. Hassan, D. Ceylan, R. Villegas, J. Saito, J. Yang, Y. Zhou, and M. J. Black. Stochastic scene-aware motion prediction. In IEEE/CVF International Conference on Computer Vision, pages 1137411384, 2021. [19] H. He, Y. Xu, Y. Guo, G. Wetzstein, B. Dai, H. Li, and C. Yang. Cameractrl: Enabling camera control for text-to-video generation. In International Conference on Learning Representations, 2025. [20] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In IEEE Conference on Computer Vision and Pattern Recognition, pages 770778, 2016. [21] M. Heusel, H. Ramsauer, T. Unterthiner, B. Nessler, and S. Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in Neural Information Processing Systems, 30, 2017. [22] J. Ho, A. Jain, and P. Abbeel. Denoising diffusion probabilistic models. Advances in Neural Information Processing Systems, 33:68406851, 2020. [23] J. Ho and T. Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [24] J. Ho, T. Salimans, A. Gritsenko, W. Chan, M. Norouzi, and D. J. Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [25] S. Huang, Z. Wang, P. Li, B. Jia, T. Liu, Y. Zhu, W. Liang, and S.-C. Zhu. Diffusion-based generation, optimization, and planning in 3d scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1675016761, 2023. [26] B. Jiang, X. Chen, W. Liu, J. Yu, G. Yu, and T. Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36:2006720079, 2023. [27] T. Karras, M. Aittala, T. Aila, and S. Laine. Elucidating the design space of diffusion-based generative models. Advances in Neural Information Processing Systems, 35:2656526577, 2022. [28] B. Kerbl, G. Kopanas, T. Leimkühler, and G. Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4):1391, 2023. [29] D. P. Kingma, M. Welling, et al. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014. [30] D. Lee, C. Kim, S. Kim, M. Cho, and W.-S. Han. Autoregressive image generation using residual quantization. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. [31] C. Li, J. Chibane, Y. He, N. Pearl, A. Geiger, and G. Pons-Moll. Unimotion: Unifying 3d human motion synthesis and understanding. In International Conference on 3D Vision, 2024. [32] L. Ma, Y. Ye, F. Hong, V. Guzov, Y. Jiang, R. Postyeni, L. Pesqueira, A. Gamino, V. Baiyya, H. J. Kim, et al. Nymeria: massive collection of multimodal egocentric daily motion in the wild. In European Conference on Computer Vision, pages 445465. Springer, 2024. [33] Z. Meng, Y. Xie, X. Peng, Z. Han, and H. Jiang. Rethinking diffusion for text-driven human motion generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2025. [34] S. Nair, A. Rajeswaran, V. Kumar, C. Finn, and A. Gupta. R3m: universal visual representation for robot manipulation. In Conference on Robot Learning, 2022. [35] D. Pathak, P. Agrawal, A. A. Efros, and T. Darrell. Curiosity-driven exploration by self-supervised prediction. In International Conference on Machine Learning, pages 27782787, 2017. [36] M. Paulich, M. Schepers, N. Rudigkeit, and G. Bellusci. Xsens mtw awinda: Miniature wireless inertialmagnetic motion tracker for highly accurate 3d kinematic applications. Xsens: Enschede, The Netherlands, pages 19, 2018. [37] W. Peebles and S. Xie. Scalable diffusion models with transformers. In IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [38] J. Pennington, R. Socher, and C. D. Manning. Glove: Global vectors for word representation. In Conference on Empirical Methods in Natural Language Processing, pages 15321543, 2014. [39] M. Petrovich, M. J. Black, and G. Varol. Action-conditioned 3d human motion synthesis with transformer vae. In IEEE/CVF International Conference on Computer Vision, pages 1098510995, 2021. [40] M. Petrovich, M. J. Black, and G. Varol. Temos: Generating diverse human motions from textual descriptions. In European Conference on Computer Vision, pages 480497, 2022. [41] E. Pinyoanuntapong, P. Wang, M. Lee, and C. Chen. Mmm: Generative masked motion model. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 15461555, 2024. 11 [42] M. Plappert, C. Mandery, and T. Asfour. The kit motion-language dataset. Big Data, 4(4):236252, 2016. [43] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena, Y. Zhou, W. Li, and P. J. Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of Machine Learning Research, 21(140):167, 2020. [44] R. Rombach, A. Blattmann, D. Lorenz, P. Esser, and B. Ommer. High-resolution image synthesis with latent diffusion models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1068410695, 2022. [45] L. Ruan, Y. Ma, H. Yang, H. He, B. Liu, J. Fu, N. J. Yuan, Q. Jin, and B. Guo. Mm-diffusion: Learning multi-modal diffusion models for joint audio and video generation. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1021910228, 2023. [46] V. Sitzmann, S. Rezchikov, B. Freeman, J. Tenenbaum, and F. Durand. Light field networks: Neural scene representations with single-evaluation rendering. Advances in Neural Information Processing Systems, 34:1931319325, 2021. [47] J. Su, M. Ahmed, Y. Lu, S. Pan, W. Bo, and Y. Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [48] Z. Teed and J. Deng. Droid-slam: Deep visual slam for monocular, stereo, and rgb-d cameras. Advances in Neural Information Processing Systems, 34:1655816569, 2021. [49] G. Tevet, S. Raab, B. Gordon, Y. Shafir, D. Cohen-Or, and A. H. Bermano. Human motion diffusion model. In International Conference on Learning Representations, 2023. [50] T. Unterthiner, S. Van Steenkiste, K. Kurach, R. Marinier, M. Michalski, and S. Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. [51] A. Van Den Oord, O. Vinyals, et al. Neural discrete representation learning. Advances in Neural Information Processing Systems, 30, 2017. [52] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin. Attention is all you need. Advances in Neural Information Processing Systems, 30, 2017. [53] J. Wang, H. Xu, J. Xu, S. Liu, and X. Wang. Synthesizing long-term 3d human motion and interaction in 3d scenes. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 94019411, 2021. [54] Z. Wang, Y. Chen, B. Jia, P. Li, J. Zhang, J. Zhang, T. Liu, Y. Zhu, W. Liang, and S. Huang. Move as you say interact as you can: Language-guided human motion generation with scene affordance. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 433444, 2024. [55] Z. Wang, Y. Chen, T. Liu, Y. Zhu, W. Liang, and S. Huang. Humanise: Language-conditioned human motion generation in 3d scenes. Advances in Neural Information Processing Systems, 35:1495914971, 2022. [56] Z. Wang, Z. Yuan, X. Wang, Y. Li, T. Chen, M. Xia, P. Luo, and Y. Shan. Motionctrl: unified and flexible motion controller for video generation. In ACM Special Interest Group on Computer Graphics and Interactive Techniques Conference, pages 111, 2024. [57] C. Wu, L. Huang, Q. Zhang, B. Li, L. Ji, F. Yang, G. Sapiro, and N. Duan. Godiva: Generating open-domain videos from natural descriptions. arXiv preprint arXiv:2104.14806, 2021. [58] J. Xing, M. Xia, Y. Zhang, H. Chen, W. Yu, H. Liu, G. Liu, X. Wang, Y. Shan, and T.-T. Wong. Dynamicrafter: Animating open-domain images with video diffusion priors. In European Conference on Computer Vision, pages 399417. Springer, 2024. [59] X. Xu, Z. Wang, G. Zhang, K. Wang, and H. Shi. Versatile diffusion: Text, images and variations all in one diffusion model. In IEEE/CVF International Conference on Computer Vision, pages 77547765, 2023. [60] Z. Yang, J. Teng, W. Zheng, M. Ding, S. Huang, J. Xu, Y. Yang, W. Hong, X. Zhang, G. Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. In International Conference on Learning Representations, 2025. [61] H. Yi, J. Thies, M. J. Black, X. B. Peng, and D. Rempe. Generating human interaction motions in scenes with text control. In European Conference on Computer Vision, pages 246263. Springer, 2024. 12 [62] L. Yu, J. Lezama, N. B. Gundavarapu, L. Versari, K. Sohn, D. Minnen, Y. Cheng, V. Birodkar, A. Gupta, X. Gu, et al. Language model beats diffusiontokenizer is key to visual generation. In International Conference on Learning Representations, 2024. [63] J. Zhang, Y. Zhang, X. Cun, Y. Zhang, H. Zhao, H. Lu, X. Shen, and Y. Shan. Generating human motion from textual descriptions with discrete representations. In IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1473014740, 2023. [64] M. Zhang, Z. Cai, L. Pan, F. Hong, X. Guo, L. Yang, and Z. Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. IEEE Transactions on Pattern Analysis and Machine Intelligence, 46(6):41154128, 2024. [65] K. Zhao, Y. Zhang, S. Wang, T. Beeler, and S. Tang. Synthesizing diverse human motions in 3d indoor scenes. In IEEE/CVF International Conference on Computer Vision, pages 1473814749, 2023."
        }
    ],
    "affiliations": [
        "Hong Kong University of Science and Technology",
        "Nanyang Technological University",
        "National University of Singapore",
        "Shanghai AI Laboratory"
    ]
}