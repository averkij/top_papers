{
    "paper_title": "M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models",
    "authors": [
        "Junxiong Wang",
        "Wen-Ding Li",
        "Daniele Paliotta",
        "Daniel Ritter",
        "Alexander M. Rush",
        "Tri Dao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce a novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages a distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at a similar scale. We also compare our generation speed with a highly performant general purpose inference engine, vLLM, and observe more than a 3x speedup compared to a same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under a fixed generation time budget using self-consistency voting. Overall, we introduce a hybrid Mamba reasoning model and provide a more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 1 ] . [ 1 9 4 4 0 1 . 4 0 5 2 : r Preprint. Under review. M1: Towards Scalable Test-Time Compute with Mamba Reasoning Models Junxiong Wang1, Wen-Ding Li2, Daniele Paliotta3 Daniel Ritter2, Alexander M. Rush2, Tri Dao1,4 1TogetherAI, 2Cornell University, 3University of Geneva, 4Princeton University"
        },
        {
            "title": "Abstract",
            "content": "Effective reasoning is crucial to solving complex mathematical problems. Recent large language models (LLMs) have boosted performance by scaling test-time computation through long chain-of-thought reasoning. However, transformer-based models are inherently limited in extending context length due to their quadratic computational complexity and linear memory requirements. In this paper, we introduce novel hybrid linear RNN reasoning model, M1, built on the Mamba architecture, which allows memory-efficient inference. Our approach leverages distillation process from existing reasoning models and is further enhanced through RL training. Experimental results on the AIME and MATH benchmarks show that M1 not only outperforms previous linear RNN models but also matches the performance of state-of-the-art Deepseek R1 distilled reasoning models at similar scale. We also compare our generation speed with highly performant general purpose inference engine, vLLM, and observe more than 3x speedup compared to same size transformer. With throughput speedup, we are able to achieve higher accuracy compared to DeepSeek R1 distilled transformer reasoning models under fixed generation time budget using self-consistency voting. Overall, we introduce hybrid Mamba reasoning model and provide more effective approach to scaling test-time generation using self-consistency or long chain of thought reasoning."
        },
        {
            "title": "Introduction",
            "content": "Robust and effective reasoning is the cornerstone for successfully performing tasks in domains such as mathematics and programming. Additionally, performance on reasoning tasks can often be boosted by generating longer sequences and/or generating many sequences in parallel (Snell et al., 2024). However, current transformer-based large language models (LLMs) face significant challenges when tasked with processing long sequences with large batch sizes. These models are constrained by quadratic increase in computational complexity as the sequence length grows, coupled with linear escalation in memory requirements. This combination makes it increasingly difficult for models to scale efficiently when handling large inputs. Although linear hybrid RNN models (Gu & Dao, 2024; Dao & Gu, 2024; Beck et al., 2024; Yang et al., 2024; Peng et al., 2023) have shown great potential as an alternative to transformerbased on general language models, their effectiveness on reasoning tasks remains unclear. Since modern reasoning models typically generate long chains of thought for challenging math questions, it is uncertain whether the performance of hybrid linear RNNs diminishes in such scenarios. In this paper, we propose M1 and show that it is possible to derive strong hybrid reasoning models by efficiently transferring reasoning capabilities from large transformer model. Work done when interned at TogetherAI 1 Preprint. Under review. Our training process involves distilling knowledge, incorporating math and reasoning abilities through supervised fine-tuning (SFT), and finally, boosting performance using reinforcement learning (RL) training. In total, the training process requires fewer than 50 billion tokens. In contrast, DeepSeek-R1-Distill-Qwen-1.5B is finetuned from Qwen2.5 MATH 1.5B which is trained using over 1 trillion MATH tokens on top of Qwen2.5. We demonstrate that our hybrid models achieve 3x speedup compared to transformers of the same size when served using highly performant general purpose inference engine, vLLM, at large batch sizes. This gain is mainly due to large batches and long sequences, decoding being generally memory-bound. Lower memory usage of hybrid models can transform this advantage into speed gain. The decoding speedup is approximately linear with the volume of models memory access (Yuan et al., 2025). Notably, this speedup can be converted to gain in reasoning accuracy. Studies (Snell et al., 2024; Li, 2025; Chen et al., 2025) show that techniques such as self-consistency (Wang et al., 2023) and verification (Cobbe et al., 2021) at test time can significantly boost model reasoning performance. Under these conditions, high-throughput model can further enhance its performance by generating more samples. The paper is organized as follows. Section 2 covers related work, Section 3 introduces our pipeline for distilling hybrid reasoning model, and Section 4.1 presents our results evaluating M1 on math benchmarks. Sections 4.2 and 4.3 evaluate the performance gains of M1 in terms of both inference speed and scaling test-time compute. Section 5 provides some additional analysis of the impact of different generation lengths when training on RL, and of the impact of the different steps of the distillation pipeline we propose on performance. Overall, we show that M1 performs on par with DeepSeek-R1-Distill-Qwen-1.5B, achieving scores of 82 on MATH500 (Hendrycks et al., 2021), 22 on AIME25 (MAA, 2025), 23 on AIME24 (MAA, 2024), and 44 on OlympiadBench (He et al., 2024), while offering 3x faster inference throughput, even compared to the highly optimized vLLM (Kwon et al., 2023) implementation for Transformer models."
        },
        {
            "title": "2 Related Work",
            "content": "2.1 Reasoning models Recent models like Deepseek-R1 (DeepSeek-AI et al., 2025) have shown the potential of RL training to improve performance on verifiable reasoning tasks, such as math problem solving and programming. Additional work has proposed methods for inducing this reasoning behavior via supervised fine-tuning, either on curated data (Muennighoff et al., 2025) or on generated pairs of traces (Yang et al., 2025). Other approaches also combine search procedures such as MCTS with language models (Qi et al., 2024) or alter standard RL training schemes to control the length of generated outputs (Aggarwal & Welleck, 2025). After training, these models solve complex tasks by generating long chains of thought, which often include subtasks of the overall problem, multiple attempted solutions, and backtracking over prior attempts (Gandhi et al., 2025). Since the performance of these models, both during training and inference, relies on generating lengthy chains of thought, more efficient architectures can enable larger scale training and less costly generation. 2.2 Enhancing Reasoning via Scaled Inference Compute Increasing the computational budget during inference has become promising approach to boost LLM performance. Methods like Chain of Thought (CoT) and its derivatives have achieved notable gains on reasoning benchmarks by breaking down complex tasks into intermediate steps (Wei et al., 2023; Yao et al., 2023). Although decomposing tasks improves reasoning, it also lengthens generation sequences and raises computational costs. Some recent studies even indicate that this extra computation might itself enhance model capabilities (Pfau et al., 2024). In addition, adaptive compute allocation during inference has been explored. For example, Goyal et al. (2024) incorporated pause tokens into the 2 Preprint. Under review. vocabulary, allowing models to distribute compute more efficiently and improve both reasoning and overall task performance. Another strategy involves generating several outputs and selecting the best one. Researchers have developed various sampling algorithms to diversify and enhance the quality of generated responses, thereby increasing the chances of retrieving the most accurate answer (Wang et al., 2023; Renze & Guven, 2024; Zhang et al., 2023). Moreover, outcome and process reward models (ORMs and PRMs) have been introduced to evaluate responses and steer intermediate generation steps (Lightman et al., 2023; Zhang et al., 2024a; Luo et al., 2024; Uesato et al., 2022). Recent investigations reveal that, under fixed compute budgets, smaller LLMs augmented with inference-time compute techniques (such as majority voting or PRM-guided search) can outperform larger models (Snell et al., 2024; Wu et al., 2024; Beeching et al., 2024). However, these results are mainly confined to Transformer-based architectures, leaving open questions about whether similar scaling laws hold for subquadratic architectures, which offer faster inference but might compromise on expressiveness. 2.3 Alternatives to Transformer Architectures Even though most reasoning models are based on the Transformer architecture (Grattafiori et al., 2024; Qwen et al., 2025), alternatives have been proposed to alleviate their high computational cost. Models built on top of RNNs (Beck et al., 2024; Peng et al., 2023), state space models (SSMs) (Gu et al., 2022; Gu & Dao, 2024), and linear attention mechanisms (Katharopoulos et al., 2020; Yang et al., 2024) demonstrate superior inference and memory efficiency, particularly for long-context tasks and large-batch generation. The Mamba series (Mamba-1 and Mamba-2) notably introduced selective state spaces to enable linear-time sequence modeling with strong performance (Gu & Dao, 2024; Dao & Gu, 2024). In addition, hybrid architectures that combine few self-attention layers with subquadratic layers (e.g., Mamba) have emerged, showing advantages over both pure Transformer and pure subquadratic designs (Lieber et al., 2024; Ren et al., 2024; Dong et al., 2024). Such architectures are particularly suited to meet the high compute demands of inference-time scaling, and our work investigates their scaling properties. 2.4 Knowledge Distillation Strategies Knowledge distillation has proven to be an effective means of transferring capabilities from large teacher models to smaller, more efficient student models (Hinton et al., 2015). In LLMs, this process compresses larger pre-trained model into more compact version while preserving core knowledge and functionality (Gu et al., 2024; Xu et al., 2024). Although larger models tend to exhibit superior reasoning abilities due to scaling properties (Xu et al., 2025; Wei et al., 2022), distillation techniques have enabled smaller models to achieve competitive reasoning performance (DeepSeek-AI et al., 2025; Labs, 2025). While most efforts have focused on intra-architecture distillation (e.g., Transformer-to-Transformer), recent studies have ventured into cross-architecture distillation. For instance, pretrained Transformers have been distilled into architectures such as RNNs (Kasai et al., 2021; Mercat et al., 2024), linear attention models (Zhang et al., 2024b; Zhang et al.), convolutional networks (Ralambomihanta et al., 2024), and SSMs (Bick et al., 2024; Wang et al., 2024; Paliotta et al., 2025). Whether the robust reasoning abilities of Deepseek R1 (DeepSeek-AI et al., 2025) distilled models can be effectively transferred across different architectures remains an open question."
        },
        {
            "title": "3 The M1 Reasoning Model",
            "content": "In this section, we present multi-stage process for building our hybrid linear RNN reasoning model, M1. The approach has three stages: distillation, SFT, and RL. We begin by distilling Transformer model into Mamba architecture, adapting the method of Wang et al. (2025), which initializes the hybrid models weights from transformer model. We then perform math-specific supervised fine-tuning (SFT) on general mathematical datasets 3 Preprint. Under review. Algorithm 1 Initializing MAMBAINLLAMA 1: Shapes: - Batch, - Length, - embed size, = D/Attention heads,N - expand 2: Input: ot: (B, D) 3: Output: output: (B, D) 4: New Params: MLP, 5: for each head WK, WQ, WV, Wo : (N, D) after expanding to same dimension do 6: Head Parameter: : (N, N) 7: 8: 9: 10: 11: 12: 13: 14: 15: end for 16: return output for all positions t: xt : (B, N) WV ot Bt : (B, N) WKot Ct : (B, N) WQot : (B, N) MLP(xt) A1:T, B1:T, C1:T : (B, N, N) DISC(A, B, C, ) LINEARRNN(A, B, C, x) output output + WOy to enhance the models mathematical performance, first without yet incorporating datasets generated by reasoning-focused models, and then with reasoning data leveraging multiple large-scale datasets generated by the R1 model series. Finally, we apply R1s GRPO method to further enhance the models math reasoning capability. Stage 1: Distillation. The first step in building our M1 model is distilling pretrained transformer model into Mamba model. We adapt the distillation approach introduced by Wang et al. (2025). The MAMBAINLLAMA framework (Wang et al., 2025) proposes distilling hybrid TransformerMamba models by reusing weights from attention layers. In this distillation procedure, outlined in Algorithm 1, linear projections for Q, K, V, and are initialized from the corresponding projections for C, B, X, and O, respectively. The newly introduced parameters in the Mamba layers are the sampling rate and the dynamic parameter A, which control the resulting Mamba module via discretization function. Specifically, the sampling rate RN discretizes Bt, Ct RN1, yielding Bt, Ct RNN1, as detailed in Algorithm 1. Different from Wang et al. (2025), we introduce two additional linear layers to project from head dim * kv head to head dim * head. This is because GQA (Ainslie et al., 2023) is used in the transformer model to reduce the KV cache. As Mamba does not utilize KV cache, this expansion can increase the expressiveness of and X. We directly reuse the MLP layers; however, unlike the original approach, we replace the attention layers with Mamba layers in single step. Subsequently, we fine-tune the entire model to expedite the training process. The distillation step involves minimizing the tokenlevel KL divergence, aligning the entire probability distribution of the student model, p(; θ), with the teacher model, p(; θT), for every candidate token at position t. We use the reverse KL divergence, DKL(p(; θ) p(; θT)), as our loss function rather than the forward KL divergence. We choose the reverse KL divergence due to its mode-seeking properties, which results in improved empirical performance. We reimplement the distillation and SFT framework using the Axolotl 1training framework. We apply the model chat template, mask the user prompt, and compute the loss only over the tokens generated in the assistants output. To speed up training, we use data packing to merge different sequences into single one until we reach the maximum sequence length which is set to 8192. We find that data packing achieves significantly better results compared to the non-packing version in distillation for the same training steps. We use the AdamW optimizer with learning rate 1 105 with cosine decay, β = (0.9, 0.95) and weight decay of 0.1. 1https://github.com/axolotl-ai-cloud/axolotl 4 Preprint. Under review. Stage 2: SFT Following the distillation procedure, we finetune the model on large set of math problems, OpenMathInstruct-2 (Toshniwal et al., 2024). As in the distillation stage, we apply the chat template to the prompts, mask the user prompt, and compute the loss only over the tokens generated in the assistants output. We train for two epochs using the same optimizer as distillation. After the initial fine-tuning stage, we finetune on an additional set of math problems and solutions generated by reasoning models. We collect mixed reasoning dataset, including OpenR1-Math-220k 2, OpenThoughts-114k-math3, and ServiceNow-AI-R1-Distill4, MagpieReasoning-250K5 for total of 8B reasoning tokens. The first two datasets were generated from R1, while the last two was generated from the R1 distilled Qwen 32B model and R1 distilled Llama 70B model. We extended the training length to 24,576 because we found that it covers 99% of the data items. We train the model for five epochs using the same optimizer as before but changing the peak learning rate to 6 106. Stage 3: Reasoning RL. To further enhance performance, we integrate Mamba with RL pipeline for further training.6 We use GRPO as the loss function. Differing from (Shao et al., 2024), we remove the KL penalty term as empirically we find it destabilizes training. Additionally, we include an entropy bonus to encourage more diverse policy. The resulting formula is, LGRPO(θ) = Eτπθold (cid:20) πθ(as) (as) πθold (cid:21) ˆA(s, a) + η H(πθ) (1) where ˆA(s, a) is the estimate of the advantage from multiple rollouts. We use batch size of 128 and PPO batch size of 64, which also determines the number of PPO iterations, µ = 2. We set the number of generations for each sequence to 8 and the maximum generation length to 32k. For optimization, we use the Adam optimizer with learning rate of 1 106. We train for 50 steps, and pick the best checkpoint with the highest critic reward. We append the simple prompt Lets think step by step and output the final answer within boxed{} to the end of each question in both training and evaluation."
        },
        {
            "title": "4 Experiments",
            "content": "Model. We adopt the Llama3.2-3B-Instruct models as distillation target models. For Mamba layers, we set the SSM state size to 16. Consequently, the number of SSM groups after expansion is 3072/16 = 192 for the 3B model. We use 6 interleaved attention layers among 28 total layers. Evaluation Dataset. Following common practice in evaluating reasoning models, we use similar set of math benchmarks, including competition-level problems: MATH500 (Hendrycks et al., 2021), AIME25 (MAA, 2025), AIME24 (MAA, 2024), AMC23 (MAA, 2023), and OlympiadBench (He et al., 2024). Evaluation Metrics. Our models performance is assessed using two key metrics: coverage and accuracy. In fields such as coding and formal proofs, where answers can be automatically verified, coverage translates directly to enhanced performance and is widely utilized (Chen et al., 2021; Brown et al., 2024). Coverage is often measured using the pass@k 2https://huggingface.co/datasets/open-r1/OpenR1-Math-220k 3https://huggingface.co/datasets/open-thoughts/OpenThoughts-114k 4https://huggingface.co/datasets/ServiceNow-AI/R1-Distill-SFT 5https://huggingface.co/datasets/Magpie-Align/Magpie-Reasoning-V2-250K-CoT-Deepseek-R1-Llama-70B 6We add it into the popular VeRL (Sheng et al., 2024) framework. In doing so, we addressed and resolved the CUDA graph incompatibility issues that previously arose during training with PyTorchs FSDP module. As result, the updated framework now efficiently supports Mamba generation with CUDA graph enabled, making it 5x faster than with CUDA Graph disabled 5 Preprint. Under review. metric, with indicating the number of samples per problem (Chen et al., 2021; Brown et al., 2024). This metric estimates the likelihood that at least one correct solution exists among the samples. To minimize variance when calculating coverage, we employ the unbiased estimation formula from Chen et al. (2021). Specifically, we generate total samples per task. The probability that correct solution exists among pool of generated samples can then be determined given the total number of correct solutions Ci for each task. pass@k = 1 # of problems # of problems i=1 (cid:32) 1 (cid:33) ) (NCi (N ) We implement this formula using numerically stable approach as recommended by Chen et al. (2021). When using additional compute, we employ multiple aggregation strategies. The most straightforward method is majority voting, also known as self-consistency decoding (Wang et al., 2023), which takes the majority response among samples as the predicted answer, and uses that to compute the accuracy. 4.1 Reasoning Evaluation Model AIME25 AIME24 MATH500 OlympiadBench AMC23 Pass@1Maj@32 Pass@1Maj@32 Pass@1 Maj@ Pass@1 Pass@1 GPT-4o-0513 Claude-3.5-1022 Qwen2.5-Math-7B-Ins R1-Llama-8B R1-Qwen-1.5B M1-3B 9.3 16.0 - 32.9 23.5 22. - - - - - 30.8 - - 13.3 50.4 28.9 23.0 - - - - 45. 74.6 78.3 79.8 89.8 83.9 81.7 - - - - 91.0 89.5 - - 40.7 54.2 43.3 43. - - 50.6 - 62.9 56.0 Table 1: Results for M1-3B, DeepSeek-R1-Distill-Qwen-1.5B, and Qwen2.5-Math-7B-Instruct. We evaluate our models using temperature setting of 0.7 and sequence length of 32k with evaluation tools in VeRL. We use 32k because it has become the standard for evaluating performance on reasoning models (DeepSeek-AI et al., 2025; Luo et al., 2025). We report the pass@1 metric averaged over 64 runs; for majority voting, we repeat the metric calculation 100 times. We report the accuracy of M1-3B and DeepSeek-R1-Distill-Qwen-1.5B in Table 1. We use the baseline DeepSeek-R1-Distill-Qwen-1.5B since 3B R1 reasoning model is still not available. Although M1-3B has more parameters than DeepSeek-R1-Distill-Qwen-1.5B, its speed is still comparable even with shorter contexts, so we believe this is fair comparison. Our models performance is competitive with state-of-the-art open reasoning models in the same model size range and outperforms larger nonreasoning math transformer models. Our model performs slightly worse on AIME24 compared to the DeepSeek-R1-Distill-Qwen-1.5B model. Notably, DeepSeek-R1-Distill-Qwen-1.5B is built on top of the Qwen2.5 MATH models, which were finetuned with over 1T MATH tokens on top of the Qwen2.5 models, significantly more training data than what M1-3B used in total. 4.2 Speed Evaluation We benchmark inference time with our model against transformer model (Llama-3.2.- 3B (Grattafiori et al., 2024)) of the same size. We use vLLM (version 0.6.3), which is the version used in VeRL for efficient rollouts. We also compare against DeepSeek-R1-DistillQwen-1.5B (DeepSeek-AI et al., 2025), reasoning transformer model that is half the size of M1. This model has the same number of layers as the 3B parameter transformer, but the hidden dimension is half the size. According to Luo et al. (2025), the average generation length of reasoning models on MATH questions is 4k to 5k. We therefore fix decoding length of 4096 (and prompt length of 256) 6 Preprint. Under review. and benchmark our model across range of batch sizes. We vary the batch size from 8 to 512, measuring the inference latency across different models. We perform our benchmarking on single NVIDIA H100 GPU with greedy decoding. To ensure that every model generates up to the set maximum number of tokens, we use ignore eos=True. Before recording results, we warm up the system with two runs. The final performance metrics are then averaged over three subsequent runs. The inference speeds of the models across batch sizes are shown in Figure 1. M1 achieves 3 speedup over similarly-sized transformers when using batch size of 512 and decoding length of 4096, demonstrating its effectiveness in large-batch generation settings. The maximum length of generated sequences is also an important factor in RL training, as longer sequences allow the model to use more compute during learning by generating longer chains-of-thought, shown in Figure 5. To benchmark our model in this setting, we fix the batch size to 128, and vary the generation length. We compare against the same two models as in the batch size varying case, and the results are shown in Figure 2. As the generated sequence length increases, M1 achieves increasing speedups relative to the baseline models, and consistently generates at least 2x faster than Llama-3.2-3B (2.64x faster for the longest sequence length). Figure 1: Inference latency when using prompt length 256 and decoding length 4096. Figure 2: Inference latency when using batch size 128. It is well-known that LLM inference comprises prefilling (compute-bound) and decoding (memory-bound) stage. For math reasoning models, it is common to assume that decoding takes much longer than prefilling, since prefilling only uses short MATH question, while decoding generates long answers. Under these settings, the process is memory-bound. Given that Mamba is highly memory-efficient and we only use SSM state size of 16, these memory advantages translate into improved speed."
        },
        {
            "title": "4.3 Test-Time Scaling",
            "content": "Given fixed time budget, M1 can generate more sequences or longer sequences compared to transformer model, which can hopefully boost its performance. We evaluate the effect of test-time compute scaling on model performance. We scale both the number of samples generated as well as the length of generated samples, to see if M1 benefits from additional compute along these axes. We aim to investigate whether the speed benefit from section 4.2 can translate into an accuracy gain. Scaling with majority vote. The left side of Figure 3 shows the effect of scaling the number of generated samples (while fixing the maximum decoding length) on AIME24 accuracy. Both the baseline model and M1 see increasing accuracy as the number of samples increases, with M1 nearly matching the baseline performance for larger sample sizes. The efficient generation of M1 also means that generating large number of samples at test-time is faster than for the baseline transformer model. We quantify this efficiency in the right side of Figure 3, which compares the number of seconds spent generating samples against the resulting accuracy. To compute the time 7 Preprint. Under review. Figure 3: Number of samples vs. AIME25 accuracy (left) and generation time (seconds) vs. AIME25 accuracy (right). Both graphs include pass@1 and majority voting accuracies for M1 and DeepSeek-R1-Distill-Qwen-1.5B. values on the x-axis, we find an optimal throughput value (in tokens per second) for each model by increasing batch sizes until throughput decreases. The optimal values were 7263 T/s for DeepSeek-R1-Distill-Qwen-1.5B, and 15169 T/s for M1. We then assume that each generated sample is maximum length (8K), and compute the seconds required for one sample from one model as 8K divided by the throughput. We then convert the left graph of Figure 3 into the right graph, by multiplying the number of samples for each datapoint by the seconds required per sample for each model. As an example, M1 requires roughly half second (8K/15K) per sample, so the accuracy value for M1 at 32 samples on the left graph appears at approximately 16 seconds on the right graph. Scaling with longer sequences Figure 4 shows the effect of scaling the maximum length of the generated answer, while fixing the number of generated samples to one. For both the baseline and M1, increasing the maximum sequence length leads to increased accuracy, as shown in the left graph in Figure 4. After converting from generation length to the seconds required to generate (done in the same way as Figure 3, but dividing the generation length by throughput), we can see the accuracy gain per time spent generating on the right side of Figure 4. In this case, M1 actually gets higher accuracy for the same amount of time spent generating at 4 of the 5 evaluated sequence lengths, showing the benefits of efficient generation for test-time compute scaling. Figure 4: Generation length vs. AIME25 accuracy (left) and generation time (seconds) vs. AIME25 accuracy (right). Sampling for both models is done using temperature of 0.8."
        },
        {
            "title": "5 Analysis",
            "content": "Increasing Training Length in RL boosts model performance With more efficient models, we can increase the length of sequences used in RL training, resulting in improved performance. Empirically, we see this in Figure 5, which shows an 8 Preprint. Under review. Figure 5: Pass@1 vs. maximum sequence length in GRPO training MATH500 AIME Distill Distill + SFT(MATH) Distill + SFT(MATH) + SFT(Reason) Distill + SFT(MATH) + SFT(Reason) + RL 38 45 74 82 0 0 17 23 Table 2: M1 Accuracy after each training stage on MATH500 and AIME24. increase in accuracy on AIME25 as we scale up the length of sequences generated when training with GRPO. Training with sequences of maximum length 4096 results in accuracy below 7.5%, while allowing sequences up to length 24K boosts the accuracy up to 22.5%. MATH Accuracy at each training stage To identify which components of our training pipeline have the greatest impact on performance, we also evaluate intermediate versions of the model on MATH500 (Hendrycks et al., 2021) and AIME24 (MAA, 2024). The results of these evaluations are presented in Table 2. Each step of the training pipeline provides boost to performance, with particularly large gains from fine-tuning on solutions from reasoning models (+29% on MATH500 and +17% on AIME24). Direct Distillation from Reasoning Models We also attempted to distill from Deepseek-R1Qwen-1.5B instead of Llama-3.2-3B. In this case, we did not SFT on OpenMathInstruct, and instead only SFT on the 8B reasoning data that we collected after distillation. We found that the distilled models performance was poor (38% and 3.3% pass@1 accuracy on MATH500 and AIME24, resspectively). Our hypothesis for why this occurs is that 8B tokens is insufficient to effectively transfer reasoning skills from the transformer to Mamba. Although curating high-quality reasoning dataset demands significant time and effort, we begin by leveraging the standard MATH distillation dataset from OpenMathInstruct (Toshniwal et al., 2024) to first distill strong MATH model. We then transform this MATH model into reasoning model via SFT on the dedicated reasoning dataset. This approach achieves strong performance with much smaller number of reasoning tokens."
        },
        {
            "title": "6 Conclusion",
            "content": "In this paper, we introduced M1, hybrid reasoning model built on the Mamba architecture, designed to address the scalability challenges of the Transformer models. We demonstrated effective techniques for distillation and finetuning to develop M1, which achieves mathematical reasoning performance comparable to state-of-the-art reasoning models of similar size. Notably, M1 delivers over 3x faster inference than similar-sized Transformer models, even when using the heavily optimized vLLM inference engine, particularly at large batch sizes. This improved efficiency can make the resource-intensive inference-time strategies, such as self-consistency, more practical. Our findings establish M1 as strong alternative to 9 Preprint. Under review. Transformer-based architectures, paving the way for more efficient and high-performing reasoning models."
        },
        {
            "title": "References",
            "content": "Pranjal Aggarwal and Sean Welleck. L1: Controlling how long reasoning model thinks with reinforcement learning, 2025. URL https://arxiv.org/abs/2503.04697. Joshua Ainslie, James Lee-Thorp, Michiel de Jong, Yury Zemlyanskiy, Federico Lebron, and Sumit Sanghai. Gqa: Training generalized multi-query transformer models from multi-head checkpoints. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, pp. 48954901, 2023. Maximilian Beck, Korbinian oppel, Markus Spanring, Andreas Auer, Oleksandra Prudnikova, Michael Kopp, unter Klambauer, Johannes Brandstetter, and Sepp Hochreiter. xlstm: Extended long short-term memory, 2024. URL https://arxiv.org/abs/2405.04517. Edward Beeching, Lewis Tunstall, and Sasha Rush. Scaling test-time compute URL https://huggingface.co/spaces/HuggingFaceH4/ with open models, 2024. blogpost-scaling-test-time-compute. Aviv Bick, Kevin Y. Li, Eric P. Xing, J. Zico Kolter, and Albert Gu. Transformers to ssms: Distilling quadratic knowledge to subquadratic models, 2024. URL https://arxiv.org/ abs/2408.10189. Bradley Brown, Jordan Juravsky, Ryan Ehrlich, Ronald Clark, Quoc V. Le, Christopher Re, and Azalia Mirhoseini. Large language monkeys: Scaling inference compute with repeated sampling, 2024. URL https://arxiv.org/abs/2407.21787. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde de Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, and et. al. Evaluating large language models trained on code, 2021. URL https://arxiv.org/abs/2107.03374. Qiguang Chen, Libo Qin, Jinhao Liu, Dengyun Peng, Jiannan Guan, Peng Wang, Mengkang Hu, Yuhang Zhou, Te Gao, and Wangxiang Che. Towards reasoning era: survey of long chain-of-thought for reasoning large language models. arXiv preprint arXiv:2503.09567, 2025. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. Training verifiers to solve math word problems, 2021. URL https://arxiv.org/abs/2110.14168. Tri Dao and Albert Gu. Transformers are ssms: Generalized models and efficient algorithms through structured state space duality, 2024. URL https://arxiv.org/abs/2405.21060. DeepSeek-AI, Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, Xiaokang Zhang, Xingkai Yu, Yu Wu, Z. F. Wu, Zhibin Gou, Zhihong Shao, Zhuoshu Li, Ziyi Gao, and et. al. Deepseekr1: Incentivizing reasoning capability in llms via reinforcement learning, 2025. URL https://arxiv.org/abs/2501.12948. Xin Dong, Yonggan Fu, Shizhe Diao, Wonmin Byeon, Zijia Chen, Ameya Sunil Mahabaleshwarkar, Shih-Yang Liu, Matthijs Van Keirsbilck, Min-Hung Chen, Yoshi Suhara, Yingyan Lin, Jan Kautz, and Pavlo Molchanov. Hymba: hybrid-head architecture for small language models, 2024. URL https://arxiv.org/abs/2411.13676. Kanishk Gandhi, Ayush Chakravarthy, Anikait Singh, Nathan Lile, and Noah D. Goodman. Cognitive behaviors that enable self-improving reasoners, or, four habits of highly effective stars, 2025. URL https://arxiv.org/abs/2503.01307. 10 Preprint. Under review. Sachin Goyal, Ziwei Ji, Ankit Singh Rawat, Aditya Krishna Menon, Sanjiv Kumar, and Vaishnavh Nagarajan. Think before you speak: Training language models with pause tokens, 2024. URL https://arxiv.org/abs/2310.02226. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and et. al. The llama 3 herd of models, 2024. URL https://arxiv.org/abs/2407.21783. Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces, 2024. URL https://arxiv.org/abs/2312.00752. Albert Gu, Karan Goel, and Christopher Re. Efficiently modeling long sequences with structured state spaces, 2022. URL https://arxiv.org/abs/2111.00396. Yuxian Gu, Li Dong, Furu Wei, and Minlie Huang. Minillm: Knowledge distillation of large language models, 2024. URL https://arxiv.org/abs/2306.08543. Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Leng Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, Jie Liu, Lei Qi, Zhiyuan Liu, and Maosong Sun. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems, 2024. Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset, 2021. URL https://arxiv.org/abs/2103.03874. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network, 2015. URL https://arxiv.org/abs/1503.02531. Jungo Kasai, Hao Peng, Yizhe Zhang, Dani Yogatama, Gabriel Ilharco, Nikolaos Pappas, Yi Mao, Weizhu Chen, and Noah A. Smith. Finetuning pretrained transformers into rnns, 2021. URL https://arxiv.org/abs/2103.13076. Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention, 2020. URL https:// arxiv.org/abs/2006.16236. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the ACM SIGOPS 29th Symposium on Operating Systems Principles, 2023. Bespoke Labs. Bespoke-stratos: The unreasonable effectiveness of reasoning distillation. www.bespokelabs.ai/blog/bespoke-stratos-the-unreasonable-effectiveness-ofreasoning-distillation, 2025. Accessed: 2025-01-22. Xinzhe Li. survey on llm test-time compute via search: Tasks, llm profiling, search algorithms, and relevant frameworks, 2025. URL https://arxiv.org/abs/2501.10069. Opher Lieber, Barak Lenz, Hofit Bata, Gal Cohen, Jhonathan Osin, Itay Dalmedigos, Erez Safahi, Shaked Meirom, Yonatan Belinkov, Shai Shalev-Shwartz, Omri Abend, Raz Alon, Tomer Asida, Amir Bergman, Roman Glozman, Michael Gokhman, Avashalom Manevich, Nir Ratner, Noam Rozen, Erez Shwartz, Mor Zusman, and Yoav Shoham. Jamba: hybrid transformer-mamba language model, 2024. URL https://arxiv.org/abs/2403.19887. Hunter Lightman, Vineet Kosaraju, Yura Burda, Harri Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step, 2023. URL https://arxiv.org/abs/2305.20050. 11 Preprint. Under review. Liangchen Luo, Yinxiao Liu, Rosanne Liu, Samrat Phatale, Meiqi Guo, Harsh Lara, Yunxuan Li, Lei Shu, Yun Zhu, Lei Meng, Jiao Sun, and Abhinav Rastogi. Improve mathematical reasoning in language models by automated process supervision, 2024. URL https: //arxiv.org/abs/2406.06592. Michael Luo, Sijun Tan, Manan Roongta, Colin Cai, Raluca Ada Popa, with 1.5b model by scaling rl. DeepScaleR-Surpassing-O1-Preview-with-a-1-5B-Model-by-Scaling-RL-19681902c1468005bed8ca303013a4e2, 2025. Notion Blog. Justin Wong, Xiaoxiang Shi, William Y. Tang, Jeffrey Luo, Tianjun Zhang, Li Erran Li, Surpassing o1-preview https://pretty-radio-b75.notion.site/ and Ion Stoica. Deepscaler: MAA. American invitational mathematics examination 2023, 2023. URL https: //artofproblemsolving.com/wiki/index.php/American Invitational Mathematics Examination?srsltid=AfmBOoqiDCiaGTLQrsRTKsZui8RFnjOZqM4qIqY3yGB3sBaqOaxwf Xt. MAA. American invitational mathematics examination 2024, 2024. URL https: //artofproblemsolving.com/wiki/index.php/American Invitational Mathematics Examination?srsltid=AfmBOoqiDCiaGTLQrsRTKsZui8RFnjOZqM4qIqY3yGB3sBaqOaxwf Xt. MAA. American invitational mathematics examination 2025, 2025. URL https: //artofproblemsolving.com/wiki/index.php/American Invitational Mathematics Examination?srsltid=AfmBOoqiDCiaGTLQrsRTKsZui8RFnjOZqM4qIqY3yGB3sBaqOaxwf Xt. Jean Mercat, Igor Vasiljevic, Sedrick Keh, Kushal Arora, Achal Dave, Adrien Gaidon, and Thomas Kollar. Linearizing large language models, 2024. URL https://arxiv.org/abs/ 2405.06640. Niklas Muennighoff, Zitong Yang, Weijia Shi, Xiang Lisa Li, Li Fei-Fei, Hannaneh Hajishirzi, Luke Zettlemoyer, Percy Liang, Emmanuel Cand`es, and Tatsunori Hashimoto. s1: Simple test-time scaling, 2025. URL https://arxiv.org/abs/2501.19393. Daniele Paliotta, Junxiong Wang, Matteo Pagliardini, Kevin Li, Aviv Bick, Zico Kolter, Albert Gu, Francois Fleuret, and Tri Dao. Thinking slow, fast: Scaling inference compute with distilled reasoners. arXiv preprint arXiv:2502.20339, 2025. Bo Peng, Eric Alcaide, Quentin Anthony, Alon Albalak, Samuel Arcadinho, Stella Biderman, Huanqi Cao, Xin Cheng, Michael Chung, Matteo Grella, Kranthi Kiran GV, Xuzheng He, Haowen Hou, Jiaju Lin, Przemyslaw Kazienko, Jan Kocon, Jiaming Kong, Bartlomiej Koptyra, Hayden Lau, and et. al. Rwkv: Reinventing rnns for the transformer era, 2023. URL https://arxiv.org/abs/2305.13048. Jacob Pfau, William Merrill, and Samuel R. Bowman. Lets think dot by dot: Hidden computation in transformer language models, 2024. URL https://arxiv.org/abs/2404. 15758. Zhenting Qi, Mingyuan Ma, Jiahang Xu, Li Lyna Zhang, Fan Yang, and Mao Yang. Mutual reasoning makes smaller llms stronger problem-solvers, 2024. URL https://arxiv.org/ abs/2408.06195. Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, and et. al. Qwen2.5 technical report, 2025. URL https://arxiv.org/abs/2412.15115. Tokiniaina Raharison Ralambomihanta, Shahrad Mohammadzadeh, Mohammad Sami Nur Islam, Wassim Jabbour, and Laurence Liang. Scavenging hyena: Distilling transformers into long convolution models, 2024. URL https://arxiv.org/abs/2401.17574. Liliang Ren, Yang Liu, Yadong Lu, Yelong Shen, Chen Liang, and Weizhu Chen. Samba: Simple hybrid state space models for efficient unlimited context language modeling, 2024. URL https://arxiv.org/abs/2406.07522. 12 Preprint. Under review. Matthew Renze and Erhan Guven. The effect of sampling temperature on problem solving in large language models, 2024. URL https://arxiv.org/abs/2402.05201. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv: 2409.19256, 2024. Charlie Snell, Jaehoon Lee, Kelvin Xu, and Aviral Kumar. Scaling llm test-time compute optimally can be more effective than scaling model parameters, 2024. URL https:// arxiv.org/abs/2408.03314. Shubham Toshniwal, Wei Du, Ivan Moshkov, Branislav Kisacanin, Alexan Ayrapetyan, and Igor Gitman. Openmathinstruct-2: Accelerating ai for math with massive open-source instruction data, 2024. URL https://arxiv.org/abs/2410.01560. Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with processand outcome-based feedback, 2022. URL https://arxiv.org/abs/2211.14275. Junxiong Wang, Daniele Paliotta, Avner May, Alexander M. Rush, and Tri Dao. The mamba in the llama: Distilling and accelerating hybrid models. arXiv preprint arXiv:2408.15237, 2024. Junxiong Wang, Daniele Paliotta, Avner May, Alexander Rush, and Tri Dao. The mamba in the llama: Distilling and accelerating hybrid models. Advances in Neural Information Processing Systems, 37:6243262457, 2025. Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models, 2023. URL https://arxiv.org/abs/2203.11171. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models, 2022. URL https://arxiv.org/abs/2206.07682. Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models, 2023. URL https://arxiv.org/abs/2201.11903. Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Inference scaling laws: An empirical analysis of compute-optimal inference for problem-solving with language models, 2024. URL https://arxiv.org/abs/2408.00724. Fengli Xu, Qianyue Hao, Zefang Zong, Jingwei Wang, Yunke Zhang, Jingyi Wang, Xiaochong Lan, Jiahui Gong, Tianjian Ouyang, Fanjin Meng, Chenyang Shao, Yuwei Yan, Qinglong Yang, Yiwen Song, Sijian Ren, Xinyuan Hu, Yu Li, Jie Feng, Chen Gao, and Yong Li. Towards large reasoning models: survey of reinforced reasoning with large language models, 2025. URL https://arxiv.org/abs/2501.09686. Xiaohan Xu, Ming Li, Chongyang Tao, Tao Shen, Reynold Cheng, Jinyang Li, Can Xu, Dacheng Tao, and Tianyi Zhou. survey on knowledge distillation of large language models, 2024. URL https://arxiv.org/abs/2402.13116. Songlin Yang, Bailin Wang, Yikang Shen, Rameswar Panda, and Yoon Kim. Gated linear attention transformers with hardware-efficient training, 2024. URL https://arxiv.org/ abs/2312.06635. Wang Yang, Hongye Jin, Jingfeng Yang, Vipin Chaudhary, and Xiaotian Han. Thinking preference optimization, 2025. URL https://arxiv.org/abs/2502.13173. 13 Preprint. Under review. Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. Tree of thoughts: Deliberate problem solving with large language models, 2023. URL https://arxiv.org/abs/2305.10601. Jingyang Yuan, Huazuo Gao, Damai Dai, Junyu Luo, Liang Zhao, Zhengyan Zhang, Zhenda Xie, YX Wei, Lean Wang, Zhiping Xiao, et al. Native sparse attention: Hardware-aligned and natively trainable sparse attention. arXiv preprint arXiv:2502.11089, 2025. Dan Zhang, Sining Zhoubian, Ziniu Hu, Yisong Yue, Yuxiao Dong, and Jie Tang. Restmcts*: Llm self-training via process reward guided tree search, 2024a. URL https: //arxiv.org/abs/2406.03816. Michael Zhang, Simran Arora, Rahul Chalamala, Benjamin Frederick Spector, Alan Wu, Krithik Ramesh, Aaryan Singhal, and Christopher Re. Lolcats: On low-rank linearizing of large language models. In The Thirteenth International Conference on Learning Representations. Michael Zhang, Kush Bhatia, Hermann Kumbong, and Christopher Re. The hedgehog & the porcupine: Expressive linear attentions with softmax mimicry, 2024b. URL https: //arxiv.org/abs/2402.04347. Shun Zhang, Zhenfang Chen, Yikang Shen, Mingyu Ding, Joshua B. Tenenbaum, and Chuang Gan. Planning with large language models for code generation, 2023. URL https://arxiv.org/abs/2303.05510."
        },
        {
            "title": "A Limitations and Future Work",
            "content": "Speedup. Our current hybrid model is only 3 faster than Transformer of the same size when serving inference with vLLM. Recently, NVIDIA introduced new hybrid Mamba kernel7, which could further boost the speed of hybrid models. Additionally, our attention implementation in hybrid models does not yet leverage the optimizations available in vLLM. Integrating M1 into vLLM could further boost performance by taking advantage of these attention speedups. Why do we not distill Qwen2.5 1.5B MATH model. We considered using the Qwen2.5 1.5B MATH Instruct model as the distillation target in the first stage. However, we found that the cross entropy loss of the Qwen 1.5B MATH model on the OpenMATH Instruct dateset (Toshniwal et al., 2024) exceeded 1.8, which is much higher than that of the Llama models (0.5). This suggests that, to mimic the Qwen2.5 model, we need dataset generated from large Qwen2.5 series model rather than this one generated from the Llama models. Dataset curation from Qwen Math models goes beyond the scope of this work. Improvement on RL training speed Recently, DeepSeek R1 (DeepSeek-AI et al., 2025) showed that reinforcement learning (RL) is key component in improving model reasoning performance during post-training. Since then, recent research has predominantly relied on reinforcement learning (RL) as training paradigm for reasoning models. However, training with RL requires the efficient generation of long sequences. For example, in VeRL (Sheng et al., 2024), the typical training batch size ranges from few thousand to several thousand. DeepscaleR (Luo et al., 2025) also shows significant accuracy boost when training RL with longer sequences, as it tends to enhance model performance by providing more steps for thorough reasoning. However, this shift towards reinforcement learning has resulted in the generation process becoming significant bottleneck in reasoning model training, taking more than three times as long as the actors weight update (forward + backward) according to the time profiling done for DeepscaleR (Luo et al., 2025). This need for efficient generation in RL presents significant challenge for transformer models, namely due to the heavy computational burden imposed by large key-value caches during generation, especially for large batch sizes. Given their generation speed advantages, linear RNN models may be better suited for scaling RL training. 7https://github.com/NVIDIA/Megatron-LM/commit/b957578e76a921209ef873cbbd389114a"
        }
    ],
    "affiliations": [
        "Cornell University",
        "Princeton University",
        "TogetherAI",
        "University of Geneva"
    ]
}