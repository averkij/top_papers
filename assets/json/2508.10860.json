{
    "paper_title": "From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms",
    "authors": [
        "Zhaokun Jiang",
        "Ziyin Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation."
        },
        {
            "title": "Start",
            "content": "From Black Box to Transparency: Enhancing Automated Interpreting Assessment with Explainable AI in College Classrooms Zhaokun Jiang1 Ziyin Zhang1* 1Shanghai Jiao Tong University 5 2 0 2 4 ] . [ 1 0 6 8 0 1 . 8 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and lack of efforts to explain model predictions. To address these gaps, we propose multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over black box predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting selfregulated learning advantages not afforded by automated scores in isolation."
        },
        {
            "title": "Introduction",
            "content": "Interpreting, or oral translation, is complex yet pivotal linguistic competency that offers extensive educational benefits by fostering advanced linguistic, communicational, cognitive, and emotional It capabilities (PÃ¶chhacker, 2001; Gile, 2021). enhances active listening (Lee, 2013), oral proficiency (Han and Lu, 2025), vocabulary acquisition (Chen, 2024), and cross-cultural communication (Stachl-Peier, 2020), while also strengthening higher-order cognitive functions (Dong and Xie, *daenerystargaryen@sjtu.edu.cn 1 2014) and anxiety management capabilities (Zhao, 2022). Given its multifaceted benefits, interpreting has increasingly been recognized as both valuable pedagogical tool and the fifth skill (Mellinger, 2018) alongside listening, speaking, reading, and writing. The intricate nature of interpreting necessitates continuous cycle of structured practice, rigorous assessment, and diagnostic feedback (Gile, 2021). However, traditional human-based assessment often requires raters to simultaneously consult the source text, the interpreted output, and detailed rating scales, cognitively demanding process that increases the risk of scoring bias and inconsistency (Lee, 2019; Han et al., 2024). The inherent limitations of human evaluation have spurred considerable interest in automated assessment. However, existing works are characterized by both thematic imbalance and methodological constraints. Among the three established dimensions of interpreting quality (fidelity, fluency, and language use), investigations have disproportionately focused on the first two, while language use has received scant scholarly attention (Yu and van Heuven, 2017; Han and Yang, 2023; Wang and Wang, 2022; Han and Lu, 2021; Lu and Han, 2022). Furthermore, prior research has predominantly relied on conventional statistical methods such as correlation and regression analyses (Yu and van Heuven, 2017; Wang and Wang, 2022; Han and Lu, 2021; Lu and Han, 2022), which are based on assumptions of linearity that often do not hold in complex, real-world datasets. The advent of machine learning (ML) algorithms and large language models (LLMs) presents novel opportunities to analyze complex data patterns that elude traditional statistical methods. Nevertheless, notable obstacle in their application is the severe imbalance in data composition. Wang and Yuan (2023), for example, find their five-class classification model unable to identify performances at Figure 1: SHAP-Based global feature importance for InfoCom (left), FluDel (middle), and TLQual (right) predictions. Warmer tones (e.g., red) signify higher feature values and cooler tones (e.g., blue) indicate lower feature values. The features are arranged in descending order along the y-axis based on their global importance. The meaning of FluDel and TLQual features are given in Table 2, 3 respectively. the distributional extremes (very poor and very good), direct consequence of the imbalanced training data distribution. Another limitation is the inherent opacity of automated scoring systems. Jia and Aryadoust (2023), for instance, find moderate correlations between GPT-4s interpreting performance assessment and human-assigned scores. Crucially, the internal decision-making processes of the LLMs remained opaque, with only the final scores being accessible. This black box nature severely restricts the diagnostic and educational utility of LLM scores. In response to these challenges, we raise the following questions in this work: 1) Can we mitigate the underperformance of interpreting assessment models with data augmentation? 2) Which specific features of fidelity, fluency, and language use exhibit the strongest predictive power in interpreting assessment models? 3) What specific feature combinations influence individual student scores for each dimension of interpreting quality? To answer these questions, we introduce novel approach that combines feature engineering, data augmentation, and explainable AI (XAI) techniques (Arrieta et al., 2019; Linardatos et al., 2020) to evaluate interpreting performance across three key dimensions: fidelity, fluency, and target language quality. After using Variational AutoEncoders (VAEs) to augment the data, we extract broad set of features including translation quality metrics, temporal measures, and syntactic complexity indices to predict interpreting performance. Based on these features, we predict performance separately for each of the three dimensions adopting multi-dimensional modeling strategy, which facilitates more fine-grained analysis of interpreting quality and provides clearer insights into the specific contributions of features to each criterion. Furthermore, we apply Shapley Value (SHAP) analysis to provide interpretable explanations at both global and individual levels. To the best of our knowledge, we represent the first systematic efforts to automate the assessment of target language quality in interpreting."
        },
        {
            "title": "2.1 Automated Interpreting Assessment",
            "content": "The field of automated interpreting assessment is witnessing paradigm shift, moving from statistical methods toward more sophisticated neural models. To date, the application of ML to interpreting quality evaluation remains nascent but growing domain. The pioneering work by Le et al. (2016) developed estimators based on features from automatic speech recognition (ASR) and machine translation (MT), finding that MT features are most influential in predicting interpretation quality. Following that, Stewart et al. (2018) adapted the QuEst++ quality estimation pipeline with Support Vector Regression to predict the performance of simultaneous interpreters. More recently, Wang and Yuan (2023) employed SVM and KNN algorithms to classify E-C interpretations, while Han et al. (2025) further advanced the domain by integrating neuralbased metrics with acoustic and linguistic indices through ordinal logistic regression."
        },
        {
            "title": "2.2 Dimensions of Interpreting Assessment",
            "content": "Information Completeness Information completeness, also known as fidelity, refers to the extent of informational, semantic, and pragmatic correspondence between source message and its translation (Han, 2018). Existing metrics for automatic fidelity assessment can be broadly categorized into two types: non-neural and neural-based. Non-neural metrics such as BLEU (Papineni 2 et al., 2002) and chrF (Popovic, 2015) mainly rely on statistical and lexical matching to quantify the overlap of word or character sequences between candidate translation and human reference. Although these metrics have been widely adopted in the past decades, they have also been criticized for their reliance on surface-level comparisons that may not capture deeper semantic equivalence (Castilho et al., 2018). In contrast, neural-based metrics are derived from pre-trained language models and transcend surface matching by comparing contextualized embeddings. Prominent examples include BERTScore (Zhang et al., 2020), BLEURT (Sellam et al., 2020), CometKiwi (Rei et al., 2022), and xCOMET (Guerreiro et al., 2024). While Han and Lu (2025) report strong aggregate correlation between these scores and human evaluations on E-C interpreting, Lu and Han (2022) find that the non-neural metrics BLEU and NIST outperform BERTScore, suggesting that non-neural and neural metrics may capture distinct, and potentially complementary, facets of interpreting quality. Fluency Fluency is another key dimension of interpreting quality, reflecting how effectively and naturally an interpretation is delivered (Stenzl, 1983). In computational modeling, fluency features are typically classified into three categories (Tavakoli and Skehan, 2005): (1) speed fluency, which captures the rate and density of delivery; (2) breakdown fluency, which measures speech continuity through the absence of interruptions and pauses; and (3) repair fluency, which quantifies self-corrections and repetitions. Within interpreting empirical research, considerable evidence has underscored the high predictive power of speed fluency features such as speech rate, phonation time ratio, and articulation rate (Han and Yang, 2023; Han, 2015; Song, 2020; Yu and van Heuven, 2017), while other works have also identified breakdown fluency features (e.g. mean length of unfilled pauses) as strong predictors (Wang and Wang, 2022; Wu, 2021). In contrast, repair fluency features are less commonly employed and seldom show strong predictive effectiveness (Han, 2015). Target Language Use In interpreting assessment, target language quality typically refers to the grammaticality and idiomaticity of the target language output (Han, 2018). Automated assessment of this dimension is facilitated by advances in computational tools such as Coh-Metrix (Graesser et al., 2004), TAASSC (Kyle, 2016), L2SCA (Lu, 2010), and CCA (Hu et al., 2022b,a), which operationalize linguistic quality by calculating wide array of features from lexical and phraseological indices to measures of syntax and discourse. While these features have been extensively applied in L2 writing and speaking research (Lu, 2010; Kyle and Crossley, 2017; Chen et al., 2018), their application to translation and interpreting contexts remains nascent, though existing findings show considerable promise (Ouyang et al., 2021; Han et al., 2025, 2022). Yet, two key challenges remain. The first is the need for more fine-grained feature design and application. While coarse-grained metrics like T-unit complexity have long been valued (Ortega, 2003), recent research advocates for supplementing them with fine-grained, usage-based indices that can capture subtle structural variations and better predict language development (Norris and Ortega, 2009; Kyle and Crossley, 2017). The second challenge concerns language specificity, as most NLP tools are developed primarily for English and may not fully account for the linguistic characteristics of other languages, such as the lack of overt morphological inflections and unique phraseological constructions in Chinese (Li and Thompson, 1989; Hu et al., 2022b) This evolving landscape is further complicated by the advent of LLMs. recent large-scale study by Zhang et al. (2024b) demonstrates that GPT-4o achieves near-human accuracy in grammatical acceptability judgment, leading to questions in the optimal combination of analytical tools from established linguistic indices to emergent LLM-based judgments that offers the most robust predictive power for assessing language use in interpreting."
        },
        {
            "title": "Assessment",
            "content": "Despite the aforementioned results in automatic interpreting assessment, the empirical application of ML is hampered by two fundamental and interrelated data challenges in this domain: small sample size and imbalanced data composition. The field is largely characterized by studies that rely on small datasets (Yu and van Heuven, 2017; Lu and Han, 2022; Wang and Yuan, 2023; Wang and Wang, 2022), substantially increasing the risk of overfitting. This problem is further exacerbated by pronounced class imbalance, as most datasets are heavily skewed toward average performance, 3 with markedly fewer samples representing either very high or very low quality (Wang and Yuan, 2023; Han et al., 2025). To surmount these obstacles, data augmentation has emerged as critical methodological intervention capable of enhancing model robustness and validity (Mumuni and Mumuni, 2022). Common augmentation approaches include perturbation-based methods (adding Gaussian noise), interpolation techniques like SMOTE (Chawla et al., 2002), and generative models such as Generative Adversarial Networks (GANs) and Variational Autoencoders (VAEs) (Mumuni and Mumuni, 2022). Among these, VAE offers three key advantages for ML-based interpreting assessment (Kingma and Welling, 2014). First, its probabilistic framework captures complex interdependencies within fidelity, fluency, and language use features. Second, the continuous latent space enables smooth interpolation between existing samples to create coherent variations. Third, VAE preserves feature-label correspondence (i.e., the direct link between each samples features and its corresponding interpreting quality score), which is crucial for maintaining assessment validity. Zhang et al. (2024a) also demostrate the empirical viability of this technique."
        },
        {
            "title": "2.4 Explainable AI (XAI) and Its Application",
            "content": "in Educational Contexts As educational AI systems become more sophisticated, XAI techniques are essential for understanding and validating these systems, thereby ensuring reliability, trust, and fairness (Gilpin et al., 2018; Rudin, 2018). Current XAI techniques fall into two main categories: intrinsic and post-hoc approaches (Gilpin et al., 2018; Rudin, 2018; Arrieta et al., 2019; Linardatos et al., 2020). Intrinsic methods prioritize inherent interpretability by using transparent model architectures such as rule-based systems, decision trees, and linear models where coefficients In contrast, directly indicate feature influence. post-hoc methods explain already-trained blackbox models without altering their structure, providing insights into complex models that would otherwise remain opaque. Most popular post-hoc methods - such as SHAP (Lundberg and Lee, 2017) and LIME (Ribeiro et al., 2016) - provide feature attribution, while other methods also exist for example-based explanations and counterfactual explanations (Arrieta et al., 2019; Linardatos et al., 2020). Based on their scope, post-hoc methods Figure 2: Methodological workflow of this study. can also be categorized as either global explanation (illuminating overall model behavior across all instances) or local explanation (clarifying individual predictions). For XAI research in education, learning analytics represents the most substantial area (Parkavi et al., 2024; Balachandar and Venkatesh, 2024), while applications have also been seen in automated language assessment, with most studies concentrating on explaining factors influencing performance quality (Kumar and Boulanger, 2020; Tang et al., 2024). To our knowledge, Wang (2024) is the only existing work to focus on explainability in automated interpreting assessment, which classifies interpreting quality into 5 levels and provides global explanations of feature importance using correlation analysis."
        },
        {
            "title": "3 Method",
            "content": "As illustrated in Figure 2, this study follows structured method. First, we compile new dataset comprising 117 student interpreting recordings in the English-Chinese direction, from which range of linguistically meaningful and theoretically motivated features are extracted. To address challenges related to the small sample size and imbalanced score distribution, we employ VAE to generate new, realistic samples (Kingma and Welling, 2014). After that, several machine learning models are trained to predict interpreting quality scores across different dimensions. Finally, to explain the inner decision-making of the trained models, we conduct series of SHAP analyses."
        },
        {
            "title": "3.1 Original Dataset",
            "content": "We compile new dataset of 117 English-Chinese consecutive interpreting samples, collected from 39 undergraduate English majors at university in Shanghai, China (Mean age = 18.47 years, SD = 1.13 years). All participants, whose L1 is Chinese 4 and L2 is English, have passed CET-4 (College English Test), demonstrating satisfactory English proficiency. Before data collection, they completed 16 weeks (32 credit hours) of interpreting training. The interpreting task uses six passages adapted from authentic public speeches, each containing an equal number of sentences and controlled for sentence length (M = 18.14 words, SD = 0.78 word). Further details about the passages, along with extracted linguistic feature values, are presented in Appendix A. These texts are converted into audio format using ElevenLabs text-to-speech technology1. The resulting audio files feature standard pronunciation and averaged approximately 2 minutes in duration. Assessment of the interpreting samples is conducted by three experienced raters, each with over three years of university-level teaching experience in domestic or international settings. The evaluation employes Han (2018)s four-band, eight-point analytic rubric, which assesses the three key dimensions of interpreting quality: InfoCom (information completeness), FluDel (fluency), and TLQual (target language quality). To ensure scoring consistency, raters underwent comprehensive training before the formal assessment. Detailed descriptions of the rater training procedures, student separation reliability, and the infit and outfit mean square statistics for each rater are provided in Appendix B. To mitigate potential inconsistencies and rater bias, Many-Facet Rasch Measurement (MFRM) analysis (Linacre, 2002) is used to calibrate raw scores and establish the final ground truth scores."
        },
        {
            "title": "3.2 Audio Processing",
            "content": "To process the audio recordings of interpreting, we first use iFLYTEK ASR system2 to transcribe them into texts. To enhance annotation reliability, we implement two-stage error detection process. In the first stage, GPT-4o3 is used for grammatical error diagnosis by adapting the framework of Rao et al. (2020) and Fu et al. (2018). structured prompt template (see Appendix C) is designed to guide GPT-4os annotations, providing explicit instructions on four major error types: Redundant Words (R), Missing Words (M), Word Selection Errors (S), and Word Ordering Errors (W). To enhance the models performance and reliability, we 1https://elevenlabs.io/ 2https://global.xfyun.cn/products/ real-time-asr 3GPT-4o-2024-08-06 with temperature set to 0. provide the it with few-shot examples and instructe it to explicitly articulate its decision-making process for each identified error and provide corresponding confidence level. Particularly, we specify in the guidelines that filled pauses (e.g., uh) are not considered errors, and analysis should focus solely on the final sentence version, disregarding repetitions, false starts, or self-corrections. In the second stage, each transcription is manually reviewed and corrected. We recruited two postgraduate students in linguistics to independently annotate 100 randomly selected sentences, following the same guidelines as GPT. Inter-annotator agreement among human annotators yields Cohens Kappa coefficient of 0.86, while agreement between GPT-4o annotations and human annotations achieves Fleiss Kappa coefficient of 0.71, indicating substantial level of consistency. 3.3 Feature Extraction Each scoring dimension of interpreting is represented by distinct set of extracted features. Fluency features are extracted from the original transcript, while other features are derived from cleaned transcripts (after removing fillers, false starts, and self-repair). For InfoCom, we use five established metrics from the field of machine translation quality assessment to measure the preservation of information from source to target language  (Table 1)  . FluDel features include 14 temporal features  (Table 2)  derived from prior research (Barik, 1973; Yu and van Heuven, 2017; Song, 2020; Wang and Wang, 2022). These features can be categorized into two groups: speed fluency features (16) and breakdown fluency features (714). Features related to unfilled pauses are extracted automatically using Python packages librosa (v0.10.2) and soundfile (v0.12.1), with pauses identified based on an intensity threshold of -18 dB as recommended by Wu (2021). Additional features are derived from time-aligned transcriptions generated by the iFLYTEK ASR system. TLQual is evaluated through 25 features related to syntactic complexity and grammatical accuracy. Among them, 21 syntactic complexity features encompassing both coarse-grained and fine-grained measures  (Table 3)  are extracted using Chinese Collocation Analyzer (CCA, Hu et al., 2022b,a), which is specifically developed for L2 Chinese texts, making it particularly appropriate for E-C interpreting studies. The remaining 4 grammatical 5 Feature Short description chrF BLEURT-20 BERTScore CometKiwi-da xCOMET-XL Measures n-gram overlap between the interpreted and reference text Assesses the semantic similarity between the interpreted text and reference text based on contextualized embeddings from BERT and RemBERT Measures the similarity between interpreted and reference translations by computing cosine similarity of their contextualized embeddings using BERT reference-free regression model based on the InfoXLM architecture, trained on direct assessments from WMT17-WMT20 and the MLQE-PE corpus An extension of COMET, designed to identify error spans and assign quality scores, achieving state-of-the-art correlation with MQM error typology-derived scores Table 1: Features adopted for InfoCom assessment. Feature Full Name Description SR AR PTR MLS MLR PSC NFP NUP MLFP MLUP NRLFP NRLUP NRSA NPSA Speech Rate The overall pace of speech, calculated as the number of syllables uttered per second. The rate of syllable production, excluding pauses. The proportion of time spent vocalizing relative to the total duration. Articulation Rate Phonation Time Ratio Mean Length of Syllables Mean Length of Run Pruned Syllable Count The total syllable count after removing filled pauses. Number Pauses The frequency of filled pauses (e.g., um, uh). The average duration of each syllable. Filled of The average number of syllables produced in continuous stream. Normalized Number of Unfilled Pauses Mean Length of Filled Pauses Mean Length of Unfilled Pauses Number of Relatively Long Filled Pauses Number of Relatively Long Unfilled Pauses Number of Relatively Slow Articulations Number of Particularly Slow Articulations The frequency of silent pauses. An unfilled pause is defined as silence of 0.35 seconds or longer, consistent with recommendations for E-C interpreting (Mead, 2005). The average duration of filled pauses. The average duration of silent pauses. The number of filled pauses longer than Q3 + 1.5 * Interquartile Range (IQR) and shorter than or equal to Q3 + 3 * IQR. The number of unfilled pauses longer than Q3 + 1.5 * IQR and shorter than or equal to Q3 + 3 * IQR. The number of syllables longer than Q3 + 1.5 * IQR and shorter than or equal to Q3 + 3 * IQR. The number of syllables longer than Q3 + 3 * IQR. Table 2: 14 FluDel features examined in this work. accuracy features are derived from the grammatical error annotations by GPT-4o, specifically Number of Redundant Words (NRW), Number of Missing Words (NMW), Number of Word Selection Errors (NWSE), and Number of Word Ordering Errors (NWOE)."
        },
        {
            "title": "3.4 Data Augmentation",
            "content": "Unlike general L2 learners, interpreting students constitute smaller pool due to the advanced linguistic competence and cognitive demands required by the task. This scarcity underscores the need for data augmentation techniques to increase the quantity and diversity of learner datasets (Mumuni and Mumuni, 2022). In line with the approach proposed by Zhang et al. (2024a), we employ Variational Autoencoder (VAE) to address the challenge of score distribution imbalance in the original dataset. The primary objective is to generate realistic, synthetic feature vectors for the three distinct dimensions of interpreting quality being assessed. To achieve this, we train separate conditional VAE for each of the three dimensions. The synthetic feature vectors generated by these VAE models are then combined with the original 117 data points, resulting in an augmented dataset comprising 500 samples. 6 Coarse-Grained Phraseological Diversity Phraseological complexity Mean Length of Sentences (MLS) Mean Length of T-units (MLTU) Number of T-units Per Sentence (NTPS) Mean Length of Clauses (MLC) Number of Clauses Per Sentence (NCPS) Verb-Object Root Type-Token Ratio (VO_RTTR) Subject-Predicate Root Type-Token Ratio (SP_RTTR) Adjective-Noun Root Type-Token Ratio (AN_RTTR) Adverb-Preposition Root TypeToken Ratio (AP_RTTR) Classifier-Noun Root Type-Token Ratio (CN_RTTR) Preposition-Postposition Root TypeToken Ratio (PP_RTTR) Preposition-Verb Root Type-Token Ratio (PV_RTTR) Predicate-Complement Root TypeToken Ratio (PC_RTTR) Verb-Object Combination Ratio (VO_RATIO) Subject-Predicate Combination Ratio (SP_RATIO) Adjective-Noun Combination Ratio (AN_RATIO) Adverb-Preposition Combination Ratio (AP_RATIO) Classifier-Noun Combination Ratio (CN_RATIO) Preposition-Postposition Combination Ratio (PP_RATIO) Preposition-Verb Combination Ratio (PV_RATIO) Predicate-Complement Combination Ratio (PC_RATIO) Table 3: 21 Syntactic complexity features adopted for TLQual assessment. 3.5 Model Training and Validation tion accuracy. Three types of machine learning models XGBoost, Random Forest (RF), and Multi-Layer Perceptron (MLP) are employed to predict the InfoCom, FluDel, and TLQual scores. The modeling process followes systematic procedure that consists of feature extraction, feature standardization, data splitting, model training and validation, and model testing (Mienye and Sun, 2022). All extracted features (as detailed in Section 3.3) are first standardized using z-score normalization. The initial dataset is then split into training (80%) and testing (20%) subsets. Following the data split, model training and validation are conducted with five-fold cross-validation and grid search for hyperparameters, using root mean square error (RMSE) as validation criterion. After cross-validation and hyperparameter optimization, the best-performing configuration is selected for each model. Each final model is then retrained on the entire training set using the optimal hyperparameters and subsequently evaluated on the held-out test set to assess its predictive performance on unseen data. Multiple evaluation metrics are employed in this stage to provide comprehensive assessment of model quality, including: (1) RMSE: measures the magnitude of prediction errors. (2) Spearmans (Ï): assesses the monotonic relationship between predicted and actual scores. (3) Mean absolute error (MAE): quantifies the average absolute deviation between predicted and actual scores, providing direct measure of predic- (4) Mann-Whitney Test: determines whether there are significant differences in the distributions of predicted and actual scores. (5) Exact Agreement rate (EAR): quantifies the proportion of predictions that exactly match the actual scores after both are rounded to the nearest integer. Rounding is required because our models predict continuous MFRM-calibrated scores (1-8), and agreement is typically assessed against discrete levels. (6) Adjacent Agreement rate (AAR): measures the proportion of predictions that fall within one integer unit (either +1 or -1) of the actual scores after both are rounded to the nearest integer. Beyond these overall metric values, we also perform case studies of prediction errors to gain more in-depth insights into specific aspects of the models performance."
        },
        {
            "title": "3.6 Result Explanation Using XAI Techniques",
            "content": "We further employ SHAP to interpret model bethe overall model (global havior at two levels: explanations) and individual predictions (local explanations). Global explanations offer broader perspective by summarizing the overall impact of features across the entire dataset. Local explanations, on the other hand, provide insights into how individual features influence single predicted outcome. These analyses are implemented using the shap library4. 4https://shap.readthedocs.io/en/latest/index. html 7 Figure 3: Pairwise correlation heatmap between features and scores."
        },
        {
            "title": "4.1 Descriptive Statistics",
            "content": "Due to the consistently reasonable level of interpreting proficiency demonstrated by all student participants, the dataset lacks samples with scores in the 1-2 range. However, Figure 4 demonstrates that data augmentation has successfully achieved an approximately uniform distribution of interpretation scores on the remaining range. Table 4 further reveals that compared with the original data, the augmented data exhibits very close mean values and marginally insreased standard deviations in all three dimensions. The descriptive statistics for all features used in this work are provided in Appendix D, and the pairwise Spearmans correlations between features and scores in both the original and augmented datasets are illustrated in Figure 3. Score Mean SD Skewness Kurtosis InfoCom FluDel TLQual Raw Aug. Raw Aug. Raw Aug. 5.32 5.33 4.93 4.95 5.21 5.24 1.35 1.47 0.77 0.98 0.95 1.06 -0.37 -0.05 -0.31 -0.10 -0.23 0. 2.25 -0.51 2.94 -0.67 3.38 -0.85 Table 4: Descriptive statistics for scores from the raw data and augmented data."
        },
        {
            "title": "4.2 Effectiveness of Models Trained on Raw",
            "content": "and Augmented Data As shown in Table 5, XGBoost trained on the augmented dataset achieves the highest performance Figure 4: Distribution of raw (left), generated (middle), and augmented (right) data. in predicting FluDel and TLQual scores, representing an improvement over its already robust performance on the raw dataset. For InfoCom prediction, the RF regressor trained on augmented data yields the best results, also substantially outperforming the same model trained on raw data. In contrast, MLP consistently exhibits the lowest performance, though also showing notable improvement when trained on augmented data. In Appendix E, we provide detailed analyses of instances where model predictions diverge greatly from human scores, offering nuanced insights into the models perfor8 Score Model Data RMSE Spearman MAE Mann-Whitney EAR AAR XGBoost InfoCom RF MLP XGBoost FluDel RF MLP XGBoost TLQual RF"
        },
        {
            "title": "MLP",
            "content": "raw aug. raw aug. raw aug. raw aug. raw aug. raw aug. raw aug. raw aug. raw aug. 1.36 1.17 1.42 1.05 2.43 1.25 0.84 0.68 0.70 0.61 1.74 1.20 0.87 0.75 0.97 0.92 1.58 1. 0.49 0.62 0.51 0.68 0.43 0.58 0.69 0.87 0.65 0.86 0.39 0.53 0.66 0.79 0.58 0.73 0.45 0.62 0.95 0.49 0.87 0.41 1.21 0.79 0.65 0.41 0.68 0.43 1.17 0.89 0.72 0.45 0.86 0.54 1.10 0.83 259 (p = 0.70) 5751 (p = 0.12) 209 (p = 0.45) 5693 (p = 0.15) 215 (p = 0.53) 5744 (p = 0.12) 272 (p = 0.49) 5375 (p = 0.36) 274 (p = 0.46) 5302 (p = 0.46) 274 (p = 0.46) 4621 (p = 0.36) 267 (p = 0.41) 5386 (p = 0.33) 232 (p = 0.42) 5522 (p = 0.20) 206 (p = 0.40) 4973 (p = 0.95) 0.63 0.71 0.67 0.77 0.54 0.68 0.69 0.72 0.71 0.75 0.54 0.64 0.67 0.76 0.63 0.78 0.58 0.69 0.83 0.86 0.88 0.90 0.75 0.77 0.83 0.91 0.83 0.93 0.71 0. 0.83 0.91 0.79 0.89 0.75 0.85 Table 5: Performance of machine learning regressors trained on raw and augmented data. < 0.01; < 0.05. mance characteristics."
        },
        {
            "title": "4.3 Global explanations of model prediction",
            "content": "Figure 1 (left) illustrates the global feature importance of the best-performing RF regressor for InfoCom score prediction. Among these, BLEURT (M = 0.32, 95% CI5 = [0.25, 0.37]), CometKiwi (M = 0.17, 95% CI = [0.08, 0.26]), and chrF (M = 0.07, 95% CI = [0.04, 0.09]) demonstrate the highest mean SHAP values. In other words, higher values of these metrics are positively associated with higher predicted InfoCom scores. As illustrated in Figure 1 (middle), NFP (M = - 0.17, 95% CI = [-0.27, -0.10]) exhibits the strongest negative effect on FluDel scores, with higher NFP values leading to lower predictions by the XGBoost regressor. Similarly, other breakdown fluency features, including MLUP, NUP, and MLFP also negatively impact predicted outcomes. Speed fluency features such as PSC, SR, PTR, and MLS have positive but very small impact on the models 5To assess the stability of feature contributions, bootstrap procedure is conducted with 1,000 resamples drawn from the augmented dataset. For each bootstrap sample, SHAP values are computed using the best-performing ML model. The mean SHAP value for each feature is recorded across iterations to estimate its average effect on predictions. 95% Confidence intervals (CI) are calculated as the 2.5th and 97.5th percentiles of the bootstrapped distribution, capturing both the direction and magnitude of each features influence. predictions, while MLR yields negative effect instead. Figure 1 (right) demonstrates that the grammatical accuracy index NWSE (M = -0.09, 95% CI = [- 0.15, -0.04]) has an inverse relationship with model predictions, indicating that higher frequency of word selection errors corresponds to lower predicted scores. Among phraseological complexity features, CN_RATIO (M = 0.25, 95% CI = [0.18, 0.31]) has the most significant influence, with higher values leading to increased predictions. In addition, group of phraseological diversity metrics also contribute positively to model output, including PP_RTTR and PV_RTTR. In contrast, AP_RTTR and PC_RTTR exhibit negative effects. For coarse-grained features, higher MLC values are associated with lower model predictions, while MLS positively influences predicted outcomes."
        },
        {
            "title": "4.4 Local explanations of model prediction",
            "content": "Figure 5 illustrates the SHAP force plot for the InfoCom prediction of Sample 25, providing detailed depiction of individual feature contributions. The plot is centered around the base value (approximately 5.4), representing the mean model output across the training dataset. The cumulative contributions of the InfoCom features slightly elevate the prediction to 5.66. Among these, BLEURT 9 Figure 5: SHAP force plot for the InfoCom prediction of Sample 25. Figure 6: SHAP waterfall plot for the predicted FluDel score of Sample 50. and COMET-Kiwi exert the most significant positive influence, whereas chrF contributes negatively. The relatively high BLEURT and COMET-Kiwi scores suggest that Sample 25 retains most of the source information, albeit with some loss, while the markedly low chrF score indicates substantial lexical and syntactic divergence from the reference text. In Figure 6, the SHAP waterfall plot for the FluDel prediction of Sample 50 is shown. The expected value E[f(x)] = 4.991 represents the mean model output across the training dataset. Feature contributions collectively reduce the prediction to f(x) = 4.746. Among these, the pause-related features - NFP, MLUP, and NUP - exhibit the most pronounced negative impact, decreasing the prediction by 0.22, 0.16, and 0.1, respectively. Conversely, MLR has the strongest positive effect, increasing the prediction by 0.2. These findings suggest that the interpreter may need to enhance pause management by minimizing both the frequency and duration of pauses while striving for more extended, uninterrupted speech production. The SHAP waterfall plot for the TLQual prediction of Sample 87 is depicted in Figure 7. The models expected value is E[f(x)] = 5.258, with Figure 7: SHAP waterfall plot for the predicted TLQual score of Sample 87. feature contributions collectively increasing the prediction to 6.466. Among these, CN_RATIO is the most influential positive factor, increasing the prediction by 0.47. Other contributing features include PC_RTTR, AP_RTTR, PV_RTTR, and AP_RATIO. Conversely, PP_RTTR exerts the most significant negative influence, reducing the prediction by 0.44, with additional negative contributions from PV_RATIO and MLC. These results indicate that the diversified and sophisticated use of CN, PC, AP, PV, and AP structures aligns with typical language patterns in this context. However, excessive use of PP structures (e.g. å¨...ä¸, å½...æ¶) appears detrimental. Additionally, the negative impact of MLC suggests that complex clauses could be restructured into simpler sentences or reformulated using topic-comment structure, common grammatical pattern in Chinese."
        },
        {
            "title": "5.1 Modeling effectiveness and the impact of",
            "content": "data augmentation Our analysis indicates that the selected machine learning algorithms demonstrate robust performance on the augmented dataset, with RF yielding the best results for InfoCom score estimation and XGBoost performing best on FluDel and TLQual. 10 Comparing with previous models that only perform well on middle range scores but failing at lower and higher ranges (Han et al., 2025; Wang and Yuan, 2023), our results underscore the importance of data augmentation in improving model performance, particularly for predicting scores at extreme ends of the scale. 5.2 Global explanations of feature importance Information completeness Our SHAP analysis identifies the two neural-based metrics, BLEURT and CometKiwi, as having the greatest influence on the global prediction of InfoCom scores, aligning with previous research by Han and Lu (2025). The superior performance of BLEURT is likely attributable to its extensive pre-training on synthetic data and its ability to incorporate diverse lexical and semantic signals, which enables the metric to capture more nuanced linguistic patterns compared to BERTScore (Han and Lu, 2025). Conversely, the relatively low performance of XCOMET may stem from misalignment between its training paradigm (error annotation) and the assessment context (analytical rubric scoring). Fluency of delivery Our findings reveal that NFP has the most pronounced negative impact on the models global prediction of FluDel scores, followed by other pause-related features including MLUP, NUP, and MLFP, aligning with previous findings (Yu and van Heuven, 2017). In contrast, most speed fluency features (e.g. PSC, PTR, SR) exhibit small positive effects, although higher MLR values are linked to decreased predictions. We hypothesize that the negative role of MLR stems from the phenomenon that excessively long runs do not reflect controlled, fluent delivery but rather form of run-on speech. The interpreter, under high cognitive load, may be rushing to output information without strategic pausing for emphasis or listener comprehension (Lennon, 1990; Mead, 2005), leading to human raters perceiving the speech as poorly managed and difficult to process. Target language quality Among the GPT-4oannotated features, NWSE exerts significant negative effect on model predictions, underscoring the foundational role of grammatical accuracy in human judgments of language quality and mirroring findings in L2 speaking assessment (Li et al., 2024). Regarding length-related features, MLS has positive effect on predictions, aligning with Zechner et al. (2017). MLC, on the other hand, yields negative impact, which is in sharp contrast to findings from other contexts such as L2 German and English speaking (Neary-Sundquist, 2017; BultÃ© and Roothooft, 2020). This divergence likely stems from typological differences: the topic-comment structure of Chinese prioritizes discourse coherence, whereas the syntactic elaboration common in English relies more heavily on complex clausal dependencies (Li and Thompson, 1989). This suggests that in the Chinese interpreting context, longer but less syntactically dense sentences are perceived as higher quality. Another key finding is the superior predictive importance of fine-grained features over coarsegrained ones. Within this category, features reflecting phraseological diversity (PC_RTTR, PP_RTTR, SP_RTTR, AP_RTTR, PV_RTTR) are more influential than the single phraseological complexity feature (CN_RATIO). Furthermore, our results reveal that Chinese-specific phraseological features (CN, PC, PP, PV) demonstrate greater importance than their language-independent counterparts (SP, AP). Taken together, these findings point towards the possibility that for E-C consecutive interpreting, robust assessment of language use relies less on traditional measures of clausal complexity and more on the diverse and accurate use of languagespecific phrasal units."
        },
        {
            "title": "5.3 The critical role of local explanations in\nautomated interpreting assessment",
            "content": "Local explanations in automated interpreting assessment offer significant value for both teaching and learning practices (Kumar and Boulanger, 2020; Tang et al., 2024; Gilpin et al., 2018; Rudin, 2018; Linardatos et al., 2020). For educators, these explanations provide actionable insights into the specific strengths and weaknesses of individual students performances by highlighting the features that positively or negatively influence predicted scores. This enables teachers to tailor feedback and instructional strategies to target precise areas for improvement. For students, local explanations empower students to take ownership of their learning by focusing on specific performance aspects that require attention. Take the SHAP-based local explanation of FluDel prediction for Sample 50 as an example. Notably, pause-related features emerge as the primary detractors: NFP reduces the prediction by 0.22, MLUP by 0.16, and NUP by 0.1, indicating the students difficulty with hesitation management. 11 To address this, instructors can implement targeted exercises such as shadowing practices, where students reproduce source language with minimal delay (Christoffels and de Groot, 2004). The instructor could also implement targeted drills requiring students to deliver short segments without hesitation, progressively extending segment length while monitoring pause reduction. For reducing unfilled pauses specifically, anticipation exercises help students predict upcoming content elements, thereby decreasing processing latency (Chmiel, 2020). Additionally, instructing in chunking strategies - organizing information into manageable units - can alleviate cognitive load that frequently manifests as extended pauses (Thalmann et al., 2019). In addition, the quantitative nature of SHAP values also allows instructors to prioritize interventions effectively. For this particular student, addressing filled pauses should take precedence over lengthy unfilled pauses, given its greater negative impact (0.22 vs. 0.16). Furthermore, tracking these SHAP contributions longitudinally across multiple performances enables instructors to monitor learning progression and intervention effectiveness, facilitating timely adjustments to teaching approaches as needed."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose an effective framework integrating feature engineering, ML models, data augmentation, and XAI for the multi-dimensional assessment of interpreting quality. key finding is that VAE-based data augmentation substantially enhances model performance. Global XAI analysis reveales that fidelity prediction is most sensitive to neural-embedding metrics such as BLEURT, while fluency scores are primarily influenced by breakdown features, with NFP exerting the strongest negative effect. Target language quality, in turn, depends heavily on language-specific phraseological features, notably CN_RATIO. These global insights are complemented by in-depth local explanations, which effectively diagnose individual strengths and weaknesses in performance. Looking forward, our method provides promising direction in translating XAI-driven insights into pedagogical tools that deliver actionable feedback to trainees, thereby bridging the gap between automated assessment and student learning."
        },
        {
            "title": "References",
            "content": "Alejandro Barredo Arrieta, Natalia DÃ­az RodrÃ­guez, Javier Del Ser, Adrien Bennetot, Siham Tabik, A. Barbado, Salvador GarcÃ­a, Sergio Gil-Lopez, Daniel Molina, Richard Benjamins, Raja Chatila, and Francisco Herrera. 2019. Explainable artificial intelligence (xai): Concepts, taxonomies, opportunities and challenges toward responsible ai. Inf. Fusion, 58:82115. V. Balachandar and K. Venkatesh. 2024. multidimensional student performance prediction model (mspp): An advanced framework for accurate academic classification and analysis. MethodsX, 14. Henri C. Barik. 1973. Simultaneous interpretation: Language and Temporal and quantitative data. Speech, 16:237 270. Bram BultÃ© and Hanne Roothooft. 2020. Investigating the interrelationship between rated l2 proficiency and linguistic complexity in l2 speech. System. Sheila Castilho, Stephen Doherty, Federico Gaspari, and Joss Moorkens. 2018. Approaches to human and machine translation quality assessment. Nitesh V. Chawla, Kevin W. Bowyer, Lawrence O. Hall, and W. Philip Kegelmeyer. 2002. SMOTE: synthetic minority over-sampling technique. J. Artif. Intell. Res., 16:321357. Lei Chen, Klaus Zechner, Su-Youn Yoon, Keelan Evanini, Xinhao Wang, Anastassia Loukina, Jidong Tao, Lawrence Davis, Chong Min Lee, Min Ma, Robert Mundkowsky, Chi-Jui Lu, Chee Wee Leong, and Binod Gyawali. 2018. Automated scoring of nonnative speech using the speechrater sm v. 5.0 engine. ETS Research Report Series, 2018:131. Sijia Chen. 2024. Effects of subtitles on vocabulary learning through videos: An exploration across different learner types. The Journal of Specialised Translation. Agnieszka Chmiel. 2020. Effects of simultaneous interpreting experience and training on anticipation, as measured by word-translation latencies. Ingrid Christoffels and Annette M.B. de Groot. 2004. Components of simultaneous interpreting: Comparing interpreting with shadowing and paraphrasing. Bilingualism: Language and Cognition, 7:227 240. Yanping Dong and Zhilong Xie. 2014. Contributions of second language proficiency and interpreting experience to cognitive control differences among young adult bilinguals. Journal of Cognitive Psychology, 26:506 519. Ruiji Fu, Zhengqi Pei, Jiefu Gong, Wei Song, Dechuan Teng, Wanxiang Che, Shijin Wang, Guoping Hu, and Ting Liu. 2018. Chinese grammatical error diagnosis using statistical and prior knowledge driven features with probabilistic ensemble enhancement. In Proceedings of the 5th Workshop on Natural Language 12 Processing Techniques for Educational Applications, NLP-TEA@ACL 2018, Melbourne, Australia, July 19, 2018, pages 5259. Association for Computational Linguistics. Daniel Gile. 2021. The effort models of interpreting as didactic construct. Advances in Cognitive Translation Studies. Leilani H. Gilpin, David Bau, Ben Z. Yuan, Ayesha Bajwa, Michael A. Specter, and Lalana Kagal. 2018. Explaining explanations: An overview of interpretability of machine learning. In 5th IEEE International Conference on Data Science and Advanced Analytics, DSAA 2018, Turin, Italy, October 1-3, 2018, pages 8089. IEEE. Arthur C. Graesser, Danielle S. McNamara, Max M. Louwerse, and Zhiqiang Cai. 2004. Coh-metrix: Analysis of text on cohesion and language. Behavior Research Methods, Instruments, & Computers, 36:193202. Nuno Miguel Guerreiro, Ricardo Rei, Daan van Stigt, LuÃ­sa Coheur, Pierre Colombo, and AndrÃ© F. T. Martins. 2024. xcomet : Transparent machine translation evaluation through fine-grained error detection. Trans. Assoc. Comput. Linguistics, 12:979995. Chao Han. 2015. (para)linguistic correlates of perceived fluency in english-to-chinese simultaneous interpretation. International Journal of Comparative Literature and Translation Studies, 3:3237. Chao Han. 2018. Using analytic rating scales to assess english/chinese bi-directional interpretation: longitudinal rasch analysis of scale utility and rater behavior. Linguistica Antverpiensia, New Series Themes in Translation Studies. Chao Han and Xiaolei Lu. 2021. Can automated machine translation evaluation metrics be used to assess students interpretation in the language learning classroom? Computer Assisted Language Learning, 36:1064 1087. Chao Han and Xiaolei Lu. 2025. Beyond bleu: Repurposing neural-based metrics to assess interlingual interpreting in tertiary-level language learning settings. Research Methods in Applied Linguistics. Chao Han, Xiaolei Lu, and Shirong Chen. 2025. Modeling rater judgments of interpreting quality: Ordinal logistic regression using neural-based evaluation metrics, acoustic fluency measures, and computational linguistic indices. Research Methods in Applied Linguistics. Chao Han and Liuyan Yang. 2023. Relating utterance fluency to perceived fluency of interpreting. Translation and Interpreting Studies. The Journal of the American Translation and Interpreting Studies Association, 18(3):421447. Chao Han, Binghan Zheng, Mingqing Xie, and Shirong Chen. 2024. Raters scoring process in assessment of interpreting: an empirical study based on eye tracking and retrospective verbalisation. The Interpreter and Translator Trainer, 18:400 422. Tianyi Han, Dechao Li, Xingcheng Ma, and Nan Hu. 2022. Comparing product quality between translation and paraphrasing: Using nlp-assisted evaluation frameworks. Frontiers in Psychology, 13. Renfen Hu, Jifeng Wu, and Xiaofei Lu. 2022a. Chinese collocation analyzer (cca). Renfen Hu, Jifeng Wu, and Xiaofei Lu. 2022b. Wordcombination-based measures of phraseological diversity, sophistication, and complexity and their relationship to second language chinese proficiency and writing quality. Language Learning. Yichen Jia and Vahid Aryadoust. 2023. The utility of generative artificial intelligence in rating interpreters accuracy: case study of chatgpt-4. Diederik P. Kingma and Max Welling. 2014. AutoIn 2nd International encoding variational bayes. Conference on Learning Representations, ICLR 2014, Banff, AB, Canada, April 14-16, 2014, Conference Track Proceedings. Vivekanandan Kumar and David Boulanger. 2020. Explainable automated essay scoring: Deep learning really has pedagogical value. In Frontiers in Education. Kristopher Kyle. 2016. Measuring syntactic development in l2 writing: Fine grained indices of syntactic complexity and usage-based indices of syntactic sophistication. Kristopher Kyle and Scott Andrew Crossley. 2017. Assessing syntactic sophistication in l2 writing: usage-based approach. Language Testing, 34:513 535. Ngoc-Tien Le, Benjamin Lecouteux, and Laurent Besacier. 2016. Joint ASR and MT features for quality estimation in spoken language translation. In Proceedings of the 13th International Conference on Spoken Language Translation, IWSLT 2016, Seattle, WA, USA, December 8-9, 2016. International Workshop on Spoken Language Translation. Sang-Bin Lee. 2019. Holistic assessment of consecutive interpretation. Interpreting. International Journal of Research and Practice in Interpreting. T. Lee. 2013. Incorporating translation into the language classroom and its potential impacts upon l2 learners. P. Alan Lennon. 1990. Investigating fluency in efl: quantitative approach. Language Learning, 40:387 417. 13 C.N. Li and S.A. Thompson. 1989. Mandarin Chinese: Functional Reference Grammar. Linguistics: Asian studies. University of California Press. Wenchao Li, Zhentao Zhong, and Haitao Liu. 2024. computer-assisted tool for automatically measuring non-native japanese oral proficiency. Computer Assisted Language Learning. John M. Linacre. 2002. What do infit and outfit, meansquare and standardized mean? Rasch Measurement Transactions, 16:878. Pantelis Linardatos, Vasilis Papastefanopoulos, and Sotiris B. Kotsiantis. 2020. Explainable ai: review of machine learning interpretability methods. Entropy, 23. Xiaofei Lu. 2010. Automatic analysis of syntactic comInternational plexity in second language writing. Journal of Corpus Linguistics, 15:474496. Xiaolei Lu and Chao Han. 2022. Automatic assessment of spoken-language interpreting based on machinetranslation evaluation metrics. Interpreting. International Journal of Research and Practice in Interpreting. Scott M. Lundberg and Su-In Lee. 2017. unified approach to interpreting model predictions. In Neural Information Processing Systems. Peter Mead. 2005. Methodological issues in the study of interpreters fluency. C. Mellinger. 2018. Translation, interpreting, and language studies: Confluence and divergence. Hispania, 100:241 246. Ibomoiye Domor Mienye and Yanxia Sun. 2022. survey of ensemble learning: Concepts, algorithms, applications, and prospects. IEEE Access, 10:99129 99149. Alhassan G. Mumuni and Fuseini Mumuni. 2022. Data augmentation: comprehensive survey of modern approaches. Array, 16:100258. Colleen A. Neary-Sundquist. 2017. Syntactic complexity at multiple proficiency levels of l2 german speech. International Journal of Applied Linguistics, 27:242 262. John M. Norris and Lourdes Ortega. 2009. Towards an organic approach to investigating caf in instructed sla: The case of complexity. Applied Linguistics, 30:555578. Lourdes Ortega. 2003. Syntactic complexity measures and their relationship to l2 proficiency: research synthesis of college-level l2 writing. Applied Linguistics, 24:492518. Kishore Papineni, Salim Roukos, Todd Ward, and WeiJing Zhu. 2002. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th Annual Meeting of the Association for Computational Linguistics, July 6-12, 2002, Philadelphia, PA, USA, pages 311318. ACL. R. Parkavi, P. Karthikeyan, and A. Sheik Abdullah. 2024. Enhancing personalized learning with explainable ai: chaotic particle swarm optimization based decision support system. Appl. Soft Comput., 156:111451. Franz PÃ¶chhacker. 2001. Quality assessment in conference and community interpreting. Meta: Translators Journal, 46:410425. Maja Popovic. 2015. chrf: character n-gram f-score for automatic MT evaluation. In Proceedings of the Tenth Workshop on Statistical Machine Translation, WMT@EMNLP 2015, 17-18 September 2015, Lisbon, Portugal, pages 392395. The Association for Computer Linguistics. Gaoqi Rao, Erhong Yang, and Baolin Zhang. 2020. Overview of nlptea-2020 shared task for chinese grammatical error diagnosis. Proceedings of the 6th Workshop on Natural Language Processing Techniques for Educational Applications. Ricardo Rei, Marcos V. Treviso, Nuno Miguel Guerreiro, Chrysoula Zerva, Ana C. Farinha, Christine Maroti, JosÃ© G. C. de Souza, Taisiya Glushkova, Duarte M. Alves, LuÃ­sa Coheur, Alon Lavie, and AndrÃ© F. T. Martins. 2022. Cometkiwi: Ist-unbabel 2022 submission for the quality estimation shared task. In Proceedings of the Seventh Conference on Machine Translation, WMT 2022, Abu Dhabi, United Arab Emirates (Hybrid), December 7-8, 2022, pages 634645. Association for Computational Linguistics. Marco TÃºlio Ribeiro, Sameer Singh, and Carlos Guestrin. 2016. \"why should trust you?\": Explaining the predictions of any classifier. In Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, San Francisco, CA, USA, August 13-17, 2016, pages 1135 1144. ACM. Cynthia Rudin. 2018. Stop explaining black box machine learning models for high stakes decisions and use interpretable models instead. Nature Machine Intelligence, 1:206 215. Thibault Sellam, Dipanjan Das, and Ankur P. Parikh. 2020. BLEURT: learning robust metrics for text generation. In Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics, ACL 2020, Online, July 5-10, 2020, pages 78817892. Association for Computational Linguistics. Ling Ouyang, Qianxi Lv, and Junying Liang. 2021. Cohmetrix model-based automatic assessment of interpreting quality. Shuxian Song. 2020. Fluency in simultaneous interpreting of trainee interpreters : the perspectives of cognitive, utterance and perceived fluency. 14 Ziyin Zhang, Yikang Liu, Weifang Huang, Junyu Mao, Rui Wang, and Hai Hu. 2024b. MELA: multilingual evaluation of linguistic acceptability. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), ACL 2024, Bangkok, Thailand, August 11-16, 2024, pages 26582674. Association for Computational Linguistics. Nan Zhao. 2022. Speech disfluencies in consecutive interpreting by student interpreters: The role of language proficiency, working memory, and anxiety. Frontiers in Psychology, 13. U. Stachl-Peier. 2020. Translating, interpreting, mediating: The cefr and advanced-level language learning in the digital age. Catherine Stenzl. 1983. Simultaneous interpretation: Groundwork towards comprehensive model. Craig Stewart, Nikolai Vogler, Junjie Hu, Jordan L. Boyd-Graber, and Graham Neubig. 2018. Automatic estimation of simultaneous interpreter performance. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics, ACL 2018, Melbourne, Australia, July 15-20, 2018, Volume 2: Short Papers, pages 662666. Association for Computational Linguistics. Xiaoyi Tang, Hongwei Chen, Daoyu Lin, and Kexin Li. 2024. Incorporating fine-grained linguistic features and explainable ai into multi-dimensional automated writing assessment. Applied Sciences. Parveneh Tavakoli and Peter Skehan. 2005. Strategic planning, task structure and performance testing. Mirko Thalmann, Alessandra S. Souza, and Klaus Oberauer. 2019. How does chunking help working memory? Journal of Experimental Psychology: Learning, Memory, and Cognition, 45:3755. Xiaoman Wang. 2024. Developing an automated graded assessment system for english/chinese interpreting. Xiaoman Wang and Binhua Wang. 2022. Identifying fluency parameters for machine-learning-based automated interpreting assessment system. Perspectives, 32:278 294. Xiaoman Wang and Lu Yuan. 2023. Machine-learning based automatic assessment of communication in interpreting. In Frontiers in Communication. Zhiwei Wu. 2021. Chasing the unicorn? the feasibility of automatic assessment of interpreting fluency. Wenting Yu and Vincent J. van Heuven. 2017. Predicting judged fluency of consecutive interpreting from acoustic measures: Potential for automatic assessment and pedagogic implications. Interpreting, 19:4768. Klaus Zechner, Su-Youn Yoon, S. Bhat, and Chee Wee Leong. 2017. Comparative evaluation of automated scoring of syntactic competence of non-native speakers. Comput. Hum. Behav., 76:672682. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Q. Weinberger, and Yoav Artzi. 2020. Bertscore: Evaluating text generation with BERT. In 8th International Conference on Learning Representations, ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020. OpenReview.net. Yidi Zhang, Margarida Lucas, Pedro Bem-haja, and LuÃ­s Pedro. 2024a. The effect of student acceptance on learning outcomes: Ai-generated short videos versus paper materials. Comput. Educ. Artif. Intell., 7:100286."
        },
        {
            "title": "A More Details on Source Materials",
            "content": "Passage Theme DESWC DESSL DESWLlt LDTTRa RDFRE RDFKGL RDL2 1 2 3 4 5 6 Migration Migration Festival Festival Social equality Social equality 185 193 182 191 179 185 19.32 18.91 19.75 18.86 19.44 19. 5.11 5.42 5.16 5.32 5.23 5.28 0.72 0.65 0.75 0.68 0.74 0.66 35.25 41.12 29.74 42.18 45.36 33.33 15.05 16.23 16.51 14.66 13.28 15.73 13.37 8.35 8.90 10.20 7.39 11.46 Table 6: Basic information on the six passages used in the interpreting tasks. DESWC: word count; DESSL: sentence length (number of words); DESWLlt: word length (mean); LDTTRa: lexical densitiy (type-token ratio); RDFRE: Flesch Reading Ease; RDFKGL: Flesch-Kincaid Grade Level; RDL2: L2 Readability."
        },
        {
            "title": "B Rater Training Procedures",
            "content": "To familiarize the raters with the assessment procedures, we arranged an online training session via video conferencing software. Two authors of this study introduced the source texts and corresponding reference interpretations, and clarified certain key terms within the analytic rating scales (e.g. filled pauses, long silence, and excessive repairs). Raters were actively encouraged to seek clarification on any aspect of the rating task, so as to ensure shared understanding of the assessment criteria. To enhance rating consistency, pre-scored, representative interpretations from each band were played and analyzed collectively. This served to illustrate the typical features associated with different performance levels. Subsequently, the raters independently completed trial ratings of five additional interpretations. After that, they engaged in collaborative discussion, comparing their scores and providing justifications for their rating decisions. The formal rating was also conducted remotely, with each rater receiving secure online access to all necessary materials, including the source texts, reference translations, and the anonymized interpretations. To ensure ample time for thorough evaluation, raters were given two weeks to complete their assessments."
        },
        {
            "title": "Infit MnSq Outfit MnSq Rater reliability Person Separation reliability",
            "content": "Rater 1 InfoCom Rater 2 Rater 3 Rater 1 Rater 2 Rater 3 Rater 1 Rater 2 Rater"
        },
        {
            "title": "FluDel",
            "content": "1.02 0.84 0.78 1.15 1.04 0.89 1.07 0.90 0.82 1.01 0.79 0.65 1.08 1.01 0.75 0.99 0.93 1.04 0.97 0.98 0.96 0. 0.81 0.76 Table 7: Infit, outfit, rater reliability, and person separation reliability statistics from the MFRM analysis."
        },
        {
            "title": "C Prompt for Chinese Grammatical Error Diagnosis",
            "content": "Prompt for Chinese grammatical error diagnosis **Instruction** You are Chinese grammar expert. Your task is to diagnose and correct grammatical errors in Chinese sentences or longer texts. Follow the steps and guidelines below meticulously: 1. Error Detection and Analysis Order Analyze the input text for potential errors in the following priority order: - Redundancy (R): Repeated words or characters that unnecessarily clutter the sentence. - Missing Words (M): Omitted words or particles that make the sentence incomplete or ambiguous. - Word Selection (S): Inappropriate or inaccurate word choices that should be replaced by more context-appropriate terms. - Word Order (W): Incorrect arrangement of words or phrases that distorts the intended meaning. 2. Error Description and Correction For each detected error: - Describe the nature of the error. - Propose correction that clarifies the meaning while preserving the original intent. - Assign confidence score (01) representing your certainty in the correction. (Scores closer to 1 indicate high confidence.) 3. Re-examination for Low Confidence If an error receives confidence score below 0.7, re-examine it by asking: - Does this correction improve the sentence without introducing ambiguity? - Is the error type correctly classified? Revise the correction if necessary before finalizing your output. 4. Handling Special Cases The following special cases should be addressed: - Filled Pauses: Words such as å, é¢, and å¯ (and similar utterance markers) are considered fillers and should be ignored during error analysis. Do not report these as grammatical errors. - Repeated Phrases, False Starts, and Self-Corrections: Only analyze the final output of the sentence. Ignore any extraneous parts resulting from repetition or self-correction. 5. Output Formatting For every detected error, output an entry using the following format: [sentence_id, start_index, end_index, error_type, corrected_text, confidence] - sentence_id: unique identifier for the sentence (or text segment) under analysis. - start_index and end_index: The character positions (based on the sentences index) where the error occurs. - error_type: One of the following codes: (Redundancy), (Missing Words), (Word Selection), or (Word Order). - corrected_text: The proposed correction. - confidence: numerical value between 0 and 1 that represents your certainty. 6. Multiple Errors Note that sentence or text passage may contain more than one error. In such cases, output each error as separate entry. 7. Examples for Illustration 17 - Example 1: Simple Redundancy Correction - Input: ææ¨å¤©å»å­¦æ ¡å­¦æ ¡äº - Expected Output: [1, 6, 7, R, å­¦æ ¡, 0.95] - Reasoning: The particle äº is repeated unnecessarily (positions 67). The extra äº should be removed. High confidence is given due to the unambiguous redundancy. - Example 2: Word Order Correction - Input: ä»è·å¾å¿«æ¯æè¿ - Expected Output: [2, 4, 6, W, æ¯æè¿å¿«, 0.85] - Reasoning: The phrase è·å¾å¿«æ¯æè¿ is mis-ordered. Reordering to æ¯æè¿å¿« aligns with natural Chinese word order. - Example 3: Word selection improvement - Input: ä¸åçç®¡çç§»æ°æ´»å¨ä¼é æç§»æ°è¿å¥è®¸å¤å±é©çè·¯çº¿, ä¹ä¼é­å°äººå£è´©åèçæ® å¿é­çª - Expected Output: Entry 1: [3, 10, 11, S, è®©, 0.95] Entry 2: [3, 25, 28, S, ç§»æ°ä¼è½å¥, 0.90] - Reasoning: Entry 1: é æ is not suitable verb. Entry 2: é­å° is not natural collocation with é­çª. The verb è½å¥ better conveys that immigrants fall into the clutches (é­çª) of human traffickers. Additionally, the extra adverb ä¹ is unnecessary. - Example 4: Handling Special Cases - Input: åæè§å¾ä»å¤©çä¼è®®å¯æ²¡å¥å¤§é®é¢ - Expected Output: No error entries. - Reasoning: å or å¯ are neglected. Only the final phrasing after self-corrections and filler pauses should be examined for genuine grammatical issues."
        },
        {
            "title": "D Complete Feature Statistics",
            "content": "Feature Mean SD Skewness Kurtosis Raw Aug. Raw Aug. Raw Aug. Raw Aug. CometKiwi BertScore chrF BLEURT-20 XCOMET NUP MLUP MLFP NFP MLR PSC PTR MLS SR AR NRSA NPSA NRLFP NRLUP NRW NMW NWSE NWOE MLC MLTU NCPS NTPS TOTAL_RTTR VO_RATIO VO_RTTR SP_RATIO SP_RTTR AN_RATIO AN_RTTR AP_RATIO AP_RTTR CN_RATIO CN_RTTR PP_RATIO PP_RTTR PV_RATIO PV_RTTR PC_RATIO PC_RTTR 0.51 0.96 0.11 0.51 0.18 34.05 1.00 0.35 15.72 16.99 197.78 0.63 0.26 1.73 3.87 3.75 0.78 0.18 1.05 1.68 2.17 4.13 0.98 16.87 19.57 3.69 3.20 5.41 0.21 2.55 0.22 2.54 0.08 1.48 0.37 3.18 0.01 0.40 0.03 0.67 0.04 0.89 0.04 0.88 InfoCom features 0.06 0.00 0.02 0.07 0.06 0.10 0.01 0.02 0.13 0. 0.51 0.96 0.11 0.50 0.17 FluDel features 34.57 0.94 0.35 15.40 17.00 196.12 0.59 0.26 1.72 3.86 3.76 0.80 0.17 0.99 14.95 0.61 0.14 8.41 2.60 55.36 0.12 0.04 0.48 0.53 2.99 1.28 0.54 1.33 15.26 0.46 0.08 6.03 1.58 34.18 0.09 0.02 0.39 0.32 1.82 1.16 0.42 1.21 TLQual features 0.55 0.67 1.48 0.36 2.46 3.87 1.64 1.55 0.81 0.04 0.54 0.11 0.52 0.02 0.47 0.05 0.62 0.01 0.44 0.02 0.39 0.02 0.41 0.03 0. 0.51 0.62 1.15 0.34 2.70 3.46 1.28 1.11 0.94 0.08 0.62 0.09 0.60 0.04 0.65 0.09 0.77 0.02 0.58 0.03 0.56 0.04 0.57 0.04 0.64 1.70 2.15 4.16 1.02 16.84 20.04 3.68 3.27 5.45 0.22 2.58 0.23 2.52 0.09 1.51 0.39 3.19 0.01 0.42 0.03 0.71 0.05 0.89 0.04 0.91 0.13 -0.73 0.14 1.14 1.06 0.78 2.01 -0.06 0.68 0.53 0.76 0.18 0.96 0.81 0.03 1.25 2.28 3.51 1.81 0.44 -0.43 1.33 0.88 0.79 0.98 1.49 1.35 0.18 0.18 -0.22 0.81 -0.06 0.24 -0.64 -0.02 0.21 1.71 0.98 1.61 -0.15 1.78 -0.41 1.22 -0.26 0.22 -1.20 0.16 1.87 1. 0.73 2.53 -0.12 0.51 0.58 0.84 0.19 1.17 0.87 0.07 1.57 2.27 5.37 1.98 0.12 -0.30 0.68 0.57 0.79 1.05 2.35 2.17 -0.10 -0.11 -0.70 0.68 -0.40 -0.14 -1.27 -0.48 -0.06 1.63 0.70 2.18 -0.69 2.01 -1.05 1.32 -0.82 -0.53 -0.32 -0.55 2.85 1.77 1.24 5.35 0.80 0.89 1.04 0.55 -0.76 3.64 1.58 2.08 1.72 5.75 9.03 4.02 1.23 2.26 2.34 1.95 1.47 1.34 3.58 2.70 1.18 1.15 2.01 3.94 -0.19 3.08 2.29 -0.69 3.40 -0.30 2.39 -1.45 4.48 5.64 -0.79 1.41 -1.01 0.82 1.56 1.23 1.52 0. 2.16 7.62 5.01 1.29 2.35 0.97 0.80 1.72 1.91 1.61 0.58 2.33 7.39 3.26 2.57 1.95 3.39 2.64 5.38 2.32 2.64 1.44 3.61 2.76 2.23 3.50 1.24 1.12 1.16 2.86 1.09 0.82 1.18 1.32 2.71 2.63 1.96 3.62 2.78 Table 8: Descriptive statistics of all extracted features on raw data and augmented data."
        },
        {
            "title": "E Case Studies of Model Prediction Errors",
            "content": "Sample 47 Key features features Key (MSD) for Score 6 samples Error analysis From the original dataset; RF model True score: 6.34; Predicted score: 5.29 BLEURT: 0.66; CometKiwi: 0.62; chrF: 0.07; BERTScore: 0.97; xCOMET: 0.35 BLEURT (0.540.13); CometKiwi (0.540.10); chrF (0.130.02); BERTScore (0.960.01); xCOMET (0.210.12) The model underestimates the InfoCom score of Sample 47 by 1.05. Upon examining samples within the 5.56.5 score range, we observe that Sample 47 exhibits particularly low chrF score (0.07). This value is more than one standard deviation below the mean (0.11) for this feature among samples in this range. Analysis of the corresponding student transcript reveals tendency to reorder sentence components during interpretation, though key information in the source speech is interpreted faithfully into the target language. For instance, when interpreting an if...then... sentence, the student processes the then clause before the if clause, which results in reduced n-gram matching and consequently lower chrF score for this sample. Table 9: Cases of notable disagreement between machine and human scores for InfoCom. Sample 95 Sample features (MSD) Features for Score 5 samples Error analysis From the original dataset; XGBoost model True score: 4.73; Predicted score: 3.48 NFP: 13; MLR: 20.64; MLUP: 1.18; NUP: 42; MLFP: 0.26; PSC: 185; SR: 1.53; PTR: 0.41; NRSA: 2; MLS: 0.25 NFP (18.165.66); MLR (17.131.11); MLUP (1.020.11); NUP (30.46.73); MLFP (0.380.12); PSC (195.9613.44); SR (1.720.25); PTR (0.450.24); NRSA (4.43.55); MLS (0.270.04) For Sample 95, the model underestimates the FluDel score by 1.25 points. Analysis of this samples features reveals notably high values for MLUP (1.18) and NUP (42), both approximately two standard deviations above their respective means. Also, the speech rate (1.53) is lower than the mean (1.72). Collectively, these feature values likely lead the model to interpret this sample as having more significant breakdowns and reduced speaking speed. However, qualitative examination of the corresponding student recording offers contrasting perspective. While the student does exhibit longer and more frequent pauses than average, these disfluencies predominantly occur at boundaries between semantic units within sentences. For human rates, this placement of pauses does not hurt perceived fluency as much as within-phrase disfluencies, which may explain why the actual perceived score is higher than the models prediction based on these automated features. Table 10: Cases of notable disagreement between machine and human scores for FluDel. 20 Sample 62 Key features Key features (MSD) for Score 6 samples Error analysis From the original dataset; XGBoost model True score: 6.22; Predicted score: 5.01 CN_RATIO: 0; PC_RTTR: 0; MLS: 19.57; PP_RTTR: 1; SP_RTTR: 0.71; AP_RTTR: 2; MLC: 14; NWSE: 0.26; PV_RTTR: 0.89; MLTU: 17.11 CN_RATIO (0.010.01); PC_RTTR (0.990.39); MLS (21.367.18); PP_RTTR (0.810.29); SP_RTTR (2.540.35); AP_RTTR (3.230.44); MLC (17.081.30); NWSE (1.690.74); PV_RTTR (0.980.34); MLTU (19.731.56) The predicted score is 1.21 points lower than that assigned by human raters. contributing factor to this discrepancy may be the notable absence of two specific Chinese structures, CN and PC expressions, in the students interpretation. Instead, the students frequently employ expressions characteristic of Westernized Chinese, style influenced by Western language structures. While human raters appear to find these alternative expressions acceptable within the context of the task, the model likely penalizes the lack of the expected native Chinese forms, leading to the observed lower scores. Table 11: Cases of notable disagreement between machine and human scores for TLQual."
        }
    ],
    "affiliations": [
        "Shanghai Jiao Tong University"
    ]
}