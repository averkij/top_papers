{
    "paper_title": "Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly a GAN Discriminator",
    "authors": [
        "Kaiwen Zheng",
        "Yongxin Chen",
        "Huayu Chen",
        "Guande He",
        "Ming-Yu Liu",
        "Jun Zhu",
        "Qinsheng Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective inherently suffers from a mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as a unified framework that bridges likelihood-based generative training and the GAN objective to bypass this fundamental constraint. Our key insight is to parameterize a discriminator implicitly using the likelihood ratio between a learnable target model and a fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of a well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in a self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58 to new records of 1.30/0.97 on CIFAR-10/ImageNet-64 datasets, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256$\\times$256."
        },
        {
            "title": "Start",
            "content": "Direct Discriminative Optimization: Your Likelihood-Based Visual Generative Model is Secretly GAN Discriminator Kaiwen Zheng 1 2 Yongxin Chen 1 Huayu Chen 2 Guande He 3 Ming-Yu Liu 1 Jun Zhu 2 Qinsheng Zhang 1 https://research.nvidia.com/labs/dir/ddo/ 5 2 0 2 3 ] . [ 1 3 0 1 1 0 . 3 0 5 2 : r Abstract While likelihood-based generative models, particularly diffusion and autoregressive models, have achieved remarkable fidelity in visual generation, the maximum likelihood estimation (MLE) objective inherently suffers from mode-covering tendency that limits the generation quality under limited model capacity. In this work, we propose Direct Discriminative Optimization (DDO) as unified framework that bridges likelihood-based generative training and the GAN objective to bypass this fundamental constraint. Our key insight is to parameterize discriminator implicitly using the likelihood ratio between learnable target model and fixed reference model, drawing parallels with the philosophy of Direct Preference Optimization (DPO). Unlike GANs, this parameterization eliminates the need for joint training of generator and discriminator networks, allowing for direct, efficient, and effective finetuning of well-trained model to its full potential beyond the limits of MLE. DDO can be performed iteratively in self-play manner for progressive model refinement, with each round requiring less than 1% of pretraining epochs. Our experiments demonstrate the effectiveness of DDO by significantly advancing the previous SOTA diffusion model EDM, reducing FID scores from 1.79/1.58 to new records of 1.30/0.97 on CIFAR-10/ImageNet-64 datasets, and by consistently improving both guidance-free and CFG-enhanced FIDs of visual autoregressive models on ImageNet 256256. 1. Introduction Modeling the distribution of high-dimensional data is fundamental challenge in machine learning (Bishop & Nasrabadi, 2006; Goodfellow et al., 2016). Recent years 1NVIDIA 2Tsinghua University 3The University of Texas at Austin. Correspondence to: Jun Zhu <dcszj@tsinghua.edu.cn>. have witnessed the domination of diffusion (Ho et al., 2020; Song et al., 2021b) and autoregressive (Van Den Oord et al., 2016) paradigms in generative modeling of continuous data and discrete data. They have achieved both theoretical and empirical success in visual tasks including image and video synthesis (Dhariwal & Nichol, 2021; Esser et al., 2021; Ramesh et al., 2021; Karras et al., 2022; Ho et al., 2022; Rombach et al., 2022; Balaji et al., 2022; Gupta et al., 2023; Esser et al., 2024; Brooks et al., 2024; Bao et al., 2024; Tian et al., 2024), forming the cornerstone of large-scale generation systems in the era of AI-generated content. Diffusion and autoregressive models are representatives of likelihood-based generative models. Compared to Generative Adversarial Networks (GANs) (Goodfellow et al., 2014) which often face unstable training and mode collapse issues, these models are distinguished by their stability, scalability, and generalizability. Likelihood-based generative models aim to learn the underlying data distribution pdata by maximizing the likelihood of the observed data under parameterized probabilistic model pθ, which is equivalent to minimizing the forward KullbackLeibler (KL) divergence: max θ Epdata(x) [log pθ(x)] min θ DKL(pdata pθ) However, this maximum likelihood estimation (MLE) objective entails inherent limitations. Forward KL is known to prioritize mode-covering and imposes extreme penalties if the model severely underestimates the likelihood of any training sample (Karras et al., 2024a). Under limited model capacity, this property forces the learned density to spread out excessively (Figure 1(a)), potentially leading to blurry samplesa phenomenon commonly observed in Variational Autoencoders (VAEs) (Kingma & Welling, 2014) and in likelihood training of diffusion models (Song et al., 2021a; Kingma et al., 2021; Lu et al., 2022a; Zheng et al., 2023b). Consequently, these models often rely heavily on guidance methods (Ho & Salimans, 2021; Kim et al., 2023a; Karras et al., 2024a) to steer the samples away from unlikely lowprobability regions and toward the core of the data manifold in order to improve overall generation quality. In contrast, GANs, which are theoretically grounded in JensenShannon (JS) divergence or Wasserstein distance (Arjovsky et al., 2017), tend to produce sharper and more realistic samples. 1 Direct Discriminative Optimization (a) (b) Figure 1. Toy example illustrating DDO. (a) Models pretrained via maximum likelihood estimation (MLE) exhibit dispersed density, while DDO imposes contrastive forces toward the data distribution. (b) The finetuned model concentrates better on the main mode. To bypass MLEs mode-covering nature, we aim to leverage the GAN loss to discriminate between the model and data distributions and produce contrastive forces that guide the model. However, typical GANs require parameterizing extra discriminator networks and alternating optimization, creating engineering complications. Applying GAN-type training to diffusion or autoregressive models is especially inefficient due to their iterative sampling processes. In this work, we introduce Direct Discriminative Optimization (DDO), framework that bridges likelihood-based generative models and GANs to push their performance beyond the limits of MLE. Our key insight is to implicitly parameterize the discriminator using the likelihood ratio between learnable target model and fixed reference model, both initialized from the pretrained model. This parameterization, inspired by Direct Preference Optimization (DPO) (Rafailov et al., 2024), offers theoretical guarantees of optimality, divergence bounds, and connections to guidance methods. It also enables direct finetuning of the pretrained model without altering the network architecture or inference protocol and supports iterative refinement via multi-round self-play. DDO achieves significant performance gains for both diffusion and autoregressive models sufficiently pretrained on standard image benchmarks. By finetuning SOTA diffusion models EDM (Karras et al., 2022) and EDM2 (Karras et al., 2024b), we achieve unprecedented FID scores of 1.30/0.97 on CIFAR-10/ImageNet-64. Finetuning the visual autoregressive model VAR (Tian et al., 2024) on ImageNet 256256 reduces the FID from 1.92 to 1.73 while removing sampling tricks. Notably, even without classifier-free guidance (CFG) (Ho & Salimans, 2021), the finetuned model achieves an FID of 1.79, surpassing the CFG-enhanced pretrained model while cutting the inference cost by half. 2. Background 2.1. Likelihood-Based Generative Models Likelihood-based generative models parameterize probability distribution pθ to learn the data distribution pdata, enabling explicit likelihood evaluation and density estimation. Among them, diffusion and autoregressive models are two predominant types that excel in visual generation. Autoregressive models (Van Den Oord et al., 2016; Brown et al., 2020) learn discrete data distributions via straightforward next-token prediction mechanism, which factorizes the joint distribution into product of conditional probabilities, allowing exact likelihood computation: log pθ(x) = (cid:88) n=1 log pθ(x(n)x(<n)) (1) where denotes the data dimension (sequence length). Each pθ(x(<n)) is parameterized via Softmax operation over the models output logits and optimized using cross-entropy loss against the ground-truth token. In visual autoregressive modeling, images are first quantized to discrete tokens within compact latent space using autoencoders (Van Den Oord et al., 2017; Esser et al., 2021). Diffusion models (Ho et al., 2020; Song et al., 2021b) learn continuous data distributions by gradually perturbing clean data x0 pdata with Gaussian noise, which generates trajectory {xt}T t=0, and then learning to reverse this process. The forward and backward dynamics can be formulated as either stochastic or ordinary differential equations (SDEs or ODEs) (Song et al., 2021b). The forward process follows closed-form transition kernel qt0(xtx0) = (αtx0, σ2 I) with predefined noise schedule αt, σt, enabling reparameterization as xt = αtx0 + σtϵ, ϵ (0, I). The model is typically parameterized as noise prediction network ϵθ(xt, t) trained to estimate ϵ via mean squared error (MSE) regression, which forms an evidence (or variational) lower bound (ELBO) on the likelihood (Song et al., 2021a): (cid:3) log pθ(x0) CEtp(t),ϵN (0,I) (cid:2)w(t)ϵθ(xt, t) ϵ2 2 (2) where xt = αtx + σtϵ, is constant irrelevant to θ, and p(t), w(t) are certain time distribution and weighting function. The ELBO provides reasonable likelihood approximation compared to the exact but cumbersome instantaneous change-of-variable formula in neural ODEs (Chen et al., 2018). Moreover, while the likelihood bound is tight only for specific p(t), w(t), alternative choices share the same optimum and can serve as surrogate objectives (Kingma & Gao, 2024). From the perspective of score matching (Song et al., 2021b), the optimal noise predictor is linked to the score function s(xt, t) := xt log qt(xt) by ϵ(xt) = σts(xt, t), where qt denotes the marginal distribution at time in the forward process. Due to the properties of MSE, the network can be parameterized in alternative yet theoretically equivalent forms, such as velocity predictor (Salimans & Ho, 2022; Zheng et al., 2023b) that estimates the tangent of the diffusion trajectory, commonly known as flow matching (Lipman et al., 2022). In our experiments, we adopt the more generalized F-parameterization introduced in EDM (Karras et al., 2022) (detailed in Appendix C). Direct Discriminative Optimization 2.2. GANs GANs (Goodfellow et al., 2014) do not explicitly model the likelihood pθ but instead directly optimize the data generation process through adversarial training. Specifically, the optimization involves an adversarial interplay between generator network gθ : Rdz (cid:55) Rd that maps latent variables Rdz p(z) (typically Gaussian noise) into synthetic samples, and discriminator network dϕ : Rd (cid:55) [0, 1] that classifies samples as real or fake: min θ max ϕ Epdata(x) [log dϕ(x)] + Epθ(x) [log(1 dϕ(x))] . (3) Here pθ(x) is the generator distribution, whose exact density is intractable but can be easily sampled from via = gθ(z), p(z). In the inner loop, the discriminator is optimized using binary cross-entropy loss (also known as noise contrastive estimation (NCE) (Gutmann & Hyvarinen, 2010)), and its optimal solution can be derived as: d(x) = pdata(x) pdata(x) + pθ(x) under which the minimax game becomes min θ 2DJS(pdata pθ) 2 log 2 (4) (5) 2 ) + 1 2 DKL(q p+q 2 DKL(p p+q where DJS(p q) = 1 2 ) is the JensenShannon (JS) divergence. This theoretically ensures that the optimal generator distribution matches the data distribution. However, in practice, training instability arises due to gradient vanishing and mode collapse, inspiring variants such as Wasserstein GANs (Arjovsky et al., 2017). GANs can be incorporated to enhance other generative models. For example, Discriminator Guidance (Kim et al., 2023a) utilizes the gradient information from the discriminator as corrective term to refine the score function in diffusion models (discussed in Section 4.2). Additionally, GANs are commonly employed as an auxiliary loss to improve one-step generation such as in diffusion distillation (Kim et al., 2023b; Yin et al., 2024; Zhou et al., 2024b). 3. Direct Discriminative Optimization Motivated by the benefits of adversarial training in enhancing generation quality, we aim to bridge likelihood-based generative models with GANs to derive an alternative training paradigm to MLE. Unlike prior works that incorporate GAN as an auxiliary loss and require additional engineering overhead, our approach seeks to (1) directly optimize likelihood-based generative models without modifying the network architecture, adding extra discriminators, complicating the training procedure or increasing inference costs, and (2) eliminate the need for backpropagation through the sampling process, making it applicable to diffusion and autoregressive models that rely on iterative sampling. 3.1. Your Likelihood-Based Generative Model is Secretly Discriminator Unlike one-step generators that learn direct mapping from noise to data, likelihood-based generative models are grounded in the probabilistic definition of the likelihood function pθ, which enables both the generation of samples pθ and the evaluation of the likelihood pθ(x), either exactly or approximately, while retaining the tractability of backpropagation through the likelihood computation. This inspires us to utilize the likelihood information embedded in the optimal discriminator (Eqn. (4)). Specifically, consider pretrained model pθref as reference to generate fake samples. The optimal discriminator dθ for Epdata(x) [log dθ(x)]Epθref (x) [log(1 dθ(x))] (6) min θ can be rewritten as: d(x) = pdata(x) pdata(x) + pθref (x) (cid:18) = σ log (cid:19) pdata(x) pθref(x) (7) where σ(x) = 1 1+ex is the Sigmoid function. The data distribution pdata is available from d. Therefore, if we parameterize the discriminator dθ using likelihood-based target generative model pθ as: (cid:18) (cid:19) dθ(x) := σ log pθ(x) pθref(x) (8) then the optimal target model that minimizes the GAN discriminator loss matches the data distribution. We formalize this induced objective in the following theorem. Theorem 3.1 (Optimality). With unlimited model capacity, the optimal likelihood-based model pθ under the objective min θ L(θ) = Epdata(x) Epθref (x) satisfies pθ = pdata. (cid:19)(cid:21) (cid:20) (cid:18) log σ log (cid:20) (cid:18) pθ(x) pθref(x) (cid:18) log 1 σ log pθ(x) pθref(x) (cid:19)(cid:19)(cid:21) (9) In contrast to previous GAN-based methods that introduce separate discriminator network dϕ, our approach implicitly defines the discriminator through target generative model pθ. While it is theoretically feasible to initialize θ, θref arbitrarily and train from scratch, stable convergence requires stronger initial conditions (Section 3.2). In practice, both θ, θref are initialized from widely available pretrained models, ensuring strong starting point and facilitating steady improvement. We refer to this approach as Direct Discriminative Optimization (DDO), drawing parallels with Direct Preference Optimization (DPO) (Rafailov et al., 2024), which aligns language models with human preferences by expressing the reward model in terms of the likelihood ratio between two policies (discussed in Section 4.1). The pipeline of DDO is illustrated in Figure 2. 3 Direct Discriminative Optimization Figure 2. Illustration of DDO. (1) Models. θref is the (pretrained) reference model frozen during training. θ is the learnable model initialized as θref. (2) Data. Samples from pdata are drawn from the training dataset. Samples from pθref are generated by the reference model, either offline or online. (3) Objective. The target model θ is optimized by applying the GAN discriminator loss with the implicitly parameterized discriminator dθ to distinguish between real samples from pdata and fake samples from pθref . What does the DDO update do? For mechanistic understanding of DDO, we can analyze the gradient of the loss function with respect to parameters θ: (cid:90) θL(θ) = )θ log pθ(x)dx )(pθ(x) pdata(x) (1 dθ(x) (cid:125) (cid:123)(cid:122) (cid:125) (cid:123)(cid:122) (cid:124) pθ(x) when <0 [0,1] (cid:124) (10) Intuitively, gradient descent increases the model likelihood pθ(x) for data points that satisfy pθ(x) < pdata(x), and decreases it otherwise, pushing pθ closer to pdata. Furthermore, the gradient magnitude is weighted by both the distance pθ(x) pdata(x) and 1 dθ(x), assigning higher weights to samples discriminated as fake. imposes constraint to the reference model regarding its mutual density coverage with the data distribution. We can expect log pθref to be lower bounded, i.e., pθref suffipdata ciently covers pdata, which aligns with the characteristics of MLE-trained models. Under this condition, the forward KL DKL(pdata pθ) remains bounded by (cid:112)L(θ) L. However, bounding the reverse KL requires an upper bound on pθref pdata , which imposes stronger constraint on pθref. 3.3. Practical Implementation We introduce several practical techniques that make DDO applicable to high-dimensional real-world data and diffusion models whose likelihood computation is expensive. 3.2. Theoretical Analysis Apart from the optimality guarantee, we also examine the behavior of the DDO objective when θ is not optimal. Specifically, we investigate the following question: Is pθ closer to pdata with lower L(θ)? Under certain assumptions, we can establish bounds on the divergence between pθ and pdata in terms of the difference between L(θ) to the optimal loss value L, as formalized in the following theorem. and log pθ Theorem 3.2 (Divergence Bounds). If log pθref are bounded, there exist some constants C1, C2 such that pθref pdata DKL(pdata pθ) DKL(pθ pdata) C2 (cid:112)L(θ) (cid:112)L(θ) (11) (12) The assumption of bounded log pθ implies that the optipθref mized distribution does not deviate significantly from the reference distribution, which is reasonable when finetuning for short duration. The assumption of bounded log pθref pdata Generalized Objective with Extra Coeffecients The loglikelihood log pθ(x) of likelihood-based generative models often scales with the data dimension and can reach magnitudes of 103. As the DDO objective in Eqn. (9) involves Sigmoid operation on log pθ(x), this can lead to gradient vanishing and numerical precision issues. To address this, we add hyperparameters α, β to control the relative weights of loss terms and scale the probability ratio: Lα,β(θ) = Epdata(x) (cid:20) (cid:18) log σ β log αEpθref (x) (cid:20) (cid:18) log 1 σ pθ(x) pθref (x) (cid:18) β log (cid:19)(cid:21) (cid:19)(cid:19)(cid:21) pθ(x) pθref(x) (13) The modified loss retains the same optimization trend, namely, increasing pθ(x) for pdata and decreasing pθ(x) for pθref , but the optimum may overshoot the data distribution for β < 1. Specifically, we have: Theorem 3.3. With unlimited model capacity, the optimal likelihood-based generative model θ that minimizes Lα,β(θ) satisfies pθ p11/β p1/β data for certain α. θref Direct Discriminative Optimization This establishes deep connection with guidance methods In practice, we observe that (discussed in Section 4.2). α and β across wide range of values1 yield reasonable performance. We sweep over them for the best results. Handling Compute-Intensive Likelihood Evaluating the model likelihood for specific data point can be computationally intensive. In particular, unlike autoregressive models, which only require single forward pass through the network to compute log pθ(x) (Eqn. (1)) due to the causal structure imposed by attention masks, diffusion models necessitate multiple forward passes over different timesteps to approximate log pθ(x) through the ELBO (Eqn. (2)). Specifically, the log-likelihood ratio in the DDO loss is: log pθ(x) pθref(x) Et,ϵ [xt,t,ϵ] (14) where xt = αtx + σtϵ and xt,t,ϵ = w(t) (cid:0)ϵθ(xt, t) ϵ2 2 ϵθref (xt, t) ϵ2 2 (15) We apply Jensens inequality pointwise to derive an upper bound for the loss using the convexity of the function log σ(x) log(1 σ(x)) for any a, 0: (cid:1) L(θ) Epdata(x) log σ (Et,ϵ []) Epθref (x) log(1 σ (Et,ϵ [])) Et,ϵ (cid:105) (cid:104) Epdata(x) log σ() + Epθref (x) log(1 σ())) (16) This treatment, analogous to the one used in DiffusionDPO (Wallace et al., 2024), enables us to approximate the diffusion DDO loss using single forward pass for each x. Multi-Round Refinement via Self-Play Due to the practical modifications for applicability, the optimization process of DDO provides useful gradient information in the early stage but does not converge to the data distribution in the final. To maximize the fine-tuning performance, we adopt multi-round refinement strategy, where the reference model pθref is iteratively updated by replacing it with an improved version from the previous round: Round n: . . . pθ n1 (cid:124) (cid:123)(cid:122) (cid:125) Reference (cid:32) σ β log (cid:33) pθn pθ n1 (cid:124) (cid:123)(cid:122) Discriminator (cid:125) Round + 1: . . . pθ (cid:124)(cid:123)(cid:122)(cid:125) Reference where θ represents the best-performing model across different hyperparameter configurations and training iterations in 1Typical choices are α [0.5, 50] and β [0.01, 0.1], while the optimal values depend on the specific model and settings. 5 Figure 3. Comparison with DPO. round n. In each round, the reference model acts as fixed generator, making the multi-round optimization analogous to the generator-discriminator interplay in GANs. However, unlike GANs, where both networks are explicitly optimized, we never update the reference (generator) model directly. Instead, the generator is obtained from the discriminator in the previous round, leading to form of self-play. This iterative refinement process is conceptually similar to Iterative DPO (Xu et al., 2023) and SPIN (Chen et al., 2024b), which extend DPO for better language model alignment. 4. Comparison with Existing Methods 4.1. Direct Preference Optimization (DPO) DPO (Rafailov et al., 2024) is lightweight surrogate objective designed for reinforcement learning from human feedback (RLHF) (Ouyang et al., 2022; Achiam et al., 2023) that enhances the instruction-following ability of pretrained language models. Standard RLHF involves two stages: (1) learning reward model rθ and (2) aligning the reference policy πθref to the target policy πθ(yx) πθref (yx)erθ(x,y)/β using RL, where is the prompt and is the response. The Bradley-Terry preference mode (Bradley & Terry, 1952) links preferences and rewards by p(yw ylx) := = σ(r(x, yw)r(x, yl)) er(x,yw) er(x,yl) + er(x,yw) (17) where yw and yl denote the winning and losing responses for given prompt x, annotated by human. DPO enables direct optimization of pretrained language models on preference data without training separate reward model: LDPO(θ) (cid:18) β log = E(x,yw,yl)D log σ πθ(ylx) πθref(ylx) (18) where the reward function rθ(yx) is implicitly parameterized by the log-likelihood ratio β log πθ(yx) πθref (yx) . πθ(ywx) πθref(ywx) β log (cid:19) Despite sharing similar insights in parameterization, DDO Direct Discriminative Optimization (cid:17)s (cid:16) pθ pϕ prove well-trained distribution pθ by amplifying its difference from degraded or less informative distribution pϕ: pθ,ϕ pθ . This superposition sharpens the MLEoptimized model distribution and suppresses low-probability outliers (Karras et al., 2024a). According to Theorem 3.3, DDO with β < 1 induces similar overshooting effect, highlighting the theoretical connection. Figure 4. Comparison of model parameter counts and inference time across different guidance methods and DDO. For DG, we measure the statistics on class-conditional CIFAR-10. For AG, we measure the statistics on ImageNet-64. is fundamentally different from DPO. As illustrated in Figure 3, DPO is designed for preference learning, requiring additional paired human-annotated data and maximizing the likelihood gap between preferred (winning) and nonpreferred (losing) responses without considering the whole distribution. In contrast, DDO focuses on distribution alignment, directly aligning the model with the ground-truth data distribution. It requires only the original training data that are unpaired with the model-generated samples. 4.2. Guidance Methods We review several types of guidance methods that enhance diffusion models2 at inference time. Let sθ(xt, t) denote the score function representation introduced in Section 2.1. Classifier-Free Guidance (CFG) (Ho & Salimans, 2021) combines the unconditional/conditional model to obtain new score predictor := sθ(xt, t, c) + s(sθ(xt, t, c) sθ(xt, t, )), where represents the unconditional case, and is the guidance scale. The unconditional model shares parameters with the conditional model and is learned by random label dropout during training. θ(xt, t, c) Discriminator Guidance (DG) (Kim et al., 2023a) trains time-dependent discriminator network dϕ that distinguishes between perturbed real data and model-generated samples. Its gradient is then used to refine the score function: sθ,ϕ(xt, t) = sθ(xt, t) + xt log dϕ(xt,t) 1dϕ(xt,t) . Similar to DDO, DG leverages the optimal discriminator (Eqn. (4)) for theoretical guarantees. However, it explicitly parameterizes the discriminator as separate, time-aware network. Autoguidance (AG) (Karras et al., 2024a) operates similarly to CFG but refines the score function by guiding the base model with an inferior variant: sθ,ϕ(xt, t) := sθ(xt, t) + s(sθ(xt, t) sϕ(xt, t)), where sϕ is degraded version of sθ obtained via reduced model capacity or under-training. For unified perspective, all these guidance methods im2They can also be adapted to autoregressive models. 6 Unlike guidance methods, DDO enhances sample quality without increasing inference costs compared to the base model (Figure 4). Moreover, in scenarios where CFG is crucial for balancing image-condition alignment and diversity (e.g., FID-IS curve), DDO can be seamlessly integrated with CFG to achieve an overall improved trade-off (Section 5.3). 5. Experiments Our experiments aim to investigate the following aspects: 1. The effectiveness and efficiency of DDO in enhancing the visual quality of well-trained diffusion models (Section 5.2) and autoregressive (Section 5.3) models. 2. The impact of the hyperparameters α, β, as well as the benefits of multi-round refinement. 5.1. Experimental Setups Datasets & Models We experiment on standard image benchmarks including CIFAR-10 (Krizhevsky et al., 2009), ImageNet-64 and ImageNet 256256 (Deng et al., 2009). For each dataset, we apply DDO to finetune state-of-the-art diffusion or autoregressive models, including EDM (Karras et al., 2022), EDM2 (Karras et al., 2024b) and VAR (Tian et al., 2024). We compare with range of advanced generative baselines, including GAN-based approaches. Training & Evaluation We evaluate Frechet inception distance (FID) (Heusel et al., 2017) on 50k images as the primary benchmark metric for all experiments, and additionally measure Inception Score (IS) (Barratt & Sharma, 2018) for ImageNet 256256. We report the number of functions evaluations (NFE) as quantification of inference efficiency. For diffusion models, we finetune over multiple rounds until further improvement is negligible. For VAR, we observe rapid convergence and only finetune for 2 rounds. The finetuning is highly efficient, with each round requiring less than 1% of pretraining iterations. Further experiment details can be found in Appendix C, and visualizations of generated samples are provided in Appendix D. 5.2. Results on Diffusion Models The EDM and EDM2 base models on CIFAR-10 and ImageNet-64 are implemented as separate unconditional or class-conditional networks. Since CFG provides limited Direct Discriminative Optimization (a) FID-Round (b) α = 4.0 Figure 5. Illustrations on diffusion models. (a) Multi-round refinement and (b)(c) training curves under different α, β. (c) β = 0.05 Table 1. Results on unconditional and class-conditional CIFAR-10. Including diffusion distillation methods with auxiliary GAN loss. Type Model NFE Uncond Cond FID FID GAN Diffusion StyleGAN2-ADA (Karras et al., 2020) StyleGAN-XL (Sauer et al., 2022) R3GAN (Huang et al., 2025) CTM (Kim et al., 2023b) GDD-I (Zheng & Yang, 2024) CAF (Park et al., 2024) SiD2A (Zhou et al., 2024b) DDPM (Ho et al., 2020) iDDPM (Nichol & Dhariwal, 2021) DDIM (Ho et al., 2020) DPM-Solver (Lu et al., 2022b) DPM-Solver-v3 (Zheng et al., 2023a) NCSN++ (Song et al., 2021b) LSGM (Vahdat et al., 2021) VDM (Kingma et al., 2021) Flow Matching (Lipman et al., 2022) i-DODE (Zheng et al., 2023b) EDM (Karras et al., 2022) + DG (Kim et al., 2023a) Ours EDM (retested) + DDO 1 1 1 1 1 1 1 1000 4000 100 48 12 2000 138 1000 142 215 35 53 35 35 2.92 - - 1.98 1.54 1.48 1.50 3.17 2.90 4.16 2.65 2.24 2.20 2.10 4.00 6.35 3.76 1.97 1.77 1.97 1. 2.42 1.85 1.96 1.73 1.44 1.39 1.40 - - - - - - - - - - 1.79 1.64 1.85 1.30 benefits on these datasets, we directly apply the diffusion DDO loss without considering the interaction with CFG. Main Results Table 1 and Table 2 present the quantitative results on CIFAR-10 and ImageNet-64. Figure 5(a) illustrate the FID reduction over multiple rounds. We highlight the advantages of DDO as follows: (1) Effectiveness. With multi-round refinement, DDO achieves record-breaking FID scores of 1.38/1.30 on CIFAR10 and 0.97 on ImageNet-64, significantly improving upon the EDM and EDM2 base models by 30% and 40%, respectively. Additionally, DDO outperforms all guidance-based and GAN-based methods requiring complex GAN-specific tuning or increasing inference costs. Table 2. Results on class-conditional ImageNet-64. Including diffusion distillation methods with auxiliary GAN loss. We find strict class balance crucial for FID on ImageNet and slightly modify the original sampling script to ensure this. Type Model NFE FID StyleGAN-XL (Sauer et al., 2022) CTM (Kim et al., 2023b) CAF (Park et al., 2024) DMD2 (Yin et al., 2024) PaGoDA (Kim et al., 2024) GDD-I (Zheng & Yang, 2024) SiD2A (Zhou et al., 2024b) DDPM (Ho et al., 2020) iDDPM (Nichol & Dhariwal, 2021) ADM (Dhariwal & Nichol, 2021) RIN (Jabri et al., 2022) EDM (Karras et al., 2022) VDM++ (Kingma & Gao, 2024) DisCo-Diff (Xu et al., 2024) EDM2-S (Karras et al., 2024b) + CFG + AG (Karras et al., 2024a) EDM2-M EDM2-L EDM2-XL EDM2-S (retested) + DDO 1 1 1 1 1 1 1 250 250 250 1000 511 511 623 63 126 126 63 63 63 63 63 1.51 1.92 1.69 1.28 1.21 1.16 1.11 11.0 2.92 2.07 1.23 1.36 1.43 1.22 1.58 1.48 1.01 1.43 1.33 1. 1.60 0.97 GAN Diffusion Ours DG. On ImageNet-64, the compact EDM2-S (280M parameters) attains an FID of 1.31 after only 3 rounds, surpassing EDM2-XL (1119M parameters), which is four times larger, demonstrating the parameter efficiency unlocked by DDO. Effects of α, β As shown in Figure 5(b)(c), we visualize the training curves under different α, β for class-conditional CIFAR-10 during the first round. We empirically find that wide range of α, β consistently improves the base model, though identifying the optimal hyperparameters requires grid searching. Moreover, tuning α while keeping β fixed or adjusting β under appropriate α yields similar effects. 5.3. Results on Autoregressive Models (2) Efficiency. Although we employ substantial training over dozens of rounds to maximize the performance and explore the upper bound of DDO, FID improves significantly within just few rounds. Notably, DDO in single round achieves FIDs of 1.72/1.58 on CIFAR-10, surpassing The VAR models rely heavily on CFG to enhance generation quality. distinctive feature of CFG is its ability to balance diversity and fidelity by adjusting the guidance scale, which is essential for creating the FID-IS trade-off. Consequently, we need to accommodate DDO to ensure that 7 Direct Discriminative Optimization (a) VAR-d (b) VAR-d30 (c) FID and IS w.r.t. guidance scale Figure 6. Illustrations on autoregressive models. (a)(b) FID-IS trade-off curves and (c) the impact of α under β = 0.02. Table 3. Results on class-conditional ImageNet 256 256. denotes guidance, including both classifier guidance and CFG. We report the NFE without guidance, which is doubled with guidance. Type Model GAN Diffusion Autoregressive BigGAN (Brock, 2018) GigaGAN (Kang et al., 2023) StyleGAN-XL (Sauer et al., 2022) ADM (Dhariwal & Nichol, 2021) LDM-4 (Rombach et al., 2022) DiT-XL/2 (Peebles & Xie, 2023) SiT-XL (Ma et al., 2024) VQGAN (Esser et al., 2021) ViT-VQGAN (Yu et al., 2021) LlamaGen-3B (Sun et al., 2024) Open-MAGVIT2-XL (Luo et al., 2024) VAR-d16 (Tian et al., 2024) VAR-d Ours VAR-d16 (w/o sampling tricks) + DDO VAR-d30 (w/o sampling tricks) + CCA (Chen et al., 2024a) + DDO w/o w/ NFE FID FID 1 1 1 250 250 250 250 256 1024 256 256 10 10 10 10 10 10 10 6.95 3.45 2. 10.94 10.56 9.62 8.3 15.78 4.17 13.58 9.63 3.62 2.17 11.33 3.12 4.74 2.54 1.79 - - - 4.59 3.60 2.27 2.06 - - 3.05 2.33 3.30 1. 3.71 2.54 1.92 - 1.73 the finetuned models remain compatible with CFG. To this end, we choose the reference and target distributions pθref , pθ as the guidance-free model corresponding to = 0. To preserve the models ability for unconditional generation, we incorporate random label dropout during DDO fine-tuning. We set α = 0 for the unconditional part to prevent it from receiving negative signals from reference samples pθref . Main Results Table 3 presents the quantitative results on ImageNet 256256. Figure 6(a)(b) illustrates the FIDIS trade-off varying the CFG scale. We summarize the advantages of DDO as follows: (1) Eliminating sampling tricks. It is worth noting that the original VAR results are based on top-k and top-p sampling strategies, which artificially lower the temperature. These heuristics introduce training-inference gap and fail to reflect the genuine capability of pretrained models. In contrast, when evaluating models finetuned with DDO, we discard all such tricks, ensuring more principled assessment of generative performance. (2) Guidance-free performance. DDO significantly reduces the guidance-free FID from 11.33/4.74 to 3.12/1.79 for VAR-d16 and VAR-d30, achieving 3.6 and 2.6 improvement. Notably, the guidance-free FIDs even outperform CFG-enhanced FIDs (3.30/1.90) of the original VAR, indicating that higher-quality samples can be generated at half the inference cost with DDO. This is also superior to methods like CCA (Chen et al., 2024a) which only aim to remove the CFG but harm the model performance. (3) CFG-enhanced performance. When combined with CFG, the finetuned VAR models achieve significantly better FID-IS trade-offs than the pretrained counterparts, even when the latter employ sampling tricks. The lowest FID improves from 3.30/1.90 to 2.54/1.79. Furthermore, the finetuned VAR-d16 (310M parameters) outperforms the 2 larger VAR-d20 (600M parameters) which has CFGenhanced FID of 2.57, showcasing the effectiveness of DDO in optimizing model efficiency. Effects of α, β Figure 6(c) visualizes FID and IS varying the CFG scale, where we finetune VAR-d16 under β = 0.02 and different α for 60 iterations. The results indicate that all α [10, 100] consistently achieve CFG-enhanced FID lower than that of the base model. Larger values of α tend to yield lower guidance-free FIDs but may slightly weaken performance when combined with CFG. 6. Conclusion In this work, we introduce new finetuning method of visual likelihood-based generative models named Direct Discriminative Optimization (DDO) aimed at enhancing generation quality, which draws inspiration from the GAN framework and the parameterization insight in DPO. Experiments demonstrate that DDO is both highly effective and efficient, yielding substantial performance improvements over state-of-the-art diffusion and autoregressive models, and achieving record-breaking FID scores on standard image benchmarks. There remain promising directions for future exploration, such as eliminating the need for hyperparameter searching, improving inference efficiency through distillation, and scaling to tasks like text-to-image genera8 Direct Discriminative Optimization tion. We leave these avenues for future work."
        },
        {
            "title": "Impact Statement",
            "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none of which we feel must be specifically highlighted here."
        },
        {
            "title": "References",
            "content": "Achiam, J., Adler, S., Agarwal, S., Ahmad, L., Akkaya, I., Aleman, F. L., Almeida, D., Altenschmidt, J., Altman, S., Anadkat, S., et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein generative adversarial networks. In International conference on machine learning, pp. 214223. PMLR, 2017. Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van Den Berg, R. Structured denoising diffusion models in discrete state-spaces. Advances in Neural Information Processing Systems, 34:1798117993, 2021. Balaji, Y., Nah, S., Huang, X., Vahdat, A., Song, J., Zhang, Q., Kreis, K., Aittala, M., Aila, T., Laine, S., et al. ediffi: Text-to-image diffusion models with an ensemble of expert denoisers. arXiv preprint arXiv:2211.01324, 2022. Bao, F., Xiang, C., Yue, G., He, G., Zhu, H., Zheng, K., Zhao, M., Liu, S., Wang, Y., and Zhu, J. Vidu: highly consistent, dynamic and skilled text-to-video generator with diffusion models. arXiv preprint arXiv:2405.04233, 2024. Barratt, S. and Sharma, R. note on the inception score. arXiv preprint arXiv:1801.01973, 2018. Bishop, C. M. and Nasrabadi, N. M. Pattern recognition and machine learning, volume 4. Springer, 2006. Bradley, R. A. and Terry, M. E. Rank analysis of incomplete block designs: I. the method of paired comparisons. Biometrika, 39(3/4):324345, 1952. Brock, A. Large scale gan training for high fidelity natural image synthesis. arXiv preprint arXiv:1809.11096, 2018. Advances in neural information processing systems, 33: 18771901, 2020. Chang, H., Zhang, H., Jiang, L., Liu, C., and Freeman, W. T. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1131511325, 2022. Chen, H., Su, H., Sun, P., and Zhu, J. Toward guidance-free ar visual generation via condition contrastive alignment. arXiv preprint arXiv:2410.09347, 2024a. Chen, R. T., Rubanova, Y., Bettencourt, J., and Duvenaud, D. K. Neural ordinary differential equations. Advances in neural information processing systems, 31, 2018. Chen, Z., Deng, Y., Yuan, H., Ji, K., and Gu, Q. Self-play fine-tuning converts weak language models to strong language models. arXiv preprint arXiv:2401.01335, 2024b. Deng, J., Dong, W., Socher, R., Li, L., Li, K., and Fei-Fei, L. ImageNet: large-scale hierarchical image database. In 2009 IEEE Conference on Computer Vision and Pattern Recognition, pp. 248255. IEEE, 2009. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pp. 41714186, 2019. Dhariwal, P. and Nichol, A. Q. Diffusion models beat GANs on image synthesis. In Advances in Neural Information Processing Systems, volume 34, pp. 87808794, 2021. Dinh, L., Sohl-Dickstein, J., and Bengio, S. Density estimation using real nvp. arXiv preprint arXiv:1605.08803, 2016. Du, Y. and Mordatch, I. Implicit generation and modeling with energy based models. Advances in Neural Information Processing Systems, 32, 2019. Esser, P., Rombach, R., and Ommer, B. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1287312883, 2021. Brooks, T., Peebles, B., Holmes, C., DePue, W., Guo, Y., Jing, L., Schnurr, D., Taylor, J., Luhman, T., Luhman, E., et al. Video generation models as world simulators. 2024. URL https://openai. com/research/videogeneration-models-as-world-simulators, 3, 2024. Esser, P., Kulal, S., Blattmann, A., Entezari, R., Muller, J., Saini, H., Levi, Y., Lorenz, D., Sauer, A., Boesel, F., et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D., Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G., Askell, A., et al. Language models are few-shot learners. 9 Goodfellow, I., Bengio, Y., and Courville, A. Deep http://www. MIT Press, 2016. Learning. deeplearningbook.org. Direct Discriminative Optimization Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville, A. C., and Bengio, Y. Generative adversarial nets. In Advances in Neural Information Processing Systems, volume 27, pp. 2672 2680, 2014. Gupta, A., Yu, L., Sohn, K., Gu, X., Hahn, M., Fei-Fei, L., Essa, I., Jiang, L., and Lezama, J. Photorealistic video generation with diffusion models. arXiv preprint arXiv:2312.06662, 2023. Gutmann, M. and Hyvarinen, A. Noise-contrastive estimation: new estimation principle for unnormalized statistical models. In Proceedings of the thirteenth international conference on artificial intelligence and statistics, pp. 297304. JMLR Workshop and Conference Proceedings, 2010. Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and Hochreiter, S. GANs trained by two time-scale update rule converge to local Nash equilibrium. In Guyon, I., von Luxburg, U., Bengio, S., Wallach, H. M., Fergus, R., Vishwanathan, S. V. N., and Garnett, R. (eds.), Advances in Neural Information Processing Systems, volume 30, pp. 66266637, 2017. Ho, J. and Salimans, T. Classifier-free diffusion guidance. In NeurIPS 2021 Workshop on Deep Generative Models and Downstream Applications, 2021. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probIn Advances in Neural Information abilistic models. Processing Systems, volume 33, pp. 68406851, 2020. Ho, J., Chan, W., Saharia, C., Whang, J., Gao, R., Gritsenko, A., Kingma, D. P., Poole, B., Norouzi, M., Fleet, D. J., et al. Imagen video: High definition video generation with diffusion models. arXiv preprint arXiv:2210.02303, 2022. Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating the design space of diffusion-based generative models. In Advances in Neural Information Processing Systems, 2022. Karras, T., Aittala, M., Kynkaanniemi, T., Lehtinen, J., Aila, T., and Laine, S. Guiding diffusion model with bad version of itself. arXiv preprint arXiv:2406.02507, 2024a. Karras, T., Aittala, M., Lehtinen, J., Hellsten, J., Aila, T., and Laine, S. Analyzing and improving the training In Proceedings of the dynamics of diffusion models. IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2417424184, 2024b. Kim, D., Kim, Y., Kwon, S. J., Kang, W., and Moon, I.-C. Refining generative process with discriminator guidance in score-based diffusion models. In International Conference on Machine Learning, pp. 1656716598. PMLR, 2023a. Kim, D., Lai, C.-H., Liao, W.-H., Murata, N., Takida, Y., Uesaka, T., He, Y., Mitsufuji, Y., and Ermon, S. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. In The Twelfth International Conference on Learning Representations, 2023b. Kim, D., Lai, C.-H., Liao, W.-H., Takida, Y., Murata, N., Uesaka, T., Mitsufuji, Y., and Ermon, S. Pagoda: Progressive growing of one-step generator from low-resolution diffusion teacher. arXiv preprint arXiv:2405.14822, 2024. Kingma, D. and Gao, R. Understanding diffusion objectives as the elbo with simple data augmentation. Advances in Neural Information Processing Systems, 36, 2024. Kingma, D. P. and Welling, M. Auto-encoding variational bayes. In International Conference on Learning Representations, 2014. Huang, Y., Gokaslan, A., Kuleshov, V., and Tompkin, J. The gan is dead; long live the gan! modern gan baseline. arXiv preprint arXiv:2501.05441, 2025. Kingma, D. P., Salimans, T., Poole, B., and Ho, J. Variational diffusion models. In Advances in Neural Information Processing Systems, 2021. Jabri, A., Fleet, D., and Chen, T. Scalable adaptive computation for iterative generation. arXiv preprint arXiv:2212.11972, 2022. Kang, M., Zhu, J.-Y., Zhang, R., Park, J., Shechtman, E., Paris, S., and Park, T. Scaling up gans for text-to-image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10124 10134, 2023. Karras, T., Aittala, M., Hellsten, J., Laine, S., Lehtinen, J., and Aila, T. Training generative adversarial networks with limited data. Advances in neural information processing systems, 33:1210412114, 2020. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Li, T., Tian, Y., Li, H., Deng, M., and He, K. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Lou, A., Meng, C., and Ermon, S. Discrete diffusion language modeling by estimating the ratios of the data distribution. arXiv preprint arXiv:2310.16834, 2023. 10 Direct Discriminative Optimization Lu, C., Zheng, K., Bao, F., Chen, J., Li, C., and Zhu, J. Maximum likelihood training for score-based diffusion odes by high order denoising score matching. In International Conference on Machine Learning, pp. 14429 14460. PMLR, 2022a. Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpmsolver: fast ode solver for diffusion probabilistic model In Advances in Neural sampling in around 10 steps. Information Processing Systems, 2022b. Luo, Z., Shi, F., Ge, Y., Yang, Y., Wang, L., and Shan, Y. Open-magvit2: An open-source project toward democratizing auto-regressive visual generation. arXiv preprint arXiv:2409.04410, 2024. Ma, N., Goldstein, M., Albergo, M. S., Boffi, N. M., VandenEijnden, E., and Xie, S. Sit: Exploring flow and diffusionbased generative models with scalable interpolant transformers. arXiv preprint arXiv:2401.08740, 2024. Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion In International Conference on probabilistic models. Machine Learning, pp. 81628171. PMLR, 2021. Ouyang, L., Wu, J., Jiang, X., Almeida, D., Wainwright, C., Mishkin, P., Zhang, C., Agarwal, S., Slama, K., Ray, A., et al. Training language models to follow instructions with human feedback. Advances in neural information processing systems, 35:2773027744, 2022. Park, D., Lee, S., Kim, S., Lee, T., Hong, Y., and Kim, arXiv preprint H. J. Constant acceleration flow. arXiv:2411.00322, 2024. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 41954205, 2023. Rafailov, R., Sharma, A., Mitchell, E., Manning, C. D., Ermon, S., and Finn, C. Direct preference optimization: Your language model is secretly reward model. Advances in Neural Information Processing Systems, 36, 2024. Ramesh, A., Pavlov, M., Goh, G., Gray, S., Voss, C., Radford, A., Chen, M., and Sutskever, I. Zero-shot text-toimage generation. In International conference on machine learning, pp. 88218831. Pmlr, 2021. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1068410695, 2022. Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. In International Conference on Learning Representations, 2022. Sauer, A., Schwarz, K., and Geiger, A. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pp. 110, 2022. Song, Y., Durkan, C., Murray, I., and Ermon, S. Maximum likelihood training of score-based diffusion models. In Advances in Neural Information Processing Systems, volume 34, pp. 14151428, 2021a. Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based generative modeling through stochastic differential equations. In International Conference on Learning Representations, 2021b. Sun, P., Jiang, Y., Chen, S., Zhang, S., Peng, B., Luo, P., and Yuan, Z. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. Tian, K., Jiang, Y., Yuan, Z., Peng, B., and Wang, L. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. Vahdat, A., Kreis, K., and Kautz, J. Score-based generative modeling in latent space. In Advances in Neural Information Processing Systems, volume 34, pp. 1128711302, 2021. Van Den Oord, A., Kalchbrenner, N., and Kavukcuoglu, In International K. Pixel recurrent neural networks. conference on machine learning, pp. 17471756. PMLR, 2016. Van Den Oord, A., Vinyals, O., et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. Wallace, B., Dang, M., Rafailov, R., Zhou, L., Lou, A., Purushwalkam, S., Ermon, S., Xiong, C., Joty, S., and Naik, N. Diffusion model alignment using direct preference optimization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 82288238, 2024. Wang, Z., Zheng, H., He, P., Chen, W., and Zhou, M. arXiv Diffusion-gan: Training gans with diffusion. preprint arXiv:2206.02262, 2022. Xiao, Z., Kreis, K., and Vahdat, A. Tackling the generative learning trilemma with denoising diffusion GANs. In International Conference on Learning Representations, 2022. 11 Direct Discriminative Optimization Xie, J., Mao, W., Bai, Z., Zhang, D. J., Wang, W., Lin, K. Q., Gu, Y., Chen, Z., Yang, Z., and Shou, M. Z. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Xu, J., Lee, A., Sukhbaatar, S., and Weston, J. Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682, 2023. Xu, Y., Corso, G., Jaakkola, T., Vahdat, A., and Kreis, K. Disco-diff: Enhancing continuous diffusion models with discrete latents. arXiv preprint arXiv:2407.03300, 2024. Yin, T., Gharbi, M., Park, T., Zhang, R., Shechtman, E., Durand, F., and Freeman, W. T. Improved distribution matching distillation for fast image synthesis. arXiv preprint arXiv:2405.14867, 2024. Yu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., Ku, A., Xu, Y., Baldridge, J., and Wu, Y. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. Zheng, B. and Yang, T. Diffusion models are innate one-step generators. arXiv preprint arXiv:2405.20750, 2024. Zheng, K., Lu, C., Chen, J., and Zhu, J. DPM-Solverv3: Improved diffusion ode solver with empirical model statistics. In Thirty-seventh Conference on Neural Information Processing Systems, 2023a. Zheng, K., Lu, C., Chen, J., and Zhu, J. Improved techniques for maximum likelihood estimation for diffusion odes. In International Conference on Machine Learning, pp. 4236342389. PMLR, 2023b. Zheng, K., Chen, Y., Mao, H., Liu, M.-Y., Zhu, J., and Zhang, Q. Masked diffusion models are secretly timeagnostic masked models and exploit inaccurate categorical sampling. arXiv preprint arXiv:2409.02908, 2024. Zhou, C., Yu, L., Babu, A., Tirumala, K., Yasunaga, M., Shamis, L., Kahn, J., Ma, X., Zettlemoyer, L., and Levy, O. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024a. Zhou, M., Zheng, H., Gu, Y., Wang, Z., and Huang, H. Adversarial score identity distillation: Rapidly surpassing the teacher in one step. arXiv preprint arXiv:2410.14919, 2024b. 12 A. Related Work Direct Discriminative Optimization Paradigms of Generative Models Beyond autoregressive (AR) and diffusion models introduced in Section 2.1, other likelihood-based generative models have been explored in the literature. Variational autoencoders (VAEs) (Kingma & Welling, 2014), energy-based models (Du & Mordatch, 2019), and normalizing flows (Dinh et al., 2016) are once popular for generative modeling of continuous data, but have since fallen out of favor due to their limited expressiveness and lack of scalability in modern large-scale generation tasks. In particular, VAEs are now primarily used as dimensionality reduction tools that compress data into latent spaces (Esser et al., 2021; Rombach et al., 2022), rather than generating samples from scratch. For discrete data generation, masked models, such as BERT (Devlin et al., 2019) for masked language modeling and MaskGIT (Chang et al., 2022) for masked image generation, offer an alternative likelihood-based paradigm to AR. While discrete diffusion language models (Austin et al., 2021; Lou et al., 2023) have recently regained interest, they are largely equivalent to the simpler masked models and suffer from numerical precision issues that lead to unfair evaluation (Zheng et al., 2024). There have also been pioneering efforts to combine different generative paradigms. MAR (Li et al., 2024) integrates masked modeling with the diffusion loss, enabling autoregressive image generation with continuous tokens. Transfusion (Zhou et al., 2024a) and Show-o (Xie et al., 2024) combine AR with diffusion/masked models for multi-modal generation, effectively synthesizing mixture of text and image data. DDO is potentially applicable to these models, and we leave such explorations for future work. Improving Generation Quality with GAN Except for using GAN as an auxiliary loss for enhancing one-step or few-step generation in diffusion distillation (Kim et al., 2023b; Yin et al., 2024; Zhou et al., 2024b), as mentioned in Section 2.2, several works have explored directly integrating diffusion models with GANs. Notably, Xiao et al. (2022) replaces the reverse denoising steps in diffusion models with sequence of conditional GAN generators, enabling few-step generation. Wang et al. (2022) modifies the GAN discriminator to distinguish between diffused real and generated samples in time-aware manner. Kim et al. (2023a) leverages the gradient information from trained discriminator to refine pretrained diffusion models. Chen et al. (2024a) adopts binary classification loss with likelihood ratio parameterization similar to our objective, but its applicability is limited to removing CFG in autoregressive models and degrades the model performance. B. Theoretical Analyses of the DDO Objective In this section, we investigate the theoretical properties of the DDO objective and provide informal proofs to Theorem 3.1, Theorem 3.2, and Theorem 3.3 in the main text. B.1. Analyses of L(θ) Optimal Solution It is straightforward to show that the optimal θ minimizing L(θ) satisfies pθ = pdata following the common GAN literature (Goodfellow et al., 2014). Specifically, let rθ(x) := log pθ(x) pθref (x) denote the log-likelihood ratio between the learnable and reference distribution. The objective L(θ) can be expressed as an integral form: (cid:90) L(θ) = L(θ)xdx (19) where L(θ)x = pdata(x) log σ(rθ(x)) pθref(x) log(1 σ(rθ(x))) > 0 (20) is the pointwise loss, and we only consider in the valid range where pdata and pθref have nonzero support. For any (a, b) R2{(0, 0)}, the function log log(1 y), [0, 1] achieves its minimum at a+b . Applying this to the pointwise loss L(θ)x, the minimizer satisfies σ(rθ (x)) = pθ (x) pθ (x) + pθref(x) = pdata(x) pdata(x) + pθref(x) pθ (x) = pdata(x) (21) Since the global minimizer of L(θ) is the pointwise minimizer of L(θ)x for all x, it follows that pθ = pdata. Loss Gradient Using the derivative identity dσ(x) σ(x). Applying these to the pointwise loss, we derive the gradient w.r.t. rθ(x): dx = σ(x)(1 σ(x)), we obtain log σ(x) dx = 1 σ(x), log(1σ(x)) dx = dL(θ)x drθ(x) = pθref(x)σ(rθ(x)) pdata(x)(1 σ(rθ(x))) = pθ(x) pdata(x) pθ(x) + pθref(x) 13 pθref(x) = (1 dθ(x))(pθ(x) pdata(x)) (22) Direct Discriminative Optimization Thus, the full loss gradient is given by (cid:90) θL(θ) = θL(θ)xdx = (cid:90) dL(θ)x drθ(x) θrθ(x)dx = (cid:90) (1 dθ(x))(pθ(x) pdata(x))θ log pθ(x)dx (23) Divergence Bounds We aim to derive bounds for the divergence between pθ and pdata when θ is not optimal, using the difference between the loss value L(θ) and its optimal counterpart L. Without any assumptions, the forward KL divergence DKL(pdata pθ) is lower bounded by L(θ) L. By definition, is the minimum loss value achieved when pθ = pdata. The difference L(θ) can be decomposed as follows: L(θ) = + (cid:90) = (cid:90) (cid:90) pdata(x) log pdata(x) log pθ(x) pθ(x) + pθref (x) pdata(x) pdata(x) + pθref (x) + pθref (x) log pθref(x) pθ(x) + pθref(x) dx + pθref (x) log pdata(x) log pdata(x) pθ(x) + (pdata(x) + pθref(x)) log pθref (x) pdata(x) + pθref(x) pθ(x) + pθref (x) pdata(x) + pθref(x) dx dx (24) =DKL(pdata pθ) DKL( pθ + pθref 2 ) pdata + pθref 2 pθ+pθref 2 Therefore, DKL(pdata pθ) = L(θ) + DKL( pdata+pθref ) L(θ) L. While this result establishes lower bound for the divergence, additional assumptions are required to derive an upper bound. Specifically, we assume that (cid:12) log pθref (cid:12) (cid:12) M, M1 pdata log pθref (x) pdata(x) M2 for all x. The pointwise loss can be expressed as function of rθ(x): are bounded, i.e., there exist constants M, M1, M2 such that rθ(x) = (cid:12) (cid:12)log pθ(x) (cid:12) pθref (x) and log pθ pθref 2 L(θ)x = (rθ(x)) where (y) := pdata(x) log σ(y) pθref(x) log(1 σ(y)) = (pdata(x) + pθref(x)) log (1 + ey) pdatay The first and second order derivatives of are given by: (y) = pθref(x)ey pdata(x) 1 + ey , (y) = (pdata(x) + pθref(x))ey (1 + ey)2 = pdata(x) + pθref (x) 2 + ey + ey Applying Taylors expansion at = rθ (x) = log pdata(x) pθref (x) , we obtain: (rθ(x)) = (rθ (x)) + (rθ (x))(rθ(x) rθ (x)) + 1 2 (ξ)(rθ(x) rθ (x))2 (25) (26) (27) (28) where ξ [min{rθ(x), rθ (x)}, max{rθ(x), rθ (x)}]. Since (rθ (x)) = 0 and rθ(x) rθ (x) = log pθ(x) pdata(x) , we get: (cid:18) log (cid:19)2 pθ(x) pdata(x) = 2 (ξ) (L(θ)x L(θ)x) (29) Note that (y) is monotonically decreasing function w.r.t. and attains its maximum at the boundary of the given range, we have: 2 (ξ) (cid:26) max = 2 max 2 (rθ(x)) (cid:26) 2 + eM + eM 2 (rθ (x)) , pdata(x) + pθref (x) (cid:27) max (cid:26) 2 (cid:27) , 2 (rθ (x)) (M ) (cid:27) , pdata(x) + pθref(x) pdata(x)pθref(x) Therefore, (cid:18) pdata(x) log (cid:19)2 pθ(x) pdata(x) 2 max (cid:26) 2 + eM + eM 1 + pθref(x)/pdata(x) , pdata(x) + pθref(x) pθref (x) (cid:27) , 1 + eM1 (L(θ)x L(θ)x) (cid:27) (L(θ)x L(θ)x) 2 max (cid:26) 2 + eM + eM 1 + eM1 14 (30) (31) Direct Discriminative Optimization Applying Jensens inequality, we derive an upper bound for the forward KL divergence: DKL(pdata pθ) = Epdata(x) (cid:20) log pdata(x) pθ(x) (cid:21) (cid:118) (cid:117) (cid:117) (cid:116)Epdata(x) (cid:34)(cid:18) log pdata(x) pθ(x) (cid:19)2(cid:35) (cid:115) (cid:90) = (cid:18) pdata(x) log (cid:19)2 dx pθ(x) pdata(x) (32) (cid:112)L(θ) (cid:114) where C1 = 2 max (cid:110) 2+eM +eM 1+eM1 , 1 + eM (cid:111) is related to the lower bound of log pθref pdata . Similarly, (cid:18) pθ(x) log (cid:19)2 pθ(x) pdata(x) 2 pθ(x) pθref(x) max (cid:26) 2 + eM + eM 1 + pdata(x)/pθref(x) , pdata(x) + pθref (x) pdata(x) (cid:27) (L(θ)x L(θ)x) 2eM max (cid:26) 2 + eM + eM 1 + eM2 (cid:27) , 1 + eM2 (L(θ)x L(θ)x) (33) By integrating over x, we obtain DKL(pθ pdata) C2 is related to the upper bound of log pθref pdata . B.2. Analyses of Lα,β(θ) (cid:112)L(θ) L, where C2 = (cid:114) 2eM max (cid:110) 2+eM +eM 1+eM , 1 + eM2 (cid:111) Once we introduce additional coefficients α, β, the generalized DDO objective Lα,β(θ) may become intractable and no longer admit pθ = pdata as the optimal solution. Specifically, the pointwise loss with α, β is The optimal θ should satisfy Lα,β(θ)x = pdata(x) log σ(βrθ(x)) αpθref(x) log(1 σ(βrθ(x))) dLα,β(θ)x drθ(x) = αβpθref (x)σ(βrθ(x)) βpdata(x)(1 σ(βrθ(x))) = 0 σ(βrθ(x)) = pθ(x) = pθref(x) pdata(x) pdata(x) + αpθref(x) (cid:19)1/β (cid:18) pdata(x) αpθref (x) (cid:18) = σ log (cid:19) pdata(x) αpθref(x) (34) (35) However, since pθ is parameterized as likelihood-based generative model, it must have self-normalized density. This optimality condition is only achieved when α is proper normalizing constant satisfying (cid:90) pθ(x)dx = 1 α = (cid:18)(cid:90) p11/β θref (x)p1/β data (x)dx (cid:19)β (36) Under this specific choice of α, the optimal solution pθ p11/β constraint (cid:82) pθ(x)dx = 1. To enforce this, we introduce Lagrange multiplier λ and define the Lagrangian: p1/β data . Otherwise, the optimization is subject to the θref (cid:90) = Lα,β(θ)xdx + λ 1 (cid:18) (cid:90) (cid:19) pθ(x)dx (37) To find the optimal pθ, we take the functional derivative of w.r.t. pθ and set it to zero: δL δpθ(x) which can be simplified to = dLα,β(θ)x dpθ(x) λ = (αβpθref (x)σ(βrθ(x)) βpdata(x)(1 σ(βrθ(x)))) drθ(x) dpθ(x) λ = 0 (38) (cid:19)β α (cid:18) pθ(x) pθref(x) λ β pθ(x) pθref(x) (cid:34) 1 + (cid:18) pθ(x) pθref (x) (cid:19)β(cid:35) = pdata(x) pθref (x) (39) This equation, combined with the constraint (cid:82) pθ(x)dx = 1, determines both λ and the optimal pθ. However, no closedform solution exists for this problem. Despite this, we can expect certain ranges of α to skew the optimal solution away from pdata toward the direction of p11/β p1/β data , even if the exact equality does not hold. θref 15 C. Experiment Details Direct Discriminative Optimization Throughout all experiments, each training run under given set of configurations (certain reference model and hyperparameters α, β) is conducted on single node with 8 NVIDIA A100 (SXM4-80GB) GPUs. Diffusion Models We follow the parameterization and noise schedule of EDM (Karras et al., 2022) and EDM2 (Karras et al., 2024b). Specifically, EDM introduces time-dependent skip connection that preconditions the denoiser Dθ (which predicts clean data x0) using free-form network Fθ, allowing Fθ to predict an adaptive mixture of signal and noise: Dθ(xt, t) = cskip(t)xt + cout(t)Fθ(cin(t)xt, cnoise(t)) where cskip(t) = data σ2 data + t2 , σ2 cout(t) = σdatat data + t2 (cid:112)σ2 , cin(t) = 1 data + t2 (cid:112)σ2 , cnoise(t) = 1 4 log (40) (41) EDM employs simple variance exploding (VE) noise schedule satisfying αt = 1, σt = t. Its worth noting that the preconditioning used in EDM actually transforms it into v-prediction under the variance preserving (VP) noise schedule, owing to the normalizing factor cin(t) and the skip connection coefficients cskip(t), cout(t) (Zheng et al., 2023b). The EDM models are pretrained by F-prediction MSE loss: LEDM(θ) = Ex0pdata,tp(t),ϵN (0,I) (cid:104) Fθ(cin(t)xt, cnoise(t)) ˆF (x0, xt, t)2 2 (cid:105) (42) where xt = x0 + tϵ, ˆF (x0, xt, t) = x0cskip(t)xt log (Pmean, 2 std), where Pmean, Pstd are hyperparameters. cout(t) is the prediction target, and p(t) is time distribution satisfying We adopt similar settings for DDO finetuning. Specifically, we use the approximation in Eqn. (16) and set the weighting w(t) to satisfy w(t)ϵθ ϵ 2 = Fθ ˆF 2 LEDM-DDO α,β (θ) = Etp(t),ϵN (0,I) Epdata(x0) log σ 2, leading to the following objective (1 σ(x) = σ(x)): (cid:16) (cid:16) (cid:104) Fθ ˆF 2 2 Fθref ˆF 2 +αEpθref (x0) log σ Fθ ˆF 2 Fθref ˆF 2 2 β (cid:16) (cid:16) β (cid:17)(cid:17) 2 (cid:17)(cid:17) (cid:105) (43) where we use the same form of time distribution log (Pmean, 2 std), and Pmean, Pstd are typically the same as pretraining. For each finetuning round, we launch 20 nodes to sweep over the hyperparameters α, β in [0.5, 6.0] [0.01, 0.1]. We disable all dropout layers in the network to ensure steady improvement. We also find numerical precision crucial for the diffusion DDO loss and disable mixed-precision training. For each round, 50k images are generated offline from the reference model as the reference dataset. For EDM on CIFAR-10, we finetune the unconditional and class-conditional model for 12 and 16 rounds, respectively. We set Pmean = 1.2, Pstd = 1.2 throughout all rounds, which is the same as pretraining. Each round has duration of 1.5M images (30 epochs, 0.75% of pretraining) with batch size of 512. The learning rate warms up linearly from 0 to 1.5e 4 during each round, and the data augmentation probability is set to 12% as pretraining. We find the exponential moving average (EMA) beneficial for stabilizing the model performance and choose relatively small EMA half-life (0.25M images) as we finetune less duration than pretraining. We evaluate the FID each time trained with 50k images and save the best model for the next round. Each round takes 3h including both training and evaluation. For EDM2-S on ImageNet-64, we finetune the model for 24 rounds, where we set Pmean = 0.8, Pstd = 1.6 for the first 16 rounds following pretraining, and increase Pstd to 3.0 in the last 8 rounds. Each round has duration of 6.4M images (5 epochs, 0.6% of pretraining) with batch size of 512. We use learning rate of 5e 5 for the first 16 rounds and 2e 5 for the last 8 rounds, along with the learning rate scheduler in EDM2 which is mix of linear warmup and inverse square root decay, where we set the ramp-up to 1M images and the decay reference to 2000 iterations. We follow the power function EMA introduced in EDM2 and set the EMA length to 0.05. We evaluate the FID each time trained with 217 ( 131k) images and save the best model for the next round. Each round takes 1d including both training and evaluation. Autoregressive Models We finetune VAR-d16 and VAR-d30 (Tian et al., 2024) both for only 2 rounds. VAR is highly efficient at inference, enabling us to sample from the reference distribution online during training by generating random latent tokens with the reference model conditioned on the same class labels as those in the dataset batch. We disable all Direct Discriminative Optimization dropout layers in the network and set the label dropout probability to 50% for unconditional training. Unlike diffusion DDO, we enable mixed-precision when finetuning VAR. For each round, we launch 10 nodes to sweep over the hyperparameters α, β in [10.0, 100.0] {0.02}. Each round has duration of 80 iterations (0.064 epoch, less than 0.03% of pretraining) with batch size of 1024. We follow the learning rate scheduler in VAR pretraining and set the peak learning rate to smaller value 4e 6. We evaluate the FIDs (corresponding to guidance-free/a moderate CFG scale) every 4 iterations and save the best model for the next round. Each round takes 5h/7.5h for VAR-d16/d30 including both training and evaluation. D. Additional Results 17 Direct Discriminative Optimization Figure 7. Random samples of EDM (CIFAR-10, Unconditional), FID 1.97. Figure 8. Random samples of EDM + DDO (CIFAR-10, Unconditional), FID 1.38. 18 Direct Discriminative Optimization Figure 9. Random samples of EDM (CIFAR-10, Class-conditional), FID 1.85. Figure 10. Random samples of EDM + DDO (CIFAR-10, Class-conditional), FID 1.30. Direct Discriminative Optimization Figure 11. Random samples of EDM2-S (ImageNet-64, Class-conditional), FID 1.60. Figure 12. Random samples of EDM2-S + DDO (ImageNet-64, Class-conditional), FID 0.97. 20 Direct Discriminative Optimization Figure 13. Illustration of the multi-round refinement process on EDM2-S (ImageNet-64). 21 Direct Discriminative Optimization VAR-d16 w/o trick (FID 11.33) VAR-d16 w/ trick (FID 3.62) VAR-d16 + DDO (FID 3.12) Figure 14. Guidance-free samples by pretrained and finetuned VAR-d16. 22 Direct Discriminative Optimization VAR-d16 w/o trick (FID 3.71) VAR-d16 w/ trick (FID 3.30) VAR-d16 + DDO (FID 2.54) Figure 15. CFG-enhanced samples by pretrained and finetuned VAR-d16. 23 Direct Discriminative Optimization VAR-d30 w/o trick (FID 4.74) VAR-d30 w/ trick (FID 2.17) VAR-d30 + DDO (FID 1.79) Figure 16. Guidance-free samples by pretrained and finetuned VAR-d30. 24 Direct Discriminative Optimization VAR-d30 w/o trick (FID 1.92) VAR-d30 w/ trick (FID 1.90) VAR-d30 + DDO (FID 1.73) Figure 17. CFG-enhanced samples by pretrained and finetuned VAR-d30."
        }
    ],
    "affiliations": [
        "NVIDIA",
        "The University of Texas at",
        "Tsinghua University"
    ]
}