{
    "paper_title": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies",
    "authors": [
        "Tao Xiong",
        "Xavier Hu",
        "Wenyan Fan",
        "Shengyu Zhang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) excel in complex tasks through advanced prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but their reliance on manually crafted, task-specific prompts limits adaptability and efficiency. We introduce Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised fine-tuning.Our experiments show that MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 6 0 6 0 0 . 7 0 5 2 : r Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies Tao Xiong 030130x@mail.dlut.edu.cn Dalian University of Technology Dalian, LiaoNing, China Wenyan Fan wenyan.17@outlook.com Zhejiang University Hangzhou, ZheJiang, China Xavier Hu xavier.hu.research@gmail.com Independent Hangzhou, ZheJiang, China Shengyu Zhang sy_zhang@zju.edu.cn Zhejiang University Hangzhou, ZheJiang, China ABSTRACT Large language models (LLMs) excel in complex tasks through advanced prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but their reliance on manually crafted, taskspecific prompts limits adaptability and efficiency. We introduce Mixture of Reasoning (MoR), training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised fine-tuning. Our experiments show that MoR significantly enhances performance, with ğ‘€ğ‘œğ‘…150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need for task-specific prompts, offering generalizable solution for robust reasoning across diverse tasks. CCS CONCEPTS Computing methodologies Natural language processing. KEYWORDS Large Language Models, Mixture of Reasoning, Natural Language Processing ACM Reference Format: Tao Xiong, Xavier Hu, Wenyan Fan, and Shengyu Zhang. 2025. Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies. In Proceedings of the 48th International ACM ICMR Conference on Research and Development in Information Retrieval (ICMR 25), June 30 July 3, 2025, Chicago, IL, USA. ACM, New York, NY, USA, 5 pages. https: //doi.org/10.1145/nnnnnnn.nnnnnnn Both authors contributed equally to this research. Coresponding Author. Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or fee. Request permissions from permissions@acm.org. ICMR 25, June 30July 3, 2025, Chicago, IL, USA. 2025 Copyright held by the owner/author(s). Publication rights licensed to ACM. ACM ISBN 979-8-4007-1592-1/25/07. . . $15.00 https://doi.org/10.1145/nnnnnnn.nnnnnnn"
        },
        {
            "title": "1 INTRODUCTION\nLarge language models (LLMs) have achieved remarkable success\nacross diverse domains, largely due to advanced prompting tech-\nniques such as Chain-of-Thought (CoT) [12] , Tree-of-Thought (ToT)\n[15], and Prompt-of-Thought (PoT) [20]. These methods guide mod-\nels to reason step-by-step or explore multiple reasoning paths, sig-\nnificantly enhancing their performance on complex tasks. However,\ntheir effectiveness heavily relies on manually crafted, task-specific\nprompts, which are time-consuming to design and challenging to\nadapt optimally across varied tasks. This dependency on prompt en-\ngineering poses a critical bottleneck, where generic prompts often\nfail to elicit robust reasoning.",
            "content": "To address this challenge, we propose Mixture of Reasoning (MoR), novel training framework that embeds diverse set of reasoning strategies directly into LLMs, enabling them to autonomously select and apply effective reasoning methods tailored to specific tasks. Unlike existing approaches [3, 19] that rely on external prompt engineering to elicit reasoning, MoR internalizes reasoning capabilities by fine-tuning models on curated supervised fine-tuning (SFT) dataset enriched with reasoning chain templates. These templates, generated by leveraging the advanced reasoning abilities of closed-source large models (e.g., GPT-4o), cover wide range of reasoning patterns, including multi-step deduction, analogical reasoning, and strategic thinking. The MoR framework operates in two key phases: (1) Thought Generation, where we produce largescale reasoning chain templates (e.g., 50, 150, 300, and 500 chains) to capture diverse problem-solving approaches, and (2) SFT Dataset Construction, where we pair these templates with samples from benchmark datasets to create training dataset that teaches models to adaptively apply reasoning strategies. By embedding these strategies into the models parameters, MoR eliminates the need for task-specific prompt design and enhances generalizability across complex reasoning tasks. Our experiments demonstrate that MoR significantly outperforms baseline models, with our best model, ğ‘€ğ‘œğ‘…150, achieving performance of 0.730 with CoT prompting (a 2.2% improvement over the baseline) and 0.734 with direct IO prompting (a 13.5% improvement), showcasing its ability to reason effectively without explicit guidance. Our contributions are as follows: ICMR 25, June 30July 3, 2025, Chicago, IL, USA. Tao Xiong, Xavier Hu, Wenyan Fan and Shengyu Zhang We introduce MoR, training framework that embeds diverse reasoning strategies into LLMs, enabling task-adaptive reasoning without reliance on specific prompts. We propose two-step methodology involving Thought Generation and SFT Dataset Construction, leveraging large-scale reasoning templates and curated datasets. We provide comprehensive experimental evidence demonstrating MoRs superiority over baseline models, with detailed analyses and case studies illustrating its logical reasoning capabilities."
        },
        {
            "title": "2 RELATED WORK\nSupervised Fine-Tuning of Large Language Models. Supervised\nFine-Tuning (SFT) [17] leverages structured (instruction-answer)\npairs to fully exploit the zero-shot capabilities of large models.\nThis process enables models to learn systematic reasoning patterns\nand produce accurate results on complex reasoning tasks. By fine-\ntuning on task-specific datasets, SFT emphasizes the development\nof logical reasoning, problem-solving skills, and domain-specific\nknowledge. In recent years, numerous studies on SFT for large\nmodels have emerged, including approaches such as zeroth-order\nfine-tuning [7] and robust fine-tuning [9]. Notably, SFT has demon-\nstrated significant advantages in reasoning-related fields, partic-\nularly in mathematics [1, 2] and code generation [10], achieving\npromising results.",
            "content": "Prompt Engineering. Thoughtful prompt design can enhance the reasoning abilities of large models, helping them tackle complex challenges. Chain-of-thought prompting is strategy that guides large language models (LLMs) to produce intermediate reasoning steps, ultimately leading to the final answer and improving problem-solving accuracy. Typical implementations include zeroshot CoT [6] and few-shot CoT [12]. Recent studies [11, 13, 16, 18] have further advanced this method by integrating more structured algorithms and search strategies. For example, Zheng et al. [18] enables LLMs to abstract high-level concepts and first principles from detailed instances, while Yasunaga et al. [16] prompts models to generate relevant examples or contextual knowledge before solving the problem. Additionally, some research [3, 19] is also exploring the use of different types of reasoning chains tailored to various task categories. Our approach, MoR, differs from these methods in that it not only produces diverse array of reasoning strategies but also employs supervised fine-tuning (SFT) to train foundational model capable of multi-chain reasoning."
        },
        {
            "title": "3.1 Thought Generation\nFor small parameter models, due to insufficient embedded knowl-\nedge and limited reasoning capabilities, simply instructing them",
            "content": "with \"Lets think step by step\" does not effectively stimulate the models capabilities. To address this issue, we first need to provide the model with effective thinking approaches for different types of problems. Existing methods [12, 16, 18] mainly focus on generating specific thinking approaches for one type of problem. We decided to leverage the reasoning ability of closed-source large models. Initially, we prompted GPT to generate large number of reasoning chain templates for reasoning tasks. In this section, we pre-generated 50, 150, 300, and 500 reasoning chains, denoted as ğ‘‡ = ğ‘¡1, ğ‘¡2, ..., ğ‘¡ğ‘€ ."
        },
        {
            "title": "3.2 SFT Dataset Construction\nAfter generating the reasoning chains in Â§3.1, we need to construct\nan MoR dataset that can be used for training. In this section, we se-\nlect several commonly used reasoning datasets, such as HotpotQA,\nStrategyQA, MMLU, BigTom, and Trivial Creative Writing (more\ndetails will be discussed in Â§4.1).",
            "content": "First, we randomly select Specified quantity samples from each dataset as training samples. Then, for the selected dataset ğ·source = {ğ‘ 1, ğ‘ 2, . . . , ğ‘ ğ¾ }, where K=N, we randomly select 5 reasoning chain templates ğ‘‡sub from the reasoning chain template set ğ‘‡ = {ğ‘¡1, ğ‘¡2, . . . , ğ‘¡ğ‘€ }, forming subset of reasoning chains. The selected samples ğ·selected along with the selected subset are then fed into GPT, which selects the reasoning chain ğ‘‡best it deems most beneficial for solving the problem based on the problem structure of the samples. Next, we create prompt by combining the selected reasoning chain template ğ‘‡best with each sample ğ‘ ğ‘– , and feed it to the model for reasoning. After evaluation, we filter out the correct answers, and the resulting set is combined into an SFT dataset ğ·SFT. Algorithm 1 SFT Dataset Construction 1: ğ·SFT 2: for ğ‘– 1 to ğ‘ do 3: ğ‘ ğ‘– ğ·selected [ğ‘–] // Get the ğ‘–-th sample ğ‘‡sub RandomSelect(ğ‘‡ , ğ‘ ) // Select ğ‘ templates ğ‘ƒğ‘Ÿğ‘œğ‘šğ‘ğ‘¡select FormatSelectPrompt(ğ‘ ğ‘–,ğ‘‡sub) ğ‘¡best LLM.infer(ğ‘ƒğ‘Ÿğ‘œğ‘šğ‘ğ‘¡select) ğ‘ƒğ‘Ÿğ‘œğ‘šğ‘ğ‘¡reason FormatReasonPrompt(ğ‘ ğ‘–, ğ‘¡best) ğ‘…ğ‘– model.infer(ğ‘ƒğ‘Ÿğ‘œğ‘šğ‘ğ‘¡reason) ğ¼ğ‘ ğ¶ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ Eval(ğ‘ ğ‘–, ğ‘…ğ‘– ) //Evaluate if ğ‘…ğ‘– is correct for ğ‘ ğ‘– if ğ¼ğ‘ ğ¶ğ‘œğ‘Ÿğ‘Ÿğ‘’ğ‘ğ‘¡ is True then ğ‘†ğ¹ğ‘‡entry FormatForSFT(ğ‘ ğ‘–, ğ‘…ğ‘– ) ğ·SFT ğ·SFT {ğ‘†ğ¹ğ‘‡entry} 4: 5: 6: 7: 8: 9: 10: 11: 12: end if 13: 14: end for 15: return ğ·SFT //Return the constructed SFT dataset"
        },
        {
            "title": "4 EXPERIMENT\n4.1 Setup\nDatasets.",
            "content": "In the experiment, we selected five reasoning datasets, with 50 samples randomly chosen from each dataset for testing. For BigTom, we selected 20 samples across four different \"belief settings,\" totaling Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies ICMR 25, June 30July 3, 2025, Chicago, IL, USA. Figure 1: Overview of our proposed MoR framework. The MoR framework can be divided into two stages: (1) Thought Generation. As shown in step 1, this involves generating large number of reasoning chain templates using GPT. (2) SFT Dataset Construction. As depicted in steps 2, 3, and 4, this includes selecting optimal reasoning chains, creating prompts, and filtering for correct responses. Method Model Prompt Hotpotqa Strategyqa MMLU BigTom Trivial Creative writing overall Qwen2.5-7B ğ‘€ğ‘œğ‘…50 ğ‘€ğ‘œğ‘…150 ğ‘€ğ‘œğ‘…300 ğ‘€ğ‘œğ‘… Qwen2.5-7B (Expend) ğ‘€ğ‘œğ‘…150(Expend) IO CoT IO CoT IO CoT IO CoT IO CoT IO CoT IO CoT 1.00 0.980 0.540 0.640 0.98 0.98 0.980 0.980 0.960 0.960 0.960 0.915 0.990 0. 0.400 0.940 0.900 0.480 0.94 0.920 0.840 0.880 0.920 0.900 0.400 0.885 0.880 0.905 0.540 0.560 0.580 0.580 0.560 0.620 0.480 0.560 0.620 0.500 0.595 0.565 0.610 0.600 0.688 0.750 0.888 0.925 0.875 0.900 0.938 0.863 0.913 0.900 0.731 0.738 0.863 0. 0.368 0.308 0.336 0.300 0.144 0.232 0.208 0.292 0.256 0.276 0.368 0.308 0.144 0.232 0.599 0.708 0.649 0.585 0.700 0.730 0.689 0.715 0.734 0.707 0.611 0.682 0.697 0.723 Table 1: Performance on reasoning tasks. We selected Qwen2.5-7B-instruct as the baseline model. We train the baseline model using our MoR approach by varying the number of reasoning chain templates. Additionally, to evaluate the effectiveness of MoR, we extend the test set from 50 to 200 instances, specifically comparing the baseline model with ğ‘€ğ‘œğ‘…150. The best results for each setting are bolded. 80 samples. The SFT dataset construction used the GPT-4o-202408-06 version of GPT, as mentioned in 3. HotpotQA [14]: HotpotQA is designed for question answering with complex, multi-hop questions and strong supervision for interpretable systems. StrategyQA [4]: StrategyQA requiring inference of reasoning steps for question answering through strategic thinking. MMLU [5]: MMLU is an extensive multitask benchmark composed of multiple-choice questions across wide range of knowledge domains. The benchmark spans 57 subjects across diverse domains. BigTom [13]: BigTom is benchmark for assessing the Theory of Mind (ToM) reasoning abilities of large language models (LLMs). It includes new social reasoning framework with 25 controls and 5,000 model-generated evaluations. ICMR 25, June 30July 3, 2025, Chicago, IL, USA. Tao Xiong, Xavier Hu, Wenyan Fan and Shengyu Zhang Figure 2: Case study comparing the baseline model and ğ‘€ğ‘œğ‘…150 using CoT prompts. The Qwen2.5-7B-instruct model follows the \"Lets think step by step.\" approach but ultimately produces incorrect answers. In contrast, the ğ‘€ğ‘œğ‘…150 model adopts the MoR reasoning method, analyzing problems logically and ultimately arriving at the correct answer. Trivial Creative Writing [11]: This dataset challenges models to generate coherent story while seamlessly incorporating answers to set of trivia questions. Model. We selected the Qwen2.5-7B-Instruct [8] model as the baseline. The models fine-tuned on different numbers of X-chain of thought datasets are used as our comparison models, denoted as ğ‘€ğ‘œğ‘…ğ‘–, ğ‘¤â„ğ‘’ğ‘Ÿğ‘’ğ‘– = 50, 150, 300, 500. We believe that after training, the model has acquired MoR capabilities, so simply using the prompt \"Lets think step by step.\" is sufficient to elicit the models multi-step reasoning ability. We refer to this prompting strategy as the CoT prompt. For comparison, we also provide setting where the model is directly instructed to answer the question without any special prompt which is called the IO prompt."
        },
        {
            "title": "4.3 Analysis\nAnalysis of results.",
            "content": "For simple tasks like HotpotQA, most models perform well, with some achieving perfect scores, indicating that basic models are already effective for direct question-answering. However, for complex tasks like StrategyQA and MMLU, MoR models using Chain-ofThought (CoT) prompts show superior performance, highlighting the importance of structured reasoning chains for complex tasks. The experiments reveal that increasing reasoning templates doesnt always improve performance, especially with limited training data. The ğ‘€ğ‘œğ‘…150 configuration achieved the optimal chain-of-thought stimulation, and as MoRs chain-of-thought and data grow, explicit guidance may be less necessary, with the IO prompt effectively stimulating reasoning in ğ‘€ğ‘œğ‘…500, achieving best result of 0.734. The MoR approach outperforms traditional methods, particularly in multi-step inference and strategy-oriented tasks. While CoT and IO prompts perform similarly, the IO prompt provides slight advantage in some tasks, showcasing task-specific benefits. These results confirm that integrating MoR training with tailored prompts enhances reasoning abilities, advancing AI in complex problemsolving. To verify these results, we expanded the test set for both the baseline model and ğ‘€ğ‘œğ‘…150 to 200 samples. As shown in Table X, the extended ğ‘€ğ‘œğ‘…150 maintains consistent advantage over the baseline. Case study of MoR methods. In Figure 2, we compare the baseline model with ğ‘€ğ‘œğ‘…150 on the BigTom dataset under CoT. This task evaluates LLMs ability to reason about others mental states and false beliefs. The baseline model fails to consider the protagonists changing beliefs, leading to incomplete reasoning and incorrect answers. In contrast, the MoR model selects effective strategies, applying logical thinking to solve the problem correctly. This example demonstrates MoRs strength in theory of mind reasoning, providing superior understanding of complex mental states compared to traditional methods."
        },
        {
            "title": "5 CONCLUSION\nThe Mixture of Reasoning (MoR) framework represents a signifi-\ncant advancement in enhancing the reasoning capabilities of large\nlanguage models by embedding diverse reasoning strategies di-\nrectly into their parameters. By eliminating the dependency on\nmanually crafted, task-specific prompts, MoR enables LLMs to\nautonomously select and apply effective reasoning methods tai-\nlored to a wide range of complex tasks. Through our two-phase\napproachâ€”Thought Generation and SFT Dataset Constructionâ€”we\nhave demonstrated that MoR not only improves performance over\nbaseline models but also achieves robust generalizability, as evi-\ndenced by ğ‘€ğ‘œğ‘…150â€™s superior results of 0.730 with CoT prompting",
            "content": "Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies ICMR 25, June 30July 3, 2025, Chicago, IL, USA. [18] Huaixiu Steven Zheng, Swaroop Mishra, Xinyun Chen, Heng-Tze Cheng, Ed H. Chi, Quoc Le, and Denny Zhou. 2024. Take Step Back: Evoking Reasoning via Abstraction in Large Language Models. arXiv:2310.06117 [cs.LG] https: //arxiv.org/abs/2310.06117 [19] Pei Zhou, Jay Pujara, Xiang Ren, Xinyun Chen, Heng-Tze Cheng, Quoc V. Le, Ed H. Chi, Denny Zhou, Swaroop Mishra, and Huaixiu Steven Zheng. 2024. Self-Discover: Large Language Models Self-Compose Reasoning Structures. arXiv:2402.03620 [cs.AI] https://arxiv.org/abs/2402.03620 [20] Shoutai Zhu, Ziqiang Yuan, Kaiyuan Wang, Yishu Zhang, and Wenqi Wei. 2024. Enhancing Financial Reasoning in Large Language Models: The Role of Gold Facts. In 2024 IEEE International Conference on Big Data (BigData). 19191928. doi:10.1109/BigData62323.2024.10825021 and 0.734. These findings underscore MoRs potential to redefine how LLMs approach reasoning, offering scalable and adaptable solution that reduces the burden of prompt engineering. Future work will explore expanding the diversity of reasoning templates and integrating MoR with other advanced training paradigms to further enhance its effectiveness across even more challenging domains. REFERENCES [1] Zhaorun Chen, Zhuokai Zhao, Zhihong Zhu, Ruiqi Zhang, Xiang Li, Bhiksha Raj, and Huaxiu Yao. 2024. AutoPRM: Automating Procedural Supervision for Multi-Step Reasoning via Controllable Question Decomposition. arXiv:2402.11452 [cs.CL] https://arxiv.org/abs/2402.11452 [2] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, Christopher Hesse, and John Schulman. 2021. Training Verifiers to Solve Math Word Problems. arXiv:2110.14168 [cs.LG] https://arxiv.org/abs/2110.14168 [3] Peizhong Gao, Ao Xie, Shaoguang Mao, Wenshan Wu, Yan Xia, Haipeng Mi, and Furu Wei. 2024. Meta Reasoning for Large Language Models. arXiv:2406.11698 [cs.CL] https://arxiv.org/abs/2406.11698 [4] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. 2021. Did Aristotle Use Laptop? Question Answering Benchmark with Implicit Reasoning Strategies. arXiv:2101.02235 [cs.CL] https://arxiv.org/ abs/2101.02235 [5] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2021. Measuring Massive Multitask Language Understanding. arXiv:2009.03300 [cs.CY] https://arxiv.org/abs/2009.03300 [6] Takeshi Kojima, Shixiang Shane Gu, Machel Reid, Yutaka Matsuo, and Yusuke Iwasawa. 2023. Large Language Models are Zero-Shot Reasoners. arXiv:2205.11916 [cs.CL] https://arxiv.org/abs/2205.11916 [7] Sadhika Malladi, Tianyu Gao, Eshaan Nichani, Alex Damian, Jason D. Lee, Danqi Chen, and Sanjeev Arora. 2024. Fine-Tuning Language Models with Just Forward Passes. arXiv:2305.17333 [cs.LG] https://arxiv.org/abs/2305. [8] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. 2025. Qwen2.5 Technical Report. arXiv:2412.15115 [cs.CL] https://arxiv.org/abs/2412.15115 [9] Junjiao Tian, Yen-Cheng Liu, James Seale Smith, and Zsolt Kira. 2023. Fast Trainable Projection for Robust Fine-Tuning. arXiv:2310.19182 [cs.CV] https: //arxiv.org/abs/2310.19182 [10] Yejie Wang, Keqing He, Guanting Dong, Pei Wang, Weihao Zeng, Muxi Diao, Yutao Mou, Mengdi Zhang, Jingang Wang, Xunliang Cai, and Weiran Xu. 2024. DolphCoder: Echo-Locating Code Large Language Models with Diverse and Multi-Objective Instruction Tuning. arXiv:2402.09136 [cs.CL] https://arxiv.org/ abs/2402.09136 [11] Zhenhailong Wang, Shaoguang Mao, Wenshan Wu, Tao Ge, Furu Wei, and Heng Ji. 2024. Unleashing the Emergent Cognitive Synergy in Large Language Models: Task-Solving Agent through Multi-Persona Self-Collaboration. arXiv:2307.05300 [cs.AI] https://arxiv.org/abs/2307.05300 [12] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed Chi, Quoc Le, and Denny Zhou. 2023. Chain-of-Thought Prompting Elicits Reasoning in Large Language Models. arXiv:2201.11903 [cs.CL] https: //arxiv.org/abs/2201.11903 [13] Alex Wilf, Sihyun Shawn Lee, Paul Pu Liang, and Louis-Philippe Morency. 2023. Think Twice: Perspective-Taking Improves Large Language Models Theory-ofMind Capabilities. arXiv:2311.10227 [cs.AI] https://arxiv.org/abs/2311. [14] Zhilin Yang, Peng Qi, Saizheng Zhang, Yoshua Bengio, William W. Cohen, Ruslan Salakhutdinov, and Christopher D. Manning. 2018. HotpotQA: Dataset for Diverse, Explainable Multi-hop Question Answering. arXiv:1809.09600 [cs.CL] https://arxiv.org/abs/1809.09600 [15] Shunyu Yao, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. Tree of Thoughts: Deliberate Problem Solving with Large Language Models. arXiv:2305.10601 [cs.CL] https://arxiv.org/abs/ 2305.10601 [16] Michihiro Yasunaga, Xinyun Chen, Yujia Li, Panupong Pasupat, Jure Leskovec, Percy Liang, Ed H. Chi, and Denny Zhou. 2024. Large Language Models as Analogical Reasoners. arXiv:2310.01714 [cs.LG] https://arxiv.org/abs/2310.01714 [17] Shengyu Zhang, Linfeng Dong, Xiaoya Li, Sen Zhang, Xiaofei Sun, Shuhe Wang, Jiwei Li, Runyi Hu, Tianwei Zhang, Fei Wu, and Guoyin Wang. 2024. Instruction Tuning for Large Language Models: Survey. arXiv:2308.10792 [cs.CL] https: //arxiv.org/abs/2308."
        }
    ],
    "affiliations": [
        "Dalian University of Technology, Dalian, LiaoNing, China",
        "Independent, Hangzhou, ZheJiang, China",
        "Zhejiang University, Hangzhou, ZheJiang, China"
    ]
}