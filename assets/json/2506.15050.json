{
    "paper_title": "Truncated Proximal Policy Optimization",
    "authors": [
        "Tiantian Fan",
        "Lingjun Liu",
        "Yu Yue",
        "Jiaze Chen",
        "Chengyi Wang",
        "Qiying Yu",
        "Chi Zhang",
        "Zhiqi Lin",
        "Ruofei Zhu",
        "Yufeng Yuan",
        "Xiaochen Zuo",
        "Bole Ma",
        "Mofan Zhang",
        "Gaohong Liu",
        "Ru Zhang",
        "Haotian Zhou",
        "Cong Xie",
        "Ruidong Zhu",
        "Zhi Zhang",
        "Xin Liu",
        "Mingxuan Wang",
        "Lin Yan",
        "Yonghui Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As a crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), a novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds. First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise a computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance. We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with a 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5x and outperforms its existing competitors."
        },
        {
            "title": "Start",
            "content": "1ByteDance Seed Full author list in Contributions"
        },
        {
            "title": "Abstract",
            "content": "Recently, test-time scaling Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities across scientific and professional tasks by generating long chains-of-thought (CoT). As crucial component for developing these reasoning models, reinforcement learning (RL), exemplified by Proximal Policy Optimization (PPO) and its variants, allows models to learn through trial and error. However, PPO can be time-consuming due to its inherent on-policy nature, which is further exacerbated by increasing response lengths. In this work, we propose Truncated Proximal Policy Optimization (T-PPO), novel extension to PPO that improves training efficiency by streamlining policy update and length-restricted response generation. T-PPO mitigates the issue of low hardware utilization, an inherent drawback of fully synchronized long-generation procedures, where resources often sit idle during the waiting periods for complete rollouts. Our contributions are two-folds. First, we propose Extended Generalized Advantage Estimation (EGAE) for advantage estimation derived from incomplete responses while maintaining the integrity of policy learning. Second, we devise computationally optimized mechanism that allows for the independent optimization of the policy and value models. By selectively filtering prompt and truncated tokens, this mechanism reduces redundant computations and accelerates the training process without sacrificing convergence performance. We demonstrate the effectiveness and efficacy of T-PPO on AIME 2024 with 32B base model. The experimental results show that T-PPO improves the training efficiency of reasoning LLMs by up to 2.5 and outperforms its existing competitors. Date: June 9, 2025 Correspondence: Tiantian Fan at fantiantian.tt@bytedance.com 5 2 0 2 8 1 ] . [ 1 0 5 0 5 1 . 6 0 5 2 : r Figure 1 AIME 2024 scores of T-PPO on the Qwen2.5-32B base model, reduces training time by 60% compared to the previous state-of-the-art (SOTA) method. The values shown are pass@1 scores, averaged over 32 samples per question."
        },
        {
            "title": "Introduction",
            "content": "Recent advances in reasoning-oriented Large Language Models (LLMs), such as OpenAIs o1 [1], DeepSeekR1 [2], and QwQ [3], have demonstrated the state-of-the-art performance across complex domains including mathematical reasoning, programming, and agent-based tasks. These models leverage extended chainof-thought (CoT) reasoning to improve inference quality, integrating backtracking and error-correction mechanisms that produce more structured and accurate outputs. This enhanced reasoning capability stems primarily from deep reinforcement learning (RL) techniques, through which LLMs learn to generate explicit, logically-sequenced reasoning steps prior to final answer production. As the predominant RL approach for LLM refinement, Proximal Policy Optimization (PPO) [4] maintains training stability through its clipped surrogate objective function. Despite its advantages, PPOs on-policy nature inherently restricts training efficiency, limitation that becomes especially apparent when processing long CoT trajectories, often leading to substantial computational overhead and extended training durations. To address this issue, researchers have developed various off-policy PPO variants that are designed to enhance sample efficiency via trajectory reuse. Specifically, Generalized Proximal Policy Optimization (GePPO) [5] extends the guarantees of policy improvement to the off-policy setting. Off-Policy PPO [6] designs clipped surrogate objective function that can utilize off-policy data and avoid excessively large policy updates. PPO-EWMA [7] employs decoupled policy objectives and an exponentially weighted moving average (EWMA) for policy updates. KIMI K1.5 [8] uses partial rollouts to improve training efficiency by reusing large chunk of previous trajectories when sampling, thus avoiding the cost of regenerating new trajectories from scratch. Although off-policy methods are more training-efficient, they typically suffer from high variance in the policy gradient estimator, resulting in unstable training and degraded performance. In this work, we present Truncated Proximal Policy Optimization (T-PPO), an enhanced on-policy reinforcement learning framework that significantly improves efficiency while maintaining or even enhancing reasoning performance. At the core of T-PPO is our Extended Generalized Advantage Estimation (EGAE) method, which enables progressive policy updates even before trajectory is fully generated. Specifically, EGAE generalizes the conventional Generalized Advantage Estimation (GAE) [9] to support policy optimization with partially generated responses. This decouples policy updates from response completion and significantly improves computational resource utilization. Furthermore, to ensure unbiased value estimation, we maintain the Monte Carlo training paradigm for value model updates, deferring these updates until full response generation is complete. This estimation relies exclusively on actual observed returns rather than estimated values, thereby eliminating approximation bias in the value function. This dual optimization strategy allows simultaneous, yet independent improvement of both policy and value models through selective token screening. In summary, our design yields three key advantages: (1) truncated rollout strategy that enhances GPU throughput, (2) complete elimination of persistent bias in value function estimation, and (3) substantially improved policy update efficiency achieved through enhanced data utilization. Our extensive experiments on the AIME 2024 benchmark demonstrate that T-PPO delivers significant efficiency gains without compromising model performance. Specifically, the algorithm exhibits robust convergence behavior through its sample-efficient learning mechanism, which consistently enhances policy optimization. These combined attributes enable T-PPO to achieve 62 pass@1 on the AIME24 benchmark while demonstrating 2.5 higher training efficiency compared to state-of-the-art synchronization algorithms. Such performance characteristics significantly expand the practical deployment potential of T-PPO in real-world applications. Remarkably, these improvements are attained without introducing additional constraints or regularization beyond standard PPO. Apart from reducing the training cost, we sincerely hope that this method can bring more inspiration for delving into specialized expert models for professional domains."
        },
        {
            "title": "2.1 Reinforcement learning framework",
            "content": "Reinforcement Learning (RL) is framework for sequential decision making. Typically, sentence generation can be formulated as Markov decision process (MDP) represented by the tuple = {S, A, p, r, d0, Î³}. Here is the state space, at each generation step t, the state st = {x, y1:t1} is the concatenation of the input question and the output response generated so far y1:t1. represents the action space and p(ss, a) : [0, 1] is the transition probability distribution. In sentence generation, the transition probability is not needed because, given the current state and action, next state is deterministic. : is the scalar reward function with r(st, at) for every intermediate time step t. Generally, the reward function r(st, at) is defined for every state-action pair to provide feedback throughout the trajectory. In this work, we focus on the challenging case where rewards are non-zero only at the terminal timestep 1, reflecting the correctness of the whole reasoning chain. Unlike dense-reward settings, this sparse-reward scenario naturally reduces to bandit problem formulation. Note that our approach can be easily applied to simpler scenarios with process rewards. d0 is the initial state distribution and Î³ [0, 1] is discount factor. The actions are taken from probability distribution called policy Ï given the current state at Ï(st). The goal is to where choose policy that maximizes the expected total discounted rewards J(Ï) = EÏ Ï Ï = (s0, a0, ..., st, at, ..., sT 1, aT 1) is the trajectory generated by the LLMs interaction with environment with being the length of the trajectory. Under given policy Ï, the state-value function is defined as Ï(st) = EÏ Ï[Gtst] where Gt = (cid:80)T 1t Î³ir(st+i, at+i) is the discount return. Similarly, the state-action value function, i.e., Q-function, is defined as QÏ(st, at) = EÏ Ï[Gtst, at], and the critical advantage function as AÏ(st, at) = QÏ(st, at) Ï(st). (cid:105) i=0 Î³ir(si, ai) (cid:104) (cid:80)T 1 i="
        },
        {
            "title": "2.2 Proximal Policy Optimization",
            "content": "PPO is popular actor-critic reinforcement learning algorithm that has become default baseline in LLMs. It optimizes LLMs by maximizing the clipped surrogate objective function JPPO(Î¸) = Et,st,atÏÎ¸old (cid:104) min (cid:16) ÏÎ¸(atst) ÏÎ¸old(atst) ËAt, clip(cid:0) ÏÎ¸(atst) ÏÎ¸old (atst) , 1 Ïµlow, 1 + Ïµhigh (cid:17)(cid:105) (cid:1) ËAt (1) , where ÏÎ¸ and Ïold represent the current and previous policy respectively. ËAt is an estimator of the advantage function, and Ïµlow and Ïµhigh are hyperparameters that control the maximum deviation from the previous policy . ËAt is computed using generalized advantage estimation (GAE) [9] based on rewards and learned value ÏÎ¸old function. The clipping objective of PPO restricts how drastically the updated policy distribution can diverge from the original policy. This moderation averts catastrophic shifts in language generation and preserves training stability. PPO limits the difference between consecutive policies by eliminating the incentive for the probability ratio ÏÎ¸(atst) to leave the clipping range [1 Ïµlow, 1 + Ïµhigh], thus resulting in stable policy ÏÎ¸old (atst) improvement throughout the learning process. However, it is well-known that high variance is major issue in reinforcement learning, so often the number of samples must be large in order for the surrogate objective to be an accurate estimator of the true objective. Because these samples must be collected under the current policy between every policy update, PPO can be very inefficient."
        },
        {
            "title": "2.3 Generalized Advantage Estimation\nGAE provides a trade-off between bias and variance in the advantage estimation by combining multiple n-step\nadvantage estimates through an exponentially weighted average controlled by the parameter Î». The advantage\nis computed as",
            "content": "ËAt = Î´t + (Î³Î»)Î´t+1 + ... + (Î³Î»)T t1Î´T 1 (2) , where Î´t = rt + Î³V (st+1) (st) is the TD (temporal difference) residual, and Î³ is the discount factor that determines how much future rewards are valued relative to immediate rewards. Î» is the GAE parameter that controls the weighting of different (3) multi-step estimates. GAE provides way to control the bias-variance trade-off by adjusting Î», which allows us to tailor the advantage estimation to the specific problem."
        },
        {
            "title": "2.4 Value Function Estimation",
            "content": "In the PPO algorithm, the critic model, often referred to as the value function, estimates the expected returns for each observed state. This estimation achieves both variance reduction in policy gradients and generation of supplementary learning signals, which are essential to T-PPOs algorithmic design. variety of different methods can be used to estimate the value function. When using network to represent the value function, the simplest approach is to solve nonlinear regression problem VÏ,CLIP(st) = clip (cid:16) VÏ(st), VÏold(st) Î¾low, VÏold(st) + Î¾high (cid:17) Jvalue(Ï) = 1 2 Et,st,atÏÎ¸old (cid:104) max (cid:0)(VÏ(st) Rt)2, (VÏ,CLIP(st) Rt)2(cid:1)(cid:105) (4) (5) are the current and previous value functions , respectively. Rt = (cid:80)T 1t , where VÏ and VÏold Î³irt+i denotes the discounted return, and indexes over all timesteps in batch of trajectories. The clipping operation enforces constraint on the value function updates through hyperparameters Î¾low and Î¾high, ensuring that the updated value function VÏ does not deviate significantly from VÏold . This is sometimes called the Monte Carlo or TD(1) [10] approach for estimating the value function. For reasoning tasks, we employ Monte Carlo estimation to maintain strictly unbiased state-value predictions, deliberately avoiding the use of GAE despite its variance-reduction benefits, as GAE introduces approximation bias through its temporal difference components. i="
        },
        {
            "title": "3 T-PPO: Truncated Proximal Policy Optimization",
            "content": "To address the training inefficiency inherent in PPO, we propose Truncated Proximal Policy Optimization (T-PPO), novel approach that enables policy optimization using incomplete trajectories. This section presents the technical framework of T-PPO through three key components: First, we introduce Extended Generalized Advantage Estimation (EGAE), generalization of conventional GAE that accommodates partially generated responses. This innovation allows for: advantage computation from unfinished trajectories progressive policy updates during response generation Second, we detail our token-level optimization strategy, which features: selective filtering of training tokens independent yet simultaneous updates for policy and value models Finally, we present: the complete T-PPO algorithm implementation comprehensive analysis of training efficiency gains"
        },
        {
            "title": "3.1 EGAE: Extended Generalized Advantage Estimation\nThis section focuses on producing an estimate ËAt of the advantage function AÏ(st, at), when the entire\ntrajectory is truncated. As discussed in the previous section, the GAE advantage estimator has a simple\nformula involving a discounted sum of Bellman residual terms. By introducing a parameter Î» that controls\nthe influence of future reward on the advantage estimate, that is \"forgotten\" after â 1/(1 â Î»Î³) timesteps,\nGAE provides a flexible bias-variance trade-off. As each TD residual term becomes more heavily discounted\nthrough a bootstrapping procedure, truncation of the trajectory has a negligible effect on the actions in\nthe front position. In addition, our heuristic framework assumes that the generation of a single token does",
            "content": "4 not significantly alter the state-value. That means for truncated trajectory Ï = (s0, a0, ..., sl1, al1) with truncation length l, we reasonably assume (sl) = (sl1) for the next state that has not been generated yet. Under this assumption, the advantage estimate has the same format as in the non-truncated case (Equation (2)) by simply replacing by l. ËAt = Î´t + (Î³Î»)Î´t+1 + ... + (Î³Î»)lt1Î´l1 (6)"
        },
        {
            "title": "3.2 Token Filtering Strategy",
            "content": "PPO batches group of prompts and waits all the prompts to complete their generation. In reasoning model training, the response length of each request varies greatly, resulting in GPU underutilization. Truncated Proximal Policy Optimization truncates the generation by given maximum length, which we refer to as the window length in the following discussion to distinguish it from the actual maximum response length which may be generated by multiple rounds. This truncated rollout strategy effectively addresses the challenges associated with batching in LLMs training. If some sequences reach an ending condition, we remove these sequences in the next training step, while incomplete samples will be retained, and the total batch size of each training step is constant. Figure 2 gives an example of this batching strategy of two consecutive steps. Figure 2 Successive batching strategy of T-PPO. Gray: prefill (computing input tokens), blue and red: decode (generating new response tokens). In step 0, sequences S2 and S3 emit an end-of-sequence token (red), so in step 1 we insert new prompts in their place (i.e. sequences S5 and S6), while the unfinished sequences continues in the next iteration. Each sequence finishes at different iterations. As shown in Figure 3, taking training step 1 as an example, since the value model is trained in Monte Carlo mode, all generated tokens of finished sequences are used to train the value model. The policy model, in contrast, is trained on response tokens generated in the current training step, regardless of whether the sequence is complete or not. Additionally, since the advantage estimation of the latest generated tokens of unfinished sequences may be of high variance, we can exclude some latest generated tokens from policy model training. Figure 3 Plots showing one step (i.e. step 1) of the training tokens for policy model (top side) and value model (bottom side)."
        },
        {
            "title": "3.3 Implementation and Training Efficiency Analysis",
            "content": "T-PPO, which we detail in Algorithm 1, represents principled approach to improving the training efficiency of PPO while retaining its approximate policy improvement guarantees. Collect prompts from last step unfinished samples and replace finished samples with new prompts Algorithm 1 T-PPO: Truncated Proximal Policy Optimization Input initial policy network parameters Î¸0, value function parameters Ï0, task prompts D, window length 1: for step = 0,1,2... do 2: from 3: 4: 5: 6: 7: 8: 9: 10: 11: end for Output ÏÎ¸ Update the old policy model ÏÎ¸old ÏÎ¸ Collect trajectories {Ïi}i[0,K] by running policy ÏÎ¸old Compute rewards ËRt for each finished trajectory Calculate advantage estimation ËAi,t by EGAE (Equation (6)) for each (t-th) policy tokens for minibatch = 0,1,2,... do Update the policy model ÏÎ¸ by maximizing the T-PPO objective (Equation (1)) Update the value function via gradient descent Jvalue(Ï) (Equation (5)) on value tokens end for in the environment until time steps As primary bottleneck of RL training lies in sample generation stage, due to the barrel effect, the walltime of sample generation phase is approximately proportional to maximum response length. When we truncate the response process, suppose that L/l = where is the actual maximum response length and is the window length, generation time saving is approximately times. In the training stage, from the perspective of each cross-turn response token, it is trained once each by the policy model and the value function, so the training time is also saved about times. Therefore, the end-to-end training efficiency ratio is about times for the same number of training steps."
        },
        {
            "title": "4 Experiments",
            "content": "In addition to the theoretical support for our algorithm presented in the previous section, we aimed to investigate the stability and training efficiency of T-PPO experimentally. In this section, we present detailed experimental results for T-PPO. We begin with an overview of the training configuration and datasets. Subsequently, we provide evaluation results and comparison of training efficiency with several baseline methods. Finally, we investigate training dynamics, with particular focus on critical metrics such as response length evolution."
        },
        {
            "title": "4.1 Experimental Setup",
            "content": "4.1.1 Models and Configurations In our experiments, we used Qwen-2.5-Base-32B as the initial checkpoint . The policy is trained with constant learning rate of 1e-6, using the AdamW optimizer (Î² = [0.9, 0.95]) with weight decay 0.1, while the critics learning rate was set as 2e-6. For fair comparisons, we applied hyperparameters similar to those of VAPO: batch size of 512 prompts, sampling 16 times per prompt, and setting the minibatch size to 512. The value network was initialized using reward model, with the GAE Î» set to 0.95 and Î³ set to 1.0. Token-level loss was used, and we set the clipping parameters Ïµlow = 0.2 and Ïµhigh = 0.28 for the policy and Î¾low = 0.5, Î¾high = 0.6 for the value function. For evaluation on AIME, we repeat the evaluation set 32 times and report avg@32 for the stability of the results. The inference hyperparameters of evaluation were set to temperature 1.0 and topp 0.7. In addition, given the significant distribution shift between the reasoning model 6 Table 1 Results of different algorithms on AIME Model DeepSeek-R1-Zero-Qwen-32B DAPO [11] VAPO [12] AIME24avg@32 47 50 GePPO [5] PPO-EWMA [7] T-PPO 50 52 62 and the base model, we removed the KL divergence from the loss function to encourage exploration. We set the maximum response length to 24k while window length to 8k in T-PPO."
        },
        {
            "title": "4.1.2 Dataset Description",
            "content": "To fully demonstrate the effectiveness and efficiency of our proposed algorithm, we conducted experiments using the American Invitational Mathematics Examination (AIME) as representative benchmark for reasoning problems. The AIME often requires long chain of thought to solve. The test set comprises AIME problems from the last year. The training set, DAPO-Math-17K [11], consists of questions from all past AIME competitions, supplemented by some artificially constructed difficult math problems. We implemented the verification-based reward function using Math-Verify, with the following minimalistic rule: (cid:40) R(x, y) = 1 if contains the correct final answer to 0 otherwise (7) We run different RL algorithms on questions sampled from the training dataset and compare the vanilla PPO, representative asynchronous PPO-EWMA with the proposed T-PPO."
        },
        {
            "title": "4.2 Experimental Results",
            "content": "4.2.1 Main Results The main results comparing T-PPO with baseline methods across the AIME dataset are presented in Figure 1 and Table 1. Our approach ultimately achieves 61.88 pass@1 on AIME 24, surpassing the performance of DeepSeek-R1-Zero-Qwen-32B and SOTA async PPO algorithm available, matching PPO with 20k response length with 60% wall-clock time reduction on the AIME24 benchmark. T-PPO exhibits high training stability and better training efficiency, making it the preferred choice in this setting. 4.2.2 Training Efficiency To further understand the efficiency improvement of T-PPO, we show its RL iteration breakdown and compare it with other algorithms. We divide one RL iteration into three main parts: the generation stage (gen), the policy training stage (train actor) and the value training stage (train critic). Figure 4 compares the time efficiency of different algorithms. The average wall-clock time consumption per 1000 steps of T-PPO is comparable to that of PPO-EWMA and much lower than that of vanilla PPO algorithm. In addition, although T-PPO and PPO-EWMA have similar per-step wall-clock time, T-PPO has significantly fewer convergence steps than PPO-EWMA (i.e., 6720 vs. 11200 steps each). Thus, both vanilla PPO and asynchronous PPO require much more total run time to converge, which may impede its applications to solving real-world problems. Beyond temporal efficiency comparisons, our Roofline analysis (Figure 5) - as reflected in the computational intensity profiles - provides more profound architectural insights into the systems performance characteristics. T-PPO demonstrates computational intensity of 249 operations/byte in policy rollout, significantly higher Figure 4 Algorithm comparison in terms of time efficiency on the AIME benchmark. Each boxplot is drawn based on the execution mean of 1000 steps. Figure 5 Demonstration of the Roofline model of Nvidia H800 GPU. The computation is in BF16. Figure 6 The metric curves of response length. than PPOs 84 operations/byte, positioning it closer to the arithmetic peak on the Roofline curve. This indicates that T-PPO better utilizes compute resources by reducing memory-bound bottlenecks through optimized GPU utilization in the generation stage. 4.2.3 Training Dynamics The length of generated responses serves as an indicator of training stability and model capability, as illustrated in Figure 6. Our analysis reveals characteristic fluctuation pattern in which response length initially increases, undergoes temporary decline, then recovers before eventually stabilizing. This non-monotonic trajectory suggests that the model continuously refines its reasoning methodology throughout the learning process. Importantly, the final stabilized responses length surpasses the vanilla PPO, demonstrating that our method preserves (and potentially enhances) the reasoning models length scaling capacity. The initial length expansion provides greater exploration space for complex reasoning behaviors, consistent with the observed \"emergence\" of lengthy chain-of-thought (CoT) through RL training [13, 14]. The eventual recovery and surpassing of initial lengths indicates the models successful navigation through this transitional phase to achieve superior reasoning capacity."
        },
        {
            "title": "5 Conclusion",
            "content": "In this work, we introduce Truncated Proximal Policy Optimization (T-PPO), novel extension of PPO that incorporates successive batching strategy to enhance the training efficiency. By leveraging an innovative extended generalized advantage estimation in conjunction with computationally efficient mechanisms to 8 optimize the policy and the value models respectively, it achieves up to 2.5 training speedup with competitive final performance. Our detailed experiments and empirical insights provide practical guidance and valuable experience for future research on efficient large-scale reinforcement learning. Further exploration of truncated or more advanced RL methods is recommended."
        },
        {
            "title": "Contributions",
            "content": "Project Lead Tiantian Fan1 Algorithm Tiantian Fan1, Yu Yue1, Qiying Yu1,2, Ruofei Zhu1, Yufeng Yuan1, Xiaochen Zuo1 Infrastructure Lingjun Liu1, Tiantian Fan1, Chi Zhang1, Zhiqi Lin1, Bole Ma1, Mofan Zhang1, Gaohong Liu1, Ru Zhang Dataset Jiaze Chen1, Chengyi Wang1 Additional Contributors Haotian Zhou1, Cong Xie1, Ruidong Zhu1 Supervision Zhi Zhang1, Xin Liu1, Mingxuan Wang1,2, Lin Yan1,2, Yonghui Wu Affiliation 1ByteDance Seed 2SIA-Lab of Tsinghua AIR and ByteDance Seed"
        },
        {
            "title": "References",
            "content": "[1] OpenAI. Learning to reason with llms, 2024. [2] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. [3] Qwen Team. Qwq-32b: Embracing the power of reinforcement learning. URL: https://qwenlm. github. io/blog/qwq-32b, 2025. [4] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. [5] James Queeney, Yannis Paschalidis, and Christos Cassandras. Generalized proximal policy optimization with sample reuse. Advances in Neural Information Processing Systems, 34:1190911919, 2021. [6] Wenjia Meng, Qian Zheng, Gang Pan, and Yilong Yin. Off-policy proximal policy optimization. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pages 91629170, 2023. [7] Jacob Hilton, Karl Cobbe, and John Schulman. Batch size-invariance for policy optimization. Advances in Neural Information Processing Systems, 35:1708617098, 2022. [8] Kimi Team, Angang Du, Bofei Gao, Bowei Xing, Changjiu Jiang, Cheng Chen, Cheng Li, Chenjun Xiao, Chenzhuang Du, Chonghua Liao, et al. Kimi k1. 5: Scaling reinforcement learning with llms. arXiv preprint arXiv:2501.12599, 2025. [9] John Schulman, Philipp Moritz, Sergey Levine, Michael Jordan, and Pieter Abbeel. High-dimensional continuous control using generalized advantage estimation. arXiv preprint arXiv:1506.02438, 2015. [10] Richard Sutton, Andrew Barto, et al. Reinforcement learning: An introduction, volume 1. MIT press Cambridge, 1998. [11] Qiying Yu, Zheng Zhang, Ruofei Zhu, Yufeng Yuan, Xiaochen Zuo, Yu Yue, Tiantian Fan, Gaohong Liu, Lingjun Liu, Xin Liu, et al. Dapo: An open-source llm reinforcement learning system at scale, 2025. URL https://arxiv. org/abs/2503.14476, 2025. [12] Yu Yue, Yufeng Yuan, Qiying Yu, Xiaochen Zuo, Ruofei Zhu, Wenyuan Xu, Jiaze Chen, Chengyi Wang, TianTian Fan, Zhengyin Du, et al. Vapo: Efficient and reliable reinforcement learning for advanced reasoning tasks. arXiv preprint arXiv:2504.05118, 2025. [13] Weihao Zeng, Yuzhen Huang, Wei Liu, Keqing He, Qian Liu, Zejun Ma, and Junxian He. 7b model and 8k examples: Emerging reasoning with reinforcement learning is both effective and efficient. [14] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model. arXiv preprint arXiv:2503.24290, 2025."
        }
    ],
    "affiliations": [
        "ByteDance"
    ]
}