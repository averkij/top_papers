{
    "paper_title": "Distilling semantically aware orders for autoregressive image generation",
    "authors": [
        "Rishav Pramanik",
        "Antoine Poupon",
        "Juan A. Rodriguez",
        "Masih Aminbeidokhti",
        "David Vazquez",
        "Christopher Pal",
        "Zhaozheng Yin",
        "Marco Pedersoli"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autoregressive patch-based image generation has recently shown competitive results in terms of image quality and scalability. It can also be easily integrated and scaled within Vision-Language models. Nevertheless, autoregressive models require a defined order for patch generation. While a natural order based on the dictation of the words makes sense for text generation, there is no inherent generation order that exists for image generation. Traditionally, a raster-scan order (from top-left to bottom-right) guides autoregressive image generation models. In this paper, we argue that this order is suboptimal, as it fails to respect the causality of the image content: for instance, when conditioned on a visual description of a sunset, an autoregressive model may generate clouds before the sun, even though the color of clouds should depend on the color of the sun and not the inverse. In this work, we show that first by training a model to generate patches in any-given-order, we can infer both the content and the location (order) of each patch during generation. Secondly, we use these extracted orders to finetune the any-given-order model to produce better-quality images. Through our experiments, we show on two datasets that this new generation method produces better images than the traditional raster-scan approach, with similar training costs and no extra annotations."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 9 6 0 7 1 . 4 0 5 2 : r a"
        },
        {
            "title": "DISTILLING SEMANTICALLY AWARE ORDERS\nFOR AUTOREGRESSIVE IMAGE GENERATION",
            "content": "Rishav Pramanik 1, Antoine Poupon 2,3, Juan A. Rodriguez 2,4,5,6, Masih Aminbeidokhti 2,6, David Vazquez 4, Christopher Pal 4,5,7,8, Zhaozheng Yin 1, Marco Pedersoli 2,4,5,6 1Stony Brook University, NY, USA 2International Laboratory on Learning Systems (ILLS) 3Universite Paris-Saclay, CentraleSupelec, France 4ServiceNow Research 5Mila-Quebec AI Institute 6 Ecole de technologie superieure, QC, Canada 7Polytechnique Montreal 8Canada CIFAR AI Chair"
        },
        {
            "title": "ABSTRACT",
            "content": "Autoregressive patch-based image generation has recently shown competitive results in terms of image quality and scalability. It can also be easily integrated and scaled within Vision-Language models. Nevertheless, autoregressive models require defined order for patch generation. While natural order based on the dictation of the words makes sense for text generation, there is no inherent generation order that exists for image generation. Traditionally, raster-scan order (from top-left to bottom-right) guides autoregressive image generation models. In this paper, we argue that this order is suboptimal, as it fails to respect the causality of the image content: for instance, when conditioned on visual description of sunset, an autoregressive model may generate clouds before the sun, even though the color of clouds should depend on the color of the sun and not the inverse. In this work, we show that first by training model to generate patches in any-givenorder, we can infer both the content and the location (order) of each patch during generation. Secondly, we use these extracted orders to finetune the any-given-order model to produce better-quality images. Through our experiments, we show on two datasets that this new generation method produces better images than the traditional raster-scan approach, with similar training costs and no extra annotations."
        },
        {
            "title": "INTRODUCTION",
            "content": "The emergence of Large Language models (LLMs) and large-scale vision models Ramesh et al. (2021); Chowdhery et al. (2023) has prompted widespread research and application of generative image models. Decoder-only autoregressive (AR) transformers have been the de-facto solution for state-of-the-art generative language models OpenAI (2020). Images can also be generated with AR transformers by decomposing the image into non-overlapping patches with discrete tokens and learning the correct sequence of discrete tokens to generate images Ramesh et al. (2021); Esser et al. (2021). As bonus, generative AR models for images can leverage the optimized architectures and efficient techniques developed for LLMs recently Anthropic (2024); Dao (2024); Kwon et al. (2023); Hu et al. (2022). This alignment with LLM advancements makes AR models strong choice for image generation from practical implementation perspective. In addition, AR transformers are the base of modern multimodal LLMs OpenAI. (2024); Team & Google. (2024); Team & Group. (2024), which have become very popular in the computer vision community due to their flexibility and scalable capabilities. Thus, using the same model for perceiving, analyzing, and generating data is quite appealing Caron et al. (2024) and is an initial motivation for this work."
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Generation with our distilled order on the Fashion Product dataset (Left) and the Multimodal CelebA-HQ dataset (Right) with the corresponding generation order produced by our Ordered Autoregressive (OAR) model. The generation order is visualized through color intensity, progressing from yellow (early patches) to violet (later patches). Our learned order typically starts with simpler regions of the image before moving to more complex ones. For the Fashion Product dataset, this often means generating the white background first, while in the CelebA-HQ dataset, the model tends to begin with facial regions like the cheeks and chin, which are generally easier to generate. In AR modeling, the joint probability representing the data distribution is decomposed with the chain rule into product of conditional distributions, which are usually simpler to learn Shih et al. (2022); Chen et al. (2018) compared to the joint probability. This equivalence between conditionals and joint distribution is valid for any order of the data sequence. Thus, the simpler order usually is selected, which is left-to-right for text generation. Nevertheless, the equivalence is only valid when the probabilities are given. In learning problem in which the conditional probabilities are to be estimated, different data orders can lead to very different results Vinyals et al. (2016). For text, the commonly used left-to-right order is quite meaningful as it mimics the natural temporal flow of the spoken language, which we assume evolved to be cognitively intuitive for human communication Rosen (1992). In contrast, images have no implicit or universally preferred generation order as they lay in two-dimensional space. Unlike text, which follows sequential narrative, images are composed of spatial relationships that do not require fixed generation order. Therefore, the lack of natural ordering makes learning semantic generation order even more sense for images. This insight forms the core motivation of our work: Can we find an optimal order for autoregressive image generation that, without any additional information, improves image generation quality? We find that the answer is yes, and in Fig.1, we show two examples of generated images and their corresponding yielded orders. Each image has different generation order (shown with color gradient from yellow to violet) that depends on its content rather than fixed generation order. To achieve this, we build on recent advancements in AR modelling Li et al. (2024); Pannatier et al. (2024). First, we train an AR model to learn chain of conditional distributions in arbitrary orders. Next, we distill the knowledge from this model to re-label the orders of the training samples. Finally, the model is finetuned using these refined orders in self-supervised manner. We observe that by doing this we can improve the quality of image generation by an AR model. The contributions of our paper are as follows: We first argue that AR image generation lacks any natural ordering. As result, learning optimal generation order can improve the quality of generated images when generated in an optimal order. We present content-dependent and semantically aware generation of orders for AR patchbased image generation to improve the overall generation quality. To achieve this, we first train model with any given orders, followed by self-supervised finetuning using the semantically aware orders generated by the any-order model. We validate our approach on Fashion dataset Aggarwal (2021) and Multimodal CelebA-HQ dataset Xia et al. (2021), and perform extensive ablation studies to identify the key factors of contribution. We report better results than traditional raster-scan AR models."
        },
        {
            "title": "2.1 LEARNING ORDERS IN AUTOREGRESSIVE MODELS",
            "content": "While decomposing joint probability distribution into sequence of conditional probability distributions is intuitive, determining the optimal order to learn it is not straightforward. Numerous studies highlight the significant effect of sequence order on the performance of sequence-to-sequence models when capturing underlying data distributions Vinyals et al. (2016); Ford et al. (2018); Papadopoulos et al. (2024). This insight has catalyzed the development of Any-Order Autoregressive Models (AO-ARM) Uria et al. (2016); Yang et al. (2019); Hoogeboom et al. (2022), which can generate sequences in any possible order. In pioneering work like NADE Uria et al. (2016), deep feed-forward neural network is trained to compute conditional distributions for any variable given any subset of others. During inference, NADE forms an ensemble by sampling multiple orderings, calculating likelihoods for each, and averaging them to introduce an orderless inductive bias. Subsequent advancements Shih et al. (2022) refine the ordering space used in training to minimize redundancy in probabilistic models, training on carefully selected subset of univariate conditionals that still enable efficient arbitrary conditional inference. In Natural Language Processing (NLP), AO-ARM models have been adapted to learn from bidirectional contexts. For instance, XLNet Yang et al. (2019) maximizes the expected log-likelihood over all possible factorization orders using different attention mechanism within the Transformer architecture. Similarly, σ-GPT Pannatier et al. (2024) dynamically modulates text generation order on per-sample basis. Alternative methods seek to determine optimal autoregressive orderings through techniques like bidirectional decoding Sun et al. (2017); Mehri & Sigal (2018) or syntax tree-based decoding Yamada & Knight (2001); Wang et al. (2018). These approaches, however, rely on heuristics rather than learning optimal orderings directly.Welleck et al. (2019) proposes tree-based recursive generation method to learn flexible generation orders. However, this method still has limited performance compared to the traditional left-to-right generation approaches. InDIGO Gu et al. (2019) extends the Transformer architecture to support flexible sequence generation in arbitrary orders by incorporating an insertion operation. This approach allows training with predefined generation or adaptive orders derived through beam search. In image captioning, Variational Order Inference Li et al. (2021) further adapts generation order based on image content by using variational inference to learn an approximate posterior over autoregressive orders. While effective, these techniques are primarily specialized for 1D sequence data in NLP and encounter challenges when applied to more complex 2D structures. Unlike previous approaches, our method first pre-trains an any-order autoregressive model in an order-agnostic manner. Then, it is improved through fine-tuning based on optimal generation orders extracted from the previous stage. 2.2 AUTOREGRESSIVE IMAGE GENERATION Extending the capabilities beyond language modeling, AR models have shown competitive performance compared to diffusion models Austin et al. (2021), reigniting interest in AR approaches within the vision community Yu et al. (2024a); Sun et al. (2024). Early generative AR image models Van Den Oord et al. (2016); Van den Oord et al. (2016); Chen et al. (2018); Parmar et al. (2018); Chen et al. (2020) processed sequences of individual pixels. Autoregressive processing has been achieved with RNNs Van Den Oord et al. (2016), CNNs Van den Oord et al. (2016), and later, with Transformers Parmar et al. (2018); Chen et al. (2020). Moving beyond pixel-level tokens, other studies introduced patch-wise tokenization, constructing codebooks of visual token embeddings, leading to significant performance improvements Van Den Oord et al. (2017); Esser et al. (2021); Razavi et al. (2019); Esser et al. (2021); Ramesh et al. (2021). More recently, embedding-free quantization methods have demonstrated effectiveness due to their simplicity and scalability, allowing the vocabulary size to grow without sacrificing model performance Yu et al. (2024a); Mentzer et al. (2024); Weber et al. (2024). Despite significant advancements, most methods still follow strict raster-scan order to process pixels or tokens, resulting in constrained, unidirectional flow of information that limits visual"
        },
        {
            "title": "Preprint",
            "content": "modeling. To overcome this limitation, methods like MaskGIT Chang et al. (2022) and MAGE Li et al. (2023) employ BERT-like Masked Language Model (MLM) Devlin et al. (2019) paired with custom decoding scheme that samples multiple tokens simultaneously to produce an output. Here, the number of tokens generated per pass is controlled by masking schedule, while confidence-based decoding scheme determines the following tokens to predict. The Masked Autoregressive (MAR) framework Li et al. (2024); Fan et al. (2024) generalizes these approaches, demonstrating that masked generative models can effectively operate under the broader framework of next-token prediction. Very recently, Randomized Autoregressive Modeling (RAR) Yu et al. (2024b) proposed to begin training from randomized sequences and gradually reduce the probability of random permutations to zero throughout training. RandAR Pang et al. (2025) adds an instruction token to define the position of the next patch in the image to perform any-order generation for increased flexibility. Current methods often sample various generation orders as regularization technique to improve dependency learning among image regions. However, they do not explicitly learn an optimal order. In our approach, rather than relying on masking strategies, we use double position encoding similar to Pannatier et al. (2024), extract the order, and use it in self-supervised manner, enhancing the training process."
        },
        {
            "title": "3 PERSPECTIVE AND MOTIVATIONS",
            "content": "As shown in previous studies Li et al. (2024), the order of generation has direct effect on the quality of the images. While the experiment section presents comprehensive analysis of the results, this section provides deeper understanding of the direct and indirect effects of orders for an AR model and our formulation is detailed in the following section. 3.1 JOINT AND CONDITIONAL CHAIN EQUIVALENCE Let sequence = [x0, x1, ..., xN ] be an i.i.d sampled datapoint from . In generative modeling, the fundamental objective is to learn the underlying data distribution p(x), which captures the structure and variability of the data. By learning this distribution, the model can generate new, realistic samples that maintain the structure and characteristics of the true data distribution. To facilitate tractable likelihood computation and to ease the training process Chen et al. (2018), we decompose the joint distribution into product of conditional probabilities: p(x0, x1, ..., xN ) = p(x0)p(x1x0), ..., p(xN x0, ..., xN 1) (1) This conditional chain can be learned with an AR model whose parameters θ can be estimated using Maximum Likelihood Estimation (MLE). For AR models, training is conducted by minimizing the next-token prediction objective with cross-entropy loss: L(x) = 1 (cid:88) i= log pθ(xix<i) (2) Suppose the AR model perfectly estimates the joint probability distribution, then the joint probability is strictly equivalent to the chain of conditional probabilities, and the factorization order inconsequential. However, crucial point that has sometimes been overlooked is that here, the joint probability is not given; it is precisely what we need to learn. In this context, the order in which we learn to generate the data has strong correlation with sampling efficiency, i.e., the data needed to reach certain generation quality. Therefore, an essential factor in optimizing given model pθ is the order in which the sequence elements are arranged during the learning process. The choice of ordering can significantly impact the models ability to capture underlying patterns and dependencies effectively. To present it formally, we denote unique permutation order through the variable l, where is permutation sequence belonging to the set of all permutations of the length-N index sequence [1, 2, ..., ]. The autoregressive nexttoken prediction loss can then be reformulated as: log pθ(xli xl<i ) (3) L(x, l) = 1 (cid:88) i="
        },
        {
            "title": "Preprint",
            "content": "Additionally, li and l<i respectively denote the i-th element and the first 1 elements of the permuted sequence."
        },
        {
            "title": "3.2 WHY IS A GENERATION ORDER BETTER THAN ANOTHER?",
            "content": "In this section, we discuss how selecting an effective generation order can improve the performance of an AR model. By nature, all possible chains of conditionals can be represented as tree, where each step of the AR model branches out to represent all possible outcomes for the current variable. In the training phase, only one path (from root to leaf node) is followed for each sample. Therefore, the goal of learning is to identify which path is optimal among all the possible paths in this tree based on the training data so that only the optimal paths are selected during the generation phase. At each level of this tree the AR model pθ should learn likelihood function p(.x<i) to represent the underlying data distribution p(x) through different levels (x0, x1, ..., xN ). At each level of this tree, it is important to make high-probability choices for two fundamental reasons: 1) Compounded probability and 2) No-recovery mechanism of the model. Compounded probabilities. At each subsequent level of this tree, the likelihoods of different nodes in given level directly depend on the previous choice. Therefore, low probability choice at the previous level will likely result in suboptimal and improbable path, making the model fit harder during training. No-recovery mechanism. During generation, the likelihood of the first token has strong hierarchical influence on the following tokens due to the sequential nature of the autoregressive process. The standard strategy consists of sampling the code ci from the learned conditional probability ci pθ(x<i) for the new patch in sequential way. In other words, the model generates one token at time based on the previously generated tokens. Therefore, if generated token is low likelihood, the model is pushed to zone of low training data, which can easily lead to poor generation. In contrast, if the model can select generation order that starts from high likelihood token, it would tend to stay longer in its high-density zone, avoiding possible generation errors or biases. The rest of the paper will explore this direction to improve AR image generation."
        },
        {
            "title": "4 OUR METHOD: ORDERED AR",
            "content": "The previous section explains why the order is crucial in AR models. This section introduces an image generation model capable of learning good orders through distillation approach. The different configurations of the decoder-only autoregressive models are outlined in the following subsections and summarized in Fig. 2. 4.1 AUTOREGRESSIVE RASTER SCAN Our basic autoregressive model architecture for conditional image generation is composed of four components: an image encoder eϕ(x, l), with parameters ϕ pre-trained and frozen during autoregressive training, that takes as input an image patch and position and provides an embedding. In addition, we condition our model on the text description of the class to be generated. As our approach does not really consider this conditioning, we do not show it in the formulation for the sake of simplicity. The transformer engine tΩ(e) takes as input an initial embedding and returns new embedding ˆe, and decoder dψ that takes as input codes and generates the corresponding image. The probability of certain code xi, given the context xj<i will be then: pθ(xix<i) = gω(tΩ(eϕ(x<i, l<i))), (4) where θ = {ω, Ω, ϕ}, and l<i are the location of the previously generated patches. In the raster-scan setting, is the identity function and does not need to be passed as conditioning to pθ."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Different Autoregressive (AR) models. (Top) raster scan is the normal approach for autoregressive generation from top left to bottom-right. The input token contains the content xi and the position li. (Middle) Any-given-order learns to generate tokens at any possible location. However, the position of the next token should be given as input in an additional positional embedding. (Bottom) Our method, Ordered Autoregressive, uses the any-given-order model but generates all possible positions and selects the most likely one (darker yellow) as the next generated token. 4.2 ANY-GIVEN-ORDER GENERATION Generating the content of each patch and the location simultaneously in which this content should be generated introduces challenging optimization landscape. Similar to single shot uniform sampling for neural architecture search Guo et al. (2020), we propose to separate the problem into two parts: i) training of all possible orders first and ii) during generation, produce in parallel all possible location patches, but select only the most promising one sequentially. To train with all possible orders, we uniformly sample all possible orders. This strategy aligns with some works that explore all orders during training Vinyals et al. (2016); Shih et al. (2022); Pannatier et al. (2024). We rework our positional encodings and use two to feed into our model. We use the location of the next patch and the location of the current patch as the positional encodings to feed into the model. We provide detailed formulation in the subsequent sub-section. Thus, now the next patch prediction pθ(li+1, xli xl<i) requires an additional input li+1, which is the location of the next patch. Considering the model architecture as the composition of an input encoding (e), transformer blocks (t), and output code generation (g), we can write as: pθ(li, xlixl<i , l<i) = gω(tΩ(eϕ(xl<i, l<i, l<i+1))), (5) where θ = {ω, Ω, ϕ}, and li are the location of the previously generated patches, and li+1 is the location of the next patch to generate. The encoder eϕ transforms the patch into an embedding with VQ-GAN Esser et al. (2021) encoder and uses half the dimension of the original position encoding to encode the current position li. The other half to encode the next position li+1. During training, the order of the patches is random shuffle of the initial order so that we attempt to ensure all orders are considered and there is the least possible bias to any specific order. 4.3 RELATIVE POSITION ENCODING The simplest position encoding assigns an embedding to each absolute patch location li embA(li) Vaswani et al. (2017). However, we argue that position embedding relative to the current"
        },
        {
            "title": "Preprint",
            "content": "patch may be more effective in learning the generation of the next patch. Therefore, we use two types of positional encodings in our approach: the current patch position is encoded as an absolute position, while the position of the next patch is encoded relative to the current patch. To formulate this, we compute the relative position embedding for the next patch as: embR(li+1) = embA(S + li+1 li), where is equal to the number of patches in one dimension of the image, denotes the absolute embedding and denotes the relative embedding. Note that parameters are not shared between the absolute and relative positional embeddings. In relative positioning, we assign positional embedding based solely on the distance of given patch from the current position. This relative position embedding helps the model to learn to generate patches locally as it has direct access to the information on the distance between the next patch and the previous one. This setup enables the transformer to leverage both types of positional information. We use axial positional embeddings Ho et al. (2020) to model the positional embeddings in both cases. We use the standard image height and width for the absolute part to construct the embedding table, while for the relative part, we use double the height and width to accommodate the relative difference. Note that the proposed relative positional embedding differs from previous approaches in encoding the relative position in the transformer Yang et al. (2019). We compare absolute and relative positioning in the result section of this paper. 4.4 ORDERED GENERATION As in standard AR model, patches are generated sequentially at generation. However, in this case, by changing lN , we can choose the location where we want to generate the next patch. While many strategies are possible, as we want to select the most favorable location for the next patch, we generate the token distributions at every location and select the tokens with top-k sampling. Their corresponding likelihood ci,l can be written as: ci,l pθ(l, xix<i, l<i)) C, (6) where are all the valid locations for the next patch. Although this might look computationally expensive, generating all locations can be performed in parallel, avoiding significant slowdown of the generated image. Now, the selected location for the patch is: = arg max pθ(ci,lx<i, l<i)). (7) With this approach, we ensure that the model generates from the easiest to the most difficult patches. 4.5 COMPUTATIONAL COST While the computational complexity during training remains comparable to that of raster-scan AR models, our inference procedure results in higher computation cost as it requires (N +1) forward pass to generate an image, where denotes the number of patches in an image. However, the time complexity remains low as the token distribution for each location is computed in parallel at each step. 2 Additionally, we adapt the KV-caching mechanism Ramesh et al. (2021) to speed up inference and provide pseudo-code of our implementation in the Appendix. 4.6 DISTANCE REGULARIZATION To further improve the order generation, we introduce regularization term during generation to push the model to generate local patches and avoid large jumps in the image location. To do that, we add term in the selection of the next token location: = arg max (pθ(l, ci,lx<i, l<i) λd(l, li1)), (8) where d(l, li1) is the distance between current patch the previous patch and the location li1. In this way, the selected next patch is now trade-off (defined by λ) between the likelihood of the sampled patch and the distance between the two patches. We favor locality in the generation with high lambda, while with lower lambda, we favor patches with likely content."
        },
        {
            "title": "Preprint",
            "content": "Figure 3: Examples of generation on the Fashion Products dataset. (Top) Generated images with raster AR mode. (Middle) Generated images with ordered AR model. (Bottom) Generation order, from yellow to violet. From these images, we see that our approach finds an order highly correlated with the image content, often resulting in better image quality. 4.7 DISTILLING THE ORDER While training to reproduce any conditional distribution order is powerful approach to learning the joint data distribution better, it pushes the model to learn much more than what is actually needed at run-time, as we are interested only in the correct generation order. Thus, after training model with any-given-order, we propose fine-tuning the same model on the yielded order. To achieve this, we use the model to sample orders for all training data and use those orders to fine-tune the model weights. In this way, the model loses in generality but gains in specificity, leading to an additional increase in generation performance. We leave an iterative procedure of estimating order and fine-tuning the model weights as future work."
        },
        {
            "title": "5 EXPERIMENTS AND ANALYSIS",
            "content": "In this section, we empirically evaluate the performance of our model and attempt to analyze our findings. In 5.2, we evaluate Frechet inception distance (FID), Inception Score (IS), and Kernel Inception Distance (KID) Kynkaanniemi et al. (2023) on the dataset. In addition, we report also the average distance between two subsequent generated patches (d). In 5.3, we conduct an ablation over different positional encodings and distance regularization coefficient configurations. Finally, in section F, we show how our learned order adapts to the image content. Evaluations are conducted on the Fashion Product dataset Aggarwal (2021) and additionally evaluated on Multimodal CelebA-HQ dataset Xia et al. (2021). The fashion dataset contains 44,400 images of fashion products and their corresponding captions. The CelebA datasset contains 30, 000 images and corresponding attributes as captions. For both the datasets, we use 90% of the data for training and the other 10% for testing purposes. Each image has white background and an object at the center. Three visual descriptions are provided for each image, with more or less details. 5.1 EXPERIMENTAL SETUP In our experiments, we train single decoder-only transformer. We use pretrained VQGAN Esser et al. (2021) encoder to encode the images, decoder, and codebook of 1024 tokens. The encoder uses patch size of 16 16 to encode the images. The same configuration is used to train all models: for each image, we randomly chose caption in the runtime among the multiple captions provided by the dataset. The text sequence is truncated to 256 tokens when longer, padded with special tokens when shorter, and concatenated to the image tokens sequence. The transformer uses 768 embeddings in dimensions, depth 6, and 14 heads of 64 dimensions each. We use learnable positional embeddings that are additively factorized for parameter efficiency Ho et al. (2020). We use layer normalization Ba et al. (2016) and apply dropout Srivastava et al. (2014) to linear projections in both feed-forward"
        },
        {
            "title": "Preprint",
            "content": "Table 1: Generation with different orders. Our ordered generation improves over the standard rasterscan order and random order generation similar to Li et al. (2024). FID is the Frechet inception distance, IS is the Inception Score and KID Kernel Inception Distance. denotes the average distance between subsequently generated patches. Train IS () KID ()"
        },
        {
            "title": "Generation",
            "content": "FID ()"
        },
        {
            "title": "Method",
            "content": "d Raster Raster Random-Raster Random Random Random Ordered (Ours) Random Fine-tuned Ordered (Ours) Ordered"
        },
        {
            "title": "Raster\nRaster\nRandom\nOrdered\nOrdered",
            "content": "4.58 4.38 4.07 3.02 2.56 1.106 1.102 1.103 1.108 1.111 0.0031 0.0031 0.0028 0.0019 0.0015 1.83 1.83 8.19 4.34 3.99 networks and self-attention layers, with probability = 0.2. We apply randomly resized crop augmentation of the data for the images. We weight the loss computed on image tokens by factor of 7 compared to the loss for text tokens, following Ramesh et al. (2021). We use the AdamW optimizer Loshchilov & Hutter (2019) with learning rate of 3 104 that we reduce by factor of 0.8 following the ReduceLROnPLateau schedule. Models are trained for 300 epochs with batch size 16 on each machine. 5. IMAGE GENERATION Table 1 presents results for different training and generation orders on the Fashion Products datasets. As already shown in other works Li et al. (2024); Fan et al. (2024); Yu et al. (2024b), we observe that the Raster scan order is not optimal and that we can easily get better generation performance by training with Random orders. However, further improvements are observed using our Ordered generation. By generating the most likely patches first, our model reduces possible drifting, consistently improving the generation. Finally, the best performance is obtained when our model is also Fine-tuned to follow an ordered generation, as the meaningful order is reinforced unsupervised during this training. In Figure 3, we present examples of images generated by our method and the corresponding generation orders and compare them with raster-scan generation. 5.3 ABLATIONS Relative vs. Absolute positional encoding. Table 2 compares performance between the absolute encodings and the relative encoding formulation introduced in Section 4.3. The model improves by nearly 1 point in FID when encoding relative positions. The use of relative embeddings enables the model to capture local dependencies and make the model aware of distance variations from the previous patch, which appears beneficial in this context. Table 2: Generation with absolute and relative positional encoding for the next token. Method li+1 - Raster Ordered Abs. Ordered Rel. FID () IS () KID () 4.58 3.96 3. 1.106 1.102 1.108 0.0031 0.0024 0.0019 1.83 5.78 4.34 Distance regularization. We tested the effect of regularizing the distance between generated tokens by introducing penalty term λ, as shown in Eqn.8. Table 3 illustrates the impact of different values of λ, which control the trade-off between the patch likelihood and the distance from the previous token, on generation performance. As observed, the impact of this regularization is minimal. We believe this limited effect is due to the regularization being applied only at generation time, without affecting the learning. We leave the application for similar regularization during training for future work."
        },
        {
            "title": "Preprint",
            "content": "Table 3: Generation with different penalty regularization. λ 0.0 0.3 0.5 0.7 FID () IS () KID () 3.15 3.11 3.02 3.05 1.106 1.106 1.108 1.106 0.0023 0.0021 0.0019 0.0019 5.34 4.65 4.34 4.09 Random-Order Training Vs. Order Fine-tuning. To evaluate the impact of fine-tuning with extracted orders, we compare the performance of our final model against baseline model trained under the same training and inference framework but without fine-tuning on orders. Both models undergo the same training epochs to ensure fair comparison, with the only distinction being that our model undergoes fine-tuning with extracted orders during the final 150 epochs. After 450 epochs of training with randomized orders, the baseline model achieves an FID of 2.98. In contrast, our model, which undergoes 300 epochs of training with randomized orders followed by 150 epochs of fine-tuning with extracted orders, achieves an improved FID of 2.56. These results show that incorporating extracted orders during fine-tuning substantially reduces FID, highlighting the crucial role of this stage in our final methodology. 5.4 EXPERIMENTS ON MULTIMODAL CELEBA-HQ DATASET Figure 4: Examples of generation on the CelebA dataset. (Top) Generated images with raster AR mode. (Middle) Generated images with ordered AR model. (Bottom) Generation order, from yellow to violet. On this dataset our model generates first the salient parts of face, leaving hair and background at the end. Our model produces images with greater smoothness, rich context and more aligned with the text Additionally, we experiment on the Multi-Modal CelebA-HQ dataset using random 90 : 10 train-test split. This dataset contains 30, 000 elebrity face images with diverse facial attributes such as eyebrow shape, iris color, makeup styles, and hair types, among others. Unlike the Fashion Product dataset, CelebA-HQ features varied backgrounds, although the primary focus remains on facial features and attributes. The primary target is to generate face based on caption which consist of facial attributes Consistent with our findings on the Fashion Product dataset, distilling the order and fine-tuning the model with the found order provides better FIDs than raster scan or any-given-order approaches. We present the qualitative results in Figure 4. Interestingly, the model follows structured generation"
        },
        {
            "title": "Preprint",
            "content": "Table 4: Generation with Different Orders on the Multi-Modal CelebA-HQ Dataset. Our ordered generation improves over the standard raster-scan order. Dist. Reg. is the distance regularization parameter (λ, see Equation: 8)"
        },
        {
            "title": "Model",
            "content": "Raster-Scan Ordered Ordered Fine Tuned Ordered Dist. Reg. FID () 1.94 1.68 1.52 1.41 pattern: It generally begins with likely-to-generate regions (e.g., cheeks and chin). It then progresses to intricate features like eyes, lips, and hair. Finally, the background is typically generated last. This pattern is consistent with our findings in product images, where the model prioritizes easily predictable regions, such as the white background. Additionally, in the fashion dataset, the imbalance of the data due to the white background makes it more likely to generate than the background. To verify these claims we also conducted experiments with random background which can be found in the Appendix of this paper. The learned orders tend to generally generate one specific object part (including background) before progressing to another part. Semantically, this lets the model complete the generation of given semantic, which helps the model to better generate the following tokens."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this work, we present Ordered Autoregressive (OAR) Image Generation, method to improve image generation by finding an optimal order in which the patches of an image are generated. To achieve this, we modify the standard autoregressive model to include the position of the next token, train an any-given-order autoregressive model and use it at each step to find the optimal location for the next patch. Finally, we fine-tune our model to closely follow that specific order. Results show that, for the first time, our OAR model can generate improved images by just finding the right generation order in self-supervised manner. Limitations and future work Most of our experiments were run on NVIDIA V100 (32 GBs) GPUs. Due to computation limitations, our experiments are smaller in scale compared to contender works. Nonetheless, we hypothesize that these conclusions generalize to larger-scale settings due to the well-known capabilities of AR transformers to scale to larger models and reasonably large amount of data. For future work we would like to distill the learned orders into new model that can estimate location and content of patch in single step. In addition, the order pseudo-labeling procedure is performed only once for the moment. We would like to explore the improvements that an iterative procedure (order pseudo-labeling in training) could bring."
        },
        {
            "title": "REFERENCES",
            "content": "Muhammad Adnan, Akhil Arunkumar, Gaurav Jain, Prashant Nair, Ilya Soloveychik, and Purushotham Kamath. Keyformer: Kv cache reduction through key tokens selection for efficient generative inference. In MLSys, volume 7, 2024. Param Aggarwal. Fashion product images dataset, 2021. URL https://www.kaggle.com/ datasets/paramaggarwal/fashion-product-images-dataset. Anthropic. The claude 3 model family: Opus, sonnet, haiku, 2024. URL https://api. semanticscholar.org/CorpusID:268232499. Jacob Austin, Daniel Johnson, Jonathan Ho, Daniel Tarlow, and Rianne Van Den Berg. Structured denoising diffusion models in discrete state-spaces. In NIPS, volume 34, pp. 1798117993, 2021. Jimmy Lei Ba, Jamie Ryan Kiros, and Geoffrey E. Hinton. Layer normalization, 2016. URL https://arxiv.org/abs/1607.06450."
        },
        {
            "title": "Preprint",
            "content": "Mathilde Caron, Alireza Fathi, Cordelia Schmid, and Ahmet Iscen. Web-scale visual entity recognition: An LLM-driven data approach. In NIPS, 2024. Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In CVPR, pp. 1131511325, 2022. Mark Chen, Alec Radford, Rewon Child, Jeff Wu, Heewoo Jun, David Luan, and Ilya Sutskever. Generative pretraining from pixels. In ICML, 2020. Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved autoregressive generative model. In ICML, pp. 864872, 2018. Aakanksha Chowdhery, Sharan Narang, Jacob Devlin, Maarten Bosma, Gaurav Mishra, Adam Roberts, Paul Barham, Hyung Won Chung, Charles Sutton, Sebastian Gehrmann, et al. Palm: Scaling language modeling with pathways. Journal of Machine Learning Research, 24(240):1113, 2023. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. In ICLR, 2024. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, pp. 41714186, 2019. Alexey Dosovitskiy. An image is worth 16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020. Philipp Dufter, Martin Schmitt, and Hinrich Schutze. Position information in transformers: An overview. Computational Linguistics, 48(3):733763, 09 2022. ISSN 0891-2017. doi: 10.1162/ coli 00445. URL https://doi.org/10.1162/coli_a_00445. Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In CVPR, pp. 1287312883, 2021. Lijie Fan, Tianhong Li, Siyang Qin, Yuanzhen Li, Chen Sun, Michael Rubinstein, Deqing Sun, Kaiming He, and Yonglong Tian. Fluid: Scaling autoregressive text-to-image generative models with continuous tokens. arXiv preprint arXiv:2410.13863, 2024. Nicolas Ford, Daniel Duckworth, Mohammad Norouzi, and George Dahl. The importance of generation order in language modeling. arXiv preprint arXiv:1808.07910, 2018. Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive KV cache compression for LLMs. In ICLR, 2024. Jiatao Gu, Qi Liu, and Kyunghyun Cho. Insertion-based decoding with automatically inferred generation order. Transactions of the Association for Computational Linguistics, 7:661676, 2019. Zichao Guo, Xiangyu Zhang, Haoyuan Mu, Wen Heng, Zechun Liu, Yichen Wei, and Jian Sun. Single path one-shot neural architecture search with uniform sampling. In ECCV, pp. 544560, 2020. Jonathan Ho, Nal Kalchbrenner, Dirk Weissenborn, and Tim Salimans. Axial attention in multidimensional transformers, 2020. Emiel Hoogeboom, Alexey A. Gritsenko, Jasmijn Bastings, Ben Poole, Rianne van den Berg, and Tim Salimans. Autoregressive diffusion models. In ICLR, 2022. Edward Hu, yelong shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-rank adaptation of large language models. In ICLR, 2022. Wouter Kool, Herke Van Hoof, and Max Welling. Stochastic beams and where to find them: The gumbel-top-k trick for sampling sequences without replacement. In ICML, pp. 34993508, 2019. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In ACM SIGOPS, 2023."
        },
        {
            "title": "Preprint",
            "content": "Tuomas Kynkaanniemi, Tero Karras, Miika Aittala, Timo Aila, and Jaakko Lehtinen. The role of imagenet classes in frechet inception distance. In ICLR, 2023. Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In CVPR, pp. 21422152, 2023. Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. In NeurIPS, 2024. Xuanlin Li, Brandon Trabucco, Dong Huk Park, Michael Luo, Sheng Shen, Trevor Darrell, and Yang Gao. Discovering non-monotonic autoregressive orderings with variational inference. In ICLR, 2021. Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. Shikib Mehri and Leonid Sigal. Middle-out decoding. In NIPS, volume 31, 2018. Fabian Mentzer, David Minnen, Eirikur Agustsson, and Michael Tschannen. Finite scalar quantization: VQ-VAE made simple. In ICLR, 2024. OpenAI. Language models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. OpenAI. Gpt-4 technical report, 2024. URL https://arxiv.org/abs/2303.08774. Ziqi Pang, Tianyuan Zhang, Fujun Luan, Yunze Man, Hao Tan, Kai Zhang, William T. Freeman, and Yu-Xiong Wang. Randar: Decoder-only autoregressive visual generation in random orders. In CVPR, 2025. Arnaud Pannatier, Evann Courdier, and Francois Fleuret. σ-gpts: new approach to autoregressive models. In ECML PKDD, pp. 143159, 2024. Vassilis Papadopoulos, Jeremie Wenger, and Clement Hongler. Arrows of time for large language models. arXiv preprint arXiv:2401.17505, 2024. Niki Parmar, Ashish Vaswani, Jakob Uszkoreit, Lukasz Kaiser, Noam Shazeer, Alexander Ku, and Dustin Tran. Image transformer. In ICML, pp. 40554064, 2018. Reiner Pope, Sholto Douglas, Aakanksha Chowdhery, Jacob Devlin, James Bradbury, Jonathan Heek, Kefan Xiao, Shivani Agrawal, and Jeff Dean. Efficiently scaling transformer inference. In MLSys, volume 5, pp. 606624, 2023. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In ICML, pp. 88218831, 2021. Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. In NIPS, volume 32, 2019. Stuart Rosen. Temporal information in speech: acoustic, auditory and linguistic aspects. Philosophical Transactions of the Royal Society of London. Series B: Biological Sciences, 336(1278):367373, 1992. Claude Elwood Shannon. mathematical theory of communication. The Bell system technical journal, 27(3):379423, 1948. Peter Shaw, Jakob Uszkoreit, and Ashish Vaswani. Self-attention with relative position representations. In NAACL, 2018. Andy Shih, Dorsa Sadigh, and Stefano Ermon. Training and inference on any-order autoregressive models the right way. In NIPS, volume 35, pp. 27622775, 2022. Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya Sutskever, and Ruslan Salakhutdinov. Journal of Machine Dropout: simple way to prevent neural networks from overfitting. Learning Research, 15(56):19291958, 2014. URL http://jmlr.org/papers/v15/ srivastava14a.html."
        },
        {
            "title": "Preprint",
            "content": "Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation, 2024. URL https: //arxiv.org/abs/2406.06525. Qing Sun, Stefan Lee, and Dhruv Batra. Bidirectional beam search: Forward-backward inference in neural sequence models for fill-in-the-blank image captioning. In CVPR, pp. 69616969, 2017. Gemini Team and Google. Gemini: family of highly capable multimodal models, 2024. URL https://arxiv.org/abs/2312.11805. Qwen Team and Alibaba Group. Qwen2 technical report, 2024. URL https://arxiv.org/ abs/2407.10671. Benigno Uria, Marc-Alexandre Cˆote, Karol Gregor, Iain Murray, and Hugo Larochelle. Neural autoregressive distribution estimation. JMLR, 17(205):137, 2016. Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, and Alex Graves. Conditional image generation with pixelcnn decoders. In NIPS, volume 29, 2016. Aaron Van Den Oord, Nal Kalchbrenner, and Koray Kavukcuoglu. Pixel recurrent neural networks. In ICML, pp. 17471756, 2016. Aaron Van Den Oord, Oriol Vinyals, and Koray Kavukcuoglu. Neural discrete representation learning. In NIPS, volume 30, 2017. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In NIPS, volume 30, 2017. Oriol Vinyals, Samy Bengio, and Manjunath Kudlur. Order matters: Sequence to sequence for sets. In ICLR, 2016. Xinyi Wang, Hieu Pham, Pengcheng Yin, and Graham Neubig. tree-based decoder for neural machine translation. In EMNLP, 2018. Mark Weber, Lijun Yu, Qihang Yu, Xueqing Deng, Xiaohui Shen, Daniel Cremers, and Liang-Chieh Chen. Maskbit: Embedding-free image generation via bit tokens. arXiv:2409.16211, 2024. Sean Welleck, Kiante Brantley, Hal Daume Iii, and Kyunghyun Cho. Non-monotonic sequential text generation. In ICML, pp. 67166726, 2019. Weihao Xia, Yujiu Yang, Jing-Hao Xue, and Baoyuan Wu. Tedigan: Text-guided diverse face image generation and manipulation. In CVPR, 2021. Yilun Xu, Shengjia Zhao, Jiaming Song, Russell Stewart, and Stefano Ermon. theory of usable information under computational constraints. In ICLR, 2020. Kenji Yamada and Kevin Knight. syntax-based statistical translation model. In ACL, pp. 523530, 2001. Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Carbonell, Russ Salakhutdinov, and Quoc Le. Xlnet: Generalized autoregressive pretraining for language understanding. In NIPS, volume 32, 2019. Lijun Yu, Jose Lezama, Nitesh Bharadwaj Gundavarapu, Luca Versari, Kihyuk Sohn, David Minnen, Yong Cheng, Agrim Gupta, Xiuye Gu, Alexander Hauptmann, Boqing Gong, Ming-Hsuan Yang, Irfan Essa, David Ross, and Lu Jiang. Language model beats diffusion - tokenizer is key to visual generation. In ICLR, 2024a. Qihang Yu, Ju He, Xueqing Deng, Xiaohui Shen, and Liang-Chieh Chen. Randomized autoregressive visual generation. arXiv preprint arXiv:2411.00776, 2024b."
        },
        {
            "title": "A GENERATION ARCHITECTURE",
            "content": "We provide detailed approach to the generation pipeline in Algorithm 1. When generating with the transformer at inference we require condition parameters, we use text embeddings to model this as text-to-image generative transformer. To ensure we can query over all of the tokens we use zero token padded before the first image token. To parallelize the computation we use batched processing where each batch dimension computes one location. We use Gumbel-Top-k sampling Kool et al. (2019) to sample the best codebook index. We use Gumbel noise with µ = 0 & σ = 1. However, since we refrain from training or updating the VQGAN-VAE, we need to reorder the images after generating all the patches to get desirable output. Algorithm 1 Generation Process Input: Condition parameters, the generative AR transformer engine tΩ, decoder dψ Output: generated image based on the given condition parameters 1: Initialize list of possible positions (x, y) of total length 2: Compute reference absolute (embA) and relative (embR) embeddings 3: Predict the first token using condition parameters with tΩ 4: for = (1, 2...n) AR steps do 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: end for 15: Discard the first (zero) token 16: Reorder the patches using recorded locations 17: Decode the patche tokens using dψ Extract the relative positions from (P ) for all locations Get embR for the relative positions Replicate the absolute positions for + 1 times Retrieve embA for the absolute positions Concatenate embA and embR Compute Eqn 5 in parallel for + 1 positions via tΩ Select the most favorable patch location (Eqn. 8)l and record it Use Gumbel-Top-k sampling trick for the best codebook index Remove from P"
        },
        {
            "title": "B ADDITIONAL RELATED WORK",
            "content": "Since the self-attention mechanism in transformers is inherently permutation-invariant, it does not account for token order. It requires an additional overhead to encode positional information Vaswani et al. (2017); Dufter et al. (2022). Early methods introduce absolute positional embeddings, encoded through sinusoidal functions or learnable embeddings, to differentiate tokens based on their positions rather than just their content Dosovitskiy (2020); Vaswani et al. (2017). However, as the number of tokens grows, absolute positional embeddings become less efficient. To address this limitation, Shaw et al. Shaw et al. (2018) introduces relative position biases directly into the attention matrix within the self-attention layers, enhancing spatial awareness and scalability. While both absolute and relative position embeddings are effective in fixed-token-size scenarios, they struggle with adapting to variable token sizes and require flexibility and extrapolation capabilities. To overcome these challenges, Su et al. Su et al. (2024) proposes an approach that encodes absolute positions using rotation matrix and integrates explicit relative positional dependencies into the self-attention formulation. This combined method enhances the adaptability to different resolutions, enabling the model to generalize better across varying spatial contexts. In our work, following the same motivation as Gu et al. (2019), we leverage relative positions to alleviate the challenges of predicting absolute positions. Our method introduces two key novelties: First, we extend the relative positional encoding to handle 2D inputs effectively. Second, we employ dual positional encoding schemepreserving the positional encoding of the current patch as absolute, while encoding the position of the next patch relative to the current one. Notably, our proposed relative positional embedding differs from prior methods, where relative positions are directly embedded within the transformer architecture itself Yang et al. (2019)."
        },
        {
            "title": "Preprint",
            "content": "C V-INFORMATION So far, we have explored how order influences both the training and generation processes. In this section, we examine the impact of order from an information-theoretic perspective Shannon (1948). When ignoring computational considerations, different factorizations of conditional probability should theoretically yield equivalent results. However, traditional information theory often overlooks essential computational factors relevant to practical applications. To address these nuances, we apply V-information Xu et al. (2020), which intuitively quantifies the additional information about random variable that can be extracted from another variable using any predictor in V. We define V-entropy as the uncertainty we have in after observing as follow: HV (Y X) = inf E[ log [x](y)], (9) where [x](y) produces probability distribution over the tokens. Information terms like IV (X ) are defined analogous to Shannon information, that is, IV (X ) = HV (Y ) HV (Y X). In our setup, we define as the Any-Order Autoregressive Model family of functions, described in Section 3, where denotes the next token and represents the context tokens. These context tokens are arranged based on two distinct ordering schemes: l0 for the raster-scan order and lτ for the learned order inferred by our model. We define cross-entropy as HV (), which we use to measure V-information. Since the next tokens differ between these two orders, we calculate the sum of the V-information from to over each token of the sequence under each order and then compute the difference between them as follows: (cid:88) IV (Xlτ <t Xlτ t+1 ) (cid:88) IV (Xl0 <t Xl0 t+1 ) = (cid:88) HV (Xl0 t+ Xl0 <t ) (cid:88) HV (Xlτ t+1 Xlτ <t ) (10) (11) This difference, resulting in value of 0.206, quantifies the increased accessibility of Xlt+1 when the context is arranged in the learned order lτ as opposed to the raster-scan order l0 within our model family."
        },
        {
            "title": "D DIFFERENT GENERATION APPROACHES",
            "content": "D.1 DECODING STRATEGIES In our study, we aim to maximize the use of the representation learned by our model to enhance image generation quality. To achieve this, we perform an ablation over decoding strategies. Instead of sampling the location according to Eqn. 8 (see section 4.6), simple alternative consists of considering the joint representation and then taking the top-k (50% of the codebook size, in our case) elements of all the patches together. We then mask the rest of the elements to ensure they are not selected. Therefore, we see the representation as joint representation as opposed to sampling the location followed by the content. We observe the FID to slightly drop to 3.04, the IS to be 1.109 and the KID to be 0.0020. D.2 ABSOLUTE AND RELATIVE ENCODINGS As shown in results, the use of relative encodings yields significant increase in performance. To further investigate the practical implications, we observe how these encodings affect the output by visualizing the order. As illustrated in Figure 5, consecutive tokens in the latent 1D-sequence are positioned more closely when generating the background, which aligns with the measured average distance between consecutive tokens for both models in Table 2 (in rsults section). While the metrics clearly indicate an enhancement in model performance, the visual differences remain subtle and difficult to discern."
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Generation order with absolute and relative positioning encoding. (Top) With absolute encoding the generation is very scattered. (Bottom) With relative positioning the generation is more localized. The average euclidean distance between the subsequently generated patches in case of absolute encoding is 5.78 whereas in case of relative encoding it is 4."
        },
        {
            "title": "E KEY VALUE CACHING",
            "content": "In generative inference, modern LLMs and AR models Ge et al. (2024); Ramesh et al. (2021) present substantial latency and throughput concerns Adnan et al. (2024). Key-value (KV) caching significantly reduces the computational cost required by storing the key and value projections of the previously generated tokens to avoid re-computing them later. We should also note that the use of KV caching is mostly enabled by causal masking. In contrast, KV caching may not be implemented for bidirectional attention or other attention strategies Chang et al. (2022); Li et al. (2024) since the token sequence is not generally fixed. In the same way, it is not straightforward to use KV caching in our method since the next token position in the sequence is not known in advance. To address this gap and speed up the inference, we remodel the traditional KV caching method Pope et al. (2023). As discussed thoroughly in the paper, our method relies on querying every available location at each AR step and generating patch at the best one only. To accelerate this inference process, we compute the possible locations in parallel, thereby reducing the overall computation time. We consider having temporary secondary cache at each AR step, to store KV representations for all possible locations. Upon selecting the final location, we update the primary cache with the appropriate KV representation from the secondary cache. Once we select the desired location, the"
        },
        {
            "title": "Preprint",
            "content": "secondary KV cache is deleted to free up memory. We provide the broad-level implementation in Algorithm 2. Algorithm 2 Caching Strategy Input: Token sequence of length i, list of available locations Output: Updated primary cache with selected key-value pair and vl 1: Compute kl for all available locations in parallel 2: Store all kl and vl in secondary cache 3: Select location 4: Store kl and vl 5: Delete secondary cache in primary cache E."
        },
        {
            "title": "IMPACT OF IMPROVED KV CACHING",
            "content": "It is important to deeply understand in terms of inference timings how our optimization strategies benefit in the generation process. To broadly classify our method leverages two key strategies to deal with efficiency concerns: i) Parallelizing the generation process and ii) KV caching. We tabulate the inference timings for single image generation in Table 5 Table 5: Inference Time Comparison Across Different Model Configurations Inference Time (sec) Model Configuration Raster Scan Raster Scan (Cached) OAR (Naıve) OAR (Parallel Evaluation) OAR (Parallel Evaluation + Optimized KV Cache) 6.33 2. 262.26 73.76 2.99 We observe that parallelizing the generation process across different locations reduces latency by approximately 3.5. Our optimized KV caching scheme accelerates inference by nearly 90. With these optimizations, our model achieves inference efficiency comparable to traditional AR models that follow raster-scan approach for single image. However, it is to be noted that the reported timings are summed over all the steps in model inference and do not include overhead like loading the model, setting up caches etc."
        },
        {
            "title": "F THE EFFECT OF BACKGROUND",
            "content": "Figure 6: Average distance between generated patches for normal Fashion with white background and our modified version with noisy background. Due to the different generation orders, the average distance is also different."
        },
        {
            "title": "Preprint",
            "content": "All images in the dataset we used for this study have large portion covered by white background. While initially, this could be considered an interesting feature to isolate the foreground object, in practice, it introduces biases in the generation process. First, the most frequent patches are the white background patches. Thus, those patches are the most likely to be generated and, therefore, based on Eqn. 7, they will be the first to be generated even though they do not contain any important information about the object of interest (see. Fig.7 (Top)). To verify that our model can learn orders that prioritize the foreground object, we run an additional training with modified version of the same dataset, in which the background is filled with random noise instead of white. As the noise varies, the model cannot fit it well, so it would not be as dominant as the white background. In this setting (Fig.7 (Bottom)), we observe that the model generates first the object of interest and later the background as expected. Additionally, visually, the generation seems to be improved, as the model can start the generation from the most discriminative part of the image. We tried to measure FID for this setting. Still, unfortunately, inception does not provide meaningful scores for images with random noise, so the estimated FID does not correlate well to estimate the quality of the generated images. Figure 7: Generation order with different backgrounds. (Top) With an easy background, such as uniform white, the model will start generating from the background. (Bottom) With more difficult background, such as random white noise, the model will start generating from the object. Another interesting variable is the average distance (d) between generated patches. Without inducing an external regularization (Eqn.8), we checked the evolution of the average distance during training. Results are reported in Fig.6 for the normal dataset and the variation with noisy background. In both cases, the average distance starts from around 3.5. However, when the background is white, the model generates the background first. This increases the average distance to around 4.0. In contrast, when the background is random noise, the model can focus on the foreground first, and the average distance is reduced to around 2.5 over time. This is because when the model focuses on the object, it learns from data that locality is useful for better generation, and therefore, it learns to generate more local patches."
        }
    ],
    "affiliations": [
        "Canada CIFAR AI Chair",
        "Ecole de technologie superieure, QC, Canada",
        "International Laboratory on Learning Systems (ILLS)",
        "Mila-Quebec AI Institute",
        "Polytechnique Montreal",
        "ServiceNow Research",
        "Stony Brook University, NY, USA",
        "Universite Paris-Saclay, CentraleSupelec, France"
    ]
}