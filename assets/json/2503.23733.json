{
    "paper_title": "AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization",
    "authors": [
        "Yiyang Du",
        "Xiaochen Wang",
        "Chi Chen",
        "Jiabo Ye",
        "Yiru Wang",
        "Peng Li",
        "Ming Yan",
        "Ji Zhang",
        "Fei Huang",
        "Zhifang Sui",
        "Maosong Sun",
        "Yang Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS, a novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 3 ] . [ 1 3 3 7 3 2 . 3 0 5 2 : r AdaMMS: Model Merging for Heterogeneous Multimodal Large Language Models with Unsupervised Coefficient Optimization Yiyang Du*,1, Xiaochen Wang*,3,4, Chi Chen*,1, Jiabo Ye5, Yiru Wang8, Peng Li (cid:66),2,6, Ming Yan5, Ji Zhang5, Fei Huang5, Zhifang Sui3, Maosong Sun1, Yang Liu(cid:66),1,2,6,7 1Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China 2Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China 3State Key Laboratory of Multimedia Information Processing, Peking University, Beijing, China 4School of Software Microelectronics, Peking University, Beijing, China 5Institute of Intelligent Computing, Alibaba Group 6Shanghai Artificial Intelligence Laboratory, Shanghai, China 7Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China 8ModelTC Open Source Organization, Beijing, China"
        },
        {
            "title": "Abstract",
            "content": "1. Introduction Recently, model merging methods have demonstrated powerful strengths in combining abilities on various tasks from multiple Large Language Models (LLMs). While previous model merging methods mainly focus on merging homogeneous models with identical architecture, they meet challenges when dealing with Multimodal Large Language Models (MLLMs) with inherent heterogeneous property, including differences in model architecture and the asymmetry in the parameter space. In this work, we propose AdaMMS1, novel model merging method tailored for heterogeneous MLLMs. Our method tackles the challenges in three steps: mapping, merging and searching. Specifically, we first design mapping function between models to apply model merging on MLLMs with different architecture. Then we apply linear interpolation on model weights to actively adapt the asymmetry in the heterogeneous MLLMs. Finally in the hyper-parameter searching step, we propose an unsupervised hyper-parameter selection method for model merging. As the first model merging method capable of merging heterogeneous MLLMs without labeled data, extensive experiments on various model combinations demonstrated that AdaMMS outperforms previous model merging methods on various vision-language benchmarks.2 *Equal contribution. (cid:66)Corresponding authors: Peng Li (lipeng@air.tsinghua.edu.cn) and Yang Liu (liuyang2011@tsinghua.edu.cn). 1AdaMMS represents Adaptive Mapping, Merging, and Searching. 2Code at https://github.com/THUNLP-MT/AdaMMS. Model merging [10] has gained increasing popularity in the field of large language models (LLMs) [10, 26, 29, 32]. This approach typically involves combining the parameters of two models with the same architecture, creating new model without requiring additional training [10]. It has proven to be an efficient method for developing models that integrate the abilities of multiple existing models [26, 29, 32], avoiding the need for extensive data and computational resources, and has been widely adopted in building powerful LLMs [4, 6]. Despite its popularity with LLMs, model merging has yet to be widely adopted for multimodal large language models (MLLMs). Some recent studies have explored the application of model merging to MLLMs [1, 19] but either prioritize extending multimodal capabilities over enhancing the performance of existing models [1] or still require additional training after merging [19]. The primary obstacle in applying model merging to MLLMs lies in the heterogeneous nature of these models [8, 12, 14, 24, 28]. This heterogeneity arises from modifications to the transformer architectures [21] within their language models [8, 28], as well as differences in their choice of modality-specific encoders and tokenizers [8, 24]. Consequently, significant challenge in merging MLLMs is that the process cannot be directly applied to models with different architectures, as their weights are not isomorphic. Recent efforts like FuseLLM [22] and FuseChat [23] have explored fusing the capabilities of heterogeneous LLMs by merging their generative distributions. Theoretically, these methods could also be applied to MLLMs. Figure 1. (a) Illustration of three steps in AdaMMS: Step-1, mapping MLLMs with different model architecture; Step-2, merging MLLMs with linear interpolation; Step-3, searching for optimal merging hyper-parameter by approximate task performance through generation consistency without labeled data. (b) The gain performance of AdaMMS on broad range of multimodal tasks in comparison with existing merging approaches. Gain refers to the improvement obtained by subtracting the average result from the result of the fused model on certain task. The result here is the average of the gains from the two MLLM pairs merging. However, they rely on supervised continued training, which incurs significant computational costs and fails to address scenarios where labeled data is unavailable. For example, FuseLLM requires training dataset with total of 1.8 billion tokens and 33 hours of training time. This underscores the need for an unsupervised model merging technique to effectively integrate heterogeneous MLLMs. In this work, we address the challenges of merging heterogeneous MLLMs by introducing novel model merging strategy named AdaMMS, as illustrated in Figure 1. First, to enable model merging across heterogeneous MLLMs with differing architectures, we design parameter mapping framework. Specifically, we focus on the scenario where MLLMs have different language model architectures due to variations in transformer block duplications [8, 25, 28]. This mapping aligns corresponding components across models, enabling merging operations even with structural differences. Next, we apply adaptive linear interpolation on the mapped parameters during merging. By adjusting the interpolation coefficient, AdaMMS optimizes performance adaptively across different tasks. This coefficient adjustment is then optimized through an unsupervised procedure in the following step. Finally, we introduce an unsupervised hyperparameter selection method to determine the interpolation coefficient for each task. This approach leverages response consistency across candidate models as performance estimate, based on our novel insight that model performance correlates with generation In addition to strong empirical support, we consistency. provide theoretical analysis to explain this insight. Extensive experiments have demonstrated the effectiveness of our proposed AdaMMS in merging MLLMs. Our main experiments are conducted on two pairs of heterogeneous MLLMs, one of them is based on Qwen [27] architecture, another pair is based on LLaMA [20] architecture. Both experiments shows that our model can effectively combine the capabilities of heterogeneous MLLMs by enabling model merging on models with different architecture. The experiments also demonstrated that the merging strategy of AdaMMS, accompanied with our unsupervised hyper-parameter selection method, outperforms previous model merging methods on various vision-language tasks. We also conducted experiments to show that the unsupervised hyper-parameter selection method can be performed with fewer unlabeled data without harming its performance, which shows the robustness of the method and further decrease the data requirements of AdaMMS. Our main contributions are as follows: We introduce novel model-merging strategy, AdaMMS, designed to address the challenges of merging heterogeneous MLLMs. By defining parameter mapping between different models, AdaMMS facilitates model merging techniques even when architectural differences exist. We propose an unsupervised hyperparameter selection method inspired by the observation that model performance can be effectively estimated through generation consistencymeasured by the variation in generated responses. Unlike previous approaches requiring labeled data, this method eliminates the dependency on annotations and can be applied to small subset of 100 target sample without sacrificing effectiveness. Comprehensive experiments conducted on various model pairs demonstrate the effectiveness of our approach. Specifically, evaluations on Qwen-based and LLaVAbased heterogeneous MLLM pairs show that AdaMMS successfully combines capabilities from distinct MLLMs. Across various vision-language tasks, AdaMMS consistently outperforms baseline strategies, achieving superior overall performance. 2. Related Work 2.1. Model Merging Recent researches on model merging techniques have contributed to building more capable models by providing an efficient approach to combine abilities on various tasks from different models that requires less data and compute. Some studies focus on merging homogeneous models with identical architecture, while others focus on tackle the challenge on heterogeneous models which have different architectures. In merging homogeneous models, Task Arithmetic [10] proposes the concept of task vectors, which subtracts fine-tuned weights from pre-train weights to obtain task-related weight difference as the object of merging. Ties-Merging [26] and DARE [29] further improve the performance by mitigating parameter interference during the merging process through parameter pruning and conflict resolving. MetaGPT [32] scales the task vectors with task-agnostic coefficients in closed-form by seperating data term and scaling coefficients in the optimization objective. Although these methods improves the performance of the merged models, they cannot be directly applied on models with architecture difference. In fusing heterogeneous models, DAMC [1] employs parameter decoupling and adaptive adjustment to enhance model merging strategies for fusing modalities on MLLMs with different modality encoders, but this work still focus on merging identical language model architecture. To consolidate LLMs with different architectures, FuseLLM [22] and FuseChat [23] applies token alignment and model fusion strategies with knowledge distillation before continue training the model, but they need labeled data and computation resources for continue training. In fact, the majority of previous works on model merging requires labeled data for validation search or supervised training [1, 10, 22, 23, 26, 29]. In this work, we eliminate the need of labeled data by leveraging our unsupervised hyper-parameter selection method, and enable model merging strategies to be applied on heterogeneous MLLMs with architecture differences. 2.2. Multimodal Large Language Models As large language models demonstrate huge success in obtaining great abilities in general, recent researches on MLLMs have successfully appending multimodal processing and generation ability on LLMs, especially on the vision modality [2, 12, 14, 24, 25, 28]. However, these models often adapts unique modifications on language model architecture, resulting in set of heterogeneous MLLMs, which prevent model merging methods to be applied on them. Specifically, there are two levels of architecture differences among MLLMs. First, two MLLMs may be designed from different pre-trained language model. For example, Qwen2VL [24] and LLaVA-OneVision-Qwen [12] are designed from Qwen2 [27], while LLaVA [14], mPLUG-Owl2 [28], CogVLM [25] and ShareGPT4V [2] are designed following the LLaMA [20] architecture. Second, two MLLMs developed from the same pre-trained language model can still be heterogeneous because they are designed with different modifications on the language model. For example, although CogVLM and mPLUG-Owl2 are both developed from LLaMA architecture, CogVLM adapts visual experts by duplicating query, key and value weights in attention head, while mPLUG-Owl2 is designed to duplicate key, value, and layer norm weights instead. The first level of differences is hard to merge, since model merging applies to parameters that are trained from the same pre-training weights [10]. In this work, we tackle the second level of architecture differences via our proposed AdaMMS method. 3. Method To tackle the heterogeneous challenges in merging MLLMs, we propose novel model merging method named AdaMMS. As shown in Figure 1(a), it involves three steps: mapping, merging and searching. In the mapping step, we define mapping function that enables the merging of parameters from different architectures. Next, in the merging step, we apply linear interpolation to adaptively optimize the performance on specific downstream tasks. Finally, in the searching step, we design unsupervised hyperparameter selection method for choosing linear interpolation coefficient during merging. This method is based on our novel discovery that the model performance in the parameter space can be approximated by the difference among model responses without the need of labeled data. 3.1. Mapping To merge the parameters of two heterogeneous models M1 and M2 into M1s architecture, we need to align their parameters by defining mapping that maps each parameter θ1 in M1 to its corresponding parameter θ2 in M2 (or ϕ if there is no such corresponding parameter). As previously discussed in Section 2.2, we only tackle the heterogeneous MLLMs that are designed from same pre-trained language model architecture, but adapts different modifications on model structure. The principle of designing the mapping is that for the shared weights between the models (e.g. the weights in the pre-trained model), we can map them directly, and for additional weights in M1, we map it to its original corresponding weight in M2 if it is duplicated multimodal parameter in M1s specific design, otherwise we map ϕ with it and apply no operation later in merging. In this way, we leverage the additional weights as much as possible without mapping irrelevant parameters together. 3.2. Merging We follow the paradigm in Task Arithmetic [10] to apply merging operation on task vectors and apply linear interpolation on them. Task vectors are defined as the finetuned parameters subtracted by pre-train weights: τi = θi θ0 (i = 1, 2), where θ1 and θ2 is the models to be merged, and θ0 is their common initialization point (e.g. the shared pre-trained weights for two different finetuned model). Linear interpolation offers the availability to directly control the tendency between the two models alongside its simplicity. This allows us to actively adapt to different downstream tasks, as different downstream tasks often requires different combination or tendency on the two models for the best performance. Linear interpolation on task vectors can be simply formatted as: θout = θ0 + (1 α)τ1 + ατ2, , where α is the linear interpolation coefficient. This is equivalent to: θout = (1 α)θ1 + αθ2. Thus, leveraging our mapping funciton in Section 3.1, we can apply our merging operation on heterogeneous MLLMs as follow: iff (θi θi out = (cid:40)θi 1, (1 α)θi 1 + αf (θi 1), 1) = ϕ otherwise (1) Note that (θ1) is the parameter in M2 that corresponds to θ1, according to the mapping function . Now we can define the merging process of two model parameters Θ1 = {θi 1} and Θ2 = {θj 2} accordingly: Merge(Θ1, Θ2; ; α) = {θi out} (2) 3.3. Searching Consider the base model, which is defined as the architecture that will be used after the merging process, containing scalar weight elements, then the merging process can be seen as the operation on -dimentional vector space RN , where the merging strategy transform two input points as initial parameters Θ1 and Θ2 to the merged parameters Θout. For simplicity, we consider the case when the merging strategy takes one hyper-parameter α (linear interpolation coefficient). Θout = (Θ1, Θ2; α), Θ1, Θ2, Θout RN Given the inputs ti on downstream task, this creates landscape on RN that each model parameter corresponds to the model performance Sti on the inputs. Sti(Θout) = Sti,F (Θ1, Θ2, α) Therefore, the goal of the hyper-parameter searching is to find the best α that maximize the merged models performance on the tasks. For simplicity, we omit the in the index. α = argmax (Sti(Θ1, Θ2, α)) α Note that as the landscape varies greatly in different tasks, the best α may be different as well. Previous model merging methods mainly relies on validation set to search for the best hyper-parameter α with supervised searching. Formally, they use the best α in validation set ˆti to approximate the best α in test set ti. ˆα = argmax (Θ1, Θ2, α)) argmax (Sti (Θ1, Θ2, α)) (S ˆti α α However, this supervised way of searching has certain disadvantages: (1) labeled data with ground truth is hard to collect in some scenarios, and (2) the distribution shift between the validation set and the test set, even on the same task, will interfere the selection of the best hyper-parameter α. To get rid of these drawbacks, we propose an unsupervised hyper-parameter selection method through performance estimation metric that requires no labeled data. Specifically, we discover that the difference of generated responses between two adjacent α candidates can be used to estimate the model performance, and the best α can be approximated by the one with the lowest adjacent difference. As shown in Figure 2, the trend of model performance is similar with its generation consistency which is measured by response differences, and the α with the highest generation consistency match the α with the highest performance. Formally, let α and α+ be adjacent candidates on both sides of α, respectively, and Dti(α; α, α+) denoting the difference of generated responses between , we take: (Dti(α; α, α+)) argmax (Sti(Θ1, Θ2, α)) α = argmin α α as the approximation of the best choice for α. This eliminates the need of labeled data with ground truth, and avoids the data distribution shift between validation set and test set. The discovery indicates that near the best performance point, the model response tends to be more stable. This can be explained via convex hypothesis. Suppose the landscape on given task ti is convex in subspace of the parameter space that covers the candidate results of merged models, that is, for any λ [0, 1] we have: Sti(λΘ1 + (1 λ)Θ2) Sti(λΘ1) + Sti((1 λ)Θ2) This guarantee that the optimum is attained where the gradient vanishes: ΘSti(Θ) = 0 where Θ RN is the optimal parameter value. In convex function, the Hessian H(Θ) = 2 ΘSti(Θ) at the optimum Θ is positive semi-definite, which implies local stability in neighborhood around Θ. The stability can be characterized by the second-order Taylor expansion: 1 2 (Θ Θ)H(Θ)(Θ Θ) Sti(Θ) Sti(Θ) + Since H(Θ) 0, small deviations from Θ will result in small increases in the performance, ensuring relative stability on the landscape. Although the convex hypothesis is ideal, we confirm the effectiveness of our unsupervised hyper-parameter selection method in various experiments. Proof in Appendix further shows the relationship between generation consistency and model performance. We also find that while the landscape often changes between the validation and test sets and across different tasks, it remains consistent between the full test set and small subset. This suggests that we can perform unsupervised hyper-parameter selection on smaller subset of the data ti without compromising its accuracy. α = argmin (D ti (α; α, α+)) argmax (Sti (θ1, θ2, α)) α α In conclusion, the process of AdaMMS is described in Algorithm 1. 4. Experiment 4.1. Baselines Previous model merging methods cannot be directly applied to heterogeneous MLLMs with architecture difference, and our mapping method enables the fusion of heterogeneous models by transforming them into homogeneous parameter space. Therefore, all the baseline experiments are conducted under the precondition of the mapping step of our proposed method. We consider the following model merging methods as our baselines: Task Arithmetic [10] introduces the idea of task vectors and integrates them into the original pre-trained model for multi-task learning. Ties-Merging [26] further addresses interferences in Task Arithmetic by removing unnecessary parameters from Task Arithmetic [10]. This process eliminates redundant parameters and resolves symbol conflicts through Trim, Elect Sign, and Disjoint Merge steps. DARE [29] tackles the parameter conflict problem in model merging by applying drop and rescale operation before merging model weights. There are two variants of DARE: Dare-Linear and Dare-Ties, which perform different merging strategies after the drop and rescale operation. Dare-Linear performs linear interpolation, and Algorithm 1 AdaMMS Procedure Input: Original MLLMs M1, M2, their parameters Θ1, Θ2 respectively, subset of test inputs t, and the candidates of the hyper-parameter {αn} Output: Merged parameters Θout on M1s architecture 1: Define mapping that maps each parameter θ1 in M1 architecture with its corresponding parameter θ2 in M2 Section 3.1 architecture (if exists) 2: Define process Generate(Θ, t) that returns the generation responses of model with parameters Θ on inputs 3: Define function DiffCnt(Gi, Gj) that counts the number of corresponding elements in Gi and Gj that do not exactly match 4: for = 1 to do 5: 6: end for for each hyper-parameter candidate αi in {αn} do cand Merge(Θ1, Θ2; ; αi) Equation (2) cand, t) Θi Gi Generate(Θi 7: 8: 9: end for 10: for = 2 to 1 do Assuming {αn} is monotonic for each hyper-parameter candidate αi in {αn} do 11: Di DiffCnt(Gi, Gi1)+DiffCnt(Gi, Gi+1) 12: end for 13: 14: end for 15: argmini(Di) 16: Θout Θi cand 17: return Θout Dare-ties performs Ties-Merging [26]. MetaGPT [32] separate the data term and scaling coefficients in the optimization objective, which leads to taskagnostic closed-form solution for the scaling coefficient. 4.2. Models We have conducted extensive experiments on the combinations of existing open-source 7B-scale MLLMs. Since most of the top-performing open-source MLLMs are currently based on two language model architectures, Qwen2 [27] and LLaMA [20], we selected representative and outstanding MLLMs derived from each model for our main experiments. Specifically, on Qwen2 architecture, we merge LLaVA-OneVision-Qwen-7B [12] into Qwen2-VL7B [24], and on LLaMA architecture, we merge LLaVAv1.5-7B [14] into CogVLM-Chat-7B [25]. We have also conducted experiments on combinations of LLaMA-based MLLMs, including combinations LLaVA-v1.5-7B, CogVLM-Chat-7B, ShareGPT4V-7B [2] and mPLUG-Owl2-LLaMA2-7B [28]. See Appendix for more details. Model MMMUval MMEsum SeedBenchall OCRBench TextVQAval OKVQA GQA VizWizval SUM Top2 Qwen2-VL(base) LLaVA-OneVision 50.11 43.44 81.44 77. 75.85 75.44 Original Models 86.00 69.60 84.12 78.47 Baselines 51.43 49.57 61.80 59. 68.32 60.97 559.07 514.37 Task Arithmetic Ties-Merging DARE-Linear DARE-Ties MetaGPT 48.44(+1.67) 51.11(+4.34) 43.78(-3.00) 45.00(-1.78) 50.67(+3.90) 82.33(+3.09) 82.65(+3.41) 66.06(-13.18) 54.43(-24.81) 81.21(+1.97) 75.81(+0.17) 76.29(+0.64) 74.32(-1.33) 74.07(-1.58) 76.35(+0.70) 77.90(+0.10) 84.40(+6.60) 72.40(-5.40) 75.20(-2.60) 85.50(+7.70) 76.22(-5.08) 79.56(-1.74) 64.65(-16.65) 78.54(-2.76) 83.63(+2.33) 50.60(+0.10) 52.56(+2.06) 43.41 (-7.09) 49.61(-0.89) 52.24(+1.74) 62.26(+1.44) 61.84(+1.02) 55.13(-5.69) 58.51(-2.31) 61.99(+1.17) 62.76(-1.89) 66.34(+1.69) 50.18(-14.47) 58.05(-6.60) 69.16(+4.51) 536.32(-0.40) 554.75(+18.03) 469.93(-66.79) 493.41 (-43.31) 560.75(+24.03) AdaMMS 51.11(+4.34) 83.36(+4.12) 76.20(+0.55) 85.50(+7.70) 83.41(+2.11) 53.56(+3.06) 62.02(+1.20) 68.40(+3.75) 563.56(+26.84) Our Method 2 1 4 0 0 5 8 Table 1. Results on merging LLaVA-OneVision-7B into Qwen2-VL-7B. All the scores have been scaled to 0-100. SUM refers to the sum of scores on all tasks after scaling. Top2 column represents the number of tasks obtained by this method from the top two among all methods. The number in the parenthesis indicates the performance improvement compared with the average score of original models. The results in the original models that are higher than all model merging methods are highlighted in italics. Model MMMUval MMEsum SeedBenchall OCRBench TextVQAval OKVQA GQA VizWizval SUM Top2 CogVLM(base) LLaVA 34.80 35.10 59.23 66.68 61.22 60.52 56.50 31.30 Original Models 77.57 46.04 Baselines 60.82 53. 59.43 61.94 37.09 54.29 446.66 409.29 Task Arithmetic Ties-Merging DARE-Linear DARE-Ties MetaGPT 36.20 (+1.25) 34.00 (-0.95) 36.80 (+1.85) 33.60 (-1.35) 34.70 (-0.25) 65.99 (+3.03) 57.29 (-5.67) 64.08 (+1.12) 46.75 (-16.21) 59.37 (-3.59) 65.85 (+4.98) 38.97 (-21.90) 65.07 (+4.20) 58.41 (-2.46) 61.29 (+0.42) 51.20 (+7.30) 55.00 (+11.10) 47.90 (+4.00) 26.50 (-17.40) 56.40 (+12.50) 68.21 (+6.40) 59.73 (-2.08) 65.35 (+3.54) 50.48 (-11.33) 76.96 (+15.15) 61.92 (+4.80) 40.31 (-16.81) 60.96 (+3.84) 53.15 (-3.97) 60.84 (+3.72) 58.82 (-1.87) 51.97 (-8.72) 58.01 (-2.68) 49.62 (-11.07) 59.44 (-1.25) 35.70 (-9.99) 24.36 (-21.33) 36.12 (-9.57) 31.43 (-14.26) 36.97 (-8.72) 443.89 (+15.91) 361.63 (-66.35) 434.29 (+6.31) 349.94 (-78.04) 445.97 (+17.99) AdaMMS 34.90 (-0.05) 69.09 (+6.13) 64.12 (+3.25) 55.70 (+11.80) 76.90 (+15.09) 61.11 (+3.99) 60.12 (-0.57) 37.27 (-8.42) 459.21 (+31.23) Our Method 2 0 4 0 2 0 5 7 Table 2. Results on merging LLaVA-v1.5-7B into CogVLM-chat-7B. 4.3. Benchmarks To evaluate the capabilities of the merged MLLMs, we have conducted experiments on various benchmarks that cover wide range of vision-language abilities. According to the classification in [13], our benchmarks fall into three categories: (1) comprehensive-evaluation, (2) cognition and reasoning, (3) text-rich VQA. The comprehensiveevaluation tasks consist of MME [5], SeedBench [11] and VizWiz [7]. Cognition and reasoning type include MMMU [30], OK-VQA [16] and GQA [9]. Text-rich VQA type encompass OCRBench [15] and TextVQA [18]. To present the overall performance of the MLLMs in standardized manner, we apply linearly normalization to the scores across all tasks, scaling them to range from 0 to 100. Specially, the total score of MME is 2800, which we have divided by 28 for scaling purposes. 4.4. Implementation To apply our unsupervised hyper-parameter selection method in the searching step, we need to specify the candidates of α (linear interpolation coefficient). We sample the candidates in subinterval of [0, 1] with fixed granularity. Subinterval We find that merging with α 0.7 often results in collapsing language ability of the merged model, therefore we empirically limit the subinterval of α candidates to [0, 0.6] for eliminating unnecessary search. Granularity We use granularity to determine the interval between two adjacent candidates of α. In our main experiments, we choose the granularity as 0.1 to obtain satisfying performance with acceptable computation cost. Evaluation Framework We evaluated the benchmarks with LMMs-Eval [31] and VLMEvalKit [3], two opensource evaluation frameworks for MLLMs. Subset Searching Instead of conducting search across the entire available input data, we strategically utilize small subset of only 100 inputs during the search phase, reducing the data volume by at least an order of magnitude. Experimental results demonstrate that this maintains performance without compromising effectiveness. 5. Results As described in Section 4.2, the main results on distinct vision-language benchmarks are conducted with two representative MLLM pairs. Specifically, Table 1 shows the results of merging LLaVA-OneVision-7Bs parameters into Qwen2-VL-7Bs parameters and architecture, and Table 2 shows the results of merging LLaVA-v1.5-7Bs parameters into CogVLM-chat-7Bs parameters and architecture. Results of other model pairs and larger models can be found in Appendix A. Method MMMUval MMEsum SeedBenchall OCRBench TextVQAval ScienceQA OKVQA GQA VizWizval SUM EM-Full Emb-Full EM-Sample100 Emb-Sample100 51.11 50.56 51.11 51. 83.36 83.36 83.36 82.36 76.34 76.34 76.20 76.34 85.50 85.50 85.50 85.50 83.41 83.41 83.41 83.41 85.69 85.69 86.55 85.69 53.56 53.56 53.56 53. 62.02 61.44 62.02 61.44 68.40 68.40 68.40 68.40 563.70 562.57 562.12 563.56 Table 3. Results on AdaMMS when merging LLaVA-OneVison-7B into Qwen2-VL-7B using exact match (EM-) and sentence embedding (Emb-) to calculate the differences in searching phase, using full test set inputs (-Full) and sampled subset of 100 inputs (-Sample100)."
        },
        {
            "title": "Model",
            "content": "α-0.00 α-0.10 α-0.20 α-0.30 α-0.40 α-0.50 α-0."
        },
        {
            "title": "MMMUval MMEsum OCRBench",
            "content": "50.11 50.56 51.11 51.22 50.67 50.00 47.00 81.44 81.46 82.36 83.36 83.03 81.37 82.06 86.00 85.50 85.20 84.40 80.70 76.40 71.20 Oracle AdaMMS 51.22(0.30) 51.11(0.20) 83.36(0.30) 83.36(0.30) 85.50(0.10) 85.50(0.10) Table 4. Results with α granularity of 0.1 when merging LLaVAOneVision-7B into Qwen2-VL-7B. The values in parentheses indicate the selected α. Oracle represents the best possible performance (upper bound) for each task, while AdaMMS shows the results achieved by our unsupervised selection method. Original Models LLaVA-OneVision Qwen2-VL Merging-Base 69.60 86.00 69.60 86. LLaVA-OneVision Qwen2-VL Baselines Task Arithmetic Ties-Merging DARE-Linear DARE-Ties MetaGPT 68.10 56.10 63.90 64.40 38.40 Linear Interpolation α-0.10 α-0.20 α-0.30 α-0.40 α-0.50 α-0.60 AdaMMS 70.60 71.70 69.90 67.00 62.00 54.40 Our Method 70.60 77.90 84.40 72.40 75.20 85.50 85.50 85.20 84.40 80.70 76.40 71.30 85.50 Table 5. Results on OCRBench when merging LLaVA-OneVision7B and Qwen2-VL-7B. AdaMMS addresses the challenges of merging for heterogeneous MLLMs and outperforms strong baselines. As demonstrated in Table 1 and Table 2, our proposed AdaMMS model merging method achieves the highest cumulative performance scores across both MLLM pairs, indicating its effectiveness in merging heterogeneous MLLMs. Ranks among the top two performs in 8 out of 9 metrics in Table 1 and 7 metrics in Table 2, demonstrating its consistent ability to adaptively improve performance across most tasks. Moreover, our method stands out as the only approach where the merged model significantly outperforms both pre-merged models, achieving an average gain of +3.36 (total gain of +26.84) over Qwen2-VL and +3.90 over CogVLM across 8 tasks. Given that most baseline methods employ supervised search techniques that incorporate additional information, our unsupervised search approach demonstrates exceptional performance on vision-language benchmarks, as detailed illustrated in Appendix D. The proposed unsupervised hyper-parameter selection method is able to select near-optimal α. We evaluate the performance across different coefficient values α, comparing the results obtained through our unsupervised hyper-parameter selection method against those achieved with the optimal α chosen by the actual best results, which serves as the theoretical upper bound. As shown in Table 4, our method consistently performs remarkably close to this upper bound, with maximum deviation of only 0.5 points. These results demonstrate the capability of our method to accurately identify near-optimal α values, achieving performance levels approaching the theoretical best. Note that on the OCRBench and TextVQA benchmarks, all model merging methods, including AdaMMS, show performance drop compared to the original base model. We hypothesize that this is due to the large performance gap between the two original models on these benchmarks. Even though, AdaMMS still outperforms most of the baselines, showing the robustness of our method on various scenarios. 6. Analysis 6.1. Different Factors for Calculating Generation"
        },
        {
            "title": "Consistency",
            "content": "We conducted analytical experiments on our generation consistency calculation methods, focusing on two key factors: the choice between using 100-sample subset versus the complete dataset, and the selection of evaluation metrics. In the searching step of our method, we employed an exact match metric to calculate DiffCnt in Algorithm 1, Figure 2. Results on merging LLaVA-v1.5-7B into Qwen2-VL7B. The α with the best perfo, bb=0 0 461 346rmance are the same as the α with the fewest response differences. which serves as our generation consistency indicator for model performance prediction. Given that exact match is binary, rigorous evaluation metric, we explored an alternative, more flexible approach to measure generation consistency. Specifically, we computed the cosine similarity between sentence embeddings generated by all-MiniLM-L6v2 [17], which was used to calculate DiffCnt. The analysis results are presented in Table 3. Although embeddings theoretically offer more fine-grained semantic representations, our results demonstrate that the embedding-based metric performs comparably to the exact match metric. Furthermore, our experiments confirm that sampling 100 instances achieves results nearly equivalent to the complete dataset. 6.2. Merging with Large Performance Gap As discussed in Section 5, all model merging methods experience performance drop after the merging on two benchmarks, OCRBench and TextVQA. It shows that merging model with significant lower performance into the base model will decrease the performance on the task. Conversely, in Table 5, the merging from Qwen2-VL-7B to LLaVA-OneVision-7B shows that merging model with significant higher performance into the base model will not necessarily improve the model performance. And in this case, AdaMMS is the only model merging method that resists the performance drop after merging. In general, we observed that original models with similar performance tends to benefit from model merging, while original models with large performance gap do not. 6.3. Asymmetry in the Parameter Space of Heterogeneous Models In Section 4.4, we discussed that merging with large α often results in collapsing language ability. To validate our choice of the subinterval [0, 0.6] in determining candidates of α, we demonstrate this phenomenon in Figure 3, which shows Figure 3. Model responses with the change of α in linear interpolation. Similar colors indicate similar responses. that the model generates consistently near the parameters of the base model with small α, and collapses gradually with larger α. We attribute the phenomenon to the asymmetry in the parameter space, as the two original models have unequal status that comes from the choice of base architecture. 6.4. Selection of Granularity for α To validate our choice of the granularity in Section 4.4, we conducted additional experiments with various granularities of α candidates on MME and OCRBench when merging LLaVA-OneVison-7B into Qwen2-VL-7B. As shown in Appendix G, the result shows that the difference of selected α and model performance do not change significantly with different granularities. This shows that our choice of granularity as 0.1 would result in comparable performance, with less computation cost. 7. Conclusion In this work, we propose novel model merging method AdaMMS to address the challenges in merging heterogeneous MLLMs. We first connect the parameters of different MLLMs through mapping function, enabling merging operations. We then apply linear interpolation to the mapped model weights to adaptively optimize performance across tasks. To optimize the interpolation coefficient without labeled data, we introduce an unsupervised hyperparameter searching method based on our discovery in the parameter space: model performance can be estimated through the generation consistency. We demonstrate that 100 data samples are enough to search for near-optimal coefficients effectively. Extensive experimental results show that AdaMMS outperforms existing model merging methods for MLLMs and successfully addresses the challenges in merging heterogeneous MLLMs. We hope that our work mitigates the limitations of heterogeneous model merging methods and provides valuable insights for future research on unsupervised performance estimation and optimization."
        },
        {
            "title": "Acknowledgment",
            "content": "This work is supported by the National Key R&D Program of China (2022ZD0160502) and the National Natural Science Foundation of China (No. 62276152)."
        },
        {
            "title": "References",
            "content": "[1] Chi Chen, Yiyang Du, Zheng Fang, Ziyue Wang, Fuwen Luo, Peng Li, Ming Yan, Ji Zhang, Fei Huang, Maosong Sun, and Yang Liu. Model composition for multimodal large language models. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), 2024. [2] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. Sharegpt4v: Improving large multi-modal models with better captions. arXiv preprint arXiv:2311.12793, 2023. [3] Haodong Duan, Junming Yang, Yuxuan Qiao, Xinyu Fang, Lin Chen, Yuan Liu, Amit Agarwal, Zhe Chen, Mo Li, Yubo Ma, Hailong Sun, Xiangyu Zhao, Junbo Cui, Xiaoyi Dong, Yuhang Zang, Pan Zhang, Jiaqi Wang, Dahua Lin, and Kai Chen. Vlmevalkit: An open-source toolkit for evaluating large multi-modality models, 2024. [4] Clementine Fourrier, Nathan Habib, Alina Lozovskaya, Konrad Szafer, and Thomas Wolf. Open llm leaderboard v2. https://huggingface.co/spaces/openllmleaderboard/open_llm_leaderboard, 2024. [5] Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, et al. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. [6] Charles Goddard, Shamane Siriwardhana, Malikeh Ehghaghi, Luke Meyers, Vlad Karpukhin, Brian Benedict, Mark McQuade, and Jacob Solawetz. Arcees mergekit: toolkit for merging large language models. arXiv preprint arXiv:2403.13257, 2024. [7] Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from In Proceedings of the IEEE conference on blind people. computer vision and pattern recognition, pages 36083617, 2018. [8] Wenyi Hong, Weihan Wang, Ming Ding, Wenmeng Yu, Qingsong Lv, Yan Wang, Yean Cheng, Shiyu Huang, Junhui Ji, Zhao Xue, et al. CogVLM2: Visual language modarXiv preprint els for image and video understanding. arXiv:2408.16500, 2024. [9] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 67006709, 2019. [10] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi, and Ali Farhadi. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089, 2022. [11] Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1329913308, 2024. [12] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. Llava-onevision: Easy visual task transfer. arXiv preprint arXiv:2408.03326, 2024. [13] Jian Li and Weiheng Lu. large language models. survey on benchmarks arXiv preprint of multimodal arXiv:2408.08632, 2024. [14] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. Improved baselines with visual instruction tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2629626306, 2024. [15] Yuliang Liu, Zhang Li, Biao Yang, Chunyuan Li, Xucheng Yin, Cheng-lin Liu, Lianwen Jin, and Xiang Bai. On the hidden mystery of ocr in large multimodal models. arXiv preprint arXiv:2305.07895, 2023. [16] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: visual question answering In Proceedings benchmark requiring external knowledge. of the IEEE/cvf conference on computer vision and pattern recognition, pages 31953204, 2019. [17] Nils Reimers and Iryna Gurevych. Sentence-bert: Sentence embeddings using siamese bert-networks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing. Association for Computational Linguistics, 2019. [18] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards vqa models that can read. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 83178326, 2019. [19] Yi-Lin Sung, Linjie Li, Kevin Lin, Zhe Gan, Mohit Bansal, and Lijuan Wang. An empirical study of multimodal model merging. arXiv preprint arXiv:2304.14933, 2023. [20] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothee Lacroix, Baptiste Roziere, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard Grave, and Guillaume Lample. Llama: Open and efficient foundation language models. [21] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Information Processing Systems. Curran Associates, Inc., 2017. [22] Fanqi Wan, Xinting Huang, Deng Cai, Xiaojun Quan, Wei Bi, and Shuming Shi. Knowledge fusion of large language models, 2024. [23] Fanqi Wan, Ziyi Yang, Longguang Zhong, Xiaojun Quan, Xinting Huang, and Wei Bi. Knowledge fusion of chat llms: preliminary technical report, 2024. [24] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024. [25] Weihan Wang, Qingsong Lv, Wenmeng Yu, Wenyi Hong, Ji Qi, Yan Wang, Junhui Ji, Zhuoyi Yang, Lei Zhao, Xixuan Song, et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. [26] Prateek Yadav, Derek Tam, Leshem Choshen, Colin Raffel, and Mohit Bansal. TIES-Merging: Resolving interference when merging models. Advances in Neural Information Processing Systems, 36, 2024. [27] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 technical report, 2024. [28] Qinghao Ye, Haiyang Xu, Jiabo Ye, Ming Yan, Anwen Hu, Haowei Liu, Qi Qian, Ji Zhang, and Fei Huang. mplugowi2: Revolutionizing multi-modal large language model with modality collaboration. In 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 1304013051. IEEE, 2024. [29] Le Yu, Bowen Yu, Haiyang Yu, Fei Huang, and Yongbin Li. Language models are super mario: Absorbing abilities from homologous models as free lunch. In Forty-first International Conference on Machine Learning, 2024. [30] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. MMMU: massive multi-discipline multimodal understanding and reasoning benchmark for exIn Proceedings of the IEEE/CVF Conference pert agi. on Computer Vision and Pattern Recognition, pages 9556 9567, 2024. [31] Kaichen Zhang, Bo Li, Peiyuan Zhang, Fanyi Pu, Joshua Adrian Cahyono, Kairui Hu, Shuai Liu, Yuanhan Zhang, Jingkang Yang, Chunyuan Li, and Ziwei Liu. Lmmseval: Reality check on the evaluation of large multimodal models, 2024. [32] Yuyan Zhou, Liang Song, Bingning Wang, and Weipeng Chen. MetaGPT: Merging large language models usarXiv preprint ing model exclusive task arithmetic. arXiv:2406.11385, 2024. A. Results on Additional Model Pairs We conducted experiments on additional model pairs, summarized in Table 9, which highlights the cumulative performance gains across tasks for six different model pairs. The model pairs include: (1) merging LLaVA-OneVision [12] into Qwen2-VL [24]  (Table 1)  , (2) merging LLaVA-v1.5 [14] into CogVLM [25]  (Table 2)  , (3) merging mPLUGOwl2 into LLaVA-v1.5, (4) merging LLaVA-v1.5 into mPLUG-Owl2 [28]  (Table 11)  , (5) merging CogVLM into mPLUG-Owl2  (Table 12)  and (6) merging mPLUG-Owl2 into CogVLM  (Table 13)  . The performance gain for each task is computed as the difference between the performance of our method (or baselines) and the average performance of the two original models, with positive values indicating an improvement. In Table 9, the SUM column presents the total performance gains across all tasks, where AdaMMS outperforms all baselines, achieving +91.92 performance gain, and consistently ranks among the top two in performance gains across all benchmarks. It is noteworthy that on GQA [9] and VizWiz [7] benchmarks in Table 11 and Table 12, all model merging methods experience performance drop. We attribute this decline to the significant performance gap between the original models on these benchmarks. In these scenarios, AdaMMS demonstrates the smallest performance decrease among them. In Table 11, Table 12 and Table 13, AdaMMS obtains the second best result in the sum of all benchmarks, with small gap compared to the best baseline. To investigate the effect of altering base models on performances, we analyze experiments on merging the same model pair with different base models. For the model pair of mPLUG-OWl2 and CogVLM, results in Table 12 use mPLUG-Owl2 as the base model, and results in Table 13 use CogVLM as the base model. On benchmarks where the original models exhibit significant performance gap, such as OCRBench [15] and TextVQA [18], model merging methods, including AdaMMS, achieve only marginal In contrast, on benchmarks performance improvements. where the original models have comparable performance, AdaMMS consistently enhances the base models performance (with the exception of GQA [9] for the mPLUGOwl2 architecture), irrespective of the choice of base model. Notably, even when merging weaker model into stronger one for specific task, AdaMMS can sometimes boost the stronger models performance. For instance, this effect is observed on SEEDBench [11], OKVQA [16], and GQA [9] in Table 13. These results highlight that our model merging technique can further optimize the performance of strong model, even when another model demonstrates weaker performance on the same task. Additionally, to demonstrate the effectiveness of our method on larger models, we conducted experiments on Cambrian and Yi-VL with 34B language model size. Table 6 shows that AdaMMS also merges the abilities in larger MLLMs effectively. Model Cambrian(base) Yi-VL AVG AdaMMS OCRBench MME 72.50 58.70 73.65 29.70 73.08 44.20 74.07 59. Table 6. Results on merging Yi-VL into Cambrian. B. Implementation Details of AdaMMS The implementation details of AdaMMS are as follows: Mapping In this step, we identify parameters in the language models that account for additional weights. For CogVLM [25], all weights within the visual experts in the attention mechanism (including the QKV matrix and the FFN of the visual expert) are treated as additional weights. For mPLUG-Owl2 [28], vision representation weights within the Modality-Adaptive Modules (such as the decoupled vision layer-norm and KV matrix) are considered additional weights. For different vision encoders, the vision encoder weights of the base model are retained as the final weights after merging, regardless of the vision encoder in the other model. Merging During this step, we first merge the weights in the language model of the base model. If the weights are not classified as additional weights in the Mapping step, they are merged using linear interpolation or other baseline merging techniques. For weights categorized as additional weights, we check whether the other model has duplicated the same weights. Based on this, we (1) merge the weights if duplicates exist, or (2) retain the original weights in the base model if no duplicates are found. Searching In the final step, we randomly select subset of 100 test inputs to determine the optimal α. For each α candidate, we generate model responses for the selected inputs. To select the best α, we apply the Exact Match metric for the total difference score: for each input, if the merged models response with given α matches the response with adjacent α values, the difference score is 0; otherwise, it is 1. The total difference score is the sum of scores across all inputs in the subset. The α with the lowest total difference score is selected as the final choice. Note that the small subset of 100 inputs is randomly sampled using the method in LMMs-Eval framework [31]. We have repeated the sampling process to ensure that the randomness in sampling does not affect the performance of our method. C. Evaluation Details We utilize LMMs-Eval [31] and VLMEvalKit [3], open-source evaluation frameworks for MLLMs, two to asProof. Using the notation in Section 3.3, for an arbitrary task ti, let Sti(α) be the ratio of correct answer at position α, and Dti(α; α) be the ratio of the difference in generated responses between position α and its adjacent candidate α. Since the difference in Sti(α) is only influenced by the subset of generated responses where the correctness status changes (i.e., transitions between correct and incorrect), we have Sti(α) Sti (α) Dti(α; α). For the same reason with α+, we can prove Sti(α) Sti(α) + Sti(α) Sti(α+) 2Dti(α; α, α+). Therefore, higher generation consistency with small Dti(α; α, α+) implies higher model performance Sti(α), due to its convexity. G. Experimental Results in Granularity for α Figure 4 presents the result of AdaMMS at different granularities of α. The point in stars indicates the best α by our unsupervised parameter selection method. The result shows that these granualities in {0.02, 0.05, 0.10} behave similarly in terms of the final performance, indicating the robustness of AdaMMS. Therefore, in practice we choose larger α so that we have fewer α candidates, which reduces the computation cost. Specifically, for evaluating MMMU sess our models. [30], MME [5], SEEDBench [11], OCRBench [15], and TextVQA [18] within the Qwen2-VL [24] architecture, we use the VLMEvalKit framework, while LMMs-Eval is employed for the others. To ensure consistency with the reported results for LLaVA and mPLUG-Owl2 on OK-VQA [16], we adapted the prompt template in the evaluation framework, as detailed in Table 7. Other prompt templates remains the same in the evaluation frameworks. D. Comparing Supervised and Unsupervised We compared AdaMMS with baseline merging methods with supervised hyper-parameter selection. Due to the absence of separate test sets, we trained the supervised baseline on either subset or the entirety of the evaluation set. This implies that the supervised baseline was in more favorable position compared to our method, as our method does not have access to the groudtruth labels. Table 8 shows that AdaMMS still outperforms it, indicating the superiority of our unsupervised method. E. Intermediate Results in Searching We present an example of the intermediate results during the selection of α. As shown in Figure 14, AdaMMS effectively identifies near-optimal α, achieving performance close to the best possible outcome. Specifically, our unsupervised hyper-parameter selection method successfully chooses the optimal α candidate in half of the benchmarks and maintains deviation of no more than 0.2 from the best α in the remaining cases. Figure 5 illustrates the relationship between model performance and generation consistency across MMMU, MME, SeedBench, and OCRBench when merging LLaVAOneVision into Qwen2-VL. The observed trends validate our approach in the search step, where model performance is approximated using generation consistency without relying on labeled data. Notably, for these tasks, the α selected by our method corresponding to the highest generation consistency deviates from the α achieving the best performance by no more than 0.1, showing that our hyper-parameter selection method achieves near-optimal performance. Framework LMMs-Eval Base Model LLaVA mPLUG-Owl Prompt Answer the question using single word or phrase. Table 7. Altered prompt for evaluation on OK-VQA. F. Supplementary Proof We provide the following proof as the theoretical justification for relationship between generation consistency correlates and model performance. Model AdaMMS Ties-Merging Ties-Merging (supervised with 100 eval. samples) Ties-Merging (supervised with all eval. data) MMMUval MMEsum SeedBenchall OCRBench TextVQAval OKVQA GQA VizWizval 34.90 34.00 37.20 37.20 60.12 51.97 55.81 57.99 76.90 59.73 76.50 76.55 37.27 24.36 37.98 38.21 61.11 40.31 61.45 61.45 69.09 57.29 57.29 63. 64.12 38.97 63.12 65.43 55.70 55.00 55.90 55.90 Sum 459.21 361.63 445.25 456.69 Diff +31.23 -66.35 +17.27 +28.71 Table 8. AdaMMS and Ties-Merging with supervised hyper-parameter selection via validation set. Figure 4. Results on linear interpolation at different granularities of α when merging LLaVA-OneVison-7B into Qwen2-VL-7B-7B. (Left: MME, Right: OCRBench) Model MMMUval MMEsum SeedBenchall OCRBench TextVQAval OKVQA GQA VizWizval SUM Top2 Task Arithmetic Ties-Merging DARE-Linear DARE-Ties MetaGPT AdaMMS 13.21 -3.32 8.15 -14.83 1.44 17.68 21.53 -24.94 -2.35 -60.56 -2.93 17. 14.54 -27.34 10.58 -6.96 -4.02 12.02 -1.80 1.20 -12.3 -47.50 15.30 13.60 -3.74 -31.59 -19.23 -47.12 0.37 18.43 13.88 -23.23 5.41 -31.08 -6.75 13.40 -2.95 -29.70 -12.76 -26.32 -23.69 1.40 -7.29 -29.20 -21.09 -32.45 -16.73 -2. 47.45 -168.05 -43.53 -266.76 -36.94 91.92 7 0 0 0 2 9 Table 9. Results of the performance gain sum among six model pairs reported in our paper, as described in Appendix A. The performance gain for each task is computed as the difference between the performance of our method (or baselines) and the average performance of the two original models, with positive values indicating an improvement. Model Unsupervised MMMUval MMEsum SeedBenchall OCRBench TextVQAval OKVQA GQA VizWizval SUM Top LLaVA(base) mPLUG-Owl2 Task Arithmetic Ties-Merging DARE-Linear DARE-Ties MetaGPT AdaMMS 35.10 34. 66.68 62.80 60.52 59.41 Original Models 31.30 34.10 46.04 55.13 Baselines 53.42 60.98 61.94 56.11 54.29 32.07 409.29 395.50 36.00 (+1.00) 33.60 (-1.40) 36.00 (+1.00) 31.70 (-3.30) 35.30 (+0.30) 67.00 (+2.26) 62.14 (-2.60) 67.00 (+2.26) 59.81 (-4.93) 67.62 (+2.88) 61.45 (+1.48) 60.32 (+0.35) 61.41 (+1.44) 60.06 (+0.09) 61.46 (+1.49) 30.40 (-2.30) 30.10 (-2.60) 30.70 (-2.00) 29.50 (-3.20) 30.60 (-2.10) 45.75 (-4.84) 42.85 (-7.73) 45.84 (-4.74) 41.90 (-8.69) 45.80 (-4.79) 56.79 (-0.41) 52.46 (-4.74) 57.06 (-0.14) 46.00 (-11.20) 56.54 (-0.66) 59.68 (+0.66) 58.37 (-0.66) 59.56 (+0.54) 57.51 (-1.52) 59.41 (+0.38) 56.49 (+13.31) 51.30 (+8.12) 55.90 (+12.72) 53.27 (+10.09) 56.66 (+13.48) 413.56 (+11.17) 391.14 (-11.25) 413.47 (+11.08) 379.75 (-22.64) 413.39 (+11.00) 38.30 (+3.30) 67.01 (+2.27) 61.82 (+1.85) 31.00 (-1.70) 46.49 (-4.09) 55.60 (-1.60) 61.81 (+2.79) 54.64 (+11.46) 416.67 (+14.28) Our Method 5 0 2 0 7 Table 10. Results on merging mPLUG-Owl2-7B into LLaVA-v1.5-7B. Model Unsupervised MMMUval MMEsum SeedBenchall OCRBench TextVQAval OKVQA GQA VizWizval SUM Top2 mPLUG-Owl2(base) LLaVA 34.90 35.10 62.80 66. 59.41 60.52 Task Arithmetic Ties-Merging DARE-Linear DARE-Ties MetaGPT AdaMMS 36.90(+1.90) 36.90(+1.90) 36.20(+1.20) 35.30(+0.30) 36.00(+1.00) 63.17(-1.57) 64.20(-0.54) 62.99(-1.75) 60.37(-4.37) 64.24(-0.50) 60.44(+0.47) 60.13(+0.16) 60.41(+0.44) 58.36(-1.61) 60.23(+0.26) 37.60(+2.60) 64.61(-0.13) 60.02(+0.05) Original Models 34.10 31.30 Baselines 33.00(+0.30) 34.40(+1.70) 32.60(-0.10) 32.00(-0.70) 33.90(+1.20) Our Method 32.20(-0.50) 55.13 46.04 60.98 53.42 56.11 61.94 32.07 54.29 395.50 409. 55.40(+4.81) 54.50(+3.91) 55.15(+4.56) 51.65(+1.06) 55.83(+5.24) 63.87(+6.67) 62.92(+5.72) 63.47(+6.27) 58.08(+0.88) 62.88(+5.68) 56.97(-2.06) 57.55(-1.48) 56.73(-2.30) 55.57(-3.46) 56.53(-2.50) 33.70(-9.48) 33.18(-10.00) 33.35(-9.83) 31.03(-12.15) 33.35(-9.83) 403.45(+1.05) 403.78(+1.38) 400.90(-1.50) 382.36(-20.04) 402.96(+0.56) 55.84(+5.25) 63.13(+5.93) 56.98(-2.05) 33.39(-9.79) 403.77(+1.37) 4 4 2 0 3 Table 11. Results on merging LLaVA-v1.5-7B into mPLUG-Owl2-7B. Figure 5. Generation consistency and model performance (score) for MME, MMMU, OCRBench and SeedBench when merging LLaVAOneVision-7B into Qwen2-VL-7B. Generation consistency is calculated as the reciprocal of the sum of different responses from models with adjacent α candidates. The horizontal axis is the α of the linear interpolation. Model Unsupervised MMMUval MMEsum SeedBenchall OCRBench TextVQAval OKVQA GQA VizWizval SUM Top2 mPLUG-Owl2(base) CogVLM 34.90 34.80 62.80 59. 59.41 61.22 Task Arithmetic Ties-Merging DARE-Linear DARE-Ties MetaGPT AdaMMS 38.80(+3.95) 27.9(-6.95) 37.60(+2.75) 32.00(-2.85) 31.30(-3.55) 64.65(+3.63) 48.96(-12.06) 62.44(+1.42) 57.90(-3.12) 56.81(-4.21) 60.85(+0.53) 52.32(-8.00) 59.81(-0.51) 57.62(-2.70) 50.81(-9.51) 39.10(+4.25) 64.65(+3.63) 60.16(-0.16) Original Models 34.10 56.50 Baselines 31.50(-13.80) 24.30(-21.00) 30.90(-14.40) 24.10(-21.20) 29.30(-16.00) Our Method 30.60(-14.70) 55.13 77.57 60.98 60.82 56.11 59.43 32.07 37. 395.50 446.66 56.99(-9.36) 42.10(-24.25) 56.41(-9.94) 43.84(-22.51) 37.96(-28.39) 60.93(+0.03) 54.15(-6.75) 61.07(+0.17) 51.56(-9.34) 43.02(-17.88) 54.44(-3.33) 43.02(-14.75) 54.11(-3.66) 52.04(-5.73) 34.12(-23.65) 32.76(-1.82) 27.56(-7.02) 32.42(-2.16) 25.67(-8.91) 15.84(-18.74) 400.92(-20.16) 320.31(-100.77) 394.76(-26.32) 344.73(-76.35) 299.16(-121.92) 55.88(-10.47) 62.11(+1.21) 55.61(-2.16) 32.69(-1.89) 400.80(-20.28) 8 0 1 0 9 Table 12. Results on merging CogVLM-7B into mPLUG-Owl2-7B. Model Unsupervised MMMUval MMEsum SeedBenchall OCRBench TextVQAval OKVQA GQA VizWizval SUM Top2 Original Models CogVLM(base) mPLUG-OWI2 Task Arithmetic Ties-Merging DARE-Linear DARE-Ties MetaGPT AdaMMS 34.80 34.90 59.23 62.80 61.22 59. 38.30(+3.45) 34.60(-0.25) 39.20(+4.35) 29.00(-5.85) 34.90(+0.05) 72.11(+11.09) 53.54(-7.48) 68.80(+7.78) 53.89(-7.12) 61.54(+0.52) 67.24(+6.92) 61.73(+1.41) 66.66(+6.34) 61.61(+1.30) 62.93(+2.62) 38.10(+3.25) 62.48(+1.46) 66.79(+6.48) 56.50 34.10 Baselines 51.90(+6.60) 50.70(+5.40) 50.90(+5.60) 42.90(-2.40) 57.30(+12.00) Our Method 56.30(+11.00) 77.57 55.13 60.82 60.98 59.43 56.11 37.09 32. 446.66 395.50 70.68(+4.33) 66.65(+0.30) 70.35(+4.00) 63.46(-2.89) 77.18(+10.83) 63.59(+2.69) 58.19(-2.71) 63.26(+2.36) 54.34(-6.56) 61.55(+0.65) 59.98(+2.21) 52.66(-5.11) 58.80(+1.03) 55.54(-2.23) 59.93(+2.16) 37.16(+2.58) 33.92(-0.66) 36.80(+2.22) 33.96(-0.62) 37.15(+2.57) 460.96(+39.88) 411.99(-9.09) 454.77(+33.69) 394.70(-26.38) 452.48(+31.40) 76.89(+10.54) 61.71(+0.81) 59.96(+2.19) 37.33(+2.75) 459.56(+38.48) 7 0 3 0 6 Table 13. Results on merging mPLUG-Owl2-7B into CogVLM-7B. Model MMMUval MMEsum SeedBenchall OCRBench TextVQAval OKVQA GQA VizWizval SUM Qwen2-VL(base) LLaVA-OneVision AVG α-0.1 α-0.2 α-0.3 α-0.4 α-0.5 α-0.6 AdaMMS Selected α Distance with the best α 50.11 43.44 46.78 50.56 51.11 51.22 50.67 50.00 47. 51.11 0.2 0.1 81.44 77.04 79.24 81.46 82.36 83.36 83.03 81.37 82.06 83.36 0.3 Original Models 86.00 69.60 77.80 75.85 75.44 75.65 Linear Interpolation 76.20 76.23 76.34 76.06 75.63 74.76 76.20 0.1 0. 85.50 85.20 84.40 80.70 76.40 71.30 Our Method 85.50 0.1 0 84.12 78.47 81.30 83.41 81.74 78.43 71.66 59.13 39.37 83.41 0.1 51.43 49.57 50.50 53.56 54.76 52.03 49.83 44.96 40.31 61.80 59.84 60.82 62.02 62.05 61.44 60.09 55.53 54.11 68.32 60.97 64.65 68.40 67.12 63.91 58.43 52.60 46. 559.07 514.37 536.72 561.11 560.57 551.13 530.47 495.62 455.30 53.56 62.02 68.40 563. 0.1 0.1 0.1 0.1 0.1 0 - - Table 14. Intermediate results on different α candidates in the linear interpolation of AdaMMS, and the α selected by our unsupervised hyper-parameter selection method on merging LLaVA-OneVision-7B into Qwen2-VL-7B. AVG indicates the average performance of the two original models."
        }
    ],
    "affiliations": [
        "Dept. of Comp. Sci. & Tech., Institute for AI, Tsinghua University, Beijing, China",
        "Institute for AI Industry Research (AIR), Tsinghua University, Beijing, China",
        "Institute of Intelligent Computing, Alibaba Group",
        "Jiangsu Collaborative Innovation Center for Language Competence, Jiangsu, China",
        "ModelTC Open Source Organization, Beijing, China",
        "School of Software Microelectronics, Peking University, Beijing, China",
        "Shanghai Artificial Intelligence Laboratory, Shanghai, China",
        "State Key Laboratory of Multimedia Information Processing, Peking University, Beijing, China"
    ]
}