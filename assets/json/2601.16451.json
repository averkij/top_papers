{
    "paper_title": "VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology",
    "authors": [
        "Peixian Liang",
        "Songhao Li",
        "Shunsuke Koga",
        "Yutong Li",
        "Zahra Alipour",
        "Yucheng Tang",
        "Daguang Xu",
        "Zhi Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Accurate semantic segmentation for histopathology image is crucial for quantitative tissue analysis and downstream clinical modeling. Recent segmentation foundation models have improved generalization through large-scale pretraining, yet remain poorly aligned with pathology because they treat segmentation as a static visual prediction task. Here we present VISTA-PATH, an interactive, class-aware pathology segmentation foundation model designed to resolve heterogeneous structures, incorporate expert feedback, and produce pixel-level segmentation that are directly meaningful for clinical interpretation. VISTA-PATH jointly conditions segmentation on visual context, semantic tissue descriptions, and optional expert-provided spatial prompts, enabling precise multi-class segmentation across heterogeneous pathology images. To support this paradigm, we curate VISTA-PATH Data, a large-scale pathology segmentation corpus comprising over 1.6 million image-mask-text triplets spanning 9 organs and 93 tissue classes. Across extensive held-out and external benchmarks, VISTA-PATH consistently outperforms existing segmentation foundation models. Importantly, VISTA-PATH supports dynamic human-in-the-loop refinement by propagating sparse, patch-level bounding-box annotation feedback into whole-slide segmentation. Finally, we show that the high-fidelity, class-aware segmentation produced by VISTA-PATH is a preferred model for computational pathology. It improve tissue microenvironment analysis through proposed Tumor Interaction Score (TIS), which exhibits strong and significant associations with patient survival. Together, these results establish VISTA-PATH as a foundation model that elevates pathology image segmentation from a static prediction to an interactive and clinically grounded representation for digital pathology. Source code and demo can be found at https://github.com/zhihuanglab/VISTA-PATH."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 3 2 ] . [ 1 1 5 4 6 1 . 1 0 6 2 : r VISTA-PATH: An interactive foundation model for pathology image segmentation and quantitative analysis in computational pathology Peixian Liang1, Songhao Li2, Shunsuke Koga1, Yutong Li3, Zahra Alipour1, Yucheng Tang4, Daguang Xu4, and Zhi Huang1,5,# 1Department of Pathology and Laboratory Medicine, University of Pennsylvania, PA 19104, USA 2Department of Electrical and System Engineering, University of Pennsylvania, PA 19104, USA 3Department of Biomedical Engineering, Georgia Institute of Technology and Emory University, Atlanta, GA 30332, USA 4NVIDIA Corporation, USA 5Department of Biostatistics, Epidemiology and Informatics, University of Pennsylvania, PA 19104, USA # To whom the correspondence should be addressed: Zhi Huang (zhi.huang@pennmedicine.upenn.edu)"
        },
        {
            "title": "ABSTRACT",
            "content": "Accurate semantic segmentation for histopathology image is crucial for quantitative tissue analysis, expert interpretation, and downstream clinical modeling. Recent segmentation foundation models have improved generalization through large-scale pretraining, yet remain poorly aligned with pathology because they either (i) lack of domain expertise in pathology; or (ii) treat segmentation as static visual prediction task. Here we present VISTA-PATH, an interactive, class-aware pathology segmentation foundation model designed to resolve heterogeneous multi-tissue structures, incorporate expert feedback, and produce pixel-level segmentation that are directly meaningful for both quantitative measurements and clinical interpretation. VISTA-PATH jointly conditions segmentation on visual context, semantic tissue descriptions, and optional expert-provided spatial prompts, enabling precise multi-class segmentation across heterogeneous pathology images. To support this paradigm, we curate VISTA-PATH Data, large-scale pathology segmentation corpus comprising over 1.6 million imagemasktext triplets spanning 9 organs and 93 tissue classes. Across extensive held-out and external benchmarks, VISTA-PATH consistently outperforms existing segmentation foundation models, particularly in settings with complex, overlapping tissue structures. It exceeds the second-best method by 2.730.9% Dice score across 9 organs on the held-out test set, and by 3.316.3% Dice across 13 organs on external datasets. Most importantly, VISTA-PATH supports dynamic human-in-the-loop refinement by propagating sparse, patch-level bounding-box annotation feedback into whole-slide, pixel-level segmentation. Across 9 evaluated datasets, VISTA-PATH improves Dice scores by 15.3%46.8% via the proposed interactive refinement. Finally, we show that the high-fidelity, class-aware segmentation produced by VISTA-PATH is preferred model for computational pathology. It improves tissue microenvironment analysis through proposed Tumor Interaction Score (TIS), which exhibits strong and significant associations with patient survival. On the TCGA-COAD cohort, VISTA-PATH improves C-index by 16.820.7% over standard multiple instance learning (MIL) model. Together, these results establish VISTA-PATH as foundation model that elevates pathology image segmentation from static prediction to an interactive and clinically grounded representation for digital pathology. Source code and demo can be found at https://github.com/zhihuanglab/VISTA-PATH."
        },
        {
            "title": "1 Main",
            "content": "Accurate pathology image segmentation is crucial step for both expert interpretation and computational modeling of disease in digital pathology, and is instrumental from tumor delineation and microenvironment profiling to biomarker discovery and prognostic modeling16. However, existing pathology segmentation models or pipelines remain narrowly focused on specific organs, tissue types, or datasets710. While such specialized models can perform well in controlled settings, they often fail to generalize to new cohorts and impose heavy annotation burden on users, who must fine-tune or retrain models for each new domain. Recent advances in segmentation foundation models have substantially improved generalization through large-scale pretraining across diverse datasets and imaging domains4, 1113. In the biomedical image domain, models such as MedSAM14 and BiomedParse15 represent important steps toward universal segmentation by reducing dataset dependency and enabling zero-shot inference. These models demonstrate that large-scale pretraining can overcome many of the 1/36 VISTA-PATH tissuelab.org limitations of dataset-specific pipelines. However, they were largely designed around radiology or natural-image tasks and therefore do not fully meet the practical demands of segmenting the heterogeneous patterns present in hematoxylin and eosin (H&E)-stained histopathology images. Therefore, there remains critical need to develop highly reliable segmentation foundation model for diverse pathology images that can help facilitate many downstream analysis tasks. However, three fundamental challenges constrain the capabilities of existing segmentation models in pathology. First, histopathology images exhibit extreme semantic and morphological heterogeneity across organs, tissue classes, staining protocols, and datasets1720. Models trained primarily on radiology or narrowly defined pathology datasets struggle to maintain consistent multi-class performance in such diverse settings15, 21. Second, due to the high diversity of tissue microenvironment and staining & scanning protocols, existing pathology segmentation models perform poorly on out-of-distribution data; most of these models treat segmentation as static, one-shot prediction and provide limited mechanisms for incorporating expert feedback during or after inference22. At present, pathologists routinely inspect, revise, and refine segmentation outputs based on localized visual evidence. However, existing methods rarely incorporate this expert interaction and therefore cannot improve during the test time. Third, existing segmentation studies are often disconnected from downstream clinical tasks and lack direct evaluation of segmentation quality using clinically meaningful measures of disease aggressiveness and patient prognosis, as well as how to translate segmentation results into clinically interpretable analyses. To address the aforementioned limitations, in this work, we introduce VISTA-PATH (Visual Interactive Segmentation and Tissue Analysis for Pathology), an interactive, class-aware pathology segmentation foundation model designed to unify heterogeneous pathology image segmentation, expert-guided refinement, and clinically grounded tissue analysis within single framework. Rather than treating segmentation as static prediction task, VISTA-PATH elevates it to foundational component for expert interaction and clinical discovery. The main contribution and advantage of VISTA-PATH are: large-scale, ontology-driven pathology segmentation dataset. We curate VISTA-PATH Data, the largest pathology segmentation corpus to date, comprising over 1.6 million imagemasktext triplets spanning 9 organs and 93 tissue classes. This resource captures both intra-organ and inter-organ morphological diversity and provides the scale and semantic coverage required to train pathology segmentation foundation model. Text-prompted, class-aware segmentation that supports diverse tissue concepts. VISTA-PATH introduces text-based class prompting as first-class interface for pathology segmentation, enabling the model to segment tissue classes specified by diverse text descriptions rather than fixed label sets. This design (i) allows flexible handling of previously unseen or hybrid class taxonomies; and (ii) allows seamless integration into large language model (LLM)-based agent systems, such as TissueLab16. Fully interactive, real-time refinement from patch-level annotations to pixel-level segmentation. VISTA-PATH supports real-time, human-in-the-loop patch-based active learning, in which sparse expert corrections at the patch level are propagated to whole-slide, pixel-level predictions. By converting localized annotations into class-aware spatial prompts, the framework enables efficient global refinement without dense relabeling or model retraining, and can greatly improve the segmentation performances. High-fidelity segmentation supports clinical discovery. VISTA-PATH extends segmentation beyond accuracyoriented evaluation by making it clinically operationalizable. With high-fidelity, class-aware tissue maps, VISTA-PATH can extract better and interpretable morphological features that helps quantify spatial tissue organization and tumormicroenvironment interactions, supporting downstream analyses that directly link tissue morphology to patient outcomes. Across extensive held-out and external evaluations, VISTA-PATH demonstrates robust and consistent improvements over existing segmentation foundation models, particularly in challenging settings with complex and overlapping tissue structures. The resulting high-fidelity, class-aware tissue maps enable clinically meaningful downstream analyses. In particular, Tumor Interaction Score (TIS) derived from VISTA-PATH segmentation shows strong and significant associations with colon cancer patient survival, improving the C-index by 16.820.7% over standard multiple instance learning (MIL) model on the TCGA-COAD cohort. Fully integrated into TissueLab pathology AI platform16, VISTA-PATH is readily accessible for various research applications. Source code and an interactive demo are also available at https://github.com/zhihuanglab/VISTA-PATH. 2/36 VISTA-PATH tissuelab.org 3/36 VISTA-PATH tissuelab.org Figure 1. Overview of the VISTA-PATH Dataset and the VISTA-PATH architecture a, Data acquisition pipeline from public pathology repositories. b, Distribution of segmentation masks across organs in the VISTA-PATH Dataset (log scale). c, Constructed tissue ontology organizing class labels across organs; accompanying bar plots indicate the number of images containing each tissue class (log scale). d, VISTA-PATH model workflow. Given an H&E image, class prompts, and bounding box, vision, text, and prompt encoders extract features that are fused via cross-attention modules. mask decoder produces class-specific segmentation maps, yielding initial predictions. Pathology experts can provide real-time bounding-box feedback to refine segmentation results. e, Human-in-the-loop refinement workflow. The original H&E image produces pixel-wise segmentations from VISTA-PATH and patch-wise segmentations via patch embedding model. pathologist revises annotations on small subset of patches. These refinements are used to learn patch-level embedding classifier, which generalizes to infer bounding boxes across the entire image. The inferred bounding boxes are incorporated into VISTA-PATH to obtain improved pixel-level segmentation of the full image with minimal human intervention. Human-in-the-loop refinement interface is implement using TissueLab16."
        },
        {
            "title": "2 Results",
            "content": "2.1 Curating large imagemasktext segmentation dataset across 9 organs Although deep learning has significantly advanced medical image analysis, large-scale and high-quality pathology image segmentation datasets remain notably scarce. Compared to other domains such as natural scene or radiology, the pathology faces unique challenges: whole-slide images are at ultra-high resolution, tissue structures are morphologically diverse, and semantic definitions of cellular and tissue entities often vary across datasets. Existing resources are typically fragmented, small in scale, and lack consistent pixel-level annotations, making it difficult to train and evaluate pathology segmentation foundation models. This scarcity of well-curated pathology datasets represents critical bottleneck for progress quantitative analysis in computational pathology. To address this limitation and support scalable pathology segmentation across diverse organs and tissue types, we constructed VISTA-PATH Data, large-scale pathology image segmentation corpus constructed from 22 publicly available datasets (Fig. 1a) from different sources across Zenodo, AIDA data hub, Competition and Challenge (e.g., Kaggle), GitHub, publications, etc. Unlike conventional datasets that are restricted to single organ or narrowly defined tissue class, VISTA-PATH Data aggregates heterogeneous pathology images spanning organs, tissue types, and image types. Comprising 1,645,706 imagemaskclass triplets, VISTA-PATH Data is by far the largest and most diverse segmentation dataset in routine H&E pathology domain with both tissue-level and instance-level segmentation annotations. In VISTA-PATH Data, each pathology image can contain multiple tissue classes and multiple instances per class. Across the entire corpus, the dataset covers 9 major organs and 93 tissue classes and including tumor, normal anatomical structures, and microenvironment-related components (Fig. 1bc). This diversity enables computational modeling of both intra-organ and inter-organ morphological variation. To ensure high data quality, we applied rigorous filtering process that retained only datasets with precise pixel-wise annotations and high-quality images. Specifically, we excluded low-intensity images and coarse annotations (for example, those generated using convex-hull labeling). We then applied unified preprocessing pipeline across all retained datasets to ensure consistency and comparability (See Methods section for details). The resulting dataset provides both the scale and semantic diversity required to train pathological segmentation foundation model. After cleaning, preprocessing, and organization, we randomly sampled 95% pathology images from each source and mix them together to form the training data (see Fig. 1ac and Extended Data Fig. 1) comprises 1,645,706 patch-level samples spanning 93 classes. The rest of the datasets are held out as test set for in-domain evaluation purposes with total of 77,107 samples from 69 classes. Test images are drawn from the same datasets used for training but consist of non-overlapping images, enabling controlled evaluation of segmentation accuracy under in-distribution conditions. Training and test sets are split at the original image level to ensure that no derived patch samples from the same image appear in both splits. Data sources of VISTA-PATH Data can be found in Extended Data Table 1, and detailed statistics are presented in Extended Data Fig. 2. To assess robustness of VISTA-PATH under domain shift, we further collect external datasets that come from completely different sources from VISTA-PATH data. Details can be found in Extended Data Table 2 and Extended Data Fig. 3. In summary, external datasets include 66,355 samples, 82 tissue classes, and 13 organs from three fundamentally different annotation regimes: additional H&E pathology segmentation datasets, Visium HD spatial transcriptomics-guided H&E 4/36 VISTA-PATH tissuelab.org segmentation datasets, and Xenium single-cell spatial transcriptomics-guided H&E segmentation datasets. For Visium HD data, spatial bins were clustered based on gene expression profiles and manually assigned semantic tissue labels by pathologists; bin-level annotations were subsequently converted into dense pixel-wise segmentation maps using platform-provided spatial geometry. For Xenium data, single-cell molecular profiles and spatial coordinates were jointly clustered, and cluster labels were propagated to pixel-level maps using platform-provided cell segmentation contours (see Methods section for details). This setting provide diverse and comprehensive analysis dimensions to probe VISTA-PATHs capability on generalization across organs, tissue classes, and imaging paradigms. 2.2 Developing VISTA-PATH foundation model for pathology segmentation Based on the large 1,645,706 training data, we innovated and trained VISTA-PATH, an interactive, class-aware, semantic segmentation pathology foundation model that unifies morpho-textural evidence, semantic tissue identity, and spatial context within unified architecture (Fig. 1d). Rather than treating class recognition and spatial localization as separate or sequential processes, VISTA-PATH conditions segmentation simultaneously on what tissue is present and where it is located, enabling robust multi-class reasoning in heterogeneous pathology images. To achieve this, VISTA-PATH comprises four components: trainable image encoder, frozen text encoder, an optional bounding-box prompt encoder, and mask decoder. The image and text encoders are initialized from the pretrained PLIP11 visionlanguage model, providing aligned visualsemantic representations specialized for histopathology. Semantic tissue identity is specified through class-aware text prompts, which encode the biological meaning of each target tissue class. Optionally, bounding-box prompts are provided as spatial priors and encoded by SAM-based prompt encoder23. These visual, textual, and spatial representations are fused through cross-attention, allowing the model to resolve tissue identity and spatial extent within unified latent space (Fig. 1d). Specifically, given an input image and target tissue description, VISTA-PATH produces class-conditioned, pixel-wise segmentation mask. When bounding-box prompt is available, it provides an additional supervision. This joint conditioning on semantic class and bounding-box guidance enables VISTA-PATH to accurately segment complex tissue microenvironments. VISTA-PATH model is trained under class-conditioned segmentation formulation, in which each forward pass predicts the foregroundbackground mask for specified tissue class. To prevent over-reliance on spatial priors and promote robustness, bounding-box prompts are generated by ground truth masks and randomly dropped during training (See Methods for details). At inference time, multi-class segmentation is obtained by aggregating the class-wise predictions into coherent tissue map. Since the model is trained using random bounding-box prompts, VISTA-PATH naturally supports human-in-the-loop refinement at test time to further rapidly reduce false positives and false negatives. Implemented within the TissueLab platform16, this iterative interaction enables performance improvements with minimal annotation effort (Fig. 1e) and facilitates adaptation of the foundation model to out-of-distribution cases. Given raw H&E whole-slide image, VISTAPATH first generates pixel-wise segmentation predictions. In parallel, complementary patch-based embedding model (MUSK)13 produces patch-level vector representations that are used to train lightweight patch classification model. pathologist can then add or revise annotations on small subset of informative patches, which are used to train the classifier. The trained classifier is rapidly applied across the whole-slide image to produce updated patch-level segmentation results. Because training occurs only after patch embeddings are computed, this process can be performed in real time. After each update, VISTA-PATH converts the refined patch-level masks into bounding-box prompts, which are fed back into the segmentation model to further improve pixel-level predictions, resulting in progressively refined whole-slide segmentation. The model was optimized using the AdamW optimizer with learning rate of 5 105 for 10 epochs under mixed-precision training on NVIDIA H200 GPUs."
        },
        {
            "title": "2.3 VISTA-PATH enables joint semantic–spatial multi-class segmentation in heterogeneous pathology",
            "content": "After the model trained, we evaluate whether VISTA-PATH fulfills the central requirement of pathology segmentation foundation model: robust joint semanticspatial multi-class segmentation under extreme tissue heterogeneity. To this end, we compare VISTA-PATH against two representative segmentation foundation models: BiomedParse, which conditions segmentation on class-aware text without explicit spatial priors, and MedSAM, which uses spatial prompts without semantic competition between tissue classes (Fig. 2a). BiomedParse conditions segmentation solely on class-aware textual prompts paired with the input image. While this design provides semantic flexibility, the absence of explicit spatial grounding (e.g., bounding-box annotation) forces the model to infer tissue localization from global visual cues, which is unreliable in histopathology images where visually similar patterns recur across different anatomical contexts. MedSAM (Fig. 2b), by contrast, incorporates bounding-box prompts that provide stronger spatial guidance but produces binary, 5/36 VISTA-PATH tissuelab.org class-isolated masks. However, when multiple tissue types coexist within overlapping spatial regions which is common scenario in pathology, the model lacks mechanism to resolve class-level ambiguity, leading to misclassification or fragmented predictions. VISTA-PATH (Fig. 2c) resolves both limitations by jointly conditioning segmentation on classaware text prompts and corresponding spatial prompts within unified visionlanguage architecture, enabling the model to reason simultaneously about what tissue is present and where it is located. PathSegmenter22 was not included in our comparison, as the pathology-specific pretrained models and code needed for evaluation were not publicly available at the time of our experiments. We first evaluate VISTA-PATH on held-out test sets from VISTA-PATH Data to assess in-distribution multi-class segmentation under controlled but heterogeneous conditions. Across the held-out internal datasets, VISTA-PATH achieves the highest average Dice scores with reduced variance and tighter 95% confidence intervals than both MedSAM and BiomedParse (Fig. 2d). VISTA-PATH outperforms MedSAM on the majority of datasets, with average gains ranging from 0.0060.414 Dice. Compared with BiomedParse, VISTA-PATH achieves higher Dice scores on all but one dataset (GlaS), with average gains ranging from 0.0100.668 Dice. These gains are observed across datasets spanning more than two orders of magnitude in size, from small cohorts to collections containing tens of thousands of images, demonstrating that the joint semanticspatial formulation scales without loss of fidelity. To demonstrate the advantage of VISTA-PATH under fair end-to-end comparison, we additionally trained an organspecific segmentation model, Res2Net24, on each organ in VISTA-PATH Data and compared its performance with VISTA-PATH without bounding-box prompts (VISTA-PATH w/o box). As results provided in Extended Data Table 3, although Res2Net is trained separately for each dataset under optimal, task-specific conditions, VISTA-PATH achieves substantially higher average Dice (0.772 for VISTA-PATH versus 0.521 for Res2Net, < 1 103) and outperforms Res2Net on 19 of the 21 testing datasets, indicating that joint semanticspatial pretraining can surpass even datasetspecific fully fine-tuned model. Moreover, while removing bounding-box prompts degrades some performances (0.698 for VISTA-PATH w/o box versus 0.772 for VISTA-PATH, < 1 103), VISTA-PATH w/o box still remarkably outperforms both MedSAM and BiomedParse on average (0.698 for VISTA-PATH w/o box versus 0.581 for MedSAM versus 0.379 for BiomedParse), demonstrating that VISTA-PATH retains strong generalization even without additional, explicit bounding-box guidance. When evaluating performance at the organ level, VISTA-PATHs advantage holds consistently across all nine organs (Fig. 2e and Extended Data Table 4). Specifically, VISTA-PATH improves over second-best model MedSAM by 0.027 0.486 Dice, with particularly large gains on heterogeneous tissues such as liver (0.835 for VISTA-PATH versus 0.349 for MedSAM, < 1 103), lung (0.558 for VISTA-PATH versus 0.215 for MedSAM, < 1 103), oral (0.602 for VISTA-PATH versus 0.338 for MedSAM, < 1 103), and kidney (0.771 for VISTA-PATH versus 0.529 for MedSAM, < 1 103). In contrast, MedSAM performs comparably only on relatively homogeneous organs such as ovary (0.851 for MedSAM versus 0.878 for VISTA-PATH) but degrades sharply on more heterogeneous tissues such as oral (0.338 for MedSAM versus 0.602 for VISTA-PATH) and lung (0.215 for MedSAM versus 0.558 for VISTA-PATH), consistent with the limitations of its binary, class-isolated prediction paradigm. In contrast, BiomedParse exhibits degradation performances across organs, with VISTA-PATH exceeding it by 0.0800.521 Dice, including dramatic gaps on kidney (0.771 for VISTA-PATH versus 0.308 for BiomedParse, < 1 103), breast (0.802 for VISTA-PATH versus 0.281 for BiomedParse, < 1 103), prostate (0.872 for VISTA-PATH versus 0.469 for BiomedParse, < 1 103), and skin (0.637 for VISTA-PATH versus 0.196 for BiomedParse, < 1 103). Only small subset of datasets such as colon (0.808 for BiomedParse versus 0.887 for VISTA-PATH) and liver (0.695 for BiomedParse versus 0.835 for VISTA-PATH) show comparable results, reflecting BiomedParses limited segmentation ability in pathology and its pretraining bias toward few of pathology tissue types. We further stratify the results by tissue category, including tumor-related, normal anatomical, and microenvironment components, and observe that VISTA-PATH continues to exhibit robust advantages beyond organ-level comparisons (Fig. 2f and Extended Data Table 5). Across tumor-related, microenvironment-related, and normal anatomical tissues, VISTA-PATH consistently achieves the highest Dice scores for the vast majority of organs, outperforming both MedSAM and BiomedParse. While occasional near-parity or reversals are observed for MedSAM on prostate microenvironment and ovary normal anatomy, these represent isolated cases. Overall, the results demonstrate that VISTA-PATHs advantage extends beyond specific organs to broad spectrum of tissue types, highlighting its robustness for multi-class, multi-tissue pathology segmentation. Representative examples in Fig. 2g further illustrate the advantage of VISTA-PATH. BiomedParse frequently produces spatially diffuse or incomplete segmentations, because semantic prompts alone cannot resolve local spatial ambiguity. MedSAM, despite strong localization, often misclassifies tissues when bounding boxes include heterogeneous tissue content. In contrast, VISTA-PATH preserves both spatial precision and semantic separation, enabling precise segmentation 6/36 VISTA-PATH tissuelab.org 7/36 VISTA-PATH tissuelab.org Figure 2. Comparison of VISTA-PATH with existing segmentation foundation models. ac, Architectural comparison of representative segmentation foundation models. BiomedParse (a) accepts an image and class-aware textual prompts, but lacks explicit spatial priors (e.g., bounding boxes). MedSAM (b) takes an image and class-specific bounding boxes as input to output binary segmentations; however, this design is prone to errors when multiple tissue types share overlapped bounding boxes. In contrast, VISTA-PATH (c) jointly leverages class-aware text prompts and corresponding bounding boxes to enable spatially informed, precise multi-class segmentation. df, Quantitative segmentation performance on the VISTA-PATH held-out test set. (d) Bar plots comparing performance across individual datasets. For each dataset, the number of images, number of classes, error bars (95% confidence intervals), and values are reported. Statistical significance is assessed using two-sided Students t-test (P < 0.05; < 1 102; < 1 103). (e) Radar plot comparing model performance across 9 organ types. (f) Line plots showing Dice scores with 95% confidence intervals across different organs, grouped by tumor-related, normal-related, and microenvironment-related tissue categories. g, Case studies illustrating representative segmentation results. in these challenging regions. 2.4 VISTA-PATH generalizes across organs, tissue classes, and annotation paradigms To evaluate robustness under domain shift, we evaluate VISTA-PATH on an external benchmark comprising 66,355 images, 82 tissue classes, and 13 organs from three fundamentally different annotation regimes: independent pathology segmentation datasets, Visium HD spatial transcriptomics-guided H&E segmentation datasets, and Xenium single-cell spatial transcriptomics-guided H&E segmentation datasets (Fig. 3, Extended Data Fig. 3). These datasets probe not only new organs and tissue types, but also distinct ways of defining tissue labels, ranging from expert-annotated histology to gene-expressionderived spatial domains. Across external pathology datasets (Fig. 3a), VISTA-PATH substantially outperforms MedSAM and BiomedParse. On LungHP25 (Lung histopathology dataset), VISTA-PATH achieves Dice score of 0.495, more than quadrupling MedSAM (Dice=0.121) and BiomedParse (Dice=0.078). On OCDC26 (oral cavity-derived cancer whole-slide images dataset), VISTA-PATH reaches 0.802, compared to 0.528 and 0.203, respectively, demonstrating strong generalization even in low-sample, low-class-count settings. The advantage of VISTA-PATH becomes more significant on spatial transcriptomics-guided H&E segmentation datasets (Fig. 3b) when introducing more complex, multi-class tissue structure. In our experiment, VISTA-PATH achieves Dice scores of 0.832 on Visium_Kidney and 0.694 on Visium_Colon, exceeding MedSAM by clear margins and outperforming BiomedParse by more than 0.5 Dice in several cases. Performance gaps widen as the number of tissue classes increases, indicating that joint semanticspatial conditioning becomes increasingly critical under heterogeneity. On twenty-two Xenium datasets (Fig. 3c), which provide dense, single-cellderived multi-class labels, VISTA-PATH again delivers consistently superior performance across breast, lung, colon, pancreas, skin, tonsil, and ovary. Dice scores frequently exceed 0.5 in these challenging settings, whereas MedSAM and BiomedParse degrade sharply, particularly when six or more tissue classes coexist. We further evaluated VISTA-PATH without bounding-box prompts (VISTA-PATH w/o box) on the external benchmarks (Extended Data Table 6). Although performance is reduced without spatial guidance, the model still achieves an average Dice of 0.340, remaining competitive with MedSAM (Dice=0.373) and clearly outperforming BiomedParse (Dice=0.199). Aggregated analyses across external datasets (Fig. 3dg and Extended Data Tables 78) show that VISTA-PATH consistently attains the highest Dice scores across the 13 evaluated organs and for most tumor-related, microenvironmentrelated, and normal anatomical tissues. In particular, VISTA-PATH maintains clear advantages over both MedSAM and BiomedParse on heterogeneous organs such as lung (0.447 for VISTA-PATH versus 0.284 for MedSAM versus 0.186 for BiomedParse), colon (0.652 for VISTA-PATH versus 0.546 for MedSAM versus 0.226 for BiomedParse), kidney (0.630 for VISTA-PATH versus 0.587 for MedSAM versus 0.185 for BiomedParse), and skin (0.494 for VISTA-PATH versus 0.411 for MedSAM versus 0.260 for BiomedParse), and this superiority extends across most tumor-related and microenvironment-related tissue compartments (Extended Data Table 8). Although isolated near-parity cases occur for MedSAM in small number of normal anatomical tissues (for example, ovary and prostate), the overall pattern remains strongly in favor of VISTA-PATH. These results indicate that coupling class-aware semantic conditioning with explicit bounding-box annotations yields stable and accurate multi-class segmentation across highly heterogeneous external pathology and spatial-omics datasets. 8/ VISTA-PATH tissuelab.org Figure 3. VISTA-PATH generalizes across annotation protocols, organs, and tissue types on external datasets. a, Segmentation evaluation of LungHP and OCDC datasets. b, Segmentation evaluation of 5 Visium HD datasets. c, Segmentation evaluation of 22 Xenium datasets. d, Radar plot comparing model performance across 13 organ types. e, Line plots showing Dice scores with 95% confidence intervals across different organs, grouped by tumor-related, microenvironment-related, and normal anatomical tissue categories. 9/36 VISTA-PATH tissuelab.org 2.5 Real-time human-in-the-loop bounding-box prompts enables efficient whole-slide refinement In clinical practice, segmentation errors are inevitable, and pathologists routinely inspect and correct automated outputs during analysis. An effective pathology foundation model must therefore support interactive expert-guided refinement, enabling localized human input to be efficiently propagated to global, whole-slide predictions. We evaluate whether VISTAPATH can serve as such an interactive segmentation substrate using real-time human-in-the-loop active learning-based whole-slide refinement pipeline using our in-house TissueLab platform16 (Fig. 4a). Starting from whole-slide image, patch-level classifier generates an initial coarse segmentation by extracting patch embeddings and predicting tissue classes. Pathologists then review small subset of patches and provide quick corrections through active learning. These refined annotations are used to update the patch-level classifier, yielding improved patch-wise predictions. This iterative process is repeated for several rounds until visually satisfactory convergence is achieved. Then, the refined patch-level segmentation masks are converted into class-aware bounding boxes, which, together with the raw image and semantic tissue descriptions, are used as bounding-box prompts for VISTA-PATH. In this way, sparse expert feedback is transformed into spatially and semantically grounded prompts that drive pixel-level refinement across the entire whole-slide image. We select spatial transcriptomics-derived segmentation datasets, including Visium HD and Xenium, as evaluation datasets because they are unseen during training and provide accurate pixel-level definitions. Fig. 4bc and Extended Data Tables 910 report the human-in-the-loop performance on Visium HD and Xenium datasets. Across all datasets, iterative refinement rapidly increases patch-level accuracy within small number of interaction rounds, typically raising Dice from initial values around 0.10.4 to 0.60.8 within 45 rounds. Importantly, VISTA-PATH consistently translates these patch-level improvements into superior pixel-wise segmentations, maintaining clear margin over both the patch-level segmentation and MedSAM at nearly every interaction stage. For example, on Visium_Colon, VISTA-PATH increases from 0.629 Dice at initialization to 0.867 Dice after iterative refinement, whereas the patch-level classifier and MedSAM reach only 0.808 Dice and 0.769 Dice, respectively. Similar trends are observed on Visium_Prostate, where VISTA-PATH rises from 0.424 to 0.810, compared with 0.763 for the patch classifier and 0.775 for MedSAM, as well as on Visium_Kidney, where VISTA-PATH improves from 0.118 to 0.563, exceeding both the patch-level baseline (Dice=0.534) and MedSAM (Dice=0.462). Compared to VISTA-PATH and patch-level segmentation results, MedSAM shows unstable behavior: despite receiving the same refined bounding boxes, its pixel-level outputs are often comparable to or worse than its patch-level predictions, reflecting its inability to translate class-conditioned spatial feedback into accurate segmentation results. On the Xenium datasets, where dense, single-cellderived annotations introduce substantial class and boundary complexity, our conclusions also hold. On Lung-Immuno, VISTA-PATH reaches Dice of 0.745, compared with 0.716 for the patch classifier and 0.626 for MedSAM, and on Breast-5K, VISTA-PATH achieves 0.693, exceeding both the patch baseline (Dice=0.610) and MedSAM (Dice=0.578), respectively. In contrast, MedSAM often fails to fully exploit the refined bounding boxes, producing pixel-level segmentations that perform worse than both VISTA-PATH and, in some cases, even the patch-level predictions. Notably, performances usually saturate after 45 iterations using only about 1,000 annotated patches, highlighting the efficiency of the proposed interactive, human-in-the-loop active learning paradigm. Representative examples in Fig. 4de further illustrate that VISTA-PATH produces accurate, coherent pixel-level segmentations for fine-grained and complex tissue structures. These results demonstrate that VISTA-PATH enables expert feedback to be operationalized at whole-slide scale, closing the loop between human expertise and foundation-model segmentation."
        },
        {
            "title": "2.6 VISTA-PATH produces accurate biomarker for enhanced outcome prediction",
            "content": "In oncology, pathology remains the gold standard for disease diagnosis, assessment of progression, and treatment planning27. These clinical decisions are grounded in the interpretation of tissue morphology and spatial organization, skills that pathologists develop through extensive training and experience. While recent segmentation foundation models have improved the accuracy of pixel-level tissue delineation, their outputs are often treated as intermediate visual predictions rather than as quantitative substrates for downstream computational analysis. As result, improvements in segmentation accuracy do not automatically translate into measurable gains in clinical or biological insight, creating disconnect between algorithmic performance and practical utility in digital pathology. 10/36 VISTA-PATH tissuelab.org 11/36 VISTA-PATH tissuelab.org Figure 4. Human-in-the-loop enhances VISTA-PATH segmentation performance. a, Starting from H&E image, patch-level segmentation model produces an initial segmentation. pathologist then reviews these predictions and provides corrective annotations for selected patches. These corrected annotations are used to fine-tune the patch-level classifier, yielding updated patch-level segmentation results. The updated patch-level segmentations are subsequently converted into bounding-box prompts and provided to VISTA-PATH to generate the final pixel-wise segmentation. b, Pixel-wise segmentation performance improves on the Visium HD dataset. c, Pixel-wise segmentation performance improves on the Xenium dataset. d, Representative visual examples showing our pixel-wise segmentation performance on the Visium_Colon dataset. e, Representative visual examples showing our pixel-wise segmentation performance on the Skin-MultiCancer dataset. GT: Ground truth. In this section, we explore whether the class-aware, high-fidelity segmentation produced by VISTA-PATH can be operationalized into clinically meaningful morphological measurements and served as preferred tool for future research. To this end, we introduce the Tumor Interaction Score (TIS), an interpretable biomarker that quantifies tumor coherence versus infiltration using both pixel-level and patch-level tumor predictions. TIS translates the spatial organization of tumor tissue into single quantitative index: TIS = i=1 Pi N i=1 Pi , (1) where Pi denotes the i-th patch identified as tumor by the patch-level classifier, represents the pixel-wise tumor segmentation mask derived from VISTA-PATH, and denotes area measured in pixels. TIS is an explainable, clinically interpretable index: high TIS reflects cohesive and spatially confined tumor architecture, whereas low TIS may capture either (i) boundary fragmentation and invasive growth, which are morphological signatures of aggressive tumors and adverse prognosis; or (ii) dynamic tumorimmune microenvironment (e.g., lymphocyte infiltration), which is commonly associated with active immune surveillance and distinct prognostic implications depending on tumor context. To evaluate the clinical value of this segmentation-derived biomarker, we incorporate TIS into survival prediction pipeline (Fig. 5a). Raw whole-slide images are first processed by patch-level encoderclassifier to identify candidate tumor regions, which are then provided as bounding-box prompts to VISTA-PATH to generate high-fidelity pixel-wise tumor maps. TIS is computed from these outputs and used as input to multi layer perceptron (MLP)28 to predict patient-level survival risk (Cox proportional hazards regression). We evaluate this framework on the TCGA-COAD cohort29 (Fig. 5bd), using patient-level splits across the TCGA-AZ (or AZ) and TCGA-A6 (or A6) sites, and compare against two baselines: (i) standard ABMIL30 pipeline using the same MUSK13 embeddings, and (ii) an otherwise identical pipeline in which VISTA-PATH is replaced by MedSAM. For this experiment, BiomedParse is not comparable because it does not support bounding-boxguided segmentation. As shown in Fig. 5cd, TIS substantially enhances survival prediction performance. Our VISTA-PATH-based pipeline achieved superior concordance indices31 and log-rank t-test P-values (C-index = 0.678 in A6 and 0.739 in AZ; = 2.7 103), outperforming both ABMIL (C-index = 0.510 in A6 and 0.533 in AZ; = 0.908) and the MedSAM-based variant (C-index = 0.621 in A6 and 0.696 in AZ; = 0.399). Ablation studies (Fig. 5e) further underscore the robustness of our approach: when the MLP is removed, the performance of the MedSAM variant degrades substantially (with the P-value increasing from 0.399 to 0.753), whereas the VISTA-PATH-derived TIS retains strong predictive power (P = 3.18 102). These results demonstrate that class-aware segmentation from VISTA-PATH can be directly transformed into robust, interpretable, and clinically meaningful biomarkers, elevating segmentation from static prediction to quantitative instrument for clinical inference. Fig. 5f shows strong inverse association between the Tumor Interaction Score (TIS) and model-predicted patient risk. Extended Data Fig. 4 presents three representative examples spanning low, intermediate, and high TIS values derived from VISTA-PATH. Low TIS (TIS=0.607) reflects infiltrative tumor budding and elevated risk; intermediate TIS (TIS=0.797) corresponds to spatially confined tumors with the lowest predicted risk; and very high TIS (TIS=0.857) captures large, cohesive tumor masses in which excessive tumor burden leads to an increase in predicted risk. These findings indicate that TIS functions as an interpretable morphological biomarker that integrates tumor boundary coherence and tumor burden, enabling clinically meaningful survival prediction that outperforms other methods and can be accurately derived only from VISTA-PATH. 12/ VISTA-PATH tissuelab.org 13/36 VISTA-PATH tissuelab.org Figure 5. VISTA-PATH enables explainable survival analysis. a, Overview of the VISTA-PATH framework for explainable survival analysis on the TCGA-COAD cohort. Given an input whole-slide image, VISTA-PATH produces pixel-wise tumor segmentation, while MUSK generates patch-level predictions. These pixel-wise and patch-wise outputs are combined to compute the Tumor Interaction Score (TIS). TIS is defined as the ratio of the area classified as tumor to the area of all tumor-related classes. The resulting TIS is used as input to multilayer perceptron (MLP) to predict patient survival outcomes. b, Data distribution of the training and test splits across the AZ and A6 sites. c, Comparison of survival prediction performance (C-index) among MedSAM, ABMIL, and VISTA-PATH on the A6 and AZ cohorts. d, KaplanMeier survival curves stratified by the predicted risk scores. e, KaplanMeier survival curves stratified by the proposed Tumor Interaction Score (TIS). f, Correlation between TIS scores and predicted risk scores. Points are colored by survival time (months), with black-edged markers indicating death events and plain markers indicating censored cases. The dashed line represents the trend, with Spearmans correlation coefficient (ρ) and P-value shown."
        },
        {
            "title": "3 Discussion",
            "content": "In this study, we developed VISTA-PATH as the first interactive and clinically grounded segmentation foundation model for pathology that supports expert reasoning, iterative refinement, and downstream clinical inference. In contrast to prior segmentation foundation models that treat segmentation as static visual prediction task, VISTA-PATH is designed to resolve extreme multi-tissue heterogeneity, incorporate expert feedback during analysis, and translate segmentation outputs into interpretable clinical signals. VISTA-PATH is trained by the large, ontology-driven VISTA-PATH Data, spanning 93 tissue classes across 9 organs and over 1.6 million masks. We first demonstrate that VISTA-PATH effectively leverages the semantic-spatial modeling for pathology segmentation, highlighting its generalization across diverse tissues and data sources. We then show that VISTA-PATH enables effective human-in-the-loop refinement, allowing limited expert corrections to propagate across whole-slide images with minimal additional annotation effort. Finally, we demonstrate that the segmentation outputs produced by VISTA-PATH can be directly integrated into downstream clinical analyses, enabling the derivation of interpretable and clinically meaningful biomarkers for survival prediction. VISTA-PATH has several unique strengths. First, VISTA-PATH effectively leverages the joint semanticspatial modeling for pathology segmentation. Histopathology images are inherently compositional: tumor epithelium, stroma, immune infiltrates, necrosis, and normal structures are densely interwoven and often visually similar while carrying distinct biological meanings. Accurate pathology segmentation therefore requires models to represent what tissue is present and where it is located at the same time. Existing foundation models decouple these two dimensions: BiomedParse relies on semantic text without explicit spatial grounding, whereas MedSAM relies on bounding boxes without semantic competition between tissue classes. As result, both struggle when multiple tissue types coexist within overlapping spatial regions. VISTA-PATH resolves this limitation by jointly conditioning segmentation on class-aware text prompts and corresponding spatial prompts within unified visionlanguage architecture. This design enables simultaneous reasoning over tissue identity and spatial extent, allowing the model to perform true multi-class segmentation in heterogeneous pathology images. The large, ontology-driven VISTA-PATH Data provides the diversity needed to learn this joint representation. The strong and consistent performances across conventional pathology datasets, Visium HD spatial transcriptomics-derived datasets, and Xenium spatial transcriptomics-derived datasets demonstrate that semanticspatial coupling generalizes across organs, tissue types, and annotation paradigms. Notably, the largest gains occur in microenvironment-related tissues, where subtle morphology and spatial intermixing dominate, highlighting how joint semanticspatial modeling directly translates into practical segmentation accuracy. Second, VISTA-PATH elevates segmentation from static inference tool to an interactive workflow. The human-in-theloop experiments show that sparse patch-level corrections, on the order of 101,000 patches, can be propagated to whole-slide, pixel-level outputs with high fidelity. This propagation succeeds because VISTA-PATH interprets bounding boxes not merely as geometric constraints, but as class-conditioned spatial signals embedded within its joint semantic spatial representation. This capability fundamentally distinguishes VISTA-PATH from prior interactive systems. Traditional approaches require either dense relabeling or full retraining to adapt to new slides, while MedSAM cannot reliably convert patch-level updates into coherent multi-class pixel-wise maps. By contrast, VISTA-PATH transforms limited expert input into global, class-consistent refinements, effectively closing the loop between pathologists and foundation models. This enables continuous refinement during real-world use, rather than one-shot deployment followed by manual correction. Finally, and most consequentially, VISTA-PATH enables its segmentation outputs to be directly operationalized as clinically meaningful biomarkers for downstream inference. The Tumor Interaction Score (TIS) shows how combining 14/36 VISTA-PATH tissuelab.org pixel-level tumor maps with patch, level tumor localization yields quantitative measure of tumor coherence versus infiltration, an established pathology concept that previously required subjective expert judgment. The strong association of TIS with patient survival in two sites from TCGA-COAD cohort, and its ability to outperform existing methods such as ABMIL, demonstrates that segmentation quality and semantic fidelity are not merely aesthetic improvements but have tangible impact on clinical inference. Crucially, when the learning model is removed and TIS alone is used for stratification, VISTA-PATH remains predictive while MedSAM collapses. This shows that the value of TIS arises not from more powerful classifier, but from biologically faithful segmentation. In summary, VISTA-PATH transforms segmentation foundation models from generic feature extractors into morphological measurement engines capable of producing interpretable biomarkers. This shift, from segmentation as static prediction to segmentation as quantitative, clinically grounded representation, fundamentally changes how foundation models can be used in digital pathology. While VISTA-PATH demonstrates that segmentation can serve as an interactive, researchand clinical-ready foundation for digital pathology, several important challenges remain that point toward future directions of this paradigm. First, although VISTA-PATH enables effective human-in-the-loop refinement through patch-level corrections, pathologists often interact with images using richer cues, including region marking, uncertainty annotation, and iterative hypothesis testing across multiple tissue slides. Extending VISTA-PATH to support these more expressive forms of expert interaction will be essential for deploying foundation models as flexible, collaborative partners in clinical workflows. Second, VISTA-PATH develops Tumor Interaction Score (TIS) illustrating how class-aware segmentation can be converted into an interpretable and clinically meaningful morphological indicator. key future direction is to move toward automated discovery of morphological biomarkers. With high-fidelity, class-aware segmentation maps as substrate, foundation models could be used to systematically search the space of spatial tissue configurations, such as tumorstroma interfaces, immune infiltration patterns, and glandular architecture, to identify new prognostic or predictive signatures that are not predefined by human heuristics. Third, although VISTA-PATH demonstrates strong generalization across organs, datasets, and molecularly derived annotations, further validation across large, multi-institutional clinical cohorts will be required to assess robustness to variations in slide preparation, staining protocols, and scanner hardware. Lastly, while VISTA-PATH can adapt to different slide magnifications, further investigation is needed to determine which magnification levels are most effective for the model, as well as to assess the impact of magnification choices in real-world settings. These factors play critical role in real research and potential clinical deployment and represent an important next step for translating interactive segmentation foundation models into future use."
        },
        {
            "title": "4.1 VISTA-PATH Data: dataset for pathology segmentation",
            "content": "VISTA-PATH Data was curated through the systematic integration of 22 publicly available segmentation resources from multiple platforms, including The Cancer Imaging Archive (TCIA), Kaggle, Grand Challenge, Scientific Data, CodaLab, and segmentation challenges hosted by the Medical Image Computing and Computer Assisted Intervention Society (MICCAI). The sources of all datasets are listed in Extended Table 1. Each image is associated with one or more tissue types and each tissue type is annotated with high-precision segmentation mask and canonical semantic label derived from tissue ontology of structured pathology. This comprehensive dataset establishes unified foundation for advancing segmentation models in computational pathology by capturing broad spectrum of tissue and object diversity across multiple organs and imaging contexts. To ensure the quality and consistency of VISTA-PATH Data, we applied stringent inclusion criteria: each image was required to be manually or semi-manually segmented at the pixel level, and each segmented object had to be associated with clearly defined name in the original dataset description. Given the diversity of data sources and formats, we standardized all inputs into unified representation through the following processing steps: Image format transformation. Pathology images were originally provided in diverse resolutions and formats, including both patch-wise and whole-slide image (WSI) levels, and were stored in heterogeneous file types such as .tif, .png, and .svs. To ensure interoperability and facilitate downstream processing, all images were converted into standardized three-channel format with consistent structure and naming convention. For WSI data, the highest resolution level was selected to preserve the original image quality and spatial details. Mask format transformation. The pixel-wise annotation masks were originally provided in variety of heterogeneous formats. Some datasets stored the masks directly as raster images (e.g., .png, .tif), whereas others 15/36 VISTA-PATH tissuelab.org represented annotations as polygon coordinates in .csv or .json files. To unify these representations, all polygon-based annotations were rasterized into pixel-level segmentation masks that are spatially aligned with their corresponding pathology images. Formally, let Ω R2 denote the image domain and {Pc}C c=1 represent the polygonal regions for each class c. The rasterized mask : Ω {0, 1, . . . ,C} is defined as M(p) = (cid:40) c, 0, Pc / (cid:83)C c=1 Pc, (2) where = (x, y) denotes pixel coordinate, and is the class index (0 corresponds to background). Each mask was then encoded as either binary or multi-class images, depending on the number of annotated structures, ensuring that each pixel is assigned to unique semantic class. This standardized mask format enables consistent downstream processing and model training across all datasets in VISTA-PATH Data. Label class construction. We manually extracted and verified the correctness of each class label to ensure consistency across datasets. The detailed class definitions are summarized in Fig. 1c and Extended Data Fig. 1. For datasets DROV and DROID, class names were extracted from the descriptions and metadata of the data set. We then performed manual filtering and standardization to remove redundant or ambiguous labels. Each validated class was subsequently mapped to its corresponding region in the segmentation mask, ensuring one-to-one correspondence between semantic labels and pixel-level annotations. For each dataset, we randomly split the samples into 95% for training and 5% for testing, and the trainingtesting splits are conducted at the sample-level. To mimic real-world usage scenarios in which pathology images are typically acquired at different magnifications, we did not restrict the dataset to single magnification level. To address the variability in image sizes across datasets, all whole-slide or patch-level images were cropped into patches of size 1024 1024 pixels. During model training, random 512 512 region was further sampled from each patch and subsequently resized to 224 224 pixels before being fed into the network. The same sampling and resizing strategy was applied to the corresponding segmentation masks to ensure spatial alignment between the images and their annotations."
        },
        {
            "title": "4.2 External Datasets",
            "content": "To further evaluate the generalization capability of our model, we conducted additional experiments on several external datasets that were not seen during the training phase. These datasets were collected from independent sources and represent different imaging conditions, organs, and tissue types compared to the training data. By evaluating the model on unseen datasets, we aim to assess its robustness and transferability to real-world scenarios, where domain shifts in acquisition protocols and biological variability commonly occur. The sources of all external datasets are listed in Extended Table 2. 4.2.1 Pathology Segmentation Datasets Two additional publicly available pathology segmentation datasets LungHP and OCDC were used to evaluate the generalizability of our models. The images, masks, and class labels from these datasets were processed using the same preprocessing pipeline described above for VISTA-PATH Data. 4.2.2 Visium HD Datasets We collected five Visium HD datasets from the 10x Genomics platform, including Colon, Kidne, Lung, Pancreas, and Prostate. Image statistics are presented in Fig 3b. Each Visium HD slide is represented as set of spatial bins {bi}N i=1, where each bin bi corresponds to local region containing molecular expression profile xi Rd. To obtain semantically meaningful tissue regions, we used k-means clustering pipeline32 provided by 10X Genomics platform to group bins with similar molecular features. Formally, k-means minimize the within-cluster variance: arg min {C j}K j=1 j=1 xiC xi µ j2 2, (3) 16/36 VISTA-PATH where denotes the j-th cluster and µ is its centroid. Each bin bi is assigned to the cluster ci = arg min j{1,...,K} xi µ j2 2. tissuelab.org (4) To connect clusters with segmentation textual class description, pathologists from Penn Medicine then manually examined the clusters {C j} on each slide to verify whether the grouping corresponded to meaningful tissue structures or biological patterns. After annotation, each cluster is assigned semantic class label. To generate pixel-level segmentation mask, we leveraged the known bin size provided by the Visium HD platform (i.e., 8 µm 8 µm per bin). For each bin, all pixels located within its spatial boundary were assigned the same class label as the cluster label. This bin-to-pixel mapping procedure converts cluster-level annotations into dense pixel-level labels, resulting in segmentation dataset. 4.2.3 Xenium Datasets We further collected 22 Xenium datasets from the 10x Genomics platform. Compared to Visium HD with bin-level aggregated spatial transcriptomic information, Xenium offers precise single-cell resolution with explicit cell segmentation contours and corresponding gene profiles. For each Xenium slide, we first obtained the cell segmentation contours provided by the platform, where each cell corresponds to segmented region and is associated with gene expression profile zi Rd. To cluster cells with similar gene expression profiles, we applied CellCharter33 to compute multi-layer spatially aggregated cell embeddings, in which each cell is represented by vector that jointly encodes its intrinsic transcriptional profile and the molecular composition of its local neighborhood. These embeddings were then clustered using K-means to group cells with similar molecular states into discrete spatial domains. The resulting domain labels were subsequently projected onto the co-registered H&E images for tissue segmentation label assignment. Spatial domain modeling via CellCharter-based neighborhood embeddings and K-means clustering. Let xi Rd denote the PCA-reduced gene expression profile of cell i, and let = (V, E) be the spatial adjacency graph constructed over all cells using Delaunay triangulation on their spatial coordinates. We employed CellCharters neighborhood aggregation operator to compute spatially contextualized embeddings by recursively aggregating features from multi-hop neighborhoods on . Specifically, starting from the initial representation (0) = [x1, . . . , xN], we generated sequence of aggregated features (l+1) = Agg(X (l), ), = 0, . . . , 1, (5) where Agg() denotes CellCharters graph-based neighbor pooling. The final embedding = [X (0), (1), . . . , (L)] concatenates information from multiple spatial scales, capturing both single-cell transcriptional states and progressively larger molecular neighborhoods. In our experiments, we used = 3 aggregation layers, resulting in multi-scale spatial embedding for each cell. We then performed unsupervised clustering on these CellCharter-derived embeddings Zi using MiniBatch K-means. Given specified number of clusters K, K-means optimizes min {ci},{µk} i=1 Zi µci2 , (6) where ci {1, . . . , K} is the cluster assignment for cell and µk denotes the centroid of cluster k. This produces hard assignments of cells to spatial domains, yielding discrete partitioning of the tissue into molecularly defined regions. Finally, the cluster labels were projected onto the co-registered H&E whole-slide image using the Xenium-to-histology coordinate transformation. After the clusters are obtained, we invite pathologists from Penn Medicine annotate each cluster with tissue classes with textual description. The pixel-level segmentation labels are obtained by assigning each pixel within cells contour the class name of the cluster to which that cell belongs. In summary, the cell-to-pixel assignment from Xenium yields dense and precise semantic segmentation mapping."
        },
        {
            "title": "4.3 Details of VISTA-PATH model architecture",
            "content": "VISTA-PATH is visual interactive segmentation foundation model for pathology tissue analysis that supports both interactive visual prompts and text prompts. Specifically, VISTA-PATH is composed of four key components: an image 17/36 VISTA-PATH tissuelab.org encoder, text encoder, bounding box prompt encoder, and mask decoder  (Fig. 1)  . The functionality and design of each module are described in detail below. Image encoder. We adopt the vision encoder from the pretrained PLIP model to extract image embedding for input pathology images. Given an input image R3224224, the encoder produces set of patch embeddings = {v i}N i=1 with RNd , where = 49 corresponds to 7 7 patch grid and = 768 is the original embedding dimension. After removing the class token, the remaining patch embeddings are projected through visual projection layer into shared latent space: = Wproj, Wproj Rdd, RNd, = 512. (7) This yields semantical embeddings which will be utilized for segmentation predictions. Text encoder. We adopt the PLIP text encoder to process textual class description (e.g., an image of {class_name}). The input text prompt is tokenized into sequence of tokens and encoded as = {t , where = 77 and = 768 is the original textual embedding dimension. These embeddings are then projected into the shared latent space used for cross-modal fusion: j=1 with RT j}T = Wtext, Wtext Rdd, RT d, = 512. (8) This yields text representations aligned with the image embedding space, enabling subsequent cross-modal interactions. Prompt encoder. We integrate frozen prompt encoder from the SAM. bounding box prompt R14 is encoded into prompt embedding = {p}, R2d, = 512. (9) Cross-modal fusion and context modeling. We employ cross-attention modules to fuse visual, textual, and spatial prompt information. Given image embeddings RNd, text embeddings RT d, and bounding-box prompt embedding R2d (with = 7 7, = 77, = 512 as defined above), fusion proceeds in two stages: Ftext = CrossAttn(cid:0)Q = V, = T, = T(cid:1), Fprompt = CrossAttn(cid:0)Q = Ftext, = P, = P(cid:1). (10) The first stage lets each visual token attend to semantically relevant textual cues, while the second stage injects localized spatial priors from the prompt embedding, yielding unified representation for downstream decoding. The fused representation Fprompt is further refined by stack of Transformer encoder layers34: Ffinal = TransformerEncoderLayerL(Fprompt), where = 4, Ffinal R51277. (11) Mask decoder. Given the fused feature map Ffinal R51277, the mask decoder produces dense per-pixel predictions ˆY RK224224 via four-stage upsampling stack. Each stage applies bilinear upsampling followed by 33 convolution, Batch Normalization, and ReLU, progressively refining spatial details while expanding the receptive field. Concretely, the resolution evolves as 7142856224, with channel widths {5122561286432}. final 11 convolution maps features to class logits per pixel. Formally, letting U() denote bilinear upsampling and φ () denote Conv(33)BNReLU, the decoder is H1 = U(cid:0)φ (Ffinal)(cid:1), H2 = U(cid:0)φ (H1)(cid:1), H3 = U(cid:0)φ (H2)(cid:1), H4 = U(cid:0)φ (H3)(cid:1), ˆY = Conv11(H4) RK224224. (12) (13) In our experiments we set = 2 (foreground and background), producing binary segmentation for each imagetext pair corresponding to the specified text prompt. 18/36 VISTA-PATH tissuelab.org 4.4 Details of VISTA-PATH model training VISTA-PATH is trained for grounded segmentation using text prompts, optionally grounded with bounding-box prompts as spatial priors. For each imagetext pair, the ground-truth mask is defined by the prompt: pixels belonging to the specified class are labeled 1 and others 0, i.e., {0, 1}HW . The model outputs two logits per pixel ˆY R2HW (cid:1) [0, 1]2 denote the per-pixel class probabilities and write pfg,i and (background/foreground). Let p,i = softmax(cid:0) ˆY,i pbg,i for the foreground and background entries, respectively. We minimize the softmax cross-entropy35: LCE ="
        },
        {
            "title": "1\nHW",
            "content": "(cid:104) i=1 j=1 Yi log pfg,i + (1 Yi j) log pbg,i (cid:105) . (14) When bounding-box prompt is available, it is used as an additional spatial prior during forward propagation; the objective remains the same. We initialize the VISTA-PATH image and text encoders from the PLIP model. Training uses mixed-precision (FP16) with the text encoder frozen. We optimize the model parameters with learning rate of 5 105 for 10 epochs using batch size of 512. Training is performed on single NVIDIA H200 GPU and completes in approximately 25 hours. For evaluation, inference is run on one NVIDIA H200 GPU with memory usage of about 2.4 GB."
        },
        {
            "title": "4.5 Evaluation metrics and statistical analysis",
            "content": "We assess segmentation using the Dice coefficient36. For image with prediction ˆYn and ground truth Yn, Dicen = 2 ˆYn Yn ˆYn + Yn . (15) For binary tasks, we compute Dicen per image and report mean over the test set. For multi-class tasks, we compute Dice per class and report the average across classes. Following common practice, if both ˆYn and Yn are empty we set Dicen=1; if only one is empty we set Dicen=0. Statistical testing. We report per-image Dice and summarize each methods performance by the mean and its 95% confidence interval computed via percentile bootstrap (10,000 resamples). Statistical significance was assessed using two-sided Students t-test on per-image Dice score differences. Statistical significance is annotated as < 0.05; < 1 102; < 1 103. value in the KaplanMeier37 survival plot was computed using the log-rank test with chi-square approximation38."
        },
        {
            "title": "4.6 Details of experiments on internal segmentation evaluation",
            "content": "We report per-dataset results for every dataset in the internal benchmark, with the macro mean (averaged across datasets) summarized in Extended Data Table 3. In the pathology domain, it is crucial to have model capable of handling different organs. To assess performance across anatomical contexts, we aggregate results by organ. For each organ, we compute the mean Dice score with 95% percentile-bootstrap confidence intervals (10,000 resamples) over all images belonging to datasets of that organ. Another important consideration is the diversity of pathology classes and how well segmentation model handles them. To assess class-wise performance, we standardize labels into three categories: tumor-related, microenvironment-related, and normal anatomical tissues (see Fig. 2). Per-category scores are obtained by pooling images whose ground-truth foreground belongs to the corresponding group and then computing the mean Dice score."
        },
        {
            "title": "4.7 Details of experiments on external segmentation evaluation",
            "content": "To further assess the generalizability of VISTA-PATH, we evaluate the model on external datasets not seen during training. In the zero-shot setting, no fine-tuning or dataset-specific adaptation is performed; we use the same inference protocol and prompts across datasets. We report per-image Dice scores. summary of the external datasets (domains, organs, modalities, and class definitions) is provided in Extended Data Fig. 3. Similar to the evaluation of internal datasets, we also report organ-wise and class-wise results. For class-wise evaluation, labels are standardized into three categories: tumor-related, microenvironment-related, and normal anatomical tissues, and we report the mean Dice. Organ-wise performance is obtained by aggregating images by organ and computing the corresponding mean Dice. 19/36 VISTA-PATH tissuelab.org 4.8 Details of experiments on Human-in-the-loop study In practice, segmentation should not be treated as one-time inference. Pathologists routinely review model outputs and iteratively refine them to correct errors and improve accuracy. Crucially, such corrections should be supported at the whole-slide image level, as whole-slide images are essential for reliable downstream analysis. Motivated by this need, we propose human-in-the-loop framework that leverages VISTA-PATH as an interactive segmentation model to iteratively refine whole-slide image segmentation results. An overview of the human-in-the-loop framework is shown in Fig. 4a. Given whole-slide image as input, the image is first tiled into fixed-size patches. MUSK model39 is then applied to extract patch embeddings and generate patch-level classification predictions, which serve as the initial segmentation output. Pathologists review these patch-level results and provide localized corrections on selected subregions. The corrected patch-level annotations are collected and used to train lightweight classifier (i.e., an XGBoost head40) on the corresponding extracted patch embeddings. The updated patch classifier is subsequently applied to all patches to produce refined patch-level segmentation results. The refined patch-level predictions are then used to derive bounding-box prompts for VISTA-PATH. Specifically, we apply sliding-window strategy over the whole-slide image; for each window, tight bounding box is computed from the local binary mask and provided as prompt to VISTA-PATH, enabling refined pixel-wise segmentation. The iterative refinement process is terminated once pathologists judge the segmentation results to be satisfactory. The number of iterative rounds and the number of patch-level annotations provided at each round are summarized in Fig. 4bc and Extended Data Tables 910."
        },
        {
            "title": "4.9 Details of experiments on TCGA-COAD cohort study",
            "content": "Explainable AI is essential for enhancing the interpretability and trustworthiness of pathology-based clinical models. In pathology, accurate prediction of cancer survival is fundamentally based on the identification, morphological assessment, and quantitative analysis of specific pathological structures, such as tumor regions. Consequently, precise segmentation of key regions and reliable extraction of morphological measures are critical for generating meaningful survival signals. In this study, we demonstrate how VISTA-PATH improves survival analysis in the TCGA-COAD cohort by providing accurate segmentation masks and enabling the extraction of robust morphological measurements, which directly improves survival predictions. We introduce the Tumor Interaction Score (TIS), metric designed to quantify tumor solidity and assess the degree of tumor interacting with other microenvironments. more solid tumor component often reflects less invasive phenotype, slower disease progression, or earlier disease stage, all of which are associated with more favorable clinical outcomes. TIS is computed through two-stage process: we first generate patch-level tumor segmentations and then perform pixel-level segmentation within the predicted tumor patches. The score is defined as the ratio between the intersection of pixeland patch-level tumor masks and the total patch-level tumor area: TIS = i=1 Pi N i=1 Pi , (16) where Pi denotes the i-th patch classified as tumor by the patch-level predictor, denotes the pixel-wise tumor segmentation mask, and indicates area measured in pixels. This metric quantifies the density and cohesiveness of the tumor component, with higher values indicating more well-demarcated solid tumor regions that correspond to reduced invasiveness and improved patient outcomes. We compare VISTA-PATH with two categories of baselines: standard multiple instance learning (MIL) approach (i.e., ABMIL) and segmentation-based methods (i.e., MedSAM). For the MIL baseline, ABMIL operates on patch embeddings extracted using the MUSK model. For the segmentation-based baseline, we replace VISTA-PATH with MedSAM and compute the corresponding TIS score for survival prediction. Under segmentation-based methods, each whole-slide image (WSI) yields single TIS score, which is fed into multilayer perceptron (MLP) to generate the final risk prediction."
        },
        {
            "title": "Acknowledgments",
            "content": "We would like to thank Zhengde (Theodore) Zhao for providing valuable insights on text prompt-based image segmentation. 20/36 VISTA-PATH"
        },
        {
            "title": "Data Availability",
            "content": "tissuelab.org DROID data used in this study were obtained from the Analytic Imaging Diagnostics Arena (AIDA). The dataset is released under permissive research license that allows use, modification, and redistribution for medical diagnostics research, provided that the original copyright and permission notices are retained. Data are available at https://datahub. aida.scilifelab.se/10.23698/aida/drsk. DROV dataset is free for use in legal and ethical medical diagnostics research. Access to the data requires approval through the AIDA data portal. The designated recipient researcher must hold PhD degree in relevant field, and the applicant must be an authorized institutional signatory with the legal authority to enter into data sharing agreements on behalf of the institution. Data are available at https://datahub. aida.scilifelab.se/10.23698/aida/drov. KPMP data is supported by the National Institute of Diabetes and Digestive and Kidney Diseases (NIDDK) through grants U01DK133081, U01DK133091, U01DK133092, U01DK133093, U01DK133095, U01DK133097, U01DK114866, U01DK114908, U01DK133090, U01DK133113, U01DK133766, U01DK13 3768, U01DK114907, U01DK114920, U01DK114923, U01DK114933, U24DK114886, UH3DK114926, UH3DK114861, UH3DK114915, and UH3DK114937. We gratefully acknowledge the essential contributions of patient participants and the support of the American public through their tax dollars. Data are available at https://www.kpmp.org/ available-data. TCGA-COAD results here are in whole or part based upon data generated by the TCGA Research Network: https://www.cancer.gov/tcga. All data resources used in this study are summarized in Extended Data Table 1 and Extended Data Table 2."
        },
        {
            "title": "Code Availability",
            "content": "VISTA-PATH (new software developed in this study) is released as an open-source Python library on GitHub at https: //github.com/zhihuanglab/VISTA-PATH."
        },
        {
            "title": "Competing interests",
            "content": "Yucheng Tang and Daguang Xu are employees at the Nvidia Corporation, USA."
        },
        {
            "title": "Author contributions statement",
            "content": "Conceptualization: Peixian Liang, Zhi Huang. Methodology: Peixian Liang, Songhao Li, Shunsuke koga, Yutong Li, Zhi Huang. Experiments: Peixian Liang, Songhao Li, Shunsuke koga, Yucheng Tang, Daguang Xu, Zhi Huang. Data curation and acquisition: Peixian Liang, Yutong Li, Zhi Huang. Expert Pathology Annotation, Interpretation, and Feedback: Shunsuke Koga, Zahra Alipour. Manuscript writing: Peixian Liang, Zhi Huang. Supervision: Zhi Huang. All authors reviewed and approved the final manuscript."
        },
        {
            "title": "References",
            "content": "1. Campanella, G. et al. Clinical-grade computational pathology using weakly supervised deep learning on whole slide images. Nat. medicine 25, 13011309 (2019). 2. Coudray, N. et al. Classification and mutation prediction from nonsmall cell lung cancer histopathology images using deep learning. Nat. medicine 24, 15591567 (2018). 3. Echle, A. et al. Deep learning in cancer pathology: new generation of clinical biomarkers. Br. journal cancer 124, 686696 (2021). 4. Xu, H. et al. whole-slide foundation model for digital pathology from real-world data. Nature 630, 181188 (2024). 5. Lu, M. Y. et al. visual-language foundation model for computational pathology. Nat. medicine 30, 863874 (2024). 6. Wang, X. et al. pathology foundation model for cancer diagnosis and prognosis prediction. Nature 634, 970978 (2024). 21/36 VISTA-PATH tissuelab.org 7. Graham, S. et al. Hover-net: Simultaneous segmentation and classification of nuclei in multi-tissue histology images. Med. image analysis 58, 101563 (2019). 8. Weigert, M., Schmidt, U., Haase, R., Sugawara, K. & Myers, G. Star-convex polyhedra for 3d object detection and segmentation in microscopy. In The IEEE Winter Conference on Applications of Computer Vision (WACV), DOI: 10.1109/WACV45572.2020.9093435 (2020). 9. Zhang, J. et al. Sam-path: segment anything model for semantic segmentation in digital pathology. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 161170 (Springer, 2023). 10. Chen, L.-C., Papandreou, G., Kokkinos, I., Murphy, K. & Yuille, A. L. Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. IEEE transactions on pattern analysis machine intelligence 40, 834848 (2017). 11. Huang, Z., Bianchi, F., Yuksekgonul, M., Montine, T. J. & Zou, J. visuallanguage foundation model for pathology image analysis using medical twitter. Nat. medicine 29, 23072316 (2023). 12. Chen, R. J. et al. Towards general-purpose foundation model for computational pathology. Nat. Medicine 30, 850862 (2024). 13. Xiang, J. et al. visionlanguage foundation model for precision oncology. Nature 638, 769778 (2025). 14. Ma, J. et al. Segment anything in medical images. Nat. Commun. 15, 654 (2024). 15. Zhao, T. et al. foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities. Nat. methods 22, 166176 (2025). 16. Li, S. et al. co-evolving agentic ai system for medical imaging analysis. arXiv preprint arXiv:2509.20279 (2025). 17. Lu, M. Y. et al. Data-efficient and weakly supervised computational pathology on whole-slide images. Nat. Biomed. Eng. (2021). 18. Litjens, G., Kooi, T., Bejnordi, B. E. et al. survey on deep learning in medical image analysis. Med. Image Analysis (2017). 19. Liang, P. et al. Enhancing whole slide image classification with discriminative and contrastive learning. In International Conference on Medical Image Computing and Computer-Assisted Intervention, 102112 (Springer, 2024). 20. Ding, T. et al. multimodal whole-slide foundation model for pathology. Nat. Medicine 37493761 (2025). 21. Zhang, S. et al. generalist foundation model and database for open-world medical image segmentation. Nat. Biomed. Eng. 116 (2025). 22. Chen, Z. et al. Segment anything in pathology images with natural language. arXiv preprint arXiv:2506.20988 (2025). 23. Kirillov, A. et al. Segment anything. In Proceedings of the IEEE/CVF international conference on computer vision, 40154026 (2023). 24. Gao, S.-H. et al. Res2net: new multi-scale backbone architecture. IEEE transactions on pattern analysis machine intelligence 43, 652662 (2019). 25. Li, Z. et al. Deep learning methods for lung cancer segmentation in whole-slide histopathology imagesthe acdc@ lunghp challenge 2019. IEEE J. Biomed. Heal. Informatics 25, 429440 (2020). 26. dos Santos, D. F. et al. Hematoxylin and eosin stained oral squamous cell carcinoma histological images dataset. arXiv preprint arXiv:2303.10172 (2023). 27. Dang, C. et al. Deep learning-powered whole slide image analysis in cancer pathology. Lab. Investig. 104186 (2025). 28. Cybenko, G. Approximation by superpositions of sigmoidal function. Math. control, signals systems 2, 303 (1989). 29. Weinstein, J. N. et al. The cancer genome atlas pan-cancer analysis project. Nat. genetics 45, 11131120 (2013). 30. Ilse, M., Tomczak, J. & Welling, M. Attention-based deep multiple instance learning. In International conference on machine learning, 21272136 (PMLR, 2018). 31. Harrell Jr, F. E., Lee, K. L. & Mark, D. B. Multivariable prognostic models: issues in developing models, evaluating assumptions and adequacy, and measuring and reducing errors. Stat. medicine 15, 361387 (1996). 32. Ahmed, M., Seraj, R. & Islam, S. M. S. The k-means algorithm: comprehensive survey and performance evaluation. Electronics 9, 1295 (2020). 22/36 VISTA-PATH tissuelab.org 33. Varrone, M., Tavernari, D., Santamaria-Martínez, A., Walsh, L. A. & Ciriello, G. Cellcharter reveals spatial cell niches associated with tissue remodeling and cell plasticity. Nat. genetics 56, 7484 (2024). 34. Vaswani, A. et al. Attention is all you need. Adv. neural information processing systems 30 (2017). 35. Mao, A., Mohri, M. & Zhong, Y. Cross-entropy loss functions: Theoretical analysis and applications. In International conference on Machine learning, 2380323828 (pmlr, 2023). 36. Dice, L. R. Measures of the amount of ecologic association between species. Ecology 26, 297302 (1945). 37. Kaplan, E. L. & Meier, P. Nonparametric estimation from incomplete observations. J. Am. statistical association 53, 457481 (1958). 38. Lee, E. T. & Wang, J. Statistical methods for survival data analysis, vol. 476 (John Wiley & Sons, 2003). 39. Xu, F. et al. Predicting axillary lymph node metastasis in early breast cancer using deep learning on primary tumor biopsy slides. Front. oncology 11, 759007 (2021). 40. Chen, T. & Guestrin, C. Xgboost: scalable tree boosting system. Proc. 22nd ACM SIGDD Int. Conf. on Knowl. Discov. Data Min. (2016). 41. Huo, X. et al. Comprehensive ai model development for gleason grading: from scanning, cloud-based annotation to pathologistai interaction. Prepr. at SSRN https://doi. org/10.2139/ssrn 4172090 (2022). 42. Polónia, A., Eloy, C. & Aguiar, P. Bach dataset: Grand challenge on breast cancer histology images. Med. Image Anal 2019, 563 (2019). 43. Amgad, M. et al. Structured crowdsourcing enables convolutional segmentation of histology images. Bioinformatics 35, 34613467 (2019). 44. Sitnik, D. et al. dataset and methodology for intraoperative computer-aided diagnosis of metastatic colon cancer in liver. Biomed. Signal Process. Control. 66, 102402 (2021). 45. Stadler, C. B. et al. Proactive construction of an annotated imaging database for artificial intelligence training. J. digital imaging 34, 105115 (2021). 46. Cruz-Roa, A. et al. High-throughput adaptive sampling for whole-slide histopathology image analysis (hashi) via convolutional neural networks: Application to invasive breast cancer detection. PloS one 13, e0196828 (2018). 47. Da, Q. et al. Digestpath: benchmark dataset with challenge review for the pathological detection and segmentation of digestive-system. Med. image analysis 80, 102485 (2022). 48. Li, J. et al. Signet ring cell detection with semi-supervised learning framework. In International conference on information processing in medical imaging, 842854 (Springer, 2019). 49. Sirinukunwattana, K. et al. Gland segmentation in colon histology images: The glas challenge contest. Med. image analysis 35, 489502 (2017). 50. Abdul, S. et al. Histo-seg: H&e whole slide image segmentation dataset. Mendeley Data 1 (2024). 51. Howard, A., HCL-Jevster, G. K., Borner, K., Ryan, H. & Jain, Y. Hubmap-hacking the human vasculature, 2023. URL https://kaggle. com/competitions/hubmap-hacking-the-human-vasculature . 52. Kidney Precision Medicine Project. Kidney precision medicine project (kpmp). https://www.kpmp.org. Accessed January 8, 2026. 53. Deng, R. et al. Kpis 2024 challenge: Advancing glomerular segmentation from patch-to slide-level. arXiv preprint arXiv:2502.07288 (2025). 54. Kludt, C. et al. Next-generation lung cancer pathology: Development and validation of diagnostic and prognostic algorithms. Cell Reports Medicine 5 (2024). 55. Martino, F. et al. Deep learning-based pixel-wise lesion segmentation on oral squamous cell carcinoma images. Appl. Sci. 10, 8285 (2020). 56. Bulten, W. et al. Artificial intelligence for diagnosis and gleason grading of prostate cancer: the panda challenge. Nat. medicine 28, 154163 (2022). 57. van Rijthoven, M. et al. Tumor-infiltrating lymphocytes in breast cancer through artificial intelligence: biomarker analysis from the results of the tiger challenge. medRxiv 202502 (2025). 58. Han, C. et al. Multi-layer pseudo-supervision for histopathology tissue semantic segmentation using patch-level classification labels. Med. Image Analysis 102487 (2022). 23/36 VISTA-PATH tissuelab.org 59. Oner, M. U. et al. An ai-assisted tool for efficient prostate cancer diagnosis in low-grade and low-volume cases. Patterns 3 (2022). 60. Oliveira, M. F. d. et al. High-definition spatial transcriptomic profiling of immune cell populations in colorectal cancer. Nat. Genet. 112 (2025). 61. Janesick, A. et al. High resolution mapping of the tumor microenvironment using integrated single-cell, spatial and in situ analysis. Nat. communications 14, 8353 (2023). 24/36 VISTA-PATH tissuelab.org Data Source AGGC41 BACH42 BCNB39 BRCA43 CoCaHis44 DROID45 DROV45 DRYAD46 DigestPath47, 48 GlaS Histo-Seg50 HuBMAP51 KPMP52 KPIs53 Lung_seg54 ORCA55 PANDA56 TIGER_wsibulk57 TIGER_wsirois57 WSSS4LUAD58 prostate_gland59 Link https://medicalimaging.ai/challenges/aggc22 https://zenodo.org/records/3632035 https://bcnb.grand-challenge.org/ https://github.com/PathologyDataScience/BCSS https://cocahis.irb.hr/ https://datahub.aida.scilifelab.se/10.23698/aida/drsk https://datahub.aida.scilifelab.se/10.23698/aida/drov https://datadryad.org/dataset/doi: 10.5061/dryad.1g2nt41#citations https://digestpath2019.grand-challenge.org/ https://www.kaggle.com/datasets/sani84/ glasmiccai2015-gland-segmentation https://data.mendeley.com/datasets/vccj8mp2cg/1 https://www.kaggle.com/competitions/ hubmap-hacking-the-human-vasculature/data https://www.kpmp.org/available-data https://www.synapse.org/Synapse:syn54077668/wiki/626475 https://zenodo.org/records/12818382 https://sites.google.com/unibas.it/orca https://www.kaggle.com/competitions/ prostate-cancer-grade-assessment/overview/citation https://tiger.grand-challenge.org/Data/ https://tiger.grand-challenge.org/Data/ https://wsss4luad.grand-challenge.org/ https://zenodo.org/records/5971764 Extended Data Table 1. Overview of internal datasets used in our VISTA-PATH Data for pathology image segmentation. 25/ VISTA-PATH tissuelab.org Extended Data Figure 1. Overview of VISTA-PATH Data. a, Bar plots showing the distribution of tissue classes across organs in the VISTA-PATH Dataset, grouped by organ type. Counts are displayed on logarithmic scale. b, Bubble plot summarizing the pathology tissue classes in VISTA-PATH Data, where each bubble represents tissue class and its size is proportional to the number of annotated imagemask instances. c, Bubble plot of pathology tissue class distribution in the BiomedParse dataset, shown using the same visualization scheme as in b, Compared with VISTA-PATH, BiomedParse exhibits substantially fewer tissue categories and more limited semantic coverage. 26/36 VISTA-PATH tissuelab.org Extended Data Figure 2. Overview of the held-out internal datasets used for evaluation. a, Sunburst diagram illustrating the hierarchical organization of tissue classes in the held-out internal evaluation datasets. Inner rings denote organ types, while outer rings correspond to pathology tissue labels. b, Bar plot showing the number of annotated image-mask instances for each organ in the held-out internal evaluation datasets. c, Sankey diagram visualizing the hierarchical semantic structure of tissue types in the held-out internal datasets. Flow width is proportional to the number of samples, with sample counts shown in parentheses. 27/36 VISTA-PATH tissuelab.org Extended Data Figure 3. Overview of the external datasets used for evaluation. a, Sunburst diagram illustrating the hierarchical organization of tissue classes in the external evaluation datasets. Inner rings denote organ types, while outer rings correspond to pathology tissue labels. b, Bar plot showing the number of annotated image-mask instances for each organ in the external evaluation datasets. c, Sankey diagram visualizing the hierarchical semantic structure of tissue types in the external datasets. Flow width is proportional to the number of samples, with sample counts shown in parentheses. 28/ VISTA-PATH tissuelab.org Extended Data Figure 4. Three representative H&E patches and corresponding tumor segmentation overlays from TCGA colorectal cancer cases illustrating low, intermediate, and high Tumor Interation Score (TIS). Risk score represents relative risk ranking derived from our survival model and is independent of sign; higher risk values indicate worse survival outcomes. TNM denotes TumorNodeMetastasis staging. a, The low TIS case is characterized by pronounced tumor budding and infiltrative growth at the invasive front, extending toward the serosal surface, consistent with poorly differentiated tumor and elevated clinical risk. b, The intermediate TIS case exhibits well-demarcated tumor boundary with prominent lymphocytic infiltration in the surrounding stroma, reflecting spatially confined tumor growth and the lowest predicted risk among the examples. c, The high TIS case corresponds to large, highly cohesive tumor mass with extensive tumor area, where excessive tumor burden is associated with renewed increase in predicted risk. 29/36 VISTA-PATH tissuelab.org Data Source Public Pathology Datasets Link LungHP25 OCDC26 Visium HD Datasets Visium_Colon60 Visium_Kidney60 Visium_Lung60 Visium_Pancreas60 Visium_Prostate"
        },
        {
            "title": "Xenium Datasets",
            "content": "Brain-Immuno61 Breast-5K61 Breast-CA61 Breast-Entire61 Cervical-5kPathway61 Colon_Preview61 Heart-MultiCancer61 Kidney-MultiCancer61 Liver-Healthy61 Lung-Cancer61 Lung-GExPanel61 Lung-Immuno61 Ovarian-5KPathway61 Ovarian-Immuno61 Pancreatic-Immuno61 Pancreatic-Cancer61 Pancreatic-Modal61 Skin-5kPathway61 Skin-GExPanel61 Skin-MultiCancer61 Tonsil-Follicular61 Tonsil-Reactive61 Xenium-1k61 https://acdc-lunghp.grand-challenge.org/ https://github.com/dalifreire/tumor_regions_segmentation https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets https://www.10xgenomics.com/datasets Extended Data Table 2. Overview of external datasets for external segmentation evaluation. 30/ VISTA-PATH tissuelab.org AGGC BACH BCNB BRCA CoCaHis DROID DROV DRYAD DigestPath GlaS Histo-Seg HuBMAP KPMP KPIs Lung_seg ORCA PANDA TIGER_wsibulk TIGER_wsirois WSSS4LUAD prostate_gland_segmentation Average VISTA-PATH VISTA-PATH (w/o box) MedSAM BiomedParse Dice (%) 0.845 0.893 0.874 0.690 0.835 0.767 0.878 0.872 0.888 0.886 0.508 0.582 0.779 0.952 0.663 0.602 0.930 0.892 0.590 0.453 0.842 0.772 Dice (%) 0.264 0.290 0.525 0.022 0.695 0.174 0.489 0.467 0.722 0.893 0.218 0.398 0.145 0.382 0.194 0.243 0.417 0.303 0.079 0.304 0.727 0.379 Dice (%) 0.580 0.739 0.729 0.592 0.831 0.670 0.796 0.823 0.808 0.884 0.441 0.549 0.731 0.924 0.483 0.608 0.913 0.846 0.433 0.432 0.841 0. Dice (%) 0.837 0.902 0.822 0.355 0.349 0.723 0.851 0.866 0.750 0.522 0.397 0.421 0.377 0.790 0.249 0.338 0.772 0.863 0.417 0.180 0.425 0.581 Res2Net Dice (%) 0.497 0.308 0.606 0.347 0.854 0.519 0.124 0.700 0.728 0.735 0.462 0.448 0.496 0.697 0.367 0.224 0.849 0.432 0.208 0.541 0.790 0.521 Extended Data Table 3. Segmentation comparison results on held-out internal datasets."
        },
        {
            "title": "Organ\nBreast\nColon\nKidney\nLiver\nLung\nOral\nOvary\nProstate\nSkin",
            "content": "VISTA-PATH MedSAM BiomedParse 0.704 0.636 0.529 0.349 0.215 0.338 0.851 0.678 0.560 0.281 0.808 0.308 0.695 0.249 0.243 0.489 0.469 0.196 0.802 0.887 0.771 0.835 0.558 0.602 0.878 0.872 0.637 MedSAM BiomedParse +0.098 +0.251 +0.242 +0.486 +0.344 +0.264 +0.027 +0.194 +0.077 +0.521 +0.080 +0.463 +0.140 +0.309 +0.359 +0.389 +0.403 +0. Extended Data Table 4. Dice scores across organs on the Internal Evaluation dataset. MedSAM = VISTA-PATH MedSAM; BiomedParse = VISTA-PATH BiomedParse. Positive values indicate VISTA-PATH outperforms the compared method. 31/36 VISTA-PATH tissuelab.org Category"
        },
        {
            "title": "Normal Anatomical Related",
            "content": "Organ Breast Colon Liver Lung Oral Ovary Prostate Breast Kidney Lung Prostate Skin Breast Colon Kidney Lung Oral Ovary Prostate Skin VISTA-PATH MedSAM BiomedParse 0.828 0.754 0.563 0.215 0.362 0.878 0.652 0.462 0.625 0.130 0.883 0.648 0.388 0.351 0.383 0.144 0.146 0.861 0.735 0.793 0.196 0.634 0.623 0.256 0.254 0.495 0.325 0.054 0.078 0.079 0.041 0.036 0.063 0.882 0.091 0.063 0.102 0.027 0.155 0.149 0.862 0.864 0.815 0.553 0.659 0.905 0.838 0.575 0.692 0.548 0.852 0.665 0.701 0.878 0.787 0.565 0.540 0.816 0.926 0.826 MedSAM BiomedParse +0.034 +0.109 +0.251 +0.338 +0.297 +0.027 +0.186 +0.113 +0.067 +0.418 -0.031 +0.018 +0.313 +0.527 +0.405 +0.421 +0.394 -0.045 +0.190 +0.033 +0.666 +0.230 +0.191 +0.297 +0.405 +0.410 +0.512 +0.521 +0.614 +0.469 +0.811 +0.630 +0.637 -0.004 +0.697 +0.503 +0.438 +0.789 +0.771 +0.677 Extended Data Table 5. Dice scores across organs, grouped into three major tissue categories, on the internal evaluation dataset. MedSAM = VISTA-PATH MedSAM; BiomedParse = VISTA-PATH BiomedParse. Statistical significance is assessed using two-sided Students t-test. < 0.05; < 1 102; < 1 103. 32/36 VISTA-PATH tissuelab.org VISTA-PATH VISTA-PATH (w/o box) MedSAM BiomedParse Dice (%) Dice (%) Dice (%) Dice (%) Public Pathology Datasets LungHP OCDC Visium HD Datasets Visium_Colon Visium_Kidney Visium_Lung Visium_Pancreas Visium_Prostate Xenium Datasets Brain-Immuno Breast-5K Breast-CA Breast-Entire Cervical-5kPathway Colon_Preview Heart-MultiCancer Kidney-MultiCancer Liver-Healthy Lung-Cancer Lung-GExPanel Lung-Immuno Ovarian-5KPathway Ovarian-Immuno Pancreatic-Immuno Pancreatic-Cancer Pancreatic-Modal Skin-5kPathway Skin-GExPanel Skin-MultiCancer Tonsil-Follicular Tonsil-Reactive Xenium-1k Average 0.495 0.802 0.694 0.832 0.506 0.373 0. 0.231 0.313 0.388 0.374 0.362 0.610 0.410 0.427 0.346 0.369 0.392 0.475 0.301 0.404 0.527 0.555 0.449 0.408 0.620 0.454 0.599 0.474 0.523 0.454 0.346 0.787 0.621 0.579 0.422 0.253 0.143 0.193 0.246 0.230 0.228 0.286 0.510 0.288 0.267 0.121 0.251 0.302 0.337 0.251 0.312 0.446 0.378 0.355 0.297 0.475 0.351 0.461 0.291 0.417 0.340 0.121 0.528 0.556 0.812 0.324 0.329 0. 0.154 0.202 0.314 0.296 0.290 0.537 0.337 0.363 0.261 0.298 0.304 0.372 0.244 0.386 0.487 0.514 0.371 0.361 0.465 0.408 0.546 0.435 0.471 0.373 0.129 0.751 0.242 0.266 0.149 0.093 0.064 0.150 0.245 0.112 0.082 0.275 0.209 0.131 0.104 0.052 0.183 0.247 0.224 0.268 0.175 0.251 0.179 0.306 0.289 0.159 0.333 0.184 0.172 0.186 0.199 Extended Data Table 6. Segmentation comparison results on external datasets. 33/ VISTA-PATH tissuelab.org"
        },
        {
            "title": "Organ\nBrain\nBreast\nCervical\nColon\nHeart\nKidney\nLung\nOral\nOvarian\nPancreas\nProstate\nSkin\nTonsil",
            "content": "VISTA-PATH MedSAM BiomedParse 0.154 0.429 0.290 0.546 0.337 0.587 0.284 0.528 0.315 0.425 0.217 0.411 0.490 0.150 0.182 0.275 0.226 0.131 0.185 0.186 0.751 0.222 0.207 0.064 0.260 0.178 0.231 0.479 0.362 0.652 0.410 0.630 0.447 0.802 0.353 0.476 0.249 0.494 0.537 MedSAM BiomedParse +0.077 +0.050 +0.071 +0.106 +0.073 +0.042 +0.163 +0.275 +0.037 +0.050 +0.033 +0.083 +0.046 +0.081 +0.297 +0.087 +0.426 +0.279 +0.445 +0.261 +0.051 +0.131 +0.268 +0.185 +0.234 +0. Extended Data Table 7. Dice scores across organs on the external evaluation datasets. MedSAM = VISTA-PATH MedSAM; BiomedParse = VISTA-PATH BiomedParse. Positive values indicate VISTA-PATH outperforms the compared method. 34/36 VISTA-PATH Category Tumor Related Microenvironment Related"
        },
        {
            "title": "Normal Anatomical Related",
            "content": "Organ Brain Breast Cervical Colon Lung Oral Ovary Pancreas Prostate Breast Cervical Colon Heart Kidney Lung Ovary Pancreas Prostate Skin Tonsil Brain Breast Cervical Colon Heart Kidney Lung Ovary Pancreas Prostate Skin Tonsil VISTA-PATH MedSAM BiomedParse 0.125 0.383 0.518 0.662 0.401 0.251 0.446 0.604 0.371 0.265 0.262 0.517 0.252 0.298 0.259 0.216 0.383 0.156 0.335 0.499 0.108 0.453 0.533 0.628 0.353 0.483 0.299 0.191 0.516 0.162 0.488 0.487 0.203 0.150 0.364 0.430 0.323 0.595 0.411 0.217 0.240 0.074 0.127 0.046 0.007 0.038 0.060 0.075 0.085 0.039 0.093 0.235 0.128 0.326 0.514 0.126 0.023 0.063 0.141 0.121 0.286 0.099 0.081 0.223 0.217 0.511 0.787 0.709 0.544 0.720 0.703 0.622 0.465 0.304 0.309 0.654 0.277 0.299 0.346 0.246 0.439 0.235 0.457 0.550 0.187 0.553 0.551 0.694 0.454 0.562 0.350 0.169 0.578 0.112 0.490 0.506 tissuelab.org MedSAM BiomedParse +0.092 +0.128 +0.269 +0.048 +0.143 +0.469 +0.257 +0.018 +0.095 +0.040 +0.046 +0.137 +0.024 +0.001 +0.087 +0.029 +0.056 +0.079 +0.122 +0.051 +0.078 +0.100 +0.019 +0.066 +0.101 +0.079 +0.051 -0.023 +0.061 -0.050 +0.003 +0.019 +0.014 +0.361 +0.423 +0.279 +0.221 +0.125 +0.292 +0.405 +0.225 +0.230 +0.181 +0.608 +0.270 +0.261 +0.286 +0.171 +0.353 +0.196 +0.364 +0.315 +0.058 +0.227 +0.037 +0.568 +0.431 +0.499 +0.209 +0.048 +0.292 +0.013 +0.409 +0.283 Extended Data Table 8. Dice scores across organs, grouped into three major tissue categories, on the external evaluation dataset. MedSAM = VISTA-PATH MedSAM; BiomedParse = VISTA-PATH BiomedParse. Statistical significance is assessed using two-sided Students t-test. < 0.05; < 1 102; < 1 103."
        },
        {
            "title": "Method",
            "content": "Visium_Colon Visium_Kidney Visium_Prostate 0 VISTA-PATH 0.629 0.568 Patch 0.563 MedSAM 0 VISTA-PATH 0.118 0.118 Patch 0.118 MedSAM VISTA-PATH 0.424 Patch 0.409 0.410 MedSAM 226 0.627 0.559 0.561 10 0.117 0.112 0.118 81 0.424 0.408 0.410 Number of Patches 1051 321 0.861 0.665 0.804 0.595 0.765 0.584 65 33 0.198 0.343 0.308 0.228 0.270 0.159 240 136 0.805 0.603 0.769 0.576 0.774 0.527 1079 0.857 0.797 0.755 146 0.401 0.418 0.291 288 0.818 0.776 0.788 1175 0.867 0.808 0.769 158 0.587 0.533 0.476 296 0.810 0.763 0.775 172 0.563 0.534 0. Extended Data Table 9. Human-in-the-loop results on Visium HD datasets. Dice scores are reported. 35/36 VISTA-PATH tissuelab.org Dataset Method Number of Patches Cervical-5kPathway Ovarian-5KPathway Breast-5K Skin-MultiCancer Lung-Immuno Lung-GExPanel 0 VISTA-PATH 0.429 0.372 Patch MedSAM 0.364 0 VISTA-PATH 0.375 0.352 Patch MedSAM 0.345 0 VISTA-PATH 0.265 0.242 Patch MedSAM 0.232 0 VISTA-PATH 0.418 Patch 0.339 0.393 MedSAM VISTA-PATH 0.413 Patch 0.335 0.319 MedSAM 0 VISTA-PATH 0.421 Patch 0.377 0.373 MedSAM 153 0.429 0.372 0.364 71 0.366 0.335 0.342 166 0.265 0.242 0.232 199 0.267 0.268 0.255 55 0.413 0.335 0.319 296 0.421 0.376 0.373 932 0.773 0.667 0.650 313 0.851 0.773 0.725 543 0.556 0.533 0.421 537 0.269 0.277 0.261 74 0.434 0.375 0.328 345 0.501 0.473 0.415 1332 0.758 0.650 0.670 376 0.845 0.745 0.725 614 0.584 0.560 0.435 702 0.474 0.507 0.427 94 0.616 0.543 0.417 1027 0.467 0.447 0.396 1350 0.771 0.724 0.586 386 0.838 0.734 0.747 744 0.558 0.457 0.473 767 0.583 0.537 0.533 139 0.666 0.569 0.468 1081 0.495 0.469 0. 390 0.843 0.748 0.750 789 0.641 0.558 0.508 877 0.693 0.610 0.578 239 0.709 0.646 0.585 1285 0.539 0.542 0.474 274 0.703 0.672 0.547 1467 0.605 0.595 0.539 354 0.723 0.678 0.590 1599 0.607 0.606 0.555 555 0.745 0.716 0.626 1707 0.553 0.541 0. 1815 0.574 0.559 0.504 Extended Data Table 10. Human-in-the-loop results on Xenium datasets. Dice scores are reported. 36/"
        }
    ],
    "affiliations": [
        "Emory University",
        "Georgia Institute of Technology",
        "NVIDIA Corporation",
        "University of Pennsylvania"
    ]
}