{
    "paper_title": "DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving via Online HD Map Diffusion",
    "authors": [
        "Zhigang Sun",
        "Yiru Wang",
        "Anqing Jiang",
        "Shuo Wang",
        "Yu Gao",
        "Yuwen Heng",
        "Shouyi Zhang",
        "An He",
        "Hao Jiang",
        "Jinhao Chai",
        "Zichong Gu",
        "Wang Jijun",
        "Shichen Tang",
        "Lavdim Halilaj",
        "Juergen Luettin",
        "Hao Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Autonomous driving requires accurate scene understanding, including road geometry, traffic agents, and their semantic relationships. In online HD map generation scenarios, raster-based representations are well-suited to vision models but lack geometric precision, while graph-based representations retain structural detail but become unstable without precise maps. To harness the complementary strengths of both, we propose DiffSemanticFusion -- a fusion framework for multimodal trajectory prediction and planning. Our approach reasons over a semantic raster-fused BEV space, enhanced by a map diffusion module that improves both the stability and expressiveness of online HD map representations. We validate our framework on two downstream tasks: trajectory prediction and planning-oriented end-to-end autonomous driving. Experiments on real-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate improved performance over several state-of-the-art methods. For the prediction task on nuScenes, we integrate DiffSemanticFusion with the online HD map informed QCNet, achieving a 5.1\\% performance improvement. For end-to-end autonomous driving in NAVSIM, DiffSemanticFusion achieves state-of-the-art results, with a 15\\% performance gain in NavHard scenarios. In addition, extensive ablation and sensitivity studies show that our map diffusion module can be seamlessly integrated into other vector-based approaches to enhance performance. All artifacts are available at https://github.com/SunZhigang7/DiffSemanticFusion."
        },
        {
            "title": "Start",
            "content": "DiffSemanticFusion: Semantic Raster BEV Fusion for Autonomous Driving via Online HD Map Diffusion Zhigang Sun1, Yiru Wang1,6, Anqing Jiang1, Shuo Wang1, Yu Gao1, Yuwen Heng1, Shouyi Zhang1, An He1, Hao Jiang3, Jinhao Chai2, Zichong Gu2, Wang Jijun4, Shichen Tang1, Lavdim Halilaj5, Juergen Luettin5, Hao Sun1 5 2 0 2 3 ] . [ 1 8 7 7 1 0 . 8 0 5 2 : r Abstract Autonomous driving requires accurate scene understanding, including road geometry, traffic agents, and their semantic relationships. In online HD map generation scenarios, raster-based representations are well-suited to vision models but lack geometric precision, while graph-based representations retain structural detail but become unstable without precise maps. To harness the complementary strengths of both, we propose DiffSemanticFusiona fusion framework for multimodal trajectory prediction and planning. Our approach reasons over semantic rasterfused BEV space, enhanced by map diffusion module that improves both the stability and expressiveness of online HD map representations. We validate our framework on two downstream tasks: trajectory prediction and planning-oriented end-to-end autonomous driving. Experiments on real-world autonomous driving benchmarks, nuScenes and NAVSIM, demonstrate improved performance over several state-of-the-art (SOTA) methods. For the prediction task on nuScenes, we integrate DiffSemanticFusion with the online HD map informed QCNet, achieving 5.1% performance improvement. For end-to-end autonomous driving in NAVSIM, DiffSemanticFusion achieves SOTA results, with 15% performance gain in NavHard scenarios. In addition, extensive ablation and sensitivity studies show that our map diffusion module can be seamlessly integrated into other vector-based approaches to enhance performance. All artifacts are available at: https: //github.com/SunZhigang7/DiffSemanticFusion I. INTRODUCTION End-to-end autonomous driving has attracted increasing attention due to recent progress in learning based perception and planning systems, including object detection [1] [3], multi-object tracking [4], [5], online mapping [6][9], prediction [10][13] and planning [14][16]. These advances have enabled the learning of driving policies directly from raw sensor inputs, bypassing the need for hand-crafted rules. Two-stage end-to-end frameworks decompose the driving task into intermediate perception and planning modules, typically leveraging structured representations such as birdseye view (BEV) maps or object lists. This design enhances interpretability and training stability, but may suffer from sub-optimal performance due to error propagation between stages and limited capacity for joint optimization. While 1 Zhigang Sun, Yiru Wang, Anqing Jiang, Shuo Wang, Yu Gao, Yuwen Heng, Shouyi Zhang, An He, Shichen Tang, Hao Sun are with Bosch Corporate Research, Bosch (China) Investment Ltd., Shanghai, China. * means equal contribution 2 Jinhao Chai, Zichong Gu are with School of Communication and Information Engineering, Shanghai University, Shanghai, China 3 Hao Jiang is with Shanghai Jiaotong University, Shanghai, China 4 Wang Jijun, is with AIR, Tsinghua University, Beijing, China 5 Lavdim Halilaj, Juergen Luettin are with Robert Bosch GmbH 6 represents corresponding author, yiru.wang@cn.bosch.com Fig. 1. Semantic Raster Image Fusion map-informed prediction and planning have shown notable benefits by using high-definition (HD) maps to provide structural context, their effectiveness often diminishes in scenarios where online maps are incomplete, noisy, or misaligned. Such limitations can significantly affect downstream tasks like motion prediction and planning, particularly in dynamic or previously unseen environments. Moreover, strict reliance on pre-defined map structures can hinder generalization and robustness in real-world where map inaccuracies are inevitable. To address these challenges, we propose online HD map diffusion, novel framework that enhances the robustness of map-informed prediction under online mapping conditions. Rather than treating the map as fixed input, map diffusion introduces learnable mechanism that iteratively refines and denoises online map representations via diffusion-based generative process. This approach allows the model to adaptively recover reliable map features, even in the presence of uncertainty or noise, thereby improving the quality of trajectory prediction and downstream planning. By bridging the gap between static HD maps and dynamic online mapping, online HD map diffusion offers resilient and scalable solution for safe and efficient autonomous driving. In contrast, one-stage end-to-end approaches [17][21] directly regress control commands or trajectories from raw sensor inputs, enabling full-system differentiability and potentially higher performance. However, they often face challenges related to interpretability, data efficiency, and deployment in safety-critical settings. To combine the advantages of both paradigms, we propose semantic raster BEV fusion framework that incorporates intermediate representations within an end-to-end trainable architecture. This design retains the interpretability and structure of two-stage pipelines while enabling the global optimization benefits of one-stage systems. Such integration has shown promise Fig. 2. DiffSemanticFusion Overview: Sparse Perception utilizes Sparse4D [25] to extract bounding boxes of dynamic objects and map elements. Dense Perception utilizes LSS [26] and BEVDet [1] to extract BEV features. Vectorized Graph utilizes SemanticFormer [27] to extract graph information. Fusion fuses heterogeneous representations to unified space and Diffusion Planner serves as trajectories decoder. in achieving both high performance and reliable decisionmaking in complex driving scenarios. The main contributions are: We propose the online HD map diffusion module, which improves the robustness of online HD maps under noisy or incomplete conditions, enhancing downstream prediction and planning performance. We design semantic raster BEV fusion architecture that combines raster-based, graph-based map and BEVfeature representations in BEV space, leveraging their complementary strengths for more stable and informative scene understanding. We validate our method on two tasks: prediction on nuScenes [22] and planning oriented end-to-end autonomous driving in NAVSIM [23]. Both achieve SOTA performance. DiffSemanticFusion improves online HD map informed QCNet [24] prediction by 5.1% and achieves 15% gain in NAVSIM Navhard planning scenarios. Extensive studies also show strong compatibility with other vector-based methods. II. RELATED WORK A. Traffic Scene Representation Raster-based representation approaches encode the entire traffic scene into birds-eye-view (BEV) images with multiple channels, where each channel represents different elements of the scenes [10], [11], [28]. On top of these raster representations, convolutional neural networks (CNNs) are typically employed to extract features from the scene. The learned representation is then passed through fully connected layers to generate trajectory predictions. However, major limitation of raster-based models is their reliance on raw pixel-level input, which lacks high-level semantic understanding and requires networks to infer complex relationships directly from image data. In contrast, graph-based representation methods model traffic scenes as collections of vectors, polylines, or graphs [13], [29][31]. These approaches operate at higher level of abstraction, removing the need to learn from low-level pixels and thereby enhancing robustness to scene variations. For example, VectorNet [13] encodes map features and agent trajectories as polylines and models their interactions using global graph. However, many graph-based models are limited to homogeneous graphs, representing only single type of entity and relation. To overcome this, recent works have introduced heterogeneous graph representations, which incorporate diverse entity types (e.g., vehicles, bicycles, pedestrians) and relation types (e.g., agent-to-lane, agent-to-agent) [27], [32][36]. For instance, SemanticFormer [27] generates multimodal trajectories by reasoning over semantic traffic scene graph using attention mechanisms and graph neural networks. While powerful, such approaches often involve complex graph construction and incur significant computational overhead. To address these challenges, we propose DiffSemanticFusion, which combines the strengths of both rasterand graphbased representations by fusing features in BEV space. This hybrid approach retains the advantages of explicit geometric structure and global semantic context, offering more expressive and efficient solution for trajectory prediction. B. Diffusion Mechanism in Autonomous Driving Diffusion models, initially proposed for image generation tasks [37], have recently been extended to broad spectrum of domains beyond computer vision. In the field of autonomous driving, they have demonstrated notable effectiveness in trajectory prediction [38], [39], motion planning [16], and end-to-end autonomous driving [40]. MotionDiffuser [38] adopts conditional diffusion model to generate target trajectories from Gaussian noise. Diffusion Planner [16] employs customized diffusion-based architecture to enable high-performance motion planning, while mitigating the dependence on hand-crafted rules. DiffusionDrive [40] further advances this line of work by introducing truncated diffusion policy and an efficient decoding mechanism for real-time, end-to-end autonomous driving. While these approaches leverage diffusion models primarily for direct trajectory generation, our work explores different dimension. To the best of our knowledge, we are the first to propose an online HD map diffusion module from the perspective of prediction and planning. This module is designed to enhance the stability and consistency of online HD maps, which are critical for robust long-term autonomy. C. End-to-End Autonomous Driving Previous end-to-end autonomous driving approaches typically rely on single scene representation. UniAD [17] pioneers the integration of multiple perception tasks to enhance planning performance within unified framework. VAD [18] introduces compact vectorized scene representations to improve computational efficiency. Building upon this, line of work [40][44] adopts the single-trajectory prediction paradigm for improved planning accuracy. More recently, VADv2 [19] explores multi-mode planning by scoring and selecting from fixed trajectory set, while Hydra-MDP [45] enhances this approach by incorporating rule-based supervision. SparseDrive [20] investigates an alternative, BEVfree pipeline. DiffusionDrive [40] proposes generative framework leveraging diffusion models for diverse and highquality trajectory generation. Despite these advances, existing methods predominantly rely on either dense BEV or sparse representations. In contrast, we present the first endto-end autonomous driving framework that unifies sparse scene representations with dense BEV features, enabling complementary strengths of both modalities. Our approach demonstrates superior planning performance by leveraging the structured abstraction of sparse and dense BEV representations in unified architecture. III. METHOD A. Preliminary t, yi t)(cid:9)0 pred = (cid:8)(xi 1) Problem Formulation: Trajectory prediction in modular autonomous driving focuses on forecasting the future motion of surrounding agents (e.g., vehicles, pedestrians, cyclists) is predicted based on their past observed trajectories. At the current time step = 0, the observed trajectory of an agent over the past Th steps is represented as obs = (cid:8)(xi τ t=Th+1. The goal is to predict the future trajectory over the next Tf steps, τ t=1. All trajectories are represented in consistent coordinate system, such as the map coordinate system or an agent-centric coordinate system. The prediction model may output one or more trajectory hypotheses per agent to account for the uncertainty and multimodal nature of real-world behaviors. Planning-oriented end-to-end autonomous driving takes raw sensor data as input and predicts the future trajectory of the ego-vehicle. The predicted trajectory is represented as sequence of waypoints, τego = {(xt, yt)}Tf t=1, where Tf denotes the planning horizon, and (xt, yt) is the location of each waypoint at time t, represented in the current egovehicle coordinate system. t)(cid:9)Tf t, yi 2) Conditional Diffusion Model: The conditional diffusion framework models the generation process by introducing noise to clean data sample through forward stochastic process. At each diffusion step i, the perturbed sample τ is generated from the original sample τ 0 by equation 1. (cid:0)τ τ 0(cid:1) = (cid:0)τ i; αi τ 0, (1 αi) I(cid:1) (1) Fig. 3. Mapless QCNet Encoder with Online HD Map Diffusion where the superscript denotes the diffusion timestep, τ 0 is the clean (ground-truth) data, and αi = (cid:81)i s=1 αs = (cid:81)i s=1(1 βs) is the accumulated product of noise coefficients. Here, βs represents the predefined noise schedule. To recover the clean sample from the noisy observation, we train reverse process model fθ(τ i, z, i), where θ are the learnable parameters and denotes the conditioning input. The model aims to estimate the original sample τ 0 conditioned on both τ and z. During inference, we begin with an initial sample τ drawn from Gaussian prior and iteratively denoise it using the learned reverse transitions. The overall sampling objective conditioned on is expressed as equation 2. pθ(τ 0 z) = (cid:90) p(τ ) (cid:89) i=1 pθ(τ i1 τ i, z) dτ 1:T (2) B. Online HD Map Diffusion Module After online map perception, we can get the map information like divider, boundary, pedestrian crossing represented by an ordered sequence of sparse lane vectors as L1:N = {v1, v2, . . . , vN }, where denotes the total vector length. Each lane vector vn = [dn,s, dn,e, an] where dn,s, dn,e denote the start and end points, respectively, and an corresponds to lane point ns attribute features, such as orientation, polyline type, etc. We first construct the diffusion process by adding Gaussian noise to sparse lane vectors {vk}N k=1 on the training set. We truncate the diffusion noise schedule to diffuse the lane vectors to the lane vector based Gaussian distribution as equation 3. τ = (cid:112) αivk + 1 αiϵ, ϵ (0, I) (3) where [1, Ttrunc] and Ttrunc is the truncated diffusion steps. During training, the diffusion decoder fθ takes as input all noisy sparse lane vectors {τ k=1 and predicts denoised sparse lane vectors {ˆτk}N k=1 = fθ({τ k}N k=1 as equation 4. k}N k=1, z) {ˆτk}N (4) where represents the conditional information. We calculate the MSELoss between all the denoised ordered sequence of sparse lane vectors {ˆτk}N k=1. Online map will generate lane ordered sequence of sparse lane vectors, then online map diffusion training objective is shown in equation 5, represents vector in ms lane. k=1 and ground truth {τgt}N = Nlane(cid:88) (cid:88) m=1 k=1 LMSE(ˆτkm, τgt) (5) TABLE PERFORMANCE ON THE NAVSIM-V2 NAVTEST BENCHMARK FOR PLANNING ORIENTED END-TO-END AUTONOMOUS DRIVING. Method Backbone Sensor NC DAC DDC TLC Human Agent Ego Status MLP - - Transfuser Diffusiondrive [40] HydraMDP++ [43] DriveSuprim [44] DiffSemanticFusion WOTE [42] Diffusiondrive [40] HydraMDP++ [43] DriveSuperim [44] DiffSemanticFusion ResNet34 ResNet34 ResNet34 ResNet34 ResNet34 V2-99 V2-99 V2-99 V2-99 V2-99 - camera camera camera camera camera camera camera camera camera camera camera 100 93. 96.9 98.0 97.2 97.5 98.4 98.5 98.2 98.4 97.8 98.5 100 77.9 89.9 96.0 97.5 96.5 96.3 96.8 96.3 98.0 97.9 97.4 99.8 92. 97.8 99.5 99.4 99.4 99.5 98.8 99.6 99.4 99.5 99.5 100 99.6 99.7 99.8 99.6 99.6 99.8 99.8 99.8 99.8 99.9 99.8 EP 87.4 86.0 87.1 87.7 83.1 88.4 88.5 86.1 87.5 87.5 90.6 88.6 TTC LK HC EC EPDMS 100 91.5 95.4 97.1 96.5 96.6 97.6 97.9 97.5 97.7 97.1 97.5 100 89. 92.7 97.2 94.4 95.5 97.5 95.5 97.1 95.3 96.6 97.7 98.1 98.3 98.3 98.3 98.2 98.3 98.4 98.3 98.3 98.3 98.3 98.3 90.1 85. 87.2 87.6 70.9 77.0 87.7 82.9 87.7 77.4 77.9 88.0 90.3 64.0 76.7 84.3 81.4 83.1 85.1 84.2 85.0 85.1 86.0 86.5 TABLE II PERFORMANCE ON THE NAVSIM-V2 NAVHARD BENCHMARK FOR PLANNING ORIENTED END-TO-END AUTONOMOUS DRIVING. Method Backbone Stage NC DAC DDC TLC EP TTC LK HC EC EPDMS PDM-Closed [46] - LTF [41] ResNet34 DiffusionDrive [40] ResNet34 WOTE [42] ResNet"
        },
        {
            "title": "DiffSemanticFusion",
            "content": "ResNet"
        },
        {
            "title": "DiffSemanticFusion",
            "content": "V2-99 Stage 1 Stage 2 Stage 1 Stage 2 Stage 1 Stage 2 Stage 1 Stage 2 Stage 1 Stage Stage 1 Stage 2 94.4 88.1 96.2 77.7 95.9 79.5 97.4 81.2 99.1 80. 98.6 81.8 98.8 90.6 79.5 70.2 84.0 72.8 88.2 77.7 88.2 73. 90.6 75.3 100 96.3 99.1 84.2 98.6 84.1 97.7 84.8 99.7 86. 99.5 87.7 99.5 98.5 99.5 98.0 99.8 98.4 99.3 98.1 99.5 98. 99.5 98.0 100 100 84.1 85.1 84.4 87.5 82.7 85.9 83.6 85. 83.4 85.0 93.5 83.1 95.1 75.6 96.0 76.2 96.4 78.5 97.5 76. 97.3 78.8 99.3 73.7 94.2 45.4 95.1 46.6 90.8 46.2 97.1 49. 96.8 48.6 87.7 91.5 97.5 95.7 97.6 96.1 97.3 96.6 97.5 95. 97.5 96.1 36.0 25.4 79.1 75.9 71.1 62.4 68.0 63.3 76.8 62. 70.6 61.5 51.3 23.1 26.1 27.9 32.2 (+15%) 33.4 (+19%) Online map diffusion module is agnostic to the coordinate system and can be seamlessly integrated into any vectorbased online HD map, whether the vectors are represented in cartesian or polar coordinates. C. Semantic Raster Image Fusion Module After the upstream hybrid perception module, three distinct representations of the traffic scene are obtained: BEV feature map RC1H1W1, an actor-specific BEV raster image RC2H2W2 and heterogeneous traffic scene graph = (V, E, τ, ϕ), which has nodes , their types τ (v), and edges (u, v) E, with edge types ϕ(u, v). To effectively exploit the complementary characteristics of these heterogeneous representations, different learning strategies are proposed to map them into unified space RCHW . For the BEV feature, we adopt the BEV feature projection method from BEVDet [1] to implicitly map raw sensor data into unified space . For BEV raster image R, any CNN architecture can be used as the base network to map the original image to unified space . Following the [10], we use MobileNet-v2 [47]. For heterogeneous traffic scene graph G, we utilize the Semanticformer [27] to map the complex graph to hidden space RC. Then the graph nodes are projected onto the unified space based on their geometric coordinates (x, y, z) by applying perspective-to-top-down transformation, enabling spatially consistent reasoning in the planar domain. Simply concatenating the graph nodes with the unified spatial representation often leads to highly sparse feature maps, where most spatial locations contain little to no meaningful information. To mitigate this, we aggregate the graph node features into through channelwise addition, promoting denser feature propagation while preserving the underlying geometric structure. The Semantic raster image fusion module projects heterogeneous representations into unified spatial space RCHW , facilitating the construction of coherent BEV feature map that effectively supports various downstream tasks such as motion prediction and planning. D. DiffSemanticFusion Architecture We employ hybrid perception module to extract sparse and dense representations of traffic scene as shown in figure 2. Dense perception module utilizes LSS [26] and BEVDet [1] to extract BEV features RC1H1W1 . And sparse perception module utilizes Sparse4D [25] to extract bounding boxes of dynamic objects and map elements as shown in equation 6 and 7. (cid:9) (6) (7) {x, y, z, ln w, ln h, ln l, sin yaw, cos yaw, vx, vy, vz} (cid:8)x0, y0, x1, y1, ..., xNp1, yNp1 The vectorized graph module encodes structured semantic information, which is first refined and completed using an online HD map diffusion module. Then, following the VectorNet [13] approach, the elements are represented as fully connected graph where information is propagated according to the geometric relationships of traffic elements in the scene. To further enhance this representation, we adopt the SemanticFormer [27] framework to perform heterogeneous graph"
        },
        {
            "title": "TABLE III\nPERFORMANCE ON THE NUSCENES BENCHMARK FOR MAPLESS PREDICTION",
            "content": "Method Source Prediction Online Map Diffusion Options ADE AHE FDE FHE MR SATP [48] SATP [48] SATP [48] Unc. [50] Unc. [50] SATP [48] SATP [48] SATP [48] Unc. [50] Unc. [50] SATP [48] SATP [48] SATP [48] Ours Ours Ours Ours Ours Ours Ours Ours Ours SD DenseTNT [49] DenseTNT [49] MapTRv2 [7] DenseTNT [49] MapTRv2 [7] + SATP DenseTNT [49] DenseTNT [49] StreamMapNet [8] StreamMapNet [8] + Unc. HiVT [51] HiVT [51] HiVT [51] HiVT [51] HiVT [51] QCNet [24] QCNet [24] QCNet [24] SD MapTRv2 [7] MapTRv2 [7] + SATP StreamMapNet [8] StreamMapNet [8] + Unc. SD MapTRv2 [7] MapTRv2 [7] + SATP QCNet [24] StreamMapNet [8] QCNet [24] QCNet [24] QCNet [24] QCNet [24] QCNet [24] QCNet [24] QCNet [24] QCNet [24] StreamMapNet [8] StreamMapNet [8] StreamMapNet [8] StreamMapNet [8] StreamMapNet [8] StreamMapNet [8] StreamMapNet [8] StreamMapNet [8] - - - - - - - - - - - - - - - - - - - - - - - - - - - - Transformer Transformer Transformer Transformer 1-D U-Net 1-D U-Net 1-D U-Net 1-D U-Net pt2pl pl2pl cart. All pt2pl pl2pl cart. All 1.379 1.174 1.017 0.949 0.903 0.454 0.399 0.366 0.397 0. 0.401 0.385 0.362 0.354 0.340 0.346 0.345 0.339 0.341 0.346 0.345 0.336 (5%) - - - - - - - - - - - - - 0.324 0.320 0.308 0.319 0.309 0.318 0.308 0.319 0.305 2.255 2.191 1.920 1.740 1.645 0.890 0.832 0.715 0.818 0. 0.871 0.801 0.673 0.717 0.699 0.699 0.698 0.681 0.705 0.699 0.698 0.673 (6%) - - - - - - - - - - - - - 0.407 0.361 0.328 0.286 0.281 0.375 0.328 0.286 0.309 0.444 0.403 0.379 0.256 0.235 0.107 0.095 0.071 0.092 0. 0.091 0.087 0.069 0.068 0.0633 0.0626 0.0624 0.0581 0.0637 0.0626 0.0624 0.0549 (19%) learning, projecting the structured traffic scene elements into graph hidden space. The Raster Image module rasterizes the structured information into an image, which is then processed by MobileNet-v2 [47] network to extract features projected into the raster image hidden space. The Fusion module combines features from the BEV representation (dense perception), graph hidden space (structured vector graph), and image hidden space (rasterized semantic map). These features are either concatenated or added before being passed to the diffusion-based planner module. The planner then decodes the final trajectory output through denoising diffusion process. E. Motion Prediction with Online HD Map Diffusion To evaluate the effectiveness of our proposed online HD map diffusion module, we integrate it into the prediction backbone of QCNet [24], as illustrated in Figure 3. Specifically, we replace the original HD map used in QCNet with the online HD map generated by StreamMapNet [8]. Since the original map module in QCNet operates in polar coordinates, we incorporate the online HD map diffusion module at both the point-to-polygon and polygon-to-polygon stages under the polar coordinate representation. To further validate the generality of our approach, we also convert the polar map representation into Cartesian coordinates and apply the online HD map diffusion module in this domain as well. The entire model is trained in an end-to-end manner, where the diffusion loss is jointly optimized with the original QCNet loss, enabling unified training and seamless integration of the proposed module. IV. EXPERIMENTS A. Dataset nuScenes [22] is real-world autonomous driving dataset, which includes 1,000 driving scenarios over 20 seconds. It provides agent trajectories, offline groundtruth HD maps, and sensor data. We apply the methodology in [50] to upsample nuScenes data frequency to fit trajectory prediction models, which aim to predict 3-second future trajectories based on 2-second historical motion information. NAVSIM [23] is planning-oriented autonomous driving dataset built on OpenScene, redistribution of nuPlan [52]. It provides eight 19201080 cameras and fused LiDAR point cloud aggregated from five sensors across the current and three previous frames. The dataset is split into navtrain (1,192 training scenes) and navhard (136 evaluation scenes). B. Metrics For trajectory prediction, We utilize standard evaluation metrics to assess the prediction performance, which include minF DE6, minADE6, R6. These metrics are calculated based on the best one out of 6 predicted trajectories. FDE refers to the L2 distance between the predicted trajectory and the ground-truth trajectory at the last frame, while ADE is the average L2 error across all predicted time steps. MR (Miss Rate) refers to the proportion of predicted trajectories with an FDE larger than 2 meters. In our tables, we just use ADE, FDE, and MR to represent minF DE6, minADE6 and R6 for simplicity. For end-to-end autonomous driving, we follow the official metrics of NAVSIM, Extend Predictive Driver Model Score (EPDMS) as its closed-loop planning metric as shown in equation 8, EPDMS = (cid:89) mMpen (cid:124) filterm(agent, human) (cid:123)(cid:122) penalty terms (cid:125) (cid:80) mMavg (cid:124) wm filterm(agent, human) (cid:80) wm mMavg (cid:123)(cid:122) weighted average terms (cid:125) (8) In equation, where EPDMS integrates two sub-metrics group: Mpen = {NC, DAC, DDC, TLC} and Mavg = {TTC, EP, HC, LK, EC}. No At-Fault Collision (NC), Drivable Area Compliance (DAC), Driving Direction Compl (DDC), Lane Keeping(LK),Time-to-Collision (TTC), History Comfort (HC), Extended Comfort(EC), Traffic Light Compl. (TLC) and Ego Progress (EP) to produce comprehensive closed-loop planning score. C. Model Implementation For trajectory prediction, we adopt the online HD map diffusion driven QCNet [24]. The hidden dimension is set to 128. The map polygon search radius is set to 150, with both the map polygon-to-agent and agent-to-agent search radii set to 50. The diffusion process uses timestep of 10. The model is trained using 2 NVIDIA A100 GPUs with batch size of 20, and 50 epochs. We employ the AdamW optimizer with an initial learning rate of 5104 and weight decay of 1 104. For planning oriented end-to-end autonomous driving, the BEV size is set 512128128 and the hidden dimension is set 512. The model is trained using 8 A100 GPUs with batch size of 128, and 100 epochs. We employ the AdamW optimizer and cosine learning rate decay policy. D. Quantitative Results For trajectory prediction, we evaluate our approach on the nuScenes benchmark, as summarized in Table III. As shown, online HD map diffusion-informed QCNet achieves SOTA performance across all metrics, improving performance by 5.1% compared to the previous best method. Notably, it exhibits significant reduction in Miss Rate (MR), indicating that the diffusion mechanism effectively enhances the stability and reliability of the online HD map. For planning-oriented end-to-end autonomous driving, we further evaluate our approach on the Navsim-v2 benchmark, which includes two splits: Navtest and Navhard. As summarized in Table and II, our proposed DiffSemanticFusion achieves the best performance in the EPDMS metric on both splits, particularly on the challenging Navhard split, where it yields an improvement of approximately 19%. This result suggests that our method is more robust and generalizes better across diverse and complex driving scenarios. E. Ablation study 1) Effect of Different BEV Sizes: We study the impact of different Birds Eye View (BEV) feature map sizes on planning performance, with results summarized in Table IV. The results demonstrate that the spatial resolution (height and width) of the BEV feature map plays more critical role than the channel dimension and has substantial influence on planning performance."
        },
        {
            "title": "BEV Sizes",
            "content": "Stage Stage II EPDMS Overall 256 32 32 512 32 32 512 64 64 512 128 128 1024 128 128 57.31 58.40 71.21 72.86 72.56 37.43 40.33 43.26 45.49 45. 22.22 23.44 30.74 33.44 33.56 2) Effect of Individual Components: DiffSemanticFusion introduces novel combination of four components: online HD map diffusion, vectorized graph embeddings, raster image embeddings, and BEV features. To evaluate their contributions, we conduct an ablation study by removing each input modality (Table V). BEV features are the most critical, with their removal leading to significant drop in performance. Vectorized graph and raster image embeddings further enhance spatial understanding by capturing topological and texture cues, highlighting the effectiveness of multimodal fusion for planning tasks."
        },
        {
            "title": "TABLE V\nABLATION STUDY OF INDIVIDUAL COMPONENTS IN NAVHARD",
            "content": "VectorizedGraph RasterImage BEVFeatures EPDMS Stage Stage II Overall 72.86 71.69 70.04 56.01 45.49 43.84 44.90 39. 33.44 31.43 32.12 20.81 3) Integration to other Models: We integrate our proposed online HD map diffusion module into other graphbased models like VectorNet and QCNet. And online map generation method is StreamMapNet. Table VI shows the experimental results indicating that the online HD map diffusion module can effectively improve the performance."
        },
        {
            "title": "Architectures",
            "content": "ADE FDE MR VectorNet [13] VectorNet + Map Diffusion QCNet [31] QCNet + Map Diffusion 1.189 1.048 0.354 0. 2.243 1.921 0.717 0.673 0.415 0.389 0.0680 0.0549 4) Effect of Diffusion Architecture and Coordinate System: As shown in Table III, we investigate the effects of different backbone architectures within the diffusion, including 1-D U-Net and Transformer models. In addition, we analyze the impact of injecting noise in different coordinate systems F. Qualitative results Fig. 4. nuscenes qualitative results 1) Nuscenes Qualitative Analysis: We analyze the online map generation results of Mapless QCNet on the nuScenes dataset as shown in figure 4 and present examples that demonstrate improvements from both prediction and planning perspectives. The left side shows the ground truth, while the right side displays the online maps generated by StreamMapNet. The proposed online HD map diffusion model effectively corrects perception errors by expanding drivable areas and ensuring lane divider consistency. 2) Navsim-v2 Qualitative Analysis: To better demonstrate the benefits of the semantic raster BEV fusion in enhancing the BEV representation, we render the dynamic perception and online maps generated from intermediate sparse 4D representations in the BEV view. As shown in figure 5, The Fig. 5. navsim qualitative results trajectories planned by DiffSemanticFusion closely follow the lane centerlines, indicating accurate semantic understanding and effective planning. V. CONCLUSION In this work, we propose DiffSemanticFusiona fusion framework for multimodal trajectory prediction and planning. Our approach reasons over semantic rasterfused BEV space, enhanced by map diffusion module that improves both the stability and expressiveness of online HD map representations. By introducing an online HD map diffusion module, our approach effectively addresses the challenges posed by noisy or incomplete online HD map inputs, significantly improving the robustness and reliability of downstream tasks. Furthermore, our semantic raster BEV fusion architecture integrates raster-based, graph-based, and BEV feature representations in unified BEV space, leveraging their complementary strengths for more comprehensive scene understanding. We demonstrate the effectiveness of our method on both prediction and planning tasks, achieving state-of-the-art performance on the nuScenes and NAVSIM benchmarks. Notably, DiffSemanticFusion improves online HD map informed QCNet prediction by 5.1% and boosts EPDMS in challenging NAVSIM Navhard scenarios by 15%. These results, along with strong compatibility with other vector-based pipelines, highlight the potential of our approach to serve as robust and generalizable solution for future autonomous driving systems. In future work, we plan to explore the integration of temporal dynamics and uncertainty modeling into the diffusion and fusion processes to further enhance long-horizon prediction and planning performance."
        },
        {
            "title": "REFERENCES",
            "content": "[1] J. Huang, G. Huang, Z. Zhu, Y. Ye, and D. Du, Bevdet: Highperformance multi-camera 3d object detection in bird-eye-view, arXiv preprint arXiv:2112.11790, 2021. [2] Z. Li, W. Wang, H. Li, E. Xie, C. Sima, T. Lu, Q. Yu, and J. Dai, Bevformer: learning birds-eye-view representation from lidar-camera via spatiotemporal transformers, IEEE Transactions on Pattern Analysis and Machine Intelligence, 2024. [3] Y. Wang, V. C. Guizilini, T. Zhang, Y. Wang, H. Zhao, and J. Solomon, Detr3d: 3d object detection from multi-view images via 3d-to-2d queries, in Conference on robot learning. PMLR, 2022, pp. 180191. [4] F. Zeng, B. Dong, Y. Zhang, T. Wang, X. Zhang, and Y. Wei, Motr: End-to-end multiple-object tracking with transformer, in European conference on computer vision. Springer, 2022, pp. 659675. [5] Y. Zhang, P. Sun, Y. Jiang, D. Yu, F. Weng, Z. Yuan, P. Luo, W. Liu, and X. Wang, Bytetrack: Multi-object tracking by associating every detection box, in European conference on computer vision. Springer, 2022, pp. 121. [6] B. Liao, S. Chen, X. Wang, T. Cheng, Q. Zhang, W. Liu, and C. Huang, Maptr: Structured modeling and learning for online vectorized hd map construction, arXiv preprint arXiv:2208.14437, 2022. [7] B. Liao, S. Chen, Y. Zhang, B. Jiang, Q. Zhang, W. Liu, C. Huang, and X. Wang, Maptrv2: An end-to-end framework for online vectorized hd map construction, International Journal of Computer Vision, vol. 133, no. 3, pp. 13521374, 2025. [8] T. Yuan, Y. Liu, Y. Wang, Y. Wang, and H. Zhao, Streammapnet: Streaming mapping network for vectorized online hd map construction, in Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, 2024, pp. 73567365. [9] A. Jiang, J. Chai, Y. Gao, Y. Wang, Y. Heng, Z. Sun, H. Sun, Z. Zhao, L. Sun, J. Zhou et al., Sparsemext unlocking the potential of sparse representations for hd map construction, arXiv preprint arXiv:2505.08808, 2025. [10] H. Cui, V. Radosavljevic, F.-C. Chou, T.-H. Lin et al., Multimodal trajectory predictions for autonomous driving using deep convolutional networks, ICRA, pp. 20902096, 2019. [11] T. Phan-Minh, E. C. Grigore, F. A. Boulton et al., CoverNet: Multimodal behavior prediction using trajectory sets, IEEE/CVF CVPR, 2019. [12] J. Li, F. Yang, M. Tomizuka et al., EvolveGraph: Multi-agent trajectory prediction with dynamic relational reasoning, NeurIPS, 2020. [13] J. Gao, C. Sun, H. Zhao, Y. Shen, D. Anguelov, C. Li, and C. Schmid, VectorNet: Encoding hd maps and agent dynamics from vectorized representation, 2020 IEEE/CVF CVPR, pp. 11 52211 530, 2020. [14] Z. Huang, H. Liu, and C. Lv, Gameformer: Game-theoretic modeling and learning of transformer-based interactive prediction and planning for autonomous driving, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 39033913. [15] J. Cheng, Y. Chen, and Q. Chen, Pluto: Pushing the limit of imitation learning-based planning for autonomous driving, arXiv preprint arXiv:2404.14327, 2024. [16] Y. Zheng, R. Liang, K. Zheng, J. Zheng, L. Mao, J. Li, W. Gu, R. Ai, S. E. Li, X. Zhan et al., Diffusion-based planning for autonomous driving with flexible guidance, arXiv preprint arXiv:2501.15564, 2025. [17] Y. Hu, J. Yang, L. Chen, K. Li, C. Sima, X. Zhu, S. Chai, S. Du, T. Lin, W. Wang et al., Planning-oriented autonomous driving, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 17 85317 862. [18] B. Jiang, S. Chen, Q. Xu, B. Liao, J. Chen, H. Zhou, Q. Zhang, W. Liu, C. Huang, and X. Wang, Vad: Vectorized scene representation for efficient autonomous driving, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 83408350. [19] S. Chen, B. Jiang, H. Gao, B. Liao, Q. Xu, Q. Zhang, C. Huang, W. Liu, and X. Wang, Vadv2: End-to-end vectorized autonomous driving via probabilistic planning, arXiv preprint arXiv:2402.13243, 2024. [20] W. Sun, X. Lin, Y. Shi, C. Zhang, H. Wu, and S. Zheng, Sparsedrive: End-to-end autonomous driving via sparse scene representation, arXiv preprint arXiv:2405.19620, 2024. [42] Y. Li, Y. Wang, Y. Liu, J. He, L. Fan, and Z. Zhang, End-to-end driving with online trajectory evaluation via bev world model, arXiv preprint arXiv:2504.01941, 2025. [43] K. Li, Z. Li, S. Lan, Y. Xie, Z. Zhang, J. Liu, Z. Wu, Z. Yu, and J. M. Alvarez, Hydra-mdp++: Advancing end-to-end driving via expertguided hydra-distillation, arXiv preprint arXiv:2503.12820, 2025. [44] W. Yao, Z. Li, S. Lan, Z. Wang, X. Sun, J. M. Alvarez, and Z. Wu, Drivesuprim: Towards precise trajectory selection for end-to-end planning, arXiv preprint arXiv:2506.06659, 2025. [45] Z. Li, K. Li, S. Wang, S. Lan, Z. Yu, Y. Ji, Z. Li, Z. Zhu, J. Kautz, Z. Wu et al., Hydra-mdp: End-to-end multimodal planning with multi-target hydra-distillation, arXiv preprint arXiv:2406.06978, 2024. [46] D. Dauner, M. Hallgarten, A. Geiger, and K. Chitta, Parting with learning-based vehicle motion planning, in misconceptions about Conference on Robot Learning. PMLR, 2023, pp. 12681281. [47] M. Sandler, A. Howard, M. Zhu, A. Zhmoginov, and L.-C. Chen, Mobilenetv2: Inverted residuals and linear bottlenecks, in Proceedings of the IEEE conference on computer vision and pattern recognition, 2018, pp. 45104520. [48] Z. Dong, R. Ding, W. Li, P. Zhang, G. Tang, and J. Guo, Leveraging sd map to augment hd map-based trajectory prediction, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 17 21917 228. [49] J. Gu, C. Sun, and H. Zhao, Densetnt: End-to-end trajectory prethe IEEE/CVF diction from dense goal sets, in Proceedings of international conference on computer vision, 2021, pp. 15 30315 312. [50] X. Gu, G. Song, I. Gilitschenski, M. Pavone, and B. Ivanovic, Producing and leveraging online map uncertainty in trajectory prediction, in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2024, pp. 14 52114 530. [51] Z. Zhou, L. Ye, J. Wang, K. Wu, and K. Lu, Hivt: Hierarchical vector transformer for multi-agent motion prediction, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2022, pp. 88238833. [52] H. Caesar, J. Kabzan, K. S. Tan, W. K. Fong, E. Wolff, A. Lang, L. Fletcher, O. Beijbom, and S. Omari, nuplan: closed-loop mlbased planning benchmark for autonomous vehicles, arXiv preprint arXiv:2106.11810, 2021. [21] A. Jiang, Y. Gao, Z. Sun, Y. Wang, J. Wang, J. Chai, Q. Cao, Y. Heng, H. Jiang, Y. Dong et al., Diffvla: Vision-language guided diffusion planning for autonomous driving, arXiv preprint arXiv:2505.19381, 2025. [22] H. Caesar, V. Bankiti, A. H. Lang, S. Vora, V. E. Liong, Q. Xu, A. Krishnan, Y. Pan, G. Baldan, and O. Beijbom, nuscenes: multimodal dataset for autonomous driving, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2020, pp. 11 62111 631. [23] D. Dauner, M. Hallgarten, T. Li, X. Weng, Z. Huang, Z. Yang, H. Li, I. Gilitschenski, B. Ivanovic, M. Pavone et al., Navsim: Data-driven non-reactive autonomous vehicle simulation and benchmarking, Advances in Neural Information Processing Systems, vol. 37, pp. 28 706 28 719, 2024. [24] Z. Zhou, J. Wang, Y.-H. Li, and Y.-K. Huang, Query-centric trajectory prediction, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 17 86317 873. [25] X. Lin, T. Lin, Z. Pei, L. Huang, and Z. Su, Sparse4d: Multi-view 3d object detection with sparse spatial-temporal fusion, arXiv preprint arXiv:2211.10581, 2022. [26] J. Philion and S. Fidler, Lift, splat, shoot: Encoding images from arbitrary camera rigs by implicitly unprojecting to 3d, in Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part XIV 16. Springer, 2020, pp. 194210. [27] Z. Sun, Z. Wang, L. Halilaj, and J. Luettin, Semanticformer: Holistic and semantic traffic scene representation for trajectory prediction using knowledge graphs, IEEE Robotics and Automation Letters, 2024. [28] H. Berkemeyer, R. Franceschini, T. Tran, L. Che, and G. Pipa, Feasible and adaptive multimodal trajectory prediction with semantic maneuver fusion, 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 85308536, 2021. [29] M. Liang, B. Yang, R. Hu, Y. Chen, R. Liao et al., Learning lane graph representations for motion forecasting, ECCV, 2020. [30] S. Casas, C. Gulino, R. Liao, and R. Urtasun, SpAGNN: Spatiallyaware graph neural networks for relational behavior forecasting from sensor data, ICRA, pp. 94919497, 2019. [31] M. Liu, H. Cheng, L. Chen, H. Broszio, J. Li, R. Zhao, M. Sester, and M. Y. Yang, LAformer: Trajectory prediction for autonomous driving with lane-aware scene constraints, in IEEE/CVF CVPR, 2024. [32] X. Mo, Z. Huang, Y. Xing, and C. Lv, Multi-agent trajectory prediction with heterogeneous edge-enhanced graph attention network, IEEE Transactions on ITS, vol. 23, pp. 95549567, 2022. [33] X. Jia, P. Wu, L. Chen, Y. Liu, H. Li, and J. Yan, HDGT: heterogeneous driving graph transformer for multi-agent trajectory prediction via scene encoding, IEEE Trans. PAMI, vol. 45, no. 11, pp. 13 860 13 875, 2023. [34] T. Monninger, J. Schmidt, J. Rupprecht, D. Raba et al., SCENE: traffic scenes using heterogeneous graph neural Reasoning about networks, IEEE Robotics and Automation Letters, vol. 8, no. 3, 2023. [35] L. Mlodzian, Z. Sun, H. Berkemeyer, S. Monka, Z. Wang, S. Dietze, L. Halilaj, and J. Luettin, nuscenes knowledge graph-a comprehensive semantic representation of traffic scenes for trajectory prediction, in Proceedings of the IEEE/CVF International Conference on Computer Vision, 2023, pp. 4252. [36] Z. Wang, Z. Sun, J. Luettin, and L. Halilaj, Socialformer: Social interaction modeling with edge-enhanced heterogeneous graph transformers for trajectory prediction, arXiv preprint arXiv:2405.03809, 2024. [37] J. Ho, A. Jain, and P. Abbeel, Denoising diffusion probabilistic models, Advances in neural information processing systems, vol. 33, pp. 68406851, 2020. [38] C. Jiang, A. Cornman, C. Park, B. Sapp, Y. Zhou, D. Anguelov et al., Motiondiffuser: Controllable multi-agent motion prediction using diffusion, in Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 2023, pp. 96449653. [39] Z. Zhong, D. Rempe, D. Xu, Y. Chen, S. Veer, T. Che, B. Ray, and M. Pavone, Guided conditional diffusion for controllable traffic simulation, arXiv preprint arXiv:2210.17366, 2022. [40] B. Liao, S. Chen, H. Yin, B. Jiang, C. Wang, S. Yan, X. Zhang, X. Li, Y. Zhang, Q. Zhang et al., Diffusiondrive: Truncated diffusion model for end-to-end autonomous driving, in Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pp. 12 03712 047. [41] K. Chitta, A. Prakash, B. Jaeger, Z. Yu, K. Renz, and A. Geiger, Transfuser: Imitation with transformer-based sensor fusion for autonomous driving, IEEE Transactions on Pattern Analysis and Machine Intelligence, vol. 45, no. 11, pp. 12 87812 895, 2022."
        }
    ],
    "affiliations": [
        "AIR, Tsinghua University, Beijing, China",
        "Bosch Corporate Research, Bosch (China) Investment Ltd., Shanghai, China",
        "Robert Bosch GmbH",
        "School of Communication and Information Engineering, Shanghai University, Shanghai, China",
        "Shanghai Jiaotong University, Shanghai, China"
    ]
}