{
    "paper_title": "RexBERT: Context Specialized Bidirectional Encoders for E-commerce",
    "authors": [
        "Rahul Bajaj",
        "Anuj Garg"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Encoder-only transformers remain indispensable in retrieval, classification, and ranking systems where latency, stability, and cost are paramount. Most general purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, a family of BERT-style encoders designed specifically for e-commerce semantics. We make three contributions. First, we release Ecom-niverse, a 350 billion token corpus curated from diverse retail and shopping sources. We describe a modular pipeline that isolates and extracts e-commerce content from FineFineWeb and other open web resources, and characterize the resulting domain distribution. Second, we present a reproducible pretraining recipe building on ModernBERT's architectural advances. The recipe consists of three phases: general pre-training, context extension, and annealed domain specialization. Third, we train RexBERT models ranging from 17M to 400M parameters and evaluate them on token classification, semantic similarity, and general natural language understanding tasks using e-commerce datasets. Despite having 2-3x fewer parameters, RexBERT outperforms larger general-purpose encoders and matches or surpasses modern long-context models on domain-specific benchmarks. Our results demonstrate that high quality in-domain data combined with a principled training approach provides a stronger foundation for e-commerce applications than indiscriminate scaling alone."
        },
        {
            "title": "Start",
            "content": "6 2 0 2 4 ] . [ 1 5 0 6 4 0 . 2 0 6 2 : r RexBERT: Context Specialized Bidirectional Encoders E-commerce for Rahul Bajaj thebajajra@gmail.com Anuj Garg anujgarg2004@gmail.com"
        },
        {
            "title": "Abstract",
            "content": "Encoder-only transformers remain indispensable in retrieval, classification and ranking systems where latency, stability and cost are paramount. Most general-purpose encoders, however, are trained on generic corpora with limited coverage of specialized domains. We introduce RexBERT, family of BERT-style encoders designed specifically for e-commerce semantics. We make three contributions. First, we release Ecom-niverse, 350 billion token corpus curated from diverse retail and shopping sources. We describe modular pipeline that isolates and extracts e-commerce content from FineFineWeb and other open web resources, and characterize the resulting domain distribution. Second, we present reproducible pre-training recipe building on ModernBERTs architectural advances. The recipe consists of three phases: general pre-training, context extension, and annealed domain specialization. Third, we train RexBERT models ranging from 17M to 400M parameters and evaluate them on token classification, semantic similarity, and general natural language understanding tasks using e-commerce dataset. Despite having 2-3 fewer parameters, RexBERT outperforms larger general-purpose encoders and matches or surpasses modern long context models on domain-specific benchmarks. Our results demonstrate that high-quality in-domain data combined with principled training approach provides stronger foundation for e-commerce applications than indiscriminate scaling alone."
        },
        {
            "title": "Introduction",
            "content": "Encoder-only language models have long been the workhorses of natural language processing. BERT Devlin et al. (2019) and its successors power classification, retrieval, and reranking systems in production because they offer favorable trade-off between inference cost and quality. However, the community has increasingly focused on decoder-only models for generation, leaving encoder research comparatively underdeveloped. Recent efforts such as ModernBERT Warner et al. (2024) demonstrate that revisiting the encoder architecture and training recipe yields substantial Pareto improvements: models trained on 2 trillion tokens with long contexts deliver state-of-the-art performance across classification and retrieval tasks while being highly efficient for inference. In parallel, domain-specific encoders such as BioClinical ModernBERT for clinical NLP Sounack et al. (2025), show that continued pre-training on large, targeted corpus further boosts domain performance without sacrificing general abilities. These advances motivate revisiting domain adaptation for the e-commerce sector. Importantly, the principles underlying our approach careful data curation, multi-phase curriculum and modern encoder architecture are not tied to commerce. The same pipeline can be extended to any specialized domain by collecting an appropriate corpus and running it through our modular cleaning and training stages. In this sense, RexBERT serves as case study in building domain-specific encoders from open data, and the techniques we describe are readily applicable to healthcare, legal, scientific or other verticals. E-commerce is high-impact domain where representation quality directly affects search, recommendations, attribute extraction and compliance routing. Generic models trained on broad web corpora often fail to capture subtle distinctions between complementary, substitute, and irrelevant products or recognize fine-grained attributes. To address this gap, we develop RexBERT, family of open-data encoders specialized for retail. Our approach draws inspiration from ModernBERT but differs in two key respects: we operate exclusively on open datasets and curate large corpus 1 focused on commerce; and we employ three-stage curriculum that gradually shifts from general to domain-specific distributions. Contributions Our work makes the following contributions: 1. We curate Ecom-niverse, 350 billion token collection of retail-relevant text distilled from the FineFineWeb corpus. The dataset covers diverse categories such as fashion, beauty, automotive and entertainment. Figure 1 visualises the domain distribution of the curated corpus. 2. We describe pre-training recipe based on modernised encoder architecture. The training progresses through (1) pre-training on diverse mixture of open web, books, code and technical documents; (2) context extension to support sequences up to 8,192 tokens; and (3) annealing, which gradually shifts the sampling distribution toward Ecom-niverse and incorporates Guided MLM masking strategy. 3. We train RexBERT models at four scales (17 M, 68 M, 150 and 400 parameters) and evaluate them on GLUE tasks and e-commerce benchmarks derived from the Amazon ESCI dataset. RexBERT encoders consistently outperform general-purpose models of similar or larger size on token classification and semantic similarity tasks. Despite 2-3x fewer parameters, our models achieve higher token classification accuracy and Spearman correlation than contemporary models in similar parameter size range. Figure 1: Domain distribution of Ecom-niverse. Sizes represent the amount of filtered data contributed by each FineFineWeb domain and additional corpora. Hobby and News supply the largest portions."
        },
        {
            "title": "2 Related Work",
            "content": "Encoder-only transformers BERT (Devlin et al., 2019) launched the era of encoder-only transformers. Subsequent variants such as RoBERTa (Liu et al., 2019), DeBERTa (He et al., 2021) and GTE incorporated training improvements, but many still rely on the original recipe and tokenizer. MosaicBERT (Portes et al., 2023) and CrammingBERT (Geiping & Goldstein, 2023) focus on efficiency rather than scaling. ModernBERT (Warner et al., 2024) modernises encoder architectures using rotary positional embeddings (RoPE), GeGLU activations, alternating global and local attention, unpadding and flash attention to achieve state-of-the-art performance on classification, retrieval and code tasks. Their training on 2 trillion tokens of diverse data and context extension to 8,192 tokens allows long-context inference with high throughput. Our work builds upon these architectural 2 advances but demonstrates that much smaller models can excel on specialised domains when paired with targeted pre-training. Domain-specific encoders Domain adaptation via continued pre-training has been successful in biomedical and clinical NLP, where BioBERT (Lee et al., 2020), ClinicalBERT (Alsentzer et al., 2019) and more recently BioClinical ModernBERT (Sounack et al., 2025) improve over general encoders. BioClinical ModernBERT pre-trains ModernBERT on 53.5 billion biomedical tokens and demonstrates state-of-the-art performance on five clinical tasks, achieving F1 scores of 90.8% on ChemProt and 60.8% on the Phenotype dataset while retaining efficiency over long contexts. These works underscore the importance of high-quality domain data and highlight the effectiveness of annealed pre-training. Other domain-specific BERT models The success of domain pre-training has inspired family of specialised BERT variants. SciBERT is trained on corpus of 1.14 million scientific papers (3.17 billion tokens) from the Semantic Scholar corpus, with 18% computer science and 82% biomedical documents. The authors build new 30K vocabulary (SCIVOCAB) tailored to scientific terms and show that SciBERT substantially improves sequence tagging, sentence classification and dependency parsing over BERT on scientific NLP tasks (Beltagy et al., 2019). FinBERT addresses the financial domain by pre-training BERT on 4.9 billion token corpus of corporate filings, earnings call transcripts and analyst reports. FinBERT demonstrates superior performance on financial sentiment classification tasks compared to general BERT and highlights the benefit of modelling domain-specific language such as quarterly report jargon (Yang et al., 2020). LEGAL-BERT investigates several strategies for adapting BERT to legal texts, including using BERT out of the box, further pre-training on legal corpora and pre-training from scratch. The authors compile diverse legal corpus including EU and UK legislation, European Court judgments, US court cases and SEC filings. Their experiments show that further pre-training or pre-training from scratch yields higher accuracy on downstream legal tasks than using BERT directly, and they release family of LEGAL-BERT models for legal NLP research. These findings led to the release of family of LEGAL-BERT models that outperform generic encoders on multiple legal benchmarks (Chalkidis et al., 2020). E-commerce-specific encoders Beyond general domain adaptation, there has been growing interest in BERT models tailored to e-commerce. E-BERT enhances BERT with phrase-level and product-level knowledge by introducing two additional pre-training tasks: Adaptive Hybrid Masking and Neighbor Product Reconstruction. These tasks enable the model to capture fine-grained product phrases and associations, yielding improved performance on review question answering, aspect extraction, aspect sentiment classification and product classification (Zhang et al., 2021). CatBERT takes different approach by training randomly initialised BERT from scratch on e-commerce catalog data via an incremental training process. The authors argue that product catalog text lacks sentence structure, uses long attribute sequences and contains restricted vocabulary; they show that training from scratch outperforms continued pre-training of generic BERT on the same data (Mallavarapu et al., 2022). These studies demonstrate the value of domain-specific pre-training for e-commerce applications and motivate our design of RexBERT as an open-data e-commerce encoder. Encoder vs. decoder studies Research comparing encoder and decoder architectures often uses incomparable models due to differences in data, parameter counts or objectives. Weller et al. (2025) introduce the Ettin suite, collection of paired encoder-only and decoder-only models ranging from 17 to 1 parameters trained on up to 2 trillion tokens with the same recipe. They show that the unified recipe yields state-of-the-art encoders and decoders, beating ModernBERT and Llama 3.2 while confirming that encoders excel at classification and retrieval whereas decoders excel at generation. However, they note that cross-objective training, continuing decoder as an encoder, does not surpass models trained with the proper objective. Our work focuses exclusively on encoder models but follows similar principles of open data, reproducibility and multi-size scaling. 3 Table 1: Selected domains from FineFineWeb. The left block lists domains requiring filtering, while the right block lists domains with almost complete overlap."
        },
        {
            "title": "Directly selected",
            "content": "Hobby News Health Entertainment Travel Food Automotive Sports Music & Dance"
        },
        {
            "title": "3 Corpus Curation",
            "content": "Fine-grained domain representation is critical for accurate e-commerce encoding. We therefore curate Ecom-niverse, large corpus of commerce-related text. Our source is FineFineWeb, 4.4 trillion token CommonCrawl-derived corpus organised into approximately fifty categories. Each entry consists of text snippet and domain label. To isolate retail content we manually inspect domains and select those with substantial overlap with commerce. We group them into two sets: domains requiring filtering and domains with near-complete overlap. Table 1 lists these categories. Notably, the Hobby domain contributes 21.2%, News contribute 11.7%, and Fashion and Beauty combined contribute 15.3%. Figure 1 visualises the resulting distribution. After selecting domains, we apply language detection to retain English text and remove boilerplate, advertisements and low-quality pages. We deduplicate near-duplicate documents using MinHash and filter profanity and adult content. The resulting corpus comprises more than 350 billion tokens of e-commerce content, making it one of the largest domain-specific corpora available. Figure 2: Ecom-niverse curation pipeline: domain selection, sampling, LLM labeling, QA auditing, fastText distillation, and thresholded filtering at scale. Source corpus and cleaning pipeline. Ecom-niverse is produced by domain-conditioned filtering pipeline applied to FineFineWeb. The pipeline first performs domain focus by selecting subset of FineFineWeb domains with high expected overlap with e-commerce content (refer Table 1), thereby reducing the labeling/search space and concentrating compute on potentially relevant distributions. From each selected domain, sampling is used to draw large, diverse tranche of items (hundreds of thousands per domain) to capture intra-domain heterogeneity. Next, the sampled items receive LLM-based binary relevance labels using an instruction-tuned model (Phi-4) that assigns Relevant vs. Not Relevant according to an operational definition of commerce relevance. Label quality is monitored through QA feedback loop in which stronger model (Llama3-70B) au4 Figure 3: Training curriculum for RexBERT. The model first trains on 1.7 trillion tokens of mixed data, then extends the context to 8,192 tokens for an additional 250 billion tokens, and finally anneals onto the Ecom-niverse corpus for 350 billion tokens. dits subset of labeled items to estimate error modes and triggers prompt/guideline refinement when quality degrades; this loop is iterated until label consistency stabilizes. The resulting LLM-labeled data are then distilled into scalable filters by training per-domain fastText classifiers that approximate the LLM decision boundary while enabling high-throughput scoring over web scale corpora. Each classifier is validated with held-out evaluation and/or cross-validation to assess generalization within its domain and to calibrate score distributions. Finally, the validated fastText models are applied at scale to their corresponding FineFineWeb domain partitions, producing relevance scores for all items; domain-wise threshold on these scores yields the retained subset, resulting in validated, e-commerce focused corpus for pre-training."
        },
        {
            "title": "4 Training Methodology",
            "content": "Our pre-training procedure adopts three-phase curriculum inspired by ModernBERT, but adapted to the statistical and semantic properties of e-commerce text (e.g., high entity density, attribute value structure, and domain-specific terminology). Let = (x1, . . . , xT) denote token sequence and θ the model parameters. We train using the masked language modeling (MLM) objective LMLM(θ) = XD, mM (cid:2) log pθ(Xm Xm)(cid:3)., (1) where indexes masked positions drawn from masking distribution and Xm denotes the unmasked context. Masking is performed dynamically with span-aware sampling to better capture multi-token lexical units and to reduce information leakage from partially observed subword fragments. To further increase learning signal on semantically salient content in Phase 3, we introduce Guided MLM, targeted masking variant that prioritizes information-rich entities and attributes. Specifically, we pre-identify domain-relevant spans using lightweight entity and attribute mining pipeline, and preferentially mask these spans to encourage robust contextual representations aligned with ecommerce semantics. During training, we interleave Guided MLM examples with standard random span-masking by sampling approximately 5% of sequences per batch from the guided masking distribution and the remainder from the baseline masking scheme. This mixture-of-masking strategy preserves the generalization benefits of unbiased MLM while allocating additional capacity to rare, high-value tokens and structured product descriptors. Tokenizer and optimizer. Like ModernBERT we adopt modern BPE tokenizer derived from OLMo with vocabulary of 50,368 tokens, which improves token efficiency over the original BERT WordPiece and facilitates long-context finetuning. We drop the next-sentence prediction objective and follow recent work in using higher masking ratio: 30% of tokens are selected for masking rather than the original 15%, with spans drawn from Poisson distribution to encourage longer masked segments. Optimisation is performed with StableAdamW, variant of AdamW that adds Adafactor-style update clipping. This optimizer improves stability compared to standard AdamW and eliminates the need for separate gradient clipping. We employ trapezoidal learning-rate sched5 ule with WarmupStableDecay (WSD) phases, holding the learning rate constant for most of training before decaying according to 1 schedule. Batch sizes are gradually increased during warm-up to maximise hardware utilisation while avoiding instability. Phase 1: general pre-training We train on diverse 1.7 trillion token mixture of curated web text, books, code, technical papers and multilingual content. Shorter sequence lengths (512 tokens) accelerate convergence and stabilise optimisation. We adopt high masking ratio (30%) and apply high dropout and moderate temperature sampling to flatten source headroom. Regular evaluation on general MLM perplexity and natural language understanding probes ensures broad linguistic coverage. This phase establishes robust token representations and attention patterns. Phase 2: context extension Building upon the Phase 1 checkpoint we increase the maximum sequence length to 8,192 tokens and train for 250 billion tokens to model long product pages, FAQs and concatenated attribute blocks. We switch to rotary positional embeddings with NTK-aware scaling to reuse learned position representations and alternate global and local attention layers, following ModernBERT. Spans cross section boundaries to teach the model to bridge headings, lists and tables. Efficient packing and bucketing maintain high token utilisation. trapezoidal learning rate schedule with stable warmup and decay is employed. Phase 3: annealing The final stage specializes the model on Ecom-niverse for approximately 350 billion tokens while preserving general knowledge. We reduce the masking ratio to 10%-15% and anneal the sampling weights to gradually upsample e-commerce data. This annealing was shown by BioClinical ModernBERT to improve domain performance without catastrophic forgetting. The learning rate decays following 1 schedule and the RoPE scaling factors remain equal for local and global attention, enabling seamless context extension. Figure 3 illustrates the token budgets for the three phases. Table 2: RexBERT model configurations."
        },
        {
            "title": "Layers\nHidden size\nIntermediate size\nAttention heads\nLearning rate\nWeight decay",
            "content": "7 256 384 4 3 103 3 104 19 512 768 8 3 103 3 104 22 768 1,152 12 8 104 1 105 28 1,024 2,624 16 5 104 1"
        },
        {
            "title": "5 Model Architecture",
            "content": "RexBERT uses BERT-style encoder with several modern enhancements based on ModernBERT. Key differences from the original BERT architecture include: Biasless layers and pre-normalisation. Following ModernBERT, we remove bias terms from linear layers and layer norms and use pre-layer normalisation to improve training stability. Rotary positional embeddings. We replace absolute positional embeddings with RoPE, enabling extrapolation to long contexts and efficient implementation. GeGLU activations. GeGLU activations provide better optimisation compared to GELU. Alternating global/local attention. Attention layers alternate between full attention and local sliding windows, reducing quadratic complexity while preserving global context. Unpadding and flash attention. We adopt unpadding to remove padding tokens and use Flash Attention for both global and local attention, improving throughput. Hardware-aware depth. The Micro, Mini, Base and Large variants follow deep-and-narrow design to maximise GPU utilisation within parameter budgets. Table 2 summarises the configuration of RexBERT models. 6 Table 3: Token Classification Accuracy Comparison Block Size 128 Product Title Top-k token Accuracy = 1 = 3 = 5 = 1 = = 5 512 128 256 512 Product Description = 1 = 3 Large Models = 5 = 1 = 3 = = 1 = 3 = 5 = 1 = 3 = RexBERT-large ModernBERT-large 72.0 65.0 82.3 75.0 85.1 78.2 74.4 68.4 84.3 78. 87.0 81.1 76.1 69.8 85.6 79.4 88.2 82.3 75.7 71.9 86.8 82. 89.8 85.7 77.9 75.0 88.4 85.3 91.1 88.2 79.7 77.4 89.6 87. 92.1 89.8 Base Models RexBERT-base ModernBERT-base 69.2 60.5 79.7 70.6 82.6 74. 71.4 64.2 81.6 74.1 84.5 77.4 72.6 65.5 82.7 75.4 85.5 78. 73.1 67.8 84.6 78.8 87.8 82.2 75.5 71.2 86.3 81.8 89.3 85. 77.4 74.0 87.7 84.1 90.4 87.0 Mini Models (70M Parameters) RexBERT-mini DistilBERT 65.58 34. 76.4 43.99 79.67 48.53 67.15 33.48 77.85 42.57 81 42.27 66.75 29. 77.62 37.78 80.81 42.49 68.26 48.19 80.39 59.93 83.93 64.58 71.99 49. 83.49 61.39 86.73 66.08 74.28 49.1 85.21 61.32 88.22 66.16 Micro Models (1017M Parameters) RexBERT-micro BERT-mini 55.73 39.94 66.75 47.94 70.56 51.47 55.49 43.85 66.57 51. 70.44 55.38 51.14 46.09 62.6 54.18 66.78 57.57 57.18 49.6 70.28 60. 74.7 64.23 61.63 52.37 74.16 62.67 78.24 66.62 64.01 54.74 76.08 64. 79.94 68."
        },
        {
            "title": "6 Evaluation",
            "content": "We evaluate RexBERT on two families of tasks derived from the Amazon ESCI dataset: classification and semantic similarity. token Dataset overview. The Amazon ESCI collection, also known as the Shopping Queries Dataset (Reddy et al., 2022), serves as our primary evaluation benchmark for e-commerce language understanding. The dataset contains approximately 130,000 unique search queries and 2.6 million manually labeled query-product pairs across English, Spanish and Japanese. For each query, list of up to 40 candidate products is provided along with four-level relevance labels: Exact (perfect match), Substitute (alternative product), Complement (related product) and Irrelevant (no relation). We restrict our experiments to the English subset and follow the publicly released train/test splits to ensure reproducibility and fair comparison with existing baselines. Compared to generic retrieval corpora such as MS Marco (Bajaj et al., 2016), ESCI poses several distinctive challenges that make it particularly suitable for evaluating e-commerce language models. The four-level relevance signal requires models to distinguish between subtle semantic relationships, such as differentiating substitutes from complementsa task that demands nuanced understanding of product semantics and user intent. Additionally, the dataset includes rich product descriptions with long-form text and structured bullet points, testing models ability to process and integrate information from diverse textual formats. As one of the first publicly available, graded relevance benchmarks for product search, ESCI provides standardised evaluation framework that captures the complexity of real-world e-commerce search scenarios. In addition to ESCI, we evaluate on the GLUE benchmark (Wang et al., 2018) to assess general language understanding capabilities. GLUE comprises multiple tasks including sentiment classification (SST-2), question-answering inference (QNLI), paraphrase detection (QQP), multi-genre natural language inference (MNLI) and semantic textual similarity (STS-B), providing comprehensive assessment of models ability to handle diverse linguistic phenomena beyond domain-specific applications. Evaluation protocols. We evaluate RexBERT on two tasks derived from the ESCI dataset that probe complementary aspects of language understanding in e-commerce contexts. The first task is masked token recovery (token classification), which assesses models ability to reconstruct product titles and descriptions from partial context. Product texts are truncated to 128, 256 or 512 tokens, and 15% of tokens are masked using span-aware masking strategy. Models are evaluated on their ability to predict the original tokens at top-k with {1, 3, 5}, measuring both lexical knowledge and contextual understanding of product attributes and descriptions. This task is particularly relevant for e-commerce applications such as catalog completion, attribute extraction and content generation. The second task is semantic similarity, which evaluates whether models can capture graded relevance relationships between queries and products. We map the four-level ESCI relevance labels to numerical scores: 1.0 for Exact, 0.66 for Substitute, 0.33 for Complement and 0.0 for Irrelevant. Models are fine-tuned using the CoSENT loss (Huang et al., 2024), which optimises pairwise orderings in cosine similarity space and has been shown to reduce anisotropy in BERT embeddings while 7 Figure 4: Spearman correlation on the ESCI semantic similarity task. The RexBERT series (Micro to Large) achieves higher correlation than ModernBERT, Ettin and EmbeddingGemma models. producing more consistent similarity rankings. We report Spearmans rank correlation between predicted cosine similarities and target scores on held-out set, which evaluates whether the embedding space preserves the ordinal structure of the ESCI labels. This metric directly assesses the models utility for retrieval and ranking applications, where understanding nuanced relevance relationships is crucial. For GLUE evaluation, we follow standard fine-tuning protocols for each task, using task-specific classification or regression heads as appropriate. Models are fine-tuned on the training splits and evaluated on the development sets using task-specific metrics: accuracy for classification tasks (SST-2, QNLI, QQP, MNLI) and Pearson/Spearman correlation for the similarity task (STS-B). This evaluation provides insight into how well domain-specialised models transfer to general language understanding tasks."
        },
        {
            "title": "6.1 Token Classification",
            "content": "We follow the masked-token recovery protocol described in ModernBERT and FineFineWeb. Product titles and descriptions are truncated to 128, 256 or 512 tokens. span-aware mask covering 15% of the tokens is applied and the model predicts the original tokens at top-k with {1, 3, 5}. As Table 3 illustrates, RexBERT-base consistently outperforms ModernBERT-base across all block sizes and top-k settings. For product titles, RexBERT-base improves top-1 accuracy from 60.5%69.2% at 128 tokens and from 65.5%72.6% at 512 tokens. For product descriptions, RexBERT-base improves top-1 accuracy from 67.8%73.1% at 128 tokens and from 74.0%77.4% at 512 tokens. Notably, even the 68M-parameter RexBERT-mini surpasses ModernBERT-base on product titles at 128 tokens (top-1: 65.58% vs. 60.5%, +5.08 point gain), highlighting the benefits of in-domain pre-training."
        },
        {
            "title": "6.2 Semantic Similarity",
            "content": "We evaluate whether RexBERT produces embedding spaces that preserve the graded relevance structure of e-commerce search, where candidate products may be exact matches, substitutes, complements, or irrelevant. We use the English subset of Amazon ESCI and represent each training example as query-product text pair. The product text is formed by concatenating the product title with the product description when available; if the description exceeds the models maximum length, it is truncated to fit the configured context window. 8 Table 4: GLUE benchmark results comparing RexBERT models to contemporary baselines across five tasks. Bold indicates best performance within each model size category."
        },
        {
            "title": "Task",
            "content": "RexBERT ModernBERT RexBERT ModernBERT RexBERT DistilBERT RexBERT ettin-17m SST-2 QNLI QQP MNLI STS-B 95.76 94.00 89.84 92.06 91.80 96.67 94.82 89.73 90.14 91.33 94.27 92.75 89.28 87.23 89.55 94.72 90.88 89.26 88.71 85. 92.78 91.51 88.52 85.23 88.53 89.23 87.99 87.42 80.75 84.84 90.37 85.59 85.90 78.26 83.00 88.42 85.30 85.54 77.66 82.62 As shown in Figure 4, RexBERT consistently yields higher Spearman correlation than generalpurpose encoders at comparable scale, indicating superior alignment with e-commerce relevance semantics. Notably, the gains are strongest from MicroMiniBase, while returns are decent at the Large scale, suggesting that careful domain pre-training and curriculum design capture most of the task-relevant signal without requiring aggressive parameter scaling. We also observe stable convergence across random seeds, consistent with CoSENTs ranking-based supervision being robust to label noise and batch composition."
        },
        {
            "title": "6.3 Natural Language Understanding",
            "content": "Remarkably, despite being trained specifically for e-commerce applications, RexBERT achieves the best performance on several general language understanding tasks. Table 4 presents results across four model size categories, comparing RexBERT to contemporary baselines. At the large scale, RexBERT-large achieves competitive performance, leading on paraphrase detection, multi-genre inference and semantic similarity tasks, while ModernBERT-large excels on sentiment classification and question-answering inference. The base models show similar pattern, with RexBERT-base demonstrating particularly strong performance on semantic similarity tasks, indicating superior understanding of graded semantic relationships. At smaller scales, RexBERT demonstrates clear advantages: RexBERT-mini outperforms DistilBERT across all five tasks, with particularly strong gains on inference and similarity tasks. Similarly, RexBERT-micro consistently outperforms ettin17m across all evaluated tasks. These results indicate that RexBERTs domain-specialised training not only excels in its target domain but also transfers effectively to general NLU tasks, with particularly strong performance on semantic similarity and inference tasks that benefit from nuanced understanding of relationships between texts."
        },
        {
            "title": "7 Discussion",
            "content": "The results in 6 show consistent pattern: when the evaluation distribution is strongly ecommerceshaped (product titles, descriptions, and queryproduct relevance), data and curriculum dominate raw scaling. Across both masked-token recovery and ESCI semantic similarity, RexBERT improves over general-purpose encoders at comparable or larger parameter counts, indicating that representation quality in this domain is bottlenecked less by model capacity and more by exposure to the right long-tail entities, attribute-value constructions, and overall training token set. Why does specialization help? E-commerce language differs from generic web text in systematic ways: it is entity-dense, highly compositional, and often expressed in semi-structured fragments. The Ecom-niverse corpus is explicitly designed to cover this space, and the gains we observe are most pronounced in settings that stress these properties. This supports the hypothesis that the encoder benefits from learning (i) robust lexical representations for domain terms and structure, and (ii) contextual rules that bind attributes to the correct head entities. Guided MLM as domain signal amplifier. The e-commerce setting contains large fraction of high-value spans that are rare in general corpora but critical for retrieval and ranking. Guided MLM is designed to allocate additional learning signal to such spans without discarding the benefits of standard random masking. Although our work does not attempt to isolate Guided MLM via an 9 ablation, the overall improvements are consistent with the mechanism it targets: better recovery of salient tokens in masked-token evaluation and improved ordering of graded relevance in embedding space. Future controlled studies can quantify the marginal contribution of guided masking relative to corpus composition and annealing. Practical deployment implications. The observed gains translate directly to several production facing use cases. First, stronger token recovery on titles/descriptions suggests improved representations for attribute completion and normalization, which can support catalog quality pipelines. Second, higher Spearman correlation on graded relevance implies more faithful embedding geometry for retrieval and candidate generation, especially in distinguishing substitute vs. complement relations that generic models often conflate. Finally, long-context capability is important for merchant pages and multi-section descriptions, single encoder that handles 8k tokens reduces the need for heuristic truncation strategies that may drop critical attributes."
        },
        {
            "title": "8 Conclusion",
            "content": "We introduced RexBERT, family of e-commerce specialized, encoder-only transformers trained entirely on open data. The key enabling asset is Ecom-niverse, 350B-token corpus curated through domain-conditioned filtering and validation pipeline. Building on modern encoder design choices (RoPE, GeGLU, alternating local/global attention, and efficient training primitives), we proposed three-phase training curriculum: general pre-training, long-context extension, and annealed domain specialization augmented with Guided MLM. Across masked-token recovery and ESCI semantic similarity, RexBERT outperforms generalpurpose encoders despite using 23 fewer parameters, and it remains competitive on GLUE, indicating that domain specialization can enhance target performance without sacrificing broad language understanding when applied through gradual curriculum. More broadly, RexBERT serves as template for building domain-specialized encoders from open data: by replacing Ecom-niverse with well-curated corpus from another vertical and applying the same training recipe, practitioners can construct efficient, high-quality encoders for other specialized domains. By releasing models and training methodology, we aim to support transparent progress in domain-specific representation learning and enable stronger, more efficient e-commerce NLP systems."
        },
        {
            "title": "Acknowledgements",
            "content": "We gratefully acknowledge Warner et al. (2024) for releasing ModernBERT and the Ettin suite authors for open sourcing checkpoints. Our Phase 1 and Phase 2 training mirror Ettins methodology; we use their 8,192-token checkpoint as the starting point for annealing. Special thanks to the creators of FineFineWeb for providing structured source of web data and to the HuggingFace community for maintaining the infrastructure that enabled this work."
        },
        {
            "title": "References",
            "content": "Emily Alsentzer, John Murphy, William Boag, Weng-Hong Yao, Katherine Zhang, Matthew McDermott, Tristan Naumann, and David Sontag. Publicly available clinical bert embeddings. In Proceedings of the Clinical NLP Workshop, 2019. P. Bajaj, D. Campos, N. Craswell, L. Deng, J. Gao, X. Liu, R. Majumder, A. McNamara, B. Mitra, T. Nguyen, M. Rosenberg, X. Song, A. Stoica, S. Tiwary, and T. Wang. Ms marco: human generated machine reading comprehension dataset. arXiv preprint arXiv:1611.09268, 2016. Iz Beltagy, Kyle Lo, and Arman Cohan. Scibert: pretrained language model for scientific text. In Proceedings of EMNLP, 2019. Ilias Chalkidis, Manos Fergadiotis, Prodromos Malakasiotis, Nikolaos Aletras, and Ion Androutsopoulos. Legal-bert: The muppets straight out of law school. In Findings of the Association for Computational Linguistics: EMNLP, pp. 28982904, 2020. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. Bert: Pre-training of deep bidirectional transformers for language understanding. In Proceedings of NAACL, 2019. Jonas Geiping and Tom Goldstein. Cramming: Training language model on single gpu in one day. arXiv preprint arXiv:2301.00761, 2023. Pengcheng He, Jianfeng Gao, and Weizhu Chen. Deberta: Decoding-enhanced bert with disentangled attention. In International Conference on Learning Representations (ICLR), 2021. Xiang Huang, Hao Peng, Dongcheng Zou, Zhiwei Liu, Jianxin Li, Kay Liu, Jia Wu, Jianlin Su, IEEE/ACM and Philip S. Yu. Cosent: Consistent sentence embedding via similarity ranking. Transactions on Audio, Speech and Language Processing, 32(9):115, 2024. Jinhyuk Lee, Kyunghyun Cho, and Jung-Woo Ha. Biobert: Pre-trained biomedical language representation model for biomedical text mining. Bioinformatics, 2020. Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: robustly optimized bert pretraining approach. arXiv preprint arXiv:1907.11692, 2019. Tejaswini Mallavarapu, Ying Xie, and Simon Hughes. Catbert: An incrementally trained language representation model for e-commerce applications. In Proceedings of the Interactive and Scalable Information Retrieval Methods for E-commerce Workshop, 2022. Andrea Portes, Mingdae Day, Emmanuel Eade, Scott Lundberg, Jay DeYoung, Oren Etzioni, and Luke Zettlemoyer. Mosaicbert: Reassembling the mosaic of bert pretraining. arXiv preprint arXiv:2309.10016, 2023. Chandan K. Reddy, Lluís Màrquez, Fran Valero, Nikhil Rao, Hugo Zaragoza, Sambaran Bandyopadhyay, Arnab Biswas, Anlu Xing, and Karthik Subbian. Shopping queries dataset: large-scale esci benchmark for improving product search. arXiv preprint arXiv:2206.06588, 2022. Thomas Sounack, Joshua Davis, Brigitte Durieux, Antoine Chaffin, Tom J. Pollard, Eric Lehman, Alistair E. W. Johnson, Matthew McDermott, Tristan Naumann, and Charlotta Lindvall. Bioclinical modernbert: state-of-the-art long-context encoder for biomedical and clinical nlp. arXiv preprint arXiv:2506.10896, 2025. Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R. Bowman. Glue: multi-task benchmark and analysis platform for natural language understanding. In Proceedings of ICLR, 2018. Benjamin Warner, Antoine Chaffin, Benjamin Clavié, Orion Weller, Oskar Hallström, Said Taghadouini, Alexis Gallagher, Raja Biswas, Faisal Ladhak, Tom Aarsen, Nathan Cooper, Griffin Adams, Jeremy Howard, and Iacopo Poli. Smarter, better, faster, longer: modern bidirectional encoder for fast, memory efficient, and long context finetuning and inference. arXiv preprint arXiv:2412.13663, 2024. Orion Weller, Kathryn Ricci, Marc Marone, Antoine Chaffin, Dawn Lawrie, and Benjamin Van Durme. Seq vs seq: An open suite of paired encoders and decoders. arXiv preprint arXiv:2507.11412, 2025. Yi Yang, Mark Christopher Siy Uy, and Allen Huang. Finbert: pretrained language model for financial communications. arXiv preprint arXiv:2006.08097, 2020. Denghui Zhang, Zixuan Yuan, Yanchi Liu, Fuzhen Zhuang, Haifeng Chen, and Hui Xiong. Ebert: phrase and product knowledge enhanced language model for e-commerce. arXiv preprint arXiv:2009.02835, 2021."
        }
    ],
    "affiliations": []
}