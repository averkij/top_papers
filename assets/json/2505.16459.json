{
    "paper_title": "MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks",
    "authors": [
        "Guiyao Tie",
        "Xueyang Zhou",
        "Tianhe Gu",
        "Ruihang Zhang",
        "Chaoran Hu",
        "Sizhe Zhang",
        "Mengqu Sun",
        "Yan Zhang",
        "Pan Zhou",
        "Lichao Sun"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, vision, and structured inputs, opening the door to complex tasks such as logical deduction, spatial reasoning, and scientific analysis. Despite their promise, the reasoning capabilities of MLLMs, particularly those augmented with intermediate thinking traces (MLLMs-T), remain poorly understood and lack standardized evaluation benchmarks. Existing work focuses primarily on perception or final answer correctness, offering limited insight into how models reason or fail across modalities. To address this gap, we introduce the MMMR, a new benchmark designed to rigorously evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) a high-difficulty dataset of 1,083 questions spanning six diverse reasoning types with symbolic depth and multi-hop demands and 2) a modular Reasoning Trace Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy through metrics like relevance, consistency, and structured error annotations. Empirical results show that MLLMs-T overall outperform non-thinking counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro suffer from reasoning pathologies such as inconsistency and overthinking. This benchmark reveals persistent gaps between accuracy and reasoning quality and provides an actionable evaluation pipeline for future model development. Overall, the MMMR offers a scalable foundation for evaluating, comparing, and improving the next generation of multi-modal reasoning systems."
        },
        {
            "title": "Start",
            "content": "MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks Guiyao Tie1 Xueyang Zhou1 Tianhe Gu1 Ruihang Zhang1 Chaoran Hu1 Sizhe Zhang1 Mengqu Sun2 Yan Zhang1 Pan Zhou1 Lichao Sun2 1Huazhong University of Science and Technology 2Lehigh University {tgy,d202480819,u202211961,u202211917,u202314532,U202312332}@hust.edu.cn mes225@lehigh.edu,{u202312543,panzhou}@hust.edu.cn,lis221@lehigh.edu"
        },
        {
            "title": "Abstract",
            "content": "Recent advances in Multi-Modal Large Language Models (MLLMs) have enabled unified processing of language, vision, and structured inputs, opening the door to complex tasks such as logical deduction, spatial reasoning, and scientific analysis. Despite their promise, the reasoning capabilities of MLLMsparticularly those augmented with intermediate thinking traces (MLLMs-T)remain poorly understood and lack standardized evaluation benchmarks. Existing work focuses primarily on perception or final answer correctness, offering limited insight into how models reason or fail across modalities. To address this gap, we introduce the MMMR, new benchmark designed to rigorously evaluate multi-modal reasoning with explicit thinking. The MMMR comprises 1) high-difficulty dataset of 1,083 questions spanning six diverse reasoning types with symbolic depth and multi-hop demands and 2) modular Reasoning Trace Evaluation Pipeline (RTEP) for assessing reasoning quality beyond accuracy through metrics like relevance, consistency, and structured error annotations. Empirical results show that MLLMs-T overall outperform non-thinking counterparts, but even top models like Claude-3.7-Sonnet and Gemini-2.5 Pro suffer from reasoning pathologies such as inconsistency and overthinking. This benchmark reveals persistent gaps between accuracy and reasoning quality and provides an actionable evaluation pipeline for future model development. Overall, the MMMR offers scalable foundation for evaluating, comparing, and improving the next generation of multi-modal reasoning systems. Project Page: https://mmmr-benchmark.github.io/. 5 2 0 2 7 2 ] . [ 2 9 5 4 6 1 . 5 0 5 2 : r Figure 1: Overview of the MMMR. Left: Six reasoning tasks covered in the MMMR. Middle: Thinking Judgements assessing alignment for relevance (RTQ, RTA), consistency (RSC), and error (Err-T). Right: Accuracy distribution shows gap between humans and state-of-the-art MLLMs-T. Preprint."
        },
        {
            "title": "Introduction",
            "content": "The rapid advancement of Multi-Modal Large Language Models (MLLMs) has significantly enhanced unified reasoning capabilities across language, vision, and structured data modalities. Early MLLMs such as Qwen-VL [8], LLaVA [24], and GPT-4 Vision [35] have demonstrated impressive performance in perception-centric tasks, including visual question answering [7], image captioning [38], and grounded retrieval [37]. However, their proficiency remains limited in tasks necessitating structured reasoning, symbolic abstraction, or sequential multi-step inference. To address this limitation, new paradigmMLLMs incorporating explicit intermediate reasoning (MLLMs-T)has emerged. Representative models such as Gemini-2.5 Pro [13] and Claude-3.7-Sonnet [6] leverage Chainof-Thought-style reasoning, decomposing complex problems into interpretable intermediate steps, thereby emulating human-like structured problem-solving in domains such as logical deduction, scientific analysis, and code reasoning. Despite significant progress, rigorously evaluating the reasoning capabilities of MLLMs-T remains challenging. Current benchmarks, including MMBench [30], MME-CoT [18], and MMMU [47], predominantly emphasize broad coverage of tasks and perceptual understanding, offering limited insights into the reasoning process itself. These benchmarks primarily focus on answer correctness without assessing the underlying reasonings consistency, coherence, or cognitive alignment. Consequently, critical research question arises: To what extent do MLLMs-T reliably generate coherent, interpretable, and cognitively aligned reasoning traces in complex multi-modal tasks? Addressing this research question demands benchmark emphasizing reasoning depth rather than breadth, evaluating not only final predictions but also the intermediate reasoning processes explicitly. Such benchmark requires 1) challenging multi-modal reasoning tasks explicitly designed to probe structured inference capabilities, and 2) robust criteria for systematically evaluating intermediate reasoning quality. Currently, no benchmark adequately fulfills these requirements [30, 18, 47, 31], leaving significant gap in diagnosing and attributing reasoning failures at the trace level. To bridge this gap, we introduce MMMR, comprehensive benchmark explicitly designed to evaluate the multi-modal reasoning capabilities of both MLLMs and MLLMs-T. As shown in Figure 1, our benchmark comprises 1,083 rigorously curated high-difficulty tasks spanning six reasoning domains: logical reasoning [18, 43], mathematical problem-solving [31, 15], spatio-temporal understanding [18, 44], code reasoning [22, 21], map-based planning [29, 28], and scientific analysis [18, 47]. Each task integrates diverse modalities, including text, images, tables, and diagrams, carefully designed to require structured, symbolic, and abstract reasoning beyond mere perception. Unlike prior benchmarks [30, 47, 31], MMMR introduces structured Reasoning Trace Evaluation Pipeline (RTEP), capturing reasoning trace relevance, logical consistency, and frequent error types. This structured evaluation identifies key reasoning pitfalls such as overthinking, trace inconsistency, and logical errors. The right panel of Figure 1 reveals pronounced performance gap between state-of-the-art MLLMs-T and human-level expert reasoning. Specifically, while the best-performing model, Gemini-2.5 Pro, achieves test accuracy of 42.45%, human experts assisted by GPT-4o reach 52.85%. This 10.3% margin underscores the persistent challenge in closing the reasoning gap, even with advanced architectures and explicit thinking mechanisms. In summary, our contributions include: comprehensive benchmark for multi-modal reasoning. We introduce MMMR, the first benchmark that systematically targets multi-modal reasoning across six ing domainsLogic, Math, Code, Map, Science, and Space-Time. Unlike prior datasets, MMMR emphasizes hard question solutions and cross-modal alignment to increase reasoning complexity. The first evaluation pipeline for thinking of MLLMs-T. We propose the Reasoning Trace Evaluation Pipeline (RTEP), the first framework to incorporate intermediate thinking trace analysis into multi-modal reasoning evaluation. RTEP assesses reasoning relevance, stepwise consistency, and alignment, which enables deeper diagnostic insight beyond accuracy. Insights into reasoning capabilities and failures. Through extensive evaluation on MMMR, we find that state-of-the-art MLLMs-T achieve high answer accuracy across tasks, yet frequently produce flawed reasoning tracesexhibiting logical inconsistency or overthinking. These findings expose critical misalignment between surface-level correctness and reasoning fidelity, offering new evaluation directions for future multi-modal model architecture. 2 Figure 2: Overview of the MMMR evaluation pipeline. Stage involves the creation of challenging multi-modal reasoning benchmark dataset. Stage II evaluates the quality and structural integrity of intermediate reasoning generated by MLLMs-T. Stage III synthesizes insights regarding reasoning strategies, effectiveness, and common failure patterns across different tasks and models."
        },
        {
            "title": "2 MMMR: Benchmarking Massive Multi-Modal Reasoning Tasks",
            "content": "The MMMR is challenging benchmark dataset meticulously crafted to evaluate the reasoning capabilities of Multi-modal Large Language Models with intermediate Thinking (MLLMs-T). Unlike previous benchmarks [7, 16, 19, 33] which predominantly measure perception or general knowledge, MMMR emphasizes complex reasoning tasks requiring deep integration across diverse modalities, such as text, images, and structured data. Motivated by recent advancements in MLLMs-T, exemplified by Gemini-2.5 Pro [12], which leverage intermediate reasoning processes to enhance performance, we propose rigorous three-stage evaluation pipeline. This pipeline is specifically designed to evaluate multi-modal reasoning quality and, crucially, assess the effectiveness and robustness of intermediate thinking. As illustrated in Figure 2, our evaluation pipeline comprises three core stages: (I) Reasoning Dataset Construction, (II) Thinking Quality Assessment, and (III) Reasoning Insights Synthesis. 2.1 Stage I: Reasoning Dataset Construction In this initial stage, we construct the MMMR dataset to thoroughly evaluate MLLMs-T across wide spectrum of reasoning scenarios. The MMMR comprises 1,083 carefully curated multi-modal tasks, systematically categorized into six distinct reasoning types: Logic (16.8%), Math (19.5%), SpaceTime (18.5%), Code (13.0%), Map (13.8%), and Science (18.3%). Each reasoning type further includes task-specific subcategories, such as deductive inference, algebraic calculation, temporal ordering, code generation, spatial planning, and hypothesis evaluation. The MMMR incorporates heterogeneous modalities including natural language texts, visual imagery, and structured data (e.g., Three-View diagrams, Plots & Charts, and Web Pages). To facilitate reproducible and granular evaluation, the dataset is partitioned into validation set (106 samples) and test set (977 samples). 2.2 Stage II: Thinking Quality Assessment This stage systematically evaluates the quality and structure of intermediate reasoning processes (i.e., thinking) produced by MLLMs-T. We propose reasoning trace evaluation pipeline (RTEP), including metrics: 1) RTQ, quantifying the relevance of thinking with the posed question, and 2) RTA, assessing logical relevance between thinking and the answer. Both metrics are normalized within the [0,1] interval and evaluated through standardized prompts designed for precise, unbiased assessment. Furthermore, we conduct an extensive error type analysis, categorizing reasoning failures into distinct types, including thinking errors and answer errors. This fine-grained analysis offers critical insights into the strengths and vulnerabilities inherent in the reasoning approaches adopted by MLLMs-T. 2.3 Stage III: Reasoning Insights Synthesis The final stage synthesizes the observations from Stage II to generate holistic reasoning insights. We pursue three primary analytical objectives: 1) comparing and benchmarking the performance of MLLMs-T against standard MLLMs across different reasoning tasks, 2) profiling the quality of intermediate reasoning to identify consistent patterns of strength and weakness, and 3) investigating how prevalent error types (especially overthinking and redundant reasoning stages) impact the overall reliability of reasoning outcomes. By aggregating and analyzing detailed results across various tasks and model variants, this stage supports informed interpretation of reasoning behavior. 2.4 Research Questions To systematically guide our evaluation and produce insightful analyses on the reasoning capabilities of MLLMs-T, we articulate the following research questions: [RQ1] How do MLLMs-T perform in comparison to standard MLLMs concerning reasoning accuracy across the diverse and challenging multi-modal tasks presented in MMMR? [RQ2] How does the quality of intermediate thinking generated by MLLMs-T vary across different levels of task complexity and modality combinations? [RQ3] Which reasoning error types are most frequently encountered by MLLMs-T within different task contexts of MMMR, and how do these errors reflect underlying challenges in multimodal integration?"
        },
        {
            "title": "3 Experiment Settings",
            "content": "Multi-Modal Language Models without Thinking (MLLMs). MLLMs solve multi-modal tasks by directly mapping perception inputs to answers, bypassing explicit reasoning steps. We evaluate representative models from both open-source and closed-source. Open-source MLLMs include LLaVA3.2-11B-Vision-Instruct [25], LLaVA-3.2-90B-Vision-Instruct [25], Qwen2.5-VL-32B-Instruct [2], Qwen2.5-VL-72B-Instruct [3], and Qwen-VL-max [1]. Closed-source MLLMs include Gemini1.5 Flash [10], GPT-4 Vision [35], and LLaMA-4-Maverick [4], recognized for their sophisticated multi-modal fusion and retrieval-based response capabilities. TJ Size Low Source Images Reason Format Dataset Table 1: Comparison of representative multi-modal reasoning datasets. (Visual Input), OC (Optical Characters), I+T (Image + Text), IL (Interleaved Format), TJ (Thinking Judgment) and Source (W: Web, T: Textbook, R: Remake). Multi-Modal Language Models with Thinking (MLLMsT). MLLMs-T extend the capabilities of MLLMs by producing intermediate reasoning traces (Thinking) before final answer generation. This architecture allows structured, stepwise reasoning and facilitates deeper interpretability in complex problem-solving. We include open models such as and QVQ-72B-Preview [5], several advanced proprietary models, including Gemini-2.0 Flash [11], Gemini-2.5 Pro [13], Claude-3.7-sonnet [6], and o4mini [36]. particularly notable configuration is our custom Dual Model, which simulates MLLMs-T behavior by integrating the strengths of two distinct models. Specifically, GPT-4V [35] is responsible for parsing the input question and image contenthandling rich visual understanding and language grounding. The parsed task is then passed to DeepSeek-R1 [14] for structured multi-step reasoning. This pipeline allows us to isolate and examine the effect of high-quality Thinking traces independently of perception noise. Moreover, since DeepSeek-R1 is optimized for textual reasoning rather than vision, this dual formulation ensures modular design and enhanced control over each reasoning stage. >1M >1M 32K 45K 14K 19K 3K 0.2K 6K 1.1K 2.7K 11.5K 30 Types VQA [7] GQA [16] VizWiz [9] TextVQA [40] OK-VQA [34] SEED [23] MMBench [30] MM-Vet [46] ScienceQA [32] MME-COT [18] EMMA [15] MMMU [47] Medium Medium Medium Medium Medium Medium Medium Medium Medium Medium V OC V+OC V+OC V+OC V+OC 5 Types - - W W+R W+R W+R W+T I+T I+T I+T I+T I+T I+T I+T I+T I+T IL IL IL High 15 Types MMMR W+T+R 1.1K Low IL 4 Figure 3: Representative multi-modal reasoning samples from MMMR. Each example consists of interleaved image-text input, complex reasoning question, domain annotation, and source information. The samples illustrate the datasets high structural variability and reasoning depth. Datasets. The MMMR is designed from first principles to meet the demands of benchmarking multimodal reasoning with Thinking.Compared to prior works like MMMU [47], MME-CoT [18], and EMMA [15], MMMR provides full-spectrum redesign of reasoning task settings. Each of its 1,083 problems is carefully constructed and categorized into six reasoning types and sixteen fine-grained subfields, providing targeted coverage of logical, mathematical, spatio-temporal, code, map-based, and scientific reasoning. The dataset incorporates rich array of visual stimuli, including charts, diagrams, 3D maps, and visual code logic, covering 15 unique image types. Unlike retrieval-centric tasks, many questions require long-horizon reasoning, abstraction, or visual-spatial synthesis. To further increase complexity, 44.6% of the items are remade or enhanced beyond web or textbook sources. Moreover, MMMR facilitates intermediate reasoning evaluation. For each sample, the source origin (Web, Textbook, Remake), and task type are documented, supporting both output-based and process-based analysis. This structure enables deep introspection into where and how reasoning fails or succeeds, making it unique testbed for evaluating MLLMs-T. Table 2: Statistics of MMMR, detailing the distribution and characteristics of reasoning tasks. Category Quantitative statistics Total Questions Total Subjects/Subfields Image Types Value 1083 6/16 15 Dataset Split Validation:Test Difficulties (Easy:Medium:Hard) 106:977 30%:40%:30% Task Type Distribution Logical Reasoning Mathematical Reasoning Spatio-Temporal Understanding Code Reasoning Map Reasoning Science Reasoning 182 (16.8%) 212 (19.5%) 200 (18.5%) 141 (13.0%) 150 (13.8%) 198 (18.4%) Content Characteristics Average Question Length Average Option Length Average Response Length Multi-modal Inputs per Question Reasoning Steps per Question 68.75 words 12.25 words 135.60 words 1.85 3. Annotation and Complexity Remade Questions Average Reasoning Depth Cross-Modal Integration Rate 483 (44.6%) 4.15 levels 95.2% Metrics. We evaluate MLLMs-T and MLLMs on the MMMR using concise suite of metrics, with scores normalized to [0, 1]: 1) ACC, the proportion of correct answers; 2) RTQ, assessing how well the Thinking process aligns with the problems requirements, independent of answer correctness; 3) RTA, evaluating the logical consistency between the thinking process and the final answer, regardless of accuracy; 4) Reasoning Step Consistency (RSC), measuring logical coherence across Thinking steps through consistency checks. 5 Table 3: Accuracy (%) comparison of baselines, MLLMs, and MLLMs-T on the MMMR benchmark. Each row highlights the per-model highest and lowest scores using green and red , respectively. For each column (task type), the best-performing model is indicated in bold and the second-best is underline. Models marked with * are closed-source. S-T denotes the Space-Time. Validation (106) Test (977) Logic Math (212) (182) S-T (200) Code (141) Map (150) Science (198) Random Choice Frequent Choice Expert (Human only) Expert (Human + GPT-4o [17]) 22.1 26.8 29.23 52.85 Baselines 23.62 26.58 - - 24.18 26.92 - - 24.06 26.42 - - 21.50 24.00 - - LLaVA-3.2-11B-Vision-Instruct [25] LLaVA-3.2-90B-Vision-Instruct [25] Qwen2.5-VL-32B-Instruct [2] Qwen2.5-VL-72B-Instruct [3] Qwen-VL-max [1] Gemma-3-27B-IT [41] Gemini-1.5 Flash* [10] GPT-4 Vision* [35] LLaMA-4-Maverick* [4] Multi-Modal Large Language Models without Thinking 28.00 35.00 45.00 47.50 46.00 33.50 37.00 49.00 46.00 18.68 21.43 25.27 24.18 24.18 22.53 28.57 28.02 30.77 31.13 34.91 45.28 46.70 47.17 42.45 37.74 35.85 44. 23.92 27.65 34.90 37.18 35.55 29.01 29.61 38.05 41.82 24.53 30.19 34.86 36.95 35.13 30.87 32.18 37.59 40.68 QVQ-72B-Preview [5] Gemini-2.0 Flash* [11] Gemini-2.5 Pro* [13] Claude-3.7-sonnet* [6] o4-mini* [36] Dual ([35] + DeepSeek-R1 [14]) Multi-Modal Large Language Models with Thinking 42.00 49.50 44.50 51.00 47.50 48.00 32.09 37.89 42.36 37.72 37.58 41.00 26.37 35.16 39.56 35.71 34.62 35. 30.94 37.63 42.45 38.28 38.64 41.26 38.21 50.47 41.51 45.75 46.23 47.64 25.53 24.82 - - 13.48 17.73 32.62 41.84 39.01 34.75 18.44 28.37 37.59 32.62 28.37 36.17 21.28 19.86 22.70 22.67 25.33 - - 22.67 25.33 36.67 42.67 35.33 26.67 24.67 32.00 30.67 31.33 30.67 37.33 34.00 29.33 22.67 23.74 29.80 - - 22.73 21.72 21.72 31.31 28.28 27.27 32.83 41.92 38.38 32.83 41.41 46.46 43.94 41.41 45."
        },
        {
            "title": "4 Empirical Results and Analysis",
            "content": "4.1 Main Results We evaluate 17 models on the MMMR, where baselines include Random Choice and Frequent Choice, which serve as naïve heuristics, and two Expert configurations that represent human upper bounds with or without model assistance (see Appendix for full descriptions). As shown in Table 3, MLLMs-T overall outperform MLLMs across six tasks, highlighting the advantage of incorporating explicit thinking mechanisms. Notably, Gemini-2.5 Pro achieves the highest overall test accuracy at 42.36%, while the Expert (Human + GPT-4o) attains an upper-bound of 52.85%, indicating remaining gap between state-of-the-art MLLMs-T and human-assisted reasoning [RQ1 Summary]. Model-wise performance reflects generalization differences. By examining the best and secondbest performers for each task column, we observe that Gemini-2.5 Pro (with 4 best and 1 second-best scores) exhibits the most stable and competitive accuracy across reasoning types. This suggests that explicit reasoning modules, when paired with carefully supervised thinking strategies, enable strong generalization across diverse task formats. Gemini-2.0 Flash performs robustly in Math (50.47%) but shows significant drop in Code, indicating limited cross-domain transferability. The Dual achieves strong results in Logic and Science, supporting the effectiveness of modular architectural design. In contrast, open-source models like Qwen2.5-VL-72B and Qwen-VL-max display isolated strengths in domains such as Spatio-Temporal reasoning and Code, but lack consistency. Weaker models (e.g., Gemma-3-27B and LLaVA-3.2-11B) show broad performance variance and struggle particularly with symbolically dense or spatial tasks, indicating limitations in reasoning depth despite model size. Task-wise analysis reveals variation in reasoning difficulty. row-wise comparison of model accuracy extremes reveals task-dependent performance variability. Math and Space-Time tasks, which dominate the best-performing entries (green highlights), are generally more tractable, suggesting progress in symbolic computation and spatial comprehension. In contrast, tasks like Logic and especially Code show lower accuracy ceilings (often below 42%) and greater inter-model dispersion, as evidenced by the concentration of minimum scores (red highlights). These patterns underscore the utility of MMMR in enabling fine-grained analysis of multimodal reasoning capabilities, offering more diagnostic lens than aggregate performance alone. 6 Figure 4: Overview of the Reasoning Trace Evaluation Pipeline (RTEP). The pipeline applies structured scoring of intermediate reasoning traces, evaluating consistency, relevance, and verbosity. 4.2 Thinking Quality Analysis To evaluate intermediate reasoning quality beyond answer correctness, we introduce the Reasoning Trace Evaluation Pipline (RTEP). This structured pipeline annotates and scores each models reasoning trace across three dimensions: Relevance to the Question (RTQ), Relevance to the Answer (RTA), and Reasoning Step Consistency (RSC), each rated on 010 scale. Leveraging GPT-4o [17] as an automated evaluator, RTEP enables scalable, semantically aligned trace assessments, avoiding the subjectivity and cost of manual annotation. As illustrated in Figure 4, this design allows for modelagnostic diagnosis of coherence, verbosity, and reasoning alignment, enabling targeted comparisons across architectures and task types. Table 4 summarizes detailed comparison between Claude-3.7-sonnet and Dual (GPT-4V + DeepSeek-R1) across all six reasoning tasks in MMMR. Claude-3.7 consistently outperforms Dual in Overall Score (OS), with especially strong results in Math and Science, where compact, logically consistent traces are essential. In contrast, the Dual system achieves marginally higher answer accuracy in several tasks (e.g., Logic: +2.79%), but at the cost of reasoning coherence, as reflected in lower OS values and significantly inflated trace lengths (TLen often 35 higher). This indicates that longer outputs, while occasionally improving accuracy, tend to dilute reasoning relevance and introduce redundancy or inconsistency. For instance, in Code and Space-Time tasks, Duals modular pipeline leads to repetitive or loosely linked steps, revealing that increased trace length does not ensure better reasoning quality. These findings emphasize that answer correctness alone is insufficient as proxy for reasoning performancemodels with higher accuracy can still produce flawed, verbose, or semantically incoherent rationales. Evaluating trace quality is thus essential for advancing the robustness and interpretability of MLLMs-T. Overall, accurate answers do not guarantee sound reasoning. Our findings reveal that high-performing MLLMs-T can still produce incoherent thought processes, suggesting that future progress hinges not only on output correctness but on the quality of the reasoning path itself [RQ2 Summary]. 7 Table 4: Comparison of reasoning quality between Claude-3.7-sonnet and Dual across six task types. RTQ, RTA, RSC are reasoning trace metrics in [010]; ACC is final answer accuracy (%); OS is weighted overall score (0.3RTQ + 0.3RTA + 0.3RSC + 0.1(ACC0.1)); TLen denotes trace length in thousands of tokens; ThinkErr indicates dominant reasoning flaw (defined in Section 4.3). Task Logic Math Space-Time Code Map Science Model RTQ RTA RSC ACC (%) OS TLen (k) ThinkErr Claude-3.7-sonnet Dual (DualClaude) Claude-3.7-sonnet Dual (DualClaude) Claude-3.7-sonnet Dual (DualClaude) Claude-3.7-sonnet Dual (DualClaude) Claude-3.7-sonnet Dual (DualClaude) Claude-3.7-sonnet Dual (DualClaude) 9.39 6.32 - 8.88 8.57 - 9.50 8.50 - 9.56 8.61 - 9.17 7.08 - 8.95 8.25 - 9.41 7.63 - 9.02 8.80 - 9.26 8.75 - 9.31 8.56 - 8.94 7.42 - 9.29 8.84 - 9.07 6.18 - 8.40 7.82 - 8.97 7.60 - 9.30 7.94 - 8.43 6.42 - 8.73 7.81 - 35.71 38.50 + 2.79 45.75 47.60 + 1.85 51.00 48.00 3.00 21.28 22.90 + 1.62 23.80 22.50 1. 43.93 45.10 + 1.17 8.72 6.42 2.30 8.35 8.03 0.32 8.83 8.32 0.51 8.82 7.93 0.89 8.57 6.99 1. 8.77 8.32 0.45 3.71 15.19 + 11.48 5.32 21.35 + 16.03 2.38 14.83 + 12.45 4.53 17.24 + 12.71 1.76 12.43 + 10. 3.61 14.50 + 10.89 Overthinking Irrelevant Thinking - Overthinking Overthinking - Overthinking Repetitive Thinking - Repetitive Thinking Inconsistency - Irrelevant Thinking Repetitive Thinking - Inconsistency Inconsistency - 4.3 Thinking and Answer Error Types Analysis To understand the structural weaknesses in multimodal reasoning, we analyze errors in both intermediate Thinking traces and final Answer predictions of Claude-3.7-sonnet on the MMMR validation set. As visualized in Figure 5, we classify each error into semantically distinct categories, allowing targeted diagnosis of reasoning failures. Thinking Errors Distribution. 1) Inconsistency (41.5%) reflects internal contradictions or self-conflicting logic, often arising in Science or Logic tasks where maintaining state across steps is nontrivial. 2) Overthinking (20.5%) denotes unnecessarily verbose or speculative reasoning paths. These are prevalent in otherwise simple tasks where compact reasoning suffices. 3) Irrelevant Thinking (18.5%) includes content unrelated to the question or answer. These errors typically occur in poorly grounded inputs or under weak alignment. 4) Repetitive Thinking (16.2%) captures duplication without informational gain, frequently observed in Code and Map, where step-tracking or termination is difficult. 5) Others (3.8%) contain rare phenomena such as speculative completion or omitted critical steps. Answer Errors Distribution. 1) Reasoning Error (43.6%) indicates logically flawed reasoning that nonetheless produces confident but incorrect answers, especially common in Math and Science. 2) Perceptual Error (28.2%) reflects misinterpretation of visual data such as spatial layouts or chartsfrequent in Map and Space-Time tasks. 3) Format Error (9.4%) denotes violations of expected output formats, such as missing labels or extraneous text, often due to instruction-following deficiencies. 8 Figure 5: Distribution of Thinking and Answer Errors on Claude-3.7-sonnet. 4) Answer Extraction Error (7.1%) occurs when models generate lengthy thinking traces but omit or fail to commit to final answerhighlighting uncertainty or incomplete reasoning convergence. 5) Reject to Answer (4.7%) involves abstention despite solvable inputs, typically due to cautious decoding or alignment penalties. 6) Others (7.0%) include ambiguous completions or partially correct statements. Error Analysis. The error distributions suggest that high answer accuracy often masks underlying reasoning path defects. The dominance of inconsistency and overthinking in reasoning traces reveals fundamental challenges in maintaining logical control and brevity. Likewise, the prevalence of reasoning-based answer errors over perceptual ones underscores that symbolic structure, rather than visual understanding, remains the primary bottleneck in high-level multimodal cognition. These findings reinforce the importance of trace-aware evaluation: coarse answer-level metrics alone cannot capture reasoning fidelity [RQ3 Summary]."
        },
        {
            "title": "5 Related Work",
            "content": "Multi-Modal Large Language Models Benchmarking. The evaluation of multimodal reasoning has progressed from VQAv2 [7], GQA [16], and VCR [48] to broader and more specialized datasets such as TextVQA [40], ScienceQA [32], AI2D [20], and SEED [23], and recent large-scale benchmarks like MMBench [30], MM-Vet [46], EMMA [15], MathVista [31], and MMMU [47]. These datasets have advanced task diversity and domain specificity, yet most focus primarily on answer accuracy with limited insight into reasoning quality. While efforts such as MME-CoT [18] introduce reasoning trace annotations, they mainly rely on additional CoT designs. In contrast, our MMMR is constructed as high-difficulty, multi-domain dataset specifically for evaluating multimodal reasoning. It not only spans six distinct reasoning types with structured task design but also supports fine-grained assessment of thinking in MLLMs-T, offering comprehensive diagnostic standard for future multimodal models. Reasoning Traces and Thinking Evaluation. In textual LLMs, reasoning trace prompting methods like Chain-of-Thought [42], ReAct [45], and Reflexion [39] have improved interpretability and performance through explicit step-by-step reasoning. Recent works propose evaluation tools such as RATER [27] and DECKARD [26] to assess coherence, faithfulness, and hallucination in these traces. However, such evaluations are still limited to language-only settings. Our work fills this gap by enabling trace-level reasoning evaluation within multimodal benchmark, supporting both processand outcome-oriented assessments."
        },
        {
            "title": "6 Conclusion",
            "content": "This paper presents MMMR, new benchmark and evaluation framework for advancing the study of multi-modal reasoning in large language models. Distinct from prior efforts that primarily emphasize perception or answer correctness, MMMR targets high-complexity, symbolic reasoning across six diverse domains, including logic, mathematics, and space-time inference. To systematically assess reasoning fidelity, we propose the Reasoning Trace Evaluation Pipeline (RTEP), which incorporates structured metrics (RTQ, RTA, RSC), length-efficiency analysis, and error-type classification to evaluate the coherence and relevance of intermediate thinking. Through extensive experiments on 17 models, we find that MLLMs-T overall outperform standard MLLMs in tasks requiring structured reasoning. Our findings suggest that improving multi-modal reasoning requires not just stronger instruction tuning or scale, but more cognitively aligned architectures that optimize for both answer correctness and thinking quality. We hope this benchmark catalyzes further research on reflective reasoning, modular cognition, and generalizable multi-modal understanding. Limitations. While MMMR emphasizes reasoning difficulty and multi-modal integration, it does not explicitly define fine-grained difficulty levels or hierarchical task groupings. This limits the granularity of comparative analysis across reasoning complexity levels. The main challenge lies in accurately quantifying reasoning difficulty across diverse modalities and task structures, which requires both task-specific cognitive modeling and robust annotation protocols. Future work should explore dynamic task stratification to better support curriculum learning, diagnostic evaluation, and model scalability studies."
        },
        {
            "title": "References",
            "content": "[1] Alibaba DAMO Academy. Qwen-vl-max: Most capable visual-language model. https: //github.com/QwenLM/Qwen-VL, 2024. Accessed: 2025-05-14. [2] Alibaba DAMO Academy. Qwen2.5-vl-32b-instruct: Vision-language model. https:// huggingface.co/Qwen/Qwen2.5-VL-32B-Instruct, 2025. Accessed: 2025-05-14. [3] Alibaba DAMO Academy. Qwen2.5-vl-72b-instruct: Vision-language model. https:// huggingface.co/Qwen/Qwen2.5-VL-72B-Instruct, 2025. Accessed: 2025-05-14. [4] Meta AI. Llama 4 maverick: Natively multimodal model. https://ai.meta.com/blog/ llama-4-multimodal-intelligence/, 2025. Accessed: 2025-05-14. [5] Qwen AI. Qvq-72b-preview: Vision-language model. https://huggingface.co/Qwen/ QVQ-72B-Preview, 2025. Accessed: 2025-05-14. [6] Anthropic. Claude 3.7 sonnet: Multimodal model. https://www.anthropic.com/ claude-3-7-sonnet, 2025. Accessed: 2025-05-14. [7] Stanislaw Antol, Aishwarya Agrawal, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. of the IEEE international conference on computer vision, pages 24252433, 2015. Jiasen Lu, Margaret Mitchell, Dhruv Batra, In Proceedings [8] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond. arXiv preprint arXiv:2308.12966, 2023. [9] Jeffrey P. Bigham, Chandrika Jayant, Hanjie Ji, Greg Little, Andrew Miller, Robert C. Miller, Robin Miller, Aubrey Tatarowicz, Brandyn White, Samuel White, and Tom Yeh. Vizwiz: Nearly real-time answers to visual questions. In Proceedings of the 23rd Annual ACM Symposium on User Interface Software and Technology (UIST), pages 333342. ACM, 2010. [10] Google DeepMind. Gemini 1.5 flash: Multimodal model. https://cloud.google.com/ vertex-ai/generative-ai/docs/models/gemini/1-5-flash, 2024. Accessed: 2025-0514. [11] Google DeepMind. Gemini 2.0 flash: Multimodal model. https://cloud.google.com/ vertex-ai/generative-ai/docs/models/gemini/2-0-flash, 2024. Accessed: 2025-0514. [12] Google DeepMind. Gemini 2.5: Our most intelligent ai model. https://blog.google/ technology/google-deepmind/gemini-model-thinking-updates-march-2025/, 2025. Accessed: 2025-05-14. [13] Google DeepMind. Gemini 2.5 pro: Multimodal model. https://cloud.google.com/ vertex-ai/generative-ai/docs/models/gemini/2-5-pro, 2025. Accessed: 2025-05-14. [14] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. https://arxiv.org/abs/2501.12948, 2025. Accessed: 2025-05-14. [15] Yunzhuo Hao, Jiawei Gu, Huichen Will Wang, Linjie Li, Zhengyuan Yang, Lijuan Wang, and Yu Cheng. Can mllms reason in multimodality? emma: An enhanced multimodal reasoning benchmark. https://arxiv.org/abs/2501.05444, 2025. Accessed: 2025-05-14. [16] Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 67006709, 2019. [17] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. 10 [18] Dongzhi Jiang, Renrui Zhang, Ziyu Guo, Yanwei Li, Yu Qi, Xinyan Chen, Liuhui Wang, Jianhan Jin, Claire Guo, Shen Yan, Bo Zhang, Chaoyou Fu, Peng Gao, and Hongsheng Li. Mme-cot: Benchmarking chain-of-thought in large multimodal models for reasoning quality, robustness, and efficiency. https://arxiv.org/abs/2502.09621, 2025. Accessed: 2025-05-14. [19] Justin Johnson, Bharath Hariharan, Laurens van der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. Clevr: diagnostic dataset for compositional language and elementary In Proceedings of the IEEE conference on computer vision and pattern visual reasoning. recognition, pages 29012910, 2017. [20] Aniruddha Kembhavi, M. Salvato, M. Seo, H. Hajishirzi, and A. Farhadi. diagram is worth dozen images. In Proceedings of the European Conference on Computer Vision (ECCV), pages 235251, 2016. [21] Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, Zhiyong Huang, and Jing Ma. Mmcode: Benchmarking multimodal large language models for code generation with visually rich programming problems. arXiv preprint arXiv:2404.09486, 2024. [22] Kaixin Li, Yuchen Tian, Qisheng Hu, Ziyang Luo, Zhiyong Huang, and Jing Ma. Web2code: Benchmarking multimodal large language models for code generation with visually rich programming problems. Findings of the Association for Computational Linguistics: EMNLP 2024, pages 736783, 2024. [23] Yixuan Li, Yujie Wang, Yujie Zhang, Yifan Wang, Yujie Wang, Yixuan Li, Yujie Wang, Yujie Zhang, and Yifan Wang. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [24] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023. [25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. https://arxiv.org/abs/2304.08485, 2023. Accessed: 2025-05-14. [26] Jiacheng Liu, Pan Lu, Hritik Bansal, Hannaneh Hajishirzi, and Jianfeng Gao. Deckard: Benchmarking reasoning traces in language models. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [27] Jiacheng Liu, Pan Lu, Hritik Bansal, Hannaneh Hajishirzi, and Jianfeng Gao. Rater: Referencefree evaluation for cot reasoning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [28] Yibo Liu et al. Mapeval-visual: benchmark for visual map-based planning tasks. arXiv preprint arXiv:2405.67890, 2024. [29] Yibo Liu et al. Multi-modal-self-instruct: Synthesizing complex visual reasoning context using language models. arXiv preprint arXiv:2405.12345, 2024. [30] Yuxin Liu, Yuxuan Zhang, Yifan Wang, Yixuan Li, Yujie Wang, Yujie Zhang, and Yifan Wang. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023. [31] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In Proceedings of the International Conference on Learning Representations (ICLR), 2024. [32] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [33] Pan Lu, Xiang Wang, Zehao Lin, Zekun Zhang, Mo Yu, Zhiyuan Yu, et al. Learn from peers: Equipping multi-modal learners with cross-modal memory. In Advances in Neural Information Processing Systems (NeurIPS), 2022. [34] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Hannaneh Hajishirzi. Ok-vqa: visual question answering benchmark requiring external knowledge. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 31953204, 2019. [35] OpenAI. Gpt-4 vision: Multimodal model. gpt-4v-system-card/, 2023. Accessed: 2025-05-14. https://openai.com/index/ [36] OpenAI. o4-mini: Multimodal model. https://openai.com/o4-mini, 2025. Accessed: 2025-05-14. [37] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Scott Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision. In Proceedings of the International Conference on Machine Learning (ICML), pages 87488763, 2021. [38] Piyush Sharma, Nan Ding, Sebastian Goodman, and Radu Soricut. Conceptual captions: cleaned, hypernymed, image alt-text dataset for automatic image captioning. In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 25562565, 2018. [39] Noah Shinn, Federico Cassano, Edward Berman, Ashwin Gopinath, and Karthik Narasimhan. Reflexion: Language agents with verbal reinforcement learning. In Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing, 2023. [40] Amanpreet Singh, Vivek Natarajan, Xinlei Jiang, Xi Chen, Marcus Rohrbach, Dhruv Batra, and Devi Parikh. Towards vqa models that can read. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 83178326, 2019. [41] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, Louis Rouillard, Thomas Mesnard, Geoffrey Cideron, Jean-Bastien Grill, Sabela Ramos, Edouard Yvinec, Michelle Casbon, Etienne Pot, Ivo Penchev, Gaël Liu, Francesco Visin, Kathleen Kenealy, Lucas Beyer, Xiaohai Zhai, Anton Tsitsulin, Robert Busa-Fekete, Alex Feng, Noveen Sachdeva, Benjamin Coleman, Yi Gao, Basil Mustafa, Iain Barr, Emilio Parisotto, David Tian, Matan Eyal, Colin Cherry, Jan-Thorsten Peter, Danila Sinopalnikov, Surya Bhupatiraju, Rishabh Agarwal, Mehran Kazemi, Dan Malkin, Ravin Kumar, David Vilar, Idan Brusilovsky, Jiaming Luo, Andreas Steiner, Abe Friesen, Abhanshu Sharma, Abheesht Sharma, Adi Mayrav Gilady, Adrian Goedeckemeyer, Alaa Saade, Alex Feng, Alexander Kolesnikov, Alexei Bendebury, Alvin Abdagic, Amit Vadi, András György, André Susano Pinto, Anil Das, Ankur Bapna, Antoine Miech, Antoine Yang, Antonia Paterson, Ashish Shenoy, Ayan Chakrabarti, Bilal Piot, Bo Wu, Bobak Shahriari, Bryce Petrini, Charlie Chen, Charline Le Lan, Christopher A. Choquette-Choo, CJ Carey, Cormac Brick, Daniel Deutsch, Danielle Eisenbud, Dee Cattle, Derek Cheng, Dimitris Paparas, Divyashree Shivakumar Sreepathihalli, Doug Reid, Dustin Tran, Dustin Zelle, Eric Noland, Erwin Huizenga, Eugene Kharitonov, Frederick Liu, Gagik Amirkhanyan, Glenn Cameron, Hadi Hashemi, Hanna Klimczak-Plucinska, Harman Singh, Harsh Mehta, Harshal Tushar Lehri, Hussein Hazimeh, Ian Ballantyne, Idan Szpektor, Ivan Nardini, Jean Pouget-Abadie, Jetha Chan, Joe Stanton, John Wieting, Jonathan Lai, Jordi Orbay, Joseph Fernandez, Josh Newlan, Ju yeong Ji, Jyotinder Singh, Kat Black, Kathy Yu, Kevin Hui, Kiran Vodrahalli, Klaus Greff, Linhai Qiu, Marcella Valentine, Marina Coelho, Marvin Ritter, Matt Hoffman, Matthew Watson, Mayank Chaturvedi, Michael Moynihan, Min Ma, Nabila Babar, Natasha Noy, Nathan Byrd, Nick Roy, Nikola Momchev, Nilay Chauhan, Noveen Sachdeva, Oskar Bunyan, Pankil Botarda, Paul Caron, Paul Kishan Rubenstein, Phil Culliton, Philipp Schmid, Pier Giuseppe Sessa, Pingmei Xu, Piotr Stanczyk, Pouya Tafti, Rakesh Shivanna, Renjie Wu, Renke Pan, Reza Rokni, Rob Willoughby, Rohith Vallu, Ryan Mullins, Sammy Jerome, Sara Smoot, Sertan Girgin, Shariq Iqbal, Shashir Reddy, Shruti Sheth, Siim Põder, Sijal Bhatnagar, Sindhu Raghuram Panyam, Sivan Eiger, Susan Zhang, Tianqi Liu, Trevor Yacovone, Tyler Liechty, Uday Kalra, Utku Evci, Vedant Misra, Vincent Roseberry, Vlad 12 Feinberg, Vlad Kolesnikov, Woohyun Han, Woosuk Kwon, Xi Chen, Yinlam Chow, Yuvein Zhu, Zichuan Wei, Zoltan Egyed, Victor Cotruta, Minh Giang, Phoebe Kirk, Anand Rao, Kat Black, Nabila Babar, Jessica Lo, Erica Moreira, Luiz Gustavo Martins, Omar Sanseviero, Lucas Gonzalez, Zach Gleicher, Tris Warkentin, Vahab Mirrokni, Evan Senter, Eli Collins, Joelle Barral, Zoubin Ghahramani, Raia Hadsell, Yossi Matias, D. Sculley, Slav Petrov, Noah Fiedel, Noam Shazeer, Oriol Vinyals, Jeff Dean, Demis Hassabis, Koray Kavukcuoglu, Clement Farabet, Elena Buchatskaya, Jean-Baptiste Alayrac, Rohan Anil, Dmitry Lepikhin, Sebastian Borgeaud, Olivier Bachem, Armand Joulin, Alek Andreev, Cassidy Hardin, Robert Dadashi, and Léonard Hussenot. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [42] Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Brian Ichter, Fei Xia, Ed H. Chi, Quoc V. Le, and Denny Zhou. Chain-of-thought prompting elicits reasoning in large language models. In Advances in Neural Information Processing Systems, 2022. [43] Yijia Xiao, Edward Sun, Tianyu Liu, and Wei Wang. Logicvista: Multimodal llm logical reasoning benchmark in visual contexts. arXiv preprint arXiv:2407.04973, 2024. [44] Yunqiu Xu, Linchao Zhu, and Yi Yang. Mc-bench: benchmark for multi-context visual grounding in the era of mllms. arXiv preprint arXiv:2410.12332, 2024. [45] Shinn Yao, Jiachang Zhao, Dian Yu, Shuyang Gao, Yujie Chen, Zhou Yu, and Karthik In Advances Narasimhan. React: Synergizing reasoning and acting in language models. in Neural Information Processing Systems, 2022. [46] Weihao Yu, Zhengyuan Yang, Lingfeng Ren, Linjie Li, Jianfeng Wang, Kevin Lin, Chung-Ching Lin, Zicheng Liu, Lijuan Wang, and Xinchao Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. [47] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. https://arxiv.org/abs/2311.16502, 2023. Accessed: 2025-0514. [48] Rowan Zellers, Yonatan Bisk, Ali Farhadi, and Yejin Choi. From recognition to cognition: Visual commonsense reasoning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 67206730, 2019."
        },
        {
            "title": "A Basic Settings",
            "content": "Figure 6 provides comprehensive summary of the MMMR benchmark, including task type distributions, instance counts, and sub-category breakdowns. The benchmark consists of 1,083 multi-modal reasoning tasks spanning six high-level domains: Logic, Math, Space-Time, Code, Map, and Science. Each domain contains diverse subtypes (e.g., deductive logic, algebraic manipulation, spatial tracking, etc.), designed to probe distinct reasoning faculties. The chart further reports the proportion of each task type, ensuring balanced but realistic coverage aligned with real-world reasoning demands. Figure 6: Task-type distribution and sub-category breakdown across the MMMR benchmark. Expert Baselines. To contextualize the capabilities of MLLMs and MLLMs-T, we introduce two upper-bound baselines referred to as Experts, representing different degrees of human involvement: Expert (Human only): This setting represents pure human reasoning without any model assistance. We selected three co-authors of this paper, each with graduate-level expertise in AI, cognitive science, or related fields, to independently solve the benchmark tasks. Participants were provided with full task descriptions and multi-modal inputs (text and images), but were not exposed to model outputs or allowed external tools. To ensure reliability, each sample was independently answered by at least two annotators; disagreements were resolved via majority voting. The inter-annotator agreement, measured by Krippendorffs alpha, reached 0.84, indicating high consistency and shared task understanding. Expert (Human + GPT-4o): This hybrid configuration simulates human-in-the-loop decision-support paradigm, where the same human experts were allowed to optionally consult GPT-4o during task resolution. Annotators first formed an independent judgment, then optionally queried GPT-4o for additional insights or solutions. Final responses reflected either acceptance or revision of GPT-4os suggestions, along with justifications. This setting measures the upper bound of human-AI collaboration in structured reasoning tasks. These expert configurations serve as practical performance ceilings: the human-only setting captures unaided expert cognition, while the hybrid setting reflects augmented performance with access to state-of-the-art MLLM support. Together, they frame the evaluation of MLLMs within broader continuum of human-machine reasoning capabilities."
        },
        {
            "title": "B Prompt Design",
            "content": "B.1 Base Prompt Example. Bases prompt example is as follows: Bases prompt example Zero-shot Prompt: {question} {choice} Please provide the final answer and store it in boxed{answer}. Critique Prompt: Review your previous answer and find problems with your answer. Improve Prompt: Based on the problems you found, improve your answer. Please reiterate your answer, with your final answer single numerical number, In the form boxed{answer}. B.2 Thinking Prompt Prompt Example. The Thinking prompt is as follows: Thinking Prompt Example {question} {choice} Please think deeply before your response. Please provide the final answer and store it in boxed{answer}. B.3 Image-text to Text Prompt Prompt Example. The Image-text to text prompt is as follows: Image-text to Text Prompt Example Based on the question and the image, please summary it in pure text. Just summary the question and image as detailed as possible, no need to give the answer. B.4 Random Choice Baseline Implementation Logic. The logic for the Random Choice baseline is as follows: Random Choice Baseline Logic def random_choice_baseline(questions, output_file): with open(output_file, w, encoding=utf-8) as f_out: for in questions: = len(q[\"choices\"]) labels = [chr(ord(A) + i) for in range(n)] prediction = random.choice(labels) correct = normalize_answer(q[\"correct\"]) result = { \"question\": q.get(\"question\", \"\"), \"prediction\": prediction, \"correct\": correct } f_out.write(json.dumps(result, ensure_ascii=False)) 15 B.5 Frequent Choice Baseline Implementation Logic. The logic for the Frequent Choice baseline is as follows: Frequent Choice Baseline Logic def frequent_choice_baseline(questions, output_file): counter = Counter() for in questions: correct = normalize_answer(q[\"correct\"]) if correct: counter[correct] += if not counter: print(\"No valid answers found for frequent choice baseline. Skip.\") return most_common_choice, _ = counter.most_common(1)[0] with open(output_file, w, encoding=utf-8) as f_out: for in questions: correct = normalize_answer(q[\"correct\"]) result = { \"question\": q.get(\"question\", \"\"), \"prediction\": most_common_choice, \"correct\": correct } f_out.write(json.dumps(result, ensure_ascii=False))"
        },
        {
            "title": "C Case Study",
            "content": "1. Logic: Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18 2D Logic: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 19 3D Logic: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20 2. Code: Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21 Generation: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 22 Code Choose: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3. Math: Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24 Algebra: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 25 Geometry: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 26 Topology: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 Calculus: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28 4. Space-Time: Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Space Reasoning: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30 Time: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 5. Map: Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 32 Route Plan: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 Street Map: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 34 6. Science: Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Chemistry: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 36 Physics: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37 Geography: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38 Biology: Non-Thinking Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39 17 Figure 7: Logic: Thinking Case 18 Figure 8: 2D Logic: Non-Thinking Case 19 Figure 9: 3D Logic: Non-Thinking Case 20 Figure 10: Code: Thinking Case 21 Figure 11: Generation: Non-Thinking Case 22 Figure 12: Code Choose: Non-Thinking Case 23 Figure 13: Math: Thinking Case 24 Figure 14: Algebra: Non-Thinking Case 25 Figure 15: Geometry: Non-Thinking Case 26 Figure 16: TopoLogy: Non-Thinking Case 27 Figure 17: Calculus: Non-Thinking Case 28 Figure 18: Space-Time: Thinking Case 29 Figure 19: Space Reasoning: Non-Thinking Case 30 Figure 20: Time: Non-Thinking Case 31 Figure 21: Map: Thinking Case 32 Figure 22: Route Plan: Non-Thinking Case 33 Figure 23: Street Map: Non-Thinking Case 34 Figure 24: Science: Thinking Case 35 Figure 25: Chemistry: Non-Thinking Case 36 Figure 26: Physics: Non-Thinking Case 37 Figure 27: Geography: Non-Thinking Case 38 Figure 28: Biology: Non-Thinking Case"
        }
    ],
    "affiliations": [
        "Huazhong University of Science and Technology",
        "Lehigh University"
    ]
}