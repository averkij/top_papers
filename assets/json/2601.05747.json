{
    "paper_title": "FlyPose: Towards Robust Human Pose Estimation From Aerial Views",
    "authors": [
        "Hassaan Farooq",
        "Marvin Brenner",
        "Peter St\\ütz"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such as parcel delivery, traffic monitoring, disaster response and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions from an aerial viewpoint. This perspective challenges existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, a lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of ~20 milliseconds including preprocessing on a Jetson Orin AGX Developer Kit and is deployed onboard a quadrotor UAV during flight experiments. We also publish FlyPose-104, a small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose."
        },
        {
            "title": "Start",
            "content": "FlyPose: Towards Robust Human Pose Estimation From Aerial Views"
        },
        {
            "title": "Marvin Brenner",
            "content": "Peter Stutz Universitat der Bundeswehr Munich {hassaan.farooq, marvin.brenner, peter.stuetz}@unibw.de 6 2 0 2 9 ] . [ 1 7 4 7 5 0 . 1 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Unmanned Aerial Vehicles (UAVs) are increasingly deployed in close proximity to humans for applications such traffic monitoring, disaster response as parcel delivery, and infrastructure inspections. Ensuring safe and reliable operation in these human-populated environments demands accurate perception of human poses and actions This perspective challenges from an aerial viewpoint. existing methods with low resolution, steep viewing angles and (self-)occlusion, especially if the application demands realtime feasibile models. We train and deploy FlyPose, lightweight top-down human pose estimation pipeline for aerial imagery. Through multi-dataset training, we achieve an average improvement of 6.8 mAP in person detection across the test-sets of Manipal-UAV, VisDrone, HIT-UAV as well as our custom dataset. For 2D human pose estimation we report an improvement of 16.3 mAP on the challenging UAV-Human dataset. FlyPose runs with an inference latency of 20 milliseconds including preprocessing on Jetson Orin AGX Developer Kit and is deployed onboard quadrotor UAV during flight experiments. We also publish FlyPose-104, small but challenging aerial human pose estimation dataset, that includes manual annotations from difficult aerial perspectives: https://github.com/farooqhassaan/FlyPose. 1. Introduction With rising levels of automation in commercial unmanned aerial vehicles (UAVs, also commonly referred to as drones), new applications such as parcel delivery, searchand-rescue, infrastructure inspection and urban traffic monitoring are being introduced. Their operation in humanpopulated spaces [2] makes the robust perception of human presence and behaviors indispensible. In both humanhuman and human-machine interaction, the configuration of body joints conveys non-verbal cues for understanding body language, physical movement and intent. The task of human pose estimation (HPE) represents widely-used founFigure 1. Applications like urban traffic monitoring and sling-load cargo deliveries with unmanned aerial vehicles pose major challenge for human pose estimation due to varying person sizes, occlusion and low-resolution overhead imagery [4, 9, 50]. dational module to extract these cues from images to enable skeleton-based downstream tasks such as action recognition, pose-based tracking, motion prediction, engagement detection or interaction via hand gestures. Based on the insights gained from these tasks, the UAV can then respond appropriately based on its own behavior model. The majority of applications for HPE cannot assume ground-level or shoulder-height camera view where the human subject is seen frontally or laterally, as this viewpoint would enable the observation of distinguishable human body features. Aerial viewpoints tend to occlude pose keypoints of the face or legs, distort body proportions and challenge pose models trained on ground-level images. Therefore, our work explores human pose estimation from an aerial perspective to aid UAV applications in human-populated environments (Figure 1). Aerial human pose estimation introduces distinct challenges compared to ground-based setups. The UAVs sensor depression angle is steep, up to 90 degrees, resulting in top-down imagery of humans with foreshortened limbs and frequent self-occlusion (Figure 2). At the same time, the UAV needs to ascend to altitudes free of obstacles, where its sensors ground sampling distance increases and hence people occupy less pixels in the image. This drastically limits the distance at which the sensor can perceive humans and the model is able to predict stable, usable poses. Additionally, UAVs must operate under strict payload weight, dimensions and power constraints during flight, further limiting available computational resources for processing higher resolution images onboard. Figure 2. Two examples from our FlyPose-104 dataset with manually annotated bounding boxes and poses, featuring frequent selfocclusions of lower body and facial joints (marked in red) [4, 50]. In this paper, we investigate these challenges by training FlyPose, top-down human pose estimation model with focus on aerial imagery that is deployable on edge-devices. Our contributions are as follows: Development of top-down human pose estimation pipeline to predict more accurate poses from aerial views by training across multiple aerial datasets to deal with top-down views and small scales. Manual annotation and release of the FlyPose-104 dataset, challenging aerial pose estimation test set. Integration of the trained models into hardware system, with reporting of model latencies on an edge device during real flight. 2. Related Work Top-down pose estimation methods like [10, 49] begin by detecting person bounding boxes using an object detector, and then run body keypoint estimation model on each cropped region. This allows for higher precision compared to bottom-up methods like [5, 26], as the pose model focuses on smaller, person-specific patch with less distractions from unconstrained backgrounds. Near groundlevel, VisionTransformer [8] based object detectors like CoDETR [61] can already localize humans robustly across variety of scene backgrounds [23]. For multi-scale aerial imagery in altitudes above 20 meters, low resolution, dense object clusters and occlusion impede large scale data annotation, as consequence the publicly available training data is typically scattered into smaller application-specific datasets, like urban surveillance [9, 25], traffic monitoring [52], Search and Rescue [3, 44, 56], Helicopter Cargo Operations [4] and Remote Sensing [21, 45, 46]. Tiny object detection requires specifically tailored two-stage architectures [32, 35, 48] which limit realtime feasibility. Since extracting human poses is not feasible for tiny person scales, we focus on popular onestage object detectors like YOLO [34] and the few realtime feasible DETR variants [7, 60] so far. They are frequently modified to deal with aerial imagery [6]. There have also been attempts to use augmentation strategies like Unified Foreground Packaging [16, 24], to improve multi-scale loss convergence [41] and reduce the amount of negative background pixels on higher resolution images during training. However, on dataset like VisDrone [9], the performance of the state-of-the-art ViT-YOLO [59] at 41 mAP on the VisDrone test set highlights limited success from the aerial perspective. At present, the higher accuracy for smaller objects requires larger input sizes and model complexity, which inturn increases the inference latency. Similar to autonomous driving [58], we envision that more generalizable, lightweight person detector would be beneficial for practitioners and could be used across UAV applications. For Pose Estimation, ViT-based architectures like ViTPose [49] also represent the current state-of-the art on the COCO test-dev set, while examples for lightweight architectures with competitive performance are RTMO [26], RTMPose [18], KAPAO [29] and YOLO-Pose [28]. Nevertheless, their accuracy drops notably when applied to UAV imagery, due to viewpoint shifts and scale variation, even when not accounting for small objects. Top-down views like in CMU Panoptic [19], ITOPS [11, 13] and PoseFES [53] are rarely explored outside controlled conditions. There have been very few recent works [17, 20] that try to address the challenge of aerial human pose estimation from UAVs. In [20], they try to evaluate the performance of out-of-the-box pose estimators on various datasets and gauge their performance on edge devices. The authors of [17] instead attempt to refine an estimated pose to resemble poses from an aerial poses latent codebook. small number of aerial datasets like Aerial Gait [31] or the synthethic dataset in Airpose [39] exist, but they are limited in variety and scale. Airpose attempts multi-view 3D human pose and shape estimation but the used methods still rely on 2D pose priors from OpenPose and AlphaPose which were only finetuned for slightly elevated views from the side without top-views or covering larger distances. UAV-Human [22] is the largest aerial pose dataset so far, offering annotated human keypoints from drone footage in diverse settings. The models and datasets in the literature have potential for improvement, as due to viewpoint shifts and scale variation, the detection and 2D poses often fail on realworld UAV topview imagery like [4, 37]. This underscores the need for HPE model designed for aerial perspectives. Our work seeks to bridge this gap by adapting and extending high-performing top-down models to perform robustly in diverse UAV-captured environments under realtime constraints. We also deploy our pipeline in real flight onboard UAV to validate the results. on small scale persons and overhead views. Generally, our strategy is to first benchmark model architectures on aerial datasets to identify suitable models regarding the accuracylatency trade-off and then finetune on additional aerial data to improve the results for our application without introducing significant computational overhead. After FlyPose is trained, we optimize the model for deployment on an edge device and test it in real flight experiments. 3. Methodology We present FlyPose, an onboard human pose estimation approach tailored for drones operating under tight computational constraints in altitudes of up to 40 meters. FlyPose follows top-down human pose estimation (HPE) pipeline where incoming Full-HD video frames from stabilized, multi-spectral camera are first processed by lightweight person detector, after which dedicated pose estimation module extracts 2D keypoints from the detected regions. An overview of the system is illustrated in Figure 3. the bottom illustrates the FlyPose Figure 3. System overview: pipeline, where the detector and pose estimation model are trained separately, the top is an example for how the aggregated information can be used for downstream tasks within the drone system for various applications. FlyPose is designed to serve as perceptual backbone for range of person-aware downstream tasks such as gesturebased control, action recognition and future pose prediction. The insights from the downstream tasks are intended to be integrated into broader drone behaviour model, which can then instruct the flight controller to move along desired trajectory or alter the current maneuver. In this work, we focus on the core challenge of achieving reliable onboard 2D human pose estimation in realtime. We choose lightweight top-down HPE approach, where the detector is separately trained for aerial person detection and heatmapbased pose estimation method is finetuned to deal with unusual, self-occluding overhead poses. We leverage combination of existing datasets to achieve better performance 3.1. Top-down Human Pose Estimation The person detector receives an input image RHW 3 and outputs set of bounding boxes B, where each bounding box bi = (xi, yi, wi, hi) represents the coordinates of the top-left corner and the width and height of the detected person. We follow the training procedure of RT-DETRv2 [27], but experiment with replacing the generalized intersection over union loss with Normalized Wasserstein Distance Loss (NWDL) [48] to achieve more stable training despite the smaller objects in aerial imagery, similar to [4]. The pose estimation is then performed on cropped image patches corresponding to each person detection bi. The pose estimation task is framed as the prediction of heatmaps for the 17 body keypoints defined by the COCO format. For each keypoint, the network outputs 2D confidence map, indicating the likelihood of the keypoints presence at each pixel location. The training objective minimizes the discrepancy between the predicted and ground-truth heatmaps using the standard mean squared error (MSE) loss. During inference, keypoints are extracted from these heatmaps using non-maximum suppression (NMS) to localize peak responses. 3.2. Evaluation Metrics For person detection, we use the COCO mean Average Precision (mAP) which averages the Average Precisions across the 0.5-0.95 intersection over union (IoU) thresholds and the Average Recall (AR) for 100 allowed predictions. Since we conduct multi-dataset training, the Average Precision and Recall across all testing sets, weighted by the number of corresponding frames, is reported for reference. We do not report classification scores, since all datasets were filtered to only include one person class. For human pose estimation, we evaluate the model performance using standard COCO mean Average Precision (mAP) for keypoints, similarly averaging across the 0.5-0.95 Object Keypoint Similarity (OKS) thresholds. We use the standard OKS metric by COCO [23] that measures how close predicted keypoints are to the ground-truth, normalized by the person scale (bounding box area) and per-keypoint tolerance. To measure the feasibility of the models in practice, we measure the latency of the models as the time taken for single forward pass through the model with batch size of 1. The latency of the person detection and pose estimation models are reported separately, with the expectation of the latency of pose estimation to increase for multiple persons because of the top-down pose estimation. Although this can be offset by batching person detections together and incurring cost of preparing the batch, further experimentation is necessary. The inference latency for the models is measured on Mobile GPU A6000 as well as on the Jetson Orin AGX Developer Kit with 32 GB RAM. Note that on the Jetson, the models were converted to FP32 TensorRT engines for lower latency. 3.3. FlyPose-104 dataset Due to the limited availability of annotated human pose data for aerial imagery, we collect and annotate small dataset of 104 images, including our own images and those sourced from [4, 15, 36, 50]. few annotated samples of our FlyPose-104 dataset can be seen in Figure 2. It covers variety of backgrounds including snow, dirt, concrete, water and grass, making it small yet challenging dataset. The viewpoints feature 90 degrees top-down viewpoints as well as heavy occlusion examples. total of 193 persons were manually annotated with person bounding boxes and 17 COCO pose keypoints, along with the keypoint visibility flag. We include samples with multi-scale person bounding boxes recorded at altitudes between 5 and 50 meters. FlyPose-104 essentially aims to capture representative images that reflect the wide range of challenges inherent to aerial human pose estimation, including variations in person scale (due to altitude), camera viewpoints (top-down and angled), background complexity, application contexts and (self-)occlusions. 3.4. Preliminary Testing For the person detector, RT-DETRv2-S was tested with alternative pretrained backbones like RegNetX-400MF [33], Hiera-Tiny [38] and bottom-heavy version [30] of ResNet18 [14] by finetuning for 50 epochs on VisDrone2019-DET at 1280 pixels input resolution. However, none of them performed higher on the VisDrone2019-DET test-dev set after training than regular ResNet-18 with 28.6 mAP which is in line with recent backbone benchmark [12]. It showed that for generic object detection on limited computational resources, ResNet-18 so far remains an efficient choice preferable to transformer-based backbones. When observing qualitative results on VisDrone, we notice that most of the precision loss for tiny backbones comes from smaller objects in the background that they cannot accurately localize, at least in single stage without bidirectional feature fusion. For RT-DETRv2-S , the preliminary detection mAP values for small (18.4 mAP), medium (40.0 mAP) and large (53.6 mAP) objects on the VisDrone test set support this hypothesis. Therefore, we decide to use ResNet-18 as backbone for RT-DETRv2 at the cost of missing tiny people in the background, that would likely not result in stable, meaningful poses for downstream tasks anyhow. Figure 4. Pose Estimation performance of pretrained models on the UAV-Human dataset, plotted against their latency on an RTX A6000 GPU. Each circles radius is proportional to the model parameter count. To evaluate the performance of current state-of-the-art pose estimation models in aerial scenarios, we perform preliminary evaluations on the UAV-Human v1 test set [22]. The authors [22] report results using pretrained AlphaPose and HigherHRNet, with AlphaPose slightly outperforming at 56.9 mAP [22]. Building on this, we evaluate range of recent 2D pose estimation architectures that have shown strong results on ground-level datasets like COCO, including ViTPose [49], RTMPose [18], AlphaPose [10], HRNet [42], LiteHRNet [51], Swin-Pose [47] and YOLOv8-Pose [28]. Our aim was to first assess how well these pretrained models perform on the aerial perspectives of the UAVHuman dataset, the results of which can be seen in Figure 4. Among the tested models, ViTPose-H pretrained on COCO achieved the highest performance with 67.52 mAP on the UAV-Human dataset. RTMPose is decent competitor for its model size but overall given both ViTPoses strong results in this setting and its leading performance on COCO, we continue with this architecture for our application. 4. Experiments and Results Based on the preliminary testing on the VisDrone and UAVHuman datasets, we continue with the RT-DETRv2 and ViTPose models for the following experiments. We train the aerial person detector and aerial pose estimation networks separately. We evaluate the performance of our trained models, combine the pipeline to test it on edge hardware and finally deploy it in real flight. COCO-Person VisDrone-Person Manipal-UAV test-dev test val HIT-UAV-Person test (infrared) AR AP FlyPose-104 Weighted Average test (ours) AR AP across test sets AR AP AP AR Method AP AR AP Baseline + VisDrone + Multi-Dataset + COCO-Person + NWD Loss 60.75 21.28 22.24 61.39 60.94 75.57 43.20 42.11 75.52 74. 10.44 21.08 21.07 20.21 20.20 AR 18.96 29.46 29.13 28.87 28.77 20.20 25.35 27.32 28.39 27.90 30.58 35.06 38.06 40.46 40.42 3.16 7.06 52.54 49.21 49. 30.18 29.45 62.73 61.37 61.28 10.26 22.42 22.67 25.05 27.41 46.17 45.91 45.13 48.71 48.96 14.33 21.43 28.21 28.07 27.96 26.76 32.61 38.20 39.21 39.14 Table 1. Person detection performance of RT-DETRv2-S after multi-dataset training and applying NWD loss at 1280 pixels input size. The average across test sets is computed column-wise and weighted by the number of corresponding images to monitor overall performance. 4.1. Aerial Person Detection For the quantitative evaluation of person detection, we used the person-only test(-dev) splits of VisDrone [9], ManipalUAV [1], HIT-UAV [43] and our custom FlyPose-104 dataset (Section 3.3). To monitor the drop in frontal-view generalizability which occurs when finetuning on larger set of aerial imagery, we additionally report changes to the COCO-Person validation set performance. The main person detection results after each training are displayed in Table 1. As first step, RT-DETRv2-S with pretrained weights from COCO [23] and Objects365 [40] is finetuned for 60 epochs on the VisDrone2019-DET dataset [9] after removing all classes other than pedestrian and person, which are combined into single person class. After the finetuning, the model performs 7.1 mAP higher on average across all of the used test-sets. This first step is standard practice but VisDrone only features imagery from nearby the city of Tianjin, therefore we proceed to gather additional aerial datasets featuring person class to further improve the models generalization ability outside urban scenarios. We add the original train/val split of eight additional aerial object detection datasets, resulting in 66849 additional images for training and 21164 for validation besides VisDrone2019-DET. To give brief overview, we describe them here: SeasDronesSea [44] and Heridal [3] represent RGB aerial imagery for search-and-rescue (SAR) purposes, where the former has maritime and the latter hilly, mediterranean terrain as background. VTSAR [56], DroneRGBT [57], VTUAV-det [55] feature both RGB and thermal imagery from UAVs for training, while HIT-UAV [43] is the only pure thermal dataset with separate testing split. Manipal-UAV [1] features person detection at altitudes from 10 to 50 meters with complex backgrounds and TinyPerson [54] is the most challenging dataset for our approach, since it focusses purely on dense clusters of tiny people from an aerial perspective. Like with VisDrone, only the person-related classes are kept in the annotations to test if the model learns robust features although presented with both aerial frontal and top-views. Rather than inspecting their individual performance, we combine them into one dataset together with VisDrone and the resulting test set performance is reported in the row Multi-Dataset in Tab. 1. Note that we did not exclude thermal imagery because for night-time applications like SAR, the model has to be able to predict on both modalities. As result, the detector gains another 6.78 mAP from the previous iteration on average but mainly caused by the introduction of the thermal imagery and the resulting improvement in the HIT-UAV testset. Since the frontal perspectives within the COCO dataset are still useful in frontal low-altitude perspectives and the initial mAP on the COCO validation set decreased after finetuning on aerial data, we reintroduce the COCO-Person train-val split into the finetuning to mitigate this effect. This step slightly decreased the average mAP on the aerial testsets by 0.14 mAP but improved the average AR by 1.01. Finally, to improve the localization on smaller objects, Normalized Wasserstein Distance Loss [48] was introduced for the final training which had only neglible impact overall but it did improve both mAP and AR on our FlyPose-104 test set. Although individual training steps led to the highest increase of mAP on one test-set (for example, only finetuning on VisDrone had the best results on the respective test set), our multi-dataset version of RT-DETRv2-S generalizes robustly across all used aerial test sets (examples in Figure 5). To establish the models realtime feasibility, we measured 13 milliseconds of inference latency for single forward pass on the Jetson Orin AGX Developer Kit with 32 GB after conversion to TensorRT FP32 engine. 4.2. Aerial Pose Estimation We experiment with the small, base, large and huge versions of ViTPose, using consistent input image size of 256x192. During inference, the detected person patch is resized while preserving its aspect ratio with the larger dimension scaled to fixed size (256px for height or 192px for width), ensuring that the patch is centered without distortion. During training, in addition to the standard augmentation techniques described in [49], we introduce downscaling augmentation (by 5-20% of the patch) to simulate smaller person sizes and lower resolutions encountered in aerial footage during training. As consistent pose annotations for low-resolution people are difficult to obtain, this aims to mimic persons that are viewed from greater distances or affected by motion blur. Method ViTPose-S ViTPose-B ViTPose-L ViTPose-H mAP (pretrained) mAP (finetuned) Latency [ms] (A6000) Latency [ms] (Jetson Orin) 61.09 63.15 66.50 67.52 65.76 67.50 70.31 73.18 110.23 116.20 198.30 322.55 6.54 11.62 22.35 - Table 2. Pose Estimation results of ViTPose on the UAV-Human v1 test-set, using the trained RT-DETRv2-S person detector. We use similar training strategy to [49], starting with COCO-pretrained weights of these models and finetune them for between 170-210 epochs on the UAV-Human v1 train-set. The performance of the pretrained and finetuned models is summarized in Table 2. ViTPose-H achieves the highest accuracy with 73.18 mAP on the UAV-Human dataset which is an improvement of 16.3 mAP over the previously reported AlphaPose result of 56.9 mAP (Section 3.4). However, ViTPose-H exhibits significantly slower inference on single NVIDIA A6000 GPU and we could not deploy it on the Jetson Orin AGX Developer Kit. As expected, using the smaller model sizes leads to lower mAP scores at faster throughput frequencies. mance on the Jetson Orin with an inference time of just 6.54 ms when converted to TensorRT FP32 engine, making it the most suitable candidate for our onboard system. Given the realtime demands of various aerial applications, ViTPose-S is selected as the core HPE model for FlyPose. qualitative example of the full FlyPose pipeline showing detection and pose estimation outputs is presented in Figure 6. In combination, our final FlyPose model consisting of RT-DETRv2-S and ViTPose-S has an inference latency of only 19.54 miliseconds on Jetson Orin AGX developer kit for single image forward pass, excluding preor postprocessing. Figure 6. Qualitative FlyPose result on the UAV-Human test-set. ViTPose-S, in particular, achieves the fastest perfor4.3. Deployment onboard the UAV To validate FlyPose in real flight with UAV, the Jetson Orin AGX developer kit with 32GB of RAM and multispectral gimbal camera were mounted onto commercial quadrotor UAV with maximum take-off weight of 35kg, and connected via Ethernet to receive images from the cameras Full-HD RTSP stream. We then run FlyPose on the embedded system to predict poses in order to estimate where person is pointing in simulated cargo pickup task. Figure 7 shows snapshot from one of the flight experiments, where the left image is view of the onboard camera and the right shows the UAV itself from an external perspective. During our experiments, we observed one-time initialization delay of approximately 300 ms when acquiring the RTSP image stream from the camera. This latency means that frames become available for processing about 300 ms after the corresponding event occurs. This cameradependent delay appears only at the start of the stream and can be reduced by using simpler camera interfaces. Once frame is acquired, the image preprocessing operations (padding and rescaling) performed with the CUDA jetsonutils library take about 0.5 ms per frame. The FlyPose inference requires about 19.54 ms (13 ms for detection, 6.54 ms for pose estimation) for single person in the scene. Figure 5. Qualitative detections (red) on the VisDrone2019-DET (top), FlyPose-104 (bottom left) and HIT-UAV (bottom right). the persons shadow and the feet for example can be visually ambiguous. Cluttered backgrounds with unfamiliar elements still frequently led to false positives, whereas increasing the confidence threshold also prevented distant objects from being picked up. Although top-down and frontal views, as well as RGB and thermal imagery showcase different visual features, we chose not to separate them into distinct classes. Instead we train on merged multi-modal dataset, where the person class still represents the same object category, and an applied model should be able to handle viewpoint and color variations. While our detection precision is not on par with larger scale state-of-the-art models on each individual dataset, RT-DETRv2-S still demonstrates good generalization across multiple test sets, views and modalities at small memory footprint and is realtime feasibile on an embedded system. Figure 8. Qualitative FlyPose Results on various aerial datasets. The baseline is the COCO-pretrained ViTPose-S and Ours is the finetuned version. For the second step of pose estimation, we note that HPE from aerial views remains underexplored, primarily due to the scarcity of pose-annotated datasets from this perspective. The current state-of-the-art models can greatly improve for aerial perspectives in the presence of good quality datasets. In this context, the FlyPose-104 dataset serves as particularly challenging benchmark, illustrating how most current models struggle in aerial views. To address the shortage of aerial pose-annotated datasets, one promising direction could be to leverage transformed 3D HPE datasets to refine predicted poses. While flight altitude and camera metadata are rarely included in existing datasets, these factors could also provide valuable priors to models about expected sizes and resolutions of persons. We additionally attempted to incorporate the PoseFES dataset which offers overhead indoor views into our trainings, but its limited size and constrained diversity did not contribute significantly to the overall model performance. Examples of the pose results of our FlyPose model on the UAV-Human, Manipal, Okutama, FlyPose-104 and Figure 7. We fly our setup in real flight to demonstrate the feasibility of FlyPose. The synchronized snapshot shows the onboard camera view (left) and the UAV in flight (right). The person detection and predicted poses using FlyPose are overlayed. We anticipate longer inference times in multi-person scenarios but further experimentation is needed to evaluate the trade-offs of batching detections for pose estimation. With total time of 20 ms from frame acquisition to pose prediction, the system is well within the range of 25fps realtime performance, leaving roughly another 20 ms per frame for downstream tasks such as tracking, gesture recognition and action recognition. This is beneficial for novel drone applications that rely on gesture-based human-drone interaction or fast responses to recognized human activity, since these scenarios require low-latency reactions from the UAV. Our hardware setup remains flexible for additional applicationspecific hardware as the UAV is capable of carrying 15kg of payload besides its own weight and the flight batteries. The integrated sensory payload currently weighs about 4kg, which leaves 11kg for task-specific hardware. 5. Discussion In this section, we discuss the key insights and limitations gathered through the development, evaluation and deployment of FlyPose. Through range of experiments across different model variants and datasets, we explored the challenges within human pose estimation from aerial perspectives. In the person detection stage of our pipeline, we learned that while training on aerial imagery significantly improved the models predictions from overhead views, lightweight detectors like RT-DETRv2 still face challenges with small objects, even after training on datasets like TinyPerson. Similar to human perception, in order to distinguish small scale objects from background noise, temporal motion features are sometimes essential. Mixing top-down aerial views with frontal imagery from datasets like COCO frequently lead to the model omitting an extended arm when viewing the person from the top or falsely including background object that resembles an arm. Especially, when the person is wearing darker clothing, the boundary between Figure 9. Qualitative FlyPose results of person detection and pose estimation on the VisDrone dataset. VisDrone datasets are shown in Figure 8. The COCOpretrained ViTPose-S baseline model frequently struggles with discerning self-occluded keypoints, these are better estimated by our finetuned model. It can be observed that it performs relatively better on low resolution patches as well. Cases where our model still struggles are with persons strongly camouflaged by the background or when detected patches contain multiple people in close proximity. Qualitative samples of thermal imagery were also inspected, but while the features did generalize to degree, the poses were not as reliable as on aerial RGB images. Due to the lack of color information at lower resolutions, left-right keypoint identity swaps occurred more frequently when it was visually ambigous which direction the person is facing. We also observe that the keypoint heatmaps show weak confidence for joint detections in low-resolution patches, as they are frequently cut-off by the used confidence threshold of 0.4 for keypoint estimates. Nevertheless, for individuals at greater distances, person detection may suffice for most applications. Our additional evaluation results on the UAVHuman dataset revealed consistent drop in average Object Keypoint Similarity (OKS) scores for facial keypoints such as nose, eyes and ears, compared to body keypoints like shoulders, elbows, wrists, hips, knees and feet. This suggests that facial landmarks are particularly challenging to localize from aerial viewpoints, likely due to occlusions and limited visibility. Although more analysis on the impact of high-person-count images on the system needs to be conducted, one way to deal with this could be an informed selection strategy that only performs keypoint estimation on relevant detected people depending on the current task of the UAV. Finally, despite the current challenges in aerial human pose estimation, FlyPose is first step to more robustly recognize human poses and perform downstream tasks like gesture and action recognition from aerial views, directly onboard the UAV. 6. Conclusion We presented FlyPose, lightweight person detection and pose estimation model for aerial views. Our person detector leverages the RT-DETRv2-S architecture, trained on nine aerial detection featuring RGB and thermal imagery. It achieves competitive performance across the test sets of VisDrone, Manipal-UAV and HIT-UAV. Our pose estimator utilizes the ViTPose-S architecture and further improves its performance on the UAV-Human dataset. The FlyPose model delivers quality pose estimation results on variety of aerial viewpoints, while maintaining small footprint suitable for embedded systems. The FlyPose model was deployed in real flight experiments to confirm the results from various altitudes and aerial viewpoints. As drones become increasingly integrated into human-populated environments, the need for intelligent, context-aware drones will grow, so it can be expected that more practicallydeployable models like FlyPose tailored for drone applications with realtime requirements will be needed."
        },
        {
            "title": "Acknowledgements",
            "content": "by funded This work was Federal Ministry and Energy the Federal Aviation Reas (BMWK), search Program LuFo VI-2 (CargoPack, Grant-ID: 20D2111D). for Economic Affairs part the German of"
        },
        {
            "title": "References",
            "content": "[1] KR Akshatha, AK Karunakar, Satish Shenoy, Phani Pavan, Dhareshwar Chinmay, et al. Manipal-uav person detection dataset: step towards benchmarking dataset and algorithms for small object detection. ISPRS Journal of Photogrammetry and Remote Sensing, 195:7789, 2023. 5 [2] Mehmet Aydin Baytas, Damla ay, Yuchong Zhang, Mohammad Obaid, Asim Evren Yantac, and Morten Fjeld. The design of social drones: review of studies on autonomous flyers in inhabited environments. In Proceedings of the 2019 CHI Conference on Human Factors in Computing Systems, pages 113, 2019. 1 [3] Dunja BoˇzicˇStulic, ˇZeljko Maruˇsic, and Sven Gotovac. Deep learning approach in aerial imagery for supporting land search and rescue missions. International Journal of Computer Vision, 127(9):12561278, 2019. 2, 5 [4] Marvin Brenner and Peter Stutz. Udw: dataset for automated monitoring of cargo-handling during sling-load operations. In 2023 Seventh IEEE International Conference on Robotic Computing (IRC), pages 223230. IEEE, 2023. 1, 2, 3, 4 [5] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Openpose: Realtime multi-person 2d pose estimation using part affinity fields, 2019. 2 [6] Chunling Chen, Ziyue Zheng, Tongyu Xu, Shuang Guo, Shuai Feng, Weixiang Yao, and Yubin Lan. Yolo-based uav technology: review of the research and its applications. Drones, 7(3):190, 2023. [7] Qiang Chen, Xiangbo Su, Xinyu Zhang, Jian Wang, Jiahui Chen, Yunpeng Shen, Chuchu Han, Ziliang Chen, Weixiang Xu, Fanrong Li, et al. Lw-detr: transformer replacement to yolo for real-time detection. arXiv preprint arXiv:2406.03459, 2024. 2 [8] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszkoreit, and Neil Houlsby. An image is worth 16x16 words: Transformers for image recognition at scale, 2021. 2 [9] Dawei Du, Pengfei Zhu, Longyin Wen, Xiao Bian, Haibin Lin, Qinghua Hu, Tao Peng, Jiayu Zheng, Xinyao Wang, Yue Zhang, et al. Visdrone-det2019: The vision meets drone object detection in image challenge results. In Proceedings of the IEEE/CVF international conference on computer vision workshops, pages 00, 2019. 1, 2, 5 [10] Hao-Shu Fang, Jiefeng Li, Hongyang Tang, Chao Xu, Haoyi Zhu, Yuliang Xiu, Yong-Lu Li, and Cewu Lu. Alphapose: Whole-body regional multi-person pose estimation IEEE Transactions on Pattern and tracking in real-time. Analysis and Machine Intelligence, 2022. 2, 4 [11] Nicola Garau, Niccolo Bisagno, Piotr Brodka, and Nicola Conci. Deca: Deep viewpoint-equivariant human pose estiIn Proceedings of the mation using capsule autoencoders. IEEE/CVF International Conference on Computer Vision, pages 1167711686, 2021. 2 [12] Micah Goldblum, Hossein Souri, Renkun Ni, Manli Shu, Viraj Prabhu, Gowthami Somepalli, Prithvijit Chattopadhyay, Mark Ibrahim, Adrien Bardes, Judy Hoffman, et al. Battle of the backbones: large-scale comparison of pretrained models across computer vision tasks. Advances in Neural Information Processing Systems, 36:2934329371, 2023. 4 [13] Albert Haque, Boya Peng, Zelun Luo, Alexandre Alahi, Serena Yeung, and Li Fei-Fei. Towards viewpoint invariant 3d In European conference on comhuman pose estimation. puter vision, pages 160177. Springer, 2016. [14] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun. Deep residual learning for image recognition. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 770778, 2016. 4 [15] HensliSage. Gopro: Dreamy ski + snowboard drone reel 5k coffee break. Youtube Channel: GoPro, 2024. 4 [16] Yecheng Huang, Jiaxin Chen, and Di Huang. Ufpmp-det: Toward accurate and efficient object detection on drone imagery. In Proceedings of the AAAI conference on artificial intelligence, pages 10261033, 2022. 2 [17] Juheon Hwang and Jiwoo Kang. Aerial view 3d human pose estimation using double vector quantized-variational autoencoders. In 2024 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW), pages 341 350, 2024. 2 [18] Tao Jiang, Peng Lu, Li Zhang, Ningsheng Ma, Rui Han, Chengqi Lyu, Yining Li, and Kai Chen. Rtmpose: Real-time multi-person pose estimation based on mmpose, 2023. 2, 4 [19] Hanbyul Joo, Tomas Simon, Xulong Li, Hao Liu, Lei Tan, Lin Gui, Sean Banerjee, Timothy Scott Godisart, Bart Nabbe, Iain Matthews, Takeo Kanade, Shohei Nobuhara, and Yaser Sheikh. Panoptic studio: massively multiview system for social interaction capture. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2017. [20] Theofanis Kalampokas, Stelios Krinidis, Vassilios Chatzis, and George Papakostas. Performance benchmark of deep learning human pose estimation for uavs. Machine Vision and Applications, 34, 2023. 2 [21] Darius Lam, Richard Kuzma, Kevin McGee, Samuel Dooley, Michael Laielli, Matthew Klaric, Yaroslav Bulatov, and Brendan McCord. xview: Objects in context in overhead imagery. arXiv preprint arXiv:1802.07856, 2018. 2 [22] Tianjiao Li, Jun Liu, Wei Zhang, Yun Ni, Wenqian Wang, and Zhiheng Li. Uav-human: large benchmark for human behavior understanding with unmanned aerial vehicles. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1626616275, 2021. 2, 4 [23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollar, and Lawrence In Zitnick. Microsoft coco: Common objects in context. Computer visionECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, pages 740755. Springer, 2014. 2, 3, 5 [24] Kai Liu, Zhihang Fu, Sheng Jin, Ze Chen, Fan Zhou, Rongxin Jiang, Yaowu Chen, and Jieping Ye. Esod: Efficient small object detection on high-resolution images. IEEE Transactions on Image Processing, 2024. 2 [25] Zhihao Liu, Zhijian He, Lujia Wang, Wenguan Wang, Yixuan Yuan, Dingwen Zhang, Jinglin Zhang, Pengfei Zhu, Luc Van Gool, Junwei Han, et al. Visdrone-cc2021: the vision meets drone crowd counting challenge results. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 28302838, 2021. [26] Peng Lu, Tao Jiang, Yining Li, Xiangtai Li, Kai Chen, and Wenming Yang. Rtmo: Towards high-performance onestage real-time multi-person pose estimation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 14911500, 2024. 2 [27] Wenyu Lv, Yian Zhao, Qinyao Chang, Kui Huang, Guanzhong Wang, and Yi Liu. Rt-detrv2: Improved baseline with bag-of-freebies for real-time detection transformer. arXiv preprint arXiv:2407.17140, 2024. 3 ceedings of the IEEE/CVF international conference on computer vision, pages 84308439, 2019. 5 [28] Debapriya Maji, Soyeb Nagori, Manu Mathew, and Deepak Poddar. Yolo-pose: Enhancing yolo for multi person pose estimation using object keypoint similarity loss. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 26372646, 2022. 2, 4 [29] William McNally, Kanav Vats, Alexander Wong, and John McPhee. Rethinking keypoint representations: Modeling keypoints and poses as objects for multi-person human pose In European Conference on Computer Vision, estimation. pages 3754. Springer, 2022. [30] Jinlai Ning, Haoyan Guan, and Michael Spratling. Rethinking the backbone architecture for tiny object detection. arXiv preprint arXiv:2303.11267, 2023. 4 [31] Asanka G. Perera, Yee Wei Law, and Javaan Chahl. Human pose and path estimation from aerial video using dynamic classifier selection. Cognitive Computation, 10(6): 10191041, 2018. 2 [32] Siyuan Qiao, Liang-Chieh Chen, and Alan Yuille. Detectors: Detecting objects with recursive feature pyramid and switchIn Proceedings of the IEEE/CVF able atrous convolution. conference on computer vision and pattern recognition, pages 1021310224, 2021. 2 [33] Ilija Radosavovic, Raj Prateek Kosaraju, Ross Girshick, Kaiming He, and Piotr Dollar. Designing network design spaces. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1042810436, 2020. 4 [34] Joseph Redmon, Santosh Divvala, Ross Girshick, and Ali Farhadi. You only look once: Unified, real-time object detection. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 779788, 2016. 2 [35] Shaoqing Ren, Kaiming He, Ross Girshick, and Jian Sun. Faster r-cnn: Towards real-time object detection with region IEEE transactions on pattern analysis proposal networks. and machine intelligence, 39(6):11371149, 2016. 2 [36] Westpac Little Ripper. The worlds first surf rescue by drone. Youtube Channel: Business InsiderAu, 2018. [37] Arturo Miguel Russell Bernal, Walter Scheirer, and Jane Cleland-Huang. Nomad: natural, occluded, multi-scale aerial dataset, for emergency response scenarios. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 85848595, 2024. 3 [38] Chaitanya Ryali, Yuan-Ting Hu, Daniel Bolya, Chen Wei, Haoqi Fan, Po-Yao Huang, Vaibhav Aggarwal, Arkabandhu Chowdhury, Omid Poursaeed, Judy Hoffman, et al. Hiera: hierarchical vision transformer without the bells-andwhistles. In International conference on machine learning, pages 2944129454. PMLR, 2023. 4 [39] Nitin Saini, Elia Bonetto, Eric Price, Aamir Ahmad, and Michael J. Black. Airpose: Multi-view fusion network for aerial 3d human pose and shape estimation. IEEE Robotics and Automation Letters, 7(2):48054812, 2022. 2 [40] Shuai Shao, Zeming Li, Tianyuan Zhang, Chao Peng, Gang Yu, Xiangyu Zhang, Jing Li, and Jian Sun. Objects365: large-scale, high-quality dataset for object detection. In Pro- [41] Bharat Singh, Mahyar Najibi, and Larry Davis. Sniper: Efficient multi-scale training. Advances in neural information processing systems, 31, 2018. 2 [42] Ke Sun, Bin Xiao, Dong Liu, and Jingdong Wang. Deep high-resolution representation learning for human pose estimation. In CVPR, 2019. [43] Jiashun Suo, Tianyi Wang, Xingzhou Zhang, Haiyang Chen, Wei Zhou, and Weisong Shi. Hit-uav: high-altitude infrared thermal dataset for unmanned aerial vehicle-based object detection. Scientific Data, 10(1):227, 2023. 5 [44] Leon Amadeus Varga, Benjamin Kiefer, Martin Messmer, and Andreas Zell. Seadronessee: maritime benchmark for detecting humans in open water. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 22602270, 2022. 2, 5 [45] Jinwang Wang, Wen Yang, Haowen Guo, Ruixiang Zhang, and Gui-Song Xia. Tiny object detection in aerial images. In 2020 25th international conference on pattern recognition (ICPR), pages 37913798. IEEE, 2021. 2 [46] Gui-Song Xia, Xiang Bai, Jian Ding, Zhen Zhu, Serge Belongie, Jiebo Luo, Mihai Datcu, Marcello Pelillo, and Liangpei Zhang. Dota: large-scale dataset for object detection in aerial images. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 39743983, 2018. 2 [47] Zinan Xiong, Chenxi Wang, Ying Li, Yan Luo, and Yu Cao. Swin-pose: Swin transformer based human pose estimation. CoRR, abs/2201.07384, 2022. 4 [48] Chang Xu, Jinwang Wang, Wen Yang, Huai Yu, Lei Yu, and Gui-Song Xia. Detecting tiny objects in aerial images: normalized wasserstein distance and new benchmark. ISPRS Journal of Photogrammetry and Remote Sensing, 190: 7993, 2022. 2, 3, [49] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for human pose estimation. Advances in neural information processing systems, 35:3857138584, 2022. 2, 4, 5, 6 [50] Dongfang Yang, Linhui Li, Keith Redmill, and Umit Ozguner. Top-view trajectories: pedestrian dataset of vehicle-crowd interaction from controlled experiments and crowded campus. In 2019 IEEE Intelligent Vehicles Symposium (IV), pages 899904. IEEE, 2019. 1, 2, 4 [51] Changqian Yu, Bin Xiao, Changxin Gao, Lu Yuan, Lei Zhang, Nong Sang, and Jingdong Wang. Lite-hrnet: lightweight high-resolution network. In CVPR, 2021. 4 [52] Hongyang Yu, Guorong Li, Weigang Zhang, Qingming Huang, Dawei Du, Qi Tian, and Nicu Sebe. The unmanned aerial vehicle benchmark: Object detection, tracking and International Journal of Computer Vision, 128: baseline. 11411159, 2020. 2 [53] Jingrui Yu, Tobias Scheck, Roman Seidel, Yukti Adya, Dipankar Nandi, and Gangolf Hirtz. Human pose estimation in monocular omnidirectional top-view images. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 64116420, 2023. 2 [54] Xuehui Yu, Yuqi Gong, Nan Jiang, Qixiang Ye, and Zhenjun Han. Scale match for tiny person detection. In Proceedings of the IEEE/CVF winter conference on applications of computer vision, pages 12571265, 2020. 5 [55] Pengyu Zhang, Jie Zhao, Dong Wang, Huchuan Lu, and Visible-thermal uav tracking: largeXiang Ruan. In Proceedings of scale benchmark and new baseline. the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 88868895, 2022. [56] Xiangqing Zhang, Yan Feng, Shun Zhang, Nan Wang, Guohua Lu, and Shaohui Mei. Robust aerial person detection with lightweight distillation network for edge deployment. IEEE Transactions on Geoscience and Remote Sensing, 62: 116, 2024. 2, 5 [57] Yan Zhang, Chang Xu, Wen Yang, Guangjun He, Huai Yu, Lei Yu, and Gui-Song Xia. Drone-based rgbt tiny person ISPRS Journal of Photogrammetry and Remote detection. Sensing, 204:6176, 2023. 5 [58] Yi Zhang, Wang Zeng, Sheng Jin, Chen Qian, Ping Luo, and Wentao Liu. When pedestrian detection meets multimodal learning: Generalist model and benchmark dataset. In European Conference on Computer Vision, pages 430448. Springer, 2024. 2 [59] Zixiao Zhang, Xiaoqiang Lu, Guojin Cao, Yuting Yang, Licheng Jiao, and Fang Liu. Vit-yolo: Transformer-based yolo for object detection. In Proceedings of the IEEE/CVF international conference on computer vision, pages 2799 2808, 2021. 2 [60] Yian Zhao, Wenyu Lv, Shangliang Xu, Jinman Wei, Guanzhong Wang, Qingqing Dang, Yi Liu, and Jie Chen. Detrs beat yolos on real-time object detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1696516974, 2024. [61] Zhuofan Zong, Guanglu Song, and Yu Liu. Detrs with colIn Proceedings of laborative hybrid assignments training. the IEEE/CVF international conference on computer vision, pages 67486758, 2023."
        }
    ],
    "affiliations": [
        "Universitat der Bundeswehr Munich"
    ]
}