{
    "paper_title": "GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer",
    "authors": [
        "Sayan Deb Sarkar",
        "Sinisa Stekovic",
        "Vincent Lepetit",
        "Iro Armeni"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Transferring appearance to 3D assets using different representations of the appearance object - such as images or text - has garnered interest due to its wide range of applications in industries like gaming, augmented reality, and digital content creation. However, state-of-the-art methods still fail when the geometry between the input and appearance objects is significantly different. A straightforward approach is to directly apply a 3D generative model, but we show that this ultimately fails to produce appealing results. Instead, we propose a principled approach inspired by universal guidance. Given a pretrained rectified flow model conditioned on image or text, our training-free method interacts with the sampling process by periodically adding guidance. This guidance can be modeled as a differentiable loss function, and we experiment with two different types of guidance including part-aware losses for appearance and self-similarity. Our experiments show that our approach successfully transfers texture and geometric details to the input 3D asset, outperforming baselines both qualitatively and quantitatively. We also show that traditional metrics are not suitable for evaluating the task due to their inability of focusing on local details and comparing dissimilar inputs, in absence of ground truth data. We thus evaluate appearance transfer quality with a GPT-based system objectively ranking outputs, ensuring robust and human-like assessment, as further confirmed by our user study. Beyond showcased scenarios, our method is general and could be extended to different types of diffusion models and guidance functions."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 7 1 ] . [ 1 6 3 1 6 1 . 0 1 5 2 : r GuideFlow3D: Optimization-Guided Rectified Flow For Appearance Transfer"
        },
        {
            "title": "Sayan Deb Sarkar\nStanford University",
            "content": "Vincent Lepetit ENPC, IP Paris Sinisa Stekovic ENPC, IP Paris"
        },
        {
            "title": "Abstract",
            "content": "Transferring appearance to 3D assets using different representations of the appearance objectsuch as images or texthas garnered interest due to its wide range of applications in industries like gaming, augmented reality, and digital content creation. However, state-of-the-art methods still fail when the geometry between the input and appearance objects is significantly different. straightforward approach is to directly apply 3D generative model, but we show that this ultimately fails to produce appealing results. Instead, we propose principled approach inspired by universal guidance. Given pretrained rectified flow model conditioned on image or text, our training-free method interacts with the sampling process by periodically adding guidance. This guidance can be modeled as differentiable loss function, and we experiment with two different types of guidance including part-aware losses for appearance and self-similarity. Our experiments show that our approach successfully transfers texture and geometric details to the input 3D asset, outperforming baselines both qualitatively and quantitatively. We also show that traditional metrics are not suitable for evaluating the task due to their inability of focusing on local details and comparing dissimilar inputs, in absence of ground truth data. We thus evaluate appearance transfer quality with GPT-based system objectively ranking outputs, ensuring robust and human-like assessment, as further confirmed by our user study. Beyond showcased scenarios, our method is general and could be extended to different types of diffusion models and guidance functions. Project Page: https://sayands.github.io/guideflow3d"
        },
        {
            "title": "Introduction",
            "content": "Transferring appearanceincluding both texture and fine geometric detailto 3D object is challenging and increasingly relevant problem across applications in gaming, augmented reality, and digital content creation. While style transfer has seen substantial progress in the 2D domain [20, 64, 65, 68], extending it to 3D introduces unique challenges due to the irregular, sparse, and diverse nature of 3D representations [26, 40, 22, 85]. Style cues may originate from 3D shapes, 2D images, or natural language, further increasing task complexity. An additional challenge lies in transferring the appearance across objects with substantial geometric differences, required in practical 3D design. Several existing methods address 3D generation by reformulating it as multi-view task, leveraging 2D diffusion models and conditioning on, e.g., rendered depth images to preserve input geometry [47, 36, 84]. However, these methods often produce geometrically inconsistent results due to discrepancies across different views. Recent advances in 3D generative modelingparticularly denoising-based approachesenable high-quality synthesis of shapes and appearances conditioned on inputs like text [54, 5, 72]. Still, these models are constrained by conditioning signals and data distributions used during training. As result, direct application to appearance transfer yields poor generalization 39th Conference on Neural Information Processing Systems (NeurIPS 2025). Figure 1: GuideFlow3D is method for 3D appearance transfer robust to strong geometric variations between objects. Given an input 3D mesh, e.g., designed using simple 3D primitives, it transfers the texture and fine geometric details of an appearance object (e.g., the rounded edges of the table on the top left and the base and mattress distinction of the bed on the top right) but preserves the geometric form of the input mesh. Its flexibility across appearance modalities like meshes or text makes GuideFlow3D efficient for generating diverse 3D assets. and limited control, particularly when the geometries between the input and appearance objects differ significantly, as we show in our experiments. Note that, in the appearance transfer setting, the input object dictates the global geometry and the appearance one the texture and finer geometric details. In this work, we present GuideFlow3D, training-free framework for 3D appearance transfer that adaptively steers pretrained generative model at inference time. Our key insight is that the inductive bias of pretrained generative model can be repurposed for new task through guided rectified flow sampling, an inference-time mechanism that interleaves flow updates with latent-space optimization. This strategy extends the concept of universal guidance to 3D generation and enables conditioning on objectives for which the base model was not originally trained. Our method builds upon rectified flow models and structured latent representations [72], and introduces differentiable guidance functions that modulate the generation process without requiring retraining. To address prior work challenges in transferring appearance to an input 3D object while maintaining its global geometry, we propose two novel forms of guidance for appearance transfer to 3D shapes that significantly improve robustness to geometric variations between given input and appearance objects: (i) part-aware appearance loss, which co-segments the input and appearance 3D shapes into semantically meaningful parts and enforces localized texture and geometry correspondence between the shapes; and (ii) self-similarity loss, which preserves intrinsic structure within regions of the input during transfer. Our method supports various representations for the appearance objectincluding mesh-image pairs or text. When mesh is available, the appearance loss is applied; otherwise, the self-similarity loss guides the generation using either an image or text. In this way, users can control whether appearance affects both geometry and texture (with mesh) or texture alone (with image or text). When only an image is provided, mesh can be generated via existing methods such as [72]. This flexibility allows GuideFlow3D to unify multiple appearance modalities under single framework, bridging geometric and perceptual style transfer in 3D. Quantitatively evaluating 3D appearance transfer remains challenging due to the absence of ground truth input shapes that have the transferred appearance and the difficulty of comparing across dissimilar geometries. While metrics like DINOv2 [48], CLIP [29], and DreamSim [25] are typically used to assess perceptual similarity and others like PSNR [31], SSIM [69], LPIPS [79], and FID [15], for reconstruction quality, they require ground truth data that does not exist in our setting. To address this, we use GPT-based evaluation scheme [15, 71] that performs pairwise output ranking in human-aligned manner. We further validate the consistency of these rankings through user study, confirming that GPT-based judgments strongly correlate with human evaluation on this task. Our experiments demonstrate that GuideFlow3D consistently outperforms baselines, generating visually appealing results that accurately reflect the intended style, while also capturing fine-grained geometry. Our contributions are as follows: 2 We introduce GuideFlow3D, novel framework for 3D appearance transfer that applies universal, differentiable guidance to pretrained rectified flow model, enforcing the generation process to respect given constraints for which it was not originally trained. We propose part-aware and self-similarity loss functions as effective forms of guidance, enabling localized and structure-preserving style transfer across diverse 3D assets. Our method is training-free, generalizable to different appearance representations and, it could be, in principle, extended to variety of 3D generative models. By decoupling style control from the generation process and enabling inference-time conditioning on novel objectives, GuideFlow3D opens new directions in controllable 3D generation and asset stylization. We make our code and benchmark publicly available on our project website."
        },
        {
            "title": "2 Related Work",
            "content": "3D Generative Models. common method for learning 3D shape generation uses autoencoders, where an encoder maps point cloud to an embedding, which decoder reconstructs into 3D shape. Previous approaches rely on normalized point flows [74] or gradient fields [9]. GAN formulations are common in 3D shape generation [70, 50, 10, 27]. Luo et al. [43] and more recent approaches [13, 83, 11, 46] formulate 3D asset generation as probabilistic diffusion processes. Some works take advantage of 2D generative models to create 3D assets [54, 63, 37]. Others [12, 67, 61] propose auto-regressive models for generating 3D meshes. As we discuss further in Sec. 3, Trellis [72] uses rectified flow formulation to learn the generation of structured 3D latents that can effectively be decoded in different 3D representations. Due to its ability to capture fine details in both geometry and texture, we develop our appearance transfer framework around this model. 2D Style Transfer. Transferring style between images has been extensively studied in computer vision. Early work [28] formulates it as an optimization problem over pretrained CNN features, later replaced by efficient feed-forward networks [32, 66]. With multimodal embeddings, StyleCLIP [49] enables text-driven style manipulation. Diffusion-based approaches [81, 68, 73, 55] now dominate the field, offering fine-grained control through score-based modeling. Cross-Image Attention [2] performs zero-shot appearance transfer by injecting cross-image attention into the diffusion process, while MambaST [8] leverages state-space models for efficient and expressive structurestyle fusion. [64] trains generator on specific contentstyle example, where structure and appearance constraints are derived from pretrained vision transformer features. Such instance-specific formulations highlight the challenge of achieving generalizable appearance transfer across diverse geometries limitation our approach directly addresses by introducing guided generative mechanism that preserves geometric consistency while adapting fine-grained style cues. 3D Style Transfer. Style transfer has been explored across diverse 3D representations, including point clouds [75], meshes [7], implicit fields [30, 14, 24, 39], and Gaussian splatting [40]. 3DStyleNet [75] separates geometry and texture through dual networks, while Mesh2Tex [7] maps style images into learned texture manifold. Recent methods such as StyleRF [39] and StyleGaussian [40] represent major progress but operate on implicit or point-based representations, producing render-only outputs that lack explicit mesh control and often require multi-view optimization. StyleGaussian [40] stylizes only color while keeping geometry fixed, with performance degrading as Gaussian count increases. In contrast, GuideFlow3D performs training-free stylization in structured latent space, enabling direct mesh editing, part-aware control, and constant inference cost. In diffusion-based 3D stylization, ControlNet-style adapters [77] have become standard for injecting additional cues such as images, edges, depth, or pose into pretrained models. For example, TEXTure [56] employs score distillation for text-guided texturing while EASI-Tex [52] extends this with depth-aware conditional diffusion. TriTex [15] learns semantic texture field for single-mesh appearance transfer. While effective, these methods are tied to specific training setups and conditioning modalities, limiting their generality. Building on the broader line of diffusion guidance research [21, 6, 76], GuideFlow3D combines the concept of universal guidance and rectified flow model from Trellis [72]. By incorporating geometric priors from PartField [41], our method achieves robust and geometry-aware appearance transfer without the need for task-specific finetuning. 3 Figure 2: GuideFlow3D introduces guided rectified flow for appearance transfer between input object Oq and an appearance object. We extend the denoising process of structured latents zq, conditioned by c, by introducing an objective function that enforces strong geometric and semantic priors during the process. We show denoised structured latents at different stages of the process, along with corresponding meshes decoded using pretrained decoder D3D. The output 3D asset displays robustness to strong geometric variations between input and appearance objects."
        },
        {
            "title": "3 Preliminaries\nStructured Latent From 3D Assets. The geometry and appearance of a 3D object mesh O can\nbe encoded using a structured latent (SLAT) representation z [72], composed of local latent codes\nanchored on a 3D voxel grid. This representation is defined as:",
            "content": "i=1, zi RC, = {(zi, pi)}L pi {0, 1, . . . , 1}3 , (1) where pi denotes the position of an active voxel intersecting the surface of O, and zi is the latent vector associated with that voxel. is the spatial resolution of the grid and is the number of active voxels, intersecting with the objects surface. Since the number of active voxels is significantly lesser than that of full grid, this representation is computationally less demanding, allowing to work at higher resolution. The set of active voxel positions pi outlines the coarse structure of the object, while the corresponding latents zi capture fine-grained geometric and visual features, as shown in [72]. This representation thus effectively captures both the global shape and detailed surface characteristics of the object. In practice, following Trellis [72], multi-view images of are rendered, and DinoV2 [48, 18] features are extracted for each image. After back-projection and aggregation of these features per voxel, shallow transformer encoder [57, 35] compresses them into SLAT. Structured latents can be decoded into 3D Gaussians [33], Radiance Fields [45], or Meshes [60] using decoders D3D that share common architecture, differing only in output layers and trained with representation-specific losses. Please note that, in our case, voxel positions pi remain fixed to preserve the coarse input geometry, while only the latent codes zi are steered during generation. Universal Diffusion Guidance. We briefly introduce the concept of universal guidance from [6]. Diffusion models are generative models that reverse the process of adding noise, typically Gaussian noise, to the data over time steps. noisy version of the input x0 at time step is defined as: xt = (cid:112)βt x0 + (cid:112)1 βtN (0, I) , (2) where βt represents the noise scale at time step t, determined by scheduling mechanism. Hence, xt is weighted sum of the original image and Gaussian noise. As increases, βt decreases, and xt approaches pure noise as . With some generalization, trained diffusion network fθ(xt) reverses the process by predicting the added noise. The denoising process can then be defined as: ˆxt1 = ˆxt + fθ(ˆxt) (3) While diffusion models can be typically guided by adding condition to the diffusion model fθ(xt, c), e.g., can be text or an image, this limits applications to more general conditions. In universal guidance from [6], any differentiable guidance function L(xt, ) can be used to condition diffusion. Then, the update in the reverse step is defined as: Such an approach enables general conditioning without altering the diffusion training process. ˆxt1 = ˆxt + fθ(ˆxt, c) + ˆxtL(ˆxt, ) (4)"
        },
        {
            "title": "4 GuideFlow3D",
            "content": "We present an overview of our approach for appearance transfer in Fig. 2. Given an input 3D object mesh Oq and an appearance object Oa, we would like to modify the appearance of Oq based on Oa, 4 (a) (b) Figure 3: GuideFlow3D with different guidance objectives. (a) When textured mesh is available for appearance object Oa, we use our co-segmentation based objective Lappearance to guide appearance transfer. It encourages consistency between structured latents za and noisy latents zq. In such case, we use an image of object Oa to condition the generative model R. (b) When textured mesh is not available, we use our geometric clustering based objective Lstructure for guidance. It encourages intra-cluster similarity and inter-cluster disparity when denoising zq. We use text or image to condition in such case. For both cases, we use decoder D3D to obtain the output 3D asset. while respecting the geometric structure of Oq. Oa can be represented as an image-mesh pair or text. We assume here that the appearance object provides the mesh alongside the image. In practice, the mesh can be generated by running [72] on the image. We first discuss how we use structured latents from [72] for our appearance transfer problem. Then, we show how we can guide the rectified flow sampling using our objectives to transfer appearance while accounting for structural consistency."
        },
        {
            "title": "4.1 Structured Latents for Appearance Transfer",
            "content": "We leverage the latent structure from [72] to represent object Oq, but in general, different representations could be considered. We initialize structured latents zq = {(zq i=1 for the input object Oq, where pq are initialized by sampling from normal distribution. In practice, pq are learnable in our appearance transfer formulation. This effectively enforces global geometric consistency of the output with Oq. Following, we define two appearance transfer objectives  (Fig. 3)  . )}Lq is not trainable parameter but the zq are the positional indices of the active voxels, as in Eq. (1). The zq , pq Appearance-based objective. When the appearance object is an image-mesh pair, we can extract latents za for the mesh of Oa using sparse VAE encoder [72]. Ideally, we seek an oracle mapping that aligns each input latent zq with semantically and geometrically corresponding appearance latent in za: e.g., in Fig. 3 (a), we would like the zq for voxels of the back leg of an input chair to be mapped to the za for the voxels of the back leg of the appearance chair. Since such correspondences are not readily available, we approximate them by assigning each zq to its nearest neighbor in za based on feature similarity. Formally, our objective for appearance transfer is defined as: Lappearance ="
        },
        {
            "title": "1\nLq",
            "content": "Lq (cid:88) i=1 zq za m2 2 , (5) where is the index of the corresponding feature in structured latents za for zq . While it could be possible to extract correspondences based on these latents, they are not trained to be part-aware, and this would lead to false assignments, as shown in Sec. C. Instead, we establish correspondences based on geometric co-segmentation clustering using part-based feature fields from [41]. In practice, we compute PartField [41] features per voxel and run k-means clustering, thus, relying on approximate matching rather than one-to-one correspondences. Self-similarity objective. In the second scenario, we assume that the mesh is not available, hence Oa is represented as an image or text. We observe that structure-based self-similarity descriptors have been shown to effectively capture structural information while being invariant to appearance [59, 4, 5 34]. We, thus, rely on loss that encourages similarity between same object parts while promoting inter-part separability in the feature space. As the appearance mesh is not available, this loss aligns structural features with the part-aware semantics implied by the textual description or image for the source. To do so, we first perform geometric clustering that assigns each voxel pq to cluster Cq(i). Given Oq, for voxels pi and pj zq, we compute pairwise cosine similarity between geometric features simij and define part-aware contrastive loss based on self-similarity. Specifically, for each voxel pi, the set of positive samples is defined as all = such that Cq(i) = Cq(j), and all other voxels serve as negatives. The objective can then be defined by: Lstructure ="
        },
        {
            "title": "1\nLq",
            "content": "Lq (cid:88) i=1 log (cid:80) exp(simij) jCq(i),j=i, (cid:80) jC q(i) exp(simij) (6) where Lq denotes the set of all voxels and is the complement set of C. Due to its inherent contrastive nature, Lstructure promotes local consistency without homogenizing appearance globally."
        },
        {
            "title": "4.2 Guiding Structured Latents for Appearance Transfer",
            "content": "Our objective is defined using Bayesian formulation. Given the input 3D object mesh Oq and appearance object Oa, we would like to maximize the posterior (OqOa): log (OqOa) = log (Oq) + log (OaOq) , (7) where prior log (Oq) models the geometric prior of Oq while likelihood log (OaOq) models the appearance. The objectives defined in the previous section are not enough to perform appearance transfer. While Lappearance models the posterior log (OqOa), solely optimizing it proves insufficient. It tends to shift the latent distribution away from the one modeled by the generative network, often leading to implausible or incoherent results across diverse shapes (Fig. 4 (a)). Meanwhile, Lstructure only models (Oq) of our Bayesian formulation. To address these issues and better align with the generative prior, we introduce appearance transfer as guidance in rectified flow. (a) (b) (c) Figure 4: Effect of different modules. (a) Using only optimization with our objective functions to transfer appearance is insufficient as it does not enforce realistic distribution over the structured latent space. (b) The rectified flow model from [72] fails to transfer appearance when appearance and input objects have significantly different geometries. (c) We obtain appealing 3D assets when using rectified flow guidance of our GuideFlow3D. Rectified Flow Guidance. Rectified flow models [42, 3, 38] define linear forward process, z(t) = (1 t)z0 + tϵ, which interpolates between data samples z0 and noise ϵ over time t. The corresponding backward process is modeled as time-dependent vector field v(z, t) = tz, which transports noisy samples back toward the data distribution. This vector field can be approximated by neural network vθ, trained via the Conditional Flow Matching (CFM) objective [38]: LCFM(θ) = Et,z0,ϵvθ(z, t) (ϵ z0)2 2 . (8) While rectified flow enables sample generation that remains close to the learned data distribution, using it alone is insufficient for semantic control, particularly when transferring fine-grained appearance or structure. As shown in Fig. 4 (b), applying rectified flow without additional constraints often yields outputs that are within the learned distribution but fail to reflect the style texture or semantic structure, especially across diverse object categories and shapes. To enforce semantic control while preserving realism, we interleave latent-space optimization with sampling steps from rectified flow model vθ(z, c), conditioned on global signal c. Based on [72], the signal can either be an image or text. Starting from an initial latent zT (0, I), reverse-time flow is defined as: ˆzt = zt + vθ(zt, c) (9) 6 where [0, 1] is linearly spaced. We adapt the equation to include guidance: ˆzt = zt + vθ(zt, c) + ztL(z c) (10) where is general guidance objective. The optimized latent ˆz is then used for the next flow step. Such optimization-flow scheme allows for flexible conditioning via differentiable objectives without retraining the model, extending universal guidance [6] to any rectified flow model R. The final output can be decoded into the 3D representation of choice based on 3D decoders from [72]. The guidance can be fit into our Bayesian formulation, where conditioned rectified flow models both prior (O) and likelihood (cO), and the guidance terms become additional factors in the prior and likelihood terms. We find that this interleaved scheme preserves global realism while enabling fine-grained control over both appearance and structure, as shown in Fig. 4 (c). Qualitative in-the-wild transfers  (Fig. 6)  further illustrate robustness under large geometric discrepancies, e.g., animalfurniture. Condition-Specific Guidance. Our method supports flexible conditioning depending on the form of the input. When reference image is provided, we may employ either appearanceor structure-based guidance. For appearance transfer, we assume access to the appearance mesh Oa from which structured latents za are extracted, and we optimize Lappearance to transfer appearance. Alternatively, even in the absence of mesh, the same reference image can be used to compute geometric features, enabling structure-aware transfer via Lstructure. When the condition is text prompt, we rely exclusively on Lstructure, using semantic clustering of geometric features to induce part-aware correspondences. This setup enables our framework to seamlessly handle both visual and textual inputs while maintaining geometric fidelity and semantic consistency."
        },
        {
            "title": "5 Experiments",
            "content": "Dataset. Since there are no publicly available datasets for our task of transferring appearance across different shapes, we create benchmark for evaluation. First, for input mesh, we generate synthetic objects using procedural models from [62]. Second, for the appearance mesh and images, we leverage the ABO dataset [16], with the text captions provided by [72]. ABO contains 8K artist-designed 3D models from Amazon, featuring complex geometries and high-resolution materials across 63 categories, mainly focused on furniture and interior decor. We use 5 categories: bed, cabinet, chair, table, and sofa, randomly sampling 100 objects from each dataset. Here onwards, we refer to the dataset of procedurally generated objects as simple and the ABO subset as complex. Using simple and complex meshes for the different categories, we create 250 input-appearance object pairs for each of our 4 experimental setups: (i) simple-complex intra-category, (ii) simple-complex inter-category, (iii) complex-complex intra-category, and, (iv) complex-complex inter-category. Evaluation Metrics. In absence of ground truth transferred texture for the input 3D mesh, encoderbased metrics are not representative of appearance transfer for 3D shapes of very different geometries (see more in Appendix Sec. F). LLMs have been previously shown to be human aligned evaluator for generation tasks capturing structural preservation and content alignment [71, 80, 51]. We use GPT-5 to carefully develop more nuanced human-aligned ranking system. This system evaluates results across six criteriaStyle Fidelity, Structure Clarity, Style Integration, Detail Quality, Shape Adaptation, and Overall Qualitycapturing both semantic intent and the perceptual quality of texture transfer in 3D. Following prior work [15, 71], we use multi-view renders of input, appearance, and output for all methods and prompt GPT-5 to give us ranking (details in Appendix Sec. H). We show that GPT-5 is aligned with human preferences on this task via user study in Appendix Sec. G. Baselines. We compare our GuideFlow3D for appearance transfer against multiple baselines: (1) UV Nearest Neighbor: We find the nearest neighbors in Euclidean space for each point in the input mesh Oq to the mesh of the appearance object Oa, and map the coordinates of the UV texture map accordingly; (2) Image-to-3D: We render the input mesh from multiple views, apply state-of-the-art 2D style transfer models [2, 8], and lift the stylized renderings to 3D via the image-conditioned Trellis model; (3) EasiTex [52]: is conditional diffusion-based method that uses edge-aware conditioning and ControlNet [77] to texture an existing 3D mesh from single RGB image, (4) Trellis [72]: serves as our baseline without guidance, enabling detailed local texture transfer using structured latents; and (5) Text-to-3D: For text-based appearance, we first generate reference image using Stable Diffusion [53] and then apply the same Image-to-3D pipeline described above."
        },
        {
            "title": "5.1 Appearance Transfer",
            "content": "Our first goal is to transfer appearance from an object given as image and textured mesh, or as text, to an untextured 3D object, within the same semantic category. This setting allows us to systematically assess how well each method preserves stylistic intent while adapting to varied geometries within each semantic category. Tab. 1 shows the results on the simple-complex intra-category set under both image-mesh (Lappearance) and text conditioning (Lstructure). As illustrated in Fig. 5 (top row), transferring the appearance from chair to another chair reveals clear differences in quality. MambaST [8] produces textures that are globally consistent due to its state-space backbone, ensuring smooth overall color and material coherence. However, residual gray tones from the input mesh persist, causing uneven blending and inconsistent local texture alignment. Cross Image Attention [2] effectively transfers local appearance patterns through image-conditioned attention, yet fails to maintain consistent mapping when applied to 3D surfaces, introducing artifacts during transfer. EASI-Tex [52] performs competitively with its ControlNet-based edge and depth conditioning, yet struggles under large geometric deviations due to limited generalization beyond its training setup. Trellis [72] improves upon this with its probabilistic structured latent generation but struggles to integrate appearance seamlessly across large geometric variations. Although it associates some textures reasonably (e.g., the chair seat transferred between the two), it often results in loss of fine-grained details and patchy surfaces. In contrast, GuideFlow3D achieves strong improvements in both appearance transfer and structural clarity preservation in image and text settings. The transferred appearance aligns well with object structure, as seen in the smooth material transitions and consistent fabric textures on the chair. This demonstrates the ability of our approach to effectively translate both visual and semantic cues into high-fidelity, structure-aware textures across diverse 3D objects (more results in Appendix Sec. I). Table 1: Quantitative comparison based on our GPT-based ranking metrics that rank quality of appearance transfer based on different criteria. Results are on the simple-complex intra-category set. Methods Fidelity Clarity Integration Quality Adaptation Overall Ranking metrics w/ Image Condition (Lappearance) UV Nearest Neighbor MambaST [8] Cross Image Attention [2] EasiTex [52] Trellis [72] GuideFlow3D (Ours) w/ Text Condition (Lstructure) UV Nearest Neighbor SDXL + Cross Image Attention Trellis [72] GuideFlow3D (Ours) 4.12 4.94 3.56 3.18 2.51 1.89 3.12 2.88 2.01 1.54 3.84 3.55 3.48 4.30 2.58 2.41 3.21 2.52 1.89 1. 4.30 4.56 3.32 4.08 2.53 2.07 3.82 3.25 2.67 2.01 4.10 4.90 3.83 3.17 2.69 2.23 3.61 3.38 2.75 2.15 4.43 4.42 3.47 4.18 2.61 2.28 3.43 3.29 2.55 2. 4.33 4.87 3.59 3.81 2.62 2.12 3.64 2.98 2.39 1.95 Furthermore, to understand cross-category generalization, we evaluate more challenging setting where the structure and appearance inputs come from different object categories, e.g., transferring the appearance of cabinet to bed (Fig. 5 , bottom row). Results under image conditioning for both simple-complex and complex-complex sets are in Tab. 2. We show results for text conditioning in the Appendix (Sec. A). This scenario tests not only fidelity to style but also the ability to adapt and maintain perceptual coherence under stronger geometric variations. GuideFlow3D maintains good performance across all metrics despite the added difficulty. In Fig. 5, compared to Cross Image Attention [2] and EASI-Tex [52], which degrade under large shape discrepancies, our method preserves both global coherence and fine structural fidelity. Trellis [72] not only fails to transfer the cabinets appearance effectively but also introduces major geometry change by closing hole on the side and texturing the altered region. Our robust performance highlights the ability to disentangle style from structure, enabling us to transfer appearance features even when spatial priors do not align. This demonstrates that our method is applicable to variety of real-world applications."
        },
        {
            "title": "5.2 Application: In-the-wild Appearance Transfer",
            "content": "To assess the generalization of GuideFlow3D beyond furniture-to-furniture, we explore appearance transfer in-the-wild using structurally diverse 3D assets from Objaverse-XL [19] and ABO [16], 8 Figure 5: Qualitative Comparisons showing quality of appearance transfer. Top and bottom rows show intra-class (chair to chair) and inter-class (cabinet to bunk bed) results respectively. In both examples, MambaST [8] blends the textures from both input and appearance objects, giving grey hue to the final result. EasiTex [52] generates non-smooth repetitions of texture and fails to generate textures for the entire object (e.g., the handles of the chair or the bottom part of the bunk bed). Cross Image Attention [2] performs better but omits texture details (cabinets wood texture) and fails to preserve good local geometry. Trellis [72] preserves better the texture of the appearance object and does better on matching it to the input object, however, it fails at providing uniform texture on the arms of the chair and does not preserve the overall geometry of the bunk bed by closing the side hole. Ours performs the best by adhering to the appearance objects texture and matching it on the input object, while preserving the overall geometry of the input object. Table 2: Quantitative comparisons ranking of our GuideFlow3D against baselines for different experimental settings. Results are shown with image conditioning (Lappearance). Methods Fidelity Clarity Adaptation Fidelity Clarity Adaptation Ranking metrics Intra-Category Inter-Category Simple-Complex UV Nearest Neighbor MambaST [8] Cross Image Attention [2] EasiTex [52] Trellis [72] GuideFlow3D (Ours) Complex-Complex UV Nearest Neighbor Cross Image Attention MambaST EasiTex Trellis [72] GuideFlow3D (Ours) 4.12 4.94 3.56 3.18 2.51 1.89 3.31 4.00 4.63 3.29 2.82 2.21 3.84 3.55 3.48 4.30 2.58 2.41 3.11 4.13 3.88 4.21 2.73 2.31 4.43 4.42 3.47 4.18 2.61 2.28 3.41 4.07 3.42 4.23 2.81 2. 4.06 4.87 3.54 3.25 2.64 1.99 3.54 3.79 4.54 3.19 2.99 2.24 3.51 3.57 3.55 4.21 2.85 2.75 2.99 3.99 3.33 4.26 3.15 2.69 4.17 4.38 3.52 4.10 2.76 2.45 3.49 3.91 3.92 4.21 3.09 2. where either dataset can provide the input or appearance object. This setting poses significant robustness challenges due to extreme variation in object categories, mesh quality, and geometric complexity. In this setting, appearance objects are represented as mesh-image pairs, allowing us to use the part-aware appearance loss Lappearance, as guidance. As illustrated in Fig. 6, our approach successfully transfers texture and fine geometric details while preserving the original structure. It transfers the appearance between objects in various semantic categories, such as animals, vehicles, and furniture. Despite the lack of semantic alignment or category overlap, we are able to apply texture in part-aware and structurally consistent manner, highlighting the strength of co-segmentation and geometric clustering-based guidance mechanism. The resulting outputs show high visual fidelity with textures that adhere naturally to the surface topology of the target mesh. Compared to Trellis [72], which often loses part-aware texture continuity under large semantic shifts, GuideFlow3D maintains structurally aligned and visually coherent material transfer across categories. Notably, this application does not require additional assumptions or adaptations, and generalizes to unseen shape categories and object styles, reinforcing the adaptability of GuideFlow3D. The ability to generate visually coherent, stylized outputs has relevance for 3D asset stylization in AR/VR and digital twin generation. 9 Figure 6: Qualitative Comparisons showing in-the-wild appearance transfer. Our GuideFlow3D robustly transfers appearance between diverse semantic categories. Patterns from the body of the giraffe are transferred to the seat, arms and backrest of the chair, while the appearance of giraffe legs is transferred to chair legs. Across other categories we observe interesting appearance transfers: chair legs to bike wheels, bed legs to golden retriever legs, and cabinet handles to airplane propeller. The smaller insets in orange boxes show Trellis [72] results, which exhibit weaker structural grounding and inconsistent material mapping, demonstrating GuideFlow3Ds advantage in preserving geometryaware texture alignment even under large semantic shifts."
        },
        {
            "title": "6 Conclusion",
            "content": "Our GuideFlow3D shows promising results in the field of 3D appearance transfer, possibly enabling exciting new applications. Given that our approach can generate realistic 3D assets from simplistic CAD designs, it could play role in democratizing and simplifying 3D content creation. Tools for 3D content creation would become more accessible to artists which would significantly reflect on rapid development, prototyping, and creativity in XR and gaming platforms for example. Limitations and Future Work. Our method is optimization based, thus it is not meant for real-time use cases. Our runtime is 96s on an NVIDIA 4090 GPU, compared to 78s for baseline [72]. In the future, we could train self-supervised feed-forward models for faster inference. Our implementation depends on the performance of [72] and [41], and failures of these models would impact our performance. Our approach assumes noiseless meshes which is limiting factor for some future applications scenarios. Developing novel guidance objectives for new applications is an interesting future research direction to address such scenarios. The scope of our main experiments includes furniture objects from [16] and [62], but in practice, our method can be applied to variety of object categories, as shown in Sec. 5.2, and downstream tasks such as scene editing (see Appendix Sec. D). Ethical Considerations. Next to the exciting possibilities, there are considerable risks that should be addressed including manipulation and Deepfakes for spreading misinformation, concerns regarding intellectual property, and bias amplifications. Ethical usage of our method includes aspects of disclosing when 3D content is generated using AI, respecting and attributing source content licenses, and building systems for understanding biases are some of the ways for tackling these issues."
        },
        {
            "title": "7 Acknowledgements",
            "content": "We thank Nicolas Dufour and Arijit Ghosh from Imagine Labs for helpful discussions on universal guidance, and Liyuan Zhu and Jianhao Zheng from Gradient Spaces Research Group for help with conducting the user study."
        },
        {
            "title": "References",
            "content": "[1] Stefan Ainetter, Sinisa Stekovic, Friedrich Fraundorfer, and Vincent Lepetit. Automatically annotating indoor images with cad models via rgb-d scans. In Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision, pages 31563164, 2023. 18 [2] Yuval Alaluf, Daniel Garibi, Or Patashnik, Hadar Averbuch-Elor, and Daniel Cohen-Or. Cross-image attention for zero-shot appearance transfer, 2023. 3, 7, 8, 9, 16, 22 [3] Michael Samuel Albergo and Eric Vanden-Eijnden. Building normalizing flows with stochastic interpolants. In ICLR, 2023. 6 [4] Shir Amir, Yossi Gandelsman, Shai Bagon, and Tali Dekel. Deep vit features as dense visual descriptors. arXiv preprint arXiv:2112.05814, 2021. 5 [5] Sudarshan Babu, Richard Liu, Avery Zhou, Michael Maire, Greg Shakhnarovich, and Rana Hanocka. Hyperfields: Towards zero-shot generation of nerfs from text. In Forty-first International Conference on Machine Learning. 1 [6] Arpit Bansal, Hong-Min Chu, Avi Schwarzschild, Soumyadip Sengupta, Micah Goldblum, Jonas Geiping, and Tom Goldstein. Universal guidance for diffusion models. 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 843852, 2023. 3, 4, 7 [7] Alexey Bokhovkin, Shubham Tulsiani, and Angela Dai. Mesh2tex: Generating mesh textures from image queries. In International Conference on Computer Vision, 2023. 3 [8] Filippo Botti, Alex Ergasti, Leonardo Rossi, Tomaso Fontanini, Claudio Ferrari, Massimo Bertozzi, In 2025 IEEE/CVF doi: and Andrea Prati. Mamba-st: State space model for efficient style transfer. Winter Conference on Applications of Computer Vision (WACV), pages 77977806, 2025. 10.1109/WACV61041.2025.00757. 3, 7, 8, 9, 22 [9] Ruojin Cai, Guandao Yang, Hadar Averbuch-Elor, Zekun Hao, Serge Belongie, Noah Snavely, and Bharath Hariharan. Learning gradient fields for shape generation. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part III 16, pages 364381. Springer, 2020. 3 [10] Eric Chan, Connor Lin, Matthew Chan, Koki Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo, Leonidas Guibas, Jonathan Tremblay, Sameh Khamis, et al. Efficient geometry-aware 3d generative adversarial networks. In Conference on Computer Vision and Pattern Recognition, 2022. 3 [11] Hansheng Chen, Jiatao Gu, Anpei Chen, Wei Tian, Zhuowen Tu, Lingjie Liu, and Hao Su. Single-stage diffusion nerf: unified approach to 3d generation and reconstruction. In International Conference on Computer Vision, 2023. 3 [12] Yiwen Chen, Tong He, Di Huang, Weicai Ye, Sijin Chen, Jiaxiang Tang, Xin Chen, Zhongang Cai, Lei Yang, Gang Yu, et al. Meshanything: Artist-created mesh generation with autoregressive transformers. International Conference for Learning Representations, 2025. 3 [13] Yen-Chi Cheng, Hsin-Ying Lee, Sergey Tulyakov, Alexander Schwing, and Liang-Yan Gui. Sdfusion: Multimodal 3d shape completion, reconstruction, and generation. In Conference on Computer Vision and Pattern Recognition, 2023. [14] Pei-Ze Chiang, Meng-Shiun Tsai, Hung-Yu Tseng, Wei-Sheng Lai, and Wei-Chen Chiu. Stylizing 3d scene via implicit representation and hypernetwork. In IEEE Winter Conference on Applications of Computer Vision, 2022. 3 [15] Dana Cohen-Bar, Daniel Cohen-Or, Gal Chechik, and Yoni Kasten. Tritex: Learning texture from single mesh via triplane semantic features. In CVPR, 2025. 2, 3, 7, 19 [16] Jasmine Collins, Shubham Goel, Kenan Deng, Achleshwar Luthra, Leon Xu, Erhan Gundogdu, Xi Zhang, Tomas Yago Vicente, Thomas Dideriksen, Himanshu Arora, Matthieu Guillaumin, and Jitendra Malik. Abo: Dataset and benchmarks for real-world 3d object understanding. CVPR, 2022. 7, 8, 10, 19 11 [17] Angela Dai, Angel X. Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner. Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proc. Computer Vision and Pattern Recognition (CVPR), IEEE, 2017. 18, [18] Timothée Darcet, Maxime Oquab, Julien Mairal, and Piotr Bojanowski. Vision transformers need registers, 2023. 4 [19] Matt Deitke, Ruoshi Liu, Matthew Wallingford, Huong Ngo, Oscar Michel, Aditya Kusupati, Alan Fan, Christian Laforte, Vikram Voleti, Samir Yitzhak Gadre, Eli VanderBilt, Aniruddha Kembhavi, Carl Vondrick, Georgia Gkioxari, Kiana Ehsani, Ludwig Schmidt, and Ali Farhadi. Objaverse-xl: universe of 10m+ 3d objects. arXiv preprint arXiv:2307.05663, 2023. 8 [20] Yingying Deng, Fan Tang, Weiming Dong, Chongyang Ma, Xingjia Pan, Lei Wang, and Changsheng Xu. Stytr2: Image style transfer with transformers. In Conference on Computer Vision and Pattern Recognition, 2022. 1 [21] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in Neural Information Processing Systems, 34, 2021. [22] Jiahua Dong and Yu-Xiong Wang. Vica-nerf: View-consistency-aware 3d editing of neural radiance fields. In NeurIPS, 2023. 1 [23] Shivam Duggal, Yushi Hu, Oscar Michel, Aniruddha Kembhavi, William T. Freeman, Noah A. Smith, Ranjay Krishna, Antonio Torralba, Ali Farhadi, and Wei-Chiu Ma. Eval3d: Interpretable and fine-grained evaluation for 3d generation. arXiv preprint, 2024. 21 [24] Zhiwen Fan, Yifan Jiang, Peihao Wang, Xinyu Gong, Dejia Xu, and Zhangyang Wang. Unified implicit neural stylization. In European Conference on Computer Vision, 2022. 3 [25] Stephanie Fu, Netanel Tamir, Shobhita Sundaram, Lucy Chai, Richard Zhang, Tali Dekel, and Phillip Isola. Dreamsim: Learning new dimensions of human visual similarity using synthetic data. In Advances in Neural Information Processing Systems, volume 36, pages 5074250768, 2023. 2, [26] Haruo Fujiwara, Yusuke Mukuta, and Tatsuya Harada. Style-nerf2nerf: 3d style transfer from style-aligned multi-view images. In SIGGRAPH Asia 2024 Conference Papers, 2024. 1 [27] Jun Gao, Tianchang Shen, Zian Wang, Wenzheng Chen, Kangxue Yin, Daiqing Li, Or Litany, Zan Gojcic, and Sanja Fidler. Get3d: generative model of high quality 3d textured shapes learned from images. Advances in Neural Information Processing Systems, 2022. 3 [28] Leon Gatys, Alexander Ecker, and Matthias Bethge. neural algorithm of artistic style. arXiv preprint arXiv:1508.06576, 2015. 3 [29] Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. CLIPScore: reference-free evaluation metric for image captioning. In EMNLP, 2021. 2, 20 [30] Hsin-Ping Huang, Hung-Yu Tseng, Saurabh Saini, Maneesh Singh, and Ming-Hsuan Yang. Learning to stylize novel views. In International Conference on Computer Vision, 2021. 3 [31] Sadeep Jayasumana, Srikumar Ramalingam, Andreas Veit, Daniel Glasner, Ayan Chakrabarti, and Sanjiv Kumar. Rethinking fid: Towards better evaluation metric for image generation. 2024 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 93079315, 2023. 2 [32] Justin Johnson, Alexandre Alahi, and Li Fei-Fei. Perceptual losses for real-time style transfer and super-resolution. In European Conference on Computer Vision, 2016. [33] Bernhard Kerbl, Georgios Kopanas, Thomas Leimkühler, and George Drettakis. 3d gaussian splatting for real-time radiance field rendering. ACM Transactions on Graphics, 42(4), July 2023. 4 [34] Nicholas Kolkin, Jason Salavon, and Gregory Shakhnarovich. Style transfer by relaxed optimal transport and self-similarity. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2019. 6 [35] Yuhan Li, Yishun Dou, Xuanhong Chen, Bingbing Ni, Yilin Sun, Yutian Liu, and Fuzhen Wang. Generalized deep 3d shape prior via part-discretized diffusion process. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1678416794, 2023. 4 [36] Zhiqi Li, Yiming Chen, Lingzhe Zhao, and Peidong Liu. Controllable text-to-3d generation via surfacealigned gaussian splatting, 2024. 12 [37] Chen-Hsuan Lin, Jun Gao, Luming Tang, Towaki Takikawa, Xiaohui Zeng, Xun Huang, Karsten Kreis, Sanja Fidler, Ming-Yu Liu, and Tsung-Yi Lin. Magic3d: High-resolution text-to-3d content creation. In Conference on Computer Vision and Pattern Recognition, 2023. 3 [38] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matthew Le. Flow matching for generative modeling. In ICLR, 2023. 6 [39] Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, and Eric Xing. Stylerf: Zero-shot 3d style transfer of neural radiance fields. In Conference on Computer Vision and Pattern Recognition, 2023. 3 [40] Kunhao Liu, Fangneng Zhan, Muyu Xu, Christian Theobalt, Ling Shao, and Shijian Lu. Stylegaussian: Instant 3d style transfer with gaussian splatting. In SIGGRAPH Asia 2024 Technical Communications. 2024. 1, [41] Minghua Liu, Mikaela Angelina Uy, Donglai Xiang, Hao Su, Sanja Fidler, Nicholas Sharp, and Jun Gao. Partfield: Learning 3d feature fields for part segmentation and beyond. 2025. 3, 5, 10, 18, 19 [42] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. In ICLR, 2023. 6 [43] Shitong Luo and Wei Hu. Diffusion probabilistic models for 3d point cloud generation. In Conference on Computer Vision and Pattern Recognition, 2021. [44] Shalini Maiti, Lourdes Agapito, and Filippos Kokkinos. Gen3deval: Using vllms for automatic evaluation of generated 3d objects. In The IEEE Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 21 [45] Ben Mildenhall, Pratul Srinivasan, Matthew Tancik, Jonathan Barron, Ravi Ramamoorthi, and Ren Ng. Nerf: Representing scenes as neural radiance fields for view synthesis. Communications of the ACM, 65 (1):99106, 2021. 4 [46] Norman Müller, Yawar Siddiqui, Lorenzo Porzi, Samuel Rota Bulo, Peter Kontschieder, and Matthias Nießner. Diffrf: Rendering-guided 3d radiance field diffusion. In Conference on Computer Vision and Pattern Recognition, 2023. 3 [47] Yeongtak Oh, Jooyoung Choi, Yongsung Kim, Minjun Park, Chaehun Shin, and Sungroh Yoon. Controldreamer: Blending geometry and style in text-to-3d. arXiv:2312.01129, 2023. 1 [48] Maxime Oquab, Timothée Darcet, Theo Moutakanni, Huy V. Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Russell Howes, Po-Yao Huang, Hu Xu, Vasu Sharma, Shang-Wen Li, Wojciech Galuba, Mike Rabbat, Mido Assran, Nicolas Ballas, Gabriel Synnaeve, Ishan Misra, Herve Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski. Dinov2: Learning robust visual features without supervision, 2023. 2, 4, [49] Or Patashnik, Zongze Wu, Eli Shechtman, Daniel Cohen-Or, and Dani Lischinski. Styleclip: Text-driven manipulation of stylegan imagery. In International Conference on Computer Vision, 2021. 3 [50] Dario Pavllo, Jonas Kohler, Thomas Hofmann, and Aurelien Lucchi. Learning generative models of textured 3d meshes from real-world images. In International Conference on Computer Vision, 2021. 3 [51] Yuang Peng, Yuxin Cui, Haomiao Tang, Zekun Qi, Runpei Dong, Jing Bai, Chunrui Han, Zheng Ge, Xiangyu Zhang, and Shu-Tao Xia. Dreambench++: human-aligned benchmark for personalized image generation. In The Thirteenth International Conference on Learning Representations, 2025. 7, 21, 22 [52] Sai Raj Kishore Perla, Yizhi Wang, Ali Mahdavi-Amiri, and Hao Zhang. Easi-tex: Edge-aware mesh texturing from single image. ACM Transactions on Graphics (Proceedings of SIGGRAPH), 43(4), 2024. 3, 7, 8, 9, [53] Dustin Podell, Zion English, Kyle Lacey, A. Blattmann, Tim Dockhorn, Jonas Muller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. ArXiv, abs/2307.01952, 2023. 7, 22 [54] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. International Conference for Learning Representations, 2022. 1, 3 [55] Tianhao Qi, Shancheng Fang, Yanze Wu, Hongtao Xie, Jiawei Liu, Lang Chen, Qian He, and Yongdong Zhang. Deadiff: An efficient stylization diffusion model with disentangled representations. In Conference on Computer Vision and Pattern Recognition, 2024. 3 13 [56] Elad Richardson, Gal Metzer, Yuval Alaluf, Raja Giryes, and Daniel Cohen-Or. Texture: Text-guided texturing of 3d shapes. In ACM SIGGRAPH, 2023. 3 [57] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models, 2021. 4 [58] Tamar Rott Shaham, Tali Dekel, and Tomer Michaeli. Singan: Learning generative model from single natural image. In Computer Vision (ICCV), IEEE International Conference on, 2019. 19 [59] Eli Shechtman and Michal Irani. Matching local self-similarities across images and videos. In CVPR, 2007. 5 [60] Tianchang Shen, Jacob Munkberg, Jon Hasselgren, Kangxue Yin, Zian Wang, Wenzheng Chen, Zan Gojcic, Sanja Fidler, Nicholas Sharp, and Jun Gao. Flexible isosurface extraction for gradient-based mesh optimization. ACM Trans. Graph., 42(4), jul 2023. ISSN 0730-0301. doi: 10.1145/3592430. 4 [61] Yawar Siddiqui, Antonio Alliegro, Alexey Artemov, Tatiana Tommasi, Daniele Sirigatti, Vladislav Rosov, Angela Dai, and Matthias Nießner. Meshgpt: Generating triangle meshes with decoder-only transformers. In Conference on Computer Vision and Pattern Recognition, 2024. 3 [62] Sinisa Stekovic, Arslan Artykov, Stefan Ainetter, Mattia DUrso, and Friedrich Fraundorfer. Pytorchgeonodes: Enabling differentiable shape programs for 3d shape reconstruction. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2025. 7, 10, 18 [63] Jiaxiang Tang, Jiawei Ren, Hang Zhou, Ziwei Liu, and Gang Zeng. Dreamgaussian: Generative gaussian splatting for efficient 3d content creation. International Conference for Learning Representations, 2022. 3 [64] Narek Tumanyan, Omer Bar-Tal, Shai Bagon, and Tali Dekel. Splicing vit features for semantic appearance transfer. In CVPR, pages 1074810757, 2022. 1, 3, 18 [65] Narek Tumanyan, Omer Bar-Tal, Shir Amir, Shai Bagon, and Tali Dekel. Disentangling structure and appearance in vit feature space. ACM Trans. Graph., nov 2023. ISSN 0730-0301. 1 [66] Dmitry Ulyanov, Vadim Lebedev, Andrea Vedaldi, and Victor Lempitsky. Texture networks: Feed-forward synthesis of textures and stylized images. International Conference on Machine Learning, 2016. 3 [67] Zhengyi Wang, Jonathan Lorraine, Yikai Wang, Hang Su, Jun Zhu, Sanja Fidler, and Xiaohui Zeng. Llama-mesh: Unifying 3d mesh generation with language models. arXiv preprint arXiv:2411.09595, 2024. 3 [68] Zhizhong Wang, Lei Zhao, and Wei Xing. Stylediffusion: Controllable disentangled style transfer via diffusion models. In International Conference on Computer Vision, 2023. 1, 3 [69] Zhou Wang, A.C. Bovik, H.R. Sheikh, and E.P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE Transactions on Image Processing, 13(4):600612, 2004. doi: 10.1109/TIP.2003.819861. 2 [70] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and Josh Tenenbaum. Learning probabilistic latent space of object shapes via 3d generative-adversarial modeling. Advances in Neural Information Processing Systems, 2016. 3 [71] Tong Wu, Guandao Yang, Zhibing Li, Kai Zhang, Ziwei Liu, Leonidas Guibas, Dahua Lin, and Gordon Wetzstein. Gpt-4v(ision) is human-aligned evaluator for text-to-3d generation. In CVPR, 2024. 2, 7, 21, 22 [72] Jianfeng Xiang, Zelong Lv, Sicheng Xu, Yu Deng, Ruicheng Wang, Bowen Zhang, Dong Chen, Xin Tong, and Jiaolong Yang. Structured 3d latents for scalable and versatile 3d generation. In CVPR, 2025. 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 16, 18, 19, 20, 21, [73] Binxin Yang, Shuyang Gu, Bo Zhang, Ting Zhang, Xuejin Chen, Xiaoyan Sun, Dong Chen, and Fang Wen. Paint by example: Exemplar-based image editing with diffusion models. In Conference on Computer Vision and Pattern Recognition, 2023. 3 [74] Guandao Yang, Xun Huang, Zekun Hao, Ming-Yu Liu, Serge Belongie, and Bharath Hariharan. Pointflow: 3d point cloud generation with continuous normalizing flows. In International Conference on Computer Vision, 2019. 3 [75] Kangxue Yin, Jun Gao, Maria Shugrina, Sameh Khamis, and Sanja Fidler. 3dstylenet: Creating 3d shapes with geometric and texture style variations. In International Conference on Computer Vision, 2021. 3 14 [76] Jiwen Yu, Yinhuai Wang, Chen Zhao, Bernard Ghanem, and Jian Zhang. Freedom: Training-free energyguided conditional diffusion model. In International Conference on Computer Vision, 2023. [77] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models, 2023. 3, 7 [78] Muru Zhang, Ofir Press, William Merrill, Alisa Liu, and Noah A. Smith. How language model hallucinations can snowball. In Proceedings of the 41st International Conference on Machine Learning, ICML24. JMLR.org, 2024. 22 [79] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In CVPR, 2018. 2 [80] Xinlu Zhang, Yujie Lu, Weizhi Wang, An Yan, Jun Yan, Lianke Qin, Heng Wang, Xifeng Yan, William Yang Wang, and Linda R. Petzold. Gpt-4v(ision) as generalist evaluator for vision-language tasks. ArXiv, abs/2311.01361, 2023. 7, 21, [81] Yuxin Zhang, Nisha Huang, Fan Tang, Haibin Huang, Chongyang Ma, Weiming Dong, and Changsheng Xu. Inversion-based style transfer with diffusion models. In Conference on Computer Vision and Pattern Recognition, 2023. 3 [82] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric P. Xing, Hao Zhang, Joseph E. Gonzalez, and Ion Stoica. Judging llm-as-ajudge with mt-bench and chatbot arena. In Proceedings of the 37th International Conference on Neural Information Processing Systems, NIPS 23, Red Hook, NY, USA, 2023. Curran Associates Inc. 22 [83] Xin-Yang Zheng, Hao Pan, Peng-Shuai Wang, Xin Tong, Yang Liu, and Heung-Yeung Shum. Locally attentional sdf diffusion for controllable 3d shape generation. ACM SIGGRAPH, 2023. 3 [84] Linqi Zhou, Andy Shih, Chenlin Meng, and Stefano Ermon. Dreampropeller: Supercharge text-to-3d generation with parallel sampling. arXiv preprint arXiv:2311.17082, 2023. [85] Liyuan Zhu, Shengqu Cai, Shengyu Huang, Gordon Wetzstein, Naji Khosravan, and Iro Armeni. Scenelevel appearance transfer with semantic correspondences. In ACM SIGGRAPH 2025 Conference Papers. Association for Computing Machinery, 2025."
        },
        {
            "title": "Appendix",
            "content": "In the appendix, we provide the following: Intraand inter-category results using text condition with Lstructure (Sec. A) Results on using rendered image as condition with Lappearance (Sec. B) Ablation study on design choices (Sec. C) Application of GuideFlow3D on scene editing (Sec. D) Experimental and evaluation setup details (Sec. E) Issue with perceptual similarity evaluation metrics (Sec. F) Results of Human Evaluation (Sec. G) Details and analysis of the GPT-based evaluation setup (Sec. H) Additional qualitative results (Sec. I)"
        },
        {
            "title": "A Text Conditioning with Lstructure",
            "content": "Similar to image conditioned results in Tab. 2, we report results on using text prompts as the condition of the rectified flow model in Tab. A.1. As shown in Fig. 7, UV Nearest Neighbor fails to capture the described materials or object semantics, often ignoring features such as drawers, metal frames, or color attributes. SDXL + Cross-Image Attention [2] produces locally coherent patterns but lacks structural grounding, resulting in oversaturated or unrealistic textures. Trellis [72] improves the plausibility of the output yet misses fine-grained attributessuch as metallic elements, accent colors, or multi-material compositions and frequently distorts local part boundaries. In contrast, GuideFlow3D achieves consistently superior performance across all ranking metrics, both in intracategory and inter-category. In particular, our method preserves structural clarity and convincingly adapts texture, even when appearance is only described in abstract terms. The intra-category results reveal GuideFlow3Ds capacity to maintain semantic coherence between structure and style despite geometric variation. More strikingly, in the inter-category setting, where structure and appearance belong to different object types, our method still maintains better ranking scores (i.e., higher quality), demonstrating its robustness to both semantic drift and structural misalignment. This showcases the strength of our self-similarity guidance, which encourages intra-part consistency and inter-part contrast, enabling meaningful texture placement without explicit mesh supervision. Our flexible guidance based sampling process bridges the gap between structure and high-level semantic intent. Table A.1: Quantitative comparison ranking of our GuideFlow3D against baselines for different experimental settings. Results are shown with text conditioning (Lstructure). Methods Fidelity Clarity Adaptation Fidelity Clarity Adaptation Ranking metrics Intra-Category Inter-Category Simple-Complex UV Nearest Neighbor SDXL + Cross-Image Attention Trellis [72] GuideFlow3D (Ours) Complex-Complex UV Nearest Neighbor SDXL + Cross-Image Attention Trellis [72] GuideFlow3D (Ours) 3.12 2.88 2.01 1.54 3.45 2.97 2.16 1. 3.21 2.52 1.89 1.63 3.52 2.78 2.05 1.48 3.43 3.29 2.55 2.04 3.61 3.22 2.49 1.53 3.45 3.18 2.48 1.85 3.58 3.15 2.35 1. 3.66 2.96 2.38 2.01 3.67 3.06 2.41 1.87 3.72 3.33 2.69 1.98 3.75 3.31 2.66 1."
        },
        {
            "title": "B Rendered Image Conditioning with Lappearance",
            "content": "In many practical scenarios, an appearance mesh is provided without the corresponding reference imageeither because the asset was procedurally generated, sourced from CAD dataset, or previews were never created. To address this scenario, we render the appearance mesh from fixed viewpoint to synthesize an image, which serves as the conditioning input for Lappearance. For all objects we 16 Figure 7: Qualitative Comparisons showing results using text condition with Lstructure. Each row shows an input 3D mesh, descriptive appearance text, and textured results generated by different methods. We show challenging examples with considerable discrepancies between the appearance text prompts and input geometries. GuideFlow3D grounds textual descriptions in geometry, producing coherent, part-aware textures. The fifth row illustrates failure case where abstract terms like metal or cushioned are not fully captured, highlighting the difficulty of interpreting underspecified text. Overall, GuideFlow3D delivers detailed, realistic appearances aligned with object semantics. Figure 8: Examples of rendered mesh views. use the same viewpoint, chosen to provide slightly angled perspective on the object as shown in Fig. 8. Such view of the appearance mesh captures sufficient visual cuessuch as texture, material, and shape contextto serve as effective guidance for our framework. This strategy enables meshbased guidance in the absence of external visual data. Table B.1 shows results for this setup across intraand inter-category pairs. Note that these ranking scores are not directly comparable to those in Tab. 2, as this experiment includes only the top performing baseline (Trellis); the absence of lower-ranked outputs can shift absolute values due to the relative nature of GPT-based evaluation. While the reference image variant performs best overall, our rendered-view approach still consistently outperforms Trellis[72], demonstrating GuideFlow3Ds robustness against limited image data. Table B.1: Quantitative comparison ranking of our GuideFlow3D against baselines for different experimental settings. Results are shown with image conditioning (Lappearance)."
        },
        {
            "title": "Methods",
            "content": "Fidelity Clarity Adaptation Fidelity Clarity Adaptation"
        },
        {
            "title": "Ranking metrics",
            "content": "Intra-Category Inter-Category Simple-Complex Trellis [72] w/ Rendered Image w/ Reference Image Complex-Complex Trellis [72] w/ Rendered Image w/ Reference Image 2.94 1.96 1.08 2.94 1.96 1."
        },
        {
            "title": "C Ablation Study",
            "content": "2.96 1.98 1.03 2.97 1.99 1.02 2.95 1.99 1.02 2.96 1.98 1.02 2.93 1.97 1.06 2.95 1.97 1. 2.97 1.99 1.02 2.97 1.99 1.02 2.96 1.98 1.01 2.97 1.98 1.01 We conduct an ablation study to analyze the impact of various design choices in our framework. Results in Tab. C.1 show performance on the simple-complex intra-category set under image conditioning. Inspired by the use of global appearance loss in [64], (i) we model global appearance for guidance. We do this using combination of minimum, maximum, and average pooling operations which we found insufficient for our application. This highlights the ineffectiveness of global latents in capturing rich semantic correspondence required for high-quality appearance transfer. (ii) Replacing such global features with nearest-neighbor (NN) matching in SLAT space slightly improves performance, especially in fidelity, but lacks robustness in integration and adaptation, confirming that unstructured similarity fails to align content meaningfully across different shapes. (iii) Next, we evaluate version using K-means-based co-segmentation over SLAT features instead of the PartField-driven segmentation [41] in GuideFlow3D. While guided flow improves generation realism, this variant still underperforms relative to the full method, indicating that semantically informed segmentation is crucial for establishing accurate part correspondences. (iv-v) Finally, we compare the two guidance objectives used in our framework: Lappearance, and Lstructure, both using image as condition. Each guidance improves transfer quality in its respective setting, with appearance guidance achieving stronger detail fidelity and structure guidance yielding better alignment and adaptability. These results validate our design of condition-specific guidance, where the choice of loss is tailored to the nature of the input, enabling flexible and semantically consistent transfer. Table C.1: Ablation study ranking design choices using image conditioning. Results are on the simple-complex intra-category set. Methods w/o Rectified Flow (i) Co-segmentation [41] & global feat. (ii) Co-segmentation [41] & NN w/ Guidance in Rectified Flow (iii) K-means Co-segmentation + NN (iv) w/ Image Condition (Lstructure) - Ours (v) w/ Image Condition (Lappearance) - Ours Fidelity Clarity Integration Quality Adaptation Overall Ranking metrics 4.52 3. 2.57 2.17 1.23 4.51 3.62 2.65 2.05 1.08 4.53 3.61 2.60 2.12 1.16 4.51 3. 2.63 2.05 1.11 4.49 3.61 2.61 2.11 1.12 4.50 3.63 2.66 2.03 1.06 Application: Scene Editing To demonstrate the broader applicability of GuideFlow3D beyond isolated object transfer, we explore its use in full-scene editing. In this setting, we start with indoor scans from ScanNet [17] and utilize per-object CAD mesh annotations from PyTorchGeoNodes [62] and Scannotate [1] to obtain 3D mesh geometries of objects in the scene. For each semantic category present in the scene, we select 18 representative appearance object from the ABO dataset [16] and perform appearance transfer individually using Lappearance on the corresponding meshes. As shown in Fig. 9, our method is capable of stylizing multiple objects across cluttered scene while preserving their geometric structure. Note that objects with inaccurate pose annotations or high computational demands for Trellis processing are excluded from the transfer process. Despite relying on clean input meshes, our framework generalizes well in complex, real-world environments and opens avenues for future extensions with learned guidance in noisy or incomplete settings. This highlights GuideFlow3Ds utility for interactive scene customization, where designers can selectively restyle environments by swapping appearance while preserving structural layout. Figure 9: Scene editing on scenes from ScanNet [17]. For each semantic category, represented with unique color in the middle column, we select single appearance object and perform appearance transfer using our GuideFlow3D. Appearance is robustly transferred to different objects in the scene, showing another potential application of our approach."
        },
        {
            "title": "E Experimental Details",
            "content": "Evaluation Setup. For evaluation, we render all assetsincluding the input, appearance, and output meshesusing Blender with smooth area lighting. Each object is rendered from 4 viewpoints, sampled around the origin at fixed radius of 2 units and pitch of 30, with yaw angles spaced every 90 starting from 45. All meshes are in canonical pose to ensure input, appearance reference, and output are aligned, avoiding mismatches like comparing the front side of one object to the back side of another. For the rendered mesh-view guidance experiments (Sec. B), we use pitch and yaw of 30 and 45, respectively, to obtain single consistent view of the appearance mesh. However, evaluation setup remains the same. Following standard practice [58, 15], we compute every metric, perceptual and GPT-based per view and per object, and report the average across all objects. This ensures that evaluations account for view-dependent variations. Implementation Details. Our implementation is based on the pre-trained models and configurations from Trellis [72] and PartField [41]. We use trellis-image-large for image conditioning and trellis-text-large for text, and adhere to their default settings for any configuration. For partaware guidance, we compute part feature fields using PartField [41] and query them using the voxel coordinates pi of each mesh. We run GuideFlow3D for single-instance optimization interleaved with rectified flow sampling over 300 steps. Optimization is performed using AdamW with learning rate of 5 104. All experiments are conducted on single NVIDIA RTX 4090 GPU. We use identical optimization settings across all conditioning types to ensure fairness and consistency."
        },
        {
            "title": "F Issues with Perceptual Similarity Evaluation",
            "content": "Quantitatively evaluating appearance transfer in 3D is challenging due to the absence of ground truth outputs. Unlike traditional image-to-3D pipelines, where the generated asset can be directly compared to reference view or textured model, our setting lacks definitive target for what the 19 Figure 10: Issue with Perceptual Similarity. Even though our GuideFlow3D visually outperforms the Trellis baseline [72], CLIP scores do not reflect these improvements. In this example, we observe that the geometric forms of the input and appearance mesh are very different. Therefore CLIP score is not suitable metric for evaluating similarity of output. We observe that the difference in scores increases for cropped part of the chair, showing the difference between local and global geometry on the metric. transferred appearance \"should\" look like. As illustrated in Fig. 10, the input and output geometries are intentionally dissimilar, and the appearance is adaptednot reconstructedmaking traditional reconstruction-based metrics inapplicable. To this end, we report commonly used perceptual similarity scores such as DINOv2 [48], CLIP score [29], and DreamSim [25] in Table F.1, on image-conditioned appearance transfer for intra-category on simple-complex set. Due to large geometric differences between appearance and input geometries, these metrics offer only coarse proxy for evaluating style transfer quality and often fail to capture the localized and semantic nuances essential to this task. Furthermore, in case of conditioning using text prompt, our CLIP score is actually slightly lower than the score for the Trellis baseline which is deceiving. This is because the text prompt often describes very different geometry which is not aligned with the geometric form of the input object making such metrics less applicable for our experimental setting (e.g. padded reclining armchair vs. input mesh of simple stool, as shown in second row of Fig. 7). For instance, Fig. 10 shows an example where our method produces visibly better texture alignment and part-aware fidelity, yet perceptual similarity scores (e.g., CLIP) remain close or even favor inferior outputs. Comparisons on image crops, where scores between methods show larger gaps, highlight how such metrics can overlook structural misalignments, texture stretching, or poor integration when focusing on the global scale that are perceptually obvious to humans. While such encoder-based metrics could be useful in setups with ground-truth part-level correspondences that would enable an evaluation on cropped regions, their effectiveness is limited in our setting where no such reference exists. These limitations underscore the need for our fine-grained, human-aligned evaluation."
        },
        {
            "title": "G Human Evaluation",
            "content": "We conducted user study on Amazon Mechanical Turk with 59 participants to compare our approach against baselines. The evaluation set consisted of 100 randomly selected object and image pairs from 20 Table F.1: Quantitative comparison based on perceptual similarity metrics. Results are on the simple-complex intra-category set. Note that DinoV2 and DreamSim can only be evaluated in an image setting. Methods w/ Image Condition (Lappearance) UV Nearest Neighbor Cross Image Attention MambaST EasiTex Trellis [72] GuideFlow3D (Ours) w/ Text Condition (Lstructure) UV Nearest Neighbor SDXL + Cross-Image Attention Trellis [72] GuideFlow3D (Ours) Perceptual similarity DinoV2 CLIP DreamSim 36.32 44.67 38.33 44.42 52.22 52.74 - - - - 81.23 84.60 81.73 83.80 87.15 87.37 25.32 25.96 27.64 27.23 42.27 41.67 47.55 41.13 36.52 36. - - - Figure 11: Results of Human Evaluation on randomly sampled outputs from both intraand intercategory on simple-complex set. the simple-complex intraand inter-category, on the image condition setting. The participants were shown the appearance image and two views (front and back at camera angle of 45 degrees) each of the structure mesh + outputs from different methods, without any knowledge about the methods. Then, they were provided with the same prompts as the LLM (Figs. 13 and 14) and asked to rate the best performing considering overall quality. As shown in Fig. 11, out of approximately 1000 evaluations, GuideFlow3D received the highest preference (27.2%), demonstrating its ability to balance structural preservation and appearance fidelity. Interestingly, UV Nearest Neighbor was ranked third, despite its lower quantitative performance in Tab. 1, likely due to its visually appealing but less semantically grounded results. Overall, the relative ranking across methods remains consistent with our LLMbased evaluation, confirming strong alignment between LLM-based and human judgments of texture fidelity and perceptual quality. GPT-based Evaluation We outline the issues with automated perceptual similarity evaluation in Sec. and resort to GPT-based evaluation mechanism for analysis. GPT-4V has shown strong alignment with human judgments across vision-language tasks, including text-to-3D generation [51], image understanding, and 3D content evaluation. [71, 44, 23, 80]. In designing the GPT-based evaluator, we follow the human-aligned evaluation protocol proposed in GPT-Eval3D [71] and adopt it for appearance transfer using gpt-5-mini. We begin by clearly explaining the task to GPT evaluator: ranking textured 3D 21 outputs based on the quality of appearance transfer from style source (image or text) to structure mesh, using single rendered view per mesh. All images are rendered from consistent viewpoint, and the evaluator is instructed to imagine each output as complete 3D object when judging texture consistency and structure preservation. As shown in Fig. 13 and Fig. 14, the instruction prompt defines six specific evaluation criteria: Style Fidelity, Structure Clarity, Style Integration, Detail Quality, Shape Adaptation, and Overall Quality. Each criterion is accompanied by detailed explanation, emphasizing semantic consistency (e.g., appropriately mapping wood textures to structural elements like legs or seats, depending on the design), 3D geometric clarity, texture sharpness, and perceptual coherence. This structured rubric guides the LLM to perform fine-grained comparisons across outputs. To ensure consistency in output formatting and facilitate large-scale evaluation, we explicitly define fixed output format corresponding to the six criteria. An example format is provided directly in the prompt to reduce ambiguity and improve parsing during automatic result aggregation. Additionally, we support in-context learning by optionally breaking the prompt into multiple stages with example completions shown before evaluation begins. These design choicesstructured rubric, visual clarity, strict formatting, and optional in-context examplesensure that our GPT-based evaluator produces reliable, human-aligned rankings across all experiments. Please note that we use output from all methods for comparison at time (three is shown as an example in Figs. 13 and 14). Limitations. While our GPTbased evaluation protocol proved practical and consistent for our purposes, prior work highlights several factors that future users should consider. Large multimodal models are known to exhibit hallucinations [71, 80] and systematic biases, such as position sensitivity [82, 78], and preference for high-contrast textures. These factors may distort rankings in ways that do not reflect actual quality. Moreover, GPT-based evaluation could be vulnerable to adversarial artifacts designed to exploit the multimodal models visual priors. Ensuring such metrics remain ungamable is an open challenge. Future research can explore more efficient comparison strategies such as single method evaluation using scale, e.g. 1 to 5 [51]."
        },
        {
            "title": "I Qualitative Results",
            "content": "We provide qualitative comparisons for both textand image-conditioned appearance transfer. Figs. 7 and 12 highlight GuideFlow3Ds ability to produce semantically faithful and geometrically aligned textures across diverse scenarios. In the text-conditioned setting  (Fig. 7)  , we evaluate challenging examples with significant discrepancies between textual descriptions and input geometries. UV Nearest Neighbor and SDXL [53] + Cross Image Attention [2] produce unrealistic outputs, often failing to capture semantic or color cues. Trellis [72] shows improvements in texture plausibility but struggles with global color consistency and local part correspondence, e.g., missing reddish-brown tones or black metal frames. In contrast, GuideFlow3D effectively grounds textual descriptions in geometry, yielding coherent, part-aware textures that accurately reflect material and structural cues. Nonetheless, when descriptions are abstract (e.g., metal, mirror, or cushioned), all methods show limitations, as shown in Fig. 7 (Row 5), highlighting the inherent ambiguity in interpreting such prompts. Under image conditioning  (Fig. 12)  , UV Nearest Neighbor frequently misplaces textures due to limited geometric reasoning, though it performs reasonably when single uniform texture dominates. MambaST [8], Cross Image Attention, and EasiTex [52] tend to oversmooth details or blend appearance cues across regions, while Trellis improves realism yet lacks explicit geometric grounding, sometimes introducing artifacts, such as dark textures from correspondence failures. GuideFlow3D achieves semantically and structurally consistent transfers, faithfully reproducing upholstery and wood patterns while maintaining sharp boundaries and material coherence. However, certain edge cases remain challenging (Fig. 12, Column 5), such as strong geometry-appearance mismatches or texture flattening under large shape variations. Overall, GuideFlow3D demonstrates robust semantic and structural fidelity across both modalities, effectively bridging the gap between appearance semantics and geometric form. These results underscore its capability to generate realistic, production-ready 3D assets. 22 Figure 12: Qualitative Comparisons showing results using image condition with Lappearance. We show 5 examples (one per column) and the results for all methods. UV Nearest Neighbor (UV NN) often misplaces textures across parts but performs better with uniform materials. MambaST, Cross Image Attention, and EasiTex blend or oversmooth textures, while Trellis improves realism yet lacks geometric grounding. GuideFlow3D achieves semantically and structurally consistent transfers, accurately mapping materials like upholstery and wood. Column 5 illustrates failure case for all methods, though UV NN yields the most visually coherent result and our interleaved guidance scheme cannot recover from the black output produced by Trellis. 23 Figure 13: Image GPT prompt. Prompt provided to GPT-5-mini for providing ranking of textured 3D outputs given an appearance object in the form of an image. Figure 14: Text GPT prompt. Prompt provided to GPT-5-mini for providing ranking of textured 3D outputs given an appearance object in the form of text."
        }
    ],
    "affiliations": [
        "ENPC, IP Paris"
    ]
}