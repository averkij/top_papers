{
    "paper_title": "MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation",
    "authors": [
        "Yanbo Ding",
        "Xirui Hu",
        "Zhizhi Guo",
        "Yali Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animation. To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT (4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens. Compared to 2D-rendered pose images, 4D motion tokens offer more robust spatio-temporal cues and avoid strict pixel-level alignment between pose image and character, enabling more flexible and disentangled control. Then, we introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention with 4D positional encodings, MV-DiT can effectively leverage motion tokens as 4D compact yet expressive context for human image animation in the complex 3D world. Hence, it marks a significant step forward in this field and opens a new direction for pose-guided human video generation. Experiments show that our MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98, surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter also generalizes well to diverse open-world characters (single/multiple, full/half-body) across various styles and scenarios. Our video demos and code are on: https://github.com/DINGYANB/MTVCrafter."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 2 8 3 2 0 1 . 5 0 5 2 : r MTVCrafter: 4D Motion Tokenization for Open-World Human Image Animation Yanbo Ding, Xirui Hu , Zhizhi Guo , Yali Wang {yb.ding, yl.wang}@siat.ac.cn 2392027275@stu.xjtu.edu.cn guozz2@chinatelecom.cn Figure 1: We propose MTVCrafter, which can effectively transfer pose sequences from driven video to diverse, unseen single or multiple characters in either full-body or half-body settings across various styles such as anime, pixel art, ink drawings, and photorealism, outperforming existing state-of-the-art methods in generation robustness and generalizability to open-world scenarios."
        },
        {
            "title": "Abstract",
            "content": "Human image animation has gained increasing attention and developed rapidly due to its broad applications in digital humans. However, existing methods rely largely on 2D-rendered pose images for motion guidance, which limits generalization and discards essential 3D information for open-world animation. To tackle this problem, we propose MTVCrafter (Motion Tokenization Video Crafter), the first framework that directly models raw 3D motion sequences (i.e., 4D motion) for human image animation. Specifically, we introduce 4DMoT (4D motion tokenizer) to quantize 3D motion sequences into 4D motion tokens. Compared to 2D-rendered pose Work done during an internship at TeleAI. Corresponding Author images, 4D motion tokens offer more robust spatio-temporal cues and avoid strict pixel-level alignment between pose image and character, enabling more flexible and disentangled control. Then, we introduce MV-DiT (Motion-aware Video DiT). By designing unique motion attention with 4D positional encodings, MV-DiT can effectively leverage motion tokens as 4D compact yet expressive context for human image animation in the complex 3D world. Hence, it marks significant step forward in this field and opens new direction for pose-guided human video generation. Experiments show that our MTVCrafter achieves state-of-the-art results with an FID-VID of 6.98, surpassing the second-best by 65%. Powered by robust motion tokens, MTVCrafter also generalizes well to diverse open-world characters (single/multiple, full/half-body) across various styles and scenarios. Our video demos and code are on: https://anonymous.4open.science/r/MTVCrafter-1B13."
        },
        {
            "title": "Introduction",
            "content": "Human image animation [1, 2, 3, 4, 5, 6, 7, 8], which aims to synthesize videos of reference human image driven by pose sequences estimated from an input video, has attracted increasing attention due to its broad applications in digital humans [9, 10], virtual try-on [11, 12], and immersive content creation [13, 14]. To meet the growing demand, numerous methods [15, 16, 17, 18, 19, 20, 21, 22] have been proposed to achieve high-quality animation with realistic motion and consistent appearance. However, as shown in Figure 2, existing methods depend on 2D-rendered pose images to provide motion guidance for the generative model. This introduces two fundamental limitations. First, although pose images provide basic structural cues, they inevitably discard rich spatio-temporal motion from the real 3D world. Hence, they struggle to synthesize physically plausible and expressive motions, especially in complex 3D scenarios (e.g., Gymnast in Figure 5). Second, when the pose is provided in image form, the model tends to blindly copy the fixed-shaped poses pixel-by-pixel without grasping the underlying motion semantics. Consequently, the animation often exhibits distortions or artifacts, especially when the pose images from the driven video significantly deviate from the reference appearance in shape or position (e.g., Hulk in Figure 2). Hence, natural question arises: can we directly model raw 4D motion rather than 2D-rendered pose images for animation guidance? To answer this question, we draw inspiration from recent advances in motion generation [23, 24, 25], where 1D motion sequences [26, 27] are quantized and modeled using Transformer-based architectures [28]. Built upon this insight, we propose MTVCrafter (Motion Tokenization Video Crafter), novel framework that combines 4D motion tokenizer with motion-aware video Diffusion Transformer (DiT) [29, 30, 31] for open-world human image animation. Firstly, to leverage richer spatio-temporal information in the 3D world than what can be captured by 2D image renderings, we propose 4DMoT (4D Motion Tokenizer) to directly quantize raw human motion data (e.g., 3D SMPL [32] sequences). The resulting motion tokens faithfully preserve the information of raw 4D motion, effectively addressing the first limitation of lacking explicit 3D information. Secondly, we propose MV-DiT (Motion-aware Video DiT) for controllable animation. We adopt the DiT architecture (e.g., CogVideoX [33]) due to its superior modeling capacity and flexibility compared to traditional U-Net-based [34] designs [35, 36]. By integrating 4D motion attention into DiT blocks, our MV-DiT effectively leverages motion tokens as context for vision tokens. This design eliminates the need to render pose images and enables the model to better learn underlying motion semantics, thereby addressing the second limitation of pixel-level copying. Crucially, we incorporate unique 4D (1D temporal and 3D spatial) positional encodings into the motion attention keys to enhance spatio-temporal relationships. For appearance preservation, unlike common ReferenceNet-based methods [2, 6, 7, 15, 19] that use network copy to model identity separately, we concatenate the video and reference image for joint interaction in self-attention, ensuring identity consistency with lower complexity and cost. By 4D motion tokenization and motion attention, MTVCrafter establishes new paradigm for human image animation with improved generalization and controllability. Our contributions are summarized as follows: (1) We introduce MTVCrafter, the first pipeline that directly models raw 4D motion instead of 2D-rendered pose images for open-world human image animation, enabling animation in the complex 3D world. (2) We introduce 4DMoT, novel 4D motion tokenizer that encodes raw human motion data into 4D compact yet expressive tokens, providing more robust spatio-temporal guidance than 2D image representations. (3) We design MV-DiT, motionaware video DiT model equipped with unique 4D motion attention and 4D positional encodings, 2 Figure 2: Our motivation is that directly tokenizing 4D motion captures more faithful and expressive information than traditional 2D-rendered pose images derived from the driven video. enabling animation effectively guided by 4D motion tokens. (4) MTVCrafter achieves state-of-the-art performance on the TikTok [37] benchmark, outperforming the second-best by 65% in FID-VID. As shown in Figure 1, MTVCrafter also generalizes well to unseen motions and characters, including both single and multiple, full-body and half-body characters across diverse styles and scenarios."
        },
        {
            "title": "2 Related Work",
            "content": "Diffusion Models for Controllable Generation Diffusion Models (DMs) [38, 39, 40] have achieved remarkable success in visual content generation, including tasks like image [41] and video [42] synthesis. Unlike traditional GAN-based [43] methods [44, 45], which often suffer from training instability and mode collapse, diffusion-based approaches offer more stable training dynamics and generate high-quality content with improved diversity. This superior performance has led Stable Diffusion series [41, 46, 47] to quickly dominate the field of vision-generative AI. To enable fine-grained control during the generation process, various methods have introduced conditional mechanisms. ControlNet [48] employs zero-initialization and network duplication to facilitate control over structural elements such as sketches and depth maps. ControlNeXt [22] improves this design with lightweight module and cross-normalization strategy. Other specialized methods extend controllability to aspects such as motion trajectories [49, 50], camera viewpoints [51, 52], scene layouts [53, 54], and lighting conditions [55, 56]. In this work, we focus on the more challenging and impactful task of open-world human image animation with precise control over human poses. Human Image Animation Early approaches [57, 58, 59, 60] on this task predominantly adopt GANs to animate the reference image, but struggled with visual artifacts. Recent advances in diffusion models for text-to-video generation [30, 33, 35, 36] have inspired their application to human image animation [15, 61]. Disco [62] first introduces hybrid diffusion architecture with disentangled control over human foreground, background, and pose. MagicAnimate [19] and AnimateAnyone [15] developed specialized reference and pose networks to control appearance and motion, respectively. Champ [18] leverages mesh renderings for enhanced controllability, while Unianimate [63] integrates Mamba [64] into diffusion for improved efficiency and addresses pose mismatches through retargeting techniques. MimicMotion [21] and Realiscance [6] implement regional loss functions to mitigate distortion. StableAnimator [17] uses HJB-based [65, 66] optimization to enhance identity preservation. AnimateX [67] and AnimateAnyone-2 [16] extend motion transfer to non-human subjects and environmental interactions, respectively. Human-DiT [20] trains key-point DiT to predict pose sequences in the absence of driving videos. Importantly, all these methods rely on 2D images for pose guidance, including skeletons (from DWPose [68], OpenPose [69] or RtmPose [26]), SMPL [70, 71] renderings, or depth maps [72]). Our work directly tokenizes 4D motion without intermediate rendering, and designs motion attention in DiT to effectively leverage 4D motion tokens."
        },
        {
            "title": "3 Method",
            "content": "Latent Diffusion Models Latent Diffusion Models (LDMs) [41] encode data into lowerdimensional latent space via Variational Autoencoder (VAE [73]) encoder E, i.e., z0 = E(x0). The diffusion process is performed in this latent space to reduce the computational load. The forward process adds noise as q(zt z0) = (zt; i=1 αi, αt = 1 βt, and βt is predefined sequence schedule. neural network ϵθ is trained to predict the added noise (cid:3), where is an optional condition by minimizing the MSE loss: EϵN (0,I),zt,c,t like text or image embeddings encoded by T5 [74] or CLIP [75]. At inference, denoising starts from Gaussian noise in latent space, and the final result is decoded by VAE decoder D, i.e., x0 = D(z0). αtz0, (1 αt)I), where αt = (cid:81)t (cid:2)ϵ ϵθ(zt; c, t)2 2 Diffusion Transformer The Diffusion Transformer (DiT) [30, 31, 33], as prevailing approach, integrates Transformer-based backbone into the diffusion process. Using Patchify [29] and Rotary Positional Encoding (RoPE) [76], the denoising network ϵθ can effectively process inputs with varying spatial and temporal dimensions, thus improving scalability and adaptability. In practice, RoPE encodes relative positional information via rotation in complex space: Ri(x, m) = (cid:20)cos(mθi) sin(mθi) cos(mθi) sin(mθi) (cid:21) (cid:20) x2i x2i+1 (cid:21) (1) where is the input query or key vectors, is the positional index, is the feature dimensional index. θi is the frequency, i.e., 100002i/D, and is the dimension of the attention layer. Overview After the preliminary introduction, we next explain our MTVCrafter in detail. In Section 3.1, we introduce 4DMoT for 4D motion tokenization. The resulting 4D motion tokens exhibit more robust spatio-temporal cues than 2D-rendered pose images. In Section 3.2, we introduce MV-DiT to leverage 4D motion tokens as vision context in powerful DiT architecture. It features unique 4D motion attention with 4D positional encodings and motion-aware classifier-free guidance (CFG) [77], enabling open-world animation guided by 4D compact yet expressive motion tokens. 3.1 4D Motion Tokenizer To guide human image animation with rich 4D guidance, we extract SMPL [71] sequences from the driven video as conditions. While prior works [6, 8, 18] also use SMPL, they naively render 3D meshes into 2D images as conditions, which often results in insufficient motion representation for open-world animation, as illustrated in Figure 2. In contrast, we directly tokenize raw SMPL sequences to 4D motion tokens. First, we construct the training dataset of SMPL motion-video sequences. Then, we design 4D motion VQVAE (Figure 3) to learn the noise-free motion representation. Motion-Video Dataset Preparation Dancing is representative task in digital human generation. However, existing open-source datasets, such as TikTok [37] and Fashion [78], are limited in both motion diversity and visual quality, which constrains their effectiveness in training high-fidelity generative models. To this end, we curate high-quality dance video dataset with 30K clips. These video clips are collected from public datasets, web-crawled sources, and AI-generated content, covering diverse human figures and scenes. They are subsequently filtered to ensure temporal consistency, high motion quality, and visual quality (See in Appendix B). For resulting videos, we use NLF-Pose [32] to estimate the SMPL parameters {θt, βt}T t=1, where is the number of video frames, θt R243 are joint rotations and βt R10 are shape parameters. The estimated SMPL parameters are then processed through forward kinematics to compute the 3D joint positions Jt R243, followed by Z-normalization to Jt using the datasets statistical mean and standard deviation: Jt = F(θt, βt; W) µdataset σdataset (2) where denotes the SMPL kinematic chain function [70, 71], and is the pre-trained joint regressor. The 3D joint positions Jt serve as input to the subsequent 4DMoT, providing more robust spatio-temporal information than traditional mesh renderings. The final dataset contains 5K SMPL motion-video pairs, averaging 600 frames each, and covering diverse motion, characters, and scenes. 4 Figure 3: Our 4D motion tokenizer consists of an encoder-decoder framework to learn spatiotemporal latent representations of SMPL sequences, and vector quantizer to learn 4D compact yet expressive tokens in unified space. All operations are in 2D space along the frame and joint axes. Model Architecture of our 4DMoT Since the VQVAE architecture is widely used for discrete tokenization in downstream tasks [24, 79, 80], we adopt and build upon its structure. As shown in Figure 3, our 4DMoT consists of an encoder-decoder structure for motion sequence reconstruction, along with lightweight quantizer for learning discrete motion tokens. The encoder-decoder preserves spatio-temporal coherence in 4D motion, while the quantizer enables the learning of 4D compact yet expressive motion representations. Specifically, given raw motion sequence = {J1, J2, ...Jf } with frames and joints, the encoder first maps it into continuous latent space, through series of residual blocks with 2D convolutions along both the temporal (f ) and spatial (j) axes, as well as downsampling blocks with average pooling layers. This yields latent representations {Em Rd}f /4j m=1 , where denotes the token dimension. Next, vector quantizer performs discretization via nearest-neighbor lookup in learnable codebook {Cn Rh}s n=1, where denotes the codebook size. The resulting motion tokens exhibit 4D compact yet expressive information in unified space, served as input condition to the subsequent MV-DiT. Following prior works [23, 25, 81], the codebook is optimized with Exponential Moving Average (EMA) and codebook resetting technique to maintain codebook usage diversity. Finally, the decoder, similar structure as the encoder but has upsampling blocks, reconstructs the motion sequence ˆM from the quantized codes C. To enhance long-range dependencies, we also incorporate dilated convolutions and sliding window strategy for temporal modeling. The complete training objective Lvq combines reconstruction loss with commitment loss to ensure faithful reconstruction and effective codebook utilization, which is defined as: Lvq = ˆM 1 (cid:123)(cid:122) (cid:125) (cid:124) reconstruction +β sg[C2 2 (cid:125) (cid:123)(cid:122) commitment (cid:124) (3) where sg[] denotes the stop-gradient operation, β is hyperparameter to control the weight of the commitment loss, and are the latents before and after quantization, respectively. 3. 4D Motion Video Diffusion Transformer After obtaining 4D motion tokens, we aim to effectively leverage them for human image animation. In this section, we describe how the 4D motion tokens are integrated as conditions into the video DiT model. our design comprises four key components: reference image preservation, 4D positional encodings, 4D motion attention, and motion-aware classifier-free guidance. Reference Image Preservation Maintaining visual and temporal consistency remains key challenge in human image animation. Unlike previous methods [2, 6, 7, 15, 19] that employ reference network with the same structure as the denoising model to learn the reference image separately, our MV-DiT opts for simple yet effective repeat-and-concatenate strategy. Specifically, given the noisy 5 Figure 4: Based on the video DiT model, we design unique 4D motion attention to leverage 4D motion tokens as context for vision generation. To better capture spatio-temporal relationships, we apply 4D RoPE over (t, x, y, z) coordinates. To further improve the generation quality and generalization, we use learnable unconditional tokens for motion-aware classifier-free guidance. video latents {zt}f t=0 Rf chw and the reference image latent zref Rchw obtained from frozen shared VAE encoder, we compute the composite vision latents in the following formulation: zvision = Concat (z0, Repeat(zref, )) Rf 2chw (4) These concatenated latents are then patchified and projected to match the attention token dimension. Thanks to the 3D full self-attention in DiT, the model can directly interact with reference image features during generation, thus preserving identity efficiently without extra reference networks. 4D Positional Encoding To enhance the spatio-temporal information of 4D motion tokens, we introduce concise 4D RoPE, which combines 1D temporal and 3D spatial RoPE. Unlike the standard 3D formulation [30, 33], our 4D RoPE captures preferable positional information of 4D motion: P3D = Concat(Rt, Rh, Rw) P4D = Concat(Rt, Rx, Ry, Rz) (5) where each implements 1D rotary embeddings [76] and repeat on other dimensions. The 3D (x, y, z) coordinates are derived from joint position global statistics in our curated 5K SMPL motionvideo dataset, providing unified and crucial human-shape positional information. Meanwhile, the 1D RoPE use frame index as position and offers temporal cues. Each RoPE contributes quarter of the total attention head dimension. More details of our 4D RoPE are provided in Appendix E. 4D Motion Attention To effectively leverage motion tokens zmotion (obtained from Section 3.1) as context for vision tokens zvision, We design 4D motion attention (Figure 4), where vision tokens are queries and 4D motion tokens are keys and values. The attention mechanism is formulated as: Attention(Q, K, V) = Softmax (cid:18) QKT dk (cid:19) Q = RoPE(LayerNorm(Wq(zvision), P3D) = RoPE(LayerNorm(Wk(zmotion), P4D) = LayerNorm(Wv(zmotion)) (6) (7) (8) (9) where Wq, Wk, Wv Rdd are learnable projection matrices, P3D, P4D are 3D and 4D RoPE for vision tokens zvision and motion tokens zmotion, respectively. And the RoPE formulation follows Equation 1. The 4D motion attention output is combined with the standard 3D full self-attention via residual connection, enabling motion-aware modulation while maintaining spatio-temporal coherence. 6 Table 1: Quantitative results on the TikTok [37] dataset. Method Video Metrics Image Metrics FVD FID-VID PSNR SSIM LPIPS FID MRAA [58] DreamPose [5] MagicAnimate [19] DisCo [62] Animate Anyone [15] Champ [18] Unianimate [63] MimicMotion [21] ControlNeXt [22] StableAnimator [17] Animate Anyone 2 [16] Human-DiT [20] MTVCrafter (Ours) 468.66 551.02 179.07 292.80 171.90 160.82 148.06 423.17 398.32 253.69 144.65 237.00 140. 71.97 78.77 21.75 59.90 65.98 21.07 - 20.64 24.29 22.79 - 24.30 6.98 18.14 12.82 - 16.55 17.23 - 20.58 19.21 18.52 18.12 - 20.50 19.37 0.646 0.511 0.714 0.668 0.718 0.802 0.811 0.759 0.763 0.771 0.778 0.815 0. 0.337 0.442 0.239 0.292 0.285 0.234 0.231 0.232 0.246 0.257 0.248 0.220 0.217 85.49 72.62 32.09 30.75 78.94 - - 35.62 39.66 40.17 - 41.60 19.46 Motion-aware Classifier-free Guidance To further improve generation quality and generalization, we introduce motion-aware classifier-free guidance (CFG). Traditional CFG is typically used for text/image condition with well-defined unconditional input (e.g., empty text or zero image), following ˆϵθ = ϵθ(zt, t, c) + w(ϵθ(zt, t, ct) ϵθ(zt, t, c)) where ϵθ denotes the denoising network, zt is the noisy latent at timestep t, and is the CFG scale controlling the strength of conditioning. When = 0, the generation is fully unconditional; when = 1, it is fully conditional on ct. Since motion tokens lack natural unconditional forms, we use learnable unconditional motion tokens that match the feature dimension of zmotion. During training, ct is randomly replaced by with predefined probability (i.e., is only updated when used). This enables joint learning of both conditioned and unconditioned generation, enhancing model robustness and controllability."
        },
        {
            "title": "4 Experiments",
            "content": "Datasets and Metrics Following prior works [2, 21, 62], we use sequences 335 to 340 in the TikTok [37] dataset for testing. The evaluation is based on six metrics: image-level metrics including Peak Signal-to-Noise Ratio (PSNR) [82], Structural Similarity Index Measure (SSIM) [83], Learned Perceptual Image Patch Similarity (LPIPS) [84], Fréchet Inception Distance (FID) [85]; and videolevel metrics including Video-level FID (FID-VID) [86], Fréchet Video Distance (FVD) [87]. Implementation Details For 4DMoT, We use codebook with size of 8,192 and code dimension of 3072. Quantization is performed using an exponential moving average (EMA) update strategy, with decay constant of λ = 0.99. To maintain codebook utilization, unused codes are periodically reset every 20 steps. The sliding window size is configured as 8. The entire VQVAE model is trained from scratch using the AdamW optimizer with β1 = 0.9, β2 = 0.99, weight decay of 1 104, and batch size of 32 per GPU. The commitment loss ratio in Equation 3 is set to 0.25. We train for 200K iterations with learning rate of 2 104, followed by an additional 100K iterations with reduced learning rate of 1 105. For MV-DiT, we adopt the DiT-based CogVideoX-5B-T2V [33] as our base model. During training, the drop probability of motion condition is set to 0.25 and the input video clips are cropped to 49 continuous frames. All modules except the 3D VAE and 4D motion tokenizer are trainable, resulting in total of approximately 7B trainable parameters. We optimize the model using the AdamW optimizer with β1 = 0.9, β2 = 0.99, weight decay of 1 102, and batch size of 4 per GPU. The model is trained for 20K iterations (8 H100 days) with learning rate of 1 105. During inference, the CFG scale for motion condition is set to 3.0 to balance condition fidelity and generation quality. All experiments are conducted on 8 NVIDIA H100 GPUs. Additional details concerning the model architecture and evaluation on the TikTok are provided in Appendix A."
        },
        {
            "title": "4.1 SOTA Comparison",
            "content": "We conduct both qualitative and quantitative comparisons with existing methods. For qualitative comparison, as shown in Figure 1 and 5, our MTVCrafter demonstrates the best animation perfor7 Figure 5: Qualitative comparison with existing methods. Our MTVCrafter consistently demonstrates the best identity preservation, motion accuracy, visual realism, and frame smoothness. mance in terms of pose accuracy and identity consistency. Furthermore, MTVCrafter exhibits strong generalization ability, handling single or multiple characters, full-body or half-body appearances, and diverse styles, motions, and scenes. Importantly, MTVCrafter remains robust even when the target pose is misaligned with the reference image (e.g., Cowboy in Figure 5), indicating its effective disentanglement of motion from the driving video. This issue cannot be fundamentally addressed by Champ [18] or UniAnimate [63] that naively attempt to retarget the pose to match the reference images scale. For quantitative comparison, Table 1 shows that MTVCrafter achieves superior performance across all metrics on TikTok, particularly for FID and FID-VID. This highlights the advantages of directly modeling motion sequences instead of rendered pose images. For SSIM and PSNR, the results are similar across methods and are less significant, as these are low-level metrics for tasks like image super-resolution. More comparisons and visualizations are in Appendix G."
        },
        {
            "title": "4.2 Ablation Study",
            "content": "To validate the effectiveness of our key designs, we conduct ablation studies on the 4D Motion Tokenizer (MT), 4D Motion Attention (MA), and CFG. As shown in Table 2, we evaluate different variants by modifying or removing specific components and measure their impact on TikTok. Motion Tokenizer (MT) We investigate the effect of removing the vector quantizer. Without quantization, the VQVAE degenerates into standard autoencoder that directly processes continuous and inconsistent motion features, resulting in degraded performance (i.e., FID-VID 9.79 vs. 6.98 in Table 2). This confirms that using discrete and unified motion tokens is crucial for stabilizing motion learning. Besides, the quantization also helps improve generalization for open-world animation. more systematic analysis of our 4D motion tokenizer is provided in Appendix D. Motion Attention (MA) We explore multiple positional encoding (PE) designs for the motion attention module: (1) Dynamic PE computes RoPE using the first-frame joint coordinates, but 8 Table 2: Ablation study on 4D motion tokenizer (MT) and 4D motion attention (MA). Module Choice Video Metrics Image Metrics FVD FID-VID PSNR SSIM LPIPS FID 4D MT (1) w/o quantize 4D MA (2) w/ dynamic PE (3) w/ learnable PE (4) w/ temporal RoPE (5) w/ spatial RoPE (6) w/o PE (7) our default design 142.89 206.18 209.16 236.02 231.94 235.57 140.60 9.79 14.94 11.24 13.83 16.48 14.15 6. 17.97 17.34 16.75 16.69 16.63 17.00 19.37 0.745 0.731 0.721 0.723 0.722 0.717 0. 0.249 0.265 0.277 0.269 0.270 0.273 0.217 19.72 23.33 22.12 21.77 22.45 21.01 19. Figure 6: Visual comparison of ablated choices. The numbers below images correspond to the rows in Table 2. The original design (denoted as the last image) achieves the best visual performance. Figure 7: Ablation of motion-aware CFG scale w. Higher CFG scale leads to better pose alignment, but also introduce more artifacts. In our experiments, scale of 3.0 achieves the best trade-off. performs poorly due to instability and training difficulties; (2) Learnable PE struggles to converge and fails to provide reliable positional cues; (3) 1D temporal RoPE applies RoPE only along the temporal axis, and (4) 3D spatial RoPE applies RoPE only along the spatial axis. Both fail to model full 4D dependencies, resulting in visual artifacts like identity drift or jittering; (5) w/o PE removes positional encoding entirely, yielding the worst performance overall (FVD: 235.57 vs. 140.60, SSIM: 0.717 vs. 0.784), highlighting the importance of explicit positional information. To better illustrate the effects, we provide visual ablations in Figure 6. It vividly demonstrates the effectiveness of the tokenizer and 4D RoPE, leading to improved motion quality and character fidelity. Motion-aware Classifier-free Guidance (CFG) Figure 7 presents the qualitative and quantitative evaluations of our motion-aware CFG scale w. On the TikTok benchmark, CFG scale of 3.0 yields the best performance, particularly for the FVD metric. For the FID-VID metric, the scale appears to have minimal impact. For visual comparisons on the right, increasing the CFG scale enhances pose alignment, but it also introduces more artifacts and potentially degrades video quality."
        },
        {
            "title": "5 Conclusion",
            "content": "We introduce MTVCrafter, novel framework that directly tokenizes raw motion sequences instead of relying on 2D-rendered pose images for human video generation. By integrating 4D motion VQVAE and motion attention within DiT, MTVCrafter effectively preserves spatio-temporal coherence and identity fidelity, while decoupling character and motion. Experiments show SOTA performance and strong generalization across diverse characters and motions, setting new paradigm in this field."
        },
        {
            "title": "References",
            "content": "[1] Di Chang, Hongyi Xu, You Xie, Yipeng Gao, Zhengfei Kuang, Shengqu Cai, Chenxu Zhang, Guoxian Song, Chao Wang, Yichun Shi, et al. X-dyna: Expressive dynamic human image animation. arXiv preprint arXiv:2501.10021, 2025. [2] Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing Yan, Yizhe Zhu, Xiao Yang, and Mohammad Soleymani. Magicpose: Realistic human poses and facial expressions retargeting with identity-aware diffusion. arXiv preprint arXiv:2311.12052, 2023. [3] Zunnan Xu, Zhentao Yu, Zixiang Zhou, Jun Zhou, Xiaoyu Jin, Fa-Ting Hong, Xiaozhong Ji, Junwei Zhu, Chengfei Cai, Shiyu Tang, et al. Hunyuanportrait: Implicit condition control for enhanced portrait animation. arXiv preprint arXiv:2503.18860, 2025. [4] Yifang Men, Yuan Yao, Miaomiao Cui, and Liefeng Bo. Mimo: Controllable character video synthesis with spatial decomposed modeling. arXiv preprint arXiv:2409.16160, 2024. [5] Johanna Karras, Aleksander Holynski, Ting-Chun Wang, and Ira Kemelmacher-Shlizerman. Dreampose: Fashion image-to-video synthesis via stable diffusion. In 2023 IEEE/CVF International Conference on Computer Vision (ICCV), pages 2262322633. IEEE, 2023. [6] Jingkai Zhou, Benzhi Wang, Weihua Chen, Jingqi Bai, Dongyang Li, Aixi Zhang, Hao Xu, Mingyang Yang, and Fan Wang. Realisdance: Equip controllable character animation with realistic hands. arXiv preprint arXiv:2409.06202, 2024. [7] Di Chang, Yichun Shi, Quankai Gao, Jessica Fu, Hongyi Xu, Guoxian Song, Qing Yan, Xiao Yang, and Mohammad Soleymani. Magicdance: Realistic human dance video generation with motions & facial expressions transfer. CoRR, 2023. [8] Yatian Pang, Bin Zhu, Bin Lin, Mingzhe Zheng, Francis EH Tay, Ser-Nam Lim, Harry Yang, and Li Yuan. Dreamdance: Animating human images by enriching 3d geometry cues from 2d poses. arXiv preprint arXiv:2412.00397, 2024. [9] Yujia Lin, Liming Chen, Aftab Ali, Christopher Nugent, Ian Cleland, Rongyang Li, Jianguo Ding, and Huansheng Ning. Human digital twin: survey. Journal of Cloud Computing, 13(1):131, 2024. [10] Martin Wolfgang Lauer-Schmaltz, Philip Cash, John Paulin Hansen, and Anja Maier. Towards the human digital twin: Definition and designa survey. arXiv preprint arXiv:2402.07922, 2024. [11] Tasin Islam, Alina Miron, Xiaohui Liu, and Yongmin Li. Deep learning in virtual try-on: comprehensive survey. IEEE Access, 2024. [12] Dan Song, Xuanpu Zhang, Juan Zhou, Weizhi Nie, Ruofeng Tong, Mohan Kankanhalli, and An-An Liu. Image-based virtual try-on: survey. International Journal of Computer Vision, pages 129, 2024. [13] Vinay Chamola, Siva Sai, Animesh Bhargava, Ashis Sahu, Wenchao Jiang, Zehui Xiong, Dusit Niyato, and Amir Hussain. comprehensive survey on generative ai for metaverse: enabling immersive experience. Cognitive Computation, 16(6):32863315, 2024. [14] Hua Xuan Qin and Pan Hui. Empowering the metaverse with generative ai: Survey and future directions. In 2023 IEEE 43rd international conference on distributed computing systems workshops (ICDCSW), pages 8590. IEEE, 2023. [15] Li Hu. Animate anyone: Consistent and controllable image-to-video synthesis for character animation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 81538163, 2024. [16] Li Hu, Guangyuan Wang, Zhen Shen, Xin Gao, Dechao Meng, Lian Zhuo, Peng Zhang, Bang Zhang, and Liefeng Bo. Animate anyone 2: High-fidelity character image animation with environment affordance. arXiv preprint arXiv:2502.06145, 2025. [17] Shuyuan Tu, Zhen Xing, Xintong Han, Zhi-Qi Cheng, Qi Dai, Chong Luo, and Zuxuan Wu. Stableanimator: High-quality identity-preserving human image animation. arXiv preprint arXiv:2411.17697, 2024. [18] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Zilong Dong, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In European Conference on Computer Vision, pages 145162. Springer, 2024. 10 [19] Zhongcong Xu, Jianfeng Zhang, Jun Hao Liew, Hanshu Yan, Jia-Wei Liu, Chenxu Zhang, Jiashi Feng, and Mike Zheng Shou. Magicanimate: Temporally consistent human image animation using diffusion model. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 14811490, 2024. [20] Qijun Gan, Yi Ren, Chen Zhang, Zhenhui Ye, Pan Xie, Xiang Yin, Zehuan Yuan, Bingyue Peng, and Jianke Zhu. Humandit: Pose-guided diffusion transformer for long-form human motion video generation. arXiv preprint arXiv:2502.04847, 2025. [21] Yuang Zhang, Jiaxi Gu, Li-Wen Wang, Han Wang, Junqi Cheng, Yuefeng Zhu, and Fangyuan Zou. Mimicmotion: High-quality human motion video generation with confidence-aware pose guidance. arXiv preprint arXiv:2406.19680, 2024. [22] Bohao Peng, Jian Wang, Yuechen Zhang, Wenbo Li, Ming-Chang Yang, and Jiaya Jia. Controlnext: Powerful and efficient control for image and video generation. arXiv preprint arXiv:2408.06070, 2024. [23] Seyed Rohollah Hosseyni, Ali Ahmad Rahmani, Seyed Jamal Seyedmohammadi, Sanaz Seyedin, and Arash Mohammadi. Bad: Bidirectional auto-regressive diffusion for text-to-motion generation. In ICASSP 2025-2025 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages 15. IEEE, 2025. [24] Chuan Guo, Yuxuan Mu, Muhammad Gohar Javed, Sen Wang, and Li Cheng. Momask: Generative masked modeling of 3d human motions. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 19001910, 2024. [25] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. Advances in Neural Information Processing Systems, 36:2006720079, 2023. [26] Matthias Plappert, Christian Mandery, and Tamim Asfour. The kit motion-language dataset. Big data, 4(4):236252, 2016. [27] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 51525161, 2022. [28] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan Gomez, Łukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems, 30, 2017. [29] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF international conference on computer vision, pages 41954205, 2023. [30] Weijie Kong, Qi Tian, Zijian Zhang, Rox Min, Zuozhuo Dai, Jin Zhou, Jiangfeng Xiong, Xin Li, Bo Wu, Jianwei Zhang, et al. Hunyuanvideo: systematic framework for large video generative models. arXiv preprint arXiv:2412.03603, 2024. [31] Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all. arXiv preprint arXiv:2412.20404, 2024. [32] István Sárándi and Gerard Pons-Moll. Neural localizer fields for continuous 3d human pose and shape estimation. arXiv preprint arXiv:2407.07532, 2024. [33] Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. [34] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical image computing and computer-assisted interventionMICCAI 2015: 18th international conference, Munich, Germany, October 5-9, 2015, proceedings, part III 18, pages 234241. Springer, 2015. [35] Andreas Blattmann, Tim Dockhorn, Sumith Kulal, Daniel Mendelevitch, Maciej Kilian, Dominik Lorenz, Yam Levi, Zion English, Vikram Voleti, Adam Letts, et al. Stable video diffusion: Scaling latent video diffusion models to large datasets. arXiv preprint arXiv:2311.15127, 2023. [36] Uriel Singer, Adam Polyak, Thomas Hayes, Xi Yin, Jie An, Songyang Zhang, Qiyuan Hu, Harry Yang, Oron Ashual, Oran Gafni, et al. Make-a-video: Text-to-video generation without text-video data. arXiv preprint arXiv:2209.14792, 2022. 11 [37] Yasamin Jafarian and Hyun Soo Park. Learning high fidelity depths of dressed humans by watching social media dance videos. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1275312762, 2021. [38] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. pmlr, 2015. [39] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [40] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denoising diffusion implicit models. arXiv preprint arXiv:2010.02502, 2020. [41] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [42] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. Advances in Neural Information Processing Systems, 35:86338646, 2022. [43] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139 144, 2020. [44] Minhyeok Lee and Junhee Seok. Controllable generative adversarial network. Ieee Access, 7:2815828169, 2019. [45] Tao Xu, Pengchuan Zhang, Qiuyuan Huang, Han Zhang, Zhe Gan, Xiaolei Huang, and Xiaodong He. Attngan: Fine-grained text to image generation with attentional generative adversarial networks. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 13161324, 2018. [46] Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Müller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning, 2024. [47] Dustin Podell, Zion English, Kyle Lacey, Andreas Blattmann, Tim Dockhorn, Jonas Müller, Joe Penna, and Robin Rombach. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952, 2023. [48] Lvmin Zhang, Anyi Rao, and Maneesh Agrawala. Adding conditional control to text-to-image diffusion models. In Proceedings of the IEEE/CVF international conference on computer vision, pages 38363847, 2023. [49] Zhenghao Zhang, Junchao Liao, Menghao Li, Zuozhuo Dai, Bingxue Qiu, Siyu Zhu, Long Qin, and Weizhi Wang. Tora: Trajectory-oriented diffusion transformer for video generation. arXiv preprint arXiv:2407.21705, 2024. [50] FU Xiao, Xian Liu, Xintao Wang, Sida Peng, Menghan Xia, Xiaoyu Shi, Ziyang Yuan, Pengfei Wan, Di Zhang, and Dahua Lin. 3dtrajmaster: Mastering 3d trajectory for multi-entity motion in video generation. In The Thirteenth International Conference on Learning Representations, 2024. [51] Chen Hou, Guoqiang Wei, Yan Zeng, and Zhibo Chen. Training-free camera control for video generation. arXiv preprint arXiv:2406.10126, 2024. [52] Hao He, Yinghao Xu, Yuwei Guo, Gordon Wetzstein, Bo Dai, Hongsheng Li, and Ceyuan Yang. Cameractrl: Enabling camera control for text-to-video generation. arXiv preprint arXiv:2404.02101, 2024. [53] Yanbo Ding, Shaobin Zhuang, Kunchang Li, Zhengrong Yue, Yu Qiao, and Yali Wang. Muses: 3dcontrollable image generation via multi-modal agent collaboration. arXiv preprint arXiv:2408.10605, 2024. [54] Weixi Feng, Wanrong Zhu, Tsu-jui Fu, Varun Jampani, Arjun Akula, Xuehai He, Sugato Basu, Xin Eric Wang, and William Yang Wang. Layoutgpt: Compositional visual planning and generation with large language models. Advances in Neural Information Processing Systems, 36:1822518250, 2023. [55] Yuxin Zhang, Dandan Zheng, Biao Gong, Jingdong Chen, Ming Yang, Weiming Dong, and Changsheng Xu. Lumisculpt: consistency lighting control network for video generation. arXiv preprint arXiv:2410.22979, 2024. [56] Chong Zeng, Yue Dong, Pieter Peers, Youkang Kong, Hongzhi Wu, and Xin Tong. Dilightnet: Fine-grained lighting control for diffusion-based image generation. In ACM SIGGRAPH 2024 Conference Papers, pages 112, 2024. [57] Zhichao Huang, Xintong Han, Jia Xu, and Tong Zhang. Few-shot human motion transfer by personalized geometry and texture modeling. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 22972306, 2021. [58] Aliaksandr Siarohin, Oliver Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. Motion representations for articulated animation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1365313662, 2021. [59] Aliaksandr Siarohin, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. First order motion model for image animation. Advances in neural information processing systems, 32, 2019. [60] Yining Li, Chen Huang, and Chen Change Loy. Dense intrinsic appearance flow for human pose transfer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 36933702, 2019. [61] Yuxuan Luo, Zhengkun Rong, Lizhen Wang, Longhao Zhang, Tianshu Hu, and Yongming Zhu. Dreamactorm1: Holistic, expressive and robust human image animation with hybrid guidance. arXiv preprint arXiv:2504.01724, 2025. [62] Tan Wang, Linjie Li, Kevin Lin, Yuanhao Zhai, Chung-Ching Lin, Zhengyuan Yang, Hanwang Zhang, Zicheng Liu, and Lijuan Wang. Disco: Disentangled control for realistic human dance generation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 93269336, 2024. [63] Xiang Wang, Shiwei Zhang, Changxin Gao, Jiayu Wang, Xiaoqiang Zhou, Yingya Zhang, Luxin Yan, and Nong Sang. Unianimate: Taming unified video diffusion models for consistent human image animation. arXiv preprint arXiv:2406.01188, 2024. [64] Albert Gu and Tri Dao. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. [65] Shige Peng. Stochastic hamiltonjacobibellman equations. SIAM Journal on Control and Optimization, 30(2):284304, 1992. [66] Martino Bardi, Italo Capuzzo Dolcetta, et al. Optimal control and viscosity solutions of Hamilton-JacobiBellman equations, volume 12. Springer, 1997. [67] Shuai Tan, Biao Gong, Xiang Wang, Shiwei Zhang, Dandan Zheng, Ruobing Zheng, Kecheng Zheng, Jingdong Chen, and Ming Yang. Animate-x: Universal character image animation with enhanced motion representation. arXiv preprint arXiv:2410.10306, 2024. [68] Zhendong Yang, Ailing Zeng, Chun Yuan, and Yu Li. Effective whole-body pose estimation with twostages distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 42104220, 2023. [69] Zhe Cao, Gines Hidalgo, Tomas Simon, Shih-En Wei, and Yaser Sheikh. Openpose: Realtime multiperson 2d pose estimation using part affinity fields. IEEE transactions on pattern analysis and machine intelligence, 43(1):172186, 2019. [70] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed AA Osman, Dimitrios Tzionas, and Michael Black. Expressive body capture: 3d hands, face, and body from single image. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1097510985, 2019. [71] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael Black. Smpl: skinned multi-person linear model, 2023. [72] Lihe Yang, Bingyi Kang, Zilong Huang, Xiaogang Xu, Jiashi Feng, and Hengshuang Zhao. Depth anything: Unleashing the power of large-scale unlabeled data. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1037110381, 2024. [73] Diederik Kingma, Max Welling, et al. An introduction to variational autoencoders. Foundations and Trends in Machine Learning, 12(4):307392, 2019. 13 [74] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and Peter Liu. Exploring the limits of transfer learning with unified text-to-text transformer. Journal of machine learning research, 21(140):167, 2020. [75] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PmLR, 2021. [76] Jianlin Su, Murtadha Ahmed, Yu Lu, Shengfeng Pan, Wen Bo, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. Neurocomputing, 568:127063, 2024. [77] Jonathan Ho and Tim Salimans. Classifier-free diffusion guidance. arXiv preprint arXiv:2207.12598, 2022. [78] Polina Zablotskaia, Aliaksandr Siarohin, Bo Zhao, and Leonid Sigal. Dwnet: Dense warp-based network for pose-guided human video generation. arXiv preprint arXiv:1910.09139, 2019. [79] Yuan Wang, Di Huang, Yaqi Zhang, Wanli Ouyang, Jile Jiao, Xuetao Feng, Yan Zhou, Pengfei Wan, Shixiang Tang, and Dan Xu. Motiongpt-2: general-purpose motion-language model for motion generation and understanding. arXiv preprint arXiv:2410.21747, 2024. [80] Chuofan Ma, Yi Jiang, Junfeng Wu, Jihan Yang, Xin Yu, Zehuan Yuan, Bingyue Peng, and Xiaojuan Qi. Unitok: unified tokenizer for visual generation and understanding. arXiv preprint arXiv:2502.20321, 2025. [81] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan. Generating human motion from textual descriptions with discrete representations. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1473014740, 2023. [82] Alain Hore and Djemel Ziou. Image quality metrics: Psnr vs. ssim. In 2010 20th international conference on pattern recognition, pages 23662369. IEEE, 2010. [83] Zhou Wang, Alan Bovik, Hamid Sheikh, and Eero Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing, 13(4):600612, 2004. [84] Richard Zhang, Phillip Isola, Alexei Efros, Eli Shechtman, and Oliver Wang. The unreasonable effectiveness of deep features as perceptual metric. In Proceedings of the IEEE conference on computer vision and pattern recognition, pages 586595, 2018. [85] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. [86] Yogesh Balaji, Martin Renqiang Min, Bing Bai, Rama Chellappa, and Hans Peter Graf. Conditional gan with discriminative filter generation for text-to-video synthesis. In IJCAI, volume 1, page 2, 2019. [87] Thomas Unterthiner, Sjoerd Van Steenkiste, Karol Kurach, Raphael Marinier, Marcin Michalski, and Sylvain Gelly. Towards accurate generative models of video: new metric & challenges. arXiv preprint arXiv:1812.01717, 2018. [88] Wentao Zhu, Yufang Huang, Xiufeng Xie, Wenxian Liu, Jincan Deng, Debing Zhang, Zhangyang Wang, and Ji Liu. Autoshot: short video dataset and state-of-the-art shot boundary detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), 2023. [89] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022. [90] Haofei Xu, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, Fisher Yu, Dacheng Tao, and Andreas Geiger. Unifying flow, stereo and depth estimation. IEEE Transactions on Pattern Analysis and Machine Intelligence, 2023. [91] Youngmin Baek, Bado Lee, Dongyoon Han, Sangdoo Yun, and Hwalsuk Lee. Character region awareness for text detection. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 93659374, 2019. [92] Lei Zhu, Fangyun Wei, Yanye Lu, and Dong Chen. Scaling the codebook size of vqgan to 100,000 with utilization rate of 99%. arXiv preprint arXiv:2406.11837, 2024."
        },
        {
            "title": "A More Implementation Details",
            "content": "Our 4D motion tokenizer consists of an encoder, quantizer, and decoder. For encoder, it begins with convolutional input layer that projects the input channels from 3 to 32, followed by three ResNet blocks and downsampling blocks, with channel dimensions of (32, 128, 512), frame downsampling rates (2, 2, 1), and joint downsampling rates (1, 1, 1). final convolutional output layer maps the features to code dimension of 3072. For quantizer, We use codebook with size of 8,192 and code dimension of 3072. For decoder, it starts with convolutional input layer that projects the code dimension from 3072 to 512, followed by three ResNet blocks and upsampling blocks symmetric to those in the encoder. final convolutional output layer maps the features back to 3 channels. The overall number of trainable parameters in our 4DMoT is approximately 50M. For MV-DiT, We adopt CogVideoX-5B-T2V [33] as our base model due to its strong performance and suitable model capacity. To better accommodate motion-centric generation, we remove the original text processing branch and add our proposed 4D Motion Attention layer after the self-attention layer into each CogVideoX block, This integration results in 42 motion attention layers across the DiT. The overall number of trainable parameters in our MV-DiT is approximately 7B. For evaluation, frames are resized while preserving the aspect ratio and then center cropped to 512 512. We adopt clip-by-clip inference strategy and concatenate the resulting clips temporally to form the final long video. We adopt DDIM [40] for sampling, performing 50 inference steps in approximately 90 seconds on single NVIDIA H100 GPU. For other competing methods, we use the same evaluation setting when their original papers did not report all metrics on the TikTok [37] benchmark, such as StableAnimator [17] and MimicMotion [21]. For methods whose codes are not publicly available, we report the results from their original papers, such as Animate Anyone 2 [16] and Human-DiT [20]."
        },
        {
            "title": "B More Details of Dataset Curation",
            "content": "We construct our dataset through multi-stage curation pipeline, combining shot segmentation, pose estimation, and quality-based filtering. The detailed procedure is described as follows: Shot segmentation. We use AutoShot [88], an automated shot boundary detection algorithm, to detect shot boundaries and segment raw videos into coherent, temporally continuous shots. This step is critical to eliminate abrupt scene changes and ensure that each resulting clip maintains smooth temporal coherence, providing reliable foundation for subsequent quality filtering operations. Pose estimation. For each segmented clip, we use NLF-Pose [32] to estimate frame-wise SMPL [71] parameters. Specifically, we extract the 3D joint rotations θt R243 for 24 body joints, along with the corresponding confidence scores for each joint. These confidence scores reflect the uncertainties of the predicted poses. We further convert the estimated joint rotation parameters into 3D joint positions using forward kinematics [70] based on the SMPL model. Single-person sub-clip extraction. Unfortunately, the current version of MTVCrafter cannot support multi-person animations with different poses. Thus, we focus on extracting continuous sub-clips ( >=49 frames) from each video containing only single human pose with valid predictions across all frames. In other words, frames with no pose or multiple poses detected are excluded. Pose uncertainty filtering. we compute the average of the maximum joint uncertainty across all frames for each video. Videos within the top 10% highest average uncertainty are discarded, as they are likely to contain unreliable pose estimations that could significantly impact the learning process. Visual and motion quality assessment. For the remaining clips, we evaluate four complementary metrics to assess overall quality: (1) Aesthetic score: we use the LAION-Aesthetics predictor [89], which is linear estimator built on top of CLIP [75], to predict the aesthetic quality of images. (2) Optical flow magnitude: we use the UniMatch model [90] to compute the optical flow between frames, assessing the extent of motion. (3) Laplacian blur score: we apply the Laplacian operator using OpenCV3 to detect blurry frames. (4) OCR text ratio: we use CRAFT [91] to detect text regions and estimate the proportion of text within each frame, filtering out clips dominated by textual content. 3OpenCV: https://docs.opencv.org/3.4/d5/db5/tutorial_laplace_operator.html 15 Figure 8: Limitations and failure cases. (1) Incorrect generation occurs when the reference characters proportions deviate significantly from normal human anatomy, as the training data lacks non-human figures. (2) Precise hand control is challenging due to insufficient detailed hand supervision. The thresholds for these metrics are set to 5.0, 2.0, 100, and 0.05, respectively. Clips that fail to meet any of the above quality thresholds are discarded. Through this rigorous filtering process, we obtain final dataset of 5K high-quality motion video clips featuring clear frames with minimal textual content, continuous and consistent single-person motions, and smooth temporal transitions."
        },
        {
            "title": "C Limitations and Discussion",
            "content": "While MTVCrafter achieves impressive performance across diverse scenarios, it still presents certain limitations, as shown in Figure 8. First, the model may generate inaccurate results when the reference character exhibits extreme body proportions or non-human anatomy. This limitation arises due to the scarcity of such examples in the training dataset. Second, precise hand articulation remains challenge, as clear and detailed hand motion is underrepresented in our SMPL motion-video dataset. In addition to these technical limitations, we recognize broader concerns in the use of MTVCrafter, such as potential misuse involving unauthorized identity manipulation or violation of data copyrights, especially when animating reference images sourced from social platforms. Besides, MTVCrafter must not be misused to fabricate harmful, misleading, or disrespectful content, such as mocking individuals or distorting artistic and cultural heritage. We request the responsible use of MTVCrafter and plan to adopt safeguards such as user consent verification and watermarking, especially in commercial or public-facing applications. We will try our best to mitigate potential misuse risks. Systematic Analysis of 4D Motion Tokenizer To evaluate the efficiency of our codebook utilization, we conduct statistical inference on test set comprising 6400 motion samples. These samples are randomly selected. The codes are categorized into three usage frequency levels based on predefined thresholds: underutilized (<1%), active (1%-15%), and frequent (>15%) as shown in Figure 9 (a). The low frequency of frequent codes (2.9%) reflects an efficient selection of core features for reconstruction, minimizing overfitting to local training patterns. The broad distribution of active codes (66.8%) ensures expressive diversity, allowing the model to capture wide range of patterns and preventing homogenization of the reconstruction. Meanwhile, moderate redundancy of underutilized codes (30.3%) improves the robustness of the tokenziation process, allowing the model to support richer feature combinations. This percent (i.e., 30.0%) is much lower than the extreme unused code rates in VQ-VAE (often >50% with large codebook size [92]). This suggests effective codebook optimization via gradient updates. Moreover, we conduct comprehensive analysis of the codebooks latent space to assess the diversity of its entries. Specifically, we computed pairwise cosine similarities between codes and visualized their distribution as shown in Figure 9 (b). The results show that most code pairs exhibit near-zero Figure 9: Quantitative analysis of the VQVAE codebook. The left panel demonstrates that up to 69.7% of the codes remain active during inference, indicating efficient utilization of the encoding space. The right figure shows that the cosine similarity of most code pairs is close to 0, confirming the models ability to learn discrete latent space characterized by highly decorrelated representations. Figure 10: Reconstruction performance of our motion VQVAE on unseen Gymnastics data. Each group consists of three images: the original image (first column), the extracted original pose (second column), and the reconstructed pose (third column). All poses are visualized as 3D joint skeletons, projected into 2D image space using joint coordinates. Our motion VQVAE demonstrates strong generalization to unseen motion data and achieves accurate and robust reconstruction quality. similarity, indicating significantly uncorrelated characteristics. This finding confirms that the model successfully constructs discrete latent space with high representational independence. To directly assess the effectiveness, we evaluate the reconstruction quality of the motion VQVAE on unseen gymnastics motion sequences, which represent challenging and highly dynamic test case. As illustrated in Figure 10, our model can accurately reconstruct complex human poses, even in highly dynamic motion scenarios. All results are visualized as 3D joint skeletons rendered in 2D image-pixel space. The reconstructed poses closely match the original inputs, demonstrating the VQVAEs strong generalization capability and its ability to preserve spatio-temporal structure. The results from these three experimental groups collectively demonstrate the effectiveness and suitability of the proposed 4D motion tokenizer for the downstream human image animation task. Figure 11: (a) We visualize the cross-attention maps at different Transformer blocks. Our 4D RoPE enables effective and structured interactions between motion and vision tokens. (b) We visualize the mean 3D joint positions across the dataset, which are used to compute the 4D RoPE. This averaged representation provides typical spatial cues that facilitate consistent cross-modal modulation. Figure 12: Training loss curves of the 4D motion tokenizer and 4D motion-guided video DiT model. The tokenizer demonstrates smooth convergence with decreasing reconstruction and commitment loss, while the video DiT model gradually learns motion-aware video generation. More Details of 4D Motion RoPE Since the tokenization disrupted the original spatio-temporal relationships of 4D motion, we introduce 4D Rotary Positional Encoding (4D RoPE). For each motion token, we compute its positional encoding based on the corresponding 4D coordinate (t, x, y, z), where denotes the frame index. The spatial coordinates (x, y, z) are centralized by subtracting the global mean joint position, which is computed over the entire dataset by averaging all joints across all frames along the joint axis. This centralization ensures that the positional encoding remains consistent and invariant to global spatial shifts. For each of the four dimensions (t, x, y, z), we compute sinusoidal RoPE features independently according to Equation 1, with each contributing quarter of the total attention head dimension, i.e., D/4. Temporal RoPE features are then broadcast across all joints, while spatial RoPE features are broadcast across all frames. This ensures that each motion token is equipped with the corresponding and structured 4D positional encoding, enabling precise modeling of both motion dynamics and spatial structure. The detailed procedure is described in Algorithm 1. To visualize the advantages of our proposed design, Figure 11 (a) presents the cross-attention maps in different attention layers. The vertical axis represents motion tokens across frames and joints, while the horizontal axis corresponds to vision tokens across frames and pixels. When positional encoding is omitted, attention maps tend to be structureless, indicating difficulty in capturing useful relationships. In contrast, when our 4D Rotary Position Embedding (RoPE) is applied, the attention patterns become increasingly structured across layers, suggesting that the model benefits from explicit spatio-temporal positional cues, enabling effective interaction between vision and motion representations. Algorithm 1 4D RoPE of Motion Tokens Require: Dataset-wide mean joint positions mean_joints RJ3, number of latent frames after 4 downsampling, and attention head dimension D. Extract spatial coordinates: mean_joints[:, 0] mean_joints[:, 1] mean_joints[:, 2] {0, 1, . . . , 1} Centralize spatial positions: ˆx mean(x) ˆy mean(y) ˆz mean(z) Compute 1D RoPE for each axis (see Equation 1): (cost, sint) RT (D/4)2 RoPE(t, D/4) (cosx, sinx) RJ(D/4)2 RoPE(ˆx, D/4) (cosy, siny) RJ(D/4)2 RoPE(ˆy, D/4) (cosz, sinz) RJ(D/4)2 RoPE(ˆz, D/4) Broadcast time RoPE over all joints: (cost, sint) RT J(D/4)2 Repeat((cost, sint), dim = 1, repeats = J) Broadcast joint RoPE over all frames: (cosx, sinx) RT J(D/4)2 Repeat((cosx, sinx), dim = 0, repeats = ) (cosy, siny) RT J(D/4)2 Repeat((cosy, siny), dim = 0, repeats = ) (cosz, sinz) RT J(D/4)2 Repeat((cosz, sinz), dim = 0, repeats = ) Concatenate positional encodings across channel dimensions: freqs_cos Concat(cost, cosx, cosy, cosz) freqs_sin Concat(sint, sinx, siny, sinz) Furthermore, we visualize the mean joint positions of the dataset used in our 4D RoPE design. As shown in Figure 11 (b), the result exhibits standard human skeleton composed of 3D joint coordinates. These averaged 3D joint positions serve as the spatial information for the 4D RoPE calculation, enabling the model to encode relative spatial relationships effectively and consistently across different motion sequences. This design not only enhances the robustness of cross-modal interaction but also aligns motion and vision tokens in physically plausible manner."
        },
        {
            "title": "F Training Curves",
            "content": "As shown in Figure 12, we plot the training loss curves of our 4D motion tokenizer and the 4D motionguided video DiT model. The 4D motion tokenizer exhibits rapid convergence, with the loss quickly decreasing early in training. In contrast, the 4D motion-guided video DiT model shows oscillatory convergence, with fluctuations in the loss curve before stabilizing. This difference highlights the distinct training dynamics of the two models, with the light motion tokenizer achieving faster convergence while the relatively heavy DiT model requires more refinement for stable learning."
        },
        {
            "title": "G More Comparisons and Visualization Results",
            "content": "In this section, we provide additional qualitative results to further demonstrate the effectiveness and robustness of our MTVCrafter across wide range of scenarios, character appearances, and motion types. As shown in Figure 13 and 14, these visualizations showcase MTVCrafters strong capabilities in preserving identity consistency, following motion sequences, and generalizing to unseen styles. Moreover, Figure 15 provides additional comparisons with existing SOTA methods. Our MTVCrafter consistently achieves the best visual performance, characterized by high-quality human motion, temporally consistent animation, and high-fidelity appearance. All these results demonstrate the effectiveness of our directly modeling 4D raw motion rather than 2D-rendered pose images. 19 Figure 13: More visualization results (1) on the test set of real humans. Each row shows an animation conditioned on different motion sequence. The first column shows the reference image, while the remaining columns present the animated frames. Our MTVCrafter consistently preserves both identity and motion accuracy across wide variety of scenarios and diverse real, human characters. These results highlight our strong robustness and high-quality open-world animation capability. 20 Figure 14: More visualization results (2) on the test set of diverse-style characters. Each row shows an animation conditioned on different motion sequence. The first column shows the reference image, while the remaining columns present the animated frames. These visualizations showcase open-world animation results featuring virtual human characters. Our MTVCrafter consistently achieves high identity consistency and motion accuracy across various styles and single/multiple characters. Figure 15: More comparisons with SOTA methods. Our MTVCrafter consistently demonstrates the best performance with high-quality human motion and high-fidelity appearance across diverse scenes."
        }
    ],
    "affiliations": [
        "chinatelecom.cn",
        "siat.ac.cn",
        "xjtu.edu.cn"
    ]
}