{
    "paper_title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation",
    "authors": [
        "Yaqi Li",
        "Peng Chen",
        "Mingyang Han",
        "Bu Pi",
        "Haoxiang Shi",
        "Runzhou Zhao",
        "Yang Yao",
        "Xuan Zhang",
        "Jun Song"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provide reward signals only at the end of the generation stage. This monolithic final-only guidance makes it difficult to identify which stages contribute positively to the final outcome and may lead to suboptimal policies. To tackle this issue, we propose a Visual-Chain of Guidance (Visual-CoG) paradigm consisting of three stages: semantic reasoning, process refining, and outcome evaluation, with stage-aware rewards providing immediate guidance throughout the image generation pipeline. We further construct a visual cognition benchmark, VisCog-Bench, which comprises four subtasks to evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%, 5%, and 19%, respectively, demonstrating the superior performance of the proposed Visual-CoG. We will release all the resources soon."
        },
        {
            "title": "Start",
            "content": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation Yaqi Li*, Peng Chen*, Mingyang Han*, Bu Pi*, Haoxiang Shi, Runzhou Zhao, Yang Yao, Xuan Zhang, Jun Song Alibaba Group {jiyan.lyq, zhaojun.cp, jingye.hmy, bupi.wj, jsong.sj}@taobao.com 5 2 0 2 5 2 ] . [ 1 2 3 0 8 1 . 8 0 5 2 : r Figure 1: Visualization of the text-to-image generation results generated by our Visual-CoG. Abstract Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chainof-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provide reward signals only at the end of the generation stage. This monolithic final-only guidance makes it difficult to identify which stages contribute positively to the final outcome and may lead to suboptimal policies. To tackle this issue, we propose Visual-Chain of Guidance (Visual-CoG) paradigm consisting of three stages: semantic reasoning, process refining, and outcome evaluation, with stage-aware rewards providing immediate guidance throughout the image generation pipeline. We further construct visual cognition benchmark, VisCog- *These authors contributed equally. Corresponding author. Bench, which comprises four subtasks to evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on GenEval, T2I-CompBench, and the proposed VisCogBench show improvements of 15%, 5%, and 19%, respectively, demonstrating the superior performance of the proposed Visual-CoG. We will release all the resources soon."
        },
        {
            "title": "Introduction",
            "content": "Recent advancements in deep generative models have revolutionized visual content synthesis, with autoregressive models emerging as one of the leading paradigms (Li et al. 2024; Tian et al. 2024). recent trend in autoregressive models is to seamlessly integrate visual understanding and image generation into single unified large model (ULM) built on multimodal large language models (MLLMs) (Xie et al. 2024; Tong et al. 2024). This integration establishes strong interdependence between vision understanding and generation, enabling ULMs to leverage the prior knowledge of MLLMs in interpreting complex instructions of image synthesis. For instance, recent works (Liao et al. 2025; Jiang et al. 2025) utilize the reasoning capabilities of MLLMs to perform prior instruction planning and enriching, achieving certain performance improvement. While ULMs have seen significant advancements in recent years, they still face challenges in generating images that align with multi-attribute (e.g., color, counting, position) and ambiguous (e.g., the longest river in the world) prompts requiring reasoning (Ge et al. 2024; Sun et al. 2024), as demonstrated by failure cases in Fig. 2(a). For multi-attribute prompts, we argue that applying chain-ofthought (CoT) to enable stage-aware visual synthesis is essential. In the same way, human artists typically follow an iterative creative process: they first understand the semantic concept, then progressively refine the object composition, and finally evaluate and analyze the result. This pipeline can be divided into three stages: semantic interpretation, progressive refinement, and result evaluation. For ambiguous prompts, we claim that only model with strong reasoning capabilities can infer the true underlying intent. Inspired by the success of reinforcement learning (RL) in improving the reasoning performance of large language models (LLMs) (Shao et al. 2024; Guo et al. 2025a), few recent works (Yang et al. 2025b; Duan et al. 2025) have explored applying the RL paradigm to ULMs for instruction understanding or visual generation tasks. However, most existing works (Wang et al. 2025) provide guidance (i.e., reward signals) only at the end of the generation process, ignoring the earlier stages such as semantic interpretation and progressive refinement. As shown in Fig. 2 and Tab. 3, this limitation makes it hard to determine which stage of the generation process contributes positively to the final outcome, potentially leading to ineffective or misleading policies. In summary, we observe that (1) most ULMs struggle with multi-attribute and ambiguous prompts, and (2) recent works have explored CoT and RL to address these challenges yet typically provide guidance only at the final stage that ignores earlier ones. This monolithic final-only guidance tends to lead to suboptimal optimization. Therefore, natural question arises: Can we design stage-aware rewards for visual generation, enabling immediate feedback to guide the entire image generation pipeline? To achieve this, we propose novel reinforcement learning-based framework named Visual-Chain of Guidance (Visual-CoG) for text-to-image generation. This framework formulates the visual generation as chain of guidance by decoupling it into three distinct stages: semantic reasoning, process refining, and outcome evaluation, with immediate rewards provided at each stage as guidance. Specifically, the semantic reasoning reward refers to the discrepancy reward between the generated images based on the original prompt and the reasoning prompt obtained via semantic reasoning. The process refining reward evaluates the effect of the intermediate generation process using an auxiliary masked patch reconstruction task, while the outcome evaluation reward involves rule-based, multidimensional assessment of the final output. Through the proposed paradigm, immediate reward signals can be obtained throughout the visual generation process, making it easier to identify the stages that require optimization. As shown in Fig. 1, the generated images demonstrate enhanced detail and diverse styles. By conducting extensive experiments, Visual-CoG exhibits significant improvement on benchmarks including GenEval (Ghosh, Hajishirzi, and Schmidt 2023) and T2ICompBench (Huang et al. 2023). To further demonstrate the importance of semantic reasoning, we develop comprehensive visual cognition benchmark called VisCog-Bench. It includes four subtasks, namely unusual position, unusual composition, unusual color, and reasoning tasks, to thoroughly evaluate models capability of interpreting implicit intentions. Quantitative and qualitative analysis demonstrate that our method, guided by stage-aware rewards, enables the model to generate high-fidelity images for multi-attribute and ambiguous prompts, as illustrated in Fig. 2. Our contributions are summarized as follows: We propose novel reinforcement learning-based framework for text-to-image generation, which implements stage-aware Visual-Chain of Guidance (Visual-CoG) pipeline comprising semantic reasoning, process refining, and outcome evaluation stages, providing immediate feedback at each stage to guide optimization. We introduce comprehensive visual cognition benchmark, VisCog-Bench, comprising four subtasks: unusual position, unusual composition, unusual color, and reasoning tasks, to thoroughly evaluate semantic reasoning ability in unusual and reasoning-demanding scenarios. Extensive experiments on GenEval, T2I-CompBench, and the proposed VisCog-Bench show that our method outperforms existing methods especially on multiattribute and ambiguous prompts that require reasoning."
        },
        {
            "title": "2.2 Unified Large Models (ULMs)\nRecently, researchers have tried to apply the next-token-\nprediction paradigm to visual foundation models for uni-\nfied visual understanding and generation. One line of meth-",
            "content": "Figure 2: Visualization of the image generation pipeline of Visual-Chain of Guidance (Visual-CoG). (a) shows that with stageaware guidance, the model is able to generate images that are more semantically aligned with prompts. (b) illustrates the effectiveness of the reward mechanisms across three stages of generation, as quantitatively evaluated on GenEval. ods (Sun et al. 2024, 2023; Ge et al. 2024) employs continuous embeddings to represent images, while relying on external image generation models (i.e., diffusion models) to complete the generation process. Additionally, Orthus (Kou et al. 2024) and Transfusion (Zhou et al. 2024) explore continuous representations for unified modeling without the need for additional integration of an image generation model. In contrast, another line of methods unifies text and image modalities using discrete tokens, ensuring simplicity in the model structure. For example, EMU3 (Wang et al. 2024) tokenizes images, text, and videos into discrete space and builds single transformer from scratch on mixture of multimodal sequences. Moreover, some models such as Unified-IO2 (Lu et al. 2024) and Janus-Pro (Chen et al. 2025) combine discrete and continuous representations via two different decoupled encoders for text and image generation respectively."
        },
        {
            "title": "3.1 Preliminary\nMask Token Prediction Mask-based generative modeling\nis a widely adopted technique in autoregressive image gen-\neration (Chang et al. 2022; Xie et al. 2024), where the model\nlearns to predict a set of masked tokens via bidirectional at-\ntention mechanisms. During inference, it initially generates\nall image tokens simultaneously and then iteratively refines\nthe image conditioned on the previously generated tokens.\nThe process can be formulated as follows:",
            "content": "LMTP = (cid:88) log pθ (Ij T1, , TS, Im, I2, , Im, IN , ) , (1) where () represents the conditional probability, parameterized by θ. Let = {T1, T2, , TS} and = {I1, I2, Im, , IN } denote text token sequence of length and image token sequence of length respectively. And Im refers to the masked image tokens that are to be predicted. This strategy serves as the basis for the image refinement process in our chain-of-guidance framework. RL For Unified Large Models (ULMs) Reinforcement Learning (RL) seeks to learn an optimal policy πθ that maximizes the reward return from interacting with an environment (Li 2017). For autoregressive ULMs, the state at time is the combination of the input (i.e., text or image) and the partially generated content < t, and the action is the generation of the next token yt. To optimize this sequential generation process, policy gradient methods are adopted to directly maximize the expected return. The advantage function At serves as critical measure of how much an action outperforms the expected average performance. widely adopted paradigm in RL is formalized as follows: L(θ) = Et [min (γt (θ) At, clip (γt (θ) , 1 ϵ, 1 + ϵ) At)] , (2) γt (θ) = πθ(yt y<t) πθold(yt y<t) , (3) where (x, y) denotes given input-output pair, ϵ is clipping parameter, At is the normalized reward for group of samples = {R1, R2, . . . , RG} computed as Ai = (cid:80)G Rimean(R) std(R) and At = 1 i=1 Ai t."
        },
        {
            "title": "3.2 Visual-Chain of Guidance\nDrawing inspiration from the iterative creative process of\nhuman artists, which involves first understanding the seman-\ntic concept, progressively refining the object composition,",
            "content": "Figure 3: An overview of Visual-CoG framework, which comprises three key stages: semantic reasoning, process refining, and outcome evaluation. Stage-aware rewards enable immediate feedback to guide the entire image generation pipeline. and finally evaluating and analyzing the result, we introduce Visual-Chain of Guidance (Visual-CoG) framework with stage-aware rewards for image generation. The pipeline includes (1) Semantic Reasoning, (2) Process Refining and (3) Outcome Evaluation, as elaborated below. Semantic Reasoning Initial semantic reasoning is stage prior to image generation, which involves reasoning about the underlying intention of the instruction and planning the scene layout. In unusual or reasoning-demanding scenarios, ULMs leverage their inherent ability to interpret visual inputs to concretize and clarify the generation instructions. Specifically, this stage begins with semantic reasoning via language modeling task performed by language model Mlm to generate reasoning prompt based on the original prompt P. and are then used as inputs of image generation task Mt2i under the same random seed, resulting in two generated images and . An outcome evaluation reward Ro then computes scores for both images and , which are designed to assess image-text alignment comprehensively (see Outcome Evaluation for details). The difference between these scores is defined as the semantic reasoning reward Rr, which reflects the effectiveness of the reasoning process. Intuitively, higher score indicates better reasoning performance, thus serving as an intermediate feedback to optimize the model. The semantic reasoning reward is as follows: Rr = Ro(Mt2i(P )) Ro(Mt2i(P)). (4) Process Refining Process Refining refers to the iterative and adaptive refinement of intermediate generation processes via mask token prediction. In contrast to methods that assess only the final output, we propose an immediate reward mechanism for the image refinement process, which provides immediate feedback during the generation process. As discussed in the section preliminary, at each intermediate generation step, the model generates an image conditioned on masked input under given masking probability. The resulting output is then used as input for the next step, thus forming an iterative masked patch reconstruction process. Intuitively, simple way to reward the generation process would be to evaluate the intermediate images directly. However, those intermediate images often suffer from blurriness or are incomplete, thus explicit image-based evaluation using detection methods is not suitable for providing reliable feedback during the generation process. Instead, given the characteristics of the generation process, we perform masked patch reconstruction task to evaluate the generation process in straightforward yet effective way. Since it would be meaningless for the policy model to recover the masked image generated by itself, we introduce teacher model τ to provide preferred distribution. The model is expected to reconstruct this distribution under the corresponding masked condition. Specifically, we define process refining reward at step tm as the discrepancy between the preferred distribution pτ = πτ (ytm y< tm) and the one reconstructed by the policy model pθ = πθ(ytm y< tm). The reward Rp serves as an indicator of how well the model performs during the current generation process under the given masked condition. Rp is computed as follows: Rp = exp (G(pθ) G(pτ )p) , where converts the distribution to an image. The process refining reward enables more effective intermediate strategy adjustments, leading to higher-quality outputs. (5) Outcome Evaluation Inspired by the rule-based reward mechanism in DeepSeek-R1 (Guo et al. 2025a), we design Table 1: Comparison to the state-of-the-art diffusion models and autoregressive models on GenEval. Method Single Obj. Two Obj. Counting Colors Position Color Attri. Overall Diffusion Models Stable v1.5 (Rombach et al. 2022) SD-XL-base-1.0 (Podell et al. 2023) PixArt-α (Chen et al. 2023) SD-3 (d=24) (Esser et al. 2024) Autoregressive Models SEED-X (Ge et al. 2024) PARM (Guo et al. 2025b) Janus-Pro-7B (Chen et al. 2025) Show-o (Baseline) (Xie et al. 2024) Visual-CoG (Ours) 97.13 98.34 98.05 98.05 97.01 99.21 99.03 98.11 99.95 38.21 74.14 50.31 74.55 58.24 85.64 88.98 80.04 92.68 35.88 39.21 44.67 63. 26.13 67.15 58.75 66.25 80.94 76.41 85.36 80.12 67.12 80.27 84.02 90.02 84.17 85.11 4.55 15.09 8.55 34.09 19.51 65.83 78.89 31.03 79.00 6.97 23.11 7.21 36. 14.90 64.04 66.12 50.14 65.50 43.19 55.87 48.15 62.19 49.34 77.64 80.29 68.29 83.86 rule-based reward framework to evaluate the quality of generated images. Specifically, we use an open-vocabulary object detector with textual queries to determine whether the target objects are present in the generated image. Based on this detection, we then evaluate the consistency of spatial, counting, and color attributes using predefined rules. To assess spatial consistency, we employ spatial validator Es to infer the relative positions (e.g., left of) of objects based on the 2D coordinates of their bounding box centroids. The inferred spatial relationships are then compared against the explicit spatial constraints specified in the prompt. Let denote the set of object pairs (i, j) with defined spatial relations, where = Ns. For each pair (i, j) S, let (i,j) represent the corresponding ground-truth relationship. The spatial reward Rs is computed as follows: Rs = 1 Ns (cid:88) (cid:16) (i,j)S Es (D (Ie, i) , (Ie, j)) = (i,j) (cid:17) , (6) For counting consistency, we check whether the number of generated objects matches the target quantity in from the prompt. Here, Nn refers to the number of objects with numerical attributes in image Ie, and En is used to convert the detection results of object into specific number. To provide smooth reward, we design an exponential function with scaling factor that penalizes larger deviations more severely as follows: Rn = 1 Nn Nn(cid:88) i=1 exp (cid:32) (cid:12) (cid:12)En (D (Ie, i)) in τ (cid:33) (cid:12) (cid:12) , (7) To evaluate color consistency, we utilize zero-shot object-centric classification approach based on pre-trained vision-language model (i.e., CLIP). For each detected object i, post-processing step Ec is applied, which involves extracting its bounding box and refining it with segmentation mask to isolate the foreground. set of textual prompts in the format photo of [color] [object] is then generated for each candidate color in the predefined color list Y. The vision-language model computes similarity scores between the processed object and each prompt Py,i, and the color with the highest score is selected as the predicted label. This prediction is compared against the ground-truth label ic to assess consistency. Here, Nc denotes the number of objects with specified color attributes. Rc ="
        },
        {
            "title": "1\nNc",
            "content": "Nc(cid:88) i=1 (cid:18) arg max yY sim(Ec (D (Ie, i)) , Py,i) = ic (cid:19) , (8) In addition to the attribute-wise rewards at the local level, it is essential to evaluate the overall aesthetic quality and alignment from global perspective. Therefore, we employ the HPS model (Wu et al. 2023) as an additional reward model to provide holistic score Rh for the image. The reward for the image outcome evaluation Ro is as follows. Note that Ro means Ro(Ie) by default for simplicity. Ro = Rn + Rc + Rs + Rh, (9) The overall reward is the sum of the three stages and the loss is defined in Eq. 2: = Rr + Rp + Ro, (10)"
        },
        {
            "title": "4.1 Experimental Setup\nImplementation Details Our training dataset consists of\ntext prompts generated using Qwen3 (Yang et al. 2025a)\nbased on several templates covering the 80 COCO (Lin\net al. 2014) object classes (see Appendix for details). These\nprompts are further processed by Qwen3 to extract ob-\njects and their associated attributes (e.g., color, position and\ncounting) for reward computation. A total of 9,368 prompts\nare generated in this manner for training. We use Show-o\n(Xie et al. 2024) as our base model and train it with a learn-\ning rate of 1e-6. For the reward modeling component, we\nadopt HPS (Wu et al. 2023) as the human preference model\nand GroundingDINO (Liu et al. 2024) as the object detec-\ntor. The proposed model is implemented using PyTorch and\ntrained on NVIDIA H20 GPUs.",
            "content": "Evaluation Metrics We evaluate the effectiveness of our method on the GenEval (Ghosh, Hajishirzi, and Schmidt 2023) and T2I-CompBench (Huang et al. 2023) benchmarks, which are widely adopted for evaluating text-image alignment from the attribute-wise perspective. We adopt the recommended protocols for both benchmarks. Table 2: Comparison to the state-of-the-art diffusion models and autoregressive models on T2I-CompBench. Method Color Shape Texture Spatial Non-Spatial Complex Diffusion Models Stable v1.5 (Rombach et al. 2022) SD-XL-base-1.0 (Podell et al. 2023) Composable v2 (Liu et al. 2022) PixArt-α (Chen et al. 2023) Autoregressive Models PARM (Guo et al. 2025b) Janus-Pro-7B (Chen et al. 2025) Show-o (Baseline) (Xie et al. 2024) Visual-CoG (Ours) 37.58 58.79 40.63 66.90 75.21 63.59 68.45 78.92 37.13 46.87 32.99 49.27 56.32 35.28 49.61 57. 41.86 52.99 36.45 64.77 66.04 49.36 65.34 67.85 11.65 21.31 8.00 20.64 29.12 20.61 38.12 43.71 31.12 31.19 31.12 31.97 31.44 30.85 30.06 30. 30.47 32.37 29.80 34.33 36.81 34.31 34.67 36.84 VisCog-Bench To further verify the effectiveness of reasoning capabilities, we propose novel visual cognition benchmark, VisCog-Bench, comprising four subtasks: unusual position, unusual composition, unusual color, and reasoning tasks. The benchmark consists of 20, 20, 20, and 40 prompts for each task respectively, totaling 100 prompts. Note that 4 images are generated for each prompt for evaluation to avoid randomness. To ensure more comprehensive evaluation, we assess the generated images using both automated metrics and human evaluations. Specifically, the prompts for unusual position, unusual composition, and unusual color are selected from GenEval, and we adopt the corresponding metrics of GenEval. For the reasoning tasks, we use Qwen3 to generate prompts that require reasoning based on common knowledge, and the resulting images are automatically evaluated using Qwen2.5-VL (Bai et al. 2025). Additionally, we conduct user study as part of the evaluator-based evaluation (see Appendix for details)."
        },
        {
            "title": "4.2 Quantitative results",
            "content": "Tab. 1 and Tab. 2 present comprehensive comparison between the proposed method and other state-of-the-art models across the visual generation benchmarks GenEval and T2ICompBench, covering both diffusion-based methods such as SD-3 (Esser et al. 2024), PixArt-α (Chen et al. 2023) and autoregressive-based models such as SEED-X (Ge et al. 2024), Janus-Pro (Chen et al. 2025). Notably, our method achieves substantial improvements over the baseline model Show-o (Xie et al. 2024), with an average enhancement of 15.57% across all metrics on GenEval. It consistently outperforms both diffusion-based and autoregressive-based approaches, achieving 13.79% improvement in the counting subtask compared to previous state-of-the-art results. Similarly, on T2I-CompBench, our method achieves strong performance across various tasks, obtaining the highest scores of 78.92% in Color and 43.71% in Spatial. These improvements are partly attributed to the semantic reasoning stage, which enables global-level planning and reasoning. And subsequent image process refining and output evaluation ensure faithful execution of the initial design intent, thereby guaranteeing accurate and coherent final results. Moreover, autoregressive models consistently outperform diffusion models in prompt interpretation, leading to more accurate text-image alignment across all attributes in both GenEval and T2I-CompBench benchmarks. Table 3: Comparison to outstanding autoregressive models with RL or CoT on GenEval. Method Position Color Attri. Overall Counting VARGPT-v1.1 (Zhuang et al. 2025) SimpleAR (Wang et al. 2025) GoT (Fang et al. 2025) RePrompt (Wu et al. 2025) PARM (Guo et al. 2025b) Visual-CoG (Ours) 48.00 - 67.00 77.00 67.15 80.94 13.00 28.00 34.00 62.00 65.83 79.00 21.00 45.00 27.00 49.00 64.04 65.50 53.00 63.00 64.00 76.00 77.64 83.86 Table 4: Effect of rewards on different stages. Rr Rp Ro Counting - - - - 66.25 76.87 72.98 78.13 80.94 - - Position Color Attri. Overall 31.03 76.51 75.09 72.01 79.00 50.14 55.97 56.59 62.88 65.50 68.29 78.12 76.23 80.11 83. Additionally, Tab. 3 displays comparison with autoregressive models that employ reinforcement learning (RL) or chain-of-thought (CoT)-based techniques. It is evident that the proposed Visual-CoG significantly outperforms others. This suggests that while these methods incorporate RL and CoT techniques to enable stage-by-stage generation, they typically provide guidance only at the final stage, ignoring earlier stages and resulting in suboptimal performance. 4.3 Ablation studies To explore the contribution of the semantic reasoning reward Rr, process refining reward Rp, and outcome evaluation reward Ro, we conduct ablation studies on the GenEval benchmark, across counting, position, and color metrics. As shown in Tab. 4, all rewards consistently enhance alignment performance. Specifically, Rr leads to considerable improvement of 6.99% in position metrics, while Rp and Ro significantly enhance counting and color metrics, with improvements of 7.96% and 9.53%, respectively. This can be attributed to Rr providing prior planning for scene construction and object layout, enabling accurate generation of position-related scenarios. Meanwhile, Rp ensures consisTable 5: VisCog-Bench results including four subtasks. Method Stable v1.5 PixArt-α Show-o Ours w/o Rr Ours Unusual Pos. Comp. 3.75 7.50 28.75 49.75 77.50 31.25 42.50 76.75 79.25 90.00 Color 11.25 18.75 62.25 64.50 70.00 Reasoning Overall Human Eval 38.98 40.78 64.54 67.89 72.50 21.30 27.38 58.07 65.34 77.50 23.78 25.98 55.21 61.30 78.55 Figure 4: Qualitative results from GenEval for various attributes such as composition, color, position and counting. tency in local details (e.g., color and counting relationships) during refinement, as the basis for subsequent outcome evaluation. Ro offers direct feedback on outcomes, explicitly evaluating attributes such as color. The combination of these rewards further enhances overall semantic alignment. As shown in Fig. 4, the generated images under different settings: using only Rr (2nd column), with Rr and Rp (3rd column), and with Rr, Rp, and Ro (4th column). These settings are assessed across on composition, color, position, and counting metrics. In the first and third rows, the images generated by the baseline misrepresent the composition or positional relationships between dogs and bears respectively. Semantic reasoning corrects these issues through accurate object layout planning. As shown in the second row, process refining enhances the details of the generated images at the local level. This ensures that the pink oven and the green motorcycle merge seamlessly, with no visual discontinuity. Additionally, images generated with outcome evaluation reward demonstrate high aesthetic quality and diversity. For instance, the image in the last column features more detailed background and natural atmosphere."
        },
        {
            "title": "4.4 Semantic Reasoning Analysis\nWe conduct experiments on the proposed VisCog-Bench to\nverify the effectiveness of reasoning capabilities in the con-\ntext of unusual or reasoning-demanding prompts. Generat-\ning accurate images for VisCog-Bench requires the model\nto infer the described objects and scenarios, emphasizing the\nrole of early semantic reasoning.",
            "content": "As demonstrated in Tab. 5, semantic reasoning leads improvements for unusual or reasoningto significant Figure 5: Qualitative results of image generation for prompts involving unusual colors (1st row) and common-sense reasoning (2nd row), with and without semantic reasoning. demanding prompts, with gains of 12.16% and 17.35% in automated and human evaluation respectively. This is due to semantic reasoning enabling better interpretation of unusual or ambiguous prompts. As shown in Fig. 5, for unusual composition and color (1st row), models without semantic reasoning tend to generate disconnected scenarios (e.g., placing the dog and dinner table in separate regions of the image). In contrast, semantic reasoning enables prompt interpretation, leading to more detailed descriptions such as dog with purple fur beside dinner table in the house, thus producing more coherent and specific visual outputs. For reasoning tasks (2nd row), semantic reasoning helps interpret ambiguous concepts The most famous tower in Paris is the Eiffel Tower, enabling accurate image generation."
        },
        {
            "title": "5 Conclusion\nIn this work, we propose a novel Visual-Chain of Guid-\nance (Visual-CoG), which consists of three stages: seman-\ntic reasoning, process refining, and outcome evaluation,\nwith stage-aware rewards providing immediate guidance\nthroughout the image generation pipeline, particularly for\nmulti-attribute and ambiguous prompts. Additionally, we in-\ntroduce a visual cognition benchmark, VisCog-Bench, com-\nprising four subtasks to evaluate the effectiveness of seman-\ntic reasoning. Extensive experiments show that the proposed\nmethod achieves outstanding performance across multiple\nbenchmarks with enhanced reasoning capabilities.",
            "content": "References Bai, S.; Chen, K.; Liu, X.; Wang, J.; Ge, W.; Song, S.; Dang, K.; Wang, P.; Wang, S.; Tang, J.; et al. 2025. Qwen2. 5-vl technical report. arXiv preprint arXiv:2502.13923. Chang, H.; Zhang, H.; Jiang, L.; Liu, C.; and Freeman, W. T. 2022. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 1131511325. Chen, J.; Yu, J.; Ge, C.; Yao, L.; Xie, E.; Wu, Y.; Wang, Z.; Kwok, J.; Luo, P.; Lu, H.; et al. 2023. Pixart-alpha: Fast training of diffusion transformer for photorealistic text-toimage synthesis. arXiv preprint arXiv:2310.00426. Chen, X.; Wu, Z.; Liu, X.; Pan, Z.; Liu, W.; Xie, Z.; Yu, X.; and Ruan, C. 2025. Janus-pro: Unified multimodal understanding and generation with data and model scaling. arXiv preprint arXiv:2501.17811. Duan, C.; Fang, R.; Wang, Y.; Wang, K.; Huang, L.; Zeng, X.; Li, H.; and Liu, X. 2025. Got-r1: Unleashing reasoning capability of mllm for visual generation with reinforcement learning. arXiv preprint arXiv:2505.17022. Esser, P.; Kulal, S.; Blattmann, A.; Entezari, R.; Muller, J.; Saini, H.; Levi, Y.; Lorenz, D.; Sauer, A.; Boesel, F.; et al. 2024. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first international conference on machine learning. Fang, R.; Duan, C.; Wang, K.; Huang, L.; Li, H.; Yan, S.; Tian, H.; Zeng, X.; Zhao, R.; Dai, J.; et al. 2025. Got: Unleashing reasoning capability of multimodal large language arXiv preprint model for visual generation and editing. arXiv:2503.10639. Ge, Y.; Zhao, S.; Zhu, J.; Ge, Y.; Yi, K.; Song, L.; Li, C.; Ding, X.; and Shan, Y. 2024. Seed-x: Multimodal models with unified multi-granularity comprehension and generation. arXiv preprint arXiv:2404.14396. Ghosh, D.; Hajishirzi, H.; and Schmidt, L. 2023. Geneval: An object-focused framework for evaluating text-to-image alignment. Advances in Neural Information Processing Systems, 36: 5213252152. Guo, D.; Yang, D.; Zhang, H.; Song, J.; Zhang, R.; Xu, R.; Zhu, Q.; Ma, S.; Wang, P.; Bi, X.; et al. 2025a. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948. Guo, Z.; Zhang, R.; Tong, C.; Zhao, Z.; Gao, P.; Li, H.; and Heng, P.-A. 2025b. Can We Generate Images with CoT? Lets Verify and Reinforce Image Generation Step by Step. arXiv preprint arXiv:2501.13926. Huang, K.; Sun, K.; Xie, E.; Li, Z.; and Liu, X. 2023. T2icompbench: comprehensive benchmark for open-world compositional text-to-image generation. Advances in Neural Information Processing Systems, 36: 7872378747. Jiang, D.; Guo, Z.; Zhang, R.; Zong, Z.; Li, H.; Zhuo, L.; Yan, S.; Heng, P.-A.; and Li, H. 2025. T2i-r1: Reinforcing image generation with collaborative semantic-level and token-level cot. arXiv preprint arXiv:2505.00703. Kou, S.; Jin, J.; Liu, Z.; Liu, C.; Ma, Y.; Jia, J.; Chen, Q.; Jiang, P.; and Deng, Z. 2024. Orthus: Autoregressive interleaved image-text generation with modality-specific heads. arXiv preprint arXiv:2412.00127. Li, T.; Tian, Y.; Li, H.; Deng, M.; and He, K. 2024. Autoregressive image generation without vector quantization. Advances in Neural Information Processing Systems, 37: 5642456445. Li, Y. 2017. Deep reinforcement learning: An overview. arXiv preprint arXiv:1701.07274. Liao, J.; Yang, Z.; Li, L.; Li, D.; Lin, K.; Cheng, Y.; and Wang, L. 2025. Imagegen-cot: Enhancing text-to-image in-context learning with chain-of-thought reasoning. arXiv preprint arXiv:2503.19312. Lin, T.-Y.; Maire, M.; Belongie, S.; Hays, J.; Perona, P.; Ramanan, D.; Dollar, P.; and Zitnick, C. L. 2014. Microsoft In Computer vision coco: Common objects in context. ECCV 2014: 13th European conference, zurich, Switzerland, September 6-12, 2014, proceedings, part 13, 740 755. Springer. Liu, N.; Li, S.; Du, Y.; Torralba, A.; and Tenenbaum, J. B. 2022. Compositional visual generation with composable difIn European Conference on Computer Vifusion models. sion, 423439. Springer. Liu, S.; Zeng, Z.; Ren, T.; Li, F.; Zhang, H.; Yang, J.; Jiang, Q.; Li, C.; Yang, J.; Su, H.; et al. 2024. Grounding dino: Marrying dino with grounded pre-training for open-set object detection. In European Conference on Computer Vision, 3855. Springer. Lu, J.; Clark, C.; Lee, S.; Zhang, Z.; Khosla, S.; Marten, R.; Hoiem, D.; and Kembhavi, A. 2024. Unified-io 2: Scaling autoregressive multimodal models with vision language audio and action. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 2643926455. Podell, D.; English, Z.; Lacey, K.; Blattmann, A.; Dockhorn, T.; Muller, J.; Penna, J.; and Rombach, R. 2023. Sdxl: Improving latent diffusion models for high-resolution image synthesis. arXiv preprint arXiv:2307.01952. Rombach, R.; Blattmann, A.; Lorenz, D.; Esser, P.; and Ommer, B. 2022. High-resolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, 10684 10695. Shao, Z.; Wang, P.; Zhu, Q.; Xu, R.; Song, J.; Bi, X.; Zhang, H.; Zhang, M.; Li, Y.; Wu, Y.; et al. 2024. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300. Sun, Q.; Cui, Y.; Zhang, X.; Zhang, F.; Yu, Q.; Wang, Y.; Rao, Y.; Liu, J.; Huang, T.; and Wang, X. 2024. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, 1439814409. Sun, Q.; Yu, Q.; Cui, Y.; Zhang, F.; Zhang, X.; Wang, Y.; Gao, H.; Liu, J.; Huang, T.; and Wang, X. 2023. Emu: arXiv preprint Generative pretraining in multimodality. arXiv:2307.05222. Tian, K.; Jiang, Y.; Yuan, Z.; Peng, B.; and Wang, L. 2024. Visual autoregressive modeling: Scalable image generation via next-scale prediction. Advances in neural information processing systems, 37: 8483984865. Tong, S.; Fan, D.; Zhu, J.; Xiong, Y.; Chen, X.; Sinha, K.; Rabbat, M.; LeCun, Y.; Xie, S.; and Liu, Z. 2024. Metamorph: Multimodal understanding and generation via instruction tuning. arXiv preprint arXiv:2412.14164. Wang, J.; Tian, Z.; Wang, X.; Zhang, X.; Huang, W.; Wu, Z.; and Jiang, Y.-G. 2025. Simplear: Pushing the frontier of autoregressive visual generation through pretraining, sft, and rl. arXiv preprint arXiv:2504.11455. Wang, X.; Zhang, X.; Luo, Z.; Sun, Q.; Cui, Y.; Wang, J.; Zhang, F.; Wang, Y.; Li, Z.; Yu, Q.; et al. 2024. Emu3: arXiv preprint Next-token prediction is all you need. arXiv:2409.18869. Wu, M.; Wang, L.; Zhao, P.; Yang, F.; Zhang, J.; Liu, J.; Zhan, Y.; Han, W.; Sun, H.; Ji, J.; et al. 2025. RePrompt: Reasoning-Augmented Reprompting for Text-toarXiv Image Generation via Reinforcement Learning. preprint arXiv:2505.17540. Wu, X.; Hao, Y.; Sun, K.; Chen, Y.; Zhu, F.; Zhao, R.; and Li, H. 2023. Human preference score v2: solid benchmark for evaluating human preferences of text-to-image synthesis. arXiv preprint arXiv:2306.09341. Xie, J.; Mao, W.; Bai, Z.; Zhang, D. J.; Wang, W.; Lin, K. Q.; Gu, Y.; Chen, Z.; Yang, Z.; and Shou, M. Z. 2024. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528. Yang, A.; Li, A.; Yang, B.; Zhang, B.; Hui, B.; Zheng, B.; Yu, B.; Gao, C.; Huang, C.; Lv, C.; et al. 2025a. Qwen3 technical report. arXiv preprint arXiv:2505.09388. Yang, L.; Zhang, X.; Tian, Y.; Shang, C.; Xu, M.; Zhang, W.; and Cui, B. 2025b. Hermesflow: Seamlessly closing the gap in multimodal understanding and generation. arXiv preprint arXiv:2502.12148. Zhou, C.; Yu, L.; Babu, A.; Tirumala, K.; Yasunaga, M.; Shamis, L.; Kahn, J.; Ma, X.; Zettlemoyer, L.; and Levy, O. 2024. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039. Zhuang, X.; Xie, Y.; Deng, Y.; Yang, D.; Liang, L.; Ru, J.; Yin, Y.; and Zou, Y. 2025. Vargpt-v1. 1: Improve visual autoregressive large unified model via iterative instrucarXiv preprint tion tuning and reinforcement learning. arXiv:2504.02949."
        },
        {
            "title": "Reproduction Checklist",
            "content": "4. Computational Experiments 1. General Paper Structure 1.1. Includes conceptual outline and/or pseudocode description of AI methods introduced (yes/partial/no/NA) yes 1.2. Clearly delineates statements that are opinions, hypothesis, and speculation from objective facts and results (yes/no) yes 1.3. Provides well-marked pedagogical references for lessfamiliar readers to gain background necessary to replicate the paper (yes/no) yes 2. Theoretical Contributions 2.1. Does this paper make theoretical contributions? (yes/no) yes If yes, please address the following points: 2.2. All assumptions and restrictions are stated clearly and formally (yes/partial/no) yes 2.3. All novel claims are stated formally (e.g., in theorem statements) (yes/partial/no) yes 2.4. Proofs of all novel claims are included (yes/partial/no) yes 2.5. Proof sketches or intuitions are given for complex and/or novel results (yes/partial/no) yes 2.6. Appropriate citations to theoretical tools used are given (yes/partial/no) yes 2.7. All theoretical claims are demonstrated empirically to hold (yes/partial/no/NA) yes 2.8. All experimental code used to eliminate or disprove claims is included (yes/no/NA) yes 3. Dataset Usage 4.1. Does this paper include computational experiments? (yes/no) yes If yes, please address the following points: 4.2. This paper states the number and range of values tried per (hyper-) parameter during development of the paper, along with the criterion used for selecting the final parameter setting (yes/partial/no/NA) yes 4.3. Any code required for pre-processing data is included in the appendix (yes/partial/no) yes 4.4. All source code required for conducting and analyzing the experiments is included in code appendix (yes/- partial/no) yes 4.5. All source code required for conducting and analyzing the experiments will be made publicly available upon publication of the paper with license that allows free usage for research purposes (yes/partial/no) yes 4.6. All source code implementing new methods have comments detailing the implementation, with references to the paper where each step comes from (yes/partial/no) yes 4.7. If an algorithm depends on randomness, then the method used for setting seeds is described in way to allow replication of results (yes/parsufficient tial/no/NA) yes 4.8. This paper specifies the computing infrastructure used for running experiments (hardware and software), including GPU/CPU models; amount of memory; operating system; names and versions of relevant software libraries and frameworks (yes/partial/no) yes 4.9. This paper formally describes evaluation metrics used and explains the motivation for choosing these metrics (yes/partial/no) yes 3.1. Does this paper rely on one or more datasets? (yes/no) 4.10. This paper states the number of algorithm runs used to compute each reported result (yes/no) yes 4.11. Analysis of experiments goes beyond singledimensional summaries of performance (e.g., average; median) to include measures of variation, confidence, or other distributional information (yes/no) yes 4.12. The significance of any improvement or decrease in performance is judged using appropriate statistical tests (e.g., Wilcoxon signed-rank) (yes/partial/no) yes 4.13. This paper lists all final (hyper-)parameters used for each model/algorithm in the papers experiments (yes/partial/no/NA) yes yes If yes, please address the following points: 3.2. motivation is given for why the experiments are conducted on the selected datasets (yes/partial/no/NA) yes 3.3. All novel datasets introduced in this paper are included in data appendix (yes/partial/no/NA) yes 3.4. All novel datasets introduced in this paper will be made publicly available upon publication of the paper with license that allows free usage for research purposes (yes/partial/no/NA) yes 3.5. All datasets drawn from the existing literature (potentially including authors own previously published work) are accompanied by appropriate citations (yes/no/NA) yes 3.6. All datasets drawn from the existing literature (potentially including authors own previously published work) are publicly available (yes/partial/no/NA) yes 3.7. All datasets that are not publicly available are described in detail, with explanation why publicly available alternatives are not scientifically satisficing (yes/- partial/no/NA) yes"
        }
    ],
    "affiliations": [
        "Alibaba Group"
    ]
}