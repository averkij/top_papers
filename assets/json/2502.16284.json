{
    "paper_title": "MolSpectra: Pre-training 3D Molecular Representation with Multi-modal Energy Spectra",
    "authors": [
        "Liang Wang",
        "Shaozhen Liu",
        "Yu Rong",
        "Deli Zhao",
        "Qiang Liu",
        "Shu Wu",
        "Liang Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Establishing the relationship between 3D structures and the energy states of molecular systems has proven to be a promising approach for learning 3D molecular representations. However, existing methods are limited to modeling the molecular energy states from classical mechanics. This limitation results in a significant oversight of quantum mechanical effects, such as quantized (discrete) energy level structures, which offer a more accurate estimation of molecular energy and can be experimentally measured through energy spectra. In this paper, we propose to utilize the energy spectra to enhance the pre-training of 3D molecular representations (MolSpectra), thereby infusing the knowledge of quantum mechanics into the molecular representations. Specifically, we propose SpecFormer, a multi-spectrum encoder for encoding molecular spectra via masked patch reconstruction. By further aligning outputs from the 3D encoder and spectrum encoder using a contrastive objective, we enhance the 3D encoder's understanding of molecules. Evaluations on public benchmarks reveal that our pre-trained representations surpass existing methods in predicting molecular properties and modeling dynamics."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 4 8 2 6 1 . 2 0 5 2 : r Published as conference paper at ICLR MOLSPECTRA: PRE-TRAINING 3D MOLECULAR REPRESENTATION WITH MULTI-MODAL ENERGY SPECTRA Liang Wang1,2Shaozhen Liu1 Yu Rong3 Deli Zhao3 Qiang Liu1,2 Shu Wu1,2 Liang Wang1,2 1New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA) 2School of Artificial Intelligence, University of Chinese Academy of Sciences 3DAMO Academy, Alibaba Group"
        },
        {
            "title": "ABSTRACT",
            "content": "Establishing the relationship between 3D structures and the energy states of molecular systems has proven to be promising approach for learning 3D molecular representations. However, existing methods are limited to modeling the molecular energy states from classical mechanics. This limitation results in significant oversight of quantum mechanical effects, such as quantized (discrete) energy level structures, which offer more accurate estimation of molecular energy and can be experimentally measured through energy spectra. In this paper, we propose to utilize the energy spectra to enhance the pre-training of 3D molecular representations (MolSpectra), thereby infusing the knowledge of quantum mechanics into the molecular representations. Specifically, we propose SpecFormer, multispectrum encoder for encoding molecular spectra via masked patch reconstruction. By further aligning outputs from the 3D encoder and spectrum encoder using contrastive objective, we enhance the 3D encoders understanding of molecules. Evaluations on public benchmarks reveal that our pre-trained representations surpass existing methods in predicting molecular properties and modeling dynamics."
        },
        {
            "title": "INTRODUCTION",
            "content": "Learning 3D molecular representations from geometric conformations offers promising approach for understanding molecular geometry and predicting quantum properties and interactions, which is significant in drug discovery and materials science (Musaelian et al., 2023; Batatia et al., 2022; Liao & Smidt, 2023; Wang et al., 2023b; Du et al., 2023b). Given the scarcity of molecular property labels, self-supervised representation pre-training has been proposed and utilized to provide generalizable representations (Hu et al., 2020; Rong et al., 2020; Ma et al., 2024). In contrast to contrastive learning (Wang et al., 2022; Kim et al., 2022) and masked modeling (Hou et al., 2022; Liu et al., 2023c; Wang et al., 2024b) on 2D molecular graphs and molecular languages (e.g., SMILES), the design of pre-training strategies on 3D molecular geometries is more closely aligned with physical principles. Previous studies (Zaidi et al., 2023; Jiao et al., 2023) have guided representation learning through denoising processes on 3D molecular geometries, theoretically demonstrating that denoising 3D geometries is equivalent to learning molecular force fields, specifically the negative gradient of molecular potential energy with respect to position. Essentially, these studies reveal that establishing the relationship between 3D geometries and the energy states of molecular systems is an effective pathway to learn 3D molecular representations. However, existing methods are limited to the continuous description (i.e., the potential energy function) of the molecular energy states within the classical mechanics, overlooking the quantized (discrete) energy level structures from the quantum mechanical perspective. From the quantum perspective, molecular systems exhibit quantized energy level structures, meaning that energy states Correspondence to Liang Wang: liang.wang@cripac.ia.ac.cn Corresponding authors: Yu Rong and Qiang Liu 1The code is released at https://github.com/AzureLeon1/MolSpectra 1 Published as conference paper at ICLR Figure 1: The conceptual view of MolSpectra, which leverages both molecular conformation and spectra for pre-training. Prior works only model classical mechanics by denoising on conformations. can only assume specific discrete values. Specifically, different types of molecular motion, such as electronic, vibrational, and rotational motion, correspond to different energy level structures. Knowledge of these energy levels is crucial in molecular physics and quantum chemistry, as they determine the spectroscopic characteristics, chemical reactivity, and many other important molecular properties. Fortunately, experimental measurements of molecular energy spectra can reflect these structures. Meanwhile, there are many molecular spectra data obtained through experimental measurements or simulations (Zou et al., 2023; Alberts et al., 2024). Therefore, incorporating the knowledge of energy levels into molecular representation learning is expected to facilitate the development of more informative molecular representations. In this paper, we propose MolSpectra, framework that incorporates molecular spectra into the pre-training of 3D molecular representations, thereby infusing the knowledge of quantized energy level structures into the representations, as shown in Figure 1. In MolSpectra, we introduce multispectrum encoder, SpecFormer, to capture both intra-spectrum and inter-spectrum peak correlations by training with masked patches reconstruction (MPR) objective. Additionally, we employ contrastive objective to distills the spectral features and its inherent knowledge into the learning of 3D representations. After pre-training, the resulting 3D encoder can be fine-tuned for downstream tasks, providing expressive 3D molecular representations without the need for associated spectral data. Extensive experiments over different downstream molecular property prediction benchmarks shows the superiority of MolSpectra. In summary, our contributions are as follows: We introduce quantized energy level structures and molecular spectra into 3D molecular representation pre-training for the first time, surpassing previous work that relied solely on physical knowledge within the scope of classical mechanics. We propose SpecFormer as an expressive multi-spectrum encoder, along with the masked patches reconstruction objective for spectral representation learning. We propose contrastive objective to align molecular representations in the 3D modality and spectral modalities, enabling the pre-trained 3D encoder to infer molecular spectral features in downstream tasks without relying on spectral data. Experiments across different downstream benchmarks demonstrate that our method effectively enhances the expressiveness of the pre-trained 3D molecular representations."
        },
        {
            "title": "2 PRELIMINARIES",
            "content": "2.1 NOTATIONS Consider molecule characterized by its 3D structure and spectra, represented as = (a, x, S). Here, {1, 2, . . . , 118}N specifies the atomic numbers, indicating the types of atoms within the molecule. The vector R3N describes the conformation of the molecule, while represents its 2 Published as conference paper at ICLR 2025 spectra. The parameter denotes the number of atoms in the molecule. Note that the atoms are arranged in the same order in both and x, ensuring consistency between the atomic numbers and their corresponding spatial coordinates. = (s1, . . . , sS) represents the set of spectra for molecule, where denotes the number of spectrum types considered. In our study, we focus on three types, so = 3. The first spectrum, s1 R601, is the UV-Vis spectrum, which spans from 1.5 to 13.5 eV with 601 data points at intervals of 0.02 eV. The second spectrum, s2 R3501, is the IR spectrum, covering range from 500 to 4000 cm1 with 3501 data points at intervals of 1 cm1. The third spectrum, s3 R3501, is the Raman spectrum, with the same range and intervals as the IR spectrum. Together, these spectra provide comprehensive description of the molecular characteristics across different spectral modalities."
        },
        {
            "title": "2.2 PRE-TRAINING 3D MOLECULAR REPRESENTATION VIA DENOISING",
            "content": "Denoising has emerged as prominent pre-training objective in 3D molecular representation learning, excelling in various downstream tasks. This method involves training models to predict and remove noise introduced deliberately into molecular structures. This approach is physically interpretable due to its proven equivalence to learning the molecular force field. Equivalence between denoising and learning molecular force fields. The equivalence between coordinate denoising and force field learning is established by Zaidi et al. (2023). For given molecule M, perturb its equilibrium structure x0 according to the distribution p(xx0), where is the noisy conformation. Assuming the molecular distribution adheres to the energy-based Boltzmann distribution with respect to the energy function E(), then LDenoising(M) = Ep(xx0)p(x0)GNNθ(x) (x x0)2 Ep(x)GNNθ(x) (xE(x))2, (1) where GNNθ(x) denotes graph neural network parameterized by θ, which processes the conformation to produce node-level predictions. The notation signifies the equivalence of different objectives. The proof of this equivalence is provided in the Appendix A. In prior research, the energy function E() has been defined in several forms. Below are three representative studies. Energy function I: mixture of isotropic Gaussians. In Coord (Zaidi et al., 2023), the energy function is approximated using mixture of isotropic Gaussians centered at the known equilibrium structures to replace the Boltzmann distribution, since these structures are local maxima of the Boltzman distribution. Leveraging the equivalence between the score-matching objective and denoising autoencoders (Vincent, 2011), the following denoising-based energy function ECoord() is derived: ECoord(x) = 1 2τ 2 (x x0)(x x0). (2) Note that this objective is derived under the assumption of isotropic Gaussian noise, i.e., p(xx0) (x0, τ 2 I3N ), where I3N represents the identity matrix of size 3N , and the subscript indicates the coordinate denoising approach. Energy function II: mixture of anisotropic Gaussians. Considering rigid and flexible components in molecular structures, isotropic Gaussian can lead to significant approximation errors. To address the anisotropic distribution, Frad (Feng et al., 2023) introduces hybrid noise on dihedral angles of rotatable bonds and atomic coordinates, incorporating fractional denoising of the coordinate noise. The equilibrium structure x0 is initially perturbed by dihedral angle noise p(ψaψ0) (ψ0, σ2 I3N ). Here, ψa, ψ0 [0, 2π)m represent to the dihedral angles of rotatable bonds in structures xa and x0, respectively, with denoting the number of rotatable bonds. The subscript indicates the fractional denoising approach. Subsequently, the energy function is induced: Im), followed by coordinate noise p(xxa) (xa, τ 2 EFrad(x) 1 2 (x x0)Σ1 τf ,σf (x x0), (3) where Στf ,σf = τ 2 dihedral angle noise into coordinate change, expressed as Cψ. CC, and R3N is matrix used to linearly transform the I3N + σ2 Energy function III: classical potential energy theory. SliDe (Ni et al., 2024) derives energy function from classical molecular potential energy theory (Alavi, 2020; Zhou & Liu, 2022). In this 3 Published as conference paper at ICLR 2025 Figure 2: Overview of the MolSpectra pre-training framework. Our pre-training framework comprises three sub-objectives: the denoising objective and the MPR objective, which respectively guide the representation learning of the 3D and spectral modalities, and the contrastive objective, which aligns the representations of both modalities. form, the total intramolecular potential energy is mainly attributed to three types of interactions: bond stretching, bond angle bending, and bond torsion. The following energy function is derived: ESliDe(r, θ, ϕ) = + 1 2 1 2 [kB (r r0)](r r0) + [kT (ϕ ϕ0)](ϕ ϕ0), 1 2 [kA (θ θ0)](θ θ0) (4) where (R0)m1, θ [0, 2π)m2 , ϕ [0, 2π)m3 represent vectors of the bond lengths, bond angles, and bond torsion angles of the molecule, respectively. r0, θ0, ϕ0 correspond to the respective equilibrium values. The parameter vectors kB, kA, kT determine the interaction strength."
        },
        {
            "title": "3 THE PROPOSED MOLSPECTRA METHOD",
            "content": "Considering the complementarity of different spectra, we introduce multiple spectra into molecular representation learning. To effectively comprehend molecular spectra, we designed Transformerbased multi-spectrum encoder, SpecFormer, along with masked reconstruction objective to guide its training. Finally, contrastive objective is employed to align the 3D encoding guided by the denoising objective with the spectra encoding guided by the reconstruction objective, endowing the 3D encoding with the capability to understand spectra and the knowledge they encompass. 3.1 SPECFORMER: SINGLE-STREAM ENCODER FOR MULTI-MODAL ENERGY SPECTRA For different types of spectra, each spectrum is independently patched and initially encoded. Then, all the resulting patch embeddings are concatenated and encoded using Transformer-based encoder. Patching. Compared to directly encoding individual frequency points, we divided each spectrum into multiple patches. This approach offers two distinct advantages: (i) By forming patches from adjacent frequency points, local semantic features, such as absorption peaks, can be captured more effectively. (ii) It reduces the computational overhead of subsequent Transformer layers. Technically, each spectrum si RLi where = 1, , is first divided into patches according to the patch length Pi and the stride Di. When 0 < Di < Pi, the consecutive patches will be overlapped 4 Published as conference paper at ICLR with overlapping region length Pi Di. When Di = Pi, the consecutive patches will be nonoverlapped. Li denotes the length of si. The patching process on each spectrum will generate sequence of patches pi RNiPi, where Ni = + 1 is the number of patches. (cid:107) (cid:106) LiPi Di Patch encoding and position encoding. Prior to be fed into the encoder, the patches of the i-th spectrum are mapped to the latent space of dimension via trainable linear projection Wi RPid. learnable additive position encoding pos RNid is applied to maintain the order = piWi + pos RNid denotes the latent representation of the of the patches: spectrum si that will be fed into the subsequent SpecFormer encoder. , where i SpecFormer: multi-spectrum Transformer encoder. Although several encoders have been proposed to map molecular spectrum into implicit representations, such as the CNN-AM (Tao et al., 2024) based on one-dimensional convolution, these encoders are designed to encode only sinIn our approach, multiple molecular spectra (UV-Vis, IR, Raman) are gle type of spectrum. jointly considered. When encoding multiple spectra of molecule simultaneously, an observation caught our attention and led us to adopt Transformer-based encoder with multiple spectra as input, similar to the single-stream Transformer in multi-modal learning (Shin et al., 2021). The observation refers to the fact that the same functional group not only causes multiple peaks within single spectrum, but also generates peaks across different spectra. As shown on the left of Figure 3, the different vibrational modes of the methyl group (-CH3) in methanol (CH3OH) result in three peaks in the IR spectrum, indicating intra-spectrum dependencies among these peaks. similar phenomenon occurs with the hydroxyl group (-OH) in methanol. Additionally, the aromatic ring in phenol (C6H5OH), shown on the right of Figure 3, not only produces multiple peaks in the IR spectrum due to different vibrational modes but also causes an absorption peak near 270 nm in the UV-Vis spectrum due to the π π transition in the aromatic ring, demonstrating the existence of inter-spectrum dependencies. Such dependencies have been theoretically studied, for example, in the context of vibronic coupling (Kong et al., 2021). Figure 3: spectrum (right) dependencies. Illustrate of intra-spectrum (left) and interTo capture intra-spectrum and inter-spectrum dependencies, we concatenate the embeddings obtained from patch encoding and position encoding of different spectra: ˆp = S R((cid:80)S i=1 Ni)d, and then input them into the Transformer encoder as depicted in Figure 2. Then each head = 1, . . . , in multi-head attention will transform them into query matrices Qh = ˆpW , key matrices Kh = ˆpW Rd : . Afterward, scaled product is utilized to obtain the attention output Oh R((cid:80)S Rddk and WV i=1 Ni) and value matrices Vh = ˆpW , where 1 , Oh = Attention(Qh, Kh, Vh) = Softmax (cid:19) (cid:18) QhK dk Vh. (5) The multi-head attention block also includes BatchNorm layers and feed forward network with residual connections as shown in Figure 2. After combining the outputs of all heads, it generates the representation denoted as R((cid:80)S i=1 Ni)d. Finally, flatten layer with representation projection head is used to obtain the molecular spectra representation zs Rd. 3.2 MASKED PATCHES RECONSTRUCTION PRE-TRAINING FOR SPECTRA Before distilling the spectra information into 3D molecular representation learning, we need first ensure that the spectrum encoder can effectively comprehend molecular spectra and generate spectral representations. Considering the success of masking modeling across various domains (Devlin et al., 2019; He et al., 2022; Hou et al., 2022; Xia et al., 2023; Wang et al., 2024b; Nie et al., 2023), we propose masked patches reconstruction (MPR) objective to guide the training of SpecFormer. 5 Published as conference paper at ICLR 2025 After the patching step, we randomly select portion of patches according to the mask ratio α and replace them with zero vectors to implement the masking. Subsequently, the masked patches undergo patch encoding and position encoding. In this way, the semantics of the masked patches (the absorption intensity at specific wavelengths) are obscured during patch encoding, while the positional information is retained to facilitate the reconstruction of the original semantics. After encoding by SpecFormer, the encoded results corresponding to the masked patches are input into spectrum-specific reconstruction head to reconstruct the original spectral values that were masked. The mean squared error (MSE) between the reconstruction results and the original masked spectra serves as the loss function for the MPR task, guiding the training of SpecFormer: LMPR = (cid:88) i=1 pi,j (cid:101)Pi ˆpi,j pi,j2 2, (6) where (cid:101)Pi denotes the set of masked patches in the i-th type of molecular spectra, and ˆpi,j denotes the reconstructed patch corresponding to the masked patch pi,j."
        },
        {
            "title": "3.3 CONTRASTIVE LEARNING BETWEEN 3D STRUCTURES AND SPECTRA",
            "content": "Under the guidance of the denoising objective for 3D representation learning and the MPR objective for spectral representation learning, we further introduce contrastive objective to align the representations across these two modalities. We treat the 3D representation zx Rd and spectral representation zs Rd of the same molecule as positive samples, and negative samples otherwise. Subsequently, the consistency between positive samples and the discrepancy between negative samples are maximized through the contrastive objective. Given the theoretical and empirical effectiveness, we employ InfoNCE (van den Oord et al., 2018) as the contrastive objective: LContrast = 1 2 Ep(zx,zs) (cid:20) log exp(fx(zx,zs)) exp(fx(zx,zs))+(cid:80) exp(fx(zj x,zs)) + log exp(fs(zs,zx)) exp(fs(zs,zx))+(cid:80) exp(fs(zj s,zx)) (cid:21) , x, zj (7) where zj are randomly sampled 3D and spectra views regarding to the positive pair (zx, zs). fx(zx, zs) and fs(zs, zx) are scoring functions for the two corresponding views, with flexible formulations. Here we adopt fx(zx, zs) = fs(zs, zx) = zx, zs. Note that the denoising objective can utilize any form from existing 3D molecular representation pre-training studies, enabling seamless integration of our method into these frameworks. 3.4 TWO-STAGE PRE-TRAINING PIPELINE Previous pre-training efforts for 3D molecular representation have been conducted on unlabeled datasets using denoising objective. These datasets typically provide only equilibrium 3D structures without offering spectra for all molecules. To enhance the pre-training effect by incorporating spectra while leveraging denoising pre-training, we employ two-stage pre-training approach. The first stage involves training on larger dataset (Nakata & Shimazaki, 2017) without spectra using only the denoising objective. Subsequently, the second stage involves training on dataset that includes spectra using the complete objective as follows: = βDenoisingLDenoising + βMPRLMPR + βContrastLContrast, where βDenoising, βMPR, and βContrast denote the weights of each sub-objective. (8)"
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "To comprehensively evaluate the impact of molecular spectra on molecular tasks, we first verify the effectiveness of molecular spectra in the training-from-scratch method for the downstream task. Furthermore, we evaluate the effectiveness of our pre-training framework MolSpectra. 4.1 EFFECTIVENESS OF MOLECULAR SPECTRA IN TRAINING FROM SCRATCH This pilot experiment aims to demonstrate the rationality for incorporating molecular spectra into pre-training. We introduce additional spectral features into train-from-scratch molecular property 6 Published as conference paper at ICLR 2025 prediction model to observe the impact of spectral information on prediction outcomes. We employ EGNN (Satorras et al., 2021), representative 3D molecular encoder, equipped with an MLP-based prediction head as the baseline model. While EGNN encodes the 3D representations, the UV-Vis spectrum of each molecule provided by the QM9S (Zou et al., 2023) dataset is encoded into spectral representations by spectrum encoder. Before making predictions with the final MLP, we concatenate the spectral and 3D representations for prediction. The results are presented in Table 1. Table 1: Performance (MAE ) when training from scratch on QM9 dataset. Task Units µ (D) α (a3 0) homo (meV) lumo (meV) gap (meV) w/o spectra 0.029 0.071 w/ spectra 0. 0.049 29 28 25 24 43 R2 (a2 0) 0.106 0.084 ZPVE (meV) U0 (meV) (meV) (meV) (meV) 1.55 1.45 10 12 11 12 10 10 ( Cv cal molK ) 0.031 0.030 We observe that by directly concatenating spectral representations, the performance of molecular property prediction can be effectively enhanced. This indicates that the information from molecular spectra is beneficial for downstream molecular property prediction. Further incorporating molecular spectra into the pre-training phase of molecular representation has the potential to enhance the informativeness and generalization capability of the representations, thereby broadly improving the performance of downstream tasks. 4.2 EFFECTIVENESS OF MOLECULAR SPECTRA IN REPRESENTATION PRE-TRAINING We conduct experiments to evaluate MolSpectra by first introducing spectral data into the pretraining of 3D representations, followed by evaluating the performance on downstream tasks. For comprehensive comparison, two types of baselines are adopted: (1) training-from-scratch including SchNet (Schutt et al., 2017), EGNN, DimeNet (Klicpera et al., 2020b), methods, DimeNet++ (Klicpera et al., 2020a), PaiNN (Schutt et al., 2021), SphereNet (Liu et al., 2021), and TorchMD-Net (Tholke & Fabritiis, 2022); and (2) pre-training methods, including TransformerM (Luo et al., 2023), SE(3)-DDM (Liu et al., 2023b), 3D-EMGP (Jiao et al., 2023), and Coord. MolSpectra can be seamlessly plugged into any existing denoising method. To evaluate the enhancement provided by our method compared to denoising alone, we select the representative coordinate denoising (Coord) as our denoising sub-objective. This method also serves as our primary baseline. 4.2.1 PRE-TRAINING DATASET. As described in Section 3.4, we first perform denoising pre-training on the PCQM4Mv2 (Nakata & Shimazaki, 2017) dataset, followed by second stage of pre-training on the QM9Spectra (QM9S) (Zou et al., 2023) dataset, which includes multi-modal molecular energy spectra. In both stages, we adopt the denoising objective provided by Coord (Zaidi et al., 2023), as defined in Eq. 2. The QM9S dataset comprises organic molecules from the QM9 (Ramakrishnan et al., 2014) dataset. The UV-Vis, IR, and Raman spectra of the molecules are calculated at the B3LYP/def-TZVP level of theory, through frequency analysis and time-dependent density functional theory (TD-DFT). 4.2.2 QM The QM9 dataset is quantum chemistry dataset comprising over 134,000 small molecules, each consisting of up to 9 hydrogen (H), carbon (C), nitrogen (N), oxygen (O), and fluorine (F) atoms. This dataset provides an equilibrium geometric conformation for each molecule along with 12 property labels. The dataset is divided into training set of 110k molecules, validation set of 10k molecules, and test set containing the remaining over 10k molecules. Prediction errors are measured using the mean absolute error (MAE). The experimental results are presented in Table 2. The 3D molecular representations pre-trained using our method are fine-tuned and used for prediction across various properties, achieving state-of-the-art performance in 8 out of 12 properties and outperforms Coord in 10 out of 12 properties. In conjunction with the observations in Section 4.1, the performance improvement can be attributed to our incorporation of an understanding of molecular spectra and the knowledge they entail into the 3D molecular representations. 7 Published as conference paper at ICLR 2025 Table 2: Performance (MAE) on QM9 dataset. The compared methods are divided into two groups: training from scratch and pre-training then fine-tuning. The best results are highlighted in bold. µ (D) α (a3 0) homo lumo (meV) (meV) gap (meV) R2 (a2 0) ZPVE (meV) U0 (meV) (meV) (meV) (meV) Cv cal molK ) ( SchNet EGNN DimeNet++ PaiNN SphereNet TorchMD-Net 0.033 0.235 0.029 0.071 0.030 0.044 0.012 0.045 0.025 0.045 0.011 0.059 Transformer-M 0.037 0.041 0.015 0.046 SE(3)-DDM 0.020 0.057 3D-EMGP 0.016 0.052 Coord 0.011 0.048 MolSpectra 41.0 29.0 24.6 27.6 22.8 20.3 17.5 23.5 21.3 17.7 15.5 34.0 25.0 19.5 20.4 18.9 17.5 16.2 19.5 18.2 14.7 13. 63.0 0.070 48.0 0.106 32.6 0.330 45.7 0.070 31.1 0.270 36.1 0.033 27.4 0.075 40.2 0.122 37.1 0.092 31.8 0.450 26.8 0.410 1.70 14.00 19.00 14.00 14.00 1.55 11.00 12.00 12.00 12.00 7.56 6.28 1.21 7.35 5.83 1.28 1.12 7.78 6.36 7.62 6.38 1.84 6.32 5.85 6.26 6.15 6.53 5.98 6.33 6.16 1.18 1.31 1.38 1.71 1. 9.37 6.92 8.60 6.57 5.67 9.41 6.99 8.60 6.11 5.45 9.39 7.09 8.70 6.45 5.87 9.63 7.65 9.30 6.91 6.18 0.033 0.031 0.023 0.024 0.022 0.026 0.022 0.024 0.026 0.020 0. Table 3: Performance (MAE) on MD17 force prediction (kcal/mol/ A). The methods are divided into two groups: training from scratch and pre-training then fine-tuning. The best results are in bold. Aspirin Benzene Ethanol Malonal -dehyde Naphtha -lene Salicy -lic Acid Toluene Uracil SphereNet SchNet DimeNet PaiNN TorchMD-Net SE(3)-DDM* Coord MolSpectra 0.430 1.350 0.499 0.338 0.245 0.453 0.211 0.099 0.178 0.310 0.187 - 0.219 - 0.169 0. 0.208 0.390 0.230 0.224 0.107 0.166 0.096 0.052 0.340 0.660 0.383 0.319 0.167 0.288 0.139 0.077 0.178 0.580 0.215 0.077 0.059 0.129 0.053 0. 0.360 0.850 0.374 0.195 0.128 0.266 0.109 0.093 0.155 0.570 0.216 0.094 0.064 0.122 0.058 0.075 0.267 0.560 0.301 0.139 0.089 0.183 0.074 0. 4.2.3 MD17 The MD17 dataset contains molecular dynamics trajectories for eight organic molecules, including aspirin, benzene, and ethanol. It offers 150k to nearly 1M conformations per molecule, with energy and force labels. Unlike QM9, MD17 emphasizes dynamic behavior in addition to static properties. We use standard limited data split: models train on 1k samples, validate on 50, and test on the rest. Performance is evaluated using MAE, with results in Table 3. Our approach also results in the expected performance improvement on MD17. MD17 is dataset comprising large number of non-equilibrium molecular structures and their corresponding force fields, which serves to evaluate models understanding of molecular dynamics. However, previous pre-training methods based solely on denoising have only learned force field patterns at static equilibrium states, failing to adequately capture the dynamic evolution of molecular systems. In contrast, our MolSpectra learns the dynamic evolution of molecules by understanding energy level transition patterns, thereby outperforming denoising-based pre-training methods. 4.3 SENSITIVITY ANALYSIS OF PATCH LENGTH Pi, STRIDE Di, AND MASK RATIO α We conduct experiments to evaluate the impact of patch length Pi, stride Di, and mask ratio α. Results are summarized in Table 4 and Table 5. From Table 4, we observe that when consecutive patches have overlap (Di < Pi), the performance of pre-training is superior compared to scenarios without overlap (Di = Pi). Specifically, the performance is optimal when the stride is half of the patch length. This is because appropriate overlap can better preserve and capture local features, particularly the information at the patch boundaries. Additionally, we find that choosing an appropriate patch length further enhances performance. In our experiments, the configuration of Pi = 20, Di = 10 yields the best results. Published as conference paper at ICLR 2025 Table 4: Sensitivity of patch length and stride. Table 5: Sensitivity of mask ratio. patch length stride overlap ratio homo lumo gap mask ratio homo lumo gap 20 20 20 20 16 30 5 10 15 20 8 15 75% 50% 25% 0% 50% 50% 15.9 15.5 16.1 15.7 16.0 15.9 13.7 13.1 13.6 13.5 13.4 14. 28.0 26.8 28.1 27.5 27.6 28.1 0.05 0.10 0.15 0.20 0.25 0.30 15.7 15.5 15.7 16.0 16.3 16.2 13.4 13.1 13.5 13.6 13.5 13.7 29.7 26.8 28.0 28.1 28.0 29.0 Regarding the mask ratio, α = 0.10 is preferable choice. small mask ratio results in insufficient MPR optimization, hindering SpecFormer training. Conversely, large mask ratio causes excessive spectral perturbation, degrading performance when aligning with the 3D representations with the contrastive objective. An appropriate mask ratio strikes balance between these two aspects. 4.4 ABLATION STUDY To rigorously demonstrate the contributions of masked patches reconstruction, the incorporation of molecular spectra, and each spectral modality, we conducted an ablation study on them. Table 6: Ablation of optimization objectives. Ablation study of masked patches reconstruction. We remove the MPR loss to analyze the impact of masked patches reconstruction, referred to as w/o MPR in Table 6. Removing the MPR objective leads to performance deterioration. This is consistent with the sensitivity analysis of the mask ratio α in Section 4.3, as removing MPR is an extreme case where α = 0. This decline is due to the lack of effective guidance in training SpecFormer. Using an undertrained SpecFormer for contrastive learning with 3D encoder outputs limits performance improvement. MolSpectra w/o MPR w/o MPR, Contrast 13.1 14.1 14. 15.5 16.4 17.5 26.8 29.7 31.2 homo lumo gap Ablation study of molecular spectra. We retain only the denoising loss, removing both the MPR loss and contrastive loss, referred to as w/o MPR, Contrast in Table 6. The only difference between this variant and MolSpectra is the incorporation of molecular spectra into the pre-training. The w/o MPR, Contrast results are inferior to those of MolSpectra, highlighting that incorporating molecular spectra effectively enhances the quality and generalizability of molecular 3D representations. Ablation study of each spectral modality. To evaluate the contributions of each spectral modality to the performance, we conduct an ablation study for each modality. The results are presented in Table 7. It can be observed that each spectral modality contributes differently, with the UV-Vis spectrum having the smallest contribution and the IR spectrum the largest, likely due to the varying information content in each modality. Table 7: Ablation of spectral modalities. UV-Vis (cid:33) - (cid:33) (cid:33) IR (cid:33) (cid:33) - (cid:33) Raman homo lumo gap (cid:33) (cid:33) (cid:33) - 15.5 15.8 16.6 16.1 13.1 13.3 14.1 13.9 26.8 27.1 28.9 28."
        },
        {
            "title": "5 RELATED WORK",
            "content": "3D molecular pre-training. Molecular 2D structures are typically represented as graphs and modeled using graph learning methods (Gilmer et al., 2017; Li et al., 2023; Jiang et al., 2024). However, 3D molecular structures provide critical geometric information that is essential for understanding physicochemical properties (Chen et al., 2023; 2024; Wang et al., 2024a; Sun et al., 2024), which cannot be directly inferred from 2D graphs or SMILES representations (Gong et al., 2024). Designing effective strategies for pre-training 3D molecular representations remains challenging due to the geometric symmetries inherent in 3D structures and their strong connection to physical knowledge, such as potential energy functions. 9 Published as conference paper at ICLR 2025 Denoising the geometric structure has been demonstrated as an effective strategy for 3D representation pre-training (Liu et al., 2023b; Jiao et al., 2023; Kim et al., 2023; Zhou et al., 2023; Wang et al., 2025). Coordinate denoising (Coord) (Zaidi et al., 2023) first theoretically proves that the denoising objective is equivalent to learning the gradient of the potential energy with respect to positions, essentially the force field. Building on this work, fractional denoising (Frad) (Feng et al., 2023) introduces dihedral angle noise to optimize the sampling of low-energy structures. Further, SliDe (Ni et al., 2024) incorporates more rigorous potential energy from classical mechanics. Another line of research simultaneously leverages both 2D and 3D structures for pre-training molecular representations, addressing the complementarity of the two modalities (Li et al., 2022; Zhu et al., 2022; Liu et al., 2023a; Du et al., 2023a; Yu et al., 2024) or the computational complexity of 3D structure determination (Liu et al., 2022; Stark et al., 2022; Wang et al., 2023a). Although these studies elucidate the relationship between molecular 3D structures and their energy states, they remain limited to the description of molecular energy states within classical mechanics, without considering the quantized energy level structures as described by quantum mechanics. Molecular spectroscopy. Molecular spectroscopy studies interactions between molecules and electromagnetic radiation. Analyzing spectra provides valuable insights into molecular structure, composition, and dynamics (Lancaster et al., 2024). When encountering unknown substances, researchers conduct spectroscopic measurements on samples and compare the observed spectra with libraries for identification. To expand library coverage, machine learning methods are widely used to predict molecules spectra (Zou et al., 2023; Wei et al., 2018; Zong et al., 2024). Some studies incorporate physical principles into spectra prediction models as inductive biases, including molecular dynamics simulations via equivariant message passing (Schutt et al., 2021), fragmentation (Duhrkop et al., 2020; Cao et al., 2020; Goldman et al., 2023a), motifs (Park et al., 2023), and long-distance atomic interactions (Young et al., 2024). Another line of research approach bypasses spectral library comparison and directly performs de novo structure elucidation from spectra (Stravs et al., 2021; Goldman et al., 2023b; Tao et al., 2024). Since different spectroscopic techniques offer complementary advantages, the joint analysis of multiple spectra can provide comprehensive information (Alberts et al., 2024). In this study, we encodes multiple spectra, and introduce them into molecular representation pre-training for the first time."
        },
        {
            "title": "6 CONCLUSION",
            "content": "In this study, we explore pre-training molecular 3D representations beyond classical mechanics. By leveraging the correlation between molecular energy level structures and molecular spectra in quantum mechanics, we introduce molecular spectra for pre-training molecular 3D representations (MolSpectra). By aligning the 3D encoder trained with denoising objective and the spectrum encoder trained with masked patch reconstruction objective, we enhance the informativeness and transferability of the resulting 3D representations. 10 Published as conference paper at ICLR"
        },
        {
            "title": "ACKNOWLEDGMENTS",
            "content": "This work is (2023ZD0120901) and National Natural Science Foundation of China (62372454, 62236010). jointly supported by National Science and Technology Major Project"
        },
        {
            "title": "REFERENCES",
            "content": "Saman Alavi. Intraand intermolecular potentials in simulations. In Chapter 3, pp. 3971. John Wiley & Sons, Ltd, 2020. ISBN 9783527699452. doi: 10.1002/9783527699452.ch3. Marvin Alberts, Oliver Schilter, Federico Zipoli, Nina Hartrampf, and Teodoro Laino. Unraveling molecular structure: multimodal spectroscopic dataset for chemistry. In NeurIPS Datasets and Benchmarks Track, 2024. Ilyes Batatia, David Peter Kovacs, Gregor N. C. Simm, Christoph Ortner, and Gabor Csanyi. Mace: Higher order equivariant message passing neural networks for fast and accurate force fields. In NeurIPS, 2022. Liu Cao, Mustafa Guler, Azat M. Tagirdzhanov, Yi-Yuan Lee, Alexey A. Gurevich, and Hosein Mohimani. Moldiscovery: learning mass spectrometry fragmentation of small molecules. Nature Communications, 12, 2020. Dingshuo Chen, Yanqiao Zhu, Jieyu Zhang, Yuanqi Du, Zhixun Li, Qiang Liu, Shu Wu, and Liang Wang. Uncovering neural scaling laws in molecular representation learning. In NeurIPS, 2023. Dingshuo Chen, Zhixun Li, Yuyan Ni, Guibin Zhang, Ding Wang, Qiang Liu, Shu Wu, Jeffrey Xu Yu, and Liang Wang. Beyond efficiency: Molecular data pruning for enhanced generalization. In NeurIPS, 2024. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey E. Hinton. simple framework for contrastive learning of visual representations. In ICML, volume 119, pp. 15971607, 2020. Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. BERT: pre-training of deep bidirectional transformers for language understanding. In NAACL-HLT (1), pp. 41714186. Association for Computational Linguistics, 2019. Weitao Du, Jiujiu Chen, Xuecang Zhang, Zhi-Ming Ma, and Shengchao Liu. Molecule joint autoencoding: Trajectory pretraining with 2d and 3d diffusion. In NeurIPS, 2023a. Weitao Du, Yuanqi Du, Limei Wang, Dieqiao Feng, Guifeng Wang, Shuiwang Ji, Carla P. Gomes, and Zhi-Ming Ma. new perspective on building efficient and expressive 3d equivariant graph neural networks. In NeurIPS, 2023b. Kai Duhrkop, Louis-Felix Nothias, Markus Fleischauer, Raphael Reher, Marcus Ludwig, Martin A. Hoffmann, Daniel Petras, William H. Gerwick, Juho Rousu, Pieter C. Dorrestein, and Sebastian Bocker. Systematic classification of unknown metabolites using high-resolution fragmentation mass spectra. Nature Biotechnology, 39:462 471, 2020. Shikun Feng, Yuyan Ni, Yanyan Lan, Zhi-Ming Ma, and Wei-Ying Ma. Fractional denoising for 3d molecular pre-training. In ICML, volume 202. PMLR, 2023. Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message passing for quantum chemistry. In ICML, 2017. Samuel Goldman, John Bradshaw, Jiayi Xin, and Connor W. Coley. Prefix-tree decoding for predicting mass spectra from molecules. In NeurIPS, 2023a. Samuel Goldman, Jeremy Wohlwend, Martin, Strazar, Guy Haroush, Ramnik J. Xavier, W. Connor, and Coley. Annotating metabolite mass spectra with domain-inspired chemical formula transformers. Nature Machine Intelligence, 2023b. Haisong Gong, Qiang Liu, Shu Wu, and Liang Wang. Text-guided molecule generation with diffusion language model. In AAAI, 2024. 11 Published as conference paper at ICLR Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollar, and Ross B. Girshick. Masked autoencoders are scalable vision learners. In CVPR, pp. 1597915988. IEEE, 2022. Zhenyu Hou, Xiao Liu, Yukuo Cen, Yuxiao Dong, Hongxia Yang, Chunjie Wang, and Jie Tang. Graphmae: Self-supervised masked graph autoencoders. In KDD, pp. 594604. ACM, 2022. Weihua Hu, Bowen Liu, Joseph Gomes, Marinka Zitnik, Percy Liang, Vijay S. Pande, and Jure Leskovec. Strategies for pre-training graph neural networks. In ICLR, 2020. Xinke Jiang, Rihong Qiu, Yongxin Xu, Wentao Zhang, Yichen Zhu, Ruizhe Zhang, Yuchen Fang, Xu Chu, Junfeng Zhao, and Yasha Wang. Ragraph: general retrieval-augmented graph learning framework. In NeurIPS, 2024. Rui Jiao, Jiaqi Han, Wenbing Huang, Yu Rong, and Yang Liu. Energy-motivated equivariant pretraining for 3d molecular graphs. In AAAI, pp. 80968104. AAAI Press, 2023. Dongki Kim, Jinheon Baek, and Sung Ju Hwang. Graph self-supervised learning with accurate discrepancy learning. In NeurIPS, 2022. Hyeonsu Kim, Jeheon Woo, Seonghwan Kim, Seokhyun Moon, Jun Hyeong Kim, and Woo Youn Kim. Geotmi: Predicting quantum chemical property with easy-to-obtain geometry via positional denoising. In NeurIPS, 2023. Johannes Klicpera, Shankari Giri, Johannes T. Margraf, and Stephan Gunnemann. and uncertainty-aware directional message passing for non-equilibrium molecules. abs/2011.14115, 2020a. Fast arXiv, Johannes Klicpera, Janek Groß, and Stephan Gunnemann. Directional message passing for molecular graphs. In ICLR, 2020b. Fan-Fang Kong, Xiao-Jun Tian, Yang Zhang, Yun-Jie Yu, Shi-Hao Jing, Yao Zhang, Guangjun Tian, Yi Luo, Jinlong Yang, Zhenchao Dong, and J. G. Hou. Probing intramolecular vibronic coupling through vibronic-state imaging. Nature Communications, 12, 2021. Noah M. Lancaster, Pavel Sinitcyn, Patrick Forny, Trenton M. Peters-Clarke, Caroline Fecher, Andrew J. Smith, Evgenia Shishkova, Tabiwang N. Arrey, Anna Pashkova, Margaret Lea Robinson, Nicholas L. Arp, Jing Fan, Julia K. Hansen, Andrea Galmozzi, Lia R. Serrano, Julie Rojas, Audrey P. Gasch, Michael S. Westphall, Hamish Stewart, Christian Hock, Eugen Damoc, David J. Pagliarini, Vlad Zabrouskov, and Joshua J. Coon. Fast and deep phosphoproteome analysis with the orbitrap astral mass spectrometer. Nature Communications, 15, 2024. Shuangli Li, Jingbo Zhou, Tong Xu, Dejing Dou, and Hui Xiong. Geomgcl: Geometric graph In AAAI, pp. 45414549. AAAI Press, contrastive learning for molecular property prediction. 2022. Zhixun Li, Liang Wang, Xin Sun, Yifan Luo, Yanqiao Zhu, Dingshuo Chen, Yingtao Luo, Xiangxin Zhou, Qiang Liu, Shu Wu, Liang Wang, and Jeffrey Xu Yu. GSLB: the graph structure learning benchmark. In NeurIPS, 2023. Yi-Lun Liao and Tess E. Smidt. Equiformer: Equivariant graph attention transformer for 3d atomistic graphs. In ICLR, 2023. Shengchao Liu, Hanchen Wang, Weiyang Liu, Joan Lasenby, Hongyu Guo, and Jian Tang. Pretraining molecular graph representation with 3d geometry. In ICLR, 2022. Shengchao Liu, Weitao Du, Zhi-Ming Ma, Hongyu Guo, and Jian Tang. group symmetric stochastic differential equation model for molecule multi-modal pretraining. In ICML, volume 202, pp. 2149721526. PMLR, 2023a. Shengchao Liu, Hongyu Guo, and Jian Tang. Molecular geometry pretraining with se(3)-invariant denoising distance matching. In ICLR, 2023b. Yi Liu, Limei Wang, Meng Liu, Xuan Zhang, Bora Oztekin, and Shuiwang Ji. Spherical message passing for 3d molecular graphs. In International Conference on Learning Representations, 2021. 12 Published as conference paper at ICLR Zhiyuan Liu, Yaorui Shi, An Zhang, Enzhi Zhang, Kenji Kawaguchi, Xiang Wang, and Tat-Seng Chua. Rethinking tokenizer and decoder in masked graph modeling for molecules. In NeurIPS, 2023c. Shengjie Luo, Tianlang Chen, Yixian Xu, Shuxin Zheng, Tie-Yan Liu, Liwei Wang, and Di He. One transformer can understand both 2d & 3d molecular data. In ICLR, 2023. Hehuan Ma, Feng Jiang, Yu Rong, Yuzhi Guo, and Junzhou Huang. Toward robust self-training paradigm for molecular prediction tasks. Journal of Computational Biology, 31(3):213228, 2024. doi: 10.1089/cmb.2023.0187. Albert Musaelian, Simon L. Batzner, Anders Johansson, Lixin Sun, Cameron J. Owen, Mordechai Kornbluth, and Boris Kozinsky. Learning local equivariant representations for large-scale atomistic dynamics. Nature Communications, 14, 2023. Maho Nakata and Tomomi Shimazaki. Pubchemqc project: large-scale first-principles electronic structure database for data-driven chemistry. Journal of chemical information and modeling, 57 6:13001308, 2017. Yuyan Ni, Shikun Feng, Wei-Ying Ma, Zhi-Ming Ma, and Yanyan Lan. Sliced denoising: physicsinformed molecular pre-training method. In ICLR, 2024. Yuqi Nie, Nam H. Nguyen, Phanwadee Sinthong, and Jayant Kalagnanam. time series is worth 64 words: Long-term forecasting with transformers. In ICLR, 2023. Jiwon Victoria Park, Jeonghee Jo, and Sungroh Yoon. Mass spectra prediction with structural motifbased graph neural networks. Scientific Reports, 14, 2023. Raghunathan Ramakrishnan, Pavlo O. Dral, Pavlo O. Dral, Matthias Rupp, and O. Anatole von Lilienfeld. Quantum chemistry structures and properties of 134 kilo molecules. Scientific Data, 1, 2014. Yu Rong, Yatao Bian, Tingyang Xu, Weiyang Xie, Ying Wei, Wenbing Huang, and Junzhou Huang. Self-supervised graph transformer on large-scale molecular data. In NeurIPS, 2020. Victor Garcia Satorras, Emiel Hoogeboom, and Max Welling. E(n) equivariant graph neural networks. In ICML, volume 139, pp. 93239332. PMLR, 2021. Kristof Schutt, Oliver T. Unke, and Michael Gastegger. Equivariant message passing for the prediction of tensorial properties and molecular spectra. In ICML, volume 139 of Proceedings of Machine Learning Research, pp. 93779388. PMLR, 2021. Kristof T. Schutt, Huziel E. Sauceda, Kindermans, Alexandre Tkatchenko, and Klaus-Robert Muller. Schnet - deep learning architecture for molecules and materials. The Journal of chemical physics, 148 24:241722, 2017. Andrew Shin, Masato Ishii, and Takuya Narihira. Perspectives and prospects on transformer architecture for cross-modal tasks with language and vision. International Journal of Computer Vision, 130:435 454, 2021. Hannes Stark, Dominique Beaini, Gabriele Corso, Prudencio Tossou, Christian Dallago, Stephan Gunnemann, and Pietro Lio. 3d infomax improves gnns for molecular property prediction. In ICML, volume 162, pp. 2047920502. PMLR, 2022. Michael A. Stravs, Kai Duhrkop, Sebastian Bocker, and Nicola Zamboni. Msnovelist: de novo structure generation from mass spectra. Nature Methods, 19:865 870, 2021. Xin Sun, Liang Wang, Qiang Liu, Shu Wu, Zilei Wang, and Liang Wang. DIVE: subgraph disagreement for graph out-of-distribution generalization. In KDD, 2024. Shijie Tao, Yi Feng, Wenmin Wang, Tiantian Han, Pieter Smith, and Jun Jiang. machine learning protocol for geometric information retrieval from molecular spectra. Artificial Intelligence Chemistry, 2024. 13 Published as conference paper at ICLR 2025 Philipp Tholke and Gianni De Fabritiis. Equivariant transformers for neural network based molecular potentials. In ICLR, 2022. Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Representation learning with contrastive predictive coding. arXiv, abs/1807.03748, 2018. Pascal Vincent. connection between score matching and denoising autoencoders. Neural Computation, 23:16611674, 2011. Liang Wang, Qiang Liu, Shaozhen Liu, Xin Sun, Shu Wu, and Liang Wang. Pin-tuning: Parameterefficient in-context tuning for few-shot molecular property prediction. In NeurIPS, 2024a. Liang Wang, Xiang Tao, Qiang Liu, Shu Wu, and Liang Wang. Rethinking graph masked autoencoders through alignment and uniformity. In AAAI, 2024b. Liang Wang, Chao Song, Zhiyuan Liu, Yu Rong, Q. Liu, Shu Wu, and Liang Wang. Diffusion models for molecules: survey of methods and tasks. arXiv, abs/2502.09511, 2025. Xu Wang, Huan Zhao, Wei-Wei Tu, and Quanming Yao. Automated 3d pre-training for molecular property prediction. In KDD, pp. 24192430. ACM, 2023a. Yiqun Wang, Yuning Shen, Shi Chen, Lihao Wang, Fei Ye, and Hao Zhou. Learning harmonic molecular representations on riemannian manifold. In ICLR, 2023b. Yuyang Wang, Jianren Wang, Zhonglin Cao, and Amir Barati Farimani. Molecular contrastive learning of representations via graph neural networks. Nature Machine Intelligence, 4(3):279 287, 2022. Jennifer N. Wei, Jennifer N. Wei, David Belanger, Ryan P. Adams, and D. Sculley. Rapid prediction of electronionization mass spectrometry using neural networks. ACS Central Science, 5:700 708, 2018. Jun Xia, Chengshuai Zhao, Bozhen Hu, Zhangyang Gao, Cheng Tan, Yue Liu, Siyuan Li, and Stan Z. Li. Mole-bert: Rethinking pre-training graph neural networks for molecules. In ICLR, 2023. Adamo Young, Bo Wang, and Hannes Rost. Tandem mass spectrum prediction for small molecules using graph transformers. Nature Machine Intelligence, 2024. Qiying Yu, Yudi Zhang, Yuyan Ni, Shikun Feng, Yanyan Lan, Hao Zhou, and Jingjing Liu. Multimodal molecular pretraining via modality blending. In ICLR, 2024. Sheheryar Zaidi, Michael Schaarschmidt, James Martens, Hyunjik Kim, Yee Whye Teh, Alvaro Sanchez-Gonzalez, Peter W. Battaglia, Razvan Pascanu, and Jonathan Godwin. Pre-training via denoising for molecular property prediction. In ICLR, 2023. Gengmo Zhou, Zhifeng Gao, Qiankun Ding, Hang Zheng, Hongteng Xu, Zhewei Wei, Linfeng Zhang, and Guolin Ke. Uni-mol: universal 3d molecular representation learning framework. In ICLR, 2023. Kun Zhou and Bo Liu. Chapter 2 - potential energy functions. In Molecular Dynamics Simulation, pp. 4165. Elsevier, 2022. ISBN 978-0-12-816419-8. doi: 10.1016/B978-0-12-816419-8. 00007-6. Jinhua Zhu, Yingce Xia, Lijun Wu, Shufang Xie, Tao Qin, Wengang Zhou, Houqiang Li, and TieYan Liu. Unified 2d and 3d pre-training of molecular representations. In KDD, pp. 26262636. ACM, 2022. Yu Zong, Yuxin Wang, Xipeng Qiu, Xuanjing Huang, and Liang Qiao. Deep learning prediction of glycopeptide tandem mass spectra powers glycoproteomics. Nature Machine Intelligence, 2024. Zihan Zou, Yujin Zhang, Lijun Liang, Mingzhi Wei, Jiancai Leng, Jun Jiang, Yi Luo, and Wei Hu. deep learning model for predicting selected organic molecular spectra. Nature Computional Science, 3(11):957964, 2023."
        },
        {
            "title": "Appendix",
            "content": "Contents of the appendix Proof of theoretical results Visualization and analysis of spectra Implementation details C.1 Hardware and software . . C.2 Model configuration . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Limitations and potential future directions More experimental results and discussions 15 15 16 16 16 17 Visualization of attention patterns and learned spectra representations in SpecFormer"
        },
        {
            "title": "A PROOF OF THEORETICAL RESULTS",
            "content": "Theorem A.1 (Equivalence between the denoising objective and the learning of molecular force fields (Zaidi et al., 2023)). Assume the conformation distribution is mixture of Gaussian distribution centered at the equilibriums: (cid:90) p(x) = p(xx0)p(x0), p(xx0) (x0, τ 2I3N ) (A1) x0, R3N are equilibrium and noisy conformation respectively, is the number of atoms in the molecule. It relates to molecular energy by Boltzmann distribution p(x) exp(E(x)). Then given sampled molecule M, the denoising loss on the conformation coordinates is an equivalent optimization target to force field prediction: LDenoising(M) = Ep(xx0)p(x0)GNNθ(x) (x x0)2 Ep(x)GNNθ(x) (xE(x))2, (A2) (A3) where GNNθ(x) denotes graph neural network with parameters θ which takes conformation as an input and returns node-level noise predictions, denotes equivalence. Proof. According to Boltzmann distribution, Eq. A3 is equal to Ep(x)GN Nθ(x)x log p(x)2. By using conditional score matching lemma (Vincent, 2011), the equation above is further equal to Ep(xx0)p(x0)GN Nθ(x) log p(xx0)2 + T1, where T1 is constant independent of θ. Then with the Gaussian assumption, it becomes Ep(xx0)p(x0)GN Nθ(x) x0x 2 + T1. Finally, τ 2 since coefficients 1 τ 2 do not rely on the input x, it can be absorbed into GNNθ, thus obtaining Eq. A2."
        },
        {
            "title": "B VISUALIZATION AND ANALYSIS OF SPECTRA",
            "content": "In this section, we visualize the three types of spectra we utilize (UV-Vis, IR, Raman) and standardize the initial spectral data based on data analysis. In Figure A1, we visualize 20 randomly sampled spectra from QM9S for each type of spectrum. notable pattern observed is that, although each spectrum consists of numerous absorption peaks, there are significant differences in the heights (absorption intensities) of these peaks. For instance, in the IR spectra, the absorption intensity at most peaks is around 200, but few peaks reach an intensity of 800. However, in qualitative analysis, the position and shape of the peaks are more critical than their heights. Therefore, the differences in peak absorption intensities can interfere with model training under the MSE loss metric. To address this issue, we pre-process the absorption intensities of the spectra by applying log10 transformation to mitigate the interference caused by peak intensity differences. 15 Published as conference paper at ICLR 2025 Figure A1: Randomly sampled examples of molecular energy spectra."
        },
        {
            "title": "C IMPLEMENTATION DETAILS",
            "content": "C.1 HARDWARE AND SOFTWARE Our experiments are conducted on Linux servers equipped with 184 Intel Xeon Platinum 8469C CPUs, 920GB RAM, and 8 NVIDIA H20 96GB GPUs. Our model is implemented in PyTorch version 2.3.1, PyTorch Geometric version 2.5.3 (https://pyg.org/) with CUDA version 12.1, and Python 3.10.14. C.2 MODEL CONFIGURATION The SpecFormer is implemented using 3-layer Transformer with 16 attention heads. Following previous works, we set both and dk as 256. TorchMD-Net (Tholke & Fabritiis, 2022) is adopted as the 3D molecular encoder. We tune the mask ratio (i.e., α) in {0.05, 0.10, 0.15, 0.20, 0.25, 0.30}, tune the stride/patch length pair (i.e., Di/Pi) in {5/20, 10/20, 15/20, 20/20, 8/16, 15/30}, and tune the weights of sub-objectives (i.e., βDenoising, βMPR, and βContrast ) in {0.01, 0.1, 1, 10}. Since our goal is to align the 3D representations and spectra representations of molecules during the pretraining phase, and not rely on molecular spectra data during downstream fine-tuning, these hyperparameters related to molecular spectra are tuned on the pre-training dataset. Based on the results of hyper-parameter tuning, we adopt α = 0.10, Di = 10, Pi = 20, βDenoising = 1.0, βMPR = 1.0, and βContrast = 1.0. Following SimCLR (Chen et al., 2020), the contrastive loss in our Eq. 7 is implemented using in-batch contrastive loss, where positive and negative pairs are constructed within each data batch. Therefore, for each anchor representation in batch, there is one positive sample and bs 1 negative samples, where bs is the batch size. In our method, bs = 128. In both pre-training stages, we use the noise generation method and denoising objective provided by Coord (Zaidi et al., 2023), specifically energy function as described in Section 2.2. The noise is added to atom positions as scaled mixture of isotropic Gaussian noise, with scaling factor of 0.04. The denoising objective is defined in Eq. 2. For baselines, we follow their recommended settings."
        },
        {
            "title": "D LIMITATIONS AND POTENTIAL FUTURE DIRECTIONS",
            "content": "One limitation of our method is the availability, scale, and diversity of molecular spectral data. Our current dataset comprises geometric structures of 134,000 molecules, each with three types of spectra (UV-Vis, IR, Raman). To effectively explore the scaling laws of pre-training methods, larger and more diverse molecular spectral datasets are necessary. Encouragingly, molecular spectroscopy has been gaining increasing attention in the research community, with larger and more diverse datasets being released, such as the recent multimodal spectroscopic dataset (Alberts et al., 2024). This development supports advancements in molecular representation learning and other related tasks. Another limitation is that our proposed SpecFormer can currently only handle one-dimensional molecular spectra. For higher-dimensional spectra, such as two-dimensional NMR and twodimensional correlation spectra, further development of sophisticated spectrum encoder is needed. 16 Published as conference paper at ICLR 2025 Looking ahead, we envision several future directions in this field. First, there is potential in investigating the scaling laws of pre-training on larger and more diverse molecular spectral datasets. Second, expanding the scope of molecular spectrum encoding to include wider range, such as NMR, mass spectra, and two-dimensional spectra, could be highly beneficial. Third, while pretrained spectral encoder has been developed in our method, we have so far only applied the pretrained 3D encoder to downstream tasks. Exploring the use of the pre-trained spectral encoder for molecular spectrum-related downstream tasks, such as automated molecular structure elucidation from spectra, represents an promising opportunity. Finally, current molecular 3D pre-training methods are designed based on TorchMD-Net (Tholke & Fabritiis, 2022). With the development of equivariant message passing neural networks, more expressive backbone architectures, such as Allegro (Musaelian et al., 2023) and MACE (Batatia et al., 2022) have been proposed, improving the prediction of molecular properties when trained from scratch. Extending pre-training strategies to these state-of-the-art architectures holds the promise of further advancing downstream tasks."
        },
        {
            "title": "E MORE EXPERIMENTAL RESULTS AND DISCUSSIONS",
            "content": "In addition to Coord, we evaluate the effect of incorporating SliDe into our MolSpectra. SliDe (Ni et al., 2024) is also denoising-based pre-training method, utilizing the TorchMD-Net (Tholke & Fabritiis, 2022) as its encoder backbone, consistent with previous pre-training work (Zaidi et al., 2023; Feng et al., 2023). The results are presented in Table A1 and Table A2. Table A1: Performance (MAE) on QM9 dataset. The better result between the two variants of each pretraining method, w/ and w/o MolSpectra, is highlighted in bold. µ makecell[c](D) Coord Coord w/ MolSpectra SliDe SliDe w/ MolSpectra 0.016 0. 0.015 0.012 α (a3 0) 0.052 0.048 0.050 0.043 homo lumo (meV) (meV) gap (meV) 17.7 15.5 18.7 17.0 14.7 13.1 16.2 15.8 31.8 26.8 28.8 28. R2 (a2 0) 0.450 0.410 0.606 0.424 ZPVE (meV) 1.71 1.71 1.78 1. Table A2: Performance (MAE) on MD17 dataset. The better result between the two variants of each pretraining method, w/ and w/o MolSpectra, is highlighted in bold. Aspirin Benzene Ethanol Malonal -dehyde Naphtha -lene Salicy -lic Acid Toluene Uracil Coord Coord w/ MolSpectra SliDe SliDe w/ MolSpectra 0.211 0.099 0.174 0.160 0.169 0.097 0.169 0. 0.096 0.052 0.088 0.055 0.139 0.077 0.154 0.088 0.053 0.085 0.048 0. 0.109 0.093 0.101 0.098 0.058 0.075 0.054 0.077 0.074 0.095 0.083 0. Integrating our method with SliDe effectively reduces the error in property prediction on the QM9 dataset and the MD17 dataset. Given that our method enhances both Coord and SliDe, this suggests that our approach is broadly effective across various denoising-based pretraining strategies. Furthermore, incorporating molecular spectra can guide the pre-trained model to acquire knowledge beyond what denoising objectives can offer, which proves beneficial for downstream property prediction."
        },
        {
            "title": "F VISUALIZATION OF ATTENTION PATTERNS AND LEARNED SPECTRA",
            "content": "REPRESENTATIONS IN SPECFORMER We visualize the attention patterns and learned spectra representations in SpecFormer. Based on the visualizations presented in Figure A2, we have made the following observations. 17 Published as conference paper at ICLR 2025 Figure A2: (a-c) Attention maps from three attention heads in SpecFormer. Different heads model distinct dependencies. (d) t-SNE visualization of the spectra representations produced by SpecFormer. In Figure A2(a-c), we visualize attention maps from three attention heads in SpecFormers second layer. The attention weights within the three blocks along the main diagonal indicate intra-spectrum dependencies, while those outside reveal inter-spectrum dependencies, as explained in Section 3.1. It can be observed that different attention heads model distinct dependencies: Head 11 focuses on intra-spectrum dependencies, Head 13 focuses on inter-spectrum dependencies, and Head 12 models both types simultaneously. In inter-spectrum dependencies, the interaction between IR spectra and Raman spectra is relatively pronounced, which may be related to their mutual association with vibrational modes. Additionally, because the intensity peaks and dependencies in molecular spectra are sparse, the attention maps in SpecFormer are generally sparse as well. In Figure A2(d), we use t-SNE to visualize the spectra representations produced by the final layer of SpecFormer. It can be observed that the distribution of representations in the latent space is relatively uniform and forms several potential clusters. This well-shaped distribution of representations reveals effective spectra representation learning and supports the structure-spectrum alignment."
        }
    ],
    "affiliations": [
        "DAMO Academy, Alibaba Group",
        "New Laboratory of Pattern Recognition (NLPR), State Key Laboratory of Multimodal Artificial Intelligence Systems (MAIS), Institute of Automation, Chinese Academy of Sciences (CASIA)",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences"
    ]
}