{
    "paper_title": "Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity",
    "authors": [
        "Weixin Liang",
        "Junhong Shen",
        "Genghan Zhang",
        "Ning Dong",
        "Luke Zettlemoyer",
        "Lili Yu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "State Space Models (SSMs) have emerged as efficient alternatives to Transformers for sequential modeling, but their inability to leverage modality-specific features limits their performance in multi-modal pretraining. Here, we propose Mixture-of-Mamba, a novel SSM architecture that introduces modality-aware sparsity through modality-specific parameterization of the Mamba block. Building on Mixture-of-Transformers (W. Liang et al. arXiv:2411.04996; 2024), we extend the benefits of modality-aware sparsity to SSMs while preserving their computational efficiency. We evaluate Mixture-of-Mamba across three multi-modal pretraining settings: Transfusion (interleaved text and continuous image tokens with diffusion loss), Chameleon (interleaved text and discrete image tokens), and an extended three-modality framework incorporating speech. Mixture-of-Mamba consistently reaches the same loss values at earlier training steps with significantly reduced computational costs. In the Transfusion setting, Mixture-of-Mamba achieves equivalent image loss using only 34.76% of the training FLOPs at the 1.4B scale. In the Chameleon setting, Mixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs at the 1.4B scale, and similar text loss with just 65.40% of the FLOPs. In the three-modality setting, MoM matches speech loss at 24.80% of the FLOPs at the 1.4B scale. Our ablation study highlights the synergistic effects of decoupling projection components, where joint decoupling yields greater gains than individual modifications. These results establish modality-aware sparsity as a versatile and effective design principle, extending its impact from Transformers to SSMs and setting new benchmarks in multi-modal pretraining. Our code can be accessed at https://github.com/Weixin-Liang/Mixture-of-Mamba"
        },
        {
            "title": "Start",
            "content": "Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity Weixin Liang * 1 Junhong Shen * 2 Genghan Zhang 1 Ning Dong 3 Luke Zettlemoyer 3 Lili Yu 3 5 2 0 2 7 2 ] . [ 1 5 9 2 6 1 . 1 0 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "State Space Models (SSMs) have emerged as efficient alternatives to Transformers for sequential modeling, but their inability to leverage modalityspecific features limits their performance in multimodal pretraining. Here, we propose Mixtureof-Mamba, novel SSM architecture that introduces modality-aware sparsity through modalityspecific parameterization of the Mamba block. Building on Mixture-of-Transformers (W. Liang et al. arXiv:2411.04996; 2024), we extend the benefits of modality-aware sparsity to SSMs while preserving their computational efficiency. We evaluate Mixture-of-Mamba across three multimodal pretraining settings: Transfusion (interleaved text and continuous image tokens with diffusion loss), Chameleon (interleaved text and discrete image tokens), and an extended three-modality framework incorporating speech. Mixture-of-Mamba consistently reaches the same loss values at earlier training steps with significantly reduced computational costs. In the Transfusion setting, Mixture-of-Mamba achieves equivalent image loss using only 34.76% of the training FLOPs at the 1.4B scale. In the Chameleon setting, Mixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs at the 1.4B scale, and similar text loss with just 65.40% of the FLOPs. In the three-modality setting, MoM matches speech loss at 24.80% of the FLOPs at the 1.4B scale. Our ablation study highlights the synergistic effects of decoupling projection components, where joint decoupling yields greater gains than individual modifications. These results establish modality-aware sparsity as versatile and effective design principle, extending its impact from Transformers to SSMs and setting new *Equal contribution 1Department of Computer Science, Stanford University 2Machine Learning Department, Carnegie Mellon University 3FAIR at Meta. Correspondence to: Weixin Liang <wxliang@cs.stanford.edu>. Preprint. 1 Figure 1. Multi-modal pretraining on interleaved text and image data. Training loss on the image modality is shown for models with 1.4B parameters: Mamba Dense (cyan), Flex-Attention Transformer (dark gray), and Mixture-of-Mamba (orange). The Mixture-of-Mamba achieves significantly lower training loss and requires 2.5x fewer training steps (indicated by the green arrow) to reach the same loss level as the other baselines. benchmarks in multi-modal pretraining. Our code can be accessed at https://github.com/ Weixin-Liang/Mixture-of-Mamba. 1. Introduction State Space Models (SSMs) (Gu et al., 2021; Gu & Dao, 2023) have emerged as efficient alternatives to Transformers for sequential modeling, offering linear scaling in sequence length and strong performance in single-modality tasks. Mamba, recent SSM variant, has demonstrated exceptional efficiency and scalability across diverse tasks by leveraging advanced gating mechanisms and selective state-space scanning (Gu & Dao, 2023). Despite these advantages, SSMs, including Mamba, remain inherently dense, applying the same set of parameters across all input tokens, regardless of modality. This uniform parameterization limits their ability to capture modality-specific features, leading to suboptimal performance in multi-modal pretraining. Recent efforts have extended SSMs to multi-modal tasks. Works like VLMamba (Qiao et al., 2024) and Cobra (Zhao et al., 2024) augment Mamba for vision-language modeling by adding LLaVA-style projection modules that map image Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity features into the token space of Mamba. In the vision domain, Vision Mamba (Zhu et al., 2024) and VMamba (Liu et al., 2024c) incorporate bidirectional scanning schemes and selective 2D scanning paths for image patch modeling. Similarly, Mamba has been explored for diffusion-based image and video generation, as seen in DiffuSSM (Yan et al., 2024) and Zigma (Hu et al., 2024), which employ unique state-space scanning patterns. While these approaches demonstrate the adaptability of Mamba, they are orthogonal to our focus, which introduces modality-aware sparsity directly into the Mamba block itself. promising approach to address such limitations is model sparsity, exemplified by Mixture-of-Experts (MoE) (Jacobs et al., 1991; Eigen et al., 2013; Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022; Jiang et al., 2024; Sukhbaatar et al., 2024). MoE reduces computational load by activating only subset of model components for each input token, allowing experts to specialize in specific aspects of the data. Despite its potential, MoE-based architectures face challenges such as imbalanced expert utilization, bi-level optimization instability, and inefficient load balancing (Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022; Shen & Yang, 2021; Xu et al., 2024). These issues motivate the need for alternative sparse architectures that are computationally efficient and easier to optimize. In multi-modal contexts, prior work (Bao et al., 2022b; Wang et al., 2022; Shen et al., 2023b; Lin et al., 2024) has introduced modality-aware sparsity in Transformer-based MoE architectures. These approaches activate specific experts or parameters based on modality, enabling models to specialize in handling diverse data types. Other methods fine-tune modality-specific modules atop dense LLM backbones (Wang et al., 2023; He et al., 2024; Shen et al., 2023a; 2024b). Such methods show that simple rule-based modality routing often outperforms learned routing, likely due to improved training stability and reduced optimization challenges. The closest work to our approach is MoE-Mamba (Pioro et al., 2024) and the related Blackmamba architecture (Anthony et al., 2024), which interleave Mamba blocks with MoE-augmented MLP layers. While effective, these hybrid designs apply sparsity only to the MLP layers, leaving the dense Mamba blocks unmodified. In contrast, we present Mixture-of-Mamba, novel architecture that directly introduces modality-aware sparsity into the Mamba block itself. Inspired by Mixture-of-Transformers (Liang et al., 2024), our approach dynamically selects modalityspecific weights in every input processing component of Mamba, enabling stable and efficient multi-modal pretraining. Furthermore, prior work (Liang et al., 2024) shows that MoE techniques can complement sparse architectures like Mixture-of-Transformers, suggesting that Mixture-ofMamba and MoE-based MLP sparsification can be combined to achieve further gains. To rigorously evaluate Mixture-of-Mamba, we conduct experiments across three multi-modal pretraining settings: Transfusion: Interleaved text and continuous image tokens with distinct autoregressive and diffusion-based objectives. Mixture-of-Mamba achieves equivalent image loss using only 34.76% of the training FLOPs at the 1.4B scale. Chameleon: Interleaved text and discrete image tokens. Mixture-of-Mamba reaches similar image loss with just 42.50% of the FLOPs and similar text loss with only 65.40% of the FLOPs at the 1.4B scale. Three-Modality: Extension of the Chameleon setting to include speech. Mixture-of-Mamba matches speech loss using only 24.80% of the FLOPs at the 1.4B scale, while maintaining strong performance across image and text modalities. Additionally, we perform an ablation study to analyze the contribution of modality-specific parameterization. Our findings reveal synergistic effect: jointly decoupling all components yields greater gains than individual modifications, underscoring the importance of modality-aware sparsity as holistic design principle. In summary, Mixture-of-Mamba establishes versatile and efficient architecture for SSMs by extending modality-aware sparsity into the Mamba block. This approach delivers robust performance gains and substantial computational savings across diverse multi-modal settings, setting new benchmarks in scalable multi-modal pretraining. 2. Method 2.1. The Mixture-of-Mamba Block Our hypothesis is that explicitly parametrizing the selection in SSMs with the modality can improve the data efficiency of multi-modality training (Liang et al., 2024). Following the setting of other SSMs (Gu et al., 2021), Mixture-of-Mamba is composed of homogeneous Mixtureof-Mamba blocks (line 1-13 of Algorithm 1). In Mixture-of-Mamba, modality-specific parameterization is applied to all projections that explicitly process input features belonging to single modality, including input projection (➊ Win proj), intermediate projections (➋ Wx proj and ➌ Wdt proj), and output projection (➍ Wout proj). Conv1D and state transitions remain shared because they operate across multiple features or on aggregated RNN-like states, where the notion of modality is not well-defined. After parametrized by modality , the linear transformation 2 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity [b,ℓ,d,n] [b,ℓ,d,n] [b,d,n] Block starts [b,ℓ,d] [b,ℓ,(r,n,n)] Algorithm 1 Mixture-of-Mamba block input Fin, A, Win proj, Wx proj, Wdt proj, Wout proj, b, output Fout 1: x, M(Fin, Win proj; ) 2: SiLU(Conv1D(x)) 3: δ, B, M(u, Wx proj; ) 4: log(1 + exp((M(δ, Wdt proj, b; )))) 5: 6: (u B) 7: = 0 8: for = 0...N 1 do 9: 10: 11: end for 12: (y + u) SiLU(z) 13: Fout M(o, Wout proj; ) 14: 15: function M(X, W, = None; ) 16: 17: 18: 19: 20: 21: 22: end function Im {i : mi = m} Xm {xi : Im} Ym XmWm + bm end for return mM Ym = Ai + Bi yi = Ci for each modality do [b,d,n] [b,d] Block ends XW + becomes M(X, W, b; ). applies the weight of modality (Wm) to tokens of modality (Xm) in parallel based on the modality mask. The output shape of is the same as the corresponding linear transformation. The shape of Win proj is [f,(d,d)] where is the feature dimension of input Fin and is the expanded feature dimension. These two projections are fused together for efficiency and Wx proj uses the same technique. Line 1, 12 and 13 can be viewed as SwiGLU (Shazeer, 2020) around the conv+SSM (Line 2-12). is passed to conv+SSM and will be transformed to the gate in SwiGLU. The Conv1D in Line 2 can help collect local information across time as observed in (Sun et al., 2024). Similarly, Conv1D can also gather local information across modalities and we keep the weight-sharing property of convolution without separating the convolution kernel into different modalities. Line 3-12 is multi-modality selective SSM. It is composed of parameter preparation (line 3-6), RNN update (line 7-11), and residual connection (line 12). is the discretization time step. It is derived from through low-rank approximation δ followed by softplus as shown in Line 3 and 4. is of shape [d,n] and is of shape [b,ℓ,d] where is batch size, ℓ is sequence length, and is the state dimension. Line 5 is broad3 Figure 2. Comparison of (a) the original Mamba block and (b) the proposed Mixture-of-Mamba block. In Mixture-of-Mamba, modality-specific parameterization is applied to all projections that explicitly process input features belonging to single modality, including input projection (➊ Win proj), intermediate projections (➋ Wx proj and ➌ Wdt proj), and output projection (➍ Wout proj). Conv1D and state transitions remain shared because they operate across multiple features or on aggregated RNN-like states, where the notion of modality is not well-defined. By selectively decoupling these projections, Mixture-of-Mamba enables modalityaware sparsity without compromising computational efficiency. cast element-wise multiplication where is unsqueezed to [b,ℓ,d,1] and repeated to [b,ℓ,d,n]. Line 6 first applies batched outer product between [b,ℓ,d] and [b,ℓ,n] whose result is element-wise multiplied with . Line 5 and 6 apply the selection to A,B and get A, B, respectively. can be viewed as gated input and can be viewed as selection gate on the state h. Line 7-10 is typical RNN operator with state and output yi. The yis are concatenated together as output y. The gate application on input is fused with gate parameter preparation at line 6 for efficiency. Line 12 first adds the input to the output as residual, which is the final output of SSM. Then, Line 12 applies the gate of SwiGLU to the output of SSM. Finally, line 13 projects back to the feature dimension. 2.2. Multi-objective Training with Diffusion Following Transfusion (Zhou et al., 2024), Mixture-ofMamba is trained on interleaved multi-modal sequences Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity of discrete text tokens and continuous image tokens using combined objective that incorporates both language modeling and diffusion-based image generation. Each image is encoded as sequence of latent patches using Variational Autoencoder (VAE), where each patch is represented as continuous vector. The patches are sequenced left-to-right, top-to-bottom, and inserted into the discrete text sequence. The diffusion process follows the Denoising Diffusion Probabilistic Models (DDPM) (Ho et al., 2020), where Gaussian noise is progressively added to the latent image patches during the forward process. Given clean latent patch x0, noised version xt at timestep is created as: xt = αtx0 + 1 αtϵ, ϵ (0, I), (1) where αt is determined by cosine noise schedule (Nichol π 2 ) & Dhariwal, 2021), approximated as with adjustments. During training, noise is added to the latent patches at randomly selected timestep t, and the model is optimized to predict the noise ϵ. αt cos( The overall training objective combines the autoregressive language modeling loss LLM, applied to the discrete text tokens, with the diffusion loss LDDPM, applied to the latent image patches: = LLM + λ LDDPM, (2) where λ balances the contributions of the two losses. Importantly, the conditioning for image generation is naturally embedded within the interleaved sequence. When denoising image patches, the preceding tokensincluding both text describing the image and prior imagesserve as context for conditional generation. This unified approach enables Mixture-of-Mamba to leverage the modality-aware sparsity to efficiently model both local intra-image dependencies and long-range inter-modal relationships across the sequence. 2.3. Training with Uniform Representations As an alternative to the multi-objective training paradigm, we explore unified representation strategy in which both text and image modalities are represented as discrete tokens. Following the Chameleon framework (Chameleon Team, 2024), we treat the image data as sequences of discrete tokens obtained through pre-trained VQ-VAE model (Gafni et al., 2022). Specifically, each image is encoded into fixed number of tokens (e.g., 1,024) by quantizing its latent features into learned codebook. These tokens are then arranged sequentially, similar to the processing of text tokens, resulting in uniform discrete representation across both modalities. During training, both text and image tokens are processed using the same autoregressive objective, where the model 4 learns to predict the next token in the sequence given all previous tokens. Formally, the training objective is: Luniform = Ex1:T [ log (xt x1:t1)] , (3) where x1:T represents the interleaved sequence of text and image tokens. This objective allows the model to treat text and image data equivalently, unifying the training process across modalities while relying solely on an autoregressive loss. The use of discrete tokens for images simplifies the training procedure by removing the need for separate loss formulations, as in the diffusion-based approach. It also aligns with the inherent sequence-to-sequence nature of Mixture-of-Mamba, where the same modality-aware sparsity design can be applied seamlessly across the discrete text and image tokens. Motivation and Robustness Testing. We include this alternative strategy to evaluate the robustness of our Mixtureof-Mamba architecture under different choices of training objectives and data representations. By experimenting with uniform discrete representations, we demonstrate that Mixture-of-Mamba consistently outperforms Mamba Dense models across various settings, including both continuous (multi-objective) and discrete (uniform) representations. This highlights the versatility of Mixture-of-Mamba and its ability to deliver performance gains regardless of the underlying choice of modality representations or training objectives. 3. Results 3.1. Results in Multi-objective Training (Transfusion) We evaluate Mixture-of-Mamba (MoM) against Mamba Dense and Flex-Attention Transformer in the Transfusion setting, where pretraining is performed on interleaved text and image data across three model scales: 163M, 760M, and 1.4B. See our training configuration in Appendix Table 5. For clarity, performance gain is quantified as: Performance Gain (%) = LossDense LossMixture LossDense 100, where LossDense and LossMixture are the final losses of Mamba Dense and Mixture-of-Mamba, respectively. Relative training FLOPs reflect the computational cost required for MoM to match the training dynamics (similar loss) of Mamba Dense. The detailed results are summarized in Table 4 and Figure 3, with further visualizations provided in Appendix Figures 4, 5, and 6.1 1Flex-Attention Transformer (i.e., Transfusion (Zhou et al., 2024)) combines both attention patterns by applying causal attention to every element in the sequence and bidirectional attention within the elements of each individual image. This makes FlexMixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity (a) 1.4B Image Training Loss (b) 1.4B Image Loss Matching (c) 1.4B Text Training Loss (d) 1.4B Text Loss Matching (e) 760M Image Training Loss (f) 760M Image Loss Matching (g) 760M Text Training Loss (h) 760M Text Loss Matching (i) 163M Image Training Loss (j) 163M Image Loss Matching (k) 163M Text Training Loss (l) 163M Text Loss Matching Figure 3. Multi-modal pretraining in the Transfusion setting on interleaved text and image data across model scales. Training loss and loss matching are reported for image and text modalities at three model sizes: 1.4B, 760M, and 163M. (a, e, i) Image training loss shows significant improvements for Mixture-of-Mamba (orange), which consistently achieves lower loss compared to Mamba Dense (cyan) and Flex-Attention Transformer (dark gray) across all scales. (b, f, j) Image loss matching compares the training dynamics and shows that Mixture-of-Mamba and Flex-Attention Transformer reach the same loss values at earlier training steps compared to Mamba Dense. (c, g, k) Text training loss shows competitive results, with Mixture-of-Mamba performing better than Mamba Dense and on par with the Flex-Attention Transformer. (d, h, l) Text loss matching illustrates that Mixture-of-Mamba and Flex-Attention Transformer exhibit more efficient training dynamics than Mamba Dense, requiring fewer steps to achieve comparable loss values, though the primary improvements are observed in the image modality. Overall, in the Transfusion setting, Mixture-of-Mamba demonstrates substantial gains in image loss and training efficiency while maintaining strong performance on text. Image Modality. Mixture-of-Mamba (MoM) consistently demonstrates superior performance in image modality training loss across all model scales. At the 1.4B scale, MoM achieves training loss of 0.2138, outperforming Mamba Dense by 2.20% while requiring only 34.76% of the training FLOPs. Similar trends are observed at smaller scales: at the 760M scale, MoM achieves training loss of 0.2172, 2.37% improvement over Mamba Dense, while reducing training FLOPs to 37.76%. The validation loss curves on the CC12M dataset ((Table 4, Appendix Figure 5) further illustrate these trends. Mixtureof-Mamba consistently achieves lower image validation loss compared to Mamba Dense and Flex-Attention Transformer, with the improvements becoming more pronounced as model size increases. Additionally, loss matching curves demonstrate that MoM reaches equivalent loss values at Attention Transformer an overestimated baseline for transformers because both Mamba and Mixture-of-Mamba are strictly causal across all elements, while Flex-Attention Transformer benefits from bidirectional attention within images. earlier training steps, highlighting its improved training efficiency. Text Modality. In the text modality, Mixture-of-Mamba consistently outperforms Mamba Dense across both training and validation metrics. At the 1.4B scale, MoM achieves lower validation losses on both the C4 (2.2695) and Wikipedia (1.7164) datasets compared to Mamba Dense, despite their similar training losses. This indicates better generalization to unseen text data. Importantly, MoM also performs comparably to or better than Flex-Attention Transformer, particularly on validation losses, as shown in Appendix Figure 4. Similar trends are observed at smaller scales (760M and 163M), where MoM reduces validation losses while maintaining high training efficiency. Loss matching results in Appendix Figure 4 (b, f, j) confirm that Mixture-of-Mamba aligns closely with or surpasses Mamba Dense, reaching comparable loss values earlier during training. These improvements highlight MoMs strong performance in text tasks while maintaining its computa5 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity"
        },
        {
            "title": "Metric Name",
            "content": "Mamba Loss () Mixture-ofMamba Loss () Performance Gain (%) () Relative Training FLOPs to Match Mamba (%) ()"
        },
        {
            "title": "Model\nScale",
            "content": "443M"
        },
        {
            "title": "Text Metrics",
            "content": "Training Loss Obelisc Val. Loss SSTK Val. Loss Training Loss Obelisc Val. Loss SSTK Val. Loss 5.3558 4.5258 5.9179 2.4637 3.0544 2."
        },
        {
            "title": "Avg Training Loss",
            "content": "3."
        },
        {
            "title": "Image Metrics",
            "content": "880M"
        },
        {
            "title": "Text Metrics",
            "content": "Training Loss Obelisc Val. Loss SSTK Val. Loss Training Loss Obelisc Val. Loss SSTK Val. Loss 5.2260 4.4127 5.7987 2.3073 2.8886 2."
        },
        {
            "title": "Avg Training Loss",
            "content": "3."
        },
        {
            "title": "Image Metrics",
            "content": "1.5B"
        },
        {
            "title": "Text Metrics",
            "content": "Training Loss Obelisc Val. Loss SSTK Val. Loss Training Loss Obelisc Val. Loss SSTK Val. Loss 5.1892 4.3692 5.7546 2.2284 2.8020 2."
        },
        {
            "title": "Avg Training Loss",
            "content": "3.4602 5.1703 4.3546 5.7471 2.3864 2.9820 2.6250 3.5364 5.1201 4.3105 5.6986 2.2438 2.8313 2. 3.4320 5.0591 4.2510 5.6335 2.1614 2.7393 2.3455 3.3670 3.46% 3.78% 2.89% 3.14% 2.37% 4.78% 3.33% 2.03% 2.32% 1.73% 2.75% 1.99% 3.67% 2.31% 2.51% 2.71% 2.10% 3.01% 2.24% 4.71% 2.69% 33.40% 35.10% 35.30% 62.00% 66.70% 54.70% 47.90% 48.40% 49.30% 50.50% 65.60% 72.80% 67.90% 58.30% 42.50% 44.50% 44.60% 65.40% 71.60% 62.10% 54.70% Table 1. Training and validation metrics across model scales in the Chameleon setting. In this setting, both image and text modalities are represented as discrete tokens. Mixture-of-Mamba achieves substantial performance improvements over Mamba Dense, with the image modality showing the largest gains. The text modality also exhibits significant improvements, in contrast to the Transfusion setting where text gains were more modest. The current table shows results for three model scales: 443M, 880M, and 1.5B, due to space constraints. See Appendix Table 7 for the full results across all five model scales: 37M, 94M, 443M, 880M, and 1.5B. These results further highlight the effectiveness and efficiency of Mixture-of-Mamba, which consistently achieves strong performance with reduced relative training FLOPs. tional efficiency. 3.2. Results in Training with Uniform Representations (Chameleon) We evaluate Mixture-of-Mamba (MoM) in the Chameleon setting, where both image and text modalities are represented as discrete tokens. See our training configuration in Appendix Table 6. Results are summarized in Table 1, with full results across all five scales (37M, 94M, 443M, 880M, and 1.5B) provided in Appendix Table 7. Training dynamics and validation loss trends are visualized in Appendix Figures 7, 8, and 9. Image Modality. Mixture-of-Mamba (MoM) consistently demonstrates better performance in image modality training loss across all model scales, achieving substantial efficiency gains over Mamba Dense. At the 443M scale, MoM achieves training loss of 5.1703, 3.46% improvement over Mamba Dense, while requiring only 33.40% of the training FLOPs. Similar trends are observed at other scales: at the largest 1.5B scale, MoM achieves training loss of 5.0591, 2.51% improvement, with only 42.50% of the Overall Performance and Efficiency. Across both image and text modalities, Mixture-of-Mamba consistently outperforms Mamba Dense in terms of loss reduction while requiring significantly fewer training FLOPs to achieve similar learning dynamics. At the 1.4B scale, MoM improves the overall training loss by 0.84% while requiring only 83.10% of the training FLOPs. At smaller scales, such as 760M and 163M, MoM reduces the overall training loss by up to 0.94%, while requiring just 82.94% and 86.11% of the FLOPs, respectively (Table 4, Appendix Figure 6). These results, summarized in Table 4 and Figure 3, and further supported by Appendix Figures 4, 5, and 6, underscoring MoMs effectiveness, scalability, and efficiency in the Transfusion setting. Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity Metric Category Metric Name Mamba Loss () Mixture-ofMamba Loss () Performance Gain (%) () Relative Training FLOPs to Match Mamba (%) () Model Scale 37M Speech Metrics Training Loss LL60K Val. Loss PPL30K Val. Loss Overall Metrics Avg Training Loss 94M Speech Metrics Training Loss LL60K Val. Loss PPL30K Val. Loss Overall Metrics Avg Training Loss 443M Speech Metrics Training Loss LL60K Val. Loss PPL30K Val. Loss Overall Metrics Avg Training Loss 880M Speech Metrics Training Loss LL60K Val. Loss PPL30K Val. Loss Overall Metrics Avg Training Loss 1.5B Speech Metrics Training Loss LL60K Val. Loss PPL30K Val. Loss Overall Metrics Avg Training Loss 1.8159 1.6756 1.8147 4.2299 1.6911 1.5235 1. 3.7756 1.5414 1.3466 1.5634 3.3317 1.4902 1.2939 1.5400 3.2289 1.4790 1.2592 1. 3.1507 1.6909 1.5217 1.6845 4.0759 1.5662 1.3747 1.6152 3.6371 1.4313 1.2113 1. 3.2096 1.4054 1.1757 1.4619 3.1571 1.3940 1.1552 1.4387 3.0545 6.88% 9.18% 7.17% 3.64% 7.38% 9.76% 4.71% 3.67% 7.14% 10.05% 5.40% 3.66% 5.69% 9.13% 5.07% 2.22% 5.75% 8.26% 5.35% 3.05% 10.30% 13.60% 13.60% 45.00% 11.90% 14.80% 12.60% 43.10% 19.20% 24.70% 22.00% 44.00% 22.40% 30.10% 24.30% 54.30% 24.80% 32.10% 27.60% 56.20% Table 2. Training and validation metrics across model scales with three modalities: image, text, and speech. This setting extends the Chameleon framework by incorporating speech alongside image and text, with all modalities represented as discrete tokens. Mixture-ofMamba achieves consistent improvements over Mamba Dense across all scales (37M, 94M, 443M, 880M, and 1.5B), particularly in the speech modality, where performance gains reach up to 9.18%. These gains are achieved with substantial reductions in training FLOPs, ranging from 10.30% to 56.20% relative to Mamba Dense. The results demonstrate that Mixture-of-Mamba generalizes effectively to multi-modal setting with three modalities while delivering significant computational efficiency. training FLOPs. At the smallest 37M scale, MoM reduces training loss to 5.9561, outperforming Mamba Dense by 2.85% while requiring just 25.90% of the FLOPs (Appendix Table 7). These results highlight MoMs ability to achieve improved performance and convergence efficiency consistently in the image modality across all model scales. Text Modality. Mixture-of-Mamba (MoM) demonstrates consistent improvements in text modality training loss across all model scales. At the largest 1.5B scale, MoM reduces training loss to 2.1614, 3.01% improvement over Mamba Dense, while requiring only 65.40% of the training FLOPs. Validation loss on Obelisc and proprietary version of the Shutterstock datasets (SSTK) exhibits similar trends, with MoM achieving notable improvements in loss values while maintaining significant efficiency gains (Appendix Figures 8 and 9). These results further highlight MoMs ability to deliver strong text performance with improved convergence efficiency. These results highlight Mixture-of-Mambas robust and efficient improvements in the Chameleon setting across both image and text modalities, with substantial computational savings. 7 3.3. Results in Training with Three Modalities (Chameleon+Speech) To evaluate the robustness and scalability of Mixture-ofMamba (MoM), we extend the Chameleon framework to include third modality: speech, alongside image and text, with all modalities represented as discrete tokens. Speech data is tokenized using an in-house tokenizer, variant of DinoSR (Liu et al., 2024a), which extracts semantic tokens with vocabulary size of 500, where each token corresponds to 40ms of audio content. Results are summarized in Table 2, with additional training dynamics and evaluation loss trends visualized in Appendix Figures 11, 12, 13, and 14. Speech Modality. Mixture-of-Mamba (MoM) achieves substantial improvements in speech modality training loss across all model scales. At the 443M scale, MoM improves speech training loss by 7.14% compared to Mamba Dense. To match the training loss achieved by Mamba Dense, MoM requires only 19.20% of the training FLOPs, demonstrating significant efficiency gains. Similar trends hold at the largest 1.5B scale, where MoM achieves 5.75% improvement in speech training loss and matches Mamba Denses loss with just 24.80% of the training FLOPs. Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity Ablation Study 443M Mamba (without ➊➋➌➍) ➊ (decouple Win proj) ➋ (decouple Wx proj) ➌ (decouple Wdt proj) ➍ (decouple Wout proj) ➊+➋ (decouple Win proj, Wx proj) ➊+➌ (decouple Win proj, Wdt proj) ➊+➍ (decouple Win proj, Wout proj) ➋+➌ (decouple Wx proj, Wdt proj) ➋+➍ (decouple Wx proj, Wout proj) ➌+➍ (decouple Wdt proj, Wout proj) ➊+➋+➌ (not decoupling Wout proj) ➊+➋+➍ (not decoupling Wdt proj) ➊+➌+➍ (not decoupling Wx proj) ➋+➌+➍ (not decoupling Win proj) ➊+➋+➌+➍ (Mixture-of-Mamba) Avg Training Loss () Performance Gain (%) () 3.3317 3.2916 3.3580 3.3525 3.3109 3.2780 3.2687 3.2599 3.3214 3.2829 3.2509 3.2593 3.2312 3.2342 3.2773 3.2096 0% (baseline) 1.22% -0.79% -0.62% 0.63% 1.64% 1.93% 2.20% 0.31% 1.49% 2.48% 2.22% 3.11% 3.01% 1.66% 3.80% Table 3. Ablation study on the Chameleon + Speech setting. This study evaluates the impact of decoupling individual components (1, 2, 3, 4) and their combinations on model performance. The results demonstrate that decoupling all components (1+2+3+4, Mixture-of-Mamba) achieves the best performance with 3.80% gain over the Mamba baseline. Notably, the performance gain achieved by decoupling all components together exceeds the sum of gains from decoupling each component individually, highlighting the synergistic effect of combined decoupling. Green shading indicates positive performance gains, with the darkest green highlighting the best configuration. Overall training loss is consistently reduced across scales. At the 1.5B scale, MoM lowers the overall training loss by 3.05%. When targeting the same loss as Mamba Dense, MoM achieves this with 56.20% reduction in relative training FLOPs, highlighting its improved computational efficiency. Performance in the image and text modalities similarly shows consistent improvements in training and validation losses relative to Mamba Dense. Full results and trends are presented in Appendix Figures 13 and 14, where MoMs robust performance across all three modalities is further validated. 3.4. Ablation Study on Decoupling Components To better understand the design choices underpinning Mixture-of-Mamba, we conduct an ablation study on the Chameleon + Speech setting at the 443M scale. We evaluate the impact of decoupling four key componentsWin-proj (➊), Wx-proj (➋), Wdt-proj (➌), and Wout-proj (➍)individually and in various combinations. This analysis enables us to test both individual and combined contributions to the models overall performance. The results show that decoupling components individually yields varying degrees of improvement, with performance gains ranging from 0.63% (Wout-proj) to 1.22% (Win-proj). Interestingly, some components (Wx-proj and Wdt-proj) ex8 hibit minimal or even slightly negative impact when decoupled alone. However, decoupling multiple components in combination leads to significantly larger gains. For example, decoupling Win-proj and Wout-proj (➊+➍) achieves 2.20% improvement, while decoupling three components (➊+➋+➍) further increases the gain to 3.11%. Most importantly, decoupling all four components simultaneously (➊+➋+➌+➍, Mixture-of-Mamba) achieves the largest improvement, with performance gain of 3.80% over the Mamba baseline. This result highlights key observation: the gain from decoupling all components together exceeds the sum of individual gains, demonstrating synergistic effect. The combination of all decoupled projections enables better parameter allocation across modalities, leading to more efficient and effective learning. In summary, the ablation study confirms that the design of Mixture-ofMamba is both effective and interdependent. Decoupling all key components simultaneously is important to achieving the observed substantial performance gains. 4. Related Work 4.1. State-Space Models and Multi-Modal Extensions State-space models (SSMs) (Gu et al., 2021; Gu & Dao, 2023) have recently gained traction as computationally efficient alternatives to Transformers for sequential modeling. Mamba (Gu & Dao, 2023), in particular, demonstrates strong performance on single-modality tasks by leveraging linear time complexity and advanced gating mechanisms. Extending Mamba to multi-modal tasks remains an active research area. In vision-language modeling, VLMamba (Qiao et al., 2024) and Cobra (Zhao et al., 2024) augment Mamba by incorporating LLaVA-style projection modules, enabling image features to be mapped into the token space of the Mamba model for sequence modeling. In the vision domain, Vision Mamba (Zhu et al., 2024) introduces bidirectional scanning by chaining forward and backward SSM blocks, while VMamba (Liu et al., 2024c) further enhances image patch processing with 2D Selective Scan (SS2D) module that traverses patches across multiple scanning paths. For diffusion-based models, works such as DiffuSSM (Yan et al., 2024) and Zigma (Hu et al., 2024) replace attention mechanisms with SSMs for image and video generation. Zigma introduces zigzag scanning scheme to improve efficiency for sequential diffusion tasks, while other approaches (Mo & Tian, 2024; Fei et al., 2024) explore bidirectional SSM architectures. While these works highlight the flexibility of Mamba in generative tasks, they focus primarily on architectural modifications for specific domains rather than general multi-modal pretraining. Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity Furthermore, our design is complementary to existing MoE techniques. Prior work (Liang et al., 2024) has demonstrated that MoE-based sparsification can be combined with sparse architectures like Mixture-of-Transformers to achieve additional gains. Similarly, Mixture-of-Mamba can serve as versatile and computationally efficient solution, offering new pathways for scalable multi-modal pretraining. 5. Conclusion In this work, we introduced Mixture-of-Mamba, novel extension of state-space models (SSMs) that incorporates modality-aware sparsity through modality-specific parameterization. By enabling modality-specific specialization while preserving the computational efficiency of SSMs, Mixture-of-Mamba consistently outperforms dense baselines across three multi-modal settings: Transfusion (interleaved text and continuous image tokens), Chameleon (interleaved text and discrete image tokens), and an extended Chameleon+Speech framework. Our results demonstrate substantial improvements in loss reduction, with training efficiency gains reaching more than double the computational efficiency compared to dense SSMs. Ablation studies further reveal synergistic effect from jointly decoupling key projection components, highlighting the effectiveness of modality-aware sparsity. These findings establish Mixtureof-Mamba as scalable and efficient architecture for multimodal pretraining, paving the way for future exploration in dynamic sparsity and broader multi-modal applications."
        },
        {
            "title": "Impact Statement",
            "content": "This work introduces efficiency improvements in multimodal machine learning systems through modality-aware sparsity techniques. The primary impact is computational efficiency - Mixture-of-Mamba reduces computational costs by up to 65% while maintaining or improving performance. This has positive environmental implications through reduced energy consumption and democratizes access to multimodal AI systems by lowering computational resource requirements. While these advances could enable beneficial applications in education, accessibility, and humancomputer interaction, we acknowledge they could also facilitate potentially concerning applications. We encourage the research community to consider appropriate guidelines for responsible deployment of such technologies. The most related work to ours is MoE-Mamba (Pioro et al., 2024) and Blackmamba (Anthony et al., 2024), which interleave Mamba blocks with MoE-augmented MLPs to introduce sparsity. However, these hybrid designs apply sparsity only to the MLP layers, leaving the dense Mamba block unmodified. In contrast, our proposed Mixture-of-Mamba integrates modality-aware sparsity directly into the Mamba block by decoupling its projection components, enabling specialized computations for different modalities. This general design complements existing methods and offers new opportunities for computationally efficient multi-modal pretraining. 4.2. Sparse Architectures for Multi-Modal Pretraining Model sparsity, particularly Mixture-of-Experts (MoE), has been extensively explored in Transformers to reduce computational cost (Jacobs et al., 1991; Eigen et al., 2013; Shazeer et al., 2017; Lepikhin et al., 2020; Fedus et al., 2022; Jiang et al., 2024). MoE selectively activates subsets of parameters for each input token, allowing the model to specialize in different aspects of the data. However, challenges such as expert imbalance, bi-level optimization, and load balancing remain prevalent (Shazeer et al., 2017; Lepikhin et al., 2020; Tu et al., 2022). In multi-modal tasks, modality-aware sparsity has emerged as an effective strategy. Works such as VLMo (Shen et al., 2023b), MoMA (Lin et al., 2024), and related approaches (Wang et al., 2022; Shen et al., 2022; Bao et al., 2022a; Long et al., 2023; Shen et al., 2025) assign modalityspecific experts to handle the unique statistical properties of text, images, and other data types. This improves specialization while avoiding the complexities of learned routing mechanisms (Liang et al., 2022). Transformer-based architectures have further extended sparsity into attention mechanisms (Wang et al., 2023; Shen et al., 2024c;d; Liu et al., 2024b; Shen et al., 2024a). CogVLM (Wang et al., 2023) applies sparse techniques on top of pre-trained Vicuna-7B model but remains limited to generating text outputs. Concurrently, Playground v3 (PGv3) (Liu et al., 2024b) integrates DiT-style image transformers with frozen LLaMA-3 backbone to achieve state-of-the-art performance in text-to-image generation. Our work differs fundamentally in two key aspects. First, Mixture-of-Mamba introduces modality-aware sparsity into the Mamba block itself, generalizing sparse architectures beyond Transformers to SSMs. Unlike prior works that sparsify only the MLP or attention components, we decouple projection components of the Mamba block, enabling efficient and specialized computations across modalities. Second, Mixture-of-Mamba is trained from scratch for multimodal generation tasks, unlike approaches like CogVLM and PGv3 that fine-tune pre-trained backbones. 9 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity"
        },
        {
            "title": "References",
            "content": "Anthony, Q., Tokpanov, Y., Glorioso, P., and Millidge, B. Blackmamba: Mixture of experts for state-space models. arXiv preprint arXiv:2402.01771, 2024. Bao, H., Wang, W., Dong, L., Liu, Q., Mohammed, O. K., Aggarwal, K., Som, S., Piao, S., and Wei, F. Vlmo: Unified vision-language pre-training with mixture-ofmodality-experts. Advances in Neural Information Processing Systems, 35:3289732912, 2022a. Bao, H., Wang, W., Dong, L., Liu, Q., Mohammed, O. K., Aggarwal, K., Som, S., and Wei, F. Vlmo: Unified vision-language pre-training with mixture-of-modalityexperts, 2022b. URL https://arxiv.org/abs/ 2111.02358. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models, 2024. URL https://arxiv. org/abs/2405.09818. Eigen, D., Ranzato, M., and Sutskever, I. Learning factored representations in deep mixture of experts. arXiv preprint arXiv:1312.4314, 2013. Fedus, W., Zoph, B., and Shazeer, N. Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity, 2022. URL https://arxiv.org/ abs/2101.03961. Fei, Z., Fan, M., Yu, C., and Huang, J. Scalable diffusion models with state space backbone. arXiv preprint arXiv:2402.05608, 2024. Gafni, O., Polyak, A., Ashual, O., Sheynin, S., Parikh, D., and Taigman, Y. Make-a-scene: Scene-based textto-image generation with human priors. arXiv preprint arXiv:2203.13131, 2022. Gu, A. and Dao, T. Mamba: Linear-time sequence modeling with selective state spaces. arXiv preprint arXiv:2312.00752, 2023. Gu, A., Goel, K., and Re, C. Efficiently modeling long sequences with structured state spaces. arXiv preprint arXiv:2111.00396, 2021. He, W., Fu, S., Liu, M., Wang, X., Xiao, W., Shu, F., Wang, Y., Zhang, L., Yu, Z., Li, H., et al. Mars: Mixture of auto-regressive models for fine-grained text-to-image synthesis. arXiv preprint arXiv:2407.07614, 2024. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. Hu, V. T., Baumann, S. A., Gui, M., Grebenkova, O., Ma, P., Schusterbauer, J., and Ommer, B. Zigma: dit-style zigzag mamba diffusion model. In ECCV, 2024. Jacobs, R. A., Jordan, M. I., Nowlan, S. J., and Hinton, G. E. Adaptive mixtures of local experts. Neural computation, 3(1):7987, 1991. Jiang, A. Q., Sablayrolles, A., Roux, A., Mensch, A., Savary, B., Bamford, C., Chaplot, D. S., de las Casas, D., Hanna, E. B., Bressand, F., Lengyel, G., Bour, G., Lample, G., Lavaud, L. R., Saulnier, L., Lachaux, M.-A., Stock, P., Subramanian, S., Yang, S., Antoniak, S., Scao, T. L., Gervet, T., Lavril, T., Wang, T., Lacroix, T., and Sayed, W. E. Mixtral of experts, 2024. URL https://arxiv. org/abs/2401.04088. Langley, P. Crafting papers on machine learning. In Langley, P. (ed.), Proceedings of the 17th International Conference on Machine Learning (ICML 2000), pp. 12071216, Stanford, CA, 2000. Morgan Kaufmann. Lepikhin, D., Lee, H., Xu, Y., Chen, D., Firat, O., Huang, Y., Krikun, M., Shazeer, N., and Chen, Z. Gshard: Scaling giant models with conditional computation and automatic sharding, 2020. URL https://arxiv.org/abs/ 2006.16668. Liang, V. W., Zhang, Y., Kwon, Y., Yeung, S., and Zou, J. Y. Mind the gap: Understanding the modality gap in multi-modal contrastive representation learning. Advances in Neural Information Processing Systems, 35: 1761217625, 2022. Liang, W., Yu, L., Luo, L., Iyer, S., Dong, N., Zhou, C., Ghosh, G., Lewis, M., Yih, W.-t., Zettlemoyer, L., et al. Mixture-of-transformers: sparse and scalable architecture for multi-modal foundation models. arXiv preprint arXiv:2411.04996, 2024. Lin, X. V., Shrivastava, A., Luo, L., Iyer, S., Lewis, M., Gosh, G., Zettlemoyer, L., and Aghajanyan, A. Moma: Efficient early-fusion pre-training with arXiv preprint mixture of modality-aware experts. arXiv:2407.21770, 2024. Liu, A. H., Chang, H.-J., Auli, M., Hsu, W.-N., and Glass, J. R. Dinosr: Self-distillation and online clustering for self-supervised speech representation learning, 2024a. URL https://arxiv.org/abs/2305.10005. Liu, B., Akhgari, E., Visheratin, A., Kamko, A., Xu, L., Shrirao, S., Lambert, C., Souza, J., Doshi, S., and Li, D. Playground v3: Improving text-to-image alignment with deep-fusion large language models, 2024b. URL https://arxiv.org/abs/2409.10695. Liu, Y., Tian, Y., Zhao, Y., Yu, H., Xie, L., Wang, Y., Ye, Q., Jiao, J., and Liu, Y. VMamba: Visual state space model. In The Thirty-eighth Annual Conference on Neural Information Processing Systems, 2024c. URL https: //openreview.net/forum?id=ZgtLQQR1K7. Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity Long, Z., Killick, G., McCreadie, R., and Camarasa, G. A. Multiway-adapater: Adapting large-scale multi-modal models for scalable image-text retrieval. arXiv preprint arXiv:2309.01516, 2023. Shen, J., Tirumala, K., Yasunaga, M., Misra, I., Zettlemoyer, L., Yu, L., and Zhou, C. Cat: Content-adaptive image tokenization, 2025. URL https://arxiv.org/abs/ 2501.03120. Mo, S. and Tian, Y. Scaling diffusion mamba with bidirectional ssms for efficient image and video generation. arXiv preprint arXiv:2405.15881, 2024. Shen, S., Yao, Z., Li, C., Darrell, T., Keutzer, K., and He, Y. Scaling vision-language models with sparse mixture of experts. arXiv preprint arXiv:2303.07226, 2023b. Nichol, A. Q. and Dhariwal, P. Improved denoising diffusion probabilistic models. In International conference on machine learning, pp. 81628171. PMLR, 2021. Shen, Y., Guo, Z., Cai, T., and Qin, Z. Jetmoe: Reaching llama2 performance with 0.1 dollars. arXiv preprint arXiv:2404.07413, 2024d. Pioro, M., Ciebiera, K., Krol, K., Ludziejewski, J., and Jaszczur, S. Moe-mamba: Efficient selective state space models with mixture of experts, 2024. Qiao, Y., Yu, Z., Guo, L., Chen, S., Zhao, Z., Sun, M., Wu, Q., and Liu, J. Vl-mamba: Exploring state space models for multimodal learning. arXiv preprint arXiv:2403.13600, 2024. Shazeer, N. Glu variants improve transformer. arXiv preprint arXiv:2002.05202, 2020. Shazeer, N., Mirhoseini, A., Maziarz, K., Davis, A., Le, Q. V., Hinton, G. E., and Dean, J. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. CoRR, abs/1701.06538, 2017. URL http:// arxiv.org/abs/1701.06538. Shen, J. and Yang, L. F. Theoretically principled deep rl acceleration via nearest neighbor function approximation. Proceedings of the AAAI Conference on Artificial Intelligence, 35(11):95589566, May 2021. doi: 10.1609/ aaai.v35i11.17151. URL https://ojs.aaai.org/ index.php/AAAI/article/view/17151. Shen, J., Khodak, M., and Talwalkar, A. Efficient architecture search for diverse tasks. In Advances in Neural Information Processing Systems (NeurIPS), 2022. Sukhbaatar, S., Golovneva, O., Sharma, V., Xu, H., Lin, X. V., Rozi`ere, B., Kahn, J., Li, D., tau Yih, W., Weston, J., and Li, X. Branch-train-mix: Mixing expert llms into mixture-of-experts llm, 2024. URL https://arxiv. org/abs/2403.07816. Sun, Y., Li, X., Dalal, K., Xu, J., Vikram, A., Zhang, G., Dubois, Y., Chen, X., Wang, X., Koyejo, S., et al. Learning to (learn at test time): Rnns with expressive hidden states. arXiv preprint arXiv:2407.04620, 2024. Tu, R., Roberts, N., Khodak, M., Shen, J., Sala, F., and Talwalkar, A. NAS-bench-360: Benchmarking neural architecture search on diverse tasks. In Advances in Neural Information Processing Systems (NeurIPS) Datasets and Benchmarks Track, 2022. Wang, W., Bao, H., Dong, L., Bjorck, J., Peng, Z., Liu, Q., Aggarwal, K., Mohammed, O. K., Singhal, S., Som, S., and Wei, F. Image as foreign language: Beit pretraining for all vision and vision-language tasks, 2022. URL https://arxiv.org/abs/2208.10442. Wang, W., Lv, Q., Yu, W., Hong, W., Qi, J., Wang, Y., Ji, J., Yang, Z., Zhao, L., Song, X., et al. Cogvlm: Visual expert for pretrained language models. arXiv preprint arXiv:2311.03079, 2023. Shen, J., Li, L., Dery, L. M., Staten, C., Khodak, M., Neubig, G., and Talwalkar, A. Cross-modal fine-tuning: align In Proceedings of the 40th International then refine. Conference on Machine Learning, 2023a. Xu, Z., Gupta, R., Cheng, W., Shen, A., Shen, J., Talwalkar, A., and Khodak, M. Specialized foundation models struggle to beat supervised baselines, 2024. URL https://arxiv.org/abs/2411.02796. Shen, J., Jain, A., Xiao, Z., Amlekar, I., Hadji, M., Podolny, A., and Talwalkar, A. Scribeagent: Towards specialized web agents using production-scale workflow data, 2024a. URL https://arxiv.org/abs/2411.15004. Yan, J. N., Gu, J., and Rush, A. M. Diffusion models without attention. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 8239 8249, 2024. Shen, J., Marwah, T., and Talwalkar, A. Ups: Towards foundation models for pde solving via cross-modal adaptation. arXiv preprint arXiv:2403.07187, 2024b. Shen, J., Tenenholtz, N., Hall, J. B., Alvarez-Melis, D., and Fusi, N. Tag-llm: Repurposing general-purpose llms for specialized domains, 2024c. Zhao, H., Zhang, M., Zhao, W., Ding, P., Huang, S., and Wang, D. Cobra: Extending mamba to multi-modal large language model for efficient inference. arXiv preprint arXiv:2403.14520, 2024. Zhou, C., Yu, L., Babu, A., Tirumala, K., Yasunaga, M., Shamis, L., Kahn, J., Ma, X., Zettlemoyer, L., and 11 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity Levy, O. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. Zhu, L., Liao, B., Zhang, Q., Wang, X., Liu, W., and Wang, X. Vision mamba: Efficient visual representation learning with bidirectional state space model. In Proceedings of the 41st International Conference on Machine Learning, Proceedings of Machine Learning Research. PMLR, 2024. 12 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity Model Scale Metric Category Metric Name Image Metrics 163M Text Metrics Training Loss CC12M Val. Loss Avg Training Loss C4 Val. Loss Wikipedia Val. Loss Overall Train Avg Loss Image Metrics 760M Text Metrics Training Loss CC12M Val. Loss Avg Training Loss C4 Val. Loss Wikipedia Val. Loss Overall Train Avg Loss Image Metrics 1.4B Text Metrics Training Loss CC12M Val. Loss Avg Training Loss C4 Val. Loss Wikipedia Val. Loss Overall Train Avg Loss Mamba Loss () Flex-Attention Transformer Loss () Mixture-ofMamba Loss () Performance Gain over Mamba (%) () Relative Training FLOPs to Match Mamba (%) () 0.2262 0.2295 2.4702 2.6917 2.1884 3.6014 0.2225 0. 2.1394 2.3593 1.8191 3.2519 0.2186 0.2264 2.0761 2.2726 1.7205 3.1693 0.2250 0. 2.4424 2.6862 2.1715 3.5674 0.2213 0.2253 2.1253 2.3559 1.8143 3.2318 0.2221 0. 2.0673 2.2728 1.7218 3.1777 0.2199 0.2255 2.4690 2.6912 2.1870 3.5685 0.2172 0. 2.1353 2.3555 1.8149 3.2214 0.2138 0.2190 2.0737 2.2695 1.7164 3.1429 2.80% 1.74% 0.05% 0.02% 0.06% 0.91% 2.37% 3.13% 0.19% 0.16% 0.23% 0.94% 2.20% 3.29% 0.12% 0.13% 0.24% 0.84% 49.21% 50.61% 98.80% 99.88% 99.81% 86.11% 37.76% 35.27% 96.82% 99.01% 99.11% 82.94% 34.76% 36.15% 98.27% 99.34% 99.30% 83.10% Table 4. Training and validation metrics across model scales in the Transfusion setting. Loss values are reported for image and text modalities at three model sizes: 163M, 760M, and 1.4B. Mixture-of-Mamba consistently achieves competitive or superior performance in image metrics and maintains strong text performance compared to Mamba Dense and Flex-Attention Transformer. The table also reports relative training FLOPs required for Mixture-of-Mamba and Flex-Attention Transformer to match Mambas training dynamics, highlighting improved training efficiency. Best loss values in each row are highlighted. Model Size Hidden Dim. Layers Heads Seq. Length Batch Size/GPU GPUs Tokens/Batch"
        },
        {
            "title": "Steps",
            "content": "163M 760M 1.4B 768 1,536 2,048 16 24 24 12 24 16 4,096 4,096 4,096 4 4 56 56 128 1,048,576 1,048,576 1,048,576 250,000 250,000 250,000 Table 5. Architectural specifications and training configurations of models across different parameter scales (Transfusion setting). 13 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity Model Size Hidden Dim. Layers Heads Seq. Length Batch Size/GPU GPUs Tokens/Batch"
        },
        {
            "title": "Steps",
            "content": "37M 94M 443M 880M 1.5B 256 512 1,024 1,536 2,048 4 8 24 24 24 8 8 16 24 16 4,096 4,096 4,096 4,096 4,096 2 2 2 2 64 64 64 64 128 524,288 524,288 524,288 524,288 524,288 160,000 160,000 160,000 120,000 120,000 Table 6. Architectural specifications and training configurations of models across different parameter scales (Chameleon setting and Chameleon+Speech setting). 14 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity"
        },
        {
            "title": "Metric Name",
            "content": "Mamba Loss () Mixture-ofMamba Loss () Performance Gain (%) () Relative Training FLOPs to Match Mamba (%) ()"
        },
        {
            "title": "Model\nScale",
            "content": "37M"
        },
        {
            "title": "Text Metrics",
            "content": "Training Loss Obelisc Val. Loss SSTK Val. Loss Training Loss Obelisc Val. Loss SSTK Val. Loss 6.1308 5.2866 6.6694 3.6262 4.1244 4."
        },
        {
            "title": "Avg Training Loss",
            "content": "4."
        },
        {
            "title": "Image Metrics",
            "content": "94M"
        },
        {
            "title": "Text Metrics",
            "content": "Training Loss Obelisc Val. Loss SSTK Val. Loss Training Loss Obelisc Val. Loss SSTK Val. Loss 5.7609 4.9231 6.3130 3.0294 3.6016 3."
        },
        {
            "title": "Avg Training Loss",
            "content": "4."
        },
        {
            "title": "Image Metrics",
            "content": "443M"
        },
        {
            "title": "Text Metrics",
            "content": "Training Loss Obelisc Val. Loss SSTK Val. Loss Training Loss Obelisc Val. Loss SSTK Val. Loss 5.3558 4.5258 5.9179 2.4637 3.0544 2."
        },
        {
            "title": "Avg Training Loss",
            "content": "3."
        },
        {
            "title": "Image Metrics",
            "content": "880M"
        },
        {
            "title": "Text Metrics",
            "content": "Training Loss Obelisc Val. Loss SSTK Val. Loss Training Loss Obelisc Val. Loss SSTK Val. Loss 5.2260 4.4127 5.7987 2.3073 2.8886 2."
        },
        {
            "title": "Avg Training Loss",
            "content": "3."
        },
        {
            "title": "Image Metrics",
            "content": "1.5B"
        },
        {
            "title": "Text Metrics",
            "content": "Training Loss Obelisc Val. Loss SSTK Val. Loss Training Loss Obelisc Val. Loss SSTK Val. Loss 5.1892 4.3692 5.7546 2.2284 2.8020 2."
        },
        {
            "title": "Avg Training Loss",
            "content": "3.4602 5.9561 5.1124 6.5023 3.5175 4.0469 3.9533 4.5247 5.6057 4.7683 6.1652 2.9414 3.5270 3. 4.0419 5.1703 4.3546 5.7471 2.3864 2.9820 2.6250 3.5364 5.1201 4.3105 5.6986 2.2438 2.8313 2. 3.4320 5.0591 4.2510 5.6335 2.1614 2.7393 2.3455 3.3670 2.85% 3.29% 2.51% 3.00% 1.88% 2.19% 2.92% 2.69% 3.14% 2.34% 2.90% 2.07% 3.54% 2.78% 3.46% 3.78% 2.89% 3.14% 2.37% 4.78% 3.33% 2.03% 2.32% 1.73% 2.75% 1.99% 3.67% 2.31% 2.51% 2.71% 2.10% 3.01% 2.24% 4.71% 2.69% 25.90% 26.60% 27.50% 60.90% 64.80% 57.50% 50.70% 35.70% 35.30% 37.00% 58.40% 62.60% 61.40% 49.80% 33.40% 35.10% 35.30% 62.00% 66.70% 54.70% 47.90% 48.40% 49.30% 50.50% 65.60% 72.80% 67.90% 58.30% 42.50% 44.50% 44.60% 65.40% 71.60% 62.10% 54.70% Table 7. Training and validation metrics across model scales in the Chameleon setting. In this setting, both image and text modalities are represented as discrete tokens. Mixture-of-Mamba achieves substantial performance improvements over Mamba Dense, with the image modality showing the largest gains across all five model scales: 37M, 94M, 443M, 880M, and 1.5B. Notably, the text modality also exhibits significant improvements, in contrast to the Transfusion setting where text gains were more modest. These results further highlight the effectiveness and efficiency of Mixture-of-Mamba, which consistently achieves strong performance with reduced relative training FLOPs. Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity (a) 163M C4 Val. Loss (b) C4 Loss Matching (c) 163M Wikipedia Val. Loss (d) Wikipedia Loss Matching (e) 760M C4 Val. Loss (f) C4 Loss Matching (g) 760M Wikipedia Val. Loss (h) Wikipedia Loss Matching (i) 1.4B C4 Val. Loss (j) C4 Loss Matching (k) 1.4B Wikipedia Val. Loss (l) Wikipedia Loss Matching Figure 4. Validation loss and loss matching for text modality across model scales (C4 and Wikipedia datasets) during multi-modal pretraining in the Transfusion setting. Results are shown for Mixture-of-Mamba, Mamba Dense, and Flex-Attention Transformer at three model scales: 163M, 760M, and 1.4B. (a, e, i) Validation loss on the C4 dataset shows that Mixture-of-Mamba achieves comparable performance at 163M and performs marginally better than Mamba Dense and Flex-Attention Transformer at the 760M and 1.4B scales. (b, f, j) Loss matching for C4 demonstrates that Mixture-of-Mamba reaches similar or slightly lower loss values at earlier training steps compared to Mamba Dense. (c, g, k) Validation loss on the Wikipedia dataset follows similar trend, with Mixture-of-Mamba showing marginal improvements at the 760M and 1.4B scales. (d, h, l) Loss matching for Wikipedia illustrates efficient training dynamics, with Mixture-of-Mamba aligning closely with Flex-Attention Transformer while reaching comparable or slightly lower loss values than Mamba Dense. Overall, Mixture-of-Mamba demonstrates moderate improvements over both baselines at the larger scales (760M and 1.4B). 16 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity (a) 163M Image Val. Loss (b) Image Val. Loss Matching (c) 760M Image Val. Loss (d) Image Val. Loss Matching (e) 1.4B Image Val. Loss (f) Image Val. Loss Matching Figure 5. Image validation loss and loss matching on the CC12M dataset across three model scales: 163M, 760M, and 1.4B during multi-modal pretraining in the Transfusion setting. (a, c, e) Validation loss curves show that Mixture-of-Mamba achieves substantially lower image validation loss compared to Mamba Dense and Flex-Attention Transformer across all scales, with the improvement becoming more pronounced as model size increases. (b, d, f) Loss matching curves demonstrate that Mixture-of-Mamba reaches the same loss values at earlier training steps compared to Mamba Dense, highlighting improved training efficiency. Overall, Mixture-of-Mamba achieves large improvements in image validation loss on the CC12M dataset, showcasing its effectiveness in the image modality. Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity (a) 163M Avg Training Loss (b) Avg Loss Matching (c) 760M Avg Training Loss (d) Avg Loss Matching (e) 1.4B Avg Training Loss (f) Avg Loss Matching Figure 6. Overall training loss and loss matching during multi-modal pretraining in the Transfusion setting. Results are shown for Mixture-of-Mamba, Mamba Dense, and Flex-Attention Transformer at three model scales: 163M, 760M, and 1.4B. (a, c, e) Training loss averaged across the image and text modalities demonstrates that Mixture-of-Mamba achieves substantial improvements over Mamba Dense, with notable reduction in training loss across all scales. (b, d, f) Loss matching results show that Mixture-of-Mamba and Flex-Attention Transformer reach the same loss values at earlier training steps compared to Mamba Dense, highlighting improved training efficiency. Note: The image loss in the Transfusion setting corresponds to the diffusion loss, which is of smaller magnitude compared to the cross-entropy loss in the text modality. Overall, Mixture-of-Mamba demonstrates significant gains in training loss and efficiency across multi-modal pretraining. 18 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity (a) 37M Image Training Loss (b) Image Loss Matching (c) Text Training Loss (d) Text Loss Matching (e) 94M Image Training Loss (f) Image Loss Matching (g) Text Training Loss (h) Text Loss Matching (i) 443M Image Training Loss (j) Image Loss Matching (k) Text Training Loss (l) Text Loss Matching (m) 880M Image Training Loss (n) Image Loss Matching (o) Text Training Loss (p) Text Loss Matching (q) 1.5B Image Training Loss (r) Image Loss Matching (s) Text Training Loss (t) Text Loss Matching Figure 7. Modality-specific pre-training loss and step matching plots across model scales (Chameleon setting). Training loss and loss matching are reported for image and text modalities across five model scales: 37M, 94M, 443M, 880M, and 1.5B. (a, e, i, m, q) Image training loss shows significant improvements for Mixture-of-Mamba (orange), which consistently achieves lower loss compared to Mamba Dense (cyan) across all scales. (b, f, j, n, r) Image loss matching compares the training dynamics and shows that Mixture-of-Mamba reaches the same loss values at earlier training steps compared to Mamba Dense, highlighting its improved efficiency. (c, g, k, o, s) Text training loss demonstrates competitive performance, with Mixture-of-Mamba achieving slightly lower loss values compared to Mamba Dense. (d, h, l, p, t) Text loss matching illustrates that Mixture-of-Mamba reaches the same loss values at earlier training steps compared to Mamba Dense, reflecting its efficient training dynamics. Overall, in the Chameleon setting, Mixture-of-Mamba achieves consistent improvements in the image modality, with substantial computational savings, while also demonstrating meaningful gains in the text modality. 19 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity (a) 37M Image Eval Loss (b) Image Loss Matching (c) Text Eval Loss (d) Text Loss Matching (e) 94M Image Eval Loss (f) Image Loss Matching (g) Text Eval Loss (h) Text Loss Matching (i) 443M Image Eval Loss (j) Image Loss Matching (k) Text Eval Loss (l) Text Loss Matching (m) 880M Image Eval Loss (n) Image Loss Matching (o) Text Eval Loss (p) Text Loss Matching (q) 1.5B Image Eval Loss (r) Image Loss Matching (s) Text Eval Loss (t) Text Loss Matching Figure 8. Training and evaluation losses for image and text modalities across model scales in the Chameleon setting on the Obelisc dataset. Results are shown for Mixture-of-Mamba and Mamba Dense across five model scales: 37M, 94M, 443M, 880M, and 1.5B. (a, e, i, m, q) Image evaluation loss demonstrates consistent improvements for Mixture-of-Mamba (orange), achieving lower loss compared to Mamba Dense (cyan) across all scales. (b, f, j, n, r) Image loss matching shows that Mixture-of-Mamba reaches the same loss values at earlier training steps compared to Mamba Dense, reflecting its improved training efficiency. (c, g, k, o, s) Text evaluation loss indicates competitive results for Mixture-of-Mamba, achieving lower losses relative to Mamba Dense. (d, h, l, p, t) Text loss matching highlights that Mixture-of-Mamba reaches the same loss values at earlier training steps, further demonstrating its efficiency in the text modality. Overall, Mixture-of-Mamba achieves strong and consistent improvements in both image and text modalities across all model scales in the Chameleon setting evaluated on the Obelisc dataset. 20 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity (a) 37M Image Eval Loss (b) Image Loss Matching (c) Text Eval Loss (d) Text Loss Matching (e) 94M Image Eval Loss (f) Image Loss Matching (g) Text Eval Loss (h) Text Loss Matching (i) 443M Image Eval Loss (j) Image Loss Matching (k) Text Eval Loss (l) Text Loss Matching (m) 880M Image Eval Loss (n) Image Loss Matching (o) Text Eval Loss (p) Text Loss Matching (q) 1.5B Image Eval Loss (r) Image Loss Matching (s) Text Eval Loss (t) Text Loss Matching Figure 9. Training and evaluation losses for image and text modalities across model scales in the Chameleon setting on the Shutterstock dataset. Results are shown for Mixture-of-Mamba and Mamba Dense across five model scales: 37M, 94M, 443M, 880M, and 1.5B. (a, e, i, m, q) Image evaluation loss demonstrates consistent improvements for Mixture-of-Mamba (orange), achieving lower loss compared to Mamba Dense (cyan) across all scales. (b, f, j, n, r) Image loss matching shows that Mixture-of-Mamba reaches the same loss values at earlier training steps compared to Mamba Dense, reflecting its improved training efficiency. (c, g, k, o, s) Text evaluation loss indicates competitive results for Mixture-of-Mamba, achieving lower losses relative to Mamba Dense. (d, h, l, p, t) Text loss matching highlights that Mixture-of-Mamba reaches the same loss values at earlier training steps, further demonstrating its efficiency in the text modality. Overall, Mixture-of-Mamba achieves strong and consistent improvements in both image and text modalities across all model scales in the Chameleon setting evaluated on the Shutterstock dataset. Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity (a) 37M Avg Training Loss (b) Avg Loss Matching (c) 94M Avg Training Loss (d) Avg Loss Matching (e) 443M Avg Training Loss (f) Avg Loss Matching (g) 880M Avg Training Loss (h) Avg Loss Matching (i) 1.5B Avg Training Loss (j) Avg Loss Matching Figure 10. Average training loss and step matching plots across model scales in the Chameleon setting. Results are shown for Mixture-of-Mamba and Mamba Dense across five model scales: 37M, 94M, 443M, 880M, and 1.5B. (a, c, e, g, i) Average training loss (across image and text modalities) demonstrates consistent reductions for Mixture-of-Mamba (orange), achieving lower loss values compared to Mamba Dense (cyan) at all model scales. (b, d, f, h, j) Average loss matching plots highlight that Mixture-of-Mamba reaches the same loss values at earlier training steps compared to Mamba Dense, reflecting improved training efficiency. Overall, Mixture-of-Mamba consistently reduces average training loss and achieves more efficient convergence across all model scales in the Chameleon setting. 22 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity (a) 37M Image Training Loss (b) Image Loss Matching (c) Text Training Loss (d) Text Loss Matching (e) 37M Speech Training Loss (f) Speech Loss Matching (g) Speech Eval Loss: LL60K (h) Speech Eval Loss: PPL30K (i) 94M Image Training Loss (j) Image Loss Matching (k) Text Training Loss (l) Text Loss Matching (m) 94M Speech Training Loss (n) Speech Loss Matching (o) Speech Eval Loss: LL60K (p) Speech Eval Loss: PPL30K Figure 11. Training and evaluation losses for image, text, and speech modalities (37M and 94M scales) in the Chameleon+Speech setting. Results are reported for Mixture-of-Mamba and Mamba Dense. (a, e, i) Image training loss demonstrates that Mixture-of-Mamba (orange) achieves consistently lower loss compared to Mamba Dense (cyan). (b, f, j) Image loss matching highlights Mixture-of-Mambas ability to reach the same loss values at earlier training steps, showing improved training efficiency. (c, g, k) Text training loss shows competitive results for Mixture-of-Mamba, improving over Mamba Dense. (d, h, l) Text loss matching confirms Mixture-of-Mambas ability to reach the same loss values at earlier training steps, showing improved training efficiency. (e, m) Speech training loss highlights significant improvements in speech modality performance. (f, n) Speech loss matching shows efficient learning dynamics for Mixture-ofMamba. (g, o) Speech evaluation loss on LL60K confirms notable performance gains, and (h, p) Speech evaluation loss on PPL30K further highlights the efficiency of Mixture-of-Mamba. 23 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity (a) 443M Image Training Loss (b) Image Loss Matching (c) Text Training Loss (d) Text Loss Matching (e) 443M Speech Training Loss (f) Speech Loss Matching (g) Speech Eval Loss: LL60K (h) Speech Eval Loss: PPL30K (i) 880M Image Training Loss (j) Image Loss Matching (k) Text Training Loss (l) Text Loss Matching (m) 880M Speech Training Loss (n) Speech Loss Matching (o) Speech Eval Loss: LL60K (p) Speech Eval Loss: PPL30K (q) 1.5B Image Training Loss (r) Image Loss Matching (s) Text Training Loss (t) Text Loss Matching (u) 1.5B Speech Training Loss (v) Speech Loss Matching (w) Speech Eval Loss: LL60K (x) Speech Eval Loss: PPL30K Figure 12. Training and evaluation losses for image, text, and speech modalities (443M, 880M, and 1.5B scales) in the Chameleon+Speech setting. Results are reported for Mixture-of-Mamba and Mamba Dense. (a, i, q) Image training loss demonstrates that Mixture-of-Mamba (orange) consistently outperforms Mamba Dense (cyan) across larger scales. (b, j, r) Image loss matching highlights improved training efficiency for Mixture-of-Mamba, reaching the same loss values at earlier training steps. (c, k, s) Text training loss shows Mixture-of-Mamba achieving better performance. (d, l, t) Text loss matching further demonstrates efficient learning dynamics. (e, m, u) Speech training loss confirms substantial gains for Mixture-of-Mamba in the speech modality, consistent across model scales. (f, n, v) Speech loss matching illustrates the improved efficiency of Mixture-of-Mamba across scales. (g, o, w) Speech evaluation loss on LL60K highlights consistent improvements, while (h, p, x) Speech evaluation loss on PPL30K demonstrates notable gains and efficient performance across scales. 24 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity (a) 37M Image Eval Loss (b) Image Loss Matching (c) Text Eval Loss (d) Text Loss Matching (e) 94M Image Eval Loss (f) Image Loss Matching (g) Text Eval Loss (h) Text Loss Matching (i) 443M Image Eval Loss (j) Image Loss Matching (k) Text Eval Loss (l) Text Loss Matching (m) 880M Image Eval Loss (n) Image Loss Matching (o) Text Eval Loss (p) Text Loss Matching (q) 1.5B Image Eval Loss (r) Image Loss Matching (s) Text Eval Loss (t) Text Loss Matching Figure 13. Training and validation losses for image and text modalities across model scales in the Chameleon+Speech setting evaluated on the Obelisc dataset. Results are shown for Mixture-of-Mamba and Mamba Dense across five model scales: 37M, 94M, 443M, 880M, and 1.5B. (a, e, i, m, q) Image evaluation loss demonstrates consistent gains for Mixture-of-Mamba (orange) over Mamba Dense (cyan), even with the inclusion of the speech modality. (b, f, j, n, r) Image loss matching shows that Mixture-of-Mamba reaches the same loss values at earlier training steps compared to Mamba Dense, highlighting improved efficiency. (c, g, k, o, s) Text evaluation loss indicates consistent reductions for Mixture-of-Mamba relative to Mamba Dense across all scales. (d, h, l, p, t) Text loss matching illustrates that Mixture-of-Mamba reaches the same loss values at earlier training steps compared to Mamba Dense, maintaining its efficiency in the text modality. Overall, Mixture-of-Mamba achieves consistent improvements in both image and text modalities while maintaining its efficiency, even with the addition of the speech modality. These results confirm the robustness of Mixture-of-Mamba in multi-modal settings. 25 Mixture-of-Mamba: Enhancing Multi-Modal State-Space Models with Modality-Aware Sparsity (a) 37M Image Eval Loss (b) Image Loss Matching (c) Text Eval Loss (d) Text Loss Matching (e) 94M Image Eval Loss (f) Image Loss Matching (g) Text Eval Loss (h) Text Loss Matching (i) 443M Image Eval Loss (j) Image Loss Matching (k) Text Eval Loss (l) Text Loss Matching (m) 880M Image Eval Loss (n) Image Loss Matching (o) Text Eval Loss (p) Text Loss Matching (q) 1.5B Image Eval Loss (r) Image Loss Matching (s) Text Eval Loss (t) Text Loss Matching Figure 14. Training and validation losses for image and text modalities across model scales in the Chameleon+Speech setting evaluated on the Shutterstock dataset. Results are shown for Mixture-of-Mamba and Mamba Dense across five model scales: 37M, 94M, 443M, 880M, and 1.5B. (a, e, i, m, q) Image evaluation loss demonstrates consistent gains for Mixture-of-Mamba (orange) over Mamba Dense (cyan), even with the inclusion of the speech modality. (b, f, j, n, r) Image loss matching shows that Mixture-of-Mamba reaches the same loss values at earlier training steps compared to Mamba Dense, highlighting improved efficiency. (c, g, k, o, s) Text evaluation loss indicates consistent reductions for Mixture-of-Mamba relative to Mamba Dense across all scales. (d, h, l, p, t) Text loss matching illustrates that Mixture-of-Mamba reaches the same loss values at earlier training steps compared to Mamba Dense, maintaining its efficiency in the text modality. Overall, Mixture-of-Mamba achieves consistent improvements in both image and text modalities while maintaining its efficiency, even with the addition of the speech modality. These results confirm the robustness of Mixture-of-Mamba in multi-modal settings."
        }
    ],
    "affiliations": [
        "Department of Computer Science, Stanford University",
        "FAIR at Meta",
        "Machine Learning Department, Carnegie Mellon University"
    ]
}