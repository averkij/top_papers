{
    "paper_title": "Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction",
    "authors": [
        "Chengzhi Xu",
        "Yuyang Wang",
        "Lai Wei",
        "Lichao Sun",
        "Weiran Huang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Recently, multimodal large language models (MLLMs) have attracted increasing research attention due to their powerful visual understanding capabilities. While they have achieved impressive results on various vision tasks, their performance on chart-to-code generation remains suboptimal. This task requires MLLMs to generate executable code that can reproduce a given chart, demanding not only precise visual understanding but also accurate translation of visual elements into structured code. Directly prompting MLLMs to perform this complex task often yields unsatisfactory results. To address this challenge, we propose {ChartIR}, an iterative refinement method based on structured instruction. First, we distinguish two tasks: visual understanding and code translation. To accomplish the visual understanding component, we design two types of structured instructions: description and difference. The description instruction captures the visual elements of the reference chart, while the difference instruction characterizes the discrepancies between the reference chart and the generated chart. These instructions effectively transform visual features into language representations, thereby facilitating the subsequent code translation process. Second, we decompose the overall chart generation pipeline into two stages: initial code generation and iterative refinement, enabling progressive enhancement of the final output. Experimental results show that, compared to other method, our method achieves superior performance on both the open-source model Qwen2-VL and the closed-source model GPT-4o."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 5 1 ] . [ 1 7 3 8 4 1 . 6 0 5 2 : r Improved Iterative Refinement for Chart-to-Code Generation via Structured Instruction Chengzhi Xu1, Yuyang Wang 1, Lai Wei 1, Lichao Sun3, Weiran Huang1,2,4 1 MIFA Lab, Shanghai Jiao Tong University 2 Shanghai Innovation Institute 3 Lehigh University 4 State Key Laboratory of General Artificial Intelligence, BIGAI"
        },
        {
            "title": "Abstract",
            "content": "Recently, multimodal large language models (MLLMs) have attracted increasing research attention due to their powerful visual understanding capabilities. While they have achieved impressive results on various vision tasks, their performance on chart-to-code generation remains suboptimal. This task requires MLLMs to generate executable code that can reproduce given chart, demanding not only precise visual understanding but also accurate translation of visual elements into structured code. Directly prompting MLLMs to perform this complex task often yields unsatisfactory results. To address this challenge, we propose ChartIR, an iterative refinement method based on structured instruction. First, we distinguish two tasks: visual understanding and code translation. To accomplish the visual understanding component, we design two types of structured instructions: description and difference. The description instruction captures the visual elements of the reference chart, while the difference instruction characterizes the discrepancies between the reference chart and the generated chart. These instructions effectively transform visual features into language representations, thereby facilitating the subsequent code translation process. Second, we decompose the overall chart generation pipeline into two stages: initial code generation and iterative refinement, enabling progressive enhancement of the final output. Experimental results show that, compared to other method, our method achieves superior performance on both the open-source model Qwen2-VL and the closed-source model GPT-4o."
        },
        {
            "title": "Introduction",
            "content": "Recently, Multimodal Large Language Models (MLLMs) [2, 13, 4, 15, 12], such as GPT-4V, LLaVA, and Qwen-VL, have attracted significant attention due to their impressive capabilities in visual understanding and reasoning. Among the many downstream tasks involving multimodal inputs, chart reasoning has emerged as important domain [20, 14, 5, 11, 6], as charts play crucial role in presenting structured data in visual format. However, recent studies show that MLLMs still struggle to handle chart-related tasks effectively, including summarization, question answering, and particularly chart-to-code generation. The unique difficulty of chart inputssuch as their high information density and complex layoutrenders direct learning from vision-language corpora insufficient. Therefore, existing MLLMs generally perform poorly on chart-to-code generation benchmarks, due to their inability to accurately capture and translate multi-dimensional chart information. To address the limitations of direct generation, some recent efforts have attempted to enhance the chart-to-code capability of MLLMs through more structured approaches. ChartCoder [24] improves Equal contribution. Corresponding author. Preprint. Under review. performance by decomposing the generation process into step-by-step procedures and applying instruction tuning to open-source models. However, due to its reliance on training, ChartCoder cannot be applied to commercial closed-source models like GPT-4o, and its final performance remains suboptimal. On the other hand, METAL [10] adopts training-free, multi-agent system that enables compatibility with both open-source and commercial MLLMs. It iteratively refines the generated code through pipeline of generation, critique, and revision based on visual evaluation metrics. Nevertheless, METAL suffers from critical limitations: it updates code based only on the lowestperforming single metric, which often leads to unbalanced optimization, and some metrics such as color are rarely used in practice (see Table 1). Moreover, optimizing one metric may degrade others, and relying on single score for update decisions can result in code that is worse overall. These issues hinder METALs ability to generate high-quality, visually faithful chart code. Metric Plot2Code ChartMimic Table 1: Total usage counts of three metrics To better understand the unique challenges behind chart-to-code generation, we conduct detailed analysis and identify two core bottlenecks: visual understanding and code translation. Visual understanding refers to the MLLMs ability to extract accurate and multi-dimensional informationsuch as structure, text, colors, and layoutfrom the input chart. Code translation involves converting this rich visual information into executable and semantically faithful code. Motivated by this observation, we propose ChartIR, training-free iterative framework that explicitly addresses both challenges. To enhance the visual understanding phase, we introduce structured, multi-dimensional chart description mechanism that provides detailed annotations over several key visual components. To improve code translation, we design an iterative generation scheme that leverages both the original image and the chart description to guide code refinement. In each iteration, the model compares differences between the generated chart and the reference chart across all key dimensionstext, color, layout, and typeenabling more holistic and targeted updates. Text Color Overall Total samples 694 4 93 0 108 1252 201 To validate the effectiveness of ChartIR, we conduct extensive experiments on two widely-used chart-to-code benchmarks: Plot2Code [19] and ChartMimic [22]. We evaluate our method on both open-source (Qwen2-VL) and closed-source (GPT-4o) models. Results show that ChartIR significantly outperforms both direct generation and METAL across comprehensive set of evaluation metrics, including GPT-4o Score, low-level metrics (Text, Layout, Type, Color), and traditional image similarity metrics (SSIM, CLIP, MSE, PSNR). Notably, the improvements in GPT-4o Scorean evaluation metric closely aligned with human judgmentdemonstrate that our approach leads to substantial gains in visual and structural fidelity of the generated charts. We further conduct ablation studies to isolate the effects of chart description and iterative difference-based refinement. Results show that removing either component leads to considerable performance degradation, confirming the necessity and effectiveness of each module in the ChartIR framework. In summary, our contributions are threefold: (1) We propose ChartIR, training-free, model-agnostic framework for enhancing chart-to-code generation in MLLMs; (2) We introduce structured chart description mechanism that captures multi-dimensional visual features of the reference chart, which is shown to be essential for accurate code generation; and (3) We design an iterative difference-based refinement process that improves code quality across all visual aspects. Our method achieves state-ofthe-art results on multiple benchmarks both on open-source and closed-source models and provides robust blueprint for future research in chart-to-code generation."
        },
        {
            "title": "2 Related work",
            "content": "2.1 Multimodal Large Language Models Multimodal large language models (MLLMs) extend the capabilities of large language models (LLMs) by incorporating vision inputs, enabling them to perform tasks involving both text and images. Opensource MLLMs such as Qwen-VL [2], DeepSeek-VL [13], LLaVA [12], and Mini-Gemini [4] have attracted attention due to their transparency, accessibility, and potential for fine-tuning. These models are often built by aligning pre-trained vision encoders with language models using multimodal instruction tuning. On the other hand, closed-source commercial models such as GPT-4o [15] and Claude [1] integrate proprietary vision-language pipelines and demonstrate superior performance 2 Figure 1: Overview of the ChartIR. The method consists of two steps: 1) Initial Code Generation: First, description is generated based on the reference chart. Then, using both the description and the reference chart, an initial code is produced. 2) Iterative Refinement: In this stage, first comparing the chart generated from the initial code with the reference chart and produces description of the difference. Based on this difference, the reference chart, and the description of the reference chart, new code is generated and then get new chart. This process is repeated iteratively. The iteration continues until convergence is achieved. across wide range of multimodal tasks. In chart-related tasks, including chart-to-code generation, these closed-source models consistently outperform their open-source counterparts. However, their closed nature restricts research access, model interpretability, and downstream adaptability, especially for tasks requiring task-specific customization or iterative optimization. 2.2 Chart-to-Code Generation Chart-to-code generation aims to convert visual charts into executable code that reproduces the original figure. Recent studies [20, 5, 11, 23] have evaluated MLLMs on this task. These works show that both open-source models (e.g., Qwen-VL, DeepSeek-VL) and closed-source models (e.g., GPT-4o, Claude) still struggle with accurately reproducing charts. Among the most recent and advanced approaches, ChartCoder improves performance by decomposing the task into sequential substeps and training dedicated modules, but is limited to open-source models and still underperforms closed-source ones. METAL introduces training-free multi-agent framework that critiques and revises code iteratively, allowing compatibility with both model types. However, it relies heavily on single evaluation metric per iteration and lacks unified optimization mechanism across all visual dimensions, leading to suboptimal convergence. Our method shares with METAL the idea of iterative refinement but differs by introducing multi-dimensional description and holistic difference-based generation strategy that ensures all critical aspects of the chart are considered in every iteration."
        },
        {
            "title": "3 Methodology",
            "content": "We first provide formal problem formulation of chart-to-code generation. Given reference chart Ir, we aim to generate executable code Cg using an MLLM such that the resulting rendered chart minimizes visual discrepancy with the reference chart Ir, i.e., min Cg disc(Render(Cg), Ir), (1) where disc represents the visual discrepancy. Common metrics for disc include SSIM [18], CLIP scores [16], etc. Algorithm 1: Initial Code Generation Input: Reference chart image Ir, MLLM-Describer, MLLM-Coder, Render. 1 Generate chart description: δ MLLM-Describer(Ir); 2 Generate initial code: Cg MLLM-Coder(Ir, δ); 3 Render initiail image:Ig Render(Cg); 4 return Cg, Ig According to the problem definition, natural strategy is to iteratively optimize the generated code based on the discrepancy evaluation function disc. Figure 1 illustrates the entire pipeline of our code generation and optimization process. Overall, our method first generates textual description of the reference chart Ir, and then produces the initial code Cg based on both the description and the reference chart Ir. We then iteratively refine the code new at each step by minimizing the visual discrepancy disc(I new , Ir), where new ). This refinement process continues until the MLLM can no longer make meaningful improvements to the generated code. In the remainder of this section, we provide detailed description of our proposed method. = Render(C new g Stage 1: Initial Code Generation In the first stage, we prompt the multimodal large language models (MLLMs) to produce the initial chart-generating code Cg by leveraging both the reference chart image Ir and textual description δ of the chart. Description Generation. To enhance the models understanding of the visual chart content, we first generate description δ that captures key semantic and structural information from Ir. This description serves as an auxiliary input to guide the model during code generation. For closedsource models such as GPT-4o, which possess strong chart understanding capabilities, we directly prompt the model with the reference chart to obtain the description δ prior to code generation. In contrast, open-source models typically demonstrate weaker performance in chart interpretation. To address this limitation, we fine-tune Qwen2.5-VL [3] specifically for the chart description task. We construct training set using the ChartX [20] dataset by prompting GPT-4o with chart images and corresponding code to generate descriptive captions. This results in dataset of approximately 5,000 (chart, description) pairs. We then fine-tune Qwen2.5-VL using this dataset to enable it to generate high-quality chart descriptions. Code Generation. Once the description δ is obtained, it is combined with the chart image Ir and fed into the MLLM to produce the initial code Cg. In the ablation experiments, we demonstrate the effectiveness of incorporating chart descriptions for both open-source and closed-source models. The overall procedure of Stage 1 is summarized in Algorithm 1. Stage 2: Iterative Refinement In the second stage, we iteratively refine the initial code Cg to improve the quality of the generated chart. The generated chart image Ig is first obtained by executing Cg. We then prompt the MLLM to describe the differences µ between Ir and Ig. Based on this difference description, the model analyzes Ir, Ig, and Cg to produce revised version of the code, denoted as new . This updated code is executed to generate new chart image new . Unlike METAL, which refines the code based on improvements in specific metric at each iteration, our method allows the MLLM to consider all visual and structural aspects of the chart holistically. The full iterative process is outlined in Algorithm 2. Update Criterion. The decision to update Cg to new is based on the discrepancy evaluation function D, defined in Equation 1. The update occurs only when the new image new is quantitatively closer to the reference image Ir than Ig isthat is, when disc(I new , Ir) < disc(Ig, Ir). To ensure robust and comprehensive evaluation, we aggregate multiple visual similarity metrics, including CLIP Score, DINO, SSIM, PSNR, and Hamming Distance. These are combined via weighted average to compute the overall discrepancy score. An update is accepted only when the aggregated score for new is less than that of Ig, ensuring improvements across multiple dimensions. g 4 Algorithm 2: Iterative Refinement Input: Reference chart image Ir, initial code Cg, description δ, MLLM-Coder, Render. Output: Final refined code. Render chart: Ig Render(Cg); Generate difference description: µ MLLM-Coder(Ir, Ig); Refine code: new Render new chart: new if disc(I new MLLM-Coder(Ir, Ig, Cg, δ, µ); Render(C new 1 Initialize convergence counter: 0; 2 repeat 3 4 5 6 7 8 9 10 11 12 13 until reaches pre-determined threshold; 14 return Cg Reset convergence counter: 0; , Ir) < disc(Ig, Ir) then Refine code: Cg new Increment convergence counter: + 1; end else ); ; Convergence Criterion. To ensure convergence, we introduce convergence counter initialized to zero. Each time the discrepancy score declines (i.e., new is closer to Ir than Ig), the counter is reset. If an iteration fails to decrease the discrepancy score, is incremented. Once the counter reaches predefined threshold, the refinement process stops, and the final code is returned. Debug. During the entire iterative refinement process, the MLLMs may occasionally generate code that fails to execute. To address this issue, we invoke specialized LLM designed for code-related tasks [7] to perform automatic bug fixing. Specifically, we provide the model with both the error message and the faulty code as input, enabling it to generate corrected version of the code."
        },
        {
            "title": "4 Experiments",
            "content": "4.1 Experiment setting The primary goal of our experiments is to evaluate whether ChartIR can enhance the ability of base multimodal models to convert chart images into code, and whether it can outperform existing approaches such as METAL [10] . Based on our experimental setting, we conduct experiments by applying ChartIR to GPT-4o and Qwen2-VL across two benchmark datasets: Plot2Code [19] and ChartMimic [22] . We also evaluate several baselines, including Direct Generation, and METAL. Baselines. We compare our approach with the following two methods: 1) Direct Generation: In this setting, we directly prompt MLLMs to generate code based on the reference chart. The MLLMs receive only generation prompt along with the reference chart, without access to any additional information or further optimization processes. 2) METAL: METAL employs an iterative multi-agent optimization framework to achieve the generation process. Dataset. In this work, we adopt two widely-used open-source datasets specifically designed for chart-to-code tasks: Plot2Code and ChartMimic. Plot2Code contains 132 manually curated test examples, whose plots exhibit wide range of diversity in terms of size, textual elements, color schemes, and chart types [19]. It supports two evaluation settings: Direct Asking and Conditional Asking. ChartMimic consists of 500 manually filtered (figure, code, instruction) triplets for the Direct Mimic task and another 500 triplets for the Customized Mimic task [22]. The plots in ChartMimic cover 18 regular types and 4 advanced types, demonstrating substantial diversity and complexity. In this work, we focus exclusively on the Direct Asking setting for Plot2Code and the 500 triplets for the Direct Mimic task for ChartMimic. Base Model. Our experiments are conducted under two distinct settings: closed-source and opensource. In the closed-source setting, commercial close-source models are allowed, whereas in Table 2: Evaluation results on the Plot2Code and ChartMimic dataset. ChartIR outperforms both Direct Generation and METAL under most evaluation metrics for both open-source (Qwen2-VL) and closed-source (GPT-4o) models. Dataset Base Model Method Low-Level Metrics Traditional Metrics GPT-4o Score Text Type Layout Color PSNR SSIM MSE o 2 P M a Qwen2-VL GPT-4o Qwen2-VL GPT-4o Direct METAL ChartIR (ours) Direct METAL ChartIR (ours) Direct METAL ChartIR (ours) Direct METAL ChartIR (ours) 0.30 0.47 0.45 0.70 0.83 0.70 0.30 0.35 0.48 0.78 0.86 0.82 0.65 0.63 0. 0.80 0.86 0.79 0.41 0.40 0.64 0.82 0.84 0.84 0.60 0.75 0.76 0.89 0.86 0.95 0.78 0.78 0. 0.97 0.96 0.96 0.45 0.50 0.50 0.74 0.71 0.68 0.41 0.33 0.49 0.74 0.80 0.76 11.91 12.00 13. 13.53 12.50 14.29 11.73 11.74 12.01 13.05 13.09 13.68 0.62 0.62 0.69 0.68 0.66 0.69 0.60 0.60 0. 0.64 0.70 0.67 16958 16380 11995 12746 13864 10676 16644 16461 15127 11846 11356 10129 3.12 3.34 (0.22) 3.54 (0.42) 5.61 6.02 (0.41) 6.56 (0.95) 2.20 2.32 (0.12) 3.86 (1.66) 6.62 6.89 (0.27) 7.15 (0.53) the open-source setting, such models are excluded. The open-source setting ensures that all user data remains local, thereby preserving user privacy. Specifically, for the closed-source setting, we employ GPT-4o [15] as our experimental model. Given GPT-4os strong inherent capabilities in chart understanding, we utilize it directly to generate chart descriptions. In contrast, for the open-source setting, we adopt Qwen2-VL [17] as our base model and generate chart descriptions using our fine-tuned version of Qwen2.5-VL [3]. Fine-Tuning. To further enhance the base models chart-to-code capability, we perform instruction tuning on Qwen2.5-VL using curated dataset derived from the ChartX [20] collection. Specifically, we select 5,000 chart samples, each containing an image and its corresponding Python source code. To construct high-quality multi-dimensional descriptions for each chart, we prompt GPT-4o to analyze the image and produce structured explanations based on the underlying code. The resulting dataset consists of (image, description, code) triplets, which we use to fine-tune Qwen2.5-VL. This finetuning step equips the model with the ability to understand and reason over visual elements in more structured and interpretable manner, forming stronger foundation for our iterative chart-to-code generation framework. Evaluation Metric. We employ the following metrics to evaluate the quality of the final generated charts: 1) GPT-4o Score. GPT has been widely adopted for evaluating variety of natural language and vision tasks [21, 9]. Therefore, we follow the evaluation benchmark Plot2Code [19] and adopt the same GPT-4o prompt: we feed both the ground-truth chart and the generated chart into GPT-4o, then guide it to output quality score ranging from 0 to 10. To ensure the reliability and precision of the GPT-4o score, we obtain three independent GPT-4o scores and compute their mean as the final score. 2) Four low-level metrics. Unlike GPT-4o Score which serves as high-level metrics, We follow the four low-level evaluation metrics proposed in the ChartMimic [22]:Text Score, Layout Score, Type Score and Color Score. They are designed to evaluate four key low-level visual components of chart: Text, Layout, Type and Color. 3) Some other traditional metrics such as PSNR, SSIM and MSE [8, 18] which are widely used for assessing image similarity. 4.2 Experiment Results As shown in Table 2, the experimental results on the Plot2Code and ChartMimic datasets demonstrate that ChartIR consistently outperforms baseline methods across most evaluation metrics. These results validate our core objectiveenhancing the chart-to-code capabilities of base multimodal models like GPT-4o and Qwen2-VLand show that ChartIR provides consistent improvements over both Direct Generation and existing approaches such as METAL. 6 GPT-4o Results. For the powerful closed-source GPT-4o model, ChartIR consistently outperforms both direct generation and METAL under all major evaluation metrics. On the Plot2Code dataset, direct generation and METAL achieve GPT-4o Scores of 5.61 and 6.02, respectively. In contrast, ChartIR reaches score of 6.56, representing relative improvement of 17% over direct generation. Besides the overall score, ChartIR also improves across low-level metrics (Text, Type, Layout, Color) and traditional visual metrics (PSNR, SSIM, MSE), demonstrating better structural preservation and appearance similarity. On the ChartMimic dataset, ChartIR again shows the strongest performance, achieving GPT-4o Score of 7.15, compared to 6.62 for direct generation and 6.89 for METAL. These results highlight ChartIRs superiority in generating chart code that best aligns with both structural accuracy and visual fidelity, particularly under the strong reasoning capabilities of GPT-4o. Qwen2-VL Results. For the open-source model Qwen2-VL, ChartIR also brings consistent and notable gains. On the Plot2Code dataset, GPT-4o Score improves from 3.12 (direct) and 3.34 (METAL) to 3.54 using ChartIR. Notably, in addition to higher structural accuracy (Layout +0.16), ChartIR greatly improves traditional metrics such as PSNR (13.61 vs. 11.91) and SSIM (0.69 vs. 0.62), highlighting enhanced image reconstruction quality. On the ChartMimic dataset, Qwen2-VL with ChartIR reaches GPT-4o Score of 3.86, significantly outperforming both direct (2.20) and METAL (2.32). Low-level metrics also show marked gains (Text from 0.30 to 0.48, Type from 0.41 to 0.64), confirming the broad effectiveness of our approach in challenging visual conditions. Analysis. These results validate the strength of ChartIR in enhancing both open-source and closedsource multimodal models in the chart-to-code generation task. Compared to METALwhich also utilizes iterative refinementour method achieves stronger layout alignment and semantic expressiveness. We believe this is due to the incorporation of an intermediate structured representation that retains chart semantics and layout cues while being model-friendly for generation.Furthermore, the improvements in the GPT-4o Scorea metric closely aligned with human preferencesuggest that the benefits of ChartIR go beyond token-level or image-level matching. Instead, ChartIR enhances the holistic perception of the chart as visually and semantically coherent artifact. This supports our motivation to incorporate intermediate representations that bridge the modality gap between vision and code, thereby benefiting both symbolic alignment and perceptual faithfulness. In summary, across both models and datasets, ChartIR leads to measurable and consistent improvements, highlighting its potential as general-purpose enhancer for chart-to-code generation tasks. It performs robustly across diverse settings and model capabilities, making it strong foundation for future multimodal reasoning and generation systems. 4.3 Ablation Study To better understand the contribution of different components in ChartIR, we conduct an ablation study focusing on the two key elements: description and difference. Specifically, we randomly sample 20% of examples from both the ChartMimic and Plot2Code datasets, resulting in combined test set of 132 samples for evaluation. We perform three separate experiments using both the open-source model Qwen2-VL and closedsource model GPT-4o on this ablation set: 1) ChartIR: the complete version incorporating both the description and difference components; 2) Only Difference: variant where the description part is removed, and only the difference information is retained; and 3) Only Description: variant where the difference information is removed, and only the description is used. This setup allows us to isolate the impact of each component and evaluate how much each contributes to the models ability to generate accurate chart code from images. Effect of Description. We first investigate the impact of including the description component in ChartIR. This component provides natural language summary of the visual content, offering high-level semantic guidance to the generation model. As shown in Table 3, removing the description results in noticeable performance drop. For Qwen2-VL, the GPT-4o score decreases from 3.96 to 3.10, and the average score of low-level structural metricsText, Chart Type, Layout, and Colordrops from 0.61 to 0.51. In addition, traditional similarity metrics such as CLIP and SSIM also show consistent degradation. While for GPT-4o, the GPT-4o score decreases from 7.13 to 7.00. On the other metrics, ChartIR and Only Difference achieved comparable results. These results suggest that 7 Table 3: Ablation study results on ChartIR components. We compare full ChartIR with variants using only static descriptions or difference-based prompts. Base Model Experiment Setting Low-Level Metrics Traditional Metrics GPT-4o Score Text Type Layout Color PSNR SSIM MSE Qwen2-VL GPT-4o ChartIR (Both Components) Only Description Only Difference ChartIR (Both Components) Only Description Only Difference 0.42 0.35 0.37 0.80 0.76 0.79 0.66 0.54 0. 0.81 0.81 0.83 0.81 0.68 0.69 0.69 0.69 0.71 0.54 0.43 0.45 0.94 0.92 0.91 12.30 12.47 12. 14.09 13.76 14.14 0.65 0.63 0.64 0.70 0.69 0.70 14500 14299 15070 9144 9883 8976 3.96 3.38 (0.58) 3.10 (0.86) 7.13 6.91 (0.22) 7.00 (0.13) the description plays critical role in helping the model better understand the charts overall structure and semantics. Effect of Difference. We then ablate the difference component, which highlights the key variations between the current chart and its potential reference template. Without this explicit contrastive information, the model must infer visual and structural changes solely from the image, which increases the generation difficulty. The results demonstrate similar trend: for Qwen2-VL, the GPT-4o score drops from 3.96 to 3.38, and the average of the four structural metrics decreases from 0.61 to 0.50; for GPT-4o, the GPT-4o score drops from 7.13 to 6.91, and the four low-level metrics also show such decline. Furthermore, traditional similarity-based measures decline in both Qwen2-VL and GPT-4o, confirming that explicitly modeling chart-specific differences significantly improves generation accuracy, particularly in compositional reasoning scenarios. Conclusion. The ablation study clearly demonstrates that both description and difference components are essential to ChartIRs performance, with their combined use yielding the best results across all evaluation metrics. While the description provides crucial semantic context for chart understanding, the difference component enables precise identification of structural variations, and their synergistic effect is particularly pronounced for open-source models like Qwen2-VL. These findings validate our design choice of incorporating both complementary representations in ChartIRs intermediate format."
        },
        {
            "title": "5 Case study",
            "content": "We conduct case study based on ChartIR, as illustrated in Figure 2. In the initial code generation stage, description instruction of the reference chart is first obtained. This description accurately captures key visual attributes such as the color and text annotations of each segment in the pie chart. Based on this description and the reference chart, the initial code is then generated. Upon execution, the resulting initial chart successfully renders the correct segment colors but fails to display the textual annotations. In the first refinement stage, difference instruction is firstly generated by comparing the initial chart with the reference chart. This instruction correctly identifies the missing text annotations in the initial chart compared to the reference chart. Guided by this difference, refined code 1 is produced. Executing this code results in chart where the annotations are correctly rendered; however, one segments color is incorrectly displayed. During the second refinement stage, new difference instruction is generated based on the comparison between refined chart 1 and the reference chart. This difference successfully captures the incorrect segment color. Based on this input, refined code 2 is generated, which corrects the color error without introducing any new issues. Subsequent iterations beyond predefined refinement threshold fail to produce further improvements, triggering termination of the refinement process. refined chart 2 is thus selected as the final output. Compared to the initial chart, the final chart demonstrates more accurate textual annotations and segment colors. 8 Figure 2: case study based on ChartIR. The initial chart lacked some textual information. The first refinement successfully recovered the missing text but introduced color error. The second refinement corrected the color error without introducing any new issues. As result, we obtained chart with overall better performance compared to the initial version."
        },
        {
            "title": "6 Discussion and conclusion",
            "content": "Limitations. Although ChartIR achieves superior performance across various metrics compared to direct prompting and Metal, it still exhibits two main limitations: 1) Compared to Metal, our method requires more computational cost, but since it makes fewer total queries, it results in slightly lower time overhead on closed-source models. 2) Limited effectiveness on closed-source models: While our structured instruction scheme demonstrates strong capabilities on open-source models, its impact on closed-source models is relatively limited. This is primarily because closed-source models like GPT-4o already possess strong inherent chart understanding abilities, rendering additional structured instructions less effective. We believe that the performance of ChartIR on such models could be further enhanced with more powerful instruction generation model. Conclusion. In this work, we present ChartIR, training-free and model-agnostic framework designed to enhance chart-to-code generation in multimodal large language models. Unlike prior approaches that rely on either direct prompting or metric-driven refinement, ChartIR introduces structured, two-stage process consisting of multi-dimensional chart description and iterative differencebased refinement. This design directly targets the core challenges of chart understanding and faithful code translation. Through comprehensive experiments on two benchmark datasets, Plot2Code and ChartMimic, we demonstrate that ChartIR significantly improves over both direct generation and the state-of-the-art method METAL, across broad range of evaluation metrics including GPT-4o Score, layout accuracy, and visual similarity. Notably, our ablation studies validate the essential roles of both 9 the chart description and difference. ChartIRs generality and compatibility with both open-source and commercial MLLMs suggest that it can serve as robust and extensible solution for future work in multimodal chart understanding and generation."
        },
        {
            "title": "7 Acknowledgments",
            "content": "This project is supported by the National Natural Science Foundation of China (No. 62406192), Opening Project of the State Key Laboratory of General Artificial Intelligence (No. SKLAGI2024OP12), and Doubao LLM Fund."
        },
        {
            "title": "References",
            "content": "[1] Anthropic. The claude 3 model family: Opus, sonnet, haiku. https://www.anthropic.com/news/ claude-3-family, March 2024. Accessed: 2025-05-15. [2] Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. [3] Shuai Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Sibo Song, Kai Dang, Peng Wang, Shijie Wang, Jun Tang, Humen Zhong, Yuanzhi Zhu, Mingkun Yang, Zhaohai Li, Jianqiang Wan, Pengfei Wang, Wei Ding, Zheren Fu, Yiheng Xu, Jiabo Ye, Xi Zhang, Tianbao Xie, Zesen Cheng, Hang Zhang, Zhibo Yang, Haiyang Xu, and Junyang Lin. Qwen2.5-vl technical report, 2025. [4] Gemini Team et al. Gemini: family of highly capable multimodal models, 2024. [5] Yucheng Han, Chi Zhang, Xin Chen, Xu Yang, Zhibin Wang, Gang Yu, Bin Fu, and Hanwang Zhang. Chartllama: multimodal llm for chart understanding and generation, 2023. [6] Wei He, Zhiheng Xi, Wanxu Zhao, Xiaoran Fan, Yiwen Ding, Zifei Shan, Tao Gui, Qi Zhang, and Xuanjing Huang. Distill visual chart reasoning ability from llms to mllms, 2024. [7] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, Kai Dang, Yang Fan, Yichang Zhang, An Yang, Rui Men, Fei Huang, Bo Zheng, Yibo Miao, Shanghaoran Quan, Yunlong Feng, Xingzhang Ren, Xuancheng Ren, Jingren Zhou, and Junyang Lin. Qwen2.5-coder technical report, 2024. [8] Q. Huynh-Thu and M. Ghanbari. Scope of validity of psnr in image/video quality assessment. Electronics Letters, 44:800801, 2008. [9] Gyeong-Geon Lee, Ehsan Latif, Xuansheng Wu, Ninghao Liu, and Xiaoming Zhai. Applying large language models and chain-of-thought for automatic scoring, 2024. [10] Bingxuan Li, Yiwei Wang, Jiuxiang Gu, Kai-Wei Chang, and Nanyun Peng. Metal: multi-agent framework for chart generation with test-time scaling, 2025. [11] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. Mmc: Advancing multimodal chart understanding with large-scale instruction tuning, 2024. [12] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning, 2023. [13] Haoyu Lu, Wen Liu, Bo Zhang, Bingxuan Wang, Kai Dong, Bo Liu, Jingxiang Sun, Tongzheng Ren, Zhuoshu Li, Hao Yang, Yaofeng Sun, Chengqi Deng, Hanwei Xu, Zhenda Xie, and Chong Ruan. Deepseekvl: Towards real-world vision-language understanding, 2024. [14] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. Chartqa: benchmark for question answering about charts with visual and logical reasoning, 2022. [15] OpenAI. Gpt-4o system card, 2024. [16] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, and Ilya Sutskever. Learning transferable visual models from natural language supervision, 2021. [17] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution, 2024. 10 [18] Zhou. Wang, Alan Conrad. Bovik, Hamid Rahim. Sheikh, and Eero P. Simoncelli. Image quality assessment: from error visibility to structural similarity. IEEE transactions on image processing : publication of the IEEE Signal Processing Society, 13(4):60012, 2004. [19] Chengyue Wu, Yixiao Ge, Qiushan Guo, Jiahao Wang, Zhixuan Liang, Zeyu Lu, Ying Shan, and Ping Luo. Plot2code: comprehensive benchmark for evaluating multi-modal large language models in code generation from scientific plots, 2024. [20] Renqiu Xia, Bo Zhang, Hancheng Ye, Xiangchao Yan, Qi Liu, Hongbin Zhou, Zijun Chen, Peng Ye, Min Dou, Botian Shi, Junchi Yan, and Yu Qiao. Chartx & chartvlm: versatile benchmark and foundation model for complicated chart reasoning, 2025. [21] Zhiyuan Yan, Junyan Ye, Weijia Li, Zilong Huang, Shenghai Yuan, Xiangyang He, Kaiqing Lin, Jun He, Conghui He, and Li Yuan. Gpt-imgeval: comprehensive benchmark for diagnosing gpt4o in image generation, 2025. [22] Cheng Yang, Chufan Shi, Yaxin Liu, Bo Shui, Junjie Wang, Mohan Jing, Linran Xu, Xinyu Zhu, Siheng Li, Yuxiang Zhang, Gongye Liu, Xiaomei Nie, Deng Cai, and Yujiu Yang. Chartmimic: Evaluating lmms cross-modal reasoning capability via chart-to-code generation, 2025. [23] Zhihan Zhang, Yixin Cao, and Lizi Liao. Enhancing chart-to-code generation in multimodal large language models via iterative dual preference learning, 2025. [24] Xuanle Zhao, Xianzhen Luo, Qi Shi, Chi Chen, Shuo Wang, Wanxiang Che, Zhiyuan Liu, and Maosong Sun. Chartcoder: Advancing multimodal large language model for chart-to-code generation, 2025."
        },
        {
            "title": "Appendix",
            "content": "A Prompt Template In this appendix, we present the prompt templates used in our experiments. A.1 Prompt for Initial Code Generation The following prompt is used to guide the model in extracting structural and stylistic information from chart image. It asks the model to describe the charts key visual elements without generating any code. Please analyze the provided image, which was generated using Pythons matplotlib.pyplot. Your task is to identify and describe the key visual elements and details necessary to recreate similar figure. Focus on the following aspects: 1. The number and location of subplots: If the chart consists of multiple subplots, please describe the number of subplots. Meanwhile, specify the function (such as subplot(212), subplot(221), subplot(311), etc.) that should be used for each subplot to place it correctly in the figure. Otherwise, please answer \"The chart contains only one subplot.\" 2. Plot type: Identify the type of the plot, such as line plot, pie chart, scatter plot, bar chart, histogram, event plot, node plot, radar plot, area plot, box plot, heatmap, bubble chart, polar plot, violin plot, text plot, etc. If the chart does not fit any known chart type, please describe which matplotlib functions should be used to generate the visual elements. 3. Axes: Describe the labels, titles, and scales (e.g., linear, logarithmic) of the x-axis and y-axis. 4. Color: Describe the color schemes. If the color is fixed value, please specify the exact color or name; if the color is dynamically generated, please describe the generation logic, such as random generation, data mapping, algorithmic calculation, or the use of specific colormap. 5. Styles: According to the type of chart, describe the stylistic features that should be noted, such as orientation, width, size, and marker types. 6. Annotations and Texts: Mention any text annotations, labels, titles, or legends present in the image. 7. Grid and Background: Describe whether grid is present and any background elements. 8. Data Characteristics: If the data is generated randomly, specify the distribution used (e.g., normal, uniform, exponential, Poisson, binomial, geometric, gamma, beta, etc.). If the chart is generated from function, identify the function type (e.g., sine, exponential growth, mixed type, etc.). For periodic functions, also include the period and phase information. If the chart uses specific given values, estimate the approximate magnitude of the data. Note: If the chart contains multiple subplots, you must describe all the above information for each subplot. You do not need to generate any code. Once the description is obtained, the following prompt is used to generate the corresponding Python code based on the extracted details. Heres description of the image: <description>. According to the description, please generate the Python script used to draw this image. A.2 Prompt for Iterative Refinement In this stage, we aim to refine previously generated code by capturing the differences between two chart images. The following prompt is used to guide the model in identifying meaningful visual differences between two images. 12 Please compare the two provided images generated using Python code. Your task is to describe the differences between the first image and the second image. Focus on the following aspects: 1. Axes: Describe the differences in the labels, titles, and scales (e.g., linear, logarithmic) of the x-axis and y-axis between the two images. 2. Color: Describe the differences in the color schemes of the two images. 3. Styles: According to the type of chart, describe the differences in styles that should be noted in the charts, such as orientation, width, size, marker types, etc. 4. Annotations and Texts: Mention any differences in text annotations, labels, titles, or legends present in the two images. 5. Grid and Background: Describe any differences in the presence of grid and background elements between the two images. 6. Others: Include any other significant differences not covered by the above aspects. Note: If the charts contain multiple subplots, you need to describe the differences for each subplot. Given the visual differences and the original code, the following prompt is used to instruct the model to modify the existing code to match the second image. Here is the Python script used to generate the first image:<code> want to modify the script to draw the second image. The difference between the two images generated by the given code is: <difference> The description of the second image is: <description> Based on the differences and the description, please provide the Python code that can be used to generate the second image. A.3 Prompt for Evaluation The following prompt is used to instruct GPT-4o to assess the visual similarity between the ground truth image and the image generated from model-produced code. You are helpful assistant. Please evaluate the similarity between reference image created using matplotlib and an image generated by code provided by an AI assistant. Consider factors such as the overall appearance, colors, shapes, positions, and other visual elements of the images. Begin your evaluation by providing short explanation. Be as objective as possible. After providing your explanation, you must rate the response on scale of 1 to 10 by strictly following this format: \"Rating: The reference image is the first image and the generated image is the second image. [[5]]\" A.4 Prompt for ChartX Code Description Generation The following prompt is designed for instructing GPT-4o to analyze given Python script and generate detailed textual description of the resulting chart. This description focuses on the visual and stylistic elements of the figure, enabling its recreation without direct access to the code. 13 Please analyze the provided code, which used Pythons matplotlib.pyplot to generate an image. Your task is to identify and describe the key visual elements and details necessary to recreate similar figure. Focus on the following aspects: 1. Plot Type: Identify the type of plot (e.g., line plot, scatter plot, bar chart, histogram, etc.). 2. Axes: Describe the labels, titles, and scales (e.g., linear, logarithmic) of the x-axis and y-axis. 3. Data Representation: Summarize how the data is represented (e.g., points, lines, bars, colors, markers, etc.). 4. Colors and Styles: Note the color schemes, line styles, marker shapes, or other stylistic elements used. 5. Annotations and Texts: Mention any text annotations, labels, titles, or legends present in the image. 6. Grid and Background: Describe whether grid is present and any background elements. 7. Data Characteristics: If the data is visible, describe its general trend, distribution, or any notable patterns (e.g., clusters, peaks, trends). 8. Random Seed or Data Source: If the image involves randomness, extract the random seed or describe the data source for reproducibility. Use clear, everyday language to describe the key elements of the code. Your response should provide enough information for someone to redraw the figure without needing to see the original code.Here is the code:<code> Detailed training settings We trained Qwen2.5-VL for generating descriptions on 4 A100 GPUs for approximately one day. We include the detailed hyperparameters for training Qwen2.5-VL in Table 4. Table 4: Training hyperparameters Epochs Lora_rank ViT freeze Learning rate Warmup_ratio Numerical precision Scheduler 30 64 yes 5.0e 0.03 bfloat16 cosine Broader Impact This work investigates the task of chart-to-code generation, which aims to translate visual chart representations into their corresponding code. Such technology holds promise for enhancing accessibility and efficiency in data analysis workflows. For example, it can support users with limited programming expertise in reproducing and modifying visualizations, and enable the reverse engineering of figures from educational or scientific publications. Nonetheless, the automatic generation process may introduce errors or misleading outputs, particularly when the source charts contain design flaws or follow non-standard conventions."
        }
    ],
    "affiliations": [
        "Lehigh University",
        "MIFA Lab, Shanghai Jiao Tong University",
        "Shanghai Innovation Institute",
        "State Key Laboratory of General Artificial Intelligence, BIGAI"
    ]
}