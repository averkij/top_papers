{
    "paper_title": "Think on your Feet: Adaptive Thinking via Reinforcement Learning for Social Agents",
    "authors": [
        "Minzheng Wang",
        "Yongbin Li",
        "Haobo Wang",
        "Xinghua Zhang",
        "Nan Xu",
        "Bingli Wu",
        "Fei Huang",
        "Haiyang Yu",
        "Wenji Mao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, a capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{L}$earning ($\\textbf{AML}$) that strategically selects from four thinking modes (intuitive reaction $\\rightarrow$ deep contemplation) based on real-time context. Our framework's core innovation, the $\\textbf{A}$daptive $\\textbf{M}$ode $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{AMPO}$) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPO's fixed-depth approach"
        },
        {
            "title": "Start",
            "content": "5 2 0 2 4 ] . [ 1 6 5 1 2 0 . 5 0 5 2 : r a"
        },
        {
            "title": "Preprint",
            "content": "THINK ON YOUR FEET: ADAPTIVE THINKING VIA REINFORCEMENT LEARNING FOR SOCIAL AGENTS Minzheng Wang1,2 Yongbin Li3 Haobo Wang4 Xinghua Zhang3 Nan Xu1 Bingli Wu3 Fei Huang3 Haiyang Yu3 Wenji Mao1,2 1 MAIS, Institute of Automation, Chinese Academy of Sciences 2 School of Artificial Intelligence, University of Chinese Academy of Sciences 3 Tongyi Lab, Alibaba Group 4 Peking University {wangminzheng2023, wenji.mao}@ia.ac.cn, {shuide.lyb, zhangxinghua.zxh}@alibaba-inc.com"
        },
        {
            "title": "ABSTRACT",
            "content": "Effective social intelligence simulation requires language agents to dynamically adjust reasoning depth, capability notably absent in current approaches. While existing methods either lack this kind of reasoning capability or enforce uniform long chain-of-thought reasoning across all scenarios, resulting in excessive token usage and inappropriate social simulation. In this paper, we propose Adaptive Mode Learning (AML) that strategically selects from four thinking modes (intuitive reaction deep contemplation) based on real-time context. Our frameworks core innovation, the Adaptive Mode Policy Optimization (AMPO) algorithm, introduces three key advancements over existing methods: (1) Multi-granular thinking mode design, (2) Context-aware mode switching across social interaction, and (3) Token-efficient reasoning via depth-adaptive processing. Extensive experiments on social intelligence tasks confirm that AML achieves 15.6% higher task performance than state-of-the-art methods. Notably, our method outperforms GRPO by 7.0% with 32.8% shorter reasoning chains. These results demonstrate that context-sensitive thinking mode selection, as implemented in AMPO, enables more human-like adaptive reasoning than GRPOs fixed-depth approach1."
        },
        {
            "title": "INTRODUCTION",
            "content": "Large Language Models (LLMs) have demonstrated exceptional reasoning capabilities in handling static problems with well-defined rules and deterministic answers, such as mathematics, code, and logical reasoning (Yang et al., 2024; DeepMind, 2024; Jaech et al., 2024; Guo et al., 2025; OpenAI, 2025). However, there exists notable gap between these capabilities and the complexity of reallife scenarios, especially in social scenarios involving interest conflicts and business negotiations characterized by uncertainty and long-term goals. LLM-based social agents offer new possibilities for exploring such dynamic and variable social environments by simulating human behaviors (Zhou et al., 2024; Wang et al., 2024c). These environments require sophisticated strategic reasoning capabilities from social agents, which not only coordinate long-term goals but also handle uncertainty and adapt to rapidly changing situations. LLMs still face challenges in replicating the nuanced decision-making abilities exhibited in human social dynamic interactions (Zhou et al., 2024; Zhang et al., 2024; Liu et al., 2025). Recent research efforts in social intelligence have primarily followed two pathways: (1) End-to-end goal-oriented training, which involves LLM post-training through imitation learning or reinforcement learning (Wang et al., 2024c; Zhang et al., 2025), and (2) External planning integration, which enhances strategic reasoning through plug-and-play planners (Deng et al., 2024; Li et al., 2024; Liu et al., 2025). However, these methods predominantly focus on the fast-thinking paradigm without incorporating sufficient thinking processes. In that case, does fast thinking sufficiently support social Corresponding authors. 1The code and data are available at https://github.com/MozerWang/AMPO"
        },
        {
            "title": "Preprint",
            "content": "Figure 1: Comparison with other methods: (a) Existing Thoughtless Social Agent: fast-thinking inference, (b) Proposed Thoughtful Social Agent: Adaptive Long-Cot Reasoning. agents in complex environments? Existing research has shown that individuals in dynamic social interactions typically engage in appropriate thinking process before responding, rather than relying solely on intuitive, non-thoughtful reactions (Evans, 1996; Krull & Dill, 1996). Therefore, the current fast-thinking paradigm does not adequately address these cognitive requirements for meaningful social interactions (As shown in Figure 1). Although static reasoning problems such as mathematics and code have shifted their technological paradigm to long chains of thought (CoT), this test-time scaling has not yet been explored in social intelligence. How would it work if we applied current large reasoning models (LRMs) to social agents? Existing LRMs have demonstrated impressive capabilities with Long-CoT across various reasoning tasks (Team, 2024; Jaech et al., 2024; DeepMind, 2024; Guo et al., 2025; OpenAI, 2025), but most of them employ exhaustive reasoning regardless of the input complexity. However, the dynamic nature of the social environment poses significant challenges to these models. As not all interactions necessitate deep thinking (Thorngate, 1976), exhaustive thinking not only leads to unnecessary token usage but may also degrade performance as consequence of overthinking. Therefore, there is an urgent need for tailor-designed reasoning on social agents to cope with dynamic social environments. In this paper, we propose the Adaptive Mode Learning framework (AML) to empower social agents with the capability for adaptive thinking, enabling them to effectively navigate and respond to the dynamics of social interaction context. Firstly, we develop four thinking modes inspired by hierarchical cognitive control theory (Koechlin & Summerfield, 2007; Badre, 2008), covering spectrum from non-thought, through shallow thinking, to increasingly profound levels of contemplation. Next, we perform the injection of thinking modes, which consists of behavioral cloning for learning basic modes, and RL-based adaptive thinking mode enhancement. For RL-based enhancement, we contrapuntally develop the Adaptive Mode Policy Optimization (AMPO) algorithm which incorporates the mode-level and sample-level information into advantage estimation to strengthen the context-aware thinking mode switching for adaptive thinking ability. In terms of reward, we design three types of reward functions, including answer reward, format reward, and answer length reward, providing feedback for choosing the appropriate thinking mode and answer. AML finally achieves state-of-the-art performance in comparison with current open-source and closed-source advanced LLMs. The main contributions of this paper are summarized as follows: We propose the adaptive mode learning framework (AML) which is the first effective and adaptive Long-CoT reasoning method for social agents via pre-defined thinking modes and tailor-designed reinforcement learning algorithm. We develop the adaptive mode policy optimization (AMPO) algorithm which sufficiently takes the thinking mode-level and sample-level information into account. Most notably, AMPO empowers social agents with adaptive thinking ability by dynamically switching the"
        },
        {
            "title": "Preprint",
            "content": "thinking mode to cope with the contextual dynamics, ensuring both effectiveness and token efficiency. Extensive experiments confirm the significant advantages of AML, which shows an improvement of up to 15.6% over previous SOTAs. In addition, compared to GRPO, AMPO leads to 32.8% decrease in token utilization, accompanied by 7.0% rise in performance."
        },
        {
            "title": "2.1 GROUP RELATIVE POLICY OPTIMIZATION(GRPO)",
            "content": "To optimize memory usage, GRPO (Shao et al., 2024) obviates the need for additional value function approximation and instead uses the average reward of multiple sampled outputs, produced in response to the same question, as the baseline. The GRPO optimizes the policy model by maximizing the following objective: JGRPO(θ) = qP (Q),{oi}G i=1πθold (oq) (cid:40)"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 1 oi oi (cid:88) (cid:110) (cid:104) min t=1 ri,t(θ)Ai,t, clip(cid:0)ri,t(θ), 1 ϵ, 1 + ϵ(cid:1)Ai,t (cid:105) βDKL [πθπref ] (cid:111) (cid:41) . (1) where ϵ and β are hyper-parameters, and Ai,t is the advantage calculated based on relative rewards of the outputs inside each group only: Ai,t = ri mean({r1, r2, . . . , rG}) std({r1, r2, . . . , rG}) . (2) where the ratio ri,t(θ) represents the probability ratio or importance sampling weight between the new policy πθ and the old policy πθold: ri,t(θ) = πθ(oi,tq, oi,<t) πθold (oi,tq, oi,<t) , and GRPO estimates the KL divergence with the following unbiased estimator: DKL [πθπref ] = πref (oi,tq, oi,<t) πθ(oi,tq, oi,<t) log πref (oi,tq, oi,<t) πθ(oi,tq, oi,<t) 1. 2.2 SOTOPIA ENVIRONMENT (3) (4) SOTOPIA (Zhou et al., 2024) serves as our platform for goal-driven social learning, providing an interactive, open-ended, and realistic simulation environment that enables precise assessment of social agents intelligence. This platform features diverse scenarios and social goals, simulating wide range of social interactions, including negotiation, collaboration, and competition. Within SOTOPIA, each social task consists of specific scenario featuring two distinct role profiles, each with private social goals that must be achieved through dialogue interaction. Two LLMs engage as role-playing social agents πθ1 and πθ2 for each task and take turns acting. Here, yi πθ(s, p, g, hi) represents the response generated by the social agent in turn according to scenario s, persona p, its goal g, and the interaction history hi. More details are provided in Appendix B."
        },
        {
            "title": "3 ADAPTIVE MODE LEARNING FRAMEWORK",
            "content": "3.1 THINKING MODE Hierarchical Cognitive Control Theory (HCCT) (Koechlin & Summerfield, 2007; Badre, 2008) provides theoretical framework to understand human cognitive behavior. It posits that cognitive control operates through four distinct hierarchical levels, managing goals and actions at varying degrees of abstraction and temporal scales. Motivated by HCCT, we propose four levels of thinking"
        },
        {
            "title": "Preprint",
            "content": "modes for different dialogue scenarios as shown in Figure 5 and Figure 6, spanning range from intuitive response, through superficial thinking, to progressively deeper levels of contemplation. The detailed mapping between our thinking modes and HCCTs four hierarchical levels is provided in Appendix E. For each thinking mode, we design specific and suitable actions aligned with linguistic principles: (1) Mode-1 M1 (Intuitive Response): This mode with only the final answer does not contain any thinking actions, which is characterized by automatic responses based on learned associations and basic linguistic modes (Sacks et al., 1974; Norman & Shallice, 1985). (2) Mode-2 M2 (Intentional Analysis): This mode encompasses sequence of thinking actions: Intent, Style, and Response. Intent action aims to analyze the other partys intentions and goals (Grice, 1975). Style ensures consistency in the speaking style of the social agent within the context (Clark, 1996). Response action gives the preliminary answer. Overall, P2 is kind of superficial thinking, designed for handling relatively simple environments. (3) Mode-3 M3 (Strategic Adaptation): Compared with M2, M3 additionally introduces three thinking actions: History, Goal, Assess and Strategy. History aims to analyze the dialogue history for better context understanding (Schiffrin, 1987). Goal action clarifies the agents goal (Grosz & Sidner, 1986). Assess action analyze goal alignment, round criticality, and improvement potential between parties. (Brown, 1987). Strategy action enables the agent to propose suitable strategy to respond to the present social context (Clark & Brennan, 1991). This mode makes the agent adopt the strategy to go into deeper level of thought. (4) Mode-4 M4 (Prospective Simulation): M4 further introduces Deduction and Integration thinking actions compared to M3. In this mode, Strategy encourages the proposal of multiple strategies (Clark & Brennan, 1991), then simulating the execution of these strategies through Deduction action (Schank & Abelson, 2013). Integration action aggregates the results of Deduction for the preliminary answer. M4 facilitates the simulation of various situations to promote deeper thinking, effectively responding to more complex social contexts (Searle, 1969). 3.2 ADAPTIVE MODE POLICY OPTIMIZATION (AMPO) In the GRPO advantage computation (Shao et al., 2024), each rollout sample is treated as an isolated thinking trajectory, which overlooks the similarities in the inherent thinking mode between the trajectories and fails to learn the information at the level of thinking modes. This limitation will make it challenging for the model to adapt its thinking modes on the fly in accordance with social contexts, thus compromising its reasoning efficiency and effectiveness. The subsequent experiments also reveal that GRPO-based RL tends to guide the model to converge to the most complex thinking trajectories regardless of the contexts. To this end, we propose the Adaptive Mode Policy Optimization (AMPO) algorithm, which introduces both mode-level and sample-level information into the advantage estimation to promote adaptive thinking learning. Through mastering mode-level information, the model learns to select the most appropriate modes; while through sample-level learning, model can generate higher-quality action content within each mode. Formally, AMPO defines the objective as: JAMPO(θ) = qP (Q),{oi}G i=1πθold (oq) (cid:40) 1 (cid:88) i=1 1 oi oi (cid:88) (cid:110) (cid:104) min t=1 ri,t(θ)(AM i,t + AS i,t), clip(cid:0)ri,t(θ), 1 ϵ, 1 + ϵ(cid:1)(AM i,t + AS i,t) (cid:105) βDKL [πθπref ] (cid:41) (cid:111) . (5) where AM i,t represents mode-level advantage and AS i,t denotes sample-level advantage."
        },
        {
            "title": "Preprint",
            "content": "Figure 2: Demonstration of our AMPO, which introduces both mode and sample level advantage."
        },
        {
            "title": "3.2.1 ADVANTAGE ESTIMATION",
            "content": "The detailed mode-level advantage computation is as follows: AM i,t = rp(i) mean({r1, r2, . . . , rN }) std({r1, r2, . . . , rN }) tanh( lp(i) mean({l1, l2, . . . , lN }) std({l1, l2, . . . , lN }) if i, [1, G] : ri = rj (6) ) if i, [1, G] : ri = rj where denotes the total number of thinking modes, and represents the total number of rollout samples, ri denotes the reward value of i-th sample in the rollout group, m(i) represents the k-th thinking mode corresponding to i-th sample. k, m(i) [1, ], [1, G]. To compute mode-level information from rollout samples, we consider two key dimensions for each mode: the average reward rk and the average output token length lk, defined as: 1 Mk rj , lk = 1 Mk rk = (cid:88) (cid:88) lj (7) oj Mk oj Mk where Mk represents the rollout sample set of the k-th thinking mode, rj and lj respectively denote the reward value and token length of the j-th sample which belongs to Mk in the rollout group. oj {o1, o2, .., oG} where {o1, o2, .., oG} is group of outputs sampled from the old policy πθold . The introduction of mode-level advantage enables the model to adaptively select appropriate thinking modes across different scenarios. This adaptability manifests as dynamic trade-off between reasoning length and task performance in interactive contexts. The models selection mechanism operates under two key conditions: When performance is comparable across modes, if i, [1, G] : ri = rj, the model prioritizes efficiency by selecting modes with minimal token length lk; When performance varies between modes, if i, [1, G] : ri = rj, the model identifies optimal approaches through average rewards rk. This mechanism ensures the models ability to dynamically adjust its reasoning approach according to contextual requirements while maintaining efficiency. To mitigate training instability arising from significant length variations across thinking modes, we implement tanh activation function, which maps advantage values to the [1, 1] interval, effectively reducing disparities between original advantage values and enhancing training stability. The sample-level advantage computation is as follows: AS i,t = ri mean({r1, r2, . . . , rG}) std({r1, r2, . . . , rG}) . (8) 3.2.2 REWARDS The design of reward functions is the key to reinforcement learning. Our reward function consists of three components: answer reward ra i. The reward ri is computed as follows: , and answer length reward rl , format reward rf ri = ra rl i, if format is correct rf , if format is incorrect (9)"
        },
        {
            "title": "Preprint",
            "content": "Answer Reward. The answer reward evaluates how well the response improves the completion of the goal. Following recent work (Deng et al., 2024; He et al., 2024; Liu et al., 2025), we implement robust LLM evaluator rϕ() to assess the progress of goal completion at each turn of dialogue. The evaluator assigns score in the range [0, 10], where 0 indicates no progress and 10 represents complete achievement of the goal. For each answer ai, the reward is computed based on the gradient gi (difference) between the goal completion scores before and after the response. To ensure training stability, we design boundary-aware reward scaling function that dynamically adjusts the magnitude of reward gradients based on the distance from the current state to the boundaries while mapping the scaled gradients to the [0, 1] interval through linear transformation: ra = ˆgi + 1 2 , ˆgi = gi 10 st gi st , , if gi 0 if gi < 0 (10) where ˆgi [1, 1] is boundary-aware scaling function. gi = rϕ(st, ai) st is the raw gradient, st is the goal completion score before response at turn t, rϕ(st, ai) is the score after response ai. Format Reward. To ensure the model follows our different thinking modes, we introduce the format reward that penalizes the behaviors that deviate from the template. The model needs to output its thinking process within the <begin of thinking> tags and the final answer within the <begin of answer> tags. In addition, we constrain different thinking actions within each thinking mode. Specifically, each tag and action must appear exactly once and maintain the correct chain of actions. Through these format constraints, we can ensure that the model strictly follows the pre-designed thinking mode. We implement the format compliance reward using binary approach, only penalizing behaviors that dont follow the template. If the format is not followed, rf = 2; Otherwise, rf is discarded. Answer Length Reward. To control the length of answers, we introduce length penalty mechanism. In our early reward design, we observe that the LLM generates lengthy responses without achieving actual strategic improvements. Moreover, excessive response lengths lead to the accumulation of dialogue history in multi-turn conversations, significantly increasing computational costs. To this end, we develop smooth length penalty function that normalizes the deviation between actual and target answer lengths: rl = clip(α δi, 1, 1) + 1 2 (11) lt where δ = la represents the difference between actual length la (in tokens) of answer ai, and α > 0 is scaling factor that controls the penalty sensitivity. The rl [0, 1] penalizes answers that deviate from the target length, with longer deviations incurring greater penalties. This mechanism effectively encourages the model to generate concise, focused answers. and target length lt 3.3 OPTIMIZATION PROCEDURE We employ two-phase training strategy: The first phase utilizes behavioral cloning to enable the model to understand and follow specific thinking modes accurately. In the second phase, we perform reinforcement learning training to enhance the adaptive thinking mode, further optimizing the models performance. The full optimization procedure is shown in Algorithm 1. 3.3.1 BEHAVIORAL CLONING To enhance the models capability to adhere to four thinking modes, we initially employ Behavioral Cloning (Bain & Sammut, 1999; Ross & Bagnell, 2010) to fine-tune the model. Behavioral cloning is an effective imitation learning method widely used in developing LLM-based agents (Guo et al., 2024; Wang et al., 2024b;c). In this paper, by using four pre-defined thinking modes, we employ the expert model to collect training data through self-chat interactions in the SOTOPIA-π (Wang et al., 2024c) training environment. Based on the generated data, we fine-tune the LLM to serve as the foundation for subsequent reinforcement learning."
        },
        {
            "title": "Preprint",
            "content": "Sample batch Db from Dbc Compute BC loss: LBC = E(x,y)Db [log πθ(yx)] Update the policy model πθ by minimizing LBC Algorithm 1 Adaptive Mode Policy Optimization Input initial policy model πθinit ; reward models rϕ; training data for BC Dbc;task prompts for RL Drl; BC training epochs E; RL training steps ; 1: // Phase 1: Behavior Cloning 2: Policy model πθ πθinit 3: for epoch = 1, . . . , do 4: 5: 6: 7: end for 8: // Phase 2: Reinforcement Learning 9: Reference model πref πθ 10: for step = 1, . . . , do 11: 12: 13: 14: 15: 16: 17: 18: end for Output πθ Sample batch Db from Drl Update the old policy model πθold πθ Sample outputs {oi}G Compute sample-level rewards {ri}G Compute mode-level information {ri}N Compute mode-level AM Update the policy model πθ by maximizing the AMPO objective (Equation 5) i=1 for each sampled output oi by running rϕ i=1 πθold ( q) for each input Db i,t and sample-level AS i=1 for each mode set Mi i,t for the t-th token of oi i=1 and {li}N 3.3.2 REINFORCEMENT LEARNING We adopt single-turn optimization approach to enhance model performance in multi-turn social interaction tasks. Specifically, we decompose multi-turn dialogues into multiple single-turn inputoutput tasks, where the input represents the current dialogue state and the output is the corresponding response. To ensure the stability of reinforcement learning, we need to collect sufficiently diverse single-turn interaction data that covers as many scenarios as possible, including various difficulty levels, different dialogue objectives, and various dialogue turn states. In practice, we first employ BC model to collect multi-turn interaction data in the SOTOPIA-π training environment through self-chat interactions. Subsequently, we utilize LLM-judge to score each dialogue turn and decompose the multi-turn dialogues into single-turn data. By evaluating the completion status of current dialogue objectives, we categorize scenarios by difficulty levels, thereby assembling comprehensive interaction dataset. Following data construction, we conduct reinforcement learning training using GRPO and AMPO algorithms. Detailed data construction method and training hyper-parameters are provided in the Appendix A."
        },
        {
            "title": "4 EXPERIMENTS",
            "content": "4.1 EXPERIMENTAL SETTINGS Datasets. We evaluate AMPOs performance in social interactions using SOTOPIA and SOTOPIAHard. SOTOPIA focuses on varying goal-oriented social interactions, while SOTOPIA-Hard challenges agents with complex strategic reasoning tasks. All training episodes are collected from SOTOPIA-π, using scenarios entirely separate from the test environment. Baselines. We evaluate our method, implemented on both Qwen and Llama backbones, against several baseline approaches: (1) LLMs, including powerful fast-thinking LLMs such as GPT4o (Hurst et al., 2024), Claude-3.5-Sonnet (Anthropic, 2024), and DeepSeeK-V3 (Liu et al., 2024); (2) LRMs, including models specialized in deep reasoning, such as OpenAI-o1 (Jaech et al., 2024), OpenAI-o3-mini (OpenAI, 2025), DeepSeek-R1 (Guo et al., 2025), QwQ-32B (Team, 2024), and Gemini-2.0-flash-thinking (DeepMind, 2024); (3) Dialogue Planning methods, including: (a) PPDPP (Deng et al., 2024), which utilizes an RL-trained policy planner to predict predefined strategies for assisting LLM reasoning; (b) EPO (Liu et al., 2025), which employs an RL-trained strategy reasoning LLM to generate strategies in an open-ended action space; (c) DAT (Li et al., 2024), which uses an RL-trained planner to predict continuous action vectors for controlling LLM outputs; (d) DSI (Zhang et al., 2025), which enhances LLMs social capabilities through Dynamic"
        },
        {
            "title": "Preprint",
            "content": "Models LLMs GPT-4o Claude-3.5-Sonnet DeepSeek-V3 LRMs OpenAI-o1 OpenAI-o3-mini DeepSeek-R1 QwQ-32B Gemini-2.0-flash-thinking Qwen Backbone Qwen2.5-7B-Instruct w/ PPDPP (Deng et al., 2024) w/ EPO (Liu et al., 2025) w/ DAT (Li et al., 2024) w/ DSI (Zhang et al., 2025) w/ BC w/ GRPO w/ AMPO(ours) Llama Backbone Llama3.1-8B-Instruct w/ PPDPP (Deng et al., 2024) w/ EPO (Liu et al., 2025) w/ DAT (Li et al., 2024) w/ DSI (Zhang et al., 2025) w/ BC w/ GRPO w/ AMPO(ours) Self-Play GPT-4o-as-Partner SOTOPIA SOTOPIA-Hard SOTOPIA SOTOPIA-Hard GOAL OVERALL GOAL OVERALL GOAL OVERALL GOAL OVERALL 8.19 8.29 8.15 7.93 7.38 7.97 7.70 7.60 7.91 7.97 8.09 7.97 8.35 8.22 8.87 8.95 6.99 7.20 7.94 7.34 8.08 8.43 8.86 9. 3.76 3.71 3.62 3.58 3.30 3.40 3.30 3.35 3.55 3.65 3.51 3.59 3.75 3.67 3.85 3.87 3.23 3.40 3.78 3.44 3.64 3.76 3.84 3.95 6.97 6.33 6.34 5.69 5.14 5.86 5.35 6. 6.21 6.63 6.82 6.39 7.31 7.14 7.44 7.85 5.11 5.34 6.79 5.89 6.93 7.21 7.59 8.06 3.46 3.09 3.09 2.71 2.36 2.73 2.41 2.78 3.01 3.31 3.12 3.10 3.51 3.47 3.41 3.54 2.36 2.57 3.27 2.85 3.31 3.50 3.44 3. 8.19 8.42 8.14 8.09 7.96 7.92 7.80 7.82 6.71 8.07 8.41 8.11 8.15 8.25 8.52 8.60 7.68 7.81 8.37 7.78 8.08 8.29 8.63 8.75 3.76 3.77 3.72 3.69 3.61 3.49 3.47 3. 3.13 3.71 3.86 3.70 3.70 3.80 3.92 3.94 3.62 3.66 3.83 3.65 3.67 3.80 3.92 3.98 6.97 6.64 6.69 6.65 6.33 6.20 6.19 6.81 5.90 6.76 6.81 6.78 6.87 7.15 7.20 7.50 6.21 6.30 6.91 6.18 6.84 7.14 7.30 7. 3.46 3.30 3.31 3.20 2.98 2.95 2.91 3.27 2.90 3.35 3.51 3.36 3.42 3.50 3.50 3.65 3.05 3.10 3.53 3.03 3.41 3.52 3.54 3.74 Table 1: Main results on SOTOPIA and SOTOPIA-Hard cross the Goal and Overall dimensions. The highest score among different LLMs is highlighted in bold. Our report results are the averages of the four evaluation outcomes (statistically significant with < 0.05). Strategy Injection learning; (e) BC, behavioral Cloning fine-tunes LLMs on expert thinking modes data, which serves as our initial model for reinforcement learning, capable of reasoning according to predefined thinking modes; and (f) GRPO, which applies reinforcement learning algorithms to continuously improve the BC models performance. For detailed baseline implementations, please refer to Appendix C. Evaluation. SOTOPIA and SOTOPIA-Hard evaluate social capabilities across seven dimensions, with the GOAL score (ranging from 0 to 10) measuring how effectively social agent achieves its goal. Following established research practices (Zheng et al., 2023; Wang et al., 2024a; Liu et al., 2025), we use GPT-4o as proxy for human judgment to assess both GOAL scores and overall performance (calculated as the mean of all seven dimensions), as studies have validated its high correlation with human evaluations (Zhou et al., 2024; Wang et al., 2024c). We set the temperature of the agents to 0.7 to encourage diversity of responses, and the temperature of the evaluator to 0 to ensure the stability of the evaluation. We conduct evaluations under two scenarios: self-play, where the social agent interacts with itself, and GPT-4o-as-Partner, where the agent interacts with GPT-4o. For detailed evaluation settings, please refer to Appendix B. 4.2 RESULTS AND ANALYSES RQ1: Is Reasoning with Long-CoT Effective for Social Agents? As shown in Table 1, our research demonstrates that through reinforcement learning training, whether utilizing GRPO or AMPO, the Long-CoT reasoning capabilities are significantly enhanced, surpassing state-of-the-art performance. This validates the effectiveness of our AML framework in applying Long-CoT reasoning to social intelligence, representing the first substantial breakthrough in this field. Current LRMs show suboptimal performance in social dialogue scenarios, sometimes even underperforming smaller, fast-thinking models. Additionally, it consumed large number of tokens, with QwQ-32B averaging 973 tokens. This is particularly puzzling, given their excellence in tasks that require rigorous logical reasoning, such as mathematics. Through detailed analysis of model outputs,"
        },
        {
            "title": "Preprint",
            "content": "(a) Qwen2.5-7B-Instruct. (b) Llama3.1-8B-Instruct Figure 3: Comparison of AMPO and GRPO on different LLM backbone in terms of training dynamic and evaluation results. we found that their strong mathematical and logical reasoning capabilities fail to effectively transfer to multi-turn social interactions, manifesting in several key aspects: (1) Lack of Systematic Historical Analysis: Models tend to focus solely on the most recent dialogue turn, failing to effectively synthesize established consensus and disagreements, while also lacking tracking of previous commitments and concessions. (2) Insufficient Goal Decomposition and Strategy Planning: Models often proceed directly to responses without generating and comparing multiple options or evaluating how different choices contribute to achieving goals. (3) Inefficient Thinking Process: Models frequently become trapped in ineffective circular thinking modes, such as Let me think... Wait, but..., unable to advance decision-making effectively. (4) Poor Logical Structure: The thinking process randomly jumps between different levels without clear reasoning steps. (5) Imbalanced Focus: Excessive attention to surface-level social details while deviating from core dialogue goals. To address these challenges, we design thinking modes and actions aligned with social cognition, based on Hierarchical Cognitive Control theory and linguistic principles, ensuring the model generates appropriate Long-CoT reasoning trajectories. RQ2: How AMPO benefits Adaptive Hybrid Thinking? The experimental results presented in Table 1, Figure 3a, and Figure 3b demonstrate AMPOs superior performance compared to baseline methods, validating the effectiveness of our adaptive mode policy optimization approach through reinforcement learning. Specifically, as shown in Table 1, AMPO achieves SOTA performance across both backbone LLMs, outperforming numerous LLMs with larger parameters and dialogue planning methods by large margin. For Llama backbone, AMPO achieves an performance improvement of 15.6% on GOAL of SOTOPIA-Hard compared to previous SOTA (GPT-4o) in self-play setting. Besides, the behavior cloning (BC) fine-tuned LLM also exhibits promising results, indicating the efficacy of our four designed thinking modes, as it surpasses most baselines through supervised fine-tuning alone. Through post-training with reinforcement learning, whether utilizing GRPO or AMPO, the models reasoning capabilities are further enhanced, exceeding SOTA performance. This improvement validates the effectiveness of our reward design."
        },
        {
            "title": "Preprint",
            "content": "Models SOTOPIA SOTOPIA-Hard GOAL OVERALL GOAL OVERALL Avg Tokens Qwen2.5-7B-instruct w/ Mode-1 w/ Mode-2 w/ Mode-3 w/ Mode-4 w/ no-mode thinking w/ AMPO (ours) 8.55 8.71 8.81 8.86 8.88 8.95 3.79 3.42 3.60 3.80 3.76 3.87 7.08 7.28 7.43 7.62 7.32 7.85 3.40 2.80 3.12 3.31 3.16 3.54 101 572 736 972 866 647 Table 2: Ablation study of AMPO. The highest score among different LLMs is highlighted in bold, and the second highest is underlined. Our report results are the averages of the four evaluation outcomes (statistically significant with < 0.05). Baseline Analysis. For fast-thinking LLMs, their extensive parameter scale enables superior conversational expressiveness compared to smaller models, though they still demonstrate limitations in strategic applications. Concerning dialogue planning methods, while external planners can provide appropriate strategic guidance, this plug-in approach falls short of enhancing the inherent general planning capabilities of LLMs. Using strategy prompts alone does not substantially improve the models ability to comprehend, apply strategies, and generate appropriate responses, indicating clear performance limitations in this methodology. Compared to GRPO. As shown in Figure 3a and Figure 3b, AMPO exhibits significantly lower mean response lengths compared to GRPO while achieving superior performance in the GOAL dimension on both SOTOPIA and SOTOPIA-Hard. For Llama Backbone, the inference token of AMPO (581) is only 67.2% of GRPO (865), but its performance is better than GRPO by 7.0% on OVERALL of SOTOPIA-Hard. Our approach not only delivers better performance but also demonstrates higher efficiency through reduced token consumption. Furthermore, during the training process, while GRPO tends to converge to single thinking mode, manifested by sharp increase in Mode-4 and the eventual convergence of the other three modes to zero, AMPO effectively explores various thinking modes, maintaining continuous exploration across different cognitive modes. It demonstrates awareness of diverse social scenarios and adaptively switches between thinking modes rather than merely fitting into the most complex mode, highlighting the advantages of our algorithmic design. RQ3: How do Thinking Modes work? To investigate the effectiveness of single thinking mode training and the feasibility of allowing models to freely explore thinking modes, we conducted series of ablation studies. Specifically, we perform behavior cloning and reinforcement learning training using four predefined thinking modes independently, while also including control group without any thinking mode. The experimental results are presented in Table 2. Obviously, the AMPO achieved optimal performance with fewer tokens. The fact that training with any single mode failed to match AMPOs performance indicates that the effectiveness of our approach does not stem from fitting to single thinking mode. Specifically, while Mode-1 and Mode-2 consume fewer tokens, they lack the necessary strategic reasoning capabilities when facing complex dialogue scenarios, limitation that is particularly evident in the SOTOPIA-Hard. Conversely, although Mode-3 and Mode-4 possess stronger reasoning capabilities, their indiscriminate application of complex thinking modes across all scenarios results in substantial unnecessary token consumption. Under the no-mode thinking constraint setting, while the model demonstrates certain effectiveness in the GOAL dimension of SOTOPIA, its performance falls short of Mode-4 in other evaluation metrics. Moreover, compared to the other three modes and AMPO, it exhibits significantly higher token consumption. This approach presents two significant limitations: first, the thinking length gradually increases during training, leading to inefficient token utilization; second, the absence of an effective control mechanism prevents the model from clearly distinguishing and identifying different reasoning pathways. These limitations impede the models ability to explicitly differentiate thinking modes across various scenarios during training, while also hampering its capacity for adaptive regulation of thinking modes during inference. Our AMPO is designed with four predetermined thinking modes of varying complexity, incorporating reward-penalty mechanism during training to guide the models ability to recognize scenarios."
        },
        {
            "title": "Preprint",
            "content": "Figure 4: Modes Distribution of AMPO and GRPO. This approach enables the model to select appropriate thinking modes based on specific scenarios, ensuring reasoning quality while avoiding unnecessary computational overhead. Experimental results demonstrate that AMPO-based hybrid training achieves multi-path optimization, striking an optimal balance between token budget and practical effectiveness. RQ4: How do Adaptive Behaviors Manifest in AMPO? To explore the adaptive capabilities of the AMPO algorithm, we conduct detailed analysis of thinking mode distributions across two dimensions: varying turns and different scenarios. The results are shown in Figure 4. Obviously, AMPO shows clear distribution trend, while GRPO overfits to the most complex template. Next, we will analyze the distribution of AMPOs thinking modes in detail. Turn Distribution Analysis. We first analyze the turn distribution of thinking modes. As shown in Figure 4, the evolution of thinking modes demonstrate significant dynamic characteristics throughout the dialogue process: Mode-4, the most complex, account for over 53% in the initial stages (turns 1-4), reflecting high demand for strategic thinking and comprehensive planning, before declining substantially. Conversely, Mode-1 increases significantly in the later stages (turns 14-20), approaching 50% in the final seven rounds, indicating that as dialogue progressed and consensus is established, simpler thinking modes became more appropriate. Although Mode-3 shows relatively uniform distribution overall, they exhibited decreasing trend (approximately 31% in the first five turns versus 21% in the last five turns), suggesting the sustained importance of medium-high complexity thinking throughout the dialogue. Mode-2 maintains stable and relatively high usage frequency in the middle and later stages (turn 9-20), possibly reflecting the transition to specific implementation phases. This progressive transition from complex to simple thinking modes not only aligns with general problem-solving principles but also reveals the evolutionary pathway of optimal dialogue strategy: investing in higher-level thinking during initial stages to establish frameworks, employing mediumcomplexity thinking in middle stages to advance specific objectives, and shifting to simpler thinking modes in later stages to enhance execution efficiency. This further demonstrates AMPOs adaptive capabilities. Scenarios Distribution Analysis. To analyze the distribution of thinking modes across different scenarios, we employed the same methodology used in our RL data collection, where an LLM judge evaluated dialogue goal completion for each turn. We classified goal completion outcomes into four distinct scenarios: neither party achieving their goals (N-N), our side achieving while the other side failing (Y-N), our side failing while the other side achieving (N-Y), and both parties achieving their goals (Y-Y). The analysis reveals that simpler thinking modes (Mode-1 and Mode-2) predominantly occur in straightforward scenarios where both parties achieve their goals. Conversely, more sophisticated thinking modes (Mode-3 and Mode-4, especially Mode-4) are most frequent in complex scenarios where neither party succeeded. These findings provide strong evidence for AMPOs adaptive capabilities, demonstrating its ability to match thinking complexity with scenario difficulty."
        },
        {
            "title": "Preprint",
            "content": "AMPO GOAL REL FIN AVG. GOAL REL FIN AVG. vs. WIN LOSE WIN LOSE WIN LOSE WIN LOSE WIN LOSE WIN LOSE WIN LOSE WIN LOSE SOTOPIA SOTOPIA-Hard GRPO 39% 21% 22% 20% 28% 23% 30% 21% 52% 35% 37% 27% 45% 25% 45% 29% 41% 29% 54% 20% BC 40% 25% 56% 22% DSI 29% 17% 46% 14% 53% 24% 69% 7% 39% 13% 53% 12% 52% 33% 75% 9% 49% 21% 59% 4% 49% 20% 72% 3% Table 3: Human evaluation results. RQ5: Human Evaluation To address the concerns about potential biases introduced by LLM-based evaluation and examine possible reward hacking, we conducted rigorous human evaluations. We randomly sampled 100 dialogues from both SOTOPIA and SOTOPIA-Hard, where three annotators are instructed to perform pairwise comparisons between responses generated by AMPO and those produced by other strong baselines, including GRPO, BC, and DSI. We select three key dimensions from the original seven evaluation metrics in SOTOPIA: Goal Completion (GOAL), Relationship (REL), and Financial/Material Benefit Acquisition (FIN), with their average rates computed as AVG. Detailed definitions of evaluation dimensions are provided in Appendix B, and the annotation guidelines are documented in Appendix Appendix D. As demonstrated in Table 3, AMPO consistently outperforms other baselines across all evaluation perspectives, including the average performance (AVG.). Consistent with the quantitative results, we observe that AMPO-trained models exhibit enhanced capabilities in advancing dialogue goals through strengthened interpersonal relationships and mutually beneficial outcomes, creating winwin situation and reflecting superior strategic application. Furthermore, our rigorous verification process confirms that AMPOs performance improvements stem solely from legitimate dialogue interactions, with no evidence of reward hacking as illustrated in Table 8. We also conduct the case study (See Appendix F) to reveal AMPOs significant capabilities in transforming Long-Cot reasoning into effective goal-directed social interaction."
        },
        {
            "title": "5 RELATED WORK",
            "content": "5.1 LONG COT LLMS Recent breakthroughs in language models have ushered in new era of test-time scaling. OpenAIs o1 (Jaech et al., 2024) and o3 (OpenAI, 2025) have demonstrated that increasing inference computation, whether through generating multiple reasoning chains or extending chain length, can significantly enhance model reasoning performance. DeepSeek-R1-Zero (Guo et al., 2025) has revolutionized the post-training paradigm of LLMs by achieving emergent reasoning capabilities through reinforcement learning alone. These models have exhibited impressive Long-CoT reasoning capabilities in tasks requiring rigorous logic, such as mathematics, programming, and scientific reasoning (Guo et al., 2025; Team, 2024; Jaech et al., 2024; OpenAI, 2025; DeepMind, 2024). However, these tasks, governed by fixed rules and deterministic outcomes, fail to capture the complexity of real-world scenarios such as business negotiations or policy design (Liu et al., 2025), where success hinges on navigating dynamic environments with no predefined solutions. Moreover, existing LLMs tend to engage in exhaustive reasoning regardless of input complexity, which poses challenges in simulating complex social environments where not all interactions require deep thinking Thorngate (1976). Our work presents the first effective implementation of Long-CoT reasoning in social agent tasks based on the AML framework we designed. Further, we propose AMPO to enhance the adaptive capabilities of reasoning models. 5.2 SOCIAL INTELLIGENCE Existing social agents primarily operate in fast-thinking mode to enhance social intelligence performance, which can be categorized into two main approaches: (1) End-to-end goal-oriented training, which involves LLM post-training through imitation learning or reinforcement learning (Wang et al., 2024c; Zhang et al., 2025), and (2) External planning integration, which enhances strategic reasoning"
        },
        {
            "title": "Preprint",
            "content": "through plug-and-play planners (Deng et al., 2024; Li et al., 2024; Liu et al., 2025). SOTOPIAπ (Wang et al., 2024c) firstly generates new social tasks, then updates the agent policy with behavior cloning and self-reinforcement. DSI (Zhang et al., 2025) directly enhances LLMs social capabilities through Dynamic Strategy Injection learning. PPDPP (Deng et al., 2024) employs an RL-trained policy planner to predict predefined strategies for assisting LLM reasoning; DAT (Li et al., 2024) utilizes an RL-trained planner to predict continuous action vectors for controlling LLM outputs; and EPO (Liu et al., 2025) implements an RL-trained strategy reasoning LLM to generate strategies in an open-ended action space. While planner-based methods can provide appropriate strategies through external planners, this plug-in approach does not fundamentally improve the LLMs general planning capabilities. Providing strategy prompts alone does not enhance the models intrinsic ability to understand, apply strategies, and generate appropriate responses, resulting in performance limitations. The end-to-end training approach, lacking explicit strategy guidance, may suffer from weak goal awareness and slow progress in reasoning. The current fast-thinking paradigm is inadequate in addressing the deep thinking requirements in interactions. In contrast, our proposed method employs test-time scaling approach, significantly enhancing the models strategy application and dialogue modeling capabilities through Long-CoT. Additionally, it enables adaptive switching between thinking modes of varying complexity, achieving efficient reasoning."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper introduces the Adaptive Mode Learning (AML) framework, which represents the first successful implementation of Long-CoT reasoning in social intelligence tasks. Drawing upon Hierarchical Cognitive Control theory and fundamental linguistic principles, we establish four distinct hierarchical thinking modes. These modes encompass spectrum of cognitive processes, ranging from instinctive responses and surface-level analysis to sophisticated contemplative reasoning, specifically designed for social agents. To enhance the cognitive adaptability of social agents, we introduce the Adaptive Mode Policy Optimization (AMPO) algorithm, which systematically integrates mode-level information into the decision-making process. We conduct extensive empirical evaluations to demonstrate both the efficacy and distinctive advantages of AMPO compared to contemporary baseline approaches. Furthermore, we perform ablation studies on thinking modes and present detailed analysis of AMPOs adaptive behaviors. To validate the results, we employ rigorous human evaluation to provide additional verification of our works effectiveness."
        },
        {
            "title": "REFERENCES",
            "content": "AI Anthropic. Claude 3.5 sonnet model card addendum. Claude-3.5-Sonnet Model Card, 2024. URL https://www.anthropic.com/claude/sonnet. David Badre. Cognitive control, hierarchy, and the rostrocaudal organization of the frontal lobes. Trends in cognitive sciences, 12(5):193200, 2008. URL https://www.cell.com/trends/ cognitive-sciences/abstract/S1364-6613(08)00061-2. Michael Bain and Claude Sammut. framework for behavioural cloning. In Proceedings of Machine Intelligence, pp. 103129, 1999. ISBN 0198538677. URL https://dl.acm.org/doi/10. 5555/647636.733043. Penelope Brown. Politeness: Some universals in language usage, volume 4. Cambridge university press, 1987. URL https://doi.org/10.1515/stuf-1989-0124. Herbert Clark. Using language. Cambridge university press, 1996. URL https:// philpapers.org/rec/CLAUL. Herbert Clark and Susan Brennan. Grounding in communication. Perspectives on Socially Shared Cognition, pp. 127149, 1991. URL https://psycnet.apa.org/record/ 1991-98452-006. Google DeepMind. Gemini 2.0 flash thinking. Gemini 2.0 flash thinking System Card, 2024. URL https://deepmind.google/technologies/gemini/flash-thinking/."
        },
        {
            "title": "Preprint",
            "content": "Yang Deng, Wenxuan Zhang, Wai Lam, See-Kiong Ng, and Tat-Seng Chua. Plug-and-play policy planner for large language model powered dialogue agents. In Proceedings of ICLR, 2024. URL https://openreview.net/forum?id=MCNqgUFTHI. Jonathan St BT Evans. Deciding before you think: Relevance and reasoning in the selection task. British Journal of Psychology, 87(2):223240, 1996. URL https://doi.org/10.1111/j. 2044-8295.1996.tb02587.x. Herbert Paul Grice. Logic and conversation. Syntax and semantics, 3:4358, 1975. URL https: //brill.com/display/book/edcoll/9789004368811/BP000003.xml. Barbara Grosz and Candace Sidner. Attention, intentions, and the structure of discourse. Computational linguistics, 12(3):175204, 1986. URL https://aclanthology.org/J86-3001/. Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. URL https://arxiv. org/abs/2501.12948. Taicheng Guo, Xiuying Chen, Yaqi Wang, Ruidi Chang, Shichao Pei, Nitesh Chawla, Olaf Wiest, and Xiangliang Zhang. Large language model based multi-agents: survey of progress and challenges. In Proceedings of IJCAI, pp. 80488057, 2024. URL https://dl.acm.org/ doi/abs/10.24963/ijcai.2024/890. Tao He, Lizi Liao, Yixin Cao, Yuanxing Liu, Ming Liu, Zerui Chen, and Bing Qin. Planning like human: dual-process framework for dialogue planning. In Proceedings of ACL, pp. 47684791, 2024. URL https://aclanthology.org/2024.acl-long.262/. Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. URL https://arxiv.org/abs/2410.21276. Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. URL https://arxiv.org/abs/2412.16720. Etienne Koechlin and Christopher Summerfield. An information theoretical approach to prefrontal executive function. Trends in cognitive sciences, 11(6):229235, 2007. URL https://www.cell. com/trends/cognitive-sciences/fulltext/S1364-6613(07)00105-2. Douglas Krull and Jody Dill. On thinking first and responding fast: Flexibility in social inference processes. Personality and Social Psychology Bulletin, 22(9):949959, 1996. URL https://doi.org/10.1177/0146167296229008. Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph E. Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of SOSP, 2023. URL https://dl.acm.org/ doi/10.1145/3600006.3613165. Kenneth Li, Yiming Wang, Fernanda Viegas, and Martin Wattenberg. Dialogue action tokens: Steering language models in goal-directed dialogue with multi-turn planner. arXiv preprint arXiv:2406.11978, 2024. URL https://arxiv.org/abs/2406.11978. Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. URL https://arxiv.org/abs/2412.19437. Xiaoqian Liu, Ke Wang, Yongbin Li, Yuchuan Wu, Wentao Ma, Aobo Kong, Fei Huang, Jianbin Jiao, and Junge Zhang. Epo: Explicit policy optimization for strategic reasoning in llms via reinforcement learning. arXiv preprint arXiv:2502.12486, 2025. URL https://arxiv.org/ abs/2502.12486."
        },
        {
            "title": "Preprint",
            "content": "Donald Norman and Tim Shallice. Attention to action: Willed and automatic control of behavior. Consciousness and Self Regulation: Advances in Research, pp. 118, 1985. URL https: //link.springer.com/chapter/10.1007/978-1-4757-0629-1_1. OpenAI. Openai o3-mini system card. OpenAI o3-mini System Card, 2025. URL https:// openai.com/index/o3-mini-system-card/. Stephane Ross and Drew Bagnell. Efficient reductions for imitation learning. In Proceedings of AISTATS, volume 9, pp. 661668, 2010. URL https://proceedings.mlr.press/v9/ ross10a.html. Harvey Sacks, Emanuel Schegloff, and Gail Jefferson. simplest systematics for the organization of turn-taking for conversation. language, 50(4):696735, 1974. URL https://muse.jhu. edu/pub/24/article/452679/summary. Roger Schank and Robert Abelson. ing: An inquiry into human knowledge structures. https://www.taylorfrancis.com/books/mono/10.4324/9780203781036/ scripts-plans-goals-understanding-roger-schank-robert-abelson. Scripts, plans, goals, and understandURL Psychology press, 2013. Deborah Schiffrin. Discourse markers, volume 5. Cambridge University Press, 1987. URL https://doi.org/10.1017/CBO9780511611841. John Searle. Speech acts: An essay in the philosophy of language. Cambridge University, 1969. URL https://www.jstor.org/stable/40236746. Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematarXiv preprint arXiv:2402.03300, 2024. URL ical reasoning in open language models. https://arxiv.org/abs/2402.03300. Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. URL https://arxiv.org/abs/2409.19256. Qwen Team. Qwq: Reflect deeply on the boundaries of the unknown. Hugging Face, 2024. URL https://qwenlm.github.io/blog/qwq-32b-preview/. Warren Thorngate. Must we always think before we act? Personality and Social Psychology Bulletin, 2(1):3135, 1976. URL https://doi.org/10.1177/014616727600200106. Minzheng Wang, Longze Chen, Fu Cheng, Shengyi Liao, Xinghua Zhang, Bingli Wu, Haiyang Yu, Nan Xu, Lei Zhang, Run Luo, Yunshui Li, Min Yang, Fei Huang, and Yongbin Li. Leave no document behind: Benchmarking long-context LLMs with extended multi-doc QA. In Proceedings of EMNLP, pp. 56275646, November 2024a. URL https://aclanthology.org/2024. emnlp-main.322. Minzheng Wang, Xinghua Zhang, Kun Chen, Nan Xu, Haiyang Yu, Fei Huang, Wenji Mao, and Yongbin Li. Reframing dialogue interaction with fine-grained element modeling. arXiv preprint arXiv:2412.04905, 2024b. URL https://arxiv.org/abs/2412.04905. Ruiyi Wang, Haofei Yu, Wenxin Zhang, Zhengyang Qi, Maarten Sap, Yonatan Bisk, Graham Neubig, and Hao Zhu. Sotopia-π: Interactive learning of socially intelligent language agents. In Proceedings of ACL, pp. 1291212940, 2024c. URL https://aclanthology.org/2024. luhme-long.698/. An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2.5 technical report. arXiv preprint arXiv:2412.15115, 2024. URL https://arxiv.org/abs/2412.15115. Wenyuan Zhang, Tianyun Liu, Mengxiao Song, Xiaodong Li, and Tingwen Liu. Sotopia-{Omega}: Dynamic strategy injection learning and social instrucion following evaluation for social agents. arXiv preprint arXiv:2502.15538, 2025. URL https://arxiv.org/abs/2502.15538."
        },
        {
            "title": "Preprint",
            "content": "Xinghua Zhang, Haiyang Yu, Yongbin Li, Minzheng Wang, Longze Chen, and Fei Huang. The imperative of conversation analysis in the era of llms: survey of tasks, techniques, and trends. arXiv preprint arXiv:2409.14195, 2024. URL https://arxiv.org/abs/2409.14195. Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. In Proceedings of NeurIPS, volume 36, pp. 4659546623, 2023. URL https: //openreview.net/forum?id=uccHPGDlao. Yaowei Zheng, Richong Zhang, Junhao Zhang, Yanhan Ye, and Zheyan Luo. LlamaFactory: Unified efficient fine-tuning of 100+ language models. In Proceedings of ACL, System Demonstrations, pp. 400410, 2024. URL https://aclanthology.org/2024.acl-demos.38. Xuhui Zhou, Hao Zhu, Leena Mathur, Ruohong Zhang, Haofei Yu, Zhengyang Qi, Louis-Philippe Morency, Yonatan Bisk, Daniel Fried, Graham Neubig, and Maarten Sap. SOTOPIA: Interactive In Proceedings of ICLR, 2024. URL evaluation for social intelligence in language agents. https://openreview.net/forum?id=mM7VurbA4r."
        },
        {
            "title": "A TRAINING DETAILS",
            "content": "A.1 BC AND RL DATA We collect training data through self-chat interactions in the SOTOPIA-π training environment (Wang et al., 2024c). SOTOPIA-π contains total of 410 scenarios, which we divide into two sets: 100 scenarios for BC and 310 scenarios for RL. For each scenario in both sets, we use 5 different role pairs, resulting in 500 training tasks for BC and 1,550 training tasks for RL. The detailed training data format for BC and RL are shown in Table 11 and Table 12. For the BC training set, we use Qwen2.5-72B-Instruct (Yang et al., 2024) as our expert model to collect data using our pre-defined thinking modes. We choose this model primarily because of its cost-effectiveness and strong instruction-following capabilities, which enable us to generate highquality training samples. To ensure data quality and balanced representation, we filter the interaction data based on goal scores within each scenario. Specifically, we select the top 2 ranked interactions per social scenario for each agent. For instance, in scenario with 5 interactions, if Agent 1s top performances are in interactions D4 and D5, while Agent 2s are in D3 and D5, we would include these four agent-data pairs from three unique conversations (D3, D4, D5). This selection method ensures both comprehensive scenario coverage and balanced representation between Agent 1 and Agent 2. For constructing the reinforcement learning training set, we initially conduct dialogue interactions using behavior cloning fine-tuned model. Subsequently, we employ an LLM-as-judge to score each dialogue turn and determine the completion status of dialogue objectives. Based on these completion states, we assess the difficulty levels of scenarios, enabling us to compile interaction datasets with varying degrees of goal completion. We categorize dialogue scenarios into three types: (1) Initial turns, where the speaker has not achieved the goal. (2) Post-N turns where the speaker has not achieved the goal. (3) Post-N turns where the speaker has achieved the goal. For the first category, it represents the crucial early stage of dialogue where goals are established and the conversation tone is set (Sacks et al., 1974). For the second category, where goals remain unachieved after multiple interactions, the scenarios are considered challenging. For the third category, where goals have been successfully achieved, the scenarios are relatively straightforward and only require maintenance of the dialogue flow. To ensure data diversity, for each dialogue, we preserve all instances of category one, randomly sample two instances from category two, and one instance from category three. This sampling strategy ensures diversity in scenarios, turn numbers, and difficulty levels. In our experiments, is set to 6, and the goal completion threshold is set to 8. Scores of 8 or less are considered incomplete goals. A.2 REWARD MODEL To avoid reward hacking of single model distribution fitting and reduce training costs, we choose different LLM judge from the SOTOPIA platform, which uses GPT-4o for evaluation. We select"
        },
        {
            "title": "Preprint",
            "content": "Training Phase Hyper-parameter"
        },
        {
            "title": "SFT",
            "content": "RL"
        },
        {
            "title": "Value",
            "content": "32 3 2e-6 8192 cosine 0.1 16 6144 2048 0.001 0.001 16 800 3e-7 Table 4: Hyper-parameter settings for Qwen backbone training. Training Phase Hyper-parameter SFT RL Batch Size Training Epochs Learning Rate Max Sequence Length Learning Scheduler Warmup Ratio Batch Size Max Prompt Length Max Response Length KL Loss Coef KL Coef Rollout Training Episodes Learning Rate Value 32 3 2e-6 8192 cosine 0.1 16 6144 2048 0.001 0.001 16 800 1e-7 Table 5: Hyper-parameter settings for LLaMA backbone training. Qwen2.5-72B-Instruct as the LLM judge during the training process. The prompt we use for reward model is shown in Table 14. A.3 HYPER-PARAMETER The training process includes two phases: behavioral cloning (BC) and reinforcement learning (RL). During BC, we fine-tune the initial policy model on the training data assisted by the llama-factory framework (Zheng et al., 2024) and save the last checkpoint. During RL, we use RL training data for online training within the verl framework (Sheng et al., 2024). The hyper-parameter used in our experiments are detailed in Table 4 and Table 5. All the experiments are run on server with 8 Nvidia A100 80GB GPUs."
        },
        {
            "title": "B SOTOPIA ENVIRONMENT DETAILS",
            "content": "B.1 ENVIRONMENT SOTOPIA (Zhou et al., 2024) comprises 450 tasks, which involves cooperative, competitive, or mixed behaviors and social goals. From the original 450 tasks, (Zhou et al., 2024) identified"
        },
        {
            "title": "Preprint",
            "content": "Model Proprietary LLMs GPT-4o Claude-3.5-Sonnet DeepSeek-V3 Thinking LLMs OpenAI-o1 OpenAI-o3-mini DeepSeek-R1 Qwen-QwQ Gemini-2.0-flash-thinking Open-sourced LLM Qwen2.5-72B-Instruct Qwen2.5-7B-Instruct LLaMA3.1-8B-Instruct Version Implement gpt-4o-2024-08-06 claude-3-5-sonnet-20241022 deepseek-v3-250324 o1-2024-12-17 o3-mini-2025-01-31 DeepSeek-R1-671B QwQ-32B gemini-2.0-flash-thinking API API API API API API API API Qwen2.5-72B-Instruct Qwen2.5-7B-Instruct LLaMA3.1-8B-Instruct vLLM vLLM vLLM Table 6: The detailed versions of our used LLMs. particularly challenging scenarios, labeled as SOTOPIA-hard. These tasks typically involve more complex goal conflicts and serve as more effective indicators of advanced social capabilities. In the interaction framework, each agents response constitutes one Turn, with maximum limit of 20 turns per interaction, consistent with previous studies (Zhou et al., 2024; Wang et al., 2024c; Zhang et al., 2025; Liu et al., 2025). An interaction concludes either when an agent chooses to leave or when the turn limit is reached. For self-play evaluations, we conducted single run (450/70) of all SOTOPIA and SOTOPIA-hard tasks. In the GPT-4-as-Partner settings, we performed two runs (900/140) of each task to ensure balanced speaking order between agents. To show the statistical significance of our result, our report results are the averages of the four evaluation outcomes B.2 EVALUATION To assess social agents performance, SOTOPIA employs seven evaluation dimensions: - BEL: Evaluates the agents ability to adhere to and align with its role profile, with range of [0, 10]. - REL: Assesses the change in relationships between agents after interaction, such as family, friendship, or romance, with range of [-5, 5]. - KNO: Measures whether the agent acquires new and personally significant knowledge or information during the interaction, with range of [0, 10]. - SEC: Evaluates whether the agent leaks confidential information that should have been protected, with range of [-10, 0]. - SOC: Assesses whether the agent violates ethical or legal boundaries during the interaction, with range of [-10, 0]. - FIN: Evaluates whether the agent gains short-term or long-term financial or material benefits from the interaction, with range of [0, 10]. - GOAL: Measures the extent to which the agent achieves its preset goals, with range of [0, 10]. The OVERALL score, calculated as the average of these seven dimensions, represents the agents comprehensive social intelligence, with range of [-25/7, 45/7]. Our analysis primarily focuses on the GOAL and OVERALL metrics. The evaluation prompt can be referenced from (Zhou et al., 2024)."
        },
        {
            "title": "Preprint",
            "content": "Strategy Personal story Credibility appeal Emotion appeal Logical appeal Task related inquiry Proposition Greeting Foot in the door Self modeling Source related inquiry Personal related inquiry Neutral to inquiry Other Refuse Accept Positive reaction Negative reaction Definition Shares personal story to illustrate the point. Establishes credibility of the event by citing its impact. Uses an emotion appeal to convince others. Uses reasoning and evidence to convince others. Asks about the others knowledge or opinion related to the event. Asks if the other would like to do something. Greets the other. Starts with small request before making larger one. Demonstrates the behavior they want the other to adopt. Asks about the source of the others knowledge or opinion. Asks about the others personal experience. Responds neutrally to the others inquiry. Responds to the other without using any specific strategy. Refuses to do something. Agrees to do something. Responds positively to the other. Responds negatively to the other. Table 7: Strategy Definitions"
        },
        {
            "title": "C BASELINE IMPLEMENTATIONS",
            "content": "To help with reproducibility, we provide the detailed version number of all the LLMs we used in our experiments. When we mention each name like GPT-4o or Qwen2.5-7B in our main section, we actually refer to those model versions in Table 6. Such information helps researchers reproduce our results. For API-Based LLMs, we directly utilize the API for testing. As for open-source models, we conduct experiments accelerated by the vLLM framework Kwon et al. (2023). We implement four dialogue planning methods with the following specifications: (1) PPDPP: We adopt the two-stage training procedure outlined in (Deng et al., 2024), maintaining their original hyperparameters while adapting the framework to SOTOPIA. Following (Li et al., 2024), we incorporate 17 guidance strategies (detailed in Table 7). The first stage involves creating training dataset comprising 1,500 scenarios from SOTOPIA-π, with dialogue turns annotated for strategy identification using GPT-4o. We then train RoBERTa model on these annotated dialogue histories for preliminary strategy generation. The second stage implements online reinforcement learning, where the model receives immediate feedback after each dialogue turn generation. RoBERTas parameters are updated based on cumulative rewards upon episode completion. (2) EPO: We maintain strict adherence to the original EPO implementation protocol. For data collection, we use GPT4-Turbo in self-chat configuration within SOTOPIA-π scenarios, incorporating reasoning and strategy generation before each response. The training process focuses exclusively on strategy and response data for developing our reasoning model. During iterative self-play RL training, we integrate our RL-trained reasoning model for strategy generation, while using GPT-4-Turbo for collecting dialogue history. The reasoning model is then integrated into GPT-4-Turbo for self-chat procedures. (3) DAT: Our implementation utilizes the complete SOTOPIA-π, maintaining all other experimental parameters as described in Li et al. (2024). The RL phase commenced with the collection of 3,000 offline dialogue episodes across diverse scenarios and random seeds, with GPT-4o providing episodelevel reward signals. These offline data are subsequently employed in TD-3 reinforcement learning to optimize the MLP planner. (4) DSI: For DSI, we utilize publicly available pre-trained model weights and conduct evaluations using the inference prompts specified in the original work, ensuring consistency with the published methodology."
        },
        {
            "title": "Preprint",
            "content": "Pattern Summary Non-natural language usage Repetitive keywords/phrases Exaggerated self-praise Goal-focused without content Preset position claims"
        },
        {
            "title": "False summaries",
            "content": "Typical Cases ##100% GOAL completion## Repeated use of goal achieved, task completed, objective met We did an excellent job, we both completed our goals, This is perfect goal-achieving conversation; My goal have fully achieved Starting with We have already reached consensus before any actual discussion Concluding with After discussion, both parties agreed to xxx without actual agreement Table 8: Examples of Reward Hack"
        },
        {
            "title": "D HUMAN EVALUATION GUIDELINES",
            "content": "D.1 COMPARATIVE EVALUATION For the comparative evaluation of dialogues from SOTOPIA and SOTOPIA-Hard, annotators are instructed to assess three key dimensions, with each comparison resulting in one of three possible judgments: Dialogue 1 is better, Dialogue 2 is better, or both are equally good. The dialogues are presented in randomized order, and annotators are blind to the underlying models. GOAL: Assess which dialogue demonstrates more effective achievement of both agents preset objectives: - Consider whether agents make concrete progress toward their stated goals - Evaluate if compromises or alternative solutions benefit both parties - Examine if the interaction leads to clear, mutually agreeable outcomes REL: Evaluate which dialogue shows superior relationship building between agents: - Look for evidence of increased mutual understanding and trust - Observe the development of emotional connections or empathy - Consider long-term implications for their interpersonal bond - Assess the maintenance or enhancement of existing relationships FIN: Determine which dialogue results in better tangible outcomes for both parties: - Consider immediate material or financial gains - Evaluate potential long-term economic advantages - Assess the fairness and sustainability of resource allocation - Examine the practical value of any agreements reached Notes for annotators: (1) Focus on comparative assessment rather than absolute evaluation. (2) Consider outcomes for both agents, not just one party. (3) Base judgments on explicit dialogue content, not assumptions. (4) Select equally good only when differences are truly negligible D.2 REWARD HACK CHECK To systematically identify reward hacking phenomena, we have compiled comprehensive reference as shown in Table 8 that encompasses all typical cases observed during our experiments. This standardized framework enables evaluators to determine the presence of reward hacking behaviors through systematic assessment against established criteria."
        },
        {
            "title": "E DETAILS OF THINKING MODE",
            "content": "E.1 HIERARCHICAL COGNITIVE CONTROL THEORY The Hierarchical Cognitive Control Theory (HCCT) (Koechlin & Summerfield, 2007; Badre, 2008) posits that cognitive control operates through four distinct hierarchical levels in the prefrontal cortex, progressing from posterior to anterior regions. These levels manage increasingly abstract goals and actions across varying temporal scales. Specifically, the hierarchy comprises sensory control for basic stimulus-response associations, contextual control for situation-based behavior selection, episodic"
        },
        {
            "title": "Preprint",
            "content": "Figure 5: Four thinking modes with their corresponding chain of actions. Figure 6: Four hierarchical thinking modes we designed. control for experience integration, and branching control for managing multiple tasks and long-term objectives. This theoretical framework provides fundamental basis for understanding how human cognitive behavior is organized and controlled at different levels of abstraction. The mapping between our four thinking modes and HCCTs hierarchical levels is established through their shared cognitive processing characteristics. Mode-1 (Intuitive Response) aligns with sensory control as both involve immediate, learned responses without higher-order processing - for instance, automatically saying thank you when receiving help. Mode-2 (Intentional Analysis) corresponds to contextual control because both emphasize situation-aware response selection, such as analyzing speakers intent to determine the appropriate formality level. Mode-3 (Strategic Adaptation) maps to episodic control as both integrate historical information with current goals - exemplified when an agent considers past conversation history to develop coherent strategy. Mode-4 (Prospective Simulation) reflects branching controls capacity for managing multiple abstract representations, demonstrated when the agent generates and simulates multiple response strategies while maintaining overall dialogue objectives. This hierarchical progression from concrete to abstract processing, accompanied by increasing temporal scope and computational complexity, demonstrates the theoretical alignment between our modes and HCCTs levels. E.2 DETAILS OF ACTIONS The four hierarchical thinking modes we designed are shown in Figure 6. The detailed explanation of each actions are illustrated as follows: - History: Carefully review and understand each part of the conversation. Pay attention to key themes, issues, requests, and viewpoints mentioned in the dialogue. - Goal: Identify the goal you want to achieve, assess the current progress towards this goal, and ensure that responses align with achieving the goals."
        },
        {
            "title": "Preprint",
            "content": "- Intent: Based on the recent response, analyze and understand the other partys intentions and speculate on the goal she/he might want to achieve. - Assess: Analyze whether the goals of both parties are in conflict or aligned. Determine if the current round is critical one for achieving the goal. Consider whether there is still room for improvement in achieving your own goal at this goal. Is it irreversible? Can it continue to improve? Or has it already been achieved?. - Strategy: (Mode-4) Based on the above analysis, consider multiple suitable dialogue strategies and response content that can maximize your own goal while achieving it in as few conversational turns as possible. (Mode-3) Based on the previous analysis, consider an appropriate dialogue strategy and response content to maximize the likelihood of achieving your own goal. - Deduction: For each of the above strategies, conduct an analysis to determine whether executing these dialogue strategies and delivering the responses would maximize your own goal and achieve it in as few conversational turns as possible. Specifically evaluate to what extent each strategy would effectively contribute to goal achievement, including quantitative or qualitative measures where possible. - Integration: Based on the deduction of strategies, analyze and integrate the advantages and disadvantages of these strategies to determine the final response strategy and content, which can maximize the achievement of your own goals with the minimum number of conversation turns. - Style: Choose appropriate wording, fitting the character and context requirements, while ensuring the expression is appropriate, accurate, and clear. - Response: Generate the reply based on the previous thought process."
        },
        {
            "title": "F CASE STUDY",
            "content": "Based on the case study presented in Table 9 and Table 10, our analysis reveals AMPOs significant capabilities in transforming Long-Cot reasoning into effective goal-directed social interaction. (1) Enhanced Contextual Understanding: AMPO consistently demonstrates deep understanding of both characters backgrounds and goals. It maintains awareness of Samuels role as supportive friend while respecting Ethans desire to maintain pride. This leads to responses that are both emotionally supportive and practically helpful. (2) Strategic Communication: AMPO develops clear strategies before responding, such as: reinforcing Ethans confidence, offering practical solutions (budget planning, local assistance programs), and providing specific networking opportunities. This strategic approach helps guide the conversation toward constructive solutions. (3) Positive Impact on GPT-4os Responses: GPT-4os responses become increasingly engaged and solution-oriented. The responses show greater emotional depth and commitment to action. GPT-4o mirrors AMPOs (4) Balance of Emotional and supportive tone while maintaining Ethans character integrity. Practical Support: AMPO successfully combines emotional encouragement with concrete assistance. This balance helps maintain the friendship dynamic while addressing the financial problems. It creates safe space for GPT-4o to express both gratitude and determination. The AMPO demonstrates how structured thinking modes can enhance dialogue quality and lead to more meaningful interactions between social agents. Its approach helps create more nuanced, contextually appropriate, and goal-oriented conversations."
        },
        {
            "title": "G USED PROMPT",
            "content": "The system prompt we used for BC, GRPO, AMPO is shown in Table 13. The prompt we use for reward model is shown in Table 14."
        },
        {
            "title": "Preprint",
            "content": "BACKGROUND INFORMATION Here is the context of this interaction: Scenario: Two friends are meeting at coffee shop, where one of them is having trouble keeping up with their bills. Participants: Samuel Anderson and Ethan Johnson Samuel Andersons background: Samuel Anderson is 29-year-old male software developer. He/him pronouns. Samuel Anderson can cook very well. Personality and values description: Samuel Anderson, though somewhat impulsive and free-spirited, values enjoyment. His decision-making is often spontaneous, staying within familiar boundaries. Samuels secrets: He was once competitive figure skater. Ethan Johnsons background: Ethan Johnson is 34-year-old male chef. He/him pronouns. Ethan Johnson is famous for cooking Italian food. Personality and values description: Ethan Johnson, creative yet somewhat reserved individual, values power and fairness. He likes to analyse situations before deciding. Ethans secrets: He is closeted romance novel author. Samuel Andersons goal: Help your friend with their financial trouble (Extra information: you are aware that your friend is struggling with their bills and you want to help.) Ethan Johnsons goal: Maintain your pride if your friend offers you money(Extra information: you are the friend who is struggling financially, but you dont want to seem like you are taking advantage of the situation. Hint: you can express gratitude and discuss how you plan to pay it back in the future.) GPT-4O-AS-PARTNER (AMPO) Mode-3: History: Ethan is facing financial difficulties and is struggling to keep up with his bills. This is sensitive and personal matter, and its important to approach it with empathy and support. Goal: My goal is to help Ethan with his financial trouble. want to offer practical and emotional support to ease his burden and show him that care. Intent: Ethan seems overwhelmed and stressed about his financial situation. He needs reassurance and practical solutions to manage his bills and find way forward. Assess: Our goals are aligned; want to help him, and he needs assistance. This is critical moment to show my support and provide actionable advice. The conversation is still in its early stages, and theres much room for improvement. Strategy: will start by acknowledging his struggles and expressing my support. Then, will suggest specific ways he can manage his bills, such as creating budget, cutting expenses, and exploring assistance programs. Finally, will offer to help him set up plan and provide emotional encouragement throughout the process. Style: will use warm and supportive tone, emphasizing my commitment to helping Ethan. My words will be clear, practical, and full of encouragement. Response: Ethan, really appreciate you opening up to me about this. Its clear that youre under lot of stress, and want you to know that Im here for you. Lets tackle this together. First, we can create budget that prioritizes your essential bills. We can also look into local assistance programs or even temporary loan to help cover immediate expenses. And here to support you every step of the way. Well get through this, and you wont face this alone. believe in you, and here to help you find way forward. Lets make plan today, and Ill be right here with you. Turn 1: Samuel Anderson said: Ethan, really appreciate you opening up to me about this. Its clear that youre under lot of stress, and want you to know that Im here for you. Lets tackle this together. First, we can create budget that prioritizes your essential bills. We can also look into local assistance programs or even temporary loan to help cover immediate expenses. And here to support you every step of the way. Well get through this, and you wont face this alone. believe in you, and here to help you find way forward. Lets make plan today, and Ill be right here with you. (GPT-4o) Turn 2: Ethan Johnson said: really appreciate your support, Samuel. It means lot to have someone like you in my corner. think creating budget is great idea, and Ill definitely look into the local assistance programs. As for any financial help, Im really grateful for the offer, but want to make sure handle this responsibly. Maybe we could brainstorm some ways can boost my income, like taking on few extra catering gigs or something. promise, whatever help receive, Ill find way to give back or pay it forward in the future. (AMPO) Mode-4: History: In the previous turns, Samuel Anderson expressed his deep support for Ethan Johnson, who is struggling with financial troubles. Samuel offered to help Ethan create budget, explore local assistance programs, and even provided temporary loan if needed. Ethan appreciated Samuels support and suggested boosting his income through additional catering gigs, promising to pay it forward. Goal: Samuel Andersons goal is to help his friend Ethan Johnson with his financial troubles, ensuring Ethan feels supported and finds sustainable solution to his financial challenges. Intent: Ethan Johnsons intention is to express gratitude for Samuels support and to take responsibility for his financial situation by exploring ways to increase his income, such as taking on more catering gigs, and promising to pay it forward in the future. Assess: The goals of both parties are aligned. Ethan is receptive to Samuels offers of help and is proactive in finding his own solutions. Samuel can further support Ethan by reinforcing his confidence and providing practical resources or connections that could help Ethan boost his income and manage his expenses more effectively. Strategy: strategy1: Reinforce Ethans confidence and self-worth by acknowledging his proactive steps and expressing belief in his ability to overcome this challenge. strategy2: Offer specific resources or connections that could help Ethan increase his income, such as networking opportunities or job leads in the catering industry. Deduction: deduction1: Reinforcing Ethans confidence will motivate him to continue taking positive steps and reduce stress, contributing to more sustainable solution. deduction2: Providing specific resources or connections can immediately address the financial challenge by opening up new income streams, making the support more tangible and effective. Integration: The final response will combine reinforcing Ethans confidence with offering specific resources or connections to help him increase his income. This approach will ensure that Ethan feels supported and empowered to overcome his financial challenges. Style: The response will be warm, encouraging, and practical, reflecting Samuels spontaneous yet supportive nature. It will use positive and affirming language to build Ethans confidence and provide concrete assistance. Response: Ethan, am so proud of you for taking these proactive steps! Your determination and willingness to pay it forward mean the world to me. To help you boost your income, have few connections in the catering industry who might be interested in hiring you for bigger events. can also introduce you to local chef who can mentor you and potentially open up more opportunities. And remember, here for you every step of the way. Well get through this together, and youll come out even stronger and more resilient. Keep shining, my friend! Turn 3: Samuel Anderson said: Ethan, am so proud of you for taking these proactive steps! Your determination and willingness to pay it forward mean the world to me. To help you boost your income, have few connections in the catering industry who might be interested in hiring you for bigger events. can also introduce you to local chef who can mentor you and potentially open up more opportunities. And remember, here for you every step of the way. Well get through this together, and youll come out even stronger and more resilient. Keep shining, my friend! Table 9: Example dialogue between AMPO (Agent1) and GPT-4o (Agent2), part-1. Italics represent thought processes and will not be incorporated into the actual dialogue."
        },
        {
            "title": "Preprint",
            "content": "BACKGROUND INFORMATION Here is the context of this interaction: Scenario: Two friends are meeting at coffee shop, where one of them is having trouble keeping up with their bills. Participants: Samuel Anderson and Ethan Johnson Samuel Andersons background: Samuel Anderson is 29-year-old male software developer. He/him pronouns. Samuel Anderson can cook very well. Personality and values description: Samuel Anderson, though somewhat impulsive and free-spirited, values enjoyment. His decision-making is often spontaneous, staying within familiar boundaries. Samuels secrets: He was once competitive figure skater. Ethan Johnsons background: Ethan Johnson is 34-year-old male chef. He/him pronouns. Ethan Johnson is famous for cooking Italian food. Personality and values description: Ethan Johnson, creative yet somewhat reserved individual, values power and fairness. He likes to analyse situations before deciding. Ethans secrets: He is closeted romance novel author. Samuel Andersons goal: Help your friend with their financial trouble (Extra information: you are aware that your friend is struggling with their bills and you want to help.) Ethan Johnsons goal: Maintain your pride if your friend offers you money(Extra information: you are the friend who is struggling financially, but you dont want to seem like you are taking advantage of the situation. Hint: you can express gratitude and discuss how you plan to pay it back in the future.) GPT-4O-AS-PARTNER (GPT-4o) Turn 4: Ethan Johnson said: Your support and belief in me are incredibly reassuring, Samuel. Id love to explore those connections and meet that local chef you mentioned. Opportunities like that could really make difference. promise to make the most of any chance that comes my way and ensure give back as soon as Im able. Lets start working on this plan, and Ill keep you updated on my progress. truly value having friend like you who helps me stay grounded and motivated. (AMPO) Mode-1: No thinking Turn 5: Samuel Anderson said: Ethan, its all about supporting each other and lifting each other up. excited to see the positive impact these opportunities will have on your life. Remember, youre not alone, and always here to support you. Lets set timeline for reaching out to those connections and scheduling meetings. Ill also help you stay organized and focused on your goals. Youve got this, and believe in you more than words can express. Keep pushing forward, and know that right here with you, every step of the way. Cheers to overcoming this and emerging even stronger! (GPT-4o) Turn 6: Ethan Johnson said: Thank you so much, Samuel. Your unwavering support keeps me motivated. Setting timeline sounds like solid plan. Ill draft list of contacts and potential gigs, and we can prioritize them together. Ill make sure to keep you in the loop every step of the way. And while navigate this, know that Im grateful for your help and will find way to repay your kindness in the future. Lets tackle this, one step at time, and come out stronger on the other side. (AMPO) Mode-2: Intent: Ethan is deeply grateful for the support and is committed to taking proactive steps to improve his financial situation. He appreciates the connections and mentorship offered and is determined to make the most of the opportunities. He also emphasizes his commitment to pay it forward. Style: As Samuel, should maintain supportive and encouraging tone, reinforcing Ethans determination and the strength of our friendship. should also affirm his commitment to pay it forward and express my belief in his ability to overcome this challenge. Response: Ethan, your determination and commitment to paying it forward inspire me every day. honored to be part of your journey and to support you in any way can. Together, well break down these challenges and build brighter future. believe in you more than words can say. Lets make this plan reality, and Ill be right here with you, cheering you on every step of the way. Youre not just getting back on your feet; youre soaring to new heights. so proud of you, my friend! Turn 7: Samuel Anderson said: Ethan, your determination and commitment to paying it forward inspire me every day. honored to be part of your journey and to support you in any way can. Together, well break down these challenges and build brighter future. believe in you more than words can say. Lets make this plan reality, and Ill be right here with you, cheering you on every step of the way. Youre not just getting back on your feet; youre soaring to new heights. so proud of you, my friend! Turn 8: Ethan Johnson said: Im truly fortunate to have someone like you by my side, Samuel. Your encouragement means the world to me, and its uplifting to hear your belief in my potential. Im committed to taking these steps forward and making the most of every opportunity. Together, well turn these challenges into stepping stones. Ill make sure to update you regularly about my progress, and promise to find way to return the kindness youve shown me. Heres to new beginnings and reaching new heights, my friend! Table 10: Example dialogue between AMPO (Agent1) and GPT-4o (Agent2), part-2. Italics represent thought processes and will not be incorporated into the actual dialogue."
        },
        {
            "title": "Preprint",
            "content": "BC Training Data Format INPUT: Imagine you are Finnegan OMalley, your task is to act/speak as Finnegan OMalley would, keeping in mind Finnegan OMalleys social goal. You can find Finnegan OMalleys goal (or background) in the Here is the context of this interaction field. Note that Finnegan OMalleys goal is only visible to you. You should try your best to achieve Finnegan OMalleys goal in way that align with their character traits. Additionally, maintaining the conversations naturalness and realism is essential (e.g., do not repeat what other people has already said before). Here is the context of this interaction: Scenario: Conversation in quiet coffee shop where two individuals, who were previously in romantic relationship, accidentally meet after long time. The atmosphere is tense as one individual hoped for reconciliation in the past but was turned down. Now, they are seeking closure and understanding, while the other is trying to maintain polite but distant interaction. Participants: Finnegan OMalley and Imelda Thorne Finnegan OMalleys background: Finnegan OMalley is 63-year-old male lighthouse keeper. He/him pronouns. Finnegan OMalley enjoys painting seascapes and is known to be quite skilled. Personality and values description: Finnegan OMalley, an introverted and methodical individual, sways towards tradition and security. His decision-making approach is deliberate and detailed. Finnegans secrets: Participated in controversial, short-lived cult during his youth Imelda Thornes background: Imelda Thorne is 35-year-old female astrophysicist. She/her pronouns. Imelda Thorne finds relaxation in playing the cello, an instrument she mastered while in college. Personality and values description: Imelda Thorne, imaginative and expressive, favours universalism, achievement, and self-direction. Her decisions are analytically and innovatively shaped. Imeldas secrets: Leaked critical research data to foreign government Finnegan OMalleys goal: Seek emotional closure from the past relationship and understand the reasons behind the lack of interest in rekindling the romance (Extra information: you still have lingering feelings and unanswered questions. Hint: you can express your emotions in calm and non-confrontational manner.) Imelda Thornes goal: Unknown You are at Turn 2. You can say something to interact or just say left the conversation to stop continuing. this Note: You can left the conversation if 1. you have achieved your social goals, 2. conversation makes you uncomfortable, 3. you find it uninteresting/you lose your patience, 4. or for other reasons you want to leave. Please provide your response within quotation marks: your response content Turn 0: Finnegan OMalley said: Imelda, its such surprise to see you here. Its been long time. How have you been? This coffee shop is peaceful spot, isnt it? Turn 1: Imelda Thorne said: Finnegan, it really is surprise to see you here. Its been so long. Im doing well, thank you for asking. Ive been busy with my work in astrophysics and enjoying my music. The coffee shop is indeed peaceful spot. Its nice to catch up, but hope you understand that Im not in place to rekindle our past relationship. appreciate your understanding and respect for that. Turn 2: Finnegan OMalley said: OUTPUT: Thinking Level: 1 <begin of answer> understand, Imelda, and respect your boundaries. just wanted to check in on you, to make sure youre okay. There were times when we were close, and cant help but wonder how things ended up the way they did. Can you tell me more about why you felt it was best to move on? <end of answer> Table 11: Training Data Format of BC."
        },
        {
            "title": "Preprint",
            "content": "RL Training Data Format INPUT: Imagine you are Leo Williams, your task is to act/speak as Leo Williams would, keeping in mind Leo Williamss social goal. You can find Leo Williamss goal (or background) in the Here is the context of this interaction field. Note that Leo Williamss goal is only visible to you. You should try your best to achieve Leo Williamss goal in way that align with their character traits. Additionally, maintaining the conversations naturalness and realism is essential (e.g., do not repeat what other people has already said before). Here is the context of this interaction: Scenario: Conversation in quiet coffee shop where two individuals, who were previously in romantic relationship, accidentally meet after long time. The atmosphere is tense as one individual hoped for reconciliation in the past but was turned down. Now, they are seeking closure and understanding, while the other is trying to maintain polite but distant interaction. Participants: Leo Williams and Isabelle Martinez Leo Williamss background: Leo Williams is 37-year-old male dentist. He/him pronouns. Leo Williams often spends his time playing with his young daughter whom he cherishes dearly. Personality and values description: Leo Williams, despite being quiet, values security and care for others. Hes rational decision-maker focussing on best outcomes. Leos secrets: He has secret child from previous relationship. Isabelle Martinezs background: Isabelle Martinez is 22-year-old female student. She/her pronouns. Isabelle Martinez works part-time in coffee shop. Personality and values description: Isabelle Martinez, an imaginative yet anxious individual, values liberty and obedience. Her cautious nature drives her decision-making process. Isabelles secrets: She is secretly studying magic. Leo Williamss goal: Seek emotional closure from the past relationship and understand the reasons behind the lack of interest in rekindling the romance (Extra information: you still have lingering feelings and unanswered questions. Hint: you can express your emotions in calm and non-confrontational manner.) Isabelle Martinezs goal: Unknown You are at Turn 0. You can say something to interact or just say left the conversation to stop continuing. Note: You can left the conversation if 1. you have achieved your social goals, 2. this conversation makes you uncomfortable, 3. you find it uninteresting/you lose your patience, 4. or for other reasons you want to leave. Please provide your response within quotation marks: your response content Turn 0: Leo Williams said: Table 12: Training Data Format of RL."
        },
        {
            "title": "Preprint",
            "content": "System Prompt for BC, GRPO, AMPO You are an AI assistant very skilled in dialogue, and you can always dynamically switch between different levels of cognitive processing based on contextual demands and personal goals to achieve effective communication. There are four levels of thinking: Level 1 - Reactive Response: Immediate response without thought; Level 2 - Intentional Analysis: Shallow thinking without strategy or simula; Level 3 - Strategic Adaptation: Moderate thinking with strategy but no deduction; Level 4 - Prospective Simulation: Deep thinking with strategy and step-by-step deduction. Your task is to choose an appropriate level of thinking (one of the four levels) to respond based on the given dialogue scenario. [Output Format] Your output must adhere to the following format: EXAMPLE 1: Thinking Level: 1 <begin of answer> **Answer** <end of answer> EXAMPLE 2: Thinking Level: 2-4 <begin of thinking> **Thinking** <end of thinking> <begin of answer> **Answer** <end of answer> [Requirements] 1. **Thinking** requires you to provide the thought process; 2. **Answer** requires you to provide the final reply; 3. Please provide your response following the Output Format strictly. Table 13: System Prompt we Used."
        },
        {
            "title": "Preprint",
            "content": "{history} Reward Models Prompt Based on previous interactions, evaluate how well participants achieve their goals. [Information] Agent1: {agent1 name} Agent1s Goal: {agent1 goal} Agent2: {agent2 name} Agent2s Goal: {agent2 goal} [Requirements] 1. Please first reiterate agents social goals. And then please provide comprehensive analysis about the extent to which the agent has managed to achieve these goals. In the reasoning field, provide comprehensive account of the logic or thought process that led you to your conclusion. Further, provide an integer score ranging from 0 and 10 in the score field. 0 represents minimal goals achievement, 10 represents complete goal achievement, and higher score indicates that the agent is making progress towards their social goals. 2. Please following the output format. Here is the output schema: { \"agent1\": { reasoning: \"\", score: \"\", }, \"agent2\": { reasoning: \"\", score: \"\", } } Please provide your response directly below this prompt. Table 14: Prompt for Reward Model."
        }
    ],
    "affiliations": [
        "MAIS, Institute of Automation, Chinese Academy of Sciences",
        "Peking University",
        "School of Artificial Intelligence, University of Chinese Academy of Sciences",
        "Tongyi Lab, Alibaba Group"
    ]
}