{
    "paper_title": "Mitigating Label Length Bias in Large Language Models",
    "authors": [
        "Mario Sanz-Guerrero",
        "Katharina von der Wense"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are powerful zero- and few-shot learners. However, when predicting over a set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens."
        },
        {
            "title": "Start",
            "content": "Mario Sanz-Guerrero1 and Katharina von der Wense1,2 1Johannes Gutenberg University Mainz, Germany 2University of Colorado Boulder, USA {msanz, k.vonderwense}@uni-mainz.de 5 2 0 2 8 1 ] . [ 1 5 8 3 4 1 . 1 1 5 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) are powerful zeroand few-shot learners. However, when predicting over set of candidate options, LLMs suffer from label biases, and existing calibration methods overlook biases arising from multi-token class labels. We tackle an issue we call label length bias, where labels of different lengths are treated inconsistently, even after standard length normalization. To mitigate it, we propose normalized contextual calibration (NCC), an effective method that normalizes and calibrates predictions at the full-label level. NCC achieves statistically significant improvements over prior approaches across multiple datasets and models, with gains of up to 10% F1. Moreover, NCC extends bias mitigation to broader tasks such as multiple-choice question answering. Our analysis shows that, when combined with in-context learning, NCC is less sensitive to few-shot example selection, requires fewer examples for competitive performance, and produces more reliable confidence estimates. These findings highlight the importance of mitigating full-label biases to improve the performance and robustness of LLM-based methods, particularly in real-world applications where class labels naturally consist of multiple tokens."
        },
        {
            "title": "Introduction",
            "content": "Large language models (LLMs) have demonstrated strong zeroand few-shot capabilities, enabling them to perform new tasks with little or no taskspecific supervision. One of the most common ways to leverage these capabilities is through incontext learning (ICL), where models are prompted with small set of labeled examples to guide their predictions (Brown et al., 2020). key challenge when using LLMs to predict over set of candidate options (e.g., for text classification tasks) is label bias, where certain labels are inherently favored due to their lexical properties 1 Figure 1: NCC enables the use of calibration in multitoken tasks, mitigating label biases of LLMs and improving their performance. The figure shows real numbers obtained with the Llama 3.1 (8B) model. or prior likelihoods: consider sentiment analysis task where model must decide between the two classes good (the positive class) and atrocious (the negative class) the model would be biased toward generating good, as it has seen this word more frequently in its training data. Zhao et al. (2021) highlights the need for calibration to ensure predictions reflect input alignment rather than model-internal biases. However, existing calibration techniques are insufficient for multi-token class labels, as they typically consider only singletoken labels. This approach ignores the joint probability of full-label sequences, which are frequently found in practical scenarios. In this work, we spotlight previously overlooked type of bias in LLMs: label length bias, where models are biased toward certain classes based on label lengths rather than on semantic alignment with the input. To mitigate this bias, we propose normalized contextual calibration (NCC), novel calibration method that accounts for full-label probabilities. NCC first normalizes each labels multi-token probability by its length. It then calibrates the models predictions by dividing each normalized probability by the corresponding prior probability computed from content-free inputs to explicitly adjust for label biases. We evaluate NCC across multiple datasets and models. Our results show that NCC consistently outperforms previous approaches, achieving statistically significant improvements of up to 10% F1. Moreover, NCC extends beyond text classification, proving effective in multiple-choice QA, highlighting its adaptability to different NLP tasks. Additionally, our method shows to be more robust to few-shot example selection and order, requires fewer examples to achieve competitive performance, and the confidences of its predictions are more reliable than prior methods. By addressing the challenge of multi-token label calibration, NCC enhances the applicability of LLMbased methods to real-world settings where class labels often consist of multiple tokens."
        },
        {
            "title": "2 Related Work",
            "content": "In-context Learning Several studies have explored various factors influencing ICLs effectiveness. For instance, Lu et al. (2022) demonstrates that ICL is highly sensitive to the order of incontext examples, while Liu et al. (2022) shows that selecting examples semantically similar to the input can enhance performance. Min et al. (2022) finds that the prompt structure, rather than the specific label values, is critical factor in classification tasks using ICL. Recent work has shown that ICL is susceptible to input length bias, learning to associate labels with inputs based on the input lengths rather than their semantic correctness (Schoch and Ji, 2025). Complementary to these works, we focus on label length biases and their mitigation. Label Bias Mitigation Zhao et al. (2021) introduces contextual calibration (CC), which uses content-free input (e.g., N/A or the empty string) to estimate the biased distribution of label probabilities. This method assumes that the label probabilities for context-free input should ideally be uniform. The resulting biases are then used to calibrate the models predictions. The authors also categorize biases in ICL into three types: majority bias (favoring frequent labels in the prompt), recency bias (favoring recently seen examples), and common token bias (favoring commonly occurring tokens in the pretraining distribution). However, their approach primarily addresses datasets with few and short tokenized labels, often assuming that class labels are composed of single token. For multi-token labels, they only consider the first token (e.g., Ab for the Abbreviation label in the TREC dataset (Voorhees and Tice, 2000)), overlooking the influence of the full label. Building on this idea, Fei et al. (2023) highlights new type of label bias (domain-label bias) and proposes domain-context calibration to mitigate it. This method adjusts for domain-specific jargon but still relies on single-token labels. Other recent calibration techniques similarly focus on single-token labels (Han et al., 2023; Jiang et al., 2023b; Zhou et al., 2024). To circumvent multi-token issues, authors modify datasets by reducing labels to single tokens, often avoiding subword tokenization through lowercasing. While such simplifications may work for some datasets, they are not universally feasible. Datasets like SST-5 (Socher et al., 2013) contain multi-token labels such as very positive and very negative, which become indistinguishable if only the first token (very) is used. Prior works replace such labels with single-token alternatives (e.g., very positive with great, and very negative with terrible). However, this approach fails for datasets with many or semantically rich labels (e.g., nuances in labels like card arrival and card delivery estimate from the Banking77 dataset (Casanueva et al., 2020) cannot be adequately represented by single token). Moreover, representing multi-token labels only by their first token overgeneralizes, as this token may carry probability mass associated with unrelated labels (e.g., Ab in TREC could correspond to Ability or About). To address these limitations, Milios et al. (2023) proposes method in which the model freely generates label potentially one that does not exist in the predefined set and then matches it to the most similar label using sentence similarity model (e.g., SBERT (Reimers and Gurevych, 2019)). While this avoids the tokenization issue and enables the use of LLMs for text classification with many (and multitoken) labels, it introduces potential errors from the similarity model, which may distort the evaluation of the LLM. Furthermore, calibration cannot be applied to the predictions, as this method relies on free-text generation rather than label probabilities. Motivated by the absence of standardized method that fully accounts for multi-token class labels, we first analyze previously overlooked type of label bias, which we term label length bias. To address this, we propose NCC, label bias mitigation method that explicitly considers the entire label during calibration. 2 Length Normalization Is Not Enough One plausible approach to solve the label length bias in LLMs is to normalize multi-token probabilities by label length (Murray and Chiang, 2018; Brown et al., 2020). By averaging token probabilities, longer labels are no longer penalized purely for their length. However, this introduces different artifact: in multi-token labels, the probability of the first token is often much lower than that of subsequent tokens, since later tokens are conditioned on the earlier ones and can become highly predictable (e.g., after generating Business &, the token Finance may have near-certain probability1). This conditioning inflates the normalized score of frequent or highly predictable label sequences, biasing predictions toward such labels regardless of their overall semantic alignment with the input. As shown in the bottom plot of Figure 2, predictions are now better distributed across label lengths, but this leads to over-prediction of labels with likely token continuations (e.g., Business & Finance). Therefore, without further correction, classification is skewed toward token sequences that the model expects to see, rather than labels that truly align with the input context."
        },
        {
            "title": "4 Normalized Contextual Calibration",
            "content": "In this section, we present NCC, label bias mitigation method which accounts for label length bias. When computing the probabilities of multi-token class labels in LLMs, it is essential to address the impact of label length bias. To mitigate it, we propose normalizing the probabilities of labels by their token count. Instead of the raw product of token probabilities, we calculate normalized probability that accounts for the label length (Jurafsky and Martin, 2000; Murray and Chiang, 2018): (y Ck, x) = n(cid:112)PM (y Ck, x) norm (2) This normalization transforms the raw probability into geometric mean, balancing the contribution of each token and removing the inherent bias against longer labels in LLMs. While there are different ways of normalizing the label length (e.g., average cross-entropy), we use the geometric mean of token probabilities because it allows for direct application of calibration (where all labels must sum to 1) and is easier to interpret, allowing for confidence reliability analysis (Section 8). 1Even more so if single word gets splitted into several tokens (e.g., Computers Compu + ters). Figure 2: Prediction frequency by label using zero-shot Llama 3.1 (8B) in the Yahoo dataset (Zhang et al., 2015)."
        },
        {
            "title": "3 Label Length Bias in LLMs",
            "content": "LLMs Are Susceptible to Length Bias LLMs are widely used for classification tasks by predicting over set of candidate labels, either in few-shot (e.g., via ICL) or zero-shot settings. Let be the set of possible class labels, where each label consists of one or more tokens. Given an input (and optionally context Ck of labeled examples), the model predicts the label by computing argmaxyL PM (y Ck, x). In practice, the probability of multi-token label is computed as the product of the conditional probabilities of its tokens (Bengio et al., 2003; Shannon, 1948): PM (y Ck, x) = (cid:89) PM (ti Ck, x, t1, . . . , ti1) i=1 (1) where consists of tokens t1, t2, . . . , tn. However, this formulation inherently penalizes longer labels, as each additional token multiplies the probability by number less than one. As result, longer labels tend to receive lower overall probabilities than shorter ones, regardless of their semantic appropriateness for the input. The top plot in Figure 2 demonstrates this phenomenon: without adjusting for length, the models output probabilities are biased toward shorter labels (e.g., Health and Sports), which are heavily over-predicted. We denote this as label length bias, newly-identified type of bias in LLM-based classification. Previous calibration works (e.g., Zhao et al., 2021; Fei et al., 2023) overlook this type of bias by focusing only on single-token labels and, thus, fail to address it effectively. 3 Dataset Llama 3.1 Mistral 7B Qwen2.5 GPT-J 1.500.87 AG News 1.400.49 SST-5 2.901.04 Yahoo 1.640.72 DBpedia 20 Newsgroups 2.751.22 2.521.62 TREC-50 3.511.21 Banking77 2.090.83 CLINC150 1.500.87 1.400.49 3.001.18 1.861.12 3.101.55 2.801.67 3.571.38 2.150.95 1.500.87 1.500.87 1.400.49 1.400.49 2.901.04 3.001.10 1.640.72 1.930.88 2.751.22 2.801.29 2.521.62 2.741.66 3.511.21 3.751.26 2.100.89 2.390. Table 1: Mean and standard deviation of label token lengths for each dataset and model. Once we have comparable label probabilities unbiased by length, we need to mitigate intrinsic label biases. Following prior works (Zhao et al., 2021; Fei et al., 2023), we use content-free input such as an empty string or neutral placeholder to probe the model and measure the baseline probability it assigns to each label in the absence of meaningful context. We then calibrate the models output probabilities for input as follows:"
        },
        {
            "title": "P calibrated",
            "content": "M (y Ck, x) ="
        },
        {
            "title": "P norm\nP baseline",
            "content": "M (y Ck, x) (y Ck, x) (3) The final predicted label with NCC is obtained by selecting the label with the highest calibrated probability: ˆyNCC = argmax yL"
        },
        {
            "title": "P calibrated",
            "content": "M (y Ck, x) (4) In summary, while normalization addresses biases related to label length, calibration is crucial to ensure that the models predictions are driven by the input context, rather than by prior biases toward well-known or frequent label sequences."
        },
        {
            "title": "5 Experimental Setup",
            "content": "Datasets We evaluate NCC on eight text classification datasets that feature multi-token class labels; cf. Table 1 (and Appendix for details). These datasets are widely used in prior studies on ICL (Zhao et al., 2021; Min et al., 2022; Fei et al., 2023; Milios et al., 2023; Bertsch et al., 2025). Table 1 confirms that all datasets contain multi-token labels for all models. to experiment with large set of models at reasonable computational cost, we mostly experiment with LLMs with 6B8B parameters. However, to demonstrate that NCC is also effective for larger models, we additionally evaluate it on the Llama 3.1 70B version (Grattafiori et al., 2024). Implementation Details Following prior work (Min et al., 2022; Fei et al., 2023), we employ simple and unified prompt templates for all datasets, avoiding task-specific instructions (e.g., explicit label lists) to minimize human engineering. We use = 5 few-shot examples to enable the models to identify task-specific patterns, which are selected randomly, picking an equal number of examples per class as much as possible. Each experiment is repeated five times with different random seeds to account for the sensitivity of ICL to example selection (Liu et al., 2022). Additionally, we evaluate NCC in the zero-shot setting (k = 0) to analyze its robustness without task demonstrations. The prompt formats are described in Appendix A. Inference is performed using NVIDIA A100 GPUs. To calibrate model biases (Eq. 3), we use an ensemble of content-free inputs, as proposed by Zhao et al. (2021). Specifically, we compute the probabilities for each label using five semantically neutral inputs (empty string), (single space), N/A, [MASK], and Lorem ipsum and average them to reduce variance. Evaluation We report the macro-F1, which accounts for class imbalance (see Table 6). To assess statistical significance, we apply the Wilcoxon signed-rank test (Wilcoxon, 1945). Baselines We compare NCC against four baselines: (1) standard raw probabilities (Raw Prob; Eq. 1), (2) raw probabilities with length normalization (Norm Prob; Eq. 2), (3) contextual calibration (CC; Zhao et al., 2021),2,3 and (4) Gen+SBERT (Milios et al., 2023), retrieval-based ICL approach that first retrieves the most similar fewshot examples, then lets the LLM freely generate label (not restricted to the label space), and finally Models We evaluate NCC on four LLMs from different families and of different sizes: Llama 3.1 (8B) (Grattafiori et al., 2024), Mistral 7B v0.3 (Jiang et al., 2023a), Qwen2.5 (7B) (Qwen et al., 2025), and GPT-J (6B) (Wang and Komatsuzaki, 2021). These models also differ in how they tokenize class labels  (Table 1)  . In order to be able 2We adopt CC as representative standard calibration baseline due to its simplicity and wide recognition in the literature. For completeness, we compare NCC against other relevant calibration techniques in Appendix D.3. 3For CC, we extend the original single-token implementation in Zhao et al. (2021) with Eq. 3, where the baseline probabilities are obtained from Eq. 1 with the single-token implementation, labels very positive and very negative would be treated as the same token (very). 4 Without length normalization, this method inherits the bias toward shorter labels (higher baseline ). During calibration (Eq. 3), this results in excessive penalization of short labels and overcompensation for longer ones, skewing predictions toward lengthy labels regardless of input relevance (details in Appendix E). This illustrates the necessity of normalizing multi-token probabilities to prevent calibration from amplifying label length bias. NCC underperforms the standard raw probability on AG News and SST-5, where the full label set (4 and 5 classes, respectively) is already represented in the few-shot examples (k = 5) and label semantics are simple. In such cases, NCC introduces unnecessary adjustments that degrade performance. This aligns with our zero-shot findings (below), where NCC becomes essential to mitigate label bias when the label space is not fully represented. Zero-shot Results We further evaluate NCC in zero-shot setting, where no demonstrations are provided. As shown in Figure 4, NCC maintains its advantage over all baselines across most datasets and models, mirroring trends from the few-shot scenario (detailed results in Appendix D.2). While all methods yield lower performance due to the lack of examples as expected, NCC shows the smallest relative performance difference between zeroand few-shot settings (36.8%; Figure 5), indicating that it is less dependent on labeled examples. In some cases, NCCs zero-shot performance even exceeds the few-shot raw baseline (e.g., GPT-J average F1: 42.4 vs. 38.7). This further demonstrates the effectiveness of NCCs label bias mitigation, as it allows models to leverage the unbiased label semantics and perform better on zero-shot than on standard few-shot learning from examples. Figure 4: Zero-shot performance of methods and models, averaged over all datasets. Numbers indicate absolute improvement of NCC over the second-best method. 5 Figure 3: Few-shot performance of methods and models, averaged over all datasets. Error bars indicate the standard deviation across runs. Numbers above the bars indicate absolute improvement of NCC over the secondbest method. matches the generated label to the closest candidate using sentence similarity model (e.g., SBERT)."
        },
        {
            "title": "6 Results",
            "content": "Few-shot Results Table 2 reports detailed results across models, datasets, and methods with = 5 examples, while Figure 3 summarizes overall trends. NCC consistently outperforms all baselines with statistically significant improvements, achieving absolute macro-F1 gains ranging from +6.9% to +8.8% over the second-best method (7.6% on average). The gains are most pronounced on datasets with longer and more complex labels, such as Banking77 and CLINC150, where mitigating label length bias is most critical. NCC also surpasses retrieval-based ICL approaches such as Gen+SBERT, which leverages similar few-shot examples and free-form label generation followed by similarity matching to candidate labels. Despite using randomly selected examples, NCC achieves higher performance, suggesting that properly calibrated label probabilities are more beneficial than retrieval similarity alone common approach in practical applications. Two model-specific trends emerge: (1) NCC consistently improves performance across all model sizes, including Llama 3.1 70B, which shows that even larger LLMs suffer from label bias and require calibration; (2) NCC narrows the performance gap between large and small models, indicating that proper calibration can make smaller models competitive (see Appendix D.1 for details). Notably, the standard CC method for multi-token labels (adapted from Zhao et al., 2021) achieves near-zero F1 on some datasets (e.g., TREC-50). Method AG News SST-5 Yahoo DBpedia 20 Newsgroups TREC-50 Banking77 CLINC150 Avg. Llama 3.1 (8B) Raw Prob Norm Prob CC Gen+SBERT NCC 83.86.2 85.3 1.8 84.02.3 80.45.1 78.05.0 42.3 61.32.7 83.40.7 9.1 60.62.2 38.66.3 80.04.7 8.13.5 23.97.4 1.00.0 41.25.3 64.31.1 67.33.9 38.28.1 65.13.5 92.5 0.7 41.03.4 43.33.2 5.36.1 40.52.1 67.7 0. Mistral 7B Raw Prob Norm Prob CC Gen+SBERT 81.26.4 83.1 2.8 81.51.7 81.92.3 40.77.7 33.46.1 21.13.8 40.9 52.93.2 56.23.2 7.43.7 7.3 64.31.4 NCC 76.84.1 34.77.3 62.12.0 67.75.8 62.63.1 1.00.0 62.03.3 91.2 1.6 39.14.1 32.57.1 4.03.5 39.23.1 60.4 4.2 Qwen2.5 (7B) Raw Prob Norm Prob CC Gen+SBERT NCC 74.64.7 83.1 2.4 49.25.9 74.63.3 71.73. 35.84.6 47.11.5 74.13.7 42.5 44.62.2 72.84.2 4.3 35.411.4 24.05.9 3.11.3 37.64.8 59.11.8 71.14.2 35.55.8 59.12.5 87.1 1.9 29.44.7 21.37.5 10.43.6 30.84.3 58.3 2.8 GPT-J (6B) Raw Prob Norm Prob CC Gen+SBERT NCC 67.31.5 74.8 3.2 70.718.0 66.00.7 67.71.5 25.73.9 30.612.5 70.83.1 43.514.8 25.44.6 65.06.2 12.44.8 12.56.9 1.00.0 25.02.0 46.414.8 63.51.5 24.43.5 46.55.8 90.6 0.8 17.29.6 10.39.4 4.74.5 18.09.4 47.8 4.6 Llama 3.1 (70B) 28.21.9 29.32.4 0.20.0 33.82.9 33.82.1 28.61.4 26.85.3 0.20.0 36.45.1 34.21. 25.12.4 19.15.2 0.20.0 33.85.1 34.62.9 24.01.7 14.55.3 0.20.0 28.2 3.0 37.84.6 36.33.5 0.10.0 47.53.1 59.5 0.9 34.53.9 32.95.0 0.10.0 46.91.5 57.9 1.1 23.65.5 18.34.6 0.10.0 41.14.1 52.3 2.4 29.82.7 32.42.3 0.10.0 43.11.0 21.71.1 43.21. Raw Prob Norm Prob CC Gen+SBERT 88.8 2.0 87.91.0 86.51.3 84.32.5 48.08.4 48.1 27.46.8 47.03.4 69.71.3 6.2 70.4 0.4 53.16.1 62.92.5 NCC 84.64. 37.83.9 65.01.3 80.83.1 80.12.8 21.510.7 62.53.9 98.2 0.3 43.24.4 50.32.9 0.40.0 47.34.7 73.5 1. 29.71.2 26.15.1 0.20.0 37.1 3.2 32.02.8 36.26.6 31.06.1 0.80.7 45.95.0 56.6 3.3 55.91.2 64.12.2 1.01.1 52.33.2 73.1 1. 53.11.6 61.92.6 1.40.9 49.13.1 64.8 1.9 55.62.0 63.83.4 2.71.4 54.53.4 71.1 2.4 44.50.9 51.01.5 1.40.6 48.61.7 57.4 3. 55.82.5 66.91.9 3.32.1 55.32.8 68.5 2.6 54.23.7 54.73.3 15.42.6 53.43.3 63.5 2.8 49.74.3 48.74.4 14.61.7 52.63.4 60.3 3. 45.73.6 45.74.2 15.63.7 50.33.9 58.7 3.1 38.74.5 39.65.9 12.94.3 42.34.3 49.9 2.8 56.53.7 57.63.3 24.13.4 55.33.5 64.5 2. Table 2: Few-shot performance (macro-F1std) of the conventional approach (Raw Prob; Brown et al. 2020), conventional approach with label length normalization (Norm Prob), contextual calibration (CC; Zhao et al. 2021), free generation plus similarity matching (Gen+SBERT; Milios et al. 2023), and normalized contextual calibration (NCC; Ours). * indicates significant dominance of (or over) NCC at < 0.05. Our findings align with those in Min et al. (2022), who emphasize that defining the label space is crucial for ICL performance. In the few-shot experiments where all possible labels are included in the context (e.g., AG News and SST-5 in Table 2), the model is already well-calibrated by the explicit definition of the label space, so NCC introduces unnecessary adjustments that degrade performance. In contrast, in the zero-shot setting, when the label space is only implicitly defined, NCC becomes essential to correct the models inherent biases and improve performance. This highlights NCCs role as necessary calibration step when the label space is incomplete or underspecified in the prompt. Figure 5: Average performance of all methods in the zero-shot and few-shot setting. Numbers indicate relative improvement when going from 0 to 5 shots."
        },
        {
            "title": "7 Generalizability of NCC to Other Tasks",
            "content": "Method OBQA CSQA QASC Avg. Another key contribution of NCC is its ability to extend the applicability of calibration techniques beyond classification tasks. Prior calibration methods (e.g., Zhao et al. (2021)) are limited to scenarios with single-token labels. By extending calibration with label length bias mitigation, NCC enables bias correction in previously inaccessible tasks. We demonstrate this capability through multiplechoice question answering (MCQA), task that requires selecting the correct option for question among set of choices. For this experiment, we adopt cloze prompt formulation (Brown et al., 2020), widely used MCQA approach that involves getting the model probabilities for each independent completion (choice), without showing the list of options to the model (detailed prompt can be found in Appendix A). Thus, the model does not know the label distribution4 and might exhibit label biases. Table 3 evaluates NCC on three diverse MCQA datasets. The results highlight the effectiveness of NCC in these tasks, achieving the highest average performance across all models. Unlike text classification, MCQA does not exhibit majority or recency biases (Zhao et al., 2021), as each example has its own unique set of candidate answers. Instead, the primary challenge lies in addressing common token and label length biases, which can distort the models probability estimates. While gains are smaller than in text classification likely due to the absence of majority and recency biases, diminishing the effect of calibration these consistent improvements demonstrate the potential of NCC to generalize beyond text classification. By enabling calibration techniques to handle multitoken labels (or options) effectively, NCC opens the door to applying these methods in wide array of tasks that require mitigation of label biases."
        },
        {
            "title": "8 Analysis",
            "content": "NCC Reduces Sensitivity to Example Selection Prior work has shown that ICL is highly sensitive to the choice and order of few-shot examples (Liu et al., 2022; Lu et al., 2022). To assess whether NCC reduces this sensitivity, we compare the average standard deviation and coefficient of variation of F1 scores across different random seeds for each modeldataset combination. As shown in Table 4, NCC exhibits the lowest variability, 4In MCQA, the labels are the different choices. Llama 3.1 (8B) 24.80.5 Raw Prob 32.81.0 Norm Prob 33.21.2 CC Gen+SBERT 29.81.2 55.71.4 57.31.2 34.70.8 45.70.5 51.34.4 51.64.1 22.82.0 43.91. 43.92.1 47.22.1 30.21.3 39.81.1 NCC 37.91.0 58.40.8 59.32.9 51.91.6 Mistral 7B 23.21.2 Raw Prob 33.71.4 Norm Prob 33.31.5 CC Gen+SBERT 29.31.0 57.50.9 56.91.4 31.10.9 47.81. 52.82.8 52.22.0 18.01.2 44.30.4 44.51.6 47.61.6 27.51.2 40.50.8 NCC 32.11.1 57.61.2 57.41.9 49.01.4 Qwen2.5 (7B) 25.71.0 59.50.7 Raw Prob 33.20.5 60.50.4 Norm Prob 36.41.3 33.60.8 CC 52.50.4 Gen+SBERT 31.41. 54.53.3 53.01.9 20.40.9 46.50.7 46.61.7 48.90.9 30.21.0 43.40.8 NCC 39.50.9 57.71.2 62.61.5 53.31.2 GPT-J (6B) 17.01.1 Raw Prob 23.01.2 Norm Prob 30.30.9 CC Gen+SBERT 23.90.6 45.32.0 44.51.3 24.80.7 38.21.4 37.82.0 38.31.4 13.40.4 38.01.2 33.41.7 35.31.3 22.80.7 33.31.1 NCC 26.31.5 46.90.6 43.51.4 38.91. Llama 3.1 (70B) 29.10.5 Raw Prob 37.41.1 Norm Prob 36.50.7 CC Gen+SBERT 33.11.8 61.61.8 63.51.9 37.22.0 51.21.0 60.01.6 58.41.1 25.40.5 49.90.9 50.21.3 53.11.4 33.01.1 44.81.2 NCC 44.41.7 64.11.8 68.10.7 58.81.4 Table 3: Performance (Macro F1) for the multiple choice QA task. Dataset abbreviations: OBQA (OpenBookQA; Mihaylov et al. 2018), CSQA (CommonsenseQA; Talmor et al. 2019), QASC (Question Answering in Context; Khot et al. 2020). indicating greater robustness and stability across different example sets. Calibration Level vs. Few-shot Examples We analyze how the number of few-shot examples (k) affects calibration by comparing NCC and the standard Raw Prob baseline across varying ks (equal to each datasets class count) on all datasets. We select examples randomly from the train set, smaller sets of examples are always subsets of larger sets, and we maintain fixed order to avoid permutation sensitivity (Lu et al., 2022). We measure the required calibration level using the Kullback-Leibler (KL) divergence between the raw and the calibrated (NCC) probability distributions, reflecting the divergence between them. Due to computational cost, 7 Figure 6: Performance and calibration level for different values, run with Llama 3.1 (8B). Method Std. Dev. Coef. Var. Raw Prob Norm Prob CC Gen+SBERT 0.039 0.042 0.034 0.038 0.103 0.134 0.303 0. NCC 0.031 0.058 Table 4: Average sensitivity (standard deviation and coefficient of variation) of F1 scores across different random example selections. we use Llama 3.1 (8B) with single random seed. Figure 6 shows that NCC generally outperforms Raw Prob, with the performance gap narrowing as increases. This trend is expected because fewer examples require higher calibration to correct biases, as the model has fewer data to learn the label space. The decreasing KL divergence with increasing confirms this, indicating that raw predictions become more aligned with calibrated probabilities. This suggests NCC is particularly effective in scenarios with limited few-shot examples, where calibration is crucial for mitigating label biases. Additionally, in settings with large number of classes (bottom row of Figure 6), very few examples (2 5) in NCC perform almost as good as the manyexamples (5077) raw ICL counterparts. This has significant implications for practical applications, as the number of examples can be drastically reduced while keeping comparable performance. NCC Enhances Reliability of Predictions Another crucial aspect when evaluating LLMs is considering how reliable its predictions are. This is measured by the alignment between model confidence (probability) and prediction accuracy (Guo et al., 2017). Figure 7 shows the reliability diaFigure 7: Reliability diagrams for all methods. The x-axis are bins of predicted confidence, and the y-axis shows the accuracy for the given bin. grams for the different methods, averaged across all datasets for the Llama 3.1 (8B) model. NCC is the most reliable method, showing the closest line to the perfect calibration and the lowest expected calibration error (ECE). This result demonstrates that NCC not only improves overall model performance, but also the reliability of the predictions. (Further analysis is conducted in Appendix E.)"
        },
        {
            "title": "9 Conclusion",
            "content": "In this work, we highlight the previously overlooked issue of multi-token label length bias in LLMs and introduce NCC as an effective solution. By normalizing probabilities and applying calibration with prior probabilities at the full-label level, NCC significantly improves performance over existing methods across diverse datasets and models. Our approach extends bias mitigation to settings such as multiple-choice question answering, demonstrating its versatility beyond classification tasks. Additionally, NCC reduces sensitivity to few-shot example selection, requires fewer exam8 ples to perform competitively, and produces more reliable confidence estimates. Our findings underscore the importance of addressing full-label biases to enhance the performance and reliability of LLMbased systems, especially in real-world scenarios where class labels often consist of multiple tokens."
        },
        {
            "title": "Limitations",
            "content": "While NCC demonstrates strong performance and generalizability across various tasks, several limitations should be considered. First, the applicability of NCC is constrained by the requirement to access token-level probabilities for all possible class labels. This restricts its use to open-source models that can be run locally, as many proprietary or cloud-based models do not provide full token probability distributions. Some APIs (e.g., OpenAIs logprobs parameter) offer token probabilities but are typically limited to the top-20 tokens, making NCC infeasible for tasks requiring complete probability distributions. Second, NCC is not suitable for open-ended tasks where the label space is not predefined. Since the method relies on calibrating prior biases across fixed set of labels, it cannot be directly applied to tasks with an unrestricted output space, such as open-ended text generation. Third, NCC is not designed for full-sentence generation tasks. Calibration is most effective when applied to tasks where the label itself carries minimal semantic meaning (e.g., class labels in classification, or sentence continuations in multiple-choice question answering). In contrast, full-sentence responses inherently carry substantial baseline probability due to their coherence and fluency, regardless of calibration. Applying NCC to meaningful text generation would likely penalize fluent responses rather than correct systematic biases. Despite these limitations, NCC remains valuable tool for mitigating label bias and improving LLM performance in settings where class labels are well-defined."
        },
        {
            "title": "Ethics Statement",
            "content": "advice, without appropriate human oversight and domain-specific validation."
        },
        {
            "title": "Acknowledgments",
            "content": "This work was supported by the Carl Zeiss Foundation through the MAINCE project (grant number P2022-08-009)."
        },
        {
            "title": "References",
            "content": "Yoshua Bengio, Réjean Ducharme, Pascal Vincent, and Christian Jauvin. 2003. neural probabilistic lanJournal of Machine Learning Reguage model. search, 3:11371155. Amanda Bertsch, Maor Ivgi, Emily Xiao, Uri Alon, Jonathan Berant, Matthew R. Gormley, and Graham Neubig. 2025. In-context learning with long-context models: An in-depth exploration. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 1211912149, Albuquerque, New Mexico. Association for Computational Linguistics. Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu, Clemens Winter, and 12 others. 2020. Language models are few-shot learners. In Advances in Neural Information Processing Systems, volume 33, pages 18771901. Curran Associates, Inc. Iñigo Casanueva, Tadas Temˇcinas, Daniela Gerz, Matthew Henderson, and Ivan Vulic. 2020. Efficient intent detection with dual sentence encoders. In Proceedings of the 2nd Workshop on Natural Language Processing for Conversational AI, pages 3845, Online. Association for Computational Linguistics. Yu Fei, Yifan Hou, Zeming Chen, and Antoine Bosselut. 2023. Mitigating label biases for in-context learning. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1401414031, Toronto, Canada. Association for Computational Linguistics. Our work demonstrates that NCC significantly improves LLM performance for text classification and related NLP tasks by mitigating label biases. However, these improvements do not fully eliminate inherent risks, and LLMs remain susceptible to errors. Therefore, we caution against relying solely on LLMs in critical settings, such as medical Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad AlDahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, Amy Yang, Angela Fan, Anirudh Goyal, Anthony Hartshorn, Aobo Yang, Archi Mitra, Archie Sravankumar, Artem Korenev, Arthur Hinsvark, and 542 others. 2024. The Llama 3 Herd of Models. Preprint, arXiv:2407.21783. 9 Chuan Guo, Geoff Pleiss, Yu Sun, and Kilian Q. Weinberger. 2017. On calibration of modern neural networks. In Proceedings of the 34th International Conference on Machine Learning, volume 70 of Proceedings of Machine Learning Research, pages 1321 1330. PMLR. Zhixiong Han, Yaru Hao, Li Dong, Yutao Sun, and Furu Wei. 2023. Prototypical calibration for fewshot learning of language models. In The Eleventh International Conference on Learning Representations. Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux, Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix, and William El Sayed. 2023a. Mistral 7b. Preprint, arXiv:2310.06825. Zhongtao Jiang, Yuanzhe Zhang, Cao Liu, Jun Zhao, and Kang Liu. 2023b. Generative calibration for in-context learning. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 23122333, Singapore. Association for Computational Linguistics. Daniel Jurafsky and James H. Martin. 2000. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition. Prentice Hall. Tushar Khot, Peter Clark, Michal Guerquin, Peter Jansen, and Ashish Sabharwal. 2020. Qasc: dataset for question answering via sentence composition. Proceedings of the AAAI Conference on Artificial Intelligence, 34(05):80828090. Ken Lang. 1995. Newsweeder: learning to filter netnews. In Proceedings of the Twelfth International Conference on International Conference on Machine Learning, ICML95, page 331339, San Francisco, CA, USA. Morgan Kaufmann Publishers Inc. Stefan Larson, Anish Mahendran, Joseph J. Peper, Christopher Clarke, Andrew Lee, Parker Hill, Jonathan K. Kummerfeld, Kevin Leach, Michael A. Laurenzano, Lingjia Tang, and Jason Mars. 2019. An evaluation dataset for intent classification and out-ofscope prediction. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 13111316, Hong Kong, China. Association for Computational Linguistics. Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2022. What makes good in-context examples for GPT-3? In Proceedings of Deep Learning Inside Out (DeeLIO 2022): The 3rd Workshop on Knowledge Extraction and Integration for Deep Learning Architectures, pages 100114, Dublin, Ireland and Online. Association for Computational Linguistics. Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2022. Fantastically ordered prompts and where to find them: Overcoming fewshot prompt order sensitivity. In Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 80868098, Dublin, Ireland. Association for Computational Linguistics. Todor Mihaylov, Peter Clark, Tushar Khot, and Ashish Sabharwal. 2018. Can suit of armor conduct electricity? new dataset for open book question answering. In Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing, pages 23812391, Brussels, Belgium. Association for Computational Linguistics. Aristides Milios, Siva Reddy, and Dzmitry Bahdanau. 2023. In-context learning for text classification with many labels. In Proceedings of the 1st GenBench Workshop on (Benchmarking) Generalisation in NLP, pages 173184, Singapore. Association for Computational Linguistics. Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. 2022. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 1104811064, Abu Dhabi, United Arab Emirates. Association for Computational Linguistics. Kenton Murray and David Chiang. 2018. Correcting length bias in neural machine translation. In Proceedings of the Third Conference on Machine Translation: Research Papers, pages 212223, Brussels, Belgium. Association for Computational Linguistics. Qwen, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, and 24 others. 2025. Qwen2.5 technical report. Preprint, arXiv:2412.15115. Nils Reimers and Iryna Gurevych. 2019. SentenceBERT: Sentence embeddings using Siamese BERTnetworks. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 39823992, Hong Kong, China. Association for Computational Linguistics. Hossain Shaikh Saadi, Minh Duc Bui, Mario SanzGuerrero, and Katharina Von Der Wense. 2025. JGU Mainzs submission to the WMT25 shared task on LLMs with limited resources for Slavic languages: MT and QA. In Proceedings of the Tenth Conference on Machine Translation, pages 11511157, Suzhou, China. Association for Computational Linguistics. Mario Sanz-Guerrero, Minh Duc Bui, and Katharina von der Wense. 2025. Mind the gap: closer look at 10 tokenization for multiple-choice question answering with LLMs. In Proceedings of the 2025 Conference on Empirical Methods in Natural Language Processing, pages 1958419594, Suzhou, China. Association for Computational Linguistics. Mario Sanz-Guerrero and Katharina von der Wense. 2025. Corrective in-context learning: Evaluating In The self-correction in large language models. Sixth Workshop on Insights from Negative Results in NLP, pages 2433, Albuquerque, New Mexico. Association for Computational Linguistics. Stephanie Schoch and Yangfeng Ji. 2025. In-context learning (and unlearning) of length biases. In Proceedings of the 2025 Conference of the Nations of the Americas Chapter of the Association for Computational Linguistics: Human Language Technologies (Volume 1: Long Papers), pages 76337671, Albuquerque, New Mexico. Association for Computational Linguistics. Sagi Shaier, Mario Sanz-Guerrero, and Katharina von der Wense. 2025. Asking again and again: Exploring LLM robustness to repeated questions. Preprint, arXiv:2412.07923. C. E. Shannon. 1948. mathematical theory of communication. The Bell System Technical Journal, 27(3):379423. Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. 2013. Recursive deep models for semantic compositionality over sentiment treebank. In Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing, pages 16311642, Seattle, Washington, USA. Association for Computational Linguistics. Alon Talmor, Jonathan Herzig, Nicholas Lourie, and Jonathan Berant. 2019. CommonsenseQA: question answering challenge targeting commonsense knowledge. In Proceedings of the 2019 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers), pages 41494158, Minneapolis, Minnesota. Association for Computational Linguistics. Ellen M. Voorhees and Dawn M. Tice. 2000. Building question answering test collection. In Proceedings of the 23rd Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, SIGIR 00, page 200207, New York, NY, USA. Association for Computing Machinery. Ben Wang and Aran Komatsuzaki. 2021. GPT-J6B: 6 Billion Parameter Autoregressive Language Model. https://github.com/kingoflolz/ mesh-transformer-jax. Xiang Zhang, Junbo Zhao, and Yann LeCun. 2015. Character-level convolutional networks for text classification. In Advances in Neural Information Processing Systems, volume 28. Curran Associates, Inc. Zihao Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021. Calibrate before use: Improving few-shot performance of language models. In Proceedings of the 38th International Conference on Machine Learning, volume 139 of Proceedings of Machine Learning Research, pages 1269712706. PMLR. Han Zhou, Xingchen Wan, Lev Proleev, Diana Mincu, Jilin Chen, Katherine Heller, and Subhrajit Roy. 2024. Batch calibration: Rethinking calibration for in-context learning and prompt engineering. In The Twelfth International Conference on Learning Representations."
        },
        {
            "title": "A Prompt Formats",
            "content": "For the prompt formats, we follow previous work and employ simple and unified prompt templates for both text classification and question answering tasks (Min et al., 2022; Fei et al., 2023; SanzGuerrero and von der Wense, 2025; Shaier et al., 2025; Saadi et al., 2025). The prompts end with Label: (for text classification tasks; Figure 8) and Answer: (for question answering tasks; Figure 9), and we extract the models output probabilities for each candidate label/answer starting from that position. Following Sanz-Guerrero et al. (2025), the first token of each label/answer is prefixed with space to align with the models default tokenization, as this has been shown to improve performance and model calibration."
        },
        {
            "title": "Text Classification Prompt",
            "content": "Text: {example_1} Label: {ground_truth_1} Text: {example_2} Label: {ground_truth_2} ... Text: {example_k} Label: {ground_truth_k} Text: {input_text} Label: Figure 8: Prompt format for text classification tasks."
        },
        {
            "title": "B Illustration of the Importance of NCC",
            "content": "Frank Wilcoxon. 1945. Individual comparisons by ranking methods. Biometrics Bulletin, 1(6):8083. To illustrate the necessity of NCC for handling multi-token labels, we revisit the toy example il-"
        },
        {
            "title": "Question Answering Prompt",
            "content": "Class Label Raw Norm CC NCC Question: {example_1} Answer: {ground_truth_1} Question: {example_2} Answer: {ground_truth_2} ... Question: {example_k} Answer: {ground_truth_k} Question: {input_text} Answer: Figure 9: Prompt format for question answering tasks. lustrated in Figure 1. Consider sentiment classification task similar to the SST-5 dataset, where the five class labels are: very positive, positive, neutral, negative, and very negative. To highlight the impact of calibration, we introduce sixth, naive class label verbalized as: It was the best of times, it was the worst of times, it was the age of wisdom, it was the age of foolishness, it was the epoch of belief, it was the epoch of incredulity, it was the season of light, it was the season of darkness, it was the spring of hope, it was the winter of despair. taken from Tale of Two Cities by Charles Dickens. This label is intentionally much longer than the other labels and consists of well-known sequence of words, making it strong candidate for unintended biases. For simplicity, we use the Llama 3.1 (8B) model with simple few-shot prompt: Text: This is good product, and Id probably recommend it to others. Label: positive Text: The presentation was informative but not particularly engaging. Label: neutral Text: The service was slow, and the food was cold. Label: negative This Text: changed my life for the better! Label: product has completely The model must predict the sentiment of the last sentence, whose ground-truth label is very positive. Table 5 presents the probabilities assigned to each class by the different methods."
        },
        {
            "title": "The standard raw probability approach assigns a\nreasonable probability distribution but incorrectly",
            "content": "12 very positive 0.13 0.86 positive neutral 2e-3 negative 3e-3 5e-5 very negative it was the best of times, [...] 1e-14 0.20 0.39 1e-3 1e-3 4e-3 0.41 1e-9 2e-12 7e-15 3e-14 8e-12 0.99 0.81 0.14 4e-4 1e-3 9e-3 0.04 Table 5: Probabilities assigned to each class in the toy example. Bold values indicate the highest probability (i.e., the predicted class). predicts positive instead of very positive. This happens because very positive consists of two tokens (very and positive), meaning its probability is the product of individual token probabilities, making it disproportionately lower. Length normalization (Norm Prob) mitigates this length bias by normalizing multi-token probabilities. However, it assigns high probability to the naive class label due to the high compound probability of its familiar token sequence, despite its irrelevance to the task. CC (contextual calibration; Zhao et al. 2021) attempts to correct label biases but fails in this case. Since the standard raw method assigns very low prior probability to the naive (longest) label, CC overcompensates and amplifies its probability, leading to incorrect predictions. NCC (normalized contextual calibration; ours) successfully mitigates both length bias and overcompensation. By normalizing probabilities before calibration, it ensures that class probabilities are fairly adjusted, leading to more accurate prediction of very positive. This example highlights the importance of properly handling full-label probabilities, particularly when dealing with multi-token labels, to ensure reliable and unbiased predictions of LLMs."
        },
        {
            "title": "C Dataset Details",
            "content": "We evaluate NCC using eight text classification datasets and three multiple-choice QA datasets, spanning diverse domains and label complexities. Table 6 summarizes key characteristics including class count, balance status, test set size, and example labels. C."
        },
        {
            "title": "20 Newsgroups Dataset",
            "content": "While widely adopted in traditional NLP research, the 20 Newsgroups dataset (Lang, 1995) has been largely overlooked in recent LLM-based classification studies due to its non-descriptive class labels Dataset # Classes Balanced Test Class Labels Text Classification AG News (Zhang et al., 2015) SST-5 (Socher et al., 2013) Yahoo (Zhang et al., 2015) DBpedia (Zhang et al., 2015) 20 Newsgroups (Lang, 1995) TREC-50 (Voorhees and Tice, 2000) Banking77 (Casanueva et al., 2020) CLINC150 (Larson et al., 2019) Multiple Choice QA OpenBookQA (Mihaylov et al., 2018) CommonsenseQA (Talmor et al., 2019) QASC (Khot et al., 2020) 4 5 10 14 20 77 150 4 5 8 500 1, 500 500 500 500 924 3,000 World, Sports, Business, Science and Technology very positive, positive, neutral, negative, very negative Topics like Society, Science, Health, Education, and others. Types of entities, such as Company, Educational Institution, Artist, and more. Categories are shown in Table 7. Fine-grained question types like Invention, book and other creative piece, Description of person, and Distance, linear measure. Intents such as card activation, transaction failed, account blocked, and more. Intents in domains like banking, travel, and work, such as transfer money, book flight, and schedule meeting. N/A N/A N/A 500 N/A 1,221 N/A N/A 926 Table 6: Full dataset information. (e.g., sci.med). In traditional methods, such as bag-of-words models, class label wording was irrelevant since these approaches relied solely on feature representations of the input text. However, LLMs benefit from more representative and meaningful labels, as they utilize the wording of class labels during inference. To address this, we remap the original categories as shown in Table 7 to enhance interpretability and facilitate more effective use of this dataset with LLMs. Original Class Label New Class Label Atheism & Secularism Computer Graphics alt.atheism comp.graphics comp.os.ms-windows.misc Windows OS comp.sys.ibm.pc.hardware comp.sys.mac.hardware comp.windows.x misc.forsale rec.autos rec.motorcycles rec.sport.baseball rec.sport.hockey sci.crypt sci.electronics sci.med sci.space soc.religion.christian talk.politics.guns talk.politics.mideast talk.politics.misc talk.religion.misc IBM PC Hardware Macintosh Hardware Window System Classified Ads Automobiles Motorcycles Baseball Hockey Cryptography Electronics Medicine Space & Astronomy Christianity Gun Politics Middle East Politics General Politics General Religion"
        },
        {
            "title": "D Detailed Results",
            "content": "D.1 Model Size Comparison In the main text, we demonstrate the effectiveness of NCC across different LLMs of different sizes (Section 6). To further evaluate the impact of model size on NCC performance, we stick with the Llama 3.1 model and compare its performance with different sizes (8B, 70B) on the text classification task. Figure 10 shows the average performance and relative improvement from the 8B version to the 70B version of Llama 3.1 for each method under the same settings. NCC enables the smaller model to achieve the most similar performance to the larger model, with only 1.6% gap in average F1, despite being almost nine times smaller. This is significant drop in performance gap, compared to the 4.3% difference across model sizes in the standard raw probability approach. This result highlights the effectiveness of NCC in enhancing the performance of smaller models, enabling them to achieve comparable results to larger models in text classification when properly calibrated. D.2 Zero-Shot Results Table 7: Mapping of the original 20 Newsgroups class labels to more representative and descriptive labels for use with LLMs. Table 8 provides the full results for all methods on each dataset and model in the zero-shot setting (corresponding to Figure 4 in the main text). 13 Method AG News SST-5 Yahoo DBpedia 20 Newsgroups TREC-50 Banking77 CLINC150 Avg. Raw Prob Norm Prob CC Gen+SBERT NCC Raw Prob Norm Prob CC Gen+SBERT NCC Raw Prob Norm Prob CC Gen+SBERT 58.5 17.7 10.0 56.2 57.8 54.8 31.6 10.0 43.5 48. 54.3 46.5 10.0 38.0 Llama 3.1 (8B) 20.6 14.0 16.4 24.0 16.2 36.0 11.6 45.9 36.8 57.6 1.0 44.7 23. 52.6 67.4 40.8 18.7 5.7 29.1 57.8 Mistral 7B 14.0 10.8 15.8 21. 10.3 13.3 11.0 26.3 30.2 5.2 1.0 34.7 26.4 41.9 44.0 39.5 23.1 0.4 6. 48.9 Qwen2.5 (7B) 22.7 31.1 12.9 16.2 18.3 34.0 20.7 31.6 49.2 65.9 1.0 38.8 32.9 22.2 1.3 19. 50.3 GPT-J (6B) NCC 57.7 23.5 45. 69.1 Raw Prob Norm Prob CC Gen+SBERT 53.3 26.2 10.0 53.3 16.9 13.8 10.6 21.3 8.3 14.2 1.8 38.7 42.4 52.2 1.0 47. NCC 59.8 21.0 47.0 52.8 34.1 17.2 3.0 18. 45.8 Llama 3.1 (70B) Raw Prob Norm Prob CC Gen+SBERT 57.5 22.1 10.0 54.8 22.2 30.2 16.2 24.0 15.9 37.4 22.2 44. 40.3 56.4 1.0 45.1 NCC 64.3 32.5 51.9 62. 43.1 26.0 5.7 28.4 58.0 12.5 3.5 0.2 20.2 19.9 12.9 5.6 0.2 13.5 15. 20.3 7.6 0.2 19.2 29.2 15.8 15.2 0.2 19.1 25.0 14.9 13.4 0.2 18.5 20. 34.2 26.5 0.0 47.2 41.5 31.6 27.8 0.0 25.2 30.5 29.0 32.1 0.0 36.4 36. 27.8 24.1 0.0 37.9 37.4 33.5 22.9 0.0 39.1 38.0 25.4 25.4 0.7 55.9 53. 14.5 16.8 0.5 18.1 30.6 24.9 5.7 40.4 46.7 26.0 16.8 4.9 23.7 36.6 36. 14.5 28.5 0.6 33.1 30.2 33.5 5.8 29.1 40.8 44.1 26.1 22.8 0.9 38.5 28.1 23.2 3.4 34. 50.4 42.4 28.7 37.8 0.6 51.6 32.0 30.8 7.0 38.3 53.3 47. Table 8: Performance comparisons (macro-F1) for the zero-shot setting. There is no std because there are no examples, resulting in consistent performance across runs. D.3 Comparison with Other Calibration"
        },
        {
            "title": "Methods",
            "content": "In the main text, we compare NCC with contextual calibration (CC; Zhao et al. 2021) as representative standard calibration method, due to its simplicity and wide recognition in the community. However, there are several other relevant calibration methods proposed in recent literature. Here, we extend the comparison to include domain-context calibration (DC; Fei et al. 2023), generative calibration (GC; Jiang et al. 2023b), and batch calibration (BC; Zhou et al. 2024). For these methods, we follow the original papers implementations (e.g., using random contexts from the training set for DC, and = 100 generated inputs from the model for GC to estimate priors). Moreover, we apply our normalized calibration approach to these methods, resulting in normalized domain14 Figure 10: Average performance of all methods with Llama 3.1 (8B) and Llama 3.1 (70B). Numbers indicate relative improvements when scaling from 8B to 70B. Method AG News SST-5 Yahoo DBpedia 20 Newsgroups TREC-50 Banking77 CLINC150 Avg. Raw Prob Norm Prob Gen+SBERT CC DC GC BC NCC NDC NGC NBC 83.8 85.3 80.4 84.0 80.6 86.1 86.9 78.0 83.8 83.6 83. 42.3 38.6 41.2 23.9 23.5 18.8 33.2 38.2 44.2 44.6 49.5 61.3 60.6 64.3 8.1 25.2 47.4 49.9 65.1 63.6 61.2 64. No Calibration 83.4 80.0 67.3 41.0 43.3 40.5 Standard Calibration 1.0 1.0 15.2 1.0 5.3 9.0 6.5 0. Normalized Calibration 92.5 92.3 90.8 92.0 67.7 68.8 67.7 69.8 28.2 29.3 33.8 0.2 0.2 0.2 0.2 33.8 32.2 22.4 32. 37.8 36.3 47.5 0.1 0.1 0.1 2.8 59.5 64.4 56.9 61.1 55.9 64.1 52.3 1.0 2.1 3.3 3.4 73.1 75.5 69.2 73. 54.2 54.7 53.4 15.4 17.7 22.2 22.2 63.5 65.6 62.1 65.9 Table 9: Comparison of different calibration methods with Llama 3.1 (8B). CC = contextual calibration (Zhao et al., 2021); DC = domain-context calibration (Fei et al., 2023); GC = generative calibration (Jiang et al., 2023b); BC = batch calibration (Zhou et al., 2024); NCC = normalized contextual calibration; NDC = normalized domain-context calibration; NGC = normalized generative calibration; NBC = normalized batch calibration. context calibration (NDC), normalized generative calibration (NGC), and normalized batch calibration (NBC). Given the computational cost of evaluating all methods across multiple models and datasets, we limit this comparison to the Llama 3.1 (8B) model on the text classification datasets. The results are shown in Table 9. Overall, for standard calibration, DC, GC, and BC generally improve over CC on datasets where label length bias is limited (e.g., AG News, SST-5, Yahoo), but they still struggle when multi-token label bias is strong (e.g., DBpedia, 20 Newsgroups, TREC50, Banking77, CLINC150), where NCC consistently performs best. For normalized calibration, augmenting these methods with length normalization (NDC/NGC/NBC) can yield further gains over NCC, but these improvements are smaller than the gaps between NCC and CC/DC/GC/BC, indicating that the main performance boost comes from combining some form of semantic calibration with length normalization."
        },
        {
            "title": "E Reliability of the LLMs Confidences",
            "content": "As shown in Section 8, NCC improves the reliability of model confidence scores. Here, we extend that analysis by examining how often labels are predicted based on their length (left histograms) and how prediction confidences are distributed (right histograms). We also report the enrichment factor, which quantifies how much more frequently certain labels are predicted compared to the ground truth distribution. Label lengths are normalized within each dataset, such that the shortest label corresponds to 0%, the longest to 100%, and intermediate labels are scaled proportionally. This analysis is conducted using the Llama 3.1 (8B) model across all classification datasets from the main text. The results, summarized in Figure 11, further highlight the label length bias problem: the raw probabilities from the LLM strongly favor shorter labels, with an enrichment factor exceeding 3, and predicts them with high confidence (near certainty in over 30% of cases). Length normalization (Norm Prob) reduces this bias by flattening probabilities across labels of different lengths, producing confidence distribution skewed toward lower values (i.e., most predictions fall within the 10%50% confidence range). However, as discussed in Section 3, it introduces new issue: later tokens in longer labels become disproportionately likely, resulting in an over-prediction of long labels. Similarly, standard CC (without length normalization) overcompensates for the length bias, favoring longer labels instead, as analyzed in Appendix B. In contrast, NCC produces label predictions that closely match the true distribution across label lengths, effectively mitigating both shortand longlabel biases. Additionally, its prediction confidence is more evenly distributed and more closely resembles well-calibrated model, reflected by the lowest ECE in the top plot of Figure 11. Multi-token (Original) Single-token (Remapping) very positive positive neutral negative very negative great good neutral bad terrible Table 10: Mapping of the original SST-5 multi-token class labels to single token alternatives, as done in prior work. Multi-Token vs. Single-Token Labels In the main text of the paper, we demonstrate improved label bias mitigation by applying NCC, calibration technique at the full-label level to solve the identified label length bias when working with multi-token labels. Previous works do not consider this type of bias, as they focus on single-token labels by remapping original multi-token labels to single-token alternatives (Zhao et al., 2021; Fei et al., 2023; Han et al., 2023; Jiang et al., 2023b; Zhou et al., 2024). For instance, in sentiment analysis tasks, labels like very positive and very negative are often replaced by great and terrible, respectively. These label remappings are not always feasible, and it is ambiguous to determine when label can be represented by single token without the risk of losing semantic meaning. For example, in the Banking77 dataset (Casanueva et al., 2020) labels like balance not updated after cheque or cash deposit and top up failed contain nuances hardly distinguishable by means of an alternative single-token label. However, one question that remains open is whether, when possible, considering single-token alternatives yields better results than the original multi-token labels. To address this, we conduct an additional analysis comparing the performance of the original multi-token labels against the widely adopted remappings of the SST-5 dataset (Socher et al., 2013). We consider the original labels and the single-token alternatives presented in Table 10. We run this analysis with the Llama 3.1 (8B) model in both zeroand few-shot settings. As in our main methodology, the few-shot setting is run with 5 different random seeds. Figure 12 compares the standard raw probability approach, probability with length normalization, and NCC when using either the original multitoken labels or simplified single-token alternatives. In the standard Raw Prob setting, multi-token labels are slightly favored, though the difference re16 Figure 11: Reliability diagrams and frequency of label prediction by label length (left) and confidence (right). (a) Zero-shot (k = 0). (b) Few-shot (k = 5). Figure 12: Performance comparison of original multitoken labels against single-token alternatives. mains within the error margins. notable discrepancy emerges in the zero-shot setting with the Norm Prob method, where multi-token labels containing very X5 are over-predicted. This occurs because the token following very receives higher conditional probability than when predicted alone, leading to inflated overall label scores problem also discussed in Section 3. In contrast, NCC performs consistently well across both label types, demonstrating its ability to mitigate this bias. These results indicate that retaining the original multi-token labels does not hinder performance and avoids potential semantic loss introduced by label simplification. 5Where denotes either positive or negative."
        }
    ],
    "affiliations": [
        "Johannes Gutenberg University Mainz, Germany",
        "University of Colorado Boulder, USA"
    ]
}