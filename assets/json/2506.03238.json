{
    "paper_title": "Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric Approach",
    "authors": [
        "Ziheng Zhao",
        "Lisong Dai",
        "Ya Zhang",
        "Yanfeng Wang",
        "Weidi Xie"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Automated interpretation of CT images-particularly localizing and describing abnormal findings across multi-plane and whole-body scans-remains a significant challenge in clinical radiology. This work aims to address this challenge through four key contributions: (i) On taxonomy, we collaborate with senior radiologists to propose a comprehensive hierarchical classification system, with 404 representative abnormal findings across all body regions; (ii) On data, we contribute a dataset containing over 14.5K CT images from multiple planes and all human body regions, and meticulously provide grounding annotations for over 19K abnormalities, each linked to the detailed description and cast into the taxonomy; (iii) On model development, we propose OminiAbnorm-CT, which can automatically ground and describe abnormal findings on multi-plane and whole-body CT images based on text queries, while also allowing flexible interaction through visual prompts; (iv) On benchmarks, we establish three representative evaluation tasks based on real clinical scenarios. Through extensive experiments, we show that OminiAbnorm-CT can significantly outperform existing methods on all the tasks and metrics."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 ] . e [ 1 8 3 2 3 0 . 6 0 5 2 : r Rethinking Whole-Body CT Image Interpretation: An Abnormality-Centric Approach Ziheng Zhao1,2, Lisong Dai3, Ya Zhang1,2, Yanfeng Wang1,2, Weidi Xie1,2 1School of Artificial Intelligence, Shanghai Jiao Tong University 2Shanghai Artificial Intelligence Laboratory 3Department of Radiology, Renmin Hospital of Wuhan University https://github.com/zhaoziheng/OminiAbnorm-CT"
        },
        {
            "title": "Abstract",
            "content": "Automated interpretation of CT imagesparticularly localizing and describing abnormal findings across multi-plane and whole-body scansremains significant challenge in clinical radiology. This work aims to address this challenge through four key contributions: (i) On taxonomy, we collaborate with senior radiologists to propose comprehensive hierarchical classification system, with 404 representative abnormal findings across all body regions; (ii) On data, we contribute dataset containing over 14.5K CT images from multiple planes and all human body regions, and meticulously provide grounding annotations for over 19K abnormalities, each linked to the detailed description and cast into the taxonomy; (iii) On model development, we propose OminiAbnorm-CT, which can automatically ground and describe abnormal findings on multi-plane and whole-body CT images based on text queries, while also allowing flexible interaction through visual prompts; (iv) On benchmarks, we establish three representative evaluation tasks based on real clinical scenarios. Through extensive experiments, we show that OminiAbnorm-CT can significantly outperform existing methods on all the tasks and metrics."
        },
        {
            "title": "Introduction",
            "content": "Computed Tomography (CT) imaging has become cornerstone of modern medicine, with over 3 billion scans performed annually around the world. The interpretation of these scansparticularly the generation of radiology reportsplays crucial role in guiding clinical decision-making. However, systematically identifying and characterizing abnormal findings in whole-body CT images remains cognitively demanding and time-consuming task for radiologists [48]. In the recent literature, artificial intelligence has started to make significant progress in CT image interpretation, particularly along two directions: first, AI models have been developed for organlevel segmentation [52, 28], that enable precise anatomical labeling and organ-grounded report generation [60, 12]. Yet critical gap remains: segmenting organs alone is insufficient for clinical utility. What ultimately matters to clinicians are abnormal findingslesions and any anomalies that inform diagnosis and treatment; second, models for automatic report generation have been trained on the recently introduced datasets with paired CT scans and corresponding radiology reports, describing clinical findings and impressions [10, 19]. However, these datasets are typically limited to specific anatomical regionsmost commonly chest CTand often lack explicit visual grounding, which reduces explainability and increases the risk of hallucinated content. To address the above-mentioned Equal contributions Corresponding author Preprint. Under review. Figure 1: OminiAbnorm-CT-14K is the first large-scale dataset for abnormal findings grounding and description across multi-plane and whole-body CT images. It covers 349 representative abnormal findings from 82 anatomical structures and 40 major systems or organs. challenges, we aim to explore the potential to develop system that automatically detects, localizes, and describes all abnormal findings across multi-plane, whole-body CT images. This paradigm shift moves beyond regional reporting and anatomical segmentation, aligning AI interpretation more closely with the practical demands of radiological diagnosis. One crucial barrier to developing such system is the absence of unified, clinically meaningful taxonomy of abnormalities, which presents major challenge for defining task scope and establishing continuous benchmarks. In this paper, we collaborated with 7 radiologists from 3 centers (each with 10-16 years of experience) to construct detailed hierarchical taxonomy covering 404 representative abnormal findings, that are organized across 40 major anatomical regions and 82 sub-regions. Building on this foundation, we introduce OminiAbnorm-CT-14K-the first large-scale dataset designed for abnormality grounding and description on multi-plane whole-body CT imaging. It comprises 14.5K CT images from Radiopedia [1], covering axial, coronal, and sagittal planes and diverse anatomical regions. All images and the paired reports have been rigorously reviewed by experienced radiologists. We invite 4 radiologists (with at least 7 years experience) to manually annotate around 19K abnormal findings on the images in the format of either bounding box or segmentation masks. All regional annotations are linked to the corresponding report descriptions, and further cast into our proposed taxonomy. These annotations cover 349 out of the 404 representative abnormal findings (86%) defined in our taxonomy across whole-body CT images. This dataset addresses critical gap in the field, by providing high-quality, clinically relevant annotations at scaleovercoming limitations in existing datasets caused by privacy constraints and the high cost of expert annotation. All our annotations will be released to the community, while the images are accessible for research purposes after applying and paying the required fee to Radiopaedia3. Leveraging OminiAbnorm-CT-14K, we develop OminiAbnorm-CT, novel system for grounded CT image interpretation. By bridging multi-modal language model with specialized segmentation module in bidirectional workflow, it enables dynamic invocation of the segmentation module during the text generation, and grounded abnormality interpretation based on the segmentation results. In contrast to prior approacheswhich often lack grounding abilities, generalizability across planes and body regions, or flexibility in prompt handlingOminiAbnorm-CT enables automatic localization and description of abnormalities across multi-plane, whole-body CT scans. More importantly, OminiAbnorm-CT is designed to robustly interpret diverse textual references and visual prompts, enabling interactive and clinically adaptive use cases aligned with radiologists workflows. For comprehensive evaluation, we define three clinically relevant tasks: (i) grounded report generation, (ii) text-guided grounded report generation, and (iii) visual-prompted report generation. 3https://radiopaedia.org/licence#data-sets Experiment results show that OminiAbnorm-CT significantly outperforms existing baselines across all tasks, marking substantial step toward explainable, abnormality-centric CT imaging interpretation. In summary, our contributions to advance abnormality-centric grounded CT image interpretation can be summarized as: (i) We propose comprehensive taxonomy for abnormalities on CT images, presented in Section 2; (ii) We build OminiAbnorm-CT-14K, the first dataset for abnormality grounding and description across multi-plane whole-body CT images, detailed in Section 3; (iii) We develop novel system OminiAbnorm-CT, for automatically localizing and reporting abnormalities, illustrated in detail in Section 4; (iv) We set up comprehensive benchmark with three clinically representative tasks, and demonstrate the superiority of OminiAbnorm-CT in Section 5."
        },
        {
            "title": "2 Taxonomy for Abnormal Findings",
            "content": "Given the diversity of abnormal findings, their complex relationships and synonyms, we first construct comprehensive taxonomy system, to categorize all the abnormalities into representative classes. Our taxonomy is guided by four fundamental principles to ensure scientific rigor, systematic organization, and clinical utility: (i) comprehensive coverage. It should include all the clinically significant abnormalities, encompassing both lesions and the abnormal changes on anatomical structures; (ii) image-based definition. Each abnormal finding is precisely characterized by its observable features on the radiology scan, such as physical properties and morphological characteristics, that form the essential foundation for diagnosis; (iii) hierarchical organization. Findings are grouped by clinically important anatomical structures and major systems or organs, aligning with standard image interpretation protocols. While cross-regional structures (like skeletal systems) remain independent groups for conciseness; (iv) modular design. Each category represents fundamental, independent finding that can be combined with others to express complex and multifaceted abnormalities. The taxonomy is developed through the following three stages, involving 7 radiologists with 10 to 16 years of experience from three clinical centers, (i) we establish an anatomical hierarchy framework based on standard atlases and classic textbooks for human anatomy [17]; (ii) the radiologists then systematically catalog the abnormal findings under each anatomical structure, based on their clinical experience and several authoritative textbooks [2, 18, 41] on medical imaging and radiological pathology; lastly, (iii) the radiologists conduct rigorous cross-validation and discussion to establish precise consensus on the definitions of controversial or ambiguous abnormalities. We finally identified 404 representative abnormalities across 82 anatomies throughout the human body, organized within 40 major organs and systems. The details can be found in Appendix Section A.11."
        },
        {
            "title": "3 Dataset Curation",
            "content": "In this section, we present OminiAbnorm-CT-14K, which establishes data foundation for abnormality grounding and interpretation across whole-body CT images. The dataset construction pipeline is illustrated in Figure 2. Here, we start with an overview of the limitations of existing datasets in Section 3.1, which motivate the dataset construction. We then describe the data source in Section 3.2, detail the annotation procedure in Section 3.3, outline the extension to instruction-style data in Section 3.4, and conclude with summary in Section 3.5. 3.1 Limitation in Existing Datasets We analyze several widely used public datasets relevant to our task (summarized in Appendix Table 6) and identify the following limitations: (i) limited annotation coverage. Most datasets provide grounding annotations for only small subset of lesion types. Despite efforts to map existing labels to our proposed taxonomy, the majority of abnormal finding categories remain unannotated; (ii) restricted imaging scope. Existing datasets are primarily limited to axial-plane CT images or focus on specific anatomical regions, overlooking the growing clinical need for interpretation across multiplane, whole-body CT scans; (iii) insufficient image-report alignment. Many datasets lack explicit associations between localized abnormalities in the images and their corresponding descriptions in radiology reports, hindering the development of grounded, explainable systems. These limitations make existing datasets inadequate for benchmarking or developing models capable of automatic detection, localization, and description of abnormal findings in multi-plane, whole-body CT imaging. 3 Figure 2: Data curation overview. (a) The CT image-report pairs are collected from an open-sourced and expert-checked website; (b) and (c) Radiologists provide grounding annotation on any abnormal findings, link to their text description in reports, and categorize into the taxonomy devised by group of senior radiologists. The annotated data are further extended to instruction data with simulated visual prompts and text queries. 3.2 Data Source Following prior works [53, 64, 54], we collect raw data from Radiopaedia [1], publicly accessible platform of peer-reviewed clinical cases contributed by clinicians worldwide, with appropriate privacy safeguards. After application, we have obtained proper permission for all data used in this study. Shown in Figure 2(a), each case includes radiology scans across various planes and anatomical regions, along with patient background information and detailed clinical reports describing findings and diagnoses. In total, we collected 46,721 CT images paired with 14,920 clinical reports. 3.3 Annotation Pipeline As illustrated in Figure 2(b) and (c), our goal is to produce grounding annotations for all abnormal findings in the collected CT images, i.e., linking them to corresponding textual descriptions in clinical reports, and categorizing them using our proposed taxonomy. To reduce annotation burden, we adopt two pre-processing strategy: (i) image-side pre-processing. Rather than annotating entire scans, we focus on the key slice and its 8 adjacent slices, as identified and verified by radiologists to be the most informative and representative; (ii) text-side pre-processing. We employ GPT-4o [25] to extract findings from the reports and identify abnormalities using RaTEScore [62]. Here, we recruited 4 clinicians with at least 7 years of radiology experience as annotators. To ensure high-quality annotations, we followed three core principles: (i) representative slices. Select the most representative slice(s) for annotation when abnormalities are present; (ii) complete annotation. Annotate all abnormal findings visible on selected slices; (iii) vision-language consistency. Only annotate abnormalities identifiable in provided slices; similarly, linked descriptions must reflect findings visible in these slices. For example, when report mentions organ enlargement, the finding should not be annotated if enlargement cannot be confidently determined from the available slices. Throughout the annotation process, we continuously refined OminiAbnorm-CT-14K based on annotator feedback: (i) taxonomy refinement. We remove or merge 4 less representative categories, add 22 previously omitted but clinically important ones, and revise 21 categories for improved clarity and expression; (ii) long-tail mitigation. To address data imbalance, we identify the underrepresented categories and use GPT-4o to filter scans referencing these rare findings, prioritizing them for annotation. As shown in Appendix Section A.6, it has significantly mitigated the long-tail distribution. 3.4 Extension to Instruction Data As shown in Figure 2(b), we extend our annotated data to support three instruction tasks that simulate realistic clinical scenarios: (i) grounded report generation simulates comprehensive examination. We format instructions to detect, ground, and describe all abnormal findings on CT images; (ii) text-guided grounded report generation involves responding to text queries about 4 Figure 3: OminiAbnorm-CT. We bridge multi-modal language model and segmentation module, to allow grounding evidence acquisition during the generation of abnormality description, and further enhance its comprehension for flexible usage with text instruction and visual prompts. specific abnormalities of interest. For instance, for patient with tuberculous empyema history, checking any abnormality related to it. We generate these queries using GPT-4o based on clinical reports, patient presentations, and medical history, simulating how radiologists approach images with prior knowledge; (iii) visual prompted report generation focuses on interpreting the marked abnormality. We create visual prompts from annotation masks using various formats, e.g., bounding box, ellipse, contour, and cropped region. This simulates semi-automated workflow where the model elaborates the clinician-identified abnormality in details. More details are in Appendix Section A.10. 3.5 Summary We prioritize axial CT images for annotation, while also incorporating coronal and sagittal images based on clinical recommendations. In total, we annotated 18,969 abnormalities on 9,990 axial scans, 2,738 coronal scans, and 1,803 sagittal scans. Representative examples are shown in Figure 1, and the distribution of annotated abnormalities is detailed in Appendix Section A.11. Annotation quality is verified by senior radiologist with 12 years experience, with details in Appendix A.5."
        },
        {
            "title": "4 OminiAbnorm-CT",
            "content": "This section presents the details of OminiAbnorm-CT, novel system for grounded CT image interpretation . We start with the problem formulation in Section 4.1, then its architectural details in Section 4.2 and finally the training details in Section 4.3. 4.1 Problem Formulation Given CT image (I RHW D), this paper aims to build model Φθ(), that enables generating the textual descriptions for abnormal regions, with intermediate groundings when necessary: {R, S} = Φθ(T , I, V), (1) where is the text instruction, RHW (optional) is the visual prompt on key slice from the users, for example, bounding boxes, ellipse, contour. denotes the generated description for abnormalities, and RHW refers to the (optional) grounding results on the key slice. In contrast to the conventional approaches, Φθ() can interpret user-provided textual queries and ground arbitrary abnormal findings across multi-plane CT images spanning the entire human body, and generate corresponding descriptions. Additionally, it also supports visual prompts that allow the model to elaborate on user-highlighted abnormalities, enabling an interactive workflow where users can iteratively refine or correct the grounding results for more precise and context-aware descriptions. 4.2 Architecture The overall architecture of OminiAbnorm-CT is illustrated in Figure 3, which comprises standard multi-modal language model and segmentation module. Our core innovation lies in the integration 5 of them, which enables localizing the abnormality and generating textual description (findings), supporting flexible interaction through both visual prompts and textual instructions. Multi-modal language model functions as the core to comprehend user instruction, invoke the segmentation module properly, and document the abnormal findings based on the grounding evidence. Specifically, in one workflow to generate grounded reports, given CT image (I) and text instruction (T ) as user input, it will first reason for the specific abnormal findings of interest for the user (or simply any abnormalities). For instance, when referring to patient history of tuberculous empyema, it should focus on related abnormalities including pleural thickening, calcifications, etc. Then, to let it acquire grounding evidence, we expand the vocabulary with special token <SEG> as grounding request, which will invoke the segmentation module during generation. We extract its hidden state hseg from the last decoder layer as the prompt for the segmentation module, which encapsulates the targeted abnormal findings: = Φseg(I, hseg), hseg Rd (2) Given the segmentation results (S), we convert it to visual prompt (V) in box format, i.e., directly overlay it on the original image to create new input image (I ), and the multi-modal language model could seamlessly continue generating the abnormality description (R) based on it: = ΦMLLM(T , ), = (3) where represents the superimposing operation. This leverages the inherent ability of multi-modal language model to perceive visual markers [8], guiding its attention to specific regions of interest. Note that, the above design naturally supports another workflow when users have manually delineated specific abnormalities, or intend to refine the grounding results. In such case, can also be user-input visual prompts on the image, and the multi-modal language model elaborates the user-highlighted abnormality following Equation 3. Segmentation module is invoked for abnormality grounding based on the CT image (I) and prompt (hseg), summarized as Equation 2. Specifically, it first adopts an encoder-decoder backbone to derive image features: (v, u) = Φseg(I), RH d , RHW d, (4) where denotes concatenation of multi-scale image embeddings along the channel dimension, which are down-sampled from each encoder layer to unified resolution, and is the pixel-level dense feature from the last decoder layer. To bridge the gap between latent spaces, we further align the segmentation prompt (hseg) with the image embeddings (v): = Φcrossattn(v, (hseg)), Rd (5) where is projection layer for dimension consistency, and Φcrossattn is cross-attention module treating as key and value, (hseg) as query. The segmentation prediction is then derived by performing dot product between the adapted feature and u: = u, RHW , (6) 4.3 Training Training object. We train the multi-modal language model and the segmentation module jointly with text generation loss and segmentation loss: Ltxt = CE( ˆR, R), Lseg = BCE( ˆS, S) + DICE( ˆS, S) (7) Training data. Our training data primarily originates from OminiAbnorm-CT-14K, and is formulated into three instruction data formats as illustrated in Section 3.4. Additionally, we incorporate two types of data as supplements: (i) lesion segmentation data. This type of data can be formulated into grounding tasks and utilized to enhance the grounding ability. detailed list of these datasets can be found in Appendix Table 6; (ii) medical VQA data. This type of data is involved to maintain our models generalization capabilities with more diverse question types. Specifically, we take CT images and corresponding question-answering data from PubMedVision [9]. 6 Table 1: Quantitative results on visual prompted report generation. Results are averaged within each category and then across all categories, with the best bolded and the second best underlined. Models optimized with medical data are marked with . Model B-1 B-2 B-3 RTSc BTSc MTR R-1 R-L RG 15.80 GPT-4o 11.57 QWen2.5-VL-7B 15.21 ViP-LLaVA MedFlamingo 11.26 MedDr 9.54 LLaVA-Med 15.55 BiomedGPT 10.80 OminiAbnorm-CT 18.03 9.34 GPT-4o 13.62 QWen2.5-VL-7B 13.85 ViP-LLaVA MedFlamingo 10.98 MedDr 8.07 LLaVA-Med 15.65 BiomedGPT 12.60 OminiAbnorm-CT 18.38 10.29 GPT-4o 13.28 QWen2.5-VL-7B 12.84 ViP-LLaVA MedFlamingo 10.57 MedDr 6.98 LLaVA-Med 15.10 BiomedGPT 12.59 OminiAbnorm-CT 17.29 Axial (n=2193) 1.59 0.76 0.69 0.00 0.41 0.55 0.71 1. 39.04 38.35 33.07 17.35 32.24 39.08 38.16 42.81 Coronal (n=750) 0.96 0.66 0.53 0.00 0.29 0.68 0.70 2.39 27.13 37.26 35.21 17.73 32.77 40.99 38.47 42.88 Sagittal (n=591) 0.84 0.63 0.38 0.00 0.28 0.66 0.73 1. 32.90 35.67 34.51 17.36 30.77 39.19 36.62 41.77 84.58 84.10 85.15 80.34 78.80 84.68 83.19 86.35 58.83 84.19 84.60 79.98 76.07 84.38 82.89 86.00 72.76 83.98 84.44 78.87 75.35 84.10 82.60 85.62 6.15 4.20 4.49 1.71 2.46 4.23 4.16 6.65 3.15 3.54 3.81 1.66 2.07 4.18 4.79 7. 3.40 3.54 3.83 1.53 1.77 4.68 4.81 6.48 19.31 16.68 13.65 9.43 9.13 15.58 15.77 19.40 9.27 13.49 12.52 9.01 8.09 14.18 15.45 19.00 10.07 13.01 11.88 8.66 7.12 14.18 15.19 17.45 18.62 14.26 17.41 11.69 12.23 17.67 13.40 21.66 12.25 14.97 17.07 11.60 11.55 18.15 14.78 21. 13.86 14.75 16.29 11.40 10.32 17.90 14.62 20.43 14.41 11.09 14.68 10.55 10.34 13.55 10.81 17.61 10.01 11.47 13.96 10.34 9.63 13.52 11.52 16.87 11.63 11.34 13.44 10.14 8.54 13.57 11.35 15.87 6.51 3.85 3.35 0.50 3.74 6.06 4.65 9.98 4.31 3.37 3.80 0.53 4.08 6.54 4.45 9. 4.04 2.91 2.86 0.36 2.42 5.11 3.37 7.51 Implementation details. In practice, we adopt QWen2.5-VL-7B [56] as the multi-modal language model, and 6-layer U-Net as the segmentation module. The cross-attention module consists of 6 layers. As shown in Appendix Section A.8, we find that expanding the input context range (i.e., using more slices) does not enhance the report generation performance of the multi-modal language model. Thus, we only feed the central slice to the model. However, for the segmentation module, we include up to 8 adjacent slices as additional input. To save computational cost, we only optimize the inserted LoRA layers [24] in the multi-modal language model while leaving the whole segmentation module trainable. More implementation details are in Appendix Section A.3."
        },
        {
            "title": "5 Experiments",
            "content": "In this section, we first present experimental settings in Section 5.1, including task formulation, metrics, baselines, and test datasets. We then provide detailed quantitative experimental results in Section 5.2, and ablation study in Section 5.3. In addition, the qualitative experimental results are in the Appendix Section A.1 and more ablation studies are in the Appendix A.8 and A.9. 5.1 Experiment Settings Based on OminiAbnorm-CT-14K, we build rigorous and comprehensive benchmark to evaluate both grounding and description generation abilities for abnormalities on whole-body CT images. Task formulation. As detailed in Section 3.4, we evaluate methods on three tasks of increasing difficulty: visual prompted report generation, grounded report generation, and text-guided grounded report generation. For text-guided grounded report generation, we generate queries for both present and absent abnormalities at 3:1 ratio to simulate realistic diagnostic scenarios. 7 Table 2: Quantitative results on grounded report generation. Results are averaged within each category and across all categories, with the best results bolded and the second best underlined. Model DSC B-1 B-2 B-3 RTSc BTSc MTR RR-L RG LiSA + LLaVA-Med BiomedParse + LLaVA-Med MedULS + LLaVA-Med OminiAbnorm-CT LiSA + LLaVA-Med BiomedParse + LLaVA-Med MedULS + LLaVA-Med OminiAbnorm-CT LiSA + LLaVA-Med BiomedParse + LLaVA-Med MedULS + LLaVA-Med OminiAbnorm-CT 20.96 15.78 15.24 36. 17.09 14.55 12.77 31.65 14.72 14.45 12.95 34.38 Axial (n=2193) 6.65 6.98 6.43 11.75 0.73 0.84 0.74 7.90 Coronal (n=750) 5.85 5.95 5.88 11.86 0.64 0.68 0.64 7.81 Sagittal (n=591) 5.60 5.46 5.32 10.63 0.58 0.52 0.50 7.10 31.38 31.50 31.05 33. 33.28 32.89 32.25 35.47 31.51 32.10 30.94 33.13 12.94 13.61 12.70 18.50 11.48 11.71 11.64 19.13 11.10 10.89 10.95 17.10 82.07 82.36 84.27 86. 82.27 81.80 84.11 85.58 81.12 79.37 83.53 85.25 19.20 19.84 18.74 21.83 16.23 16.88 16.45 20.66 16.18 15.68 15.39 19.05 14.62 15.28 14.67 21. 14.36 14.75 14.71 22.14 14.07 14.06 14.12 20.92 12.44 13.02 12.42 18.75 12.12 12.57 12.34 18.26 11.96 11.95 11.83 17.67 2.79 2.74 2.31 3. 2.74 3.35 2.96 4.55 2.18 2.13 1.87 3.70 Metrics. For grounding results, we calculate Dice Similarity Coefficient (DSC); for generated descriptions, we calculate BLEU (B) [45], RaTEScore (RTSc) [62], BERTScore (BTSc) [59], METEOR (MTR) [7], ROUGE (R) [35] and RadGraph (RG) [14]. In the text-guided grounded report generation, we prompt models to output specific response when the queried abnormalities are confirmed absent. Specificity (SP) is then calculated as the proportion of correctly identified absent abnormalities out of all truly absent abnormalities in the test set. Meanwhile, when models miss the queried abnormalities, both DSC and all description metrics are assigned as 0. Baseline. For visual prompted report generation, we compare with multi-modal LLMs in both the medical domain (MedDr [20], BiomedGPT [58], LLaVA-Med [33], and Med-Flamingo [44]) and the general domain (GPT-4o [25], Qwen2.5-VL [56], and ViP-LLaVA [8]). For (text-guided) grounded report generation, as no such solution exists in the literature, we prompt LLaVA-Med to describe abnormal findings based on segmentation results from auxiliary grounding models. Since the images in OminiAbnorm-CT-14K lack the original DICOM metadata, we only consider 2D segmentation models as baselines, including BiomedParse [61], LiSA [31], and MedULS [13]. More details are in Appendix Section A.4. Data. We independently split the train and test data for axial, coronal, and sagittal images. To guarantee diversity, we allocate at least 5 samples per category to the axial test set (2 for coronal and sagittal), or all samples if fewer exist, with the remaining data following 3:1 train-test ratio. To avoid data leakage, images from the same patient are restricted to the same set. 5.2 Experiment Results Visual Prompted Report Generation. As depicted in Table 1, OminiAbnorm-CT consistently outperforms all baselines across all planes and metrics. Specifically, compared to the strongest baseline LLaVA-Med, OminiAbnorm-CT achieves average improvements of 2.5 in BLEU-1, 2.7 in RaTEScore, and 3.1 in RadGraph across all three planes. These results demonstrate OminiAbnormCTs effectiveness in semi-automated workflows, where radiologists identify abnormalities and the model provides accurate and detailed interpretations. Grounded Report Generation. As shown in Table 2, OminiAbnorm-CT demonstrates superior grounding capabilities, outperforming the best baselines by 15.08, 14.56, and 19.66 in DSC across axial, coronal, and sagittal planes respectively. For report generation, OminiAbnorm-CT achieves the best performance across all metrics, with notable improvements in both RadGraph and RaTEScore across all anatomical planes, indicating that the generated reports can better align with clinical standards. These results demonstrate OminiAbnorm-CTs potential for fully automated abnormality localization and report generation in clinical settings. Text-guided Grounded Report Generation. As shown in Table 3, OminiAbnorm-CT demonstrates superior grounding capabilities, with DSC scores improving by 14.24, 14.14, and 14.54 in axial, 8 Table 3: Quantitative results on text-guided grounded report generation. Results are averaged within each category and then across all categories, with the best bolded and the second best underlined. Model DSC SP B-1 B-2 B-3 RTSc BTSc MTR RR-L RG MedULS + LLaVA-Med LiSA + LLaVA-Med BiomedParse + LLaVA-Med OminiAbnorm-CT MedULS + LLaVA-Med LiSA + LLaVA-Med BiomedParse + LLaVA-Med OminiAbnorm-CT MedULS + LLaVA-Med LiSA + LLaVA-Med BiomedParse + LLaVA-Med OminiAbnorm-CT 14.53 17.48 16.57 32. 12.26 12.04 13.60 27.74 11.89 11.23 15.35 29.89 6.10 21.35 19.87 58.43 5.37 13.76 17.79 45.16 7.66 16.67 17.57 45.41 Axial (n=2193) 9.54 10.57 10.66 11.94 2.18 2.41 2.38 3.33 0.19 0.28 0.26 0.74 Coronal (n=750) 10.73 11.58 11.21 12.51 2.60 2.71 2.64 4. 0.26 0.22 0.25 1.22 Sagittal (n=591) 10.25 11.14 11.07 12.80 2.69 2.72 2.72 4.51 0.29 0.30 0.39 1.42 27.35 29.18 29.66 34. 29.34 30.76 30.24 36.56 27.40 29.79 31.12 36.04 82.98 83.47 83.39 83.85 82.89 83.21 83.33 82.98 82.49 82.88 83.01 83.97 11.45 11.59 11.74 12. 11.40 11.36 11.47 13.61 11.24 11.15 11.66 14.02 11.48 12.34 12.62 14.65 12.79 13.40 13.50 15.83 12.17 13.36 13.85 16.20 9.20 9.80 9.86 11. 9.82 10.23 10.31 12.36 9.53 10.07 10.40 12.79 1.61 1.61 2.16 4.32 1.61 1.83 2.34 5.72 1.23 1.42 1.78 4.85 coronal, and sagittal planes respectively compared to the best baselines. For report generation, OminiAbnorm-CT excels across both lexical overlap metrics (average improvements of 1.3 on BLEU1 and 2.2 on ROUGE-1 across all planes) and clinical relevance metrics (average gains of 4.0 on RaTEScore and 2.9 on RadGraph across all planes), confirming its superior ability to generate reports that align with clinical assessments. Additionally, OminiAbnorm-CT achieves significantly higher Specificity, demonstrating its robustness to queries about non-existing abnormalities. 5.3 Ablation Study The integration of the segmentation module allows OminiAbnorm-CT to acquire grounding evidence for abnormal findings and generate descriptions accordingly. To validate its necessity, we conduct an ablation study comparing the full model against variant where the segmentation module is removed (denoted as OminiAbnorm-CT w/o Seg). This modified version directly generates findings descriptions without anatomical localization guidance. In the text-guided grounded report generation task, as shown in Table 4, OminiAbnorm-CT without the segmentation module demonstrates notable performance degradation, with inferior results on 23 of 27 metrics across three planes. Furthermore, detailed analysis reveals that without the segmentation module, the model struggles to accurately detect the queried abnormalities in CT images, exhibiting pronounced negative bias that leads to high specificity scores. Similarly, in the grounded report generation task, as depicted in Table 5, removing the segmentation module leads to consistent performance degradation on all the metrics, further confirming that segmentation-based evidence is crucial for generating high-quality medical reports. Table 4: Ablation study of the segmentation module for text-guided grounded report generation task. Results are averaged within each category and then across all categories, with the best bolded Model SP B-1 B-2 B-3 RTSc BTSc MTR R-1 R-L RG Axial (n=2193) OminiAbnorm-CT w/o Seg OminiAbnorm-CT 95.39 58.43 4.50 11.94 2.33 3. 1.21 0.74 20.53 34.80 31.06 83.85 7.04 12.94 8.50 14.65 7.02 11. 7.25 4.32 Coronal (n=750) OminiAbnorm-CT w/o Seg OminiAbnorm-CT 95.71 45.16 4.50 12.51 2.47 4. 1.27 1.22 19.07 36.56 28.34 82.98 6.27 13.61 7.96 15.83 6.43 12. 6.17 5.72 Sagittal (n=591) OminiAbnorm-CT w/o Seg OminiAbnorm-CT 92.08 45.41 4.17 12.80 2.16 4. 1.12 1.42 16.62 36.04 25.43 83.97 5.84 14.02 7.06 16.20 5.53 12. 5.67 4.85 9 Table 5: Ablation study of the segmentation module in grounded report generation task. Results are averaged within each category and then across all categories, with the best bolded Model B-1 BB-3 RTSc BTSc MTR R-1 R-L RG Axial (n=2193) OminiAbnorm-CT w/o Seg OminiAbnorm-CT 7.73 19.00 4.74 12.26 3.16 8.40 27.87 33.85 73.14 86. 17.65 22.45 15.18 22.27 13.58 19.43 3.42 4.60 Coronal (n=750) OminiAbnorm-CT w/o Seg OminiAbnorm-CT 8.12 19.28 4.81 12.03 3.12 7.96 31.03 35.40 75.55 85.56 15.64 20. 15.27 22.35 13.23 18.47 3.84 4.65 Sagittal (n=591) OminiAbnorm-CT w/o Seg OminiAbnorm-CT 8.79 16. 5.21 10.41 3.33 6.98 31.21 33.45 77.41 84.94 16.18 18.77 15.86 20. 13.87 17.61 3.91 3."
        },
        {
            "title": "6 Related Work",
            "content": "Medical image segmentation aims to delineate clinically meaningful regions on medical images. For radiology images, most datasets focus on annotations for important organs and anatomical structures [52, 47, 40, 29] or lesions [22, 4, 43, 13, 55]. Specialist segmentation models [26, 49, 65, 66] have achieved remarkable success in the past decade, while recently, building foundational generalist models [38, 61, 63, 16] has received increased attention. However, few works contribute to the grounding of abnormal image findings, which encompass broader spectrum including abnormal organs, potential lesions, and more generally, any anomalies meaningful for clinical decision-making. Medical image report generation aims to faithfully interpret the medical images with formal text, which is indispensable for numerous clinic procedures [48]. Existing datasets with image-report pairs are limited to specific modalities and regions, such as chest X-ray [30], brain MRI [32] and chest CT [19, 10], overlooking the clinical demands for multi-plane whole-body CT image report generation. Recent progress [60, 12, 32, 11] on grounded report generation are limited to structured organ-level segmentation and description, failing to localize and interprete fine-grained abnormalities. Generative visual-language foundation models for medicine are built on large-scale multi-modal medical data, and have demonstrated exceptional performance and generalization capabilities across diverse medical tasks [53, 44, 58, 33, 20]. Despite their potential application to multi-plane, wholebody CT images, however, most existing approaches fail to simultaneously provide grounded evidence during generation, leading to poor explainability and hallucinations that are more difficult to detect."
        },
        {
            "title": "7 Conclusion",
            "content": "This paper advances the automatic grounded interpretation of CT imaging from an abnormality-centric view. To support this, we develop hierarchical taxonomy of 404 representative abnormal findings on CT images. We contribute OminiAbnorm-CT-14K, meticulously annotated dataset with detailed grounding and description annotation for around 19K abnormalities, from 14.5K multi-plane CT images across the entire human body. Built on this dataset, OminiAbnorm-CT enables automatic grounding and description of abnormalities, while supporting flexible clinical usage with text queries or visual prompts. Comprehensive evaluations across three clinically representative tasks demonstrate OminiAbnorm-CTs superior performance. Our work pioneers abnormality-centric CT interpretation, enhancing diagnostic transparency through accurate grounding and detailed characterization of findings, with significant potential to transform radiological practice."
        },
        {
            "title": "Acknowledgments and Disclosure of Funding",
            "content": "Weidi would like to acknowledge the funding from Scientific Research Innovation Capability Support Project for Young Faculty (ZY-GXQNJSKYCXNLZCXM-I22)."
        },
        {
            "title": "References",
            "content": "[1] Radiopaedia. https://radiopaedia.org, [insert year accessed]. Accessed: [insert access date here]. [2] Andy Adam, Adrian K. Dixon, Jonathan H. Gillard, and Cornelia Schaefer-Prokop. Grainger & Allisons Diagnostic Radiology. Elsevier, 7th edition, 2020. [3] AA Ahmed, MM Elmohr, Fuentes, MA Habra, SB Fisher, ND Perrier, Zhang, and KM Elsayes. Radiomic mapping model for prediction of ki-67 expression in adrenocortical carcinoma. Clinical Radiology, 75(6):479e17, 2020. [4] Michela Antonelli, Annika Reinke, Spyridon Bakas, Keyvan Farahani, Annette Kopp-Schneider, Bennett Landman, Geert Litjens, Bjoern Menze, Olaf Ronneberger, Ronald Summers, et al. The medical segmentation decathlon. Nature communications, 13(1):4128, 2022. [5] Samuel Armato III, Geoffrey McLennan, Luc Bidaut, Michael McNitt-Gray, Charles Meyer, Anthony Reeves, Binsheng Zhao, Denise Aberle, Claudia Henschke, Eric Hoffman, et al. The lung image database consortium (lidc) and image database resource initiative (idri): completed reference database of lung nodules on ct scans. Medical physics, 38(2):915931, 2011. [6] Shaimaa Bakr, Olivier Gevaert, Sebastian Echegaray, Kelsey Ayers, Mu Zhou, Majid Shafiq, Hong Zheng, Jalen Anthony Benson, Weiruo Zhang, Ann NC Leung, et al. radiogenomic dataset of non-small cell lung cancer. Scientific Data, 5(1):19, 2018. [7] Satanjeev Banerjee and Alon Lavie. Meteor: An automatic metric for mt evaluation with improved correlation with human judgments. In Proceedings of the acl workshop on intrinsic and extrinsic evaluation measures for machine translation and/or summarization, pages 6572, 2005. [8] Mu Cai, Haotian Liu, Siva Karthik Mustikovela, Gregory Meyer, Yuning Chai, Dennis Park, and Yong Jae Lee. Vip-llava: Making large multimodal models understand arbitrary visual prompts. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 1291412923, 2024. [9] Junying Chen, Chi Gui, Ruyi Ouyang, Anningzhe Gao, Shunian Chen, Guiming Hardy Chen, Xidong Wang, Ruifei Zhang, Zhenyang Cai, Ke Ji, et al. Huatuogpt-vision, towards injecting medical visual knowledge into multimodal llms at scale. arXiv preprint arXiv:2406.19280, 2024. [10] Yinda Chen, Che Liu, Xiaoyu Liu, Rossella Arcucci, and Zhiwei Xiong. Bimcv-r: landmark dataset for 3d ct text-image retrieval. In Medical Image Computing and Computer-Assisted Intervention, pages 124134. Springer, 2024. [11] Chen, Varma, JB Delbrouck, et al. vision-language foundation model to enhance efficiency of chest x-ray interpretation. Preprint at https://arxiv. org/abs/2401.12208, 2024. [12] Zhixuan Chen, Yequan Bie, Haibo Jin, and Hao Chen. Large language model with region-guided referring and grounding for ct report generation. IEEE Transactions on Medical Imaging, 2025. [13] MJJ de Grauw, Th Scholten, EJ Smit, MJCM Rutten, Prokop, van Ginneken, and Hering. The uls23 challenge: baseline model and benchmark dataset for 3d universal lesion segmentation in computed tomography. arXiv preprint arXiv:2406.05231, 2024. [14] Jean-Benoit Delbrouck, Pierre Chambon, Zhihong Chen, Maya Varma, Andrew Johnston, Louis Blankemeier, Dave Van Veen, Tan Bui, Steven Truong, and Curtis Langlotz. Radgraph-xl: large-scale expert-annotated dataset for entity and relation extraction from radiology reports. In Findings of the Association for Computational Linguistics, pages 1290212915, 2024. [15] Reuben Dorent, Roya Khajavi, Tagwa Idris, Erik Ziegler, Bhanusupriya Somarouthu, Heather Jacene, Ann LaCasce, Jonathan Deissler, Jan Ehrhardt, Sofija Engelson, et al. Lnq 2023 challenge: Benchmark of weakly-supervised techniques for mediastinal lymph node quantification. arXiv preprint arXiv:2408.10069, 2024. 11 [16] Yuxin Du, Fan Bai, Tiejun Huang, and Bo Zhao. Segvol: Universal and interactive volumetric medical image segmentation. Advances in Neural Information Processing Systems, 37:110746 110783, 2024. [17] Henry Gray and Standring. Grays anatomy. Arcturus Publishing London, 2008. [18] John R. Haaga and Daniel Boll. CT and MRI of the Whole Body, 2-Volume Set. Elsevier, 6th edition, 2016. [19] Ibrahim Ethem Hamamci, Sezgin Er, Furkan Almas, Ayse Gulnihan Simsek, Sevval Nil Esirgun, Irem Dogan, Muhammed Furkan Dasdelen, Bastian Wittmann, Enis Simsar, Mehmet Simsar, et al. foundation model utilizing chest ct volumes and radiology reports for supervised-level zero-shot detection of abnormalities. CoRR, 2024. [20] Sunan He, Yuxiang Nie, Zhixuan Chen, Zhiyuan Cai, Hongmei Wang, Shu Yang, and Hao Chen. Meddr: Diagnosis-guided bootstrapping for large-scale medical vision-language learning. arXiv preprint arXiv:2404.15127, 2024. [21] Yuting He, Guanyu Yang, Jian Yang, Rongjun Ge, Youyong Kong, Xiaomei Zhu, Shaobo Zhang, Pengfei Shao, Huazhong Shu, Jean-Louis Dillenseger, et al. Meta grayscale adaptive network for 3d integrated renal structures segmentation. Medical image analysis, 71:102055, 2021. [22] Nicholas Heller, Fabian Isensee, Darya Trofimova, Resha Tejpaul, Nikolaos Papanikolopoulos, and Christopher Weight. Kidney and kidney tumor segmentation. Springer, 2021. [23] Nicholas Heller, Fabian Isensee, Dasha Trofimova, Resha Tejpaul, Zhongchen Zhao, Huai Chen, Lisheng Wang, Alex Golts, Daniel Khapun, Daniel Shats, et al. The kits21 challenge: Automatic segmentation of kidneys, renal tumors, and renal cysts in corticomedullary-phase ct. arXiv preprint arXiv:2307.01984, 2023. [24] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, Weizhu Chen, et al. Lora: Low-rank adaptation of large language models. 1(2):3, 2022. [25] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [26] Fabian Isensee, Paul Jaeger, Simon AA Kohl, Jens Petersen, and Klaus Maier-Hein. nnu-net: self-configuring method for deep learning-based biomedical image segmentation. Nature Methods, 18(2):203211, 2021. [27] Fabian Isensee, Maximilian Rokuss, Lars Krämer, Stefan Dinkelacker, Ashis Ravindran, Florian Stritzke, Benjamin Hamm, Tassilo Wald, Moritz Langenberg, Constantin Ulrich, et al. nninteractive: Redefining 3d promptable segmentation. arXiv preprint arXiv:2503.08373, 2025. [28] Alexander Jaus, Constantin Seibold, Kelsey Hermann, Alexandra Walter, Kristina Giske, Johannes Haubold, Jens Kleesiek, and Rainer Stiefelhagen. Towards unifying anatomy segmentation: automated generation of full-body ct dataset via knowledge aggregation and anatomical guidelines. arXiv preprint arXiv:2307.13375, 2023. [29] Yuanfeng Ji, Haotian Bai, Chongjian Ge, Jie Yang, Ye Zhu, Ruimao Zhang, Zhen Li, Lingyan Zhanng, Wanling Ma, Xiang Wan, et al. Amos: large-scale abdominal multi-organ benchmark for versatile medical image segmentation. Advances in Neural Information Processing Systems, 35:3672236732, 2022. [30] Alistair EW Johnson, Tom Pollard, Seth Berkowitz, Nathaniel Greenbaum, Matthew Lungren, Chih-ying Deng, Roger Mark, and Steven Horng. Mimic-cxr, de-identified publicly available database of chest radiographs with free-text reports. Scientific data, 6(1):317, 2019. [31] Xin Lai, Zhuotao Tian, Yukang Chen, Yanwei Li, Yuhui Yuan, Shu Liu, and Jiaya Jia. Lisa: Reasoning segmentation via large language model. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 95799589, 2024. 12 [32] Jiayu Lei, Xiaoman Zhang, Chaoyi Wu, Lisong Dai, Ya Zhang, Yanyong Zhang, Yanfeng Wang, Weidi Xie, and Yuehua Li. Autorg-brain: Grounded report generation for brain mri. arXiv preprint arXiv:2407.16684, 2024. [33] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large language-and-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36:2854128564, 2023. [34] Xiangyu Li, Gongning Luo, Kuanquan Wang, Hongyu Wang, Jun Liu, Xinjie Liang, Jie Jiang, Zhenghao Song, Chunyue Zheng, Haokai Chi, et al. The state-of-the-art 3d anisotropic intracranial hemorrhage segmentation on non-contrast head ct: The instance challenge. arXiv preprint arXiv:2301.03281, 2023. [35] Chin-Yew Lin. Rouge: package for automatic evaluation of summaries. In Text summarization branches out, pages 7481, 2004. [36] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. [37] Xiangde Luo, Jia Fu, Yunxin Zhong, Shuolin Liu, Bing Han, Mehdi Astaraki, Simone Bendazzoli, Iuliana Toma-Dasu, Yiwen Ye, Ziyang Chen, et al. Segrap2023: benchmark of organs-at-risk and gross tumor volume segmentation for radiotherapy planning of nasopharyngeal carcinoma. Medical Image Analysis, page 103447, 2025. [38] Jun Ma, Yuting He, Feifei Li, Lin Han, Chenyu You, and Bo Wang. Segment anything in medical images. Nature Communications, 15(1):654, 2024. [39] Jun Ma, Yixin Wang, Xingle An, Cheng Ge, Ziqi Yu, Jianan Chen, Qiongjie Zhu, Guoqiang Dong, Jian He, Zhiqiang He, et al. Toward data-efficient learning: benchmark for covid-19 ct lung and infection segmentation. Medical physics, 48(3):11971210, 2021. [40] Jun Ma, Yao Zhang, Song Gu, Cheng Ge, Shihao Mae, Adamo Young, Cheng Zhu, Xin Yang, Kangkang Meng, Ziyan Huang, et al. Unleashing the strengths of unlabelled data in deep learning-assisted pan-cancer abdominal organ quantification: the flare22 challenge. The Lancet Digital Health, 6(11):e815e826, 2024. [41] Jacob Mandell. Core Radiology: Visual Approach to Diagnostic Imaging. Cambridge University Press, 2013. [42] Mojtaba Masoudi, Hamid-Reza Pourreza, Mahdi Saadatmand-Tarzjan, Noushin Eftekhari, Fateme Shafiee Zargar, and Masoud Pezeshki Rad. new dataset of computed-tomography angiography images for computer-aided detection of pulmonary embolism. Scientific Data, 5, 2018. [43] Bjoern Menze, Andras Jakab, Stefan Bauer, Jayashree Kalpathy-Cramer, Keyvan Farahani, Justin Kirby, Yuliya Burren, Nicole Porz, Johannes Slotboom, Roland Wiest, et al. The multimodal brain tumor image segmentation benchmark (brats). IEEE transactions on medical imaging, 34(10):19932024, 2014. [44] Michael Moor, Qian Huang, Shirley Wu, Michihiro Yasunaga, Yash Dalmia, Jure Leskovec, Cyril Zakka, Eduardo Pontes Reis, and Pranav Rajpurkar. Med-flamingo: multimodal medical few-shot learner. In Machine Learning for Health (ML4H), pages 353367. PMLR, 2023. [45] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Association for Computational Linguistics, pages 311318, 2002. [46] João Pedrosa, Guilherme Aresta, Carlos Ferreira, Gurraj Atwal, Hady Ahmady Phoulady, Xiaoyu Chen, Rongzhen Chen, Jiaoliang Li, Liansheng Wang, Adrian Galdran, et al. Lndb challenge on automatic lung cancer patient management. Medical image analysis, 70:102027, 2021. 13 [47] Chongyu Qu, Tiezheng Zhang, Hualin Qiao, Yucheng Tang, Alan Yuille, Zongwei Zhou, et al. Abdomenatlas-8k: Annotating 8,000 ct volumes for multi-organ segmentation in three weeks. Advances in Neural Information Processing Systems, 36, 2023. [48] Vishwanatha Rao, Michael Hla, Michael Moor, Subathra Adithan, Stephen Kwak, Eric Topol, and Pranav Rajpurkar. Multimodal generative ai for medical image interpretation. Nature, 639(8056):888896, 2025. [49] Olaf Ronneberger, Philipp Fischer, and Thomas Brox. U-net: Convolutional networks for biomedical image segmentation. In Medical Image Computing and Computer-Assisted Intervention, pages 234241. Springer, 2015. [50] Holger Roth, Le Lu, Ari Seff, Kevin Cherry, Joanne Hoffman, Shijun Wang, Jiamin Liu, Evrim Turkbey, and Ronald Summers. new 2.5 representation for lymph node detection using random sets of deep convolutional neural network observations. In Medical Image Computing and Computer-Assisted Intervention, pages 520527. Springer, 2014. [51] T. Urban, E. Ziegler, S. Pieper, J. Kirby, D. Rukas, B. Beardmore, B. Somarouthu, E. Ozkan, G. Lelis, B. Fevrier-Sullivan, S. Nandekar, A. Beers, C. Jaffe, J. Freymann, D. Clunie, G. J. Harris, and J. Kalpathy-Cramer. Crowds cure cancer: Crowdsourced data collected at the rsna 2018 annual meeting, 2019. [52] Jakob Wasserthal, Hanns-Christian Breit, Manfred Meyer, Maurice Pradella, Daniel Hinck, Alexander Sauter, Tobias Heye, Daniel Boll, Joshy Cyriac, Shan Yang, et al. Totalsegmentator: Robust segmentation of 104 anatomic structures in ct images. Radiology: Artificial Intelligence, 5(5), 2023. [53] Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Towards generalist foundation model for radiology by leveraging web-scale 2d&3d medical data. arXiv preprint arXiv:2308.02463, 2023. [54] Haoning Wu, Ziheng Zhao, Ya Zhang, Weidi Xie, and Yanfeng Wang. Mrgen: Diffusion-based controllable data engine for mri segmentation towards unannotated modalities. arXiv preprint arXiv:2412.04106, 2024. [55] Ke Yan, Xiaosong Wang, Le Lu, Ling Zhang, Adam Harrison, Mohammadhadi Bagheri, and Ronald Summers. Deep lesion graphs in the wild: relationship learning and organization of significant radiology image findings in diverse large-scale lesion database. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, 2018. [56] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [57] Jiancheng Yang, Rui Shi, Liang Jin, Xiaoyang Huang, Kaiming Kuang, Donglai Wei, Shixuan Gu, Jianying Liu, Pengfei Liu, Zhizhong Chai, Yongjie Xiao, Hao Chen, Liming Xu, Bang Du, Xiangyi Yan, Hao Tang, Adam Alessio, Gregory Holste, Jiapeng Zhang, Xiaoming Wang, Jianye He, Lixuan Che, Hanspeter Pfister, Ming Li, and Bingbing Ni. Deep rib fracture instance segmentation and classification from ct on the ribfrac challenge. IEEE Transactions on Medical Imaging, 2025. [58] Kai Zhang, Rong Zhou, Eashan Adhikarla, Zhiling Yan, Yixin Liu, Jun Yu, Zhengliang Liu, Xun Chen, Brian Davison, Hui Ren, et al. generalist visionlanguage foundation model for diverse biomedical tasks. Nature Medicine, pages 113, 2024. [59] Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In Proceedings of the International Conference on Learning Representations. [60] Xiaoman Zhang, Chaoyi Wu, Ziheng Zhao, Jiayu Lei, Ya Zhang, Yanfeng Wang, and Weidi Xie. Radgenome-chest ct: grounded vision-language dataset for chest ct analysis. arXiv preprint arXiv:2404.16754, 2024. 14 [61] Theodore Zhao, Yu Gu, Jianwei Yang, Naoto Usuyama, Ho Hin Lee, Sid Kiblawi, Tristan Naumann, Jianfeng Gao, Angela Crabtree, Jacob Abel, et al. foundation model for joint segmentation, detection and recognition of biomedical objects across nine modalities. Nature Methods, pages 111, 2024. [62] Weike Zhao, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. Ratescore: metric for radiology report generation. Proceedings of the Conference on Empirical Methods in Natural Language Processing, 2024. [63] Ziheng Zhao, Yao Zhang, Chaoyi Wu, Xiaoman Zhang, Ya Zhang, Yanfeng Wang, and Weidi Xie. One model to rule them all: Towards universal segmentation for medical images with text prompts. arXiv preprint arXiv:2312.17183, 2023. [64] Qiaoyu Zheng, Weike Zhao, Chaoyi Wu, Xiaoman Zhang, Lisong Dai, Hengyu Guan, Yuehua Li, Ya Zhang, Yanfeng Wang, and Weidi Xie. Large-scale long-tailed disease diagnosis on radiology images. Nature Communications, 15(1):10147, 2024. [65] Hong-Yu Zhou, Jiansen Guo, Yinghao Zhang, Xiaoguang Han, Lequan Yu, Liansheng Wang, and Yizhou Yu. nnformer: volumetric medical image segmentation via 3d transformer. IEEE Transactions on Image Processing, 2023. [66] Zongwei Zhou, Md Mahfuzur Rahman Siddiquee, Nima Tajbakhsh, and Jianming Liang. Unet++: Redesigning skip connections to exploit multiscale features in image segmentation. IEEE Transactions on Medical Imaging, 39(6):18561867, 2019."
        },
        {
            "title": "Contents",
            "content": "1 Introduction 2 Taxonomy for Abnormal Findings 3 Dataset Curation"
        },
        {
            "title": "3.2 Data Source .",
            "content": ". . . ."
        },
        {
            "title": "3.3 Annotation Pipeline .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
        },
        {
            "title": "3.5 Summary .",
            "content": ". . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 OminiAbnorm-CT 4.1 Problem Formulation . 4.2 Architecture . 4.3 Training . . . . . . . . . . . 5 Experiments 5.1 Experiment Settings . 5.2 Experiment Results . 5.3 Ablation Study . . . . . . . . . . . . . . 6 Related Work 7 Conclusion Appendix A.1 Qualitative Experiment Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.2 Limitations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . A.3 More Implementation Details of OminiAbnorm-CT . . . . . . . . . . . . . . . . . A.4 Implementation Details of Baselines . . . . . . . . . . . . . . . . . . . . . . . . . A.5 Quality Verification on OminiAbnorm-CT-14K . . . . . . . . . . . . . . . . . . . A.6 The Long-Tail Distribution of OminiAbnorm-CT-14K . . . . . . . . . . . . . . . . A.7 Comparison between OminiAbnorm-CT-14K and Existing Datasets . . . . . . . . A.8 Ablation Study on Context Range . . . . . . . . . . . . . . . . . . . . . . . . . . A.9 Ablation Study on Different Visual Prompts . . . . . . . . . . . . . . . . . . . . . A.10 Generation of Text Queries and Visual Prompts . . . . . . . . . . . . . . . . . . . A.11 Detailed Distribution of Abnormalities in OminiAbnorm-CT-14K . . . . . . . . . 16 3 3 3 4 4 5 5 5 5 6 7 8 9 10 10 17 20 20 21 21 22 22 23 25 26 A.1 Qualitative Experiment Results Fig. 4 presents several examples from the grounded report generation task, comparing the segmentation and report generation results between OminiAbnorm-CT and BiomedParse+LLaVA-Med. The results clearly demonstrate that BiomedParse fails to detect any abnormalities in the latter two cases, consequently leading LLaVA-Med to generate reports that are hardly relevant to the images. In contrast, OminiAbnorm-CT successfully localizes the abnormal regions across all examples and produces comparatively more accurate reports for the abnormalities. Figure 4: Qualitative comparison on the grounded report generation task. 17 In Fig. 5, we demonstrate several examples from the text-guided grounded report generation task, and compare the segmentation and report generation results of OminiAbnorm-CT and BiomedParse+LLaVA-Med. As illustrated in the first two examples, BiomedParse fails to accurately localize the queried abnormalities. While in the latter two examples, LLaVA-Med is unable to correctly interpret the segmentation results when generating reports. In contrast, OminiAbnorm-CT can successfully localize the queried abnormalities across all examples and generates more precise and clinically relevant reports. Figure 5: Qualitative comparison on the text-guided grounded report generation task. In Fig. 6, we demonstrate several representative examples in the visual prompted report generation task, comparing the abnormality descriptions generated by OminiAbnorm-CT, LLaVA-Med, and QWen2.5-VL-7B. It shows that LLaVA-Med and QWen2.5-VL-7B exhibit some fundamental errors in their results. For instance, in the first case, LLaVA-Med misidentifies the kidney as the gallbladder; While in the last case, QWen2.5-VL-7B fails to distinguish between the liver and gallbladder. By contrast, OminiAbnorm-CT is able to precisely capture and identify the abnormal findings highlighted by the visual prompts and generate high quality reports. 18 Figure 6: Qualitative comparison on the visual prompted report generation task. 19 A.2 Limitations As pioneering attempt toward automatic grounding and interpretation of abnormalities on multiplane and whole-body CT images, our work certainly has several limitations: (i) Regarding data annotation, despite providing large-scale dataset, some rare abnormalities (55 out of 404 in the taxonomy) remain uncovered, as shown in Section A.11. Additionally, many abnormalities have very few annotations, which significantly restricts the models capability range. Expanding annotations for these rare categories is treated as our future work. Meanwhile, the CT images in OminiAbnormCT-14K may also exhibit demographic and regional bias. (ii) Due to resource constraints, we currently provide grounding annotation for few representative slices on each CT image, rather than full volumetric segmentation. In the future, we could potentially address this limitation by using interactive segmentation models such as nnInteractive [27]. (iii) Limited by the 3D image perception capability of multi-modal language models, which is also common challenge in current generative visual-language foundation models for medicine [53, 44, 58, 33, 20], we only considered single slice as input when generating reports. Enhancing models ability to reason across adjacent slices and integrate volumetric context represents critical direction for our future research. A.3 More Implementation Details of OminiAbnorm-CT Training Hyperparameters. We adopt 8 NVIDIA A100-80G GPUs for training, with 1 sample per device and gradient accumulation set to 4. We train OminiAbnorm-CT for 250K iterations in total, and use 5% steps for warming up. We take AdamW [36] as optimizer with learning rate of 2e-4. When optimizing the multi-modal language model and the segmentation module jointly, we assign equal weights to the text generation loss and segmentation loss, as defined in Equation 7. We set the rank of LORA to 32. Training Data. We pad and rescale all images to 512512. For training data sampling from four tasks: visual prompted report generation, grounded report generation, text-guided grounded report generation, and general VQA, we control the ratio to be 1:1:1:1. Additionally, in the abnormality grounding task, we maintain ratio of 6:4 between OminiAbnorm-CT-14K and public lesion segmentation data. For each abnormality description, we use the following prompt to have GPT rewrite three different versions, then randomly select one as the ground truth: Your task is to produce THREE different rewrites of medical reports while preserving all original medical content and diagnostic information, changing only the expression style and sentence structure. Rules: 1. Strictly maintain all medical information, diagnostic results, numerical values, and key findings. 2. Do not add any new medical content or conclusions. 3. Do not remove any medical information from the original report. 4. Only change the wording, sentence structure, word order, and vocabulary choices. 5. Maintain professional medical language and terminology. 6. Preserve the overall structure of the original report (such as section divisions). 7. Ensure all rewritten reports remain medically rigorous and accurate. 8. Create THREE distinct rewrites with different phrasing and structure. Input: Original medical report written by doctor Output: THREE rewritten versions with identical content but different expression Please rewrite the following medical report in three different ways: $Findings Description$ Your response must follow this exact format: $Output Template$ 20 A.4 Implementation Details of Baselines In visual prompted generation task, we prompt each abnormality with all types of visual prompts, and take the maximum score. All the methods are prompted with the following template: You are helpful medical assistant. Describe the abnormal findings indicated by the $Visual Prompt$. Please use precise medical terminology, maintain the concise reporting style used in formal radiology reports and provide only the specific radiological findings. Do not list general possibilities, explanations, or recommendations. In grounded report generation task, we integrate LLaVA-Med with 2D segmentation models as baselines. Specifically, we re-implement the MedULS [13] with 2D nnU-Net [26] and the public lesion segmentation datasets in Table 6, covering all the datasets in the official ULS-23 challenge and 10 additional ones. To our knowledge, this represents the segmentation model with the broadest capability range (in terms of abnormality variety) that can be constructed from currently available public datasets. For BiomedParse [61] and LiSA [31], we prompt them with Abnormal findings on the CT image to derive segmentation results for all the abnormalities on the input CT image. Then, we combine each segmentation model with LLaVA-Med by converting the segmentation results into bounding boxes overlaid on the CT image, and prompt LLaVA-Med to generate the report based on these visual cues: You are helpful medical assistant. The abnormal findings are highlighted in red boxes on this CT image, if present. Please describe each abnormal finding indicated by the red boxes using the format Finding 1: [description], Finding 2: [description], etc. Use precise medical terminology, maintain the concise reporting style used in formal radiology reports and provide only the specific radiological findings. Do not list general possibilities, explanations, or recommendations. Respond with dont see any abnormalities on the image. if no abnormalities are present. In text-guided grounded report generation task, we use the simulated text queries as prompts for BiomedParse and LiSA to derive segmentation results for the queried abnormalities, which are detailed in Section 3.4. For MedULS, since it doesnt support text-prompted segmentation, we simply use its unconditioned segmentation results. Similarly, we convert their segmentation results into bounding boxes overlaid on the CT image, and prompt LLaVA-Med to generate the report based on them: You are helpful medical assistant. The abnormal findings are highlighted in red boxes on this CT image, if present. Please describe the abnormal findings indicated by the red boxes. Use precise medical terminology, maintain the concise reporting style used in formal radiology reports and provide only the specific radiological findings. Do not list general possibilities, explanations, or recommendations. Respond with dont see any relevant abnormalities on the image. if no abnormalities are present. A.5 Quality Verification on OminiAbnorm-CT-14K We conducted rigorous and comprehensive quality verification on the annotations in OminiAbnormCT-14K. For each annotator, we randomly sampled 100 annotated images on the axial plane, 50 on the coronal, and 50 on the sagittal plane. Then senior radiologist with 12 years of experience assesses the annotation quality on four key metrics: (i) Detection rate measures the percentage of abnormalities successfully identified and annotated by the annotator, without any omission; (ii) Grounding precision evaluates the percentage of grounding annotations that properly encompass the primary regions of the abnormality, while minimizing false positive areas; (iii) Report concordance quantifies the percentage of description annotations that faithfully reflect the linked abnormal findings on the image; (iv) Classification accuracy measures the percentage of abnormalities that are correctly categorized. Our verification results demonstrated that: (i) 3 out of 4 annotators achieved perfect scores across all four metrics (100%). (ii) Only one annotator had 10 abnormality annotations where the descriptions were not sufficiently accurate and the category labels were also incorrect, resulting in Report concordance and Classification accuracy of 95%, while maintaining 100% Detection rate and Grounding precision. These results confirm the high quality of annotations in our OminiAbnorm-CT-14K. A.6 The Long-Tail Distribution of OminiAbnorm-CT-14K To mitigate the long-tail distribution in OminiAbnorm-CT-14K, we identified underrepresented organs in our annotated corpus and strategically employed GPT-4o to analyze unlabeled reports. Using the following prompt, we efficiently filtered cases containing abnormal findings related to these underrepresented organs: This is report of CT scan: $Report$ Please help me carefully check if the report mentions any abnormal findings that belong to the following anatomical areas: $List of Underrepresented Organs$. If there are, please output YES, otherwise output NO. Do not output any other information. These identified cases were then prioritized in our annotation pipeline. To evaluate the effectiveness of this strategy, we randomly sampled 1,000 images before and after implementation. As shown in Fig. 7, prior to our intervention, the top 20 organs (out of 82 total) accounted for 79.8% of all annotations, while the top 85 abnormality categories (out of 340 total) constituted 80.1%. Following our strategy, the underrepresented organs and rare abnormality categories received notably increased annotation coverage: the representation of underrepresented organs increased by 19.6%, while rare abnormality categories increased by 11.6%, significantly enhancing the diversity of our dataset. Figure 7: The distribution before and after prioritizing annotation for underrepresented organs. (a) Comparison of organ distribution in annotations; (b) Comparison of abnormality category distribution in annotations. A.7 Comparison between OminiAbnorm-CT-14K and Existing Datasets We compare OminiAbnorm-CT-14K with existing public CT image datasets in Table 6, including those widely used for lesion segmentation, lesion detection, organ segmentation, or report generation. In contrast to the substantial limitations exhibited by these datasets, as detailed in Sec. 3.1, OminiAbnorm-CT-14Kepresents the first large-scale dataset designed for abnormality grounding and description across multi-plane and whole-body CT images. A.8 Ablation Study on Context Range In clinical practice, the contextual information provided by adjacent slices is critical for the interpretation of CT images, which is also widely validated in tasks such as medical image segmentation [26] and diagnosis [64]. To investigate the impact of context range on report generation, in the visual prompted report generation task, we adjust the number of input slices and evaluate its impact on Table 6: Comparison of key characteristics between OminiAbnorm-CT-14K and widely-used public CT imaging datasets. Note that CCC18 and DeepLesion has no category label for each annotated lesion. Even though some scans in these datasets are isotropic, they are acquired as axial-plane. Abbreviations: Rpt = Report; Cat = Abnormality Category; Img = Image; Seg. = Segmentation; Det. = Detection; Gen = Generation. Dataset Task Anatomy Plane Rpt #Cat #Img ULS Bone [13] Lesion Seg. Bone ULS Pancreas [13] Lesion Seg. Pancreas MSD Liver [4] MSD Lung [4] MSD Colon [4] Lesion Seg. Lesion Seg. Liver Lung Lesion Seg. Colon MSD Pancreas [4] Lesion Seg. Pancreas COVID19 [39] Lesion Seg. Lung KiTS23 [23] KiPA22 [21] NSCLC [6] LIDC IDRI [5] LNDb [46] INSTANCE22 [34] Lesion Seg. Lesion Seg. Lesion Seg. Lesion Seg. Lesion Seg. Lesion Seg. Kidney Kidney Lung Lung Lung Brain Seg.Rap2023 [37] Lesion Seg. Head & Neck FUMPE [42] RibFrac [57] Lesion Seg. Lesion Seg. Lung Rib Adrenal ACC Ki67 [3] Lesion Seg. Adrenal Gland Axial Axial Axial Axial Axial Axial Axial Axial Axial Axial Axial Axial Axial Axial Axial Axial Axial Axial Axial 1 1 1 1 1 1 2 2 1 1 1 2 1 1 1 1 LNQ2023 [15] NIH-LN [50] CCC-18 [51] Lesion Seg. Lesion Seg. Lung Lung Lesion Seg. Chest & Abdomen Axial DeepLesion [55] Lesion Det. Whole Body TotalSegmentator [52] Organ Seg. Whole Body Axial Axial AbdomenAtlas [47] Organ Seg. Chest & Abdomen Axial CTRATE [18] BIMCV-R [10] Report Gen. Report Gen. Lesion Seg. Chest Chest OminiAbnorm-CT-14K Lesion Det. Whole Body Report Gen. Axial Axial Axial Coronal Sagittal 255 223 151 119 131 126 281 20 489 70 750 236 100 120 35 29 393 175 404 33K 1.2K 8K 26K 8K 10K 2K 1.6K base QWen2.5-VL-7B model. As shown in Table 7, when we increase the input slices, there is no consistent performance improvement on any metric. We speculate this is mainly because multi-modal language models, which generally encode and project each input image into tokens independently, can hardly capture the subtle spatial relationships across adjacent slices. Therefore, we only take the center slice as image input to the multi-modal language model in the subsequent training and evaluation of OminiAbnorm-CT. A.9 Ablation Study on Different Visual Prompts Recent studies have revealed that VLMs exhibit varying perceptual capabilities for different forms of visual prompts [8]. Therefore, in the visual-prompted report generation task, we investigated the impact of different visual prompt formats to give insight on optimal design choices in application. Specifically, for each abnormality in our dataset, we simulated four distinct visual prompt types: center cropping, ellipse, contour, and bounding box, as input and evaluated OminiAbnorm-CTs 23 Table 7: Impact of multi-slice context range on visual prompted report generation, with the base Qwen2.5-VL-7B model as baseline. Abbrevations: = BLEU, RTSc = RaTEScore, BTSc = BERTScore, MTR = METEOR, = Rouge, RG = RadGraph. Input Slices B-1 B-2 B-3 RTSc BTSc MTR R-1 R-L RG Axial (n=2193) 4 Adjacent Slices 2 Adjacent Slices No Adjacent Slices 9.39 9.24 11.57 4 Adjacent Slices 2 Adjacent Slices No Adjacent Slices 10.93 10.89 13. 4 Adjacent Slices 2 Adjacent Slices No Adjacent Slices 9.39 11.48 13.28 2.98 3.03 4.20 3.93 3.85 3.54 2.98 4.16 3.54 0.45 0.46 0. 31.12 31.47 38.35 83.52 83.49 84.10 14.90 15.05 16.68 11.51 11.41 14.26 9.01 8.99 11.09 Coronal (n=750) 0.85 0.73 0.66 32.95 33.06 37.26 83.49 83.48 84.19 15.82 15.87 13.49 13.29 13.34 14.97 9.79 9.85 11. Sagittal (n=591) 0.45 0.94 0.63 31.12 33.86 35.67 83.52 83.32 83.98 14.90 15.70 13.01 11.51 13.64 14. 9.01 9.97 11.34 2.46 2.49 3.85 3.62 3.71 3.37 2.46 3.75 2.91 generated reports respectively. We also take the maximum score among them for each abnormality, representing the theoretical best performance achievable by selecting the optimal visual prompt type for each specific abnormality, depending on its shape, location, and etc. As shown in Table 8, cropping the prompted area consistently underperforms across nearly all metrics (24 out of 27 total measurements), likely due to the loss of crucial contextual information outside the prompted region. The other individual visual prompting methods (ellipse, contour, and bounding box) demonstrate comparable performance. Notably, integrating multiple visual prompting techniques yields substantial improvement across all metrics compared to the best individual prompt for each measure. This indicates significant performance variation when using different visual prompts for the same abnormality, suggesting that selecting appropriate prompt types based on abnormality morphology and location could be an effective strategy. Table 8: Comparison of different visual prompt formats in visual prompted report generation task, with OminiAbnorm-CT fixed as baseline. Abbrevations: = BLEU, RTSc = RaTEScore, BTSc = BERTScore, MTR = METEOR, = Rouge, RG = RadGraph. Prompt Type B-1 B-2 B-3 RTSc BTSc MTR RR-L RG Center Cropping Ellipse Contour Bounding Box Max Center Cropping Ellipse Contour Bounding Box Max Center Cropping Ellipse Contour Bounding Box Max 11.70 12.43 12.60 12.35 18. 10.94 12.49 12.61 12.15 18.38 10.98 11.37 11.08 11.29 17.29 2.87 3.60 3.79 3.69 6.65 2.96 3.79 3.73 3.70 7.19 2.73 3.10 2.95 2.98 6.48 Axial (n=2193) 0.68 1.15 1.22 1.26 1.80 33.03 35.14 35.74 35.46 42.81 84.84 84.87 85.09 84.92 86.35 Coronal (n=750) 0.80 1.13 1.02 1.12 2.39 33.57 36.09 35.50 35.12 42. 84.48 84.56 84.71 84.38 86.00 Sagittal (n=591) 0.39 0.65 0.57 0.67 1.82 32.23 34.36 34.19 34.07 41.77 84.20 84.01 84.22 84.13 85.62 12.77 13.67 14.12 13.65 19.40 11.92 13.44 13.45 13.27 19.00 11.98 12.19 12.01 12.16 17.45 14.58 15.55 15.97 15.76 21.66 14.23 15.94 15.75 15.68 21.30 14.45 14.45 14.38 14.84 20. 11.75 12.63 12.92 12.80 17.61 11.36 12.43 12.25 12.13 16.87 11.01 10.95 11.10 11.24 15.87 3.92 5.27 5.81 5.63 9.98 3.87 5.16 4.87 4.68 9.44 3.05 3.46 3.22 3.45 7. A.10 Generation of Text Queries and Visual Prompts We simulate four visual prompts for each annotated abnormality, mimicking how clinicians would select the most appropriate highlighting method based on the abnormalitys shape and location: (i) cropped region. We extract the minimum bounding box that completely contains the annotated lesion region, add 50-pixel padding around this region, and crop the original image to center the abnormality; (ii) ellipse. We fit an optimal ellipse to the largest contour extracted from the lesion mask. (iii) contour. We smooth the lesion mask using Gaussian blur, detect its contours, and refine them with polygon approximation. (iv) bounding box. We identify the minimum bounding box that completely contains the annotated lesion region, and add 10-pixel padding around. To simulate radiologists approaching CT images with prior knowledge, we provide GPT-4o with patient information (complaints, medical history, etc) to generate text queries that inquire specific abnormality based on such preliminary information. We use the following prompt template: Assuming CT image has one or more abnormal findings, will provide detailed information about them. Please help me generate prompts to test VLMs ability to localize and analyze specific abnormalities. Requirements: 1. Generate prompts in English. 2. Ensure accurate information. The prompt content must stem from the real abnormality information, CT image reports, and doctors discussion results provided. You may use equivalents expressions in medical terminology, but do not introduce any content beyond these provided information. 3. Clear indication. Ensure each prompt refers to the corresponding abnormality without confusion with other abnormalities in the image. 4. Clear task. Each prompt should end with clear request for the VLM to perform localization and analysis tasks. 5. Simulate realistic pre-examination clinical queries. Create prompts that reflect how clinicians approach CT images with preliminary information (such as the patients complaints, medical history, other test results, etc., if available) and medical knowledge (such as common abnormalities associated with the patients information or typically found in this imaged region), without being overly specific. Importantly, prompts should be broad enough to guide examination of suspicious areas and must not include detailed descriptions or conclusions from findings, reports and discussion results that would only be available after examining the CT image, such as specific abnormality details, exact measurements, precise locations, or definitive characteristics that could only be determined after image interpretation. Some appropriate examples: $Some Example Queries$ 6. Avoid data leakage. To evaluate the VLMs ability to localize and analyze abnormalities independently, do not provide complete findings or diagnostic conclusions in the query. This prevents the VLM from bypassing the analytical process by retrieving answers directly from the prompt, while maintaining challenge authenticity. 7. Diversity. Generate at least 1 and at most 5 prompts with different perspectives for each abnormality. Each prompt should have clearly different focuses, avoiding content redundancy. The information about all the abnormalities on the CT images: $Abnormal Findings$ The clinical presentation of the patient corresponding to this CT image is: $Presentation$ The overall report for the CT image containing these abnormalities is: $Whole Report$ The doctors discussion results for the patient corresponding to this CT image are: $Impression$ Your response must follow this exact format: $Output Template$ 25 A.11 Detailed Distribution of Abnormalities in OminiAbnorm-CT-14K Figure 8: Distribution of OminiAbnorm-CT-14K across 82 anatomical structures and 40 major systems or organs. Darker blue indicates higher sample density. Abd. & Pelv. = Abdominal and Pelvic Cavity; Test., Epid., Scrot. = Testes, Epididymis, Scrotum; Morph. = Morphology; Pos. = Positional; Vent. & Subarach. = Ventricles and Subarachnoid. 26 Table 9: Detailed distribution of abnormality categories in OminiAbnorm-CT-14K. Organ Anatomical Structure Category Axial Coronal Sagittal Cerebral parenchyma Brain Ventricles and cisterns Meninges (including dura mater, pia mater and arachnoid mater) Pituitary and Sellar Region Spinal Cord Spine Intervertebral disc Spinal meninge (including dura mater, arachnoid mater, and pia mater) Brain parenchymal atrophy Brain parenchymal edema Brain parenchymal soft tissue mass Brain parenchymal thin-walled cystic mass Brain parenchymal thick-walled cystic mass Brain parenchymal hemorrhage or contusion Acute infarct Lacunar infarct Encephalomalacia Brain parenchymal morphological altrenation Other non-mass effect lesions Intracranial air Hyperdense lesions in brain parenchyma Others Ventricular or cisternal enlargement Ventricular or cisternal soft tissue mass Ventricular or cisternal cystic mass Ventricular or cisternal hemorrhage Others Meningeal cystic mass Meningeal soft tissue mass Meningeal hemorrhage Meningeal effusion Meningeal thickening Others Pituitary stalk thickening Pituitary stalk lateral displacement Pituitary enlargement Pituitary atrophy Pituitary calcification Pituitary or sella region soft tissue mass Pituitary or sella region cysitc mass Others Spinal cord compression Spinal cord soft tissue mass Spinal cord hemorrhage or contusion Spinal morphological alteration Syringomyelia Others Intervertebral disc morphological alteration Intervertebral disc ossification or calcification Intervertebral disc gas Intervertebral disc extrusion Intervertebral disc soft tissue mass Others Spinal meningeal hemorrhage Spinal meningeal effusion Spinal meningeal thickening Meningeal soft tissue mass Meningeal cystic mass Others 27 31 49 280 52 198 88 54 27 102 44 14 72 35 122 49 13 60 11 4 8 137 32 6 4 0 0 0 0 0 20 3 1 3 12 0 0 0 4 4 4 0 5 0 0 0 0 7 0 2 2 13 57 1 0 42 1 11 31 5 8 24 13 25 8 1 4 2 0 9 27 9 0 0 0 0 0 2 0 16 5 0 0 1 0 0 0 0 2 0 3 0 1 0 0 0 0 2 2 1 4 4 3 1 31 0 8 3 23 5 10 44 6 25 6 0 3 3 0 6 10 3 0 2 0 0 2 4 3 27 6 0 2 6 0 0 0 1 0 2 0 1 1 0 1 0 6 0 5 Table 10: (Continued) Detailed distribution of abnormality categories in OminiAbnorm-CT-14K. Organ Anatomical Structure Category Axial Coronal Sagittal Eyeball Eye Ocular Adnexa Lacrimal gland and lacrimal sac External Ear Middle Ear Ear Inner Ear Sinus cavity Sinus ostium Nasal septum Sinus 1 7 20 3 3 0 2 36 4 43 3 0 12 2 0 1 5 1 0 23 2 2 0 2 1 4 0 2 1 8 3 85 5 25 3 4 0 1 6 1 2 2 6 14 1 0 0 0 3 11 7 0 3 5 0 1 2 1 0 0 3 0 0 1 0 0 0 0 0 0 0 3 33 1 7 2 0 1 0 0 0 0 Eyeball atrophy Eyeball positioning or morphological alteration Eyeball density alteration Eyeball soft tissue mass Eyeball wall thickening morphological alteration Complete or partial absence of eyeball structure Others Orbital density changes Intraorbital gas (e.g., soft tissue mass, fluid accumulation) Extraocular muscle hypertrophy or atrophy Optic nerve thickening or soft tissue mass Optic nerve atrophy Others Lacrimal gland and lacrimal sac enlargement or mass Lacrimal gland and lacrimal sac calcification or fluid Others External auditory canal stenosis or atresia External auditory canal soft tissue mass Others Middle ear fluid or hemorrhage Middle ear soft tissue mass Ossicular chain destruction or deformity Tympanic membrane thickening or calcification Middle ear gas density change Others Inner ear congenital structural alteration Inner ear bone destruction or sclerosis Internal auditory canal enlargement or narrowing Labyrinthine structural alteration Others Sinus effusion Sinus hemorrhage Sinus soft tissue mass Sinus cystic mass Sinus mucosal thickening Others Sinus obstruction or stenosis Sinus widening Others Nasal septum deviation or thickening Nasal septum perforation or defect Others 4 44 45 19 5 2 2 54 4 16 7 0 3 4 1 5 11 1 5 23 7 2 6 1 2 11 2 4 1 18 6 98 8 24 9 1 0 1 7 1 6 28 Table 11: (Continued) Detailed distribution of abnormality categories in OminiAbnorm-CT-14K. Organ Anatomical Structure Category Axial Coronal Sagittal Pharynx (including nasopharynx, oropharynx, and hypopharynx) Pharynx Larynx Pharyngeal space Parotid gland Parotid gland Thyroid gland Thyroid gland Trachea Tracheal lumen pulmonary parenchyma Lung Bronchi Pharyngeal narrowing or obstruction Pharyngeal soft tissue mass Pharyngeal cystic mass Pharyngeal wall thickening Pharyngeal foreign body Others Laryngeal narrowing or obstruction Vocal cord asymmetry Vocal cord soft tissue mass Laryngeal cartilage calcification Laryngeal cystic mass Laryngeal foreign body Others Pharyngeal space soft tissue mass Pharyngeal space cystic mass Pharyngeal emphysema Others Parotid gland enlargement Parotid gland atrophy Parotid gland soft tissue mass Parotid gland cystic mass Parotid gland calcification or stone Others Thyroid enlargement Thyroid atrophy Thyroid soft tissue mass Thyroid cystic mass Thyroid calcification Ectopic thyroid gland Others Tracheal stenosis or obstruction Tracheal dilatation Tracheal soft tissue mass Tracheal wall thickening Tracheal wall calcification Tracheal wall defect Others Atelectasis Incomplete lung expansion Pulmonary consolidation Pulmonary ground-glass opacities Pulmonary emphysema Pulmonary solitary nodule or mass Pulmonary diffusely distributed multiple nodules Pulmonary parenchymal fibrosis Pulmonary thin-walled cavitation Pulmonary thick-walled cavities Pulmonary cystic mass Others Bronchiectasis Bronchial wall thickening, stenosis, or occlusion Bronchial foreign body Others 4 46 10 4 0 3 3 2 6 0 1 1 1 46 13 7 5 20 4 33 16 15 10 12 1 41 3 1 5 6 12 11 12 9 3 3 1 106 52 203 343 45 226 285 42 48 44 53 14 97 15 4 3 2 14 2 3 0 4 2 0 1 0 1 0 0 23 6 0 7 2 0 2 0 0 0 2 0 4 1 0 1 0 4 5 6 0 1 0 0 10 3 24 52 10 76 12 6 1 14 6 19 11 1 0 1 10 4 0 0 1 0 0 1 0 0 0 0 16 2 0 1 0 0 2 0 2 0 2 0 0 0 0 0 1 7 2 1 0 4 2 2 3 1 2 5 2 10 0 1 0 1 1 6 5 0 1 Table 12: (Continued) Detailed distribution of abnormality categories in OminiAbnorm-CT-14K. Organ Anatomical Structure Category Axial Coronal Sagittal Lung interstitial Lung Pleura Mediastinum Mediastinal soft tissue Diaphragm Diaphragm Thymus Thymic parenchyma Oral Soft Tissue Oral cavity Teeth and alveolar bone Cardiac chambers (atrium or ventricle) Myocardium Heart valve Pericardium Heart Lung interstitial fibrosis and thickening Honeycomb lung Others Pleural thickening Pleural effusion Pneumothorax Pleural calcification Pleural soft tissue mass Others Mediastinal shift Mediastinal soft tissue mass Mediastinal cystic mass Mediastinal hemorrhage Mediastinal emphysema Others Diaphragmatic hernia Diaphragmatic elevation Diaphragmatic soft tissue mass Others Thymic enlargement Thymic atrophy Thymic soft tissue mass Thymic cystic mass Thymic calcification Others Oral soft tissue mass Oral cystic mass Oral soft tissue calcification Others Dental developmental anomalies Dental positional anomalies Dental calcification or caries Alveolar bone resorption or hyperplasia Alveolar soft tissue mass Alveolar cystic mass Others Cardiac chamber enlargement Cardiac chamber mass Others Myocardial hypertrophy Myocardial thinning Myocardial calcification Myocardial density alteration Others Valvular calcification Others Pericardial thickening Pericardial calcification Pericardial effusion Pericardial hemorrhage Pericardial emphysema Others 30 78 34 2 39 268 36 24 50 8 29 200 19 15 58 10 19 5 7 0 1 0 6 0 4 1 8 3 0 0 1 3 3 3 8 8 0 41 28 17 4 3 4 6 3 3 3 9 6 54 2 6 4 10 0 0 10 27 2 0 8 1 4 31 0 0 7 0 16 4 2 2 0 0 0 0 0 0 0 0 0 0 0 4 0 0 2 1 1 7 7 1 1 0 1 0 3 0 0 0 1 7 0 1 0 0 0 0 1 3 0 1 0 1 0 15 2 0 1 0 12 0 0 0 0 0 0 0 0 0 0 0 0 0 0 5 0 0 0 1 0 3 2 0 1 0 0 0 0 0 0 2 0 2 0 0 0 Table 13: (Continued) Detailed distribution of abnormality categories in OminiAbnorm-CT-14K. Organ Anatomical Structure Category Axial Coronal Sagittal Heart Coronary arteries Vascular structure Artery Vein Capillary Gastrointestinal lumen Gastrointestinal tract (including esophagus, stomach, and intestines) Gastrointestinal tract wall Gastrointestinal positioning abnormality Gastrointestinal morphological abnormalities Mesentery Coronary artery myocardial bridge Coronary artery dilation Others Arterial widening Aneurysm Atherosclerotic plaque of arterial wall Arterial wall ulcer Arterial dissection or intramural hematoma Arterial stenosis Arterial occlusion Arterial filling defect Arterial contour abnormality Arteriovenous fistula Arterial wall inflammatory exudate Others Venous dilation Varicosity Venous wall inflammation Venous stenosis Venous occlusion Venous filling defect Venous morphological abnormality Venous wall inflammatory exudate Others Capillary dilation Capillary malformation proliferation Others Gastrointestinal dilatation Gastrointestinal narrowing Gastrointestinal foreign body Gastrointestinal air-fluid level Gastrointestinal luminal contents abnormal density Others Gastrointestinal wall thickening Gastrointestinal wall mass Gastrointestinal wall rupture and perforation Gastrointestinal wall ulceration Others Gastrointestinal herniation or deformity Others Gastrointestinal diverticulum Gastrointestinal malrotation and volvulus Gastrointestinal annular or concentric abnormality Others Mesenteric volvulus Mesenteric edema Mesenteric panniculitis Mesenteric soft tissue mass Others 31 1 0 3 66 138 39 8 22 24 55 80 51 16 4 39 41 23 0 7 5 48 29 2 13 0 1 0 265 20 2 17 8 398 103 43 0 7 117 5 53 28 2 3 10 26 28 0 0 0 1 24 44 13 2 8 9 23 23 7 0 9 4 11 0 1 2 12 6 1 8 0 0 0 212 5 6 8 6 3 212 56 12 0 9 1 35 17 22 1 4 3 6 6 4 0 0 0 9 39 4 8 10 0 8 23 1 0 7 6 9 0 1 1 3 3 0 0 0 0 0 112 6 0 5 4 1 120 26 4 0 30 1 15 3 10 2 2 3 6 9 2 Table 14: (Continued) Detailed distribution of abnormality categories in OminiAbnorm-CT-14K. Organ Anatomical Structure Category Axial Coronal Sagittal Liver Hepatic parenchyma Gallbladder lumen Gallbladder wall Gallbladder Gallbladder morphology Extrahepatic bile ducts Pancreas Pancreatic parenchyma Pancreatic duct Pancreatic morphology Splenic parenchyma Spleen Spleen morphology Hepatic parenchymal morphological alteration Hepatic parenchymal hyperdensity Hepatic parenchymal hypodensity Hepatic parenchymal soft tissue mass Hepatic parenchymal thin-walled cystic mass Hepatic parenchymal thick-walled cystic mass Liver contusion or hemorrhage Intrahepatic bile duct emphysema Intrahepatic bile duct fluid accumulation Intrahepatic biliary stones or calcification Others Gallbladder distension Gallbladder atrophy or shrinkage Gallbladder stone Gallbladder contents density change Others Gallbladder wall thickening Gallbladder wall calcification Gallbladder wall mass Gallbladder wall rupture Others Gallbladder position alteration Gallbladder congenital morphological variation Others Extrahepatic bile duct dilation Extrahepatic bile duct wall thickening Extrahepatic bile duct soft tissue mass Extrahepatic bile duct cystic mass Extrahepatic bile duct stenosis or obstruction Extrahepatic bile duct content density alteration Extrahepatic bile duct stone Extrahepatic bile duct injury or rupture Others Pancreatic parenchymal soft tissue mass Pancreatic cystic mass Pancreatic calcification Pancreatic enlargement Pancreatic atrophy Others Pancreatic ductal dilatation Pancreatic ductal stone Others Pancreatic congenital anomaly Pancreatic positional displacement Others Splenic parenchymal calcification Splenic parenchymal soft tissue mass Splenic parenchymal cystic mass Splenic parenchymal infarct Splenic parenchymal rupture Others Spleen enlargement Others 32 99 18 71 439 124 21 10 13 0 7 16 39 4 87 32 3 53 9 16 11 3 1 0 1 26 2 1 1 2 6 15 0 1 137 69 20 60 8 16 13 3 0 5 1 1 15 52 27 19 12 18 102 17 34 2 25 58 23 6 3 1 0 0 4 10 1 22 5 1 11 1 2 1 1 0 0 1 8 0 0 1 0 1 5 0 0 8 7 1 1 1 0 3 0 0 0 0 0 0 11 10 1 1 2 22 1 11 0 9 20 10 4 0 0 0 0 1 1 0 2 0 0 1 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 0 4 4 0 1 1 0 0 0 0 0 0 0 1 3 0 0 0 0 5 0 Table 15: (Continued) Detailed distribution of abnormality categories in OminiAbnorm-CT-14K. Organ Anatomical Structure Category Axial Coronal Sagittal Abdominopelvic peritoneum Abdominopelvic peritoneum Renal Parenchyma Kidney Renal pelvis and ureter Renal morphology and position abnormalities Bladder cavity Bladder Bladder wall Bladder morphology Peritoneal inflammatory exudate Peritoneal thickening Peritoneal calcification Peritoneal soft tissue mass Peritoneal cystic mass Abdominopelvic fluid Abdominopelvic hemorrhage Abdominopelvic free air Retroperitoneal fibrosis Peritoneal or retroperitoneal lymph enlargement Extravasation of gastrointestinal content Abdominopelvic contrast agent leakage Others Renal parenchymal soft tissue mass Renal parenchymal cystic mass Renal parenchymal calcification Renal parenchymal or subcapsular hemorrhage Renal infarct Others Hydronephrosis Ureteral dilatation Ureteral stricture or obstruction Ureteric stone Double renal pelvis and/or double ureter anomaly Renal pelvis soft tissue mass Renal pelvic cystic mass Others Renal morphological anomaly Renal enlargement Renal atrophy Ectopic or transplanted kidney Others Bladder distention Bladder stone Bladder content density change Others Bladder wall diffuse thickening Bladder wall calcification Bladder wall focal thickening or soft tissue mass Bladder wall defect or fistula Bladder wall emphysema Bladder wall diverticulum Others Bladder position displacement Bladder morphological anomaly Others 33 75 19 5 364 102 280 68 61 1 0 22 12 150 239 9 11 8 27 81 35 1 59 5 13 1 4 33 56 23 17 3 4 24 13 4 17 1 4 9 31 1 9 2 1 19 3 1 135 34 96 3 17 0 18 1 13 12 81 68 9 3 3 8 63 29 1 3 4 3 5 20 16 6 13 2 0 5 2 3 5 3 7 0 0 2 0 2 0 0 10 1 0 90 22 43 1 10 0 0 4 2 29 21 2 2 0 1 3 4 0 19 1 0 0 2 1 1 2 5 0 1 3 1 0 6 1 1 1 2 0 2 0 2 Table 16: (Continued) Detailed distribution of abnormality categories in OminiAbnorm-CT-14K. Organ Anatomical Structure Category Axial Coronal Sagittal Adrenal gland Adrenal gland Prostate Prostate Seminal vesicle Seminal vesicle Testis Testes, epididymis, and scrotum Scrotum Epididymis Penis Penis Uterus Uterus Adrenal soft tissue mass Adrenal cystic mass Adrenal calcification Adrenal gland thickening Adrenal atrophy Others Prostate enlargement Prostate atrophy Prostatic cystic mass Prostatic soft tissue density anomaly Prostatic calcification Prostatic hemarrhage Others Seminal vesicle soft tissue mass Seminal vesicle calcification Seminal vesicle cystic mass Others Testicular enlargement Testicular atrophy Testicular soft tissue mass Testicular calcification Testicular cystic mass Testicular torsion Testicular hemorrhage and rupture Others Scrotal effusion Scrotal hematoma Scrotal soft tissue mass Scrotal wall thickening Others Epididymis enlargement Epididymal soft tissue mass Epididymal calcification Epididymal cystic mass Epididymal thickening Others Penile morphological anomaly Penile soft tissue mass Penile calcification Urethral calculi or foreign body Urethral Stricture Urethral dilation Others Uterine morphological anomaly Uterine enlargement Uterine soft tissue mass Uterine calcification Uterine cavity effusion Uterine cavity hemorrhage Uterine cystic mass Others 34 122 32 0 16 0 15 18 0 2 2 2 0 0 1 0 3 0 1 0 1 0 0 0 0 0 1 0 1 0 2 0 0 0 0 0 0 0 0 0 6 0 1 4 7 15 50 7 2 2 4 4 17 0 3 0 0 0 12 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 1 3 4 0 0 0 0 1 0 0 0 0 0 14 0 0 1 2 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 6 7 2 1 0 0 1 Table 17: (Continued) Detailed distribution of abnormality categories in OminiAbnorm-CT-14K. Organ Anatomical Structure Category Axial Coronal Sagittal Fallopian tube Fallopian tube Ovary Ovary Vagina and vulva Vagina and vulva Breast gland Breast Breast duct Nipple Areola Skeletal system Skeletal system Fallopian tube thickening Fallopian tube cystic mass Fallopian tube soft tissue mass Fallopian tube effusion Fallopian tube hemorrhage Fallopian tube calcification Others Ovarian enlargement Ovarian Atrophy Ovarian cystic mass Ovarian soft tissue mass Ovarian calcification Ovarian torsion Others Vaginal soft tissue mass Vaginal cystic mass Vaginal hemorrhage Vaginal emphysema Vaginal anatomical anomaly Others Breast gland enlargement Breast gland atrophy Breast gland soft tissue mass Breast gland calcification Breast gland cystic mass Others Breast duct dilation Breast duct calcification Others Nipple retraction Nipple calcification Others Areola thickening Others Osteoporosis Osteomalacia Bone destruction or soft tissue mass Bone cystic mass Osseous sclerosis Osteonecrosis Bone fracture Periosteal reaction Periosteal thickening Bone callus and post-fracture healing Scar of fracture fixation removal Bone deformation Skeletal asymmetry Cartilage calcification Chondral calcification Others 35 3 2 12 2 0 0 0 9 0 85 96 3 9 1 0 9 1 3 3 6 2 0 36 0 0 6 0 0 0 0 0 0 1 0 9 2 219 19 179 64 308 23 8 10 0 67 10 4 5 30 0 0 4 0 0 0 0 3 0 12 6 1 0 1 0 0 0 0 0 0 0 0 9 0 0 0 0 0 0 0 0 0 0 0 1 0 124 32 80 13 172 9 5 8 0 44 2 4 0 50 0 1 2 0 0 0 0 0 0 6 3 0 0 0 0 0 0 4 1 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 8 1 117 17 121 24 247 5 1 2 0 73 1 8 1 Table 18: (Continued) Detailed distribution of abnormality categories in OminiAbnorm-CT-14K. Organ Anatomical Structure Category Axial Coronal Sagittal Joint Joint Muscle Muscle Skin and subcutaneous fat Skin and subcutaneous fat Agenesis or ectopia Agenesis or ectopia Implantation of artificial object Implantation of artificial object 3 3 2 1 0 2 0 4 0 2 22 2 2 1 0 2 3 1 0 0 13 1 1 28 2 1 15 2 0 0 6 9 14 5 1 0 53 12 0 0 1 0 3 0 0 0 5 14 1 12 5 1 9 13 0 0 0 3 0 4 47 6 3 2 10 0 0 3 5 35 16 7 0 91 3 Joint space narrowing Joint space widening Joint cartilage degradation Joint cartilage calcification Joint capsule thickening Intra-articular effusion Intra-articular hemorrhage Intra-articular gas Joint periarticular soft tissue swelling Irregular articular surfaces Joint subluxation or dislocation Intra-articular loose body Others Muscle swelling Muscle atrophy Muscular soft tissue mass Muscular cystic mass Muscular hemorrhage Muscle calcification Muscle open injury and tear Tendon calcification Tendon tear or rupture Others Subcutaneous soft tissue mass Subcutaneous edema Subcutaneous effusion Subcutaneous inflammatory exudate Subcutaneous swelling Subcutaneous open wound and laceration Subcutaneous calcification Subcutaneous fat necrosis Abdominal wall hernia Others Congenital developmental anomaly Postoperative changes Situs inversus Others Implantation of artificial object Others 6 3 1 3 0 13 4 1 12 30 8 5 8 13 40 11 7 3 0 19 1 7 141 18 26 10 60 15 12 2 48 33 42 10 2 179"
        }
    ],
    "affiliations": [
        "Department of Radiology, Renmin Hospital of Wuhan University",
        "School of Artificial Intelligence, Shanghai Jiao Tong University",
        "Shanghai Artificial Intelligence Laboratory"
    ]
}