{
    "paper_title": "Vector-ICL: In-context Learning with Continuous Vector Representations",
    "authors": [
        "Yufan Zhuang",
        "Chandan Singh",
        "Liyuan Liu",
        "Jingbo Shang",
        "Jianfeng Gao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLM's embedding space through lightweight projectors, we observe that LLMs can effectively process and learn from these projected vectors, which we term Vector-ICL. In particular, we find that pretraining projectors with general language modeling objectives enables Vector-ICL, while task-specific finetuning further enhances performance. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and domain-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 8 ] . [ 1 9 2 6 5 0 . 0 1 4 2 : r Preprint. VECTOR-ICL: IN-CONTEXT LEARNING WITH CONTINUOUS VECTOR REPRESENTATIONS Yufan Zhuang1, Chandan Singh2, Liyuan Liu2, Jingbo Shang1, and Jianfeng Gao2 1UC San Diego 2Microsoft Research"
        },
        {
            "title": "ABSTRACT",
            "content": "Large language models (LLMs) have shown remarkable in-context learning (ICL) capabilities on textual data. We explore whether these capabilities can be extended to continuous vectors from diverse domains, obtained from black-box pretrained encoders. By aligning input data with an LLMs embedding space through lightweight projectors, we observe that LLMs can effectively process and learn from these projected vectors, which we term Vector-ICL. In particular, we find that pretraining projectors with general language modeling objectives enables VectorICL, while task-specific finetuning further enhances performance. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and domain-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms."
        },
        {
            "title": "INTRODUCTION",
            "content": "In-context learning (ICL) has emerged as powerful paradigm in large language models (LLMs), allowing generalization from limited examples within given context [9, 41]. By providing demonstrations in the context during inference, ICL allows models to adapt to new tasks and formats without the need for retraining. However, since LLMs are trained on discrete natural language tokens, ICL is generally learned and used through natural language, limiting its applicability to non-textual data. We explore whether LLMs can perform ICL directly on continuous vectors, capability that could dramatically expand their applicability. Many data modalities, such as sensor readings, financial time series, or scientific measurements, lack natural text representation. Moreover, even for text data, information like numbers might be better represented via continuous vectors than token sequences. In our study, we observe that LLMs can indeed understand and process continuous context via embedding projection. This technique, which we term Vector-ICL, acts as bridge between continuous data and the LLMs embedding space. Simple linear projections are often sufficient, though for cross-modal taskssuch as those involving non-textual data like time-series or graphs, non-linear transformations may be required. We demonstrate that training the embedding projector using straightforward next-token prediction objective enables Vector-ICL, effectively teaching the LLM to read continuous vectors. Moreover, fine-tuning the projector on downstream tasks further enhances the effectiveness of continuous context, outperforming few-shot ICL and domain-specific models or tuning. Our investigation begins with the task of text reconstruction, where we assess whether LLMs can recover information encoded in text embedding. This serves as proof-of-concept for Vector-ICL, showing that LLMs can indeed extract meaningful information from projected continuous vectors. We then turn to the more complex challenge of arithmetics. Although state-of-the-art LLMs can Code will be available at: https://github.com/EvanZhuang/vector-icl. Preprint. Figure 1: Comparing regular in-context learning to vector in-context learning. (a) In regular ICL, textual demonstrations are given as context during LLM inference. (b) In Vector-ICL, the input space is extended across multiple modalities. The input data is first encoded as embeddings, then transformed into continuous vectors which represent as box tokens () via embedding projection. During inference, we provide box tokens in prompts as demonstrations for ICL. We consider box tokens representing text, numerical data, brain fMRI, time series, and graphs in this study. solve Olympiad mathematical problems [51, 41], they struggle with precise number processing due to the limitations inherent in their tokenization schemes. Our results demonstrate that Vector-ICL offers more effective approach for function approximation, particularly for large numbers that span multiple tokens, potentially opening new avenues for enhancing LLMs numerical reasoning capabilities. Finally, we extend our analysis to broad range of modalities and tasks, including text classification, summarization, molecule captioning, brain fMRI reconstruction and classification, time-series classification, and graph classification. Across these diverse domains, LLMs exhibit competitive and often superior performance when employing Vector-ICL, revealing previously untapped capabilities of these models. This work highlights the potential of continuous representations in enhancing LLMs in-context learning capacities, pushing the boundaries of what these models can achieve beyond token-based paradigms."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Empirical results of in-context learning ICL has empirically shown strong performance in diverse natural-language processing tasks with very few demonstrations [9, 41]. In modern LLMs with long context windows, ICL has even shown performance improvements as the number of demonstrations grows to hundreds or even thousands, sometimes outperforming finetuning [1, 25, 7]. Empirically, different factors play key roles in ICL. In smaller LLMs, ground-truth demonstrations are not required for in-context learning, while other factors such as the label space, input text distribution, and overall sequence format play an important role [35]. Moreover, these LLMs can sometimes achieve strong performance even when demonstrations are intentionally irrelevant or even pathologically misleading [53]. More recently, Wei et al. [54] characterize these behaviors of LLMs with respect to model size, and show that larger language models perform in-context learning differently in the presence of flipped or semantically unrelated labels. Orthogonally, different works find ways to improve ICL, e.g. by including explanations [22], or chaining ICL calls [36]. ICL has shown some success in multimodal models [57, 21] or when applied to tabular data [64]. Understanding ICL Many works have investigated ICL and found that it is able to learn linear models [3, 62], discrete functions [8], and more general algorithms [27]. Some works have explicitly connected ICL in specific settings to implementing optimization steps analogous to gradient descent [30, 52, 2] and higher-order optimization methods [11, 15, 16, 62]. complementary di2 Preprint. Figure 2: Pretraining and finetuning the projectors. Vector-ICL requires updating the parameters of lightweight projector while keeping the encoder and decoder parameters fixed. The encoder first compresses the input into single token embeddings, and then the projector will project it to the aligned representation space for LLMs later use. (a) Pretraining the projector on general language modeling corpus (or modality-to-text dataset) enables Vector-ICL. (b) Task-specific fine-tuning makes Vector-ICL outperform few-shot ICL on natural language tasks, as well as with domainspecific models on non-language tasks. rection aims to establish statistical complexity and generalization bounds of in-context learning in transformers [5, 27, 55, 56]. Finally, one recent work suggests that ICL may arise from parallel structures in pretraining data [10]. Learning to learn in-context In contrast to the emergent ICL capabilities of LLMs, existing works have also studied how to explicitly improve ICL. Min et al. [34] propose MetaICL, metatraining framework for finetuning pretrained LLMs to perform in-context learning on large and diverse collection of tasks. In the tabular domain, TNP [39] and PFNs [37] train transformer models to perform in-context prediction for family of functions, which allows in-context generalization to unseen functions after training. Zhao et al. [63] also propose meta-learning transformers to incontext learn group preferences, serving as an in-context learned reward model that adapts to diverse group preferences."
        },
        {
            "title": "3 METHOD: VECTOR CONTEXT VIA EMBEDDING PROJECTION",
            "content": "3.1 EMBEDDING PROJECTION To perform Vector-ICL, an input must be transformed into vector context through an embedding projection. Given dataset = {xi}n i=1, we assume the existence of an encoder fenc, that transforms the data into an abstract representation (alternatively, the raw data may already be continuous vector). The encoded embeddings, fenc(x), are then projected into box tokens, denoted as x. Throughout the paper, we will use the terms box tokens and projected embeddings interchangeably. For the decoding step, we employ an LLM, referred to as fllm, to generate the output given the prompts. We impose no constraints on the form of the input data ; it can come from any modality. The only requirement is that the encoder fenc maps each data point into vector space, defined as: fenc : Rdenc , The LLM typically processes discrete tokens {tok1, tok2, . . . , tokl}, then maps them to text embedding space {emb1, emb2, . . . , embl}, i, embi Rddec . Since we operate mostly in embedding space, we omit the tokenization step for simplicity and directly refer to text inputs as their embedding representations. (1) The process of embedding projection is then carried out as follows. For linear projection, we construct projection matrix Rdencddec and make the following transformations to obtain projected embedding given input x: := fenc(x) (2) 3 Preprint. Figure 3: Main results: LLMs can perform Vector-ICL ( = better). We show that training the embedding projector with simple next-token prediction objective enables Vector-ICL. Fine-tuning the projector on downstream tasks further improves the use of continuous context, surpassing the performance of few-shot ICL and domain-specific baseline models. The study begins with text reconstruction to assess LLMs ability to interpret box token embeddings, followed by function regression to evaluate reasoning capabilities. We then demonstrate Vector-ICLs effectiveness and applicability across various downstream tasks, including text classification, summarization, timeseries classification, graph classification, and brain fMRI decoding & classification. Results in each panel are averaged over different encoders and LLMs for the diverse tasks we study; error bars show 95% confidence intervals. In cases where more expressive power is needed, we utilize two-layer multi-layer perceptron (MLP) to perform the projection: := MLP(fenc(x)) (3) The MLP follows the architecture of the MLP block in Llama [50], with an additional input projection layer to map the input dimension Rdenc to output dimension Rddec . 3.2 PROJECTED EMBEDDINGS AS CONTEXT The projected embeddings are then utilized as context in Vector-ICL, functioning as the equivalent of the original input data. For example, in NLP tasks, the original text snippets will first be encoded as embeddings fenc(x), then projected to become x, inserted into the prompt like the following: xs sentiment is . . . / xs summarization is . . . Preprint. Using them as the context in ICL is then natural: 1s sentiment is happy. 2s sentiment is sad. xs sentiment is . . . where 1 and 2 are in-context examples and is the input."
        },
        {
            "title": "3.3 TRAINING THE EMBEDDING PROJECTORS",
            "content": "The projectors need to be trained to achieve effective projections. We discovered that pretraining these projectors with language modeling objectives enables the ICL capabilities with vector context, and finetuning them on task datasets further improves ICL performance. The pretraining process is depicted in Fig. 2(a). For each text snippet, we cut it into two pieces with the cutting point randomly sampled from the end of sentences. The first half is encoded and projected while the second is kept intact. The rest is the same with any pretraining process, the language model generated the next token distribution at each input position, except for the ones preceding the projected embeddings, and cross-entropy loss is imposed on top of this. With the encoder and LLM frozen, the gradient backpropagates to the projector, updating its parameters. For non-text data modalities, pretraining can be more flexible. We define this pretraining as involving general, non-task-specific objectives, such as reconstructing number from its embeddings (e.g., is 32768), performing basic algebra (e.g., + = 16384), or predicting the next token from brain fMRI embeddings. The finetuning process is shown in Fig. 2(b). It utilizes additional structured prompts and trains with task-specific datasets. Similarly, the input is first mapped into the embedding space and projected into tokens. They are then inserted into structured prompts, while the LLM is trained with conditional generation loss given those prompts."
        },
        {
            "title": "4 EXPERIMENTAL SETUP",
            "content": "Table 1 gives an overview of our experimental setup, including specifics for the task, datasets, encoders, LLMs, and task-specific prompts we use. Across different tasks, we project to four openweights LLMs. We now provide details for individual tasks. Text Pretraining To pretrain our text projectors, we leverage the WikiText-103 [33] dataset, consisting of over 100 million tokens from verified high-quality Wikipedia articles. This smaller language modeling corpus is chosen for its suitability to the lightweight nature of our projectors. The pretraining process is illustrated in Fig. 2(a), where text snippets are divided at random sentence-end points. The first half is embedded and projected, while next-token generation loss is applied to the second half. Text Reconstruction We investigate LLMs ability to decode original text from projected embeddings using two datasets: Parallel Sentence Talks [49] and Quora [48]. These datasets consist of concise text pieces that convey clear meaning. Projectors are trained on the training sets of both datasets, and performance is evaluated using the BLEU score [43, 44]. Arithmetic and Function Regression For the arithmetic tasks, we generated synthetic datasets containing 10-digit numbers, which are particularly challenging for LLMs as they require splitting the numbers into multiple text tokens. These numbers are represented using concatenated one-hot encoding per digit. For instance, 10-digit number is represented as 10 10 matrix, flattened into 100-dimensional vector. The pretraining phase includes two key tasks: number reconstruction, where the model is tasked with recovering the original number from its embedding, and basic arithmetic, where the model performs algebraic addition operations on the projected embeddings. To evaluate the models arithmetic reasoning abilities, we employ non-linear function regression y. The model is provided with inputs and y, task, where the function is defined as (x, y) = and it must predict the integer part of the function output. Performance is measured using the mean relative error, calculated as the ℓ1 difference between the predicted and true values, normalized by the ground truth. This task allows us to assess the models ability to perform more complex numerical reasoning beyond simple arithmetic operations. 5 Preprint. Table 1: Overview of the tasks, datasets, encoders, large language models, and prompts used in the experiments. Each task utilizes encoders and LLMs to perform functions across multiple modalities. The table highlights the diversity of models and configurations applied to each task. Task Dataset Encoder LLM Prompt Text Reconstruction Quora [48] PST [49] , , , Translate the text in brackets: () Translation: [Text] Function Regression 10 Digits Regression Digit Embedding Text Classification IMDB [29] Rotten Tomatoes [42] SST2 [46] Emotion [45] Financial Phrasebank [31] Text Summarization XSum [38] XLSum [17] , , , , , , , , , , = x, = y, function(x, y) equals to (digits): [Solution] ()s sentiment is: [Label] ()s summarization is: [Summary] Molecule Captioning Language + Molecule-24 [14] ()s molecule caption is: [Caption] Brain fMRI [23], [47] PCA of fMRI [Question] Input: Response: [Answer] Time-series Classification FordA, FordB [12] Chronos-base [4] ()s class (positive, negative) is: [Label] Graph Classification ogbg-molhiv [18] Graphormer [59] , ()s class (positive, negative) is: [Label] Encoders: NV-Embed ([24]; nvidia/NV-Embed-v1), dunzhang/stella en 1.5B v5), GTR-t5 ([40]; sentence-transformers/gtr-t5-base) LLMs: mistralai/Mistral-7B-Instruct-v0.3), Qwen2-7B ([58]; Qwen/Qwen2-7B-Instruct), Yi-1.5-9B ([60]; 01-ai/Yi-1.5-9B-Chat) Llama-3.1-8B ([13]; meta-llama/Llama-3.1-8B-Instruct), Mistral-7B ([20]; SFR ([32]; Salesforce/SFR-Embedding-2 R), Stella ([61]; Text Classification We assess whether Vector-ICL can be applied effectively to text classification. Both binary and multi-class classification datasets are used, and the results are compared across few-shot ICL and soft prompt tuning. The classification performance is measured by accuracy. Text Summarization Following the classification tasks, we explore Vector-ICLs capability in summarizing text based on the projected embeddings. The datasets, encoders, LLMs, and prompt templates can be found in Table 1. Performance is evaluated using RougeL [28]. Molecule Captioning We also extend our approach to the unconventional task of molecule captioning, using molecule sequence-caption pairs from the Language + Molecules-24 (LPM24) [14] dataset. sample molecule-caption pair looks like the following: Molecule: Cc1c(Cl)cccc1-n1ccn2c(SCC(=O)c3ccccc3C(F)(F)F)nnc2c1=O Caption: The molecule is pain treatment that impacts inflammatory disease treatment. This task explores whether LLMs can extract useful information from projected embeddings of outof-distribution chemical sequences, with performance evaluated via BLEU score. Brain fMRI Decoding and Classification We analyze data from LeBel et al. 2022 and Tang et al. 2023, which consists of fMRI responses for 3 human subjects as they listen to 20+ hours of narrative stories from podcasts. We preprocessed the data following Benara et al. 2024, by converting the fMRI responses into 200-dimensional output using principal components analysis and assigning classification labels to 10-grams of the story text at 2-second intervals using an LLM. We use the same pretraining methodology as text to pretrain on the brain fMRI data (projecting on 20% of time points and imposing next-token generation loss on the remaining 80%) . We evaluate the LLMs capability to decode projected brain fMRI by giving them randomly sampled context from the train set, that could come from different human subjects or from different story, and ask them to decode segments from the test set. 6 Preprint. Table 2: We compare Vector-ICL with few-shot ICL and soft prompt tuning on the five sentiment analysis datasets. Details can be found in Appendix A.2. Method Rotten Tomatoes SST2 IMDB Emotion Financial Phrasebank V-ICL (pretrained) Few-shot ICL Soft prompt [26] V-ICL (finetuned) 80.60 87.31 93.24 88.80 78.90 91.74 96.21 98.16 95.04 93.50 95.26 97.28 41.20 55.20 74.15 85.20 60.72 71.78 78.22 81. Table 3: We compare the performance of Vector-ICL, few-shot ICL, and soft prompt tuning across two text summarization datasets. Details can be found in Appendix A.2. Method XSum XLSum V-ICL (pretrained) Few-shot ICL Soft prompt V-ICL (finetuned) 15.25 19.53 12.84 20.08 15.89 19.41 12.70 20. In addition to text reconstruction, we decode the binary labels from the fMRI responses, corresponding to questions about the underlying text, e.g. Does the sentence contain proper noun? The decoding random baseline is constructed by giving the LLM the randomly sampled, shuffled text from the training set, and generating text according to it. We measure the performance using the RougeL score between the generated text and the ground truth text. The classification random baseline is 50% accuracy, as we have balanced the dataset. Time-series We take the output of the last time step from Chronos-base [4] as the time-series representation. We use the base encoder with trained classification head as the baseline and we measure the prediction performance with accuracy. Graphs We use Graphormer [59] as the encoder model, specifically the one that was pretrained on quantum chemistry graph datasets [19]. Since the down-stream, ogbg-molhiv [18], is molecule property prediction dataset, and with strong class imbalance (3-4% positive classes), we finetune the encoder on the training set to provide meaningful baselines and embeddings. We take the output prior to the classification layer of the Graphormer as the graph embedding. Weighted sampling is adopted in the finetuning of both the baseline Graphormer and the embedding projector to yield meaningful predictions. We use the finetuned Graphormer as our baseline and use the F1 score as the performance metric. Projector Configurations Both linear and non-linear projectors are utilized, as shown in Fig. 3, with input and output dimensions matching the encoder-decoder pairs. Early stopping with patience of 500 steps is used during finetuning, as projectors converge quickly due to their small parameter sizes. Details of the hyperparameters used in training are provided in Table 4."
        },
        {
            "title": "5 RESULTS: UNLOCKING VERSATILE APPLICATIONS ACROSS MODALITIES",
            "content": "Fig. 3 presents our main results, where each subplot corresponds to one of the nine tasks. We begin by exploring text reconstruction, to see whether LLMs can comprehend the information encoded within the box tokens. Next, we investigate the tasks of function regression to evaluate whether LLMs can leverage the box tokens during reasoning processes and whether this approach outperforms reasoning with plain text. Finally, we proceed to range of downstream tasks, including text classification, text summarization, time-series classification, graph classification, and brain fMRI decoding. This comprehensive evaluation allows us to assess the versatility and effectiveness of Vector-ICL across different domains and task types. Text Reconstruction: LLM Understanding of Projected Embeddings We first verify LLMs ability to understand projected embeddings. Our results demonstrate that Vector-ICL successfully 7 Preprint. reconstructs original text from projected embeddings, with performance improving as the number of examples (shots) increases, mirroring standard in-context learning (ICL) behavior. This suggests that LLMs can effectively decode the information compressed into the box tokens, with more context leading to better reconstruction. The similarity to ICL behavior indicates that Vector-ICL leverages similar learning mechanisms, but with the flexibility of working with continuous representations. Function Regression: Enhanced Reasoning with Continuous Context We then study would it sometimes be better for LLMs to receive continuous context instead of discrete tokens. After pretraining projectors on 10-digit number reconstruction and addition between two numbers, we task the LLM with learning an unknown function in-context. Results show Vector-ICL consistently outperforms few-shot ICL with raw number inputs, that have to span multiple tokens. This suggests that the continuous representations capture numerical relationships more effectively than discrete tokens, enabling LLMs to better infer and apply mathematical patterns. The improvement is particularly noteworthy given that LLMs are typically challenged by precise numerical computations. Text Classification In this classical NLP task, we aggregate mean accuracy across five datasets, four encoders, and three LLMs. Results indicate that pretrained projectors provide meaningful continuous context for ICL, outperforming the random baseline. LLMs achieve optimal performance with continuous context from finetuned projectors, surpassing both regular few-shot ICL and soft prompt tuning, with details shown in Table 2. This demonstrates the versatility of Vector-ICL across different text classification scenarios and its ability to outperform established prompt tuning methods. The success across multiple datasets and LLMs suggests that the benefits of continuous context are robust and generalizable. Text Summarization This task demands longer text generation and deeper information comprehension. We aggregate mean RougeL scores across two datasets, and three encoders. Findings show that pretrained projectors enable LLMs to extract and condense information from single box token, while finetuned projectors provide more effective continuous context than original textual input and soft prompts, with details shown in Table 3. The ability to compress and later expand information from single token is particularly impressive, suggesting that Vector-ICL captures high-level semantic content effectively. The superior performance of finetuned projectors highlights the benefits of task-specific optimization in continuous space. Molecule Captioning: An Unconventional NLP Task We explore LLMs ability to comprehend continuous vector context for out-of-distribution inputs like molecule sequences. Evaluating captioning performance with BLEU scores, we find that both pretrained and finetuned projectors provide better context than original molecule sequence text, despite encoders likely never trained on such sequences. This result is particularly intriguing as it demonstrates Vector-ICLs ability to bridge the gap between specialized domains and general language understanding. It suggests that continuous representations can capture and translate domain-specific information in way thats more accessible to LLMs than raw specialized notation. Time-series Classification We finetune non-linear projectors on the training sets of two datasets and evaluate LLM performance with the resulting continuous context. Aggregating average accuracy, we find LLMs outperform baseline domain-specific models with finetuned classification heads. The success here suggests that continuous context can effectively capture temporal dependencies and patterns, translating time-series data into form that LLMs can process effectively. Graph Classification Since the task dataset is heavily imbalanced and out of the pretraining distribution of the graph encoder. We first finetune the base encoder on the target dataset to establish baseline, then train non-linear projectors on the graph classification dataset. Averaging F1 scores across two LLMs, results indicate that Vector-ICL enables LLMs to outperform the finetuned baseline model. This is noteworthy achievement, as graph data is structurally very different from the text data that LLMs are trained on. The success here suggests that Vector-ICL can effectively translate graph structures into continuous representations that preserve relational information in way thats interpretable to LLMs. Preprint. Brain fMRI Decoding and Classification We pretrain projectors on the training set using nexttoken generation loss, then apply them to recover original text from brain fMRI signals in the test set. Results show LLMs can surpass random baselines by leveraging projected 200-dimensional fMRI PCA factors, with performance improving as context increases. This application to neuroscience data is particularly exciting, demonstrating Vector-ICLs potential in bridging neural activity and language understanding."
        },
        {
            "title": "6.1 THE IMPACT OF ENCODER QUALITY ON VECTOR-ICL PERFORMANCE",
            "content": "We investigate the relationship between the intrinsic capabilities of encoders and their effectiveness when used with Vector-ICL for downstream tasks. To quantify the encoders intrinsic abilities, we evaluate their performance on text reconstruction task, which serves as proxy for the amount of information preserved in the embeddings. Our analysis focuses on text classification as the downstream task. We examine the correlation between encoder rankings on the reconstruction task and their corresponding rankings on the classification task. This correlation test is performed across 5 datasets and 3 LLMs, resulting in 15 configurations. The results of this analysis, presented in Fig. 4a, demonstrate consistent positive correlation between an encoders text reconstruction performance and its effectiveness in downstream classification tasks when used with Vector-ICL. Notably, in 4 of the 15 configurations, we observe particularly strong correlation, with values approaching 1. Our findings suggest that an encoders performance on the text reconstruction task can serve as reliable predictor of its potential effectiveness in downstream tasks when integrated with Vector-ICL. This insight could prove valuable for practitioners in selecting encoders for Vector-ICL. 6.2 CASE STUDY: WHAT HAS BEEN LEARNED IN THE PROJECTIONS? Fig. 4b provides visualization of the normalized Euclidean distances between projected embeddings, where the and axes correspond to 1024 different numbers uniformly sampled from 0 to 1e10 and the color intensity reflects the distance between each pair. Several key patterns emerge from this visualization that offer insights into what the projector has learned. Analysis of the numerical embedding distance matrix reveals key properties of our projection method. Embeddings for similar numbers cluster along the diagonal, indicated by lighter colors, demonstrating the preservation of local structure. Conversely, increasing distances from the diagonal, shown by darker colors, indicate effective separation of numerically distant values in the embedding space. Another notable feature of the distance matrix is the block structure that emerges. This pattern comes from the way we construct the numerical embeddings and it is likely beneficial for the LLM to understand numerical inputs. 6.3 CASE STUDY: WHAT INFORMATION WAS PERCEIVED FROM PROJECTED BRAIN FMRI? Fig. 4c illustrates the mean accuracy achieved in decoding different categories of brain activity based on fMRI data. The categories, ranging from Physical Actions and Movements to Conflict, Urgency, and Change, represent diverse cognitive and perceptual domains. The grouping and corresponding questions are listed in Table 5. The input data is noisy, and the projector is only trained with pretraining objectives, i.e., to predict the next piece of text given the current fMRI signal. We are surprised that with this pure unsupervised training, the LLM can still pick up meaningful signals from the projected embeddings. Notably, decoding tasks associated with Physical Actions and Movements and Cognitive and Reflective Aspects demonstrate higher mean accuracy, suggesting that these categories are more distinguishable based on the fMRI embeddings. Preprint. (a) (b) (c) Figure 4: Key Insights from Encoders and Projections. (a) The impact of encoders informationpreserving capabilities on downstream task performance with Vector-ICL. We observe that there is positive correlation, suggesting that an encoders ability to retain information may be strong predictor of its effectiveness in Vector-ICL. (b) The Euclidean distance matrix between uniformly sampled numbers after projection, using the projector trained with Qwen2-7B as an example. The projected number embeddings exhibit block-diagonal patterns, indicating structures and sensible distance measures. (c) Analyzing LLMs understanding of projected brain fMRI embeddings after only unsupervised pretraining. We categorize the underlying questions related to the text and measure the mean accuracy for each category, highlighting the LLMs ability to interpret the embeddings, with only the next token prediction pretraining."
        },
        {
            "title": "7 DISCUSSION",
            "content": "Limitations and Future Directions In this study, we explored variety of settings: utilizing different encoders, LLM architectures, modalities, and datasets. Our results demonstrate that LLMs are capable of performing Vector-ICL on both language and non-language inputs. However, our experiments did not cover all possible combinations of these variables. There are still many unexplored areas, such as additional modalities, tasks, and encoder-decoder configurations, that could further benefit from Vector-ICL. Also, we only experimented with single-token encoders, while there exist encoders that produce variable-sized embeddings, that can potentially be more powerful and flexible. We leave this extensive exploration for future research to fully understand the broader applicability and limitations of our approach across diverse domains. Conclusion In this work, we explore whether large language models trained only on text can perform in-context learning on continuous vectors from different domains. Our findings suggest that LLMs can indeed understand and process continuous context via embedding projection. Simple linear projections are often sufficient, though for cross-modal taskssuch as those involving non-textual data like time-series or graphsnon-linear transformations may be required. In our experiments across various tasks and modalities, including text reconstruction, numerical function regression, text classification, summarization, molecule captioning, time-series classification, graph classification, and fMRI decoding, Vector-ICL often surpasses both few-shot ICL and task-specific model or tuning. We further conduct analyses and case studies, indicating the potential of LLMs to process vector representations beyond traditional token-based paradigms."
        },
        {
            "title": "REFERENCES",
            "content": "[1] Rishabh Agarwal, Avi Singh, Lei Zhang, Bernd Bohnet, Stephanie Chan, Ankesh Anand, Zaheer Abbas, Azade Nova, John Co-Reyes, Eric Chu, et al. Many-shot in-context learning. arXiv preprint arXiv:2404.11018, 2024. [2] Kwangjun Ahn, Xiang Cheng, Hadi Daneshmand, and Suvrit Sra. Transformers learn to implement preconditioned gradient descent for in-context learning. Advances in Neural Information Processing Systems, 36, 2024. 10 Preprint. [3] Ekin Akyurek, Dale Schuurmans, Jacob Andreas, Tengyu Ma, and Denny Zhou. What learning algorithm is in-context learning? investigations with linear models. arXiv preprint arXiv:2211.15661, 2022. [4] Abdul Fatir Ansari, Lorenzo Stella, Caner Turkmen, Xiyuan Zhang, Pedro Mercado, Huibin Shen, Oleksandr Shchur, Syama Sundar Rangapuram, Sebastian Pineda Arango, ShubarXiv preprint ham Kapoor, et al. Chronos: Learning the language of time series. arXiv:2403.07815, 2024. [5] Yu Bai, Fan Chen, Huan Wang, Caiming Xiong, and Song Mei. Transformers as statisticians: Provable in-context learning with in-context algorithm selection. Advances in neural information processing systems, 36, 2024. [6] Vinamra Benara, Chandan Singh, John Morris, Richard Antonello, Ion Stoica, Alexander Huth, and Jianfeng Gao. Crafting interpretable embeddings by asking llms questions. arXiv preprint arXiv:2405.16714, 2024. [7] Amanda Bertsch, Maor Ivgi, Uri Alon, Jonathan Berant, Matthew Gormley, and Graham Neubig. In-context learning with long-context models: An in-depth exploration. arXiv preprint arXiv:2405.00200, 2024. [8] Satwik Bhattamishra, Arkil Patel, Phil Blunsom, and Varun Kanade. Understanding in-context learning in transformers and llms by learning to learn discrete functions. arXiv preprint arXiv:2310.03016, 2023. [9] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems, 33:18771901, 2020. [10] Yanda Chen, Chen Zhao, Zhou Yu, Kathleen McKeown, and He He. Parallel structures in pre-training data yield in-context learning. arXiv preprint arXiv:2402.12530, 2024. [11] Damai Dai, Yutao Sun, Li Dong, Yaru Hao, Shuming Ma, Zhifang Sui, and Furu Wei. Why can gpt learn in-context? language models secretly perform gradient descent as meta-optimizers. In Findings of the Association for Computational Linguistics: ACL 2023, pp. 40054019, 2023. [12] Hoang Anh Dau, Anthony Bagnall, Kaveh Kamgar, Chin-Chia Michael Yeh, Yan Zhu, Shaghayegh Gharghabi, Chotirat Ann Ratanamahatana, and Eamonn Keogh. The ucr time series archive. IEEE/CAA Journal of Automatica Sinica, 6(6):12931305, 2019. [13] Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. [14] Carl Edwards, Qingyun Wang, Lawrence Zhou, and Heng Ji. L+m-24: Building dataset for language+molecules @ acl 2024. arXiv preprint arXiv:2403.00791, 2024. [15] Deqing Fu, Tian-Qi Chen, Robin Jia, and Vatsal Sharan. Transformers learn higher-order optimization methods for in-context learning: study with linear models. arXiv preprint arXiv:2310.17086, 2023. [16] Angeliki Giannou, Liu Yang, Tianhao Wang, Dimitris Papailiopoulos, and Jason Lee. How well can transformers emulate in-context newtons method? arXiv preprint arXiv:2403.03183, 2024. [17] Tahmid Hasan, Abhik Bhattacharjee, Md. Saiful Islam, Kazi Mubasshir, Yuan-Fang Li, YongBin Kang, M. Sohel Rahman, and Rifat Shahriyar. XL-sum: Large-scale multilingual abstractive summarization for 44 languages. In Findings of the Association for Computational Linguistics: ACL-IJCNLP 2021, pp. 46934703, Online, August 2021. Association for Computational Linguistics. URL https://aclanthology.org/2021.findings-acl.413. 11 Preprint. [18] Weihua Hu, Matthias Fey, Marinka Zitnik, Yuxiao Dong, Hongyu Ren, Bowen Liu, Michele Catasta, and Jure Leskovec. Open graph benchmark: Datasets for machine learning on graphs. In Hugo Larochelle, Marc Aurelio Ranzato, Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin (eds.), Advances in Neural Information Processing Systems 33: Annual Conference on Neural Information Processing Systems 2020, NeurIPS 2020, December 612, 2020, virtual, 2020. URL https://proceedings.neurips.cc/paper/2020/ hash/fb60d411a5c5b72b2e7d3527cfc84fd0-Abstract.html. [19] Weihua Hu, Matthias Fey, Hongyu Ren, Maho Nakata, Yuxiao Dong, and Jure Leskovec. Ogblsc: large-scale challenge for machine learning on graphs. arXiv preprint arXiv:2103.09430, 2021. [20] Albert Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel, Guillaume Lample, Lucile Saulnier, et al. Mistral 7b. arXiv preprint arXiv:2310.06825, 2023. [21] Yixing Jiang, Jeremy Irvin, Ji Hun Wang, Muhammad Ahmed Chaudhry, Jonathan Chen, and Andrew Ng. Many-shot in-context learning in multimodal foundation models. arXiv preprint arXiv:2405.09798, 2024. [22] Andrew Lampinen, Ishita Dasgupta, Stephanie CY Chan, Kory Matthewson, Michael Henry Tessler, Antonia Creswell, James McClelland, Jane Wang, and Felix Hill. Can language models learn from explanations in context? arXiv preprint arXiv:2204.02329, 2022. [23] Amanda LeBel, Lauren Wagner, Shailee Jain, Aneesh Adhikari-Desai, Bhavin Gupta, Allyson Morgenthal, Jerry Tang, Lixiang Xu, and Alexander Huth. natural language fmri dataset for voxelwise encoding models. bioRxiv, pp. 202209, 2022. [24] Chankyu Lee, Rajarshi Roy, Mengyao Xu, Jonathan Raiman, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. Nv-embed: Improved techniques for training llms as generalist embedding models, 2024. [25] Mukai Li, Shansan Gong, Jiangtao Feng, Yiheng Xu, Jun Zhang, Zhiyong Wu, and LingarXiv preprint learning with many demonstration examples. peng Kong. In-context arXiv:2302.04931, 2023. [26] Xiang Lisa Li and Percy Liang. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190, 2021. [27] Yingcong Li, Muhammed Emrullah Ildiz, Dimitris Papailiopoulos, and Samet Oymak. TransIn International formers as algorithms: Generalization and stability in in-context learning. Conference on Machine Learning, pp. 1956519594. PMLR, 2023. [28] Chin-Yew Lin. ROUGE: package for automatic evaluation of summaries. In Text Summarization Branches Out, pp. 7481, Barcelona, Spain, July 2004. Association for Computational Linguistics. URL https://aclanthology.org/W04-1013. [29] Andrew L. Maas, Raymond E. Daly, Peter T. Pham, Dan Huang, Andrew Y. Ng, and Christopher Potts. Learning word vectors for sentiment analysis. In Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics: Human Language Technologies, pp. 142150, Portland, Oregon, USA, June 2011. Association for Computational Linguistics. URL http://www.aclweb.org/anthology/P11-1015. [30] Arvind Mahankali, Tatsunori Hashimoto, and Tengyu Ma. One step of gradient descent is provably the optimal in-context learner with one layer of linear self-attention. arXiv preprint arXiv:2307.03576, 2023. [31] P. Malo, A. Sinha, P. Korhonen, J. Wallenius, and P. Takala. Good debt or bad debt: Detecting semantic orientations in economic texts. Journal of the Association for Information Science and Technology, 65, 2014. [32] Rui Meng, Ye Liu, Shafiq Rayhan Joty, Caiming Xiong, Yingbo Zhou, and Semih Yavuz. Sfrembedding-mistral: enhance text retrieval with transfer learning. Salesforce AI Research Blog, 3, 2024. 12 Preprint. [33] Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models, 2016. [34] Sewon Min, Mike Lewis, Luke Zettlemoyer, and Hannaneh Hajishirzi. Metaicl: Learning to learn in context. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 27912809, 2022. [35] Sewon Min, Xinxi Lyu, Ari Holtzman, Mikel Artetxe, Mike Lewis, Hannaneh Hajishirzi, and Luke Zettlemoyer. Rethinking the role of demonstrations: What makes in-context learning work? In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pp. 1104811064, 2022. [36] John Morris, Chandan Singh, Alexander Rush, Jianfeng Gao, and Yuntian Deng. Tree prompting: efficient task adaptation without fine-tuning. arXiv preprint arXiv:2310.14034, 2023. [37] Samuel Muller, Noah Hollmann, Sebastian Pineda Arango, Josif Grabocka, and Frank Hutter. Transformers can do bayesian inference. In International Conference on Learning Representations, 2021. [38] Shashi Narayan, Shay B. Cohen, and Mirella Lapata. Dont give me the details, just the topic-aware convolutional neural networks for extreme summarization. ArXiv, summary! abs/1808.08745, 2018. [39] Tung Nguyen and Aditya Grover. Transformer neural processes: Uncertainty-aware meta In International Conference on Machine Learning, pp. learning via sequence modeling. 1656916594. PMLR, 2022. [40] Jianmo Ni, Chen Qu, Jing Lu, Zhuyun Dai, Gustavo Hernandez Abrego, Ji Ma, Vincent Zhao, Yi Luan, Keith Hall, Ming-Wei Chang, et al. Large dual encoders are generalizable retrievers. arXiv preprint arXiv:2112.07899, 2021. [41] OpenAI. GPT-4 technical report, 2023. [42] Bo Pang and Lillian Lee. Seeing stars: Exploiting class relationships for sentiment categorization with respect to rating scales. In Proceedings of the ACL, 2005. [43] Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. BLEU: method for autoIn Proceedings of the 40th Annual Meeting of the matic evaluation of machine translation. Association for Computational Linguistics, pp. 311318, Philadelphia, Pennsylvania, USA, July 2002. Association for Computational Linguistics. [44] Matt Post. call for clarity in reporting BLEU scores. In Proceedings of the Third Conference on Machine Translation: Research Papers, pp. 186191, Belgium, Brussels, October 2018. Association for Computational Linguistics. URL https://www.aclweb.org/ anthology/W18-6319. [45] Elvis Saravia, Hsien-Chi Toby Liu, Yen-Hao Huang, Junlin Wu, and Yi-Shin Chen. CARER: In Proceedings of the 2018 Contextualized affect representations for emotion recognition. Conference on Empirical Methods in Natural Language Processing, pp. 36873697, Brussels, Belgium, October-November 2018. Association for Computational Linguistics. doi: 10.18653/ v1/D18-1404. URL https://www.aclweb.org/anthology/D18-1404. [46] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts. Recursive deep models for semantic compositionality over senIn Proceedings of the 2013 Conference on Empirical Methods in Natural timent treebank. Language Processing, pp. 16311642, Seattle, Washington, USA, October 2013. Association for Computational Linguistics. URL https://www.aclweb.org/anthology/ D13-1170. [47] Jerry Tang, Amanda LeBel, Shailee Jain, and Alexander Huth. Semantic reconstruction of continuous language from non-invasive brain recordings. Nature Neuroscience, 26(5):858 866, 2023. 13 Preprint. [48] Nandan Thakur, Nils Reimers, Andreas Ruckle, Abhishek Srivastava, and Iryna Gurevych. BEIR: heterogeneous benchmark for zero-shot evaluation of information retrieval models. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=wCu6T5xFjeJ. [49] Jorg Tiedemann. Parallel data, tools and interfaces in opus. In Lrec, volume 2012, pp. 2214 2218. Citeseer, 2012. [50] Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. [51] Trieu Trinh, Yuhuai Wu, Quoc Le, He He, and Thang Luong. Solving olympiad geometry without human demonstrations. Nature, 625(7995):476482, 2024. [52] Johannes Von Oswald, Eyvind Niklasson, Ettore Randazzo, Joao Sacramento, Alexander Mordvintsev, Andrey Zhmoginov, and Max Vladymyrov. Transformers learn in-context by gradient descent. In International Conference on Machine Learning, pp. 3515135174. PMLR, 2023. [53] Albert Webson and Ellie Pavlick. Do prompt-based models really understand the meaning of their prompts? In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies, pp. 23002344, 2022. [54] Jerry Wei, Jason Wei, Yi Tay, Dustin Tran, Albert Webson, Yifeng Lu, Xinyun Chen, Hanxiao Liu, Da Huang, Denny Zhou, et al. Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846, 2023. [55] Noam Wies, Yoav Levine, and Amnon Shashua. The learnability of in-context learning. Advances in Neural Information Processing Systems, 36, 2024. [56] Jingfeng Wu, Difan Zou, Zixiang Chen, Vladimir Braverman, Quanquan Gu, and Peter Bartlett. How many pretraining tasks are needed for in-context learning of linear regression? In The Twelfth International Conference on Learning Representations, 2023. [57] Junda Wu, Zhehao Zhang, Yu Xia, Xintong Li, Zhaoyang Xia, Aaron Chang, Tong Yu, Sungchul Kim, Ryan Rossi, Ruiyi Zhang, et al. Visual prompting in multimodal large language models: survey. arXiv preprint arXiv:2409.15310, 2024. [58] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, et al. Qwen2 technical report. arXiv preprint arXiv:2407.10671, 2024. [59] Chengxuan Ying, Tianle Cai, Shengjie Luo, Shuxin Zheng, Guolin Ke, Di He, Yanming Shen, and Tie-Yan Liu. Do transformers really perform badly for graph representation? Advances in neural information processing systems, 34:2887728888, 2021. [60] Alex Young, Bei Chen, Chao Li, Chengen Huang, Ge Zhang, Guanwei Zhang, Heng Li, Jiangcheng Zhu, Jianqun Chen, Jing Chang, et al. Yi: Open foundation models by 01. ai. arXiv preprint arXiv:2403.04652, 2024. [61] Dun Zhang. Stella, 2024. URL https://huggingface.co/dunzhang/stella_en_ 1.5B_v5. [62] Ruiqi Zhang, Spencer Frei, and Peter Bartlett. Trained transformers learn linear models in-context. arXiv preprint arXiv:2306.09927, 2023. [63] Siyan Zhao, John Dang, and Aditya Grover. Group preference optimization: Few-shot alignment of large language models. arXiv preprint arXiv:2310.11523, 2023. [64] Siyan Zhao, Tung Nguyen, and Aditya Grover. Probing the decision boundaries of in-context learning in large language models. arXiv preprint arXiv:2406.11233, 2024. 14 Preprint."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 DETAILED EXPERIMENT SETUP Text Reconstruction We use two datasets for the text reconstruction task, Parallel Sentence Talkss English subset [49] and Quora [48]. Parallel Sentence Talks consist of sentences used in movie conversations, and Quora is built on wide range of online questions. They represent short pieces of text that convey clear information. We aim to explore whether LLMs can decode the original message from the projected text embeddings. We use the following prompt template: Translate the text in brackets: (), translation: [Original Text] We train the projectors on the training set of these two datasets and evaluate their performance on the test tests. We measure the reconstruction performance with the BLEU score [43, 44]. Arithmetic and Function Regression We created synthetic datasets of numerical data to pretrain our linear number projectors, experimenting with two configurations: one using 3-digit numbers and the other using 10-digit numbers. In Llama3.1-8Bs tokenizer, 3-digit numbers are represented as single tokens, while in Mistral-7B, Qwen2-7B, and Yi-1.5-9B, numbers larger than 10 are split into multiple tokens. Consequently, 10-digit numbers consistently span multiple tokens across all models, which increases the complexity of performing arithmetic operations. To represent the numbers, we use concatenated-and-flattened one-hot vector encoding for each digit. For instance, 3-digit number is represented as 3 10 matrix (one hot per digit place), which is then flattened into 30-dimensional vector. Similarly, 10-digit number is represented as 10 10 matrix, flattened into 100-dimensional vector. The pretraining involves two tasks. The first task is number reconstruction, we use the following prompt template, given the number is 128: = x, equals to (digits): 128 The second task is basic addition, we use the following prompt template, given the numbers are = 128 = 256, = 1, = 1: = x, = y, + equals to (digits): 384 Here, and are randomly sampled from [0, 1] with up to two decimal places, and the solution is the integer part of the sum. For evaluation, we use function regression task with non-linear function: (x, y) = y. The LLM is given inputs and y, along with the integer part of the output (x, y). The model is then tasked with predicting the output for new pairs of and y. The prompt for in-context learning is structured as follows: = x, = y, function(x, y) equals to (digits): [Solution] For few-shot ICL, the box tokens will be replaced with the actual numbers. We measure the function regression performance with the mean relative error, where the relative error is computed as the ℓ1 difference divided by the ground truth value. Text Classification We use five datasets for the text classification task. For binary classification, we include IMDB [29], Rotten Tomatoes [42], and the Stanford Sentiment Treebank (SST2) [46]. For multi-class classification, we use the Emotion dataset [45] and the Financial Phrasebank [31]. The binary classification datasets (IMDB, Rotten Tomatoes, and SST2) involve classifying movie reviews as positive or negative. The Emotion dataset classifies Twitter tweets into six categories: anger, fear, joy, love, sadness, and surprise. The Financial Phrasebank categorizes financial news into positive, negative, or neutral sentiments. We use the following prompt in ICL: ()s sentiment is [Input Class] 15 Preprint. For few-shot ICL, the box tokens will be replaced with the actual text. For soft prompt tuning, we use 20 virtual tokens and train for one epoch over the entire training set. We measure the classification performance with accuracy on the test set. Text Summarization We use two datasets for the summarization task, XSum [38] and the English subset of XLSum [17]. They contain newspaper articles and their summaries. We use the following prompt in ICL: ()s summarization is: [Summary of the Input] For few-shot ICL, the box token will be replaced by the article. We measure the performance using the RougeL score with the ground truth summary on the test sets. Molecule Captioning We use the Language + Molecules-24 (LPM24) dataset for the molecule captioning task, it was created for the task of molecule-language translation, and contains 161K pairs of molecule strings and their captions in the combined training and test set. sample molecule-caption pair looks like the following: Molecule: Cc1c(Cl)cccc1-n1ccn2c(SCC(=O)c3ccccc3C(F)(F)F)nnc2c1=O Caption: The molecule is pain treatment that impacts inflammatory disease treatment. And we use the following prompt for ICL: ()s molecule caption is: [Caption of the Input Molecule] For few-shot ICL, we replace the box token with the actual molecule string. We measure the performance using the BLEU score between the generated caption and the ground truth caption. Brain fMRI Decoding and Classification We analyze data from LeBel et al. 2022 and Tang et al. 2023, which consists of fMRI responses for 3 human subjects as they listen to 20+ hours of narrative stories from podcasts. We preprocessed the data following Benara et al. 2024, by converting the fMRI responses into 200-dimensional output using principal components analysis and labeling 10-grams of the story text at 2-second intervals using an ensemble of LLMs. The data was separated into train set and test set by holding out the same three podcast stories from the three human subjects. We use the same pretraining methodology as text to pretrain on the brain fMRI data. As the data comes in as segments of text and the recorded fMRI, we randomly sample 20% of the segments to be in fMRI form and projected into box tokens, and we impose next token generation loss on the rest 80%. We evaluate the projectors by giving them randomly sampled context from the train set, that could come from different human subjects or from different story, and ask them to decode segments from the test set. We use the following prompt in ICL in our decoding experiments: What is the English translation of the input? Input: , Response: the input in English is [Text Corresponding to fMRI] The random baseline is constructed by giving LLM the randomly sampled, shuffled text from the training set, and generating text according to it. We measure the performance using the RougeL score between the generated text and the ground truth. We construct the classification questions around the properties of the underlying text, for example, Does the sentence contain proper noun?, Does the input mention anything related to arguing?. The ground truth is obtained via GPT4o [41] as binary labels. We use the following prompt in ICL in our classification experiments, using one of the example questions: Does the input mention anything related to arguing? Input: , Response (Yes or No): according to the English text of the input, the answer is [Yes/No] The random baseline is 50%, as we have downsampled and balanced the data. And we use accuracy as the performance metric. 16 Preprint. Time-series We use the Chronos [4] time-series Transformers as the encoder. Chronos was pretrained on large scale time-series and is designed to generate the next segments of the time-series. It has proven effective on wide range of time-series forecasting benchmarks. We take the output of the last time step from Chronos-base as the time-series representation. We use two datasets for time-series classification, FordA, and FordB, they are both part of the UCR Time Series Classification Archive [12] ranging from 4000 to 5000 time-series for each dataset. We use the following prompt in ICL: ()s class (positive, negative) is: [Input Class] We use the base encoder with trained classification head as the baseline and we measure the prediction performance with accuracy. Graphs We use Graphormer [59] as the encoder model, specifically the one that was pretrained on large scale quantum chemistry graph datasets [19]. Since the down-stream task (ogbg-molhiv [18]) is molecule property prediction dataset, and with strong class imbalance (3% positive classes), we finetune the encoder on the training set to provide meaningful baselines and embeddings. We take the output prior to the classification layer of the Graphormer as the graph embedding. We use the ogbg-molhiv [18] dataset for the graph classification task. ogbg-molhiv is molecule property prediction dataset consisting of total 41.12K graphs with node features and edge attributes. It has strong class imbalance of having around 3% positive class and 97% negative class. Hence weighted sampling is adopted in the finetuning of both the baseline Graphormer and the embedding projector to yield meaningful predictions. We use the following prompt in ICL: ()s class (positive, negative) is: [Input Class] We use the finetuned Graphormer as our baseline and use the F1 score as the performance metric due to the significant class imbalance. A.2 TEXT CLASSIFICATION AND TEXT SUMMARIZATION CONFIG IN TABLES Llama3.1-8B is used as the common LLM and NV-Embed-v1 is used as the text encoder for VectorICL. The ICL methods are prompted with up to 50 shots while the soft prompt is trained over the entire training set. 17 Preprint. Table 4: Hyperparameters for V-ICL training. Hyperparameter Value Learning Rate Learning Rate Schedule Optimizer β1 β2 Training dtype Batch Size Generation Temperature 1e-3 Cosine Annealing AdamW 0.9 0.999 bf16 128 2e-1 A.3 ADDITIONAL EXPERIMENTAL RESULTS Figure 5: Text Reconstruction Figure 6: Function Regression - 10 digits (upper) and 3 Digits (lower) Preprint. Figure 7: Text Classification - Pretrained Projectors Figure 8: Text Classification - Finetuned Projectors 19 Preprint. Figure 9: Text Classification - Few-shot ICL Figure 10: Text Summarization - Pretrained Projectors Figure 11: Text Summarization - Finetuned Projectors 20 Preprint. A.4 BRAIN FMRI QUESTION CATEGORIES Table 5: Question Categories and Their Associated Questions Category Questions Sensory and Perceptual Descriptions Social, Emotional, and Interpersonal Content Cognitive and Reflective Aspects Language, Structure, and Syntax Physical Actions and Movements Numbers and Quantitative Information Health, Disease, and Biological Aspects Conflict, Urgency, and Change Does the input mention or describe taste? Does the input mention or describe sound? Does the sentence include specific sound or auditory description? Does the input mention or describe visual experience? Does the input mention or describe texture? Does the sentence describe sensory experience? Does the input mention anything related to color? Does the input mention or describe smell? Does the input mention anything related to eyes? Does the sentence describe visual experience or scene? Does the input describe specific texture or sensation? Does the sentence describe specific sensation or feeling? Does the input mention anything related to arguing? Does the input mention anything related to empathy? Does the sentence involve discussion about personal or social values? Does the input discuss societal issue or social justice topic? Does the input mention or describe high emotional intensity? Does the sentence describe relationship between people? Does the input mention or describe highly positive emotional valence? Does the input mention or describe highly negative emotional valence? Does the input mention anything related to conflict? Does the sentence describe personal or social interaction that leads to change or revelation? Does the sentence express philosophical or existential query or observation? Does the sentence involve an expression of personal values or beliefs? Does the sentence express sense of belonging or connection to place or community? Is the sentence emotionally positive? Is the sentence reflective, involving self-analysis or introspection? Does the input involve planning or organizing? Does the text include planning or decision-making process? Does the sentence convey decision or choice made by the narrator? Does the sentence describe personal reflection or thought? Is the input about discovery or realization? Does the input contain sense of ambiguity? Is the sentence providing an explanation or rationale? Does the input mention anything related to knowledge? Does the sentence contain proper noun? Does the sentence include conditional clause? Does the sentence contain negation? Does the sentence use unique or unusual word? Does the sentence include direct speech quotation? Does the sentence include dialogue? Does the sentence contain cultural reference? Does the sentence involve recount of social or community event? Does the input include comparison or metaphor? Does the sentence include technical or specialized terminology? Is the sentence abstract rather than concrete? Does the sentence include an account of miscommunication or misunderstanding? Does the text describe mode of communication? Is the sentence conveying the narrators physical movement or action in detail? Does the input mention anything related to motor movements? Does the sentence describe physical action? Does the sentence describe physical sensation? Does the sentence describe an activity related to daily life or routine? Does the input mention anything related to an action? Does the sentence involve spatial reasoning? Does the input mention number less than 5? Does the input contain number? Does the input mention number greater than 100? Does the input mention anything related to arithmetic? Does the input mention anything related to calculation? Does the input contain measurement? Does the input mention anything related to diseases? Does the input mention anything related to food? Does the input mention anything related to age? Does the input mention anything related to gender? Does the input mention anything related to disgust? Does the input mention anything related to children? Does the sentence involve an unexpected incident or accident? Does the input mention anything related to anger? Does the sentence convey sense of urgency or haste? Does the sentence describe change in physical or emotional state? Does the sentence describe moment of relief or resolution of tension? Does the sentence express the narrators opinion or judgment about an event or character?"
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "UC San Diego"
    ]
}