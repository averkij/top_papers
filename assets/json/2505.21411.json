{
    "paper_title": "Pangu Pro MoE: Mixture of Grouped Experts for Efficient Sparsity",
    "authors": [
        "Yehui Tang",
        "Xiaosong Li",
        "Fangcheng Liu",
        "Wei Guo",
        "Hang Zhou",
        "Yaoyuan Wang",
        "Kai Han",
        "Xianzhi Yu",
        "Jinpeng Li",
        "Hui Zang",
        "Fei Mi",
        "Xiaojun Meng",
        "Zhicheng Liu",
        "Hanting Chen",
        "Binfan Zheng",
        "Can Chen",
        "Youliang Yan",
        "Ruiming Tang",
        "Peifeng Qin",
        "Xinghao Chen",
        "Dacheng Tao",
        "Yunhe Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The surgence of Mixture of Experts (MoE) in Large Language Models promises a small price of execution cost for a much larger model parameter count and learning capacity, because only a small fraction of parameters are activated for each input token. However, it is commonly observed that some experts are activated far more often than others, leading to system inefficiency when running the experts on different devices in parallel. Therefore, we introduce Mixture of Grouped Experts (MoGE), which groups the experts during selection and balances the expert workload better than MoE in nature. It constrains tokens to activate an equal number of experts within each predefined expert group. When a model execution is distributed on multiple devices, this architectural design ensures a balanced computational load across devices, significantly enhancing throughput, particularly for the inference phase. Further, we build Pangu Pro MoE on Ascend NPUs, a sparse model based on MoGE with 72 billion total parameters, 16 billion of which are activated for each token. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and 800I A2 through extensive system simulation studies. Our experiments indicate that MoGE indeed leads to better expert load balancing and more efficient execution for both model training and inference on Ascend NPUs. The inference performance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further improved to 1528 tokens/s per card by speculative acceleration, outperforming comparable 32B and 72B Dense models. Furthermore, we achieve an excellent cost-to-performance ratio for model inference on Ascend 300I Duo. Our studies show that Ascend NPUs are capable of training Pangu Pro MoE with massive parallelization to make it a leading model within the sub-100B total parameter class, outperforming prominent open-source models like GLM-Z1-32B and Qwen3-32B."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 8 2 ] . [ 2 1 1 4 1 2 . 5 0 5 2 : r PANGU PRO MOE: MIXTURE OF GROUPED EXPERTS FOR EFFICIENT SPARSITY TECHNICAL REPORT Pangu Team, Huawei pangutech@huawei.com"
        },
        {
            "title": "ABSTRACT",
            "content": "The surgence of Mixture of Experts (MoE) in Large Language Models (LLMs) promises small price of execution cost for much larger model parameter count and learning capacity, because only small fraction of parameters are activated for each input token. However, it is commonly observed that some experts are activated far more often than others, leading to system inefficiency when running the experts on different devices in parallel. Existing heuristics for balancing the expert workload can alleviate but not eliminate the problem. Therefore, we introduce Mixture of Grouped Experts (MoGE), which groups the experts during selection and balances the expert workload better than MoE in nature. It constrains tokens to activate an equal number of experts within each predefined expert group. When model execution is distributed on multiple devices, which is necessary for models with tens of billions of parameters, this architectural design ensures balanced computational load across devices, significantly enhancing throughput, particularly for the inference phase. Further, we build Pangu Pro MoE on Ascend NPUs, sparse model based on MoGE with 72 billion total parameters, 16 billion of which are activated for each token. The configuration of Pangu Pro MoE is optimized for Ascend 300I Duo and 800I A2 through extensive system simulation studies. Our experiments indicate that MoGE indeed leads to better expert load balancing and more efficient execution for both model training and inference on Ascend NPUs. The inference performance of Pangu Pro MoE achieves 1148 tokens/s per card and can be further improved to 1528 tokens/s per card by speculative decoding on Ascend 800I A2, outperforming comparable 32B and 72B Dense models. Furthermore, we achieve an excellent cost-to-performance ratio for model inference on Ascend 300I Duo. Our studies show that Ascend NPUs are capable of training Pangu Pro MoE with massive parallelization to make it leading model within the sub-100B total parameter class, outperforming prominent open-source models like GLM-Z1-32B and Qwen3-32B."
        },
        {
            "title": "Introduction",
            "content": "The Mixture-of-Experts (MoE) model [30, 4, 16] is becoming standard component in Large Language Models (LLMs) [8, 37, 39, 24, 47], where scaling model size brings clear benefits. MoE models only activate subset of experts for each token to effectively reduce the computation cost of the gigantic language models. However, expert load imbalance [18, 44] is critical challenge for materializing the computation gain on distributed training and inference systems. Because of the gigantic scale of recent LLMs, the model parameters are usually loaded separately on different computing devices, like Ascend NPUs [34]. It is commonly observed that some experts are frequently activated for input tokens while some remain rarely used. Since experts are typically distributed across multiple devices, the device with busiest experts straggle both training and inference processes, and further hinder computational efficiency and throughput. To overcome this limitation, we develop the Mixture of Grouped Experts (MoGE) architecture. When selecting the experts for token, we divide the experts into equal groups and then choose experts from each of the groups. Each group has an identical number of activated experts. In typical distributed deployments, the experts are assigned to the devices according to the group ID. MoGE effectively balances the computational load across all participating devices. This design offers substantial improvements in throughput in training and inference scenarios. 1https://gitcode.com/ascend-tribe/pangu-pro-moe . 1 Figure 1: Illustration of the Mixture of Grouped Experts (MoGE) architecture. The total experts are evenly partitioned into non-overlapping groups, with each group typically assigned to distinct computational device. For each input token, initial gating scores for all experts are computed via global softmax router. Subsequently, within each expert group, the experts with the highest scores are chosen based on these initial scores. The scores corresponding to unselected experts are effectively set to zero. The final output is obtained by weighted sum of the outputs from the activated experts and shared expert. Based on MoGE, we build Pangu Pro MoE, with 72 billion parameters, 16 billion of which are activated for each input token. We configure the model structure based on extensive simulation studies, aiming for optimized runtime performance on Ascend 300I Duo and 800I A2 platforms. The inference strategy is also optimized for Pangu Pro MoE on multiple aspects, including system, algorithms, and kernel design, specifically tailored for the Ascend NPUs. At the system level, we propose hierarchical & hybrid parallelism and communication strategy, co-designed with the model architecture and Ascends interconnect topology. This design significantly reduces redundant computation and communication overhead. Compression algorithms such as quantization are also applied to further reduce computational cost. At the kernel level, we develop high-performance MulAttention and SwiftGMM kernels, custom-optimized for Ascend NPUs. The experiments demonstrate that the optimized inference of Pangu Pro MoE, configured as 72BA16B MoE, on the Ascend NPUs, achieves low latency in low-concurrency scenarios and high throughput in high-concurrency settings, outperforming comparable dense models like 32B and 72B dense models. Pangu Pro MoE is pre-trained on diverse and high-quality corpus comprising 13 trillion tokens using 4K Ascend NPUs. Following this extensive pre-training phase, we employ Supervised Fine-Tuning (SFT) and subsequent Reinforcement Learning (RL) to further enhance its reasoning capabilities. Our comprehensive evaluation results shows Pangu Pro MoE is forefront model with total parameters under 100 billion. It outperforms several strong open-source models, including GLM-Z1-32B [11], Qwen3-32B [43], and Gemma327B [35], across wide range of competitive benchmarks."
        },
        {
            "title": "2 Mixture of Grouped Experts",
            "content": "In this section, we introduce the MoGE, novel MoE architecture designed to intrinsically achieve perfect load balancing across devices. MoGE achieves this by partitioning experts into distinct groups, with each group typically mapped to specific device, and then enforcing routing strategy that activates fixed number of experts from each group. We begin by revisiting the expert imbalance problem in conventional MoE, then detail the MoGE architecture and its associated routing mechanisms, and finally discuss the auxiliary losses designed to optimize its performance. 2 (a) The distribution of activated experts for single token. (b) Distribution of imbalance score. Figure 2: Comparison of expert activation patterns and load imbalance between conventional Top-K routing in MoE and MoGEs group-balanced routing. (a) Illustrates how experts are selected for single input token. In this example, 8 experts are chosen from total of 24 experts, with an expert parallelism of 4. Conventional Top-K routing (left) selects 8 experts that are unevenly distributed across the 4 devices (i.e., Device 1 gets zero experts while Device 2 gets 4). In contrast, MoGE (right) divides the 24 experts into 4 groups and selects exactly 2 experts from each group. (b) Shows the estimated probability distribution of the Imbalance Score (IS) for both routing mechanisms, where lower IS indicates better balance. MoGE inherently achieves an IS of 0, signifying perfect load balance. Conventional Top-K routing, however, exhibits high probability of IS values greater than 0, indicating frequent load imbalance. 2.1 Expert Imbalance in Conventional MoE We consider an input hidden state from the space Rd. The MoE layer comprises distinct expert networks, denoted as {E1, E2, , EN } where each Ei : maps input hidden states to output representations in space Y. The output of the MoE module for given input hidden states is the weighted sum over the expert networks: = (cid:88) i=1 G(h)i Ei(h), (1) where G(x)i represents the gating score, or weight, assigned by router mechanism to the i-th expert Ei. These scores determine how much influence each expert has on the final output. Typically, G(h) is implemented by taking the softmax over the Top-K [30] inner-product between the router matrix Rdn and h: (cid:16) (cid:17)(cid:17) (cid:16) G(h) = Softmax TopK h, , TopK(v, k) = (cid:26)vi, if vi is in the top values of v. , otherwise. , (2) where Softmax(u) = exp(ui)(cid:14) (cid:80) exp(ui) and TopK(v, k) selects the top elements based on the values in v. denotes the number of experts activated per token. This routing method allows for sparse activation of experts by skipping computations for expert Ei where G(h)i = 0. In practical implementations, especially for large models with tens of billions of parameters, these experts are typically distributed evenly across computational devices (e.g., NPUs) to enable parallel processing. However, the standard Top-K routing mechanism described in Eq. (2) places no constraints on which experts are selected. For given token, the chosen experts could, by chance, be concentrated on few devices, as depicted in Figure 2(a) (left). This uneven assignment of active experts to devices leads to load imbalance. Some devices might be busy processing many tokens (if their resident experts are frequently chosen), while others might be idle or underutilized. This creates \"straggler\" problem: the overall processing speed is dictated by the slowest (most heavily loaded) device, leading to inefficient use of computational resources and increased latency. To quantify this load imbalance, we introduce the Imbalance Score (IS). Imagine we are processing batch of input data (e.g., set of tokens) X. For this batch, let Ti(X) be the total number of expert computations (or tokens routed to experts) handled by device i. The Imbalance Score is then defined as the difference between the maximum and minimum load across all devices, normalized by the batch size X: IS(X) = 1 (cid:20) max i{1, ,M } Ti(X) min i{1, ,M } (cid:21) . Ti(X) (3) higher IS value signifies greater disparity in workload among devices, indicating more severe imbalance. Conversely, an IS of 0 implies perfect load balance, where every device processes an equal amount of work. This metric is crucial because it directly reflects the potential for performance bottlenecks due to uneven workload distribution. Even if the average load per device is reasonable, high IS means some devices are overloaded while others are waiting, reducing overall system efficiency. To understand the propensity of Top-K routing towards imbalance, we can estimate the distribution of the Imbalance Score. If we assume, for simplicity, that each of the experts chosen for token is selected independently and uniformly at random from the total experts, we can use Monte Carlo simulation. For given set of parameters, we can simulate the routing process many times (say, times) and calculate the IS for each simulation. The probability of observing specific IS value can then be estimated as: (IS = v) ="
        },
        {
            "title": "1\nD",
            "content": "D (cid:88) (IS(Xj) = v) , (4) j=1 where Xj is the j-th simulated batch, and I() denotes the indicator function. Figure 2(b) (right, blue bars) illustrates such an estimated distribution for configuration with = 64, = 8, = 8, and small batch size of = 16 tokens. The distribution clearly shows that non-zero IS values are highly probable. In fact, for these parameters, the probability of IS > 0 (i.e., some level of imbalance) is nearly 1. This signifies that with standard Top-K routing, load imbalance is almost an inevitable occurrence, especially when the batch size is small, as theres less opportunity for random selections to average out perfectly across devices. 2.2 Basic Architecture of MoGE To directly tackle the load imbalance problem inherent in conventional MoE, we propose the Mixture of Grouped Experts (MoGE) architecture, which employs novel group-balanced routing strategy. The core idea is to ensure that for each token, an equal number of expert computations are distributed to each of the devices. This guarantees an Imbalance Score (IS) of 0 by design, as illustrated by the single yellow bar in Figure 2(b). The MoGE architecture (visualized in Figure 1) achieves this as follows: 1. Expert Partitioning: The total experts are deterministically partitioned into distinct, nonoverlapping groups. Each group, say group (where = 1, 2, , ), contains Ng = N/M experts. Crucially, each group of experts is typically assigned to reside on specific computational device. 2. Group-Balanced Routing: For each input token h, the routing mechanism ensures that fixed number of = K/M experts are activated from each of the groups. This means total of experts are also activated per token, but now with strict per-device activation count. First, similar to standard MoE, we compute initial scores for all experts using the input token and router weight matrix Rdn. Instead of directly applying global Top-K, we first apply global softmax to these raw scores to obtain normalized probabilities or affinities, S, for all experts: = Softmax (cid:16) (cid:17) (5) Here, is vector of length , where Si is the initial score for expert Ei. Next, we consider these scores group by group. Let Sj denote the sub-vector of containing the scores for the Ng = N/M experts belonging to group j. Within each group j, we perform local Top-K selection based on the scores in Sj. The gating scores G(h) for MoGE are then constructed by concatenating the results of these local Top-K operations from all groups: G(h) = (TopK (S1, ) , , TopK (SM , )) . Effectively, for each group j, we select the experts with the highest scores within that group. Experts not selected within their group receive weight of 0. common and particularly impactful configuration is when we want to select exactly one expert from each group, meaning = 1. In this case, Eq. (6) simplifies to performing Top-1 selection within each group. Pangu Pro MoE follows this setting as shown in Table 1. (6) Figure 3: Simulation results of candidate model configurations. Throughput under 100 ms and 50 ms TPOT constraints on Ascend 300I Duo and 800I A2 platforms, respectively, is recorded. All results are normalized relative to randomly selected candidate. The final output yMoGE is then computed similarly to Eq. (1), but using these group-derived gating scores: yMoGE = (cid:88) i=1 G(h)i Ei(h). (7) Auxiliary Load Balancing Loss While MoGE structurally guarantees inter-group load balance (i.e., across devices), it is still crucial to ensure that the routing mechanism learns to distribute the workload reasonably among experts within each group. To achieve this, we employ batch-level auxiliary load balancing loss: ℓaux = α (cid:88) i=1 fipi, where the hyperparameter α controls the strength of the auxiliary loss. Here, fi represents the fraction of tokens within the batch routed to expert Ei, and pi is the average gating score assigned to expert Ei: fi = KB (cid:88) tB {Token selects Expert i} , pi = 1 (cid:88) tB Si,t, (8) where I{} is the indicator function, and Si,t denotes the gating score of expert Ei for token (derived from the global softmax scores in Eq. (5) before group-wise Top-K selection). Our ablation studies indicate that computing this auxiliary balancing loss based on the global softmax weights (i.e., considering all experts collectively for the loss signal) yields superior performance compared to calculating the loss independently and locally for each group based on its intra-group routing decisions. 2.3 Architecture Simulation for Ascend NPUs This section primarily focuses on how we determine the key hyperparameters of the model to ensure better affinity with Ascend hardware. During the model design process, hierarchical strategy is employed, progressing from coarse to fine granularity to balance accuracy and inference efficiency on Ascend 300I Duo and 800I A2 platforms. The strategy comprises three stages: first, coarse-grained filter determines the parameter range based on memory bandwidth and latency constraints of single server; second, domain expertise is used to shortlist potential models, narrowing the design space; third, the performance of candidate models is evaluated using an operator-level simulator that correlates system hardware parameters, such as TFLOPS, memory access bandwidth, memory capacity, and interconnection topology, and automatically searches for optimal parallelism. 5 Table 1: Model configuration for Pangu Pro MoE."
        },
        {
            "title": "Pangu Pro MoE",
            "content": "Figure 3 presents the simulation results for candidate models. These models are configured within specific parameter ranges, including hidden dimensions (4096-8192), query heads (32-64), key-value heads (8-16), number of layers (40-64) and routed expert counts (3264). All configurations maintain total parameter count within the range of 6080 billion."
        },
        {
            "title": "Vocabulary Size\nHidden Size\nIntermediate Size\nQuery Heads\nKV Heads\nHead Size\nLayers\nRouted Experts\nActivated Experts\nShared Experts",
            "content": "153376 5120 1344 40 8 128 48 64 8 4 During the decoding phase, the inference performance is primarily influenced by memory access and communication overhead. The model architecture, particularly the width-to-depth ratio, significantly impacts communication efficiency. For example, the hidden size determines the data volume per communication, while the number of layers affects the communication frequency. The trade-off between these two factors can be effectively evaluated by considering the systems static communication latency and available bandwidth. From the memory access perspective, under identical parameter scales, higher sparsity ratio reduces the number of parameters that need to be loaded under the same concurrency conditions. Through the hierarchical strategy and fine-grained simulation, the orange star-marked model in Figure 3 exhibits superior performance for Ascend 300I Duo and 800I A2 platforms in all candidates under the specified conditions, which serves as the model configuration for Pangu Pro MoE, as shown in Table 1. # Activated Parameters (B) # Total Parameters (B) 16.50 71."
        },
        {
            "title": "3 Model Training",
            "content": "In this section, we present the training pipeline for Pangu Pro MoE. Our framework comprises two key phases: pre-training and post-training. Each phase employs specific training strategies and data curricula to progressively enhance the model capabilities. The model configuration of Pangu Pro MoE is shown in Table 1. 3.1 Pre-Training We first detail the data construction used in the pre-training phase, emphasizing their relevance to model capabilities. Next, we explain the pre-training strategies that ensure stable model convergence. 3.1.1 Data Construction The pre-training dataset of Pangu Pro MoE contains total number of 13 trillion tokens produced by our tokenizer with vocabulary size of 153,376 tokens. The tokenizer is designed by domain-aware vocabulary strategy that aims to maintain balanced representation between domains, and more details can be found in [47]. This dataset contains diverse and high-quality content from various sources, such as web pages, books, multilingual, code, STEM (Science, Technology, Engineering, and Mathematics), industrial domains, reasoning and synthetic data. Training Phases The overall pre-training process of Pangu Pro MoE is structured into three sequential phases grounded in cognitive development theories: the general phase (9.6T), the reasoning phase (3T), and the annealing phase (0.4T). The three phases are designed to progressively develop core capabilities of Pangu Pro MoE, such as general knowledge and linguistic ability in the first phase, then to improve reasoning skills of the model in the second phase, and to further refine model knowledge and behavior in the third phase. Apart from the general data from various sources, we particularly involve lot of high-quality data from multiple industrial domains in the first general phase. In general, the first phase is trained with 4K sequence length, and the latter two are trained with 32K sequence length. The second reasoning phase targets the reasoning skills of Pangu Pro MoE by significantly increasing the proportion of more complicated data such as STEM, coding and internal data. We put great effort into the amount and quality of the reasoning data, by optimizing the data cleaning, data generating, and data evaluating pipeline. We particularly design synthetic short and long chain-of-thought (CoT) for those difficult samples. To better align with long CoT responses, we use 32K sequence length. In addition, Pangu Pro MoE is trained with significantly larger range of high-quality reasoning data than the previous model [34, 47], which therefore helps it achieve good performance even in relatively small model size. 6 The third annealing phase serves to bridge the transition from pre-training to post-training, where the instruction style data increases to approximately 20% of the corpus. In this phase, priority is given to the data with extremely higher quality and difficulty scores, following curriculum-based sampling strategy throughout all three phases. We also intentionally increase the number of data on an advanced level of STEM education, which is totally 18% of the corpus. Using proxy model with 7 billion parameters, we perform intensive ablation studies on data selection and data recipe in this phase, aiming to evaluate how different strategies affect the model. Our training shows that, by equipping itself with proper data recipe and decayed learning rate, this third phase can still bring about large improvement in model performance. Data Evaluation The quality of our training corpus is continuously monitored and improved using our domain-aware model-based evaluation, where we fine-tune some Pangu series models as evaluators. Specifically, we design annotated datasets in this fine-tuning process for different domains and thus create few domain-aware evaluators. Our ablation study on the small proxy model shows that this data evaluation system produces better evaluation performance than using only one unified evaluator. All of our data samples are sent through this evaluation system and assigned scores in multiple dimensions, including cleanliness, fluency, educational value, and richness. These scores are used in our data selection and sampling strategy. In addition, category label is given to each data sample, with 188 categories in total. This data label acts as classifier, making it easier to classify and group data, and more importantly, it optimizes the data mixture in fine-grained manner and ensures that our training corpus covers reasonable distribution of various topics. 3.1.2 Pre-training Parameters Training Details During the pre-training stage, our models are trained for single epoch using the AdamW optimizer with hyperparameters β1 = 0.9 and β2 = 0.95. cosine learning rate schedule is employed throughout, encompassing three progressive phases. In the general phase, the learning rate decays from 3 104 to 3 105 with batch size of 4 million tokens. This is followed by the reasoning phase, during which the learning rate further decreases from 3 105 to 1 105, and the batch size is increased to 16 million tokens to enhance training on complex reasoning tasks. In the final annealing phase, the learning rate is gradually reduced from 1 105 to 1 107, with the batch size maintained at 16 million tokens to ensure stable convergence. This structured, multi-phase training strategy facilitates both robust generalization and effective specialization across different learning objectives. Training Devices The proposed architecture is trained and evaluated using the Huawei Ascend 800T A2. The Ascend 800T A2 is high-efficiency AI server designed with Huaweis proprietary DaVinci architecture [23]. single accelerator in Ascend 800T achieves computational throughput of 256 TeraFLOPS for half-precision floating-point (FP16) operations and 512 TeraOPS for integer precision (INT8) calculations. Despite its superior performance, it has maximum power consumption of only 310W, significantly lower than its design specification of 350W. The integration of diverse computing units within the DaVinci architecture enhances the completeness and efficiency of AI computations, thereby extending its applicability and significantly improving the overall performance of AI systems while reducing deployment costs. 3.2 Post-Training Alignment Supervised Fine-Tuning The post-training supervised finetuning data for Pangu Pro MoE is categorized into reasoning and non-reasoning subsets, with sampling ratio of 3:1 in favor of reasoning tasks. Specifically, the reasoning samples primarily includes tasks such as mathematical problem-solving, code generation, and logical inference, while the non-reasoning samples focuses on general language instruction following, question answering, text generation, long-context understanding, semantic classification, and tool usage. The reasoningintensive tasks, especially those involving multi-step computation, symbolic manipulation, or formal logic, are often underrepresented in instruction-tuning corpora, yet they are critical for the emergence of slow thinking capabilities in large language models. By allocating more weight to such tasks, we aim to explicitly encourage the model to develop robust intermediate reasoning skills and deeper cognitive representations that generalize beyond surface-level pattern matching. For each sub-domain, we utilize simple yet effective diversity-based metric [46] to select representative instructions from large pool of high-quality, expert-curated, and synthetic instruction data. This diverse task composition constitutes multi-dimensional training space, designed to enhance the models generalization capability across both specialized and general-purpose tasks. Furthermore, the training adopts two-stage progressive optimization strategy over six training rounds, with global batch sizes of 64 and 32 in each respective stage. The phased design allows for curriculum-like learning process, where the model initially focuses on broader instruction-following behavior and gradually transitions to mastering more complex reasoning tasks with finer gradient updates. This progressive training schedule not only stabilizes convergence but also reduces catastrophic forgetting when reasoning tasks are emphasized in later rounds. From the perspective of slow thinking model, this design provides two key advantages in order for the model to have stronger reasoning capabilities. First, emphasizing reasoning early on builds strong inductive biases toward stepwise problem solving. Second, the staged finetuning allows the model to consolidate general linguistic capabilities before being pushed toward more cognitively demanding tasks. Together, this formulation enables the model to balance fluency with depth, yielding improved performance on benchmarks that require thoughtful, multi-step reasoning. Parameter updates are performed using the AdamW optimizer with weight decay of 0.1. The learning rate follows cosine decay schedule, gradually decreasing from an initial value to 10% of its peak by the end of training. Stage-specific learning rates are initialized at 1e-5 and 3e-6, respectively, implementing phased learning rate scheduling approach to balance convergence speed and training stability. Checkpoint Merging Model merging has proven to be an effective technique for integrating diverse task capabilities and enhancing generalization. Existing studies primarily focus on merging heterogeneous models, such as combining SFT models trained with different data mixtures or hyperparameters. Beyond these prior findings, we explore an alternative paradigm: merging homogeneous intermediate checkpoints derived from single SFT training trajectory. Our approach consolidates checkpoints saved at different stages of the same training run, leveraging their implicit complementary behaviors to improve final model robustness and generalization. Specifically, we first partition all intermediate checkpoints into distinct groups based on training phases, using epoch indices as indicators. Checkpoints saved within the same epoch are assigned to the same group, ensuring that each group captures unique behavioral characteristics while maintaining temporal coherence. We then perform weighted aggregation of delta parameters between the SFT checkpoints and the initial base model parameters. Unlike existing methods, we apply two-layer merging strategy: 1) Intra-group merging: We first aggregate epoch-wise delta parameters within each group; 2) Inter-group merging: The resulting epoch-wise deltas are then merged across different groups to obtain the final model parameters. By incorporating the model merging, we effectively capture and integrate complementary knowledge from different training stages. Mathematically, we obtain the merged model parameters as follows: Θmerged = Θbase + (cid:88) λk Nk(cid:88) k=1 i=1 1 Nk (Θk Θbase), (9) where Θmerged denotes the final merged model parameters, Θbase denotes the initial model parameters of an SFT run, Nk is the number of model checkpoints in the k-th group, and Θk denotes the model parameters of the i-th checkpoint in the k-th group. Reinforcement Learning We employ the Group Relative Policy Optimization (GRPO) algorithm for the RL policy learning, which is currently common practice in the post-training RL phase. challenge arises when all responses generated for given prompt receive identical rewards. In such cases, the normalized advantage becomes zero, potentially causing the GRPO objective to degrade into simple behavior cloning loss, thereby stifling policy exploration. To address this, we introduce \"Zero-Advantage-Mask\" mechanism. This mechanism nullifies the loss contribution from samples where the advantage is zero. Consequently, policy updates are driven only by \"effective\" data exhibiting clear learning signal (non-zero advantage), promoting more robust exploration and learning. To provide nuanced and task-specific guidance, we utilize multi-source reward system that dynamically routes prompts and their generated responses to appropriate evaluators based on task characteristics. This system comprises three key modules: Correctness Rewards: For tasks with verifiable ground-truth, such as mathematics or coding, correctness-based rewards are assigned. Mathematical problems are assessed by hybrid system combining rule-based verifiers for standard formats and LLM-based verifiers for more nuanced interpretations. Code responses undergo multi-stage evaluation: extraction, syntax verification, execution via an online interpreter, and comparison against test cases. Rewards can be structured hierarchically (stage reward) or based on pass rates (continuous reward). Preference Rewards: For open-domain tasks where ground-truth is unavailable (e.g., creative writing), the system incorporates preference reward model. This model, typically another LLM 8 Table 2: MFU comparison between baseline and after optimization"
        },
        {
            "title": "Baseline",
            "content": "TP1, EP8, CP8, PP6, VPP4 -"
        },
        {
            "title": "MFU",
            "content": "- Optimized TP8, EP2, CP1, PP5, VPP5 Hierarchical EP All-to-All communication Adaptive Pipeline Overlap Mechanism Fused operators increase by 35% trained as judge, emulates human preferences. Its output scores are normalized before use in GRPO to ensure stability and consistent scaling across diverse prompts. Auxiliary Rewards: The reward system also includes auxiliary components. format validator acts as an initial filter, penalizing responses that violate predefined structural requirements. lightweight repetition penalty, based on string hashing, discourages overly verbose or redundant outputs. These operate orthogonally to the primary reward signals, maintaining output quality without distorting the main learning objectives. To improve training efficiency and effectiveness, we implement curriculum data mixing strategy. Queries that are consistently too easy (yielding all correct responses) or too difficult (yielding all incorrect responses) produce constant rewards. These offer minimal learning signal for GRPO yet incur computational costs. Our curriculum approach assesses data complexity in model-aware manner: the current LLM generates multiple diverse responses to prompt, with complexity determined by the pass rate (for verifiable tasks) or loss (for non-verifiable tasks). Lower pass rates or higher losses indicate greater complexity. Training then proceeds by feeding the model progressively curated mix of samples with varying complexities, ensuring the model continually receives meaningful reward signals and facilitating more effective and efficient policy updates."
        },
        {
            "title": "Infrastructure",
            "content": "4.1 Training System Optimized for Ascend NPUs The advanced accelerate techniques as introduced in Pangu Ultra MoE [34] have been further optimized to deliver improved performance, including refined Hierarchical EP All-to-All Communication with shorter communication volumes, finer-grained operator scheduling and more effective overlapping in Adaptive Pipeline Overlap Mechanism, and additional recomputing and swap modules in memory optimization strategies. These optimizations not only boost the Model FLOPs Utilization (MFU) for Pangu Ultra MoE (details will be published soon), but are also adaptable to Pangu Pro MoE, achieving significant training efficiency improvements with 35% relative increase in MFU, as demonstrated in Table 2. To fully exploit these optimizations on Ascend NPUs, the training adopts carefully tuned parallelism configuration: Tensor Parallelism (TP) = 8, Expert Parallelism (EP) = 2, Pipeline Parallelism (PP) = 5, Virtual Pipeline Parallelism (VPP) = 5, and Context Parallelism (CP) = 1. The sizes of TP and EP are specifically selected to maximize performance under the Hierarchical EP All-to-All Communication scheme. Compared to Pangu Ultra MoE, the EP size is reduced to 2 to minimize EP communication volume when memory capacity allows. The model contains 48 transformer layers, and to achieve better load balancing across pipeline stages, 2 additional no-op layers are appended, increasing the total number of layers to 50. These layers are then evenly partitioned across 5 pipeline stages, with further subdivision using 5 virtual pipeline stages. This 55 PPVPP configuration ensures balanced computation and communication overheads across devices, enhancing the overall scalability and throughput of the training process. Due to the reduced sizes of Pangu Pro MoE and PP-VPP compared with Pangu Ultra MoE [34], the accumulated activation memory during the warm-up phase is significantly decreased. This diminished memory demand enables stable training without reliance on previously required memory optimizations for Pangu Ultra MoE, including fine-grained recomputation and tensor swapping strategies. Consequently, the training process is further accelerated by eliminating the redundant overhead. Additionally, due to reduced EP communication volumes and the removal of communication overhead in permute recomputation, operator scheduling in Adaptive Pipeline Overlap mechanism achieves full compatibility with Pangu Pro MoE by maximizing the overlap of communication with computation. 9 Figure 4: Overview of the inference system optimization. H2P strategy is employed to achieve highefficiency distributed parallel inference across different modules. Additionally, two key fused operators, MulAttention and SwiftGMM, are specifically designed for the Ascend platform to accelerate model inference. Moreover, the proposed Mixture of Grouped Experts (MoGE) architecture in Pangu Pro MoE effectively mitigates the computational load imbalance across devices by over 50%, which is quantified through the reduced maximum disparity in execution time for permute and gmm_up operators. We also optimize operator kernels through architectural refinements. For fundamental operators like Matmul, block size is adjusted during initial data transfers from general memory to L1 cache based on the smaller L0 capacity, which enables earlier L1-to-L0 data transfers, ultimately reducing latency and increasing cube utilization by >10%. We further optimize the cache strategy based on the feature of the Ascend architecture. By refining the matrix block partitioning method and adjusting the data transfer orders, combined with staggered allocation of computing blocks to mitigate access conflicts, the utilization of HBM bandwidth is significantly enhanced, leading to 5% - 10% improvement in operator performance. Collectively, these systematic optimizations reduce computational redundancy, improve data flow efficiency, and boost training throughput by fully leveraging hardware capabilities. 4.2 Inference System Optimized for Ascend NPUs 4.2.1 Parallel Optimization Hierarchical & Hybrid Parallelism Pangu Pro MoE implements fused expert system, where sparse expert modules contain 95% of total parameters while attention mechanisms retain only 5%. Through systematic analysis of the architectural configuration and hardware specifications of the Ascend computing platform, hierarchical & hybrid parallel (H2P) strategy was devised for the deployment of inference, which can eliminate redundant computational operations and inter-process communication bottlenecks to achieve high computing efficiency, as shown in Figure 4. For attention modules, hybrid DP2+TP4 parallelism strategy is used to reduce cross-CPU communication overhead on the 300I Duo NPU, where four chips are controlled by one CPU. Requests are grouped along the batch dimension to balance computation between CPU domains. For expert modules, combination of Tensor Parallelism (TP) and Expert Parallelism (EP) is adopted to address both memory and latency challenges. EP retains full expert matrices for computation efficiency, but causes load imbalance. TP partitions expert matrices for balanced workload, but may reduce efficiency due to suboptimal shapes. hybrid TP2+EP 10 strategy balances these trade-offs based on empirical performance analysis. For shared experts, due to uniform workloads, shared experts use TP8 for dense, efficient, and balanced computation. By applying fine-grained hybrid parallelism strategies tailored to the model and the Ascend computing platform, the approach achieves optimized inference performance through balanced computation, reduced communication overhead, and efficient hardware utilization. Communication Strategy Building upon the optimized hybrid parallel strategy, we conduct further optimization on the associated communication operations to minimize computational and communication redundancy, as shown in Figure 4. For the attention module, we adopt the parallel strategy with DP2+TP4. Specifically, inputs are split into two mini-batches where each batch is inferred across four devices. This configuration originally required one AllReduce and one AllGather operation. To optimize communication efficiency, we replace the AllReduce operation with Reduce-Scatter operation followed by AllGather operation, effectively reducing communication data by 50%. Furthermore, we strategically reposition the AllGather operator after the RMSNorm operator rather than before it. This adjustment enables parallel execution of RMSNorm computations across devices, achieving 75% reduction in computational load through distributed processing. For the MoE module implementation, we employ combined TP2+EP4 parallel strategy with shared experts processed through TP8 parallelism. The modules final output integrates results from both routed and shared experts via global AllReduce operation. To maintain compatibility with the subsequent DP2 attention module while avoiding repartitioning overhead, we decompose the global AllReduce into global Reduce-Scatter followed by local AllGather operation across four devices. Communication-Compute Overlap Building on prior work in parallelism and communication optimization, we further reduce communication latency by streamlining the interaction between adjacent computation and communication streams on the Ascend platform. Multi-stream fusion maps computation and communication tasks to separate hardware units, enabling concurrent execution by decoupling data dependencies. In the Pangu Pro MoE model, global all-gather and reduce-scatter operations in the Expert component contribute approximately 8% of the total network latency. To address this, we propose fine-grained, operator-level compute-communication fusion strategy that decomposes traditionally sequential tasks into interleaved subtasks. By leveraging multi-stream architecture of the Ascend computing platform, we introduce two fused strategies: GMMRS (GroupedMatMul + ReduceScatter) and AGMM (AllGather + MatMul). These enable fine-grained pipeline overlap, improving overall execution efficiency. 4.2.2 Quantization Compression Expert-Aware Quantization Quantizing Mixture-of-Experts (MoE) models introduces unique challenges due to their sparse and dynamic computation patterns. First, activation outliers in MoE layers exhibit expertspecific distributions, as tokens are routed to distinct subsets of experts. Second, the routers expert selection mechanism is highly sensitive to quantization-induced logit perturbations. Even minor deviations in gate scores can disrupt the Top-K expert assignment logic, degrading model performance due to misrouted tokens. Third, expert activation sparsity creates calibration bottlenecks: rarely activated experts receive insufficient data coverage during parameter calibration, resulting in inaccurate estimation of quantization parameters and large quantization errors. To tackle these challenges, we propose novel expert-aware post-training quantization method. Our approach begins with an expert-aware smoothing aggregation strategy designed to suppress activation outliers across MoE experts. By constructing unified channel-wise smoothing vector that aggregates maximum scaling requirements from both expert weights and router logits, we redistribute outlier magnitudes while preserving mathematical equivalence through parameter fusion with preceding normalization layers. For token vector Rd with channels, and an MoE layer with local experts. We achieve this through channel-wise maximization over expert-specific and router-specific requirements: sj = max max i[1,n] (cid:124) (cid:32) max(xj)α max(Wi (cid:123)(cid:122) Expert requirements j)1α (cid:33) , (cid:125) max(xj)α max(Wgate (cid:124) (cid:123)(cid:122) Router requirement )1α (cid:125) , (10) where subscript denotes the j-th input channel, α denotes the migration strength, Wi denotes the first weight matrix of the i-th local expert and Wgate denotes the weight matrix of the router layer. 11 To ensure consistent expert selection post-quantization, we introduce router logits distribution alignment through dual-objective calibration process that minimizes both logit reconstruction error and KullbackLeibler divergence between full-precision and quantized routing probabilities. This guarantees stable Top-K expert activation despite quantization-induced perturbations. Finally, we resolve expert-level activation sparsity through expert-level calibration data balance, where underutilized experts receive prioritized sampling from augmented datasets until their activation counts meet parity with computationally derived expectations. KV Cache Quantization and Sparsity KV cache compression [41, 10, 25, 22, 45] is essential to optimize inference infrastructure efficiency, particularly for throughput, context length, and batch size scalability. Quantization and sparsity techniques can stably mitigate KV preemption while enhancing inference efficiency and overall user experience. The KVTuner algorithm [22] enables an optimized balance between inference efficiency and model accuracy through hardware-friendly mixed-precision quantization. This Ascend-affinitive framework leverages offline profiling and multi-objective optimization to derive Pareto-optimal layer-wise quantization configurations for coarse-grained KV cache segments. KVTuners adaptability ensures effective KV cache compression in MoGE architectures by addressing layer-wise sensitivity and dynamic token-expert interactions. 4.2.3 Kernel Fusion MulAttention With increasing concurrency levels and iterative expansion of sequence lengths, the key-value (KV) cache exhibits linear growth in memory footprint, causing the latency of attention operations to rise to 30%-50% of total inference time. Consequently, the attention module emerges as key bottleneck in MoE inference. Profiling results reveal that KV vector data transfer accounts for approximately 70% of attention computation time, followed by vector operations and matrix multiplications. Therefore, optimizing both memory access bottlenecks and vector computation constraints within operator pipelines has become critical challenge in improving the efficiency of attention mechanisms. Figure 5: Computation flow of the MulAttention operator. large-packet KV transfer strategy is adopted to improve memory bandwidth utilization, as indicated by steps (1)(5). Furthermore, dual-loop pipeline with pingpong scheduler is introduced for KV processing to enhance MTE2 utilization, as indicated by steps (2)(3)(4) and (6)(7)(8). To address this challenge, we propose MulAttention, fused attention operator optimized for Ascend hardware, specifically designed for the decoding stage in LLM inference. Specifically, MulAttention improves memory bandwidth utilization through large-packet KV transfer strategy, as illustrated in steps (1) and (5) in Figure 5 . Leveraging the MTE2 transfer unit, KV vectors are block-loaded from global memory (GM) into the Unified Buffer (UB) of the vector computation unit, where NZ layout transpose is performed simultaneously. Furthermore, we propose dual-loop pipeline with pingpong scheduler for KV processing. By decoupling operations with distinct compute patterns, specifically Cube and Vector operations, into separate loops, we eliminate pipeline bubbles caused by interleaved execution of Key, softmax, and Value computations, 12 Figure 6: Overview of SwiftGMM. (a) tiling cache strategy leverages historical profiling to predict optimal tiling parameters under dynamic workloads. (b) dual-buffer mechanism overlaps data transfer and computation to maximize MTE2 utilization on the Ascend 300I Duo. as illustrated in steps (2) to (4) and (6) to (8) in Figure 5. In addition, the use of ping-pong buffering mechanism allows for the overlapping of KV data prefetching and computation. This overlap effectively hides memory latency and increases the utilization of the MTE2 pipeline to over 89%. Through these optimizations, MulAttention achieves 4.5 end-to-end attention speedup, and significantly enhances hardware utilization. SwiftGMM In high-concurrency scenarios, the GroupMatmul (GMM) operator accounts for over 50% of end-to-end latency, with dynamic workloads further exacerbating challenges in maintaining computational efficiency. Therefore, we propose SwiftGMM, GMM acceleration technique optimized for the Ascend platform. SwiftGMM introduces tiling cache strategy tailored to dynamic workloads by leveraging historical profiling data to predict optimal tiling parameters, as illustrated in Figure 6 (a), thereby reducing the overhead of frequent recalculations caused by load imbalances. It also dynamically selects between GEMV and GEMM execution modes based on workload intensity to maximize computational throughput. SwiftGMM further capitalizes on the large L1 cache of the Ascend 300I Duo NPU to load entire matrices in single pass, substantially minimizing redundant memory transfers. dual-buffer mechanism is implemented to overlap data movement with computation, thereby enhancing MTE2 pipeline utilization, as shown in Figure 6 (b). Experimental evaluations show that SwiftGMM achieves up to 95% MTE2 utilization, bringing operator performance close to the theoretical upper bound constrained by weight data transfer bandwidth. 4.2.4 Analysis of Prefill and Decode Stage During the computationally intensive Prefill stage, only the Top-8 experts are activated per token for the MoE architecture, which effectively reduces the model size to an equivalent 16B dense model. This sparse activation mechanism significantly reduces computational cost and communication overhead. Moreover, the adoption of minimal-card deployment strategy can further enhance computational efficiency in Prefill stage. Compared to popular dense models with sizes of 32B and 72B, Pangu Pro MoE achieves much lower latency and higher input throughput under the same hardware conditions. In the decode stage dominated by memory-intensive operations, Pangu Pro MoE maintains low latency within several tens of milliseconds for small batch sizes such as single batch. For large batch sizes such as 64, the model leverages dimensional compression and depth reduction synergistically with its sparse expert activation paradigm. These features efficiently minimize KV cache memory footprint and inter-node communication overhead, also mitigating computational bottlenecks. As result, Pangu Pro MoE exhibits significantly higher output throughput within an latency such as 100 ms, compared to dense models of similar scale. Detailed experiments on the performance of model inference are presented in Section 5.3."
        },
        {
            "title": "5 Experiments and Results",
            "content": "In this section, we first present the performance of Pangu Pro MoE across comprehensive benchmarks, followed by comparative analysis with state-of-the-art models. Then, we analyzed and compared the inference efficiency of Pangu Pro MoE with dense models. Finally, we investigate the models expert characteristics through detailed analysis of expert activation patterns during inference to better understand the proposed method. 5.1 Pre-Trained Model Following pre-training on 13T high-quality text tokens, Pangu Pro MoE achieves significant improvements in linguistic comprehension and reasoning capabilities. To systematically evaluate the models performance across multiple task dimensions, we conduct comprehensive evaluation covering three core areas: Chinese language processing, English language understanding, and complex reasoning tasks. 5.1.1 Evalutaion Benchmarks comprehensive evaluation suite was constructed to assess model capabilities across English, coding, mathematics, and Chinese. Established benchmarks were selected to reflect diverse cognitive skills, with detailed metrics provided in Table 3. English: General Reasoning: Big-Bench-Hard [33], Massive Multitask Language Understanding(MMLU) [12] (multi-subject QA), MMLU-Pro [38] (complex multi-step reasoning). Reading Comprehension: DROP [9] (discrete reasoning over paragraphs), RACE-M/H [17] (middle/high school exams). Commonsense Reasoning: HellaSwag [48] (contextual completion), PIQA [3] (physical commonsense), WinoGrande [29] (large-scale Winograd schema). Chinese: General Knowledge: C-Eval [14] (academic QA), CMMLU [19] (linguistic understanding). Reading Comprehension: CMRC [7] (span-extraction RC), C3 [32] (multiple-choice RC). Cultural & Contextual: CCPM [21] (classical poetry matching), CLUEWSC [42] (Chinese-language Winograd schema resolution). Reasoning: Complex reasoning: HumanEval [5] (Python function synthesis). GSM8K [6] (grade school math problem solving), MATH [13] (challenging competition mathematics), MGSM [31] (multilingual mathematical reasoning), CMath [40] (Chinese mathematical problem solving). 5.1.2 Evalutaion Results As demonstrated in Table 3, Pangu Pro MoE emerges as one of the most competitive architectures across multiple evaluation dimensions. The model establishes state-of-the-art performance in critical English-language benchmarks including MMLU and HellaSwag, while simultaneously dominating most Chinese-language evaluations (C-Eval, C3, and CCPM). Its mathematical reasoning capabilities, as quantified by the GSM8K benchmark, further confirm the architectures cross-domain competence. Notably, these achievements are attained through computationally efficient MoE design. When benchmarked against contemporary base models including Qwen3-32B-base [43], GLM4-32B-base [11], Gemma3-27B-base [35], and Llama-4-Scoutbaset [1], Pangu Pro MoE demonstrates consistent performance advantages. 5.2 Instruct Model Benefiting from efficient SFT and RL training, Pangu Pro MoE has developed robust instruction-following capabilities and complex reasoning skills. To comprehensively evaluate these capabilities, we conducted systematic assessments on series of challenging tasks. 14 Table 3: Comparison between Pangu Pro MoE and other representative base models across diverse set of benchmarks for evaluating language and reasoning skills. Bold values represent the best results in each line. Benchmark (Metric) # Shots Qwen2.5 32B Base GLM4 Gemma Llama-4 32B Base 27B Base Scout Base Pangu Pro MoE Base Architecture # Activated Params # Total Params BBH (EM) MMLU (EM) MMLU-Pro (EM) DROP (F1) HellaSwag (EM) PIQA (EM) WinoGrande (EM) RACE-Middle (EM) RACE-High (EM) CLUEWSC (EM) C-Eval (EM) CMMLU (EM) CMRC (EM) C3 (EM) CCPM (EM) HumanEval (Pass@1) MATH (EM) CMath (EM) GSM8K (EM) English Chinese Reasoning - - - Dense 32B 32B Dense 32B 32B Dense 27B 27B MoE 17B 109B 3-shot 5-shot 5-shot 3-shot 10-shot 0-shot 5-shot 5-shot 5-shot 5-shot 5-shot 5-shot 1-shot 0-shot 0-shot 0-shot 4-shot 3-shot 8-shot 82.4 84.2 58.0 79.2 93.1 90.6 80.7 95.3 93.0 88.3 87.7 88.9 66.2 96.3 86.6 57.9 57.9 83.5 83.0 83.3 82.0 55.8 86.2 92.6 91.8 87.1 94.0 92.4 85.7 84.1 83.8 76.3 94.5 87. 59.1 46.7 77.5 85.4 77.7 78.6 50.3 77.2 84.1 83.3 71.9 93.8 90.5 82.0 69.4 70.4 70.8 62.8 69.9 48.8 50.0 76.8 82.6 79.5 78.3 50.3 77.5 81.9 88.9 71.4 92.3 88.9 83.8 74.8 76.8 67.4 94.4 81. 54.6 52.7 83.7 79.2 MoE 16B 72B 81.2 87.4 63.5 81.1 93.5 89.3 75.6 95.8 93.9 85.0 90.6 89.0 79.8 95.4 90.3 63.7 55.9 79.7 86.5 5.2.1 Evalutaion Settings Evaluation Benchmarks We evaluate instructed models on multiple benchmarks across three domains. For general-domain English and Chinese evaluation, we test on: MMLU [12], MMLU-Pro [38], MMLU-Redux, DROP [9], IF-Eval [49], Arena-Hard [20], CLUEWSC [42], C-Eval [14], and CMMLU [19]. To assess reasoning capabilities, we employ code datasets: LiveCodeBench [15], and MBPP+ [2]. GPQA-Diamond [28] and SuperGPQA [36] is evaluated for scientific reasoning. Mathematical reasoning is evaluated through: AIME 2024 [26], AIME 2025 [27], MATH-500, and CNMO 20241. Compared Baselines We evaluate our instruct model against state-of-the-art models of comparable scale across multiple architectures. The baseline models include dense models (Qwen3-32B [43], GLM4-Z132B [11], Gemma3-27B [35]), and MoE models (Llama4-Scout [1]). For models with public APIs, we conducted evaluations through their official interfaces using standardized configurations. For open-source models without API access, we deployed local instances and performed consistent evaluations under identical settings. Detailed Evaluation Configurations To comprehensively assess the performance of the post-training model across diverse datasets, we have adopted standardized and unified evaluation framework. Specifically, for the LiveCodeBenc, MBPP+, and IF-EVAL datasets, we utilized the evaluation methods officially provided by their respective websites. For the ArenaHard dataset, we employed referee model to conduct scoring, ensuring fair and objective assessment. For the remaining datasets, we adopted matching and exact matching techniques to evaluate the models performance. For LiveCodeBench, we use versions 24.8.1-25.1.1, which cover the data and problem sets between this time period. Consistent with the original evaluation protocols, we followed the default prompts provided by the dataset creators for all datasets. To ensure the models capability to handle extensive input and output, we set the maximum input length to 4k tokens and the maximum output length to 28k tokens. This configuration allows us to thoroughly assess the post-training models ability to process and generate full responses within the specified constraints. 15 Table 4: Comparison between Pangu Pro MoE and other representative models across diverse set of benchmarks for evaluating language and reasoning skills. Bold values represent the best results in each line. Benchmark (Metric) Qwen3-32B GLM-Z1-32B Gemma3-27B Llama4-Scout Pangu Pro MoE Architecture # Activated Params # Total Params MMLU (EM) MMLU-Pro (EM) MMLU-Redux (EM) DROP (F1) IF-Eval (Prompt Strict) Arena-Hard CLUEWSC (EM) C-Eval (EM) CMMLU (EM) LiveCodeBench (Pass@1) MBPP+ (Pass@1) GPQA-Diamond (Pass@1) SuperGPQA AIME2024 (Pass@1) AIME2025 (Pass@1) MATH-500 (EM) CNMO2024 (Pass@1) English Chinese Reasoning 5.2.2 Evaluation Results Dense 32B 32B Dense 32B 32B Dense 27B 27B 89.2 78.6 83.2 91.3 84.3 94.7 94.6 89.2 84.6 62.6 82.0 68.2 49.8 80.4 70.9 96.6 70.4 - - 88.2 - 84.5 90.6 93.6 82.7 - 59.1 75.7 66.1 52.6 80.8 63.6 94.4 71. 77.6 67.5 81.7 88.4 83.4 89.8 91.3 65.2 65.9 29.7 73.8 42.4 35.6 26.9 22.6 87.8 40.7 MoE 17B 109B 79.4 73.8 84.8 89.3 85.2 68.9 87.6 77.3 70. 33.1 73.5 53.5 45.0 29.0 10.2 82.4 20.4 MoE 16B 72B 89.3 82.6 81.5 91.2 85.7 93.6 94.7 91.1 87.1 59.6 80.2 73.7 54.8 79.2 68.1 96.8 70.8 English Benchmarks As shown in Table 4, Pangu Pro MoE demonstrates exceptional performance in English reasoning tasks. On the MMLU-PRO benchmark, which extends MMLU with greater scale and difficulty to rigorously evaluate LLM capabilities. Pangu Pro MoE significantly outperforms mainstream dense models (including Qwen3-32B, GLM-Z1-32B, and Gemma3-27B) and the MoE-based Llama4-Scout, achieving state-of-the-art results. Notably, Pangu Pro MoE achieves competitive score of 91.2 on the DROP reading comprehension task, nearly matching Qwen3-32Bs 91.3 score. This demonstrates its semantic understanding in complex English contexts reaches leading levels. Chinese Benchmarks As shown in Table 4, Pangu Pro MoE maintains comparable expertise in Chinese evaluations. It scores 91.1 on C-Eval (EM), surpassing Qwen3-32B(89.2). For Chinese commonsense reasoning, Pangu Pro MoE achieves 94.7 on CLUEWSC (EM), outperforming Gemma3-27B (91.3) by 3.4 points while matching Qwen3-32B (94.6). These results validate the models strong performance in Chinese semantic understanding and commonsense reasoning. Reasoning Benchmarks As shown in Table 4, Pangu Pro MoE demonstrates superior logical reasoning capabilities with efficient computation. For code generation, it achieves 80.2 on MBPP+, comparable to Qwen3-32Bs 82.0. In mathematical reasoning, Pangu Pro MoE scores 96.8 on MATH-500 (EM), surpassing Qwen3-32B (96.6), and achieves 70.8 on CNMO2024 versus 70.4 for Qwen3-32B. Notably, Pangu Pro MoE obtains 54.8 on SuperGPQA, significantly outperforming dense models like GLM-Z1-32B (52.6). Remarkably, Pangu Pro MoE matches 32B-scale state-of-the-art models reasoning capabilities using only 16B activated parameters. This efficiency stems from the innovative MoGE architecture, which enhances inference speed while maintaining reasoning accuracy. 5.3 Inference Efficiency Performance on Ascend 800I A2 Pangu Pro MoE, configured as 72BA16B MoE, exhibits remarkable inference efficiency under the W8A8 quantization on Ascend 800I A2. In the prefill stage, with batch size of 2 and sequence length of 2k, the model achieves an average input throughput of 4828 tokens/s per card, attaining the lowest prefill latency. Compared to 72B Dense and 32B Dense, this corresponds to performance improvements of 203% and 42%, respectively, as shown in Table 5. The computational efficiency derives principally from the models optimized parameter activation pattern, where per-token operational parameters 1https://www.cms.org.cn/Home/comp/comp/cid/12.html 16 Table 5: Inference performance comparison during the Prefill stage on the Ascend 800I A2 NPU. The input sequence length is 2048 with batch size of 2. TTFT (Time To First Token) measures the forward latency to generate the first token. Throughput is calculated per card."
        },
        {
            "title": "Model",
            "content": "TTFT (ms) Input Throughput (tokens/s) 72BA16B MoE 32B Dense 72B Dense 424.21 604.07 1282.94 4828 3390 1596 Table 6: Inference performance comparison during the Decode stage on the Ascend 800I A2 NPU. The input sequence length is 2048 tokens. TPOT (Time Per Output Token) represents the forward latency for generating each output token. Throughput is calculated per card. represents the acceleration with MTP module and related optimization at high acceptance rate."
        },
        {
            "title": "Batch size",
            "content": "TPOT (ms) Output Throughput (tokens/s) 72BA16B MoE 32B Dense 72B Dense 1 456 1 336 1 228 18.44 99.31 95.56 16.26 86.34 26.88 97.74 14 1148 1528 15 973 9 583 constitute merely 22% and 50% of those required by the dense model, thereby substantially mitigating computational demands. In the Decode stage, four accelerators are deployed for the model. For low concurrency scenarios such as batch size of 1 and the sequence length is 2k, 72BA16B MoE achieves low latency with weight transfer volume of approximately 16B. For high concurrency scenarios with hundreds of batch size, meeting typical 100 ms latency constraint, Pangu Pro MoE achieves an average output throughput of 1148 tokens/s per card, outperforming 72B Dense and 32B Dense by 97% and 18%, as shown in Table 6. Furthermore, the models output throughput can be increased to 1528 tokens/s per card when incorporating multi-token prediction (MTP) decoding and related optimization. Profiling results indicate that weight transfer accounts for only 29% of total latency, with the remaining time primarily spent on KV caching, computation and communication. Pangu Pro MoE adopts smaller hidden dimension of 5120 and fewer layers of 48, leading to reductions in KV cache size and communication volume by approximately 40%/25% and 63%/25% compared to 72B Dense and 32B Dense, respectively. These structural optimizations fully exploit the computational and memory access advantages brought about by sparse activation and lightweight architecture, significantly enhancing overall inference throughput. Performance on Ascend 300I Duo Through deep integration and optimization of 72BA16B MoE with the Ascend platform, the Ascend 300I Duo inference accelerator enables efficient and cost-effective inference for billion-scale MoE models. Pangu Pro MoE is quantized under the W8A8 configuration during inference. In the Prefill stage, by employing two Ascend 300I Duo accelerators with batch size of 2, 72BA16B MoE achieves 1.94s latency for 2k-length input sequences, with an input throughput of 1055 tokens/s per card. In the Decode stage, using the aforementioned H2P deployment on four Ascend 300I Duo accelerators, the system achieves approximately 50 ms latency in low-concurrency scenarios with single batch and sequence length of 2k, enabling low-latency inference. In high-concurrency settings with batch sizes of 80, it sustains per-card throughput of 201 tokens/s with 99.5 ms latency, meeting high-throughput demands, as shown in Table 7. With the acceleration of MTP decoding and related optimization, the models output throughput can be increased to 321 tokens/s. Through the co-design of the Pangu model and the Ascend platform, we achieve an excellent cost-to-performance ratio for sub-100B model inference on the Ascend 300I Duo. 17 Table 7: Inference performance comparison during the Prefill and Decode stage on the Ascend 300I Duo NPU. The input sequence length is 2048. Throughput is calculated per card. represents the acceleration with MTP module and related optimization at high acceptance rate."
        },
        {
            "title": "Batch size",
            "content": "Latency (ms) Throughput (tokens/s) 72BA16B MoE"
        },
        {
            "title": "Prefill\nDecode\nDecode",
            "content": "2 80 128 1940.3 99.5 99.7 1055 201 321 Table 8: Quantization accuracy loss under different quantization bit-widths. Method Accuracy loss MATH-"
        },
        {
            "title": "Live CodeBench Arena Hard GPQA Diamond",
            "content": "W8A8 W4A8 0.80 1.20 0.37 3.68 0.60 3.40 0.51 2.53 Performance of Quantization We performed extensive evaluations of quantization performance across wide range of benchmarks. As shown in Tables 8, our approach achieves near-lossless accuracy with W8A8 quantization configurations. Even when quantized with W4A8, the loss of accuracy remains within an acceptable range. 5.4 Experimental Analysis In order to better understand the effectiveness of the MoGE architecture in Pangu Pro MoE , we systematically examine key characteristics intrinsic to MoE models, such as domain specialization, the dynamics of expert co-activation, intra-group expert distribution and global expert distribution. Domain Specialization The pattern of expert specialization serves as key indicator of the effectiveness of an MoE layer, as it reflects the extent to which experts have successfully learned and internalized knowledge from the data. In this section, we examine the phenomenon of expert specialization across range of tasks to understand how this pattern varies under different conditions. Our analysis is based on four diverse datasetsC-Eval, MMLU, GSM8K, and HumanEvalwhich respectively correspond to Chinese language proficiency, English language proficiency, and advanced reasoning abilities in mathematics and programming. As shown in Figure 7, we analyze the token-to-expert assignment at three representative layersshallow, middle, and deep (i.e., Layers 0, 23, and 47). Across different tasks, tokens at the same layer are preferentially routed to different experts, leading to substantial variability in expert specialization. In the shallow layers (Layer 0), the experts exhibit highly uniform activation pattern. In contrast, experts in deeper layers demonstrate increasing specialization: those in Layer 47 display higher degree of specialization than those in Layer 23, which in turn surpass those in Layer 0. This progressive trend suggests that expert specialization intensifies with network depth. Furthermore, for tasks that primarily assess general language understandingsuch as C-Eval, and MMLUthe distribution of expert activations tends to be more balanced across the expert set. In contrast, for reasoning-intensive tasks such as GSM8K and HumanEval, expert activations exhibit higher degree of specialization, indicating more selective and task-specific routing behavior. Our analysis of expert specialization reveals that Pangu Pro MoE has developed substantial task-specific differentiation among experts, which enhances the models representational capacity and contributes to its overall performance. Expert Co-Activation To analyze expert interaction behavior, we visualize expert co-activation patterns using co-activation matrix, where each entry represents the empirical probability that pair of experts are simultaneously activated for the same input token. Higher co-activation scores indicate stronger correlations in routing decisions, thereby reflecting higher degree of collaborative behavior among experts. To ensure comprehensive coverage across the network, we select three representative layers from different depthsthree each from the shallow, middle, and deep stages of the model. As illustrated in Figure 8, blank regions along the diagonal of the co-activation matrix indicate the absence of co-activation among experts 18 Figure 7: Expert specialization in Pangu Pro MoE . Each subplot illustrates the token-to-expert distribution within specific layer for given task. Bars represent the proportion of tokens routed to individual experts, normalized by the total number of tokens. Pangu Pro MoE employs 64 experts per layer, with 8 experts activated for each token, yielding an expected uniform distribution of 12.5% per expertindicated by the gray dashed line. The observed distributions, however, deviate substantially from this uniform baseline, highlighting pronounced degree of expert specialization. This specialization is indicative of effective expert differentiation and contributes to enhanced model training and overall performance. Figure 8: Expert co-activation across three layers (shallow, middle, and deep), evaluated on random 0.5% subset of the C4 validation set. The first four expert groups, each containing four experts, are displayed with their expert IDs on the xand y-axes. Color intensity reflects the co-activation scores between expert pairs. within the same group. This sparsity arises directly from our group-wise routing strategy, which enforces mutual exclusivity in expert selection at the group level, thereby promoting modularization and reducing potential overlap in learned representations. Additionally, the co-activation scores between experts from different groups remain consistently low across layers, suggesting that inter-group interactions are minimal. This observation supports the hypothesis that our model achieves low degree of expert redundancy and encourages specialization, where different experts are responsible for distinct aspects of representation learning. 19 Figure 9: Intra-group expert distribution in Pangu Pro MoE . the observed token distributions closely align with this theoretical baseline, demonstrating that Pangu Pro MoE effectively maintains balanced utilization across experts within each group. Such balanced activation helps prevent expert underuse or oversaturation, thereby promoting stable training dynamics and maximizing the collective capacity of the expert pool Figure 10: Global expert distribution on the first MoE layer, evaluated using random 0.5% subset of the C4 validation set. Pangu Pro MoE exhibits more balanced expert utilization, with token proportions closer to the ideal 12.5% per expert. Interestingly, we observe non-uniform trend across layers: co-activation scores are slightly elevated in the shallow and deep layers relative to those in the middle layers. One possible explanation is that the model benefits from broader expert collaboration during early-stage feature extractionwhere general-purpose patterns are learnedand during late-stage integrationwhere diverse signals must be combined for complex task-specific predictions. In contrast, the middle layers may prioritize more fine-grained, isolated processing, leading to greater specialization and reduced inter-expert dependency. Intra-Group Expert Distribution The design of MoGE enforces the activation of exactly the same experts per group, which inherently promotes balanced expert utilization across groups. To further examine whether this balance also holds within groups, we conduct detailed analysis of intra-group expert distribution. As illustrated in Figure 9, we visualize the expert activation frequency among experts in the first four groups across three representative layers (shallow, middle, and deep). Overall, the distribution of tokens among intra-group experts appears approximately uniform, with each expert receiving close to 12.5% of the tokensconsistent with the theoretical average under top-1 activation in group of 8 experts. This observation 20 further supports the claim that the MoGE architecture facilitates not only inter-group but also intra-group load balancing, making it inherently friendly to balanced expert utilization. Notably, we observe slight deviations from perfect uniformity in the deeper layers, where token allocation becomes marginally more skewed. This trend is consistent with the increasing specialization observed in expert routing at greater model depths (Figure 7), suggesting that deeper layers may adaptively modulate expert usage to capture more task-specific or abstract representations. Global Expert Distribution The balance of expert load in MoE architectures remains critical topic, as more uniform expert activation is generally associated with improved resource efficiency and more stable model behavior. To analyze this aspect, we conduct comparative analysis between DeepSeek-V2 and Pangu Pro MoE. As illustrated in the Figure 10, DeepSeek-V2 exhibits noticeable imbalance, with the most heavily loaded expert processing up to 30% of the total tokens. In contrast, Pangu Pro MoE demonstrates nearly uniform distribution across experts, with each expert handling approximately 12.5% of the tokensclosely aligning with the theoretical ideal. This balanced activation pattern in Pangu Pro MoE reflects more effective use of the expert capacity and may contribute to enhanced training stability and generalization. The comparison highlights the importance of load balancing in achieving efficient and scalable performance in large-scale MoE models."
        },
        {
            "title": "6 Conclusion",
            "content": "We propose Pangu Pro MoE, 72B sparse LLM based on the proposed Mixture of Grouped Experts (MoGE) architecture, designed to inherently balance computational workloads across distributed Ascend NPUs. By grouping experts and enforcing balanced token-to-expert assignments, MoGE eliminates device load imbalance issues inherent in conventional MoE, enabling efficient parallel execution during training and inference. Optimized for Ascend 300I Duo and 800I A2 through systematic hardware-aligned co-design, Pangu Pro MoE activates 16B parameters per token while achieving superior throughput. Extensive experiments validate that Pangu Pro MoE outperforms leading open-source models such as GLM-Z1-32B and Qwen3-32B, establishing its state-of-the-art capabilities. Our work demonstrates the effectiveness of co-designing sparse architectures with Ascend NPUs for scalable, high-throughput LLM deployment."
        },
        {
            "title": "References",
            "content": "[1] Meta AI. The llama 4 herd: The beginning of new era of natively multimodal ai innovation. https: //ai.meta.com/blog/llama-4-multimodal-intelligence/, 2025. Accessed: 2025-04-05. [2] Jacob Austin, Augustus Odena, Maxwell Nye, Maarten Bosma, Henryk Michalewski, David Dohan, Ellen Jiang, Carrie J. Cai, Michael Terry, Quoc V. Le, and Charles Sutton. Program synthesis with large language models. ArXiv, abs/2108.07732, 2021. [3] Yonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. Piqa: Reasoning about physical commonsense in natural language. In AAAI Conference on Artificial Intelligence, 2019. [4] Weilin Cai, Juyong Jiang, Fan Wang, Jing Tang, Sunghun Kim, and Jiayi Huang. survey on mixture of experts. arXiv preprint arXiv:2407.06204, 2024. [5] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Pondé, Jared Kaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, Alex Ray, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sastry, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov, Alethea Power, Lukasz Kaiser, Mo Bavarian, Clemens Winter, Philippe Tillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, Fotios Chantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol, Igor Babuschkin, Suchir Balaji, Shantanu Jain, Andrew Carr, Jan Leike, Joshua Achiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight, Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, Dario Amodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. Evaluating large language models trained on code. ArXiv, abs/2107.03374, 2021. [6] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [7] Yiming Cui, Ting Liu, Wanxiang Che, Li Xiao, Zhipeng Chen, Wentao Ma, Shijin Wang, and Guoping Hu. span-extraction dataset for Chinese machine reading comprehension. In Kentaro Inui, Jing Jiang, Vincent Ng, and Xiaojun Wan, editors, Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), pages 58835889, Hong Kong, China, November 2019. Association for Computational Linguistics. [8] Nan Du, Yanping Huang, Andrew Dai, Simon Tong, Dmitry Lepikhin, Yuanzhong Xu, Maxim Krikun, Yanqi Zhou, Adams Wei Yu, Orhan Firat, et al. Glam: Efficient scaling of language models with mixture-of-experts. In International Conference on Machine Learning, pages 55475569. PMLR, 2022. [9] Dheeru Dua, Yizhong Wang, Pradeep Dasigi, Gabriel Stanovsky, Sameer Singh, and Matt Gardner. Drop: reading comprehension benchmark requiring discrete reasoning over paragraphs. In North American Chapter of the Association for Computational Linguistics, 2019. [10] Suyu Ge, Yunan Zhang, Liyuan Liu, Minjia Zhang, Jiawei Han, and Jianfeng Gao. Model tells you what to discard: Adaptive kv cache compression for llms. arXiv preprint arXiv:2310.01801, 2023. [11] Team GLM, Aohan Zeng, Bin Xu, Bowen Wang, Chenhui Zhang, Da Yin, Diego Rojas, Guanyu Feng, Hanlin Zhao, Hanyu Lai, Hao Yu, Hongning Wang, Jiadai Sun, Jiajie Zhang, Jiale Cheng, Jiayi Gui, Jie Tang, Jing Zhang, Juanzi Li, Lei Zhao, Lindong Wu, Lucen Zhong, Mingdao Liu, Minlie Huang, Peng Zhang, Qinkai Zheng, Rui Lu, Shuaiqi Duan, Shudan Zhang, Shulin Cao, Shuxun Yang, Weng Lam Tam, Wenyi Zhao, Xiao Liu, Xiao Xia, Xiaohan Zhang, Xiaotao Gu, Xin Lv, Xinghan Liu, Xinyi Liu, Xinyue Yang, Xixuan Song, Xunkai Zhang, Yifan An, Yifan Xu, Yilin Niu, Yuantao Yang, Yueyan Li, Yushi Bai, Yuxiao Dong, Zehan Qi, Zhaoyu Wang, Zhen Yang, Zhengxiao Du, Zhenyu Hou, and Zihan Wang. Chatglm: family of large language models from glm-130b to glm-4 all tools, 2024. [12] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020. [13] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the math dataset. arXiv preprint arXiv:2103.03874, 2021. [14] Yuzhen Huang, Yuzhuo Bai, Zhihao Zhu, Junlei Zhang, Jinghan Zhang, Tangjun Su, Junteng Liu, Chuancheng Lv, Yikai Zhang, Jiayi Lei, Fanchao Qi, Yao Fu, Maosong Sun, and Junxian He. C-eval: multi-level multi-discipline chinese evaluation suite for foundation models. ArXiv, abs/2305.08322, 2023. 22 [15] Naman Jain, King Han, Alex Gu, Wen-Ding Li, Fanjia Yan, Tianjun Zhang, Sida Wang, Armando Solar-Lezama, Koushik Sen, and Ion Stoica. Livecodebench: Holistic and contamination free evaluation of large language models for code. arXiv preprint arXiv:2403.07974, 2024. [16] Albert Jiang, Alexandre Sablayrolles, Antoine Roux, Arthur Mensch, Blanche Savary, Chris Bamford, Devendra Singh Chaplot, Diego de las Casas, Emma Bou Hanna, Florian Bressand, et al. Mixtral of experts. arXiv preprint arXiv:2401.04088, 2024. [17] Guokun Lai, Qizhe Xie, Hanxiao Liu, Yiming Yang, and Eduard H. Hovy. Race: Large-scale reading comprehension dataset from examinations. ArXiv, abs/1704.04683, 2017. [18] Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang, Maxim Krikun, Noam Shazeer, and Zhifeng Chen. {GS}hard: Scaling giant models with conditional computation and automatic sharding. In International Conference on Learning Representations, 2021. [19] Haonan Li, Yixuan Zhang, Fajri Koto, Yifei Yang, Hai Zhao, Yeyun Gong, Nan Duan, and Timothy Baldwin. Cmmlu: Measuring massive multitask language understanding in chinese. arXiv preprint arXiv:2306.09212, 2023. [20] Tianle Li, Wei-Lin Chiang, Evan Frick, Lisa Dunlap, Banghua Zhu, Joseph E. Gonzalez, and Ion Stoica. From live data to high-quality benchmarks: The arena-hard pipeline, April 2024. [21] Wenhao Li, Fanchao Qi, Maosong Sun, Xiaoyuan Yi, and Jiarui Zhang. Ccpm: chinese classical poetry matching dataset, 2021. [22] Xing Li, Zeyu Xing, Yiming Li, Linping Qu, Hui-Ling Zhen, Wulong Liu, Yiwu Yao, Sinno Jialin Pan, and Mingxuan Yuan. Kvtuner: Sensitivity-aware layer-wise mixed precision kv cache quantization for efficient and nearly lossless llm inference, 2025. [23] Heng Liao, Jiajin Tu, Jing Xia, and Xiping Zhou. Davinci: scalable architecture for neural network computing. In 2019 IEEE Hot Chips 31 Symposium (HCS), pages 144. IEEE Computer Society, 2019. [24] Aixin Liu, Bei Feng, Bin Wang, Bingxuan Wang, Bo Liu, Chenggang Zhao, Chengqi Dengr, Chong Ruan, Damai Dai, Daya Guo, et al. Deepseek-v2: strong, economical, and efficient mixture-of-experts language model. arXiv preprint arXiv:2405.04434, 2024. [25] Zirui Liu, Jiayi Yuan, Hongye Jin, Shaochen Zhong, Zhaozhuo Xu, Vladimir Braverman, Beidi Chen, and Xia Hu. Kivi: tuning-free asymmetric 2bit quantization for kv cache. arXiv preprint arXiv:2402.02750, 2024. [26] MAA. Codeforces. American Invitational Mathematics Examination - AIME 2024, 2024. https:// maa.org/math-competitions/american-invitational-mathematics-examination-aime. [27] MAA. Codeforces. American Invitational Mathematics Examination - AIME 2025, 2025. https:// maa.org/math-competitions/american-invitational-mathematics-examination-aime. [28] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. In First Conference on Language Modeling, 2024. [29] Keisuke Sakaguchi, Ronan Le Bras, Chandra Bhagavatula, and Yejin Choi. Winogrande: An adversarial winograd schema challenge at scale, 2019. [30] Noam Shazeer, Azalia Mirhoseini, Krzysztof Maziarz, Andy Davis, Quoc Le, Geoffrey Hinton, and Jeff Dean. Outrageously large neural networks: The sparsely-gated mixture-of-experts layer. arXiv preprint arXiv:1701.06538, 2017. [31] Freda Shi, Mirac Suzgun, Markus Freitag, Xuezhi Wang, Suraj Srivats, Soroush Vosoughi, Hyung Won Chung, Yi Tay, Sebastian Ruder, Denny Zhou, Dipanjan Das, and Jason Wei. Language models are multilingual chain-of-thought reasoners. In The Eleventh International Conference on Learning Representations, ICLR 2023, Kigali, Rwanda, May 1-5, 2023. OpenReview.net, 2023. [32] Kai Sun, Dian Yu, Dong Yu, and Claire Cardie. Investigating prior knowledge for challenging chinese machine reading comprehension, 2019. [33] Mirac Suzgun, Nathan Scales, Nathanael Scharli, Sebastian Gehrmann, Yi Tay, Hyung Won Chung, Aakanksha Chowdhery, Quoc V. Le, Ed H. Chi, Denny Zhou, and Jason Wei. Challenging big-bench tasks and whether chain-of-thought can solve them. In Annual Meeting of the Association for Computational Linguistics, 2022. 23 [34] Yehui Tang, Yichun Yin, Yaoyuan Wang, Hang Zhou, Yu Pan, Wei Guo, Ziyang Zhang, Miao Rang, Fangcheng Liu, Naifu Zhang, Binghan Li, Yonghan Dong, Xiaojun Meng, Yasheng Wang, Dong Li, Yin Li, Dandan Tu, Can Chen, Youliang Yan, Fisher Yu, Ruiming Tang, Yunhe Wang, Botian Huang, Bo Wang, Boxiao Liu, Changzheng Zhang, Da Kuang, Fei Liu, Gang Huang, Jiansheng Wei, Jiarui Qin, Jie Ran, Jinpeng Li, Jun Zhao, Liang Dai, Lin Li, Liqun Deng, Peifeng Qin, Pengyuan Zeng, Qiang Gu, Shaohua Tang, Shengjun Cheng, Tao Gao, Tao Yu, Tianshu Li, Tianyu Bi, Wei He, Weikai Mao, Wenyong Huang, Wulong Liu, Xiabing Li, Xianzhi Yu, Xueyu Wu, Xu He, Yangkai Du, Yan Xu, Ye Tian, Yimeng Wu, Yongbing Huang, Yong Tian, Yong Zhu, Yue Li, Yufei Wang, Yuhang Gai, Yujun Li, Yu Luo, Yunsheng Ni, Yusen Sun, Zelin Chen, Zhe Liu, Zhicheng Liu, Zhipeng Tu, Zilin Ding, and Zongyuan Zhan. Pangu ultra moe: How to train your big moe on ascend npus, 2025. [35] Gemma Team, Aishwarya Kamath, Johan Ferret, Shreya Pathak, Nino Vieillard, Ramona Merhej, Sarah Perrin, Tatiana Matejovicova, Alexandre Ramé, Morgane Rivière, et al. Gemma 3 technical report. arXiv preprint arXiv:2503.19786, 2025. [36] M-A-P Team, Xinrun Du, Yifan Yao, Kaijing Ma, Bingli Wang, Tianyu Zheng, Kang Zhu, Minghao Liu, Yiming Liang, Xiaolong Jin, Zhenlin Wei, Chujie Zheng, Kaixing Deng, Shuyue Guo, Shian Jia, Sichao Jiang, Yiyan Liao, Rui Li, Qinrui Li, Sirun Li, Yizhi Li, Yunwen Li, Dehua Ma, Yuansheng Ni, Haoran Que, Qiyao Wang, Zhoufutu Wen, Siwei Wu, Tianshun Xing, Ming Xu, Zhenzhu Yang, Zekun Moore Wang, Junting Zhou, Yuelin Bai, Xingyuan Bu, Chenglin Cai, Liang Chen, Yifan Chen, Chengtuo Cheng, Tianhao Cheng, Keyi Ding, Siming Huang, Yun Huang, Yaoru Li, Yizhe Li, Zhaoqun Li, Tianhao Liang, Chengdong Lin, Hongquan Lin, Yinghao Ma, Zhongyuan Peng, Zifan Peng, Qige Qi, Shi Qiu, Xingwei Qu, Yizhou Tan, Zili Wang, Chenqing Wang, Hao Wang, Yiya Wang, Yubo Wang, Jiajun Xu, Kexin Yang, Ruibin Yuan, Yuanhao Yue, Tianyang Zhan, Chun Zhang, Jingyang Zhang, Xiyue Zhang, Xingjian Zhang, Yue Zhang, Yongchi Zhao, Xiangyu Zheng, Chenghua Zhong, Yang Gao, Zhoujun Li, Dayiheng Liu, Qian Liu, Tianyu Liu, Shiwen Ni, Junran Peng, Yujia Qin, Wenbo Su, Guoyin Wang, Shi Wang, Jian Yang, Min Yang, Meng Cao, Xiang Yue, Zhaoxiang Zhang, Wangchunshu Zhou, Jiaheng Liu, Qunshu Lin, Wenhao Huang, and Ge Zhang. Supergpqa: Scaling llm evaluation across 285 graduate disciplines, 2025. [37] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023. [38] Yubo Wang, Xueguang Ma, Ge Zhang, Yuansheng Ni, Abhranil Chandra, Shiguang Guo, Weiming Ren, Aaran Arulraj, Xuan He, Ziyan Jiang, et al. Mmlu-pro: more robust and challenging multi-task language understanding benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. [39] Yunhe Wang, Hanting Chen, Yehui Tang, Tianyu Guo, Kai Han, Ying Nie, Xutao Wang, Hailin Hu, Zheyuan Bai, Yun Wang, et al. Pangu-π: Enhancing language model architectures via nonlinearity compensation. arXiv preprint arXiv:2312.17276, 2023. [40] Tianwen Wei, Jian Luan, Wei Liu, Shuang Dong, and Bin Wang. Cmath: Can your language model pass chinese elementary school math test?, 2023. [41] Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023. [42] Liang Xu, Hai Hu, Xuanwei Zhang, Lu Li, Chenjie Cao, Yudong Li, Yechen Xu, Kai Sun, Dian Yu, Cong Yu, et al. Clue: chinese language understanding evaluation benchmark. arXiv preprint arXiv:2004.05986, 2020. [43] An Yang, Anfeng Li, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Gao, Chengen Huang, Chenxu Lv, et al. Qwen3 technical report. arXiv preprint arXiv:2505.09388, 2025. [44] An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, et al. Qwen2. 5 technical report. arXiv preprint arXiv:2412.15115, 2024. [45] Qingyue Yang, Jie Wang, Xing Li, Zhihai Wang, Chen Chen, Lei Chen, Xianzhi Yu, Wulong Liu, Jianye Hao, Mingxuan Yuan, et al. Attentionpredictor: Temporal pattern matters for efficient llm inference. arXiv preprint arXiv:2502.04077, 2025. [46] Mingjia Yin, Chuhan Wu, Yufei Wang, Hao Wang, Wei Guo, Yasheng Wang, Yong Liu, Ruiming Tang, Defu Lian, and Enhong Chen. Entropy law: The story behind data compression and llm performance. arXiv preprint arXiv:2407.06645, 2024. 24 [47] Yichun Yin, Wenyong Huang, Kaikai Song, Yehui Tang, Xue-Fei Wu, Wei Guo, Peng Guo, Yaoyuan Wang, Xiaojun Meng, Yasheng Wang, Dong Li, Can Chen, Dandan Tu, Yin Li, Fisher Yu, Ruiming Tang, Yunhe Wang, Baojun Wang, Bin Wang, Bo Wang, Boxiao Liu, Changzheng Zhang, Duyu Tang, Fei Mi, Hui Jin, Jiansheng Wei, Jiarui Qin, Jinpeng Li, Jun Zhao, Liqun Deng, Lin Li, Minghui Xu, Naifu Zhang, Nianzu Zheng, Qiang Li, Rongju Ruan, Shengjun Cheng, Tianyu Guo, Wei He, Wei Li, Wei-Wei Liu, Wu-Peng Liu, Xinyi Dai, Yong Dong, Yu Pan, Yue Li, Yufei Wang, Yujun Li, Yunsheng Ni, Zhe Liu, Zhenhe Zhang, and Zhicheng Liu. Pangu ultra: Pushing the limits of dense large language models on ascend npus. 2025. [48] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can machine really finish your sentence? In Annual Meeting of the Association for Computational Linguistics, 2019. [49] Jeffrey Zhou, Tianjian Lu, Swaroop Mishra, Siddhartha Brahma, Sujoy Basu, Yi Luan, Denny Zhou, and Le Hou. Instruction-following evaluation for large language models. arXiv preprint arXiv:2311.07911, 2023."
        },
        {
            "title": "A Contributions and Acknowledgments",
            "content": "Core Contributors Yehui Tang, Xiaosong Li, Fangcheng Liu, Wei Guo, Hang Zhou, Yaoyuan Wang, Kai Han, Xianzhi Yu, Jinpeng Li, Hui Zang, Fei Mi, Xiaojun Meng, Zhicheng Liu, Hanting Chen, Binfan Zheng, Can Chen, Youliang Yan, Ruiming Tang, Peifeng Qin, Xinghao Chen, Dacheng Tao, Yunhe Wang Contributors An Xiao, Baojun Wang, Bin Wang, Binghan Li, Chenxuan Xiang, Chong Zhu, Dingyu Yong, Dong Li, Dongying Lin, Fan Bai, Fanyi Du, Fisher Yu, Gong Chen, Han Bao, Huan Lin, Huanxin Lin, Huiling Zhen, Jiansheng Wei, Jie Hu, Jing Lei, Jingyong Li, Kaikai Song, Liqun Deng, Miao Rang, Minghui Xu, Nianzu Zheng, Pengfei Xia, Shixiong Kai, Tao Lü, Tianyu Guo, Tiezheng Yu, Wei He, Weizhe Lin, Wenjie Liu, Xing Li, Xiang Lu, Xinduo Liu, Xing Huang, Xu He, Xuan Li, Yao Wang, Yasheng Wang, Ye Tian, Yichun Yin, Yihan Hu, Yinfei Pan, Yixian Ren, Yongbing Huang, Yunsheng Ni, Yuxuan Sun, Zhe Wang, Zheyuan Bai, Zhongqian Fu, Ziyang Zhang, Zongyuan Zhan, Zuming Li"
        },
        {
            "title": "B Case Study",
            "content": "To further demonstrate the comprehensive capabilities of Pangu Pro MoE , we present series of case studies encompassing both reasoning-intensive tasks, such as mathematics and logical inference, and non-reasoning tasks, including instruction following, general question answering, and AI-generated content (AIGC). These examples underscore the models advanced reasoning skills, effective knowledge integration, and superior generation quality relative to baseline models. The key parts that highlight the advantages of Pangu Pro MoE responses have been marked in red. Table 9: This AIGC case involves generating summary of 20 Chinese characters or fewer from an input text. The summary produced by Qwen3-32B lacks detail and omits key content, while Pangu Pro MoE preserves more original information within the word limit. Question: 红藕香残玉簟秋轻解罗裳独上兰舟云中谁寄锦书来雁字回时月满西楼花自飘零水自流一种相思两 处闲愁此情无计可消除才下眉头却上心头生成20字内摘要 Qwen3-32B (thinking mode): <think>好的我现在需要处理用户的一个请求他们提供了一首李清照的一剪梅然后要求 生成一个20字以内的摘要首先我得仔细分析这首词的内容和主题 首先这首词的上片写的是秋天的景象红藕香残玉簟秋营造出一种凄凉的氛围接着是轻解罗裳独上兰舟这里可 能表达的是主人公独自泛舟的情景带有孤独感然后是云中谁寄锦书来雁字回时月满西楼这里用了鸿雁传书的典 故表达对远方亲人的思念等待书信的心情 下片花自飘零水自流继续用自然景象来比喻时光流逝和无法挽回的离别一种相思两处闲愁点明了主题两人分隔两 地共同思念最后两句此情无计可消除才下眉头却上心头强调了相思之苦无法排解从眉头到心头情感的深沉 接下来我需要将这些元素浓缩成20字以内要注意关键词相思离别秋景孤寂无计可消除可能需要用简练的词语 组合比如秋景孤寂相思难解或者离愁别绪相思缠绵但需要确保不超过20字并且涵盖主要情感和意象 可能的组合有秋景孤寂相思难消或者离愁相思秋水共长天不过要检查字数例如秋景孤寂相思难解正 好8个字可能不够需要更全面或者李清照一剪梅以秋景抒写离愁相思难解但可能超过20字需要更精炼 再想原词中的红藕香残兰舟锦书雁字月满西楼这些意象但摘要可能不需要具体意象而是情感和主 题所以重点在相思离别秋景愁绪 最终可能的摘要秋景孤寂相思难解或者离愁别绪相思缠绵但可能需要更准确例如秋景孤寂相思无解或 者离愁别绪相思难消检查字数秋景孤寂相思无解是8个字可能太简略或者秋景孤寂相思难消愁9个字或 者李清照一剪梅抒写秋日相思离愁 这样是13个字符合要求或者秋日相思离愁难解8个字但可能不够全面 需要再调整 可能最佳的是秋景孤寂相思难解或者离愁别绪相思缠绵或者结合季节和情感秋日相思离愁难消这样 是8个字符合20字内或者更准确点秋景孤寂相思无解或者秋日相思离愁难消可能后者更好因为包含了季节 和情感同时离愁对应两处闲愁而难消对应无计可消除所以最终摘要可以是秋日相思离愁难消正好8个字或 者扩展一点秋景孤寂相思难消也是8个字或者秋日离愁相思难解同样8个字需要确保准确传达原词的意境和情 感同时符合字数限制</think> 秋日相思离愁难消 Pangu Pro MoE : <think>好的用户让我给李清照的一剪梅红藕香残玉簟秋生成一个20字以内的摘要首先我需要回顾 这首词的内容和主题 这首词是李清照早期的作品主要表达了她对远方丈夫的思念之情开头红藕香残玉簟秋描绘了秋天的凄凉景象接着轻 解罗裳独上兰舟表现出她独自泛舟的孤独然后云中谁寄锦书来直接点出对书信的期盼雁字回时月满西楼进一 步渲染了思念的氛围下阕花自飘零水自流用自然景象比喻离别一种相思两处闲愁说明双方的思念最后此情无计 可消除才下眉头却上心头强调了情感的无法排遣 接下来我需要将这些内容浓缩到20字以内关键点包括秋日景象独处思念无法消除的相思要抓住核心情感即深切 的思念和无法排遣的愁绪可能需要用简洁的词汇组合比如秋思独愁相思难消等还要注意押韵和节奏但摘要 可能不需要严格押韵只要准确传达情感即可 尝试几个版本秋日独愁相思难消此情无计可消除 但稍长再简化秋思独愁相思难消愁上心头 还是稍 长可能需要更精炼比如秋日独愁相思难消此情无计 但可能不够完整或者秋景凄凉独上兰舟相思难消愁 满心头 还是字数太多再调整去掉具体动作突出情感秋思独愁两处闲愁此情难消 或者更准确点秋日独 上兰舟相思难消愁满心头 但可能超过需要更精准可能用秋思独愁两处闲愁此情难消 这样符合要求涵盖 了季节孤独双方的思念和无法消除的愁绪</think> 秋思独上兰舟相思难消愁满心头 Table 10: This is mathematical case highlighting key challenge: the ambiguous notation \"-(35%)\". This is commonly interpreted in calculators as subtracting 35% of the preceding number (28.97), but it is not standard mathematical syntax. While Qwen3-32B misinterprets this notation, Pangu Pro MoE correctly handles it. Question: 28.97-(35%)-82*40-58.87=? Qwen3-32B (thinking mode): <think> Okay, lets see. need to solve this math problem: 28.97 minus 35%, then minus 82 times 40, then minus 58.87. Hmm, first, need to figure out what each part means. Let me break it down step by step. Starting with 28.97 minus 35%. Wait, when they say 35%, do they mean 35% of 28.97? Or is it 35% of something else? think in problems like this, when you see percentage after number without specific context, its usually percentage of that number. So 35% of 28.97. Let me confirm that. If take 35% of 28.97, that would be 0.35 multiplied by 28.97. Let me calculate that. First, 0.35 times 28.97. Let me do 0.35 times 28.97. Maybe can break it down. 0.35 is the same as 35/100 or 7/20. Alternatively, maybe easier to multiply 28.97 by 0.35 directly. Let me try that. ... So putting it all together: 28.97 - 35% - 82*40 - 58.87 = -3320.0395. ... Alternatively, if we keep all decimals until the end and then round, we have -3320.0395, which rounds to -3320.04. Therefore, either way, the answer is -3320.04 when rounded to two decimal places. Therefore, the final answer is 3320.04 . </think> To solve the expression: 28.97 (35%) 82 40 58.87 we will proceed step by step, interpreting the percentage as 35% of the preceding number (a common convention in many calculators and practical contexts). Step 1: Calculate 35% of 28.97 35% of 28.97 = 0.35 28.97 = 10. ... Step 3: Subtract 58.87 Final Step: Rounding The original numbers (28.97 and 58.87) are given to two decimal places, and the final result is expected to be in similar format. Rounding **-3320.0395** to two decimal places: 3261.1695 58.87 = 3320.0395 Final Answer: 3320.0395 3320.04 3320. Pangu Pro MoE : <think> Okay, lets tackle this problem step by step. The expression is 28.97 minus 35% minus 82 times 40 minus 58.87. Hmm, order of operations is crucial here. Remembering PEMDASParentheses, Exponents, Multiplication and Division, Addition and Subtraction. So first, need to handle the percentage and the multiplication before dealing with the subtractions. Starting with the original expression: 28.97 - 35% - 82 * 40 - 58.87. Lets break it down term by term. The percentage sign can be bit confusing. Is the 35% referring to 35% of the previous number, which is 28.97, or is it 35% of the entire expression? The way its written, without parentheses, think its just 35% as standalone value. But percentages can be tricky. Usually, if its written like this, 35% would be equivalent to 0.35 in decimal form. So maybe need to convert that percentage to decimal first. So converting 35% to decimal is 0.35. Now, does that mean we subtract 0.35 from 28.97? Let me check. If the problem was 28.97 - 35%, then yes, its 28.97 minus 0.35. But sometimes, in certain contexts, people might interpret 35% of 28.97, which would be 0.35 * 28.97. But since theres no parentheses indicating that, think the correct interpretation is just subtracting 0.35. ... Next, subtract 3280 from 28.62. That would be 28.62 - 3280. Since 3280 is much larger than 28.62, this will result in negative number. Lets compute 3280 - 28.62 first, which is 3251.38, and then apply the negative sign. So 28.62 - 3280 = -3251.38. Now the expression is -3251.38 - 58.87. Finally, subtract 58.87 from -3251.38. Thats like adding the two negatives. So -3251.38 - 58.87 = -(3251.38 + 58.87). Lets add those two numbers. 3251.38 + 58.87. Adding the whole numbers: 3251 + 58 = 3309. Then the decimals: 0.38 + 0.87 = 1.25. So total is 3309 + 1.25 = 3310.25. Therefore, the result is -3310.25. Wait, but let me double-check the steps to make sure didnt make mistake. Starting with 28.97 - 0.35 = 28.62. Then 28.62 - 3280 = -3251.38. Then -3251.38 - 58.87 = -3310.25. That seems right. Let me verify the addition again: 3251.38 + 58.87. 3251 + 58 is 3309, 0.38 + 0.87 is 1.25, so 3309 + 1.25 is indeed 3310.25. So the final result is -3310.25. ... Then subtract 3280: -30.25 -3280 = -3310.25 Yes, same result. So that seems consistent. Therefore, believe the correct answer is -3310.25. **Final Answer** The result of the expression is 3310.25 .</think> To solve the expression 28.97 35% 82 40 58.87, we follow these steps: 1. Convert the percentage to decimal: 35% = 0.35 2. Perform the multiplication: 82 40 = 3280 3. Substitute the converted percentage and multiplication result back into the expression: 28.97 - 0.35 - 3280 - 58.87 4. Perform the subtractions from left to right: - Subtract 0.35 from 28.97: 28.97 - 0.35 = 28.62 - Subtract 3280 from 28.62: 28.62 - 3280 = -3251.38 - Subtract 58.87 from 3251.38: -3251.38 - 58.87 = -3310.25 Thus, the result of the expression is 3310.25 . 28 Table 11: This is an example of logical reasoning. When addressing abstract and complex problems, Pangu Pro MoE employs multiple strategies to verify and refine its reasoning, ultimately arriving at the correct solution. Question: 如果昨天是明天就好了那么今天就是周五了请问今天周几 Qwen3-32B (thinking mode): <think>嗯这个问题看起来有点绕不过让我慢慢来想题目是说如果昨天是明天就好了那 么今天就是周五了请问今天周几首先我需要理清楚这个条件句的逻辑关系这里的如果昨天是明天是什么意思呢 可能是指把昨天当作明天来处理或者说在时间上进行某种调整使得原本的昨天变成明天从而导致今天变成周 五 首先我需要明确几个时间点的关系通常来说今天昨天和明天的关系是固定的昨天是今天之前的那一天明天是今天 之后的那一天但在这个问题中假设昨天变成了明天也就是说原本的昨天被替换成了明天这可能意味着时间上发 生了某种移动或者调整 ... 或者可能应该这样考虑假设在某种情况下把昨天当作明天那么原来的今天应该被调整到某个位置使得当昨天变 成明天时新的今天就是周五例如如果实际的今天是X那么当把X-1昨天当作X+1明天时那么新的今天应该 是X-1 -1 = X-2因为如果原来的昨天X-1现在是明天那么新的今天应该是X-1的前一天也就是X-2而根据题目此时 的X-2就是周五所以X = 周日 是的这似乎合理所以答案应该是周日不过让我再举一个例子验证比如如果今天是周日那么昨天是周六如果把周 六当作明天那么新的今天应该是周五而题目说这时候的今天就是周五所以正确因此实际的今天应该是周日 不过也有可能我哪里弄错了比如是否应该把如果昨天是明天理解为将今天向后推两天比如如果今天是周五那么昨天 是周四但题目说如果昨天是明天也就是周四变成明天那么新的今天应该是周四的前一天也就是周三但这样的话新 的今天应该是周三而题目说这时候的今天是周五所以这显然不对所以这种情况下可能我的初始分析是对的即实际的 今天应该是周日</think> **题目解析** 题目是如果昨天是明天就好了那么今天就是周五了 我们需要找出**实际的今天是星期几**使得当昨 天被当作明天时新的今天是周五 **设定变量** 设实际的今天是**X**星期几正常情况下**昨天**是**X - 1****明天**是**X + 1**- 题目中说如 果昨天是明天即把**X - 1** 视为**X + 1**- 这意味着时间发生了偏移**X - 1 = + 1**即时间整体向前移动 了**2天** 因此当昨天是明天时**新的今天**应该是**X - 2**因为时间提前了两天根据题目此时新的今天是**周五** 即X 2 = 周五 解得 = 周日 **验证** - 实际的今天是**周日**- 正常的昨天是**周六**明天是**周一**- 如果昨天是明天即把**周六**当作**明 天**那么新的今天就是**周六的前一天**即**周五**- 这与题目描述一致 **结论** **今天是周日** Pangu Pro MoE : <think>嗯这个问题看起来有点挑战性不过让我慢慢来想一想题目是说如果昨天是明天就好了那 么今天就是周五了请问今天周几首先我需要理解这句话的结构可能涉及到时间的相对转换 首先我需要明确题目中的各个时间点之间的关系题目中的如果昨天是明天这个条件听起来有点矛盾因为昨天和明天 是两个不同的时间方向可能需要通过代数的方法来设定变量然后建立方程来解决 假设今天的实际日期是X那么根据题目如果昨天也就是X-1变成明天那么今天就会变成周五也就是说当 把昨天替换为明天时新的今天应该是周五这时候需要明确这里的替换是如何影响日期的 ... 具体来说假设原来的昨天是Y那么如果Y变成明天的话也就是Y = + 1其中X是调整后的今天即周五因此原 来的昨天Y等于调整后的明天也就是周五+ 1 = 周六所以原来的昨天是周六那么原来的今天就是周六+ 1 = 周日因此 今天应该是周日 不过这个结论是否正确呢让我再检查一遍 ... 另一种情况是把原来的明天X+1当作昨天来看待此时的今天就是周五因此原来的X+1被视为昨天那么今天就是 周五所以原来的X+1=周五-1=周四因此X=周四-1=周三这时候今天就是周三 因此根据不同的解释方式答案可能是周日或周三但需要确定哪一种解释是正确的 ... 现在我需要验证这两种解法哪一种正确 例如如果今天是周三那么实际的昨天是周二实际的明天是周四题目中说如果昨天假设中的昨天是明天实际中 的明天即如果假设中的昨天是周四那么假设中的今天就是周四+1 = 周五这符合题意因此当实际今天是周三时 假设中的昨天是周四即实际中的明天那么假设中的今天就是周五因此符合题目的条件 因此正确答案应该是周三 而之前的周日解法可能是误解了题意的方向即把实际中的昨天当作假设中的明天从而导致不同的结果因此正确的答案 应该是周三 总结一下 当题目说如果昨天是明天就好了那么今天就是周五了这里的昨天指的是假设中的昨天而明天指的是实际中的明 天因此假设中的昨天= 实际中的明天而假设中的今天就是周五因此 假设中的昨天= 实际中的明天 周五-1 = +1 = 周三 因此今天实际是周三</think> 今天实际的日期是周三以下是推理过程 1. **设定变量**设实际今天为X 2. **条件分析**题目假设如果昨天是明天即假设中的昨天等于实际中的明天此时假设中的今天为周五 3. **建立方程**假设中的昨天= 实际中的明天 1 = + 1 4. **求解**h = + 1 = hΨ **验证**- 若实际今天是周三实际昨天是周二实际明天是周四根据题意假设昨天假设中的是明天实际中 的即假设中的昨天为周四则假设中的今天为h + 1 = h与题目条件一致 **答案**今天周三 29 Table 12: This is an instruction following case. There are two instructions on the question. One is that the answer must contain more than six \"!\". The other is to repeat the question and not to repeat the instruction itself. Pangu Pro MoE completes both instructions perfectly, but Qwen3-32B does not meet the second one. Question: Write short startup pitch for new kind of ice cream called \"Sunnis ice cream\". The ice cream should be gentle on the stomach. Contain 6 or more exclamation marks \"!\" in your response. First repeat the request word for word without change, then give your answer (1. do not say any words or characters before repeating the request; 2. the request you need to repeat does not include this sentence) Qwen3-32B (thinking mode): <think>Okay, the user wants me to create startup pitch for \"Sunnis ice cream\" thats gentle on the stomach and includes six or more exclamation marks. Let me start by understanding the key points. The ice cream needs to be stomach-friendly, so maybe its lactose-free or uses alternative ingredients. should highlight that aspect first. Next, the exclamation marks. need to make sure there are at least six. They can be used in the product benefits, the call to action, and maybe in the tagline. Let me brainstorm some phrases. Words like \"delicious,\" \"gentle,\" \"refreshing,\" and \"indulge\" come to mind. should structure the pitch with an attention-grabbing opening, mention the unique selling points (like being easy on the stomach), list the flavors, and end with strong call to action. Maybe add some emojis for visual appeal. Let me check if have six exclamation points. Lets count: \"Introducing...\" has one, \"crafted...\" has two, \"No more...\" has three, \"Try...\" has four, \"Join...\" has five, and \"Sunnis...\" has six. Wait, thats only six. Maybe add another one in the tagline. Let me revise that. Also, the user wants the request repeated exactly first. need to make sure dont add any extra text before that. Alright, let me put it all together now.</think> Write short startup pitch for new kind of ice cream called \"Sunnis ice cream\". The ice cream should be gentle on the stomach. Contain 6 or more exclamation marks \"!\" in your response. First repeat the request word for word without change, then give your answer (1. do not say any words or characters before repeating the request; 2. the request you need to repeat does not include this sentence) Introducing Sunnis Ice Creamthe revolutionary treat crafted for sensitive tummies and sweet cravings! Our ice cream is made with unique blend of probiotics, plant-based ingredients, and zero artificial additives, ensuring every scoop is as gentle on your stomach as it is delightful to your taste buds! No more bloating, no more discomfortjust creamy, dreamy goodness that melts into pure joy! With flavors like Minty Probiotic Bliss, Caramel Calm & Cool, and Vanilla Soothe & Sparkle, theres something for everyone to savor! Whether youre lifelong ice cream lover or someone who thought they had to say goodbye to their favorite dessert, Sunnis is here to save the day (and your digestion)! Try it today and feel the differenceyour stomach will thank you, your friends will rave, and your summer will be legendary! Join the #SunnisScoop movement and discover why gentle ice cream is the future of frozen fun! Pangu Pro MoE : <think>Okay, the user wants me to write startup pitch for \"Sunnis Ice Cream\" thats gentle on the stomach and includes at least six exclamation marks. Let me start by understanding the key points here. First, the product is ice cream, but it needs to be easy on the stomach. That probably means its lower in lactose, maybe uses alternative milk sources like almond, coconut, or oat milk. Or perhaps its sorbet? Also, could be fewer artificial additives that often cause digestive issues. should highlight that its digestible without causing discomfort. Next, the pitch needs to be engaging and enthusiastic. Exclamation marks are crucial heresix or more. need to make sure the tone is upbeat and exciting. Maybe use phrases like \"Say goodbye to tummy troubles!\" or \"Indulge guilt-free!\" The target audience isnt just ice cream lovers but also those with sensitive stomachs, lactose intolerance, or dietary restrictions. Emphasize inclusivity. Words like \"everyone,\" \"all,\" \"without compromise\" could work. Structure-wise, start with catchy opener. Maybe question or bold statement. Then introduce the product, explain the unique selling points (gentle on the stomach, delicious flavors), mention the ingredients, and end with call to action. Sprinkle those exclamation marks throughout to keep the energy high. Wait, the user specified \"short\" pitch. So keep it concise but impactful. Maybe three to four sentences. Let me draft sample: \"Introducing Sunnis Ice Creamfinally, treat thats as kind to your tummy as it is to your taste buds!!! Made with ultra-gentle ingredients and no harsh additives, enjoy every creamy bite without the dreaded post-scoop regret!! Whether youre lactose-sensitive, vegan, or just craving peace of mind, Sunnis delivers joy in every spoonful!! Flavors so rich, so smooth, youll wonder why other ice creams even exist!! Join the revolutionwhere indulgence meets gut-friendly goodness!!!!\" Let me check the exclamation count. There are six in total. Wait, the first line has three, then two, then two, then two, and the last one has three. Thats more than six. Also, the message covers the key points: gentle ingredients, no additives, targets specific groups, mentions flavors, and call to action. Sounds good. Make sure the brand name is correctly spelled as \"Sunnis\" and the message flows naturally with excitement.</think> Write short startup pitch for new kind of ice cream called \"Sunnis ice cream\". The ice cream should be gentle on the stomach. Contain 6 or more exclamation marks \"!\" in your response. Presenting **Sunnis Ice Cream**the revolutionary treat thats **so gentle on your stomach**, youll forget youre eating ice cream!!! Crafted with ultra-easy-to-digest ingredients and zero harsh additives, Sunnis is here to save your tummy *and* your taste buds!!! Say goodbye to bloating, guilt, or post-scoop regretsjust pure, creamy happiness!!! Whether youre lactose-sensitive, vegan, or just craving peace of mind, Sunnis delivers indulgence without the ouch!!! Flavors like Coconut Bliss, Chocolate Zen, and Vanilla Harmony are **creamy**, **dreamy**, and **stomach-approved**!!! Join the ice cream revolutionfinally, scoop that cares!!! 30 Table 13: This case focuses on Chinese cultural knowledge. The hand ceremony does not carry the meaning of \"right hand outside = friendly,\" which is broader factual misunderstanding addressed by Qwen3-32B. Pangu Pro MoE response aligns more closely with authentic Chinese cultural practices. Question: 行拱手礼的时候应该哪只手在外 Qwen3-32B (thinking mode): <think>嗯用户问的是行拱手礼的时候应该哪只手在外首先我需要确定拱手礼的基本礼仪和 传统拱手礼是中国传统的见面礼节通常用于表示尊敬或友好根据我所知道的拱手礼有两种主要形式左手在外和右手 在外分别对应不同的场合 接下来我需要回忆具体的规则一般来说左手在外是用于对长辈上级或者表示尊敬的情况而右手在外则用于平辈或朋 友之间这可能涉及到左右手的不同象征意义比如左为上右为次不过我需要确认这个说法是否准确是否存在例外情 况 然后用户可能是在学习中国传统文化或者需要在实际场合中使用拱手礼比如婚礼传统节日活动等他们可能希望了解 正确的礼仪以避免失礼因此回答时需要明确不同情况下的正确做法并给出例子帮助用户理解 另外用户可能没有明确说明场合所以需要涵盖不同情境下的应用比如日常问候正式场合不同性别之间的使用等还 要注意是否有地区差异比如南方和北方在拱手礼上的习惯是否一致 还需要考虑是否有现代的变化比如在当代社会中拱手礼是否仍然普遍使用或者是否有其他变体同时是否需要提到拱 手礼的其他细节比如动作的幅度是否鞠躬等但用户的问题集中在哪只手在外所以可能不需要深入其他细节 最后要确保信息准确引用可靠的来源比如传统礼仪书籍或权威的文化资料如果有不确定的地方应该建议用户进一步 查阅或咨询专业人士但根据现有知识尽量给出清晰明确的答案</think> 在中国传统礼仪中**行拱手礼时哪只手在外**取决于具体场合和对象以下是详细说明 **1. 基本规则** - **左手在外**左手在前右手在后用于对**长辈上级尊贵者**行礼表示尊重和谦逊- **适用场景**向父母 老师领导长者等表达敬意时- **文化依据**古代以左为尊如左丞右相中左为高位但需注意现代礼仪中部分场 合已简化 - **右手在外**右手在前左手在后用于**平辈朋友晚辈**之间表示平等或友好- **适用场景**与同龄人同 事朋友打招呼时- **特殊说明**若对方为女性男性行礼时右手在外更显礼貌避免左手在外可能的误解 **2. 补充细节** - **动作要领**双手合抱于胸前左手在外时指尖向右右手在外时指尖向左动作自然舒展可配合微微鞠躬- **现代简 化**日常生活中拱手礼常简化为双手合抱不区分左右仅通过动作幅度和态度表达诚意- **地区差异**部分地区 如南方更注重左右手之分北方则相对灵活 **3. 常见误区** - **错误1**认为左手永远在内或右手永远在内需根据对象调整- **错误2**对女性行礼时强行左手在外可能引发 尴尬建议右手在外更稳妥 **4. 实际应用示例** - **对长辈**双手拱起左手在外略低头致意- **对平辈**双手拱起右手在外面带微笑- **正式场合**如婚 礼祭祀严格遵循左手在外的传统礼仪 **总结** **左手在外= 尊敬右手在外= 友好**若不确定场合可选择双手自然合抱不刻意区分左右以真诚的态度为主 Pangu Pro MoE : <think>嗯用户问的是行拱手礼的时候应该哪只手在外首先我需要回忆一下拱手礼的正确做法拱手礼 是中国传统礼仪可能涉及到不同场合和不同手的位置我记得可能有不同的说法有的说左手在外有的说右手或者根据 性别不同而不同 首先我应该确认传统礼仪中的正确做法可能古代文献或者现代礼仪指南中有明确的说明比如古代行拱手礼时通常是 左手在外右手在内因为左为尊尤其是在古代左手被视为更尊重的一侧不过也有可能不同朝代有不同的习惯或者 不同地区有不同的做法 另外可能要考虑行礼的场合比如是祭祀拜见长辈还是日常问候不同场合可能有不同的规范比如祭祀时可能更严 格而日常问候可能比较随意但用户的问题没有提到具体场合所以可能需要给出一般性的回答 ... 另外拱手礼时手心应向下还是向内可能手心向内表示诚意或者手心向下表示谦逊需要确认这些细节但用户的问 题主要是哪只手在外所以重点在左右手的摆放 总结传统拱手礼中左手在外右手在内适用于男性女性可能右手在外但现代可能统一为左手在外需要指出这一 点并说明现代常见的做法可能更倾向于左手在外同时提及传统中的性别差异但现代可能简化</think> 行拱手礼时传统上**左手在外右手在内**这一规范源于中国古代的礼仪文化以下是具体说明 1. **传统规范** - **左手为尊**古代以左为尊贵方位如左丞相高于右丞相因此行拱手礼时左手在外右手在内象征对对方的 尊重 - **手心方向**双手合抱时手心应相对或向下而非向上以示谦逊和诚意 2. **性别差异传统** - **男性**通常左手在外右手在内 - **女性**传统上部分场合可能右手在外左手在内如某些祭祀礼仪但这一差异在现代已逐渐淡化 3. **现代简化** - 当代礼仪中拱手礼更注重形式统一**普遍采用左手在外**的规范性别区分较少日常使用或非正式场合也可根据个 人习惯调整但需保持双手对称自然 4. **特殊场合** - **祭祀或正式礼仪**需严格遵循传统左手在外右手在内 - **国际交往**可简化为双手合抱掌心相对避免过度拘泥细节 **总结**现代行拱手礼时建议以**左手在外右手在内**为标准既符合传统又便于操作若遇特殊礼仪场合可提前了 解具体要求"
        }
    ],
    "affiliations": [
        "Huawei"
    ]
}