{
    "paper_title": "Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation",
    "authors": [
        "Enshu Liu",
        "Qian Chen",
        "Xuefei Ning",
        "Shengen Yan",
        "Guohao Dai",
        "Zinan Lin",
        "Yu Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Image Auto-regressive (AR) models have emerged as a powerful paradigm of visual generative models. Despite their promising performance, they suffer from slow generation speed due to the large number of sampling steps required. Although Distilled Decoding 1 (DD1) was recently proposed to enable few-step sampling for image AR models, it still incurs significant performance degradation in the one-step setting, and relies on a pre-defined mapping that limits its flexibility. In this work, we propose a new method, Distilled Decoding 2 (DD2), to further advances the feasibility of one-step sampling for image AR models. Unlike DD1, DD2 does not without rely on a pre-defined mapping. We view the original AR model as a teacher model which provides the ground truth conditional score in the latent embedding space at each token position. Based on this, we propose a novel \\emph{conditional score distillation loss} to train a one-step generator. Specifically, we train a separate network to predict the conditional score of the generated distribution and apply score distillation at every token position conditioned on previous tokens. Experimental results show that DD2 enables one-step sampling for image AR models with an minimal FID increase from 3.40 to 5.43 on ImageNet-256. Compared to the strongest baseline DD1, DD2 reduces the gap between the one-step sampling and original AR model by 67%, with up to 12.3$\\times$ training speed-up simultaneously. DD2 takes a significant step toward the goal of one-step AR generation, opening up new possibilities for fast and high-quality AR modeling. Code is available at https://github.com/imagination-research/Distilled-Decoding-2."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 3 2 ] . [ 1 3 0 0 1 2 . 0 1 5 2 : r Distilled Decoding 2: One-step Sampling of Image Auto-regressive Models with Conditional Score Distillation Enshu Liu Tsinghua University Beijing, China Qian Chen Tsinghua University Beijing, China Xuefei Ning Tsinghua University Beijing, China Shengen Yan Infinigence-AI Beijing, China Guohao Dai Shanghai Jiaotong University Shanghai, China Zinan Lin Microsoft Research Redmond, WA, USA Yu Wang Tsinghua University Beijing, China"
        },
        {
            "title": "Abstract",
            "content": "Image Auto-regressive (AR) models have emerged as powerful paradigm of visual generative models. Despite their promising performance, they suffer from slow generation speed due to the large number of sampling steps required. Although Distilled Decoding 1 (DD1) was recently proposed to enable few-step sampling for image AR models, it still incurs significant performance degradation in the one-step setting, and relies on pre-defined mapping that limits its flexibility. In this work, we propose new method, Distilled Decoding 2 (DD2), to further advance the feasibility of one-step sampling for image AR models. Unlike DD1, DD2 does not without rely on pre-defined mapping. We view the original AR model as teacher model that provides the ground truth conditional score in the latent embedding space at each token position. Based on this, we propose novel conditional score distillation loss to train one-step generator. Specifically, we train separate network to predict the conditional score of the generated distribution and apply score distillation at every token position conditioned on previous tokens. Experimental results show that DD2 enables one-step sampling for image AR models with minimal FID increase from 3.40 to 5.43 and 4.11 to 7.58 on ImageNet-256, while achieving 8.0 and 238 speedup with VAR and LlamaGen models, respectively. Compared to the strongest baseline DD1, DD2 reduces the gap between the one-step sampling and original AR model by 67%, with up to 12.3 training speed-up simultaneously. DD2 takes significant step toward the goal of one-step AR generation, opening up new possibilities for fast and high-quality AR modeling. Code is available at https://github.com/ imagination-research/Distilled-Decoding-2."
        },
        {
            "title": "Introduction",
            "content": "Image autoregressive (AR) models have recently achieved state-of-the-art performance in high-fidelity image synthesis, surpassing other generative approaches such as VAEs, GANs, and diffusion models [38, 3, 6, 27, 15, 45, 2, 16, 37, 35, 17, 9, 14, 8, 31, 10, 34]. Despite their strong generation ability, key limitation of AR models lies in their inherent sequentially modeling manner, which leads to the token-by-token sampling process and significantly slower Project Advisor: Zinan Lin. Correspondence to Zinan Lin (zinanlin@microsoft.com) and Yu Wang (yu-wang@mail.tsinghua. edu.cn). 39th Conference on Neural Information Processing Systems (NeurIPS 2025). inference speed. Numerous methods have been proposed to reduce sampling steps [41, 1, 18, 36, 11, 2, 37, 17], but nearly all fail to achieve single-step sampling without significant performance degradation, leaving room for further speedup. Please refer to Sec. 4.1 for more details. Distilled Decoding 1 (DD1) [21] marks significant breakthrough in reducing the sampling steps for AR models, as it is the first method capable of compressing the sampling process of an AR image model to only single step. DD1 introduces flow matching [22, 20] into the AR sampling pipeline. Specifically, instead of sampling the next token directly from probability vector output by the AR model, DD1 leverages flow matching in the codebook embedding space to transform noise token into data token. This enables token-wise deterministic mapping from noise to data while preserving the output distribution of the original AR model. By iteratively conducting this process following the original AR sampling order, DD1 obtains complete mapping from noise token sequence to data token sequence. Then, new model is distilled to directly learn this mapping, allowing the generation of the entire token sequence in single forward pass. Figure 1: Our goal is to distill multistep AR model in to one-step generator while keeping its distribution. However, the constructed mapping is inherently challenging for the model to learn, resulting in noticeable performance drop compared to the original AR model. In addition, training generative model to directly fit predefined mapping may impose constraints on the flexibility. In contrast, models like GANs and VAEs, which do not learn explicit inputoutput correspondences, have shown broad applicability across downstream generation tasks [19]. This insight leads us to ask: Can we train one-step generative model whose output distribution matches given AR model, without relying on any predefined mapping? To answer this problem, we propose Distilled Decoding 2 (DD2) as completely new method. Inspired by DD1, our key motivation is to reinterpret the AR model, which originally outputs discrete probability vector for the next token qi, as conditional score model that predicts the gradient of the log conditional probability density (i.e., the conditional score) in the codebook embedding space. Specifically, we view the generation of each token as conditional flow matching process. Based on this, given all previous tokens q1,...,i1 as the condition, we can use the teacher models output probability vector to define conditional score s(qt q<i), where denotes the flow matching timestep. Unlike DD1, where the conditional score is used solely to construct an ODE-based mapping, we aim to make fuller use of this signal. We borrow ideas from score distillation methods (e.g., [26, 40, 24, 43, 48]), which match the score of ax one-step generators distribution to that of teacher diffusion model and have recently shown strong performance in diffusionbased generation. Specifically, our DD2 jointly trains one-step generator and conditional guidance network that learns the conditional score of the generator distribution. We propose novel Conditional Score Distillation (CSD) loss for training, which aligns the conditional score between the guidance network and the teacher AR model at every token position. We show that when the CSD loss is minimized to its optimality, the output distribution of the one-step generator matches exactly that of the original AR model. , tq<i) = qt log p(qt It is important to highlight that our method is fundamentally different from diffusion score distillation. Although both approaches involve aligning scores, AR models and diffusion models follow completely different modeling approaches and generation processes. As result, the goals and challenges in this paper are inherently distinct from previous works. More discussion about the differences between the two methods can be found at Sec. 6.1. To validate the effectiveness of DD2, we follow the evaluation setup of DD1 and conduct experiments on ImageNet-256 [4] with two strong autoregressive models: VAR [37] and LlamaGen [35]. On VAR, we reduce the sampling steps from 10 to 1 with marginal FID increase less than 2.5 (e.g., from 4.19 to 6.21), achieving up to 8.1 speedup. Compared to DD1, DD2 reduce the performance gap between the 1-step model and the original AR model by up to 67%. On LlamaGen, we compress the sampling process from 256 steps to 1 with an FID degradation from 4.11 to 8.59, resulting in 238 speedup. Compared to DD1, our 1-step model achieves an FID improvement of 2.76. Further comparisons between DD2 and other baseline methods are presented in Fig. 2. Additionally, DD2 is highly efficient to train: compared to DD1, it achieves up to 12.3 training speedup. We hope this 2 Figure 2: Comparison of DD2 models, DD1 models, pre-trained models, and other acceleration methods for pre-trained models. DD2 achieves significant speedup compared to pre-trained models while outperforming DD1 by large margin. Other methods fail to achieve one-step sampling. For DD2, DD1, and the pre-trained model, each point corresponds to different model size, whereas for the skip-last method, each point corresponds to different number of skipped final steps. work can inspire future research toward making image AR models maximally efficient while keeping their superior sample quality."
        },
        {
            "title": "2 Preliminary",
            "content": "In this section, we introduce the formulation of standard image AR models to understand DD2. 2.1 Image Tokenizer To train an image AR model, we first need to convert continuous-valued images into discrete token sequences, so that the probability of each token can be explicitly outputted by the model. Recent AR models mostly rely on vector quantization (VQ) [39], which leverages an encoder E, quantizer Q, and decoder to discretize and reconstruct visual content. The process begins by encoding the input image R3HW into latent representation: = E(x), where = (z1, z2, . . . , zhw) RChw is lower-resolution feature map containing embeddings, each of dimension C. For each embedding zi, the quantizer selects the nearest code vector qi from learned codebook = (c1, c2, . . . , cV ) RV C. The resulting discrete token sequence is denoted as = (q1, q2, . . . , qhw), where each qi is token cj in V. To reconstruct the original image, the decoder takes as input and produces: ˆx = D(z). During training, reconstruction loss l(ˆx, x) is used to ensure fidelity between the original and the reconstructed image. This VQ-based framework underpins many state-of-the-art image AR models [16, 2, 37, 35, 17]. 2.2 Auto-regressive Modeling Once well trained image tokenizer is available, an AR model can be employed for image generation. We assume that an image is represented as sequence of discrete tokens = (q1, , qn), where each qi corresponds to an embedding from the codebook V: qi {c1, . . . , cV }. The AR model is trained to estimate the conditional probability distribution of each token given all previous tokens: p(qiq<i) = p(qiqi1, qi2, , q1) = (p1, . . . , pj, . . . , pV ), where pj denotes the probability that the next token corresponds to the j-th entry in the codebook. At generation time, the model samples tokens one by one in order, and the likelihood of the full sequence is given by: p(Z) = (cid:81)n i=1 p(qiq<i). This generation procedure requires autoregressive steps, which is often large number, resulting in slow inference speed and limited efficiency."
        },
        {
            "title": "3 Distilled Decoding 2",
            "content": "In this section, we first introduce the formal problem definition in Sec. 3.1. Then, we propose Conditional Score Distillation (CSD) loss as the core component of DD2 in Sec. 3.2. Then we discuss our initialization method in Sec. 3.3, which plays crucial role in training speed and performance. Finally, we present the full training pipeline of our approach. 3 (a) Training loss of the generator. (b) Training loss of the guidance network. Figure 3: Training process using CSD loss. For the generator, the teacher AR model and the guidance network give the true and fake conditional score of each noisy token based on all previous clean tokens, respectively. Then the true and fake conditional score are used to calculate the score distillation loss, which produces gradient to train the generator. The guidance network learn the conditional score of each noisy token given all previous clean tokens, by optimizing with standard AR-diffusion loss [17]. The generator and the guidance model are trained alternately. 3.1 Problem Formulation Suppose image can be encoded as sequence of length n, and we have well trained teacher AR model pΦ, which gives the next token probability conditioned on all previous tokens pΦ(xix<i) and will be fixed. Our goal is to train one-step generator Gθ, which can output generated sequence zθ = (q1, . . . , qn) in one run given latent variable ε drawn from the prior distribution: zθ = Gθ(ε). We hope the distribution of zθ can match the distribution of the teacher AR model. 3.2 Conditional Score Distillation Loss In this section, we first introduce our idea of viewing teacher AR model as conditional score model in Sec. 3.2.1, and propose the objective based on it in Sec. 3.2.2. Then, we present how to train the conditional score model for the generator distribution in Sec. 3.2.3, as it is required by the objective. 3.2.1 Teacher AR as Conditional Score Model Considering the generation process of the i-th token qi given all previous tokens (q1, . . . , qi1) as the condition, we have the probability vector = (p1, . . . , pV ) outputted by the teacher AR model, where pj 0 denote the probability of j-th token cj and (cid:80)V j=1 pj = 1. Inspired by DD1 [21], we view the sampling process as continuous transformation of flow matching [22, 20] from source Gaussian distribution at = 1 to sum of Dirac function δ() weighted by at = 0: p(qi) = (cid:80)V j=1 pjδ(qi cj). By choosing the noise schedule of RectFlow [22], the score function can be expressed in closed form as: s(xt, t, p) = (cid:80)V j=1 pj(xt (1 t)cj)e j=1 pje t2 (cid:80)V (xt(1t)cj )2 2t2 (xt(1t)cj )2 2t2 (1) For more details on the derivation of this expression, refer to Sec. C.1. By substituting qi as and (p1, . . . , pV ) = pΦ(qiq<i) to Eq. (1), we rewrite the left side of Eq. (1) in the form of conditional score function s(xt, t, p) = sΦ(qt is noisy version of the clean token qi: qt = (1 t)qi + tϵ, ϵ (0; I). The term conditional score refers to the score of qt conditioned on q<i. Note that the condition term q<i consists of previous clean tokens without any noise injection, so it can also be noted as q0 , tq<i). Here qt <i. 3.2.2 Training Objective With access to the true score function, we aim to make full use of this information rather than using it merely to construct an ODE mapping as in DD1. To this end, we draw inspiration from score distillation methods. These methods seek to align the distribution generated by model with that of teacher by matching their respective score functions. We first present general formulation of score distillation. Let RC be random variable. Denote pΦ and sΦ as the probability density function and its score function given by the teacher model Φ, pθ and sf ake as the probability density function and its score function of the generator θ. general score distillation loss can be given as: LSD = Et[0,T ],x0pθ,ϵN (0;I)d(sΦ(αtx0 + σtϵ, t), sf ake(αtx0 + σtϵ, t)), (2) where is function satisfying that minimal LSD guarantees RC, pθ(x) = pΦ(x). In practice, we choose SiD loss [48] due to its effectiveness, giving: = ω(t) σ4 α2 (sΦ sf ake)T (sΦ + ϵ σt α(sΦ sf ake)), (3) where ω(t) is the weight function and α is hyper-parameter, which we set to 1.0. In our scenario, however, we are not aligning the distribution of single random variable like score distillation for diffusion models [43, 24, 48], but sequence of random variables with auto-regressive correspondence. Specifically, we aim to match the generators conditional distribution at each token position with that of the teacher AR model. This motivates us to minimize the score distillation loss on all token positions. Additionally, we have to replace the score term in Eq. (2) with the conditional score given all previous tokens and αt, σt with the noise schedule of RectFlow [22]. By incorporating above modifications to Eq. (2), we propose our conditional score distillation (CSD) loss: (cid:88) d(sΦ(qti , tisg(q<i)), sf ake(qti , tisg(q<i))), (4) LCSD = Eti,(q1,...,qn)pθ,ϵN (0;I) = (1 t)qi + tϵ and sg() means the stop gradient operation. We give the following where qt proposition to show the correctness of our CSD loss, with brief proof in Sec. A. Proposition 1. Minimal LCSD guarantees = (q1, . . . , qn) RnC, pθ(z) = pΦ(z). i= Intuitively, Eq. (4) encourages progressive alignment of the token sequence distributions. Consider the first token q1, which has no constraints by any other tokens. Its associated loss term reduces to standard score distillation loss Et1,q1pθ,ϵN (0;I)d(sΦ(qt1 1 , t1)), which encourages pθ(q1) to align with pΦ(q1). Once the first tokens distribution is aligned, we then consider the loss for the second token: Et2,q2pθ,ϵN (0;I)d(sΦ(qt2 2 , t2)sg(q1)). Optimizing this ensures pθ(q2q1) = pΦ(q2q1). Given that pθ(q1) = pΦ(q1) has already been achieved, it follows pθ(q1, q2) = pΦ(q1, q2). By sequentially matching the distribution on each token position, we can finally align the entire distribution pθ(q1, . . . , qn) with pΦ(q1, . . . , qn). 2 , t2sg(q1)), sf ake(qt2 1 , t1), sf ake(qt1 3.2.3 Learning the Conditional Score of the Generator To optimize Eq. (4), we need to access the conditional score of the generator sf ake(qti , tiq<i). Following previous works of diffusion score distillation [24, 43, 42, 48], we train separate model ψ to output this term, which we refer to as the conditional guidance network. Specifically, our guidance network consists of decoder-only transformer backbone and lightweight MLP head with negligible cost. The training procedure is inspired by MAR [17]. Given generated token sequence (q1, . . . , qn) from the generator, we first process it with the causal transformer backbone, yielding sequence of hidden features (f1, . . . , fn). Each feature fi only corresponds to tokens q<i and thus captures strictly causal context. For each token position i, the MLP takes as input noised version of the token qti , the corresponding timestep ti, and the contextual feature fi. Since fi only corresponds to q<i as the conditioning, we denote the outputted score function as the fake conditional score sψ(qti , ti q<i). We train the model across all AR positions in parallel and then present the following loss: LF CS = Eti,(q1,...,qn)pθ,ϵN (0;I) (cid:88) sψ(qti , tiq<i) qti log p(qti qi)2, (5) i=1 log p(qti qi) can be simplified to ϵ [34]. The MLP and where qt transformer backbone are jointly optimized with LF CS. = (1 t)qi + tϵ, and qti In practice, the guidance network ψ and generator θ are trained alternately using Eq. (5) and Eq. (4), respectively. During generator training with Eq. (4), the score term sf ake is entirely replaced by sψ, with gradients blocked from propagating into ψ. The training algorithm and an illustration of the pipeline are provided in Alg. 1 and Fig. 3. 5 Figure 4: Training loss for initialization. Figure 5: Overall pipeline. Performance alignment is an optional technique in the CSD training stage, which is introduced in Sec. B.4. 3. Initialization of Generator and Guidance Network With the training procedure outlined in Alg. 1, we are now ready for DD2 training. However, directly applying this method does not yield satisfactory results. We attribute this to the poor model initialization. We delve into this issue and propose our solutions in the following part of this section. We find that good initialization is crucial for score distillation methods: poor initialization can lead to slow convergence or even training collapse. To validate this, we conduct diffusion distillation experiments on the ImageNet-64 dataset using the original DMD [43, 42] approach under different initialization schemes: (1) Default: both the guidance and generator models are initialized from pretrained teacher diffusion model, (2) Random Guidance: the guidance model is randomly initialized, (3) Random Generator: the generator is randomly initialized, and (4) Partial Random Generator: only the final layer of the generator is randomly initialized. As shown in Fig. 6, improper initialization of either the guidance or the generator leads to significant training degradation. Even randomly initializing just the final layer of the generator severely impacts the performance. This is because initialization determines both the internal knowledge stored in the network and the generators initial distribution, both of which are critical to stable and efficient score distillation training as discussed in [46]. In our setting, both the generator and the conditional guidance network output continuous values, while the teacher AR model produces probability vectors. This structural mismatch makes it impossible to directly reuse the model weights from the teacher AR model to initialize the output heads. To address this, we propose novel initialization strategy: we first replace the teacher AR models classification head with lightweight MLP, and fine-tune the new model with AR-diffusion loss [17] to align its distribution with the teacher AR model. This process is similar to the training of the conditional guidance network and MAR model [17] but with key difference: we introduce Ground Truth Score (GTS) loss by replacing the Monte Carlo Estimation in Eq. (5) with the ground truth score calculated using the teacher AR model with Eq. (1), giving: LGT = Eti,(q1,...,qn)pΦ,ϵN (0;I) (cid:88) i=1 sψ(qti , tiq<i) sΦ(qti , tiq<i)2. (6) This loss significantly improves training stability and convergence speed, as demonstrated in experiments in Tab. 7. The training process is shown at Alg. For both generator and guidance network, we adopt the same architecture composed of transformer backbone and lightweight MLP head, both initialized from the tuned AR diffusion model. For generator, we sample noise sequence ε = (ϵ1, . . . , ϵn) as the latent variable input, where each ϵi (0; I). This sequence is fed directly into the MLP, while one-step offset version is provided to the transformer backbone. Model architectures are shown in Fig. 7. Such strategy serves as strong initialization for both generator and guidance network, improving the training significantly as demonstrated in Sec. 5.5. Overall pipeline The complete training process consists of two stages: an initialization tuning phase with Eq. (6) and main training phase with Eq. (4) (for the generator) and Eq. (5) (for the guidance network). The workflow is illustrated in Alg. 3 and Fig. 5. More techniques can be found in Sec. C. Multi-step sampling involving the teacher model. To achieve more flexible trade-off between sample quality and steps, we can use the teacher model to refine the last several steps of the one-step generated sequence. Details of this sampling method are shown in Sec. and Alg. 4. 6 Algorithm 1: Training with CSD loss Require: the pre-trained teacher AR model Φ. 1: while not converged do 2: = (q1, . . . , qn) Gθ(ϵ) Algorithm 2: Tuning the AR-diffusion model Require: dataset D, the pre-trained teacher AR model Φ, AR-diffusion model Ψ. 1: initialize the backbone of Ψ with the backbone of Φ, randomly initialize the MLP head of Ψ // Train generator θ Sample = (t1, . . . , tn), sample zϵ = (ϵ1, . . . , ϵn) from Gaussian distribution zt = (qt1 ) (1 t)z + tzϵ update θ with LCSD(zt, z, t, ψ, Φ) // Eq. (4). 1 , . . . , qtn // Train guidance network ψ Sample another = (t1, . . . , tn), sample another zϵ = (ϵ1, . . . , ϵn) from Gaussian distribution zt = (qt1 update ψ with LF CS(zt, z, t, ψ) // Eq. (5). ) (1 t)z + tzϵ 1 , . . . , qtn 3: 4: 5: 6: 7: 8: 9: end while 10: return θ"
        },
        {
            "title": "4 Related Works",
            "content": "2: while not converged do 3: 4: Sample = (q1, . . . , qn) Sample = (t1, . . . , tn), sample zϵ = (ϵ1, . . . , ϵn) from Gaussian distribution zt = (qt1 1 , . . . , qtn update Ψ with LGT S(zt, z, t, Ψ, Φ) Eq. (6). ) (1 t)z + tzϵ 5: 6: 7: end while 8: return Ψ Algorithm 3: Overall Pipeline Require: dataset D, the pre-trained teacher AR model θΦ. 1: train the AR-diffusion model Ψ with Alg. 2 2: duplicate the trained Ψ as generator θ and guidance network ψ 3: train generator θ and guidance network ψ with Alg. 1 4: return θ 4.1 Reducing the Sampling Steps of AR Models Many prior works have attempted to reduce the sampling steps of AR models. Set prediction is commonly used approach in image AR modeling, where the model is trained to predict the probability of set of tokens simultaneously [2, 16, 37, 17]. It significantly reduces the number of sampling steps to around 10. However, this method struggles to sample with very few steps (e.g., 1), due to the complete loss of token correlation within each set. As the set size increases, this loss becomes increasingly detrimental to sample quality as discussed in [21]. For example, consider the case where the dataset contains 2 data samples with 2 dimensions: = {(0, 0), (1, 1)}. The one-step sampling yields uniform distribution among {(0, 0), (1, 1), (0, 1), (1, 0)}, which is incorrect. For more details, please refer to the Section 3.1 of DD1 [21]. Speculative decoding is another method of step reduction, which is widely used in large language models (LLMs) [41, 1, 18] due to its training-free property. It generates several draft tokens with more efficient sampling method and then verifies them in parallel using the target model. Speculative decoding can achieve only limited compression ratio of sampling steps (less than 3) in image AR generation [36, 11], due to the weak modeling capacity of the draft generator. Distilled Decoding 1 (DD1) [21] is the first work that compress the sampling steps of image AR models to 1 without performance collapse. The key idea of DD1 is to construct deterministic mapping between sequence of noise tokens and sequence of target tokens. Specifically, given the probability vector outputted by the teacher AR model when generating the next token, DD1 replaces the multinomial sampling process of the original AR model with flow-matching sampling. The required velocity field can be accurately calculated through Eq. (1). By conducting this process following the original AR order, DD1 can map noise sequence to data sequence. Then DD1 simply train neural network to fit this mapping. Although DD1 enables one-step sampling, it suffers from significant performance degradation and relatively slow training. Moreover, its reliance on predefined mapping limits flexibility. As new one-step training framework for AR models, DD2 effectively alleviates all the issues above. 4.2 Score Distillation for Diffusion Models Similar to AR models, diffusion models (DMs) also suffer from large number of sampling steps required by solving the diffusion ODE/SDE. Score distillation [26, 40] serves as method to distill the multi-step teacher DM into one-step generator [24, 43, 42, 48]. The main idea of score distillation is to make the distribution of the generator indistinguishable to the distribution of the teacher DM. general formulation of score distillation generator loss has been given in Eq. (2). guidance network is introduced to approximate the score function sf ake of the generator. Both models are trained in turn. Different score distillation methods choose different types of generator loss. For example, [24, 43, 42] take the KL divergence between the two distributions as the objective, which gives = sf ake sΦ. SiD [48] aims to optimize the l2 distance between the score functions of the two distributions, giving Eq. (3) as the loss. Compared to traditional diffusion distillation method 7 based on ODE mapping [23, 29, 33, 13, 32], score distillation methods are more flexible and have better results. By introducing training data, [46] eliminates the teacher guidance in score distillation through class-ratio estimation. Recently, there are also methods applying DMD to temporal causal data type like video [44]. The main difference between our paper and [44] lies in the problems they aim to solve: our work focuses on few-step sampling for AR models, where an AR model is available as the teacher, while [44] targets to decrease the step of DMs and assumes access to teacher DM."
        },
        {
            "title": "5 Experiments\nIn this section, we apply DD2 to existing pretrained AR models to demonstrate DD2’s strong ability\nto compress AR sampling into a single step.",
            "content": "5.1 Setup Base Models and Benchmark. In line with DD1 [21], we choose VAR [37] and LlamaGen [35] as the base AR models due to their popularity and strong generation quality. Moreover, these two models differ significantly across several key aspects, which makes them ideal testbeds for evaluating the generality of DD2: (1) Tokenizer training: VARs codebook is trained to support multi-resolution image tokens, while LlamaGens tokens are derived solely from the original resolution space; (2) Token ordering: VAR constructs the full sequence by concatenating sub-sequences across different resolutions, whereas LlamaGen follows traditional raster-scan order; (3) Generation steps: VAR has 10 sampling steps, while LlamaGen requires 256 steps. These differences allows us to evaluate how DD2 performs across wide range of AR setups. We choose the popular and standard ImageNet256 dataset as the benchmark. Generation. We use the one-step sample quality as our main results in Tab. 1. Additionally, following DD1, we involve the teacher AR model in sampling for smoother trade-off of quality and steps. Results are listed in Tab. 2. Baselines. Since DD1 [21] is the only method that enables few-step sampling for image AR models, we take it as our main baseline. We also report the results of several weak baselines in the DD1 paper: (1) directly skip last several steps, and (2) predicting the distribution of all tokens in one step, which is the extreme case of set-of-token prediction method. Details of baseline can be found in Sec. E.5. 5.2 Results of One-step Generation We demonstrate the main results of DD2 in Tab. 1. Since the model parameter sizes and inference latency of DD2 and DD1 are similar, we compare them under the same number of sampling steps. The key takeaways are: The performance gap between DD2 and the teacher AR model is minimal. For VAR models across all model sizes, compressing the teacher model to 1 step and achieving up to 8.1 speedup with an mere FID increase of less than 2.5. For LlamaGen models, DD2 achieves 238 speed-up in FID increase of only 4.48. Such performance drop is acceptable. DD2 outperforms the strongest baseline DD1 significantly. For VAR models across all model sizes, DD2 decreases the performance gap between the teacher AR model and one-step model by up to 67% compared to DD1. DD2 even outperforms the 2 step sampling results of DD1 by large margin for all VAR models. For LlamaGen model, DD2 also achieves 2.76 better FID than DD1. All weak baselines fail to generate in one-step. These results show the effectiveness of DD2. 5.3 Results of Multi-step Sampling It is better to offer smoother trade-off curve between quality and step. To achieve this, we use the teacher model to refine the last several AR positions of the generated content. The detailed algorithm is shown at Alg. 4. Results are reported in Tab. 2. For DD1 baseline, we use its default multistep sampling schedule and control the number of sampling steps to ensure fair comparison. The sample quality increases consistently with more sampling steps, offering more choices for the users. 5.4 Training Efficiency In addition to its superior performance over DD1, DD2 offers another significant advantage: much faster convergence. As shown in Tab. 3, DD2 requires substantially much fewer GPU hours to train, achieving up to 12.3 training speedup while having better performance than DD1. Detailed analysis of DD2s training cost can be found at Sec. B.4. Table 1: Generative performance on class-conditional ImageNet-256. #Step indicates the number of model inference to generate one image. Time is the wall-time of generating one image in the steady state. Results with are taken from [21], while * denotes results obtained with more training. IS Model #Step #Para FID Rec Time Type Pre StyleGan-XL [30] 2.30 265. GAN Diff. Diff. Diff. Mask. AR AR AR AR AR AR AR ADM [5] LDM-4-G [28] DiT-L/2 [25] MaskGIT [2] VQGAN [6] ViTVQ [45] RQTran. [15] VAR-d16 [37] VAR-d20 [37] VAR-d24 [37] LlamaGen-L [35] Weak Baseline Weak Baseline Weak Baseline Weak Baseline VAR-skip-2 VAR-onestep* LlamaGen-skip-156 LlamaGen-onestep* DD1 DD1 DD1 DD1 DD1 DD1 DD1 DD DD2 (ours) DD2 (ours) DD2 (ours) DD2 (ours) DD2 (ours) DD2 (ours) VAR-d16 VAR-d16 VAR-d20 VAR-d20 VAR-d24 VAR-d24 LlamaGen-L LlamaGen-L VAR-d16 VAR-d20 VAR-d24 VAR-d24* LlamaGen-L LlamaGen-L* 10.94 3.60 5.02 101.0 247.7 167.2 6. 182.1 15.78 4.17 7.55 4.15 3.40 2.86 4.11 40.09 157.5 80.72 220.2 9.94 7.82 9.55 7.33 8.92 6.95 11.35 7.58 6.21 5.43 5.06 4.91 8.59 7. 74.3 175.1 134.0 278.7 305.1 312.9 283.5 56.8 12.13 193.6 197.0 197.2 204.5 202.8 222.5 193.6 237.5 213.0 233.7 254.7 282.2 229.1 238.7 0. 0.69 0.75 0.80 0.85 0.84 0.82 0.85 0.46 0.17 0.80 0.80 0.78 0.82 0.78 0.83 0.81 0. 0.84 0.85 0.85 0.87 0.77 0.77 0.53 0.63 0.57 0.51 0.41 0.47 0.51 0. 0.50 0.20 0.37 0.41 0.38 0.40 0.39 0.43 0.30 0.37 0.39 0.41 0.39 0.39 0.32 0.34 166M 554M 400M 458M 227M 1.4B 1.7B 3.8B 310M 600M 1.03B 343M 310M 343M 327M 327M 635M 635M 1.09B 1.09B 326M 326M 329M 619M 1.04B 1.04B 335M 335M 250 250 250 8 256 1024 68 10 10 10 256 8 1 100 1 1 2 1 2 1 2 1 1 1 1 1 1 1 0.3 168 31 0.5 24 >24 21 0.133 0.184 0.251 5. 0.098 1.95 0.021 0.036 0.027 0.047 0.034 0.059 0.023 0.043 0.019 (7.0) 0.023 (8.0) 0.031 (8.1) 0.031 (8.1) 0.021 (238) 0.021 (238) Table 2: Generation quality of involving the pre-trained AR model when sampling. The notation pre-trained-n-m means that the pre-trained AR model is used to re-generate the + 1-th to m-th tokens in the sequence generated in the first step by the few-step generator. Type AR DD1 DD1 DD1 Model VAR-d16 [37] VAR-d16-pre-trained-4-5 VAR-d16-pre-trained-3-5 VAR-d16-pre-trained-0-5 DD2 (ours) DD2 (ours) DD2 (ours) VAR-d16-pre-trained-8-10 VAR-d16-pre-trained-7-10 VAR-d16-pre-trained-5FID IS Pre Rec #Para #Step 4.19 6.54 5.47 5.03 5.24 4.88 4.47 230.2 210.8 230.5 242.8 238.9 248.7 277. 0.84 0.83 0.84 0.84 0.85 0.86 0.87 0.48 0.42 0.43 0.45 0.40 0.41 0. 310M 327M 327M 327M 329M 329M 329M 10 3 4 6 3 4 5.5 Ablation Study: the Importance of Initialization As discussed in Sec. 3.3, initialization is dispensable for DD2. We provide results on LlamaGen-L and VAR-d24 models to verify this, by only initializing one of the generator and guidance network with the tuned AR-diffusion model, while the other uses the backbone from the teacher AR model and randomly initialized output head. Results are shown in Tab. 4. We find that missing proper initialization for either of them can lead to significant performance degradation or even collapse, highlighting the importance of good initialization for both components."
        },
        {
            "title": "6 Discussions\n6.1 Distinction with diffusion score distillation\nIn this section, we discuss the differences between score distillation for diffusion models and DD2.",
            "content": "Fundamentally Different Task. Traditional score distillation methods aim to reduce the number of sampling steps for diffusion models. In contrast, our work focuses on one-step sampling for pre-trained AR models, which is fundamentally different generative paradigm from diffusion models. Despite the competitive or even superior performance of AR models compared to diffusion models, one-step sampling for AR models is under explored, which highlights the contribution of DD2. 9 Table 3: Training cost of DD2 and speed-up compared with DD1. All experiments are done on 8 NVIDIA A800 GPUs. Method Model Param Cost (8GPU h) Speed-up DD1 DD1 DD1 DD1 DD2 (ours) DD2 (ours) DD2 (ours) DD2 (ours) VAR-d16 VAR-d20 VAR-d24 LlamaGen-L VAR-d16 VAR-d20 VAR-d24 LlamaGen-L 327M 635M 1.09B 326M 329M 619M 1.04B 335M 296.9 484.4 604.2 647.7 115.5 174.4 96.1 52.6 1 1 1 1 2.6 2.8 6.3 12.3 Table 4: Impact of Initialization. FID-5k Model Param Gui-Init Gen-Init LlamaGen-L 335M LlamaGen-L 335M LlamaGen-L 335M VAR-d24 1.04B 14.77 16.08 21.76 11. VAR-d24 1.04B Collapse(>200) VAR-d24 1.04B Collapse(>200) Table 5: Perceptual path length of DD2 and DD1. DD1 DD PPL 18437.6 7231.9 Technical Adaptations. Directly applying standard score distillation to AR models is not feasible. We have made multiple technical innovations to tackle this problem: (1) we train the guidance network to learn the conditional score of the generator instead of the score, (2) we replace the classification layer in AR models with MLP head to ensure continuous output, and (3) we propose to adapting the pre-trained AR model into an AR-diffusion model as an initialization. We further replace the standard AR-diffusion loss with our GTS loss for better convergence of this process. Our strong results offer new perspective on training one-step generative models. Currently, the dominant strategy for training such models focuses on diffusion-based frameworks. In contrast, our method demonstrates that distilling an AR model is also highly competitive approach, as our results surpass many representative diffusion distillation techniques shown in Tab. 6. 6.2 Benefits of Eliminating Pre-defined Mapping in DD1 Compared to DD1, key feature of DD2 is that it does not rely on any pre-defined mapping, which brings several potential benefits: (1) More efficient utilization of model knowledge. In DD1, the pre-defined mapping provides only single end-to-end signal, whereas DD2 explicitly trains the model at every token position, offering more fine-grained supervisory signal. (2) Reduced accumulation of errors. In DD1, if the model fails to correctly learn the noise-to-data mapping at certain position, this error propagates to subsequent positions because their conditions depend on earlier predictions. In contrast, in DD2, the teacher model provides ground-truth distributions for each token position based on the generators current condition. Since the teacher model possesses strong generalization ability, the impact of imperfect conditions is greatly mitigated. (3) Smoother latent representations. More generally, training generative models without pre-defined mappings allows them to automatically discover smoother latent representations of the target data distribution, which benefits learning because smoother representations are easier to optimize. To quantify this property, we measure the Perceptual Path Length (PPL) metric [12] for both DD2 and DD1, where lower value indicates smoother interpolation in the latent space. As shown in Tab. 5, DD2 achieves significantly smoother latent interpolation than DD1."
        },
        {
            "title": "7 Future Works and Limitations",
            "content": "Compatibility with Image AR Models without VQ. In addition to the commonly used discretespace AR models based on VQ hidden space, continuous-space AR models [17] has recently gained increasing popularity. These models generate each token through diffusion process. Our method is naturally compatible with such models as well, since they directly provide the conditional score. We leave the application of our approach to this class of models as future work. Scaling to Larger Tasks. Image AR models have also been used in larger-scale tasks, such as text-to-image task [9, 47]. Extending our method to these models offers practical impact. Performance Gap to the Teacher Model. Although DD2 achieves significant speedup, the distilled models still exhibit certain performance gap compared to the original AR models. Addressing this performance drop to make one-step AR models match or even surpass the quality of pretrained AR models remains an important and promising direction for future research. Acknowledgement This work was supported by National Natural Science Foundation of China (62506197, No. 62325405, 62104128, U19B2019, U21B2031, 61832007, 62204164, 92364201), Tsinghua EE Xilinx AI Research Fund, and Beijing National Research Center for Information Science and Technology (BNRist). We would like to thank all anonymous reviewers for their suggestions. We also thank all the support from Infinigence-AI."
        },
        {
            "title": "References",
            "content": "[1] Tianle Cai, Yuhong Li, Zhengyang Geng, Hongwu Peng, Jason Lee, Deming Chen, and Tri Dao. Medusa: Simple llm inference acceleration framework with multiple decoding heads. arXiv preprint arXiv:2401.10774, 2024. [2] Huiwen Chang, Han Zhang, Lu Jiang, Ce Liu, and William Freeman. Maskgit: Masked generative image transformer. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1131511325, 2022. [3] Xi Chen, Nikhil Mishra, Mostafa Rohaninejad, and Pieter Abbeel. Pixelsnail: An improved In International conference on machine learning, pages autoregressive generative model. 864872. PMLR, 2018. [4] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: largescale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. [5] Prafulla Dhariwal and Alexander Nichol. Diffusion models beat gans on image synthesis. Advances in neural information processing systems, 34:87808794, 2021. [6] Patrick Esser, Robin Rombach, and Bjorn Ommer. Taming transformers for high-resolution image synthesis. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1287312883, 2021. [7] Kevin Frans, Danijar Hafner, Sergey Levine, and Pieter Abbeel. One step diffusion via shortcut models. arXiv preprint arXiv:2410.12557, 2024. [8] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM, 63(11):139144, 2020. [9] Jian Han, Jinlai Liu, Yi Jiang, Bin Yan, Yuqi Zhang, Zehuan Yuan, Bingyue Peng, and Xiaobing Liu. Infinity: Scaling bitwise autoregressive modeling for high-resolution image synthesis. arXiv preprint arXiv:2412.04431, 2024. [10] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. Advances in neural information processing systems, 33:68406851, 2020. [11] Doohyuk Jang, Sihwan Park, June Yong Yang, Yeonsung Jung, Jihun Yun, Souvik Kundu, Sung-Yub Kim, and Eunho Yang. Lantern: Accelerating visual autoregressive models with relaxed speculative decoding. arXiv preprint arXiv:2410.03355, 2024. [12] Tero Karras, Samuli Laine, and Timo Aila. style-based generator architecture for generative adversarial networks. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 44014410, 2019. [13] Dongjun Kim, Chieh-Hsin Lai, Wei-Hsiang Liao, Naoki Murata, Yuhta Takida, Toshimitsu Uesaka, Yutong He, Yuki Mitsufuji, and Stefano Ermon. Consistency trajectory models: Learning probability flow ode trajectory of diffusion. arXiv preprint arXiv:2310.02279, 2023. [14] Diederik Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114, 2013. [15] Doyup Lee, Chiheon Kim, Saehoon Kim, Minsu Cho, and Wook-Shin Han. Autoregressive image generation using residual quantization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1152311532, 2022. [16] Tianhong Li, Huiwen Chang, Shlok Mishra, Han Zhang, Dina Katabi, and Dilip Krishnan. Mage: Masked generative encoder to unify representation learning and image synthesis. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 21422152, 2023. [17] Tianhong Li, Yonglong Tian, He Li, Mingyang Deng, and Kaiming He. Autoregressive image generation without vector quantization. arXiv preprint arXiv:2406.11838, 2024. [18] Yuhui Li, Fangyun Wei, Chao Zhang, and Hongyang Zhang. Eagle: Speculative sampling requires rethinking feature uncertainty. arXiv preprint arXiv:2401.15077, 2024. [19] Zinan Lin, Kiran Thekumparampil, Giulia Fanti, and Sewoong Oh. Infogan-cr and modelcentrality: Self-supervised model training and selection for disentangling gans. In international conference on machine learning, pages 61276139. PMLR, 2020. [20] Yaron Lipman, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. [21] Enshu Liu, Xuefei Ning, Yu Wang, and Zinan Lin. Distilled decoding 1: One-step sampling of image auto-regressive models with flow matching. arXiv preprint arXiv:2412.17153, 2024. [22] Xingchao Liu, Chengyue Gong, and Qiang Liu. Flow straight and fast: Learning to generate and transfer data with rectified flow. arXiv preprint arXiv:2209.03003, 2022. [23] Eric Luhman and Troy Luhman. Knowledge distillation in iterative generative models for improved sampling speed. arXiv preprint arXiv:2101.02388, 2021. [24] Weijian Luo, Tianyang Hu, Shifeng Zhang, Jiacheng Sun, Zhenguo Li, and Zhihua Zhang. Diffinstruct: universal approach for transferring knowledge from pre-trained diffusion models. Advances in Neural Information Processing Systems, 36:7652576546, 2023. [25] William Peebles and Saining Xie. Scalable diffusion models with transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 41954205, 2023. [26] Ben Poole, Ajay Jain, Jonathan Barron, and Ben Mildenhall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv preprint arXiv:2209.14988, 2022. [27] Ali Razavi, Aaron Van den Oord, and Oriol Vinyals. Generating diverse high-fidelity images with vq-vae-2. Advances in neural information processing systems, 32, 2019. [28] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. Highresolution image synthesis with latent diffusion models. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 1068410695, 2022. [29] Tim Salimans and Jonathan Ho. Progressive distillation for fast sampling of diffusion models. arXiv preprint arXiv:2202.00512, 2022. [30] Axel Sauer, Katja Schwarz, and Andreas Geiger. Stylegan-xl: Scaling stylegan to large diverse datasets. In ACM SIGGRAPH 2022 conference proceedings, pages 110, 2022. [31] Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International conference on machine learning, pages 22562265. PMLR, 2015. [32] Yang Song and Prafulla Dhariwal. Improved techniques for training consistency models. arXiv preprint arXiv:2310.14189, 2023. [33] Yang Song, Prafulla Dhariwal, Mark Chen, and Ilya Sutskever. Consistency models. arXiv preprint arXiv:2303.01469, 2023. [34] Yang Song, Jascha Sohl-Dickstein, Diederik Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic differential equations. arXiv preprint arXiv:2011.13456, 2020. [35] Peize Sun, Yi Jiang, Shoufa Chen, Shilong Zhang, Bingyue Peng, Ping Luo, and Zehuan Yuan. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. [36] Yao Teng, Han Shi, Xian Liu, Xuefei Ning, Guohao Dai, Yu Wang, Zhenguo Li, and Xihui Liu. Accelerating auto-regressive text-to-image generation with training-free speculative jacobi decoding. arXiv preprint arXiv:2410.01699, 2024. 12 [37] Keyu Tian, Yi Jiang, Zehuan Yuan, Bingyue Peng, and Liwei Wang. Visual autoregressive modeling: Scalable image generation via next-scale prediction. arXiv preprint arXiv:2404.02905, 2024. [38] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, et al. Conditional image generation with pixelcnn decoders. Advances in neural information processing systems, 29, 2016. [39] Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems, 30, 2017. [40] Zhengyi Wang, Cheng Lu, Yikai Wang, Fan Bao, Chongxuan Li, Hang Su, and Jun Zhu. Prolificdreamer: High-fidelity and diverse text-to-3d generation with variational score distillation. Advances in Neural Information Processing Systems, 36:84068441, 2023. [41] Heming Xia, Tao Ge, Peiyi Wang, Si-Qing Chen, Furu Wei, and Zhifang Sui. Speculative decoding: Exploiting speculative execution for accelerating seq2seq generation. In Findings of the Association for Computational Linguistics: EMNLP 2023, pages 39093925, 2023. [42] Tianwei Yin, Michaël Gharbi, Taesung Park, Richard Zhang, Eli Shechtman, Fredo Durand, and Bill Freeman. Improved distribution matching distillation for fast image synthesis. Advances in neural information processing systems, 37:4745547487, 2024. [43] Tianwei Yin, Michaël Gharbi, Richard Zhang, Eli Shechtman, Fredo Durand, William Freeman, and Taesung Park. One-step diffusion with distribution matching distillation. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 66136623, 2024. [44] Tianwei Yin, Qiang Zhang, Richard Zhang, William Freeman, Fredo Durand, Eli Shechtman, and Xun Huang. From slow bidirectional to fast causal video generators. arXiv preprint arXiv:2412.07772, 2024. [45] Jiahui Yu, Xin Li, Jing Yu Koh, Han Zhang, Ruoming Pang, James Qin, Alexander Ku, Yuanzhong Xu, Jason Baldridge, and Yonghui Wu. Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627, 2021. [46] Mingtian Zhang, Jiajun He, Wenlin Chen, Zijing Ou, José Miguel Hernández-Lobato, Bernhard Schölkopf, and David Barber. Towards training one-step diffusion models without distillation. arXiv preprint arXiv:2502.08005, 2025. [47] Qian Zhang, Xiangzi Dai, Ninghua Yang, Xiang An, Ziyong Feng, and Xingyu Ren. Var-clip: Text-to-image generator with visual auto-regressive modeling. arXiv preprint arXiv:2408.01181, 2024. [48] Mingyuan Zhou, Huangjie Zheng, Zhendong Wang, Mingzhang Yin, and Hai Huang. Score identity distillation: Exponentially fast distillation of pretrained diffusion models for one-step generation. In Forty-first International Conference on Machine Learning, 2024."
        },
        {
            "title": "NeurIPS Paper Checklist",
            "content": "1. Claims Question: Do the main claims made in the abstract and introduction accurately reflect the papers contributions and scope? Answer: [Yes] Justification: The claims are supported by experimental results in Sec. 5. Guidelines: The answer NA means that the abstract and introduction do not include the claims made in the paper. 13 The abstract and/or introduction should clearly state the claims made, including the contributions made in the paper and important assumptions and limitations. No or NA answer to this question will not be perceived well by the reviewers. The claims made should match theoretical and experimental results, and reflect how much the results can be expected to generalize to other settings. It is fine to include aspirational goals as motivation as long as it is clear that these goals are not attained by the paper. 2. Limitations Question: Does the paper discuss the limitations of the work performed by the authors? Answer: [Yes] Justification: Limitations are discussed in Sec. 7 Guidelines: The answer NA means that the paper has no limitation while the answer No means that the paper has limitations, but those are not discussed in the paper. The authors are encouraged to create separate \"Limitations\" section in their paper. The paper should point out any strong assumptions and how robust the results are to violations of these assumptions (e.g., independence assumptions, noiseless settings, model well-specification, asymptotic approximations only holding locally). The authors should reflect on how these assumptions might be violated in practice and what the implications would be. The authors should reflect on the scope of the claims made, e.g., if the approach was only tested on few datasets or with few runs. In general, empirical results often depend on implicit assumptions, which should be articulated. The authors should reflect on the factors that influence the performance of the approach. For example, facial recognition algorithm may perform poorly when image resolution is low or images are taken in low lighting. Or speech-to-text system might not be used reliably to provide closed captions for online lectures because it fails to handle technical jargon. The authors should discuss the computational efficiency of the proposed algorithms and how they scale with dataset size. If applicable, the authors should discuss possible limitations of their approach to address problems of privacy and fairness. While the authors might fear that complete honesty about limitations might be used by reviewers as grounds for rejection, worse outcome might be that reviewers discover limitations that arent acknowledged in the paper. The authors should use their best judgment and recognize that individual actions in favor of transparency play an important role in developing norms that preserve the integrity of the community. Reviewers will be specifically instructed to not penalize honesty concerning limitations. 3. Theory assumptions and proofs Question: For each theoretical result, does the paper provide the full set of assumptions and complete (and correct) proof? Answer: [Yes] Justification: See Sec. A. Guidelines: The answer NA means that the paper does not include theoretical results. All the theorems, formulas, and proofs in the paper should be numbered and crossreferenced. All assumptions should be clearly stated or referenced in the statement of any theorems. The proofs can either appear in the main paper or the supplemental material, but if they appear in the supplemental material, the authors are encouraged to provide short proof sketch to provide intuition. Inversely, any informal proof provided in the core of the paper should be complemented by formal proofs provided in appendix or supplemental material. 14 Theorems and Lemmas that the proof relies upon should be properly referenced. 4. Experimental result reproducibility Question: Does the paper fully disclose all the information needed to reproduce the main experimental results of the paper to the extent that it affects the main claims and/or conclusions of the paper (regardless of whether the code and data are provided or not)? Answer: [Yes] Justification: See Sec. 3, Sec. and Sec. E. Guidelines: The answer NA means that the paper does not include experiments. If the paper includes experiments, No answer to this question will not be perceived well by the reviewers: Making the paper reproducible is important, regardless of whether the code and data are provided or not. If the contribution is dataset and/or model, the authors should describe the steps taken to make their results reproducible or verifiable. Depending on the contribution, reproducibility can be accomplished in various ways. For example, if the contribution is novel architecture, describing the architecture fully might suffice, or if the contribution is specific model and empirical evaluation, it may be necessary to either make it possible for others to replicate the model with the same dataset, or provide access to the model. In general. releasing code and data is often one good way to accomplish this, but reproducibility can also be provided via detailed instructions for how to replicate the results, access to hosted model (e.g., in the case of large language model), releasing of model checkpoint, or other means that are appropriate to the research performed. While NeurIPS does not require releasing code, the conference does require all submissions to provide some reasonable avenue for reproducibility, which may depend on the nature of the contribution. For example (a) If the contribution is primarily new algorithm, the paper should make it clear how to reproduce that algorithm. (b) If the contribution is primarily new model architecture, the paper should describe the architecture clearly and fully. (c) If the contribution is new model (e.g., large language model), then there should either be way to access this model for reproducing the results or way to reproduce the model (e.g., with an open-source dataset or instructions for how to construct the dataset). (d) We recognize that reproducibility may be tricky in some cases, in which case authors are welcome to describe the particular way they provide for reproducibility. In the case of closed-source models, it may be that access to the model is limited in some way (e.g., to registered users), but it should be possible for other researchers to have some path to reproducing or verifying the results. 5. Open access to data and code Question: Does the paper provide open access to the data and code, with sufficient instructions to faithfully reproduce the main experimental results, as described in supplemental material? Answer: [No] Justification: We have not released our code yet. Guidelines: The answer NA means that paper does not include experiments requiring code. Please see the NeurIPS code and data submission guidelines (https://nips.cc/ public/guides/CodeSubmissionPolicy) for more details. While we encourage the release of code and data, we understand that this might not be possible, so No is an acceptable answer. Papers cannot be rejected simply for not including code, unless this is central to the contribution (e.g., for new open-source benchmark). 15 The instructions should contain the exact command and environment needed to run to reproduce the results. See the NeurIPS code and data submission guidelines (https: //nips.cc/public/guides/CodeSubmissionPolicy) for more details. The authors should provide instructions on data access and preparation, including how to access the raw data, preprocessed data, intermediate data, and generated data, etc. The authors should provide scripts to reproduce all experimental results for the new proposed method and baselines. If only subset of experiments are reproducible, they should state which ones are omitted from the script and why. At submission time, to preserve anonymity, the authors should release anonymized versions (if applicable). Providing as much information as possible in supplemental material (appended to the paper) is recommended, but including URLs to data and code is permitted. 6. Experimental setting/details Question: Does the paper specify all the training and test details (e.g., data splits, hyperparameters, how they were chosen, type of optimizer, etc.) necessary to understand the results? Answer: [Yes] Justification: See Sec. 3, Sec. and Sec. E. Guidelines: The answer NA means that the paper does not include experiments. The experimental setting should be presented in the core of the paper to level of detail that is necessary to appreciate the results and make sense of them. The full details can be provided either with the code, in appendix, or as supplemental material. 7. Experiment statistical significance Question: Does the paper report error bars suitably and correctly defined or other appropriate information about the statistical significance of the experiments? Answer: [No] Justification: Some of our baselines are taken from previous papers, which do not contain error bars. Besides, we believe that the metrics used in our paper are not significantly affected by randomness. Guidelines: The answer NA means that the paper does not include experiments. The authors should answer \"Yes\" if the results are accompanied by error bars, confidence intervals, or statistical significance tests, at least for the experiments that support the main claims of the paper. The factors of variability that the error bars are capturing should be clearly stated (for example, train/test split, initialization, random drawing of some parameter, or overall run with given experimental conditions). The method for calculating the error bars should be explained (closed form formula, call to library function, bootstrap, etc.) The assumptions made should be given (e.g., Normally distributed errors). It should be clear whether the error bar is the standard deviation or the standard error of the mean. It is OK to report 1-sigma error bars, but one should state it. The authors should preferably report 2-sigma error bar than state that they have 96% CI, if the hypothesis of Normality of errors is not verified. For asymmetric distributions, the authors should be careful not to show in tables or figures symmetric error bars that would yield results that are out of range (e.g. negative error rates). If error bars are reported in tables or plots, The authors should explain in the text how they were calculated and reference the corresponding figures or tables in the text. 16 8. Experiments compute resources Question: For each experiment, does the paper provide sufficient information on the computer resources (type of compute workers, memory, time of execution) needed to reproduce the experiments? Answer: [Yes] Justification: Please see Tab. 1, Tab. 3 for the computational cost. Guidelines: The answer NA means that the paper does not include experiments. The paper should indicate the type of compute workers CPU or GPU, internal cluster, or cloud provider, including relevant memory and storage. The paper should provide the amount of compute required for each of the individual experimental runs as well as estimate the total compute. The paper should disclose whether the full research project required more compute than the experiments reported in the paper (e.g., preliminary or failed experiments that didnt make it into the paper). 9. Code of ethics Question: Does the research conducted in the paper conform, in every respect, with the NeurIPS Code of Ethics https://neurips.cc/public/EthicsGuidelines? Answer: [Yes] Justification: We followed the NeurIPS Code of Ethics. Guidelines: The answer NA means that the authors have not reviewed the NeurIPS Code of Ethics. If the authors answer No, they should explain the special circumstances that require deviation from the Code of Ethics. The authors should make sure to preserve anonymity (e.g., if there is special consideration due to laws or regulations in their jurisdiction). 10. Broader impacts Question: Does the paper discuss both potential positive societal impacts and negative societal impacts of the work performed? Answer: [NA] . Justification: Our method is an acceleration method for AR models, which is not related to any domains that may have societal impacts. Guidelines: The answer NA means that there is no societal impact of the work performed. If the authors answer NA or No, they should explain why their work has no societal impact or why the paper does not address societal impact. Examples of negative societal impacts include potential malicious or unintended uses (e.g., disinformation, generating fake profiles, surveillance), fairness considerations (e.g., deployment of technologies that could make decisions that unfairly impact specific groups), privacy considerations, and security considerations. The conference expects that many papers will be foundational research and not tied to particular applications, let alone deployments. However, if there is direct path to any negative applications, the authors should point it out. For example, it is legitimate to point out that an improvement in the quality of generative models could be used to generate deepfakes for disinformation. On the other hand, it is not needed to point out that generic algorithm for optimizing neural networks could enable people to train models that generate Deepfakes faster. The authors should consider possible harms that could arise when the technology is being used as intended and functioning correctly, harms that could arise when the technology is being used as intended but gives incorrect results, and harms following from (intentional or unintentional) misuse of the technology. 17 If there are negative societal impacts, the authors could also discuss possible mitigation strategies (e.g., gated release of models, providing defenses in addition to attacks, mechanisms for monitoring misuse, mechanisms to monitor how system learns from feedback over time, improving the efficiency and accessibility of ML). 11. Safeguards Question: Does the paper describe safeguards that have been put in place for responsible release of data or models that have high risk for misuse (e.g., pretrained language models, image generators, or scraped datasets)? Answer: [NA] Justification: The paper poses no risks. Guidelines: The answer NA means that the paper poses no such risks. Released models that have high risk for misuse or dual-use should be released with necessary safeguards to allow for controlled use of the model, for example by requiring that users adhere to usage guidelines or restrictions to access the model or implementing safety filters. Datasets that have been scraped from the Internet could pose safety risks. The authors should describe how they avoided releasing unsafe images. We recognize that providing effective safeguards is challenging, and many papers do not require this, but we encourage authors to take this into account and make best faith effort. 12. Licenses for existing assets Question: Are the creators or original owners of assets (e.g., code, data, models), used in the paper, properly credited and are the license and terms of use explicitly mentioned and properly respected? Answer: [Yes] Justification: All models and datasets are properly cited and discussed in Sec. 5. Guidelines: The answer NA means that the paper does not use existing assets. The authors should cite the original paper that produced the code package or dataset. The authors should state which version of the asset is used and, if possible, include URL. The name of the license (e.g., CC-BY 4.0) should be included for each asset. For scraped data from particular source (e.g., website), the copyright and terms of service of that source should be provided. If assets are released, the license, copyright information, and terms of use in the package should be provided. For popular datasets, paperswithcode.com/datasets has curated licenses for some datasets. Their licensing guide can help determine the license of dataset. For existing datasets that are re-packaged, both the original license and the license of the derived asset (if it has changed) should be provided. If this information is not available online, the authors are encouraged to reach out to the assets creators. 13. New assets Question: Are new assets introduced in the paper well documented and is the documentation provided alongside the assets? Answer: [NA] Justification: We have not released any assets yet. Guidelines: The answer NA means that the paper does not release new assets. 18 Researchers should communicate the details of the dataset/code/model as part of their submissions via structured templates. This includes details about training, license, limitations, etc. The paper should discuss whether and how consent was obtained from people whose asset is used. At submission time, remember to anonymize your assets (if applicable). You can either create an anonymized URL or include an anonymized zip file. 14. Crowdsourcing and research with human subjects Question: For crowdsourcing experiments and research with human subjects, does the paper include the full text of instructions given to participants and screenshots, if applicable, as well as details about compensation (if any)? Answer: [NA] Justification: The paper does not involve crowdsourcing nor research with human subjects Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Including this information in the supplemental material is fine, but if the main contribution of the paper involves human subjects, then as much detail as possible should be included in the main paper. According to the NeurIPS Code of Ethics, workers involved in data collection, curation, or other labor should be paid at least the minimum wage in the country of the data collector. 15. Institutional review board (IRB) approvals or equivalent for research with human subjects Question: Does the paper describe potential risks incurred by study participants, whether such risks were disclosed to the subjects, and whether Institutional Review Board (IRB) approvals (or an equivalent approval/review based on the requirements of your country or institution) were obtained? Answer: [NA] . Justification: The paper does not involve crowdsourcing nor research with human subjects. Guidelines: The answer NA means that the paper does not involve crowdsourcing nor research with human subjects. Depending on the country in which research is conducted, IRB approval (or equivalent) may be required for any human subjects research. If you obtained IRB approval, you should clearly state this in the paper. We recognize that the procedures for this may vary significantly between institutions and locations, and we expect authors to adhere to the NeurIPS Code of Ethics and the guidelines for their institution. For initial submissions, do not include any information that would break anonymity (if applicable), such as the institution conducting the review. 16. Declaration of LLM usage Question: Does the paper describe the usage of LLMs if it is an important, original, or non-standard component of the core methods in this research? Note that if the LLM is used only for writing, editing, or formatting purposes and does not impact the core methodology, scientific rigorousness, or originality of the research, declaration is not required. Answer: [NA] Justification: The core method development in this research does not involve LLMs as any important, original, or non-standard components. Guidelines: The answer NA means that the core method development in this research does not involve LLMs as any important, original, or non-standard components. 19 Please refer to our LLM policy (https://neurips.cc/Conferences/2025/LLM) for what should or should not be described. 20 Discussion of Prop. 1 In this section, we provide the proof of Prop. 1 using simple induction. Proof. Assuming the neural network has sufficiently large capacity, minimizing LCSD is then equivalent to minimizing each individual term LCSDi = d(sΦ(qti , tisg(q<i))) for any i. , tisg(q<i)), sf ake(qti Base case (i = 1): At 1 , t1), sf ake(qt1 d(sΦ(qt1 this term guarantees pθ(q1) = pΦ(q1). the first position = 1, the corresponding term LCSD1 = 1 , t1)) degrades to traditional score distillation loss, so that minimizing Inductive Hypothesis (i = 1): Assume that for token position = k, the generator correctly models the distribution of all tokens before this position: pθ(q<k) = pΦ(q<k). Inductive Step (i = k) Minimizing LCSDk = d(sΦ(qtk guarantees pθ(qkq<k) = pΦ(qkq<k), pΦ(qkq<k)pΦ(q<k) = pΦ(q<k+1) holds , tksg(q<k)), sf ake(qtk , tksg(q<k))) so that pθ(q<k+1) = pθ(qkq<k)pθ(q<k) = Conclusion: By mathematical pΦ(q1, . . . , qn). induction, minimizing LCSD guarantees pθ(q1, . . . , qn) ="
        },
        {
            "title": "B More Experimental Details",
            "content": "In this section, we provide some additional experimental settings and results. B.1 Comparison with Diffusion Distillation Methods We compare DD2 with several commonly used diffusion distillation methods to show our effectiveness. Results are shown in Tab. 6. Table 6: Comparison between DD2 and diffusion distillation methods. Results are taken from the paper of Shortcut model [7]. DD2-VAR-d16 DD2-VAR-d24 DD2-VAR-d Shortcut[7] Reflow[22] CD[33] PD[29] CT[33] FID 6.21 5.43 4.91 35.6 136.5 69. 44.8 10.6 B.2 Training Curve of the Original DMD Method We demonstrate our reproduced FID-iteration curve of the original DMD method with different initialization setting in Fig. 6. As discussed in Sec. 3.3, an inappropriate score distillation initialization strategy can lead to slower convergence and bad training stability, which motivates us to use the AR-diffusion model for initialization. B.3 Performance of the AR-diffusion Model for Initialization Table 7: Performance of AR-diffusion models. FIDs are evaluated wit 5k generated images. Model Param FID-5k Training Iter Loss GTS GTS GTS GTS VAR-d16 329M 11.41 VAR-d20 619M 11.41 VAR-d 1.04B 11.24 LlamaGen-L 335M 15.66 diffusion VAR-d 329M 17.98 21 330k 330k 230k 100k 500k Figure 6: Training results for different initialization strategies. default indicates that both the generator and the guidance components are initialized using the teacher model [43], random_init_gen_last_layer refers to the setting where only the last layer of the generator is randomly initialized, while the rest of the generator and the guidance are initialized from the teacher model. random_init_gen means the entire generator is randomly initialized, whereas the guidance is initialized from the teacher model. random_init_gui denotes the case where only the guidance is randomly initialized, with the generator initialized from the teacher model. Table 8: Wall time (hour) of each training stage under different settings. All time costs are profiled on 8 NVIDIA A800 GPUs. Model VAR-d16 VAR-d20 VAR-d24 LlamaGen-L Continuous Adaptation AR-diffusion Tuning (Head Only) AR-diffusion Tuning (Full Model) Main Training Performance Alignment - 60.1 8.0 42.6 4.8 - 72.6 9.1 88.0 4.7 Overall 115.5 174. - 59.9 17.2 19.0 - 96.1 3.3 - 8.6 40.7 - 52.6 We report the performance of the tuned AR-diffusion Model in Tab. 7, which serves as the initialization of both generator and guidance network. We use 10-step Euler solver for the sampling process of every token. To verify the effectiveness of the proposed GTS loss Eq. (6), we also report the results of using traditional diffusion loss where the target is Monte Carol of the ground truth score. The key takeaways are: (1) the tuned AR-diffusion model demonstrates strong sample quality, making it suitable choice for initialization, and (2) performance degrades significantly if we use traditional diffusion loss, highlighting the effectiveness and necessity of our GTS loss. B.4 Training Details and Cost The training mainly cost consists of two parts: AR-diffusion tuning and the main training process with CSD loss. In the first stage, we use the same setting as in evaluation to generate the training data. Specifically, for all VAR models, we apply top_k as 900 and classifier-free-guidance scale as 2.0; for LlamaGen model we use top_k as 8000 and classifier-free-guidance scale as 2.0. 22 Table 9: Converged performance before/after using performance alignment operation. IS Model #Step #Para FID Rec Type Pre DD2 (Before) DD2 (Before) DD2 (After) DD2 (After) VAR-d16 VAR-d20 VAR-d16 VAR-d20 8.35 6.57 6.21 5.43 176.7 201.4 213.0 233.7 0.80 0.81 0.84 0.85 0.42 0.43 0.39 0.41 329M 618M 329M 618M 1 1 1 1 For VAR models, in the first stage, we first fine-tune only the output head, and subsequently train the entire model. For the second part, we apply performance alignment to d16 and d20 models, which takes additional cost of training the guidance network. We list the cost of each part in Tab. 8. For LlamaGen model, there is an additional stage where we fine-tune the teacher model to support continuous input. For AR-diffusion tuning, we directly train the entire model. We dont apply performance alignment. Detailed results are listed in Tab. 8. B.5 The Effectiveness of Performance Alignment We provide the convergence performance of the d16 and d20 models before and after performing the performance alignment procedure in Tab. 9, demonstrating the effectiveness of this operation."
        },
        {
            "title": "C Implementation Techniques",
            "content": "In this section, we introduce several techniques we use in our pipeline. C.1 Computing Score Function with Probability Vector In this section, we derive Eq. (1) starting from the preliminaries of flow matching. Flow matching [20, 22] defines an invertible transformation with ordinary differential equation (ODE) dx = (xt, t)dt between two distributions π0(x) and π1(x). The velocity function under linear noise schedule xt = (1 t)x0 + tx1 can be given as: (xt, t) = Ex0,x1π0,1(x0,x1)(x1 x0(1 t)x0 + tx1 = xt), (7) where π0,1(x0, x1) is any joint distribution that satisfies temporal boundary conditions at both ends: (cid:82) π0,1(x0, x1)dx0 = π1(x1) and (cid:82) π0,1(x0, x1)dx1 = π0(x0). Since the source distribution π1(x) is Gaussian distribution here, the relationship between the score function s(xt, t) and the velocity (xt, t) is as follows: v(xt, t) = ts(xt, t) + xt 1 And the score function can be given as: s(xt, t) = Ex0,x1π0,1(x0,x1)( x1 (1 t)x0 + tx1 = xt) (8) (9) In our problem, the target distribution is weighted sum of Dirac functions: π0(x) = (cid:80)V j=1 pjδ(x cj), and is independent of the source distribution, resulting in only finite number of possibilities. Therefore, we only need compute the product of the source distribution probability and the target distribution probability for each possible case, and then use this as the weight function to compute the expectation of x1 . With the above explanations, we can easily arrive at Eq. (1). C.2 Multiple Noisy Samples for Fake Conditional Score Learning As discussed in Sec. 3.2.2, the output of the guidance network sψ(qti ,i q<i) is computed in two stages. First, transformer backbone process the input sequence (q1, . . . , qn) and outputs feature sequence (f1, . . . , fn), where each fi is causally conditioned on q<i. Then, lightweight MLP takes 23 , tiq<i). , timestep ti and the feature fi as input, then outputs the estimated conditional the noisy token qti score sψ(qti Every conditioning fi defines continuous distribution over noisy inputs (xt, t), and the model must learn to predict score function across the entire space and all timesteps. To ensure sufficient training and improve generalization, we draw inspiration from the MAR implementation (see https: //github.com/LTH14/mar for more details) and apply multi-sample training strategy. Specifically, for generated sequence (q1, . . . , qn), we sample multiple noise sequences (ϵ1, . . . , ϵn)1,...,m to create multiple noisy versions of the generated sequence. For each noisy sequence, we apply Eq. (5) as the loss function and take the average across all samples. The resulting training objective is: LF CS = E(q1,...,qn)pθ (cid:88) (cid:88) j=1 i=1 sψ(qti,j , ti,jq<i) log p(qti,j qi)2, ti,j (10) where ti,j is the j-th sample at the i-th token position, qti,j denotes the j-th noise sample at the i-th token position. = (1 ti,j)qi + ti,jϵi,j, with ϵ (0; I) C.3 Performance Alignment for both Generator and Guidance network We find there are two issues during training: (1) there is discrepancy between the guidance network and the generators score, and (2) unstable training dynamics. Specifically, we observe that the samples generated by the conditional guidance network via AR-diffusion tend to under-perform those generated by the generator, indicating training gap of the guidance network. Additionally, we notice that the generators FID fluctuates significantly during training. However, applying the Exponential Moving Average (EMA) technique to the generator leads to much more stable performance curve. These findings motivate us to introduce performance alignment procedure for both generator and guidance network after an initial phase of training. Specifically, (1) for the generator, we replace the regular model weights directly with EMA weights, then (2) we fix the generator and only train the conditional guidance network for certain period to adapt it to the new generator distribution. Once this alignment process is complete, we resume the standard training process. We empirically found that this technique is particularly helpful for training VAR-d16 and VAR-d20 models, significantly improving performance even after the models have already converged. Results are shown at Tab. 9. C.4 Larger Update Frequency for the Guidance Network Training the guidance network is crucial, as it is responsible for producing accurate conditional scores of the generator distribution. However, this task is challenging because the generator distribution is also evolving during training. To address this issue, we adopt higher update frequency for the guidance network, following the strategy used in DMD2 [42]. Specifically, in each training iteration, we update the guidance network times with (K > 1) while updating the generator only once. The specific values of used for different models are provided in Sec. E. C.5 Details of Model Architectures We use the same architecture for both generator and guidance network. Inspired by MAR [17], our model architecture consists of transformer backbone and lightweight MLP head. As discussed in Sec. 3, for the guidance network, the transformer backbone takes the token sequence as input and output causal feature sequence, while the MLP head takes the feature, noisy token and timestep as input and output the predicted conditional score. For the generator, the transformer backbone takes the shifted noise sequence as input, while the MLP head takes the noise sequence and the feature sequence as input and give the final generated sequence. We demonstrate the model architectures in Fig. 7. Algorithms of Multi-step Sampling Method In this section, we present the pseudo algorithm of the multi-step sampling method in Sec. 3. Suppose we have sequence = (q1, . . . , qn). We denote the indexing operations X[t] = qt and X[: t] = (q1, . . . , qt1). The pseudo algorithm is presented in Alg. 4, with results reported in Tab. 2. Figure 7: Demonstration of the model architectures and the corresponding inputs/outputs. Table 10: Hyperparameters used for AR-diffusion model tuning in the initialization phase. Actual BS/iter refers to the actual batch size used in each training iteration. The training iterations for VAR are reported in the format of only head + full model to reflect the two-phase training procedure. Hyperparameter Learning Rate Batch Size Grad Accumulation Actual BS/iter Adam β0 Adam β1 Training Iterations VAR-d16 2e-4 512 4 128 0.9 0.95 300k+30k VAR-d20 2e-4 512 8 64 0.9 0.95 300k+30k VAR-d24 2e-4 512 16 32 0.9 0.95 200k+30k LlamaGen-L 1e-4 512 4 128 0.9 0.95 100k Algorithm 4: Sampling with the teacher AR model Require: : The distilled one-step model θ, the pre-trained AR model Φ, total sampling steps > 1. Sampling Process 1: = (q1, . . . , qn) one-step sampling by θ 2: for in {n + 2, . . . , n} do Sample pΦ(X[: t]) 3: 4: X[t] 5: end for 6: return X"
        },
        {
            "title": "E Detailed Experimental Settings",
            "content": "In this section, we present the settings of our training process. E.1 Model Parameterization In this work, we parameterize the network as velocity prediction network, due to its widely verified good properties. Specifically, we use v(xt, t) = σts(xt,t)xt to convert between velocity and score. We use the velocity function to train the AR-diffusion model and the guidance network. For the generator, we use xθ = ϵ vθ as its final output, where ϵ is the input noise and vθ is the model output. αt 25 Table 11: Hyperparameters used for DD2. Actual BS/iter refers to the actual batch size used in each training iteration. \"Gen\" and \"Gui\" stand for the generator and guidance network, respectively. The training iterations of the generator for VAR-d16 and VAR-d20 are reported in the format of \"before performance alignment + after performance alignment\", while the training iterations of the guidance network for VAR-d16 and VAR-d20 are reported in the format of \"before performance alignment + alignment + after performance alignment\". Hyperparameter VAR-d16 VAR-d20 Start Learning rate End Learning rate Batch size Grad Accumulation Actual BS/Iter Adam β0 Adam β1 Training iterations Gui ψ 1e-6 2e-4 512 4 128 0.9 0.95 40k+8k+15k Gen θ 1e-6 2e-4 512 4 128 0.9 0.95 8k+7.5k Gui ψ 1e-6 2e-4 512 8 64 0.9 0.95 80k+8k+36k Gen θ 1e-6 2e-4 512 8 64 0.9 0.95 16k+18k VAR-d24 LlamaGen-L Gui ψ Gen θ Gui ψ Gen θ 1e-6 4e-4 1024 32 32 0.9 0.95 30k 1e-6 5e-5 1024 32 32 0.9 0.95 15k 1e-6 2e-4 1024 8 128 0.9 0.95 60k 1e-6 1e-4 1024 8 128 0.9 0.95 12k Guidance Update Freq 5(Before Align), 2(After Align) 5(Before Align), 2(After Align) 2 5 E.2 Stage 1: AR-diffusion Tuning For VAR models, we first freeze the transformer backbone and tune the output MLP for certain iterations. Then we remove the constraints and tune all parameters. For LlamaGen model, we directly tune all parameters from the start. Settings are listed in Tab. 10. E.3 Stage 2: Training with CSD Loss Calculation of the Real Conditional Score We follow the default sampling settings of original AR models for probability vector calculation. For VAR models, we set classifier-free guidance scale to 2.0, top-k to 900 and top-p to 0.95. For LlamaGen model, we set classifier-free guidance scale to 2.0, top-k to 900 and top-p to 1.0. Optimization Settings We list the optimization settings in Tab. 11. For the learning rate of the generator, we apply linear warm up strategy from the start learning rate to the end learning rate in 40K guidance network training iterations for VAR-d16, VAR-d20 and LlamaGen-L models. For VAR-d24, we set the warm up length as 20K. EMA We find that EMA is very important for the stability of training. Since the model performs badly at the beginning of the training process, we use progressive EMA rate. Specifically, we use small EMA rate in the early stage of training. Then we use dynamic EMA rate min(0.9999, (iter + 1)/(iter + 10)), where iter is the training iteration. This progressive EMA schedule ensures both training stability and fast convergence. E.4 Continuous Input Adaptation for LlamaGen Models For VAR teacher models, we directly conduct our workflow since they naturally support continuous embeddings as input. However, for LlamaGen teacher model, we need to modify the models input head to accept continuous latent embeddings instead of discrete token indices. This allows the model to handle the continuous outputs from the generator. Specifically, we replace the original nn.embedding layer emb with MLP mlp. We first train this MLP with loss (cid:80)V i=1 mlp(ci) emb(i)2, where is the token and is the total number of tokens in the codebook. This process is fast, but incurs performance loss. Then we fine-tune the whole model with standard AR loss implemented by LlamaGen for 200K iterations, to align its performance with the original model. Finally, we obtained an AR LlamaGen model that incurs no performance loss and supports continuous embeddings as input, which we use as the teacher model. E.5 Baselines Set Prediction Method. Set prediction is commonly used technique for reducing the sampling steps of Image AR models [2, 37, 17]. However, it is fundamentally incapable of reducing the sampling process to single step. As pointed out in the DD1 paper [21], when training model under the one-step sampling setting with this approach, the optimal solution is equivalent to independently sampling each token according to the overall token frequency at this position in the dataset. Therefore, 26 we can directly evaluate its one-step generation performance without actually training model. This method completely ignores the dependencies between tokens, which leads to the failure case reported in the onestep* rows of Tab. 1. Pre-trained AR Models. For VAR models, we set top_k, top_p and classifier-free-guidance scale as 900, 0.95 and 1.5, respectively. For LlamaGen models, we use 8000 as top_k, 1.0 as top_p, and 2.0 as classifier-free-guidance scale"
        },
        {
            "title": "F Visualizations",
            "content": "We show some generated examples in Figs. 8 to 11. Figure 8: Generated by DD2-VAR-d16 model. 27 Figure 9: Generated by DD2-VAR-d20 model. Figure 10: Generated by DD2-VAR-d24 model. Figure 11: Generated by DD2-LlamaGen-L model."
        }
    ],
    "affiliations": [
        "Infinigence-AI Beijing, China",
        "Microsoft Research Redmond, WA, USA",
        "Shanghai Jiaotong University Shanghai, China",
        "Tsinghua University Beijing, China"
    ]
}