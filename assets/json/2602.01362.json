{
    "paper_title": "Balancing Understanding and Generation in Discrete Diffusion Models",
    "authors": [
        "Yue Liu",
        "Yuzhong Zhao",
        "Zheyong Xie",
        "Qixiang Ye",
        "Jianbin Jiao",
        "Yao Hu",
        "Shaosheng Cao",
        "Yunfan Liu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong few-step generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via a stationary noise kernel. XDLM offers two key contributions: (1) it provides a principled theoretical unification of MDLM and UDLM, recovering each paradigm as a special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLM's superior potential for long-term scaling. Code is available at https://github.com/MzeroMiko/XDLM"
        },
        {
            "title": "Start",
            "content": "Yue Liu 1 Yuzhong Zhao 1 Zheyong Xie 2 Qixiang Ye 1 Jianbin Jiao 1 Yao Hu 2 Shaosheng Cao 2 Yunfan Liu 1 6 2 0 2 1 ] . [ 1 2 6 3 1 0 . 2 0 6 2 : r a"
        },
        {
            "title": "Abstract",
            "content": "In discrete generative modeling, two dominant paradigms demonstrate divergent capabilities: Masked Diffusion Language Models (MDLM) excel at semantic understanding and zero-shot generalization, whereas Uniform-noise Diffusion Language Models (UDLM) achieve strong fewstep generation quality, yet neither attains balanced performance across both dimensions. To address this, we propose XDLM, which bridges the two paradigms via stationary noise kernel. XDLM offers two key contributions: (1) it provides principled theoretical unification of MDLM and UDLM, recovering each paradigm as special case; and (2) an alleviated memory bottleneck enabled by an algebraic simplification of the posterior probabilities. Experiments demonstrate that XDLM advances the Pareto frontier between understanding capability and generation quality. Quantitatively, XDLM surpasses UDLM by 5.4 points on zero-shot text benchmarks and outperforms MDLM in few-step image generation (FID 54.1 vs. 80.8). When scaled to tune an 8B-parameter large language model, XDLM achieves 15.0 MBPP in just 32 steps, effectively doubling the baseline performance. Finally, analysis of training dynamics reveals XDLMs superior potential for long-term scaling. Code is available at this http URL. 1. Introduction Diffusion models have achieved remarkable success in continuous domains, particularly in image and audio generation (Ho et al., 2020; Dhariwal & Nichol, 2021; Rombach et al., 2022; Kong et al., 2020). Inspired by this potential, Discrete Denoising Diffusion Probabilistic Model (D3PM) has emerged as promising paradigm shift to the discrete state space (Austin et al., 2021). Notably, these Discrete 1UCAS 2Xiaohongshu Inc.. Yunfan <caoshaosheng@xiaohongshu.com>. <liuyunfan@ucas.ac.cn>, Liu Preprint. February 3, 2026. Correspondence to: Shaosheng Cao Diffusion Models (DDMs) are now demonstrating strong performance in language modeling, field long dominated by auto-regressive (AR) architectures. Within this landscape, DDM studies have diverged into two distinct branches: Masked Diffusion Language Models (MDLMs) (Sahoo et al., 2024) and Uniform-noise Diffusion Language Models (UDLMs) (Schiff et al., 2024). While MDLMs achieve superior performance in likelihood modeling and zero-shot generalization, they often struggle to generate coherent, contextually consistent outputs with limited inference steps. Conversely, UDLMs excel at low-step generation but frequently lag behind MDLMs when the number of inference steps increases. Despite their individual merits, neither achieves balanced performance across these dimensions. This imbalance carries significant practical implications. In domains like image synthesis, the required inference steps are typically far fewer than the total number of patches. In these few-step regimes, UDLM (and its hidden Gaussian counterparts (Schiff et al., 2024; Sahoo et al., 2025)) consistently outperforms the masked paradigm. Concretely, our experiments demonstrate that UDLM outperforms MDLM by 17.6% when generating ImageNet-1K images in an 8step regime. Consequently, balancing the robust few-step generation of UDLMs with the superior semantic understanding of MDLMs remains critical open challenge. To this end, we propose miXed Diffusion Language Modeling (XDLM), balanced theoretical formulation that establishes rigorous bridge between uniform and masked noise distributions. Unlike GIDD (von Rütte et al., 2025), which relies on computationally expensive time-inhomogeneous process where noise characteristics shift at every timestep, XDLM enforces stationary noise kernel where incremental noise structurally matches the marginal noise, as shown in Fig. 1 (left). This consistency allows our training objective to be efficiently factorized into static constants and dynamic schedules, avoiding the complex re-computation of noise distributions required by GIDD. In addition, we prove that MDLM and UDLM are limiting cases of our formulation. Furthermore, XDLM enables memory-efficient implementation by algebraically simplifying posterior calculations, allowing XDLM to scale to large vocabulary sizes without prohibitive computational costs. Balancing Understanding and Generation in Discrete Diffusion Models Figure 1. Left: XDLM combines the noise kernel of UDLM (u) and MDLM (m) to achieve favorable trade-off between the two methods. [NORMAL] denotes normal tokens, while [MASK] represents the mask token. Right: The trade-off between understanding capability (zero-shot perplexity; lower is better) and generation capability (generation perplexity in 32 sampling steps; lower is better). The proposed XDLM with mixing ratio of = 0.1 achieves the optimal balance, labeled as the Sweet Spot. Experimental results demonstrate that XDLM consistently achieves superior performance across diverse modalities, validating the efficacy of our balanced discrete diffusion approach. As illustrated in Fig. 1 (right), XDLM advances the Pareto frontier between understanding capability and generative quality. Notably, as the interpolation parameters shift toward their extremes, XDLM matches the performance of UDLM and MDLM respectively, confirming the models theoretical foundation. Concretely, in zero-shot language benchmarks, XDLM surpasses UDLM by 5.4 points in averaged metrics and trails MDLM by only 0.45. In conditional image generation, XDLM excels in both efficiency and quality: it significantly outperforms MDLM in few-step generation (reducing FID from 80.8 to 54.1 at 4 steps) and ranks first at 16 steps, surpassing UDLM by 0.4 points. Furthermore, large-scale continual pretraining on 8B-parameter LLMs (Nie et al., 2025) yields an MBPP score of 15.0 in only 32 sampling steps, improving upon the vanilla LLaDA baseline by over 120% (15.0 vs. 6.8). Finally, analysis of training dynamics reveals distinct performance crossover: while masked baselines converge quickly but plateau early, XDLM sustains steady improvement throughout training, demonstrating superior long-term scaling. 2. Preliminary Discrete Diffusion Models (DDMs) are latent generative models defined by two Markov processes: forward process and reverse process. Considering single categorical variable x0, represented as one-hot vector in {0, 1}N , where = denotes the vocabulary size. Let zt represent the latent state at time [0, 1], with z0 := x0. The forward process progressively corrupts the data via transition kernels q(zt zs) for < t, and the backward process pθ(zs zt) parameterized by θ iteratively denoises the latent states to recover the clean data x0. Forward Process The forward transition probabilities are governed by row-stochastic matrix Qts RN : q(zt zs) = Cat(zt; tszs), (1) where zt and zs are one-hot vectors due to the discreteness of the latent states, and Cat(x; p) is categorical distribution. Common structural choices for Qts include uniform transitions (UDLM (Schiff et al., 2024)) and absorbing-state transitions (MDLM (Sahoo et al., 2024)). Reverse Process Using Bayes rule and the Markov property, the posterior probability of zs given zt and x0 can be expressed as (Austin et al., 2021): q(zs zt, x0) = (Qtszt) (Q Q t0x s0x0) . (2) In literature, this reverse transition is approximated using the x0-parameterization (Austin et al., 2021; Sahoo et al., 2024; Schiff et al., 2024), where neural network fθ is adopted to predict the distribution of clean data x0 = fθ(zt). Thus, the parameterized reverse transition can be written as pθ(zs zt) = q(zs zt, x0). Training Objective The variational lower bound (VLB, ELBO) is optimized during training, which can be decomposed into three components: prior, diffusion, and reconstruction terms. Mathematically, define = (i 1)/T and Balancing Understanding and Generation in Discrete Diffusion Models = i/T , the objective Lvb can be formulated as: (cid:34) Lvb = Eq(x0) DKL(q(z1 x0) p(z1)) (cid:123)(cid:122) (cid:125) (cid:124) Lprior + (cid:88) i=2 Eq(ztx0)[DKL(q(zs zt, x0) pθ(zs zt))] (cid:123)(cid:122) (cid:125) (cid:124) Ldiffusion Eq(z1/T x0)[log pθ(x0 z1/T )] (cid:125) (cid:123)(cid:122) Lrecons (cid:124) (cid:35) . Lemma 3.1 (Construction of Mixing Kernel). Let π RN be stationary target distribution defined as mixture of the uniform distribution and point-masses on special tokens S. If the noise kernel satisfies the property of instantaneous mixing (i.e., convergence to π in single step), it admits the decomposition: = + (cid:88) iS µiMi, (6) (3) where is the all-ones matrix, Mi is the absorbing matrix for state i, and the weights satisfy + (cid:80) µi = 1. In the continuous time limit (T ), the prior and reconstruction terms reduce to 0, simplifying the objective to the diffusion term (Schiff et al., 2024): Lvb = EtU (0,1) [T DKL(q(zs zt, x0) pθ(zs zt))] . (4) We specialize Lemma 3.1 (derivation in Appendix A) to standard MDLM setting with single mask token, represented by the one-hot vector m. By letting and µ denote the weights of the uniform and masking components, respectively, the resulting noise kernel matrix naturally interpolates between the uniform noise and the mask state: 3. miXed Diffusion Language Modeling via"
        },
        {
            "title": "Stationary Noise Kernels",
            "content": "This section details the proposed miXed Diffusion Language Modeling (XDLM) framework. It begins by introducing the forward process with stationary noise kernel in Sec. 3.1, and then derives scalar formulation for efficient sampling and training in Sec. 3.2. Finally, Sec. 3.3 establishes that UDLM and MDLM can be viewed as limiting cases of XDLM under particular settings of the hybrid parameter. 3.1. The Forward Process with Stationary Kernels The forward diffusion process of DDMs from time to is governed by transition matrix Qts, defined as convex combination of the signal-preserving identity matrix and stationary noise kernel K: Qts = αtsI + βtsK, (5) where αts, βts [0, 1] represent scalar schedule satisfying αts + βts = 1. critical design choice in our method is the stationarity of K. Unlike GIDD (von Rütte et al., 2025), which utilizes time-dependent mixing distributions that inherently couple the noise structure with the diffusion timeline, we enforce to remain invariant across all time steps. This stationarity decouples the noise characteristics from the scheduling dynamics, ensuring that the diffusion process follows geometrically consistent trajectory toward fixed target distribution, which aligns with the optimal transport path proposed in the Flow Matching (Lipman et al., 2022). Under this constraint, the noise kernel must map every input state toward target noise distribution π, regardless of the starting state. This requirement is formalized next. = + µM. (7) Fig. 1 illustrates the transition dynamics within the noise kernel K, driven by uniform noise and absorbing noise (mask state), labeled and m, respectively. 3.2. Efficient Sampling and Training via Scalar Formulation By substituting the definitions of the noise kernel (Eq. 6) and transition matrix Qts (Eq. 5) into Eq. 2 and Eq. 4, the detailed formulations for the posterior q(zs zt, x) and KL divergence can be readily obtained. However, direct evaluation of these expressions via matrix operations is prohibitively expensive in terms of both memory and computation, particularly for large vocabularies. In this section, we derive that the posterior and KL divergence can be reformulated into an equivalent scalar form with Lemma 3.3 and Lemma 3.4. By further incorporating the asymptotic limit behavior defined in Lemma 3.5, we establish tractable and numerically stable training objective that bypasses the expensive explicit computation of large vocabulary size. Concretely, let denote an arbitrary one-hot basis vector, the posterior probability of transitioning to state can thus be written as q(zs = ezt, x) = (αtszt + βtsKzt) (αsx + βsK x) zt (αtx + βtK x) , (8) We observe that Eq. 8 can be reformulated into an equivalent scalar form by defining set of helper functions, which can significantly reduce the computational complexity for both the posterior and the KL divergence. Balancing Understanding and Generation in Discrete Diffusion Models Definition 3.2 (Scalar Primitives). Let px,e := ex represent the probability mass of distribution at token e. We define the noise rate function r(e) and the forward diffusion map ft(x, e) as: This substitution bypasses the expensive explicit computation of posterior distributions and their KL divergence, facilitating an efficient and stable implementation for XDLM training and sampling. r(e) = ft(x, e) = αtpx,e + βtr(e) + µδe,m (10) (9) 3.3. Relationship to MDLM and UDLM These helper functions yield the following efficient expressions for the posterior and KL divergence, with full derivations deferred to Appendix B. Lemma 3.3 (Scalar Posterior). The posterior probability of transitioning to state can be formulated as: q(zs = zt, x) = fs(x, e)fts(e, zt) ft(x, zt) . (11) The models reverse transition pθ follows the same form by substituting with the predicted distribution x0. Lemma 3.4 (Scalar KL Divergence). The term DKL(q(zs zt, x) pθ(zs zt)) can be written as: DKL = βtsαsr(zt) ft(x, zt) ht(x, zt, x0), (12) where ht is an auxiliary function collecting the logarithmic difference terms: ht(x, zt, x0) = fs(x, zt) r(zt) 1 αs kβs αs log + log αts βtsαs ft(x, zt) ft(x0, zt) fs(x, zt)ft(x0, zt) ft(x, zt)fs(x0, zt) fs(x, x) fs(x0, x) + log (cid:88) eV log fs(x, e) fs(x0, e) . (13) As for the limiting case where t, approximating continuous time, the auxiliary function ht simplifies, thereby circumventing the numerical instability associated with the first logarithmic term in Eq. 13. Lemma 3.5 (Limiting Case). As t, the function ht converges to: ht(x, zt, x0) log ft(x, x) ft(x0, x) + log px,zt px0,zt ft(x0, zt) (cid:88) log kβt αt 1 αt ft(x, e) ft(x0, e) eV ft(x, zt) ft(x0, zt) + . (14) The proposed XDLM generalizes existing DDM frameworks. Through the modulation of the uniform noise weight (Eq. 7) , and the consequent adjustment of the absorbing strength, XDLM reduces to UDLM and MDLM at the parameters limits. The relationship between XDLM and these models is outlined in this section, with detailed analysis deferred to Appendix C. Connection with MDLM. Setting = 0 (which implies µ = 1) reduces the noise kernel to pure masking operation. Thus, the posterior probability simplifies to: pθ(zs = zt) βs βt = δzt,mδe,m + δzt,mδe=m βtsαspθ,e βt + δzt=mδe,zt = βs δm,zt, βt (αsαt) 1αt δe,zt, = pθ,e, zt = m, = zt = m, = (16) Consequently, the KL divergence collapses to the standard cross-entropy loss on masked tokens, matching the MDLM objective (Sahoo et al., 2024): LKL = βtsαs βt log pθ(x zt). (17) Connection with UDLM. Setting = 1 (implying µ = 0) yields the uniform noise kernel. In this limit, the posterior probability becomes: pθ(zs = zt) = δe,zt(N αtpθ,e + βsαts) + (αs αt)pθ,e + βsβts/N αtpθ,zt + βt (18) Define xj = Nft(x, ej), (xθ)j = Nft(x0, ej) and considering the limit t, our derived KL divergence aligns exactly with the UDLM loss derived in (Schiff et al., 2024): Consequently, by substituting the asymptotic behavior derived in Lemma 3.4 and 3.5 into the training objective defined in Eq. 4, we arrive at tractable scalar formulation for the training loss: lim Lvb = Et,x0,zt (cid:20) α tr(zt) ft(x, zt) ht(x, zt, x0) (cid:21) . (15) KL = βtsαs αt (xθ)i xi xj xi log (xθ)i xj (xθ)j xi (cid:88) js.t.(zt)j =0 (19) where is the index of the active state in zt. Balancing Understanding and Generation in Discrete Diffusion Models Table 1. Validation PPL on the OWT dataset. XDLM achieves performance comparable to MDLM and outperforms UDLM under the same training steps. Model MDLM GIDD XDLM UDLM PPL 23.321 23. 24.097 25.937 4. Experiment The experimental setup is first introduced in Sec. 4.1, with additional details provided in Appendix F. Sec. 4.2 then reports the performance of XDLM on zero-shot likelihood, language generation quality, and image generation tasks, demonstrating its superiority over existing methods, with more results presented in Appendix G, and I. In addition, to evaluate scalability to larger model sizes, XDLM is further extended to LLaDA-XDLM via continual pretraining on LLaDA, which achieves strong performance on standard benchmarks (More details in Appendix J). Finally, Sec. 4.3 presents analysis and visualizations across different tasks, showing that XDLM achieves better balance between understanding and generation than other approaches (More results in Appendix K, L, and E). 4.1. Experimental Setup All experiments were conducted on cluster of 8H800 GPUs. We standardized key hyperparameters where applicable, utilizing the AdamW optimizer (β1 = 0.9, β2 = 0.999, weight decay 0), mixing ratio of = 0.1, global batch size of 512, and an EMA rate of 0.9999. 4.2. Performance Evaluation of Zero-Shot Likelihood. To assess the generalization capabilities of XDLM, we evaluate zero-shot perplexity (PPL) on seven external datasets after training on OpenWebText (OWT). Aligning with prior work (Sahoo et al., 2024; 2025), our evaluation suite includes the validation splits of AG News (Zhang et al., 2015), LAMBADA (Paperno et al., 2016), LM1B (Chelba et al., 2013), Penn Treebank (Marcus et al., 1993), Scientific Papers from ArXiv and PubMed (Cohan et al., 2018), and WikiText (Merity et al., 2016). The validation PPL is estimated via the negative Evidence Lower Bound (ELBO) with Monte Carlo sampling. To ensure fair comparison, we adopt the identical sampling configuration in (Sahoo et al., 2024; 2025). Figure 2. Language Generation Quality. Results on OWT (top) and LM1B (bottom) demonstrate that XDLM achieves better balance across both few-step and multi-step regimes. For clarity, PPL and Generation Steps are reported in logarithmic scale. Language Generation Quality. We evaluate performance using perplexity (quality) and token entropy (diversity) under ancestral sampling (Sahoo et al., 2024; Schiff et al., 2024; Sahoo et al., 2025). Fig. 2 shows that XDLM strikes robust balance between masked and uniform diffusion paradigms. In few-step regimes (832 steps on OWT), XDLM benefits from uniform noise dynamics, achieving generation quality comparable to UDLMs and clearly surpassing purely masked models. In contrast, under multi-step regimes (5121024 steps), masked noise becomes dominant, allowing XDLM to outperform UDLMs and reach MDLMlevel performance. similar trend is observed on LM1B, confirming the efficacy of XDLM in securing the efficiency of uniform noise while preserving the rigorous structure of masking. Comprehensive numerical results analyzing the impact of the mixing ratio are provided in Appendix H. As shown in Tab. 1, XDLM achieves PPL of 24.097 on the OWT validation set, comparable to MDLM and GIDD and significantly outperforming UDLM (25.937). This retention of in-domain capability extends to zero-shot benchmarks. As listed in Tab. 2, XDLM obtains an average PPL of 54.110 across seven datasets, performing similarly to MDLM (53.650) and GIDD (53.384) while UDLM clearly lags behind (59.574). Image Generation Performance. We evaluate domain generality via class-conditional generation on ImageNet-1K. Tab. 3 reports FID and IS scores without Classifier-Free Guidance (CFG). In few-step regimes, XDLM and UDLM consistently outperform mask-based baselines (i.e., MDLM and GIDD). XDLM proves highly competitive with UDLM, achieving the lowest FID (25.77) at 16 steps. demonstrating its capacity for efficient, high-quality generation. When 5 Balancing Understanding and Generation in Discrete Diffusion Models Table 2. Zero-shot performance comparison across various benchmarks. XDLM maintains performance parity with MDLM and GIDD, significantly outperforming UDLM. Models were reimplemented under unified setting using 8H800 GPUs. Dataset AG News LAMBADA LM1B-GPT2 PTB ArXiv PubMed WikiText Average"
        },
        {
            "title": "MDLM\nGIDD\nXDLM\nUDLM",
            "content": "61.374 60.607 62.768 69.402 47.967 47.811 45.608 51.272 65.629 65.898 68.229 75.572 89.049 86.911 90.796 95.986 37.457 39.019 37.232 42.671 41.981 42.634 41.391 47. 32.093 30.809 32.748 34.933 53.650 53.384 54.110 59.574 Table 3. ImageNet-1K with standard conditioning. Performance comparison of image generation on Model FID IS step 4 step 8 step 16 step 4 step 8 step 16 MDLM 80.752 47.732 28.785 16.287 29.178 44.656 GIDD 86.842 54.933 35.403 14.559 24.297 35.698 XDLM 54.085 34.109 25.774 24.829 36.964 43.903 UDLM 49.861 30.144 26.242 27.049 38.832 41.801 Table 4. ImageNet-1K with CFG scale = 2.0. Performance comparison of image generation on Model FID step 4 step 8 step 16 step 4 IS step 8 step 16 MDLM 33.468 11.144 GIDD 41.000 15.151 XDLM 13.550 8.956 UDLM 14.055 9.718 6.725 7.076 8.625 8. 54.740 119.150 172.664 44.084 95.789 148.148 107.403 148.723 165.916 97.859 123.582 132.099 evaluating with CFG (scale=2.0), as shown in Tab. 4, the pure mask-based MDLM achieves the lowest overall FID (6.73) at 16 steps, suggesting that masking excels given sufficient steps. However, by partially inheriting these absorbing noise properties, XDLM significantly outperforms UDLM in the few-step regime, achieving the best FID scores at 4 steps (13.55) and 8 steps (8.96). Extended analysis is provided in Appendix I. Continual Pretraining of Large Language Models (LLMs). To validate XDLM as scalable upgrade for stateof-the-art LLMs, we adapt LLaDA (Nie et al., 2025), pretrained 8B-MDLM, into the XDLM formulation with compute-efficient pretraining of 600 steps (termed LLaDAXDLM). To rigorously isolate the source of improvement, we compare against two controls: LLaDA-MDLM (standard continual pretraining for the same duration) and LLaDAXDLM-infer (LLaDA coupled with our sampling strategy). Evaluations on OpenCompass (32 steps) show that LLaDAXDLM consistently outperforms the original LLaDA and both controls on GSM8k, MATH, and BBH (Fig. 3 (a)). The impact is most visible in MBPP code generation, where LLaDA-XDLM doubles the performance of LLaDA (15.0 vs. 6.8) by minimizing non-compilable errors, thereby confirming enhanced structural coherence. As for the control models, while LLaDA-MDLM suffers performance drop compared to the original LLaDA, likely due to distribution shift between the tuning and pretraining corpora, LLaDA-XDLM achieves significant improvements under identical conditions. This confirms that the gains stem from the efficacy of the XDLM formulation rather than the additional 600 training steps alone. Furthermore, LLaDA-XDLM remarkably outperforms LLaDA-XDLMinfer, demonstrating that the improvement is intrinsic to the learned model rather than an artifact of sampling strategies. 4.3. Analysis Performance Crossover in Training Dynamics. Analysis of the training dynamics reveals performance crossover phenomenon, where the relative efficacy of competing models shifts significantly as training progresses. This trend is particularly evident in the LM1B text generation task, as shown by the Generation PPL curves in Figure 4 (a). While GIDD and MDLM demonstrate advantages in the initial phase (<200k steps, pink region), they suffer from rapid saturation and are eventually outperformed by UDLM (yellow region). Leveraging the uniform nature inherited from UDLM, XDLM demonstrates substantial late-stage improvement, eventually matching or surpassing MDLM after 700k steps (green region). This trajectory is in sharp contrast to GIDD, which exhibits early saturation and only marginal PPL gains throughout the training process. different, yet similar in trends, dynamic emerges in image generation on ImageNet-1K, as shown in Fig. 4 (b). Here, XDLM demonstrates superior performance from the start, consistently achieving the lowest FID scores. While UDLM eventually converges with XDLM in the final training phase (> 500k steps), both models significantly outperform the MDLM and GIDD baselines throughout the process. Impact of Mixing Ratio. The mixing ratio acts as pivotal control parameter, interpolating between two distinct paradigms: UDLM (k 1) and MDLM (k 0), characterized by their divergent capabilities in generation and understanding. As shown in Fig. 1 (right), these boundary cases establish linear Reference Trade-off Line (dashed black line), representing zero-sum scenario where gains in generation quality (lower Generation PPL) come at the cost of understanding capability (higher Zero-Shot PPL). Balancing Understanding and Generation in Discrete Diffusion Models Figure 3. Evaluation of adapting LLaDA-8B to our XDLM formulation (LLaDA-XDLM): (a) LLaDA-XDLM consistently outperforms baselines across diverse benchmarks; (b) Improvements are particularly pronounced in code generation (MBPP), where the model substantially reduces generation failures. However, the empirical behavior of XDLM reveals that the model does not merely traverse this linear path. Crucially, varying shifts the models operating point along continuous, smooth trajectory. This coherent evolution, as opposed to stochastic fluctuations or random walks, validates the architectural design, confirming that XDLM successfully synthesizes the mechanical properties of both UDLM and MDLM into unified framework. Moreover, this trajectory bows inward to form superior Optimal Trade-off Frontier (solid red line), demonstrating synergistic effect where the mixed objective yields performance gains that exceed the sum of the parts. By navigating this frontier, we identify an empirical sweet spot at = 0.1. As shown by the red marker, this configuration effectively breaks the Pareto frontier of the baselines, achieving robust understanding capabilities without the severe degradation in generation performance typical of pure masked models. Computational Efficiency of XDLM. We analyze the computational efficiency across inference, training, and sam7 Figure 4. Training dynamics for (a) text and (b) image generation tasks. Colored regions indicate the dominant model during each phase, while transitions between colors mark points of performance crossover. pling. While MDLM remains the fastest baseline due to its simple absorbing kernel, XDLM emerges as the most efficient model among those involving uniform noise. Benefiting from our scalar reformulated strategy, XDLM achieves forward throughput of 396,398 tokens/s, nearly double GIDDs 199,516 tokens/s, and training throughput of 137,372 tokens/s. In generation tasks, XDLM maintains sampling speed of 7,108 tokens/s, significantly surpassing UDLM (2,882 tokens/s). This efficiency extends to memory consumption, where XDLM requires only 31.4 GB compared to UDLMs 59.7 GB and GIDDs 40.9 GB. These results confirm that XDLM effectively mitigates largevocabulary bottlenecks without compromising the expressivity of complex noise kernels. For more detailed results, please refer to Appendix K. Inference Dynamics. To investigate the internal generation mechanism of XDLM, we closely inspect single generation trajectory over budget of = 32 steps. The result shows that XDLM effectively leverages hybrid noise process: it constructs semantic structure via absorbing noise ([MASK] Token) while refining content through uniform noise (Token Token). Crucially, XDLM distinguishes itself with re-masking strategy (transforming tokens back to [MASK]). Unlike standard iterative models that exhibit Balancing Understanding and Generation in Discrete Diffusion Models high sensitivity to initialization noise, this mechanism allows XDLM to actively reject low-probability tokens derived from uniform noise, effectively escaping local optima. For detailed analysis, please refer to Appendices and E. 5. Related work Masked Diffusion Language Modeling. Building on the general and flexible discrete diffusion framework introduced by D3PM (Austin et al., 2021), absorbing-statebased corruptions (often using special [MASK] token) have emerged as popular paradigm for discrete diffusion in language, frequently referred to as Masked Diffusion Language Models (MDLMs). MDLM (Sahoo et al., 2024) and MD4 (Shi et al., 2024) extend this framework and derive continuous-time variational objective for masked diffusion as simple weighted integral of cross-entropy losses, yielding streamlined and general training recipe for discrete language models. From continuous-time perspective, CTMC (Campbell et al., 2022) formalizes the forward process via continuous-time Markov chains, while SEDD (Lou et al., 2023) learns density ratios directly, providing discrete analogue of score matching. These advances have facilitated scaling masked diffusion LMs to larger model sizes and datasets, leading to large diffusion language models such as LLaDA (Nie et al., 2025) and Dream (Ye et al., 2025), as well as extensions to multimodal settings (You et al., 2025; Yang et al., 2025; Yu et al., 2025). Uniform-noise Diffusion Language Modeling. The transition kernel of uniform-noise diffusion is mixture of the identity and uniform distribution over the vocabulary. This idea was introduced for discrete spaces in Argmax/Multinomial diffusion (Hoogeboom et al., 2021) and generalized by D3PM (Austin et al., 2021). Plaid (Gulrajani & Hashimoto, 2023) takes notable step toward narrowing the likelihood gap between autoregressive models and diffusion-based language models. UDLM (Schiff et al., 2024) shows that, with appropriate guidance mechanisms, uniform-noise diffusion can yield stronger controllability. Duo (Sahoo et al., 2025) highlights duality between Gaussian diffusion language models (e.g., Plaid) and uniform-noise diffusion, and proposes distilling uniform-noise model from an auxiliary latent Gaussian model. Mixed Language Modeling. D3PM (Austin et al., 2021) observes that BERT can be viewed as one-step instance of mixed corruption with transition matrix. However, BERT is trained with cross-entropy objective on the masked positions only, whereas diffusion-based approaches optimize objectives integrated over noise schedule (or time), leading to different training dynamics and inductive biases. Recent research has further bridged and generalized these (Fathi et al., 2025) unify autoregressive and paradigms. diffusion models by assigning distinct noise schedules to individual token positions. Similarly, ReMDM (Wang et al., 2025) relaxes the strict absorbing constraint of the forward process, allowing [MASK] tokens to revert to their original states, thereby enabling plug-and-play editing capabilities. Our most related work is GIDD (von Rütte et al., 2025), which also explores interpolating between masked and uniform-noise diffusion. However, GIDD generally relies on constructing transition matrices that blend these mechanisms dynamically. In contrast, our XDLM framework enforces stationary noise kernel. This design choice explicitly decouples the noise characteristics from the temporal dynamics, allowing us to derive highly efficient scalar formulation that simplifies both sampling and training. 6. Conclusion In this paper, we presented the XDLM, unified approach that theoretically bridges the gap between Masked and Uniform-noise diffusion. By redefining the forward process as weighted row-stochastic transition, we proved that XDLM recovers existing paradigms (MDLM and UDLM) as special cases. To ensure practicality, we derived memoryefficient implementation that reduces computational complexity, enabling training on large vocabularies. Empirically, XDLM breaks the Pareto frontier between understanding and generation. We identified mixing ratio of = 0.1 as the optimal sweet spot, where the model combines the robust zero-shot likelihoods of masking models with the superior sample diversity and few-step generation quality of uniform noise models. These advantages extend across domains, achieving state-of-the-art on ImageNet-1K and demonstrating significant scalability in the continual pretraining of 8B-parameter LLMs, where XDLM doubled performance on code generation benchmarks. Limitations Despite these contributions, several avenues for future research remain. First, we have not yet trained XDLM from scratch at large scale; such pre-training would likely allow for more comprehensive exploration of the models emergent properties. Second, we did not fully investigate the performance crossover phenomenon, wherein UDLM and XDLM appear to outperform MDLM in generation tasks involving large sampling steps (approaching autoregressive decoding). Third, domain-specific sampling strategies for XDLM in language modeling and image generation have not yet been optimized. Furthermore, while we confirmed that XDLM balances understanding and generation, the interaction and balance between textual and visual modalities within single unified model remain uninvestigated. Finally, the development of post-training schemas and inference acceleration techniques for XDLM remains subject for future work. 8 Balancing Understanding and Generation in Discrete Diffusion Models"
        },
        {
            "title": "Impact Statement",
            "content": "The primary objective of this research is to advance the technical state-of-the-art in machine learning. Consequently, it entails societal considerations parallel to those found in the general field of language and foundation models."
        },
        {
            "title": "References",
            "content": "Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and Van Den Berg, R. Structured denoising diffusion models in discrete state-spaces. NeurIPS, 34:1798117993, 2021. Campbell, A., Benton, J., De Bortoli, V., Rainforth, T., Deligiannidis, G., and Doucet, A. continuous time framework for discrete denoising models. NeurIPS, 35: 2826628279, 2022. Chelba, C., Mikolov, T., Schuster, M., Ge, Q., Brants, T., Koehn, P., and Robinson, T. One billion word benchmark for measuring progress in statistical language modeling. arXiv preprint arXiv:1312.3005, 2013. Cohan, A., Dernoncourt, F., Kim, D. S., Bui, T., Kim, S., Chang, W., and Goharian, N. discourse-aware attention model for abstractive summarization of long documents. arXiv preprint arXiv:1804.05685, 2018. Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., and Fei-Fei, L. Imagenet: large-scale hierarchical image database. In IEEE CVPR, pp. 248255. Ieee, 2009. Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert: Pre-training of deep bidirectional transformers for language understanding. In NAACL, pp. 41714186, 2019. Dhariwal, P. and Nichol, A. Diffusion models beat gans on image synthesis. NeurIPS, 34:87808794, 2021. Fathi, N., Scholak, T., and Noël, P.-A. Unifying autoregressive and diffusion-based sequence generation. arXiv preprint arXiv:2504.06416, 2025. Gokaslan, A. and Cohen, V. Openwebtext corhttp://Skylion007.github.io/ pus. OpenWebTextCorpus, 2019. Gulrajani, I. and Hashimoto, T. B. Likelihood-based diffusion language models. NeurIPS, 36:1669316715, 2023. Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. NeurIPS, 33:68406851, 2020. Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B. Diffwave: versatile diffusion model for audio synthesis. arXiv preprint arXiv:2009.09761, 2020. Krizhevsky, A., Hinton, G., et al. Learning multiple layers of features from tiny images. 2009. Lipman, Y., Chen, R. T., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative modeling. arXiv preprint arXiv:2210.02747, 2022. Lou, A., Meng, C., and Ermon, S. Discrete diffusion language modeling by estimating the ratios of the data distribution. 2023. Marcus, M., Santorini, B., and Marcinkiewicz, M. A. Building large annotated corpus of english: The penn treebank. Computational linguistics, 19(2):313330, 1993. Merity, S., Xiong, C., Bradbury, J., and Socher, R. arXiv preprint Pointer sentinel mixture models. arXiv:1609.07843, 2016. Nie, S., Zhu, F., You, Z., Zhang, X., Ou, J., Hu, J., Zhou, J., Lin, Y., Wen, J.-R., and Li, C. Large language diffusion models. arXiv preprint arXiv:2502.09992, 2025. Paperno, D., Kruszewski, G., Lazaridou, A., Pham, N.-Q., Bernardi, R., Pezzelle, S., Baroni, M., Boleda, G., and Fernández, R. The lambada dataset: Word prediction requiring broad discourse context. In ACL, pp. 1525 1534, 2016. Peebles, W. and Xie, S. Scalable diffusion models with transformers. In IEEE ICCV, pp. 41954205, 2023. Penedo, G., Kydlíˇcek, H., Lozhkov, A., Mitchell, M., Raffel, C. A., Von Werra, L., Wolf, T., et al. The fineweb datasets: Decanting the web for the finest text data at scale. NeurIPS, 37:3081130849, 2024. Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., Sutskever, I., et al. Language models are unsupervised multitask learners. OpenAI blog, 1(8):9, 2019. Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and Ommer, B. High-resolution image synthesis with latent In IEEE CVPR, pp. 1068410695, diffusion models. 2022. Ronneberger, O., Fischer, P., and Brox, T. U-net: Convolutional networks for biomedical image segmentation. In MICCAI, pp. 234241. Springer, 2015. Hoogeboom, E., Nielsen, D., Jaini, P., Forré, P., and Welling, M. Argmax flows and multinomial diffusion: Learning categorical distributions. NeurIPS, 34:1245412465, 2021. Sahoo, S., Arriola, M., Schiff, Y., Gokaslan, A., Marroquin, E., Chiu, J., Rush, A., and Kuleshov, V. Simple and effective masked diffusion language models. NeurIPS, 37:130136130184, 2024. 9 Balancing Understanding and Generation in Discrete Diffusion Models Sahoo, S. S., Deschenaux, J., Gokaslan, A., Wang, G., Chiu, J., and Kuleshov, V. The diffusion duality. arXiv preprint arXiv:2506.10892, 2025. Schiff, Y., Sahoo, S. S., Phung, H., Wang, G., Boshar, S., Dalla-torre, H., de Almeida, B. P., Rush, A. M., PIERROT, T., and Kuleshov, V. Simple guidance mechanisms for discrete diffusion models. In ICLR, 2024. Shi, J., Han, K., Wang, Z., Doucet, A., and Titsias, M. Simplified and generalized masked diffusion for discrete data. NeurIPS, 37:103131103167, 2024. Sun, P., Jiang, Y., Chen, S., Zhang, S., Peng, B., Luo, P., and Yuan, Z. Autoregressive model beats diffusion: Llama for scalable image generation. arXiv preprint arXiv:2406.06525, 2024. von Rütte, D., Fluri, J., Ding, Y., Orvieto, A., Schölkopf, B., and Hofmann, T. Generalized interpolating discrete diffusion. arXiv preprint arXiv:2503.04482, 2025. Wang, G., Schiff, Y., Sahoo, S. S., and Kuleshov, V. Remasking discrete diffusion models with inference-time scaling. arXiv preprint arXiv:2503.00307, 2025. Yang, L., Tian, Y., Li, B., Zhang, X., Shen, K., Tong, Y., and Wang, M. Mmada: Multimodal large diffusion language models. arXiv preprint arXiv:2505.15809, 2025. Ye, J., Xie, Z., Zheng, L., Gao, J., Wu, Z., Jiang, X., Li, Z., and Kong, L. Dream 7b: Diffusion large language models. arXiv preprint arXiv:2508.15487, 2025. You, Z., Nie, S., Zhang, X., Hu, J., Zhou, J., Lu, Z., Wen, J.-R., and Li, C. Llada-v: Large language diffusion models with visual instruction tuning. arXiv preprint arXiv:2505.16933, 2025. Yu, R., Ma, X., and Wang, X. Dimple: Discrete diffusion multimodal large language model with parallel decoding. arXiv preprint arXiv:2505.16990, 2025. Zhang, X., Zhao, J., and LeCun, Y. Character-level convolutional networks for text classification. NeurIPS, 28, 2015. 10 Balancing Understanding and Generation in Discrete Diffusion Models A. The Forward Process with Stationary Kernels Lemma 1 (Construction of Mixing Kernel). Let π RN be stationary target distribution defined as mixture of the uniform distribution and point-masses on special tokens S. If the noise kernel satisfies the property of instantaneous mixing (i.e., convergence to π in single step), it admits the decomposition: where is the all-ones matrix, Mi is the absorbing matrix for state i, and the weights satisfy + (cid:80) µi = 1. = + (cid:88) iS µiMi, (20) Proof. The condition of instantaneous mixing implies that the transition probability to destination state is solely determined by the target marginal πj, independent of the antecedent state s. In terms of the stochastic matrix, this imposes row-equivalence: Ksj = πj for all s. This is algebraically equivalent to the rank-1 factorization = 1π. Substituting the mixture definition π = ku + (cid:80) iS µiei into this factorization yields: (cid:32) = 1 ku + (cid:33) (cid:88) µiei iS (cid:88) = k(1u) + µi(1e ). iS (21) (22) Identifying the resulting outer products with the canonical transition matrices 1u = 1J (uniform transition) and 1e = Mi (absorbing transition) concludes the proof. B. Efficient Sampling and Training via Scalar Formulation Before proofing lemma, we introduce this definition: Definition B.1. g(s, t, x, zt) = αtzt + αtsβszt + βtsαsKzt + βtsβsKzt = αtsfs(x, zt)zt + βtsr(zt) αsx + βs (cid:18) (cid:19)(cid:19) 1 + µm (cid:18) Thus, we have eg(s, t, x, zt) (cid:18) (cid:18) = αtsfs(x, zt)ezt + βtsr(zt) = fs(x, zt)αtsezt + βtsr(zt) (cid:0)αsex + βsr(e)(cid:1) = fs(x, e)αtsezt + βtsr(zt)fs(x, e) = fs(x, e)fts(e, zt) αsex + βs e1 + µem (cid:19)(cid:19) Lemma 2 (Scalar Posterior). The posterior probability of transitioning to state can be formulated as: q(zs = zt, x) = fs(x, e)fts(e, zt) ft(x, zt) . The models reverse transition pθ follows the same form by substituting with the predicted distribution x0. 11 (23) (24) (25) (26) (27) (28) (29) (30) (31) Balancing Understanding and Generation in Discrete Diffusion Models Proof. q(zs = ezt, x) = (Qtszt) (Q zt Q x) = (αtszt + βtsKzt) (αsx + βsK x) zt (αtx + βtK x) = αtzt + αtsβszt + βtsαsKzt + βtsβsKzt x + βtzt x αtzt = g(s, t, x, zt) ft(x, zt) fs(x, e)fts(e, zt) ft(x, zt) = Lemma 3 (Scalar KL Divergence). The term DKL(q(zs zt, x) pθ(zs zt)) can be written as: DKL = βtsαsr(zt) ft(x, zt) ht(x, zt, x0), where ht is an auxiliary function collecting the logarithmic difference terms: ht(x, zt, x0) = fs(x, zt) r(zt) 1 αs kβs αs log + log αts βtsαs ft(x, zt) ft(x0, zt) fs(x, zt)ft(x0, zt) ft(x, zt)fs(x0, zt) fs(x, x) fs(x0, x) + log (cid:88) eV log fs(x, e) fs(x0, e) . 12 (32) (33) (34) (35) (36) (37) (38) (39) Balancing Understanding and Generation in Discrete Diffusion Models (cid:88) (cid:88) (cid:88) q(zs = ezt, x) log q(zs = ezt, x) pθ(zs = ezt) q(zs = ezt, x) log fs(x, e)fts(e, zt) ft(x, zt) q(zs = ezt, x) q(zs = ezt, x0) (cid:18) log fs(x, e)fts(e, zt) ft(x, zt) log fs(x0, e)fts(e, zt) ft(x0, zt) (cid:19) (cid:88) (cid:88) fs(x, e)fts(e, zt) log fs(x, e)ft(x0, zt) ft(x, zt)fs(x0, e) fs(x, e) (cid:0)αtsδe,zt + βtsr(zt)(cid:1) log fs(x, e)ft(x0, zt) ft(x, zt)fs(x0, e) log fs(x, zt)ft(x0, zt) ft(x, zt)fs(x0, zt) (cid:88) fs(x, e) ft(x0, zt) ft(x, zt) fs(x, e) log fs(x, e) fs(x0, e) log fs(x, zt)ft(x0, zt) ft(x, zt)fs(x0, zt) βtsr(zt) ft(x, zt) log ft(x, zt) ft(x0, zt) (cid:88) (αsδx,e + βsr(e)) log fs(x, e) fs(x0, e) log fs(x, zt)ft(x0, zt) ft(x, zt)fs(x0, zt) fs(x, x) fs(x0, x) log βtsr(zt) ft(x, zt) log ft(x, zt) ft(x0, zt) (cid:88) log fs(x, e) fs(x0, e) + µβtsβsr(zt) ft(x, zt) log fs(x, m) fs(x0, m) Proof. DKL = = = = = = + + = + = + + = 1 ft(x, zt) log (cid:88) 1 ft(x, zt) αtsfs(x, zt) ft(x, zt) βtsr(zt) ft(x, zt) βtsr(zt) ft(x, zt) αtsfs(x, zt) ft(x, zt) βtsr(zt) ft(x, zt) αtsfs(x, zt) ft(x, zt) βtsαsr(zt) ft(x, zt) kβtsβsr(zt) ft(x, zt) βtsαsr(zt) ft(x, zt) ht(x, zt) (40) (41) (42) (43) (44) (45) (46) (47) (48) (49) (50) (51) (52) (53) We omit the term log fs(x, m)/fs(x0, m) as it equals zero for all and x0. Lemma 4 (Limiting Case). As t, the function ht converges to: ht(x, zt, x0) log ft(x, x) ft(x0, x) + log px,zt px0,zt ft(x0, zt) (cid:88) log kβt αt 1 αt ft(x, e) ft(x0, e) eV ft(x, zt) ft(x0, zt) + . (54) Proof. Notice that when t, βtsαs 0, which is 0/0 div. by applying LHôpitals rule. (the up and down both 13 Balancing Understanding and Generation in Discrete Diffusion Models differentiate by αts), we have lim st = = = = = log log fs(x, zt)ft(x0, zt) ft(x, zt)fs(x0, zt) fs(x, zt)ft(x0, zt) ft(x, zt)fs(x0, zt) 1 βtsαs 1 αts αs (cid:18) log fs(x, zt)ft(x0, zt) 1 αs αts (cid:18) log ft(x0, zt) 1 αts αs 1 1 ft(x0, zt) αs 1 1 ft(x0, zt) αs px0,zt r(zt) ft(x0, zt) px,zt r(zt) ft(x, zt) + (cid:18) (cid:18) = (cid:19) log ft(x, zt) αts (αtpx0,zt + (1 αt)r(zt)) αts (αspx0,zt αsr(zt)) log ft(x, zt)fs(x0, zt) αts (cid:19) 1 ft(x, zt) (cid:19) ft(x, zt) αts (cid:19) 1 ft(x, zt) ft(x, zt) αts = = = (αtpx0,zt + βtr(zt))(px,zt r(zt)) (αtpx,zt + βtr(zt))(px0,zt r(zt)) ft(x, zt)ft(x0, zt) αtpx0,zt r(zt) + βtpx,ztr(zt) + αtpx,ztr(zt) βtpx0,ztr(zt) ft(x, zt)ft(x0, zt) r(zt)(px,zt px0,zt) ft(x, zt)ft(x0, zt) (cid:18) fs(x, zt) r(zt) αts βtsαs lim st log fs(x, zt)ft(x0, zt) ft(x, zt)fs(x0, zt) (cid:19) = px,zt px0,zt ft(x0, zt) so C. Relationship to MDLM and UDLM Lemma 5. XDLM can be reduced to MDLM by setting = 0, with the posterior probability: pθ(zs = ezt) = δzt,mδe,m βs βt + δzt,mδe=m βtsαspθ,e βt + δzt=mδe,zt = βs δm,zt, βt (αsαt) 1αt δe,zt, = pθ,e, zt = m, = zt = m, = and kl divergence Proof. since = 0, µ = 1, we have KL = βtsαs βt log pθ,x r(e) = δe,m px0,e = δe=mpx0,e 14 (55) (56) (57) (58) (59) (60) (61) (62) (63) (64) (65) (66) (67) (68) (69) (70) Balancing Understanding and Generation in Discrete Diffusion Models hence q(zs = ezt, x0) = fs(x0, e)fts(e, zt) ft(x0, zt) (αspx0,e + βsr(e))(αtspe,zt + βtsr(zt)) αtpx0,zt + βtr(zt) = = αtpx0,eδe,zt + αsβtspx0,eδzt,m + αtsβsδe,ztδe,m + βsβtsδe,mδzt,m αtpx0,zt + βtδzt,m αsβtspx0,e + αtsβsδe,zt + βsβtsδe,zt βt = δzt,m + δzt=m αtpx0,eδe,zt αtpx0,zt = δzt,m = δzt,m (cid:18) αsβts βt αsβts βt px0,e + βs βt (cid:19) δe,zt + δzt=mδe,zt px0,e px0,zt px0,e + βs βt δzt,mδe,zt + δzt=mδe,zt for kl, we have ht(x, zt) = = = = fs(x, zt) r(zt) 1 αs log fs(x, zt) r(zt) log 1 αs fs(x, zt) r(zt) (cid:18) 1 αs fs(x, zt) r(zt) log so DKL = αts βtsαs ft(x, zt) ft(x0, zt) αts βtsαs log fs(x, zt)ft(x0, zt) ft(x, zt)fs(x0, zt) fs(x, x) fs(x0, x) + kβs αs + log (cid:88) log fs(x, e) fs(x0, e) log (αspx,zt + βsδzt,m)(αtpx0,zt + βtδzt,m) (αtpx,zt + βtδzt,m)(αspx0,zt + βsδzt,m) αtpx,zt + βtδzt,m αtpx0,zt + βtδzt,m αts βtsαs (cid:18) log δzt,m δzt,m βtδzt,m βtδzt,m log px0,x βsδzt,mβtδzt,m βtδzt,mβsδzt,m (cid:19) αtpx,zt αtpx0,zt + δzt=m log px0,x αts βtsαs log (1) 1 αs δzt=m log px,zt px0,zt log px0,x + δzt=m αspx,ztαtpx0,zt αtpx,ztαspx0,zt (cid:19) ht(x, zt) βtsαsr(zt) ft(x, zt) βtsδzt,m ft(x, zt) = δzt=m log px,zt px0,zt βtsαsδzt,m αtδx,zt + βtδzt,m log px0,x = δzt,m βtsαs βt log px0,x Lemma 6. XDLM can be reduced to UDLM by setting = 1, with the posterior probability: pθ(zs = ezt) = δe,zt (N αtpθ,e + βsαts) + (αs αt)pθ,e + βsβts/N αtpθ,zt + βt 15 (71) (72) (73) (74) (75) (76) (77) (78) (79) (80) (81) (82) (83) (84) (85) (86) (87) (88) Balancing Understanding and Generation in Discrete Diffusion Models and kl divergance KL = βtsαs αt 1 ft(x0, zt) 1 ft(x, zt) (cid:88) e=zt ft(x, e) ft(x, zt) log ft(x0, zt)ft(x, e) ft(x0, e)ft(x, zt) Proof. since = 1, µ = 0, we have e, r(e) = ="
        },
        {
            "title": "1\nN",
            "content": "hence q(zs = ezt, x0) = fs(x0, e)fts(e, zt) ft(x0, zt) = = = (αspx0,e + βs/N )(αtspe,zt + βts/N ) αtpx0,zt + βt/N αtpx0,eδe,zt + αsβtspx0,e + αtsβsδe,zt + βsβts/N αtpx0,zt + βt δe,zt(N αtpx0,e + αtsβs) + (αs αt)px0,e + βsβts/N αtpx0,zt + βt given t, for kl, we have ht(x, zt) = px,zt px0,zt ft(x0, zt) 1 αt log ft(x, zt) ft(x0, zt) + log ft(x, x) ft(x0, x) + kβt αt (cid:88) log ft(x, e) ft(x0, e) = ft(x, zt) px,zt px0,zt ft(x, zt)ft(x0, zt) log ft(x, zt) ft(x0, zt) + log ft(x, x) ft(x0, x) (cid:88) log ft(x0, zt) ft(x, zt) βt αt (cid:19) log ft(x, zt) ft(x0, zt) + log ft(x, x) ft(x0, x) 1 αt βt αt (cid:88) log ft(x0, zt)ft(x, e) ft(x0, e)ft(x, zt) (cid:18) log 1 ft(x, zt) 1 ft(x0, zt) ft(x0, zt)ft(x, e) ft(x0, e)ft(x, zt) + = + = + = ft(x, zt) αt βt αt (cid:88) ft(x, zt) αt (cid:88) 1 αt ft(x, zt) αt (cid:18) 1 ft(x0, zt) 1 ft(x, zt) (cid:19) log ft(x, e) log ft(x0, zt)ft(x, e) ft(x0, e)ft(x, zt) (cid:19) (cid:18) 1 ft(x0, zt) 1 ft(x, zt) ft(x, x) ft(x0, x) + log ft(x, zt) ft(x0, zt) ft(x0, zt)ft(x, x) ft(x0, x)ft(x, zt) log + 1 αt (cid:88) ft(x, e) log ft(x0, zt)ft(x, e) ft(x0, e)ft(x, zt) so DKL = = βtsαsr(zt) ft(x, zt) (cid:32) βtsαs αt ht(x, zt) 1 ft(x, zt) 1 ft(x0, zt) (cid:88) ft(x, e) ft(x, zt) log ft(x0, zt)ft(x, e) ft(x0, e)ft(x, zt) (cid:33) 16 (89) (90) (91) (92) (93) (94) (95) (96) (97) (98) (99) (100) (101) (102) (103) (104) (105) Balancing Understanding and Generation in Discrete Diffusion Models D. LM1B Sampling Case We investigate the relationship between the inference computational budget (defined by the number of sampling steps ) and the perceptual quality of the generated text. Tab. 5 presents randomly selected, non-curated samples from the XDLM model trained on the LM1B. Each sample represents the final output of distinct diffusion process constrained to specific total step count {4, 8, 16, 32, 64, 128}. At the lower bound of the sampling budget (T = 4), the model fails to converge to the data manifold. the output is characterized by disjointed lexical retrieval (e.g., the advertising -tag prong) where individual tokens are valid but lack syntactic binding. Increasing the budget to = 8 yields the emergence of superficial syntactic structures (e.g., curt schilling is man of newness), yet the semantic content remains illogical, indicating that the diffusion process resolves local grammatical dependencies prior to higher-order semantic meaning. As the sampling steps increase to the intermediate regime (T = 16 and = 32), we observe transition from mere grammatical correctness to narrative plausibility. At = 16, the text exhibits valid sentence structures but suffers from tautological repetition and thematic drift (e.g., middle east in the middle east). By = 32, the model generates cohesive clauses with clear subject-verb-object relationships, although the specific entities often remain synthetic or hallucinatory (e.g., mr poundhead). This suggests that while 32 steps are sufficient for the model to learn the structural rules of the language, the denoising process has not yet fully aligned the output with the specific factual distributions of the training corpus. distinct phase transition in sample quality is observed at = 64, which appears to represent the saturation point for high-fidelity generation. At this stage, the model produces semantically robust text indistinguishable from natural news data, successfully handling complex entity lists (e.g., Congo, Benin, Ivory Coast) and domain-specific terminology (e.g., National Transportation Safety Board). Extending the process to = 128 yields marginal improvements in long-range consistency and the precision of quantitative figures, but the perceptual gain over = 64 is significantly smaller than the leap observed from = 32. This trajectory indicates that while XDLM requires minimum threshold of approximately 64 steps to resolve fine-grained semantic details, further increases in computational cost yield diminishing returns in generation quality. Table 5. Uncurated samples generated by XDLM trained on LM1B. Each sample represents the output of distinct diffusion process constrained to specific total step count . The sequence length is fixed at 128 tokens. Steps (T ) Generated Text Sample = 4 = [CLS] the advertising -tag prong those former candidates is [CLS] new and aspiring contenders boasted present, confusing [CLS]n. sept. not candidates not only for 15 - running bonds. [CLS] passengers on board the puerto rico jet jet out to the ash oftwa, hawaii, wednesday that afternoon. [CLS] as result of the fusion going from in horsepower and to okter and the ram and to gm, rentalearing / thirds was the infall. [CLS] times square may undergo restorations [CLS] and \" old allah \" is an homage to \" the a, the \" god \" in snowy islamic faith. christina sees, [CLS] [CLS] unfortunately [CLS] curt schilling is man of newness. [CLS] be obese to go over. [CLS] mr middle had, having not started the business for 15 years, wondered for one only anyone on board had once wanted from kraft. [CLS] the new oft - appointed owner proposed wednesday that the administration should take \" punitive tax supplement from treasury \" and insisted on indiana and michigan adhering to gms contract terms. cummings : one at times decides to undergo restoration. now pakistani parliament is now resigned to democratic, destabilization vote in parting zardari, which sees itself [CLS] [CLS] ms. [CLS] and for 17 Balancing Understanding and Generation in Discrete Diffusion Models Steps (T ) Generated Text Sample (Continued) = 16 = = 64 = 128 [CLS] \" fails to clear advertising restrictions. is the only known new middle east in the middle east, is expected for about 10 to 15 years. pleased, how was able to explain it in me. the pre - lot of contemporary televised - auto show - marquees - cars. gm bio tasted like two - thirds of iconic americans : one - third of the time that ford used and around 4 years the original was back to building it made years of. [CLS] in contrast, zardari, who sees washington [CLS] [CLS] believe the people on board had called very [CLS] the network, which [CLS] the studio 86 is nearly [CLS] that sense of neutrality is likely [CLS] me, to clear the books. to force some postponements of the dispute, which was resolved wednesday. [CLS] 15 told the telegraph that he always believed the board had convinced him it was time to take the train. the city council to create series of new visitorsareas. according to le monde, \" is the afghans to steal or take and foot out \" - - money the taliban has often used to buy from pakistani taliban, not pakistans. [CLS] though some of its friends in the west zardari said he sees [CLS] [CLS] mr poundhead wants [CLS] marfor, [CLS] me, was the crew of the xv230, which is carrying congo, benin, ivory coast, cape verde, manchester russians and coastguards at luanda. [CLS] the national transportation safety board had called the electrical life support vests to safe spot above hawaii. [CLS] the daily herald said analysts expect earnings of $ 394 million, or 43 cents share, due to better - than - expected growth in the mini market. [CLS] general cuddy writes that the controls could be used to prevent revenge attacks, but not to halt the protests by those suspected of disturbing network in the island, throwing up the theory. [CLS] [CLS] [CLS] me to represent the cuban people they represent. leadership is still taking control of the company, but the problem has not been resolved. [CLS] to tony fernanda santos, of eey investments board, it thought up lifes 328 million - and one - pound tax loss of $ 86 million and projected earnings of $ 37 million next year to 2011. [CLS] next up is \" best of belies, \" the first mini - series. [CLS] cusuoga, the nationalist method often used to describe revenge attacks, is not experienced in most protests against presence of the tigers in the islands sinhala territories. [CLS] [CLS] [CLS] the To investigate the internal generation mechanism of XDLM, we visualize single generation trajectory with total budget of = 32 steps and fixed sequence length of 128 tokens. Tab. 6 details the evolution of the sequence at steps {0, 1, 8, 16, 24, 32}. We utilize color-coded schema to distinguish the specific transition types inherent to the hybrid noise process. Green denotes transitions from the absorbing state ([MASK]) to token (typical of Masked DLMs), Blue denotes token-to-token refinement (typical of Uniform DLMs), and Red denotes the re-masking operation (token back to [MASK]). This red transition is particular to XDLM, where the forward process is mixture of absorbing and uniform noise, allowing the model to stochastically backtrack by rejecting previously generated tokens. The process initializes at = 0 with mixture of masks and random tokens derived from uniform noise, such as coffin, slippery, and trumpets. critical dynamic is observed immediately at the transition to = 1. The model actively rejects the majority of these random initializations. We observe extensive Red markings where tokens like coffin and trumpets are reverted to [MASK]. This demonstrates that XDLM is not strictly bound by its random initialization from the uniform noise. Unlike standard iterative refinement models that might struggle to escape local minima induced by poor random seeds, XDLM possesses the capacity to identify low-probability tokens and clean the slate via re-masking. Simultaneously, valid anchor tokens begin to emerge (Green), such as shillings and blocked, establishing an initial semantic direction. As the generation progresses through the intermediate steps (t = 1 = 16), the model engages in dynamic interplay of generation and error correction. The presence of all three transition types indicates non-monotonic search process. The model generates new candidates (Green) and refines existing ones (Blue), but crucially, it continues to utilize re-masking (Red) to prune inconsistent branches. For instance, tokens generated tentatively in early steps are masked again when they 18 Balancing Understanding and Generation in Discrete Diffusion Models fail to align with the evolving global context. By the final phase (t = 24 = 32), the dynamics shift characteristically. The Red re-masking transitions disappear entirely, indicating structural convergence. The process focuses exclusively on filling remaining gaps (Green) and performing fine-grained lexical substitutions (Blue). Notable refinements occur here, such as the token life infancy at step 24 being refined to was time at step 32, and from pound refining to from pound-land. This trajectory confirms that XDLM successfully anneals from high-temperature exploration statecharacterized by active deletion and re-generationto low-temperature refinement state, ensuring the final output is both globally coherent and locally precise. Table 6. Step-wise evolution of generated sequence (T = 32). Text colors indicate the transition dynamics inherent to the hybrid noise process: Green represents new tokens generated from masks; Blue represents lexical refinement; and Red highlights the re-masking operation where previously generated tokens are rejected and reverted to [MASK]. Steps (T ) Generated Text Sample = 0 = 1 = 8 [MASK] [MASK] faltered [MASK] [MASK] coffin [MASK] [MASK] [MASK] [MASK] och [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] , [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] ##agawa [MASK] [MASK] [MASK] [MASK] slippery [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] harrington [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] trumpets surpassing [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] refuse [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] ##was veteran jan [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] begs [MASK] [MASK] [MASK] [MASK] faltered [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] ##fo [MASK] is [MASK] [MASK] shillings [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] 1879 [MASK] [MASK] [MASK] [MASK] [MASK] ##anda [MASK] blocked [MASK] ##agawa [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] engined [MASK] [MASK] [MASK] [MASK] [MASK] ##itive [MASK] [MASK] [MASK] [MASK] [MASK] mar [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] distributing [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] tertiary [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] strode [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] whatever [MASK] ##rda [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] to clear [MASK] [MASK] [MASK] [CLS] [MASK] [MASK] of [MASK] is [MASK] congo [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] ##rians santa , [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] 15 [MASK] ##anda [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] board had [MASK] ##ffi [MASK] [MASK] [MASK] ##illo [MASK] the [MASK] [MASK] [CLS] [MASK] hawaii [MASK] [MASK] the [MASK] 86 to [MASK] [MASK] of process [MASK] [MASK] [MASK] [MASK] [CLS] mar [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] , [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] brazil [MASK] [MASK] [MASK] [MASK] [MASK] the [MASK] [MASK] [MASK] used [MASK] [MASK] [MASK] pakistani [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] ##west [MASK] [MASK] [MASK] [MASK] in [MASK] [MASK] za ##rda [MASK] [MASK] [MASK] sees [MASK] [MASK] 19 Balancing Understanding and Generation in Discrete Diffusion Models Steps (T ) Generated Text Sample (Continued) = 16 = 24 = 32 [MASK] [MASK] [MASK] to clear [MASK] [MASK] [MASK] [CLS] [MASK] [MASK] of [MASK] is [MASK] congo [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] the [MASK] , which [MASK] resolved obscene . [CLS] 15 [MASK] ##anda [MASK] [MASK] [MASK] [MASK] [MASK] [MASK] board had convinced ##ffi [MASK] life [MASK] [MASK] take the [MASK] [MASK] [CLS] [MASK] hawaii [MASK] [MASK] the city 86 to [MASK] [MASK] of new [MASK] [MASK] areas [MASK] [CLS] mar [MASK] vice [MASK] [MASK] le [MASK] , [MASK] [MASK] [MASK] afghan [MASK] to steal [MASK] [MASK] [MASK] [MASK] out [MASK] - [MASK] [MASK] the taliban has often used to [MASK] from pakistani [MASK] [MASK] not pakistan [MASK] [MASK] though [MASK] of its [MASK] in [MASK] [MASK] za ##rda ##ri said [MASK] sees [MASK] [MASK] [MASK] [MASK] [MASK] to clear [MASK] [MASK] . [CLS] that [MASK] of neutrality is [MASK] congo force some post ##pone ##ments of the [MASK] , which [MASK] resolved [MASK] . [CLS] 15 [MASK] the telegraph that [MASK] [MASK] believed the board had convinced him [MASK] life infancy [MASK] take the train [MASK] [CLS] from pound [MASK] , the city plans to [MASK] [MASK] of new [MASK] [MASK] areas [MASK] [CLS] mar [MASK] , according [MASK] le monde , \" is the afghan ##s to steal or [MASK] and foot out \" - - [MASK] the taliban has often used to [MASK] from pakistani taliban [MASK] not pakistan [MASK] [MASK] though [MASK] of its friends in the [MASK] za ##rda ##ri said he sees [MASK] [MASK] [CLS] that kind of neutrality is likely [CLS] me , to clear the flag . to force some post ##pone ##ments of the dispute , which was resolved wednesday . [CLS] 15 told the telegraph that he always believed the board had convinced him it was time to take the train . [CLS] from pound ##land , the city plans to build series of new retail residential areas . [CLS] mar ##for , according to le monde , \" is the afghan ##s to steal or take and foot out \" - - money the taliban has often used to buy from pakistani taliban , not pakistan . in the west za ##rda ##ri said he sees [CLS] [CLS] though some of its friends E. ImageNet Sampling Case We conduct qualitative comparison of XDLM against the baseline models MDLM, GIDD, and UDLM on ImageNet-1K. The results are analyzed across two settings: unguided generation  (Fig. 5)  and generation with slight classifier-free guidance  (Fig. 6)  . The visual evidence underscores XDLMs unique ability to combine the strengths of absorbing and uniform noise strategies while mitigating their individual weaknesses. The unguided setting serves as stress test for structural priors. Here, the limitations of MDLM and GIDD are most apparent. These models struggle to form coherent geometries, resulting in chaotic texture blobs (e.g., the flamingo and ladybug columns) rather than distinct objects. Conversely, UDLM, which relies solely on uniform noise, successfully captures global shapes but fails to generate high-frequency details, resulting in blurry, out-of-focus images. XDLM distinguishes itself by balancing these two extremes. By integrating the structural rigidity of MDLM with the refinement flexibility of UDLM, XDLM achieves structural coherence without sacrificing sharpness. For instance, in the rocket launch example, XDLM is the only model that renders clear vertical fuselage and distinct smoke plumes without guidance. As illustrated in Fig. 6, introducing guidance scale of 2.0 improves general sample quality, yet the characteristic behaviors of the models persist. MDLM continues to suffer from lack of detail refinement, often producing over-saturated textures, with poor semantic structure. UDLM remains overly smooth; while the images are clean, they lack the crispness required for photorealism. XDLM leverages the guidance most effectively to enhance detail. This is evident in the flock of flamingos. XDLM renders distinct, individual birds with sharp feathers, whereas MDLM merges them into mass and UDLM blurs the boundaries. Similarly, in the food categories (strawberries and pizza), XDLM generates realistic surface textures and lighting reflections that are absent in the UDLM samples and distorted in the MDLM samples. This confirms that XDLMs hybrid noise 20 Balancing Understanding and Generation in Discrete Diffusion Models mechanism provides superior foundation for high-fidelity generation. Figure 5. Qualitative comparison of class-conditional generation on ImageNet-1K without CFG. In the absence of guidance, baseline models struggle significantly: MDLM and GIDD produce chaotic artifacts with poor structural coherence, while UDLM yields recognizable but over-smoothed images. XDLM, by effectively balancing the characteristics of MDLM and UDLM, generates the most coherent and semantically correct samples (e.g., the distinct rocket structure and pizza toppings) even without guidance. Figure 6. Qualitative comparison of class-conditional generation on ImageNet-1K with CFG scale of 2.0. With the addition of guidance, all models improve, but quality disparities remain. MDLM and GIDD still exhibit texture distortion and \"burn\" artifacts. UDLM produces clean but blurry outputs, lacking fine-grained texture. XDLM demonstrates superior fidelity, combining sharp high-frequency details (seen in the strawberry and volcanoes) with accurate global structure. We further analyze the sampling dynamics of XDLM (k = 0.1) over an 8-step generation process on ImageNet-1K with CFG of 2.0, comparing it against MDLM, GIDD, and UDLM. The visual progression highlights distinct behaviors in how each model handles noise and feature refinement. critical advantage of XDLM is its strong refinement capability, facilitated by the integration of uniform noise throughout the sampling process. As observed in rows 3 and 4, both XDLM and UDLM begin generating key semantic features, specifically the dogs nose and eyes, at the very beginning of the generation cycle. In contrast, MDLM and GIDD adopt 21 Balancing Understanding and Generation in Discrete Diffusion Models more conservative approach. They exhibit little to no refinement of these specific features. Consequently, by the final step [8/8], MDLM and GIDD still fail to resolve the proper structure of the nose and eyes, leaving them distorted or missing. XDLM, however, establishes these features early and refines them continuously, resulting in anatomically correct and aesthetically pleasing final output. The models also differ significantly in their ability to generate fine textures, such as the dogs fur. Because MDLM lacks the ability to correct or refine previously generated tokens, the resulting fur texture appears rough and incoherent. While GIDD produces more delicate fur than MDLM, its performance is hindered by its reliance on time-variant noise, where the beneficial uniform noise mechanism only functions effectively during the very last step, limiting the potential for gradual improvement. Conversely, XDLM refines the fur texture consistently across all steps, ultimately producing the most realistic and high-fidelity texture among the group. Last but not least, the comparison demonstrates that relying exclusively on uniform noise, as seen in UDLM, is insufficient for optimal image quality. While UDLM shares XDLMs ability to locate features early, the final generated image is significantly more blurred. This lack of sharpness results in loss of fine-grained details compared to XDLM, which successfully balances structural coherence with sharp, detailed textures. Figure 7. Visual comparison of sampling dynamics over 8 steps on ImageNet-1K. The figure contrasts the generation trajectories of MDLM, GIDD, XDLM (k = 0.1), and UDLM with CFG scale = 2.0. XDLM demonstrates superior structural coherence and detail refinement compared to baseline models, successfully transitioning from noise to high-fidelity image. F. Detailed Configurations of Experiments Language Modeling Following established methodologies (Sahoo et al., 2025; Schiff et al., 2024; Sahoo et al., 2024), we evaluate our model on standard benchmarks using the LM1B (Chelba et al., 2013) and OpenWebText (OWT) (Gokaslan & Cohen, 2019) datasets. For LM1B, we detokenize the dataset following (Lou et al., 2023; Sahoo et al., 2024; 2025) and evaluate with (Austin et al., 2021; Sahoo et al., 2025) sequence packing. Sequences are processed with context length of 128 using the bert-base-uncased tokenizer (Devlin et al., 2019). For OWT, we use the GPT-2 tokenizer (Radford et al., 2019) with context length of 1,024. Sequences are packed to maximum length of 1,024 with an EOS token delimiter. As OWT lacks standard validation split, we reserve the last 100k documents for validation (Sahoo et al., 2024). Consistent with prior work (Sahoo et al., 2024; Schiff et al., 2024; Sahoo et al., 2025), we parameterize both XDLM and baselines using the modified Diffusion Transformer (DiT) architecture (Peebles & Xie, 2023) adapted from (Lou et al., 2023). All models employ 12 layers, 12 attention heads, hidden dimension of 768, and timestep embedding dimension of 22 Balancing Understanding and Generation in Discrete Diffusion Models 128. The dropout rate is set to 0.1, and input/output word embeddings are untied. To ensure fair comparison, we retain the AdaLN branch but fix the input to zero instead of using log-linear scheduled time embedding. We train using AdamW (β1 = 0.9, β2 = 0.999, weight decay 0) with global batch size of 512. The learning rate is warmed up to 3 104 over the first 2,500 steps and held constant thereafter. We apply an Exponential Moving Average (EMA) rate of 0.9999 (Sahoo et al., 2024; 2025). Image Generation We evaluate on discretized CIFAR-10 (Krizhevsky et al., 2009) and ImageNet-1K (Deng et al., 2009). On ImageNet-1K dataset, We use the VQ-VAE tokenizer from LlamaGen (Sun et al., 2024), , which compresses images into sequences of length 256. The backbone architecture mirrors our language modeling transformer, augmented with classconditional embeddings to enable class-conditional generation. Training hyperparameters follow the language modeling configuration, with the training duration set to 0.5M steps. For CIFAR-10, we treat raw RGB pixel values as discrete tokens (vocabulary size 256) and flatten the images into sequences of length 32 32 3, following (Schiff et al., 2024). We employ U-Net backbone (Ronneberger et al., 2015; Ho et al., 2020) with discretized truncated logistic output distribution (Austin et al., 2021; Sahoo et al., 2024). Class conditioning is implemented by adding label embeddings to the timestep embeddings (Dhariwal & Nichol, 2021). The model is trained for 300k steps using AdamW (β1 = 0.9, β2 = 0.999, weight decay 0) with batch size of 512. We use learning rate warmup of 5,000 steps peaking at 2 104 and maintain an Exponential Moving Average (EMA) rate of 0.9999. Large Language Model Tuning We scale XDLM to large language modeling by performing continual pretraining on LLaDA LLaDA (Nie et al., 2025), model originally trained via MDLM. We utilize 10-billion-token subset of the FineWeb-Edu dataset (Penedo et al., 2024). The training configuration employs sequence length of 4,096 and global batch size of 512. The learning rate is warmed up to 2 105 over 100 steps and held constant for the remaining 500 steps. Unless otherwise specified, we set the mixing ratio to = 0.1. All experiments were conducted on node equipped with 8 Nvidia H800 GPUs. G. Details of Zeroshot Capability of XDLM Trained on OWT To comprehensively assess the generalization capabilities of XDLM, we evaluate zero-shot perplexity on seven external datasets following training on OWT. In this section, we provide the detailed experimental setup, an analysis of the hyperparameter sensitivity regarding the mixing ratio k, and breakdown of the training dynamics. Aligning with prior work (Sahoo et al., 2024; 2025), our evaluation suite includes the validation splits of AG News (Zhang et al., 2015), LAMBADA (Paperno et al., 2016), LM1B (Chelba et al., 2013), Penn Treebank (Marcus et al., 1993), Scientific Papers from ArXiv and PubMed (Cohan et al., 2018), and WikiText (Merity et al., 2016). The validation PPL is estimated via the negative Evidence Lower Bound (ELBO) with Monte Carlo sampling. We employ an evaluation batch size of 128 on single GPU, and all text is pre-packed into sequence length of 1024. To ensure fair comparison, we adopt the identical sampling configuration used in(Sahoo et al., 2024; 2025). We investigate the impact of the mixing ratio on model performance. Tab. 7 and Tab. 8 present the validation PPL on OWT and the zero-shot PPL on external benchmarks, respectively. As shown in Tab. 7, XDLM exhibits performance highly correlated with k. With smaller values (e.g., = 1e-3 and = 0.1), XDLM achieves PPL of 23.495 and 24.097, respectively, which is comparable to MDLM (23.321) and GIDD (23.136). Conversely, as increases towards 0.9, the performance degrades to 25.731, approaching the UDLM baseline (25.937). This trend is further supported by the zero-shot benchmark results in Tab. 8. XDLM (k = 0.1) achieves an average PPL of 54.110, performing similarly to MDLM (53.650) and significantly outperforming UDLM (59.574). This demonstrates that XDLM successfully retains the strong modeling capabilities of the masked diffusion paradigm. Notably, the superior performance of XDLM is not merely an endpoint phenomenon but consistent property observed throughout the training process. As illustrated in Fig. 8, the PPL trajectory of XDLM closely aligns with that of MDLM, steadily decreasing from 64.85 to 54.11. In contrast, the UDLM curve remains notably higher across all training steps. Tab. 9 details the average zero-shot PPL at 100k step intervals. The data confirms that XDLM (k = 0.1) maintains learning dynamic nearly identical to MDLM from the early stages (100k steps) to convergence (1M steps), highlighting the inherent 23 Balancing Understanding and Generation in Discrete Diffusion Models stability and efficiency of the proposed method. Table 7. Validation PPL on OWT after 1M training steps. Performance is highly correlated with the mixing ratio k: lower values (k 0.1) maintain parity with the strong MDLM baseline (23.321), while higher values (k 0.9) degrade towards the UDLM baseline (25.937). Model MDLM GIDD XDLM(k=1e-3) XDLM(k=0.1) XDLM(k=0.5) XDLM(k=0.9) UDLM PPL 23.321 23. 23.495 24.097 24.818 25.731 25.937 Table 8. Zero-shot PPL evaluation on seven external benchmarks. XDLM (k = 0.1) demonstrates robust generalization, achieving an average PPL (54.110) comparable to pure mask-based methods (MDLM/GIDD) and significantly outperforming UDLM (59.574). Dataset AG News LAMBADA LM1B-GPT2 PTB ArXiv PubMed WikiText Average MDLM GIDD XDLM(k=1e-3) XDLM(k=0.1) XDLM(k=0.5) XDLM(k=0.9) UDLM 61.374 60.607 62.247 62.768 67.352 71.791 69.402 47.967 47.811 45.982 45.608 47.966 49.094 51.272 65.629 65.898 65.944 68.229 71.252 74.116 75.572 89.049 86.911 89.575 90.796 91.683 97.266 95.986 37.457 39.019 38.294 37.232 38.460 39.658 42.671 41.981 42.634 42.206 41.391 43.373 44.701 47. 32.093 30.809 31.809 32.748 33.570 35.175 34.933 53.650 53.384 53.722 54.110 56.236 58.829 59.574 Figure 8. Average Zero-Shot PPL trajectory throughout training. Consistent with the final results, XDLM (k = 0.1) closely aligns with the MDLM baseline from early stages (64.85) to convergence (54.11), while the UDLM curve remains notably higher across all training steps. 24 Balancing Understanding and Generation in Discrete Diffusion Models Table 9. Detailed evolution of Zero-Shot PPL throughout training. Evaluated at 100k-step intervals, the data confirms that XDLM (k = 0.1) maintains learning dynamic nearly identical to MDLM, highlighting the methods stability and efficiency from early training to convergence. Train Steps 100k 200k 300k 400k 500k 600k 700k 800k 900k 1M MDLM GIDD XDLM(k=1e-3) XDLM(k=0.1) XDLM(k=0.5) XDLM(k=0.9) UDLM 64.209 61.097 63.748 64.847 68.074 70.264 71.220 59.315 57.900 59.097 60.187 62.524 65.384 65.789 57.411 56.326 57.319 58.253 60.489 63.147 63.692 56.129 55.389 56.374 57.201 59.353 61.758 62.320 55.434 54.848 55.576 56.288 58.396 60.977 61.258 54.943 54.294 54.937 55.797 57.881 60.263 60. 54.466 53.960 54.577 55.271 57.437 59.723 60.577 54.201 53.807 54.336 54.872 56.976 59.438 60.192 53.839 53.258 53.977 54.500 56.530 58.915 59.711 53.650 53.384 53.722 54.110 56.236 58.829 59.574 H. Detailed Language Generation Results In this section, we present the comprehensive numerical results for the language generation experiments initially discussed in Sec. 4.2. To systematically analyze the trade-off between the absorbing (MDLM-like) and uniform (UDLM-like) diffusion processes, we evaluate XDLM across varying mixing ratios {103, 0.1, 0.5, 0.9}. For generation, we adopt the ancestral sampling setting (Sahoo et al., 2024; Schiff et al., 2024; Sahoo et al., 2025), employing parallel token sampling from the estimated posterior via Gumbel noise. The resulting performance is evaluated based on sample quality, quantified by GPT-2 Large perplexity (PPL ), and diversity, measured by Token Entropy (). Tab. 10 presents the comprehensive evaluation on the OWT dataset. The data quantitatively confirms that governs the models performance across different numbers of sampling steps, effectively interpolating between the behaviors of UDLM and MDLM. In the few-step regime (e.g., 8 to 32 sampling steps), increasing significantly improves performance, aligning XDLM with the efficiency of UDLM. For instance, with only 8 sampling steps, XDLM (k = 0.9) yields low perplexity of 189.750, which is highly competitive with UDLM (183.991) and vastly superior to the mask-based baselines, MDLM (711.382) and GIDD (654.841). Conversely, in the multi-step regime (e.g., 512 to 1024 sampling steps), retaining mask-like properties (lower k) remains advantageous for achieving high-fidelity generation. XDLM (k = 0.1) demonstrates an optimal balance. It dramatically improves upon MDLM when sampling budget are limited, while retaining the capacity to reach very low perplexity scores (52.609 at 1024 steps) when more sampling steps are available, all without sacrificing the diversity indicated by entropy. These findings are corroborated by the results on the LM1B dataset, detailed in Tab. 11. We observe consistent trend where higher dominates when the number of sampling steps is small. At 4 sampling steps, XDLM (k = 0.5) achieves perplexity of 232.702, effectively matching UDLM (232.403) and significantly outperforming MDLM (377.177). As the number of sampling steps increases, the performance gaps narrow, yet XDLM remains robust across the spectrum. Collectively, these results demonstrate that by tuning k, XDLM bridges the gap between the two paradigms: it captures the efficiency of uniform noise at few sampling steps while preserving the high-quality potential of masking at larger steps. 25 Balancing Understanding and Generation in Discrete Diffusion Models Table 10. Quality (PPL) vs. Diversity (Entropy) on OWT. The results demonstrate that the optimal mixing ratio depends on the sampling budget. In the few-step regime (e.g., 8 steps), higher is essential for efficiency, allowing XDLM to match UDLM performance. Conversely, in the multi-step regime, lower becomes advantageous for achieving high fidelity. Notably, XDLM (k = 0.1) strikes an optimal balance, dramatically outperforming MDLM at low step counts while reaching superior perplexity at high step counts. Model step 8 step 16 step 32 step 64 step 128 step step 512 step 1024 MDLM GIDD XDLM(k=1e-3) XDLM(k=0.1) XDLM(k=0.5) XDLM(k=0.9) UDLM 711.382 654.841 613.647 301.640 218.572 189.750 183.991 288.157 273.829 253.897 161.724 127.335 117.234 111.237 Perplexity 156.576 151.331 146.567 114.795 98.817 93.239 88.632 106.098 102.558 104.090 94.837 86.956 84.068 79.509 Entropy MDLM GIDD XDLM(k=1e-3) XDLM(k=0.1) XDLM(k=0.5) XDLM(k=0.9) UDLM 5.872 5.832 5.837 5.662 5.574 5.546 5.536 5.767 5.748 5.746 5.630 5.565 5.547 5. 5.687 5.675 5.673 5.600 5.555 5.548 5.543 5.614 5.605 5.613 5.573 5.547 5.548 5.535 80.198 77.548 80.301 83.205 81.051 80.614 76.611 5.543 5.534 5.546 5.548 5.538 5.548 5.536 64.376 62.077 65.245 73.824 77.155 79.456 74.019 5.472 5.462 5.475 5.516 5.525 5.547 5. 51.756 49.822 52.650 63.888 73.503 78.707 73.345 5.387 5.370 5.393 5.467 5.514 5.544 5.524 41.545 39.981 42.797 52.609 66.136 78.238 106.666 5.287 5.269 5.298 5.390 5.476 5.542 5.769 Table 11. Quality (PPL) vs. Diversity (Entropy) on LM1B. Corroborating the OWT findings, higher proves dominant when the sampling budget is strictly limited. At 4 steps, XDLM (k = 0.5) effectively matches UDLM. Across the full spectrum of sampling steps, XDLM bridges the gap between paradigms, capturing the efficiency of uniform noise for small budgets while preserving the high-quality potential of masking for larger budgets. Model step 4 step 8 step 16 step 32 step 64 step Perplexity MDLM GIDD XDLM(k=1e-3) XDLM(k=0.1) XDLM(k=0.5) UDLM 377.177 392.002 343.246 246.134 232.702 232.403 215.579 215.873 199.457 155.487 141.121 136.914 152.939 151.824 146.612 123.129 116.063 110.575 124.731 124.014 124.444 113.006 106.182 102. 112.049 110.247 111.960 105.509 103.419 98.571 102.142 101.211 103.544 101.983 100.754 96.385 MDLM GIDD XDLM(k=1e-3) XDLM(k=0.1) XDLM(k=0.5) UDLM 4.405 4.401 4.382 4.326 4.309 4.303 Entropy 4.379 4.373 4.363 4.329 4.315 4. 4.366 4.361 4.357 4.334 4.325 4.320 4.357 4.359 4.358 4.342 4.335 4.327 4.353 4.354 4.354 4.345 4.340 4.334 4.347 4.348 4.351 4.346 4.342 4.336 I. Detailed Image Generation Results We present an extended analysis of XDLMs image generation capabilities on ImageNet-1K and CIFAR-10, specifically examining the impact of the mixing ratio parameter k. As defined in our methodology, governs the balance between the mask-based and uniform-noise processes. higher increases the proportion of uniform noise (effectively approaching the UDLM setting), while lower retains more mask-like properties. The results in Tab. 12, 13, 14, 15 empirically validate the utility of this interpolation. We observe two key behaviors: first, consistent convergence of XDLM towards 26 Balancing Understanding and Generation in Discrete Diffusion Models UDLM behavior as increases; and second, the fact that the optimal performance often lies within the interpolated regime (0 < < 1) rather than at the extremes (pure MDLM or UDLM). Tab. 12 and 13 detail the performance on ImageNet-1K, providing compelling evidence that the optimal varies depending on the generation setting. In the standard conditioning regime (Tab. 12), balanced mix is preferred: XDLM with = 0.5 achieves an FID of 23.417 at 16 steps, significantly outperforming both the low-mixing variant (k = 103) and the specialized UDLM baseline (26.242). However, when Classifier-Free Guidance (CFG) is applied (Tab. 13), the optimal operating point shifts. Here, = 0.1 emerges as the sweet spot, achieving an FID of 8.625, which surpasses both the pure UDLM (8.980) and the = 0.5 variant (8.790). These findings highlight the advantage of XDLM. By exploring the interpolation space between masking and uniform noise, we can identify configurations that outperform both individual baselines. Tab. 14 and 15 further illustrate the behavioral shift controlled by on CIFAR-10. On this benchmark, where UDLM demonstrates distinct advantage, we observe clear monotonic improvement in generation quality as rises. For instance, under standard conditioning, increasing from 103 to 0.5 improves the 32-step FID from 164.040 to 56.299, dramatically closing the gap with UDLM (41.027). similar trend holds with CFG, where = 0.5 approaches the UDLM benchmark. Collectively, these results confirm that XDLM successfully bridges the gap between domains. Crucially, it demonstrates that there is no single fixed that is optimal for all scenarios; instead, the flexibility to tune within the interval (0, 1) is essential for maximizing performance across different datasets and guidance regimes. Table 12. Comparison of FID () and IS () on ImageNet-1K. For image generation, XDLM proves to be very strong contender, achieving performance highly comparable to the specialized UDLM model. Notably, XDLM outperforms all baselines, including UDLM, at 16 generation steps, confirming its robust performance beyond the text domain. Model FID IS step 4 step 8 step step 32 step 4 step 8 step 16 step 32 MDLM GIDD XDLM(k=1e-3) XDLM(k=0.1) XDLM(k=0.5) UDLM 80.752 86.842 81.992 54.085 45.808 49.861 47.732 54.933 52.590 34.109 29.362 30.144 28.785 35.403 35.046 25.774 23.417 26.242 18.928 24.588 25.637 22.187 21.486 25.661 16.287 14.559 15.597 24.829 29.310 27.049 29.178 24.297 24.813 36.964 40.482 38. 44.656 35.698 35.097 43.903 46.620 41.801 57.803 46.376 44.032 48.118 48.911 41.850 Table 13. Performance on ImageNet-1K with CFG (scale = 2.0). The application of guidance shifts the optimal operating point. Here, = 0.1 emerges as the sweet spot, achieving an FID of 8.625 at 16 steps. This configuration surpasses both the pure UDLM baseline (8.980) and the higher-mixing variant (k = 0.5), demonstrating the importance of tuning for specific guidance regimes. Model FID IS step 4 step 8 step 16 step 32 step 4 step step 16 step 32 MDLM GIDD XDLM(k=1e-3) XDLM(k=0.1) XDLM(k=0.5) UDLM 33.468 41.000 32.876 13.550 11.945 14.055 11.144 15.151 12.872 8.956 9.038 9.718 6.725 7.076 7.612 8.625 8.790 8. 7.485 6.314 7.588 8.914 8.939 8.650 27 54.740 44.084 53.351 107.403 114.135 97.859 119.150 95.789 102.601 148.723 145.718 123.582 172.664 148.148 144.019 165.916 156.339 132.099 203.722 181.017 167.660 171.046 159.829 132. Balancing Understanding and Generation in Discrete Diffusion Models Table 14. Class-conditional generation on CIFAR-10. On this benchmark, where UDLM holds distinct advantage, we observe clear monotonic improvement in XDLM quality as increases. Increasing from 103 to 0.5 dramatically closes the performance gap with UDLM (e.g., improving 32-step FID from 164.040 to 56.299), confirming that higher effectively aligns XDLM with uniform noise dynamics. Model MDLM GIDD XDLM(k=1e-3) XDLM(k=0.1) XDLM(k=0.5) UDLM step 32 211.983 193.487 164.040 77.985 56.299 41. FID step 128 69.151 72.579 50.638 48.702 37.307 27.822 step 512 step 32 IS step 128 step 33.961 41.809 29.079 43.210 33.030 25.144 2.685 2.737 3.427 5.316 6.077 6.892 5.615 5.176 6.223 6.284 6.801 7.345 6.801 6.209 7.079 6.513 6.956 7.372 Table 15. Performance of XDLM and baselines on image generation with CFG scale = 2.0 on CIFAR-10. All mask-based methods (MDLM, GIDD, XDLM) show significant improvement when CFG is applied. The basic trend follows the results with no cfg. Model MDLM GIDD XDLM(k=1e-3) XDLM(k=0.1) XDLM(k=0.5) UDLM step 32 181.453 179.916 130.106 59.600 40.850 30.328 FID step 128 51.113 60.443 35.980 36.889 27.445 20.720 step step 32 IS step 128 step 512 22.762 30.812 19.877 32.444 24.445 18.592 3.329 3.054 4.559 6.699 7.560 8.098 6.969 6.124 7.575 7.596 7.881 8. 8.078 7.216 8.186 7.670 7.893 8.433 J. Detailed LLaDA Continual Pretraining Results To empirically validate the efficacy of the XDLM objective within the LLaDA framework, we conducted continual pre-training experiment utilizing the LLaDA 8B Base checkpoints. We integrated the XDLM modeling definitions into the transformers codebase and trained on 10 billion token sample of the FineWeb-Edu dataset (Penedo et al., 2024). The training process spanned 600 steps with global batch size of 512 and sequence length of 4,096. The learning rate was warmed up to 2 105 over the first 100 steps and held constant for the subsequent 500 steps, with the XDLM mixing ratio set to 0.1. All experiments were conducted on compute node equipped with 8 Nvidia H800 GPUs. To rigorously attribute performance gains to the architectural paradigm rather than confounding variables, we established two robust baselines: LLaDA-MDLM, control condition where the model is continual pre-trained for the same duration using the standard Masked Diffusion Language Model objective; and LLaDA-XDLM-infer, an ablation where the XDLM sampling strategy is applied to the base model without further training. To deploy XDLM sampling within this architecture, we extended the standard low-confidence remasking strategy used in LLaDA. Algorithm 1 details the generation logic, highlighting the novel contributions in the conditional block controlled by the mixing ratio k. While standard LLaDA sampling operates via monotonic reduction of uncertainty, prioritizing only the filling of existing [MASK] tokens, our implementation introduces refinement branch. When > 0.0, the algorithm calculates refine_priority by identifying tokens that are currently unmasked ( is_masked) but for which the model predicts conflicting token ID (pred_tokens != token_ids) with confidence score exceeding the current tokens confidence. These tokens are flagged in update_mask alongside the standard empty masks. This mechanism fundamentally transforms the generation from purely additive process into dynamic error-correction loop, allowing the model to stochastically change its mind and revise previous outputs during the denoising trajectory. The empirical results, summarized in Tab. 16, demonstrate the superiority of this formulation across reasoning (GSM8K, MATH, BBH) and code generation (HumanEval, MBPP) benchmarks, particularly in low-compute regimes. At 32 sampling steps, LLaDA-XDLM significantly outperforms all baselines. The most distinct advantage is observed in the MBPP code generation task, where LLaDA-XDLM achieves score of 15.00, effectively doubling the performance of the base LLaDA (6.80) and the continued MDLM control (4.40). granular analysis of the MBPP failure modes reveals that this improvement Balancing Understanding and Generation in Discrete Diffusion Models is driven by fundamental enhancement in generative fidelity; LLaDA-XDLM drastically reduces the incidence of Failed cases, defined as non-compilable or syntactically invalid code, from 429 (LLaDA) to 304. By successfully converting these previously unproductive attempts into Passed outputs (75 vs. 34), the model confirms that the XDLM objective, combined with the refinement sampling strategy, provides the necessary structural coherence to self-correct syntactic errors that would otherwise lead to compilation failures. As sampling steps increase to 64, 128, and 256 (the total generation length), LLaDA-XDLM maintains competitive edge, validating that the performance gains stem directly from the internal representation learned during XDLM training rather than being an artifact of the sampling algorithm alone. Notably, we observe that the continual pretraining baseline (LLaDA-MDLM) exhibits performance decline compared to the original LLaDA, likely due to the distribution shift between the tuning data and the original pretraining corpus. Consequently, the superior performance of LLaDA-XDLM over LLaDA-MDLM confirms that our gains stem from the effectiveness of the XDLM formulation rather than merely the additional 600 training steps. Furthermore, LLaDA-XDLM significantly outperforms the ablation baseline LLaDA-XDLM-infer, demonstrating that the improvement is intrinsic to the learned model rather than being an artifact of sampling tricks. Algorithm 1 Generation Strategy of LLaDA-XDLM def generate(steps, seq_length, mask_id, topk_absorb, topk_uniform, k=0.1): \"\"\" Args: steps (int): Total number of sampling steps. seq_length (int): Target sequence length to generate. mask_id (int): Vocabulary index of the [MASK] token. topk_absorb (list[int]): Schedule for the number of tokens to decode at each step. topk_uniform (list[int]): Schedule for the number of tokens to refine at each step. (float): The mixing ratio. Returns: token_ids (torch.Tensor): The generated sequence of token IDs with shape (seq_length,). \"\"\" token_ids = torch.full((seq_length,), fill_value=mask_id, dtype=torch.long) for in range(steps): is_masked = (token_ids == mask_id) logits = model(token_ids).logits logits_with_noise = add_gumbel_noise(logits) pred_tokens = torch.argmax(logits_with_noise, dim=-1) probs = torch.softmax(logits, dim=-1) pred_confidence = torch.squeeze(torch.gather(probs, dim=-1, index=torch.unsqueeze(pred_tokens, -1)), -1) mask_priority = torch.where(is_masked, pred_confidence, -np.inf) if > 0.0: current_confidence = torch.squeeze(torch.gather(probs, dim=-1, index=torch.unsqueeze(token_ids, -1)), -1) can_be_refined = (is_masked) & (pred_tokens != token_ids) & (pred_confidence >= current_confidence) refine_priority = torch.where(can_be_refined, pred_confidence, -np.inf) update_mask = torch.zeros_like(pred_tokens, dtype=torch.bool) _, top_mask_indices = torch.topk(mask_priority, k=topk_absorb[i]) update_mask[top_mask_indices] = True if > 0.0: _, top_refine_indices = torch.topk(refine_priority, k=topk_uniform[i]) update_mask[top_refine_indices] = True token_ids[update_mask] = pred_tokens[update_mask] return token_ids 29 Balancing Understanding and Generation in Discrete Diffusion Models Table 16. Evaluation of LLaDA-XDLM on reasoning and code generation benchmarks. We compare the proposed method with the base LLaDA, an inference-only ablation (LLaDA-XDLM-infer), and an MDLM control (LLaDA-MDLM) across varying sampling steps. MBPP scores are decomposed into failure modes to illustrate structural correctness. Dataset BBH GSM8K MATH HumanEval MBPP Failed Wrong"
        },
        {
            "title": "Score",
            "content": "LLaDA LLaDA-XDLM-infer LLaDA-MDLM LLaDA-XDLM LLaDA LLaDA-XDLM-infer LLaDA-MDLM LLaDA-XDLM LLaDA LLaDA-XDLM-infer LLaDA-MDLM LLaDA-XDLM LLaDA LLaDA-XDLM-infer LLaDA-MDLM LLaDA-XDLM 42.68 41.74 43.21 42.76 47.16 46.76 48.32 45. 47.49 47.89 48.95 46.41 47.77 47.93 49.84 46.07 32 steps / 256 tokens 4.80 4.86 2.70 4.72 5.49 1.22 6.71 10.98 64 steps / 256 tokens 16.10 16.12 13.42 16.48 12.80 4.88 14.63 15.85 128 steps / 256 tokens 26.12 26.70 24.34 25.06 24.39 8.54 25.00 25.00 256 steps / 256 tokens 29.98 29.34 29.70 29.22 33.54 6.71 32.32 31.71 24.49 24.26 24.41 29.26 57.16 53.75 54.97 57.85 67.93 66.41 68.61 68.01 72.25 71.34 72.93 73. 6.80 5.40 4.40 15.00 17.80 17.60 13.40 23.60 27.60 27.40 30.00 30.80 40.40 37.00 37.60 34.60 429 438 447 304 328 328 348 161 187 169 130 48 74 51 90 37 35 31 121 83 84 84 182 199 174 178 215 248 238 260 34 27 22 75 89 88 67 118 138 137 150 154 202 185 188 173 K. Detailed Computational Efficiency Analysis We evaluate the computational efficiency of the proposed XDLM against baseline models by measuring speed and memory the forward-only case (representing NELBO calculation for perplexity consumption across three distinct scenarios: estimation), the forward-backward case (used in training), and the sampling case (used for generation). For the forward and forward-backward evaluations, we utilize randomly generated sequences with batch size of 32 and sequence length of 1024 to simulate real data processing. To ensure robust measurements, models are warmed up for 10 iterations, and we report the mean value calculated over the subsequent 100 runs. For the sampling scenario, we employ standard ancestral sampling to generate sequences of length 1024 over 32 steps. The evaluation batch size is set to 32. Models are warmed up for 1 iteration, and we report the mean value calculated over the subsequent 10 runs. As detailed in Tab. 17, MDLM exhibits the highest throughput and lowest memory cost across all settings, result expected due to the simplicity of its absorbing noise kernel. However, among models incorporating uniform noise distributions, XDLM achieves the highest throughput across the forward, forward-backward, and sampling regimes. This performance is directly attributed to our scalar reformulated efficient sampling and training strategy, which circumvents expensive matrix operations. In contrast, while UDLM maintains the third-highest throughput in forward and training modes, its sampling throughput is the lowest among the comparison group. Conversely, GIDD achieves the third-highest sampling throughput but performs poorly in NELBO calculation, reaching only half the forward throughput of XDLM. This trend is mirrored in memory consumption. While MDLM is the most lightweight overall, XDLM ranks first among methods whose noise kernel includes uniform noise component, effectively avoiding the high memory overhead observed in GIDD and UDLM. 30 Balancing Understanding and Generation in Discrete Diffusion Models Table 17. Computational Efficiency Comparison. We report throughput (tokens/s, ) and peak memory usage (GB, ) for MDLM, GIDD, XDLM, and UDLM. The metrics cover three scenarios: forward (inference/perplexity estimation), forward-backward (training), and sample (generation). Leveraging the scalar reformulated strategy, XDLM achieves highly competitive throughput and memory efficiency, outperforming other baselines incorporating uniform noise kernels (GIDD, UDLM) in most metrics. Model MDLM GIDD XDLM (k=0.1) UDLM forward 424294 199516 396398 370952 Throughput (token/s) forward-backward sample forward Memory (GB) forward-backward 141990 95395 137372 142276 8789 6336 7108 2882 6.285 25.131 18.850 18.850 35.354 51.060 41.634 44.777 sample 18.848 40.856 31.414 59. L. Detailed Training Dynamics In this section, we provide the comprehensive numerical data corresponding to the training dynamics analysis discussed in Sec. 4.3 and visualized in Fig. 4. These tables supplement the visual analysis by providing exact metric values for Perplexity, Entropy, FID, and IS across the training trajectory. Tab. 18 details the training dynamics on the LM1B benchmark over the course of 1 million training steps. The models were evaluated using fixed budget of 128 sampling steps. As indicated in the main text, while MDLM exhibits superior performance in the initial phase, UDLM demonstrates strong scaling in later training stages, achieving the lowest perplexity among baselines by 1M steps (96.385). Furthermore, the table details the impact of varying the mixing ratio within our proposed XDLM method. Complementing the text generation results, Tab. 19 presents the quantitative training dynamics for image generation on ImageNet-1K. Evaluations were conducted over 500k training steps using fixed budget of 16 sampling steps. The numerical data corroborates the visual trends: XDLM establishes and maintains decisive performance advantage. Notably, XDLM with = 0.5 achieves the lowest reported FID of 23.417 and the highest IS of 46.620 at the final checkpoint, consistently outperforming the MDLM and GIDD baselines. While UDLM starts with higher FID, it converges rapidly to competitive score (26.242), yet XDLM remains the leading model throughout the majority of the training phases. Table 18. Quantitative training dynamics on the LM1B benchmark, supplementing the visual analysis in Fig. 4. We report Perplexity () and Entropy () over 1M training steps, utilizing fixed budget of 128 sampling steps. The table details the impact of varying the mixing ratio in XDLM compared to MDLM, GIDD, and UDLM baselines. Train Steps 100k 200k 300k 400k 500k 600k 700k 800k 900k 1M Perplexity MDLM GIDD XDLM(k=1e-3) XDLM(k=0.1) XDLM(k=0.5) UDLM 113.218 107.366 112.628 119.632 117.846 116.021 107.864 103.473 108.381 110.803 111.024 107.800 106.735 103.386 106.406 109.017 108.130 104.508 105.839 101.854 104.913 106.265 105.955 102.466 104.594 102.367 105.156 105.428 104.953 101.480 104.106 101.745 103.817 104.572 103.048 99. 103.559 101.904 103.624 103.645 102.464 99.660 103.174 101.957 102.731 103.348 101.040 98.147 102.769 101.490 103.638 102.223 102.015 97.503 102.142 101.211 103.544 101.983 100.754 96.385 Entropy MDLM GIDD XDLM(k=1e-3) XDLM(k=0.1) XDLM(k=0.5) UDLM 4.350 4.352 4.351 4.353 4.350 4.349 4.348 4.350 4.347 4.347 4.346 4.343 4.350 4.351 4.349 4.347 4.345 4.341 4.347 4.349 4.347 4.344 4.344 4.340 4.349 4.351 4.350 4.344 4.342 4.338 4.349 4.349 4.347 4.344 4.343 4. 4.348 4.350 4.347 4.346 4.342 4.338 4.349 4.350 4.349 4.344 4.343 4.336 4.349 4.351 4.351 4.344 4.344 4.335 4.347 4.348 4.351 4.346 4.342 4.336 31 Balancing Understanding and Generation in Discrete Diffusion Models Table 19. Quantitative training dynamics on ImageNet-1K generation, corresponding to the curves in Fig. 4. We report FID () and IS () over 500k training steps, utilizing fixed budget of 16 sampling steps. The numerical data corroborates the visual trends: XDLM establishes and maintains decisive performance advantage. Train Steps MDLM GIDD XDLM(k=1e-3) XDLM(k=0.1) XDLM(k=0.5) UDLM 100k 200k 40.060 42.152 45.381 38.358 37.567 43. 33.723 36.878 38.967 31.464 30.098 34.525 FID 300k 31.090 35.200 36.061 27.524 26.687 30.381 400k 500k 100k 200k 29.436 35.677 36.287 26.853 23.790 28.086 28.785 35.403 35.046 25.774 23.417 26.242 28.978 27.337 24.967 27.949 27.854 23.619 36.136 33.042 30.512 35.839 35.682 31.188 IS 300k 39.804 35.544 33.948 41.150 40.724 35.963 400k 500k 42.893 35.173 34.031 42.222 45.976 38.588 44.656 35.698 35.097 43.903 46.620 41."
        }
    ],
    "affiliations": [
        "UCAS",
        "Xiaohongshu Inc."
    ]
}