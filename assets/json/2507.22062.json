{
    "paper_title": "MetaCLIP 2: A Worldwide Scaling Recipe",
    "authors": [
        "Yung-Sung Chuang",
        "Yang Li",
        "Dong Wang",
        "Ching-Feng Yeh",
        "Kehan Lyu",
        "Ramya Raghavendra",
        "James Glass",
        "Lifei Huang",
        "Jason Weston",
        "Luke Zettlemoyer",
        "Xinlei Chen",
        "Zhuang Liu",
        "Saining Xie",
        "Wen-tau Yih",
        "Shang-Wen Li",
        "Hu Xu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Contrastive Language-Image Pretraining (CLIP) is a popular foundation model, supporting from zero-shot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIP's training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., \"curse of multilinguality\" that is common in LLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present a recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, MetaCLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 9 2 ] . [ 1 2 6 0 2 2 . 7 0 5 2 : r MetaCLIP 2: Worldwide Scaling Recipe Yung-Sung Chuang1,2,, Yang Li1, Dong Wang1, Ching-Feng Yeh1, Kehan Lyu1, Ramya Raghavendra1, James Glass2, Lifei Huang1, Jason Weston1, Luke Zettlemoyer1, Xinlei Chen1,$, Zhuang Liu3, Saining Xie4, Wen-tau Yih1, Shang-Wen Li1,, Hu Xu1, 1FAIR, Meta, 2MIT, 3Princeton University, 4New York University Core Contributors, Work done during an internship, $Work done when working at Meta, Project Leads Contrastive Language-Image Pretraining (CLIP) is popular foundation model, supporting from zeroshot classification, retrieval to encoders for multimodal large language models (MLLMs). Although CLIP is successfully trained on billion-scale image-text pairs from the English world, scaling CLIPs training further to learning from the worldwide web data is still challenging: (1) no curation method is available to handle data points from non-English world; (2) the English performance from existing multilingual CLIP is worse than its English-only counterpart, i.e., curse of multilinguality that is common in LLMs. Here, we present MetaCLIP 2, the first recipe training CLIP from scratch on worldwide web-scale image-text pairs. To generalize our findings, we conduct rigorous ablations with minimal changes that are necessary to address the above challenges and present recipe enabling mutual benefits from English and non-English world data. In zero-shot ImageNet classification, MetaCLIP 2 ViT-H/14 surpasses its English-only counterpart by 0.8% and mSigLIP by 0.7%, and surprisingly sets new state-of-the-art without system-level confounding factors (e.g., translation, bespoke architecture changes) on multilingual benchmarks, such as CVQA with 57.4%, Babel-ImageNet with 50.2% and XM3600 with 64.3% on image-to-text retrieval. Date: July 30, 2025 Correspondence: Hu Xu huxu@meta.com, Shang-Wen Li shangwel@meta.com Code and Model: https://github.com/facebookresearch/MetaCLIP"
        },
        {
            "title": "1 Introduction",
            "content": "Contrastive Language-Image Pre-training (CLIP) (Radford et al., 2021) has become an essential building block of modern vision and multimodal models, from zero-shot image classification and retrieval to serving as vision encoders in multimodal large language models (MLLMs) (Grattafiori et al., 2024; Team et al., 2023; Liu et al., 2023; Bai et al., 2023). CLIP and its majority variants (Ilharco et al., 2021; Xu et al., 2024) adopt an English-only setting, and MetaCLIP (Xu et al., 2024) introduces scalable data curation algorithm to meticulously extract billion-scale English dataset that exhausts long-tailed concepts in Common Crawl. The algorithm transforms the distribution of the raw Internet into controllable and balanced training distribution defined by metadata (e.g., visual concepts composed by human experts) and training distribution is known as one key contributor to performance. In contrast, popular CLIP reproductions outsource such key contributor to external resources, e.g., OpenCLIP (Ilharco et al., 2021) trained on LAION (Schuhmann et al., 2021, 2022b) and DFN (Fang et al., 2023) rely on pretrained CLIP models for black-box filtering to keep only high-confidence data. Such approaches resemble distillation of an existing CLIP teacher model and produce untractable distributions owned by an outsourcing party. Although being the most widely used foundation models, most CLIP variants, including the scalable MetaCLIP, rely on English-only curation and thus discard the other, e.g., 50.9% (Wikipedia, 2025) of nonEnglish, worldwide web data. To extend CLIP training and data to the worldwide web for the next level of scaling, we inevitably have to handle these non-English image-text pairsa barrier we refer to as the worldwide scaling challenges, which are issues not yet being solved after years of attempts to train CLIP on multilingual data: 1 Figure 1 (Left) CLIP training suffers from the curse of multilinguality that the English performance of CLIP model trained on worldwide (i.e., English + non-English), billion-scale data is worse than its English-only counterpart, even when applying our recipe on ViT-L/14; scaling to ViT-H/14 enables non-English data helps English-only CLIP. (Right) English data also helps non-English CLIP. Challenge #1: Lack of fundamental data curation method to handle non-English data at scale. Existing attempts either conduct no curation on the raw, non-English image-text pair data at all (e.g., distilling from English CLIP (Chen et al., 2023a) or machine translation (Carlsson et al., 2022; Nguyen et al., 2024)), or rely on proprietary and private data sources (e.g., WebLI (Chen et al., 2023b) that drives mSigLIP and SigLIP 2 (Zhai et al., 2023; Tschannen et al., 2025) is built from Google Image Search (Juan et al., 2019)). Challenge #2: Worse English performance than English-only CLIP. This is also known as curse of multilinguality in text-only large language models (LLMs). For instance, mSigLIP is 1.5% worse than its English-only counterpart, SigLIP, on ImageNet (Zhai et al., 2023), while SigLIP 2 (Tschannen et al., 2025) prioritizes English performance at the cost of even worse multilingual results than mSigLIP. Hence, disparate models have to be used to optimize English and non-English performance at the same time. This work. We present MetaCLIP 2, the first ever recipe developing CLIP with training from scratch on native worldwide image-text pairs, without relying on outsourced resources, such as any private data, machine translation, or distillation. We empirically show that the curse of multilinguality in CLIP is the consequence of insufficient scaling due to the lack of proper recipe for worldwide data curation and model training. When metadata, data curation, model capacity, and training are carefully designed and scaled jointly, we show that not only the performance trade-offs between English and non-English data disappear, but the two become mutually beneficial. Achieving such worldwide scaling is highly desirable, especially when English Internet data is exhausted soon (Villalobos et al., 2022). Our MetaCLIP 2 recipe is built on top of English MetaCLIP, where overlapping with OpenAI CLIPs vanilla architecture is deliberately maximized. The overlap makes our findings generalizable to CLIP and its variants, compared to system works (c.f., (Zhai et al., 2023; Tschannen et al., 2025; Bolya et al., 2025)) aiming at state-of-the-art (SoTA) performance with combination of all available techniques. Such combination involves confounding factors or comparison on outsourced resources instead of CLIP itself. The MetaCLIP 2 recipe introduces three principled innovations for scaling to worldwide. 1) Metadata. We scale the English MetaCLIP metadata to 300+ languages on Wikipedia and multilingual WordNet. 2) Curation algorithm. We build per-language substring matching and balancing to curate concept distribution for non-English data similar to the English counterpart. 3) Training framework. We design the first worldwide CLIP training framework, including an increase of seen image-text pairs during training proportional to the increased data size from the added non-English data examples, and study on minimal viable model capacity to learn from worldwide scale data. As shown in Fig. 1, although ViT-L/14 (the largest model size used by OpenAI) still suffers the curse of multilinguality, ViT-H/14 breaks the curse. English accuracy rises from 80.5% to 81.3% on ImageNet and surprisingly new SoTA is set with minimal CLIP architecture changes for multilingual image-to-text 2 retrieval (XM3600 64.3%, Babel-ImageNet 50.2%, and CVQA 57.4%). Together, MetaCLIP 2 enables the following desirable results by nature. 1) Mutual benefits from the English and non-English worlds. Non-English data now can better support an English-only model and vice versa, which is critical in the era when English data is depleting. 2) Full multilingual support. MetaCLIP 2 never drops imagetext pairs simply by languages and yields models outperforming all the previous multilingual systems, such as mSigLIP (Zhai et al., 2023) and SigLIP 2 (Tschannen et al., 2025). 3) Native-language supervision. Models learn directly from alt-texts written by native speakers rather than synthetic machine translations (Pouget et al., 2024; Nguyen et al., 2024). 4) Cultural diversity. MetaCLIP 2 retains the entire global distribution of images and thus inherits the comprehensive cultural and socioeconomic coverage advocated by (Pouget et al., 2024). Such coverage improves geo-localization and region-specific recognition. 5) No-filter philosophy. With the curation algorithm designed towards worldwide data, MetaCLIP 2 removes the last filter (i.e., whether the alt-text is in English) in pipeline, achieving better diversity and minimizing biases introduced by filters (Pouget et al., 2024). 6) Broader impacts on foundation data. This work provides foundational dataset of image-text pairs at worldwide scale, and benefits not only CLIP, but also efforts using CLIP data such as MLLM (Grattafiori et al., 2024; Team, 2024), SSL (Web-DINO (Fan et al., 2025)) and image generation (DALL-E (Ramesh et al., 2021) and diffusion models (Yang et al., 2023))."
        },
        {
            "title": "2.1 Evolution of CLIP and its Data Processing",
            "content": "CLIP (Radford et al., 2021) and its variants (Jia et al., 2021; Ilharco et al., 2021; Zhai et al., 2023) learn versatile image and text representations that are generally useful for downstream tasks (Grattafiori et al., 2024; Dai et al., 2023; Liu et al., 2023). Such multimodal contrastive learning and transformer architectures become standard components in vision and multimodal research. Data is key contributor to CLIPs performance (Gadre et al., 2023; Xu et al., 2024). Two major processing approaches for CLIP data emerge: curation1 from scratch, and distillation from external resources. One key difference is that the former yields more controllable distribution and the latter has untractable distribution owned by an outsourcing party. Curation from scratch. OpenAI CLIP (Radford et al., 2021) curates training dataset of 400M image-text pairs from scratch and publicizes high-level curation guidance. MetaCLIP (Xu et al., 2024) makes OpenAIs guidance as formal curation algorithm and scales the curation to 2.5B pairs. The algorithm is model-free, no blackbox filtering, and fully transparent to enable training entirely from scratch on public data source, where the data distribution is curated to align with metadata composed by human experts (e.g., WordNet and Wikipedia). Distillation from external resources. Distillation-based methods usually have good performance and save compute by learning from teacher models knowledge (Hinton et al., 2015). However, in the context of CLIP training the teacher is usually an external blackbox system, which introduces untractable bias. For example, LAION-400M/5B (Schuhmann et al., 2021, 2022a) (used by OpenCLIP (Ilharco et al., 2021)) relies on OpenAI CLIP-filter and DFN (Fang et al., 2023) using filter model trained on high-quality private data (Ranasinghe et al., 2023). Recently, SigLIP (Zhai et al., 2023) and SigLIP 2 (Tschannen et al., 2025) learn from data source WebLI (Chen et al., 2023b), which is derived from Google Image Search (Juan et al., 2019)."
        },
        {
            "title": "2.2 Vision Encoding",
            "content": "CLIP-style models are widely used as vision encoders in MLLM, where language supervision in CLIP training helps to learn compact and semantic-rich visual representations. In contrast, traditional visual representation learning is based on self-supervised learning (SSL) methods like SimCLR (Chen et al., 2020), DINOv2 (Oquab et al., 2024), and purely relies on the full visual signal without language bias. There are variants that take advantage of both. SLIP (Mu et al., 2021) combines language and SSL supervision; LiT (Zhai et al., 2022) trains vision encoder first and conducts language alignment later; Perception Encoder (Bolya et al., 1Here, curation refers to select and align training data distribution with human from raw data source, excluding data filtering that is also referred to as curation in many works like DataComp (Gadre et al., 2023; Li et al., 2024) and DFN (Fang et al., 2023). 3 2025) shows early layers of CLIP representation yields vision-driven features with less semantic alignment. Recently, Web-DINO (Fan et al., 2025) shows SSL has better scalability on MetaCLIP curated large-scale data. In summary, CLIP focuses on human-aligned representations optimized for compact models and efficient downstream uses; SSL models aim to preserve all visual information as general pretraining approach. We envision more synergy from the two research lines due to complementarity."
        },
        {
            "title": "2.3 Multilingual CLIP Models",
            "content": "Due to the lack of open source curation for public worldwide data, initial attempts to multilingual CLIP models are mainly distillation approaches. M-CLIP (Carlsson et al., 2022) and mCLIP (Chen et al., 2023a) simply leverage existing English-only CLIP as the vision encoder and trains multilingual text encoder with low-quality multilingual pairs. To incorporate non-English data, subsequent works (Santos et al., 2023; Nguyen et al., 2024; Pouget et al., 2024) leverage machine translation techniques, either translating non-English captions into English or vice versa. These distillation-based models carry existing English CLIP bias or translation bias on nonhuman-captioned data. mSigLIP (Zhai et al., 2023) substantially advanced multilingual performance by leveraging multilingual data from WebLI (Chen et al., 2023b), which is an undisclosed dataset built with private data processing pipeline instead of publicly available worldwide data curation algorithm. However, mSigLIP and other multilingual CLIP models suffer from the curse of multilinguality, e.g., mSigLIP is 1.5% worse in ImageNet accuracy than its English-only counterpart SigLIP. Recently, SigLIP 2 adopts notably English-centric design of having 90% of its data in English, which is much higher than mSigLIP. Mixed results are also observed (Wang et al., 2025) on English benchmarks when scaling SigLIP from WebLIs 10B to 100B raw data, suggesting the challenges of scaling WebLI beyond."
        },
        {
            "title": "3 The MetaCLIP 2 Recipe",
            "content": "Our recipe of scaling CLIP to native worldwide data and training comprises three steps shown in Fig. 2: (1) constructing worldwide metadata, (2) implementing worldwide curation algorithm, and (3) building training framework for worldwide model. For generalizable recipe and findings, MetaCLIP 2 is designed to maximize overlapping with OpenAI CLIP and MetaCLIP, and only adopts necessary changes to learn from worldwide data. Figure 2 Overview of MetaCLIP 2 recipe: scaling CLIP data and training to worldwide scope."
        },
        {
            "title": "3.1 Revisit of MetaCLIP Algorithm",
            "content": "We revisit the original MetaCLIP algorithm to illustrate how English-based CLIP data is curated with metadata constructed from human knowledge. The algorithm first constructs metadata M, list of highquality visual concepts, from corpora written by human experts. contains 500k entries, combination and deduplication of entities from four high-quality sources: 1) all English WordNet Synsets, 2) Wikipedia 4 English unigrams, and 3) bigrams, and 4) Wikipedia page titles. Then, the algorithm performs substring matching on each alt-text (from given image-text pair in the data pool D) using metadata to obtain list matched_entry_ids. Global counting is conducted to calculate the number of matches over for each entry in as entry_count. Finally, the algorithm applies balancing to transform the raw image-text pair distribution into distribution that is balanced for head and tail concepts and ready for training, by associating each pair with sampling probability. Specifically, the count per entry is first converted into probability of sampling each entry, entry_prob, where tail entries (defined as entry_count < t) have probability set to 1, and all the other head entries have t/entry_count as sampling probabilities. Each pair is then sampled based on probabilities of matched entries in its alt-text. Here, is threshold to decide head vs. tail entries and set to 20k in OpenAI CLIP; MetaCLIP raised to 170k for scaling to billion English pairs."
        },
        {
            "title": "3.2 Worldwide Metadata",
            "content": "We address the first challenge for worldwide scaling by constructing the missing metadata to cover the non-English world. We maintain independent metadata per language since such design is intuitive (e.g., the same word mit has different meaning in English and Germany), has better performance (see ablation in Sec. 4.2.2), and is flexible for adding and curating new set of languages in future. Our metadata is from the same four sources as OpenAI CLIP and MetaCLIP, but beyond English. Key changes are highlighted as follows. 1) Multilingual WordNet: we include all synsets from 31 languages. 2) Wikipedia Unigrams and 3) Bigrams: we process unigram and bigram from Wikipedia dumps dated on May 2024, which include corpora in 329 languages. We clean the corpora into plain text with WikiExtractor (Attardi, 2015). For most languages, we use space and punctuation to tokenize text into words, and then count unigrams and bigrams. For languages without space separation (e.g., some Asian languages), we use open-source tokenizers (see Table 5 in Appendix) developed by local communities to properly split text into words and meanwhile maintain the semantic integrity. 4) Wikipedia Titles: we use page titles from 40 random dates of Wikipedia snapshots and rank these titles by click-through traffic for each language."
        },
        {
            "title": "3.3 Curation Algorithm",
            "content": "Next, we scale curation to worldwide data language-by-language. The curation algorithm is detailed below and summarized in pseudo-code as Algorithm 1. First, we conduct language identification (LID) (Grave et al., 2018) to classify the language of the alt-text from an image-text pair, and choose language-specific metadata to match concepts. The sets of languages covered by LID and metadata sources (e.g., Wikipedia) are usually different, so we first establish mapping between one language in LID to unique set of languages in metadata entries. The languages in the metadata mapped to the same language in LID are merged into one group. This ends with dictionary representation of metadata, M, where the keys are each language in LID and the values are the combined metadata of each group of languages. We also include key other for metadata of languages that cannot be associated with any language in LID. Each alt-text (text) in is applied with LID for predicting its language (text.lang). After that, as in the MetaCLIP algorithm summarized in Sec. 3.1, we run substring matching with metadata corresponding to predicted languages: matched_entry_ids = substr_match(text, M[text.lang]), and aggregate global count, the number of matches of each entry, in entry_counts. With counts calculated, we balance occurrence of concepts across pairs. In data curation for English CLIP described above, threshold is designed to limit the matches per metadata entry, where entries with matches fewer than are defined as tail entries (or concepts) and otherwise head. Image-text pairs from head concepts are downsampled by sampling probability derived from to balance training data distribution. Thus, depends on the size of raw data pool (e.g., larger pool has higher counts for the same entry). OpenAI CLIP sets to 20k for 400M pairs; MetaCLIP (Xu et al., 2024) tunes to 170k for scaling the training dataset to 2.5B and keeping the same ratio, 6% of matches from tail concepts, that OpenAI CLIP leverages to obtain the 400M pairs. For worldwide data, the data size and the counts of matches differ greatly across languages, so should be language-dependent. Applying single threshold to all languages yields suboptimal performance, e.g., larger for language with fewer pairs may yield too many pairs of head concepts and dilutes tail concepts in the curated data (see Sec. 4.2.2). 5 To derive for each language, we leverage the invariance assumption adopted in MetaCLIP algorithm design, the percentage of tail matches (i.e., 6%), and apply it across languages. With this assumption, we determine in two steps. (1) From ten to p: we calculate the global tail proportion for all languages, based on matches of English tail entries decided by ten. (2) From to tlang: for each non-English language, we reversely find the language-specific threshold tlang based on the calculated to ensure the same tail proportion across all languages. Detailed implementation of these two steps is shown as the t_to_p() and p_to_t() functions in Algorithm 1. With tlang, entry_counts is converted to entry_probs similarly as in MetaCLIP but for each language. Putting everything computed together, Algorithm 1 takes raw image-text pairs D, metadata M, and an arbitrary threshold for English ten as input, and outputs curated dataset of balanced and diverse training pairs, D, with three stages. Stage 1 performs language-specific substring matching for each alt-text, text, based on LID results and corresponding metadata, and obtains match counts, entry_counts, for each language and entry. Stage 2 computes thresholds tlang from ten. Stage 3 samples image-text pairs based on matched entries in text with probabilities entry_probs. Pairs matched with tail entries are always selected (i.e., probability = 1.0); pairs with head entries have sampling probabilities tlang / entry_counts[lang]. Sampled pairs compose D. Algorithm 1: Pseudo-code of MetaCLIP 2 Curation Algorithm in Python/NumPy. \"\"\" Input: D(list) raw (image, text) pairs: each text is assigned with language \"text.lang\" by LID; M(dict) worldwide metadata: key->language code; value(list)->metadata for that language; t_en(int) English threshold on counts of head/tail entry cutoff: OpenAI CLIP->20k, MetaCLIP->170k; Output: D_star(list): curated image-text pairs; \"\"\" # helper functions to compute for each language. def t_to_p(t, entry_count): return entry_count[entry_count < t].sum() / entry_count.sum() def p_to_t(p, entry_count): sorted_count = np.sort(entry_count) cumsum_count = np.cumsum(sorted_count) cumsum_prob = cumsum_count / sorted_count.sum() return sorted_count[(np.abs(cumsum_prob - p)).argmin()] # Stage 1: sub-string matching. entry_counts = {lang: np.zero(len(M[lang])) for lang in M} for image, text in D: # call substr_match which returns matched entry ids. text.matched_entry_ids = substr_match(text, M[text.lang]) entry_counts[text.lang][text.matched_entry_ids] += 1 # Stage 2: compute for each langauge. = t_to_p(t_en, entry_counts[\"en\"]); = {} for lang in entry_counts: t[lang] = p_to_t(p, entry_counts[lang]) # Stage 3: balancing via indepenent sampling per language. entry_probs = {} for lang in entry_counts: entry_counts[lang][entry_counts[lang] < t[lang]] = t[lang] entry_probs[lang] = t[lang] / entry_counts[lang] D_star = [] for image, text in D: for entry_id in text.matched_entry_ids: if random.random() < entry_probs[text.lang][entry_id]: D_star.append((image, text)) break"
        },
        {
            "title": "3.4 Training Framework",
            "content": "Adopting data prepared with worldwide curation in current CLIP training framework addresses the first challenge, but curse of multilinguality still exists as shown in Fig. 1. Thus, we further design the worldwide CLIP training framework. To make our framework and findings generalizable to CLIP and its variants, our framework follows OpenAI/MetaCLIPs training setting and model architecture with three additions: (1) multilingual text tokenizer, (2) scaling seen training pairs, and (3) study of minimal viable model capacity. The first is required to support worldwide languages and discussed in Sec. 4.2.2 for various choices; details of the latter two are described below. Scaling seen pairs. Expanding from an English-only dataset and distribution to worldwide naturally increases the number of available image-text pairs. Training CLIP for worldwide distribution with the same number of seen pairs as English CLIP downsamples English training pairs and harms English performance. Hence, we scale seen pairs proportionally to the growth of data size from non-English pairs, to ensure the amount of English seen pairs unchanged during the worldwide CLIP training. This is achieved by increasing the global training batch size, which encourages cross-lingual learning, and meanwhile keeping the other training hyperparameters unchanged. We choose 2.3 scaling of global batch to reflect that English pairs constitute 44% of our training data. We ablate other choices of global batch size in Sec. 4.2.1. Minimal viable model capacity. Lastly, we study the minimal model expressivity to enable learning on extra seen pairs and break the curse of multilinguality. As in Fig. 1, we find that even ViT-L/14 (largest model provided by OpenAI) suffers from the curse due to deficient capacity, and ViT-H/14 is the inflection point to break the curse (strong performance improvement in both English and non-English tasks)."
        },
        {
            "title": "4.1 Dataset and Training Setup",
            "content": "Following MetaCLIP pipeline, we collect image-text pairs sourced from the Internet that are publicly available. After LID, there are about 44% of alt-texts are in English, which are on par with the scale of English-only data from MetaCLIP (Xu et al., 2024). For generalizable recipe and findings, we base our training setup on OpenAI CLIPs ViT-L/14 and MetaCLIP ViT-H/14, except changes necessary for enabling worldwide capability, as described in Sec. 3.4 and ablated in later subsections. The full details can be found in Table 6 and Appendix B."
        },
        {
            "title": "4.2 Evaluation",
            "content": "We first present the main ablations of MetaCLIP 2 on wide range of English and multilingual zero-shot transfer benchmarks, along with other multilingual CLIP baselines for comparison (Sec. 4.2.1); then we conduct comprehensive ablation study on the variants of metadata, curation and tokenizer (Sec. 4.2.2). Lastly, we evaluate the embedding quality of MetaCLIP 2 on downstream tasks for culture diversity (Sec. 4.2.3). Additionally, we conduct analysis on embedding alignment and uniformity (Wang and Isola, 2020) in Sec. 4.2.4. 4.2.1 Main Ablation We first ablate the effects of scaling seen training pairs and minimal viable model capacity that break the curse of multilinguality, with the following two groups of 6 training runs. Two trainings are in ViT-L/14 on worldwide curated data and its English portion, where global batch size and seen pairs are set to 2.3 and 1.0 compared to OpenAI CLIP and MetaCLIP setting (i.e., 1.0 has 12.8B seen pairs, or 400M for 32 epoches as in OpenAI CLIP). Four runs are on ViT-H/14 with different subsets of curated data to demonstrate the effects of English data helping multilingual performance and vice versa. We denote each run based on subsets trained with and corresponding seen pairs: 1) Worldwide (2.3) with the full-fledged worldwide curated data; 2) Worldwide (1.0) with 1) downsampled; 3) English (1.0) with English portion of 1); 4) Non-English (1.3) with the non-English portion. 7 We adopt the following two groups of zero-shot transfer benchmarks: 1) English-only benchmarks on ImageNet (IN val) (Russakovsky et al., 2015), SLIP 26 tasks (SLIP 26 avg.) (Mu et al., 2021), and DataComp 37 tasks (DC 37 avg.) (Gadre et al., 2023); 2) multilingual benchmarks on Babel-ImageNet (Babel-IN) (Geigle et al., 2024) (averaged zero-shot classification on IN with classes and prompts translated into 280 languages), XM3600 (Thapliyal et al., 2022) (multilingual text-to-image, TI, and image-to-text, IT, retrieval with an averaged recall@1 on 36 languages), CVQA (Mogrovejo et al., 2024) (multilingual multi-choice visual question answering with English and local averaged answer accuracy), Flickr30k-200 (Visheratin, 2023) (Flickr30k test set translated into 200 languages), XTD-10 (Aggarwal and Kale, 2020) (multilingual image-text retrieval on MSCOCO (Chen et al., 2015) averaged Recall@1 over 7 languages), and XTD-200 (Visheratin, 2023) (XTD10 translated into 200 languages). The main ablation is shown in Table 1. We observe that MetaCLIP 2 on ViT-H/14 with worldwide data and scaled seen pairs consistently outperforms its counterparts English (1.0) and Non-English (1.3), on both English and multilingual tasks, effectively breaking the curse of multilinguality. The curse still exists in non-scaled seen pairs, Worldwide (1.0) or smaller ViT-L/14 model even with Worldwide (2.3)). English Benchmarks Multilingual Benchmarks Model ViT Size (Res.) Data XLM-CLIP(Ilharco et al., 2021) mSigLIP(Zhai et al., 2023) mSigLIP(Zhai et al., 2023) SigLIP 2(Tschannen et al., 2025) MetaCLIP(Xu et al., 2024) MetaCLIP 2 MetaCLIP 2 H/14(224) LAION-5B B/16(256) WebLI(12B) SO400M(256) WebLI(12B) SO400M(256) WebLI(12B) English(2.5B) L/14(224) English(2.5B) H/14(224) English Worldwide English Non-Eng. Worldwide Worldwide H/14(224) L/14(224) Seen Pairs 32B (2.5) 40B (3.0) 40B (3.0) 40B (3.0) 13B (1.0) 13B (1.0) 13B (1.0) 29B (2.3) 13B (1.0) 17B (1.3) 13B (1.0) 29B (2.3) IN val 77.0 75.1 80.6 83.2 79.2 80.5 79.5 78.8 80.4 71.4 79.5 81.3 SLIP 26 avg. DC 37 avg. Babel -IN XM3600 TI IT CVQA EN LOC 69.4 63.8 69.1 73.7 69.8 72.4 69.5 67.2 72.6 63.1 71.1 74.5 65.5 60.8 65.5 69.4 65.6 66.5 66.0 63.5 68.7 61.7 67.2 69.6 34.0 40.2 46.4 40.8 - - - 44.2 - 49.9 47.1 50.2 50.4 / 60.5 44.5 / 56.6 50.0 / 62.8 48.2 / 59. 56.1 / 48.2 51.8 / 45.7 56.8 / 49.8 58.5 / 49.0 - - - - - - - - - - - - Flicker30k -200 TI IT 43.2 / 46.2 34.0 / 36.0 39.9 / 42.0 36.6 / 40.3 - - - - - - XTD-10 TI IT XTD-200 TI IT 87.1 / 88.4 80.8 / 84.0 85.6 / 88.8 86.1 / 87. 42.5 / 45.2 37.8 / 40.6 42.5 / 45.2 40.3 / 44.5 - - - - - - - - - - - - 45.3 / 58. 59.2 / 55.1 41.9 / 45.8 82.8 / 85.0 41.9 / 44.8 - - 46.9 / 59.9 49.6 / 62.6 51.5 / 64.3 - - 59.8 / 56.8 59.9 / 56.0 61.5 / 57.4 - - 47.5 / 50.5 49.1 / 52.1 50.9 / 53.2 - - 83.2 / 85.7 85.2 / 87.1 86.1 / 87.5 - - 46.6 / 49.2 47.0 / 49.7 48.9 / 51.0 Table 1 Main ablation: MetaCLIP 2 breaks the curse of multilinguality when adopting ViT-H/14, with seen pairs scaled (2.3) proportional to the added non-English data. MetaCLIP 2 outperforms mSigLIP with fewer seen pairs (72%), lower resolution (224px vs. 256px), and comparable architectures (H/14 vs. SO400M). We grey out baselines those are SoTA-aiming systems with confounding factors. Here, numbers of seen pairs are rounded to the nearest integer (e.g., 12.8B->13B). Although SoTA is non-goal for MetaCLIP 2, its full recipe demonstrates strong performance with fewer seen pairs (72% of SigLIP series) and lower resolution (224px vs mSigLIPs 256). MetaCLIP 2 surpasses mSigLIP on IN, SLIP 26, and DC 37, and the recent SigLIP 2 on later two. More significantly, MetaCLIP 2 sets many SoTA multilingual benchmarks, e.g., Babel-IN (+3.8%), XM3600 (+1.1%/+1.5%), CVQA (+3%/+7.6%), Flicker-30k-200 (+7.7%/+7%), and XTD-200 (+6.4%/+5.8%). SigLIP 2 prioritizes English (90% of its training data in English), while it is worse than mSigLIP on multilingual tasks and MetaCLIP 2 on most English benchmarks except IN. Ablation Steps Metadata Alt-texts IN Babel-IN 1: English CLIP 2: remove English filter 3: no language isolation 4: language isolation with tlang = ten 5: language specific tlang English English all, in 1 set all, by lang. all, by lang. English all, in 1 set all, in 1 set all, by lang. all, by lang. 67.5 66.9 62.1 61.1 64. - - 31.2 31.5 31.5 XM3600 TI IT - - - - 49.7 37.8 49.4 37.9 50.0 38.1 CVQA EN LOCAL - - 49.8 49.0 50.3 - - 45.8 46.5 46.6 Table 2 Ablation study of metadata and alt-texts combination on ViT-B/32 using English 1.0 and Worldwide 1.0 with mT5 multilingual tokenizer. tlang is the count thresholds for each language and ten for English. 4.2.2 Ablation on Metadata, Curation, and Tokenizer We further ablate the transition from metadata and curation focuses solely on English to their worldwide equivalents using the ViT-B/32 encoder for efficiency. We evaluate zero-shot transfer on IN for English and Babel-IN, XM3600 and CVQA for multilingual. As in Table 2, starting from English-only CLIP, we first remove the English filter on alt-texts so that all alt-texts are curated by English metadata, resulting in 0.6% drop on IN, indicating English isolation separating text or metadata by LID before matching is important. 8 Then, we replace English metadata using all metadata merged without separation, yielding even worse English performance but start building up multilingual capability. Next, we isolate substring matching and curate alt-text language-by-language, with the same ten over all languages. This further lowers English performance since ten is too high for non-English and let head data dominate curation. Lastly, we compute tlang, to keep the same ratio of head-to-tail concepts for each language. This improves English and non-English performance, while curse of multilinguality remains unresolved in ViT-B/32 until the main ablation described above. To minimize changes in model architecture, we only swap the English tokenizer for multilingual one. Four popular tokenizers are studied on our zero-shot benchmarks. As shown in Table 3, the XLM-V vocabulary yields the strongest performance in both the English and non-English world. Tokenizer Vocab. Size IN val Babel-IN avg. mT5 (mSigLIP) (Xue et al., 2020) Gemma (SigLIP 2) (Team et al., 2024) XLM-Roberta (Conneau et al., 2019) XLM-V (Liang et al., 2023) 250k 256k 250k 900k 64.7 63.7 64.0 64. 31.5 26.1 31.1 32.7 XM3600 TI IT 50.0 38.1 47.8 36.1 49.8 38.0 CVQA EN LOCAL 46.6 50.3 44.0 48.3 46.1 49.8 40.0 51.4 50. 47.4 Table 3 Ablation study of various multilingual tokenizers with ViT-B/32 and Worldwide 1.0. 4.2.3 Cultural Diversity Following protocols in Pouget et al. (2024) and Wang et al. (2025), we perform zero-shot classification and few-shot geo-localization on range of geographically diverse benchmarks. Specifically, we include zero-shot classification with Dollar Street (Gaviria Rojas et al., 2022), GeoDE (Ramaswamy et al., 2023), and GLDv2 (Weyand et al., 2020) in Table 4, and few-shot geo-localization (Pouget et al., 2024) on Dollar Street, GeoDE and XM3600 in Fig. 3. We find that only changing the training data distribution, from 13B English to 13B worldwide pairs, yields significantly better performance, and scaling to 29B worldwide pairs improves further, except for the on-par, probably saturated performance in GeoDE. Fig. 3 shows similar trend for evaluating on few-shot geo-localization. Model Data WebLI(12B) (Chen et al., 2023b) mSigLIP (Zhai et al., 2023) SigLIP 2 (Tschannen et al., 2025) WebLI(12B) (Chen et al., 2023b) MetaCLIP 2 English Non-English Worldwide Worldwide Seen Pairs 40B (3.0) 40B (3.0) 13B (1.0) 17B (1.3) 13B (1.0) 29B (2.3) Dollar Street Top-1 Top-5 GLDv2 GeoDE 94.5 36.0 95.2 36.7 93.4 37.2 91.7 35.7 94.3 37.2 93.4 37.9 62.5 61.9 63.3 61.3 63.7 64. 45.3 48.5 52.8 68.6 65.8 69.0 Table 4 Zero-shot classification accuracy on cultural diversity benchmarks. MetaCLIP 2 models are in ViT-H/14 and mSigLIP/SigLIP 2 are in ViT-SO400M. mSigLIP/SigLIP 2 are SoTA-aiming systems with many factors changed and thus greyed out. Figure 3 Few-shot geo-localization accuracy on cultural diversity benchmarks. 9 4.2.4 Alignment and Uniformity Following (Wang and Isola, 2020), we further measure the embeddings quality across different CLIP models. To avoid various unknown biases from different benchmarks, we use 5k holdout image-text pairs not used in our training and report alignment and uniformity scores, where alignment measures the relevance of an image and text and uniformity measures how images distributed in vision encoders embedding space. Note that we have no control on whether these 5k pairs are leaked in other baselines. From Fig. 4, we can see that MetaCLIP 2 exhibits good scores in both alignment and uniformity (lower is better), whereas mSigLIP or SigLIP 2 may have non-trivial bias on our collected holdout data. Figure 4 Alignment and uniformity scores (Wang and Isola, 2020) calculated on our collected 5k holdout data, WW indicates worldwide data."
        },
        {
            "title": "5 Conclusion",
            "content": "We present MetaCLIP 2, the first CLIP trained with worldwide image-text pairs from scratch. Existing CLIP training pipelines, designed primarily for English, cannot straightforwardly generalize to worldwide setting without incurring an English performance degradation due to lack of curation for worldwide data or the curse of multilinguality. Our careful study suggests that the curse can be broken by scaling metadata, curation, and training capacity, where English and non-English world benefit each other. Specifically, MetaCLIP 2 (ViT-H/14) surpasses its English-only counterpart on zero-shot IN (80.5% 81.3%) and sets new SoTA on multilingual benchmarks such as XM3600, Babel-IN and CVQA with one single model. We envision our findings along with the fully open-sourced metadata, curation and training code encourage the community to move beyond English-centric CLIP and embrace the worldwide multimodal web."
        },
        {
            "title": "Acknowledgments",
            "content": "We thank Thao Nguyen and Bernie Huang for insightful discussion, Wei Cheng and Guan Pang for evaluation on downstream use cases."
        },
        {
            "title": "References",
            "content": "Pranav Aggarwal and Ajinkya Kale. Towards zero-shot cross-lingual image retrieval. arXiv preprint arXiv:2012.05107, 2020. Giusepppe Attardi. Wikiextractor. https://github.com/attardi/wikiextractor, 2015. Jinze Bai, Shuai Bai, Shusheng Yang, Shijie Wang, Sinan Tan, Peng Wang, Junyang Lin, Chang Zhou, and Jingren Zhou. Qwen-vl: versatile vision-language model for understanding, localization, text reading, and beyond, 2023. https://arxiv.org/abs/2308.12966. Daniel Bolya, Po-Yao Huang, Peize Sun, Jang Hyun Cho, Andrea Madotto, Chen Wei, Tengyu Ma, Jiale Zhi, Jathushan Rajasegaran, Hanoona Rasheed, et al. Perception encoder: The best visual embeddings are not at the output of the network. arXiv preprint arXiv:2504.13181, 2025. Fredrik Carlsson, Philipp Eisen, Faton Rekathati, and Magnus Sahlgren. Cross-lingual and multilingual clip. In Proceedings of the thirteenth language resources and evaluation conference, pages 68486854, 2022. Guanhua Chen, Lu Hou, Yun Chen, Wenliang Dai, Lifeng Shang, Xin Jiang, Qun Liu, Jia Pan, and Wenping Wang. mclip: Multilingual clip via cross-lingual transfer. In Proceedings of the 61st Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1302813043, 2023a. Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. simple framework for contrastive learning of visual representations. In International conference on machine learning, pages 15971607. PmLR, 2020. Xi Chen, Xiao Wang, Soravit Changpinyo, AJ Piergiovanni, Piotr Padlewski, Daniel Salz, Sebastian Goodman, Adam Grycner, Basil Mustafa, Lucas Beyer, Alexander Kolesnikov, Joan Puigcerver, Nan Ding, Keran Rong, Hassan Akbari, Gaurav Mishra, Linting Xue, Ashish Thapliyal, James Bradbury, Weicheng Kuo, Mojtaba Seyedhosseini, Chao Jia, Burcu Karagol Ayan, Carlos Riquelme Ruiz, Andreas Peter Steiner, Anelia Angelova, Xiaohua Zhai, Neil Houlsby, and Radu Soricut. PaLI: jointly-scaled multilingual language-image model. In The Eleventh International Conference on Learning Representations, 2023b. https://openreview.net/forum?id=mWVoBz4W0u. Xinlei Chen, Hao Fang, Tsung-Yi Lin, Ramakrishna Vedantam, Saurabh Gupta, Piotr Dollár, and Lawrence Zitnick. Microsoft coco captions: Data collection and evaluation server. arXiv preprint arXiv:1504.00325, 2015. Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Francisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised cross-lingual representation learning at scale. arXiv preprint arXiv:1911.02116, 2019. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. InstructBLIP: Towards general-purpose vision-language models with instruction tuning. In Thirtyseventh Conference on Neural Information Processing Systems, 2023. https://openreview.net/forum?id= vvoWPYqZJA. Terrance De Vries, Ishan Misra, Changhan Wang, and Laurens Van der Maaten. Does object recognition work for everyone? In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition workshops, pages 5259, 2019. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248255. Ieee, 2009. David Fan, Shengbang Tong, Jiachen Zhu, Koustuv Sinha, Zhuang Liu, Xinlei Chen, Michael Rabbat, Nicolas Ballas, Yann LeCun, Amir Bar, et al. Scaling language-free visual representation learning. arXiv preprint arXiv:2504.01017, 2025. Alex Fang, Albin Madappally Jose, Amit Jain, Ludwig Schmidt, Alexander Toshev, and Vaishaal Shankar. Data filtering networks. arXiv preprint arXiv:2309.17425, 2023. Samir Yitzhak Gadre, Gabriel Ilharco, Alex Fang, Jonathan Hayase, Georgios Smyrnis, Thao Nguyen, Ryan Marten, Mitchell Wortsman, Dhruba Ghosh, Jieyu Zhang, Eyal Orgad, Rahim Entezari, Giannis Daras, Sarah Pratt, Vivek Ramanujan, Yonatan Bitton, Kalyani Marathe, Stephen Mussmann, Richard Vencu, Mehdi Cherti, Ranjay Krishna, Pang Wei Koh, Olga Saukh, Alexander Ratner, Shuran Song, Hannaneh Hajishirzi, Ali Farhadi, Romain Beaumont, Sewoong Oh, Alex Dimakis, Jenia Jitsev, Yair Carmon, Vaishaal Shankar, and Ludwig Schmidt. Datacomp: In search of the next generation of multimodal datasets, 2023. 11 William Gaviria Rojas, Sudnya Diamos, Keertan Kini, David Kanter, Vijay Janapa Reddi, and Cody Coleman. The dollar street dataset: Images representing the geographic and socioeconomic diversity of the world. Advances in Neural Information Processing Systems, 35:1297912990, 2022. Gregor Geigle, Radu Timofte, and Goran Glavaš. Babel-imagenet: Massively multilingual evaluation of vision-andlanguage representations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 50645084, 2024. Aaron Grattafiori, Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Alex Vaughan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Édouard Grave, Piotr Bojanowski, Prakhar Gupta, Armand Joulin, and Tomáš Mikolov. Learning word vectors for 157 languages. In Proceedings of the Eleventh International Conference on Language Resources and Evaluation (LREC 2018), 2018. Geoffrey Hinton, Oriol Vinyals, and Jeff Dean. Distilling the knowledge in neural network, 2015. https://arxiv. org/abs/1503.02531. Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade Gordon, Nicholas Carlini, Rohan Taori, Achal Dave, Vaishaal Shankar, Hongseok Namkoong, John Miller, Hannaneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Openclip, July 2021. https://doi.org/10.5281/zenodo.5143773. If you use this software, please cite it as below. Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh, Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom Duerig. Scaling up visual and vision-language representation learning with noisy text supervision. In International conference on machine learning, pages 49044916. PMLR, 2021. Da-Cheng Juan, Chun-Ta Lu, Zhen Li, Futang Peng, Aleksei Timofeev, Yi-Ting Chen, Yaxi Gao, Tom Duerig, Andrew Tomkins, and Sujith Ravi. Graph-rise: Graph-regularized image semantic embedding. arXiv preprint arXiv:1902.10814, 2019. Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Uijlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan Popov, Matteo Malloci, Alexander Kolesnikov, et al. The open images dataset v4: Unified image classification, object detection, and visual relationship detection at scale. International journal of computer vision, 128(7):19561981, 2020. Jeffrey Li, Alex Fang, Georgios Smyrnis, Maor Ivgi, Matt Jordan, Samir Yitzhak Gadre, Hritik Bansal, Etash Guha, Sedrick Scott Keh, Kushal Arora, et al. Datacomp-lm: In search of the next generation of training sets for language models. Advances in Neural Information Processing Systems, 37:1420014282, 2024. Davis Liang, Hila Gonen, Yuning Mao, Rui Hou, Naman Goyal, Marjan Ghazvininejad, Luke Zettlemoyer, and Madian Khabsa. Xlm-v: Overcoming the vocabulary bottleneck in multilingual masked language models. arXiv preprint arXiv:2301.10472, 2023. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. Advances in neural information processing systems, 36:3489234916, 2023. David Orlando Romero Mogrovejo, Chenyang Lyu, Haryo Akbarianto Wibowo, Santiago Góngora, Aishik Mandal, Sukannya Purkayastha, Jesus-German Ortiz-Barajas, Emilio Villa Cueva, Jinheon Baek, Soyeong Jeong, Injy Hamed, Zheng Xin Yong, Zheng Wei Lim, Paula Mónica Silva, Jocelyn Dunstan, Mélanie Jouitteau, David LE MEUR, Joan Nwatu, Ganzorig Batnasan, Munkh-Erdene Otgonbold, Munkhjargal Gochoo, Guido Ivetta, Luciana Benotti, Laura Alonso Alemany, Hernán Maina, Jiahui Geng, Tiago Timponi Torrent, Frederico Belcavello, Marcelo Viridiano, Jan Christian Blaise Cruz, Dan John Velasco, Oana Ignat, Zara Burzo, Chenxi Whitehouse, Artem Abzaliev, Teresa Clifford, Gráinne Caulfield, Teresa Lynn, Christian Salamea-Palacios, Vladimir Araujo, Yova Kementchedjhieva, Mihail Minkov Mihaylov, Israel Abebe Azime, Henok Biadglign Ademtew, Bontu Fufa Balcha, Naome Etori, David Ifeoluwa Adelani, Rada Mihalcea, Atnafu Lambebo Tonja, Maria Camila Buitrago Cabrera, Gisela Vallejo, Holy Lovenia, Ruochen Zhang, Marcos Estecha-Garitagoitia, Mario Rodríguez-Cantelar, Toqeer Ehsan, Rendi Chevi, Muhammad Farid Adilazuarda, Ryandito Diandaru, Samuel Cahyawijaya, Fajri Koto, Tatsuki Kuribayashi, Haiyue Song, Aditya Nanda Kishore Khandavally, Thanmay Jayakumar, Raj Dabre, Mohamed Fazli Mohamed Imam, Kumaranage Ravindu Yasas Nagasinghe, Alina Dragonetti, Luis Fernando DHaro, Olivier NIYOMUGISHA, Jay Gala, Pranjal Chitale, Fauzan Farooqui, Thamar Solorio, and Alham Fikri Aji. CVQA: Culturally-diverse multilingual visual question answering benchmark. In The Thirty-eight Conference on Neural Information Processing Systems Datasets and Benchmarks Track, 2024. https://openreview.net/forum?id=E18kRXTGmV. 12 Norman Mu, Alexander Kirillov, David Wagner, and Saining Xie. Slip: Self-supervision meets language-image pre-training. arXiv preprint arXiv:2112.12750, 2021. Thao Nguyen, Matthew Wallingford, Sebastin Santy, Wei-Chiu Ma, Sewoong Oh, Ludwig Schmidt, Pang Wei Koh, and Ranjay Krishna. Multilingual diversity improves vision-language representations. Advances in Neural Information Processing Systems, 37:9143091459, 2024. Maxime Oquab, Timothée Darcet, Théo Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, et al. Dinov2: Learning robust visual features without supervision. Transactions on Machine Learning Research Journal, pages 131, 2024. Angéline Pouget, Lucas Beyer, Emanuele Bugliarello, Xiao Wang, Andreas Steiner, Xiaohua Zhai, and Ibrahim Alabdulmohsin. No filter: Cultural and socioeconomic diversity in contrastive vision-language models. Advances in Neural Information Processing Systems, 37:106474106496, 2024. Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In International conference on machine learning, pages 87488763. PMLR, 2021. Vikram Ramaswamy, Sing Yu Lin, Dora Zhao, Aaron Adcock, Laurens van der Maaten, Deepti Ghadiyaram, and Olga Russakovsky. Geode: geographically diverse evaluation dataset for object recognition. Advances in Neural Information Processing Systems, 36:6612766137, 2023. Aditya Ramesh, Mikhail Pavlov, Gabriel Goh, Scott Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya Sutskever. Zero-shot text-to-image generation. In International conference on machine learning, pages 88218831. Pmlr, 2021. Kanchana Ranasinghe, Brandon McKinzie, Sachin Ravi, Yinfei Yang, Alexander Toshev, and Jonathon Shlens. Perceptual grouping in contrastive vision-language models. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 55715584, 2023. Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge. International journal of computer vision, 115:211252, 2015. Gabriel Oliveira dos Santos, Diego AB Moreira, Alef Iury Ferreira, Jhessica Silva, Luiz Pereira, Pedro Bueno, Thiago Sousa, Helena Maia, Nádia Da Silva, Esther Colombini, et al. Capivara: Cost-efficient approach for improving multilingual clip performance on low-resource languages. arXiv preprint arXiv:2310.13683, 2023. Christoph Schuhmann, Richard Vencu, Romain Beaumont, Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m: Open dataset of clip-filtered 400 million image-text pairs. arXiv preprint arXiv:2111.02114, 2021. Christoph Schuhmann, Romain Beaumont, Cade Gordon, Ross Wightman, Theo Coombes, Aarush Katta, Clayton Mullis, Patrick Schramowski, Srivatsa Kundurthy, Katherine Crowson, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. 2022a. Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale dataset for training next generation image-text models. Advances in neural information processing systems, 35:2527825294, 2022b. Shreya Shankar, Yoni Halpern, Eric Breck, James Atwood, Jimbo Wilson, and Sculley. No classification without representation: Assessing geodiversity issues in open data sets for the developing world. arXiv preprint arXiv:1711.08536, 2017. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew Dai, Anja Hauth, Katie Millican, et al. Gemini: family of highly capable multimodal models. arXiv preprint arXiv:2312.11805, 2023. Gemma Team, Thomas Mesnard, Cassidy Hardin, Robert Dadashi, Surya Bhupatiraju, Shreya Pathak, Laurent Sifre, Morgane Rivière, Mihir Sanjay Kale, Juliette Love, et al. Gemma: Open models based on gemini research and technology. arXiv preprint arXiv:2403.08295, 2024. 13 Ashish Thapliyal, Jordi Pont Tuset, Xi Chen, and Radu Soricut. Crossmodal-3600: massively multilingual multimodal evaluation dataset. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 715729, 2022. Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin, Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, et al. Siglip 2: Multilingual vision-language encoders with improved semantic understanding, localization, and dense features. arXiv preprint arXiv:2502.14786, 2025. Pablo Villalobos, Anson Ho, Jaime Sevilla, Tamay Besiroglu, Lennart Heim, and Marius Hobbhahn. Will we run out of data? limits of llm scaling based on human-generated data. arXiv preprint arXiv:2211.04325, 2022. Alexander Visheratin. Nllb-cliptrain performant multilingual image retrieval model on budget. arXiv preprint arXiv:2309.01859, 2023. Tongzhou Wang and Phillip Isola. Understanding contrastive representation learning through alignment and uniformity on the hypersphere. In International Conference on Machine Learning, pages 99299939. PMLR, 2020. Xiao Wang, Ibrahim Alabdulmohsin, Daniel Salz, Zhe Li, Keran Rong, and Xiaohua Zhai. Scaling pre-training to one hundred billion data for vision language models. arXiv preprint arXiv:2502.07617, 2025. Tobias Weyand, Andre Araujo, Bingyi Cao, and Jack Sim. Google landmarks dataset v2-a large-scale benchmark for instance-level recognition and retrieval. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pages 25752584, 2020. Wikipedia. Scriptio continua. https://en.wikipedia.org/wiki/Scriptio_continua. Wikipedia. Languages used on the internet. https://en.wikipedia.org/wiki/Languages_used_on_the_ Internet, 2025. It reports 50.9% of web content is non-English by 2025. Accessed: 2025-05-15. Hu Xu, Saining Xie, Xiaoqing Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying CLIP data. In The Twelfth International Conference on Learning Representations, 2024. https://openreview.net/forum?id=5BCFlnfE1g. Linting Xue, Noah Constant, Adam Roberts, Mihir Kale, Rami Al-Rfou, Aditya Siddhant, Aditya Barua, and Colin Raffel. mt5: massively multilingual pre-trained text-to-text transformer. arXiv preprint arXiv:2010.11934, 2020. Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Wentao Zhang, Bin Cui, and MingHsuan Yang. Diffusion models: comprehensive survey of methods and applications. ACM Computing Surveys, 56 (4):139, 2023. Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner, Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer. Lit: Zero-shot transfer with locked-image text tuning. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 1812318133, 2022. Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid loss for language image pre-training. In Proceedings of the IEEE/CVF international conference on computer vision, pages 1197511986, 2023."
        },
        {
            "title": "A Implementation Details for Metadata and Curation",
            "content": "A.1 Unigram and Bigram Tokenizer for Special Languages Most modern languages around the world adopt writing systems that use spaces to separate words, except for some of the Asian languages, known as scriptio continua(Wikipedia). We find several open source tokenizers for many of these languages developed by local communities, as shown in the Table 5, in order to properly split text into words while preserve semantic integrity. Note these tokenizers are only used to process Wikipedia dump labeled with the listed wiki codes (e.g., not on alt-texts substring matching). Wiki Code bo,dz ja,ryu km lo my th zh,zh_classical,zh_yue Tokenizer Name Tibetan Tokenizer Japanese Tokenizer Khmer Tokenizer Lao Tokenizer Myanmar Tokenizer Thai Tokenizer Chinese Tokenizer Table 5 Tokenizers for special languages. A.2 Scaling Curation Worldwide scaling of data curation significantly increases time and space complexity due to store metadata across hundreds of languages. To efficiently handle this complexity, we leverage several efficient implementation to enable worldwide curation (that English only curation does not have): Efficient String Matching: We adopt the Aho-Corasick algorithm 23, which utilizes prefix trees (tries), for rapid substring matching. The matching speed is about 2k times faster than MetaCLIPs brute-force implementation, enabling matching with million-scale metadata. Lazy Metadata Loading: We pre-build and store the metadata into an Aho-Corasick automaton for each language separately, loading these automaton dynamically and only when encountering new language for alt-text during processing, thereby minimizing the total number of languages encountered for each shard of data and saving re-compiling time for automation on new shard. Memory Management for Probabilities: To address memory constraints during sampling for balancing, we utilize memory-mapped file loading (mmap) to efficiently access counts per entry across all languages, preventing out-of-memory errors caused by loading all the counts from different languages. These implementation choices ensure the worldwide data curation is computationally feasible and scalable to billions of image-text pairs from hundreds of languages. Mitigation and Benchmark Deduplication We run state-of-the-art safety classifier to remove NSFW contents (e.g., adult, sexual, violence) from training data. We also apply face detector to remove human biometrics and personally identifiable information from data. To avoid benchmark leakage, we remove any overlap with ImageNet evaluation sets by performing deduplication using 64-bit hashes. These hashes are generated by applying random projection to feature embeddings from similarity search model, reducing them to 64 dimensions followed by sign-based quantization. 2https://en.wikipedia.org/wiki/Aho-Corasick_algorithm 3https://pypi.org/project/pyahocorasick"
        },
        {
            "title": "B Training Setup",
            "content": "To remove confounding factors and generalize our findings, we follow OpenAI CLIP and MetaCLIP training setup with changes for worldwide scaling, detailed in Table 6. Our data curation algorithm is running in parallel with 800 jobs (each job has 40GB CPU memory) and it takes 1 hour to substring match and count for all alt-text pairs. Hyperparameter OpenAI CLIP / MetaCLIP MetaCLIP 2 Activation Function Seen Pairs Batch Size Learning Rate Warm-up QuickGELU 12.8B 32768 4.0e-4 (L/14, H/14) 2k QuickGELU 29B (2.3) 75366 (2.3) 4.0e-4 (H/14) 2k Table 6 Hyperparameters of OpenAI CLIP / MetaCLIP vs MetaCLIP 2."
        },
        {
            "title": "C Limitation on Benchmark",
            "content": "High-quality benchmarks are essential for researchers to understand the efficacy of proposed changes. After decades of meticulous efforts, the community has established reliable and diverse datasets to enable research advancement in vision and multimodal areas (Deng et al., 2009; Russakovsky et al., 2015; Radford et al., 2021). However, these datasets consist mainly of content scraped from North America and Western Europe (NA+EU) and focus on English (Shankar et al., 2017; De Vries et al., 2019). It is long and resource-intensive endeavor to build similar benchmarks for unbiased and comprehensive evaluation of worldwide data and resulting representations, for the world outside NA+EU or English-speaking community, due to the complexity of covering diverse concepts across geo-locations, cultures, and languages. XM3600 (Thapliyal et al., 2022) aims to build geographically diverse datasets by selecting images from Open Images Dataset (Kuznetsova et al., 2020) based on metadata of GPS coordinates, but later research (Pouget et al., 2024) suggests Open Images Dataset is biased towards Western images or specific activities (e.g., tourism). GeoDE (Ramaswamy et al., 2023) recruits human workers on crowdsourcing platform to collect geographically diverse images for predefined object classes. Crowdsourcing is an economic way to collect human annotations, but the demographic background and proficiency of the workers are not guaranteed, nor is the quality of the collected data. Few efforts such as CVQA (Mogrovejo et al., 2024) attempt to scale annotation and control quality simultaneously by utilizing experts in machine learning community or existing materials as seeds. These efforts offer relatively unbiased evaluation with reasonable coverage in capabilities (e.g., cultural diversity, multimodal problem solving for exam questions across countries) of interests. We believe benchmarks of similar quality but built for evaluating more general and comprehensive capabilities will reveal the true potential of worldwide data and resulting representations developed in this work."
        }
    ],
    "affiliations": [
        "FAIR, Meta",
        "MIT",
        "New York University",
        "Princeton University"
    ]
}