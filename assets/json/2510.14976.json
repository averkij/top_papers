{
    "paper_title": "Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation",
    "authors": [
        "Shaowei Liu",
        "Chuan Guo",
        "Bing Zhou",
        "Jian Wang"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, a simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of close-contact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) a pose animator that uses the temporal prior to generate dynamic motion sequences from interactive poses, and (2) a pose generator that applies the spatial prior to synthesize interactive poses from a single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 6 1 ] . [ 1 6 7 9 4 1 . 0 1 5 2 : r Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation Shaowei Liu Chuan Guo2 Bing Zhou2 1University of Illinois Urbana-Champaign Jian Wang2 2Snap Inc. https://stevenlsw.github.io/ponimator/"
        },
        {
            "title": "Input",
            "content": "Generated Interaction Animation (leftright: time steps) Two-person image Estimated interactive pose Single-person image Generated interactive pose Single-person image + Lift the other onto his back Figure 1. Ponimator enables versatile interaction animation applications anchored on interactive poses. For two-person images (top), Ponimator generates contextual dynamics from estimated interactive poses (green box). For single-person images (middle) with optional text prompts (bottom), Ponimator first generates partner interactive poses (magenta box) and then fulfill the interaction dynamics. Generated interactive pose"
        },
        {
            "title": "Abstract",
            "content": "Close-proximity human-human interactive poses convey rich contextual information about interaction dynamics. Given such poses, humans can intuitively infer the context and anticipate possible past and future dynamics, drawing on strong priors of human behavior. Inspired by this observation, we propose Ponimator, simple framework anchored on proximal interactive poses for versatile interaction animation. Our training data consists of closecontact two-person poses and their surrounding temporal context from motion-capture interaction datasets. Leveraging interactive pose priors, Ponimator employs two conditional diffusion models: (1) pose animator that uses *Work done at an internship at Snap Research NYC, Snap Inc. Co-corresponding author the temporal prior to generate dynamic motion sequences from interactive poses, and (2) pose generator that applies the spatial prior to synthesize interactive poses from single pose, text, or both when interactive poses are unavailable. Collectively, Ponimator supports diverse tasks, including image-based interaction animation, reaction animation, and text-to-interaction synthesis, facilitating the transfer of interaction knowledge from high-quality mocap data to open-world scenarios. Empirical experiments across diverse datasets and applications demonstrate the universality of the pose prior and the effectiveness and robustness of our framework. Codes and video visualization can be found at https://stevenlsw.github.io/ponimator/ 1. Introduction The interplay between humans plays crucial role in our daily lives. These interactions convey key social signals that reflect relationships and intentions. For example, simple hug typically expresses closeness, handshake serves as formal greeting, while combat indicates opposing stances. key observation is that interactive poses in close proximity (e.g., handshake) carry rich prior information about interaction dynamics. Specifically, pair of such poses reveals contextual cues about spatial relationships, constraints, and intent, often suggesting probable ranges of past and future motions. These interactive poses can act as bridge for modeling interaction dynamics with reduced complexity while inherently preserving prior knowledge of close interactions. In this paper, we present Ponimator, novel framework that leverages the dynamics priors embedded in interactive poses through generative model, demonstrating its versatility across various interaction animation tasks. We develop this interaction prior using combination of two highquality human-human interaction datasets: Inter-X [65] and Dual-Human [7]. From these datasets, we construct collection of two-person poses in close proximity, as shown in Fig. 2, along with their preceding and subsequent interaction motions. Using this collection, we train conditional diffusion model to generate contextual interaction dynamics given pair of closely interactive poses. We first demonstrate the application of our learned poseto-dynamic interactive priors for open-domain images. Social interactions are frequently depicted in images, yet existing works [7, 9, 10, 39] typically focus only on reconstructing static interactive poses, lacking the temporal dynamics of these interactions. Meanwhile, video diffusion models [3, 16, 18] can animate images over time but often struggle to maintain motion and interaction integrity. In contrast, Ponimator seamlessly transfers learned interaction prior knowledge from high-quality 3D mocap datasets to these in-the-wild images through estimated interactive poses, as shown in Fig. 1 (top). For broader applications, we developed an additional conditional diffusion model that leverages the spatial prior to generate interactive poses from multiple input types, including text descriptions, single poses, or both. Thus, when only single person appears in an image, Ponimator can first generate partner pose with an optional text prompt, and then animate the interactive poses over time (see Fig. 1). Furthermore, by anchoring on these interactive poses, Ponimator is able to generate shortclip two-person motions with proximal contact (see Fig. 8) directly from text input. Our key contributions are summarized as follows: 1) We present Ponimator, simple framework designed to learn the dynamics prior of interactive poses from motion capture data, particularly focusing on proximal human-human Figure 2. Interactive poses refer to two-person poses in proximity and close contact. The top row displays interactive (green) and non-interactive (red) poses within one sequence. Interactive poses allow observers to intuitively infer the temporal context, while non-interactive poses are more ambiguous and difficult to interpret. The bottom row showcases common daily interactive poses. interaction animations; 2) The learned prior is universal and generalizes effectively to poses extracted from open-world images, enabling animation of social interactions in human images; 3) Ponimator can generate interactive poses from single-person pose, text, or both, combined with interactive pose animation, enabling diverse applications including reaction animation and text-to-interaction synthesis. 2. Related work Human-human Interactions in Images. Humanhuman interactions are prevailing in social images. Significant progress has been made in interactive pose estimation [9, 10, 39] and interaction sequence reconstruction [20, 60]. Ugrinovic et al. integrate physical simulator into the human mesh recovery pipeline to capture the physical significance of interactive poses. Huang et al. [20] use VectorQuantised representation learning and specialized losses to learn discrete interaction prior, but suffer from limited interpretability and generalization. In contrast, our method directly anchors on interactive poses for interaction modeling without relying on additional physical simulators or intricate model designs. Our simple and interpretable prior generalizes well to in-the-wild settings, adhering to the principle that simplicity leads to robustness. The interactive pose prior is also explored in BUDDI [39], which estimates twoperson static poses from images but is limited to static pose modeling and overlooks the rich dynamics of interactions. In contrast, our work unlocks interactive motions for both animation and generation in arbitrary open-world images. Human-human Motion Synthesis. Generating human motion dynamics has been long-standing task [1, 2, 28, 29, 32, 38]. Utilizing generative models have gained widespread popularity recently [1214, 25, 27, 30, 43, 44, 58, 59, 63, 64, 71, 72]. With the success of applying generative models in single-person motion synthesis and the release of large-scale two-person interaction datasets, such as InterGen [31] and Inter-X [65], there has been surge in Figure 3. Framework overview. Ponimator consists of pose generator and animator, bridged by interactive poses. The generator takes single pose, text, or both as input to produce interactive poses, while the animator unleashes interaction dynamics from static poses. research [5, 6, 11, 24, 34, 35, 45, 50, 51, 53, 58, 59, 66] focused on multi-person motion generation. However, most existing studies generate two-person motions following input text, but often overlooking close-contact dynamics. For example, Liang et al. [31] proposed diffusion model for two-person motion generation, but it relies on detailed text In contrast, input and struggles with realistic interaction. our framework focuses on short-range interactions by leveraging generalizable interaction priors from static interactive poses, naturally ensuring physical contact between individuals and seamlessly generalizes to open-world scenarios. Human-human Motion Prediction. body of work focuses on tracking multi-person motions from videos [22, 23, 52], forecasting future multi-person motions based on past movements [15, 42, 55, 56, 62, 67, 68] and generating reactive motion based on an individuals full motion sequence [5, 8, 11, 35, 49, 53, 66]. However, existing methods rely on long history context or full individual motions while treating interactive poses and human dynamics separately. In contrast, our approach bridges these two modalities by anchoring on interactive poses and leveraging their prior for dynamics forecasting. This integration enables our model to generate both past and future interaction dynamics while supporting flexible inputs with fewer constraints, such as text, single-pose, or both, unlocking diverse applications in animation and generation. 3. Approach Ponimator leverages interactive pose priors as intermediates for interaction animation, as shown in Fig. 3. We first introduce interactive poses and motion modeling (Sec. 3.1). Then, we present the pose animator (Sec. 3.2), which transforms interactive poses into motion, followed by the pose generator (Sec. 3.3), which generates interactive poses from various inputs. Finally, in Sec. 3.4, we explore Ponimators applications to real-world images and text. 3.1. Interactive Pose and Motion Modeling. , xb Interactive pose and motion. Our work defines interactive poses as the poses of two individuals in proximity and close contact. For person a, we use the SMPLX parametric body model [40] to model the pose xa = (ϕa, θa, γa) and shape βa R10. Here, θa R213 is the joint rotations, ϕa R13 and γa R13 represents the global orientation and translation. The interactive pose of two individuals and is given as xI = (xa ). An interaction motion consists of short pose sequence of length , centered around an interaction moment, along with shape parameters i=1, β = (βa, βb). θ of both individuals, where = {xi}N includes an pair of interactive poses xI at interaction moment index within the sequence, and its nearby past poses x1:I and future poses xI+1:N . An example of interactive pose and motion is shown in Fig. 2. Interaction motion modeling. The interactive pose xI encodes rich temporal and spatial priors. As shown Fig. 2, interactive poses convey motion dynamics (top row) and spatial relationships (bottom row) between individuals. The strong prior make it easier for models to learn, whereas noninteractive poses lack clear interaction cues, making learning more challenging. Therefore, we model the interaction motion (X , β) by anchoring on its interactive pose xI . p(X , β) = p(X ; xI , β) p(xI , β) (1) temporal prior spatial prior Learning prior from diffusion model. Each priors distribution in Eq. (1) is captured by generative diffusion model [17] G, trained on high-quality mocap data. To Two-person Image Interaction Animation Single-person Image Interaction Animation Text-to-Interaction Synthesis Figure 4. Applications. Our framework enables two-person image animation, single-person interaction generation, and text-to-interaction synthesis. For two-person images, we estimate interactive poses using an off-the-shelf model [39]. For single-person images, we first estimate the pose by [4] and generate its interactive counterpart. For text input, our unified pose generator could synthesize the pose directly. These poses are then fed into our animator to generate human dynamics. model the underlying distribution of data z0, the diffusion model introduces noise ϵ to the clean data z0 in the forward 1 αtϵ, ϵ (0, I), αtz0 + pass, following zt = where αt (0, 1) are constants, is the diffusion timestep [0, Tdiffusion]. The model aims to recover clean input by ˆz0 = G(zt, t, c) from the noisy observations zt and condition c, optimizing the objective: LD = Ez0,c,ϵN (0,I),t[z0 G(zt, t, c)b 2] (2) During inference, the model iteratively predicts G(zt, t, c) from = Tdiffusion to = 0, gradually denoising the sample until it recovers the original clean data ˆz0. Close-proximity training data. We collect large-scale training data from public mocap datasets, InterX [65] and DualHuman [7], without requiring contact annotations. Interactive poses are detected by spatial proximity, and if within threshold, we extract the pose with its past and future frames to form 3-second interaction clip. 3.2. Unveiling Dynamics from Interactive Poses i=1 where ˆxI xI , as shown in Fig. 3 (c). The interactive pose animator captures the temporal prior in p(X ; xI , β) given an interactive pose xI and two persons shape β. The objective is to generate the motion sequences ˆX = {ˆxi}N Interactive pose-centered representation. We anchor the entire sequence on the interactive pose xI and define the denoising target z0 as the motion residuals with respect to interactive poses z0 = {xi xI }N i=1 This learning objective enforces model to learn the contextual dynamics strongly shaped by interactive poses. During inference, we recover the predicted pose sequence {ˆxi}N i=1 by ˆz0 + xI . We encode the interactive time index with onehot vector mI OneHot(I) {0, 1}N , where mi = 1 iff = I. To better preserve the spatial structure of interactive pose at time in pose sequences, we apply an imputation strategy to the diffusion model, where the noise input zt in Eq. (2) is substituted with zt: zt = (1 mI ) zt + mI 0, = (mI , xI , β), (3) , βa), FK(xb where denotes element-wise multiplication and is the input condition. After imputation, noise is added to interactive poses (i.e., zt + xI ) before fed into the network. Condition encoding. The interaction time condition mI is concatenated with the initial model input along the feature dimension. We encode the remaining conditions (xI , β) by leveraging the SMPLX joint forward kinematics (FK) function FK(, ) to compute joint positions of interactive pose jI = (FK(xa , βb)). Here, jI inherently encodes both individuals poses and shapes. It is further embedded through single-layer MLP and injected into the model layers via AdaIN [21]. Architecture and training. We adopt the DiT [41] architecture as our diffusion model, built on stacked Transformer blocks [61] that alternate spatial attention for human contact and temporal attention for motion dynamics. To train the model, besides diffusion loss LD in Eq. (2), we apply the SMPL loss Lsmpl as the MSE between the denoised pose sequence and the clean input. We also use an interaction loss Linter [31] and velocity loss [59]. Lvel Input Interaction Animation (leftright: time steps) Figure 5. Interactive pose image animation on FlickrCI3D dataset [9]. Left shows the input image, right shows the animated interaction motions. Interactive-pose frame is labeled in green box. encourages contact between individuals in close proximity, while Lvel ensures motion coherence. The total loss = λDLD +λsmplLsmpl +λinterLinter +λvelLvel. To improve robustness and generalization to noisy real-world poses, we apply augmentation by adding random noise to interactive pose xI . Please refer to Sec. for details. 3.3. Interactive Pose Generator The interactive pose generator models p(xI , β) in Eq. (1), leveraging the spatial prior to generate xI , β from various conditions, as shown in Fig. 3(a). Unified input conditioning. Given various input condiI , βa), or both, tions, including text c, single person pose (xa , βb), 0 = (xb , βa) and zb the model generates za which together form the diffusion target z0 = (za 0) in Eq. (2). To integrate these conditions into unified model, we introduce two masks, mc and ma, to encode the presence of text and pose conditions, respectively. These masks are sampled independently from Bernoulli distribution with probability pcondition during training. We modify the model input zt and text condition to in Eq. (2) as: 0 = (xa 0, zb zt = ((1 ma) za = mc c. (4) This design enables the model to accommodate multiple combinations of conditions. + ma za 0, zb t), rest rest using inverse kinematics (IK). In SMPL, human shapes are coupled with genders {male, female, neutral}. To enable more generic shape condition, we instead use the global joint positions of rest pose j{a,b} , which inherently capture both shape and gender information, and define the diffusion target as z0 = (x{a,b} , j{a,b} ). After generation, we can recover β{a,b} rest from j{a,b} Architecture and training. We use the same architecture as pose animator with modifications below. (1) The text condition is encoded via CLIP [48], processed by two trainable Transformer layers, and injected by AdaLN [21]. (2) We retain spatial attention layers and remove temporal attentions. The model is trained with standard diffusion loss LD in Eq. (2), SMPL loss Lsmpl, and bone length loss Lbone minimizes the MSE with ground-truth lengths in the SMPLX [40] kinematic tree. Total loss = λDLD + λsmplLsmpl + λboneLbone. Please see Sec. for details. 3.4. Applications Our framework supports two-person interactive pose image animation, single-person pose interaction generation, and text-to-interaction synthesis, as shown in Fig. 4. Interactive pose image animation. As shown in 1st row of Fig. 4, given two-person image, we estimate the interactive pose ˆxI using an off-the-shelf model [39]. The Input Interaction Generation (leftright: time steps) two person pose for photo one person lift another one up Figure 6. Single-person image interaction generation on Motion-X [33] dataset. Left shows the single person image input, right shows the generated two-person interaction dynamics. The generated interactive pose frame is labeled in magenta box. Top two rows display single-person pose inputs, while the bottom two show the same with accompanying text below the input image. estimated pose is fed into our interactive pose animator (Sec. 3.2) to generate motions guided by the temporal prior in interactive poses. Our model provides flexible interaction timing control by adjusting in Eq. (3), where = 0 predicts future motion, = reconstructs the past, and generally, = 2 enables symmetric animation. Open-world animation results are shown in Fig. 5. Single-person pose interaction generation. As shown in the 2nd row of Fig. 4, given single-person image, we estimate the pose ˆxa using off-the-shelf model such as [4] and feed it into our interactive pose generator (Sec. 3.3). We set ma = 0, mc = 0 in Eq. (4) as model input, disabling text input and allowing ˆxa to generate its interactive counterpart xb using the spatial prior in interactive poses. Alternatively, setting mc = 1 enables additional text conditioning. Once the interactive pose ˆxI = (ˆxa , ˆxb ) is obtained, it is fed into the interactive pose animator (Sec. 3.2) to synthesize motion dynamics. Open-world results are presented in Fig. 6. Text-to-interaction synthesis. As shown in 3rd row of Fig. 4, given short phrase, we generate the interactive pose ˆxI by setting ma = 0, mc = 1 in Eq. (3). The generated ˆxI is then passed to the pose animator to produce the corresponding motion. Examples for two-person hugging together and push are presented in Figs. 4 and 8. 4. Experiments Implementation details. We extract interactive poses by detecting SMPL-X vertices contacts [39] below threshold in each mocap dataset within 3s window. The interactive pose animator has 8 layers (latent dim 1024) and is trained using AdamW [37] (LR 1e-4). All loss weights are 1 except λinter = 0.5.To handle real-world noise, we augment training by adding Gaussian noise (scale 0.02) to interactive poses. At inference, DDIM [54] samples 50 steps, generating 3s motions at 10fps in 0.24s on an A100. The interactive pose generator follows similar setup with ptext = 0.8, ppose = 0.2, and frozen CLIP-ViTL/14 [48] text encoder. The pose generation take 0.21s. Models are trained for 4000 epochs with batch sizes of 256 (pose animator) and 512 (pose generator). Please see Sec. for details. Datasets. We train and test our model on two large-scale datasets: Inter-X [65] (11k sequences) and Dual-Human [7] (2k sequences). We follow the official split for Inter-X and use 3:1 training-testing split for Dual-Human, excluding non-interactive motion sequences. Metrics. We follow the evaluation metrics in [47, 50, 59]: Frechet Inception Distance (FID), the feature distribution against ground truth (GT). We compute it by training motion autoencoder to encode motion into features for each task; Precision (Pre.), the likelihood that generated motions fall within the real distribution; Recall (Rec.), the likelihood that real motions fall within the generated distribuInteractive Pose Interaction Animation (leftright: time steps) Inter-X test set [65] Dual-Human test set [7] Duolando dataset [53] Hi4D dataset [70] Interhuman dataset [31] Multi-person interaction animation Interactive pose animation on in-domain datasets Figure 7. [7]), dataset out-of-domain (Inter-X[65], Dual-Human (Duolando [53], Hi4D [70], Interhuman [31]), and random composed multi-person pose. Each row: leftinteractive pose, rightanimation sequence. Our learned interactive pose prior is universal, generalizing across datasets and enabling multi-person interactions (6th row) without modification or retraining. tion; Diversity, the variance of generated motions. We also evaluate the physics plausibility via Contact Frame Ratio (CR., %)proportion of frames with two-person contactand averaged Inter-person Penetration (Pene., cm). 4.1. Effectiveness of Anchoring on Interactive Poses Previous works model human-human interaction dynamics either by finetuning on single-person motion priors with interaction data (e.g., ComMDM [50], RIG [58]) or by learning interaction dynamics from scratch (e.g., InterGen [31]). In this work, we model interaction dynamics by anchoring on proximal interactive poses. To evaluate the effectiveness of these approaches, we employ simple task unconstrained generation. We further adapt MDM [59] to accommodate two-person motions in our setting. Ponimator seamlessly supports unconstrained generation by setMethod FID Pre. Rec. Div. CR. Pene. GT 0.3 1. 1.0 10.1 70.6 MDM* [59] 62.6 ComMDM [50] 88. RIG [58] InterGen [31] Ours 65.2 56.6 22. 0.79 0.37 0.46 0.57 0.58 0. 0.49 0.65 0.46 0.72 9.8 10. 10.6 10.1 10.2 66.4 44.3 44. 50.9 68.1 3.8 5.3 4.7 4. 4.3 5.0 Table 1. Unconstrained interaction synthesis comparison on Inter-X [65] dataset. means the closer to ground truth the better the result. Method in is adapted from ours for two-person interaction. Our method largely outperforms others in motion quality and contact ratio, naturally ensuring physical contact and motion realism by anchoring on interactive poses. Inter-X Dual-Human Method FID Div. CR. Pene.FID Div. CR. Pene. GT 0.3 10.1 70.6 3. 2.1 12.0 70.4 InterGen* 18.9 10.6 w/o anchor - time - joints 7.1 6.3 5.6 random-pose 5. 9.8 10.3 10.0 10.1 ours 5. 9.9 44.4 67.3 66.9 67.6 67. 68.5 4.3 5.1 5.2 5.1 5. 5.1 88.8 11.9 36.9 11.6 30.3 12.6 29.9 12.3 30.1 12. 24.2 11.8 44.3 70.7 67.3 70.2 69. 70.4 3.4 4.1 4.5 5.1 4. 4.5 4.5 Table 2. Interactive pose animation comparison on Inter-X [65] and Dual-Human [7] dataset. InterGen* is adapted to take interactive poses input but lacks explicit interaction modeling, limiting its use of pose priors. Interactive pose anchoring, condition encoding, and interactive frames are crucial for the performance. ting ma = 0 and mc = 0. Experimental results on our dataset collection from Inter-X [65] are shown in Tab. 1. We observe that previous methods [31, 50, 58] struggle to synthesize close-contact interactions, while the adapted MDM* [59] exhibits lower interaction motion quality. In contrast, by simply anchoring on interactive poses, our model achieves superior motion realism (FID of 22.6) and physical contact (contact ratio of 68.1). 4.2. Interactive Pose Animation To evaluate the interactive pose animator, we compare against baselines and key ablations on Inter-X [65] and Dual-Human [7] datasets in Tab. 2. We ablate key components of pose animator: w/o anchor removes interactive pose anchoring, replacing the denoising target z0 with {xi}N i=1; - time removes the interaction time encoding mI ; - joints removes joints condition encoding; InterGen* replaces text conditions with interactive pose condition while keeping all other settings unchanged; random-pose uses Method FID Div. MModality CR. Pene. Method FID Pre. Rec. Div. CR. Pene. GT 0. 6.78 InterGen 2.87 w/o anchor 2.74 6.76 6. Ours 1.82 6.78 - 1.42 1. 1.46 70.6 39.8 39.0 45.9 3. 3.9 4.0 4.3 Table 3. Text-to-interaction synthesis results on Inter-X [65] dataset. Our unified pipeline outperforms end-to-end w/o interactive pose as anchor method in short-term interaction synthesis. GT 0. 1.0 1.0 10.1 70.6 w/o anchor 40.0 Ours 27.8 0.87 0.91 0.43 0.48 9. 9.7 67.5 73.3 3.8 5.0 5. Table 4. Single pose-to-interaction synthesis results on InterX [65] dataset. Compared to without anchor baseline, our method uses interactive poses for more effective interaction modeling. Single Pose Interaction Generation (leftright: time steps) ] 1 3 [ r I c o / u Figure 8. Text-to-interaction comparison for push. Anchored on interactive poses, our method achieves better contact and more realistic dynamics than InterGen [31] and the end-to-end baseline. random instead of interactive frames as anchor. All baselines are trained under the same setting. Tab. 2 highlights the importance of interactive pose anchoring and interaction InterGen* overlooks input poses, resulting conditioning. in poorer performance. In contrast, our method explicitly models interaction and contact and achieves better results. Universal interactive pose prior. We visualize the animated motion in Fig. 7 on in-domain datasets (InterX[65], Dual-Human [7]) and out-of-domain datasets (Duolando [53], Hi4D [70], Interhuman [31]). Our approach generalizes to unseen subjects and interactions using the universal interactive pose prior. Our model is surprisingly capable of generating interactions beyond two persons without modification or retraining (see last row in Fig. 7). Open-world two-person image animation. Our model generalizes to open-world images by extracting interactive poses from FlickrCI3D [9] dataset using [39]. As shown in Fig. 5, it transforms static poses into realistic motion. 4.3. Interaction Motion Generation We evaluate interaction motion generation on the Inter-X dataset [65] using text and single-person poses. Text-to-interaction synthesis We focus on 3s interaction generation, evaluating FID, Diversity, and MModalitythe ability to generate diverse interactions from the same text [31, 59]. We compare with InterGen [31] and an end-to-end w/o interactive pose baseline, both trained and h o / u Figure 9. Single pose-to-interaction comparison on Inter-X dataset [65]. Compared to the model without interactive pose anchors, our method generates more natural human interactions. Single Pose Interactive Generation (leftright: time steps) + two person hug Figure 10. Diverse interactive motion generation. From single pose, our framework generates varied interactive poses (magenta box) and motions (1st, 2nd rows) and text-driven ones (3rd row). tested on the same data. As shown in Tab. 3 and Fig. 8, they struggle with contact modeling, while ours excels in shortterm interaction generation using interactive pose priors. Interaction synthesis from single pose We evaluate single pose-to-interaction synthesis on Inter-X [65] dataset, comparing our method with an end-to-end without interactive pose baseline, which struggles in the large motion space, as shown in Tab. 4 and Fig. 9. Our method leverages interactive poses to generate diverse motions under varying input conditions in Fig. 10. Open-world single-person image animation. Our model generalizes to open-world single-person images by estimating poses [4], generating interactive counterparts, and animating motion. Fig. 6 shows results on Motion-X [65] dataset. Input Image Interaction Generation (leftright: time steps) Figure 11. Interactive human video generation. Given single input image (left), our method generates interactive human motions that serve as intermediate results for video generation. We use an off-the-shelf human reconstruction model [46] to recover textured humans from single image. By pairing the generated motion with an arbitrary second person and applying the corresponding textures, we can produce realistic human interaction videos. 4.4. Interaction Video Generation Our method generated interactive human motion could serve as intermediate outputs for downstream video generation. While existing video diffusion models [3, 16, 18, 26] can synthesize human videos, their motions often lack temporal consistency and realism. In contrast, our generated motions provide stable and realistic foundation for interactive human video synthesis, either through pose-guided video diffusion models [19, 69, 73] or by texturizing motion sequences. As shown in Fig. 11, we use an off-theshelf human reconstruction model [46] to recover textured humans from single image. The generated interactive motion is then paired with an arbitrary second persons texture to produce realistic human interaction videos. 4.5. Limitations Our method has few limitations: (1) it focus on short interaction segments; (2) it relies solely on human poses, ignoring scene context; (3) pose inaccuracies may cause contact errors and foot sliding; (4) close interactions may lead to inter-person penetration. Please refer to the Sec. for more details. 5. Conclusion We introduce Ponimator, which integrates pose animator and generator for interactive pose animation and generation using conditional diffusion models. The animator leverages temporal priors for dynamic motion generation, the generator uses spatial priors to create interactive poses from single pose, text, or both. Ponimator enables open-world image interaction animation, single-pose interaction generation, and text-to-interaction synthesis, exhibiting strong generalization and realism across datasets and applications."
        },
        {
            "title": "References",
            "content": "[1] Chaitanya Ahuja and Louis-Philippe Morency. Language2pose: Natural language grounded pose forecasting. In 3DV. IEEE, 2019. 2 [2] Okan Arikan and David Forsyth. Interactive motion generation from examples. TOG, 2002. 2 [3] Andreas Blattmann, Robin Rombach, Huan Ling, Tim Dockhorn, Seung Wook Kim, Sanja Fidler, and Karsten Kreis. Align your latents: High-resolution video synthesis with latent diffusion models. In CVPR, 2023. 2, 9 [4] Zhongang Cai, Wanqi Yin, Ailing Zeng, Chen Wei, Qingping Sun, Wang Yanjun, Hui En Pang, Haiyi Mei, Mingyuan Zhang, Lei Zhang, Chen Change Loy, Lei Yang, and Ziwei Liu. SMPLer-X: Scaling up expressive human pose and shape estimation. In NeurIPS, 2023. 4, 6, 8, [5] Baptiste Chopin, Hao Tang, Naima Otberdout, Mohamed Daoudi, and Nicu Sebe. Interaction transformer for human reaction generation. Multimedia, 2023. 3 [6] Ke Fan, Junshu Tang, Weijian Cao, Ran Yi, Moran Li, Jingyu Gong, Jiangning Zhang, Yabiao Wang, Chengjie Wang, and Lizhuang Ma. Freemotion: unified framework arXiv preprint for number-free text-to-motion synthesis. arXiv:2405.15763, 2024. 3 [7] Qi Fang, Yinghui Fan, Yanjun Li, Junting Dong, Dingwei Wu, Weidong Zhang, and Kang Chen. Capturing closely interacted two-person motions with reaction priors. In CVPR, 2024. 2, 4, 6, 7, 8, 16, 17, 18, 21 [8] Yanwen Fang, Jintai Chen, Peng-Tao Jiang, Chao Li, Yifeng Geng, Eddy KF Lam, and Guodong Li. Pgformer: Proxy-bridged game transformer for multi-person highly interactive extreme motion prediction. arXiv preprint arXiv:2306.03374, 2023. 3 [9] Mihai Fieraru, Mihai Zanfir, Elisabeta Oneata, Alin-Ionut Popa, Vlad Olaru, and Cristian Sminchisescu. Threedimensional reconstruction of human interactions. In CVPR, 2020. 2, 5, 8, [10] Mihai Fieraru, Mihai Zanfir, Teodor Szente, Eduard Bazavan, Vlad Olaru, and Cristian Sminchisescu. Remips: Physically consistent 3d reconstruction of multiple interacting people under weak supervision. NeurIPS, 2021. 2 [11] Anindita Ghosh, Rishabh Dabral, Vladislav Golyanik, Christian Theobalt, and Philipp Slusallek. Remos: Reactive 3d motion synthesis for two-person interactions. arXiv preprint arXiv:2311.17057, 2023. 3 [12] Chuan Guo, Xinxin Zuo, Sen Wang, Shihao Zou, Qingyao Sun, Annan Deng, Minglun Gong, and Li Cheng. Action2motion: Conditioned generation of 3d human motions. In Multimedia, 2020. 2 [13] Chuan Guo, Shihao Zou, Xinxin Zuo, Sen Wang, Wei Ji, Xingyu Li, and Li Cheng. Generating diverse and natural 3d human motions from text. In CVPR, 2022. 13 [14] Chuan Guo, Xinxin Zuo, Sen Wang, and Li Cheng. Tm2t: Stochastic and tokenized modeling for the reciprocal generIn ECCV. Springer, ation of 3d human motions and texts. 2022. 2 [15] Wen Guo, Xiaoyu Bie, Xavier Alameda-Pineda, and Francesc Moreno-Noguer. Multi-person extreme motion prediction. In CVPR, 2022. [16] Yuwei Guo, Ceyuan Yang, Anyi Rao, Zhengyang Liang, Yaohui Wang, Yu Qiao, Maneesh Agrawala, Dahua Lin, and Bo Dai. Animatediff: Animate your personalized textto-image diffusion models without specific tuning. arXiv preprint arXiv:2307.04725, 2023. 2, 9 [17] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffusion probabilistic models. NeurIPS, 2020. 3 [18] Jonathan Ho, Tim Salimans, Alexey Gritsenko, William Chan, Mohammad Norouzi, and David Fleet. Video diffusion models. NeurIPS, 2022. 2, 9 [19] Li Hu. Animate anyone: Consistent and controllable imageto-video synthesis for character animation. In CVPR, pages 81538163, 2024. 9 [20] Buzhen Huang, Chen Li, Chongyang Xu, Liang Pan, Yangang Wang, and Gim Hee Lee. Closely interactive human reconstruction with proxemics and physics-guided adaption. In CVPR, 2024. [21] Xun Huang and Serge Belongie. Arbitrary style transfer in In ICCV, real-time with adaptive instance normalization. 2017. 4, 5, 13 [22] Eldar Insafutdinov, Mykhaylo Andriluka, Leonid Pishchulin, Siyu Tang, Evgeny Levinkov, Bjoern Andres, and Bernt Schiele. Arttrack: Articulated multi-person tracking in the wild. In CVPR, 2017. 3 [23] Umar Iqbal, Anton Milan, and Juergen Gall. Posetrack: Joint multi-person pose estimation and tracking. In CVPR, 2017. 3 [24] Muhammad Gohar Javed, Chuan Guo, Li Cheng, and Intermask: 3d human interaction generaarXiv preprint Xingyu Li. tion via collaborative masked modelling. arXiv:2410.10010, 2024. [25] Biao Jiang, Xin Chen, Wen Liu, Jingyi Yu, Gang Yu, and Tao Chen. Motiongpt: Human motion as foreign language. NeurIPS, 2023. 2 [26] Yang Jin, Zhicheng Sun, Ningyuan Li, Kun Xu, Hao Jiang, Nan Zhuang, Quzhe Huang, Yang Song, Yadong Mu, and Zhouchen Lin. Pyramidal flow matching for efficient video arXiv preprint arXiv:2410.05954, generative modeling. 2024. 9 [27] Korrawe Karunratanakul, Konpat Preechakul, Supasorn Suwajanakorn, and Siyu Tang. Guided motion diffusion for controllable human motion synthesis. In ICCV, 2023. 2 [28] Lucas Kovar and Michael Gleicher. Flexible automatic moIn Symposium on tion blending with registration curves. Computer Animation. San Diego, CA, USA, 2003. 2 [29] Lucas Kovar, Michael Gleicher, and Frederic Pighin. Motion graphs. In SIGGRAPH, 2008. 2 [30] Peizhuo Li, Kfir Aberman, Zihan Zhang, Rana Hanocka, and Olga Sorkine-Hornung. Ganimator: Neural motion synthesis from single sequence. TOG, 2022. [31] Han Liang, Wenqian Zhang, Wenxuan Li, Jingyi Yu, and Lan Xu. Intergen: Diffusion-based multi-human motion generation under complex interactions. IJCV, 2024. 2, 3, 4, 7, 8, 13, 17, 19 [32] Angela Lin, Lemeng Wu, Rodolfo Corona, Kevin Tai, Qixing Huang, and Raymond Mooney. Generating animated videos of human activities from natural language descriptions. Learning, 2018. 2 [33] Jing Lin, Ailing Zeng, Shunlin Lu, Yuanhao Cai, Ruimao Zhang, Haoqian Wang, and Lei Zhang. Motion-x: large-scale 3d expressive whole-body human motion dataset. NeurIPS, 2024. 6, 15, 17 [34] Shaowei Liu, Yang Zhou, Jimei Yang, Saurabh Gupta, and Shenlong Wang. Contactgen: Generative contact modeling for grasp generation. In ICCV, pages 2060920620, 2023. 3 [35] Yunze Liu, Changxi Chen, and Li Yi. Interactive humanoid: Online full-body motion reaction synthesis with social affordance canonicalization and forecasting. arXiv preprint arXiv:2312.08983, 2023. 3 [36] Matthew Loper, Naureen Mahmood, Javier Romero, Gerard Pons-Moll, and Michael J. Black. SMPL: skinned multiperson linear model. TOG, 2015. 17, 19 [37] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. arXiv preprint arXiv:1711.05101, 2017. 6 [38] Julieta Martinez, Michael Black, and Javier Romero. On human motion prediction using recurrent neural networks. In CVPR, 2017. 2 [39] Lea Muller, Vickie Ye, Georgios Pavlakos, Michael Black, and Angjoo Kanazawa. Generative proxemics: prior for 3d social interaction from images. In CVPR, 2024. 2, 4, 5, 6, 8, 13, 15 [40] Georgios Pavlakos, Vasileios Choutas, Nima Ghorbani, Timo Bolkart, Ahmed A. A. Osman, Dimitrios Tzionas, and Michael J. Black. Expressive body capture: 3d hands, face, and body from single image. In CVPR, 2019. 3, 5, 17, 19 [41] William Peebles and Saining Xie. Scalable diffusion models with transformers. In ICCV, 2023. 4, 13 [42] Xiaogang Peng, Yaodi Shen, Haoran Wang, Binling Nie, Yigang Wang, and Zizhao Wu. Somoformer: Social-aware motion transformer for multi-person motion prediction. arXiv preprint arXiv:2208.09224, 2022. [43] Mathis Petrovich, Michael Black, and Gul Varol. Actionconditioned 3d human motion synthesis with transformer vae. In ICCV, 2021. 2 [44] Mathis Petrovich, Michael Black, and Gul Varol. Temos: Generating diverse human motions from textual descriptions. In ECCV. Springer, 2022. 2 [45] Pablo Ruiz Ponce, German Barquero, Cristina Palmero, Sergio Escalera, and Jose Garcia-Rodriguez. in2in: Leveraging individual information to generate human interactions. arXiv preprint arXiv:2404.09988, 2024. 3 [46] Lingteng Qiu, Xiaodong Gu, Peihao Li, Qi Zuo, Weichao Shen, Junfei Zhang, Kejie Qiu, Weihao Yuan, Guanying Chen, Zilong Dong, and Liefeng Bo. Lhm: Large animatable human reconstruction model from single image in seconds. In ICCV, 2025. 9 [47] Sigal Raab, Inbal Leibovitch, Peizhuo Li, Kfir Aberman, Olga Sorkine-Hornung, and Daniel Cohen-Or. Modi: UnIn CVPR, conditional motion synthesis from diverse data. 2023. 6 [48] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learning transferable visual models from natural language supervision. In ICML. PMLR, 2021. 5, [49] Muhammad Rameez Ur Rahman, Luca Scofano, Edoardo De Matteis, Alessandro Flaborea, Alessio Sampieri, and Fabio Galasso. Best practices for 2-body pose forecasting. In CVPR, 2023. 3 [50] Yoni Shafir, Guy Tevet, Roy Kapon, and Amit Haim Bermano. Human motion diffusion as generative prior. In ICLR, 2024. 3, 6, 7 [51] Mengyi Shan, Lu Dong, Yutao Han, Yuan Yao, Tao Liu, Ifeoma Nwogu, Guo-Jun Qi, and Mitch Hill. Towards open domain text-driven synthesis of multi-person motions. arXiv preprint arXiv:2405.18483, 2024. 3 [52] Bing Shuai, Alessandro Bergamo, Uta Buechler, Andrew Berneshawi, Alyssa Boden, and Joseph Tighe. Large scale real-world multi-person tracking. In ECCV. Springer, 2022. 3 [53] Li Siyao, Tianpei Gu, Zhitao Yang, Zhengyu Lin, Ziwei Liu, Henghui Ding, Lei Yang, and Chen Change Loy. Duolando: learning for Follower gpt with off-policy reinforcement dance accompaniment. arXiv preprint arXiv:2403.18811, 2024. 3, 7, 8, 16, 18 [54] Jiaming Song, Chenlin Meng, Denoising diffusion implicit models. arXiv:2010.02502, 2020. 6, 13 and Stefano Ermon. arXiv preprint [55] Sebastian Starke, Yiwei Zhao, Taku Komura, and Kazi Zaman. Local motion phases for learning multi-contact character movements. TOG, 2020. 3 [56] Sebastian Starke, Yiwei Zhao, Fabio Zinno, and Taku Komura. Neural animation layering for synthesizing martial arts movements. TOG, 2021. 3 [57] Roman Suvorov, Elizaveta Logacheva, Anton Mashikhin, Anastasia Remizova, Arsenii Ashukha, Aleksei Silvestrov, Naejin Kong, Harshith Goka, Kiwoong Park, and Victor Lempitsky. Resolution-robust large mask inpainting with fourier convolutions. In WACV, 2022. 15 [58] Mikihiro Tanaka and Kent Fujiwara. Role-aware interaction generation from textual description. In ICCV, 2023. 2, 3, 7 [59] Guy Tevet, Sigal Raab, Brian Gordon, Yoni Shafir, Daniel Cohen-or, and Amit Haim Bermano. Human motion diffusion model. In ICLR, 2023. 2, 3, 4, 6, 7, 8, [60] Nicolas Ugrinovic, Boxiao Pan, Georgios Pavlakos, Despoina Paschalidou, Bokui Shen, Jordi Sanchez-Riera, Francesc Moreno-Noguer, and Leonidas Guibas. Multiphys: multi-person physics-aware 3d motion estimation. In CVPR, 2024. 2 [61] Vaswani. Attention is all you need. NeurIPS, 2017. 4, 13 [62] Jiashun Wang, Huazhe Xu, Medhini Narasimhan, and Xiaolong Wang. Multi-person 3d motion prediction with multirange transformers. NeurIPS, 2021. 3 [63] Jingbo Wang, Sijie Yan, Bo Dai, and Dahua Lin. Sceneaware generative network for human motion synthesis. In CVPR, 2021. 2 [64] Jingbo Wang, Yu Rong, Jingyuan Liu, Sijie Yan, Dahua Lin, and Bo Dai. Towards diverse and natural scene-aware 3d human motion synthesis. In CVPR, 2022. 2 [65] Liang Xu, Xintao Lv, Yichao Yan, Xin Jin, Shuwen Wu, Congsheng Xu, Yifan Liu, Yizhou Zhou, Fengyun Rao, Xingdong Sheng, et al. Inter-x: Towards versatile humanhuman interaction analysis. In CVPR, 2024. 2, 4, 6, 7, 8, 16, 17, 18, 19, 21 [66] Liang Xu, Yizhou Zhou, Yichao Yan, Xin Jin, Wenhan Zhu, Fengyun Rao, Xiaokang Yang, and Wenjun Zeng. Regennet: Towards human action-reaction synthesis. In CVPR, 2024. 3 [67] Qingyao Xu, Weibo Mao, Jingze Gong, Chenxin Xu, Siheng Chen, Weidi Xie, Ya Zhang, and Yanfeng Wang. Jointrelation transformer for multi-person motion prediction. In ICCV, 2023. [68] Sirui Xu, Yu-Xiong Wang, and Liangyan Gui. Stochastic multi-person 3d motion forecasting. In ICLR, 2023. 3 [69] Jingyun Xue, Hongfa Wang, Qi Tian, Yue Ma, Andong Wang, Zhiyuan Zhao, Shaobo Min, Wenzhe Zhao, Kaihao Zhang, Heung-Yeung Shum, et al. Follow-your-pose v2: Multiple-condition guided character image animation for stable pose control. arXiv preprint arXiv:2406.03035, 2024. 9 [70] Yifei Yin, Chen Guo, Manuel Kaufmann, Juan Zarate, Jie Song, and Otmar Hilliges. Hi4d: 4d instance segmentation of close human interaction. In CVPR, 2023. 7, 8 [71] Jianrong Zhang, Yangsong Zhang, Xiaodong Cun, Yong Zhang, Hongwei Zhao, Hongtao Lu, Xi Shen, and Ying Shan. Generating human motion from textual descriptions with discrete representations. In CVPR, 2023. 2 [72] Mingyuan Zhang, Zhongang Cai, Liang Pan, Fangzhou Hong, Xinying Guo, Lei Yang, and Ziwei Liu. Motiondiffuse: Text-driven human motion generation with diffusion model. arXiv preprint arXiv:2208.15001, 2022. 2 [73] Shenhao Zhu, Junming Leo Chen, Zuozhuo Dai, Yinghui Xu, Xun Cao, Yao Yao, Hao Zhu, and Siyu Zhu. Champ: Controllable and consistent human image animation with 3d parametric guidance. In ECCV, 2024. 9 Ponimator: Unfolding Interactive Pose for Versatile Human-human Interaction Animation"
        },
        {
            "title": "Supplementary Material",
            "content": "https://stevenlsw.github.io/ponimator/"
        },
        {
            "title": "Abstract",
            "content": "The supplementary material provides implementation details, limitation analysis, qualitative results and future work. In summary, we include Sec. A. Implementation details and model architecture of the interactive pose animator and generator. Sec. B. Limitation analysis of our current approach. Sec. C. Additional qualitative results of long interactive motion generation, complex interaction synthesis, single-person image interaction generation, interactive pose animation, text-to-interaction motion synthesis, and single-pose-tointeraction motion synthesis. two-person image animation, A. Implementation details Interactive pose extraction. Given two-person pose from motion sequence, we determine close contact by measuring the minimum distance between their SMPL-X meshe vertices. Following [39], we downsample the mesh based on predefined contact regions and compute pairwise distances. If the smallest distance is below 1.3cm, we classify the pose as proximity poseindicating contact between the individuals. This interactive pose is then used to train human interaction dynamics. Model architecture. Our pose animator and pose generator follow the DiT architecture [41], which consists of stacked Transformer blocks [61], each incorporating an attention mechanism and feed-forward network (FFN). Both the animator and generator comprise 8 Transformer layers, with the animator utilizing both spatialand temporal-attention blocks, while the generator employs only spatial attention. The model has latent dimension of 1024, with 8-head multi-head attention, and uses the GELU activation function. The input motions are first encoded with positional encoding before being processed by Transformer blocks. The input has the shape (B, P, N, D), where is batch size, = 2 represents the number of individuals, and corresponds to number of frames, and is the dimension of diffusion target z0. Spatial attention operates along the - dimension to model interactions between individuals, while temporal attention captures motion dynamics along the - dimension. The models output layer is linear MLP, initialized with zero weights, which generates residual motion outputs. These residual motions are added to the interactive pose to produce the final output. Conditional information is incorporated into the model using Adaptive Instance Normalization [21]. Training. We apply training data augmentation to interactive poses in the interactive pose animator by adding random noise with scale of 0.02 to account for real-world inaccuracies in pose estimation. This ensures that even if the interactive pose estimator introduces noise, the animator can still produce reasonable results. This augmentation is performed online during training. Following prior work [13, 31], we align one persons pose in the interactive pose to face the positive direction and center it at the origin. The interaction loss in the pose animator follows [31] and consists of **contact loss**, which encourages contact between two individuals when their joints are close, and **relative orientation loss**, which aligns their global orientations with the ground truth. The velocity loss Lvel, following MDM [59], ensures motion coherence by minimizing the velocity difference between the generated motion and the ground truth. For diffusion training, we use cosine scheduler with 1000 diffusion steps and DDIM sampling [54] for 50 steps during inference. The model is trained with learning rate of 1e-4 and weight decay of 0.00002 for 4000 epochs. The batch size is 256 for the interactive pose animator and 512 for the interactive pose generator. Training takes 2 days for the pose animator and 1 day for the pose generator on 4A100 GPUs. Inference speed comparison. Our interactive pose generation takes 0.21s on single A100 on average, the interactive pose animator generates 3s motion at 10fps in 0.24s, comparable to InterGen [31] which requires 0.76s for the same motion length. B. Limitation Analysis Our method has the limitations below. The common failure modes are illustrated in Fig. 12. Short motion modeling. Our method is mainly focus on short interactive motion segments. While our framework could support longer generation by interactive pose chaining as shown in Fig. 13, the benefit of interactive pose prior would diminish over time. In text-to-interaction synthesis, our framework prioritizes interactive motion-relevant information, which can result in partial rather than complete motion sequences when the input text describes extended Input Interaction Animation (leftright: time steps) Figure 12. Method limitation analysis. The first two rows show in-the-wild interactive pose animation results. In the first sample, severe interpenetration occurs as our method does not explicitly model penetration between two individuals. In the second, the generated motion is physically implausible due to the lack of scene context awareness, leading to collisions with the environment. The bottom two rows illustrate interaction motion generation from single pose input. Due to inaccuracies in interactive pose generation, our method fails to produce realistic contact, resulting in unnatural motion. human interactions. Moreover, our pose animatortaking only interactive poses as inputcannot fully capture the semantic context or temporal ordering in text (e.g., distinIncorporatguishing lifting up from putting down). ing text conditioning into the pose-to-interaction stage is promising avenue for improving text-to-interactionspecific tasks. However, since our main focus is on pose-tointeraction animation without enforced text input, this ambiguity can be strength, enabling multiple valid and physically plausible motion interpretations from the same interactive pose. Inter-person penetrations. While our method enhances it does not expliccontact itly model interpenetration between individuals. Consequently, in close-contact scenariossuch as the first row in in two-person interactions, Fig. 12some interpenetration may occur in the generated motion sequences. Achieving balance between realistic contact and preventing interpenetration remains challenging problem, as enforcing strict physical constraints could compromise natural motion quality. Addressing interpenetration modeling and ensuring physically plausible twoperson interaction motion generation is an important direction for future work. Lack of scene awareness. When applied to in-thewild two-person pose animation or motion generation, our method relies solely on human pose information and ignores the surrounding environment. As result, generated motions may appear physically implausible in certain cases, such as the 2nd row of Fig. 12, where collisions occur. Moreover, interactive poses can sometimes be ambiguFigure 13. Longer motion generation by chaining interactive poses. We reuse the last generated pose as the next input, resetting interactive time to zero, enabling sliding-window synthesis of longer motions (key-frame in magenta box). Interactive Pose Interaction Animation (leftright: time steps) Figure 14. Complex interactive pose animation. Given an interactive pose, our pose animator can synthesize high-dynamics (1st row) and close-contact (2nd row) human-human motions, leveraging the strong interactive prior learned from high-quality mocap data. ous, causing noticeable motion errors when used as the sole input. more robust approach would integrate additional scene information (e.g. image features) to improve motion prediction and dynamics forecasting. Inaccurate contact. The interactive pose estimator or our interactive pose generator may occasionally produce inaccurate interactive poses, resulting in poor humanhuman contact in the generated motions, as seen in the 3rd and 4th rows of Fig. 12. These inaccuracies result in unrealistic motion due to the lack of precise interactive pose inputs. Since the pose animator primarily models temporal dynamics and depends on the interactive pose for spatial information, it often cannot correct errors arising from inaccurate interactive poses. Additionally, our generated interaction motions may exhibit artifacts such as foot sliding, common issue in human motion synthesis. While such artifacts can often be mitigated through post-processing, we do not apply any post-processing in our examples. C. Qualitative results Longer interactive motion generation. Our framework is designed for short-term interaction generation but naturally extends to longer sequences. The pose animator takes an interactive pose together with an interactive time to synthesize both past and future motions centered on that pose. Longer sequences are produced by chaining segments in slidingthe last generated pose of one segment window manner: is reused as the starting pose for the next, the interactive time index is reset to zero (beginning of the new segment), and generation continues. Repeating this process yields coherent long-term interactions, as shown in Fig. 13, where key-frames are labeled in magenta box. Complex interactive pose animation. As shown in Fig. 14, beyond daily motions, our pose animator can synthesize complex interactive motions involving high dynamics (1st row) and close contact (2nd row) between two people, benefiting from the strong interaction dynamics learned from high-quality mocap data. Two person image human motion animation. We provide additional in-the-wild interactive pose animation results in Fig. 15. Given an interactive frame, we extract two-person poses using an off-the-shelf model [39], and animate the them with our interactive pose animator. To render the interaction, we use an off-the-shelf inpainting model [57] to remove the original individuals and overlay the generated motion. The results demonstrate that our model generalizes well to in-the-wild interactive poses, producing realistic human-human interactions. Single-person image human motion interaction generation. We present additional single-person image interaction motion generation results on the Motion-X dataset [33] in Fig. 16. Given single-person image, we first extract the pose using an off-the-shelf pose estimator [4] and then generate interactive poses with our interactive pose generator. As shown, our model synthesizes plausible interactions Input Interaction Animation (leftright: time steps) Figure 15. Interactive pose image animation on FlickrCI3D dataset [9]. Left shows the input image, right shows the animated interaction motions. Interactive-pose frame is labeled in green box. Our model generalizes well to in-the-wild interactive poses, producing realistic human-human interaction dynamics. from diverse single-person inputs. Finally, we apply our interactive pose animator to generate two-person dynamics, demonstrating its effectiveness in challenging in-the-wild scenarios. Interactive pose animation. We provide additional visualizations of interactive pose animation on the InterX dataset [65], Dual-Human dataset [7], and Duolando dataset [53] in Fig. 17. Our model could successfully synInput Interaction Animation (leftright: time steps) One person pushes the other Figure 16. Single-person pose interaction generation on Motion-X dataset [33]. Left shows the single person image input, right shows the generated two-person interaction dynamics. The generated interactive pose frame is labeled in magenta box. The bottom row show the single-pose input with accompanying text input. Given different single-person poses, our interactive pose generator produces plausible interactive poses under flexible conditions, while our interactive pose animator synthesizes realistic human-human motions. Our model demonstrates strong performance in challenging in-the-wild settings. method effectively generates realistic two-person interactions from short phrases or simple words. By leveraging an intermediate interactive pose representation, our approach ensures consistent interaction and maintains accurate contact between the two individuals. Single pose-to-interaction motion synthesis. We present single pose-to-interaction motion synthesis results on the Inter-X [65] and Dual-Human [7] datasets in Fig. 21. As shown, our method generates appropriate interactive poses from various input poses while effectively capturing vivid underlying human dynamics. thesize realistic dancing motions from out-of-domain interactive poses on the unseen Duolando dataset. We further evaluate our method on the InterHuman dataset [31], more challenging out-of-distribution benchmark, with results shown in Fig. 18. InterHuman provides SMPLH [36] annotations for two-person interactions, primarily for text-to-motion generation, but with less accurate contact. To fit our framework, we convert the SMPLH [36] representation to SMPLX [40] and extract interactive poses from the test sequences. Despite annotation noise and diverse pose distributions, our model produces realistic and coherent interactions, demonstrating strong generalization of the interactive pose prior. We also provide qualitative comparison with two baselinesInterGen* and the random-pose variant (see Tab. 2)in Fig. 19. InterGen [31] and the random-pose model exhibit poorer contact and more body penetration than ours, highlighting the effectiveness of interactive pose priors for realistic contact and interaction synthesis. Text-to-interaction synthesis. We present additional textto-interaction motion synthesis results in Fig. 20. Our Interactive Pose Interaction Animation (leftright: time steps) Inter-X Dataset [65] Dual-Human Dataset [7] Figure 17. More interactive pose animation visualization on Inter-X dataset [65], Dual-Human dataset [7], Duolando dataset [53]. Our pose animator generalizes well to out-of-domain interactive poses and synthesizes realistic dancing motions on the unseen Duolando two-person dancing motion dataset. Duolando Dataset [53] Interactive Pose Interaction Animation (leftright: time steps) Figure 18. Interhuman dataset [31] interactive pose animation results. We convert dataset provided SMPLH [36] to SMPLX [40] representation and select interactive poses from test motion sequences. Despite contact inaccuracies due to dataset conventions and pose variations, our model synthesizes reasonable motions, demonstrating the strong generalization capability of interactive poses for guiding human interaction animation. Interactive Pose Interaction Animation (leftright: time steps) ] 1 3 [ r I c W / O Figure 19. Interactive pose animation comparison on Inter-X dataset [65]. Compared to InterGen [31] and model trained with random poses, our method achieves better contact and human dynamics. Both baselines exhibit severe body penetration and less accurate contact, while our approach, guided by interactive poses, ensures more realistic interactions. Input Text: One person chases the other person Input Text: One person sits down first, another sits on his/her lap Input Text: One person goes to the other persons ear and whispers to him/her Input Text: hand shake Input Text: hug Figure 20. More text-to-interaction motion synthesis results. Our method synthesizes realistic two-person interactions from short phrases or single words. Input Text: posing Single Pose Interaction Generation (leftright: time steps) Inter-X Dataset [65] Figure 21. Single-pose guided interaction motion synthesis result on Inter-X [65] and Dual-Human [7] datasets. The input singleperson pose is shown on the left. Our method generates appropriate interactive poses from various inputs, capturing vivid underlying human dynamics. Dual-Human Dataset [7]"
        }
    ],
    "affiliations": [
        "Snap Inc.",
        "University of Illinois Urbana-Champaign"
    ]
}