{
    "paper_title": "SageAttention: Accurate 8-Bit Attention for Plug-and-play Inference Acceleration",
    "authors": [
        "Jintao Zhang",
        "Jia wei",
        "Haofeng Huang",
        "Pengle Zhang",
        "Jun Zhu",
        "Jianfei Chen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "The transformer architecture predominates across various models. As the heart of the transformer, attention has a computational complexity of O(N^2), compared to O(N) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although quantization has proven to be an effective method for accelerating model inference, existing quantization methods primarily focus on optimizing the linear layer. In response, we first analyze the feasibility of quantization in attention detailedly. Following that, we propose SageAttention, a highly efficient and accurate quantization method for attention. The OPS (operations per second) of our approach outperforms FlashAttention2 and xformers by about 2.1 times and 2.7 times, respectively. SageAttention also achieves superior accuracy performance over FlashAttention3. Comprehensive experiments confirm that our approach incurs almost no end-to-end metrics loss across diverse models, including those for large language processing, image generation, and video generation. The codes are available at https://github.com/thu-ml/SageAttention."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 1 ] . [ 2 7 6 3 2 0 . 0 1 4 2 : r SAGEATTENTION: ACCURATE 8-BIT ATTENTION FOR PLUG-AND-PLAY INFERENCE ACCELERATION Jintao Zhang, Jia Wei, Haofeng Huang, Pengle Zhang, Jun Zhu, Jianfei Chen Tsinghua University zhang-jt24@mails., jianfeic@, dcszj@ { tsinghua.edu.cn }"
        },
        {
            "title": "ABSTRACT",
            "content": "The transformer architecture predominates across various models. As the heart of the transformer, attention has computational complexity of O(N 2), compared to O(N ) for linear transformations. When handling large sequence lengths, attention becomes the primary time-consuming component. Although quantization has proven to be an effective method for accelerating model inference, existing quantization methods primarily focus on optimizing the linear layer. In response, we first analyze the feasibility of quantization in attention detailedly. Following that, we propose SageAttention, highly efficient and accurate quantization method for attention. The OPS (operations per second) of our approach outperforms FlashAttention2 and xformers by about 2.1x and 2.7x, respectively. SageAttention also achieves superior accuracy performance over FlashAttention3. Comprehensive experiments confirm that our approach incurs almost no end-to-end metrics loss across diverse modelsincluding those for large language processing, image generation, and video generation. The codes are available at https://github.com/thu-ml/SageAttention. Figure 1: An example of SageAttention on video generation (CogvideoX on RTX4090)."
        },
        {
            "title": "INTRODUCTION",
            "content": "Attention is the fundamental component of transformers (Vaswani, 2017), and efficiently computing attention is crucial for transformer-based applications. Moreover, there is recent trend in processing longer sequences, which further strengthens the need for faster attention. In tasks like video generation (Yang et al., 2024) and language model prefilling (Dubey et al., 2024), the sequence length can easily go up to 8K 128K. Due to its quadratic complexity, the cost of attention dominates all other operations in such scenarios, as illustrated in Figure 2. Quantization is an effective strategy for enhancing neural networks computational and memory efficiency by reducing the numerical precision. There are abundant works on accelerating training (Sun et al., 2019; Xi et al., 2024; Peng et al., 2023) and inference (Jacob et al., 2018; Xiao et al., 2023a) with low-precision numerical formats such as FP8, INT8, or INT4. However, existing works primarily focused on quantizing the linear layer, where attention is left unaccelerated in high-precision, 1 Figure 2: Latency of attention. Figure 3: comparison example. such as FP16. There is not yet work that systematically investigates the quantization of attention. Moreover, many quantization methods require extra training, and the cost can be prohibitive for large-scale models. While FlashAttention3 (Shah et al., 2024) was released recently and offers an FP8 version tailored and only can be used for the Nvidia Hopper architecture, this exclusive optimization limits its broader applicability. Furthermore, our analysis demonstrates that directly implementing the FP8 version can lead to performance degradation, as detailed in Table 1. Quantizing attention is challenging. The computation of attention is more complex than that of linear operations. Attention includes softmax operation and two matrix multiplication (Matmul) operations: QK and . Direct 8-bit quantization and dequantization of the matrices (Q, K, P, ) in attention will result in significantly degraded performance across various models. For example, the text-to-image model Unidiffuser (Bao et al., 2023) will generate completely blurry image with both INT8 and FlashAttention3s FP8 implementation (See Figure 3), and Llama2 only achieves random-guessing-level accuracy of 25.5% on the MMLU dataset with INT8 attention. After investigating deeply, we identified two primary challenges: (C1) The matrix exhibits significant channel-wise outlier, leading to substantial accuracy loss during quantization. (C2) Simply quantizing (P, ) into INT8 does not consistently ensure the accuracy of across various scenarios. In this paper, we propose SageAttention, quantization method to accelerate attention while preserving accuracy. SageAttention is easy-to-use. As post-training quantization method, it can be used in plug-and-play manner in inference time by simply replacing the original highprecision implementation. We propose several techniques to achieve this goal. First, we opt to quantize the tensors in attention to INT8 rather than FP8. This decision is based on the fact that INT8 Matmul on some commonly used GPUs, e.g., RTX4090 and 3090, are four times faster than in FP16 and two times faster than FP8. Moreover, INT8 quantization for matrices (Q, K) is more precise than FP8 in attention (See Table 2). To address (C1), we propose method to smooth the matrix. This method significantly enhances accuracy with negligible time overhead (<0.2%). To address (C2), alternative to quantizing (P, ) to 8-bit, we propose more accurate yet efficient method for the Matmul : we maintain (P, ) in FP16 and use low-precision FP16 accumulator. This strategy doubles Matmuls speed without sacrificing any accuracy. Finally, we implement several versions of attention with different speed-accuracy tradeoffs and propose method to select the fastest attention implementation for each layer while preserving accuracy. We offer high-performance implementation of SageAttention on RTX4090 and 3090 GPUs using Triton (Tillet et al., 2019). Our implementation contains fused kernel combining ROPE with quantization and fast self-attention kernel inspired by FlashAttention-style tiling. The implementation utilizes the fast INT8 mma(u8.u8.s32) and FP16-with-FP16-accumulator mma(f16.f16.f16) instructions of Nvidia Tensor Core. Our kernel is about 2.1 faster than FlashAttention2 and xformers, respectively. Notably, it achieves 340 TOPS on RTX4090 at headdim=64 and headdim=128, reaching 52% of the theoretical INT8 throughput. In contrast, the peak for the stateof-the-art FlashAttention2 is only 165 TOPS. Moreover, at headdim=64, our throughput on RTX 4090 is even close to the 490 TOPS throughput of FlashAttention3, which is exclusive to the much more powerful and expensive Hopper GPUs. We extensively evaluate the end-to-end metrics of our approach on state-of-the-art image/video generation, image classification, and language models. On all tasks, SageAttention can be directly adopted in plug-and-play manner with negligible loss in model performance, while offering more than 2 speedup than FlashAttention2 and xformers. and 2."
        },
        {
            "title": "2 RELATED WORK",
            "content": "We categorize efficient Attention works into three groups: (1) Sparse Attention. This strategy only selects parts of sequence from given context for processing with standard Attention. Implementations like Swin transformer (Liu et al., 2021), Twins (Chu et al., 2021), UniFormer (Li et al.), Attentionsinks (Xiao et al., 2023b), InfLLM (Xiao et al., 2024), LongLora (Chen et al., 2023), Minference (Jiang et al., 2024), and SkipAttention (Venkataramanan et al., 2023) show promise. However, these methods limitations are that they only work in few scenarios because omitted calculations are not always useless. (2) Linear Attention. Techniques that transform Attention computation to reduce time complexity, for example, Linformer (Wang et al., 2020), Performer (Choromanski et al., 2020), MetaFormer (Yu et al., 2022), and LinearAttention (Katharopoulos et al., 2020), which lower the time complexity of Attention from O(N 2) into O(N ). These methods excel in specific scenarios while standard Attention remains prevalent. (3) Kernel Optimization. Rather than simplifying calculations, these methods exploit hardware capacities to enhance speed. The xformers (Lefaudeux et al., 2022) platform accelerates Attention with customizable blocks and dedicated CUDA kernels. FlashAttention (Dao et al., 2022) proposes tiling to reduce the memory reads/writes between GPU global memory and on-chip SRAM for significant speedups. FlashAttention2 (Dao, 2023) refine the parallelism and warps partition of FlashAttention. Bikshandi & Shah (2023) further optimize FlashAttention2 by kernel fusion. FlashAttention3 (Shah et al., 2024) is proposed for Hopper architecture. However, FlashAttention3 is exclusive to the Hopper GPU architecture, and the accuracy of its quantization version is significantly lower than our method (See Table 1). RingAttention (Liu et al.) scales FlashAttention across multiple GPUs. I-bert (Kim et al., 2021) quantizes all tensors in transformer block into INT8 but is restricted to RoBERTa. Our method falls under the third category, and is orthotopic with the first and second categories."
        },
        {
            "title": "3 PRELIMINARY",
            "content": "Our method builds on FlashAttention-2 and adopts dynamic quantization. We will begin by reviewing FlashAttention-2, followed by brief introduction to dynamic quantization techniques. 3.1 FLASHATTENTION (cid:80) d, while the matrix S, are The computation of self-attention can be formulated as follows: = QK /d, = σ(S), = exp(Sik) is the softmax operation. The matrices Q, K, and , where σ(S)ij = exp(Sij)/ each have dimensions . While is typically small, e.g., 64 matrices (S, ) are much larger or 128, can be thousands if not millions. Therefore, the than (Q, K, ), and naive implementation suffers from the huge amount of global memory I/O for (S, ) reads/writes. FlashAttention (Dao, 2023) proposes to tile Q, K, and from the token dimension into blocks with block size of bq, bkv, bkv, respectively. Then, to avoid , Ki} the memory I/O for (S, ), it uses online softmax (Milakov & Gimelshein, 2018) to progressively compute each block of O, i.e., Oi: Vi} { , it computes the following equations iteratively: First, for each block of , Qi} { , Ki} Vi} { { { = exp(mj lj mj1 and lj are bq Sj = QiK )lj + rowsum( /d, (mj , ), Oj ) = σ(mj1 exp(mj = diag (cid:101) , Sj ), mj1 ) Oj1 + Vj (1) (2) (cid:16) (cid:17) and 0 respectively. σ() is an online (cid:101) Where mj softmax operator: mj = exp(Sj )1Oj Finally, the output Oi can be computed by Oi = diag(lj . (cid:101) (cid:101) 1 vectors, which are initialized to , rowmax(Sj ) mj1 = max { , } mj ). 3.2 DYNAMIC QUANTIZATION matrix multiplication = AB can be accelerated with quantization as: (δA, ˆA) = ψ(A), (δB, ˆB) = ψ(B), ˆC = ˆA ˆB, = ψ1 δAδB ( ˆC) (3) 3 Here, ψ is quantizer which converts high-precision (e.g., FP32) matrix to low-precision format ˆA (e.g., INT8 or FP8) with scale δA, and ψ1 is dequantizer to convert back to highA. The actual matrix multiplication ˆA ˆB is carried in lowprecision. We should have ψ1 precision. In modern GPUs, low-precision matrix multiplication is usually multiple times faster than higher-precision ones. δA ( ˆA) Many quantizers depend on the numerical format and granularity, e.g., how many elements share common scale factor. For example, an INT8 per-tensor dynamic quantizer first computes the scale as the maximum absolute value of the entire tensor, scales the elements to the maximum representable range of INT8 [-127, +127], and then casts to INT8 with rounding: ˆA = , δA = )/127. Likewise, per-token quantizer assigns scale factor for each token of tensor: max( ˆA[i, :] = )/127. Also, per-channel quantizer assigns scale A[i, :] , δA[i, :] = max( A[i, :]/δA factor for each channel of the tensor, i.e., along the channel dimension: A[:, i] = , δA = )/127. Based on the tiling approach of FlashAttention, we can apply per-block quanA[:, i] max( tization correspondingly. per-block quantizer asigns scale factor for every = tokens: ˆA[m : n, :] = )/127. Dequantization simply involves element-wise scaling: ψ1 A[m : n, :]/δA A[m : n, :] A[:, i]/δA , δA = max( A/δA δA ( ˆA) = δA ˆA."
        },
        {
            "title": "4 SAGE ATTENTION",
            "content": "In this section, we propose SageAttention, fast yet accurate method to accelerate attention computation with 8-bit quantization. Considering that most networks are not natively trained with quantized attention, SageAttention is designed to be plug-and-play. Unlike linear layers, which are easy to quantize, quantizing attention is more complicated. Extra treatment is required to ensure both good accuracy and fast speed. First, we will formulate quantized attention in Section 4.1, followed by introducing our approach. 4.1 FORMULATION Based on the description of FlashAttention and dynamic quantization in Section 3.1 and 3.2, we formulate the quantized attention as follows. Quantization: (δQ, ˆQ) = ψQ(Q/ Attention: = ψ1 ( ˆQ ˆK ), (m, ) = σ(m, S), = diag (cid:0)exp(m m)(cid:1) + ψ d), (δK , ˆK) = ϕK (K), (δP , ˆP ) = ψP ( (cid:101)P ), (δV , ˆV ) = ψV (V ) (4) ( ˆP ˆV ) (5) δQδK δP δV ϕK is transformation to obtain quantized K, which we shall discuss in subsequent sections. For simplicity, we omit all superscripts and subscripts, but the matrices used in attention are still tiles, and the computation is still organized as FlashAttention described in Section 3.1. Compared to the original full-precision version, as shown in Eq. 4, 5, SageAttention adds quantizers to and . Online Q, K, P, and dequantizers to the product to accelerate both Matmuls of QK softmax is left in full-precision. Figure 4: Typical examples of data distribution of (Q, K, V). 4 Table 1: End-to-end metrics comparison of different quantization methods. Quantization (Q, K) Full-Precision Per-token Per-block Per-tensor Smoothing - FlashAttn3 (with quant) Llama WikiText 5.823 5.824 5.824 5.825 5.824 5.826 5.824 5. CogVideo (Fscore) 3.768 1.924 3.734 2.014 3.718 1.902 3.640 3.394 Unidiffuser (FID) 163.33 221.18 166.52 229.08 166.93 267.06 167.65 394.13 UltraPixel (FID) 179.78 193.36 179.79 195.67 179.98 196.26 180.21 383.61 TIMM ImageNet 84.79% 84.21% 84.74% 84.18% 84.76% 84.12% 84.69% 84.70%"
        },
        {
            "title": "4.2 SMOOTH MATRIX K",
            "content": "Directly quantizing Q, often results in large error. Particularly, quantizing Q, to INT8 yields completely blurry image/video in text-to-image/video tasks. As shown in Figure 4.1, we visualize two typical groups of Q, K, from text-to-image model Unidiffuser (Bao et al., 2023) and text-to-video model CogvideoX (Yang et al., 2024). Notably, exhibits distinct channel-wised outlier. However, per-channel quantization cannot be applied for K, because quantization can only be performed at the outer axis (token dim) of the Matmul QK . Moreover, the previous smoothing technique proposed for linear layers (Xiao et al., 2023a) cannot be applied since is also heavily affected by outliers. Fortunately, the channel outliers of have pattern: Each tokens key is actually large bias shared by all tokens, plus small token-wise signal. Therefore, the outlier is not from large variation across tokens, but simply the large bias. Based on this observation, we propose to smooth the matrix by transform γ, which subtracts averaged across all tokens: γ(K) = mean(K) (6) t=1 K[t, :] is the average key, with shape where mean(K) = 1 d. Note that such transformation does not change the attention score , because for any query q, we have mean(K)) = σ(qK ). Finally, the transformation from σ(q(K full-precision to quantized ˆK can be written as ϕK(K) = ψK γ, where ψK is quantizer. In other words, full-precision is substracted with the mean, before eventually being quantized. mean(K))) = σ(qK (cid:80) Table 1 presents end-to-end metrics for different quantization methods with and without smoothing on various models. The results demonstrate that smoothing offers significant benefits of accuracy. Moreover, the speed overhead of smoothing for attention is less than 0.2% (See Table10). Table 2: Average accuracy using different data types across all layers of real models. Q, , Cos Sim 99.94% E4M3 INT8 E5M2 99.81% (cid:101) 99.70% INT8 99.81% E4M3 99.68% E5M2 99.58% INT8 99.37% E4M3 99.22% E5M2 99.13% INT E4M3 E5M2 Relative L1 0.0345 0.0572 0.1035 0.0607 0.0769 0.1199 0.1107 0.1213 0.1583 RMSE 3.53e-3 6.11e-3 6.82e-3 5.93e-3 7.72e-3 8.31e-3 1.09e-2 1.20e-2 1.24eTable 3: Worst accuracy using different data types across all layers of real models. Q, INT8 , Cos Sim 76.36% E4M3 78.98% E5M2 (cid:101) 56.40% INT8 FP16 99.99% Relative L1 0.5899 0.4233 0.7921 0. RMSE 0.4311 0.4371 0.5405 0.0091 4.3 QUANTIZATION FOR Q, K, P, Quantization granularity for Q, K: ψQ(Q) and ψK(K) are set with the granularity of per-token, per-block or per-tensor. This is because per-channel quantization is not feasible, since the scale factors of the inner axis of QK cannot be used to do dequantization (Xiao et al., 2023a). 5 Data type of Q, K: We choose INT8 for ψQ(Q) and ψK(K) for two reasons. First, Table 2 shows the average accuracy using different data types (INT8, E4M3, E5M2) for Q, K, , across all layers of Llama2 (7B) (Touvron et al., 2023) and Unidiffuser. It shows that quantizing Q, to INT8 performs higher accuracy than using E4M3 and E5M2. Second, Matmul using INT8 is two times faster than using FP8 in many commonly used GPUs, e.g., RTX4090. (cid:101) Quantization granularity for , : We propose to use ψP ( ) in per-block and ψV (V ) in perchannel for three reasons. First, per-channel quantization for and per-token quantization for (cid:101) are not viable because dequantization requires scale factors of outer axis. Second, = exp(Si rowmax(Si)), where Si is the Matmul result of block of and , the max value in each row (cid:101) of , whose accuracy equals 127 to block (cid:101) per-token quantization. Third, per-channel quantization can address the channel-wised outlier of . is 1. Hence, we can assign single static scale = 1 (cid:101) (cid:101) (cid:101) , : We choose INT8 for ψP ( (cid:101) Data type of ) and ψV (V ) because Matmul using INT8 is two times faster than using FP8 in some commonly used GPUs, and although the accuracy using ψP ( ) and ψV (V ) in INT8 is worse than E4M3 and E5M2, the average accuracy is similar (See Table 2). (cid:101) (cid:101) Accuracy metrics. We use three metrics to assess the accuracy of quantized attention output compared to attention output in full-precision O: First, we flatten and into vectors in the shape of 1 , RMSE= n. Then, Cosine Sim= (1/n) O2, Relative L1= OO/ O)2. / O2 (O (cid:80) (cid:112)(cid:80) (cid:112)(cid:80) (cid:80) (cid:80) (cid:112) (cid:80) Table 4: Average accuracy using different accumulators across all layers of real models. RMSE 2.94e-3 2.94e-3 Accum. Cos Sim 99.98% 99.98% Relative L1 0.0156 0.0156 FP32 FP16 Table 5: Worst accuracy using different accumulators across all layers of real models. Accum. Cos Sim 99.84% 99.84% FP32 FP16 Relative L1 0.0511 0.0511 RMSE 4.229e-3 4.229e-3 4.4 FP16 ACCUMULATOR: MUCH MORE ACCURATE AND EFFICIENT SOLUTION The above solution for ψP ( ) and ψV (V ) has one problem, that is, the accuracy using INT8 is very poor in some model layers. Table 3 shows the worst accuracy using different data types for ) and Q, K, ψV (V ) bring an unacceptable error. , across all layers of Llama2 and Unidiffuser. It shows that INT8 ψP ( (cid:101) (cid:101) In response, we propose very accurate and also efficient solution. Specifically, we propose to use with an FP16 accumulator. FP16 as the data type of Matmul (cid:101) (cid:101) The benefit of such solution is obvious. First, in the context of some commonly used GPUs, e.g., RTX4090 and 3090, the speed of Matmul in FP16 with an FP16 accumulator is 2x faster than that with an FP32 accumulator. Moreover, using FP16 accumulators can save more register resources than using FP32 accumulators, accelerating the computation speed. Second, Table 3 shows that using FP16 for , is much more accurate than using all the other 8-bit data types. Moreover, using FP16 accumulators incurs no accuracy loss than using FP32 accumulators. Specifically, Table 4 and 5 show the average and worst accuracy using FP16 or FP32 accumulators on all layers of Llama2 and Unidiffuser, showing that there is no accuracy loss of using the FP16 accumulator. (cid:101) Table 6: Four kernel implementations of SageAttention. Kernel SAGEAttn-T SAGEAttn-B (Algorithm 1) SAGEAttn-vT (Figure 5(a)) SAGEAttn-vB ψP (P ) ψQ(Q), ψK(K) per-token, INT8 FP16, FP16 Accumulator FP16, FP16 Accumulator per-block, INT8 FP16, FP16 Accumulator FP16, FP16 Accumulator per-token, INT8 per-block, INT8 per-channel, INT8 per-channel, INT8 per-block, INT8 per-block, INT8 ψV (V ) Figure 5: Workflow of SageAttention. Algorithm 1: Implementation of SAGEAttn-B. Input: Matrices Q(FP16), K(FP16), (FP16) RN d, block size bq, bkv. Preprocessing: = mean(K) ; // Subtracting the mean value across tokens Quantization: (δQ, ˆQ) = ψQ(Q/ Divide ˆQ into Tm = N/bq blocks { ˆQi}, and divide ˆK, into Tn = N/bkv blocks { ˆKi} and {Vi}; for in [1, Tm] do ; // Outer loop is paralleled in SMs (stream processors) d), (δK , ˆK) = ψK (K) ; // INT8 per-block quant Load ˆQi and δQ[i] into SM ; for in [1, Tn] do ) δQ[i] δK [j]; Load ˆKj, Vj, and δK [j] into the SM ; = Matmul( ˆQi, ˆK Sj mj = max(mj1 = diag(emj1 Oj )OTn ; , rowmax(Sj )1Oj1 mj Oi = diag(lTn Write Oi ; return = {Oi}; )), (cid:101)P = exp(Sj + Matmul( (cid:101)P ), lj = emj1 mj .to(FP16), Vj, Accum type = FP16) ; mj + rowsum( (cid:101)P i ) ; 4.5 ADAPTIVE QUANTIZATION ) and ψV (V ) in INT8 or retaining Based on the discussion in Section 4.3 and 4.4, we implement four attention kernels (See Table 6) based on two sets of choices: (1) Using ψQ(Q) and ψK(K) in per-token or per-block. (2) Using , in FP16 with an FP16 accumulator. The speed of these ψP ( kernels is in the order (SAGEAttn-vB > SAGEAttn-vT > SAGEAttn-B > SAGEAttn-T), but the accuracy order is opposite. Generally, SAGEAttn-B is accurate enough for all models (see Table 1) and can achieve 2x speedup (See Figure 6 and 7). However, SAGEAttn-vB and SAGEAttn-vT are also accurate for some layers in model. Therefore, we use various inputs to test the cosine similarity of SAGEAttn-B and SAGEAttn-T for each layer of model. Then, we will select SAGEAttn-vB or SAGEAttn-vT if their cosine similarity is bigger than 99.8% (the worst similarity of SAGEAttn-B). (cid:101) (cid:101) 4.6 FUSION TRICKS AND PERFORMANCE ANALYSIS Fusion Tricks. To reduce the overhead of quantization, we fuse the quantization process with the operator preceding the attention layer. For instance, we fuse quantization within the ROPE (Rotary Position Embedding) (Su et al., 2021) layer. Specifically, before the ROPE result (A) is written from shared memory into global memory, we perform δA, ˆA = ψ(A). Subsequently, the δA, ˆA are written into global memory. Additionally, we also fuse the coefficient (1/d) of QK into the quantization process rather than leaving it in the attention layer. Specifically, we multiply by (1/d) on chip before quantizating Q. Performance Analysis. We will take SAGEAttn-B as an example to discuss the acceleration effects on actual hardware: (1) Matmul acceleration. Utilizing INT8 matrix multiplication units on current mainstream hardware can achieve 2-4 throughput. While FP16 accumulators do not offer 7 throughput improvements on most compute cards, on-edge accelerators, such as the RTX4090, can still achieve 2x improvement over FP32 accumulators. (2) Quantization overhead. Quantization and dequantization are considered the main overhead in current quantization methods (Lin et al., 2024). The computational overhead can not be avoided, but through fusing the quantization of Q, with ROPE, we avoid the IO overhead of quantization. (3) Cache and registers. Currently, mainstream accelerators need to store data in cache (such as SharedMemory) during computation. Using 8-bit data for calculations can reduce the usage of the general cache, and using fp16 accumulators can also reduce the usage of accumulation registers. (4) Dram access. Using 8-bit data can halve the tensors transfer overhead from DRAM to the compute units. Although quantization introduces additional FP32 scales, these scales can be considered negligible compared to the tensors."
        },
        {
            "title": "5 EXPERIMENTS",
            "content": "Main results. The speed of SageAttention is approximately 2.1 2. Furthermore, SageAttention achieves an average real speedup of 2.83 original attention in various models, with negligible loss in end-to-end metrics. faster than FlashAttentioncompared to the 5.1 EXPERIMENTAL SETUP Models. We validate the effectiveness of SageAttention across diverse set of representative models from the fields of language, image, and video generation. Specifically, we conduct experiments on five models: Llama2 (7B) (Touvron et al., 2023) for text2text, CogvideoX (Yang et al., 2024) for text2video, Unidiffuser (Bao et al., 2023) and UltraPixel (Ren et al., 2024) for text2image, TIMM (Wightman, 2019) for image classification. Datasets. Llama2 is evaluated on three zero-shot tasks: WikiText (Merity et al., 2022) to assess the models prediction confidence, LAMBADA (Paperno et al., 2016) evaluate contextual understanding, and MMLU (Hendrycks et al., 2020) for measuring knowledge across various subjects. CogvideoX is evaluated using the open-sora (Zheng et al., 2024) prompt sets. Both UltraPixel and Unidiffuser are assessed on the COCO annotations (Lin et al., 2014), featuring (prompt, image) pairs. TIMM is evaluated on on three image datasets: ImageNet (Deng et al., 2009), ImageNetSketch (Sketch) (Wang et al., 2019), and ImageNet-Rendition (ImageNet-r) (Hendrycks et al., 2021). Metrics. For Llama2, we use perplexity (ppl.) (Jelinek et al., 1977) for WikiText, and Accuracy (Acc.) for LAMBADA and MMLU. For CogvideoX, folowing (Zhao et al., 2024), we evaluate the quality of generated videos on five metrics: CLIPSIM and CLIP-Temp (CLIP-T) (Liu et al., 2024) to measure the text-video alignment; (VQA-a) and (VQA-t) to assess the video aesthetic and technical quality, respectively; and Flow-score (FScore) for temporal consistency (Wu et al., 2023). For UltraPixel and Unidiffuser, generated images are compared with the images in COCO annotations dataset in three aspects: FID (Heusel et al., 2017) and sFID (Salimans et al., 2016) for fidelity evaluation, Clipscore (CLIP) (Hessel et al., 2021) for text-image alignment, and ImageReward (IR) (Xu et al., 2024) for human preference. For TIMM, we use Accuracy. Figure 6: Speed comparison between SageAttention and baselines (RTX4090, headdim=64). 5.2 SPEED AND ACCURACY OF ATTENTION KERNELS Speed. We conduct experiments to compare the Speed of SageAttention against baselines using configurations with headdim=64 or headdim=128, both with and without Causal Mask Vaswani (2017). Specifically, Figure 6 and Figure 7 show the Speed of SageAttention and baselines across varying sequence lengths on RTX4090. These results indicate that SageAttention Figure 7: Speed comparison between SageAttention and baselines (RTX4090, headdim=128). Figure 8: Speed comparison between SageAttention and baselines (RTX3090, headdim=64). Figure 9: Speed comparison between SageAttention and baselines (RTX3090, headdim=128). achieves peak of 341 TOPS and is 2x faster than FlashAttention2 and 2.9x faster than xformers on average. Figure 8 and Figure 9 illustrate the results on RTX3090, showing similar speedup performance. Accuracy. Table 9 shows the numerical error of four implementations of SageAttention compared with attention in full-precision. This experiment is conducted using set of (Q, K, V) conforming to normal distribution. We find that the error of the four implementations is rather small. Notably, SAGEAttn-T and SAGEAttn-B achieve 100% cosine similarity and RMSE in e-4 level. Table 7: Real speedup (TOPS) of SageAttention on RTX4090. Model CogvideoX Llama2 UltraPixel Unidiffuser TIMM Shape of Q, K, Original attention 163.37 (FlashAttn2) (2, 30, 17776, 64) 130.99 (FlashAttn2) (4, 32, 1536, 128) 152.03 (FlashAttn2) (2, 32, 7285, 64) 105.68 (xformers) (4, 24, 1105, 64) 18.910 (Torch) (12, 64, 197, 64) SageAttention 327.57 231.74 325.18 246.93 111.41 Speedup 2.01x 1.77x 2.14x 2.34x 5.89x 5.3 END-TO-END PERFORMANCE Speedup. We measure the real speed of SageAttention and the original attention on Unidiffuser, UltraPixel, CogvideoX, Llama2 and TIMM on RTX4090. Table 7 shows that SageAttention outperforms original attention across all models. Specifically, SageAttention yields 2.83x speedup compared to the original attentions on average. Metrics loss. We assessed the end-to-end metrics of various models using SageAttention compared to using attention in full-precision. Detailed evaluation results are presented in Table 8 for Llama2, CogvideoX, Unidiffuser, UltraPixel, and TIMM, respectively. The results indicate that SageAttention successfully matches the performance of attention in full-precision across all models. Specifically, on Llama2, CogvideoX, UltraPixel, and Unidiffuser, 9 Table 8: End-to-end metrics loss across text, image, and video generation models. Model Llama2 attention Full-Precision SageAttention WikiText (Ppl.) 5.823 5.824 Lambda (Acc.) 0.886 0.887 MMLU (Acc.) 0.46 0.46 Model CogvideoX attention Full-Precision SageAttention CLIPSIM 0.1837 0.1836 CLIP-T 0.9976 0. VQA-a 68.962 68.839 VQA-t 75.925 75.037 FScore 3.7684 3.8339 Model attention Unidiffuser Full-Precision SageAttention UltraPixel Full-Precision SageAttention FID 163.33 166.49 179.78 179.79 sFID 145.08 143.18 141.35 141.63 CLIP 0.3152 0.3154 0.3132 0.3131 IR 0.1609 0.1521 0.6169 0.6110 Model TIMM attention Full-Precision SageAttention ImageNet (Acc.) 84.79% 84.74% Sketch (Acc.) 45.32% 45.78% ImageNet-r (Acc.) 59.55% 60.32% SageAttention resulted in only minor average degradation of 0.2% compared to attention in full-precision. Moreover, on TIMM, SageAttention even surpasses attention in full-precision. Table 9: Accuracy of SageAttention kernels. attention SAGEAttn-T SAGEAttn-B SAGEAttn-vT SAGEAttn-vB Relative L1 0.019 0.021 0.064 0.138 Cos Sim 1.0 1.0 99.9% 98.9% RMSE 6.8e-4 7.3e-4 0.065 0.067 Table 10: Overhead of smoothing K. Model CogvideoX UltraPixel Smooth TOPS 327.57 327.52 325.18 324.56 Table 11: Benefit of adaptive quantization. attention SAGEAttn-T SageAttention model CogvideoX CLIPSIM 0.1827 0.1835 TOPS 292.17 327.57 Model Llama MMLU 0.46 0.46 TOPS 208.59 231.74 5.4 ABLATION STUDY Overhead of smoothing K. Table 10 presents the overhead associated with smoothing on the attention speed in real models. The results indicate minimal reduction, less than 0.2%. Benefit of adaptive quantization. We analyzed the performance differences between using only SAGEAttn-T and employing an adaptive strategy (SageAttention). Table 11 presents the metrics and average speed of attention on CogvideoX and Llama2. The results indicate that the adaptive strategy increases the speed of attention by 11.7% without any loss in metrics."
        },
        {
            "title": "6 CONCLUSION AND FUTURE WORK",
            "content": "We introduce SageAttention, an efficient and precise INT8 quantization method for attention. First, we propose method to smooth matrix K, enhancing the accuracy with under 0.2% speed overhead. Second, we use FP16 accumulators in the Matmul of (P, V) to boost both accuracy and speed. Third, we use adaptive quantization to further improve OPS by 12% without sacrificing accuracy. Our method surpasses FlashAttention2 and xformers by approximately 2.1x and 2.7x, respectively. Extensive testing confirms that our approach maintains end-to-end metrics across various models, including language, image, and video generation models. Future Work. We leave the implementation in Hopper architecture for future work."
        },
        {
            "title": "REFERENCES",
            "content": "Fan Bao, Shen Nie, Kaiwen Xue, Yue Cao, Chongxuan Li, Hang Su, and Jun Zhu. All are worth words: vit backbone for diffusion models. In CVPR, 2023. Ganesh Bikshandi and Jay Shah. case study in cuda kernel fusion: Implementing flashattention-2 on nvidia hopper architecture using the cutlass library. arXiv preprint arXiv:2312.11918, 2023. Yukang Chen, Shengju Qian, Haotian Tang, Xin Lai, Zhijian Liu, Song Han, and Jiaya Jia. Longlora: Efficient fine-tuning of long-context large language models. arXiv preprint arXiv:2309.12307, 2023. Krzysztof Choromanski, Valerii Likhosherstov, David Dohan, Xingyou Song, Andreea Gane, Tamas Sarlos, Peter Hawkins, Jared Davis, Afroz Mohiuddin, Lukasz Kaiser, et al. Rethinking attention with performers. arXiv preprint arXiv:2009.14794, 2020. Xiangxiang Chu, Zhi Tian, Yuqing Wang, Bo Zhang, Haibing Ren, Xiaolin Wei, Huaxia Xia, and Chunhua Shen. Twins: Revisiting the design of spatial attention in vision transformers. Advances in neural information processing systems, 34:93559366, 2021. Tri Dao. Flashattention-2: Faster attention with better parallelism and work partitioning. arXiv preprint arXiv:2307.08691, 2023. Tri Dao, Dan Fu, Stefano Ermon, Atri Rudra, and Christopher Re. Flashattention: Fast and memoryefficient exact attention with io-awareness. Advances in Neural Information Processing Systems, 35:1634416359, 2022. Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: large-scale hierarchical image database. In 2009 IEEE conference on computer vision and pattern recognition, pp. 248255. Ieee, 2009. Abhimanyu Dubey, Abhinav Jauhri, Abhinav Pandey, Abhishek Kadian, Ahmad Al-Dahle, Aiesha Letman, Akhil Mathur, Alan Schelten, Amy Yang, Angela Fan, et al. The llama 3 herd of models. arXiv preprint arXiv:2407.21783, 2024. Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring massive multitask language understanding. 2020. Dan Hendrycks, Steven Basart, Norman Mu, Saurav Kadavath, Frank Wang, Evan Dorundo, Rahul Desai, Tyler Zhu, Samyak Parajuli, Mike Guo, Dawn Song, Jacob Steinhardt, and Justin Gilmer. The many faces of robustness: critical analysis of out-of-distribution generalization. ICCV, 2021. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 75147528, 2021. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Benoit Jacob, Skirmantas Kligys, Bo Chen, Menglong Zhu, Matthew Tang, Andrew Howard, Hartwig Adam, and Dmitry Kalenichenko. Quantization and training of neural networks for efficient integer-arithmetic-only inference. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 27042713, 2018. Fred Jelinek, Robert Mercer, Lalit Bahl, and James Baker. Perplexitya measure of the difficulty of speech recognition tasks. The Journal of the Acoustical Society of America, 62(S1): S63S63, 1977. Huiqiang Jiang, Yucheng Li, Chengruidong Zhang, Qianhui Wu, Xufang Luo, Surin Ahn, Zhenhua Han, Amir Abdi, Dongsheng Li, Chin-Yew Lin, et al. Minference 1.0: Accelerating pre-filling for long-context llms via dynamic sparse attention. arXiv preprint arXiv:2407.02490, 2024. 11 Angelos Katharopoulos, Apoorv Vyas, Nikolaos Pappas, and Francois Fleuret. Transformers are rnns: Fast autoregressive transformers with linear attention. In International conference on machine learning, pp. 51565165. PMLR, 2020. Sehoon Kim, Amir Gholami, Zhewei Yao, Michael Mahoney, and Kurt Keutzer. I-bert: Integeronly bert quantization. In International conference on machine learning, pp. 55065518. PMLR, 2021. Benjamin Lefaudeux, Francisco Massa, Diana Liskovich, Wenhan Xiong, Vittorio Caggiano, Sean Naren, Min Xu, Jieru Hu, Marta Tintore, Susan Zhang, Patrick Labatut, Daniel Haziza, Luca Wehrstedt, Jeremy Reizenstein, and Grigory Sizov. xformers: modular and hackable transhttps://github.com/facebookresearch/xformers, former modelling library. 2022. Kunchang Li, Yali Wang, Gao Peng, Guanglu Song, Yu Liu, Hongsheng Li, and Yu Qiao. Uniformer: Unified transformer for efficient spatial-temporal representation learning. In International Conference on Learning Representations. Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr In Computer Dollar, and Lawrence Zitnick. Microsoft coco: Common objects in context. VisionECCV 2014: 13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part 13, pp. 740755. Springer, 2014. Yujun Lin, Haotian Tang, Shang Yang, Zhekai Zhang, Guangxuan Xiao, Chuang Gan, and Song Han. Qserve: W4a8kv4 quantization and system co-design for efficient llm serving, 2024. URL https://arxiv.org/abs/2405.04532. Hao Liu, Matei Zaharia, and Pieter Abbeel. Ringattention with blockwise transformers for nearinfinite context. In The Twelfth International Conference on Learning Representations. Yaofang Liu, Xiaodong Cun, Xuebo Liu, Xintao Wang, Yong Zhang, Haoxin Chen, Yang Liu, Tieyong Zeng, Raymond Chan, and Ying Shan. Evalcrafter: Benchmarking and evaluating large video generation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2213922149, 2024. Ze Liu, Yutong Lin, Yue Cao, Han Hu, Yixuan Wei, Zheng Zhang, Stephen Lin, and Baining Guo. Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision, pp. 1001210022, 2021. Stephen Merity, Caiming Xiong, James Bradbury, and Richard Socher. Pointer sentinel mixture models. In International Conference on Learning Representations, 2022. Maxim Milakov and Natalia Gimelshein. Online normalizer calculation for softmax. arXiv preprint arXiv:1805.02867, 2018. Denis Paperno, German Kruszewski, Angeliki Lazaridou, Ngoc-Quan Pham, Raffaella Bernardi, Sandro Pezzelle, Marco Baroni, Gemma Boleda, and Raquel Fernandez. The lambada dataset: Word prediction requiring broad discourse context. In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 15251534, 2016. Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, et al. Fp8-lm: Training fp8 large language models. arXiv preprint arXiv:2310.18313, 2023. Jingjing Ren, Wenbo Li, Haoyu Chen, Renjing Pei, Bin Shao, Yong Guo, Long Peng, Fenglong Song, and Lei Zhu. Ultrapixel: Advancing ultra-high-resolution image synthesis to new peaks. arXiv preprint arXiv:2407.02158, 2024. Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki Cheung, Alec Radford, and Xi Chen. Improved techniques for training gans. Advances in neural information processing systems, 29, 2016. 12 Jay Shah, Ganesh Bikshandi, Ying Zhang, Vijay Thakkar, Pradeep Ramani, and Tri Dao. Flashattention-3: Fast and accurate attention with asynchrony and low-precision. arXiv preprint arXiv:2407.08608, 2024. Jianlin Su, Yu Lu, Shengfeng Pan, Ahmed Murtadha, Bo Wen, and Yunfeng Liu. Roformer: Enhanced transformer with rotary position embedding. arXiv preprint arXiv:2104.09864, 2021. Xiao Sun, Jungwook Choi, Chia-Yu Chen, Naigang Wang, Swagath Venkataramani, Vijayalakshmi Viji Srinivasan, Xiaodong Cui, Wei Zhang, and Kailash Gopalakrishnan. Hybrid 8-bit floating point (hfp8) training and inference for deep neural networks. Advances in neural information processing systems, 32, 2019. Philippe Tillet, H. T. Kung, and David Cox. Triton: an intermediate language and compiler for tiled neural network computations. MAPL 2019, pp. 1019, New York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450367196. Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov, Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models. arXiv preprint arXiv:2307.09288, 2023. Vaswani. Attention is all you need. Advances in Neural Information Processing Systems, 2017. Shashanka Venkataramanan, Amir Ghodrati, Yuki Asano, Fatih Porikli, and Amirhossein Habibian. Skip-attention: Improving vision transformers by paying less attention. arXiv preprint arXiv:2301.02240, 2023. Haohan Wang, Songwei Ge, Zachary Lipton, and Eric Xing. Learning robust global representations by penalizing local predictive power. Advances in Neural Information Processing Systems, 32, 2019. Sinong Wang, Belinda Li, Madian Khabsa, Han Fang, and Hao Ma. Linformer: Self-attention with linear complexity. arXiv preprint arXiv:2006.04768, 2020. Ross Wightman. Pytorch image models. https://github.com/rwightman/ pytorch-image-models, 2019. Haoning Wu, Erli Zhang, Liang Liao, Chaofeng Chen, Jingwen Hou, Annan Wang, Wenxiu Sun, Qiong Yan, and Weisi Lin. Exploring video quality assessment on user generated contents from aesthetic and technical perspectives. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 2014420154, 2023. Haocheng Xi, Yuxiang Chen, Kang Zhao, KAI JUN TEH, Jianfei Chen, and Jun Zhu. Jetfire: Efficient and accurate transformer pretraining with int8 data flow and per-block quantization. In Forty-first International Conference on Machine Learning, 2024. Chaojun Xiao, Pengle Zhang, Xu Han, Guangxuan Xiao, Yankai Lin, Zhengyan Zhang, Zhiyuan Liu, and Maosong Sun. Infllm: Training-free long-context extrapolation for llms with an efficient context memory. In First Workshop on Long-Context Foundation Models@ ICML 2024, 2024. Guangxuan Xiao, Ji Lin, Mickael Seznec, Hao Wu, Julien Demouth, and Song Han. Smoothquant: In International Accurate and efficient post-training quantization for large language models. Conference on Machine Learning, pp. 3808738099. PMLR, 2023a. Guangxuan Xiao, Yuandong Tian, Beidi Chen, Song Han, and Mike Lewis. Efficient streaming language models with attention sinks. arXiv preprint arXiv:2309.17453, 2023b. Jiazheng Xu, Xiao Liu, Yuchen Wu, Yuxuan Tong, Qinkai Li, Ming Ding, Jie Tang, and Yuxiao Dong. Imagereward: Learning and evaluating human preferences for text-to-image generation. Advances in Neural Information Processing Systems, 36, 2024. Zhuoyi Yang, Jiayan Teng, Wendi Zheng, Ming Ding, Shiyu Huang, Jiazheng Xu, Yuanming Yang, Wenyi Hong, Xiaohan Zhang, Guanyu Feng, et al. Cogvideox: Text-to-video diffusion models with an expert transformer. arXiv preprint arXiv:2408.06072, 2024. 13 Weihao Yu, Mi Luo, Pan Zhou, Chenyang Si, Yichen Zhou, Xinchao Wang, Jiashi Feng, and Shuicheng Yan. Metaformer is actually what you need for vision. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 1081910829, 2022. Tianchen Zhao, Tongcheng Fang, Enshu Liu, Wan Rui, Widyadewi Soedarmadji, Shiyao Li, Zinan Lin, Guohao Dai, Shengen Yan, Huazhong Yang, Xuefei Ning, and Yu Wang. Vidit-q: Efficient and accurate quantization of diffusion transformers for image and video generation, 2024. Zangwei Zheng, Xiangyu Peng, Tianji Yang, Chenhui Shen, Shenggui Li, Hongxin Liu, Yukun Zhou, Tianyi Li, and Yang You. Open-sora: Democratizing efficient video production for all, March 2024. URL https://github.com/hpcaitech/Open-Sora."
        }
    ],
    "affiliations": [
        "Tsinghua University"
    ]
}