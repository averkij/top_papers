{
    "paper_title": "DeepCritic: Deliberate Critique with Large Language Models",
    "authors": [
        "Wenkai Yang",
        "Jingwen Chen",
        "Yankai Lin",
        "Ji-Rong Wen"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is a promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose a novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K long-form critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 1 ] . [ 1 2 6 6 0 0 . 5 0 5 2 : r DeepCritic: Deliberate Critique with Large Language Models Wenkai Yang1, Jingwen Chen2, Yankai Lin1, Ji-Rong Wen1 1Gaoling School of Artificial Intelligence, Renmin University of China 2School of Computer Science and Technology, Beijing Jiaotong University {wenkaiyang, yankailin}@ruc.edu.cn"
        },
        {
            "title": "Abstract",
            "content": "As Large Language Models (LLMs) are rapidly evolving, providing accurate feedback and scalable oversight on their outputs becomes an urgent and critical problem. Leveraging LLMs as critique models to achieve automated supervision is promising solution. In this work, we focus on studying and enhancing the math critique ability of LLMs. Current LLM critics provide critiques that are too shallow and superficial on each step, leading to low judgment accuracy and struggling to offer sufficient feedback for the LLM generator to correct mistakes. To tackle this issue, we propose novel and effective two-stage framework to develop LLM critics that are capable of deliberately critiquing on each reasoning step of math solutions. In the first stage, we utilize Qwen2.5-72B-Instruct to generate 4.5K longform critiques as seed data for supervised fine-tuning. Each seed critique consists of deliberate step-wise critiques that includes multi-perspective verifications as well as in-depth critiques of initial critiques for each reasoning step. Then, we perform reinforcement learning on the fine-tuned model with either existing human-labeled data from PRM800K or our automatically annotated data obtained via Monte Carlo sampling-based correctness estimation, to further incentivize its critique ability. Our developed critique model built on Qwen2.5-7B-Instruct not only significantly outperforms existing LLM critics (including the same-sized DeepSeek-R1-distill models and GPT-4o) on various error identification benchmarks, but also more effectively helps the LLM generator refine erroneous steps through more detailed feedback."
        },
        {
            "title": "Introduction",
            "content": "Large Language Models (LLMs) [29, 32, 23] have demonstrated superior performance that even surpasses human capabilities across wide range of tasks [16, 25, 30]. LLMs achieve strong and generalizable performance by training on human-provided knowledge data [31]. This makes their evolution highly dependent on the effective human supervision [1]. However, as LLMs become increasingly intelligent, providing effective and scalable human supervision will become highly challenging, as collecting human feedback would be too costly and difficult [35]. LLM critics [27], which leverage LLMs as critique models, have recently emerged as promising approach to enabling scalable oversight [35] on evolving LLMs. LLM critics generate critiques of LLM-generated content, which identify flaws and errors to help the LLM generator refine its outputs, paving the way for the automatic supervision and continuously improvement of LLMs [26, 17, 7, 38]. The work was done while Jingwen Chen was at internship in Renmin University of China. Corresponding author. 3Data and models are available at https://github.com/RUCBM/DeepCritic. Work in progress. Figure 1: Comparison of critiques generated by current LLM critics and our developed critic. The red highlights in the solution steps represent the erroneous part. The critiques of current LLM critics (e.g., Qwen2.5-7B-Instruct) are overly superficial, primarily consisting of declarative statements rather than in-depth analysis or critical evaluation. In contrast, our critic can generate deliberate reasoning process before making judgment, incorporating iterative evaluation, multi-perspective verification, and meta-critiquing. The effectiveness and the potential of the utilizing LLM critics heavily depends on the inherent critique capabilities of the LLM critics. However, existing studies have shown that current LLM critics still exhibit limited critique capabilities in complex domains such as mathematical reasoning tasks [53, 54], making it difficult to provide accurate and reliable feedback. By analyzing the critiques generated by existing LLM critics on math problems and solutions, we find that their generated critiques are often overly superficial and lack critical thinking, as shown in the examples in Figure 1. In particular, their critiques merely follow and echo the same reasoning process of the original steps, re-iterating rather than challenging them with their own critical reasoning, leading to premature judgment result. This behavior can lead to two problems: (1) The critiques of reasoning steps lack careful deliberation, leading to low accuracy in the judgment results. (2) The critiques lack informativeness, offering limited guidance for the LLM generator to perform targeted refinements. In this work, we aim to address the aforementioned limitations of shallow critiques generated by the LLM critics, particularly in the domain of mathematical reasoning. Specifically, we propose the DeepCritic framework, which enhances the critique capabilities of LLMs through two-stage training pipeline. In the first stage, to enable LLMs to acquire preliminary capability for generating fine-grained critiques, we first curate dataset of 4.5K long-form critiques by iteratively prompting Qwen2.5-72B-Instruct [32] to critique on small subset of labeled solutions in PRM800K [21]. Each above critique includes step-wise critiques of all reasoning steps if the solution is correct, or up to the first erroneous step otherwise. When constructing each step-wise critique, we first generate an initial and preliminary critique of the specified reasoning step. Then, in order to enable our critic to conduct critiques more critically and from more diverse perspectives, we further generate an in-depth critique of the initial critique. The in-depth critique is supposed to validate the step from alternative perspectives and critically examine the initial critique itself. Finally, we merge the initial 2 and in-depth critique into one deliberate critique for the specified step. By supervised fine-tuning on the curated and filtered critique data, we obtain an initial critique model that is capable of performing multi-perspective evaluation and meta-critiquing through reflection on and correction of its prior erroneous judgments. Then, in the second stage, we perform reinforcement learning (RL) to the SFT model to further boost its deep critique ability. We perform RL under two different settings based on the source of RL data: (1) When human-labeled data is available, such as PRM800K, we directly use it for RL; (2) Otherwise, we automatically generate annotated RL data through Monte Carlo sampling-based correctness estimation on each reasoning step [40] and then perform RL. Experimental results show that our developed deep critic significantly outperforms existing process reward models (PRMs) and LLM critics (including the advanced R1-distilled reasoning models [6] and GPT-4o [29]) on various error identification benchmarks, demonstrating the effectiveness of our pipeline in enabling LLM critics to provide more accurate supervision. Furthermore, we demonstrate promising test-time scaling properties for both our deep critic and the LLM generator within our framework: (1) the judgment accuracy of the critic consistently improves with increased test-time sampling effort; and (2) the performance of the LLM generator is effectively enhanced either by employing our deep critic as verifier in majority voting or through refinement guided by the critics more informative feedback."
        },
        {
            "title": "2 Related Work",
            "content": "Critique Ability of LLMs In the current era of rapidly evolving LLMs, leveraging and improving the critique ability of LLMs to facilitate effective scalable oversight [2, 35] and superalignment [3, 49] has become increasingly important. Regarding the utilization of the critique ability of LLMs, LLM-as-aJudge [10] and LLM-as-a-Critic [27] serve as promising directions for facilitating automatic supervision of LLM generations [55], enabling the self-evolution of LLMs [52, 42], and refining LLM outputs at test time [26]. Benchmarking the critique ability of LLMs [24, 19, 22] paves the way to better understanding the potential and limitations of current LLMs on critique tasks. Finally, series of studies aim to create more powerful critique models on mathematical reasoning [46, 56, 7, 44, 38], code generation [27, 45] or other open-domain tasks [17]. This work analyzes the limitations of current math critique models and proposes novel and effective pipeline to enhance the math critique ability of LLMs. Reasoning Ability of LLMs The reasoning abilities of LLMs has long been topic of widespread interest in the community. Previous studies explore LLM reasoning in various domains, such as code [34, 14], math [23, 48], commonsense knowledge [33, 9], etc. This work mainly focuses on the math reasoning domain. The studies in this line can be divided into several categories: (1) Designing diverse and challenging math reasoning benchmarks to probe the boundaries of existing LLMs reasoning abilities [4, 12, 8]. (2) Enhancing the math reasoning abilities of LLMs in the training time either by collecting high-quality math datasets for fine-tuning [51, 20], or by proposing advanced optimization algorithms [18, 5]. (3) Improving reasoning accuracy by increasing the test-time compute either by performing search-based sampling [41, 43] with process reward models (PRMs) [21, 40], or by extending the Chain of Thought (CoT) length [30, 6, 50]. This work, taking pioneer step to have the critique model provide judgments after detailed and deliberate reasoning, can also improve LLMs math reasoning by providing accurate and detailed feedback on erroneous solutions and assisting LLMs in correcting them."
        },
        {
            "title": "3 Methodology",
            "content": "3.1 Problem Formulation Here, we introduce the preliminaries of our studied critique problem setting. Let = {(P, S)} denote dataset comprising pairs of problems and their corresponding solutions S. Each solution is in step-by-step format denoted as = (s1, s2, , sn), where si represents the i-th step. Denote θc as an LLM critic whose role is to review each step in sequentially, identify in which step the first error occurs, and return the step index of that first erroneous step. If all steps are deemed correct, it returns -1 to indicate that the entire solution is correct. Formally, the output of the critic can be formulated as (c1, j1, , ck, jk, a) πθc(P, s1, , sn), (1) 3 Figure 2: The two-stage pipeline of training our deep critique models. In Stage 1, we first utilize Qwen2.5-72B-Instruct to generate an initial step-wise critique for each step in the solution, followed by an in-depth critique of the initial critique. Then, we use Qwen2.5-72B-Instruct to merge these two critiques into one deliberate critique in the long-CoT form. Finally, we perform SFT on above created critique data to teach the model the format of deliberately critiquing. In Stage 2, we perform RL to the SFT model on either existing human-annotated data or auto-labeled data via Monte Carlo sampling-based correctness estimation, to further stimulate the critique ability of the critic. where ci represents the CoT critique of step si, ji {1, 1} represents the judgment result (i.e., 1 for correct or -1 for incorrect) indicating the correctness of si. The final judgment result is equal to if jk = 1, indicating the first incorrect step; otherwise, if all ji = 1 for = n, then = 1. As mentioned before (refer to Figure 1), current LLM-based critique models exhibit limitations in conducting thoughtful critiques, as their step-wise critiques ci tend to be overly brief and superficial, often echoing the original reasoning content without deeper and critical analysis. Thus, their critique performance is greatly limited as shown in previous studies [53, 54], and the shallow, uninformative critiques fail to provide useful guidance for the LLM generator to revise its solutions. In this work, we aim to improve the LLMs critique ability and enable it to produce more deliberate and thoughtful critiques ci before making the judgment result ji, enhancing both the accuracy and quality of its generated critiques. 3.2 DeepCritic: Deliberate Critique Model In this section, we will introduce our two-stage pipeline to create deliberate critique models in detail, including the SFT data generation and training stage in Section 3.2.1, and the RL data curation and optimization stage in Section 3.2.2. 3.2.1 Teaching LLMs to Deliberately Critique Given that existing LLM critics struggle to produce well-reasoned and in-depth critiques, our first stage aims to teach LLMs how to deliberately critique. In this stage, we first leverage Qwen2.5-72BInstruct [32] to iteratively perform initial and in-depth critiquing, and then merge the two critiques into single long-form critique to serve as the seed critique. We subsequently perform SFT on the curated seed critique data to teach the target model the format and structure of deliberate critiquing. The brief illustration is displayed in the left part of Figure 2, and the detailed procedure is described below. Initial Critique Generation First, we sample small set of labeled data from PRM800K [21] as the seed task inputs. Each task input contains problem and step-by-step solution = (s1, , sn), 4 Table 1: Statistics of step-level critiques in our SFT dataset, categorized by the correctness of their corresponding initial critiques."
        },
        {
            "title": "Label of Reasoning Step",
            "content": "# Correct Initial Critiques # Incorrect Initial Critiques 1 -1 22968 3535 738 565 along with the human-labeled label li {1, 1}4 indicating the correctness of each reasoning step si. Thus, we can create the step-by-step critiques on these seed inputs using an LLM θ , which is chosen as Qwen2.5-72B-Instruct in this work. However, instead of creating the step-by-step critique of the entire solution directly in single pass just like Eq. (1), which often leads current LLMs to generate overly brief critiques for each step as mentioned before, we adopt an approach that critiques each step independently. Specifically, in each prompting of Qwen2.5-72B-Instruct, we provide the problem and entire solution as inputs, but instruct Qwen2.5-72B-Instruct to critique only one specified step: (cinit , jinit ) = πθ (P, s1, , sn, starget = si), = 1, , k, (2) where starget is the additional requirement that specifies the i-th step to be critiqued only, (cinit , jinit ) represents the initial CoT critique and judgment result of the specified step, represents the index of the first step where li = 1, or = when all li is 1. In-Depth Critique Generation After initial critique generation, we find that many of the initial critiques merely adopt direct verification approach that directly follows the logic of the original reasoning steps and perform repetitive or similar calculations, resulting in relatively low accuracy when identifying incorrect steps. To enable our critique model to learn to perform critical evaluations, we introduce second round of in-depth critique generation based on the initial critiques. Specifically, for each reasoning step in the solution, we instruct Qwen2.5-72B-Instruct again to either assess the reasoning step from different perspective or using different evaluation method than that used in the initial critique, or to critique the initial critique itself in order to identify whether there exist flaws in the initial critique that lead to the incorrect judgment about the reasoning step. Therefore, the in-depth critique cdeep and its judgment result jdeep are generated as , jdeep ) = πθ(P, s1, , sn, cinit (cdeep , jinit , starget = si), = 1, , k. (3) This process allows initial critiques that previously led to mismatches between the initial judgment result and the ground truth label (i.e., jinit li) to be revised into correct critiques. Then, we only retain the solutions in which the in-depth judgment results of all steps in the solution align with the ground truth labels (i.e., jdeep = li, = 1, , n), as well as their initial and in-depth critiques. Table 1 shows the proportion of step-level critiques in our final SFT dataset that are successfully corrected through the second-round in-depth critique generation process. We observe that certain number of step-level critiques benefit from the in-depth critique generation process. Incorporating these samples into training equips the model with the capabilities of reflection and self-correction in critiquing. Final Critique Synthesis In the last step of SFT data generation, we use in-context learning with two manually-written examples to instruct Qwen2.5-72B-Instruct to merge the initial and in-depth critiques of each step into single deliberate critique: , jf inal ) = πθ(cinit = 1, , k, (cf inal , {exl}), , jdeep , cdeep , jinit (4) where {exl} are in-context learning examples. Finally, we only need to concatenate all step-level critiques to form complete solution-level critique: = (cf inal 1 , jf inal 1 , , cf inal , jf inal , a), = { 1 if jf inal if jf inal = 1, = 1. (5) Such deliberate critiques enable the model to perform iterative evaluations, multi-perspective verifications, reflection, and meta-critiquing in the inference stage, thereby improving its judgment accuracy. All prompt templates used in the seed data generation stage and the corresponding generation hyper-parameters can be found in Appendix and Appendix B.1 respectively. 4In the original PRM800K dataset, there are some steps labeled with 0, indicating that these steps is not incorrect but do not make any progress. We consider the label for these steps to be 1. 5 Supervised Fine-Tuning In total, we obtain approximately 4.5K seed solution-level critiques, and their label distribution (i.e., the distribution of the step index of the first erroneous step) is shown in Figure 7. We then perform SFT on the target model to teach it the format for performing deliberate critique and obtain an initial critique model θSF : θSF = arg min θ E(P,S,C)DSF [ log Pθ(CP, S)], (6) where DSF is the SFT critique dataset in which the input includes the problem and the solution, and the output is the solution-level deliberate critique. 3.2.2 Incentivizing LLMs to Deliberately Critique Once the seed critique model has acquired certain level of critique capability, in the second stage, we aim to stimulate and elicit its full potential through continued incentivization. We follow the recent exciting advancements in LLM reasoning domain [30, 6, 13] to employ reinforcement learning (RL) on θSF in the second stages training. The acquisition of RL data is critical in RL stage. In the following, we explore RL under two different settings based on the sources of data. (1) First, the ideal source for RL should ne the high-quality labeled data obtained through human annotation. Therefore, in the first setting we directly use the existing human-labeled dataset PRM800K [21] for RL. (2) However, in some cases human annotation may become impractical or even infeasible due to high cost. Thus, in the second setting where human annotation is infeasible, we construct the task data automatically via Monte Carlo samplingbased correctness estimation method [40]. Specifically, we sample portion of GSM8K, MATH and Olympiads problems from NuminaMath-CoT dataset [20], and leverage Qwen2.5-1.5B/3B/7BInstruct [32] to generate multiple step-by-step solutions for each problem. Problems where all solutions are either fully correct or fully incorrect are discarded, as such cases are deemed too easy or too challenging. Then, for each incorrect solution, to measure the correctness of specific step si, we follow Wang et al. [40] to truncate the solution after si, and use an LLM generator (i.e., Qwen2.5-7B-Instruct in this work) to rollout the subsequent reasoning path times independently. We define the first erroneous step as the first step from which, along with all subsequent steps, all rollouts generated by the generator are incorrect; while for its all preceding steps, more than half of rollouts reach the correct answers. If such steps do not exist, we discard those incorrect solutions. For solutions with correct final answers, prior studies [54, 38] have pointed out that their intermediate steps can still be incorrect. Therefore, we perform the same Monte Carlo sampling procedure and assign label of -1 only to those correct solutions whose all intermediate steps have corresponding rollouts where more than half reach the correct answers. The illustration of above data construction procedure is shown in the right part of Figure 2. The detailed data generation settings are put in Appendix B.1. In our experiments, we explore training the seed critique model with RL either using 40.7K PRM800K data or 14.2K automatically constructed data. The label distributions of these two data sources are shown in Figure 8 and Figure 9 respectively."
        },
        {
            "title": "4 Experiments and Analysis",
            "content": "4.1 Experimental Settings Base Model We choose Qwen2.5-7B-Instruct as the initial base model. We first perform SFT to get our seed critique model DeepCritic-7B-SFT. Then, we perform RL on two distinct types of RL data separately, resulting in two variants: DeepCritic-7B-RL-PRM800K and DeepCritic-7B-RLNumina. Benchmarks We select three widely used error identification benchmarks to systematically evaluate the critique and judgment performance of each model, including the subset of MR-GSM8K [53] in which the questions are from original GSM8K [4] dataset, the Phase-2 test set5 of PRM800K [21], and ProcessBench [54]. Each testing example in all datasets contains problem, step-by-step solution and label that either represents the step index of the first erroneous step or is -1 if the solution is entirely correct. The detailed description of the three benchmarks is provided in Appendix B.2. 5https://github.com/openai/prm800k/blob/main/prm800k/data/phase2_test.jsonl 6 Table 2: The evaluation results of various PRMs, instruction-followed LLMs that are served as critique models and our critique models on three benchmarks assessing the mathematical critique ability. The reported metric is the F1 score [54] (i.e., harmonic mean) of the judgment accuracy on incorrect solutions and the judgment accuracy on correct solutions. Model MRGSM8K PRM800K ProcessBench GSM8K MATH OlympiadBench Omni-Math Process Reward Models (PRMs) Math-Shepherd-PRM-7B RLHFlow-PRM-8B-Mistral RLHFlow-PRM-8B-DeepSeek Qwen2.5-Math-7B-PRM800K 61.8 66.6 44.8 70.8 21.7 25.2 18.5 55.6 Large Language Models, served as Critique Models LLaMA3.1-8B-Instruct Qwen2.5-7B-Instruct Qwen2.5-Math-7B-Instruct DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Qwen-7B LLaMA3.1-70B-Instruct Qwen2.5-72B-Instruct Qwen2.5-Math-72B-Instruct GPT-4o Our Critique Models DeepCritic-7B-SFT DeepCritic-7B-RL-Numina DeepCritic-7B-RL-PRM800K 31.6 48.1 35.6 69.4 77.9 72.4 72.6 73.6 69.7 67.1 77.2 77.3 16.0 25.6 19.4 55.7 57.4 34.1 45.3 41.0 45. 48.0 55.9 60.1 48.2 50.9 32.3 70.5 23.8 42.9 23.1 65.0 71.9 72.5 72.2 68.6 72.1 59.2 70.7 74.0 27.1 32.0 34.2 64.7 18.9 36.6 22.0 62.7 69.9 47.6 52.4 48.5 57. 61.2 65.9 72.9 20.5 13.8 16.0 50.0 18.3 25.5 9.2 58.4 56.4 41.0 41.9 28.6 50.5 46.0 57.6 60.9 16.3 15.7 18.3 42.7 17.2 25.9 10.4 51.7 46.8 36.8 43.1 27.3 53. 43.0 53.5 57.2 Avg. 32.6 34.0 27.4 59.7 21.0 34.1 20.0 60.5 63.4 50.7 54.6 47.9 58.2 54.1 63.5 67.1 Baselines We compare our critique models against two categories of baselines: (1) Process Reward Models (PRMs): In this category, we select Math-Shepherd-PRM-7B [40], RLHFlow-PRM-8BMistral/DeepSeek [47], Qwen2.5-Math-7B-PRM800K [54] for comparison. (2) LLM Critics: We prompt the following leading LLMs to serve as critique models: LLaMA3.1-8B/70B-Instruct [28], Qwen2.5-7B/72B-Instruct [32], Qwen2.5-Math-7B/72B-Instruct [48], and GPT-4o [15]. Also, we include two of the most advanced reasoning LLMs, DeepSeek-R1-Distill-Llama-8B and DeepSeekR1-Distill-Qwen-7B [6], and use them as reasoning-enhanced critique models for comprehensive comparison. Training Settings In SFT stage, the learning rate is 1 105, the batch size is 64, and we fine-tune for 3 epochs. In RL stage, we adopt verl [37] as our training framework, and use Group Relative Policy Optimization (GRPO) [36] as RL algorithm. In RL, an accuracy reward of 1.0 is given if the final judgment is correct; otherwise, it is 0.0. During RL, we observe that in very few cases, the policy model generates critiques that are mixed with different languages, which is consistent with the findings in DeepSeek-R1 [6]. However, this issue gradually diminishes as training progresses, so we do not introduce language consistency reward here. The detailed training settings in both SFT and RL stages are in Appendix B.3. Evaluation Settings In our main evaluation, we use consistent sampling settings across all critique models, with temperature set to 0.6, top_p to 0.9, and max_generation_length to 32K during inference. We only sample once for each task input, while we explore the performance of majority voting over eight samplings in Section 5.1. The evaluation prompt is mainly based on [54], and is put in Appendix B.4. The main evaluation metric is the F1 score [54], which is the harmonic mean of the judgment accuracy on the step index of first erroneous step in incorrect solutions and the judgment accuracy on correct solutions. Further details on the evaluation settings are provided in Appendix B.4. 4.2 Main Results The overall results on all benchmarks are displayed in Table 2. We put the detailed results of separate accuracy on both incorrect and correct solutions in Appendix F. 7 (a) Results on MR-GSM8K (b) Results on PRM800K (c) Results on PB-GSM8K (d) Results on PB-MATH (e) Results on PB-OlympiadBench (f) Results on PB-Omni-Math Figure 3: Majority voting results (Maj@8) of each model across all benchmarks. Pass@1 results are from Table 2. PB denotes ProcessBench. First, we can observe that existing instruct models, especially those of small sizes, exhibit very limited critique capabilities, as reflected in their poor judgment performance. As the model size increases, the corresponding critique capability also increases. Second, improvements in the models reasoning ability have positive impact on its critique capability. This is reflected in that the strong reasoning abilities of the DeepSeek-R1-Distill models obtained in other domains can be transferred to the critique task and bring substantial performance gains. Third, our seed critique model DeepCritic7B-SFT, trained on 4.5K carefully curated deliberate critique data, achieves 20-point F1 score improvement (34.154.1) over the corresponding base model Qwen2.5-7B-Instruct. Its overall performance is even comparable to that of Qwen2.5-72B-Instruct. This demonstrates the high quality of our constructed seed critique data and validates our motivation that teaching LLMs to perform deliberate critiquing can indeed lead to significant performance improvements. Regarding the RL performance, we can see that RL with only 14.2K automatically constructed data (i.e., DeepCritic-7B-RL-Numina) can effectively boost the models critique performance from 54.1 to 63.5. This validates the potential of automatically constructing supervision data and paves the promising way for the automated scalable oversight. Furthermore, when trained with larger scale RL data with higher quality, the resulted model DeepCritic-7B-RL-PRM800K outperforms all baselines, including GPT-4o and two same-sized DeepSeek-R1-Distill models, in 5 out of 6 evaluation sets and achieves the best overall performance. All in all, the above experimental results demonstrate that our proposed two-stage training paradigm is highly effective in enhancing the critique and verification capabilities of LLMs."
        },
        {
            "title": "5 Test-Time Scaling Results",
            "content": "In this section, we explore the test-time scaling properties within the critique framework, from the perspectives of both critics and generators. In the following experiments, we choose our most powerful critique model so far DeepCritic-7B-RL-PRM800K as the target model, and refer to it as DeepCritic-7B-RL for brevity. We take the base model Qwen2.5-7B-Instruct along with the strongest baseline DeepSeek-R1-Distill-Qwen-7B (abbreviated to DS-R1-Distill-Qwen-7B) for comparison. 5.1 Test-Time Scaling Results of Critics Here, we investigate the effectiveness of the majority voting practice [41] in enhancing the critique performance. For each critique model, the final judgment on each input is the majority voting 8 (a) Results on MATH500 (b) Results on AIME24Figure 4: Verified majority voting results of Qwen2.5-7B/72B-Instruct on MATH500 and AIME20242025 by taking different models as verifiers. result over eight samplings (Maj@8). We put the comparison results between Maj@8 and Pass@1 in Figure 3. As shown in Figure 3, the majority voting practice improves performance across all models. The Maj@8 results of our critique model outperform DeepSeek-R1-Distill-Qwen-7B in half of the settings, and the average F1 score of our model (70.5) is also higher than that of DeepSeek-R1-Distill-Qwen-7B (69.9), demonstrating the good test-time scaling property of our critique model. 5.2 Test-Time Scaling Results of Generators Critics can also be used to improve the performance of LLM generators via scaling the test-time compute of generators. On the one hand, similar to the role of PRMs, critics can act as verifiers to assess whether the responses sampled by the generator are correct. By filtering out identified incorrect responses, more accurate majority voting results of solutions can be achieved. On the other hand, the generator can revise potentially erroneous responses based on the feedback from the critic, thereby arriving at the correct answer through refinement. In the following, we delve into above two aspects separately. We select two generators of different sizes for experiments: Qwen2.5-7B-Instruct and Qwen2.5-72B-Instruct. We conduct evaluations on MATH500 and AIME2024-2025. We use Math-Verify6 as the rule-based verifier to determine whether the predicted answer matches the ground truth answer. 5.2.1 Results of Verified Majority Voting We put the majority voting results under different numbers of sampled solutions from the generators, filtered taking the critique model as the verifier,7 in Figure 4. The sampling temperature for generators is set to 1.0.8 We observe that when the critique model performs poorly (e.g., Qwen2.5-7B-Instruct), using it as the verifier in majority voting can be counterproductive. In contrast, our critique model can yield greater improvements to generators majority voting performance in most sampling settings compared to baselines. 5.2.2 Results of Critique-Based Refinement In this setting, we first prompt generators to produce step-by-step solutions for each problem. Then, we leverage each critic to critique the solutions and prompt the generators to revise those deemed 6https://github.com/huggingface/Math-Verify 7If all candidate solutions are identified as incorrect by the critique model, we fall back to perform majority voting over the original solutions. 8In this setting and the following refinement experiments, to ensure that the generators produce responses in required step-by-step formatenabling subsequent critiques by the critique modelwe adopt system prompt (refer to Appendix D) different from the original one used by Qwen2.5 models. As result, the evaluation results in Figure 4 and Table 3 may differ from the original results. Table 3: Results of critique-based refinement. wc represents the proportion of cases where the model initially produces wrong solution but arrives at the correct answer after judgment and refinement, and cw represents the the ratio of cases where correct solutions turns into incorrect one after judgment and refinement. Acc. represents the average accuracy on all testing samples. * denotes that the refinement results are biased due to DS-R1-Distill-Qwen-7B directly producing the correct answers during critique. Qwen2.5-7B-Instruct Qwen2.5-72B-Instruct Critique Model MATH500 AIME24MATH500 AIME24-25 wc cw Acc. wc cw Acc. wc cw Acc. wc cw Acc. before refinement after refinement Qwen2.5-7B-Instruct DeepCritic-7B-RL 74.00 6.67 77.00 11. 0.80 2.60 72.20 1.67 0.00 8.33 1.60 2.40 76.20 1.67 0.00 13.33 4.60 1.40 77.20 5.00 0.00 11.67 7.00 2.00 82.00 5.00 1.67 15.00 after refinement (answer leakage) DS-R1-Distill-Qwen-7B* 7.20 1.20 80.00 8.33 0.00 15.00 7.40 1.00 83.40 3.33 0.00 15.00 incorrect, based on the critics feedback. We use greedy decoding for the generators for determinism. In experiments, we observe that DeepSeek-R1-Distill-Qwen-7B frequently continues critiquing until the end of the solution and produces the correct answer, even though we explicitly instruct the model in the system prompt to stop after identifying the first incorrect step. This issue can cause the refinement results to be biased and greatly influenced by DeekSeek-R1-Distill-Qwen-7Bs own problem-solving capability. Therefore, we present its results independently for reference. The refinement results are shown in Table 3. We can see that our critique model can effectively assist the generators in correcting errors by providing more detailed feedback, leading to improved performance of the generators. Notably, our 7B critique model is also capable of supervising and correcting the outputs of 72B generator, demonstrating potential of weak-to-strong supervision [3]."
        },
        {
            "title": "6 Conclusion",
            "content": "In this work, we propose an effective pipeline to enhance the math critique ability of LLMs. We first carefully construct 4.5K long-form critiques incorporating multi-perspective verification and meta-critiquing. These serve as seed data for supervised fine-tuning, enabling the target model to acquire an initial ability of deliberately critiquing. We then further enhance the critique capability of the model via reinforcement learning. The deep critique model we developed demonstrates superior performance across range of error identification benchmarks, and exhibits promising potential in supervising and improving the reasoning capabilities of LLM generators that are even more capable than itself. We hope our work provides valuable insights to advance future research in deliberate reasoning and scalable oversight."
        },
        {
            "title": "References",
            "content": "[1] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort, Deep Ganguli, Tom Henighan, et al. Training helpful and harmless assistant with reinforcement learning from human feedback. arXiv preprint arXiv:2204.05862, 2022. [2] Samuel Bowman, Jeeyoon Hyun, Ethan Perez, Edwin Chen, Craig Pettit, Scott Heiner, Kamile Lukošiute, Amanda Askell, Andy Jones, Anna Chen, et al. Measuring progress on scalable oversight for large language models. arXiv preprint arXiv:2211.03540, 2022. [3] Collin Burns, Pavel Izmailov, Jan Hendrik Kirchner, Bowen Baker, Leo Gao, Leopold Aschenbrenner, Yining Chen, Adrien Ecoffet, Manas Joglekar, Jan Leike, et al. Weak-to-strong generalization: eliciting strong capabilities with weak supervision. In Proceedings of the 41st International Conference on Machine Learning, pages 49715012, 2024. 10 [4] Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. [5] Ganqu Cui, Lifan Yuan, Zefan Wang, Hanbin Wang, Wendi Li, Bingxiang He, Yuchen Fan, Tianyu Yu, Qixin Xu, Weize Chen, et al. Process reinforcement through implicit rewards. arXiv preprint arXiv:2502.01456, 2025. [6] DeepSeek. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning, 1 2025. URL https://github.com/deepseek-ai/DeepSeek-R1/blob/main/DeepSeek_R1.pdf. [7] Bofei Gao, Zefan Cai, Runxin Xu, Peiyi Wang, Ce Zheng, Runji Lin, Keming Lu, Dayiheng Liu, Chang Zhou, Wen Xiao, et al. Llm critics help catch bugs in mathematics: Towards better mathematical verifier with natural language feedback. arXiv preprint arXiv:2406.14024, 2024. [8] Bofei Gao, Feifan Song, Zhe Yang, Zefan Cai, Yibo Miao, Qingxiu Dong, Lei Li, Chenghao Ma, Liang Chen, Runxin Xu, et al. Omni-math: universal olympiad level mathematic benchmark for large language models. arXiv preprint arXiv:2410.07985, 2024. [9] Mor Geva, Daniel Khashabi, Elad Segal, Tushar Khot, Dan Roth, and Jonathan Berant. Did aristotle use laptop? question answering benchmark with implicit reasoning strategies. Transactions of the Association for Computational Linguistics, 9:346361, 2021. [10] Jiawei Gu, Xuhui Jiang, Zhichao Shi, Hexiang Tan, Xuehao Zhai, Chengjin Xu, Wei Li, Yinghan Shen, Shengjie Ma, Honghao Liu, et al. survey on llm-as-a-judge. arXiv preprint arXiv:2411.15594, 2024. [11] Chaoqun He, Renjie Luo, Yuzhuo Bai, Shengding Hu, Zhen Thai, Junhao Shen, Jinyi Hu, Xu Han, Yujie Huang, Yuxiang Zhang, et al. Olympiadbench: challenging benchmark for promoting agi with olympiad-level bilingual multimodal scientific problems. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 38283850, 2024. [12] Dan Hendrycks, Collin Burns, Saurav Kadavath, Akul Arora, Steven Basart, Eric Tang, Dawn Song, and Jacob Steinhardt. Measuring mathematical problem solving with the MATH dataset. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and Benchmarks Track (Round 2), 2021. URL https://openreview.net/forum?id=7Bywt2mQsCe. [13] Jingcheng Hu, Yinmin Zhang, Qi Han, Daxin Jiang, Xiangyu Zhang, and Heung-Yeung Shum. Open-reasoner-zero: An open source approach to scaling up reinforcement learning on the base model, 2025. URL https://arxiv.org/abs/2503.24290. [14] Binyuan Hui, Jian Yang, Zeyu Cui, Jiaxi Yang, Dayiheng Liu, Lei Zhang, Tianyu Liu, Jiajun Zhang, Bowen Yu, Keming Lu, et al. Qwen2. 5-coder technical report. arXiv preprint arXiv:2409.12186, 2024. [15] Aaron Hurst, Adam Lerer, Adam Goucher, Adam Perelman, Aditya Ramesh, Aidan Clark, AJ Ostrow, Akila Welihinda, Alan Hayes, Alec Radford, et al. Gpt-4o system card. arXiv preprint arXiv:2410.21276, 2024. [16] Carlos Jimenez, John Yang, Alexander Wettig, Shunyu Yao, Kexin Pei, Ofir Press, and Karthik Narasimhan. SWE-bench: Can language models resolve real-world github issues? In The Twelfth International Conference on Learning Representations, 2024. URL https: //openreview.net/forum?id=VTF8yNQM66. [17] Pei Ke, Bosi Wen, Andrew Feng, Xiao Liu, Xuanyu Lei, Jiale Cheng, Shengyuan Wang, Aohan Zeng, Yuxiao Dong, Hongning Wang, Jie Tang, and Minlie Huang. CritiqueLLM: Towards an informative critique generation model for evaluation of large language model generation. In Lun-Wei Ku, Andre Martins, and Vivek Srikumar, editors, Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 1303413054, Bangkok, Thailand, August 2024. Association for Computational Linguistics. doi: 10.18653/v1/2024.acl-long.704. URL https://aclanthology.org/2024.acl-long.704/. [18] Xin Lai, Zhuotao Tian, Yukang Chen, Senqiao Yang, Xiangru Peng, and Jiaya Jia. Stepdpo: Step-wise preference optimization for long-chain reasoning of llms. arXiv preprint arXiv:2406.18629, 2024. [19] Tian Lan, Wenwei Zhang, Chen Xu, Heyan Huang, Dahua Lin, Kai Chen, and Xian-Ling Mao. Criticeval: Evaluating large-scale language model as critic. Advances in Neural Information Processing Systems, 37:6690766960, 2024. 11 [20] Jia Li, Edward Beeching, Lewis Tunstall, Ben Lipkin, Roman Soletskyi, Shengyi Huang, Kashif Rasul, Longhui Yu, Albert Jiang, Ziju Shen, et al. Numinamath: The largest public dataset in ai4maths with 860k pairs of competition math problems and solutions. Hugging Face repository, 13, 2024. [21] Hunter Lightman, Vineet Kosaraju, Yuri Burda, Harrison Edwards, Bowen Baker, Teddy Lee, Jan Leike, John Schulman, Ilya Sutskever, and Karl Cobbe. Lets verify step by step. In The Twelfth International Conference on Learning Representations, 2023. [22] Zicheng Lin, Zhibin Gou, Tian Liang, Ruilin Luo, Haowei Liu, and Yujiu Yang. Criticbench: Benchmarking llms for critique-correct reasoning. In Findings of the Association for Computational Linguistics ACL 2024, pages 15521587, 2024. [23] Aixin Liu, Bei Feng, Bing Xue, Bingxuan Wang, Bochao Wu, Chengda Lu, Chenggang Zhao, Chengqi Deng, Chenyu Zhang, Chong Ruan, et al. Deepseek-v3 technical report. arXiv preprint arXiv:2412.19437, 2024. [24] Liangchen Luo, Zi Lin, Yinxiao Liu, Lei Shu, Yun Zhu, Jingbo Shang, and Lei Meng. Critique ability of large language models. arXiv preprint arXiv:2310.04815, 2023. [25] Xiaoliang Luo, Akilles Rechardt, Guangzhi Sun, Kevin Nejad, Felipe Yáñez, Bati Yilmaz, Kangjoo Lee, Alexandra Cohen, Valentina Borghesani, Anton Pashkov, et al. Large language models surpass human experts in predicting neuroscience results. Nature human behaviour, pages 111, 2024. [26] Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, et al. Self-refine: Iterative refinement with self-feedback. Advances in Neural Information Processing Systems, 36:4653446594, 2023. [27] Nat McAleese, Rai Michael Pokorny, Juan Felipe Ceron Uribe, Evgenia Nitishinskaya, Maja Trebacz, and Jan Leike. Llm critics help catch llm bugs. arXiv preprint arXiv:2407.00215, 2024. [28] MetaAI. Introducing llama 3.1: Our most capable models to date. https://ai.meta.com/blog/ meta-llama-3-1/, 2024. [29] OpenAI. Gpt-4 technical report. arXiv, pages 230308774, 2023. [30] OpenAI. Learning to reason with llms, 2024. URL https://openai.com/index/ learning-to-reason-with-llms. [31] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback. Advances in Neural Information Processing Systems, 35:2773027744, 2022. [32] Qwen Team. Qwen2. 5: party of foundation models. Qwen (Sept. 2024). url: https://qwenlm. github. io/blog/qwen2, 5, 2024. [33] David Rein, Betty Li Hou, Asa Cooper Stickland, Jackson Petty, Richard Yuanzhe Pang, Julien Dirani, Julian Michael, and Samuel Bowman. Gpqa: graduate-level google-proof q&a benchmark. arXiv preprint arXiv:2311.12022, 2023. [34] Baptiste Roziere, Jonas Gehring, Fabian Gloeckle, Sten Sootla, Itai Gat, Xiaoqing Ellen Tan, Yossi Adi, Jingyu Liu, Romain Sauvestre, Tal Remez, et al. Code llama: Open foundation models for code. arXiv preprint arXiv:2308.12950, 2023. [35] William Saunders, Catherine Yeh, Jeff Wu, Steven Bills, Long Ouyang, Jonathan Ward, and Jan Leike. Self-critiquing models for assisting human evaluators. arXiv preprint arXiv:2206.05802, 2022. [36] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. [37] Guangming Sheng, Chi Zhang, Zilingfeng Ye, Xibin Wu, Wang Zhang, Ru Zhang, Yanghua Peng, Haibin Lin, and Chuan Wu. Hybridflow: flexible and efficient rlhf framework. arXiv preprint arXiv:2409.19256, 2024. 12 [38] Zhengyang Tang, Ziniu Li, Zhenyang Xiao, Tian Ding, Ruoyu Sun, Benyou Wang, Dayiheng Liu, Fei Huang, Tianyu Liu, Bowen Yu, et al. Enabling scalable oversight via self-evolving critic. arXiv preprint arXiv:2501.05727, 2025. [39] Jonathan Uesato, Nate Kushman, Ramana Kumar, Francis Song, Noah Siegel, Lisa Wang, Antonia Creswell, Geoffrey Irving, and Irina Higgins. Solving math word problems with process-and outcome-based feedback. arXiv preprint arXiv:2211.14275, 2022. [40] Peiyi Wang, Lei Li, Zhihong Shao, Runxin Xu, Damai Dai, Yifei Li, Deli Chen, Yu Wu, and Zhifang Sui. Math-shepherd: Verify and reinforce llms step-by-step without human annotations. In Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 94269439, 2024. [41] Xuezhi Wang, Jason Wei, Dale Schuurmans, Quoc Le, Ed H. Chi, Sharan Narang, Aakanksha Chowdhery, and Denny Zhou. Self-consistency improves chain of thought reasoning in language models. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.net/forum?id=1PL1NIMMrw. [42] Tianhao Wu, Weizhe Yuan, Olga Golovneva, Jing Xu, Yuandong Tian, Jiantao Jiao, Jason Weston, and Sainbayar Sukhbaatar. Meta-rewarding language models: Self-improving alignment with llm-as-a-meta-judge. arXiv preprint arXiv:2407.19594, 2024. [43] Yangzhen Wu, Zhiqing Sun, Shanda Li, Sean Welleck, and Yiming Yang. Scaling inference computation: Compute-optimal inference for problem-solving with language models. In The 4th Workshop on Mathematical Reasoning and AI at NeurIPS24, 2024. URL https: //openreview.net/forum?id=j7DZWSc8qu. [44] Zhiheng Xi, Dingwen Yang, Jixuan Huang, Jiafu Tang, Guanyu Li, Yiwen Ding, Wei He, Boyang Hong, Shihan Do, Wenyu Zhan, et al. Enhancing llm reasoning via critique models with test-time and training-time supervision. arXiv preprint arXiv:2411.16579, 2024. [45] Zhihui Xie, Liyu Chen, Weichao Mao, Jingjing Xu, Lingpeng Kong, et al. Teaching language models to critique via reinforcement learning. arXiv preprint arXiv:2502.03492, 2025. [46] Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm, 2024. [47] Wei Xiong, Hanning Zhang, Nan Jiang, and Tong Zhang. An implementation of generative prm. https://github.com/RLHFlow/RLHF-Reward-Modeling, 2024. [48] An Yang, Beichen Zhang, Binyuan Hui, Bofei Gao, Bowen Yu, Chengpeng Li, Dayiheng Liu, Jianhong Tu, Jingren Zhou, Junyang Lin, et al. Qwen2. 5-math technical report: Toward mathematical expert model via self-improvement. arXiv preprint arXiv:2409.12122, 2024. [49] Wenkai Yang, Shiqi Shen, Guangyao Shen, Wei Yao, Yong Liu, Zhi Gong, Yankai Lin, and JiRong Wen. Super (ficial)-alignment: Strong models may deceive weak models in weak-to-strong generalization. arXiv preprint arXiv:2406.11431, 2024. [50] Wenkai Yang, Shuming Ma, Yankai Lin, and Furu Wei. Towards thinking-optimal scaling of test-time compute for llm reasoning. arXiv preprint arXiv:2502.18080, 2025. [51] Longhui Yu, Weisen Jiang, Han Shi, Jincheng YU, Zhengying Liu, Yu Zhang, James Kwok, Zhenguo Li, Adrian Weller, and Weiyang Liu. Metamath: Bootstrap your own mathematical questions for large language models. In The Twelfth International Conference on Learning Representations, 2024. URL https://openreview.net/forum?id=N8N0hgNDRt. [52] Weizhe Yuan, Richard Yuanzhe Pang, Kyunghyun Cho, Xian Li, Sainbayar Sukhbaatar, Jing Xu, and Jason Weston. Self-rewarding language models, 2024. URL https://arxiv. org/abs/2401.10020. [53] Zhongshen Zeng, Pengguang Chen, Shu Liu, Haiyun Jiang, and Jiaya Jia. Mr-gsm8k: metareasoning benchmark for large language model evaluation. arXiv preprint arXiv:2312.17080, 2023. [54] Chujie Zheng, Zhenru Zhang, Beichen Zhang, Runji Lin, Keming Lu, Bowen Yu, Dayiheng Liu, Jingren Zhou, and Junyang Lin. Processbench: Identifying process errors in mathematical reasoning. arXiv preprint arXiv:2412.06559, 2024. [55] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural Information Processing Systems, 36:4659546623, 2023. 13 [56] Xin Zheng, Jie Lou, Boxi Cao, Xueru Wen, Yuqiu Ji, Hongyu Lin, Yaojie Lu, Xianpei Han, Debing Zhang, and Le Sun. Critic-cot: Boosting the reasoning abilities of large language model via chain-of-thoughts critic. arXiv preprint arXiv:2408.16326, 2024."
        },
        {
            "title": "A Prompt Templates for Critique Data Generation",
            "content": "Prompt Template for Initial Critique Generation You are math expert and are tasked with evaluating the solution path for mathematical problem. The solution is presented as step-by-step chain of thought. Each step is separated with the index indicator Step i:, with indexed starting from 1. You are required to only critique the specific step carefully and comprehensively. You need to thoroughly consider the logical consistency of the specified step with the problem statement and previous steps, ensuring each step aligns with the overall and correct objective. You should consider the cases where the steps are merely irrelevant transitions as correct if there is no critical information missing. For steps involving numerical calculations, carefully verify the accuracy of the calculations to ensure all results are correct. You should first generate critical reasoning process before giving the final judgment. ##Format for Evaluation## For each specified step in the solution path, perform your evaluation by following the below format: **Critique of Step <current_step>**: First generate detailed reasoning thought to evaluate the step. **Judgement**: Based on the above critique, give your final judgement in the form of #### The boxed{1-1}, where 1 represents correct and -1 represents correctness of Step <current_step> is: incorrect. The judgement result should be either 1 or -1. <Problem> {problem} </Problem> <Solution Path> {solution} </Solution Path> Now, please critique Step {step_index} in the above solution path. Prompt Template for In-Depth Critique Generation You are math expert and are tasked with evaluating the critique for specific step in solution to mathematical problem. You will be given the problem, the solution path, and the critique for specified step in the solution path. You need to critique the critique for the specified step and provide your judgement on whether the critique is correct or incorrect, and then determine the final correctness of the specified step. You need to think about how you would approach evaluating the step if you were asked to do so, without referring to the original critique. You can either re-evaluate the specified step using different valid approaches or from different perspectives than the original critique to see if different methods can reach the same conclusion; or alternatively, you can critique the original critique itself to check if it is correct and whether it is fair and reasonable. You should first generate critical reasoning process before giving the final judgment. ##Format for Evaluation## Perform your evaluation to the critique by following the below format: **Critique of the critique of Step <current_step>**: First generate detailed critique either by re-evaluating the specified step with different ways or by directly evaluating the original critique of the step. **Judgement**: Based on the results of original critique and critiques critique, give your final judgement on the correctness of the specified step in the form of #### The correctness of Step <current_step> is: boxed{1-1}, where 1 represents correct and -1 represents incorrect. The judgment result should be either 1 or -1. <Problem> {problem} </Problem> <Solution Path> {solution} </Solution Path> <Original Critique> {original_critique} </Original Critique> Now, please critique the original critique of the Step {step_index} and give your final judgement on the correctness of Step {step_index}. Prompt Template for Final Critique Synthesis You are math expert and good math critic. You will be provided with an initial critique and critique of the initial critique. Your task is to merge the two critiques into single, deliberate critique. You should merge the two critiques as if they were generated in one go, as if the model first generated critique and then wanted to further verify that step or the critique itself. You should make the merged critique smooth by adding some transitional, pausing, reflective, thinking words or sentences. Do not use terms like the original critique as the merged critique should be considered as generated in one go. Here are two examples that can serve as references for the tone and format of the merged deliberate critique: <Merged Deliberate Critique Example 1>{example1}</Merged Deliberate Critique Example 1> <Merged Deliberate Critique Example 2>{example2}</Merged Deliberate Critique Example 2> Please follow the above examples to generate the merged deliberate critique for the below sample: <Original Critique>{original_critique}</Original Critique> <Critique of the Original Critique>{critique_of_original_critique}</Critique of the Original Critique> 15 Figure 5: Training hyper-parameters in SFT. Figure 6: Training hyper-parameters in RL. Hyper-parameter"
        },
        {
            "title": "LR\nLR Scheduler\nBatch Size\nEpochs\nMaximum Sequence Length\nWarmup Ratio",
            "content": "Value 1 105 cosine 64 3 16384 0.1 Hyper-parameter"
        },
        {
            "title": "Value",
            "content": "128 128 8 2048 8192 1.0 0.9 1 106 2 0."
        },
        {
            "title": "B Detailed Experimental Settings",
            "content": "B.1 Hyper-Parameters in The Data Generation Stage In the initial critique generation stage, for each reasoning step in the given solution, we prompt Qwen2.5-72B-Instruct once with temperature = 0.7 and top_p = 0.9. In the in-depth critique generation stage, based on the problem, solution and the initial critique, we prompt Qwen2.5-72BInstruct 16 times with temperature = 1.0 and top_p = 0.9. We then randomly select one critique from those whose judgment results jdeep are consistent with the ground truth labels, and use it as the in-depth critique for the corresponding step. Finally, during critique merging, the sampling parameters for Qwen2.5-72B-Instruct are temperature = 0.7 and top_p = 0.9. B.2 Overview of Evaluation Benchmarks For MR-GSM8K [53], we only choose the subset in which the questions are from the original GSM8K [4] dataset, containing 693 correct solutions and 725 incorrect solutions. For PRM800K [21], we adopt the Phase-2 test set that contains 586 correct solutions and 2078 incorrect solutions. The third evaluation benchmark is ProcessBench [39], which includes four subsets in which the questions originates from different sources: GSM8K, MATH [12], OlympiadBench [11] and Omni-Math [8]. The total number of solutions for these four subsets are 400, 1000, 1000, and 1000, respectively. B.3 Detailed Training Settings The complete training hyper-parameters in SFT and RL are put in Table 5 and Table 6 respectively. In RL, an accuracy reward of 1.0 is given if the final judgment is correct; otherwise, it is 0.0. B.4 Detailed Evaluation Settings In the main evaluations  (Table 2)  , we use consistent sampling settings across all critique models, with temperature set to 0.6, top_p to 0.9, and max_generation_length to 32K during inference. We only sample once for each task input in this setting. We then calculate the judgment accuracy on the step index of first erroneous step in incorrect solutions and the judgment accuracy on correct solutions. The F1 score is then calculated as the harmonic mean of the above two numbers. In the experiments of majority voting (Section 5.1), for each selected critique model, we sample 8 responses with temperature set to 1.0 and top_p to 0.9. The evaluation prompt is mainly based on that used in [54], and is put in below. We do not directly use the prompt used in [54], because we find that it will make the model tend to directly generate the judgment results without the reasoning process. Evaluation Prompt The following is math problem and solution. The solution is presented as step-by-step chain of thought. Each step is separated with the index indicator Step i:, with indexed starting from 1. <Problem> {problem} </Problem> <Solution> {tagged_response} </Solution> Your task is to evaluate the solution and identify the earliest step that contains an error, or confirm that all steps are correct. Please first review and generate critique for each step. After reviewing each step, once you identify an error in the step, stop the critique and return the index of that step as this is the step where the earliest error occurs. Otherwise, continue reviewing the subsequent steps. If all steps are correct, return the index of -1 (which typically denotes not found). Finally, put your final answer (i.e., the step index) in boxed{}. Figure 7: Solution-level label distribution of seed critique data in SFT stage. Figure 8: Solution-level label distribution of RL data based on PRM800K. Figure 9: Solution-level label distribution of RL data based on NuminaMath-CoT."
        },
        {
            "title": "C Label Distributions of SFT and RL Data",
            "content": "We show the solution-level label distribution (i.e., he distribution of the step index of the first erroneous step) of our curated SFT dataset in Figure 7. The label distributions of RL data based on PRM800K and NuminaMath-CoT are show in Figure 8 and Figure 9 respectively. We note that due to limited computational resources, we set the max_response_length to 8192 during the RL phase. As result, we apply filtering and constraints to the RL data, retaining only task inputs whose solutions contain number of reasoning steps within certain range. This avoids the situation where large number of rollouts would be truncated due to excessive lengths, which could degrade training performance. However, we believe that with sufficient computational resources, incorporating 17 inputs with more steps and increasing the max_response_length during RL can further improve RL performance. Prompt Templates in Verified Majority Voting and Critique-Based"
        },
        {
            "title": "Refinement Experiments",
            "content": "The prompt templates to create step-by-step solutions and critique-based refinements for generators are put below separately. In the experiments of critique-based refinements, we further add instructions to instruct DeepSeek-R1-Distill-Qwen-7B not to produce the final answer and to stop critiquing after identifying the first incorrect step. However, even with this constraint, we still find that in certain number of cases, DeepSeek-R1-Distill-Qwen-7B does not follow the instruction and continues critiquing until the end. Prompt Template for Generating Step-by-Step Solutions System: You are helpful assistant. Please reason step by step, and put your final answer within boxed{}. User: {problem} Assistant: Step 1: Prompt Template for Critique-Based Refinement System: You are helpful assistant. Please reason step by step, and put your final answer within boxed{}. User: {problem} Assistant: {initial_solution} User: There might be some problems in your solution. Here is the critique of the above solution: {critique} Please carefully refine the solution based on the critique. Assistant:"
        },
        {
            "title": "E Case Study",
            "content": "We present complete case study of our DeepCritic-7B-RL-PRM800K model in Figure 10."
        },
        {
            "title": "F Detailed Results on All Benchmarks",
            "content": "We present the detailed resultsincluding the separate judgment accuracy on both correct and incorrect solutionson each benchmark in Table 4, 5, and 6. 18 Figure 10: complete case study of DeepCritic-7B-RL-PRM800K. 19 Table 4: Detailed results of various models on MR-GSM8K and PRM800K. erroneous represents the judgment accuracy on incorrect solutions, correct represents the judgment accuracy on correct solutions, F1 represents the harmonic mean. Model MR-GSM8K PRM800K erroneous correct F1 erroneous correct 84.1 96.7 99.0 89.2 28.7 86.0 89.8 78.5 84.3 92.5 95.2 81.7 92.6 92.6 80.0 85.1 94.2 92.9 93.4 61.8 66.6 44.8 70.8 31.6 48.1 50.3 35.6 69.4 77.9 80.3 72.4 72.6 73.6 69.7 67.1 77.2 77.3 78. 14.0 15.6 10.3 43.4 17.2 15.2 17.1 10.9 44.3 42.3 49.6 21.9 31.0 26.6 31.5 35.0 45.4 49.6 53.7 47.8 65.0 89.6 77.3 15.0 81.7 82.8 88.9 75.3 89.2 92.5 76.8 84.3 89.8 84.3 76.3 72.5 76.3 81. 21.7 25.2 18.5 55.6 16.0 25.6 28.3 19.4 55.7 57.4 64.5 34.1 45.3 41.0 45.9 48.0 55.9 60.1 64.6 Process Reward Models (PRMs) Math-Shepherd-PRM-7B RLHFlow-PRM-8B-Mistral RLHFlow-PRM-8B-DeepSeek Qwen2.5-Math-7B-PRM800K 48.8 50.8 29.0 58. Large Language Models, served as Critique Models LLaMA3.1-8B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct (Maj@8) Qwen2.5-Math-7B-Instruct DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-7B (Maj@8) LLaMA3.1-70B-Instruct Qwen2.5-72B-Instruct Qwen2.5-Math-72B-Instruct GPT-4o Our Critique Models DeepCritic-7B-SFT DeepCritic-7B-RL-Numina DeepCritic-7B-RL-PRM800K DeepCritic-7B-RL-PRM800K (Maj@8) 35.0 33.4 34.9 23.0 59.0 67.3 69.4 65.0 59.7 61.1 61.8 55.3 65.4 66.2 68. 20 Table 5: Detailed results of various models on the GSM8K and MATH testing sets of ProcessBench. erroneous represents the judgment accuracy on incorrect solutions, correct represents the judgment accuracy on correct solutions, F1 represents the harmonic mean. Model GSM8K MATH erroneous correct F1 erroneous correct F1 94.3 98.4 99.0 93.8 17.6 82.4 91.2 99.5 79.3 97.4 99.5 88.6 96.9 96.9 95.3 92.7 94.3 94.3 97. 48.2 50.9 32.3 70.5 23.8 42.9 48.8 23.1 65.0 71.9 79.1 72.5 72.2 68.6 72.1 59.2 70.7 74.0 77.8 16.2 20.0 21.7 50.8 23.7 23.2 24.6 12.5 58.4 57.7 65.7 33.7 36.2 32.5 41.2 47.0 57.4 64.3 67. 83.7 79.3 80.8 88.9 15.8 86.0 89.2 94.1 67.7 88.4 92.9 81.0 94.8 95.3 94.3 87.7 77.3 84.2 85.5 27.1 32.0 34.2 64.7 18.9 36.6 38.5 22.0 62.7 69.9 76.9 47.6 52.4 48.5 57.3 61.2 65.9 72.9 75. Process Reward Models (PRMs) Math-Shepherd-PRM-7B RLHFlow-PRM-8B-Mistral RLHFlow-PRM-8B-DeepSeek Qwen2.5-Math-7B-PRM800K 32.4 34.3 19.3 56.5 Large Language Models, served as Critique Models LLaMA3.1-8B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct (Maj@8) Qwen2.5-Math-7B-Instruct DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-7B (Maj@8) LLaMA3.1-70B-Instruct Qwen2.5-72B-Instruct Qwen2.5-Math-72B-Instruct GPT-4o Our Critique Models DeepCritic-7B-SFT DeepCritic-7B-RL-Numina DeepCritic-7B-RL-PRM800K DeepCritic-7B-RL-PRM800K (Maj@8) 36.7 29.0 33.3 13.0 55.1 57.0 65.7 61.4 57.5 53.1 58.0 43.5 56.5 60.9 64.7 21 Table 6: Detailed results of various models on the OlympiadBench and Omni-Math testing sets of ProcessBench. erroneous represents the judgment accuracy on incorrect solutions, correct represents the judgment accuracy on correct solutions, F1 represents the harmonic mean. Model OlympiadBench Omni-Math erroneous correct F1 erroneous correct 78.8 48.4 55.5 85.0 16.5 86.4 90.9 85.8 69.3 79.9 87.3 67.3 94.4 96.8 88.4 76.7 68.4 72.9 75.2 20.5 13.8 16.0 50.0 18.3 25.5 27.3 9.2 58.4 56.4 64.0 41.0 41.9 28.6 50.5 46.0 57.6 60.9 66. 9.1 9.4 11.1 28.7 20.9 15.4 18.1 5.5 40.6 33.9 40.8 25.6 28.1 15.9 39.2 30.2 45.8 48.0 52.3 79.3 49.4 52.7 83.4 14.5 80.9 86.3 88.0 71.4 75.5 81.3 65.6 92.5 95.0 83.8 75.1 64.3 71.0 68. 16.3 15.7 18.3 42.7 17.2 25.9 29.9 10.4 51.7 46.8 54.4 36.8 43.1 27.3 53.4 43.0 53.5 57.2 59.5 Process Reward Models (PRMs) Math-Shepherd-PRM-7B RLHFlow-PRM-8B-Mistral RLHFlow-PRM-8B-DeepSeek Qwen2.5-Math-7B-PRM800K 11.8 8.0 9.4 35. Large Language Models, served as Critique Models LLaMA3.1-8B-Instruct Qwen2.5-7B-Instruct Qwen2.5-7B-Instruct (Maj@8) Qwen2.5-Math-7B-Instruct DeepSeek-R1-Distill-Llama-8B DeepSeek-R1-Distill-Qwen-7B DeepSeek-R1-Distill-Qwen-7B (Maj@8) LLaMA3.1-70B-Instruct Qwen2.5-72B-Instruct Qwen2.5-Math-72B-Instruct GPT-4o Our Critique Models DeepCritic-7B-SFT DeepCritic-7B-RL-Numina DeepCritic-7B-RL-PRM800K DeepCritic-7B-RL-PRM800K (Maj@8) 20.4 15.0 16.0 4.8 50.4 43.6 50.5 29.5 26.9 16.8 35.3 32.8 49.8 52.3 59."
        }
    ],
    "affiliations": [
        "Gaoling School of Artificial Intelligence, Renmin University of China",
        "School of Computer Science and Technology, Beijing Jiaotong University"
    ]
}