{
    "paper_title": "NVILA: Efficient Frontier Visual Language Models",
    "authors": [
        "Zhijian Liu",
        "Ligeng Zhu",
        "Baifeng Shi",
        "Zhuoyang Zhang",
        "Yuming Lou",
        "Shang Yang",
        "Haocheng Xi",
        "Shiyi Cao",
        "Yuxian Gu",
        "Dacheng Li",
        "Xiuyu Li",
        "Yunhao Fang",
        "Yukang Chen",
        "Cheng-Yu Hsieh",
        "De-An Huang",
        "An-Chieh Cheng",
        "Vishwesh Nath",
        "Jinyi Hu",
        "Sifei Liu",
        "Ranjay Krishna",
        "Daguang Xu",
        "Xiaolong Wang",
        "Pavlo Molchanov",
        "Jan Kautz",
        "Hongxu Yin",
        "Song Han",
        "Yao Lu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, a family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, we improve its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This \"scale-then-compress\" approach enables NVILA to efficiently process high-resolution images and long videos. We also conduct a systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across a wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5X, fine-tuning memory usage by 3.4X, pre-filling latency by 1.6-2.2X, and decoding latency by 1.2-2.8X. We will soon make our code and models available to facilitate reproducibility."
        },
        {
            "title": "Start",
            "content": "NVILA: Efficient Frontier Visual Language Models 2024-12-6 Zhijian Liu1, Shang Yang1,2 Xiuyu Li1,3 An-Chieh Cheng4 Daguang Xu1 Hongxu Yin1, Ligeng Zhu1, Haocheng Xi1,3 Yunhao Fang1,4 Baifeng Shi1,3 Shiyi Cao1,3 Yukang Chen Vishwesh Nath1 Xiaolong Wang1,4 Song Han1,2, Jinyi Hu2,6 Pavlo Molchanov1 Yao Lu1,, Zhuoyang Zhang1,2 Yuxian Gu2,6 Cheng-Yu Hsieh5 Sifei Liu1 Yuming Lou Dacheng Li1,3 De-An Huang1 Ranjay Krishna5 Jan Kautz1 1NVIDIA 2MIT 3UC Berkeley Equal contribution Equal advisory 4UC San Diego 5University of Washington 6Tsinghua University 4 2 0 2 5 ] . [ 1 8 6 4 4 0 . 2 1 4 2 : r Abstract: Visual language models (VLMs) have made significant advances in accuracy in recent years. However, their efficiency has received much less attention. This paper introduces NVILA, family of open VLMs designed to optimize both efficiency and accuracy. Building on top of VILA, we improve its model architecture by first scaling up the spatial and temporal resolutions, and then compressing visual tokens. This scale-then-compress approach enables NVILA to efficiently process high-resolution images and long videos. We also conduct systematic investigation to enhance the efficiency of NVILA throughout its entire lifecycle, from training and fine-tuning to deployment. NVILA matches or surpasses the accuracy of many leading open and proprietary VLMs across wide range of image and video benchmarks. At the same time, it reduces training costs by 4.5, fine-tuning memory usage by 3.4, pre-filling latency by 1.6-2.2, and decoding latency by 1.2-2.8. We will soon make our code and models available to facilitate reproducibility. Links: Code (on GitHub) Models (on Hugging Face) Demo Subscribe Figure 1 Overview of NVILA: Efficient Frontier VLM. NVILA pioneers the development of highly efficient designs that span from the training phase to the deployment stage. Fig. (a,b,c): NVILAs innovative techniques lead to remarkable 4.5x acceleration in training speed, significant 3.4x reduction in memory usage and 100x saving in parameters during fine-tuning. During inference, NVILA achieves 1.8x improvement in Time to First Token and 1.2x boost in throughput; Fig. (d): The efficient designs do no compromise the accuracy, instead, NVILA sustains competitive performance with existing SOTA models on both video and image benchmarks. 1. Introduction Visual language models (VLMs) have shown remarkable abilities in processing and integrating both visual and textual information, enabling advanced vision-language interactions and dialogues. In recent years, the research community has made tremendous progress in enhancing their accuracy [1, 2, 3, 4, 5] and broadening their applications across diverse domains, including robotics [6, 7], autonomous driving [8], and medical imaging [9]. However, there has been much less focus on improving their efficiency. VLMs are expensive across multiple dimensions. First, training VLM is time-consuming. For example, training state-of-the-art 7B VLM [5] can take up to 400 GPU days, let alone even larger models. This creates significant entry barrier for researchers. 2024 NVIDIA. All rights reserved. NVILA: Efficient Frontier Visual Language Models Figure 2 NVILAs qualitative examples. NVILA: Efficient Frontier Visual Language Models Figure 3 NVILAs qualitative examples. Second, VLMs often require adaptation when applied to specialized domains (e.g., medical imaging), but fine-tuning VLM is memory-intensive. For example, fully fine-tuning 7B VLM can require over 64GB of GPU memory, far beyond the available memory of most consumer-level GPUs. Finally, VLMs are often deployed in edge applications with limited computational budget (e.g., laptops, robots), so deploying VLM is resource-constrained. Addressing these challenges requires systematic solution to improve VLM efficiency across all these dimensions. In this paper, we introduce NVILA, family of open VLMs designed to optimize both efficiency and accuracy. Building on VILA [2], we improve its model architecture by first scaling up the spatial and temporal resolution, followed by compressing visual tokens. Scaling preserves more details from visual inputs, raising the accuracy upper bound, while compression squeezes visual information to fewer tokens, improving computational efficiency. This scale-then-compress strategy allows NVILA to process high-resolution images and long videos both effectively and efficiently. In addition, we conduct systematic study to optimize the efficiency of NVILA throughout its entire lifecycle, including training, fine-tuning, and deployment. Thanks to these innovations, NVILA is both efficient and accurate. It reduces training costs by 4.5, fine-tuning memory usage by 3.4, pre-filling latency by 1.62.2, and decoding latency by 1.22.8. It also matches or surpasses the accuracy of many leading open VLMs [3, 4, 2] and proprietary VLMs [10, 11] across wide range of image and video benchmarks. Furthermore, NVILA enables new capabilities including temporal localization, robotic navigation, and medical imaging. We will release our code and models soon to support full reproducibility. We hope our work will inspire further research on efficient VLMs. 2. Approach In this section, we begin by designing an efficient model architecture for NVILA, first by scaling up spatial and temporal resolutions, and then by compressing the visual tokens. Next, we present strategies to improve NVILAs efficiency across its entire lifecyclefrom training and fine-tuning to deployment. Unless otherwise specified, all analysis in this section will be based on the 8B model. 2.1. Efficient Model Architecture We build NVILA on top of VILA [2]. As in Figure 4, it is an auto-regressive VLM composed of three components: visual encoder that extracts features from visual inputs (e.g., images, videos); projector that aligns embeddings across visual and language modalities; and token processor, typically instantiated with LLM, which takes both visual and language tokens as input and outputs language tokens. Specifically, NVILA uses SigLIP [12] as its vision encoder, two-layer MLP as its projector, and Qwen2 [13] of different sizes as its token processor. The original VILA has very limited spatial and temporal resolutions: i.e., it resizes all images to 448448, NVILA: Efficient Frontier Visual Language Models improve spatial and temporal details with fewer total tokens. 2.1.1. Spatial Scale-Then-Compress For spatial scaling, it is very natural to directly increase the image resolution of the vision encoder, for example, to 896896. While this may improve performance, applying uniformly high resolution to all images would be inefficient, especially for smaller images that do not require extensive detail. To address this, we apply S2 [14] to efficiently extract multi-scale high-resolution features with image tiling. For example, given vision encoder pre-trained at 4482 resolution and an input image with any size, S2 first resizes the image into multiple scales (e.g., 4482, 8962, 13442), and for each scale, it splits the image into tiles of 4482. Each tile is then individually processed by the encoder. The feature maps of each tile from the same scale are stitched back together into the feature map of the whole image at that scale. Finally, feature maps from different scales are interpolated into the same size and concatenated on the channel dimension. S2 always resizes images into square, regardless of the original aspect ratio. This can cause distortion, particularly for images that are either tall and narrow or short and wide. To address this, we propose Dynamic-S2, which adaptively processes images with varying aspect ratios. Dynamic-S2 follows the approach of S2 but, at the largest image scale, instead of resizing to square, it adjusts the image dimensions to the closest size that maintains the original aspect ratio and is divisible by 4482 tiles. This is inspired by the dynamic resolution strategy in InternVL [15]. After processing the tiles, the feature maps from all scales are interpolated to match the size of the largest scale and concatenated. Equipped with Dynamic-S2, the model benefits from high-resolution information from the image, resulting in up to 30% accuracy improvements on text-heavy benchmarks  (Table 1)  . Our goal, then, shifts to compressing the spatial tokens. VILA [2] finds that applying simple 22 spatial-to-channel (STC) reshape can reduce the token count by factor of 4 without sacrificing accuracy. However, pushing this further results in notable drop in performance: i.e., nearly 10% decrease in accuracy on DocQA, when reducing the number of minimal tiles and increasing the STC to 33. We hypothesize that more aggressive reductions make the projector significantly harder to train. To address this, we introduce an additional visual encoder pre-training stage to jointly tune the vision encoder and projectors. This helps recover most of the accuracy loss from spatial token 4 Figure 4 Model architecture. regardless of their original size or aspect ratio, and samples up to 14 frames from videos. Both spatial resizing and temporal sampling will introduce significant loss of information, limiting the models capability to effectively process larger images and longer videos. This can also be observed in Table 8 and Table 9, where VILA lags behind leading VLMs, especially on text-heavy and long-video benchmarks. In this paper, we advocate for the scale-thencompress paradigm: first, we scale up the spatial/temporal resolutions to improve accuracy, and then we compress the visual tokens to improve efficiency. Scaling resolutions up improves the performance ceiling, but doing so alone will significantly increase the computational cost. For example, doubling the resolution will double the number of visual tokens, which will increase both training and inference costs by more than 2, as self-attention scales quadratically with the number of tokens. We can then cut this cost down by compressing spatial/temporal tokens. Compressed visual tokens have higher information density, allowing us to preserve or even This is the configuration for VILA-1.5 40B. Their other variants, such as VILA-1.5 3B, only use 384384 resolution and 8 frames. NVILA: Efficient Frontier Visual Language Models Table 1 Spatial scale-then-compress. Increasing the spatial resolution with Dynamic-S2 can greatly improve the models accuracy, particularly on text-heavy benchmarks. Compressing the visual tokens with spatial pooling can effectively reduce both the number of tiles and tokens per tile, with moderate accuracy loss. This loss can be further reduced by adding an additional visual encoder pre-training (VEP) stage. In this and following tables, Image-10 refers to the average validation scores from the 10 benchmarks listed in Table 8. Spatial Pooling #Tokens/Tile #Tiles/Image AI2D DocVQA TextVQA Image-10 Baseline (VILA-1.5) Scale (Dynamic-S2) Scale + Compress Scale + Compress + VEP Alternative Designs TokenLearner Perceiver Resampler 2 22 33 33 256 (=1616) 256 (=1616) 121 (=1111) 121 (=1111) 121 121 9-12 1-12 1-12 1-12 1-12 87.0 90.1 87.4 89.8 90.0 76.8 61. 91.1 82.3 88.8 86.5 71.8 67.5 77.0 74.1 76.1 75.6 65.3 61. 71.5 67.1 70.8 69.8 59.4 Table 2 Temporal scale-then-compress. Scaling up the temporal resolution can improve the models video understanding performance. Compressing the visual tokens with temporal averaging can effectively reduce the number of tokens with only marginal accuracy drop. #Frames Temporal Pooling #Tokens/Video Video-MME (w/o sub.) Short Medium Long Overall Baseline (VILA-1.5) Scale Scale + Compress 8 32 32 Scale + Compress 1 1 4 8 2048 (=1628) 8192 (=16232) 2048 (=16232/4) 65. 73.2 73.7 53.8 58.9 56.7 47.7 50.9 50.0 55. 61.0 60.1 8192 (=162256/8) 75.0 62.2 54.8 64. reduction, achieving 2.4 speedup in both training and inference. There are many alternative designs for spatial token compression, such as TokenLearner from RT-1 [6] and Perceiver Resampler from MiniCPM-V [16]. However, with the same token reduction ratio, these learnable compression methods surprisingly do not perform better than the simple spatial-to-channel design, even with an additional stage 1.5. We believe this is more of an optimization problem and is beyond the scope of this paper. 2.1.2. Temporal Scale-Then-Compress For temporal scaling, we simply increase the number of uniformly sampled frames from the input video. Following previous methods [17], we train the model with additional video-supervised fine-tuning (SFT) to extend its capability to process more frames. From Table 9, extending the number of frames from 8 to 32 can increase the models accuracy on Video-MME by more than 5%. However, this will also increase the number of visual tokens by 4. Similar to spatial token compression, we will then reduce these visual tokens. Since there is intrinsic temporal continuity in the video, we adopt temporal averaging [18] for compression, which first partitions the frames into groups and then temporally pools visual tokens within each group. This will reduce temporal redundancy (since consecutive frames often contain similar information) while still retaining important spatiotemporal information. Empirically, compressing the visual tokens by 4 leads to an acceptable accuracy drop. When compared to the original baseline with the same number of tokens, the first scaled and then expanded result costs almost the same, but has much higher accuracy. We have also used this approach to further scale the number of frames and the compression ratio, leading to state-of-the-art 7B model on this benchmark (see Table 9). 2.2. Efficient Training While state-of-the-art VLMs boast impressive capabilities, training such VLM is often costly and compute-intensive. This section explores systemalgorithm co-design to enable efficient VLM training. On the algorithm front, we examine novel unsupervised dataset pruning method to streamline training data. At the system level, we investigate FP8 mixed precision for acceleration. We will need to run visual encoder for more frames, but this is usually not the runtime bottleneck. 5 NVILA: Efficient Frontier Visual Language Models Figure 5 DeltaLoss visualizations in NVILA training: Left, Middle, and Right sections show examples that are too easy, distracting, and helpful for training, respectively. 2.2.1. Dataset Pruning In order to improve model accuracy, previous work [19, 5, 20] kept grabbing high quality SFT datasets from various sources and can show improvement on Benchmark scores. However, not all data contributes equally to the model and continuous growth of datasets lead to much redundancy. In NVILA, we follow the Scale-Then-Compress concept to first increase our SFT dataset mixture and then trying to compress the dataset. However, selecting highquality examples from various sources is challenging. While there have been explorations of vision inputs [21, 22, 23] and text-only inputs [24, 25, 26], few studies have addressed this problem in VLM training, where images and texts are mixed during training. NVILAs training involves more than 100M data, making it necessary to prune the training set while maintaining accuracy. Inspired by recent works in knowledge distillation [27], we leverage DeltaLoss to score the training set: 洧냥 = 洧 洧녰=1 top-洧 { log 洧녷large(洧논) 洧녷small(洧논) } 洧논 洧냥洧녰 , (1) where 洧냥洧녰 is the 洧녰-th subset of the full fine-tuning datasets and 洧냥 is the pruned training set. 洧녷large(洧논) and 洧녷small(洧논) are the output probabilities on the answer tokens. The main motivation is to filter out examples that are either too easy or too hard. To elaborate, If both answer correctly or wrongly, log 洧녷large(洧논) 洧녷small(洧논) is close to 0. When the small model answers correctly but the becomes negative, large model fails, log 洧녷large(洧논) 洧녷small(洧논) Table 3 Dataset pruning. DeltaLoss consistently rivals other data selection methods and shows negligeble performance drop when pruning 50% of data. Method Image-10 MMMU DocVQA TextVQA 100% (baseline) 75.6 48.0 90.1 78.8 50% DeltaLoss [27] Cluster Pruning Random Pruning 30% DeltaLoss [27] Cluster Pruning Random Pruning 10% DeltaLoss [27] Cluster Pruning Random Pruning 75.5 74.5 74.0 74.0 73.5 73.1 72.4 72.2 72.0 48.1 47.8 47.6 47.8 47.7 47. 47.1 47.4 47.0 89.7 88.3 87.1 87.9 84.1 82.9 84.4 79.6 77.3 78.4 77.0 76.6 76.4 76.0 75. 74.5 73.2 72.6 suggesting these examples tend to distract learning and will eventually be forgotten by more powerful model. When the small model answers incorrectly but the large model solves it, log 洧녷large(洧논) is positive, 洧녷small(洧논) suggesting these examples provide strong supervision, as challenging for small models but learnable by larger ones. Thereby we can apply DeltaLoss to each sub-dataset and prune the training set with different ratios. To evaluate the data pruning criterion, we compare DeltaLoss and the random pruning baseline in Table 3. For random pruning, data is randomly selected and we run the results three times and report the average. For cluster pruning, we apply k-means NVILA: Efficient Frontier Visual Language Models Table 4 FP8 training. FP8 accelerates the training of NVILA while maintaining the accuracy, especially when gradient checkpointing (GC) is not enabled. In this table, the throughput results are obtained with the maximum achievable batch size (BS) on 64 H100 GPUs. VideoMME results come from an 8-frame setting and with subtitle information. Table 5 Fine-tuning. PEFT Study for Efficient VLM Finetuning. (BS=1, #image=1, VE image resolution=448, patch size=14, LLM context length=300, gradient checkpointing disabled) Training time is profiled on NVIDIA A100-80G for one forward+backward pass. GC BS Throughput MMMU Video-MME BF16 FP8 BF16 FP 4 16 30 36 199.2 (1.0) 390.1 (2.0) 491.7 (2.5) 579.9 (2.9) 47.9 47.0 47.8 47.7 52.9 53.0 53.1 53.0 clustering with siglip features and prune the data evenly across each centroid. Our experiments report the average performance across 10 benchmarks, with focus on key tasks to demonstrate the methods effectiveness. We examine three pruning threshold 10%, 30% and 50% and notice that DeltaLoss consistently outperforms the random baseline, especially on the GQA and DocVQA tasks the random pruning shows significant performance degradation while DeltaLoss stays accurate. We notice 50% is relatively safe threshold where the average score maintains competitive while the training can be speedup by 2. Thus we set the threshold to 50% for later experiments. 2.2.2. FP8 Training FP16 and BF16 [28] have become standard precisions for model training, since they offer acceleration without accuracy loss, supported natively by NVIDIA GPUs. With the advent of the NVIDIA Hopper and Blackwell architectures, new GPUs (such as H100 and B200) now provide native support for FP8, which has emerged as promising precision due to its potential for larger computational and memory efficiency. Many researchers have already applied FP8 to LLM training. NVIDIAs Transformer Engine performs matrix multiplications (GEMM) in FP8 precision, resulting in faster training speeds. FP8-LM [29] builds upon this by also quantizing the gradients, weight master copy, and first-order momentum into FP8, thereby reducing communication overhead and memory footprint. COAT [30] further compresses activations and the optimizers second-order momentum to enhance memory efficiency while maintaining accuracy. In this paper, we borrow the FP8 implementation from COAT [30] to accelerate the training of NVILA. One key difference between LLM and VLM training workloads lies in the variability of sequence lengths across batches. In LLM training, samples generally have uniform lengths, and increasing the batch size ViT LLM LoRA LoRA LoRA LN LoRA FT LoRA QLoRA QLoRA LN FT FT Accuracy (Avg of 5) Memory (GB) Throughput (iter/s) 71.8 71.4 70.1 70.8 70.9 77.7 20.1 19.2 21. 11.1 10.2 63.5 3.4 4.5 4.2 2. 3.1 6.1 Table 6 Quantization. The impact of quantization on models accuracy. While W4A16 quantization on LLM backbone may introduce small accuracy drop, W8A8 quantization on ViT is nearly lossless. AI2D MMMU VideoMME TTFT (s) NVILA-8B 91. + W4A16 LLM 90.9 90.9 + W8A8 ViT 50.7 49.2 49.3 63.9 62.0 62.1 0. 0.77 0.65 beyond certain point has minimal effect on training throughput. However, in VLM training, samples can vary significantly in length: video samples may require tens of thousands of tokens, image samples may need hundreds, and text-only samples require far fewer. As result, workloads with fewer tokens are generally underutilized and can benefit greatly from increasing the batch size. As shown in Table 4, applying FP8 to both weights and activations allows NVILA to increase the batch size from 4 to 16, resulting in 2 speedup. When gradient checkpointing is enabled, quantizing activations becomes less essential. Instead, we integrate the cross-entropy kernel from Liger [31] to reduce peak memory usage due to Qwens large vocabulary size. In this case, FP8 training can still provide 1.2 speedup compared to BF16 training. 2.3. Efficient Fine-Tuning Once foundation VLM is trained, domain-specific fine-tuning is needed to adapt the model for specialized tasks or domains. While fine-tuning effectively improves domain-specific vocabulary and concepts, conventional Parameter Efficient Fine-Tuning has been focusing on LLM and text-related tasks, but how to best fine-tune VLM remains less explored. In NVILA, we find that (i) The learning rate should be set differently for ViT and LLMs (ii) The tuning parts should be chosen dependently for different 7 NVILA: Efficient Frontier Visual Language Models downstream tasks. When fine-tuning the vision encoder (ViT) and language model (LLM) together using PEFT methods, we observe that the learning rate should be set differently for VE and LLM: the learning rate for the ViT part will be 5-50 smaller than that for the LLM part. On the other hand, we also observe that finetuning the vision encoder with Layernorm can achieve comparable performance as LoRA (Table. 5) while being more computational-efficient: it can reduce the training time by 25% compared to applying LoRA for the vision encoder. With the curated configuration setup, NVILA can be quickly fine-tuned to various downstream tasks under 24 GB memory with on-par performance. 2.4. Efficient Deployment VLMs are often integrated in edge applications as robotic where computational budget is tight. In this section, we will introduce our inference engine with quantization to accelerate the deployment. We develop specialized inference engine with quantization techniques to efficiently deploy NVILA. The inference process is divided into two phases: prefilling and decoding In the compute-bounded prefilling stage we first apply token compression techniques (Section 2.1) to reduce the inference workload for LLM backbone, after which the vision tower becomes the primary bottleneck, accounting for over 90% of the prefilling latency. To tackle this, we implement W8A8 quantization for the vision tower to reduce NVILAs Time-To-First-Token (TTFT) in this compute-bounded stage. For the memory-bounded decoding stage, we follow AWQ [32] for W4A16 quantization of the LLM backbone to accelerate. We further optimize the original AWQ implementation by introducing FP16 accumulation to the W4A16 GEMM kernels, resulting to total 1.7 kernel speedup without compromising accuracy. We attach detailed comparison in Figure. 6 3. Experiments 3.1. Implementation Details We follow five-stage pipeline to train NVILA: (1) projector initialization, (2) visual encoder pre-training, (3) token processor pre-training, (4) image instructiontuning, and (5) video instruction-tuning. Among them, Stages 1, 3, and 4 are also included in VILA training. The additional Stage 2 is used to recover the accuracy loss due to spatial token compression (as in Table 1), and the additional Stage 5 is helpful for extending the models long video understanding capability. We Table 7 Training recipe. Building upon VILA, we introduce two additional stages for NVILA: Stage 2, which focuses on pre-training the visual encoder to reduce performance loss due to spatial token compression, and Stage 5, which focuses on video instruction tuning to improve the models long video capability. Visual Encoder Projector Token Processor (MLP) (LLM) (ViT) Initial from [12] random from [13] Stage 1 Stage 2 Stage 3 Stage 4 Stage 5 frozen trainable frozen trainable trainable trainable trainable trainable trainable trainable frozen frozen trainable trainable trainable LR 110-3 510-5 510-5 210-5 210provide the detailed training recipe in Table 7 and data recipe in Table A1. Our implementation is built upon PyTorch 2.3.0 [33, 34] and Transformers 4.46.0 [35]. We use DeepSpeed 0.9.5 [36] to shard large models across devices and use gradient checkpointing to reduce memory usage. We adopt FlashAttention-2 [37] to accelerate training in both the LLM and visual encoder. We also implement functional-preserving, on-the-fly sequence packing to fuse samples with different lengths, which leads to an around 30% speedup. We train all models using 128 NVIDIA H100 GPUs with global batch size of 2048 across all stages. All optimizations are carried out using AdamW with no weight decay. We adopt cosine learning rate decay schedule with linear warmup for the first 3% of the schedule. The initial learning rate varies across stages, as detailed in Table 7. 3.2. Accuracy Results 3.2.1. Image Benchmarks As presented in Table 8, we conduct comprehensive evaluations across diverse range of image benchmarks: AI2D [38], ChartQA [39], DocVQA [40], InfographicVQA [41], MathVista [42], MMMU [43] (with zero-shot CoT), RealworldQA [44], SEED-Bench [45], TextVQA [46], and VQAv2 [47]. Our NVILA performs comparably to top opensource models in each size category, including Qwen2VL [3], InternVL [4], and Pixtral. For general visual question answering tasks (ChartQA, DocVQA, InfoVQA, TextVQA, VQAv2, Seed), NVILA-8B and NVILA-15B achieve competitive or even better results compared to proprietary models (GPT-4o, Gemini). On science related benchmarks (AI2D), NVILA-8B achieves state-of-the-art performance among opensource models. When scaling to 15B, NVILA demon8 NVILA: Efficient Frontier Visual Language Models Table 8 Image benchmark results. We mark the best performance bold and the second-best underlined. AI2D ChartQA DocVQA InfoVQA MathVista MMMU GPT-4o Claude 3.5 Sonnet Gemini 1.5 Pro LLaVA-1.5 VILA-1.5 Cambrian-1 LLaVA-OV Llama 3.2 InternVL2 Qwen2-VL NVILA-Lite NVILA LLaVA-1.5 VILA-1.5 Cambrian-1 Pixtral NVILA-Lite NVILA VILA-1.5 Cambrian-1 LLaVA-NeXT InternVL2 LLaVA-OV Llama 3.2 NVLM-D-1. test 94.2 94.7 94.4 7B 55.5 8B 76.6 8B 73.0 7B 81.4 11B 91.9 8B 83.8 7B 83.0 8B 91.0 8B 92.3 13B 61.1 13B 79.9 13B 73.6 12B 79.0 15B 92.0 15B 94.1 40B 88.9 34B 79.7 34B 40B 87. 72B 85.6 90B 92.3 78B 94.2 test 85.7 90.8 87.2 17.8 52.7 73.3 80.0 83.4 83.3 83.0 84.8 86. 18.2 59.5 73.8 81.8 81.8 test 92.8 85.2 93.1 28.1 40.6 77.8 87.5 88.4 91.6 94.5 91.7 93.7 30.3 58.6 76.8 90.7 90. test 79.2 74.3 81.0 25.8 25.9 41.6 68.8 74.8 76.5 67.9 70.7 29.4 30.4 50.8 69.3 86. 94.0 73.5 67.8 75.6 86.2 83.7 85.5 86.0 58.6 75.5 93.9 91.3 90.1 92. 38.4 46.0 78.7 74.9 testmini val/test/pro 63.8 67.7 63.9 25.6 36.7 49.0 63.2 51.5 58.3 58.2 64. 65.4 27.7 42.7 48.0 58.0 61.7 66.1 49.3 53.2 46.5 63.7 67.5 57.3 65.2 69.1/64.7/51.9 68.3/63.7/51.5 62.2/57.6/43. 35.7// 38.6/32.7/ 42.7// 48.8/42.8/24.1 50.7// 51.2/42.6/29.0 54.1/46.6/30.5 50.7/45.7/26.5 49.9/44.4/27.8 37.0// 37.9/33.6/ 40.0// 52.5// 58.7/51.8/33.7 56.7/51.8/33.8 51.9/46.9/25.0 49.7// 48.1/44.5/22.9 55.2/47.4/34.2 56.8/52.3/31.0 60.3//39.5 59.7/54.6/ RealWorldQA 75.4 60.1 70. 54.8 52.7 64.2 66.3 64.2 70.1 65.6 68.6 55.3 57.5 63.0 65.4 67.1 SEED TextVQA VQAv2 image 76.2 66.1 73.8 74.7 75.4 76.2 76.3 76.5 68.2 72.6 74.4 75.6 val 77.4 74.1 78.7 58.2 68.5 71.7 78.3 77. 84.3 78.1 80.1 61.3 65.0 72.8 75.7 77.3 testdev 78.7 70.7 80.2 78.5 83.0 81.2 84.0 75.2 76.7 82.9 85.0 85. 80.0 82.8 80.2 83.7 69.5 76.6 80.0 84.8 60.8 67.8 71. 71.9 69.7 69.1 75.3 75.9 78.2 75.4 73.6 76.7 69.5 83.0 80.5 82.1 84.3 83.8 83.7 85.2 85.4 strates competitive performance with proprietary models. Further, on reasoning and knowledge benchmarks such as MMMU, RealworldQA, and MathVista, scores improves more when the model size increase. For benchmarks that require OCR capability such as TextVQA, AI2D, ChartQA, DocVQA, InfoVQA, 8B model can also do great job. We also show few qualitative examples in Figure. 2-3 to demonstrate the OCR, reasoning and multi-image capability of NVILA model. 3.2.2. Video Benchmarks We evaluate our models on range of video understanding benchmarks, spanning short videos of few seconds to longer videos up to an hour in duration. Table 9 presents the performance of NVILA compared to baseline models. NVILA features long-context capability and can process up to 256 frames. With the scale-then-compress design, NVILA-8B achieves impressive results, setting new state-of-the-art performance across all benchmarks. Notably NVILA reaches performance levels comparable to GPT-4o mini with only 8B parameters and outperforms many larger models. 3.3. Efficiency Results NVILA achieves competitive performance on image and video benchmarks while maintaining efficiency through scale-then-compress. Architecturally, We initially scale up to native resolution (112 more tiles), then compress tokens by 2.4, achieving higher accuracy with slightly more tokens than previous solutions. Dataset-wise, we curate diverse 10M sample dataset, compress it using DeltaLoss, and prune to high-quality 5M subset, consistently outperforms LlaVa-Onevision, which trained on 8M+ data. Besides, we integrate FP8 for acceleration, optimize learning rates for fine-tuning, and use W8A8 format to improve latency and throughput. These full-stack optimizations enable NVILA to train with fewer resources while achieving better performance, less memory usage, and faster inference. We compare NVILAs inference performance against Qwen2-VL [3] as shown in Figure 6. For fair comparison, both models process video inputs by sampling 64 frames, with all experiments conducted on single NVIDIA RTX 4090 GPU. Qwen2-VL is quantized to W4A16 and deployed with vLLM [48], LLM/VLM serving engine with state-of-the-art inference speed. For NVILA, we quantize the LLM 9 NVILA: Efficient Frontier Visual Language Models Table 9 Video benchmark results. ActivityNet-QA LongVideoBench MLVU MVBench NExT-QA Video-MME #F acc. score GPT-4o mini GPT-4o LLaVA-NeXT-Video 7B Video-XL LLaVA-OneVision Oryx-1.5 LongVILA LongVU Qwen2-VL 32 7B 2048 7B 32 7B 128 7B 256 7B 1fps 7B 2fps 8B 256 8B 256 NVILA-Lite NVILA 61.9 53.5 56.6 59.5 3.2 val 56.5 66. 43.5 49.5 56.5 56.3 57.1 55.6 test 58.8 66.7 43.5 51.3 56.8 m-avg test mc w/o sub. w/ sub. 64.6 64.9 64.7 67.5 65.4 69.2 33.7 55.3 56.7 67.6 67.1 66.9 67.0 77.2 79.4 81.8 80.7 79.6 64.8 71.9 46.5 55.5 58.2 58.8 60.1 60.6 63.3 60.9 68.9 77.2 61.0 61.5 64.2 65.1 69.0 68. 60.9 3.7 57.7 58.7 70.1 68. 82.2 64.2 70.0 Table 10 Temporal localization on ActivityNet. Mean IoU Precision @ 0.5 LITA-7B LITA-13B NVILA-8B (Ours) 24.1 28.6 34.8 21.1 25.9 32.1 Table 11 Robotic navigation on the R2R ValUnseen split. We report standard metrics in VLN-CE: Navigation Error (NE), Oracle Success Rate (OS), Success Rate (SR), and Success-weighted Path Length (SPL). backbone to W4A16 and vision tower to W8A8. With our specialized inference engine, NVILA achieves up to 2.2 speedup in prefilling stage and up to 2.8 higher decoding throughput over Qwen2-VL. R2R Val-Unseen Method NE OS SR SPL Seq2Seq-RGB [49] CMA-RGB [49] NaVid [7] NVILA (Ours) 11.2 9.55 5.47 6.72 12.2 14.8 49.0 0.0 0.0 37.0 0.0 0.0 35.0 55. 42.5 36.6 4. More Capabilities 4.1. Temporal Localization Following LITA, we also add support for temporal localization in NVILA. We add discrete time tokens to indicate the timestamps in the video, and use the smoothed cross entropy loss to train the model. From the results in Table 10, we can clearly see that NVILA substantially outperforms all baselines for all metrics. 4.2. Robotic Navigation NVILA can serve as strong foundation for robotic agents in Vision-Language Navigation [49] and empower real-time deployment on non-server devices. At each time step 洧노, the agent receives language instruction and video observation, plans the next action, and transitions to the next state 洧노+1, where it receives new observation. NVILAs efficient and flexible handling of multi-frame inputs enables seamless integration of historical and current observations into VLMs. The NaVILA framework [50] introduces tailored navigation prompt and fine-tunes NVILA using navigation-specific SFT data curated from the simulator [51]. Quantitative results in Table 11 show that NVILAs straightforward design achieves state-of-theart results on VLN-CE. Visual results of real-time deployment of the navigation model based on NVILA-8B on single laptop GPU for navigation tasks are presented in Fig. 7. The entire system can operate seamlessly with an end-to-end (cameraGPUaction) pipeline running at 1Hz. 4.3. Medical Multi-Modal VILA-M3 NVILA also offers transformative potential in the medical domain. Such integration promises advancements in diagnostic accuracy, clinical decision-making, and data interpretation. The NVILA-M3 framework [9] introduces novel approach by integrating multiple domain-expert models tailored to specific medical tasks, such as image segmentation and classification  (Fig. 8)  . These expert models are designed to extract and interpret intricate features that are otherwise difficult for general VLMs to discern. By coupling these specialized models with vision-language learning paradigm, NVILAM3 achieves enhanced performance, facilitating the learning of nuanced relationships between visual inputs and their textual annotations. This integration NVILA: Efficient Frontier Visual Language Models Figure 6 NVILA demonstrates superior inference efficiency over the Qwen2-VL model [3] for both image and video understanding tasks. We benchmark NVILA-7B against Qwen2-VL-7B. Qwen2-VL-7B is served by vLLM [48] for W4A16 LLM quantization, while NVILA is quantized and deployed with our specialized inference engine. Specifically, we ablate the efficiency gains achieved with different optimization techniques we introduced in NVILA. NVILA demonstrates 1.6-2.2 faster prefilling and up to 2.8 higher decoding throughput compared to Qwen2-VL. Table 12 Performance of best M3 model on key benchmarks is shown. Task-specific SOTA baselines and datasets are described in the experiments section [9]. Datasets where * is shown for Med-Gemini are either if subset is used or pre-processing details have not been disclosed. Metrics for VQA is accuracy, for report generation ROUGE score and for classification F1 score have been utilized Model VQA Rep. Gen. Classif. Dataset Rad Path CXR CheXpert Med-Gemini 78.8* 83.3 Task Spcf. SOTA 84.2 91.7 NVILA-M3 (ours) 90.4 92.7 28.3* 30.6 32. 48.3 51.5 61.0 not only improves task-specific outcomes but also sets foundation for the development of more robust and context-aware VLMs in the healthcare domain. NVILA-M3 indicated that an overall improvement of 9% can be achieved via usage of expert models over existing SOTA, few key results can be observed in Table. 12. This underscores the importance of leveraging domain expertise to bridge the gap between generalized AI capabilities and the demands of specialized applications, demonstrating the potential for VLMs to revolutionize fields where precision and specificity are paramount. 5. Related Work 5.1. Visual Language Models VLMs, especially proprietary ones, have advanced rapidly over the past two years. For example, OpenAI has upgraded from GPT-4V [52] to GPT-4o [10], achieving 510% performance gain across image and video QA benchmarks. Google has extended the context length to 1M in Gemini Pro 1.5 [53], significant improvement over Gemini 1.0 [54]. It now ranks at the top of the Video-MME leaderboard [55] for long video understanding. Anthropic has released Claude 3.5 [11], which demonstrates better benchmark scores than GPT-4o, showcasing notable improvements over Claude 3 [56]. Other proprietary models have similar advancements, such as Apples upgrade from MM1 to MM1.5 [57] and xAIs upgrade from Grok-1.5 [44] to Grok-2 [58]. Meanwhile, open-source VLMs continue to evolve, improving at both the system/framework level [59] and the algorithm/recipe level [2], progressively narrowing the performance gap between proprietary and open-source models [17, 60, 61, 62]. These recent advancements have led many open VLM models to claim performance levels comparable to, or even exceeding, leading proprietary models such as GPT-4V and GPT-4o. Some representative examples include InternVL2 [4], Qwen2-VL [3], LLaVA-OneVision [5], 11 NVILA: Efficient Frontier Visual Language Models Figure 7 NVILA deployed as Vision-Language Navigation agent, navigating environments using language instructions and visual observations (Top: simulation, Bottom: real-world). The real-world setup features Unitree Go2 robot equipped with LiDAR sensor at the base of its head and an Intel RealSense Camera mounted on top. On the server side, an RTX 4090 GPU powers the NVILA-8B model, configured with an 8-frame context length for action generation. Llama 3.2 Vision [63], Molmo [64], NVLM [62], and MiniCPM-V [16]. Despite significant advancements in model performance, much less focus has been placed on enhancing the efficiency of training, inference, and fine-tuning for these models. This paper aims to explore how to develop VLMs that are not only highly accurate but also optimized for end-to-end efficiency. 5.2. Efficiency Prior works such as [65, 66, 67, 68, 69, 70, 71, 72] have explored token reduction techniques in both spatial and temporal dimensions. However, none have focused on reducing the number of tokens for frontier Vision-Language Model (VLM). For dataset pruning, promising approaches have been proposed for selecting pretraining data for Large Language Models (LLMs), such as domain-mixing [73], sample-wise data selection [25, 74], and theory-driven optimal selection [26]. In this work, we specifically focus on pruning supervised fine-tuning (SFT) datasets for VLMs. Regarding low-precision training, FP8 training [75, 76] has gained popularity for LLMs, yet no prior work has demonstrated its feasibility for VLMs without sacrificing accuracy. Techniques such as pruning, distillation, and quantization are commonly applied to LLMs. Figure 8 Overview of M3 depicting integration of domain expert model with NVILA. Expert model cards are information for NVILA to decide which expert model to call and then utilize results before providing response to the user. Top: Reference model cards can be seen and multiple use-case queries that user could present such as segmentation and identification of tumor. Bottom: With guidance of expert models M3 can provide more accurate responses [77, 78] apply pruning/distillation to LLM. However, their application to VLMs presents an open question: Should an LLM be pruned or distilled first before integrating vision encoder, or should the VLM itself be pruned or distilled after training? Similarly, quantization techniques like AWQ [32] and GPTQ [79] are well-documented for LLMs, and VILA [2] has shown that AWQ can be directly applied to VLMs. However, little attention has been given to quantizing vision encoders, which becomes critical when handling higher-resolution images or videos due to the increased computational demands. Parameter-efficient fine-tuning methods such as LoRA [80], DoRA [81], QLoRA [82], and GaLoRA [83] are widely used for LLMs to reduce memory requirements. However, for VLMs, which combine vision encoder with an LLM, efficient fine-tuning techniques are still underexplored. Addressing this gap is crucial for advancing VLM fine-tuning with limited computational resources. 6. Conclusion This paper introduces NVILA, family of open VLMs designed to strike an optimal balance between efficiency and accuracy. By adopting the scale-then12 NVILA: Efficient Frontier Visual Language Models compress paradigm, NVILA can efficiently process high-resolution images and long videos while maintaining high accuracy. We also systematically optimize its efficiency across the entire lifecycle, from training to fine-tuning to inference. NVILA delivers performance that matches or exceeds current leading VLMs, while being significantly more resource-efficient. Moreover, NVILA opens up new possibilities for applications such as temporal localization, robotic navigation, and medical imaging. We will make our models available soon. We hope NVILA can empower researchers and developers to fully unlock its potential across wide range of applications and research domains."
        },
        {
            "title": "References",
            "content": "[1] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual Instruction Tuning. In Conference on Neural Information Processing Systems (NeurIPS), 2024. [2] Ji Lin, Hongxu Yin, Wei Ping, Yao Lu, Pavlo Molchanov, Andrew Tao, Huizi Mao, Jan Kautz, Mohammad Shoeybi, and Song Han. VILA: On Pretraining for Visual Language Models. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [3] Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, Yang Fan, Kai Dang, Mengfei Du, Xuancheng Ren, Rui Men, Dayiheng Liu, Chang Zhou, Jingren Zhou, and Junyang Lin. Qwen2-VL: Enhancing Vision-Language Models Perception of the World at Any Resolution. arXiv:2409.12191, 2024. [4] Zhe Chen, Jiannan Wu, Wenhai Wang, Weijie Su, Guo Chen, Sen Xing, Muyan Zhong, Qinglong Zhang, Xizhou Zhu, Lewei Lu, Bin Li, Ping Luo, Tong Lu, Yu Qiao, and Jifeng Dai. InternVL: Scaling up Vision Foundation Models and Aligning for Generic Visual-Linguistic Tasks. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [5] Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Peiyuan Zhang, Yanwei Li, Ziwei Liu, and Chunyuan Li. LLaVA-OneVision: Easy Visual Task Transfer. arXiv:2408.03326, 2024. [6] Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, Ofir Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. RT-1: Robotics Transformer for Real-World Control at Scale. In Robotics: Science and Systems (RSS), 2022. [7] Jiazhao Zhang, Kunyu Wang, Rongtao Xu, Gengze Zhou, Yicong Hong, Xiaomeng Fang, Qi Wu, Zhizheng Zhang, and Wang He. NaVid: Videobased VLM Plans the Next Step for Vision-andIn Robotics: Science and Language Navigation. Systems (RSS), 2024. [8] Xiaoyu Tian, Junru Gu, Bailin Li, Yicheng Liu, Yang Wang, Zhiyong Zhao, Kun Zhan, Peng Jia, Xianpeng Lang, and Hang Zhao. DriveVLM: The Convergence of Autonomous Driving and Large VisionLanguage Models. In Conference on Robot Learning (CoRL), 2024. [9] Vishwesh Nath, Wenqi Li, Dong Yang, Andriy Myronenko, Mingxin Zheng, Yao Lu, Zhijian Liu, Hongxu Yin, Yee Man Law, Yucheng Tang, et al. Vila-m3: Enhancing vision-language models with medical expert knowledge. arXiv preprint arXiv:2411.12915, 2024. [10] OpenAI. GPT-4o, 2024. [11] Anthropic. Claude 3.5, 2024. [12] Xiaohua Zhai, Basil Mustafa, Alexander Kolesnikov, and Lucas Beyer. Sigmoid Loss for Language Image Pre-Training. In IEEE/CVF International Conference on Computer Vision (ICCV), 2023. [13] An Yang, Baosong Yang, Binyuan Hui, Bo Zheng, Bowen Yu, Chang Zhou, Chengpeng Li, Chengyuan Li, Dayiheng Liu, Fei Huang, Guanting Dong, Haoran Wei, Huan Lin, Jialong Tang, Jialin Wang, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Ma, Jianxin Yang, Jin Xu, Jingren Zhou, Jinze Bai, Jinzheng He, Junyang Lin, Kai Dang, Keming Lu, Keqin Chen, Kexin Yang, Mei Li, Mingfeng Xue, Na Ni, Pei Zhang, Peng Wang, Ru Peng, Rui Men, Ruize Gao, Runji Lin, Shijie Wang, Shuai Bai, Sinan Tan, Tianhang Zhu, Tianhao Li, Tianyu Liu, Wenbin Ge, Xiaodong Deng, Xiaohuan Zhou, Xingzhang Ren, Xinyu Zhang, Xipin Wei, Xuancheng Ren, Xuejing Liu, Yang Fan, Yang Yao, Yichang Zhang, Yu Wan, Yunfei Chu, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, Zhifang Guo, and Zhihao Fan. Qwen2 Technical Report. arXiv:2407.10671, 2024. [14] Baifeng Shi, Ziyang Wu, Maolin Mao, Xin Wang, and Trevor Darrell. When Do We Not Need Larger Vision Models? In European Conference on Computer Vision (ECCV), 2024. [15] Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, Ji Ma, Jiaqi Wang, Xiaoyi Dong, Hang Yan, Hewei Guo, Conghui He, 13 NVILA: Efficient Frontier Visual Language Models Botian Shi, Zhenjiang Jin, Chao Xu, Bin Wang, Xingjian Wei, Wei Li, Wenjian Zhang, Bo Zhang, Pinlong Cai, Licheng Wen, Xiangchao Yan, Min Dou, Lewei Lu, Xizhou Zhu, Tong Lu, Dahua Lin, Yu Qiao, Jifeng Dai, and Wenhai Wang. How Far Are We to GPT-4V? Closing the Gap to Commercial Multimodal Models with Open-Source Suites. arXiv:2404.16821, 2024. [16] Yuan Yao, Tianyu Yu, Ao Zhang, Chongyi Wang, Junbo Cui, Hongji Zhu, Tianchi Cai, Haoyu Li, Weilin Zhao, Zhihui He, Qianyu Chen, Huarong Zhou, Zhensheng Zou, Haoye Zhang, Shengding Hu, Zhi Zheng, Jie Zhou, Jie Cai, Xu Han, Guoyang Zeng, Dahai Li, Zhiyuan Liu, and Maosong Sun. MiniCPM-V: GPT-4V Level MLLM on Your Phone. arXiv:2408.01800, 2024. [17] Fuzhao Xue, Yukang Chen, Dacheng Li, Qinghao Hu, Ligeng Zhu, Xiuyu Li, Yunhao Fang, Haotian Tang, Shang Yang, Zhijian Liu, Ethan He, Hongxu Yin, Pavlo Molchanov, Jan Kautz, Linxi Fan, Yuke Zhu, Yao Lu, and Song Han. LongVILA: Scaling Long-Context Visual Language Models for Long Videos. arXiv:2408.10188, 2024. [18] Limin Wang, Yuanjun Xiong, Zhe Wang, Yu Qiao, Dahua Lin, Xiaoou Tang, and Luc Van Gool. Temporal Segment Networks: Towards Good Practices for Deep Action Recognition. In European Conference on Computer Vision (ECCV), 2016. [19] Shengbang Tong, Ellis Brown, Penghao Wu, Sanghyun Woo, Manoj Middepogu, Sai Charitha, Jihan Yang, Shusheng Yang, Adithya Iyer, Xichen Pan, Austin Wang, Rob Fergus, Yann LeCun, and Saining Xie. Cambrian-1: Fully Open, VisionCentric Exploration of Multimodal LLMs. In Conference on Neural Information Processing Systems (NeurIPS), 2024. [20] Hugo Lauren칞on, L칠o Tronchon, Matthieu Cord, and Victor Sanh. What Matters When Building Vision-Language Models? In Conference on Neural Information Processing Systems (NeurIPS), 2024. [21] Cody Coleman, Christopher Yeh, Stephen Mussmann, Baharan Mirzasoleiman, Peter Bailis, Percy Liang, Jure Leskovec, and Matei Zaharia. Selection via Proxy: Efficient Data Selection for Deep Learning. In International Conference on Learning Representations (ICLR), 2020. [22] Hu Xu, Saining Xie, Xiaoqing Ellen Tan, Po-Yao Huang, Russell Howes, Vasu Sharma, Shang-Wen Li, Gargi Ghosh, Luke Zettlemoyer, and Christoph Feichtenhofer. Demystifying CLIP Data. In International Conference on Learning Representations (ICLR), 2024. [23] Amro Abbas, Kushal Tirumala, D치niel Simig, Surya Ganguli, and Ari Morcos. SemDeDup: DataEfficient Learning at Web-Scale through Semantic Deduplication. arXiv:2303.09540, 2023. via Document De-Duplication and Diversification. In Conference on Neural Information Processing Systems (NeurIPS), 2023. [25] Mengzhou Xia, Sadhika Malladi, Suchin Gururangan, Sanjeev Arora, and Danqi Chen. LESS: Selecting Influential Data for Targeted Instruction Tuning. In International Conference on Machine Learning (ICML), 2024. [26] Yuxian Gu, Li Dong, Hongning Wang, Yaru Hao, Qingxiu Dong, Furu Wei, and Minlie Huang. Data Selection via Optimal Control for Language Models. arXiv:2410.07064, 2024. [27] Yuxian Gu, Hao Zhou, Fandong Meng, Jie Zhou, and Minlie Huang. MiniPLM: Knowledge Distillation for Pre-Training Language Models. arXiv:2410.17215, 2024. [28] Dhiraj Kalamkar, Dheevatsa Mudigere, Naveen Mellempudi, Dipankar Das, Kunal Banerjee, Sasikanth Avancha, Dharma Teja, Nataraj Jammalamadaka, Jianyu Huang, Hector Yuen, Jiyan Yang, Jongsoo Park, Alexander Heinecke, Evangelos Georganas, Sudarshan Srinivasan, Abhisek Kundu, Misha Smelyanskiy, Bharat Kaul, and Pradeep Dubey. Study of BFLOAT16 for Deep Learning Training. arXiv:1905.12322, 2019. [29] Houwen Peng, Kan Wu, Yixuan Wei, Guoshuai Zhao, Yuxiang Yang, Ze Liu, Yifan Xiong, Ziyue Yang, Bolin Ni, Jingcheng Hu, Ruihang Li, Miaosen Zhang, Chen Li, Jia Ning, Ruizhe Wang, Zheng Zhang, Shuguang Liu, Joe Chau, Han Hu, and Peng Cheng. FP8-LM: Training FP8 Large Language Models. arXiv:2310.18313, 2023. [30] Haocheng Xi, Han Cai, Ligeng Zhu, Yao Lu, Kurt Keutzer, Jianfei Chen, and Song Han. COAT: Compressing Optimizer States and Activation for Memory-Efficient FP8 Training. arXiv:2410.19313, 2024. [31] Pin-Lun Hsu, Yun Dai, Vignesh Kothapalli, Qingquan Song, Shao Tang, Siyu Zhu, Steven Shimizu, Shivam Sahni, Haowen Ning, and Yanning Chen. Liger Kernel: Efficient Triton Kernels for LLM Training. arXiv:2410.10989, 2024. [32] Ji Lin, Jiaming Tang, Haotian Tang, Shang Yang, Wei-Ming Chen, Wei-Chen Wang, Guangxuan Xiao, Xingyu Dang, Chuang Gan, and Song Han. AWQ: Activation-Aware Weight Quantization for On-Device LLM Compression and Acceleration. In Conference on Machine Learning and Systems (MLSys), 2024. [33] Adam Paszke, Sam Gross, Francisco Massa, Adam Lerer, James Bradbury, Gregory Chanan, Trevor Killeen, Zeming Lin, Natalia Gimelshein, Luca Antiga, et al. PyTorch: An Imperative Style, HighPerformance Deep Learning Library. In Conference on Neural Information Processing Systems (NeurIPS), 2019. [24] Kushal Tirumala, Daniel Simig, Armen Aghajanyan, and Ari Morcos. D4: Improving LLM Pretraining [34] Jason Ansel, Edward Yang, Horace He, Natalia Gimelshein, Animesh Jain, Michael Voznesensky, 14 NVILA: Efficient Frontier Visual Language Models Bin Bao, Peter Bell, David Berard, Evgeni Burovski, et al. PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation. In ACM International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), 2024. [35] Thomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, R칠mi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-Art Natural Language Processing. In Conference on Empirical Methods in Natural Language Processing (EMNLP), 2020. [36] Jeff Rasley, Samyam Rajbhandari, Olatunji Ruwase, and Yuxiong He. DeepSpeed: System Optimizations Enable Training Deep Learning Models with Over 100 Billion Parameters. In ACM SIGKDD International Conference on Knowledge Discovery and Data Mining (KDD), 2020. [37] Tri Dao. FlashAttention-2: Faster Attention with Better Parallelism and Work Partitioning. In International Conference on Learning Representations (ICLR), 2024. [38] Aniruddha Kembhavi, Mike Salvato, Eric Kolve, Minjoon Seo, Hannaneh Hajishirzi, and Ali Farhadi. Diagram is Worth Dozen Images. In European Conference on Computer Vision (ECCV), 2016. [39] Ahmed Masry, Do Xuan Long, Jia Qing Tan, Shafiq Joty, and Enamul Hoque. ChartQA: Benchmark for Question Answering about Charts with Visual and Logical Reasoning. In Annual Meeting of the Association for Computational Linguistics (ACL), 2022. [40] Minesh Mathew, Dimosthenis Karatzas, and CV Jawahar. DocVQA: Dataset for VQA on Document Images. In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2021. [41] Minesh Mathew, Viraj Bagal, Rub칟n Tito, Dimosthenis Karatzas, Ernest Valveny, and CV Jawahar. InfographicVQA. In IEEE/CVF Winter Conference on Applications of Computer Vision (WACV), 2022. [42] Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts. In International Conference on Learning Representations (ICLR), 2024. [43] Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, Cong Wei, Botao Yu, Ruibin Yuan, Renliang Sun, Ming Yin, Boyuan Zheng, Zhenzhu Yang, Yibo Liu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MMMU: Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [44] xAI. Grok-1.5, 2024. [45] Bohao Li, Rui Wang, Guangzhi Wang, Yuying Ge, Yixiao Ge, and Ying Shan. SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [46] Amanpreet Singh, Vivek Natarajan, Meet Shah, Yu Jiang, Xinlei Chen, Dhruv Batra, Devi Parikh, and Marcus Rohrbach. Towards VQA Models That Can Read. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [47] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the in VQA Matter: Elevating the Role of Image Understanding in Visual Question Answering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017. [48] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient Memory Management for Large Language Model Serving with PagedAttention. In ACM Symposium on Operating Systems Principles (SOSP), 2023. [49] Jacob Krantz, Erik Wijmans, Arjun Majumdar, Dhruv Batra, and Stefan Lee. Beyond the NavGraph: Vision-and-Language Navigation in Continuous Environments. In European Conference on Computer Vision (ECCV), 2020. [50] An-Chieh Cheng, Yandong Ji, Zhaojing Yang, Xueyan Zou, Jan Kautz, Erdem Biyik, Hongxu Yin, Sifei Liu, and Xiaolong Wang. Navila: Legged robot vision-language-action model for navigation. arXiv preprint, 2024. [51] Peter Anderson, Qi Wu, Damien Teney, Jake Bruce, Mark Johnson, Niko S칲nderhauf, Ian Reid, Stephen Gould, and Anton Van Den Hengel. Visionand-Language Navigation: Interpreting visuallygrounded navigation instructions in real environments. In IEEE/CVF Computer Vision and Pattern Recognition Conference (CVPR), 2018. [52] OpenAI. GPT-4V, 2023. [53] Google. Gemini 1.5: Unlocking Multimodal Understanding Across Millions of Tokens of Context. arXiv:2403.05530, 2024. [54] Google. Gemini: Family of Highly Capable Multimodal Models. arXiv:2312.11805, 2023. [55] Chaoyou Fu, Yuhan Dai, Yondong Luo, Lei Li, Shuhuai Ren, Renrui Zhang, Zihan Wang, Chenyu Zhou, Yunhang Shen, Mengdan Zhang, Peixian Chen, Yanwei Li, Shaohui Lin, Sirui Zhao, Ke Li, Tong Xu, Xiawu Zheng, Enhong Chen, Rongrong Ji, and Xing Sun. Video-MME: The First-Ever Comprehensive Evaluation Benchmark of Multi-Modal LLMs in Video Analysis. arXiv:2405.21075, 2024. NVILA: Efficient Frontier Visual Language Models [56] Anthropic. Claude 3, 2024. [57] Haotian Zhang, Mingfei Gao, Zhe Gan, Philipp Dufter, Nina Wenzel, Forrest Huang, Dhruti Shah, Xianzhi Du, Bowen Zhang, Yanghao Li, Sam Dodge, Keen You, Zhen Yang, Aleksei Timofeev, Mingze Xu, Hong-You Chen, Jean-Philippe Fauconnier, Zhengfeng Lai, Haoxuan You, Zirui Wang, Afshin Dehghan, Peter Grasch, and Yinfei Yang. MM1.5: Methods, Analysis & Insights from Multimodal LLM Fine-Tuning. arXiv:2409.20566, 2024. [58] xAI. Grok-2, 2024. [59] Oleksii Kuchaiev, Jason Li, Huyen Nguyen, Oleksii Hrinchuk, Ryan Leary, Boris Ginsburg, Samuel Kriman, Stanislav Beliaev, Vitaly Lavrukhin, Jack Cook, et al. NeMo: Toolkit for Building AI Applications using Neural Modules. arXiv:1909.09577, 2019. [60] Yunhao Fang, Ligeng Zhu, Yao Lu, Yan Wang, Pavlo Molchanov, Jan Kautz, Jang Hyun Cho, Marco Pavone, Song Han, and Hongxu Yin. VILA2: VILA Augmented VILA. arXiv:2407.17453, 2024. [61] Min Shi, Fuxiao Liu, Shihao Wang, Shijia Liao, Subhashree Radhakrishnan, De-An Huang, Hongxu Yin, Karan Sapra, Yaser Yacoob, Humphrey Shi, et al. Eagle: Exploring The Design Space for Multimodal LLMs with Mixture of Encoders. arXiv:2408.15998, 2024. [62] Wenliang Dai, Nayeon Lee, Boxin Wang, Zhuoling Yang, Zihan Liu, Jon Barker, Tuomas Rintamaki, Mohammad Shoeybi, Bryan Catanzaro, and Wei Ping. NVLM: Open Frontier-Class Multimodal LLMs. arXiv:2409.11402, 2024. [63] Meta. Llama 3, 2024. [64] Matt Deitke, Christopher Clark, Sangho Lee, Rohun Tripathi, Yue Yang, Jae Sung Park, Mohammadreza Salehi, Niklas Muennighoff, Kyle Lo, Luca Soldaini, Jiasen Lu, Taira Anderson, Erin Bransom, Kiana Ehsani, Huong Ngo, YenSung Chen, Ajay Patel, Mark Yatskar, Chris Callison-Burch, Andrew Head, Rose Hendrix, Favyen Bastani, Eli VanderBilt, Nathan Lambert, Yvonne Chou, Arnavi Chheda, Jenna Sparks, Sam Skjonsberg, Michael Schmitz, Aaron Sarnat, Byron Bischoff, Pete Walsh, Chris Newell, Piper Wolters, Tanmay Gupta, KuoHao Zeng, Jon Borchardt, Dirk Groeneveld, Jen Dumas, Crystal Nam, Sophie Lebrecht, Caitlin Wittlif, Carissa Schoenick, Oscar Michel, Ranjay Krishna, Luca Weihs, Noah Smith, Hannaneh Hajishirzi, Ross Girshick, Ali Farhadi, and Aniruddha Kembhavi. Molmo and PixMo: Open Weights and Open Data for State-of-the-Art Multimodal Models. arXiv:2409.17146, 2024. [65] Daniel Bolya, Cheng-Yang Fu, Xiaoliang Dai, Peizhao Zhang, Christoph Feichtenhofer, and Judy Hoffman. Token Merging: Your ViT But Faster. In International Conference on Learning Representations (ICLR), 2023. [66] Liang Chen, Haozhe Zhao, Tianyu Liu, Shuai Bai, Junyang Lin, Chang Zhou, and Baobao Chang. An Image is Worth 1/2 Tokens After Layer 2: Plugand-Play Inference Acceleration for Large VisionLanguage Models. In European Conference on Computer Vision (ECCV), 2024. [67] Yizhe Xiong, Hui Chen, Tianxiang Hao, Zijia Lin, Jungong Han, Yuesong Zhang, Guoxin Wang, Yongjun Bao, and Guiguang Ding. PYRA: Parallel Yielding Re-Activation for Training-Inference Efficient Task Adaptation. In European Conference on Computer Vision (ECCV), 2024. [68] Xiaoqian Shen, Yunyang Xiong, Changsheng Zhao, Lemeng Wu, Jun Chen, Chenchen Zhu, Zechun Liu, Fanyi Xiao, Balakrishnan Varadarajan, Florian Bordes, Zhuang Liu, Hu Xu, Hyunwoo Kim, Bilge Soran, Raghuraman Krishnamoorthi, Mohamed Elhoseiny, and Vikas Chandra. LongVU: Spatiotemporal Adaptive Compression for Long Video-Language Understanding. arXiv:2410.17434, 2024. [69] Joonmyung Choi, Sanghyeok Lee, Jaewon Chu, Minhyuk Choi, and Hyunwoo Kim. vid-TLDR: Training Free Token Merging for Light-Weight Video Transformer. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [70] Peng Jin, Ryuichi Takanobu, Wancai Zhang, Xiaochun Cao, and Li Yuan. Chat-UniVi: Unified Visual Representation Empowers Large Language Models with Image and Video Understanding. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2024. [71] Mingze Xu, Mingfei Gao, Zhe Gan, Hong-You Chen, Zhengfeng Lai, Haiming Gang, Kai Kang, and Afshin Dehghan. SlowFast-LLaVA: Strong TrainingFree Baseline for Video Large Language Models. arXiv:2407.15841, 2024. [72] An-Chieh Cheng, Hongxu Yin, Yang Fu, Qiushan Guo, Ruihan Yang, Jan Kautz, Xiaolong Wang, and Sifei Liu. Spatialrgpt: Grounded spatial reasoning in vision language model. arXiv preprint arXiv:2406.01584, 2024. [73] Sang Michael Xie, Hieu Pham, Xuanyi Dong, Nan Du, Hanxiao Liu, Yifeng Lu, Percy Liang, Quoc Le, Tengyu Ma, and Adams Wei Yu. DoReMi: Optimizing Data Mixtures Speeds Up Language Model Pretraining. In Conference on Neural Information Processing Systems (NeurIPS), 2023. [74] Qianlong Du, Chengqing Zong, and Jiajun Zhang. MoDS: Model-Oriented Data Selection for Instruction Tuning. arXiv:2311.15653, 2023. [75] Maxim Fishman, Brian Chmiel, Ron Banner, and Daniel Soudry. Scaling FP8 Training to TrillionToken LLMs. arXiv:2409.12517, 2024. [76] Paulius Micikevicius, Dusan Stosic, Neil Burgess, Marius Cornea, Pradeep Dubey, Richard Grisenthwaite, Sangwon Ha, Alexander Heinecke, Patrick 16 NVILA: Efficient Frontier Visual Language Models Judd, John Kamalu, Naveen Mellempudi, Stuart Oberman, Mohammad Shoeybi, Michael Siu, and Hao Wu. FP8 Formats for Deep Learning. arXiv:2209.05433, 2022. [77] Saurav Muralidharan, Sharath Turuvekere Sreenivas, Raviraj Bhuminand Joshi, Marcin Chochowski, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro, Jan Kautz, and Pavlo Molchanov. Compact Language Models via Pruning and Knowledge Distillation. In Conference on Neural Information Processing Systems (NeurIPS), 2024. [78] Lucio Dery, Steven Kolawole, Jean-Fran칞ois Kagy, Virginia Smith, Graham Neubig, and Ameet Talwalkar. Structured Pruning of LLMs with only Forward Passes. arXiv:2402.05406, 2024. Everybody Prune Now: [79] Elias Frantar, Saleh Ashkboos, Torsten Hoefler, and Dan Alistarh. GPTQ: Accurate Post-Training Quantization for Generative Pre-Trained Transformers. In International Conference on Learning Representations (ICLR), 2023. [80] Edward Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen. LoRA: Low-Rank Adaptation of Large Language Models. In International Conference on Learning Representations (ICLR), 2021. [81] Shih-Yang Liu, Chien-Yi Wang, Hongxu Yin, Pavlo Molchanov, Yu-Chiang Frank Wang, Kwang-Ting Cheng, and Min-Hung Chen. Dora: Weightdecomposed low-rank adaptation. arXiv preprint arXiv:2402.09353, 2024. [82] Tim Dettmers, Artidoro Pagnoni, Ari Holtzman, and Luke Zettlemoyer. QLoRA: Efficient Finetuning of Quantized LLMs. In Conference on Neural Information Processing Systems (NeurIPS), 2024. [83] Jiawei Zhao, Zhenyu Zhang, Beidi Chen, Zhangyang Wang, Anima Anandkumar, and Yuandong Tian. GaLore: Memory-Efficient LLM Training by Gradient Low-Rank Projection. In International Conference on Machine Learning (ICML), 2024. [84] Guiming Hardy Chen, Shunian Chen, Ruifei Zhang, Junying Chen, Xiangbo Wu, Zhiyi Zhang, Zhihong Chen, Jianquan Li, Xiang Wan, and Benyou Wang. Allava: Harnessing gpt4v-synthesized data for lite vision-language model, 2024. [85] Hugo Lauren칞on, Andr칠s Marafioti, Victor Sanh, and L칠o Tronchon. Building and Better Understanding Vision-Language Models: Insights and Future Directions. arXiv:2408.12637, 2024. [86] Pablo Montalvo and Ross Wightman. Pdf association dataset (pdfa). https://huggingface.co/ datasets/pixparse/pdfa-eng-wds, 2024. Accessed: 2024-12-05. [87] Yipeng Sun, Zihan Ni, Chee-Kheng Chng, Yuliang Liu, Canjie Luo, Chun Chet Ng, Junyu Han, Errui Ding, Jingtuo Liu, Dimosthenis Karatzas, Icdar 2019 Chee Seng Chan, and Lianwen Jin. competition on large-scale street view text with partial labeling rrc-lsvt, 2019. [88] Chee Kheng Chng, Yuliang Liu, Yipeng Sun, Chun Chet Ng, Canjie Luo, Zihan Ni, ChuanMing Fang, Shuaitao Zhang, Junyu Han, Errui Ding, et al. Icdar2019 robust reading challenge on arbitraryshaped text-rrc-art. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15711576. IEEE, 2019. [89] Minwoo Byeon, Beomhee Park, Haecheon Kim, Sungjun Lee, Woonhyuk Baek, and Saehoon Kim. COYO-700M: Image-Text Pair Dataset, 2022. [90] Lin Chen, Jinsong Li, Xiaoyi Dong, Pan Zhang, Conghui He, Jiaqi Wang, Feng Zhao, and Dahua Lin. ShareGPT4V: Improving Large Multi-Modal Models with Better Captions. In European Conference on Computer Vision (ECCV), 2024. [91] Ahmed Masry, Parsa Kavehzadeh, Xuan Long Do, Enamul Hoque, and Shafiq Joty. Unichart: universal vision-language pretrained model for chart comprehension and reasoning. arXiv preprint arXiv:2305.14761, 2023. [92] Wanrong Zhu, Jack Hessel, Anas Awadalla, Samir Yitzhak Gadre, Jesse Dodge, Alex Fang, Youngjae Yu, Ludwig Schmidt, William Yang Wang, and Yejin Choi. Multimodal C4: An Open, BillionScale Corpus of Images Interleaved with Text. In Conference on Neural Information Processing Systems (NeurIPS), 2024. [93] Hugo Lauren칞on, L칠o Tronchon, Matthieu Cord, and Victor Sanh. What matters when buildarXiv preprint ing vision-language models? arXiv:2405.02246, 2024. [94] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. MSRVTT: Large Video Description Dataset for Bridging Video and Language. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2016. [95] Jonathan Krause, Justin Johnson, Ranjay Krishna, and Li Fei-Fei. Hierarchical Approach for Generating Descriptive Image. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017. [96] Justin Johnson, Bharath Hariharan, Laurens Van Der Maaten, Li Fei-Fei, Lawrence Zitnick, and Ross Girshick. CLEVR: Diagnostic Dataset for Compositional Language and Elementary Visual Reasoning. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017. [97] Ryota Tanaka, Kyosuke Nishida, and Sen Yoshida. VisualMRC: Machine Reading Comprehension on Document Images. In AAAI Conference on Artificial Intelligence (AAAI), 2021. [98] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: dataset for image captioning with reading comprehension. In Computer VisionECCV 2020: 16th European Conference, Glasgow, UK, August 2328, 2020, Proceedings, Part II 16, pages 742758. Springer, 2020. NVILA: Efficient Frontier Visual Language Models [99] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question answering by reading text in images. In 2019 international conference on document analysis and recognition (ICDAR), pages 947952. IEEE, 2019. [100] Ali Furkan Biten, Ruben Tito, Andres Mafla, Lluis Gomez, Mar칞al Rusinol, Ernest Valveny, CV Jawahar, and Dimosthenis Karatzas. Scene Text Visual Question Answering. In IEEE/CVF International Conference on Computer Vision (ICCV), 2019. [101] Jianfeng Kuang, Wei Hua, Dingkang Liang, Mingkun Yang, Deqiang Jiang, Bo Ren, and Xiang Bai. Visual Information Extraction in the Wild: Practical Dataset and End-to-end Solution. In International Conference on Document Analysis and Recognition (ICDAR), 2023. [102] Zheng Huang, Kai Chen, Jianhua He, Xiang Bai, Dimosthenis Karatzas, Shijian Lu, and CV Jawahar. Icdar2019 competition on scanned receipt ocr and information extraction. In 2019 International Conference on Document Analysis and Recognition (ICDAR), pages 15161520. IEEE, 2019. [103] Geewook Kim, Teakgyu Hong, Moonbin Yim, JeongYeon Nam, Jinyoung Park, Jinyeong Yim, Wonseok Hwang, Sangdoo Yun, Dongyoon Han, and Seunghyun Park. OCR-Free Document Understanding Transformer. In European Conference on Computer Vision (ECCV), 2022. [104] Lei Li, Yuqi Wang, Runxin Xu, Peiyi Wang, Xiachong Feng, Lingpeng Kong, and Qi Liu. Multimodal arxiv: dataset for improving scientific comprehension of large vision-language models. arXiv preprint arXiv:2403.00231, 2024. [105] Yanzhe Zhang, Ruiyi Zhang, Jiuxiang Gu, Yufan Zhou, Nedim Lipka, Diyi Yang, and Tong Sun. Llavar: Enhanced visual instruction tuning for text-rich image understanding. ArXiv preprint, abs/2306.17107, 2023. [106] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to Explain: Multimodal Reasoning via Thought Chains for Science Question Answering. In Conference on Neural Information Processing Systems (NeurIPS), 2022. [107] Stanislaw Antol, Aishwarya Agrawal, Jiasen Lu, Margaret Mitchell, Dhruv Batra, Lawrence Zitnick, and Devi Parikh. Vqa: Visual question answering. In Proceedings of the IEEE international conference on computer vision, pages 24252433, 2015. [108] Paul Lerner, Olivier Ferret, Camille Guinaudeau, Herv칠 Le Borgne, Romaric Besan칞on, Jos칠 Moreno, and Jes칰s Lov칩n Melgarejo. ViQuAE, Dataset for Knowledge-based Visual Question AnIn International swering about Named Entities. ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2022. [109] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jos칠 MF Moura, Devi Parikh, and Dhruv Batra. Visual Dialog. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2017. [110] Drew Hudson and Christopher Manning. GQA: New Dataset for Real-World Visual Reasoning and Compositional Question Answering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [111] Jiahui Gao, Renjie Pi, Jipeng Zhang, Jiacheng Ye, Wanjun Zhong, Yufei Wang, Lanqing Hong, Jianhua Han, Hang Xu, Zhenguo Li, and Lingpeng Kong. G-LLaVA: Solving Geometric Problem with MultiModal Large Language Model. arXiv:2312.11370, 2023. [112] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Mitigating Hallucination in Large Multi-Modal Models via Robust Instruction Tuning. In International Conference on Learning Representations (ICLR), 2024. [113] Licheng Yu, Patrick Poirson, Shan Yang, Alexander Berg, and Tamara Berg. Modeling Context in Referring Expressions. In European Conference on Computer Vision (ECCV), 2016. [114] Jiaqi Chen, Jianheng Tang, Jinghui Qin, Xiaodan Liang, Lingbo Liu, Eric Xing, and Liang Lin. Geoqa: geometric question answering benchmark towards multimodal numerical reasoning. arXiv preprint arXiv:2105.14517, 2021. [115] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. OK-VQA: Visual Question Answering Benchmark Requiring External Knowledge. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019. [116] Pan Lu, Liang Qiu, Kai-Wei Chang, Ying Nian Wu, Song-Chun Zhu, Tanmay Rajpurohit, Peter Clark, and Ashwin Kalyan. Dynamic Prompt Learning via Policy Gradient for Semi-Structured Mathematical Reasoning. In International Conference on Learning Representations (ICLR), 2023. [117] Xinyu Wang, Yuliang Liu, Chunhua Shen, Chun Chet Ng, Canjie Luo, Lianwen Jin, Chee Seng Chan, Anton van den Hengel, and Liangwei Wang. On the General Value of Evidence, and Bilingual Scene-Text Visual Question Answering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [118] Kushal Kafle, Brian Price, Scott Cohen, and Christopher Kanan. DVQA: Understanding Data Visualizations via Question Answering. In IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2018. [119] Keqin Chen, Zhao Zhang, Weili Zeng, Richong Zhang, Feng Zhu, and Rui Zhao. Shikra: Unleashing Multimodal LLMs Referential Dialogue Magic. arXiv:2306.15195, 2023. 18 NVILA: Efficient Frontier Visual Language Models [120] Tianyu Yu, Jinyi Hu, Yuan Yao, Haoye Zhang, Yue Zhao, Chongyi Wang, Shan Wang, Yinxv Pan, Jiao Xue, Dahai Li, Zhiyuan Liu, Hai-Tao Zheng, and Maosong Sun. Reformulating Vision-Language Foundation Models and Datasets Towards Universal Multimodal Assistants. arXiv:2310.00653, 2023. [121] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. Aligning Large Multi-Modal Model with Robust Instruction Tuning. arXiv:2306.14565, 2023. [122] Bo Zhao, Boya Wu, Muyang He, and Tiejun Huang. SVIT: Scaling Up Visual Instruction Tuning. arXiv:2307.04087, 2023. [123] Fuxiao Liu, Xiaoyang Wang, Wenlin Yao, Jianshu Chen, Kaiqiang Song, Sangwoo Cho, Yaser Yacoob, and Dong Yu. MMC: Advancing Multimodal Chart Understanding with Large-scale Instruction Tuning. In Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (NAACL-HLT), 2024. [124] Yangzhou Liu, Yue Cao, Zhangwei Gao, Weiyun Wang, Zhe Chen, Wenhai Wang, Hao Tian, Lewei Lu, Xizhou Zhu, Tong Lu, Yu Qiao, and Jifeng Dai. MMInstruct: High-Quality Multi-Modal Instruction Tuning Dataset with Extensive Diversity. arXiv:2407.15838, 2024. [125] Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew Dai, and Quoc Le. Finetuned Language Models are Zero-Shot Learners. In International Conference on Learning Representations (ICLR), 2021. [126] Xiang Yue, Xingwei Qu, Ge Zhang, Yao Fu, Wenhao Huang, Huan Sun, Yu Su, and Wenhu Chen. MAmmoTH: Building Math Generalist Models through Hybrid Instruction Tuning. In International Conference on Learning Representations (ICLR), 2024. [127] Databricks. Dolly, 2023. [128] Zheng Yuan, Hongyi Yuan, Chengpeng Li, Guanting Dong, Keming Lu, Chuanqi Tan, Chang Zhou, and Jingren Zhou. Scaling Relationship on Learning Mathematical Reasoning with Large Language Models. arXiv:2308.01825, 2023. [129] Xudong Xie, Ling Fu, Zhifei Zhang, Zhaowen Wang, and Xiang Bai. Toward Understanding WordArt: Corner-Guided Transformer for Scene Text Recognition. In European Conference on Computer Vision (ECCV), 2022. [130] Krishna Srinivasan, Karthik Raman, Jiecao Chen, Michael Bendersky, and Marc Najork. WIT: Wikipedia-based Image Text Dataset for Multimodal Multilingual Machine Learning. In International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR), 2021. [131] Jianhao Shen, Ye Yuan, Srbuhi Mirzoyan, Ming Zhang, and Chenguang Wang. Measuring VisionLanguage STEM Skills of Neural Models. In International Conference on Learning Representations (ICLR), 2024. [132] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. PathVQA: 30000+ Questions for Medical Visual Question Answering. arXiv:2003.10286, 2020. [133] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. SLAKE: SemanticallyLabeled Knowledge-Enhanced Dataset for Medical Visual Question Answering. In IEEE International Symposium on Biomedical Imaging (ISBI), 2021. [134] Zhou Yu, Dejing Xu, Jun Yu, Ting Yu, Zhou Zhao, Yueting Zhuang, and Dacheng Tao. ActivityNet-QA: Dataset for Understanding Complex Web Videos via Question Answering. In AAAI Conference on Artificial Intelligence (AAAI), 2019. [135] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just Ask: Learning to Answer Questions from Millions of Narrated Videos. In IEEE/CVF International Conference on Computer Vision (ICCV), 2021. [136] Luowei Zhou, Chenliang Xu, and Jason Corso. Towards automatic learning of procedures from web instructional videos. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 32, 2018. [137] Xin Wang, Jiawei Wu, Junkun Chen, Lei Li, YuanFang Wang, and William Yang Wang. Vatex: large-scale, high-quality multilingual dataset for video-and-language research. In Proceedings of the IEEE/CVF international conference on computer vision, pages 45814591, 2019. [138] Ruohong Zhang, Liangke Gui, Zhiqing Sun, Yihao Feng, Keyang Xu, Yuanhan Zhang, Di Fu, Chunyuan Li, Alexander Hauptmann, Yonatan Bisk, et al. Direct preference optimization of video large multimodal models from language model reward. arXiv preprint arXiv:2404.01258, 2024. [139] Yuanhan Zhang, Jinming Wu, Wei Li, Bo Li, Zejun Ma, Ziwei Liu, and Chunyuan Li. Video Instruction Tuning With Synthetic Data. arXiv:2410.02713, 2024. NVILA: Efficient Frontier Visual Language Models Table A1 Data recipe. Stage 1: Projector Alignment Feature Align LLaVA-CC3M-Pretrain [1] Stage 2: Vision Encoder Alignment Recaptioned Data ALLAVA [84] Document Docmatix [85] PDFA [86] OCR LSVT [87] ArT [88] Stage 3: Pre-Training Recaptioned Data COYO [89] (25M Subset and recaptioned by VILA2 [60]), ShareGPT4v-Pretrain [90] Document Docmatix [85] UniChart-Pretrain [91] Interleaved Data MMC4 [92] Stage 4: Image Instruction-Tuning Hybrid ShareGPT4V-SFT [90],Molmo(subset) [64],The Cauldron(subset) [93], Cambrian(subset) [19], LLaVA-OneVision(subset) [5] Captioning MSR-VTT [94], Image Paragraph Captioning [95], ShareGPT4V-100K [90] Reasoning CLEVR [96], NLVR, VisualMRC [97] Document DocVQA [40], UniChart-SFT [91], ChartQA [39] OCR TextCaps [98], OCRVQA [99], ST-VQA [100], POIE [101], SORIE [102], SynthDoG-en [103], TextOCR-GPT4V, ArxivQA [104], LLaVAR [105] General VQA ScienceQA [106], VQAv2 [107], ViQuAE [108], Visual Dialog [109], GQA [110] , Geo170K [111], LRVInstruction [112], RefCOCO [113], GeoQA [114], OK-VQA [115], TabMVP [116], EstVQA [117] Diagram & Dialogue DVQA [118], AI2D [38], Shikra [119], UniMM-Chat [120] Instruction LRV-Instruction [121], SVIT [122], MMC-Instruction [123], MM-Instruction [124] Text-only FLAN-1M [125], MathInstruct [126], Dolly [127], GSM8K-ScRel-SFT [128] Knowledge WordART [129], WIT [130], STEM-QA [131] Medical PathVQA [132], Slake [133], MedVQA [133] Video ActivityNet-QA [134], MSRVTT-QA [94], iVQA [135], Youcook2 [136], VaTeX [137], ShareGPTVideo [138] Stage 5: Video Instruction-Tuning Video LLaVA-Video-178K [139] Image LLaVA-OneVision(subset) [5]"
        }
    ],
    "affiliations": [
        "MIT",
        "NVIDIA",
        "Tsinghua University",
        "UC Berkeley",
        "UC San Diego",
        "University of Washington"
    ]
}