{
    "paper_title": "GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning",
    "authors": [
        "Bo Liu",
        "Xiangyu Zhao",
        "Along He",
        "Yidi Chen",
        "Huazhu Fu",
        "Xiao-Ming Wu"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Medical visual question answering aims to support clinical decision-making by enabling models to answer natural language questions based on medical images. While recent advances in multi-modal learning have significantly improved performance, current methods still suffer from limited answer reliability and poor interpretability, impairing the ability of clinicians and patients to understand and trust model-generated answers. To address this, this work first proposes a Thinking with Visual Grounding (ThinkVG) dataset wherein the answer generation is decomposed into intermediate reasoning steps that explicitly ground relevant visual regions of the medical image, thereby providing fine-grained explainability. Furthermore, we introduce a novel verifiable reward mechanism for reinforcement learning to guide post-training, improving the alignment between the model's reasoning process and its final answer. Remarkably, our method achieves comparable performance using only one-eighth of the training data, demonstrating the efficiency and effectiveness of the proposal. The dataset is available at https://huggingface.co/datasets/BoKelvin/GEMeX-ThinkVG."
        },
        {
            "title": "Start",
            "content": "5 2 0 2 2 2 ] . [ 1 9 3 9 7 1 . 6 0 5 2 : r GEMeX-ThinkVG: Towards Thinking with Visual Grounding in Medical VQA via Reinforcement Learning Bo Liu1, Xiangyu Zhao1, Along He2, Yidi Chen3, Huazhu Fu4, Xiao-Ming Wu1* 1Department of Data Science and Artificial Intelligence, The Hong Kong Polytechnic University, Hong Kong 2National Engineering Laboratory for Big Data System Computing Technology, Shenzhen University, China 3Department of Radiology, West China Hospital of Sichuan University, China 4Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore"
        },
        {
            "title": "Abstract",
            "content": "Medical visual question answering aims to support clinical decision-making by enabling models to answer natural language questions based on medical images. While recent advances in multi-modal learning have significantly improved performance, current methods still suffer from limited answer reliability and poor interpretability, impairing the ability of clinicians and patients to understand and trust model-generated answers. To address this, this work first proposes Thinking with Visual Grounding (ThinkVG) dataset wherein the answer generation is decomposed into intermediate reasoning steps that explicitly ground relevant visual regions of the medical image, thereby providing fine-grained explainability. Furthermore, we introduce novel verifiable reward mechanism for reinforcement learning to guide post-training, improving the alignment between the models reasoning process and its final answer. Remarkably, our method achieves comparable performance using only one-eighth of the training data, demonstrating the efficiency and effectiveness of the proposal. The dataset is available at https://huggingface.co/ datasets/BoKelvin/GEMeX-ThinkVG. 1. Introduction Medical Visual Question Answering (VQA) has emerged as promising paradigm for supporting clinical decisionmaking by enabling models to answer natural language questions based on medical images [16]. Recent advances in multi-modal learning have led to significant performance improvements across range of Medical VQA benchmarks [6, 8, 13, 19], especially in the era of large vision language models [3, 17, 34, 36]. However, despite these gains, existing methods remain limited in their ability to * Corresponding author. Figure 1. Interpretability comparison: (Top) Traditional answer with grounding provided by GEMeX: separate textual and visual prompts; (Bottom) Our integrated, more detailed thinking process for answer generation, which explicitly grounds evidence in specific regions (shown in the colored texts) of the medical image, i.e., anatomical areas that support inferences, thereby enhancing the understanding of questions and answers. provide interpretable and trustworthy responsesan essential requirement for real-world deployment in clinical settings. In particular, most current approaches generate answers without revealing the underlying reasoning process or the specific image regions that inform their decisions. This lack of transparency undermines user trust and hinders the integration of such systems into clinical workflows. recent work GEMeX [21] introduced new dataset in which each VQA triplet is paired with textual reason and corresponding visual region as an explanation, bridging this gap to some extent. However, the textual and visual prompts are independent and lack coherence. Moreover, the textual reason merely serves as an explanation of the answer rather than providing guidance on how to solve the question based on the image. Therefore, based on GEMeX, we propose new dataset GEMeXThinkVG that enhances interpretability in Medical VQA through introducing Thinking with Visual Grounding mechanism, as shown in Figure 1. Different from the traditional chain-of-thought reasoning [35] and recent approaches [15, 24] that rely solely on reinforcement learning to incentivize the thinking process for solving multimodal tasks, our ThinkVG explicitly decomposes the answer generation process into sequence of intermediate steps, along with grounding corresponding visual evidence within the medical image. By linking reasoning steps to spatially localized regions, we offer clinicians fine-grained explanation of how the model arrives at its conclusions. While leveraging the GEMeX-ThinkVG dataset for Medical VQA enables the modeling of explicit thinking, our empirical findings indicate that supervised fine-tuning (SFT) alone is insufficient to fully unleash the reasoning capabilities of the large vision language models (LVLMs). To address this, we adopt reinforcement learning (RL) framework for post-training that builds upon the SFT-tuned model. It introduces novel verifiable reward mechanism to incentivize alignment between the reasoning trajectory and relevant visual and textual cues, thereby enhancing both the accuracy of intermediate thinking steps and final answer generation. In contrast to existing methods that require large-scale datasets, our approach is data-efficient, achieving comparable performance using only one-eighth of the full training data. In summary, our contributions are threefold: We introduce GEMeX-ThinkVG, Thinking with Visual Grounding dataset for Medical VQA, which provides step-by-step reasoning grounded in specific anatomical regions of medical images, offering better interpretability for understanding the questions and answers. We perform supervised fine-tuning on the LVLM using our GEMeX-ThinkVG dataset, and further apply reinforcement learning with novel verifiable reward mechanism to incentivize reasoning abilities, enabling the generation of more accurate thinking paths and final answers. From comprehensive experiments, we demonstrate the effectiveness of the proposed dataset and reward mechanism that empower LVLM to achieve comparable performance with only one-eighth of the training data. 2. Related Work 2.1. Medical VQA Dataset Recent years have witnessed significant progress in medical visual question answering (VQA) through the development of specialized datasets targeting diverse clinical challenges. VQA-RAD [16] established the foundation with 3,000 question-answer pairs focused on radiology images. SLAKE [20] expanded the scope with over 14,000 manually annotated QA pairs spanning CT, MRI, and X-ray modalities. OmniMedVQA [8] further broadened coverage across multiple body regions and imaging modalities to enhance model generalization capabilities. For specialized tasks, PathVQA [6] offers 32,000 QAs on histopathology images. MIMIC-Diff-VQA [7] emphasizes differential diagnosis between paired X-rays. GEMeX [21] enhances VQA with both textual and visual explanations, facilitating better understanding of the answers for patients. However, none of them incorporate reasoning process for problemsolving, which hinders patients and junior doctors from fully understanding the questions and answers. To fill this gap, we propose thinking with visual grounding dataset, GEMeX-ThinkVG, in this work. 2.2. Reinforcement Learning Recently, the growing popularity of reasoning-focused large language models such as GPT-o1 [10] and DeepSeek-R1 [5] has brought increasing attention to the underlying mechanisms of reinforcement learning (RL). This interest has spurred extensive research and led to remarkable advancements in tasks such as mathematics [23, 32], code generation [12, 33], and hallucination detection [30, 37]. notable contribution is the Group Relative Policy Optimization (GRPO) algorithm proposed in DeepSeekMath [28], which significantly simplifies the training process. Unlike traditional RL algorithms like PPO [12] that require critic model to evaluate policy performance, GRPO compares groups of candidate responses directly, eliminating the need for an additional critic. GRPOs simplicity has inspired further exploration into the reasoning capabilities of large vision-language models (LVLMs) for multimodal and visual tasks [9, 22, 29]. In the medical domain, GRPO has also been applied to incentivize the reasoning abilities of LVLMs for tasks such as out-of-distribution detection [24] and visual question answering [15]. Unlike these works that merely elicit textual descriptions of visual content, we propose visually grounded reasoning paradigm that provides improved interpretability. 3. Construction of GEMeX-ThinkVG 3.1. Task Definition Unlike conventional chain-of-thought approaches used in general tasks such as VQA [31], which typically generate only textual rationales before outputting the final answer, we propose multimodal-level thinking process with visual grounding tailored for medical VQA. As illustrated in Figure 1, it decomposes the reasoning process into several steps with visual attention to specific image regions, aiming to deliver more detailed explanations and improved interpretability for patients and junior doctors. Formally, given question for image v, we hope to provide multi-modal thinking process involving visual grounding to attain answer via (cid:16) (cid:17) = {ri}Nr i=1, {tj}Nt j=1 , (1) where {ri}Nr of relevant textual descriptions. i=1 is set of image regions and {tj}Nt j=1 is set Figure 2. An illustration of the proposed uncertainty-driven multi-agent framework for generating the GEMeX-ThinkVG dataset. The left (orange) section depicts the data generation process, while the right (blue) section shows the uncertainty-driven quality assurance pipeline, i.e., the not sure option prompts the agent to engage in deeper reflection and more deliberate decision-making. 3.2. An Uncertainty-Driven Multi-Agent Framework As illustrated in Figure 2, we construct the ThinkVG dataset based on GEMeX [21], the largest Medical VQA dataset that provides detailed visual and textual explanations for each VQA triplet. To ensure the high quality of ThinkVG, we introduce an uncertainty-driven multi-agent framework, centered around DeepSeek-R1 [5], which also coordinates multiple agents and leverages an uncertainty mechanism to enhance the accuracy of the generated data. 3.2.1. Data Generation In Stage 1, we primarily use DeepSeek-R1 to generate the thinking process for each VQA triplet due to its outstanding reasoning capabilities. However, as DeepSeek-R1 is textbased LLM and lacks the ability to directly perceive visual inputs, we utilize the grounded reports from GEMeX as an intermediary. In GEMeXs reports, each sentence is aligned with specific anatomical region, enabling DeepSeek-R1 to simulate multimodal reasoning process required for solving the tasks. To optimize the prompt design, we randomly sample 50 questions from each question type (e.g., openended) in the training setresulting in 200 questions in totalto evaluate performance and guide prompt iteration. The final optimized prompt is presented in Table 1. 3.2.2. Uncertainty-driven Quality Assurance Although manual verification and prompt modification are applied, the generated data still suffers from quality issues. To address this, we design pipeline which contains three steps (Stage 2 to Stage 4) to ensure quality: (1) We first introduce an uncertainty-aware mechanism that enables DeepSeek-R1 to self-reflect and evaluate the reliability of its own outputs. Specifically, after generating the ThinkVG response, we prompt DeepSeek-R1 with the question: Do messages = [role:user, content: fSuppose you are viewing CXR that shows the following: The hilar contours are normal [visual location: bilateral hilar structures ([116, 112, 227, 182])] .... Given the question: YOUR QUESTION, provide detailed thinking process (around 100 words) including specific visual location (e.g., (region [x1,y1,x2,y2])) about how to solve this question with answer ANSWER. You must assume that you are viewing the CXR image rather than reading the textual findings, thus do not output words like observe the report or from report or report states or given findings or provided findings or described findings. Table 1. Given question-answer pair, our tailored prompt guides DeepSeek-R1 to generate ThinkVG data. you trust your thinking process? A. Yes B. No C. Not sure. This self-reflection with uncertainty choice Not sure allows the model to carefully reassess its previous reasoning, which has been shown to improve output accuracy [2]. We retain only those ThinkVG responses for which the model answers A. Yes.; (2) To avoid bias caused by relying on single LLM for both data generation and evaluation [25], we introduce additional agents to assess the quality of the generated data. As shown in Stage 3, we employ three additional agentsQwen-2.5-72B [26], OpenBioLLM-70B1, and GPT-4o [1]as external reviewers. These agents are given grounded reports, questions, and answers to assess the accuracy of the thinking process. Similar to the selfreflection stage, we introduce an uncertain option to provide the agents with greater flexibility for consideration and decision-making. This process helps eliminate incorrect reasoning paths generated by DeepSeek-R1 based on 1https://huggingface.co/aaditya/Llama3-OpenBioLLM-70B Open-ended Closed-ended Single-choice Multi-choice 62,712 37, 53,905 52,114 Table 2. Distribution of generated ThinkVG data across the four question types in GEMeX. 4. Fine-tuning on GEMeX-ThinkVG: Learning to Think with Visual Grounding in Medical VQA 4.1. Supervised Fine-tuning After generating the ThinkVG data, we first train the large vision language model (LVLM) using Supervised Finetuning (SFT) to equip it with task-aware thinking capabilities. To generate the outputs required by GEMeXwhich include both the final answer and its corresponding visual location (omitting the reasons, as we provide more detailed preceding thinking path)the model must produce structured outputs in two distinct parts. First, it should output thinking trace that incorporates explicit visual grounding, enclosed within the <think>...</think> tags. Second, it should deliver concise final response that includes the predicted answer and the summarized location, wrapped within designated XMLvisual like structure: <response> <answer>...</answer> <location>...</location></response>. This format ensures both interpretability and alignment with the GEMeX evaluation requirements. An illustrative example is shown in Figure 3. 4.2. Reinforcement Fine-tuning Although SFT can significantly improve thinking capabilities, we have observed in practice that: (1) due to the complexity of the task, the model sometimes generates incorrect thinking process after SFT, for example, incorrect analysis of images, which may be because SFT is better at helping LVLMs memorize rather than truly understand [4]; (2) inaccurate region localization within the thinking path can propagate errors to the final answer, ultimately leading to degraded performance. Therefore, we adopt Reinforcement Fine-tuning (RFT), specifically through Group Relative Policy Optimization (GRPO) [28], as post-training strategy to further activate and enhance the models thinking capabilities with newly proposed reward mechanism. 4.2.1. Group Relative Policy Optimization Given question from question set Q, GRPO first samples set of outputs {oi}G from the policy model πθold during each learning iteration. Then it optimizes the LVLM πθ by Figure 3. An example from the SFT and RFT stages: after SFT, the model learns to think carefully and incorporates grounding to specific pathological regions for visual evidence. During the RFT stage, since both the answer and the grounded regions are correct, the two reward scores are each 1.0. its internal knowledge. Then, we adopt voting mechanism: the ThinkVG is retained only if all three agents unanimously select A. Correct option; (3) Finally, we apply post-processing to eliminate ThinkVG outputs with significant errors, such as incorrect location information or instances that quote the report or findings directly, rather than generating content from an observational perspective. 3.3. Radiologist Validation After auto-generation, radiologist randomly samples 200 examples from each of the four question types in the dataset for manual inspection. Cross-validation reveals no clinically inconsistent reasoning chains, demonstrating the effectiveness of the proposed framework. In total, we generate 206,071 ThinkVG instances for 21,994 images, all drawn from the GEMeX training set. The detailed distribution is presented in Table 2. messages = [role:user, content: fWe would like to request your feedback on the performance of two AI assistants in response to the user question displayed above. For your reference, the visual content in the image is represented with caption describing the same image. Please rate the accuracy (most important), relevance of their responses, considering both answer and reason (if any). Each assistant receives an overall score on scale of 1 to 10, where higher score indicates better overall performance. Please output both scores and your reaon in JSON format { assistant1: score, assistant2: score, reason:your reason }. vide the visual locations involved in solving the questions. Therefore, we first check whether the number of bounding boxes predicted by the model matches the number in the GT. If they match, we then evaluate whether the mean Intersection over Union (mIoU) exceeds 0.75. reward score of 1.0 is assigned only when both conditions are satisfied; otherwise, the reward is 0. Note that we leave out the common format reward as after the SFT stage, the model has learned to follow the format requirement. 5. Experiments Table 3. Our designed prompt for Semantics Reward. 5.1. Dataset maximizing the following objective: (cid:34) JGRPO(θ) = qP (Q), {oi}G i=1πθold (Oq) (cid:32) min"
        },
        {
            "title": "1\nG",
            "content": "G (cid:88) i=1 πθ(oi q) πθold(oi q) Ai,clip (cid:18) πθ(oi q) πθold(oi q) (cid:19) (cid:33) , 1 ε, 1 + ε Ai (cid:35) βDKL(πθ πref) std({r1,r2,...,rG}) (2) where Ai = rimean({r1,r2,...,rG}) is the relative advantage over group rewards {ri}G , DKL is the KL divergence between πθ and πref (the model after SFT stage) to prevent catastrophic forgetting caused by over optimization, ϵ and β are the PPO clipping hyper-parameter [27] and the coefficient for the KL penalty, respectively. 4.2.2. Reward Functions We propose two reward functions, tailored for the two output parts of GEMeX: answer and involved visual locations. (1) Semantics Reward: In GEMeX, four types of questions are defined  (Table 2)  . For choice-based and closed-ended questions, reward design is straightforward, as we can directly use Accuracy Reward [24] to evaluate whether the models outputs are correct. Nevertheless, designing reward function for open-ended questions is particularly challenging due to the absence of fixed answers. Unlike [18], which employs traditional machine learning metrics such as BLEU as rewardsmetrics that have been shown to poorly capture essential medical content [11]we introduce semantics-aware accuracy reward. It enables the integration of all four types into unified, coherent reward paradigm. Specifically, we input both the ground truth (GT) answer and the model-generated answer into OpenBioLLM-70B, which can provide two accuracy scores at the semantics level. When these two scores are close, i.e., if the absolute difference between them is less than 2, we assign reward of 1 point; otherwise, the reward is 0. (2) Grounding The specific prompt is shown in Table 3. Reward: Besides giving answers, the model needs to proWe conduct experiments on the GEMeX dataset and randomly sample portion of its training set to generate ThinkVG data. We denote the version with the complete training set of 1.59M VQA triples as GEMeXFull. The 200K subset that includes ThinkVG data is denoted as GEMeX-ThinkVG-200K, while the subset without ThinkVG is referred to as GEMeX-200K. For evaluation, we use the original GEMeX test set, which consists of 300 images and 3,960 questions (1,144 open-ended questions, 543 closed-ended questions, 1,300 single-choice questions, and 973 multiple-choice questions). 5.2. Training Details We mainly explore the Qwen2.5-VL-7B-Instruct2 for experimental verification. During the SFT stage, we fine-tune both the visual projection layers and the LLM components by jointly predicting ThinkVG paths and task-related results. Particularly, the model is trained for 2 epochs on 8 NVIDIA H100 GPUs with batch size of 256. The network is warmed up in the first 0.05 epochs with linear learning rate from 3e-7 to 1e-4, which further decays by cosine schedule. The optimizer is AdamW. To accelerate training, we employ the DeepSpeed ZeRO-2 and the bf16 (Brain Floating Point) data format. During the RFT stage, the model is trained for 1 epoch on 8 NVIDIA H100 GPUs with batch size of 128. The optimizer is AdamW with learning rate of 2e 6. Similar to SFT, we also fine-tune both the visual projector and LLM components. Regarding the hyper-parameters in Equation 2, we generate 8 outputs (i.e., G) for each input and set β and ϵ to 1e 3 and 0.2, respectively. The sample generator and LLM used for semantics reward are deployed by vLLM [14] framework. 5.3. Main Results We report the main results in Table 4. It can be seen that (1) Compared to zero-shot learning, fine-tuning can significantly improve the models performance; (2) When SFT is conducted with only 200K vanilla data (i.e., GEMeX200K), the model lags behind the one trained on the full 2https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct Training Paradigm Open-ended Closed-ended Single-choice Multi-choice Dataset SFT RFT A-score V-score A-score V-score A-score V-score A-score V-score A-score.Avg Zero-shot - Fine-tuning GEMeX-Full (cid:33) GEMeX-200K (cid:33) GEMeX-ThinkVG-200K (cid:33) GEMeX-ThinkVG-200K (cid:33) - - (cid:37) (cid:33) 85.13 97.31 92.54 94.71 96. 12.31 60.51 56.59 56.52 59.63 30. 82.32 79.93 87.11 90.97 20.23 69. 66.04 66.23 68.39 49.00 83.08 78. 80.08 83.31 27.81 67.38 61.39 61. 64.52 12.02 74.51 67.93 70.20 73. 7.59 55.98 49.71 51.13 54.49 44. 84.31 79.83 83.03 85.98 Table 4. Performance of Qwen2.5-VL-7B with different evaluating paradigms on GEMeX. The A-score indicates answer or choice accuracy (%), and the V-score represents mIoU (%). indicates that the accuracy of the answer is judged by the LLM, as the question is open-ended. The best results are bolded, and the second-best are underlined in each column. Task Closed. Single. SFT Data 200K ThinkVG-200K 200K ThinkVG-200K Original A-score 79.93 87.11 78.92 80.08 Per.1 76.98 88.39 79.00 80.15 Per.2 83.06 86.56 77.69 80. Per.3 75.69 87.66 77.31 79.69 Std. 3.214 0.752 0.723 0.192 Table 5. Model performance under different perturbations. means the standard deviation across the three perturbations from original A-score. dataset by large margin, with an average A-score that is around 4.5% lower; (3) When ThinkVG data (GEMeXThinkVG-200K) is further used for SFT, the models performance improves substantially, even surpassing that of the model trained on the full dataset in certain tasks, e.g., an improvement of approximately 5% on closed-ended tasks. Nevertheless, it can be observed that when only SFT is used, the improvement in visual grounding is quite limited. This is often due to inaccurate location reasoning in the thinking path after learning ThinkVG with SFT, which leads to error propagation in the final outputs; (4) To address this issue, further applying RFT with semantics reward and grounding reward yields noticeable enhancement, 1.5%3% improvement in both answer accuracy and localization performance. It is worth noting that, compared to using the full training data, the model further fine-tuned with RFT achieves better results in answer generation (i.e., higher average A-score) and comparable performance in localization. These findings demonstrate the effectiveness of our proposed ThinkVG dataset and RFT rewards. 5.4. Robustness to Input Variations We conduct two common perturbations to generate input variations: (1) we modify closed-ended questions while preserving their semantic integrityfor example, e.g., changing Is the heart size abnormal in this CXR? by randomly replacing the interrogative word is with isnt, or replacing abnormal with normal, and adjusting the corresponding answer accordingly. During the test phase, each question has 50% chance of being modified by one of the two modes mentioned above. If question does not meet the criteria for either mode, no modification will be applied; (2) we randomly change the order of the options in single-choice questions. For each type of perturbation, we conduct three rounds of test and show models performance in Table 5. To more clearly compare the effectiveness of ThinkVG, we compare the performance of the models only after SFT. The results show that after fine-tuning the model with ThinkVG, its robustness against interference is significantly improved (i.e., lower standard deviation), highlighting another advantage. 5.5. Qualitative Evaluation As shown in Figure 4 and 5, we compare the performance of models on several challenging examples, trained with and without ThinkVG on GEMeX-200K. When using ThinkVG, we present the model outputs after RFT stage. It can be seen that when using ThinkVG with proper tuning, not only are the answers more accurate, but the model also carefully thinks with analyzing corresponding regions in the image when addressing questions, leading to stronger overall interpretability. 6. Conclusion In this work, we address the critical issue of interpretability in Medical Visual Question Answering by introducing thinking with visual grounding dataset (ThinkVG). It explicitly links thinking steps to corresponding regions in the medical image, offering fine-grained visual explanations that enhance transparency and trust in model-generated answers. To further guide the model in generating accurate thinking paths, we proposed new reward mechanism for reinforcement learning that aligns the models outputs with relevant visual and textual evidence. Despite using subset of training data, the trained model is able to achieve impressive performance. We believe this work marks step toward making Medical VQA systems more explainable and clinically usable, paving the way for safer and more reliable AI-assisted diagnosis. Figure 4. Challenging examples from GEMeX answered by models trained with and without ThinkVG. ((cid:33)) or ((cid:37)) in outputs highlight correct or incorrect reasons or answers. The colored words indicate the thinking with visual grounding process. Figure 5. Challenging examples from GEMeX answered by models trained with and without ThinkVG. ((cid:33)) or ((cid:37)) in outputs highlight correct or incorrect reasons or answers. The colored words indicate the thinking with visual grounding process."
        },
        {
            "title": "References",
            "content": "[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. 3 [2] Jiuhai Chen and Jonas Mueller. Quantifying uncertainty in answers from any language model and enhancing their trustworthiness. arXiv preprint arXiv:2308.16175, 2023. 3 [3] Xiaolan Chen, Ziwei Zhao, Weiyi Zhang, Pusheng Xu, Le Gao, Mingpu Xu, Yue Wu, Yinwen Li, Danli Shi, and Mingguang He. Eyegpt: Ophthalmic assistant with large language models. arXiv preprint arXiv:2403.00840, 2024. 1 [4] Tianzhe Chu, Yuexiang Zhai, Jihan Yang, Shengbang Tong, Saining Xie, Dale Schuurmans, Quoc Le, Sergey Levine, and Yi Ma. Sft memorizes, rl generalizes: comparative study of foundation model post-training. arXiv preprint arXiv:2501.17161, 2025. 4 [5] Daya Guo, Dejian Yang, Haowei Zhang, Junxiao Song, Ruoyu Zhang, Runxin Xu, Qihao Zhu, Shirong Ma, Peiyi Wang, Xiao Bi, et al. Deepseek-r1: Incentivizing reasoning capability in llms via reinforcement learning. arXiv preprint arXiv:2501.12948, 2025. 2, 3 [6] Xuehai He, Yichen Zhang, Luntian Mou, Eric Xing, and Pengtao Xie. Pathvqa: 30000+ questions for medical visual question answering. arXiv preprint arXiv:2003.10286, 2020. 1, [7] Xinyue Hu, Lin Gu, Qiyuan An, Mengliang Zhang, Liangchen Liu, Kazuma Kobayashi, Tatsuya Harada, Ronald Summers, and Yingying Zhu. Expert knowledgeaware image difference graph representation learning for difference-aware medical visual question answering. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pages 41564165, 2023. 2 [8] Yutao Hu, Tianbin Li, Quanfeng Lu, Wenqi Shao, Junjun He, Yu Qiao, and Ping Luo. Omnimedvqa: new large-scale comprehensive evaluation benchmark for medical lvlm. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 2217022183, 2024. 1, 2 [9] Wenxuan Huang, Bohan Jia, Zijie Zhai, Shaosheng Cao, Zheyu Ye, Fei Zhao, Zhe Xu, Yao Hu, and Shaohui Lin. Vision-r1: Incentivizing reasoning capability in multimodal large language models. arXiv preprint arXiv:2503.06749, 2025. 2 [10] Aaron Jaech, Adam Kalai, Adam Lerer, Adam Richardson, Ahmed El-Kishky, Aiden Low, Alec Helyar, Aleksander Madry, Alex Beutel, Alex Carney, et al. Openai o1 system card. arXiv preprint arXiv:2412.16720, 2024. 2 [11] Saahil Jain, Ashwin Agrawal, Adriel Saporta, Steven QH Truong, Du Nguyen Duong, Tan Bui, Pierre Chambon, Yuhao Zhang, Matthew Lungren, Andrew Ng, et al. Radgraph: Extracting clinical entities and relations from radiology reports. arXiv preprint arXiv:2106.14463, 2021. 5 [12] Fangkai Jiao, Geyang Guo, Xingxing Zhang, Nancy Chen, Shafiq Joty, and Furu Wei. Preference optimization for reasoning with pseudo feedback. arXiv preprint arXiv:2411.16345, 2024. [13] Yash Khare, Viraj Bagal, Minesh Mathew, Adithi Devi, Deva Priyakumar, and CV Jawahar. Mmbert: Multimodal bert pretraining for improved medical vqa. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 10331036. IEEE, 2021. 1 [14] Woosuk Kwon, Zhuohan Li, Siyuan Zhuang, Ying Sheng, Lianmin Zheng, Cody Hao Yu, Joseph Gonzalez, Hao Zhang, and Ion Stoica. Efficient memory management for large language model serving with pagedattention. In Proceedings of the 29th Symposium on Operating Systems Principles, pages 611626, 2023. 5 [15] Yuxiang Lai, Jike Zhong, Ming Li, Shitian Zhao, and Xiaofeng Yang. Med-r1: Reinforcement learning for generalizable medical reasoning in vision-language models. arXiv preprint arXiv:2503.13939, 2025. 2 [16] Jason Lau, Soumya Gayen, Asma Ben Abacha, and Dina Demner-Fushman. dataset of clinically generated visual questions and answers about radiology images. Scientific data, 5(1):110, 2018. 1, 2 [17] Chunyuan Li, Cliff Wong, Sheng Zhang, Naoto Usuyama, Haotian Liu, Jianwei Yang, Tristan Naumann, Hoifung Poon, and Jianfeng Gao. Llava-med: Training large languageand-vision assistant for biomedicine in one day. Advances in Neural Information Processing Systems, 36, 2024. 1 [18] Xu Liang. Group relative policy optimization for image captioning. arXiv preprint arXiv:2503.01333, 2025. [19] Bo Liu, Li-Ming Zhan, and Xiao-Ming Wu. Contrastive pre-training and representation distillation for medical visual In Mediquestion answering based on radiology images. cal Image Computing and Computer Assisted Intervention MICCAI 2021: 24th International Conference, Strasbourg, France, September 27October 1, 2021, Proceedings, Part II 24, pages 210220. Springer, 2021. 1 [20] Bo Liu, Li-Ming Zhan, Li Xu, Lin Ma, Yan Yang, and Xiao-Ming Wu. Slake: semantically-labeled knowledgeenhanced dataset for medical visual question answering. In 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI), pages 16501654. IEEE, 2021. 2 [21] Bo Liu, Ke Zou, Liming Zhan, Zexin Lu, Xiaoyu Dong, Yidi Chen, Chengqiang Xie, Jiannong Cao, Xiao-Ming Wu, and Huazhu Fu. Gemex: large-scale, groundable, and explainable medical vqa benchmark for chest x-ray diagnosis. arXiv preprint arXiv:2411.16778, 2024. 1, 2, 3 [22] Ziyu Liu, Zeyi Sun, Yuhang Zang, Xiaoyi Dong, Yuhang Cao, Haodong Duan, Dahua Lin, and Jiaqi Wang. VisualarXiv preprint rft: Visual reinforcement fine-tuning. arXiv:2503.01785, 2025. 2 [23] Trung Quoc Luong, Xinbo Zhang, Zhanming Jie, Peng Sun, Xiaoran Jin, and Hang Li. Reft: Reasoning with reinforced fine-tuning. arXiv preprint arXiv:2401.08967, 3, 2024. 2 [24] Jiazhen Pan, Che Liu, Junde Wu, Fenglin Liu, Jiayuan Zhu, Hongwei Bran Li, Chen Chen, Cheng Ouyang, and Daniel Rueckert. Medvlm-r1: Incentivizing medical reasoning capability of vision-language models (vlms) via reinforcement learning. arXiv preprint arXiv:2502.19634, 2025. 2, 5 [25] Arjun Panickssery, Samuel Bowman, and Shi Feng. Llm evaluators recognize and favor their own generations. Advances in Neural Information Processing Systems, 37: 6877268802, 2024. [26] Qwen, :, An Yang, Baosong Yang, Beichen Zhang, Binyuan Hui, Bo Zheng, Bowen Yu, Chengyuan Li, Dayiheng Liu, Fei Huang, Haoran Wei, Huan Lin, Jian Yang, Jianhong Tu, Jianwei Zhang, Jianxin Yang, Jiaxi Yang, Jingren Zhou, Junyang Lin, Kai Dang, Keming Lu, Keqin Bao, Kexin Yang, Le Yu, Mei Li, Mingfeng Xue, Pei Zhang, Qin Zhu, Rui Men, Runji Lin, Tianhao Li, Tianyi Tang, Tingyu Xia, Xingzhang Ren, Xuancheng Ren, Yang Fan, Yang Su, Yichang Zhang, Yu Wan, Yuqiong Liu, Zeyu Cui, Zhenru Zhang, and Zihan Qiu. Qwen2.5 technical report, 2024. 3 [27] John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347, 2017. 5 [28] Zhihong Shao, Peiyi Wang, Qihao Zhu, Runxin Xu, Junxiao Song, Xiao Bi, Haowei Zhang, Mingchuan Zhang, YK Li, Wu, et al. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300, 2024. 2, 4 [29] Zhaochen Su, Linjie Li, Mingyang Song, Yunzhuo Hao, Zhengyuan Yang, Jun Zhang, Guanjie Chen, Jiawei Gu, Juntao Li, Xiaoye Qu, et al. Openthinkimg: Learning to think with images via visual tool reinforcement learning. arXiv preprint arXiv:2505.08617, 2025. 2 [30] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui, Yu-Xiong Wang, Yiming Yang, et al. Aligning large multimodal models with factually augmented rlhf. arXiv preprint arXiv:2309.14525, 2023. 2 [31] Yaoting Wang, Shengqiong Wu, Yuecheng Zhang, Shuicheng Yan, Ziwei Liu, Jiebo Luo, and Hao Fei. Multimodal chain-of-thought reasoning: comprehensive survey. arXiv preprint arXiv:2503.12605, 2025. 2 [32] Huaiyuan Ying, Shuo Zhang, Linyang Li, Zhejian Zhou, Yunfan Shao, Zhaoye Fei, Yichuan Ma, Jiawei Hong, Kuikun Liu, Ziyi Wang, et al. Internlm-math: Open math large language models toward verifiable reasoning. arXiv preprint arXiv:2402.06332, 2024. 2 [33] Kechi Zhang, Ge Li, Yihong Dong, Jingjing Xu, Jun Zhang, Jing Su, Yongfei Liu, and Zhi Jin. Codedpo: Aligning code models with self generated and verified source code. arXiv preprint arXiv:2410.05605, 2024. 2 [34] Sheng Zhang, Yanbo Xu, Naoto Usuyama, Hanwen Xu, Jaspreet Bagga, Robert Tinn, Sam Preston, Rajesh Rao, Mu Wei, Naveen Valluri, et al. Biomedclip: multimodal biomedical foundation model pretrained from fifteen million scientific image-text pairs. arXiv preprint arXiv:2303.00915, 2023. 1 [35] Zhuosheng Zhang, Aston Zhang, Mu Li, Hai Zhao, George Karypis, and Alex Smola. Multimodal chain-ofarXiv preprint thought reasoning in language models. arXiv:2302.00923, 2023. [36] Xiangyu Zhao, Wanghan Xu, Bo Liu, Yuhao Zhou, Fenghua Ling, Ben Fei, Xiaoyu Yue, Lei Bai, Wenlong Zhang, and Xiao-Ming Wu. Msearth: benchmark for multimodal scientific comprehension of earth science. arXiv preprint arXiv:2505.20740, 2025. 1 [37] Zhiyuan Zhao, Bin Wang, Linke Ouyang, Xiaoyi Dong, Jiaqi Wang, and Conghui He. Beyond hallucinations: Enhancing lvlms through hallucination-aware direct preference optimization. arXiv preprint arXiv:2311.16839, 2023."
        }
    ],
    "affiliations": [
        "Department of Data Science and Artificial Intelligence, The Hong Kong Polytechnic University, Hong Kong",
        "Department of Radiology, West China Hospital of Sichuan University, China",
        "Institute of High Performance Computing, Agency for Science, Technology and Research, Singapore",
        "National Engineering Laboratory for Big Data System Computing Technology, Shenzhen University, China"
    ]
}