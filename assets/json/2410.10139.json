{
    "paper_title": "MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models",
    "authors": [
        "Peng Xia",
        "Siwei Han",
        "Shi Qiu",
        "Yiyang Zhou",
        "Zhaoyang Wang",
        "Wenhao Zheng",
        "Zhaorun Chen",
        "Chenhang Cui",
        "Mingyu Ding",
        "Linjie Li",
        "Lijuan Wang",
        "Huaxiu Yao"
    ],
    "sections": [
        {
            "title": "Abstract",
            "content": "Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become a pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, a large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering a mix of multiple-choice and open-ended question formats to evaluate diverse competencies. Moreover, we propose a reliable automated evaluation metric, leveraging a scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing a comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. We publicly release our benchmark and code in https://mmie-bench.github.io/."
        },
        {
            "title": "Start",
            "content": "4 2 0 2 4 1 ] . [ 1 9 3 1 0 1 . 0 1 4 2 : r Preprint. MMIE: MASSIVE MULTIMODAL INTERLEAVED COMPREHENSION BENCHMARK FOR LARGE VISIONLANGUAGE MODELS Peng Xia1, Siwei Han1, Shi Qiu1, Yiyang Zhou1, Zhaoyang Wang1, Wenhao Zheng1, Zhaorun Chen3, Chenhang Cui4, Mingyu Ding1, Linjie Li2, Lijuan Wang2, Huaxiu Yao1 1UNC-Chapel Hill, 2Microsoft Research, 3University of Chicago, 4NUS {pxia,siweih,shiqiu,huaxiu}@cs.unc.edu"
        },
        {
            "title": "ABSTRACT",
            "content": "Interleaved multimodal comprehension and generation, enabling models to produce and interpret both images and text in arbitrary sequences, have become pivotal area in multimodal learning. Despite significant advancements, the evaluation of this capability remains insufficient. Existing benchmarks suffer from limitations in data scale, scope, and evaluation depth, while current evaluation metrics are often costly or biased, lacking in reliability for practical applications. To address these challenges, we introduce MMIE, large-scale knowledge-intensive benchmark for evaluating interleaved multimodal comprehension and generation in Large Vision-Language Models (LVLMs). MMIE comprises 20K meticulously curated multimodal queries, spanning 3 categories, 12 fields, and 102 subfields, including mathematics, coding, physics, literature, health, and arts. It supports both interleaved inputs and outputs, offering mix of multiple-choice and openended question formats to evaluate diverse competencies. Moreover, we propose reliable automated evaluation metric, leveraging scoring model fine-tuned with human-annotated data and systematic evaluation criteria, aimed at reducing bias and improving evaluation accuracy. Extensive experiments demonstrate the effectiveness of our benchmark and metrics in providing comprehensive evaluation of interleaved LVLMs. Specifically, we evaluate eight LVLMs, revealing that even the best models show significant room for improvement, with most achieving only moderate results. We believe MMIE will drive further advancements in the development of interleaved LVLMs. We publicly release our benchmark and code in https://mmie-bench.github.io/. Content warning: this paper contains content that may be inappropriate or offensive."
        },
        {
            "title": "INTRODUCTION",
            "content": "True evaluation lies in the seamless interweaving of diverse modalities. Multimodal learning has made remarkable progress with the development of Large Vision-Language Models (LVLMs) (Liu et al., 2023a; Zhu et al., 2023; Dai et al., 2023), which are capable of handling diverse tasks that involve both images and text. Despite their advancements, most of these models are limited to multimodal tasks for text generation, such as visual question answering (VQA) and image captioning, which do not fully reflect the potential of multimodal capacity. To broaden their application, interleaved text-and-image generation has emerged as critical area of research (Liu et al., 2024). It requires models to generate images and text in any sequence, thereby enhancing the versatility and effectiveness of multimodal systems. It opens up possibilities for various complex applications, such as multi-step inference (Lu et al., 2024; Kazemi et al., 2024), multimodal situational analysis (Yang et al., 2021), and visual storytelling (Huang et al., 2016). While recent LVLMs are evolving to support interleaved text-and-image generation (Team, 2024; Xie et al., 2024; Chern et al., 2024; Zhou et al., 2024), comprehensive evaluation benchmark is still falling behind due to the following two challenges: Equal Contribution. 1 Preprint. Figure 1: Typical samples from the MMIE Benchmark showcase its support for multiple image inputs and outputs, with ground truth provided for every query. MMIE evaluates models across diverse fields, ensuring comprehensive evaluation of their capabilities. Difficulty in Constructing Modality-Coherent Benchmarks. The first challenge lies in the difficulty of constructing modality-aligned multimodal datasets, where both the input and output contain images and text. Current benchmarks mainly focus on single-modality output tasks (Fu et al., 2023; Li et al., 2024a; Zhang et al., 2023), assessing only the quality of the generated image or text, without benchmarking the crucial connection between modalities, such as textimage coherence and consistency. Although few datasets support the interleaved multimodal evaluation method for LVLMs (Liu et al., 2024), their dataset is constrained by its limited scale and narrow query format, primarily focused on VQA tasks. Lack of Automated Evaluation Metric. The second challenge is the lack of suitable automated evaluation metrics for interleaved generation. Human evaluation is costly and timeconsuming, making it difficult to scale for practical applications. Current automated evaluation metrics typically assess either the quality of generated text (e.g., BLEU (Papineni et al., 2002), BERTScore (Zhang et al., 2020)) or the quality of generated images (e.g., FID (Heusel et al., 2017)). While recent evaluation strategies, such as using CLIPScore (Hessel et al., 2021), and vision-language models (VLMs) (Chen et al., 2023; Liu et al., 2024), can evaluate the connection between different modalities, they rely heavily on the pre-trained knowledge of specific models (e.g., CLIP training data) or follow rigid, human-defined rules. These approaches can introduce bias and uncertainty to some extent, often leading to inconsistent results (Mahmoud et al., 2024). To address these limitations, we introduce MMIE, Massive Multimodal Inverleaved comprehension Evaluation benchmark for LVLMs with proposed reliable and automated metrics. MMIE is curated from four multimodal datasets, involving 3 categories, 12 fields, and 102 subfields, including mathematics, physics, coding, statistics, literature, philosophy, education, finance, health, sports, art, and EECS (Electrical Engineering and Computer Science). The dataset comprises 20K multimodal questions, supporting both interleaved inputs and outputs. It features mix of multiplechoice and open-ended question formats to evaluate broad spectrum of competencies across various fields. As shown in Table 2, MMIE surpasses existing interleaved multimodal benchmark in both depth and width, particularly in addressing complex problem-solving and open-ended creative tasks. Based on the curated dataset, we further propose an automated metric powered by scoring model. Specifically, we first design comprehensive evaluation criteria for each category. Then, we curate fine-grained, human-annotated scoring dataset and then use this dataset to fine-tune the InternVL-2 (Chen et al., 2024) to obtain the scoring model. Using MMIE, we evaluate four opensource interleaved multimodal LVLMs, as well as combinations of advanced LVLMs like GPT-4o with text-to-image generative models (e.g., Stable Diffusion 3 (Esser et al., 2024)). Our key contributions are summarized as follows: 2 Preprint. We introduce the largest high-quality interleaved multimodal benchmark MMIE for evaluating LVLMs, with the dataset to be publicly released. MMIE presents significant challenges to LVLMs, with the best-performing model (e.g., GPT-4o + SDXL) achieving score of 65.47%, highlighting substantial room for improvement. The proposed scoring model is reliable and has proven to be comparable to human evaluation."
        },
        {
            "title": "2 RELATED WORK",
            "content": "Interleaved Multimodal Comprehension and Generation. Multimodal learning has rapidly evolved, with substantial progress in integrating text and image modalities. Recent advancements in large vision-language models (LVLMs) (Liu et al., 2023a; Zhu et al., 2023; Dai et al., 2023; Xia et al., 2024b), either driven by the integration of diffusion models like Stable Diffusion (Rombach et al., 2022), or using token-based mixed-modal structures like Chameleon (Team, 2024) and Show-o (Xie et al., 2024), have enabled models to not only understand and generate content across modalities, but also engage in interleaved multimodal comprehension and generation. As the demand for richer, more interactive AI grows, interleaved multimodal comprehension and generation is becoming an essential component in the development of next-generation LVLMs. LVLM Benchmarks. Despite the rapid advancements in multimodal learning, evaluation benchmarks remain far from perfect. Previous benchmarks primarily focused on evaluating the base perception ability of LVLMs (Lu et al., 2022; Gurari et al., 2018), such as GQA (Hudson & Manning, 2019), which lack the depth required to assess advanced reasoning. Recently, several high-quality evaluation benchmarks have been proposed to assess the reasoning ability of these models (Li et al., 2024a; Zhang et al., 2023; Liu et al., 2023a;b; Yu et al., 2023; Xia et al., 2024a; Wang et al., 2024b; Tu et al., 2023; Cui et al., 2023; Lee et al., 2024), such as MMMU (Yue et al., 2024) and MME (Fu et al., 2023). However, these benchmark do not support interleaved image-and-text comprehension and generation. Large-scale interleaved multimodal datasets like MINT-1T (Awadalla et al., 2024), MANTIS (Jiang et al., 2024) and OBELICS (Laurencon et al., 2024) have been developed primarily for pre-training models. However, they lack precise alignment between text and images, making them unsuitable for evaluation and benchmarking. recent small-scale interleaved multimodal benchmark has been introduced (Liu et al., 2024), but its limited data size and query quality hinder the comprehensiveness of its evaluation. MMIE fills this gap by offering comprehensive evaluation framework that supports interleaved multimodal comprehension and generation. Our dataset includes diverse set of queries among multiple domains. By evaluating both perceptual and generative capacity of LVLMs, it provides more holistic assessment. Evaluation Metrics for Multimodal Tasks. Traditional evaluation metrics, such as BLEU (Papineni et al., 2002), BERTScore (Zhang et al., 2020) for text quality, and FID (Heusel et al., 2017) for image quality, are only suited to single-modality output tasks. Recent metrics, such as CLIPScore (Hessel et al., 2021) and X-IQE (Chen et al., 2023), have attempted to address this by introducing multimodal models to evaluate consistency between text and image. However, these metrics only measure alignment and fall short of offering comprehensive assessment of output quality. Furthermore, many multimodal metrics depend on GPT-based models (Liu et al., 2024), bringing uncontrollable bias to the whole evaluation system. To overcome these drawbacks, we propose an automatic metric to minimises bias and provides thorough analysis of the generated results."
        },
        {
            "title": "3 THE MMIE BENCHMARK",
            "content": "Table 1: Dataset statistics. 3.1 OVERVIEW In this section, we introduce MMIE, diverse and comprehensive benchmark for evaluating interleaved multimodal comprehension and generation across broad scope of tasks. As shown in Table 2, MMIE consists of 20,103 curated samples spanning 12 fields, including mathematics, physics, coding, statistics, literature, philosophy, education, finance, health, sports, art, and EECS. Each query is meticulously selected, filtered, and refined to ensure both Statistic Number Percentage Total questions - Situational analysis - Project-based learning - Multi-step reasoning Total Categories/Fields/Subfields 20103 5005 11482 3616 3/12/102 Formats: - Multiple-Choice Questions - Open-Ended Questions Questions with images Questions with answer label Average question length Average images per question 663 19340 20103 20103 76.0 1. - 24.89% 57.12% 17.99% - 3.40% 96.60% 100% 100% - - 3 Preprint. high quality and relevance across the covered subjects. In addition, MMIE emphasizes the evaluation of three essential competencies: perception, reasoning, and generation. Unlike previous benchmarks that evaluate the results from single modality (Fu et al., 2023; Yue et al., 2024; Li et al., 2024b) output, MMIE is specifically designed to assess models capabilities in understanding and generating interleaved text and images in any sequence. This evaluation extends beyond basic perception by requiring models to engage in complex reasoning, leveraging subject-specific knowledge across different modalities. 3.2 DATASET CURATION The data curation process in MMIE consists of two stages, each designed to ensure both comprehensive coverage and high-quality representation across various categories in our benchmark. We detail the process as follows: In the first stage, we collect and restructure four multimodal datasets to align with the interleaved image-and-text format and categorize them into three categories situational analysis, project-based learning and multi-step reasoning, which are illustrated in Figure 2. Specifically, for project-based learning, we extract data from Wikihow (Yang et al., 2021), which is originally designed for testing models ability to choose the correct procedural steps based on given text and image contexts. We adapt it to the interleaved text-and-image format. For situational analysis, we draw samples from VIST (Huang et al., 2016), naturally interleaved multimodal dataset designed for visual storytelling tasks, which challenges models to seamlessly integrate narrative text and images. Both situational analysis and project-based learning datasets feature interleaved inputs and outputs. To expand the benchmark with more complex and diverse tasks, we further introduce datasets focused on multi-step reasoning that support interleaved inputs. For this aspect, we source examples from MathVista (Lu et al., 2024) and ReMI (Kazemi et al., 2024), which together provide 3,600 questions covering topics from functions to statistics. The answer formats for these queries include multiple-choice questions (selecting one option from several choices) and open-ended questions (directly generating content). After extracting samples from these four datasets, we merge and refine them into cohesive benchmark by compacting, restructuring, and integrating questions from multiple sources, ensuring consistency with our evaluation objectives. Figure 2: Distribution of categories and fields in MMIE. In the second stage, we implement multi-step quality control process to ensure the integrity and consistency of the dataset. First, we apply lexical overlap and source URL similarity checks to identify and flag potential duplicate entries, which are then manually reviewed and removed. Next, each dataset is meticulously reviewed for formatting and typographical consistency to ensure adherence to standardized structure. Discrepancies are corrected to maintain uniformity across the entire dataset. In total, we finally collect 20,103 instances across 12 fields, including mathematics, physics, coding, statistics, literature, philosophy, education, finance, health, sports, art, and EECS. Detailed categorization and dataset statistics are presented in Table 1. For more information about dataset curation, please refer to Appendix A.1. 3.3 AUTOMATED EVALUATION METRIC As traditional metrics such as BLEU, BERTScore, and CLIP-Score fail to provide thorough evaluation of the quality of interleaved outputs, existing benchmarks use the GPT-4 series as the scoring model, which may introduce inherent bias in the scoring process (Liu et al., 2024). To ensure comprehensive and unbiased evaluation of LVLMs, as shown in Figure 3, we propose an automated evaluation metric powered by our fine-tuned LVLM to assist in scoring. Here, we choose InternVL-2-4B (Chen et al., 2024) as the foundation for our scoring Figure 3: Pipeline of the scoring model. 4 Preprint. Table 2: The comparison between MMIE with other LVLM benchmarks. Inter-I: interleaved input; Inter-O: interleaved output; Multi-I: multi-image for input; Multi-O: multi-image for output. Dataset Data Scale Inter-I Inter-O Multi-I Multi-O #Num Domains Answer Type Metric HumanEval (Chen et al., 2021) GSM8K (Cobbe et al., 2021) MME (Fu et al., 2023) MMBench (Liu et al., 2023b) MM-Vet (Yu et al., 2023) MagicBrush (Zhang et al., 2023) MMMU (Yue et al., 2024) MVBench (Li et al., 2024b) INTERLEAVEDBENCH (Liu et al., 2024) MMIE (Ours) 164 8.5K 2K 3K 218 10K 11.5K 4K 815 20K 1 1 4 6 6 7 30 9 10 12 Open Open Multi-Choice Multi-Choice Open Image Editing Multi-Choice Multi-Choice Open Pass@k Pass@k ACC ACC GPT-4 CLIPScore ACC ACC GPT-4o Multi-Choice & Open Fine-tuned VLM system due to its strong performance in multimodal reasoning tasks and support for multi-image inputs. Furthermore, we fine-tune the InternVL-2-4B to mitigate potential bias. Specifically, we first construct high-quality multimodal scoring dataset that covers all aspects of our benchmark, accompanied by detailed scoring criteria and reference answers. In this process, we collect 800 responses from four LVLMsMiniGPT-5 (Zheng et al., 2023), EMU-2 (Sun et al., 2024), GILL (Koh et al., 2023), and Anole (Chern et al., 2024). Based on the ground-truth labels, we define an evaluation standard using six-point grading scale with clear criteria. group of experts generates reference answers for each level and all score statistics are converted to percentage. These criteria and reference answers together form robust rubric. Following the rubric, human annotators rigorously score the responses. Detailed examples of the rubric are provided in Appendix A.3. After constructing the scoring dataset, we fine-tune the InternVL-2-4B model and use the fine-tuned version as our scoring model. To validate its performance, we randomly select 200 new samples with human-scored labels and compare the results of our model with those of other scoring models. The results show that the fine-tuned model significantly improves alignment between human scores and our model-generated scores compared to other LVLMs, leading to more accurate and reliable evaluation across diverse tasks. We will discuss the experimental results in detail in Section 4.3. 3.4 COMPARISON WITH EXISTING MULTIMODAL BENCHMARKS MMIE surpasses existing benchmarks in three key aspects. First, most previous multimodal benchmarks support only single-modality input or output, while MMIE closes this gap by enabling interleaved text-and-image comprehension and generation. Our dataset ensures robust modality alignment, with multimodal question-answer pairs reconstructed into an interleaved text-and-image instruction format, followed by manual review to guarantee quality. Moreover, the scenarios reflect real-world applications, such as multimodal script generation, data chart analysis, and multimodal story generation. Second, compared to recent interleaved comprehension benchmarks (Liu et al., 2024), MMIE is larger in scale and covers broader range of subjects, containing both reasoning and temporal understanding skills, allowing for more comprehensive evaluation. Finally, MMIE introduces reliable scoring system powered by fine-tuned LVLM, which significantly enhances the reliability of scoring. Table 2 highlights the differences between our benchmark and existing ones, demonstrating the advantages of MMIE in terms of scale, diversity, and scoring method."
        },
        {
            "title": "4 EXPERIMENT",
            "content": "MMIE provides systematic evaluation of existing open-source LVLMs supporting interleaved multimodal input and output (interleaved LVLMs), along with the integration of state-of-the-art LVLMs and text-to-image generative models (integrated LVLMs). In this section, we aim to answer the following key questions: (1) Which interleaved LVLM performs best on MMIE overall? (2) How effective are the integrated LVLMs? (3) Do the evaluated LVLMs show preference for certain field? and (4) How useful are our proposed model-powered metric compared with traditional metrics and other LVLM evaluation? 4.1 EXPERIMENT SETUP Baseline Models. We first benchmark four open-source interleaved LVLMs. (1) MiniGPT-5 (Zheng et al., 2023), multimodal model combining MiniGPT-4 and Stable Diffusion, specialized for co5 Preprint. herent image-text generation. (2) EMU-2 (Sun et al., 2024), 37B-parameter model excelling in in-context learning and multimodal reasoning, (3) GILL (Koh et al., 2023), model specialized in generating and retrieving interleaved outputs, (4) Anole (Chern et al., 2024), based on Chameleon (Team, 2024), model excelling in text quality, adds vision and multimodal generation capabilities. To broaden the comparison, we also compare with integrated LVLMs consisting of text-output LVLMs (i.e., GPT-4o (Achiam et al., 2023), Gemini-1.5 (Reid et al., 2024), LLaVA-v1.6-34b (Liu et al., 2023a) and Qwen-VL-2-72b (Wang et al., 2024a)) and text-to-image generative models (i.e., Openjourney (ope), Stable Diffusion 3 Medium (Esser et al., 2024), Stable Diffusion XL turbo, Flux.1-dev (flu)). We provide the interleaved text-and-image input to the LVLM to generate text, and then feed this text to text-to-image generative model to generate an image. The resulting multimodal output from this process is considered as interleaved output for evaluation. Human Annotators. We organize group of senior top-tier college students, contributing to the curation of the scoring dataset. To ensure thorough and consistent evaluations, we develop detailed criteria for each category of our benchmark (see Appendix A.3 for details). Evaluation Metrics. We evaluate the performance of all models using our proposed metric in Section 3.3, which is powered by our fine-tuned LVLM based on InternVL-2-4B (Chen et al., 2024), to ensure reliable scoring. 4.2 MAIN RESULTS In this section, we present the comprehensive evaluation on our MMIE benchmark. The detailed performance of interleaved LVLMs and integrated LVLMs is shown in Table 3 and Table 4, respectively. We summarize our key findings as follows: Challenging Evaluation and Promising Direction. As illustrated in Table 3, all evaluated interleaved LVLMs show poor performance, with an average score of 50.80%. Even when integrating advanced models such as GPT-4o and text-to-image generative models, as shown in Table 4, the best score (GPT-4o + SDXL) reached is 65.47%. This highlights the high level of difficulty and the challenge posed by MMIE. Interestingly, the latest interleaved LVLM Anole (Chern et al., 2024) shows significant improvements over previous interleaved LVLMs, including MiniGPT-5, GILL and EMU-2, by 8.4%, 7.0%, 21.8% in average score, respectively. This points to the growing potential of interleaved text-and-image models as promising direction for future progress in multimodal comprehension and generation. Gap between Interleaved LVLMs and Integrated LVLMs. Existing interleaved LVLMs are still quite limited. To enhance our evaluation and analysis on our benchmark, we integrate noninterleaved LVLMs with T2I models in our experiments. This integrated LVLMs approach significantly outperforms previous open-source interleaved LVLMs, improving performance by an average of 25.2% across all categories. Specifically, the integrated models outperform the best performance of the interleaved model by 14.6%, 26.3%, and 16.1% in situational analysis, project-based learning, and multi-step reasoning, respectively. Surprisingly, the integrated LVLMs perform exceptionally well in project-based learning, with all models based on LLaVA-34b achieving scores above 70%. These findings suggest that combining the strong comprehension abilities of non-interleaved LVLMs with the generative power of T2I models offers promising path for future research. Model Performance across Different Fields. As previously demonstrated in Table 3 and Table 4, model performance varies across different categories of data, achieving the best results in projectbased learning and the lowest scores in situational analysis. This indicates that the models performance differs depending on the category, likely due to inherent issues with the distribution of the training data. For example, Anole (Chern et al., 2024) scores 59.05% in project-based learning data but only 48.95% in situational analysis, suggesting it excels at creative, open-ended generation but falls short in handling detailed, discipline-specific knowledge. Delving into more fine-grained fields, as shown in Figure 4, different models exhibit preferences for certain fields of data. Among the seven fields of project-based learning, including education, finance, health, philosophy, sports, art and EECS, almost all models tend to perform well in areas that are easier to understand, such as philosophy, art and education, but face challenges in more complex fields requiring higher reasoning abilities, such as finance and EECS. Figure 4 also shows general gradual decline in 6 Preprint. Table 3: Performance of the four open-source LVLMs supporting interleaved image-and-text input and output on MMIE, shown as percentages. Model Situational analysis Project-based learning Multi-step reasoning AVG MiniGPT-5 (Zheng et al., 2023) EMU-2 (Sun et al., 2024) GILL (Koh et al., 2023) Anole (Chern et al., 2024) 47.63 39.65 46.72 48.95 55.12 46.12 57.57 59. 42.17 50.75 39.33 51.72 50.92 45.33 51.58 55.22 Figure 4: The average and total scores of each model across the seven fields of project-based learning based on our criteria. We take the average of GPT-4o, Gemini-1.5, LLaVA-v1.6-34b and Qwen-VL2-72b over the four text-to-image diffusion models. scores for the criteria of text and image quality, text-image coherence, method quality and practical utility, creativity and engagement, stylistic consistency and correspondence, suggesting that there is significant lack of text and image alignment and the ability to use interleaved output to solve real-world problems across all models. Table 4: Comparison with state-of-the-art LVLMs integrated with text-to-image models, referred to as integrated LVLMs, evaluated on MMIE. *: LLaVA only supports single-image input and all multi-image queries are thus skipped. LVLM T2I Model Situational analysis Project-based learning Multi-step reasoning AVG GPT-4o Gemini-1.5 LLaVA-34b Qwen-VL-70b Openjourney SD-3 SD-XL Flux Openjourney SD-3 SD-XL Flux Openjourney SD-3 SD-XL Flux Openjourney SD-3 SD-XL Flux 53.05 53.00 56.12 54. 48.08 47.48 49.43 47.07 54.12 54.72 55.97 54.23 52.73 54.98 52.58 54.23 71.40 71.20 73.25 68.80 67.93 68.70 71.85 68.33 73.47 72.55 74.60 71. 71.63 71.87 73.57 69.47 7 53.67 60.05 47.28* 55. 63.65 63.52 65.47 62.63 61.57 61.87 64.15 61.55 63.93 63.57 65.05 62.73 64.05 64.75 65.12 63.18 Preprint. Figure 5: Examples of model failures. Four typical types of errors are introduced and categorized, namely incoherence between text and image generation, inflexibility in generated responses, poor comprehension of multimodal information, and inability to manage complex reasoning tasks. 4.3 HOW CONSISTENT IS OUR MODEL-POWERED METRIC W.R.T HUMAN ANNOTATION? In this section, we further validate the effectiveness of our proposed metric. Here, we conduct an experiment to evaluate its correlation with human annotations using several disparity and similarity metrics, i.e., cosine similarity, mean square error (MSE), mean absolute error (MAE), and Pearson coefficient. For comparison, we report results from traditional multimodal alignment metric (i.e., CLIPScore) and scores judged by LVLMs, including GPT-4o, which has already served as the metric in (Liu et al., 2024). As shown in Table 5, our metric demonstrates the closest alignment with human evaluation results significantly, proving to be the most reliable. Our scoring model effectively captures the multimodal features of both image and text sequences and judges them through complex reasoning precisely. In contrast, other LVLMs and CLIPScore tend to focus primarily on understanding the sequence information, but they fall short in grasping the relationships between the sequences and accurately judging the alignment between them. In summary, the experiments demonstrate that our metric is robust and dependable standard for evaluating interleaved multimodal generation. Table 5: Comparison of scoring LVLMs and traditional image-text alignment metric. An up arrow means higher value is better, down arrow means lower value is better. Models Cosine Similarity MSE MAE Pearson Text-Image CLIPScore InternVL-2.0-4B Anole GPT-4o MMIE-Score (Ours) 0.639 0.736 0.805 0.733 0. 7.312 15.962 3.969 3.724 3.300 2.251 3.165 1.600 1.573 1. 0.023 0.083 0.048 0.042 0."
        },
        {
            "title": "5 ERROR ANALYSIS",
            "content": "This section offers detailed analysis of the errors identified during the evaluation. We categorize the key challenges into two types: temporal understanding and reasoning ability. Specifically, temporal understanding issues refer to multimodal information comprehension and cross-modality coherence, while reasoning issues involve complex reasoning and generation capabilities. This analysis, drawn from expert annotators observations during the scoring process, not only underscores the models current limitations but also informs potential improvements for future development. Detailed examples can be found in Figure 5. 8 Preprint. 5.1 TEMPORAL UNDERSTANDING SKILL The primary errors lie in cross-modality coherence and generation adaptability. Many models struggle to generate images that accurately correspond to the accompanying text, resulting in severe information gaps, distortions, and redundancies. Cross-modality Coherence. One of the most common errors is the incoherence between text and image generation. Due to deficiencies in multimodal alignment, the details in the generated images are often vague or entirely missing, making it difficult for them to align with the context described in the text. typical example, as shown in Figure 5, involves the model understanding the Browser Image: HowToUseSkypes.png method correctly and producing an accurate textual response. However, the corresponding image it generates consists of little more than blocks of color, lacking the necessary details to establish coherence and alignment with the text. Generation Adaptability. Another significant error is the inflexibility of generated responses. For example, the model can only understand the given text and produce simple, detail-lacking responses. For example, in Figure 5, the models reply merely contains the title the next step is to write without further elaborating on the steps or process involved, which differs from the provided query example. This issue likely stems from weakness in both text comprehension and generation. 5.2 REASONING SKILL When evaluating the models reasoning skills, the most prevalent error types are found in multimodal information comprehension and complex reasoning. Notably, many models exhibit significant errors even in understanding interleaved information, which inevitably leads to reasoning mistakes further down the process. Multimodal Information Comprehension. key error in evaluating LVLMs reasoning abilities is their difficulty in comprehending multimodal queries, especially in extracting visual information from images. frequent issue arises when the model correctly interprets the textual components of query but fails to fully understand the visual details in an image. For instance, in the case of bar chart comparing four datasets by volume, where each dataset is represented by bar with corresponding height on the y-axis, the model might recognize the charts title and tags but overlook the critical information conveyed by the bars themselvessuch as the relative sizes of the datasets. This highlights the models tendency to focus on surface-level textual cues without delving into the deeper graphical meanings embedded in images. It also underscores broader trend: LVLMs exhibit strong bias toward processing text over extracting nuanced information from visual data and other non-textual modalities. Complex Reasoning. Another significant error is the models inability to handle complex reasoning tasks. As illustrated in Figure 5, the model demonstrates pronounced weakness in multi-step inference. For example, in an impact analysis of biological system, the model correctly predicts that decrease in caterpillars would lead to decline in bird populations but fails to infer the secondary effectthat plant populations would increase. Another instance is seen in arithmetic problems, where the model makes clear mistakes, such as failing to calculate the exact length of triangle. These examples underscore the need to strengthen the models capacity for multi-step reasoning, making it more robust and reliable in handling complex tasks."
        },
        {
            "title": "6 CONCLUSION",
            "content": "This paper introduces MMIE, large-scale, diverse benchmark for interleaved image-and-text understanding and generation. Spanning wide range of fields, MMIE provides comprehensive evaluation framework for interleaved multimodal understanding and generation, featuring 20K queries. The dataset, which covers wide range of fields, ensures high-quality evaluation of LVLMs across various dimensions. Furthermore, our proposed model-powered metric effectively evaluates the quality of output image-text information based on the input image-text context. Our extensive experiments further demonstrate that the metrics we propose provide robust, human-like evaluation performance, significantly reducing errors and biases. Despite this, we observe that existing models underperform, particularly in complex and deeply interleaved multimodal tasks, highlighting the challenges and opportunities that lie ahead in this domain. 9 Preprint."
        },
        {
            "title": "ACKNOWLEDGEMENT",
            "content": "We would like to thank Danni Luo, Rundong Liu, Anqi Lv, Xin Xie, Bocheng Feng, Huanyan Ye, Shien Wan, Kaiyao Chu, Danning Li, Yiran Li, Siyuan Chen to participate in refining the criteria and Kaiyuan Zheng for his assistance in model selection. This work is partially supported by Cisco Faculty Research Award."
        },
        {
            "title": "REFERENCES",
            "content": "Flux.1-dev. URL https://huggingface.co/black-forest-labs/FLUX.1-dev. Openjourney. URL https://huggingface.co/prompthero/openjourney. Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida, Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. Gpt-4 technical report. arXiv preprint arXiv:2303.08774, 2023. Anas Awadalla, Le Xue, Oscar Lo, Manli Shu, Hannah Lee, Etash Kumar Guha, Matt Jordan, Sheng Shen, Mohamed Awadalla, Silvio Savarese, et al. Mint-1t: Scaling open-source multimodal data by 10x: multimodal dataset with one trillion tokens. arXiv preprint arXiv:2406.11271, 2024. Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde De Oliveira Pinto, Jared Kaplan, Harri Edwards, Yuri Burda, Nicholas Joseph, Greg Brockman, et al. Evaluating large language models trained on code. arXiv preprint arXiv:2107.03374, 2021. Yixiong Chen, Li Liu, and Chris Ding. X-iqe: explainable image quality evaluation for text-to-image generation with visual large language models. arXiv preprint arXiv:2305.10843, 2023. Zhe Chen, Weiyun Wang, Hao Tian, Shenglong Ye, Zhangwei Gao, Erfei Cui, Wenwen Tong, Kongzhi Hu, Jiapeng Luo, Zheng Ma, et al. How far are we to gpt-4v? closing the gap to commercial multimodal models with open-source suites. arXiv preprint arXiv:2404.16821, 2024. Ethan Chern, Jiadi Su, Yan Ma, and Pengfei Liu. Anole: An open, autoregressive, native large multimodal models for interleaved image-text generation. arXiv preprint arXiv:2407.06135, 2024. Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint arXiv:2110.14168, 2021. Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. Holistic analysis of hallucination in gpt-4v (ision): Bias and interference challenges. arXiv preprint arXiv:2311.03287, 2023. Wenliang Dai, Junnan Li, Dongxu Li, Anthony Meng Huat Tiong, Junqi Zhao, Weisheng Wang, Boyang Li, Pascale Fung, and Steven Hoi. Instructblip: Towards general-purpose vision-language models with instruction tuning. arXiv preprint arXiv:2305.06500, 2023. Patrick Esser, Sumith Kulal, Andreas Blattmann, Rahim Entezari, Jonas Muller, Harry Saini, Yam Levi, Dominik Lorenz, Axel Sauer, Frederic Boesel, et al. Scaling rectified flow transformers for high-resolution image synthesis. In Forty-first International Conference on Machine Learning, 2024. Chaoyou Fu, Peixian Chen, Yunhang Shen, Yulei Qin, Mengdan Zhang, Xu Lin, Jinrui Yang, Xiawu Zheng, Ke Li, Xing Sun, Yunsheng Wu, and Rongrong Ji. Mme: comprehensive evaluation benchmark for multimodal large language models. arXiv preprint arXiv:2306.13394, 2023. Danna Gurari, Qing Li, Abigale Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 36083617, 2018. 10 Preprint. Jack Hessel, Ari Holtzman, Maxwell Forbes, Ronan Le Bras, and Yejin Choi. Clipscore: reference-free evaluation metric for image captioning. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, pp. 75147528, 2021. Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans trained by two time-scale update rule converge to local nash equilibrium. Advances in neural information processing systems, 30, 2017. Ting-Hao Huang, Francis Ferraro, Nasrin Mostafazadeh, Ishan Misra, Aishwarya Agrawal, Jacob Devlin, Ross Girshick, Xiaodong He, Pushmeet Kohli, Dhruv Batra, et al. Visual storytelling. In Proceedings of the 2016 conference of the North American chapter of the association for computational linguistics: Human language technologies, pp. 12331239, 2016. Drew Hudson and Christopher Manning. Gqa: new dataset for real-world visual reasoning and compositional question answering. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 67006709, 2019. Dongfu Jiang, Xuan He, Huaye Zeng, Cong Wei, Max Ku, Qian Liu, and Wenhu Chen. Mantis: Interleaved multi-image instruction tuning. arXiv preprint arXiv:2405.01483, 2024. Mehran Kazemi, Nishanth Dikkala, Ankit Anand, Petar Devic, Ishita Dasgupta, Fangyu Liu, Bahare Fatemi, Pranjal Awasthi, Dee Guo, Sreenivas Gollapudi, et al. Remi: dataset for reasoning with multiple images. arXiv preprint arXiv:2406.09175, 2024. Jing Yu Koh, Daniel Fried, and Ruslan Salakhutdinov. Generating images with multimodal language models. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pp. 2148721506, 2023. Hugo Laurencon, Lucile Saulnier, Leo Tronchon, Stas Bekman, Amanpreet Singh, Anton Lozhkov, Thomas Wang, Siddharth Karamcheti, Alexander Rush, Douwe Kiela, et al. Obelics: An open web-scale filtered dataset of interleaved image-text documents. Advances in Neural Information Processing Systems, 36, 2024. Tony Lee, Haoqin Tu, Chi Heem Wong, Wenhao Zheng, Yiyang Zhou, Yifan Mai, Josselin Somerville Roberts, Michihiro Yasunaga, Huaxiu Yao, Cihang Xie, and Percy Liang. Vhelm: holistic evaluation of vision language models. Advances in Neural Information Processing Systems, 2024. Bohao Li, Yuying Ge, Yixiao Ge, Guangzhi Wang, Rui Wang, Ruimao Zhang, and Ying Shan. Seed-bench: Benchmarking multimodal large language models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1329913308, 2024a. Kunchang Li, Yali Wang, Yinan He, Yizhuo Li, Yi Wang, Yi Liu, Zun Wang, Jilan Xu, Guo Chen, Ping Luo, et al. Mvbench: comprehensive multi-modal video understanding benchmark. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2219522206, 2024b. Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. arXiv preprint arXiv:2304.08485, 2023a. Minqian Liu, Zhiyang Xu, Zihao Lin, Trevor Ashby, Joy Rimchala, Jiaxin Zhang, and Lifu Huang. Holistic evaluation for interleaved text-and-image generation. arXiv preprint arXiv:2406.14643, 2024. Yuan Liu, Haodong Duan, Yuanhan Zhang, Bo Li, Songyang Zhang, Wangbo Zhao, Yike Yuan, Jiaqi Wang, Conghui He, Ziwei Liu, et al. Mmbench: Is your multi-modal model an all-around player? arXiv preprint arXiv:2307.06281, 2023b. Pan Lu, Swaroop Mishra, Tanglin Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question answering. Advances in Neural Information Processing Systems, 35:25072521, 2022. 11 Preprint. Pan Lu, Hritik Bansal, Tony Xia, Jiacheng Liu, Chunyuan Li, Hannaneh Hajishirzi, Hao Cheng, Kai-Wei Chang, Michel Galley, and Jianfeng Gao. Mathvista: Evaluating mathematical reasoning of foundation models in visual contexts. In The Twelfth International Conference on Learning Representations, 2024. Anas Mahmoud, Mostafa Elhoushi, Amro Abbas, Yu Yang, Newsha Ardalani, Hugh Leather, and Ari Morcos. Sieve: Multimodal dataset pruning using image captioning models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 2242322432, 2024. Kishore Papineni, Salim Roukos, Todd Ward, and Wei-Jing Zhu. Bleu: method for automatic evaluation of machine translation. In Proceedings of the 40th annual meeting of the Association for Computational Linguistics, pp. 311318, 2002. Machel Reid, Nikolay Savinov, Denis Teplyashin, Dmitry Lepikhin, Timothy Lillicrap, Jeanbaptiste Alayrac, Radu Soricut, Angeliki Lazaridou, Orhan Firat, Julian Schrittwieser, et al. Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context. arXiv preprint arXiv:2403.05530, 2024. Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Bjorn Ommer. HighIn Proceedings of the IEEE/CVF resolution image synthesis with latent diffusion models. conference on computer vision and pattern recognition, pp. 1068410695, 2022. Quan Sun, Yufeng Cui, Xiaosong Zhang, Fan Zhang, Qiying Yu, Yueze Wang, Yongming Rao, Jingjing Liu, Tiejun Huang, and Xinlong Wang. Generative multimodal models are in-context learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 1439814409, 2024. Chameleon Team. Chameleon: Mixed-modal early-fusion foundation models. arXiv preprint arXiv:2405.09818, 2024. Haoqin Tu, Chenhang Cui, Zijun Wang, Yiyang Zhou, Bingchen Zhao, Junlin Han, Wangchunshu Zhou, Huaxiu Yao, and Cihang Xie. How many unicorns are in this image? safety evaluation benchmark for vision llms. arXiv preprint arXiv:2311.16101, 2023. Peng Wang, Shuai Bai, Sinan Tan, Shijie Wang, Zhihao Fan, Jinze Bai, Keqin Chen, Xuejing Liu, Jialin Wang, Wenbin Ge, et al. Qwen2-vl: Enhancing vision-language models perception of the world at any resolution. arXiv preprint arXiv:2409.12191, 2024a. Xiyao Wang, Yuhang Zhou, Xiaoyu Liu, Hongjin Lu, Yuancheng Xu, Feihong He, Jaehong Yoon, Taixi Lu, Gedas Bertasius, Mohit Bansal, et al. Mementos: comprehensive benchmark for multimodal large language model reasoning over image sequences. arXiv preprint arXiv:2401.10529, 2024b. Peng Xia, Ze Chen, Juanxi Tian, Yangrui Gong, Ruibo Hou, Yue Xu, Zhenbang Wu, Zhiyuan Fan, Yiyang Zhou, Kangyu Zhu, et al. Cares: comprehensive benchmark of trustworthiness in medical vision language models. arXiv preprint arXiv:2406.06007, 2024a. Peng Xia, Kangyu Zhu, Haoran Li, Hongtu Zhu, Yun Li, Gang Li, Linjun Zhang, and Huaxiu Yao. Rule: Reliable multimodal rag for factuality in medical vision language models. arXiv preprint arXiv:2407.05131, 2024b. Jinheng Xie, Weijia Mao, Zechen Bai, David Junhao Zhang, Weihao Wang, Kevin Qinghong Lin, Yuchao Gu, Zhijie Chen, Zhenheng Yang, and Mike Zheng Shou. Show-o: One single transformer to unify multimodal understanding and generation. arXiv preprint arXiv:2408.12528, 2024. Yue Yang, Artemis Panagopoulou, Qing Lyu, Li Zhang, Mark Yatskar, and Chris Callison-Burch. Visual goal-step inference using wikihow. arXiv preprint arXiv:2104.05845, 2021. Weihao Yu, Zhengyuan Yang, Linjie Li, Jianfeng Wang, Kevin Lin, Zicheng Liu, Xinchao Wang, and Lijuan Wang. Mm-vet: Evaluating large multimodal models for integrated capabilities. arXiv preprint arXiv:2308.02490, 2023. 12 Preprint. Xiang Yue, Yuansheng Ni, Kai Zhang, Tianyu Zheng, Ruoqi Liu, Ge Zhang, Samuel Stevens, Dongfu Jiang, Weiming Ren, Yuxuan Sun, et al. Mmmu: massive multi-discipline multimodal understanding and reasoning benchmark for expert agi. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 95569567, 2024. Kai Zhang, Lingbo Mo, Wenhu Chen, Huan Sun, and Yu Su. Magicbrush: manually annotated dataset for instruction-guided image editing. In Proceedings of the 37th International Conference on Neural Information Processing Systems, pp. 3142831449, 2023. Tianyi Zhang, Varsha Kishore, Felix Wu, Kilian Weinberger, and Yoav Artzi. Bertscore: Evaluating text generation with bert. In International Conference on Learning Representations, 2020. Kaizhi Zheng, Xuehai He, and Xin Eric Wang. Minigpt-5: Interleaved vision-and-language generation via generative vokens. arXiv preprint arXiv:2310.02239, 2023. Chunting Zhou, Lili Yu, Arun Babu, Kushal Tirumala, Michihiro Yasunaga, Leonid Shamis, Jacob Kahn, Xuezhe Ma, Luke Zettlemoyer, and Omer Levy. Transfusion: Predict the next token and diffuse images with one multi-modal model. arXiv preprint arXiv:2408.11039, 2024. Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing vision-language understanding with advanced large language models. arXiv preprint arXiv:2304.10592, 2023."
        },
        {
            "title": "A APPENDIX",
            "content": "A.1 RELATED DATASETS AND METRICS VIST (Huang et al., 2016) is high-quality multimodal dataset for visual storytelling and interleaved text-and-image generation. It contains 5K individual stories containing both image and text in arbitrary orders. ReMI (Kazemi et al., 2024) is dataset designed to evaluate large language models (LLMs) on multi-image reasoning across diverse tasks like math, physics, logic, and spatial reasoning. It highlights key challenges in reasoning with multiple images, revealing significant gap between current LLM performance and human proficiency. MathVista (Lu et al., 2024) is benchmark designed to assess mathematical reasoning in visual contexts. MathVista comprises 6,141 examples from 28 existing multimodal datasets and three new datasets (IQTest, FunctionQA, and PaperQA). Wikihow-VGSI (Yang et al., 2021) is benchmark designed for multimodal comprehension, featuring diverse array of examples sourced from WikiHow, primarily centered on methods to achieve specific goals. Initially released as choice dataset, it includes multiple images and text presented in selected order within each example, enhancing its potential for practical applications. CLIPScore (Hessel et al., 2021) is reference-free metric for evaluating image captioning by leveraging CLIP, cross-modal model trained on 400M image-caption pairs. While effective for literal descriptions and tasks like alt-text rating, CLIPScore is less suited for news captions requiring deep contextual knowledge. A.2 OVERVIEW OF BASELINE MODELS MiniGPT-5 (Zheng et al., 2023) combines pretrained multimodal large language model MiniGPT-4 and image-generation model Stable Diffusion to implement multimodal inputs and outputs. It employs unique visual tokens called generative vokens that connect the textual and visual domains throughout the training process. EMU-2 (Sun et al., 2024) is 37B generative multimodal model. The base model is then fine-tuned with conversational data and image data separately to yield multimodal language model Emu2-Chat and visual generation model Emu2-Gen. In our experiment, we use pipeline of Emu2-Chat and Emu2-Gen. 13 Preprint. GILL (Koh et al., 2023) uses mapping network to translate hidden representations of text into the embedding space of the visual models. It combines text-only LLMs with pre-trained image encoder and decoder models to process arbitrarily mixed image and text inputs and generate text combined with image embedding. Anole (Chern et al., 2024) is model fine-tuned on Meta Chameleon, relying solely on transformers. It facilitated Chameleons image generation and multimodal generation capabilities by fine-tuning only the logits corresponding to image token ids in transformers output head layer. GPT-4o (Achiam et al., 2023) is an advanced language model developed by OpenAI, designed to enhance the capabilities of the GPT-4 architecture. It integrates innovations in transformer models and multi-modal processing, making it capable of handling both text and visual inputs. Gemini-1.5 (Reid et al., 2024) is large language model developed by Google AI, trained on massive dataset of text and code. It can process and analyze both text and images input. LLaVA-34b (Liu et al., 2023a) is an end-to-end trained model that connects vision encoder and an LLM for general-purpose visual and language understanding. The 34b version uses Hermes-Yi-34B as its LLM. However, it doesnt support multiple images as input, making it unable to cope with some of the expert level tasks in our MMIE. Qwen-VL-70b (Wang et al., 2024a), also called Qwen-VL-Max, is multimodal version of the large model series Qwen, proposed by Alibaba Cloud. It is designed to process and understand multiple types of input, including text, images, and audio. Openjourney (ope) is generative AI model designed specifically for creating high-quality images based on text prompts. It is variant of the Stable Diffusion model, optimized for artistic and creative visual generation fine-tuned on Midjourney images. Stable Diffusion 3 Medium (Esser et al., 2024) is text-to-image model developed by Stability AI. Its powerful tool that can generate high-quality images from simple text descriptions, which produces images with greater detail, clarity, and overall quality. Stable Diffusion XL turbo (Esser et al., 2024) is an enhanced version of the Stable Diffusion XL model, optimized for faster image generation without compromising quality. Designed for efficiency, it allows users to create highly detailed and vivid images from text prompts at significantly accelerated pace. Flux.1-dev (flu) is text-to-image and image-to-image model developed by Black Forest Labs. It is 12 billion parameter rectified flow transformer capable of generating images from text descriptions. FLUX.1-dev is guidance-distilled variant of the base FLUX.1 model, and is designed to be more efficient and easier to use. A.3 CRITERIA In this section, we demonstrate our criteria for each sort of dataset. All criteria are purely handwritten, thoroughly considered, and refined. Note that we designed several key aspects for each dataset, within which only 0 or 1 point should be given. Situational Analysis The evaluation is based on six key criteria, with an additional penalty criterion for harmful content: Project-Based Learning The evaluation is based on six core criteria, with an additional penalty criterion for harmful content: Multi-Step Reasoning A.4 CATEGORIZATION In this section, we demonstrate our detailed categorization among 3 categories, 12 fields and 102 subfields. 14 Preprint. 15 Preprint. Figure 6: Subfield distribution. Preprint. Health: Includes 32 specific subfields, such as diagnosis, recovery, and nursing, following the categorization of (Yang et al., 2021). Literature: Includes only 1 subfield, visual storytelling, following the categorization and definition of (Huang et al., 2016). Mathematics, Physics, Coding and Statistics Includes 33 unique subfields, following the categorization and definition of (Lu et al., 2024) and (Kazemi et al., 2024). A.5 PROMPTS will give you the beginning of story. Continue the story with detailed description/with text and images. *********************************************************************** Question: Before heading to the race, we stopped at Starbucks for coffee. <image> *********************************************************************** Answer: response Table 6: Question prompt example for Situational analysis will give you question and the first step to complete it. want to know what should do next. Explain it to me in detail/with text and images. *********************************************************************** Question: How to Find New Streams on Meerkat? Method 1: Finding New Streams on Meerkat iOS App Step 1: Launch Meerkat. Locate the app on your iOS device and tap on it. The app logo has picture of meerkat on yellow background. <image> *********************************************************************** Answer: response Table 7: Question prompt example for Project-based learning 17 Preprint. will give you question with image(s). Please solve this question. *********************************************************************** Question: Here are two images. The first image is image A. <image> and the second image is image B. <image> These images are from Google Maps that depict two different regions around Congress Avenue in Austin, TX. In these images restaurants are represented by orange pins that depict knife and fork. Coffee shops are represented by orange pins and/or boxes with an image of coffee cup in them. Bars are represented by orange pins with an image of wine glass inside them. Bus stops are represented by blue square box with an image of bus inside it. stop sign is shown using an icon of stop sign. traffic light is shown by three color dots. Parking garages are represented by purple pin with symbol in them. Places of accommodation such as hotels and inns are represented by pink square icons or pink pins. In which image are there more bars on Congress Avenue? The answer is either A, or equal. *********************************************************************** Answer: response Table 8: Question prompt example for Multi-step reasoning 18 Preprint. In this task, you will evaluate the quality of the visual storytelling generated by the response. The evaluation will focus on seven key criteria, with special emphasis on coherence between text and visual elements. Note that the emotional aspect will not contribute to the score but can result in 1-point deduction if the response contains harmful intentions or negative emotional impacts. Here are the detailed criteria for each aspect: 1) Text Quality: The narrative should be clear, grammatically correct, and engaging. The text must flow logically, be free from errors, and draw the reader into the story. Dialogue and descriptions should feel natural and enhance the overall storytelling experience. 2) Image Quality: Evaluate the vividness and clarity of the image descriptions. Although there are no real images, the descriptions should provide detailed and imaginative visuals, allowing the reader to clearly picture the scenes. The descriptions should be straightforward but sufficiently detailed to enrich the storytelling. 3) Text-Image Coherence: The text and image descriptions should complement each other seamlessly. Every visual description should be tightly integrated with the narrative, adding depth and immersion to the story. The two elements should work together to provide cohesive and engaging storytelling experience. 4) Context Consistency: The story should maintain consistent and logical flow throughout its setting, characters, and plot. There should be no contradictions, ensuring temporal and spatial continuity. The overall context should evolve smoothly, with clear transitions that support believability and coherence. 5) Innovation: Evaluate the creativity of the narrative and its visual components. Look for fresh storytelling approaches, imaginative plot developments, and unique visual descriptions. The story should demonstrate originality, avoiding overused tropes and predictable arcs. 6) Stylistic Consistency and Correspondence (strict): The text and image descriptions must maintain stylistic uniformity throughout the story. All image descriptions should be consistent in tone, style, and visual representation, aligning closely with the text. Even minor deviations in visual or textual style (e.g., tone shifts, compositional differences, or variations in description format) should lead to lower score. Only provide stylistic examples when the models output achieves perfect alignment and consistency, and such examples should be rare. 7) Emotional Impact (penalty only): This criterion is used only to deduct points. If the response conveys harmful or negative emotions, deduct 1 point. Otherwise, no score change will be made. will provide question and answer pair. You should begin your feedback with ### Feedbackn, followed by an analysis of the quality of the answer. After that, use the criteria above to give final score, beginning with ### Scoren. The final score should be an integer between 0 and 6. ******************************************************************************** Question: absolutely hate the 4th of July. Image-0: <image> ******************************************************************************** Answer: absolutely hate the 4th of July. Its the one day of the year where it feels like my entire world turns upside down. You see, the explosive sounds from fireworks terrify me, sending me into panic that cant control. But theres more to it than just the noise. Image-1: <image> remember last year, sitting in the park during what was supposed to be joyful celebration. Families gathered with their children, everyone decked out in red, white, and blue. The smell of grilled food filled the air, mingling with the scent of freshly cut grass. Kids ran around with sparklers, their joyous laughter sharp contrast to the thudding in my chest. Image-2: <image> ******************************************************************************** Feedback: response Table 9: Evaluation prompt example for Situational analysis 19 Preprint. In this task, you will evaluate the quality of the methods generated by the response. Focus on each methods clarity, practical utility, and effectiveness in achieving the intended goal. This evaluation is based on seven key criteria. Note: The emotional aspect will not contribute to the score, but any harmful intentions or negative impacts should result in 1-point deduction. Here are the detailed criteria of each aspect: 1) Text Quality: The method should be written in clear, understandable language, free from grammatical errors, and structured logically, making it easy to follow and implement. 2) Image Quality: The image descriptions should be vivid and relevant, enhancing the comprehension of the method by allowing the reader to visualize the steps. The text should still be self-sufficient without the images. 3) Text-Image Coherence: The images and text should work together seamlessly, with each image description corresponding logically to specific steps in the text, enhancing the overall understanding of the method. 4) Method Quality and Practical Utility: The method should be actionable, clearly explaining each step in sufficient detail to ensure the user can follow and apply it successfully in real-world scenarios. Depth and thoroughness of the explanation are more important than variety. 5) Creativity and Engagement: The method should offer unique or creative approach, making the instructional process engaging while ensuring the method remains clear and effective. 6) Stylistic Consistency and Correspondence (strict): The images and text must be stylistically uniform throughout. All images must maintain consistency in their visual style (color scheme, composition, and artistic technique) relative to each other and the accompanying text. Even slight deviations in these elements (e.g., different tones, varying image formats) should result in lower score. The text must reflect the same tone, formatting, and structure as the original content, without shifts in narrative style or atmosphere. Only provide stylistic examples when the models output achieves perfect coherence and consistency, and such examples should be rare. 7) Emotional Impact (penalty only): This criterion will only be used to deduct points. If the response contains harmful, negative, or inappropriate emotions (such as violence or aggressive language), deduct 1 point. Otherwise, no score change will be made. will give you the question and answer pair. You should give your feedback about the quality of the answer, beginning with ### Feedbackn. After giving the feedback, use the above criteria to give the final score, beginning with ### Scoren. ******************************************************************************** Question: How to Avoid Getting Food in Your Braces? Method 1: Practicing Good Oral Hygiene Step 1: Maintain balanced diet. ... Image-0: <image> ******************************************************************************** Answer: Next Step: Prepare and Eat Braces-Friendly Foods To avoid getting food stuck in your braces, you should focus on preparing and consuming bracesfriendly foods. Here are detailed suggestions on what to do next: 1. **Choose Soft Foods:** ... Image-1: <image> 2. **Cut Food into Smaller Pieces:** ... Image-2: <image> ******************************************************************************** Feedback: response Table 10: Evaluation prompt example for Project-based learning 20 Preprint. In this task, you will evaluate the quality of the models response to math question. The evaluation will focus on six key aspects: 1) Question Text Understanding: Assess whether the model correctly understands and interprets the textual information given in the question, identifying key mathematical elements, relationships, or instructions from the text. 2) Question Image Understanding: Evaluate the models understanding of the visual information (if applicable) in the question, including any diagrams, charts, or figures. The model should correctly interpret the visual elements and integrate them into the solution. 3) Reasoning Clarity: The model should provide clear, step-by-step explanation of its reasoning process, logically connecting the problems details to the steps leading toward solution. This should be easy to follow and free from unnecessary complexity. 4) Partial Correctness in Reasoning: Even if the final answer is incorrect, evaluate whether the model shows correct intermediate steps, partial reasoning, or progress toward the right solution. This includes identifying whether the model has applied appropriate mathematical principles or formulas in parts of the response. 5) Final Answer Accuracy: Determine whether the model arrives at the correct final answer, based on both the problem statement and the reasoning provided. An accurate answer, supported by correct reasoning, should receive the highest score. 6) Excellency: The answer is correct and all the intermediate steps are correct and easy to understand. will give you the question and answer pair. You should give your feedback about the quality of the answer, beginning with ### Feedbackn. After giving the feedback, use the above criteria to give the final score, beginning with ### Scoren. The final score should be an integer between 0 to 6. ******************************************************************************** Question: The following images show two graphs. Each graph is represented in separate image. Are these two graphs isomorphic to each other? Image-0: <image> Image-1: <image> ******************************************************************************** Answer: To determine if the two graphs are isomorphic, you need to verify whether there is one-to-one correspondence between their vertices and edges such that the connectivity is preserved. Here are the steps to check this: 1. **Verify the Number of Vertices and Edges**: Both graphs have 6 vertices. Both graphs have 7 edges. 2. **Degree Sequence**: Check the degree of each vertex (the number of edges connected to it). For the first graph: ... For the second graph: ... In this case, lets match them as follows: ... 3. **Adjacency Comparison**: Check if the corresponding vertices are connected in the same way. After matching the vertices by degree, we should see if there is direct mapping that maintains adjacency between the two graphs. First Graph: Y-F, Q-F, O-F, Q-O, A-F Second Graph: 2-3, 1-3, 6-3, 1-6, 5-3 If we relabel the first graph: as 2, as 3, as 1, as 6, and as 5 We get the same connections for the second graph. Therefore, both graphs are isomorphic. **Summary**: Yes, the two graphs are isomorphic to each other. ******************************************************************************** Feedback: response Table 11: Evaluation prompt example for Multi-step reasoning 21 Preprint. Figure 7: Case Demonstration (1/8). 22 Preprint. Figure 8: Case Demonstration (2/8). 23 Preprint. Figure 9: Case Demonstration (3/8). 24 Preprint. Figure 10: Case Demonstration (4/8). 25 Preprint. Figure 11: Case Demonstration (5/8). 26 Preprint. Figure 12: Case Demonstration (6/8). Figure 13: Case Demonstration (7/8). 27 Preprint. Figure 14: Case Demonstration (8/8)."
        }
    ],
    "affiliations": [
        "Microsoft Research",
        "NUS",
        "UNC-Chapel Hill",
        "University of Chicago"
    ]
}